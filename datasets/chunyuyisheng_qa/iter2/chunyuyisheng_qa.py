
import json
from tqdm import tqdm
import re
import random

pattern_en = [


    ]


pattern_zh = [
    [r'^(ç—…äºº|åŒ»ç”Ÿ)ï¼š[ğŸ¶ğŸ˜ğŸ˜ŸâœŒğŸ˜·ğŸ˜ªğŸ˜ğŸ™„ğŸ‘ƒğŸ™ŠğŸ¸ğŸ™„ğŸ·ğŸ¤—ğŸŒğŸ™‚ğŸ˜™ğŸ˜‹ğŸ˜ŒğŸ˜€ğŸš¶ğŸ¤­ğŸ¤£ğŸ ğŸ˜´ğŸ‘‹ğŸ˜³ğŸ˜ƒğŸ’”ğŸ‘ğŸ˜¯ğŸ”¥ğŸ˜”ğŸ˜±ğŸŒ¹ğŸ˜â¤ğŸ’ªğŸ¼ãŠ—ï¸ãŠ—ğŸ˜ğŸ‘€ğŸ‘ŒğŸ»ğŸ˜œğŸ˜¨ğŸ»ğŸ’ğŸ™ï¿¼ğŸ’ŠğŸ‘ğŸ˜ğŸ˜‚ğŸ˜„ğŸ˜›ğŸ˜ŠğŸ‘ŒğŸ˜“ğŸ˜­â˜ºğŸ˜˜ğŸ˜¢ğŸ€]{1,}$', r''],  # å•è¡Œåªæœ‰è¡¨æƒ…åŒ…
    [r'[ğŸ¶ğŸ˜ğŸ˜ŸâœŒğŸ˜·ğŸ˜ªğŸ˜ğŸ™„ğŸ‘ƒğŸ™ŠğŸ¸ğŸ™„ğŸ·ğŸ¤—ğŸ™‚ğŸŒğŸ˜™ğŸ˜‹ğŸ˜ŒğŸ˜€ğŸš¶ğŸ¤­ğŸ¤£ğŸ ğŸ˜´ğŸ‘‹ğŸ˜³ğŸ˜ƒğŸ’”ğŸ‘ğŸ˜¯ğŸ”¥ğŸ˜”ğŸ˜±ğŸŒ¹ğŸ˜â¤ğŸ’ªğŸ¼ãŠ—ï¸ãŠ—ğŸ˜ğŸ‘€ğŸ‘ŒğŸ»ğŸ˜œğŸ˜¨ğŸ»ğŸ’ğŸ™ï¿¼ğŸ’ŠğŸ‘ğŸ˜ğŸ˜ŠğŸ‘ŒğŸ˜„ğŸ˜“ğŸ˜‚â˜ºğŸ˜˜ğŸ˜›ğŸ˜¢ğŸ˜­ğŸ€]{1,}',r''], #è¡¨æƒ…åŒ…
    [r'^(åŒ»ç”Ÿï¼š.*(æ³¨å†Œ|äºŒç»´ç |è¯„ä»·|è‡ªåŠ¨å›å¤|å¥½è¯„|ä¸»é¡µ|åŠ æˆ‘|https?|å…³æ³¨æˆ‘).*)', r'åˆ é™¤1:<u>\1</u>'],   # åŒ¹é…ä¸€ä¸ªå¥å­åˆâ€˜åŒ»ç”Ÿï¼šâ€™å¼€å¤´å¥å­ä¸­æœ‰'è¯„ä»·'
    [r'(.*è¯­éŸ³æ–‡ä»¶.*)',r'åˆ é™¤2:<u>\1</u>'],    # è¯­éŸ³å¯¹è¯
    [r'(^(ç—…äºº|åŒ»ç”Ÿ)ï¼š\s{0,}$)',r'åˆ é™¤3:<u>\1</u>'],   # ç©ºè¡Œ åªæœ‰å‰é¢çš„äºº
    [r'(_\(:Ğ·\)âˆ \)_ï¼Œéº»è›‹äº†)',r''],   #
    [r'ï¼ˆ(\s?[ï¼Ÿã€‚ï¼Œï¼šï¼›]){1,5}ï¼‰',r''], # ç©ºæ‹¬å·

    [r'(\()?(\^_\^|&quot;â–”ã‰¨â–”|â•®(â•¯â–½â•°)â•­|T.T|ï¼ _ï¼ !!!)(\))?',r''],
    [r'(^(ç—…äºº|åŒ»ç”Ÿ)ï¼š(\/\:\:[^\u4e00-\u9fa5]*|emm{1,}ï¼Ÿ?|[^\u4e00-\u9fa5\d\.ï¼Ÿ]*)$)',r'åˆ é™¤4:<u>\1</u>'],
    [r'(\/\:\:[^\u4e00-\u9fa5]*|emm{1,}ï¼Ÿ?)',r'åˆ é™¤5:<u>\1</u>'],
    [r'(.*æ— æ³•æ˜¾ç¤º.*)',r'åˆ é™¤6:<u>\1</u>'],        # ä¸€äº›æ— æ³•æ˜¾ç¤ºçš„æ–‡ä»¶
    [r'ğŸ',r'é˜³'],
    [r'ğŸ¦‹',r'è´è¶'],
    [r'ğŸˆ¶',r'æœ‰'],
    [r'â­•',r'åœˆ'],
    [r'(^(ç—…äºº|åŒ»ç”Ÿ)ï¼šhttps?.*)',r'åˆ é™¤7:<u>\1</u>'],
]
Line_feed_rules = [
    [r'^(ç—…äºº|åŒ»ç”Ÿ)ï¼š',r'^(?!ç—…äººï¼š|åŒ»ç”Ÿï¼š).+']
]
class clean_pattern:
    def __init__(self):
        pass

    # é€šç”¨åˆ é™¤ä»æ–‡ç« å¼€å¤´åˆ°æŸä¸€æ®µ
    def delete_page_start(self, context):
        """
        é€šç”¨åˆ é™¤ä»æ–‡ç« å¼€å¤´åˆ°æ–‡ç« æŸä¸€æ®µç»“æŸ
        :param context: ä¼ å…¥åˆ†å‰²å¥½çš„æ–‡æœ¬contextï¼Œåˆ—è¡¨ç»“æ„
        :param end_patternçš„æ¯ä¸€é¡¹[0]: ä¼ å…¥åˆ°æŸæ®µç»“æŸåˆ é™¤ç‰¹å¾çš„æ­£åˆ™å½¢å¼
        :param end_patternçš„æ¯ä¸€é¡¹[1]: æ ¹æ®å½“æ®µæ˜¯å¦åˆ é™¤è®¾ç½®1æˆ–0
        :return: å¸¦æœ‰æ ‡ç­¾çš„context
        """
        # é¿å…é‡å¤åŠ æ ‡ç­¾ï¼Œç‰¹å¾æœ€å¥½åˆå¹¶ä¸º1-2æ¡ï¼Œå½“æ®µä¿ç•™ä¸€æ¡ï¼Œå½“æ®µåˆ é™¤ä¸€æ¡ã€‚
        end_pattern = [
            [r'(^[#\s]*(Abstract|ABSTRACT)ï¼š?\s*)', 0],
            [r'(^[#\s]*(Background|Introduction)ï¼š?\s*)', 0],

        ]
        end_index = 0
        for end in end_pattern:
            for index, item in enumerate(context):
                if re.search(end[0], item):
                    end_index = index + end[1]
            if end_index > 0:
                for i in range(0, end_index):
                    # context[i] = "é€šç”¨å¼€å¤´åˆ é™¤-1:<u>{}</u>".format(context[i])
                    context[i] = ""
        return context

    # é€šç”¨åˆ é™¤ä»æŸä¸€ä¸ªæ®µå¼€å§‹åˆ°æ–‡ç« ç»“æŸ
    def delete_page_ending(self, context):
        """
        é€šç”¨åˆ é™¤ä»æŸä¸€æ®µå¼€å§‹åˆ°æ–‡ç« ç»“æŸ
        :param context: åˆ†å‰²å¥½çš„æ–‡æœ¬ï¼Œåˆ—è¡¨ç»“æ„
        :param ending_startsçš„æ¯ä¸€é¡¹: ä»æŸæ®µå¼€å§‹åˆ é™¤å¼€å§‹çš„ç‰¹å¾çš„æ­£åˆ™å½¢å¼
        :return: åˆ—è¡¨ç»“æ„çš„æ–‡æœ¬
        """
        # é¿å…é‡å¤åŠ æ ‡ç­¾ï¼Œç‰¹å¾æœ€å¥½åˆå¹¶ä¸º1-2æ¡ï¼Œå½“æ®µä¿ç•™ä¸€æ¡ï¼Œå½“æ®µåˆ é™¤ä¸€æ¡ã€‚
        ending_starts = [
            [r'^[#\*]{0,4}\s?(References?ï¼š?|Funding( Sources| Statement| and Disclosure)?|Polls on Public|Ethics Approval|Author[s\' ]*Contribution|Acknowledge?ment|Conflicts? of [Ii]nterest|Source of (Support|Funding))s?[#\*]{0,4}\s{0,}($|\n)'],
            [r'ç—…äººï¼šåŒ»ç”Ÿå•Šï¼Œæˆ‘æœ€è¿‘å¿ƒæ…Œï¼Œå’‹å›äº‹ï¼Œè€æ˜¯åšå™©æ¢¦ï¼Œæ™šä¸Šç¡ä¸å¥½'],
        ]

        for start in ending_starts:
            references_started = False  # å®šä¹‰ä¸€ä¸ªåˆ é™¤referenceçš„å¼€å…³  åªè¦å‡ºç°å›ºå®šæ ¼å¼çš„è¡¨è¿°å°±å¯¹åé¢çš„å†…å®¹è¿›è¡Œåˆ é™¤
            for index, item in enumerate(context):
                if re.search(start[0], item.strip()):
                    references_started = True
                if references_started:
                    # context[index] = "é€šç”¨ç»“å°¾åˆ é™¤-1:<u>{}</u>".format(context[index])
                    context[index] = ''
        return context

    # é€šç”¨å¥ä¸­æŸä¸€éƒ¨åˆ†çš„åˆ é™¤
    def delete_page_middle(self, context):
        """
        é€šç”¨åˆ é™¤æŸä¸€éƒ¨åˆ†æ–¹æ³•
        :param context: åˆ‡åˆ†è¿‡çš„å†…å®¹ï¼Œåˆ—è¡¨ç»“æ„
        :param start_to_endçš„æ¯ä¸€é¡¹[0]: ä»æŸä¸€æ®µå¼€å§‹çš„ç‰¹å¾
        :param start_to_endçš„æ¯ä¸€é¡¹[1]: åˆ°æŸä¸€æ®µç»“æŸçš„ç‰¹å¾
        :param start_to_endçš„æ¯ä¸€é¡¹[2]: æ ¹æ®ç»“æŸæ®µæ˜¯å¦åˆ é™¤è®¾ç½®1æˆ–0
        :return: è¿”å›æ‰“è¿‡æ ‡ç­¾çš„åˆ—è¡¨
        """
        start_to_end = [
            # æ ·ä¾‹
            # [r'funding|...', r'Acknowledgments', 1],
        ]
        for middle in start_to_end:
            delete_line_index = []
            for index, item in enumerate(context):
                if re.search(middle[0], item):
                    satrt = [index, 0]
                    delete_line_index.append(satrt)
                if re.search(middle[1], item):
                    end = [index, 1]
                    delete_line_index.append(end)

            length = len(delete_line_index)
            if length >= 2:
                for i in range(1, length):
                    if delete_line_index[i - 1][1] < delete_line_index[i][1]:
                        start_index = delete_line_index[i - 1][0]
                        end_index = delete_line_index[i][0]
                        for i in range(start_index, end_index + middle[2]):
                            # context[i] = "é€šç”¨é—´è·åˆ é™¤-1:<u>{}</u>".format(context[i])
                            context[i] = ""

        return context

    # è§£å†³å¤šä½™æ¢è¡Œé—®é¢˜
    def more_line_feed(self, context, Line_feed_rules):
        """
        æœ¬æ–¹æ³•æ˜¯å®ç°æœ‰å¤šä½™æ¢è¡Œçš„è¿æ¥æ“ä½œã€‚éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„å…ƒç´ ä¸ºï¼š
        1. ä»…æœ‰ [current_line_rule]ï¼Œåªéœ€è¦åŒ¹é…å½“å‰è¡Œï¼›
        2. åŒ…å« [current_line_rule, next_line_rule]ï¼Œéœ€è¦åŒæ—¶åŒ¹é…å½“å‰è¡Œå’Œä¸‹ä¸€è¡Œã€‚
        :param context: æ®µè½å†…å®¹åˆ—è¡¨
        :param Line_feed_rules: æ¢è¡Œè§„åˆ™åˆ—è¡¨ï¼Œè§„åˆ™ä¸º [å½“å‰è¡Œçš„è§„åˆ™, ä¸‹ä¸€è¡Œçš„è§„åˆ™] æˆ–è€… [å½“å‰è¡Œçš„è§„åˆ™]
        :return: å¤„ç†åçš„ context
        """
        # å»é™¤ç©ºå­—ç¬¦ä¸²
        context = [item for item in context if item.strip() != ""]

        index = 0
        while index < len(context):
            item = context[index]
            stripped_item = item.strip()
            # æ£€æŸ¥æ‰€æœ‰æ¢è¡Œè§„åˆ™
            for line_feed_rule in Line_feed_rules:
                current_line_rule = line_feed_rule[0]
                # å¦‚æœè§„åˆ™é•¿åº¦ä¸º 2ï¼Œéœ€è¦åŒæ—¶åŒ¹é…å½“å‰è¡Œå’Œä¸‹ä¸€è¡Œ
                if len(line_feed_rule) == 2:
                    next_line_rule = line_feed_rule[1]
                    if index + 1 < len(context) and re.search(current_line_rule, stripped_item) and re.search(next_line_rule, context[index + 1].strip()):
                        # åˆå¹¶å½“å‰æ®µå’Œä¸‹ä¸€æ®µ
                        # context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ|" + context[index + 1].lstrip()
                        context[index] = item.rstrip() + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€æ®µ
                        del context[index + 1]
                        index = index-1
                        break

                # å¦‚æœè§„åˆ™é•¿åº¦ä¸º 1ï¼Œåªéœ€è¦åŒ¹é…å½“å‰è¡Œ
                elif len(line_feed_rule) == 1:
                    if index + 1 < len(context) and re.search(current_line_rule, stripped_item):
                        # åˆå¹¶å½“å‰æ®µå’Œä¸‹ä¸€æ®µ
                        # context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ|" + context[index + 1].lstrip()
                        context[index] = item.rstrip() + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€æ®µ
                        del context[index + 1]
                        # index = index-1
                        break

            index += 1
        return context

    # è§£å†³ç¼ºå°‘æ¢è¡Œé—®é¢˜
    def lack_line_feed(self, context, line_feed_rules):
        """
        æœ¬æ–¹æ³•æ˜¯å®ç°ç¼ºå°‘æ¢è¡Œçš„æ·»åŠ æ“ä½œã€‚éœ€è¦ä¼ å…¥ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„å…ƒç´ ä¸ºï¼š
        [current_line_rule, complete_rule]ï¼Œéœ€è¦ä¼ å…¥å½“å‰çš„å†…å®¹è§„åˆ™å’Œä¿®æ”¹åçš„å†…å®¹è§„åˆ™ã€‚
        :param context: æ®µè½å†…å®¹åˆ—è¡¨
        :param line_feed_rules: ç¼ºå°‘æ¢è¡Œçš„è§„åˆ™åˆ—è¡¨ï¼Œæ¯ä¸ªè§„åˆ™åŒ…å«å½“å‰è¡ŒåŒ¹é…çš„è§„åˆ™å’Œä¿®æ”¹åçš„å†…å®¹è§„åˆ™
        :return: å¤„ç†åçš„ context
        """
        index = 0
        while index < len(context):
            item = context[index]
            stripped_item = item.strip()
            # éå†æ¯ä¸ªæ¢è¡Œè§„åˆ™
            for line_feed_rule in line_feed_rules:
                current_line_rule = line_feed_rule[0]
                complete_rule = line_feed_rule[1]
                # å¦‚æœå½“å‰è¡Œç¬¦åˆ current_line_ruleï¼Œåˆ™æ ¹æ® complete_rule è¿›è¡Œå¤„ç†
                if re.search(current_line_rule, stripped_item):
                    # æ ¹æ® complete_rule æ’å…¥æ¢è¡Œæ“ä½œï¼Œå¯ä»¥æ˜¯æ¢è¡Œç¬¦æˆ–å…¶å®ƒæ ¼å¼
                    context[index] = re.sub(current_line_rule, complete_rule, stripped_item)
            # ç»§ç»­å¤„ç†ä¸‹ä¸€æ®µ
            index += 1
        return context


class speicalProces:
    def __init__(self):
        pass





def clean_text(context, lang):
    split_token = "\n"
    cp = clean_pattern()
    sp = speicalProces()
    context = context.split(split_token)

    # è‹¥æœ‰éœ€è¦å†è¡¥å……æ­£åˆ™å¹¶è°ƒç”¨ï¼Œæ­£åˆ™åœ¨å¯¹åº”çš„å‡½æ•°é‡Œè¡¥å……
    context = cp.delete_page_start(context)
    context = cp.delete_page_ending(context)
    # context = cp.delete_page_middle(context)
    context = cp.more_line_feed(context,Line_feed_rules)

    final_results = []
    for item in context:
        # 1.æ­£åˆ™
        if lang == "en":
            for pattern_item in pattern_en:
                src = pattern_item[0]
                tgt = pattern_item[1]
                item = re.sub(src, tgt, item)
        else:
            for pattern_item in pattern_zh:
                src = pattern_item[0]
                tgt = pattern_item[1]
                item = re.sub(src, tgt, item)
        final_results.append(item)
    for index,item in enumerate(final_results):
        print(index,item)
    context = split_token.join(final_results)

    return context

def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    # å¯¹å¤šæ ‡ç‚¹è¿›è¡Œæ›¿æ¢
    context = re.sub(r'([ã€‚ï¼Œï¼Ÿï¼›])(\s?[ï¼Ÿã€‚ï¼Œï¼šï¼›]){1,5}',r'\1',context)
    context = re.sub(r'([,\.?])(\s?[?,\.]){1,5}',r'\1',context)
    return context




fw = open(r"C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\reclean2_chunyuyisheng_qa.jsonl", "w", encoding="utf-8")
with open(r"C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\original_data\chunyuyisheng_qa_preformat.jsonl", "r", encoding="utf-8") as fs:
    lines = fs.readlines()
    for items in tqdm(lines):
        item = json.loads(items.strip())
        # if item["seq_id"] == "96dce1c6-4852-4356-b0de-b7fe1305243c":
        context = item["text"]
        lang = item["lang"]
        title = item["title"]
        context = re.sub(r'\xa0', r' ', context)
        context = clean_text(context, lang)
        context = post_process(context)
        # print(context)
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        print(item)
        fw.write(item + "\n")
# fw.close()
