# coding:utf-8
# https://github.com/shibing624/pycorrector?tab=readme-ov-file
import itertools
import json
import re
import time
from collections import defaultdict

from tqdm import tqdm
import sys
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
import transformers
sys.path.append("../../../")
# from basic_tools import sentence_correct
import threading
# çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—
from queue import Queue
import spacy
import jieba
import numpy as np
import torch.nn.functional as F
from transformers import AutoModelForSequenceClassification, AutoTokenizer


pattern_list_zh = [
    # fig cite
    # [r"([è§å’Œ]å›¾\s?\d+\s?[ã€‚ï¼Ÿï¼\.]?)", r"åˆ é™¤1ï¼š<u>\1</u>"],
    # å‚è€ƒå¼•ç”¨ï¼šæ•°å­—ç±»å‹ï¼ŒåŠ å…¥ç©ºæ ¼åšæ³›åŒ–ï¼š[1,2 - 3, 4, 5]
    [r'(\[\s?(\d{1,3}\s?[-â€“,ï¼Œ\.]+\s?)+\d+\s?\]\s?\*?)', r"åˆ é™¤2:<u>\1</u>"],
    # å‚è€ƒè¡¥å……ï¼Œä¸€äº›ç¼ºå°‘]çš„ï¼Œä»¥ç¬¦å·ä½œä¸ºå³è¾¹ç•Œ
    [r"(\[\s?\"?\s?(\d{1,3}\s?[-â€“,\.]+\s?)+\d?\s*\*?)([ã€‚\.,ï¼Œ\u4e00-\u9fa5a-zA-Z\(ï¼ˆ]\s?)", r"åˆ é™¤4:<u>\1</u>\3"],
    # [r"([\u4e00-\u9fa5]\s[^\u4e00-\u9fa5\d~ã€‚ï¼Ÿï¼]*?)(\s?\d+\s*?)(ã€‚|\s\.)", r"\1åˆ é™¤5:<u>\2</u>\3"],
    # [r"([\u4e00-\u9fa5])(\s*?\.\s*?\d+\s*?)([\(\u4e00-\u9fa5ã€‚\n])", r"\1åˆ é™¤6:<u>\2</u>\3"],
    # [r'([\u4e00-\u9fa5ã€‹)])(\s?\d+\s?)(ã€‚|\s\.)', r"\1åˆ é™¤7:<u>\2</u>\3"],

    # # å‚è€ƒåº”ç”¨çš„å¤æ‚æ ¼å¼ï¼šå­—æ¯(Ia,\nb)
    [r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]', r"åˆ é™¤3:<u>\1</u>"],

    # å¢åŠ æ¢è¡Œï¼šä¸¤ç§æ¦‚å¿µæƒ…å†µï¼Œä¸€ç§æ•°å­—å°æ ‡é¢˜ï¼Œä¸€ç§# æ ‡é¢˜
    [r'([:ï¼š]\s?)(\d+[\.ã€]\s?[\u4e00-\u9fa5])', r'\1\n\2'],
    [r'^(?!\n)(\d\.(\d{1,2}\.){0,10}\d?)( ?[^\d])', r'\n\1\3'],
    [r'(\u4e00-\u9fa5\s{3,}|ã€‚|ï¼Ÿ|ï¼)(\d\.(\d{1,2}\.){0,10}\d?)(\s{1,10}[^\da-zA-Z\|%])', r'\1\nå¢åŠ æ¢è¡Œ6:\2\3'],
    # [r'^(?!\n)([#â€¢]{1,4}\s{,4})', r'\nå¢åŠ æ¢è¡Œ7:_\1_'],
    [r'([ã€‚:ï¼š;.?ï¼Ÿ!ï¼"â€>ã€‹\u4e00-\u9fa5A-Za-z]) *(\d{1,2}\.)(?!( ?\d+))',r'\1\nå¢åŠ æ¢è¡Œ7:\2'],
    [r'[ã€‚\u4e00-\u9fa5A-Za-z] *(\d{1,2} *\.)(?! *[\d])', r'\n\1'],#åºå·æ¢è¡Œ
    [r'[ã€‚\u4e00-\u9fa50-9A-Za-z] *?([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€])', r'\nå¢åŠ æ¢è¡Œ9:\1'],#[ã€‚\u4e00-\u9fa50-9A-Za-z] *?([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.ã€])
    # -------å¢åŠ æ¢è¡Œï¼š(7)
    [r'([ ;ï¼›ã€‚ï¼ï¼Œ,\.] ?)([\(ï¼ˆ] *\d+ *[ï¼‰\)] ?)', r'\1\n\2'],
    [r'([ã€‚ï¼ï¼Ÿ] ?)(ã€[\u4e00-\u9fa5]*?ã€‘)', r'\1\nå¢åŠ æ¢è¡Œ2:\2'],
    [r'[^#\s] ?([\(ï¼ˆ] ?[â€”ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å-]+ ?[\)ï¼‰][^ã€‚])', r'\nå¢åŠ æ¢è¡Œ3:\1'],
    [r'([\u4e00-\u9fa5)ï¼‰])([A-E] ?\. ?[\u4e00-\u9fa5])',r'\1\n\2'],

    [r'[ï¼Œ,ã€‚ï¼Ÿï¼.?!]([ã€[ã€–] ?[\u4e00-\u9fa5]{2,5} ?(\d* )?[ã€—\]ã€‘])(.*)',r'\n\1\3'],
    [r'([ã€[ã€–] ?[\u4e00-\u9fa5]{2,5} ?(\d* )?[ã€—\]ã€‘])(.*)([ã€[ã€–] ?[\u4e00-\u9fa5]{2,5} ?(\d* )?[ã€—\]ã€‘])(.*)',r'\1\3\n\4\6'],


    # å¢åŠ å‚è€ƒé”™è¯¯çš„å‚è€ƒcase:[....]
    [r'(\[[â€¦\.\*\sâ€˜â€™\"]*?\])', r"åˆ é™¤8:<u>\1</u>"],
    [r'([[ã€][â€¦[\.\* â€˜â€™\"]*?[\dA-Za-z]+ ?\])(?=\n|$)', r"åˆ é™¤9:<u>\1</u>"],
    [r'(\s?\d+\s\[\s?\d+\s?)', r"åˆ é™¤10:<u>\1</u>"],


    [r'(\s?(ğŸ‘|â–¶|â—|Â©|Â®|\(?â†‘\)?|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â—†|â™•|áŠ¤|â™¦|Å¼|â|â–¡|âœ“|âœ”|â|â–²|â­|â—|â—|Û|\s__\s|ï¿¥|âˆ©|â€¡|â™‚|Â¿|â˜|âŒ’|ï¿½|\$|ï¿¼|à¤­|Â±|Â£|â™€|Å“|Â¾|Ã¤|ã‚‚|ã¸|\^|ã™|&|âª|Ã˜|ã€Œ|\sâ€¦\s)\s?)', r'åˆ é™¤28:<u>\1</u>'],
    [r'((ï¼ˆ|\(|ã€ˆ|\[) ?(å›¾|å›´|è¡¨|é™„|è§|è§è¡¨|å›°|å‚è§|è‡³å›¾|é™„å›¾) ?(\d+[-|â€“\.\/,\sã€~]+)+(\d+)?([^\n.ã€‚\u4e00-\u9fa5\d]*(ï¼‰|\)|ã€‰|\])))',
     r'åˆ é™¤29:<u>\1</u>'],
    [r'(å›¾|å›´|è¡¨|é™„|è§è¡¨|å›°|å‚è§|é™„å›¾)( *(\d+[-|â€“\.\/, ã€~]+)+(\d+)?([^\n.ã€‚\u4e00-\u9fa5\d]*))', r'\1åˆ é™¤30:<u>\2</u>'],
    [r'(?<!^)(?<!\n)([\[]( ?\d+[\s,\.\-\]ï¼‰ã€]+)+)',r'åˆ é™¤31:<u>\1</u>'],#[26]
    [r'(\(\s?[\-â€“]\d+\s?\))',r'åˆ é™¤32:<u>\1</u>'],
    [r'([\(ï¼ˆ]\s?\d{2,}\s?[\)ï¼‰]\n)', r'åˆ é™¤33:<u>\1</u>'],
    [r'(\b([a-zA-Z] ){3,}[a-zA-Z]?\b)', r'åˆ é™¤34:<u>\1</u>'],
    [r'(?<=\n)((##)? ?[A-Za-z.\-@]{1,3}|[\u4e00-\u9fa5])(?=\n|$)|(?<=^)((##)? ?[A-Za-z.\-@]{1,3}|[\u4e00-\u9fa5])(?=\n|$)',r'åˆ é™¤35:<u>\1\3</u>'],
    [r'(?<=ã€‚|\.)( ?\(\d+\))(?=\n|$)',r'åˆ é™¤36:<u>\1</u>'],
    [r'(?<=^)([A-Za-z ]*[Ff]i[Gg](ures?)?[A-Za-z 0-9]*\.?[0-9 ]*?)(?=\n|$)|(?<=\n|ã€‚|\.)([A-Za-z ]*[Ff]i[Gg](ures?)?[A-Za-z 0-9]*\.?[0-9 ]*?)(?=\n|$)',r'åˆ é™¤37:<u>\1\3</u>'],
    [r'(?<=^)([\(ï¼ˆã€\[]\d+[\.-]\d+[\)ï¼‰ã€‘\]])(?=\n|$)|(?<=\n)([\(ï¼ˆã€\[]\d+[\.-]\d+[\)ï¼‰ã€‘\]])(?=\n|$)',r'åˆ é™¤38:<u>\1\2</u>'],#(15.52) (16-12)
    [r'(Chapter[ 0-9]*(\.\d)*[ \n])',r'åˆ é™¤39:<u>\1</u>'],#Chapter 3
    [r'(?<=^)([\(ï¼ˆ] ?(å¼• *è‡ª *)?[\u4e00-\u9fa5]( *[\u4e00-\u9fa5] *){1,2} ?[ï¼‰\)]?)(?=\n|$)|(?<=\n)([\(ï¼ˆ] ?(å¼• *è‡ª *)?[\u4e00-\u9fa5]( *[\u4e00-\u9fa5] *){1,2} ?[ï¼‰\)]?)(?=\n|$)',r'åˆ é™¤40:<u>\1\4</u>'],#ï¼ˆäººåï¼‰
    [r'([,ï¼Œ:ï¼š;ï¼›ã€‚.~\-â€”])(( ?[,ï¼Œ:ï¼š;ï¼›ã€‚~] ?){2,})',r'\1åˆ é™¤å¤šä½™ç¬¦å·ï¼š<u>\2<u>'],
    [r'((?:https?:\/\/)?(?:www\.)[a-zA-Z0-9-]+(?:\.[a-zA-Z]{2,})+(?:\/[^\s]*)?)',r'åˆ é™¤ç½‘å€ï¼š<u>\1<u>'],
    [r'([\(ï¼ˆ][^\u4e00-\u9fa5\nA-Za-z%Ï€//]*?( ?[â€“\-= ï¼Œ,)] ?\d+){2,}[^\u4e00-\u9fa5\nA-Za-z%Ï€//]*?[ï¼‰\)])',r'åˆ é™¤æ— å…³æ•°å­—2ï¼š<u>\1<u>'],#( 2 53-3-3 )
    [r'([(ï¼ˆ][^\n()ï¼ˆï¼‰\\]*?å‚è§[^\n()ï¼ˆï¼‰\\]*?[ï¼‰)])',r'åˆ é™¤å‚è§ï¼š<u>\1<u>'],






    # å›¾å¼•ç”¨1
    [r'(^[ã€‚\n].*?è§å›¾[a-zA-Z\d]+.*?)[ã€‚\n]', r"åˆ é™¤11:<u>\1</u>"],
    # å›¾å¼•ç”¨2
    [r'([\(ï¼ˆ]\s?å›¾\s?[a-zA-Z\d]\s?[å’Œä¸,ï¼Œã€]?\s?å›¾\s?[a-zA-Z\d]\s?[\)ï¼‰])', r"åˆ é™¤12:<u>\1</u>"],
    # å›¾å¼•ç”¨3
    [r'(\(å›¾[a-zA-Z\d]+\))', r"åˆ é™¤13:<u>\1</u>"],
    # å›¾æ³¨1
    [r'(å›¾[a-zA-Z\d]+\s{0,2}[ã€‚\n]?)', r"åˆ é™¤14:<u>\1</u>"],
    # å›¾æ³¨2
    [r'(Fig[\s\.\d]+[a-zA-Z]*?[ã€‚\n]?)', r"åˆ é™¤15:<u>\1</u>"],



    # ç‰¹æ®Šcase(èƒŒæ™¯|é¡µçœ‰|é¡µè„š)
    # [r'(å°ç‹—(ç§‘ç ”|å…¬å¼€|ç©¶|è¯¾|è®¾|ç¨‹)+)', r"åˆ é™¤16:<u>\1</u>"],
    # [r'((å°ç‹—)?ç§‘ç ”(å…¬å¼€|ç©¶)(è¯¾|è®¾)ï¼Ÿ)', r"åˆ é™¤17:<u>\1</u>"],
    [r'^[ã€‚\n](.*?å¹´ç¬¬?\d+å·ç¬¬?\d+æœŸ[ã€‚\n])', r"åˆ é™¤18:<u>\1</u>"],
    [r'((æœ¬æ–‡)?ç¼–è¾‘: .*)', r"åˆ é™¤19:<u>\1</u>"],
    [r'(^[Aa]dd\s?[sS]?)', r"åˆ é™¤20:<u>\1</u>"],
    [r'(\.[Aa]dd[sS]\.?$)', r"åˆ é™¤21:<u>\1</u>"],
    # [r"(\s?[\xAC00-\xD7AF]\s?)",r"åˆ é™¤201:<u>\1</u>"],
    [r"([^\u4e00-\u9fa5ã€‚\n\]]*)(å°[ç‹—è¡—].{0,3}[ç§‘ç ”è‘¡å…¬å¼€è®¾è®¨ç¤¾èŠ±ç©¶æ‰€è¯¾ç³»å¿ƒ,\d\sa-zA-Z]+\s?)([^\u4e00-\u9fa5ã€‚\n]?)",
     r"\1åˆ é™¤23:<u>\2</u>\3"],
    # [r"(çš„?ç§‘ç ”ç©¶*å…¬*å…±*é€š*)", r"åˆ é™¤24:<u>\1</u>"],
    # [r"(\( *-* *\))", r"åˆ é™¤25:<u>\1</u>"],
    # åˆ é™¤é¡µç 
    [r'(â€¢?\s?\d{1,10}\s?â€¢)', r"åˆ é™¤26:<u>\1</u>"],
    [r'(?<=^)([-.]* ?[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+[\.-]?\d* ?[-.]*)(?=\n|$)|(?<=\n|ã€‚)([-.]* ?[\dâ‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]+[\.-]?\d* ?[-.]*)(?=\n|$)',r'åˆ é™¤æ— å…³æ•°å­—:<u>\1\2</u>'],

    # åˆ é™¤è‹±æ–‡å¼€å¤´å’Œç»“å°¾çš„
    # [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])",r"åˆ é™¤26:<u>\1</u>\2"],
    # [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$',r"\1åˆ é™¤22:<u>\2</u>"],
    [r"([^\u4e00-\u9fa5ã€‚\n]+ä¸‹è½½.*?)([\sã€‚\n][\u4e00-\u9fa5ã€‚\n])", r"åˆ é™¤27:<u>\1</u>\2"],

]

context_pattern = [
    [r"[ã€‚ï¼Œ?!,]([ã€‚ï¼Œ?!,])", r"\1"],

]


# ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-generic')
class speicalProces:
    def __init__(self):
        # self.sc = sentence_correct.SentenceCorrector()
        self.special_token = ["Â¶", ",", ".", "-", "|"]
        self.special_mapping = {
            "æœ¨": "æœ¯",
        }
        self.skip_token = ["è«–", "å¦„"]
        # self.en_nlp = spacy.load("zh_core_web_trf")
        # self.zh_nlp = spacy.load("zh_core_web_sm")
        self.zh_nlp = spacy.load("zh_core_web_sm")
        # self.ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-generic')
        self.zeroshot_classifier = transformers.pipeline("zero-shot-classification", model="deberta-v3-large-zeroshot-v2.0")

        self.model = AutoModelForSequenceClassification.from_pretrained("autonlp-Gibberish-Detector-492513457",
                                                                   use_auth_token=True)
        self.tokenizer = AutoTokenizer.from_pretrained("autonlp-Gibberish-Detector-492513457", use_auth_token=True)

    def fix_details(self, ditail_list):
        new_detail_list = []
        for detail_item in ditail_list:
            detail_pos_start = detail_item[2]
            detail_pos_end = detail_item[3]
            raw_token = detail_item[0]
            new_token = detail_item[1]
            if raw_token in ["è«–", "å¦„"]:
                continue
            if len(re.findall("\d", raw_token)) > 0:
                continue
            if len(re.findall(r"[^\u4e00-\u9fa5]", raw_token)) > 0:
                continue
            if raw_token in self.special_mapping:
                new_token = self.special_mapping[raw_token]
            new_detail_list.append([raw_token, new_token, detail_pos_start, detail_pos_end])
        return new_detail_list

    def dict_merge(self,xy_num, set_x, flag=True):

        sorted_keys = sorted(xy_num.keys())
        # åˆå§‹åŒ–
        merged_dict = defaultdict(float)
        current_key_group = []
        current_value_sum = 0
        # éå†æ’åºåçš„ keys å¹¶è¿›è¡Œåˆå¹¶
        for key in sorted_keys:
            if not current_key_group:
                current_key_group.append(key)
                current_value_sum += xy_num[key]
            else:
                if key - current_key_group[-1] <= 18:
                    current_key_group.append(key)
                    current_value_sum += xy_num[key]
                else:
                    # è®¡ç®—å½“å‰ç»„çš„å¹³å‡ key å’Œç´¯åŠ çš„ value
                    avg_key = sum(current_key_group) / len(current_key_group)
                    merged_dict[avg_key] = current_value_sum
                    # é‡ç½®ç»„
                    current_key_group = [key]
                    current_value_sum = xy_num[key]
        # æœ€åä¸€ç»„çš„å¤„ç†
        if current_key_group:
            avg_key = sum(current_key_group) / len(current_key_group)
            merged_dict[avg_key] = current_value_sum

        if flag:  # å·¦ä¾§  æŒ‰ç…§å€¼ï¼ˆvalueï¼‰çš„é™åºå’Œé”®ï¼ˆkeyï¼‰çš„å‡åºæ’åº
            res_left = dict(sorted(merged_dict.items(), key=lambda x: (-x[1], x[0])))
            # print(res_left)
            if len(res_left) >= 2:
                keys = list(res_left.keys())
                values = list(res_left.values())
                if values[0] - values[1] <= 6 and keys[1] < keys[0]:
                    keys[0], keys[1] = keys[1], keys[0]
                    values[0], values[1] = values[1], values[0]
                    res_left = dict(zip(keys, values))
            # first_key = next(iter(res_left.keys()))
            for x in res_left.keys():
                if x < set_x / 3 and res_left[x] >= 4:
                    first_key = x
                    break
                else:
                    first_key = 0
            # print(res_left)
        else:  # å³ä¾§  æŒ‰ç…§å€¼ï¼ˆvalueï¼‰çš„é™åºå’Œé”®ï¼ˆkeyï¼‰çš„é™åºæ’åº
            res_right = dict(sorted(merged_dict.items(), key=lambda x: (x[1], x[0]), reverse=True))
            # print(res_right)
            if len(res_right) >= 2:
                keys = list(res_right.keys())
                values = list(res_right.values())
                if values[0] - values[1] <= 6 and keys[1] > keys[0]:
                    keys[0], keys[1] = keys[1], keys[0]
                    values[0], values[1] = values[1], values[0]
                    res_right = dict(zip(keys, values))
            # first_key = next(iter(res_right.keys()))
            for x in res_right.keys():
                if x > set_x * 2 / 3 and res_right[x] >= 4:
                    first_key = x
                    break
                else:
                    first_key = set_x
            # print(res_right)
        return first_key

    def is_page_foot(self,least_bbox, img_box, first_left, first_right):
        """
        å·¦å³è¾¹è§’ï¼Œå®½åº¦å·®å€¼å°block[2]-block[0.]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­å·¦å³è¾¹ç•Œ
        ä¸Šä¸‹è¾¹è§’ï¼Œé«˜åº¦å·®å€¼å°block[3]-block[1]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­ä¸Šä¸‹è¾¹ç•Œ
        """
        # æ£€æµ‹å³æµ‹è¾¹è§’
        if least_bbox[0] >= first_right:
            return True
        # æ£€æµ‹ä¸‹é¢è¾¹è§’
        elif img_box[3] - least_bbox[1] <= img_box[3] * 0.05:
            return True
        # æ£€æµ‹å·¦ä¾§è¾¹è§’
        elif least_bbox[2] <= first_left:
            return True
        # ä¸Šè¾¹è§’ä¼šé‡åˆ°æ ‡é¢˜è¿™ä¸ªé—®é¢˜ï¼Œè¦ä¸è¦è§£å†³ï¼Ÿ
        elif least_bbox[3] - img_box[1] <= img_box[3] * 0.05:
            return True
        else:
            return False

    def is_fig(self,block_text):
        if re.search('^(\s?[å¦‚é™„]?[å›¾è¡¨] ?[\dA-Za-z])', block_text):
            # print(block_text)
            return True
        return False

    def step1_drop_Pagefooter(self,item):
        # print(json.dumps(item, ensure_ascii=False))
        """
        1.éå†full_blocksåˆ¤æ–­æ˜¯å¦ä¸ºé¡µè¾¹è§’
        2.åœ¨textä¸­æ‰¾åˆ°å†…å®¹ç»™åˆ æ‰
        """
        raw_info = item['attr']['raw_info']
        img_box = item['attr']['img_box']

        if raw_info != []:
            set_x = img_box[2]
            x1_num = {}
            x2_num = {}
            for raw in raw_info:
                raw_context = raw['raw_context']
                for bb in raw_context:
                    bbox = bb["bbox"]
                    x1, y1, x2, y2 = bbox
                    x1_num[x1] = x1_num.setdefault(x1, 0) + 1
                    x2_num[x2] = x2_num.setdefault(x2, 0) + 1
            first_left = int(self.dict_merge(x1_num, set_x))
            first_right = int(self.dict_merge(x2_num, set_x, flag=False))
            # replace_num = 0
            for index, raw in enumerate(raw_info):
                full_blocks = raw['full_blocks']
                block_text = raw['block_text'].strip()
                if not block_text or len(block_text.strip()) <= 3:
                    continue
                if self.is_page_foot(full_blocks, img_box, first_left, first_right):

                    # replace_num += 1
                    # å¯¹ least_text è¿›è¡Œæ­£åˆ™è½¬ä¹‰
                    escaped_least_text = re.escape(block_text)

                    # æ„å»ºæ­£åˆ™æ¨¡å¼ï¼ŒåŒ¹é…å¯èƒ½çš„å‰åç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œè¿å­—ç¬¦
                    # pattern = re.compile(escaped_least_text + r'[\s\n-]{0,5}')

                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åŒ¹é…çš„æ–‡æœ¬
                    try:
                        item['text'] = re.sub(rf'{escaped_least_text}', rf'åˆ é™¤è¾¹è§’:<u>{block_text}</u>', item['text'])
                    except re.error as e:
                        print(f"Regex error: {e}")
                if self.is_fig(block_text):
                    escaped_least_text = re.escape(block_text)
                    # æ„å»ºæ­£åˆ™æ¨¡å¼ï¼ŒåŒ¹é…å¯èƒ½çš„å‰åç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œè¿å­—ç¬¦
                    # pattern = re.compile(escaped_least_text + r'[\s\n-]{0,5}')
                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åŒ¹é…çš„æ–‡æœ¬
                    try:
                        item['text'] = re.sub(rf'{escaped_least_text}', rf'åˆ é™¤å›¾ç‰‡æè¿°:<u>{block_text}</u>', item['text'])
                        # item['text'] = re.sub(rf'{escaped_least_text}', rf'', item['text'])


                    except re.error as e:
                        print(f"Regex error: {e}")
            # if replace_num > 10:
            #     item['text'] = '(æœ¬é¡µåˆ é™¤)æœ¬é¡µä¸­æœ‰è¶…è¿‡10ä¸ªè¾¹è§’çš„åˆ¤æ–­æ€€ç–‘æ˜¯ç›®å½•é¡µæœ¬é¡µåˆ é™¤' + item['text']
            # print(item['text'])
        return item

    # def step1_correct_error_token(self, context):
    #     # å•è¯çº é”™
    #     rank_length = 480
    #     context_list = []
    #     for pos in range(0, len(context), rank_length):
    #         context_list.append(context[pos:pos + rank_length])
    #     if len(context_list) == 0:
    #         return ""
    #     sc_result = self.sc.process_text(context_list)
    #
    #     final_correct_list = []
    #     for item, sc_item in zip(context_list, sc_result):
    #         # çº é”™å†…å®¹åå¤„ç†
    #         sc_details = self.fix_details(sc_item["details"])
    #         # ä¿è¯çº é”™çš„æ•°é‡
    #         if len(sc_details) == 0 or len(sc_details) > 3:
    #             final_correct_list.append(item)
    #         else:
    #             final_content = ""
    #             pre_idx = 0
    #             for detail_item in sc_details:
    #                 [raw_token, new_token, detail_pos_start, detail_pos_end] = detail_item
    #                 final_content += "{}{}<u>çº é”™:{}</u>".format(item[pre_idx:detail_pos_start], new_token, raw_token)
    #                 pre_idx = detail_pos_end
    #             final_content += item[pre_idx:]
    #             final_correct_list.append(final_content)
    #     return "".join(final_correct_list)

    def step2_rm_kongge(self, context):
        context = context.lstrip().rstrip()
        content = context.split(" ")
        first_content = content[0]
        last_content = " ".join(content[1:])
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', last_content)
        # å¤šæ‰§è¡Œä¸€æ¬¡ï¼Œå¼¥è¡¥æ­£åˆ™è¾¹ç•Œé‡å é—®é¢˜ï¼›
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', final_content)
        if len(final_content) == 0:
            return first_content

        merge_piece = first_content + final_content.lstrip()[0]

        split_word = list(jieba.cut(merge_piece, cut_all=False))
        if len(split_word[-1]) > 1:
            return first_content + final_content
        return first_content + " " + final_content

    def step3_rm_noise_dig_alpha(self, context):
        # åˆ é™¤å•ç‹¬çš„é›¶ç¢ï¼ˆå­—æ¯+æ•°å­—ï¼‰
        import numpy as np
        # æœ‰æ±‰å­—
        if len(re.findall(r'[\u4e00-\u9fa5]', context)) > 0:
            return context

        # å¦‚æœæ˜¯åºå·
        if len(re.findall(r'[\da-zA-Z]+\.', context)) == 0:
            return context

        # # æ²¡æœ‰æ•°å­—
        # if len(re.findall(r'\d',context)) == 0:
        #     return context

        if len(context.split(" ")) == 1 or len(context.split(" ")) > 5:
            return context

        if "." in context and len(re.findall(r'[A-Z]', context)) == 0:
            return context

        split_word = [len(word) for word in context.split(" ")]
        mean_word_len = np.mean(split_word)
        if mean_word_len > 3:
            return context
        else:
            return "æœ¬æ®µåˆ é™¤1:<u>{}</u>".format(context)
            # return ""




    def get_person_idx(self, context):
        if re.findall(r'(#+|\d+ ?\.|[\(ï¼ˆ] ?[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ ?[ï¼‰\)])',context)!=[]:
            return [],[]
        doc = self.zh_nlp(context)
        person_block = []
        person_num = 0
        ent_dict = {
            "person": [],
            "org": [],
        }

        for ent in doc.ents:
            if ent.label_ == "PERSON":
                key = "person"
            else:
                key = "org"
            if len(context[ent.start_char:ent.end_char])>1:
                ent_dict[key].append(context[ent.start_char:ent.end_char])
        return ent_dict["person"], ent_dict["org"]




    def step4_rm_cite(self,context_raw):
        context = str(context_raw)
        cite_index = 1 if len(re.findall(r'\[\d+\]', context)) > 0 else 0
        cite_year = 1 if len(re.findall(r'\d\d\d\d', context)) > 0 else 0
        cite_J = 1 if len(re.findall(r'\[[Jj]\]', context)) > 0 else 0

        cite_doi = 1 if len(re.findall(r'[dD][oO][iI]', context)) > 0 else 0
        cite_cip = 1 if len(re.findall(r'[cC][iI][pP]', context)) > 0 else 0

        cite_etal = 1 if "et al" in context else 0
        za_zhi = 1 if "æ‚å¿—" in context else 0
        cite_vol = 1 if len(re.findall(r'[vV][oO][lL]', context)) > 0 else 0
        cite_jan = 1 if len(re.findall(r'[jJ][aA][nN]', context)) > 0 else 0
        cite_email = 1 if "mail" in context else 0
        cite_com = 1 if "com" in context else 0
        cite_fix = 1 if "ä¿®è®¢" in context else 0
        cite_comm = 1 if "å§”å‘˜ä¼š" in context else 0
        cite_auth = 1 if "ä½œè€…" in context or "ä¸»ç¼–" in context else 0
        cite_zhuanjia = 1 if "ä¸“å®¶" in context else 0
        cite_chuban = 1 if "å‡ºç‰ˆ" in context else 0
        cite_rexian = 1 if "çƒ­çº¿" in context else 0
        cite_ISBN = 1 if len(re.findall(r'[iI][sS][bB][nN]', context)) > 0 else 0

        cite_tag = [cite_index, cite_year, cite_J, cite_doi, cite_cip, cite_etal,
                    za_zhi, cite_vol, cite_jan, cite_email, cite_com,
                    cite_fix, cite_comm, cite_auth, cite_zhuanjia, cite_chuban, cite_rexian, cite_ISBN]
        # print(cite_tag)
        if sum(cite_tag) >= 3 and len(context) < 200:
            return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(context_raw)
            # return ""
        if sum(cite_tag) >= 2 and len(context) < 100:
            return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(context_raw)
            # return ""

        # åŸºäºäººååˆ¤å®š
        try:
            # print(context)
            name_list, ORG_list = self.get_person_idx(context)
            # print(name_list,ORG_list)
            name_num, name_lens = len(name_list), len(''.join(name_list) )

            if len(context) == 0:
                return ""
            if name_lens / len(context.replace(' ','')) >= 0.3:
                return "å‚è€ƒåˆ é™¤-1:<u>{}</u>".format(context_raw)

            if name_num > 2:  # è¶…è¿‡2ä¸ªäººå
                # print(123, context_raw)
                hypothesis_template = "The type of this text is {}"
                classes_verbalized = ["Text content", "reference","acknowledgments",'personage introduction']
                output = self.zeroshot_classifier(context_raw, classes_verbalized, hypothesis_template=hypothesis_template,
                                             multi_label=False)
                # print(output["labels"][0], output['scores'][0])
                # print(output)
                if output["labels"][0] != 'Text content' and output['scores'][0] > 0.7:
                    return "å‚è€ƒåˆ é™¤-2:<u>{}</u>".format(context_raw)
                else:
                    return context_raw
            # elif cite_index and person_num > 0 and len(context) < 150:
            #     return "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(context_raw)
            # return ""
            else:
                return context_raw
        except:
            return context_raw


    def is_complete(self, last_sentence, curr_sentence):

        last_sentence = last_sentence.lstrip(" ").rstrip(" ").strip("\n")
        curr_sentence = curr_sentence.lstrip(" ").lstrip("\n")
        conjunction=[r'\d{1,4}\.[^\d]',r'((?:\(|ï¼ˆ) ?[0-9-ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ ?(?:ï¼‰|\)))',r'([-ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}[\.ã€])[^\d]',r'##','[A-E]\.','[ã€[ã€–] ?[\u4e00-\u9fa5]{2,4} ?(\d* )?[ã€—\]ã€‘]']
        No_closing_remarks=["ã€", "å¯¹", "ä¸", "å’Œ", "åŠ", "å…¶", "ä½†", "ä¸”", "ä¹ƒ", "~", "ï½","çš„", "ç”±","äº", "è‡´","â€”","-",",","ï¼Œ"]
        formula_switch=True
        for word in conjunction:
            if re.findall(r'[\u4e00-\u9fa5]',last_sentence)==[]:
                if '+' in last_sentence or r'/' in last_sentence or '=' in last_sentence or "â€”" in last_sentence:
                    formula_switch=False
            if re.findall(word, last_sentence.replace(' ', '')[:6]) == []:
                last_sentence_switch=True
            else:
                last_sentence_switch=False
                break
        for word in conjunction:
            if re.findall(r'[\u4e00-\u9fa5]', curr_sentence) == []:
                if '+' in curr_sentence or r'/' in curr_sentence or '=' in curr_sentence or "â€”" in curr_sentence:
                    formula_switch = False
            if re.findall(word, curr_sentence.replace(' ', '')[:6]) == []:
                curr_sentence_switch=True
            else:
                curr_sentence_switch=False
                break

        # print(last_sentence)
        # print(last_sentence_switch)
        # print('--')
        # print(curr_sentence)
        # print(curr_sentence_switch)
        # print()
        if "|" in last_sentence and curr_sentence_switch or "|" in curr_sentence and last_sentence_switch:
            return True
        if len(last_sentence) == 0:
            return False
        if not formula_switch:
            return True
        if last_sentence.strip(" ")[-1] in ["ã€‚", "ï¼Ÿ", "ï¼", ".", "?", "!", ")"]:
            return True
        elif last_sentence.strip(" ")[-1] in No_closing_remarks and last_sentence.strip(" ")[-2:] not in ['ç›®çš„','æ€»å’Œ'] and  curr_sentence_switch and len(last_sentence)>1 :
            return False
        # elif "#" in last_sentence or "#" in curr_sentence:
        elif len(re.findall(r'[^a-zA-Z] ?[Oo]\n',last_sentence))>0:
            return True
        elif len(re.findall(r'^#', last_sentence)) > 0 or len(re.findall(r'^#', curr_sentence)) > 0:
            return True
        elif len(re.findall(r'[-ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å](\s\.|ã€)', last_sentence + curr_sentence)) > 0:
            return True
        elif len(re.findall(r'^[^#\d\(ï¼ˆ](?:.*?)([a-zA-Z\d]$)(?<!g)', last_sentence)) > 0 and curr_sentence_switch  or len(re.findall(r'^[a-zA-Z\d][^\.][^\.]', curr_sentence)) > 0 and last_sentence_switch:
            # print(111)
            return False

        # try:
        #     if len(last_sentence) > 6:
        #         merge_sentence = last_sentence[-5:] + curr_sentence[0]
        #
        #     else:
        #         merge_sentence = last_sentence[-2:] + curr_sentence[0]
        # except:
        #     return True
        # # print(merge_sentence)
        # split_word = list(jieba.cut(merge_sentence, cut_all=False))
        # # print(split_word)
        # if len(split_word[-1]) > 1 and re.findall(r'([\u4e00-\u9fa5]{2})',split_word[-1])!=[]:
        #
        #     print(222)
        #
        #     return False
        return True

    def rm_lid_piece(self, context):
        context_list = context.split("\n")
        final_list = []
        final_list.append(context_list[0])
        idx = 1
        while idx < len(context_list):
            if self.is_complete(final_list[-1], context_list[idx]):
                final_list.append(context_list[idx])
            else:
                # print("æ¢è¡Œåˆ é™¤1", final_list[-1], "-----------",context_list[idx])
                # final_list[-1] = "{}åˆ é™¤æ¢è¡Œ1ï¼š{}".format(final_list[-1], context_list[idx])
                final_list[-1] = "{}{}".format(final_list[-1], context_list[idx])

            idx += 1
        return "\n".join(final_list)

    def rm_lid_context(self, context):
        context_list = context.split("\n\n")
        final_list = []
        final_list.append(context_list[0])

        idx = 1
        while idx < len(context_list):
            sent_complete = self.is_complete(final_list[-1], context_list[idx])
            if sent_complete:
                final_list.append(context_list[idx])
            else:
                # print("æ¢è¡Œåˆ é™¤2",final_list[-1],context_list[idx])
                # final_list[-1] = "{}åˆ é™¤æ¢è¡Œ2ï¼š{}".format(final_list[-1], context_list[idx])
                final_list[-1] = "{}{}".format(final_list[-1], context_list[idx])

            idx += 1

        return "\n\n".join(final_list)

    def step5_rm_english_noise(self, context):
        find_pattern = [
            [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])", 0],
            [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$', -1]
        ]
        pattern = [
            [r"(^[a-zA-Z ]+)([\u4e00-\u9fa5])", r"\2"],
            [r'([\u4e00-\u9fa5]+)\s?([a-zA-Z\s]+)$', r"\1"],
        ]

        for idx, item in enumerate(find_pattern):
            [p_item, p_index] = item
            result = re.findall(p_item, context)
            # print(result)
            if len(result) == 0:
                continue
            elif len(result[0][p_index].strip().split(" ")) == 1:
                continue
            else:
                # print(result[0][p_index].strip().split(" "))
                context = re.sub(pattern[idx][0], pattern[idx][1], context)
        return context


    def step6_rm_short_context(self, context):
        context_list = context.split("\n\n")
        context_lens = [len(item) for item in context_list]
        if context.count("|") >= 3:
            return context
        if len(context_list) >= 5:
            return context
        if max(context_lens) > 100:
            return context
        elif len(context_list) > 2 and max(context_lens) >= 50:
            return context
        else:
            # print(context)
            # print("="*20)
            return "æ•´é¡µåˆ é™¤\n\n{}".format(context)
            # return ""

    # def step7_rm_digital(self, context):
    #     res = re.findall(r"\d", context)
    #     no_digital = re.sub(r"\d", "", context)
    #     # print(res)
    #     # print(no_digital)
    #     # print(list(set(no_digital.split(" "))))
    #     context_lens = len(list(set(no_digital.split(" "))))
    #
    #     if len(res) > 8 and (context_lens) < 5:
    #         return "åˆ é™¤38:<u>{}</u>".format(context)
    #         # return ""
    #     return context
    def step7_judgment_connection(self,context):
        copy_target=[]

        while re.findall(r'(^[^#\d\n\(ï¼ˆç¬¬ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å].*?[\u4e00-\u9fa5,ï¼Œ(ï¼ˆ])( *\n{1,} *)([)ï¼‰:ï¼š,ï¼Œí•™\u4e00-\u9fa5])',context, flags=re.MULTILINE):
            target = re.findall(r'(^[^#\d\n\(ï¼ˆç¬¬ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å].*?[\u4e00-\u9fa5,ï¼Œ(ï¼ˆ])( *\n{1,} *)([)ï¼‰:ï¼š,ï¼Œí•™\u4e00-\u9fa5].*)',context, flags=re.MULTILINE)
            # print(copy_target,'---',target)
            if target!=copy_target:
                for single in target:
                    # if 'åˆ é™¤æ¢è¡Œ' in single[0][0:5] or 'åˆ é™¤æ¢è¡Œ' in single[2][0:5]:
                    #     continue
                    if len(single[0]) <= 9 and re.findall(r'[,ï¼Œ.ã€‚?ï¼Ÿ!ï¼ï¼›;â€œ"â€˜\']',single[0])==[]or len(single[2]) <= 9 and re.findall(r'[,ï¼Œ.ã€‚?ï¼Ÿ!ï¼ï¼›;â€œ"â€˜\']',single[2])==[]:

                        continue
                    totoly_word = single[0] + single[1] + single[2]
                    # ans = single[0] +'åˆ é™¤æ¢è¡Œ3ï¼š'+ single[2]
                    ans = single[0] + single[2]

                    context = context.replace(totoly_word, ans)
                copy_target=target
                    # print(context)
            else:
                return context
        return context


    def step9_remove_en_noise(self,text):
        common_elements = ["Na", "Mg", "Al", "Si", "Cl", "Ca", "CH", "Ag", "Hg", "Cu", "mol", "nm", "mm", "cm", "kg",
                           "Hz", "Hb", "xOy", "OH", "H 2 O","SO","RNA","DNA","Fe"]
        suspect_lis, origin_lis, same= [], [],set([])
        target = re.sub(r'\(.*?\)', 'æ›¿', text)
        en_lis = re.findall(
            r'(([A-ZÃ€-Ã–Ã˜-ÃŸĞ-Ğ¯ÅÄ†Ä‚Ã‚ÄÃŠÃ”Æ Æ¯ÃÃÃ†Ã˜Ã…a-zÃ -Ã¶Ã¸-Ã¿Ğ°-ÑÅŸÄ‡ÄƒÃ¢Ä‘ÃªÃ´Æ¡Æ°Ã¾Ã°Ã¦Ã¸Ã¥à¤­à¤¯à¥‡à¤° ]{1,}) ?[\d]* ?([*A-ZÃ€-Ã–Ã˜-ÃŸĞ-Ğ¯ÅÄ†Ä‚Ã‚ÄÃŠÃ”Æ Æ¯ÃÃÃ†Ã˜Ã…a-zÃ -Ã¶Ã¸-Ã¿Ğ°-ÑÅŸÄ‡ÄƒÃ¢Ä‘ÃªÃ´Æ¡Æ°Ã¾Ã°Ã¦Ã¸Ã¥à¤­à¤¯à¥‡à¤° \d]+)[A-ZÃ€-Ã–Ã˜-ÃŸĞ-Ğ¯ÅÄ†Ä‚Ã‚ÄÃŠÃ”Æ Æ¯ÃÃÃ†Ã˜Ã…a-zÃ -Ã¶Ã¸-Ã¿Ğ°-ÑÅŸÄ‡ÄƒÃ¢Ä‘ÃªÃ´Æ¡Æ°Ã¾Ã°Ã¦Ã¸Ã¥à¤­à¤¯à¥‡à¤°])',
            target)
        for i in en_lis:
            origin_lis.append(i[0])
        origin_lis=list(set(origin_lis))
        origin_lis.sort(key=lambda x: x.lstrip().rstrip())

        # print(origin_lis)
        for idx, str in enumerate(origin_lis):
            if idx + 1 == len(origin_lis):
                continue
            if len(str)>10:
                continue
            if str.replace(' ', '')[:2].lower() == origin_lis[idx + 1].replace(' ', '')[:2].lower():
                same.add(origin_lis[idx])
                same.add(origin_lis[idx + 1])
        for str in same:
            origin_lis.remove(str)
        # # print(origin_lis)
        # for key, group in itertools.groupby(origin_lis):
        #     if len(list(group)) == 1:
        #         suspect_lis.append(key)

        # print('åŸ', origin_lis)
        for word in origin_lis:
            word_copy = word.lstrip().rstrip()
            if word_copy.isupper()  and len(word_copy)<5:
                continue
            elif any(substr in word_copy for substr in common_elements):
                continue

            # print(word)

            inputs = self.tokenizer(word, return_tensors="pt")
            outputs = self.model(**inputs)
            probs = F.softmax(outputs.logits, dim=-1)
            # print(probs[0])
            if probs[0][0] < 0.1 or probs[0][1] > 0.85 or probs[0][2] > 0.85 or probs[0][3] > 0.85:
                text = text.replace(word, ' è‹±æ–‡å™ªå£°åˆ é™¤ï¼š<u>' + word + '</u> ')
                # text = text.replace(word, '')

        return text

    def step10_Irrelevant_long_text(self,context):
        split_token = "\n\n"
        if split_token not in context:
            split_token = "\n"

        count= 0
        result,suspected = [],False
        key_words=['æ•™æç¼–å†™','å‰è¨€','ä½œè€…ç®€ä»‹','ä½œè€…ä»‹ç»','ç‰ˆæƒæ‰€æœ‰','ä¸»ç¼–ä»‹ç»','é™„å½•','å‚è€ƒæ–‡çŒ®']
        for word in key_words:
            if word in context:
                suspected=True
                break
        if suspected:
            # print(context)
            context_lis = context.split('\n')
            # print(context_lis)
            for paragraph in context_lis:
                if not paragraph:
                    result.append(paragraph)
                    continue
                # print(paragraph)
                hypothesis_template = "The type of this text is {}"
                classes_verbalized = ["Text content", "reference", "acknowledgments", 'personage introduction',
                                      'Guide book', ]
                output = self.zeroshot_classifier(paragraph, classes_verbalized, hypothesis_template=hypothesis_template,
                                             multi_label=False)
                # print(output["labels"][0], output['scores'][0])
                if output["labels"][0] != 'Text content':
                    paragraph = "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(paragraph)
                    count += 1
                result.append(paragraph)
            context_lis=set(context_lis)
            context_lis.discard('')

            if count >= len(context_lis) / 2:
                # return ''
                return '(æœ¬é¡µåˆ é™¤)æœ¬é¡µä¸€åŠçš„æ®µè½ä¸­å‘ç°ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾' + '\n' + context
            else:
                return '\n'.join(result)
        else:
            return context


def clean_text(context, lang, sp):
    # print("-"*10)
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"
    pattern_list = pattern_list_zh

    # åˆ†è§£å¤„ç†
    result = []
    for idx, item in enumerate(context.split(split_token)):

        if idx == 0:
            item = item.lstrip().rstrip()
            res = re.findall(r"\d+", item)
            if len(item)==0:
                continue
            if len(res) / len(item) > 0.5:
                # item = "åˆ é™¤0:<u>{}</u>".format(item)
                # result.append(item)
                continue
        item = sp.step4_rm_cite(item)

        # 1.æ­£åˆ™
        # print(item)
        # print('----------------')
        for pattern_item in pattern_list:
            item = item.lstrip(" ").rstrip(" ")
            # print(pattern_item)
            item = re.sub(pattern_item[0], pattern_item[1], item)
        # print(item)

        # 2.æ›¿æ¢ å›ºå®šå†…å®¹ï¼ˆéæ³›åŒ–ï¼‰
        for replace_item in context_pattern:
            item = re.sub(replace_item[0], replace_item[1], item)

        # print(item)
        item = sp.step5_rm_english_noise(item)
        item = sp.step3_rm_noise_dig_alpha(item)
        # print('---')
        # print(item)

        item = sp.rm_lid_piece(item)


        item = sp.step2_rm_kongge(item)


        # print(item)
        # item = sp.step9_remove_en_noise(item)


        result.append(item)

    # æ•´åˆ

    # print(context)
    context = split_token.join(result)
    while re.findall(r'(#?#? *ç¬¬[â€”ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å-]+[ç« èŠ‚] *\.?)(\n+)#?#?( *[\u4e00-\u9fa5]{3,10})',context):
        # print(1)
        context=re.sub(r'(#?#? *ç¬¬[â€”ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å-]+[ç« èŠ‚] *\.?)(\n+)#?#?( *[\u4e00-\u9fa5]{3,10})',r'\1\3',context)
    # print(context)
    context = sp.step9_remove_en_noise(context)
    context=sp.step7_judgment_connection(context)

    context = sp.rm_lid_context(context)
    context = sp.step10_Irrelevant_long_text(context)

    context = sp.step6_rm_short_context(context)
    # print(context)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


def process_line(items, sp):
    try:
        item = json.loads(items.strip())
        item=sp.step1_drop_Pagefooter(item)

        context = item["text"]
        # if item["seq_id"] != "87231848-4d52-4de0-ab9c-84ec43bb7acc":
        #     return
        # if need
        lang = item["lang"]

        context = clean_text(context, lang, sp)
        context = post_process(context)
        item["text"] = context
    except:
        print("error")
        exit(0)
    item = json.dumps(item, ensure_ascii=False)
    return item
sp=speicalProces()
fw = open(r'F:\zeroshots\reclean7_medicalpdfv2__preformat_zh.jsonl', 'w', encoding='utf-8')
with open(r'F:\zeroshots\medicalpdfv2_0724_preformat_zh.jsonl', "r", encoding="utf-8") as file:
    for item in tqdm(file.readlines()):

        item=process_line(item,sp)
        # with open(r'F:\zeroshots\reclean7_medicalpdfv2__preformat_zh.jsonl', 'a', encoding='utf-8') as f:
        fw.write(item+'\n')


