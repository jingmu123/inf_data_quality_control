import json
import re
from tqdm import tqdm
import math
import spacy


pattern_list_en = [


    # [r'([^\n\.]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(\d+\.html))[^\n]*)', r'åˆ é™¤10:<u>\1</u>'],
    # [r'([^\.?\n]*\(?\s*https?://[^\s]+\s*\)?[^\.?\n]*)', r'åˆ é™¤12:<u>\1</u>'],
    # [r'([^\.\n]*\b(?:[Ww]{2,3}\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b[^\n\.]*)', r'åˆ é™¤13:<u>\1</u>'],
    # å»é™¤å¸¦æœ‰ç½‘å€çš„å¥å­,å…³é”®è¯   wwwã€comã€htmlã€http
    # todo www,httpæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå³è¾¹ç•Œçš„å‡†ç¡®æ€§éœ€è¦è€ƒè™‘
    # todo com,htmlæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå·¦è¾¹ç•Œéœ€è¦è€ƒè™‘
    [r'([^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',r'åˆ é™¤10:<u>\1</u>'],

    [r'(\n\s*[a-zA-Z\.]\s*\n)', r'åˆ é™¤15:<u>\1</u>'],
    [r'([^\n]*Copyright[^\n]*)', r'åˆ é™¤18:<u>\1</u>'],

    # å‚è€ƒæ–‡çŒ®

    [r'(Circulation\s*\d{4},\s*\d+:[A-Z0-9-]*\s*[Dd]oi:\s*[A-Z0-9-.//]*\s*^([A-Z][a-z]*))', r'åˆ é™¤å‚è€ƒæ–‡çŒ®:<u>\1</u>'],
    [r'([^\n]*([A-Z]+\w+)\s*[A-Z]*\w*\.?(\s*\d+?\s*(?:-\d+)*,?)?(\d+;)?\s*\d{4}\s*[,;]\s*(((\d+\s*(\(\d+\))?:)|(\s*\b[Pp]p\b\s*[:.]?))\s*\d+(?:[-;â€“]\d+)+)?[^\n]*)',r'åˆ é™¤å‚è€ƒæ–‡çŒ®:<u>\1</u>'],
    [r'([^\n]*[A-Z]\w+,\s*[A-Z]\.\s*[A-Za-z()\.&,]*[^\\n]*\d{4}[^\n]*)', r'åˆ é™¤å‚è€ƒæ–‡çŒ®:<u>\1</u>'],
    [r'([^\[\.]*[Pp]age[s]?\s*\d+(-\d+)?.*?[\.\]])', r'åˆ é™¤å‚è€ƒæ–‡çŒ®:<u>\1</u>'],


    [r'(ISBN\s*[A-Z0-9-]*)', r'åˆ é™¤19:<u>\1</u>'],



    # å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
    [r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”)', r'åˆ é™¤0:<u>\1</u>'],




    # æ— å…³æ–‡æœ¬
    # æ— å…³å›¾ç‰‡å¼•ç”¨
    [r'((\()\b([F|f]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|Fig|p)\b:?\.?\s*(\d+)?\.?.*?(\)))',r'åˆ é™¤5:<u>\1</u>'],
    [r'([^\n\.]*\b(Fig[s]?[ure]?[s]?)\b\.?[\s*Â ](\d+)?\.?[^\n]*)', r'åˆ é™¤4:<u>\1</u>'],

    # æ•°å­—å¼•ç”¨
    ##1.å¸¦æ‹¬å· book (1ï¼Œ2).
    # [r'[^0-9]\.']
    # [r'([^0-9]\.)(\s*\(\d+\s*(?:[-,;â€“.]\d+)*\)\s*)([\n.A-Z])', r'åˆ é™¤1:<u>\2</u>'],
    [r'(\([\d\s,\.â€“]{1,20}\))',r'åˆ é™¤1:<u>\1</u>'],

    ##2.ä¸å¸¦æ‹¬å·ä½†æ˜¯æ•°å­—å‰æ˜¯å¥å·. æ•°å­—åæ˜¯æ¢è¡Œæˆ–è€…å¤§å†™å­—æ¯
    # [r'([^0-9]\.)(\s*\d+\s*(?:[-,;â€“.]\d+)*)(?=$)', r'åˆ é™¤2:<u>\2</u>'],
    # ä¸å¸¦æ‹¬å·
    [r'([^\d])(\d{1,3}(\s?[â€“,]\s?\d{1,3}){1,4})([^\d])',r'\1åˆ é™¤2:<u>\2</u>'],
    # ä¸å¸¦æ‹¬å·æ•°å­—æˆ–æ•°å­—åŠ ç‚¹ï¼ˆå®¹æ˜“è¯¯åˆ ï¼‰
    [r'([\.,])(\s\d+([,\.]\d+){0,5})',r'\1åˆ é™¤3:<u>\2</u>'],




    # å¥é¦–æ— å…³æ•°å­—
    # [r'([^\d]\.)([\d+\.\sâ€“]{3,10})(\s?[A-Z])',r'\1åˆ é™¤6:<u>\2</u>\3']
    # å¦‚æœä¸æ˜¯å±…é¦–

 ]

context_pattern = [
    [r'(Â¬\s*)', r'åˆ é™¤16:<u>\1</u>'],
    [r'(\(\s*\))', r'åˆ é™¤17:<u>\1</u>']
]

nlp = spacy.load("en_core_web_sm")
def NIL_tool(nlp,item):
    """
    :param item:ä¼ å…¥çš„æ˜¯contextä¸­æœ€å°å•ä½ï¼Œæ˜¯ä¸€æ®µè¯
    :return:
    person_numè¿™æ®µè¯æœ‰å¤šå°‘åå­—
    names_start2stopäºŒç»´åˆ—è¡¨[[start,stop],[start,stop]...]
    """
    doc = nlp(item)
    person_num = 0
    names_start2stop = []
    for ent in doc.ents:
        name_start2stop = []
        if ent.label_ == "PERSON":
            person_num += 1
            name_start2stop.append(ent.start_char)
            name_start2stop.append(ent.end_char)
            names_start2stop.append(name_start2stop)


    return person_num, names_start2stop

# def step5_2_isreference(item,person_num,names_start2stop):
#     len_item = len(item)
#     len_names = 0
#     for name in names_start2stop:
#         len_name = name[1] - name[0]
#         len_names += len_name
#
#     # æœ‰äººå,æœ‰\d+-\d+
#     if person_num and re.search("\d+\s?[-]{1,3}\s\d+",item):
#         return True
#     # æœ‰äººåï¼Œæœ‰å¹´ä»½ä¸€å®šæ˜¯å››ä½æ•°å­—
#     elif person_num and re.search("\s\d{4}[^\d]",item) and re.search("^\d+\.",item):
#         return True
#     elif person_num >= 10:
#         return True
#     elif person_num >= 5 and len_names > len_item * 0.3:
#         return True
#     elif person_num >= 3 and len_names > len_item * 0.5:
#         return True
#     elif len_names > len_item * 0.2:
#         return True
#     else:
#         return False

class speicalProces:
    def __init__(self):
        pass

    def is_not_upper(self, char):
        return not char.isupper()

    # å»é™¤å·¦å³ä¾§æ 
    def step1_drop_Sidebar(self, item):
        left_min, right_max = math.inf, 0
        text_boundary = []
        for dic in item["attr"]["raw_info"]:
            text_boundary.append(len(dic["raw_context"]))
        k = sorted(text_boundary)[int(len(text_boundary) // 2)]
        if k > 12:
            k = 12
        for dic in item["attr"]["raw_info"]:
            if len(dic["raw_context"]) >= k and dic["full_blocks"][2] > right_max:
                right_max = dic["full_blocks"][2]
            if len(dic["raw_context"]) >= k and dic["full_blocks"][0] < left_min:
                left_min = dic["full_blocks"][0]
        for dic in item["attr"]["raw_info"]:
            if dic["full_blocks"][0] > right_max or dic["full_blocks"][2] < left_min:
                for dic_text in dic["raw_context"]:
                    item["text"] = item["text"].replace(dic_text["text"], "")
                    item["text"] = item["text"].replace(dic_text["text"].strip("-"), "")
        return item

    # å»é™¤é¡µè„š
    def step2_drop_Pagefooter(self, item):
        down_max = 0
        text_boundary = []

        for dic in item["attr"]["raw_info"]:
            text_boundary.append(len(dic["raw_context"]))
        k = sorted(text_boundary)[int(len(text_boundary) // 2) - 1]

        for dic in item["attr"]["raw_info"]:
            if len(dic["raw_context"]) >= k and dic["full_blocks"][3] > down_max:
                down_max = dic["full_blocks"][3]
                # print(down_max)

        for dic in item["attr"]["raw_info"]:
            # len
            if dic["full_blocks"][1] > down_max:
                for dic_text in dic["raw_context"]:
                    item["text"] = item["text"].replace(dic_text["text"], "")
                    item["text"] = item["text"].replace(dic_text["text"].strip("-"), "")

        return item

    def step3_removeLinefeed(self, nums):  # å»é™¤å¤šä½™æ¢è¡Œ
        for index in range(len(nums)):
            # print(num[index])
            nums[index] = (nums[index]).replace("\n", "--huan hang ti huan fu hao--").strip()
        slow, fast = 0, 1
        while fast < len(nums):
            nums[slow] = nums[slow].strip()
            nums[fast] = nums[fast].strip()
            if len(nums[fast]) == 0:
                fast = fast + 1
                continue
            try:
                if (nums[slow].endswith("and") or nums[slow].endswith("of") or nums[slow].endswith("any") or nums[
                    slow].endswith("a") or nums[slow].endswith("is") or nums[slow].endswith("for") or nums[
                        slow].endswith(
                    "the") or nums[slow].endswith("or") or nums[slow].endswith(",")) and self.is_not_upper(
                    nums[fast][0]):

                    nums[slow] = nums[slow] + " " + nums[fast]
                elif "#" not in nums[slow] and "*" not in nums[slow] and re.match(r'.*[a-z0-9-,:"]$', nums[slow]) and (
                        re.match(
                            r'^[a-z-,]', nums[fast])):

                    nums[slow] = nums[slow] + " " + nums[fast]
                else:
                    slow = slow + 1
                    nums[slow] = nums[fast]
                fast = fast + 1
            except:
                print(nums[slow], "|{}|".format(nums[fast]))
                raise Exception
        for idx, item in enumerate(nums[0:slow + 1]):
            if item.count("|") > 5:
                item = item.replace("--huan hang ti huan fu hao--", "\n")
            item = re.sub(r'([\.?!:]"?)--huan hang ti huan fu hao--', r'\1\n', item)
            nums[idx] = item.replace("--huan hang ti huan fu hao--", " ")
        return nums[0:slow + 1]

    def step4_linefeed(self, context):
        index = 0
        while index < len(context):
            if index > 0:
                item = context[index]
                # å°å†™ã€æ‹¬å·å¼€å¤´åˆå¹¶åˆ°å‰ä¸€ä¸ªitem
                # todo åç»­çœ‹æ ‡æ³¨çš„æƒ…å†µè¡¥å……æ¡ä»¶å¤šçš„è¯å¯ä»¥å•ç‹¬å†™ä¸€ä¸ªåˆ¤å®šæ–¹æ³•
                if item[0].strip().islower() or item[0].strip() in ["(", "[", ")", "]", "."]:
                    # print(item)
                    # åˆå¹¶åˆ°å‰ä¸€ä¸ªitem
                    context[index - 1] = context[index - 1].rstrip() + " " + item.lstrip()
                    # åˆ é™¤å½“å‰item
                    del context[index]
                    # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                    continue
            index += 1


        # print(context)

        return context


    def step5_1_removepage(self, context):
        # context æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ª item æ˜¯ä¸€æ®µå†…å®¹
        context_lens = len(context)
        # ç”¨äºç»Ÿè®¡æœ‰å¤šå°‘ä¸ªæ®µè½ä¸­å‡ºç°äº†äººå
        num = 0
        # new_context = []
        for item in context:
            person_num, names_start2stop = NIL_tool(nlp,item)

            # åˆ¤æ–­å¥å­ä¸­äººåå å¥å­æ€»ä½“æ¯”ä¾‹æ¥åˆ¤æ–­å¥å­æ˜¯å¦ä¸ºå‚è€ƒæ–‡çŒ®
            # if step5_2_isreference(item,person_num,names_start2stop):
            #     # itemé‡å†™
            #     item = fr'æ•´æ®µåˆ é™¤å‚è€ƒæ–‡çŒ®:<u>{item}</u>'
            #     new_context.append(item)
            # else:
            #     new_context.append(item)

            if re.search(r'åˆ é™¤å‚è€ƒæ–‡çŒ®',item) and person_num >= 1:
                # å¦‚æœå½“å‰æ®µè½ä¸­æœ‰äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾
                num += 1
        if num >= context_lens * 0.5:
            context.insert(0, "æœ¬é¡µåœ¨è¶…è¿‡ä¸€åŠçš„æ®µè½ä¸­å‘ç°äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾")
        return context


    def step6_is_shortpage(self,context):
        duanluo_num = len(context)
        short_duanluo_num = 0
        if duanluo_num <= 3:
            for item in context:
                if len(item.strip()) < 100:
                    short_duanluo_num += 1
            if short_duanluo_num > 1:
                context.insert(0, "æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº3ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰2æ¡ä»¥ä¸Šåœ¨100ä»¥ä¸‹")
        elif duanluo_num <= 5:
            for item in context:
                if len(item.strip()) < 80:
                    short_duanluo_num += 1
            if short_duanluo_num > 3:
                context.insert(0, "æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº5ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰4æ¡ä»¥ä¸Š80ä»¥ä¸‹")
        else:
            # æ®µè½çŸ­
            for item in context:
                if len(item.strip()) < 50:
                    short_duanluo_num+=1
            if short_duanluo_num > duanluo_num * 0.5:
                context.insert(0, "æœ¬é¡µæœ‰è¶…è¿‡ä¸€åŠçš„æ®µè½é•¿åº¦å°äº50å­—ç¬¦")   # å¦‚æœæœ‰å¾ˆå¤šæ ‡é¢˜æ€ä¹ˆåŠï¼Œä¸€ä¸ªæ ‡é¢˜ä¸€æ®µæ–‡å­—
        return context


def clean_text(context, lang):
    split_token = "\n\n"

    if lang == 'en':
        pattern_list = pattern_list_en

    result = []

    sp = speicalProces()

    context = sp.step1_drop_Sidebar(context)
    context = sp.step2_drop_Pagefooter(context)
    context = post_process(context["text"])
    context = context.split(split_token)
    # print(context)

    context = sp.step3_removeLinefeed(context)

    context = sp.step4_linefeed(context)


    for item in context:
        item = item.strip(split_token).strip()

        if "## References" in item:
            continue
        for pattern_item in pattern_list:
            item = re.sub(pattern_item[0], pattern_item[1], item)
            item = item.strip()
        for pattern_item in context_pattern:
            item = re.sub(pattern_item[0], pattern_item[1], item)
        result.append(item)
    # ä½¿ç”¨NLPæ¨¡å‹åˆ¤æ–­å‚è€ƒæ–‡çŒ®
    context = sp.step5_1_removepage(result)


    context = sp.step6_is_shortpage(context)
    context = split_token.join(context)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    return context


fw = open("C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\medical_stage4_surya_preformat_4.jsonl", "w", encoding="utf-8")
with open("C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\medical_stage4_surya_preformat.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        # if item["seq_id"] == "24adf16c-e17b-44a5-a4b9-a3a49bff444d":
            # print(item)
        context = item
        # lang = item["lang"]


        context = clean_text(context, "en")
        context = post_process(context)
        if len(context) < 100:
            continue
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        fw.write(item + "\n")




