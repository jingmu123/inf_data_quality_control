import json
import os
import re
from tqdm import tqdm
import math
import numpy as np


pattern_list_en = [
    #å¸¦ç‰¹æ®Šç¬¦å·çš„å›¾ç‰‡ä¸­å†…å®¹
    [r'[^\n]*(â– |â)[^\n]*',''],
    [r'ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢', ''],
    #å¥æœ«æ•°å­—å¼•ç”¨
    ##1.å¸¦æ‹¬å· book (1ï¼Œ2).
    [r'(?<=([^0-9]|\.))\s*\(\d+\s*(?:[-,;â€“.]\d+)*\)\s*(?=(\n|\.|,))',''],
    ##2.ä¸å¸¦æ‹¬å·ä½†æ˜¯æ•°å­—å‰æ˜¯å¥å·. æ•°å­—åæ˜¯æ¢è¡Œæˆ–è€…å¤§å†™å­—æ¯
    [r'(?<=[^0-9]\.)\s*\d+\s*(?:[-,;â€“.]\d+)*(?=$)',''],
    ##3.å¸¦æ‹¬å·(30Â° downward approximately).(5.19)
    [r'(?<=\.)\(\d+\s*(?:[-,;â€“.]\d+)*\)',''],
    #æ— å…³å›¾ç‰‡å¼•ç”¨
    [r'[^\n]*\b(Fig(s?)(ure)?(s?))\b\.(\s*|Â )(\d+)?\.?[^\n]*',''],
    [r'(\()\b(Figure(s?)|figure(s?)|Section|diagram|Appendix|Box|(S|s)ource(s?)|Fig|p)\b:?\.?\s*(\d+)?(\.?).*?(\))',''],
    #å¥é¦–æ— å…³æ•°å­—
    [r'(?<=[^0-9]\.)\s*\d+(?:\s*[-;,â€“.]\s*\d+\s*)*\s*(?=[A-Z])',''],
    #å‚è€ƒæ–‡çŒ®
    [r'[^\n]*((\d+\s*(\(\d+\))?:)|(\s*\b(P|p)p\b\s*(:|\.)?))\s*\d+(?:[-;â€“]\d+)*[^\n]*',''],
    [r'[^\n]*[A-Z]+[a-z]*\s*[A-Z]*[a-z]*\.(\s*\d+?\s*(?:-\d+)*,?)?(\d+;)?\s*\d{4}[^\n]*',''],
    [r'[^\n]*([A-Z]|[a-z])ï¿¥[^\n]*',''],

    #å»é™¤å¸¦æœ‰ç½‘å€çš„å¥å­
    [r'[^\n\.]*(\()?\s*https?://[^\s]+\s*(\))?[^\n\.]*',''],
    [r'[^\n]*\b(?:(W|w)ww\.)?[-a-zA-Z0-9@:%._\+~#=]{2,256}\.[a-z]{2,6}\b[^\n]*', ''],
    [r'[^\n]*(D|d)oi:[^\n]*','']


        ]

context_pattern=[
    [r'Â¬\s*',''],
    [r'\(\s*\)','']


        ]
class speicalProces:
    def __init__(self):
        pass

    #å»é™¤å·¦å³ä¾§æ 
    def step1_drop_Sidebar(self,item):
        left_min, right_max = math.inf, 0
        text_boundary = []
        for dic in item["attr"]["raw_info"]:
            text_boundary.append(len(dic["raw_context"]))
        k = np.percentile(sorted(text_boundary), 60)

        for dic in item["attr"]["raw_info"]:
            if len(dic["raw_context"]) > k and dic["full_blocks"][2] > right_max:
                right_max = dic["full_blocks"][2]
                # left_min = dic["full_blocks"][0]
                print(right_max)
                # print(left_min)
            if len(dic["raw_context"]) > k and dic["full_blocks"][0] < left_min:
                left_min = dic["full_blocks"][0]

        for dic in item["attr"]["raw_info"]:
            if dic["full_blocks"][2] > right_max or dic["full_blocks"][0] < left_min:
                for dic_text in dic["raw_context"]:
                    item["text"] = re.sub(dic_text["text"], "", item["text"])
                    item["text"] = re.sub(dic_text["text"].strip("-"), "", item["text"])
        return item

    #å»é™¤é¡µè„š
    def step2_drop_Pagefooter(self,item):
        down_max = 0
        text_boundary = []

        for dic in item["attr"]["raw_info"]:
            text_boundary.append(len(dic["raw_context"]))
        k = np.percentile(sorted(text_boundary), 60)

        for dic in item["attr"]["raw_info"]:
            if len(dic["raw_context"]) > k and dic["full_blocks"][3] > down_max:
                down_max = dic["full_blocks"][3]

        for dic in item["attr"]["raw_info"]:
            if dic["full_blocks"][3] > down_max:
                for dic_text in dic["raw_context"]:
                    item["text"] = re.sub(dic_text["text"], "", item["text"])
                    item["text"] = re.sub(dic_text["text"].strip("-"), "", item["text"])
        return item



def clean_text(context, lang):
    split_token = "\n\n\n"

    if lang == 'en':
        pattern_list = pattern_list_en

    result = []
    sp = speicalProces()
    try:
        context=sp.step1_drop_Sidebar(context)
    except Exception as e:
        pass
    try:
        context=sp.step2_drop_Pagefooter(context)
    except Exception as e:
        pass

    context = context["text"]

    for item in context.split(split_token):
        # print(item)
        item = item.strip(split_token).strip()
        item = re.sub("\n\n","ï¿¥",item)

        for pattern_item in pattern_list:
            item = re.sub(pattern_item[0], pattern_item[1], item)
            item = item.strip()
        for pattern_item in context_pattern:
            item = re.sub(pattern_item[0], pattern_item[1], item)
        item = re.sub("ï¿¥","\n\n",item)

        result.append(item)
    context = split_token.join(result)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    #
    return context

fw = open("../../../../full_data/medical_stage4_surya/medical_stage4_surya_clean.jsonl", "w", encoding="utf-8")
with open("../../../../full_data/medical_stage4_surya/drop_o_imgbox/medical_stage4_surya_preformat.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())

        context=item
        # lang = item["lang"]
        context = clean_text(context, "en")
        context = post_process(context)
        print(context)
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        fw.write(item + "\n")

