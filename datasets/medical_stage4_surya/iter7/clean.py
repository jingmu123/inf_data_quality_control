import json
import re
from tqdm import tqdm
import math
import spacy
import random
from nltk.corpus import wordnet
import inflect
inflect = inflect.engine()
pattern_list = [



    # å»é™¤å¸¦æœ‰ç½‘å€çš„å¥å­,å…³é”®è¯   wwwã€comã€htmlã€http
    # todo www,httpæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå³è¾¹ç•Œçš„å‡†ç¡®æ€§éœ€è¦è€ƒè™‘
    # todo com,htmlæ”¾ä¸€èµ·è€ƒè™‘ï¼Œå·¦è¾¹ç•Œéœ€è¦è€ƒè™‘
    [r'([^\d][^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',r''],

    [r'(\n\s*[a-zA-Z\.]\s*\n)', r''],
    [r'([^\n]*Copyright[^\n]*)', r''],
    [r'(ISBN\s*[A-Z0-9-]*)', r''],
    # å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
    [r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â)', r''],
    # æ— å…³æ–‡æœ¬
    # æ— å…³å›¾ç‰‡å¼•ç”¨
    [r'((\()\b([Ff]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|Fig|p)\b:?\.?\s*(\d+)?\.?.*?(\)))',r''],
    [r'([^\n\.]*\b(Fig[s]?[ure]?[s]?)\b\.?[\s*Â ](\d+)?\.?[^\n]*)', r''],
    [r'((\([^\(]{0,20})\s[Ff]igures?[^\.\(\)]*.)',r''],
    [r'([\(\[]([Tt]able|[sS]ee)[^\)\]]*?[\)\]])',r''],




    # æ•°å­—å¼•ç”¨
    #1.å¸¦æ‹¬å· book (1ï¼Œ2).
    [r'(\([\d\s,\.\-â€“]{1,50}\))',r''],
    #2.ä¸å¸¦æ‹¬å·ä½†æ˜¯æ•°å­—å‰æ˜¯å¥å·. æ•°å­—åæ˜¯æ¢è¡Œæˆ–è€…å¤§å†™å­—æ¯
    # å¸¦[]ä¸ºguidelineså†™çš„
    [r'([^\d])(\[\s?\d{1,4}(\s{0,3}[\-â€“â€”,\.]\s{0,3}\d{1,4}){0,20}\s?\])',r'\1'],
    # [r'([^\d])([1-9]{1,3}(\s{1,3}[\-â€“,\.]\s{1,3}[1-9]{1,3}){1,20})([^\d]?)', r'\1åˆ é™¤2:<u>\2</u>\4'],
    # # ä¸å¸¦æ‹¬å·
    [r'([^\d])([1-9][0-9]{1,4}(\s{1,3}[\-â€“,\.]\s{1,3}[1-9][0-9]{1,4}){1,20})([^\d]?)', r''],
    # ä¸å¸¦æ‹¬å·æ•°å­—æˆ–æ•°å­—åŠ ç‚¹ï¼ˆå®¹æ˜“è¯¯åˆ ï¼‰
    [r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)', r""],
    # å‚è€ƒå¼•ç”¨ï¼šæ•°å­—ç±»å‹ï¼ŒåŠ å…¥ç©ºæ ¼åšæ³›å[1,2 - 3, 4, 5]
    [r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)', r""],
    # # å‚è€ƒåº”ç”¨çš„å¤æ‚æ ¼å¼ï¼šå­—æ¯(Ia,\nb)
    [r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]', r""],
    # ä¸Šä¸€å¥çš„å¥å·\dä¸­é—´æ²¡æœ‰ç‚¹[å¤§å†™]ï¼Œè¿™é‡Œçš„\dåé¢æ²¡æœ‰ç‚¹ä¸æ˜¯åºå·å¯èƒ½æ˜¯ä¸Šä¸€å¥çš„å¼•ç”¨æ•°å­—
    [r'([^\d][\.,])((\s{1,3}\d+[\s\n]{1,3}){1,5}\.?)(\s?[A-Z])',r'\1\4'],
    # ä»‹è¯å‰é¢æœ‰æ•°å­—ä¼šæœ‰é—®é¢˜
    # [r'((\d+[\.,]?){1,5})(\s(and|or|the|:)\s)',r'åˆ é™¤7:<u>\1</u>\3'],
    # ç»™guidelinesè¡¥å……
    [r'([^\d][\.,]\s?)([1-9][0-9]{1,4}(\s{0,3}[\-â€“,\.]\s{0,3}[1-9][0-9]{1,4}){1,20})(\n|\s?[A-Z])',r'\1\4'],
    # ç»“å°¾å¥å·åé¢ä¸ºæ•°å­—å’Œåºå·åŒºåˆ«å¼€åºå·åé¢è¿˜æœ‰ä¸€ä¸ª.
    [r'\.(\s?\d+)\n',r''],
    [r'(#{1,3})\n',r'\1']
 ]

context_pattern = [
    [r'(Â¬\s*)', r''],
    [r'(\(\s*\))', r'']
]

# nlp = spacy.load("en_core_web_trf")
nlp = spacy.load("en_core_web_sm")

class speicalProces:
    def __init__(self):
        pass

    def is_not_upper(self, char):
        return not char.isupper()


    def is_page_foot(self,least_bbox,img_box):
        """
        å·¦å³è¾¹è§’ï¼Œå®½åº¦å·®å€¼å°block[2]-block[0]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­å·¦å³è¾¹ç•Œ
        ä¸Šä¸‹è¾¹è§’ï¼Œé«˜åº¦å·®å€¼å°block[3]-block[1]<=20ï¼Œä¸”æ›´é è¿‘ç›’å­ä¸Šä¸‹è¾¹ç•Œ
        """
        # æ£€æµ‹å³æµ‹è¾¹è§’
        if img_box[2] - least_bbox[0] <= 80:
            return True
        # æ£€æµ‹ä¸‹é¢è¾¹è§’
        elif img_box[3] - least_bbox[1] <= 80:
            return True
        # æ£€æµ‹å·¦ä¾§è¾¹è§’
        elif least_bbox[2] - img_box[0] <= 80:
            return True
        # ä¸Šè¾¹è§’ä¼šé‡åˆ°æ ‡é¢˜è¿™ä¸ªé—®é¢˜ï¼Œè¦ä¸è¦è§£å†³ï¼Ÿ
        elif least_bbox[3] - img_box[1] <= 80:
            return True
        else:
            return False


    def step1_drop_Pagefooter(self, item):
        # print(json.dumps(item, ensure_ascii=False))
        """
        1.éå†æœ€å°çš„å—åˆ¤æ–­æ˜¯å¦ä¸ºé¡µè¾¹è§’
        2.åœ¨textä¸­æ‰¾åˆ°å†…å®¹ç»™åˆ æ‰
        """
        raw_info = item['attr']['raw_info']
        img_box = item['attr']['img_box']
        for raw in raw_info:
            raw_context = raw['raw_context']
            for least_block in raw_context:
                least_bbox = least_block['bbox']
                if self.is_page_foot(least_bbox,img_box):
                    least_text = least_block['text']
                    # print(least_text)
                    # å¯¹ least_text è¿›è¡Œæ­£åˆ™è½¬ä¹‰
                    escaped_least_text = re.escape(least_text)

                    # æ„å»ºæ­£åˆ™æ¨¡å¼ï¼ŒåŒ¹é…å¯èƒ½çš„å‰åç©ºæ ¼ã€æ¢è¡Œç¬¦å’Œè¿å­—ç¬¦
                    pattern = re.compile(escaped_least_text + r'[\s\n-]{0,5}')

                    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢åŒ¹é…çš„æ–‡æœ¬
                    item['text'] = re.sub(pattern, '', item['text'])

        # print(item)

        return item

    def delete_photopage(self,item):
        raw_info = item['attr']['raw_info']
        img_box = item['attr']['img_box']
        img_area = (img_box[2] - img_box[0])*(img_box[3] - img_box[1])
        all_block_area = []
        for raw in raw_info:
            full_blocks = raw['full_blocks']
            block_area = (full_blocks[2]-full_blocks[0])*(full_blocks[3]-full_blocks[1])
            all_block_area.append(block_area)

        if len(raw_info) <= 3 and sum(all_block_area) < img_area * 0.2:
            item['text'] = "(æœ¬é¡µåˆ é™¤)æ­¤é¡µçš„å†…å®¹éƒ¨åˆ†æ‰€å çš„æ¯”ä¾‹å°äº0.2"+item['text']

        return item

    def step2_more_linefeed(self, context):
        # print("Before processing:", context)
        index = 0
        while index < len(context):
            item = context[index]
            # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
            preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A']

            # å°† item æŒ‰ "\n" åˆ†å‰²
            item_sections = re.split(r'\n', item)
            section_index = 0

            while section_index < len(item_sections) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ

                if re.search(r'\s\d+\.$', item_sections[section_index]):  # åŒ¹é…æ®µè½ç»“å°¾æ˜¯æ•°å­—å’Œå¥ç‚¹
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                elif re.search(r'^([\d+A-Z][\.,]){1,3}$', item_sections[section_index]) and \
                        item_sections[section_index + 1].lstrip()[0].isupper():  # åŒ¹é…æ®µè½ä¸­åªæœ‰åºå·ä¸”ä¸‹ä¸€æ®µæ˜¯å¤§å†™å¼€å¤´çš„æƒ…å†µ
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                elif any(item_sections[section_index].rstrip().endswith(" " + prep) for prep in
                         preposition_list):  # åŒ¹é…åŒä¸€æ®µä¸­ä»‹è¯ç»“å°¾çš„
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                elif item_sections[section_index].rstrip()[-1] in ['-']:
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                elif "#" not in item_sections[section_index] and "*" not in item_sections[section_index] and re.search(
                        r'[^\.?!]$', item_sections[section_index]) and re.match(r'^[a-z]', item_sections[
                    section_index + 1].lstrip()):
                    item_sections[section_index] += " æ®µå†…åˆ é™¤æ¢è¡Œ-5 " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                elif re.search(r'\([^\)]*$|\[[^\]]*$', item_sections[section_index]) and re.match(r'^[^\(\[]*[\)\]]',
                                                                                                  item_sections[
                                                                                                      section_index + 1]):  # å‰ä¸€ä¸ªæ®µè½æœ‰ä¸€ä¸ªæœªå¯¹åº”çš„å·¦æ‹¬å·ï¼Œä¸‹ä¸€æ®µå‰é¢æœ‰ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„å³æ‹¬å·
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]

                else:
                    section_index += 1  # åªæœ‰åœ¨ä¸åˆå¹¶æ—¶æ‰è‡ªå¢

            # æ›´æ–° item ä»¥åæ˜ åˆå¹¶çš„æ®µè½
            item = '\n'.join(item_sections)
            context[index] = item

            # åˆå¹¶ä»¥å°å†™å­—æ¯æˆ–ç‰¹å®šæ ‡ç‚¹ç¬¦å·å¼€å¤´çš„æ®µè½
            stripped_item = item.strip()
            # print(stripped_item)
            if index > 0:
                if stripped_item and (stripped_item[0].islower() or stripped_item[0] in ["(", "[", ")", "]", "."]):
                    # ä¸Šä¸€æ®µä¸èƒ½å‡ºç°#ï¼Œå‡ºç°#è¯æ˜æ˜¯æ ‡é¢˜æ®µ
                    if not re.search(r'#',context[index - 1]):
                        # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                        context[index - 1] = context[index - 1].rstrip() + " " + item.lstrip()
                        # åˆ é™¤å½“å‰ item
                        del context[index]
                        # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue

                # æ–°å¢æ¡ä»¶: å¦‚æœ stripped_item ç»“å°¾çš„å•è¯åœ¨ preposition_list ä¸­
                elif any(stripped_item.rstrip().endswith(" " + prep) for prep in preposition_list):
                    if index + 1 < len(context):
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + " " + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue

                elif stripped_item[-1] == '-':
                    if index + 1 < len(context):
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + " " + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue

                elif "#" not in item and re.search(r'[^\.?!:ï¼šâ€"]$', item.strip()):

                    if index + 1 < len(context) and "#" not in context[index + 1]:
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + " " + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index-1
                        continue

            index += 1
        # print("After processing:", context)
        return context

    def step3_lack_linefeed(self,context):
        new_context = []
        for item in context:
            # æŸ¥æ‰¾ #â€¢ï¼Œå¹¶åœ¨å…¶å‰ååŠ æ¢è¡Œç¬¦
            # parts = re.split(r'(?<=\s)([#â€¢]{1,3}\s?[A-Z][^#â€¢]*)', item)
            # å®šä¹‰å¤šä¸ªåˆ†éš”ç¬¦å¹¶ç”¨ç«–çº¿è¿æ¥
            splitchar = r'([#â€¢]{1,3}\s)'
            # ä½¿ç”¨è¿æ¥åçš„æ­£åˆ™è¡¨è¾¾å¼æ‹†åˆ†
            parts = re.split(splitchar, item)
            new_parts = []
            for part in parts:
                if re.search(splitchar, part):
                    new_parts.append(part.strip())

                elif re.search(r'([\.\)])(\s\d{1,2}\.\s?[A-Z])',part):
                    part = re.sub(r'([\.\)])(\s\d{1,2}\.\s?[A-Z])',r'\1\n\2',part)
                    new_parts.append(part.strip() + '\n')
                # å¦‚æœæ˜¯ä¸€ä¸ªç½‘ç«™æ€ä¹ˆåŠ  www.sadadssa.com
                # elif re.search(r'([\.?!]\s)([a-z])',part):
                #     # åœ¨å¥æœ«æ ‡ç‚¹ç¬¦å·ï¼ˆã€‚ï¼Ÿï¼...ï¼‰åé¢æ¥å°å†™å­—æ¯çš„æƒ…å†µè¿›è¡Œæ¢è¡Œ
                #     part = re.sub(r'([\.?!]\s)([a-z])', r'\1\nå¢åŠ æ¢è¡Œ\2', part)
                #     new_parts.append(part.strip() + '\n')

                else:
                    new_parts.append(part.strip() + '\n')

            item = "".join(new_parts)
            # print(item)
            new_context.append(item)
        return new_context

    def get_person_idx(self, item):
        doc = nlp(item)
        person_block = []
        person_num = 0
        # print(doc.ents)
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if len(person_block) == 0:
                    person_block.append([ent.start_char, ent.end_char])
                elif ent.end_char - person_block[-1][-1] > 5:
                    person_block.append([ent.start_char, ent.end_char])
                else:
                    person_block[-1][-1] = ent.end_char
                person_num += 1
        return person_block, person_num

    def step4_rm_cite(self, item):
        # patterns = [
        #     r'\.\s?\b\d{4}\b',  # å¹´ä»½ï¼Œæ¯”å¦‚ . 2010
        #     r'\b\d{4}\b\s?;',  # å¹´ä»½ï¼Œæ¯”å¦‚ 2010;
        #     r'(?:Journal|Proceedings|Conference|Studies|Review|BMJ|JAMA|Pediatrics|Crit Care Med|Nurs Crit Care|Acad Emerg Med|Health Serv Res)',
        #     # æœŸåˆŠæˆ–ä¼šè®®åçš„å…³é”®è¯
        #     r'(?:doi:\s*\S+)',  # DOI
        #     r'(?:vol\.?\s*\d+)',  # å·å·
        #     r'(?:no\.?\s*\d+)',  # æœŸå·
        #     r'(?:pp\.?\s*\d+\s*-\s*\d+)',  # é¡µç èŒƒå›´
        #     r'\d+\s?:\d+\s?[â€“-]\s?\d+',  # é¡µç èŒƒå›´æ ¼å¼
        #     r'\set al\.'  # et al
        # ]
        cite_tag = []
        cite_index = 1 if len(re.findall(r'\[\d+\]', item)) > 0 else 0
        cite_year = 1 if len(re.findall(r'\[\d\d\d\d\]', item) or re.findall(r'\.\s?\b\d{4}\b',item) or re.findall(r'\b\d{4}\b\s?;',item)) else 0      # å¹´ä»½ï¼Œæ¯”å¦‚ . 2010 ã€ 2010;
        cite_page = 1 if len(re.findall(r'\d+\s?:\d+\s?[â€“-]\s?\d+',item)) else 0
        cite_J = 1 if len(re.findall(r'\[[Jj]\]', item)) else 0
        cite_doi = 1 if " doi " in item else 0
        cite_etal = 1 if " et al" in item else 0
        cite_vol = 1 if " vol. " in item else 0
        # cite_page = 1 if len(re.search(r'\.\s?\b\d{4}\b',item)) else 0
        cite_phonenum = 1 if re.search(r" [Pp]hone:|Fax:", item) else 0
        cite_tag = [cite_index, cite_year, cite_J, cite_doi, cite_etal, cite_page, cite_vol, cite_phonenum]
        if sum(cite_tag) > 1:
            return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(item)

        person_block, person_num = self.get_person_idx(item)
        # è¶…è¿‡5ä¸ªäººå
        person_block_lens = [block_item[1] - block_item[0] for block_item in person_block]
        person_lens = sum(person_block_lens)
        if person_lens / len(item) > 0.5 and len(item) > 100:
            return "å‚è€ƒåˆ é™¤-1:<u>{}</u>".format(item)
        if person_num > 5:
            return "å‚è€ƒåˆ é™¤-2:<u>{}</u>".format(item)
        # elif cite_index and person_num > 0:
        #     return "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(item)
        else:
            return item


    def is_plural(self, word):
        # ç»Ÿä¸€ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
        word = word.strip().lower()

        # å¦‚æœæ˜¯ç‰¹å®šçš„å•ä½ï¼Œä¸”ä¸åœ¨å•æ•°åˆ—è¡¨ä¸­ï¼Œåˆ™è®¤ä¸ºæ˜¯å¤æ•°
        if word in ['million','mm','cm','m','km','mg','g','kg','billion','Percent','percent','ratio','light-years','million', 'billion', 'cm', 'm', 'km', 'mg', 'g', 'kg', 'percent', 'ratio',
                    'dollar', 'yen', 'pound', 'peso', 'rand','baht', 'meter', 'metre', 'decimeter', 'liter', 'litre', 'gallon','dozen', 'score', 'series', 'species', 'headquarters', 'works',
                    'information', 'rice', 'money', 'advice', 'equipment', 'machinery','cannon', 'graffiti', 'pollen', 'offspring', 'bream', 'herring','shears', 'decade', 'century', 'year',
                    'month', 'hair', 'debris','research', 'piano', 'trombone','horse','day']:
            return True

        # å°è¯•è·å–å•è¯çš„å•æ•°å½¢å¼
        singular_form = inflect.singular_noun(word)

        # å¦‚æœæ²¡æœ‰å•æ•°å½¢å¼ï¼Œæˆ–è€…å•æ•°å½¢å¼ä¸åŸå•è¯ä¸åŒï¼Œåˆ™è®¤ä¸ºæ˜¯å¤æ•°
        if singular_form == False:
           return False
        elif singular_form != word:
            return True

        return False

    def check_number(self,context):
        new_context = []
        for item in context:
            # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ•°å­—æˆ–æ•°å­—èŒƒå›´åŠå…¶åé¢çš„å•è¯
            pattern = re.compile(r'[^\d](\b[\d]{1,3}(?:[,\.]\d{1,3})*(\s*(?:or|and|to|:|ï¼š|with|of|-)\s*[\d]{1,3}(?:[,\.]\d{1,3})*){0,3}\b)\s+(\w+)')

            matches = pattern.findall(item)

            for number,kongbuhuozu,word in matches:
                # print(number,word)
                # å¤æ•°ä¸åšå¤„ç†ï¼Œå¦åˆ™åšå¤„ç†
                if not self.is_plural(word):
                    # print(number,word)
                    item = re.sub(rf'\b({re.escape(number)})(\s+{re.escape(word)})\b', r'æ£€æŸ¥å¤æ•°åˆ é™¤:<u>\1</u>\2',item)
            new_context.append(item)
        return new_context

    def step4_removepage(self, context):
        # context æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ª item æ˜¯ä¸€æ®µå†…å®¹
        context_lens = len(context)
        # ç”¨äºç»Ÿè®¡æœ‰å¤šå°‘ä¸ªæ®µè½ä¸­å‡ºç°äº†äººå
        num = 0
        new_context = []
        for item in context:
            # è¿”å›çš„itemæ˜¯å·²ç»è¢«é‡å†™è¿‡çš„item
            item = self.step4_rm_cite(item)
            # æ–°çš„itemé‡æ–°åŠ å…¥ä¸€ä¸ªæ–°çš„åˆ—è¡¨
            new_context.append(item)
            # åˆ¤æ–­itemæ˜¯å¦è¢«åˆ¤å®šæœªå‚è€ƒæ–‡çŒ®
            if re.search(r'å‚è€ƒåˆ é™¤',item):
                # å¦‚æœå½“å‰æ®µè½ä¸­æœ‰äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾
                num += 1
        # å¯¹æ•´é¡µçš„ä¸€ä¸ªåˆ¤æ–­
        if context_lens >= 4 and num >= context_lens * 0.5:
            new_context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µåœ¨è¶…è¿‡ä¸€åŠçš„æ®µè½ä¸­å‘ç°äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾")
        return new_context


    def step5_is_shortpage(self,context):
        duanluo_num = len(context)
        short_duanluo_num = 0
        if duanluo_num <= 3:
            for item in context:
                if len(item.strip()) < 100:
                    short_duanluo_num += 1
            if short_duanluo_num > 1:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº3ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰2æ¡ä»¥ä¸Šåœ¨100ä»¥ä¸‹")
        elif duanluo_num <= 5:
            for item in context:
                if len(item.strip()) < 80:
                    short_duanluo_num += 1
            if short_duanluo_num > 3:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µçš„æ®µè½æ•°é‡å°äºç­‰äº5ä¸”è‡³å°‘æ®µè½é•¿åº¦æœ‰4æ¡ä»¥ä¸Š80ä»¥ä¸‹")
        else:
            # æ®µè½çŸ­
            for item in context:
                if len(item.strip()) < 50:
                    short_duanluo_num+=1
            if short_duanluo_num > duanluo_num * 0.7:
                context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µæœ‰è¶…è¿‡ä¸€åŠçš„æ®µè½é•¿åº¦å°äº50å­—ç¬¦")   # å¦‚æœæœ‰å¾ˆå¤šæ ‡é¢˜æ€ä¹ˆåŠï¼Œä¸€ä¸ªæ ‡é¢˜ä¸€æ®µæ–‡å­—
        return context


def clean_text(context, lang):
    split_token = "\n\n"

    # if lang == 'en':
    #     pattern_list = pattern_list_en

    result = []

    sp = speicalProces()
    """
    ç›®å‰çš„é¡ºåº
    1.åˆ é™¤é¡µè„š
    æ–°åŠ  æ ¹æ®å—çš„é¢ç§¯æ¥åˆ¤æ–­æ­¤é¡µå†…å®¹ä¸ºå›¾ç‰‡çš„æè¿°  åˆ é™¤æ­¤é¡µ
    2.è§£å†³æ¢è¡Œçš„é—®é¢˜åŒ…æ‹¬å¤šäºæ¢è¡Œã€ç¼ºå°‘æ¢è¡Œ
    3.åˆ¤å®šå‚è€ƒæ–‡çŒ®
    4.æ­£åˆ™æ›¿æ¢
    5.åˆ¤æ–­æ•´é¡µé•¿çŸ­é—®é¢˜
    """
    context = sp.step1_drop_Pagefooter(context)

    context = sp.delete_photopage(context)

    context = post_process(context["text"])
    # contextæ˜¯ä¸€ä¸ªä»¥ä¸¤ä¸ªæ¢è¡Œç¬¦ä¸ºåˆ‡å‰²æ¡ä»¶çš„åˆ—è¡¨
    context = context.split(split_token)
    # å¤šä½™æ¢è¡Œ
    context = sp.step2_more_linefeed(context)
    # ç¼ºå°‘æ¢è¡Œ
    context = sp.step3_lack_linefeed(context)

    # åˆ¤å®šå‚è€ƒæ–‡çŒ®
    context = sp.step4_removepage(context)
    # print(context)
    for item in context:
        item = item.strip(split_token).strip()

        if "## References" in item:
            continue
        for pattern_item in pattern_list:
            item = re.sub(pattern_item[0], pattern_item[1], item)
            item = item.strip()
        for pattern_item in context_pattern:
            item = re.sub(pattern_item[0], pattern_item[1], item)
        result.append(item)
    # ä½¿ç”¨NLPæ¨¡å‹åˆ¤æ–­å‚è€ƒæ–‡çŒ®

    # åˆ¤æ–­æ•´é¡µçŸ­è·¯é•¿çŸ­
    context = sp.step5_is_shortpage(result)

    # æ•°å­—åˆ¤å®š
    # context = sp.check_number(context)

    context = split_token.join(context)
    return context


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)

    # context = re.sub("[li][\.,]" , '1.' ,context)
    return context



fw = open("C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\medical_stage4_surya_preformat_6.jsonl", "w",encoding="utf-8")
with open("C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\medical_stage4_surya_preformat.jsonl", "r",encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        # if item["seq_id"] == "c2f61d69-f1e9-4016-981a-60737def4dbe":

        context = item
        # lang = item["lang"]

        context = clean_text(context, "en")
        context = post_process(context)
        if len(context) < 100:
            continue
        item["text"] = context
        item = json.dumps(item, ensure_ascii=False)
        # print(item)
        fw.write(item + "\n")
#



# # æ–‡ä»¶è·¯å¾„
# input_file_path = "C:\\pycharm\\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\\pdf\\clean_json\\guidelines_liangyong_surya_preformat_en.jsonl"
# output_file_path = "C:\\pycharm\\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\\pdf\\clean_json\\guidelines_liangyong_surya_preformat_en_2_5000.jsonl"
#
# # è¯»å–æ‰€æœ‰è®°å½•
# with open(input_file_path, "r", encoding="utf-8") as fs:
#     lines = fs.readlines()
#     # éšæœºæŠ½å–5000æ¡è®°å½•
#     sampled_lines = random.sample(lines, 5000)
# # å¤„ç†å¹¶ä¿å­˜æŠ½å–çš„è®°å½•
# with open(output_file_path, "w", encoding="utf-8") as fw:
#     for items in tqdm(sampled_lines):
#         item = json.loads(items.strip())
#         # if item["seq_id"] == "f7afd344-b77a-4e73-92f1-65eb9910689a":
#         context = item
#
#         # æ¸…æ´—å’Œå¤„ç†æ–‡æœ¬
#         context = clean_text(context, "en")
#         context = post_process(context)
#
#         if len(context) < 100:
#             continue
#
#         item["text"] = context
#         item = json.dumps(item, ensure_ascii=False)
#         # print(item)
#         fw.write(item + "\n")



