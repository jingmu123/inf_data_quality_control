import json
import os
import re
import random

import jieba
from langdetect import detect
from langdetect import detect_langs
from tqdm import tqdm

url_pattern = r"(https?:\/\/)?(www\.)?([\da-z\.\-@]+)\.([a-z\.]{2,6})([\/\w \.-]+)?\/?"
pattern_list_zh = [
        [r'æ–‡ä¸­æœ‰.*å…·ä½“æŒ‡å¯¼ã€‚', ''],
        ['\\((åŸºç¡€ç¯‡|é«˜çº§ç¯‡|Beyond the Basics|è§ä¸‹æ–‡|é™„è§†é¢‘)\\)', ''],  #ï¼ˆåŸºç¡€ç¯‡ï¼‰ã€ï¼ˆé«˜çº§ç¯‡ï¼‰ã€ï¼ˆBeyond the Basicsï¼‰ã€ï¼ˆè§ä¸‹æ–‡ï¼‰ã€ï¼ˆé™„è§†é¢‘ï¼‰
        [r'\(å‚è§[^\(\)]*(\([^\(\)]*\)[^\(\)]*){0,5}\)', ''], #ï¼ˆå‚è§...ï¼‰  ï¼ˆå‚è§ï¼ˆ...ï¼‰{0,5}ï¼‰7/24uptodata_newä¿®æ”¹
        ['(((åŸºç¡€|é«˜çº§)ç¯‡)?(\\(å‚è§.*\\))?)|(æ‚£è€…æ•™è‚²\\s*[ï¼š:â€”-].*(\\(åŸºç¡€ç¯‡\\))?)|(Patient education:.*)', ''], # 1ã€åŸºç¡€ç¯‡å‚è§...2ã€...æ‚£è€…æ•™è‚²ï¼š...3ã€...æ‚£è€…æ•™è‚²-...4ã€...æ‚£è€…æ•™è‚²â€”...5ã€Patient education:...
        ['\\((æµç¨‹å›¾|figure|NCT|Grade|è§†é¢‘|è®¡ç®—å™¨|æ³¢å½¢|è¡¨|è¡¨æ ¼|å›¾|å›¾ç‰‡|å›¾è¡¨)\\s*\\d+.*?\\)', ''],  #ï¼ˆæµç¨‹å›¾ 1ï¼‰ã€ï¼ˆfigure 1ï¼‰ç­‰
        ['\\d{3}-\\d{3}-\\d{4}', ''], #232-432-4122
        ['(<sup>)?(\\\\)?(\\[|ï¼»)(\\d+|(\\d+[,.ã€~ï½-]\\s*\\d+.*))(\\\\)?(\\]|ï¼½)(</sup>)?', ''],   #[1,2] [1.2] [1ã€2] [1-2] \[1.2\] <sup>\[1.2\]</sup>
        # ['\\(å‚è§\\s*[^ï¼ˆ]+(\\([^)]*\\))?[^)]*\\)', ''],#ï¼ˆå‚è§ï¼ˆ...ï¼‰ï¼‰
        ['å‚è§ç½‘é¡µ\\(here\\)', ''],
        ['å…³äº.*å‚è§.*(\\n.*[ï¼š:].*)*', ''],
        # ['\\((\\[?)\\s*#?((\\d+-\\s*\\d+-\\s*\\d+)|(\\d+-\\s*\\d+)|(\\d+(,|ï¼Œ)\\s*\\d+.*)|(\\d+))(\\]?).*?\\)', ''], #1.(23-1-32...) (12,dadada) ([12.åŒ»ç–—æ•°æ®])
        ['(.*)å­¦ä¼šæŒ‡å—é“¾æ¥(.*)', ''],
        # ['[^ã€‚](è§?)([^ã€‚]*)ä¸“é¢˜(.*)', ''],
        ['æ›´å¤š|æ€»ç»“ä¸æ¨è|(\\(å½±åƒ.*?\\))|åœ¨çº¿èµ„æºå’Œæ”¯æŒç»„ç»‡|ä¿¡æ¯å‚è§ç½‘ç«™|å¦‚å›¾æ‰€ç¤ºï¼š', ''], #æ›´å¤šã€æ€»ç»“ä¸æ¨èã€æ€»ç»“ã€åœ¨çº¿èµ„æºå’Œæ”¯æŒç»„ç»‡ã€ï¼ˆå½±åƒ...ï¼‰ã€ä¿¡æ¯å‚è§ç½‘ç«™
        ['â—|â€¢|â¤ï¸', ''],
        ['(^\\s*(â€“|â€”))|((-|â€“|â€”)\\s*$)', ''], #-åŒ»ç–—ã€åŒ»é™¢-
        [r'[^ã€‚]*(è¯¦[^ã€‚]*è§|è§?[^ã€‚]*ä¸“é¢˜|è§?é™„è¡¨|è§(ä¸‹|ä¸Š)æ–‡(æµç¨‹å›¾)?|é™„å›¾)[^ã€‚]*ã€‚',r''],  # 7/30
        ['[^ã€‚](å‚è§é™„å›¾|è¯¦è§).*', ''],
        ['.*è§?å‚è€ƒæ–‡çŒ®.*', ''],
        ['.*the website.*', ''],
        ['è‡´è°¢.*', ''],
        ['(ï¼Œ|ã€‚)ã€‚','ã€‚'],
        # 7/24uptodata_newä¿®æ”¹
        [r'\\\[[\d\s\-,â€”\\]{0,100}\]',''],
        [r'\([^\(\)]{1,50}(æµç¨‹å›¾|figure|NCT|Grade|è§†é¢‘|è®¡ç®—å™¨|æ³¢å½¢|è¡¨|è¡¨æ ¼|å›¾|å›¾ç‰‡|å›¾è¡¨|å½±åƒ)[^\(\)]{1,50}\)',''],
        # 7/25
        [r'^ç”± UpToDate çš„åŒ»ç”Ÿ.*',r''],
        [r'^There is a newer version of this topic available in English.*',r''],
        [r'^è¯¥ä¸“é¢˜æœ‰ä¸€ä¸ªæ›´æ–°ç‰ˆæœ¬.*',r''],
        [r'^è¯·é˜…è¯»æœ¬é¡µæœ«çš„.*',r''],


]

pattern_list_en = [
                    ['\\s*â—\\(See.*', ''],
                    ['\\[\\d+([,ï¼Œã€-]\\d+)*\\]', ''], #[1,2] [1.2] [1ã€2] [1-2]
                    ['\\*\\s*urn:lims:.*?â€¢?\\s*No\\s*', ''],#*   urn:lims:b498:s2691415 No
                    ['Yes', ''],
                    [' ?View Patient Education', ''],
                    ['\\((picture|figure|table)\\s*.*\\)', ''], #(picture1) (figure 2-1xxx)
                    [r'[\sâ€¢\\-]{0,5}\((See|see|ESMO|ESC|ASCO)[^\(\)]*(\([^\(\)]*\)[^\(\)]*){0,}\)', ''],#(see table...)    7/25ä¿®æ”¹
                    ['\\(show table.*\\)', ''], #(show table...)
                    ['(.*)(for|For) additional information(.*)', ''],
                    ['(.*)See individual agents(.*)', ''],
                    ['(.*)Reference Range(.*)', ''],
                    ['(.*)Consumer Information Use and Disclaimer(.*)', ''],
                    ['(.*Last Reviewed Date.*)|(SUMMARY AND RECOMMENDATIONS)|SUMMARY|ACKNOWLEDGMENTS|(SOCIETY GUIDELINE LINKS)', ''],#...Last Reviewed Date...ã€SUMMARY AND RECOMMENDATIONSã€SUMMARYã€ACKNOWLEDGMENTSã€SOCIETY GUIDELINE LINKS
                    ['\\(å‚è§.*\\)', ''],
                    ['(.*)è§(.*)ä¸“é¢˜(.*)', ''],
                    ['(.*\\(ç¬¬\\d+ç‰ˆ\\))|(.*ä¸“å®¶(å…±è¯†|å»ºè®®)(\\(\\d+.*ç‰ˆ\\))?)|(.*(ä¸´æ—¶|é˜²æ§)æŒ‡å—)(ä¸“é¢˜)?|(å­¦ä¼šæŒ‡å—é“¾æ¥ï¼š.*)|(Society guideline links:.*)', ''], #...(ç¬¬1ç‰ˆ)ã€...ä¸“å®¶å…±è¯†ã€...æŒ‡å—ã€å­¦ä¼šæŒ‡å—é“¾æ¥ï¼š...ã€Society guideline links:...
                    ['(More on this topic)|(Patient education:.*)', ''],
                    ['(ğŸ‘|â–¶|â—|Â©|Â®|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ|ï¿½|âˆ‘|âœ¦|â¤ï¸|â¤)', ''],
                    ['(^\\s*(â€“|â€”))|((-|â€“|â€”)\\s*$)', ''], #-patientã€doctor-
                    ['\\((\\[?)\\s*#?((\\d+-\\s*\\d+-\\s*\\d+)|(\\d+-\\s*\\d+)|(\\d+(,|ï¼Œ)\\s*\\d+.*)|(\\d+))(\\]?).*?\\)', ''], #1.(23-1-32...) (12,dadada) ([12.åŒ»ç–—æ•°æ®])

                    [r'\\\[[\d\s\-,â€”\\]{0,100}\]',''],

                    [r'(\([^\(\)]{1,50}){1,}([fF]igure|NCT|Grade|[pP]icture|FIGURE|PICTURE|[iI]mage|[tT]able)\s([^\(\)]{1,50}\)){1,}',''],  #   ( figure 2 ) ( ( figure 2 ), panels A and C)
                    [r'\(\s+Ref\s+\)',''],
                    [r'\([^\)\(]{1,50}\d{4};[^\)\(]{1,200}\)',''],

                    [r'\([^\(\)]{0,100}algorithm\s[^\(\)]{0,100}\)',''],
                    [r'\(\s?[A-Z][^\(\)]{0,20}\s\d{4}[^\(\)]{0,50}\)',''],
                    [r'^Contributor Disclosures',''],
                    [r'^\s?(Please read the Disclaimer at the end of this page|Links to society and government-sponsored guidelines|Beyond the Basics topics).*',''],
                    [r'\([^\(\)]{1,50}(waveform|movie|calculator)[^\(\)]{1,50}\)','']
                     ]


class speicalProces:
    def __init__(self):
        pass
    def step1_drop_sentenc(self,content):
        pattern3=r'ã€‚?.*è§.*è¯¦.*?[ã€‚ï¼Œ]'
        pattern1=r'ã€‚?.*é¢˜ä¸“.*è§.*?[ã€‚ï¼Œ]'
        pattern2=r'ã€‚.*è¡¨é™„è§.*?[ã€‚ï¼Œ]'
        pattern4=r'ã€‚(å›¾ç¨‹æµ)?æ–‡(ä¸‹|ä¸Š)è§.*?[ã€‚ï¼Œ]'
        text=content.strip('\n').split("\n")
        for i in range(len(text)):
            text[i] = re.sub(pattern3, 'ã€‚', text[i][::-1])[::-1]
            text[i]=re.sub(pattern1,'ã€‚',text[i][::-1])[::-1]
            text[i] = re.sub(pattern2, 'ã€‚', text[i][::-1])[::-1]
            text[i] = re.sub(pattern4, 'ã€‚', text[i][::-1])[::-1]
        text='\n'.join(text)
        return text

    def step2_endding_filter(self,content):
        if "æ‰“å°" in content or "é‚®ä»¶" in content or "è‡´è°¢" in content or "æ„Ÿè°¢" in content or "å‚è§" in content or "ä¸‹æ–‡" in content or "ä¸Šæ–‡" in content or "æµç¨‹å›¾" in content or "ç½‘ç«™" in content:
            return True
        if "uptodate" in content.lower():
            return True
        if re.search(r'ä¸“é¢˜[^ã€‚]*ç‰ˆæœ¬',content):
            return True
        # if len(re.findall(url_pattern, content)) >= 1:
        #     return True
        return False

    def step3_reference(self, context):
        new_context = []
        references_started = False   #å®šä¹‰ä¸€ä¸ªåˆ é™¤referenceçš„å¼€å…³  åªè¦å‡ºç°å›ºå®šæ ¼å¼çš„è¡¨è¿°å°±å¯¹åé¢çš„å†…å®¹è¿›è¡Œåˆ é™¤
        introduce = 0
        introduce_index = []
        Inc = 0
        Inc_index = []

        guidelines = 0
        guidelines_index = []
        for index, item in enumerate(context):
            if re.search(r'^(References|å‚è€ƒæ–‡çŒ®|è§å‚è€ƒæ–‡çŒ®|è‡´è°¢|REFERENCES|ACKNOWLEDGMENT|The following organizations also provide reliable health information|More on this topic|Topic[^\.]*Version|For country code abbreviations|ACKNOWLEGMENT)', item.strip()):
                references_started = True
            if references_started:
                item = ""

            if re.search(r'^2024Â© UpToDate, Inc', item.strip()):
                Inc += 1
                Inc_index.append(index)
            if re.search(r'ALERT: ', item.strip()):
                Inc -= 1
                Inc_index.append(index)

            # è¦åˆ é™¤ä»Authoråˆ°å¼•è¨€ è®¾å®šäº†ä¸¤ä¸ªæ¡ä»¶åœ¨å¾ªç¯æ—¶åŒæ—¶å‡ºç°Authorå’Œå¼•è¨€ï¼Œè®°ä¸‹indexï¼Œå¯¹ç›¸åº”çš„indexè¿›è¡Œåˆ é™¤
            if re.search(r'^(Author)', item.strip()):
                introduce += 1
                introduce_index.append(index)
            if re.search(r'^(å¼•è¨€|ç®€ä»‹)', item.strip()) or re.search(r'^INTRODUCTION', item.strip()) or re.search(r'^Please read the Disclaimer at the end of this page',item.strip()):
                introduce -= 1
                introduce_index.append(index)

            if re.search(r'^(SOCIETY GUIDELINE LINKS)',item.strip()):
                guidelines += 1
                guidelines_index.append(index)
            if re.search(r'^INFORMATION FOR PATIENT',item.strip()) and guidelines == 0:
                guidelines += 1
                guidelines_index.append(index)
            if re.search(r'^.{,5}(Basics topics|Beyond the Basics topics)', item.strip()):
                guidelines -= 1
                guidelines_index.append(index)

            new_context.append(item)

        if introduce <= 0 and len(introduce_index) >= 2:
            start_index = introduce_index[0]
            end_index = introduce_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''


        if Inc <= 0 and len(Inc_index) >= 2:
            start_index = Inc_index[0]
            end_index = Inc_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''



        if guidelines <= 0 and len(guidelines_index) >= 2:
            start_index = guidelines_index[0]
            end_index = guidelines_index[-1]
            # å¾ªç¯éå†éœ€è¦æ›¿æ¢çš„ç‰‡æ®µ
            for i in range(start_index, end_index + 1):
                # å°†å½“å‰ç´¢å¼•å¤„çš„å­—ç¬¦æ›¿æ¢ä¸ºä½ æƒ³è¦çš„å­—ç¬¦ï¼Œè¿™é‡Œä»¥ç©ºå­—ç¬¦ä¸ºä¾‹
                new_context[i] = ''

        return new_context
    def step4_rm_kongge(self, context):
        context = context.lstrip().rstrip()
        content = context.split(" ")
        first_content = content[0]
        last_content = " ".join(content[1:])
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', last_content)
        # å¤šæ‰§è¡Œä¸€æ¬¡ï¼Œå¼¥è¡¥æ­£åˆ™è¾¹ç•Œé‡å é—®é¢˜ï¼›
        final_content = re.sub(r'([\u4e00-\u9fa5]) {1,3}([\u4e00-\u9fa5])', r'\1\2', final_content)
        if len(final_content) == 0:
            return first_content

        merge_piece = first_content + final_content.lstrip()[0]

        split_word = list(jieba.cut(merge_piece, cut_all=False))
        if len(split_word[-1]) > 1:
            return first_content + final_content
        return first_content + " " + final_content

def clean_text(context, lang):
    split_token = "\n\n"
    if split_token not in context:
        split_token = "\n"


    if lang == "zh":
        pattern_list= pattern_list_zh
    elif lang=='en':
        pattern_list= pattern_list_en
    else:
        pattern_list = pattern_list_en+pattern_list_zh

    # åˆ†è§£å¤„ç†
    result = []
    sp = speicalProces()

    # special_processï¼š
    # context = sp.step1_drop_sentenc(context)
    context = context.split(split_token)

    # 7/24uptodata_newä¿®æ”¹
    context = sp.step3_reference(context)

    for item in context:
        # 1.æ­£åˆ™
        for pattern_item in pattern_list:
            src = pattern_item[0]
            tgt = pattern_item[1]
            # print(pattern_item)
            # print(re.findall(src, item))
            item = re.sub(src, tgt, item)
        if lang == "zh":
            item = sp.step4_rm_kongge(item)
        if "url:" not in item and sp.step2_endding_filter(item):
            # print(item)
            continue

        result.append(item)
    for item in result:
        print(item)
    # æ•´åˆ
    context = split_token.join(result)

    return context

def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    # å¯¹å¤šæ ‡ç‚¹è¿›è¡Œæ›¿æ¢
    context = re.sub(r'[ã€‚ï¼Œ\.](\s?[ã€‚ï¼Œ\.ï¼š]){1,5}',r'ã€‚',context)
    context = re.sub(r'[,\.](\s+[,\.]){1,5}',r'.',context)
    return context


#è¯»jsonl
fw = open(r"C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\reclean3_uptodate_new_preformat_zh.jsonl", "w", encoding="utf-8")
with open(r"C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\original_data\uptodate_new_preformat.jsonl", "r", encoding="utf-8") as fs:
    lines = fs.readlines()

    # éšæœºæŠ½å–5000æ¡è®°å½•
    sampled_lines = random.sample(lines, 1000)
    for items in tqdm(sampled_lines):
        item = json.loads(items.strip())
        # if item["seq_id"] == "aecebefc-b489-471e-82ee-a9b12fb2ee91":
        context = item["text"]
        lang = item["lang"]
        if lang == "zh":
            if re.search("Links to related guidelines are provided separately", context):
                continue
            context = clean_text(context, lang)
            context = post_process(context)
            # print(context)
            item["text"] = context
            item = json.dumps(item, ensure_ascii=False)
            print(item)
            fw.write(item + "\n")






