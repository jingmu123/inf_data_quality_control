import json
import re

import numpy as np
from tqdm import tqdm
import math
import spacy
import random
from nltk.corpus import wordnet
import inflect
inflect = inflect.engine()
import kenlm
from nltk.tokenize import word_tokenize
import fasttext
from collections import defaultdict
model = kenlm.LanguageModel(r"C:\Users\Administrator\Desktop\4k_gram.klm")
fasttext_model = fasttext.load_model('fenlei_model.bin')


import signal
from tqdm import tqdm
from contextlib import contextmanager


# å¸¦æ ‡ç­¾ç‰ˆ
pattern_list = [
# å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
[
    r'(\([^\(\)]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\)\(]*\))',
    r'åˆ é™¤20:<u>\1</u>'
],
[
    r'([^\d][^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',
    r'åˆ é™¤1:<u>\1</u>'
],
[
    r'(\n\s*[a-zA-Z\.]\s*\n)',
    r'åˆ é™¤2:<u>\1</u>'
],
[
    r'([^\n]*Copyright[^\n]*)',
    r'åˆ é™¤3:<u>\1</u>'
],
[
    r'(ISBN\s*[A-Z0-9-]*)',
    r'åˆ é™¤4:<u>\1</u>'
],
[
    r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ|ï¿½|âˆ‘|âœ¦|â¤ï¸|â¤)',
    r'åˆ é™¤5:<u>\1</u>'
],
[
    r'(\(\s?\b([Ff]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|[Ff]igs?|p|FIGURE)\b[^\)\n]*\))',
    r''
],

[
    r'((\([^\(]{0,20})\s[Ff]igures?[^\(\)]*.)',
    r'åˆ é™¤8:<u>\1</u>'
],
[
    r'([\(\[]\s?([Tt]able|[sS]ee)[^\)\]]*?[\)\]])',
    r'åˆ é™¤9:<u>\1</u>'
],
# [
#     # r'([^\n\.]*\bFigs?(ure)?s?\b\.?[\s*Â ](\d+)?\.?[^\n]*)',
#     r'([^\n\.]*(Figs?(ure)?s?|FIGURE)\s?\.?\s?(\d+\.\d+)?[^\n\.]*\.?)',
#     r'åˆ é™¤7:<u>\1</u>'
# ],
# [
#     r'([^\n\.]*Fig[s]?(ure)?s?(\s\d\.)?[^\n\.]*\.)',
#     r'åˆ é™¤10:<u>\1</u>'
# ],
[
    r'(\([\d\s,\.\-â€“]{1,50}\))',
    r'åˆ é™¤11:<u>\1</u>'
],
[
    r'([^\d])(\[\s?\d{1,4}(\s{0,3}[\-â€“â€”,\.]\s{0,3}\d{1,4}){0,20}\s?\])',
    r'\1åˆ é™¤12:<u>\2</u>'
],
[
    r'([^\d])([1-9][0-9]{1,4}(\s{0,3}[\-â€“,\.]\s{1,3}[1-9][0-9]{1,4}){1,20})([^\d]?)',
    r'\1åˆ é™¤13:<u>\2</u>\4'
],
[
    r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)',
    r'åˆ é™¤14:<u>\1</u>'
],
[
    r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]',
    r'åˆ é™¤15:<u>\1</u>'
],
[
    r'([^\d][\.,])((\s{1,3}\d+[\s\n]{1,3}){1,5}\.?)(\s?[A-Z])',
    r'\1åˆ é™¤16:<u>\2</u>\4'
],
[
    r'([^\d][\.,]\s?)(\d{1,4}(\s{0,3}[\-â€“,\.\s]\s{0,3}\d{1,4}){0,20})(\n|\s?[A-Z]|$)',
    r'\1åˆ é™¤17:<u>\2</u>\4'
],
[
    r'(#{1,3})\n',
    r'åˆ é™¤18:<u>\1</u>'
],
[
    r'(\([^\(\)]{0,100}(\set[\s\xa0]{1,3}al|\d{4})[^\(\)]{0,100}\))',
    r'åˆ é™¤19:<u>\1</u>'
],
# [
#     r'([a-z\)])(---)(a-z)',
#     r'\1<u>\2æ›¿æ¢ä¸º-</u>\3'
# ],
# [
#     r'^\s?(---)\s?$',
#     r'<u>\1æ›¿æ¢ä¸º-</u>'
# ]
[
    r'(\^\d+(\s?,\s?\d+){0,5})',
    r'åˆ é™¤21:<u>\1</u>'
]
]

context_pattern = [
    [r'(Â¬\s*)', r''],
    [r'(\(\s*\))', r'']
]

# nlp = spacy.load("en_core_web_trf")
nlp = spacy.load("en_core_web_sm")
# å¢åŠ æœ€å¤§é•¿åº¦é™åˆ¶
# nlp.max_length = 10000000  # å°†æœ€å¤§é•¿åº¦å¢åŠ åˆ°200ä¸‡å­—ç¬¦ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰

class speicalProces:
    def __init__(self):
        pass

    def is_not_upper(self, char):
        return not char.isupper()

    def step1_delete_photo_describe(self, duans):
        """
        ç”¨\nå¯¹æ®µè¿›è¡Œåˆ‡åˆ†ï¼Œåˆ‡åˆ†æˆè¡Œï¼Œè¿›è¡Œåˆ¤æ–­ï¼Œä»¥è¡Œä¸ºå•ä½åˆ é™¤å›¾ç‰‡æè¿°çš„
        """
        new_duans = []
        for duan in duans:
            lines = duan.split('\n')
            for line in lines:
                # print(line.strip())
                # print("*"*50)
                if re.search('^([#*]{0,5}).{0,5}\s?([Ff]igs?(ure)?s?|FIGS?(URE)?s?|å›¾)',line.strip()) or re.search('\.(png|jpg|PNG|JPG)',line.strip()):
                    escaped_least_text = re.escape(line)
                    try:
                        duan = re.sub(rf'{escaped_least_text}', rf'åˆ é™¤å›¾ç‰‡æè¿°:<u>{line}</u>', duan)
                    except re.error as e:
                        print(f"Regex error: {e}")
            new_duans.append(duan)
        return new_duans

    def get_score(self,sentence):
        tokenize_text = word_tokenize(sentence)
        final_text = " ".join(tokenize_text)
        score = model.score(final_text, bos=False, eos=False)
        length = len(tokenize_text)
        if length > 0:
            score = (10 ** (-score / length))
        return score

    def is_mulupage(self,text):

        label = fasttext_model.predict(text.strip().replace('\n', ''))
        print(label[0][0])
        if label[0][0] in ['__label__mulu', '__label__cankao']:
            text = "(æœ¬é¡µåˆ é™¤)æœ¬é¡µè¢«æ¨¡å‹åˆ¤æ–­ä¸ºå‚è€ƒé¡µæˆ–ç›®å½•é¡µ" + text
        return text

    def step2_is_pagefoot(self, duans):
        # duansæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå–ç¬¬ä¸€æ®µå’Œæœ€åä¸€æ®µè¿›è¡Œifåˆ¤æ–­ï¼Œåˆ¤æ–­é¡µçœ‰é¡µè„š
        if re.search("\n",duans[0]):
            first_line = duans[0].split("\n", 1)[1]  # åªä»¥ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦åˆ†å‰²
        else:
            first_line = duans[0]
        if re.search("ã€‘",duans[-1]):
            last_paragraph = duans[-1].split("ã€‘")[1]
        else:
            last_paragraph = duans[-1]
        # print(first_line)
        # print(self.get_score(first_line))
        # print(last_paragraph)
        # print(self.get_score(last_paragraph))

        if self.get_score(first_line) > 5000 and not re.search("#",first_line) and not re.search('\|',first_line):
            duans[0] = "ç–‘ä¼¼é¡µçœ‰" + duans[0]
        elif self.get_score(first_line) > 10000 and not re.search('\|',first_line):
            duans[0] = "ç–‘ä¼¼é¡µçœ‰" + duans[0]

        if self.get_score(last_paragraph) > 3500:
            duans[-1] = "ç–‘ä¼¼é¡µè„š" + duans[-1]
        return duans

    def is_merge_ngram(self,text, next_text):
        """
        1.ç»™textæ‰“åˆ†ï¼Œnext_textåˆ†åˆ«æ‰“åˆ†
        2.ç»™marge_textæ‰“åˆ†
        3.åˆ¤å®šmarge_textæ»¡è¶³åˆ†æ•°å°äºtextå’Œnext_textï¼Œä¸”å°äºæŸä¸ªå€¼ è¿™ä¸ªå€¼å¯èƒ½æ˜¯5000ã€3000ã€2000... è¿”å›True
        :return:
        """
        text_score = self.get_score(text)
        next_text_score = self.get_score(next_text)
        merge_text = text + " " + next_text
        merge_text_sorce = self.get_score(merge_text)
        if (merge_text_sorce < text_score or merge_text_sorce < next_text_score) and merge_text_sorce < 5000:
            # print(merge_text)
            return True

    def is_merge_duannei(self, text, next_text):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A']
        if re.search(r'\s\d+\.$', text): #
            # print("1")
            return True
        elif re.search(r'^([\d+A-Z][\.,]){1,3}$', text) and next_text.lstrip()[0].isupper():  # åŒ¹é…æ®µè½ä¸­åªæœ‰åºå·ä¸”ä¸‹ä¸€æ®µæ˜¯å¤§å†™å¼€å¤´çš„æƒ…å†µ
            # print("2")
            return True
        elif any(text.rstrip().endswith(" " + prep) for prep in preposition_list):  # åŒ¹é…åŒä¸€æ®µä¸­ä»‹è¯ç»“å°¾çš„
            # print("3")
            return True
        elif text.rstrip() and text.rstrip()[-1] in ['-','â€”']:
            # print("4")
            return True
        elif "#" not in text and "*" not in text and re.search(r'[^\.?!]$', text) and re.match(r'^[a-z]', next_text.lstrip()):
            # print("5")
            return True
        elif re.search(r'\([^\)]*$|\[[^\]]*$', text) and re.search(r'^[^\(\[]*[\)\]]',next_text):  # å‰ä¸€è¡Œæœ‰ä¸€ä¸ªæœªå¯¹åº”çš„å·¦æ‹¬å·ï¼Œä¸‹ä¸€è¡Œå‰é¢æœ‰ä¸€ä¸ªä¸ä¹‹å¯¹åº”çš„å³æ‹¬å·
            # print("6")
            return True
        # å¯ä»¥å…ˆæ³¨é‡Šæ‰
        # elif ((text.rstrip()[-1].islower() or text.rstrip()[-1] in [',']) and (next_text.lstrip()[0].isupper() or next_text.lstrip()[0] in ['(',')','"','â€','â€œ']) and "#" not in text and "#" not in next_text and self.is_merge_ngram(text, next_text)):   # å‰ä¸€è¡Œå°å†™æˆ–é€—å·ç»“å°¾ï¼Œä¸‹ä¸€è¡Œå¤§å†™å¼€å¤´ï¼Œä¸”ä¸æ ‡é¢˜æ— å…³ï¼ŒåŠ å…¥ngramåˆ¤æ–­
        #     return True
        elif re.search(r'^#{1,3}\s?[\d\.]{1,10}$',text) and next_text.lstrip()[0].isupper():
            # print("7")
            return True
        elif len(text.strip()) == 1 and text == '.' and next_text[0].isupper():
            # print(text, next_text)
            # print("8")
            return True
        elif re.search(r'^#',text) and next_text[0].isupper() and self.is_merge_ngram(text, next_text):
            return True
        return False
    def is_merge_duan(self,stripped_item,next_item):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A']
         # æ–°å¢æ¡ä»¶: å¦‚æœ stripped_item ç»“å°¾çš„å•è¯åœ¨ preposition_list ä¸­
        if any(stripped_item.rstrip().endswith(" " + prep) for prep in preposition_list):
            # print("1")
            return True
        elif stripped_item and stripped_item[-1] in ['-']:
            # print("2")
            return True
        elif re.search(r'Figs?\.$',stripped_item):
            # print("3")
            return True
        elif re.search(r'\([^\)]*$|\[[^\]]*$', stripped_item) and re.search(r'^[^\(\[]*[\)\]]',next_item):
            # print("4")
            return True
        elif len(stripped_item) == 1 and stripped_item in ['.','Â·']:
            # print("5")
            return True
        return False

    def step3_1_more_linefeed_duannei(self, context):
        # print(context)
        new_context = []
        for item in context:
            # å°† item æŒ‰ "\n" åˆ†å‰²
            item_sections = re.split(r'\n', item)
            section_index = 0
            while section_index < len(item_sections) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                if self.is_merge_duannei(item_sections[section_index],item_sections[section_index + 1]):
                    item_sections[section_index] += "|åˆ é™¤æ®µå†…æ¢è¡Œ|" + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]
                else:
                    section_index += 1  # åªæœ‰åœ¨ä¸åˆå¹¶æ—¶æ‰è‡ªå¢

            # æ›´æ–° item ä»¥åæ˜ åˆå¹¶çš„æ®µè½
            item = '\n'.join(item_sections)
            new_context.append(item)
        return new_context
    def step3_2_more_linefeed_duan(self,context):
        index = 0
        while index < len(context):
            item = context[index]

            # åˆå¹¶ä»¥å°å†™å­—æ¯æˆ–ç‰¹å®šæ ‡ç‚¹ç¬¦å·å¼€å¤´çš„æ®µè½
            stripped_item = item.strip()
            # print(stripped_item)
            if index >= 0:
                if index > 0 and stripped_item and context[index - 1][-1] not in ['.', '!', '?', '|',':'] and not re.search('#',stripped_item) and (
                        stripped_item[0].islower() or stripped_item[0] in ["(", "[", ")", "]", "."]) and not re.search('^[a-z]\s?\.',stripped_item.lstrip()):
                    """
                    é‡åˆ°å°å†™å¼€å¤´çš„æ®µ 1.ä¸Šä¸€æ®µæ²¡æœ‰#ç›´æ¥è¿ä¸Šå» 2.ä¸Šä¸€æ®µæœ‰#ä½†æ˜¯ä¸æ­¢ä¸€è¡Œï¼Œåˆ‡ä¸Šä¸€æ®µçš„æœ€åä¸€è¡Œå’Œå½“å‰çš„ç¬¬ä¸€è¡Œï¼Œä½¿ç”¨æ¨¡å‹åˆ¤æ–­è¯¥ä¸è¯¥è¿ 3.ä¸Šä¸€æ®µæœ‰#ä½†æ˜¯åªæœ‰ä¸€è¡Œï¼Œå»ä¸Šä¸Šæ®µåˆ‡æœ€åä¸€è¡Œå’Œå½“å‰çš„ç¬¬ä¸€è¡Œï¼Œæ¨¡å‹åˆ¤æ–­è¯¥ä¸è¯¥è¿
                    """
                    # ä¸Šä¸€æ®µä¸èƒ½å‡ºç°#ï¼Œå‡ºç°#è¯æ˜æ˜¯æ ‡é¢˜æ®µ
                    if not re.search(r'#', context[index - 1]):
                        # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                        context[index - 1] = context[index - 1].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-1|" + item.lstrip()
                        # åˆ é™¤å½“å‰ item
                        del context[index]
                        # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue
                    elif len(re.split(r'\n', context[index - 1])) >= 2:
                        # ä¸Šä¸€æ®µæœ‰æ ‡é¢˜ä¹Ÿæœ‰æ­£æ–‡ï¼Œåˆ†å‰²context[index-1]æœ€åä¸€è¡Œï¼Œåˆ†å‰²stripped_itemçš„ç¬¬ä¸€è¡Œ
                        previous_paragraph_lines = re.split(r'\n', context[index - 1])
                        last_line_of_previous = previous_paragraph_lines[-1].strip()
                        first_line_of_current = stripped_item.splitlines()[0].strip()
                        if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                            # åˆå¹¶åˆ°å‰ä¸€ä¸ª item
                            context[index - 1] = context[index - 1].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-2|" + item.lstrip()
                            # åˆ é™¤å½“å‰ item
                            del context[index]
                            # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                            index = index - 1
                            continue
                    elif len(re.split(r'\n', context[index - 1])) == 1:
                        if index - 2 and len(re.split(r'\n', context[index - 2])) > 1 and context[index - 2][-1] not in ['.', '!', '?']:
                            previous_paragraph_lines = re.split(r'\n', context[index - 2])
                            last_line_of_previous = previous_paragraph_lines[-1].strip()
                            first_line_of_current = stripped_item.splitlines()[0].strip()
                            if self.is_merge_ngram(last_line_of_previous, first_line_of_current):
                                # åˆå¹¶context[index - 2]
                                context[index - 2] = context[index - 2].rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-3|" + item.lstrip()
                                # åˆ é™¤å½“å‰ item
                                del context[index]
                                # ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                                index = index - 1
                                continue
                if index + 1 < len(context) and re.search(r'\([^\)]*$|\[[^\]]*$', stripped_item) and re.search(r'^[^\(\[]*[\)\]]',context[index + 1]):
                    if index + 1 < len(context) and "#" not in context[index + 1] and not re.search(r'^[Ff]ig', context[
                        index + 1].lstrip()):
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-6|" + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue
                elif index + 1 < len(context) and self.is_merge_duan(stripped_item,context[index + 1]) and stripped_item is not None:
                    if index + 1 < len(context) and "#" not in context[index + 1] and not re.search(r'^[Ff]ig', context[
                        index + 1].lstrip()):
                        # åˆå¹¶åˆ°ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + "|åˆ é™¤æ®µä¹‹é—´æ¢è¡Œ-4|" + context[index + 1].lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue

            index += 1
            # print(context)

        return context
def clean_text(text):
    sp = speicalProces()
    text = sp.is_mulupage(text)
    # print(text)
    split_token = "\n\n"
    # åˆ‡åˆ†ä¸€ä¸‹æ•°æ®
    duans = text.split(split_token)
    print(duans)
    result = []
    # åˆ é™¤å›¾ç‰‡æè¿°
    duans = sp.step1_delete_photo_describe(duans)
    duans = sp.step2_is_pagefoot(duans)
    duans = sp.step3_1_more_linefeed_duannei(duans)
    duans = sp.step3_2_more_linefeed_duan(duans)
    # æ­£åˆ™æ›¿æ¢
    if duans and len(duans) > 0 and duans is not None:
        for item in duans:
            # if "## References" in item:
            #     continue
            for pattern_item in pattern_list:
                item = re.sub(pattern_item[0], pattern_item[1], item)
                item = item.strip()
            for pattern_item in context_pattern:
                item = re.sub(pattern_item[0], pattern_item[1], item)
            result.append(item)

    for item in result:
        print(item)
    text = split_token.join(result)
    # context = sp.step9_complete_sequence_number(context)
    return text


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)

    # context = re.sub("[li][\.,]" , '1.' ,context)
    return context

def process_line(items, sp):
    item = json.loads(items.strip())
    context = clean_text(item,  sp)
    context = post_process(context)
    if len(context) < 100:
        return
    item["text"] = context
    item = json.dumps(item, ensure_ascii=False)
    return item

fw = open("/pdf/clean_json/recleanB2_guidelines_ly_gptpdf.jsonl", "w", encoding="utf-8")
with open("/pdf/clean_json/recleanB_guidelines_ly_gptpdf_label.jsonl", "r", encoding="utf-8") as fs:
    for items in tqdm(fs.readlines()):
        item = json.loads(items.strip())
        # if item["seq_id"] == "86005fe7-ca59-4115-860e-641b357f5f4c":
        # print(item)
        print(item["seq_id"])
        text = item['text']
        text = clean_text(text)
        text = post_process(text)
        # print(context)
        item["text"] = text
        item = json.dumps(item, ensure_ascii=False)
        # print(item)
        fw.write(item + "\n")


