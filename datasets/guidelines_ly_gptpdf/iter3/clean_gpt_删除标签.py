import json
import re
from langdetect import detect
import numpy as np
from tqdm import tqdm
import math
import spacy
import random
from nltk.corpus import wordnet
import inflect
inflect = inflect.engine()
import kenlm
from nltk.tokenize import word_tokenize
import fasttext
from collections import defaultdict
model = kenlm.LanguageModel(r"C:\Users\Administrator\Desktop\4k_gram.klm")
fasttext_model = fasttext.load_model('fenlei_model.bin')
# -*- coding: utf-8 -*-
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
import transformers
import re
from transformers import AutoTokenizer
# ner_pipeline = pipeline(Tasks.named_entity_recognition, 'damo/nlp_raner_named-entity-recognition_chinese-base-generic')
# zeroshot_classifier = transformers.pipeline("zero-shot-classification", model="../model/deberta-v3-large-zeroshot-v2.0")

import signal
from tqdm import tqdm
from contextlib import contextmanager


# å¸¦æ ‡ç­¾ç‰ˆ
pattern_list = [
# å¸¦ç‰¹æ®Šç¬¦å·çš„æ— å…³å†…å®¹
[
    r'(\([^\(\)]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\)\(]*\))',
    r''
],
[
    r'([^\d][^\.\n]*(([Ww]{2,3}\.)|(\.[Cc][Oo][Mm])|(http)|(\.html))[^\n]*)',
    r''
],
[
    r'(\n\s*[a-zA-Z\.]\s*\n)',
    r''
],
[
    r'([^\n]*Copyright[^\n]*)',
    r''
],
[
    r'(ISBN\s*[A-Z0-9-]*)',
    r''
],
[
    r'(ğŸ‘|â–¶|â—|Â©|Â®|([^\n]*â†‘[^\n]*)|â€ |Â¶|â•‘|Â§|âˆ§|â„¢|â– |â|â–¡|âœ“|âœ”|â|ğŸ˜ƒ|ï¿½|âˆ‘|âœ¦|â¤ï¸|â¤|â˜†|â˜…)',
    r''
],
[
    r'(\(\s?\b([Ff]igure[s]?|Section|diagram|Appendix|Box|[Ss]ource[s]?|[Ff]igs?|p|FIGURE)\b[^\)\n]*\))',
    r''
],

[
    r'((\([^\(]{0,20})\s[Ff]igures?[^\(\)]*.)',
    r''
],
[
    r'([\(]\s?[^\(]{0,50}([Tt]able|[sS]ee|è§è¡¨|å›¾)[^\)]*?[\)])',
    r''
],
[
    r'([\[]\s?[^\[]{0,50}([Tt]able|[sS]ee|è§è¡¨|å›¾)[^\]]*?[\]])',
    r''
],
# [
#     # r'([^\n\.]*\bFigs?(ure)?s?\b\.?[\s*Â ](\d+)?\.?[^\n]*)',
#     r'([^\n\.]*(Figs?(ure)?s?|FIGURE)\s?\.?\s?(\d+\.\d+)?[^\n\.]*\.?)',
#     r'åˆ é™¤7:<u>\1</u>'
# ],
# [
#     r'([^\n\.]*Fig[s]?(ure)?s?(\s\d\.)?[^\n\.]*\.)',
#     r'åˆ é™¤10:<u>\1</u>'
# ],
[
    r'(\([\d\s,\.\-â€“]{1,50}\))',
    r''
],
[
    r'([^\d])(\[\s?[\dA-Za-z]{1,4}(\s{0,3}[\-â€“\^~â€”,\.]\s{0,3}[\dA-Za-z]{1,4}){0,20}\s?\])',
    r'\1'
],
[
    r'([^\d])([1-9][0-9]{1,4}(\s{0,3}[\-â€“,\.]\s{1,3}[1-9][0-9]{1,4}){1,20})([^\d\%]?)',
    r'\1\4'
],
[
    r'(\[\s?(\d{1,3}\s?[-,ï¼Œ]?\s?)+\d?\s?\]\s?\*?)',
    r''
],
[
    r'(\(\s?[Iâ…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©â…ªâ…«a-zA-Z]?\s?[a-zA-Z]?\s?,\s?[a-zA-Z]\s?\)\s?)[ã€‚\.]',
    r''
],
[
    r'([^\d][\.,])((\s{1,3}\d+[\s\n]{1,3}){1,5}\.?)(\s?[A-Z])',
    r'\1\4'
],
[
    r'([^\d][\.,]\s?)(\d{1,4}(\s{0,3}[\-â€“,\.\s]\s{0,3}\d{1,4}){0,20})(\n|\s?[A-Z]|$)',
    r'\1\4'
],
[
    r'(#{1,3})\n',
    r''
],
[
    r'(\([^\(\)]*(\set[\s\xa0]{1,3}al|\d{4})[^\(\)]*\))',
    r''
],
# [
#     r'([a-z\)])(---)(a-z)',
#     r'\1<u>\2æ›¿æ¢ä¸º-</u>\3'
# ],
# [
#     r'^\s?(---)\s?$',
#     r'<u>\1æ›¿æ¢ä¸º-</u>'
# ]
[
    r'(\^\d+([\s,\-\d+]{0,6})+\^?)',
    r''
],
[
    r'(\$\^\{\d+\}\$)',
    r''
],
[
    r'([^m])(<sup>[\d+\s,a-z]{1,}</sup>)',
    r'\1'
],
[
    r'([*]{0,3}\[[^\[\]]{0,50}\d{4}[^\[\]]{0,50}\][*]{0,3})',
    r''
],
[
    r'(.*ä¸­å›½åˆ†ç±»å·.*)',
    r''
],
[
    r'([ã€‚ï¼ï¼Ÿ]{1,3}\s?)(\d+([\-â€”ï¼Œ]\d+){0,3})([ã€‚ï¼ï¼Ÿ\s]{0,3})$',   # åŒ¹é…åœ¨æ®µè½ç»“å°¾å‡ºç°çš„  \d+ã€‚æˆ– ã€‚\d+
    r'\1\4'
],
# 27 28ä¸€å®šè¦åˆ†å¼€åªèƒ½ä¸€è¾¹æœ‰ä¸­æ–‡ï¼Œå¦åˆ™ä¼šå°†ä¸­æ–‡ä¸­é—´çš„æ•°å­—éƒ½åˆ æ‰ï¼Œä¸”è¿™é‡Œçš„æ ‡ç‚¹åªèƒ½æ˜¯è¡¨ç¤ºå¥å­ç»“æŸçš„æ ‡ç‚¹ï¼ŒåŠå¥å‡ºç°æ•°å­—æ˜¯å¸¸è§çš„
[
    r'([ã€‚ï¼ï¼Ÿ]\s?)(\d+([\-â€”ï¼Œ,]\d+){0,3})([ã€‚ï¼ï¼Ÿ\u4e00-\u9fa5])',    # åŒ¹é…[ã€‚ï¼ï¼Ÿ,]\d+[ã€‚ï¼ï¼Ÿ,\u4e00-\u9fa5]
    r'\1\4'
],
[
    r'([ã€‚ï¼ï¼Ÿ\u4e00-\u9fa5]\s?)(\d+([\-â€”ï¼Œ,]\d+){0,3})([ã€‚ï¼ï¼Ÿ])',  # åŒ¹é…[ã€‚ï¼ï¼Ÿ,\u4e00-\u9fa5]\d+[ã€‚ï¼ï¼Ÿ,]
    r'\1\4'
],
[
    r'([\u2070-\u2079\u2080-\u2089Â¹]+)',
    r''
],
[
    r'([\(ï¼ˆ] *(\d+[\-,]?[A-Za-z]+) *[\)ï¼‰])',
    r''
],
[
    r'(^\d$)',
    r''
],
[
    r'((\\+\[\d+[ ,\-\d]*\\+\])|\*+)',
    r''
]


]

context_pattern = [
    [r'(Â¬\s*)', r''],
    [r'(\(\s*\))', r''],
]

# nlp = spacy.load("en_core_web_trf")
nlp = spacy.load("en_core_web_sm")

class speicalProces:
    def __init__(self):
        pass

    def is_not_upper(self, char):
        return not char.isupper()

    def step1_delete_photo_describe(self, duans):
        """
        ç”¨\nå¯¹æ®µè¿›è¡Œåˆ‡åˆ†ï¼Œåˆ‡åˆ†æˆè¡Œï¼Œè¿›è¡Œåˆ¤æ–­ï¼Œä»¥è¡Œä¸ºå•ä½åˆ é™¤å›¾ç‰‡æè¿°çš„
        """
        new_duans = []
        for duan in duans:
            if re.search(r'[A-Z]{5,}\*\*[\s\n]{0,5}[a-z]',duan):
                escaped_least_text = re.escape(duan)
                duan = re.sub(rf'{escaped_least_text}', r'', duan)
            lines = duan.split('\n')
            for line in lines:
                # print(line.strip())

                if re.search('^([#*]{0,5}).{0,5}\s?([Ff]igs?(ure)?s?|FIGS?(URE)?s?|å›¾)[\s\d\.]',line.strip()) or re.search('\.(png|jpg|PNG|JPG)',line.strip()):
                    escaped_least_text = re.escape(line)
                    # print(escaped_least_text)
                    try:
                        duan = re.sub(rf'{escaped_least_text}', r'', duan)
                    except re.error as e:
                        print(f"Regex error: {e}")
                # print("*" * 50)
            new_duans.append(duan)
        return new_duans

    def get_score(self,sentence):
        tokenize_text = word_tokenize(sentence)
        final_text = " ".join(tokenize_text)
        score = model.score(final_text, bos=False, eos=False)
        length = len(tokenize_text)
        if length > 0:
            score = (10 ** (-score / length))
        return score

    def step2_is_pagefoot(self, duans,lang):
        # duansæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå–ç¬¬ä¸€æ®µå’Œæœ€åä¸€æ®µè¿›è¡Œifåˆ¤æ–­ï¼Œåˆ¤æ–­é¡µçœ‰é¡µè„š
        if lang == "en":
            first_line = duans[0]
            last_paragraph = duans[-1]
            # æå–ä¸­é—´æ®µè½
            middle_duans = duans[1:-1]
            middle_content = " ".join(middle_duans)
            middle_content = re.sub(r"ã€\d+ã€‘",r'',middle_content)
            first_line_score = self.get_score(first_line)
            last_paragraph_score = self.get_score(last_paragraph)
            middle_content_score = self.get_score(middle_content)
            # print(first_line_score,first_line)
            # print(middle_content_score,middle_content)
            # print(last_paragraph_score,last_paragraph)
            if first_line_score > 4000 and not re.search('\|',first_line) and len(first_line) < 150 and re.search(r'[A-Z][a-z]{1,}\s[A-Z][a-z]{1,}',first_line):
                duans[0] = ''
            elif first_line_score > 8000 and not re.search("#",first_line) and not re.search('\|',first_line) and len(first_line) < 150 and re.search(r'[A-Z][a-z]{1,}\s[A-Z][a-z]{1,}',first_line):
                duans[0] = ''

            if last_paragraph_score > 4000 and not re.search('\|',last_paragraph) and len(last_paragraph) < 150 and re.search(r'[A-Z][a-z]{1,}\s[A-Z][a-z]{1,}',last_paragraph):
                duans[-1] = ''
        return duans

    def is_merge_ngram(self,text, next_text):
        """
        1.ç»™textæ‰“åˆ†ï¼Œnext_textåˆ†åˆ«æ‰“åˆ†
        2.ç»™marge_textæ‰“åˆ†
        3.åˆ¤å®šmarge_textæ»¡è¶³åˆ†æ•°å°äºtextå’Œnext_textï¼Œä¸”å°äºæŸä¸ªå€¼ è¿™ä¸ªå€¼å¯èƒ½æ˜¯5000ã€3000ã€2000... è¿”å›True
        :return:
        """
        text_score = self.get_score(text)
        next_text_score = self.get_score(next_text)
        merge_text = text + " " + next_text
        merge_text_sorce = self.get_score(merge_text)
        if (merge_text_sorce < text_score or merge_text_sorce < next_text_score) and merge_text_sorce < 5000:
            # print(merge_text)
            return True

    def is_merge_duannei(self, text, next_text):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        if re.search(r'^#{0,5}\s?(\d+[\.,\s]?){0,5}$', text) and re.search(r'^#{0,5}\s?[A-Z]',next_text):  # åŒ¹é…æ®µè½ä¸­åªæœ‰åºå·ä¸”ä¸‹ä¸€æ®µæ˜¯å¤§å†™å¼€å¤´çš„æƒ…å†µ éƒ½å¯ä»¥å¸¦#
            return True
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A', 'with', '&',
                            'And', 'their', 'his', 'her','that']
        # æ–°å¢æ¡ä»¶: å¦‚æœ stripped_item ç»“å°¾çš„å•è¯åœ¨ preposition_list ä¸­
        if any(text.rstrip().endswith(" " + prep) for prep in preposition_list) and not re.search("^#",next_text.strip()):
            return True
        if text and text[-1] in ['-', '"', ',']:
            return True
        if text.strip() and next_text.strip() and text.strip()[-1].islower() and next_text.strip()[0].islower():
            return True
        return False

    def is_merge_duan(self, stripped_item, next_item):
        # å®šä¹‰ä¸€ä¸ªä»‹è¯åˆ—è¡¨
        if re.search(r'^#{0,5}\s?(\d+[\.,\s]?){0,5}$', stripped_item) and re.search(r'^#{0,5}\s?[A-Z]',next_item):  # åŒ¹é…æ®µè½ä¸­åªæœ‰åºå·ä¸”ä¸‹ä¸€æ®µæ˜¯å¤§å†™å¼€å¤´çš„æƒ…å†µ éƒ½å¯ä»¥å¸¦#
            return True
        preposition_list = ['of', 'in', 'and', 'or', 'not', 'the', 'a', 'any', 'for', 'is', 'The', 'A', 'with', '&',
                            'And', 'their', 'his', 'her','that']
        # æ–°å¢æ¡ä»¶: å¦‚æœ stripped_item ç»“å°¾çš„å•è¯åœ¨ preposition_list ä¸­
        if any(stripped_item.rstrip().endswith(" " + prep) for prep in preposition_list) and not re.search("^#", next_item.strip()):
            return True
        if stripped_item and stripped_item[-1] in ['-', '"', ',']:
            return True
        return False

    def step3_1_more_linefeed_duannei(self, context):
        # print(context)
        new_context = []
        for item in context:
            # å°† item æŒ‰ "\n" åˆ†å‰²
            item_sections = re.split(r'\n', item)
            # print(item_sections)
            section_index = 0
            while section_index < len(item_sections) - 1:  # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                if self.is_merge_duannei(item_sections[section_index],item_sections[section_index + 1]):
                    item_sections[section_index] += " " + item_sections[section_index + 1].lstrip()
                    del item_sections[section_index + 1]
                else:
                    section_index += 1  # åªæœ‰åœ¨ä¸åˆå¹¶æ—¶æ‰è‡ªå¢

            # æ›´æ–° item ä»¥åæ˜ åˆå¹¶çš„æ®µè½
            item = '\n'.join(item_sections)
            new_context.append(item)
        return new_context
    def step3_2_more_linefeed_duan(self,context):
        index = 0
        while index < len(context):
            item = context[index]

            # åˆå¹¶ä»¥å°å†™å­—æ¯æˆ–ç‰¹å®šæ ‡ç‚¹ç¬¦å·å¼€å¤´çš„æ®µè½
            stripped_item = item.strip()
            if index >= 0:
                if index + 1 < len(context) and self.is_merge_duan(stripped_item,context[index + 1]) and stripped_item is not None:
                    if index + 1 < len(context) and not re.search(r'^[Ff]ig', context[
                        index + 1].lstrip()):
                        # åˆå¹¶ä¸‹ä¸€ä¸ª item
                        context[index] = item.rstrip() + " " + context[index + 1].lstrip().lstrip()
                        # åˆ é™¤ä¸‹ä¸€ä¸ª item
                        del context[index + 1]
                        # ä¸å¢åŠ  index, ç»§ç»­æ£€æŸ¥å½“å‰ç´¢å¼•ä½ç½®çš„å…ƒç´ 
                        index = index - 1
                        continue
            index += 1
            # print(context)

        return context


    def get_person_idx(self, item):
        doc = nlp(item)
        person_block = []
        person_num = 0
        # print(doc.ents)
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                if len(person_block) == 0:
                    person_block.append([ent.start_char, ent.end_char])
                elif ent.end_char - person_block[-1][-1] > 5:
                    person_block.append([ent.start_char, ent.end_char])
                else:
                    person_block[-1][-1] = ent.end_char
                person_num += 1
        return person_block, person_num

    def step4_rm_cite(self, item):
        cite_tag = []
        cite_index = 1 if len(re.findall(r'\[\d+\]', item)) > 0 else 0
        cite_year = 1 if len(
            re.findall(r'\[\d\d\d\d\]', item) or re.findall(r'\.\s?\b\d{4}\b', item) or re.findall(r'\b\d{4}\b\s?[;\.]',
                                                                                                   item)) else 0  # å¹´ä»½ï¼Œæ¯”å¦‚ . 2010 ã€ 2010;
        cite_page = 1 if len(re.findall(r'\d+\s?:\d+\s?[â€“-]\s?\d+', item)) else 0
        cite_page2 = 1 if len(re.findall(r'[\.,]\s?\d+\s?[â€“-]\s?\d+[\.,]', item)) else 0
        cite_J = 1 if len(re.findall(r'\[[Jj]\]', item)) else 0
        cite_doi = 1 if " doi " in item else 0
        cite_etal = 1 if (" et al" in item or 'et\u00A0al' in item or 'etÂ al' in item) else 0
        cite_vol = 1 if (" vol. " in item or " Vol " in item or " Vol. " in item or " vol " in item) else 0
        cite_pp = 1 if (re.search("\s[Pp]{1,2}(\.)?\s",item)) else 0
        # cite_page = 1 if len(re.search(r'\.\s?\b\d{4}\b',item)) else 0
        cite_phonenum = 1 if re.search(r" [Pp]hone:|Fax:", item) else 0
        mulu = 1 if re.search(r'[\.\s]{15,}', item) and not re.search("|",item) else 0
        cite_tag = [cite_index, cite_year, cite_J, cite_doi, cite_etal, cite_page, cite_vol, cite_phonenum, cite_pp,cite_page2]
        # if sum(cite_tag) > 1 and '|' not in item:
        #     return "å‚è€ƒåˆ é™¤-0:<u>{}</u>".format(item)
        person_block, person_num = self.get_person_idx(item)
        # è¶…è¿‡5ä¸ªäººå
        person_block_lens = [block_item[1] - block_item[0] for block_item in person_block]
        person_lens = sum(person_block_lens)
        if len(item) > 0 and person_lens / len(item) > 0.5 and len(item) > 100 and len(item) < 400:
            return "å‚è€ƒåˆ é™¤-1:<u>{}</u>".format(item)
        # åªæœ‰ä¸ªåå­—æ•°é‡
        elif person_num > 5 and '|' not in item and len(item) < 200:
            return "å‚è€ƒåˆ é™¤-2:<u>{}</u>".format(item)
        elif sum(cite_tag) > 0 and person_num > 0 and len(item) < 400:
            # print(item)
            return "å‚è€ƒåˆ é™¤-3:<u>{}</u>".format(item)
        elif mulu:
            return "ç›®å½•åˆ é™¤:<u>{}</u>".format(item)
        else:
            return item

    def step4_removepage(self, context):
        # context æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ª item æ˜¯ä¸€æ®µå†…å®¹
        context_lens = len(context)
        # ç”¨äºç»Ÿè®¡æœ‰å¤šå°‘ä¸ªæ®µè½ä¸­å‡ºç°äº†äººå
        num = 0
        mulu_num = 0
        new_context = []
        references_started = False
        for item in context:
            # è¿”å›çš„itemæ˜¯å·²ç»è¢«é‡å†™è¿‡çš„item
            # if item.strip() in ["##References","## References","## Suggested Readings","##Suggested Readings"]:
            if re.search(r'^#{1,3}\s?(Reference|Suggested Reading|å‚è€ƒæ–‡çŒ®|Acknowledgment|è‡´è°¢)s?',item.strip()):
                references_started = True
            if references_started:
                item = "å‚è€ƒåˆ é™¤-4:<u>{}</u>".format(item)
            elif not re.search("[\u4e00-\u9fa5]",item):
                item = self.step4_rm_cite(item)
            # æ–°çš„itemé‡æ–°åŠ å…¥ä¸€ä¸ªæ–°çš„åˆ—è¡¨
            new_context.append(item)
            # åˆ¤æ–­itemæ˜¯å¦è¢«åˆ¤å®šæœªå‚è€ƒæ–‡çŒ®
            if re.search(r'å‚è€ƒåˆ é™¤',item):
                # å¦‚æœå½“å‰æ®µè½ä¸­æœ‰äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾
                num += 1
        # print(new_context)
        # å¯¹æ•´é¡µçš„ä¸€ä¸ªåˆ¤æ–­
        if context_lens >= 4 and num >= context_lens * 0.5 and not references_started:
            new_context.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µåœ¨è¶…è¿‡ä¸€åŠçš„æ®µè½ä¸­å‘ç°äººåä¸”ç¬¦åˆå‚è€ƒæ–‡çŒ®çš„ç‰¹å¾")
            return []
        return new_context

    def is_cankaopage(self,duans,lang):
        if lang == "en":
            text = "\n\n".join(duans)
            text = re.sub(r'ã€\d+ã€‘', '', text)
            label = fasttext_model.predict(text.strip().replace('\n', ''))
            if label[0][0] in ['__label__cankao'] and not re.search(r'å‚è€ƒåˆ é™¤-4', text):
                # duans.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µè¢«æ¨¡å‹åˆ¤æ–­ä¸ºå‚è€ƒé¡µ")
                return []
            if label[0][0] in ['__label__mulu']:
                # duans.insert(0, "(æœ¬é¡µåˆ é™¤)æœ¬é¡µè¢«æ¨¡å‹åˆ¤æ–­ä¸ºç›®å½•é¡µ")
                return []

        return duans
def clean_text(text,lang):
    sp = speicalProces()

    split_token = "\n\n"
    # åˆ‡åˆ†ä¸€ä¸‹æ•°æ®
    duans = text.split(split_token)
    result = []
    """
    step1:åˆ é™¤å›¾ç‰‡çš„æè¿°
    step2:é¡µçœ‰é¡µè„šçš„åˆ¤æ–­
    step3_1:å¤šäºæ¢è¡Œ
    step3_2:ç¼ºå°‘æ¢è¡Œ
    step4:é¡µåˆ¤æ–­
    step5:æ­£åˆ™æ›¿æ¢
    step6:å‚è€ƒé¡µåˆ¤æ–­
    """
    # åˆ é™¤å›¾ç‰‡æè¿°
    duans = sp.step1_delete_photo_describe(duans)
    duans = sp.step2_is_pagefoot(duans,lang)
    duans = sp.step3_1_more_linefeed_duannei(duans)
    duans = sp.step3_2_more_linefeed_duan(duans)
    duans = sp.step4_removepage(duans)

    # æ­£åˆ™æ›¿æ¢
    if duans and len(duans) > 0 and duans is not None:
        for item in duans:
            # if "## References" in item:
            #     continue
            for pattern_item in pattern_list:
                item = re.sub(pattern_item[0], pattern_item[1], item)
                item = item.strip()
            for pattern_item in context_pattern:
                item = re.sub(pattern_item[0], pattern_item[1], item)
            result.append(item)
    duans = sp.is_cankaopage(result, lang)
    deleted_context = []
    for item in duans:
        if re.search(r"(ç›®å½•|å‚è€ƒ)åˆ é™¤",item):
            continue
        deleted_context.append(item)

    text = split_token.join(deleted_context)



    # context = sp.step9_complete_sequence_number(context)
    return text


def post_process(context):
    context = context.strip(" ").strip("\n").strip(" ").strip("\n")
    # æ¶ˆé™¤åˆ†ç•Œç¬¦å¤±æ•ˆ  --*- å‰é¢éœ€è¦æœ‰è¿ç»­ä¸¤ä¸ª\n;
    context = re.sub('\n    --', "\n\n    --", context)
    # æ¶ˆé™¤ç©ºæ ¼é—®é¢˜
    context = re.sub(r'\n +\n', "\n\n", context)
    context = re.sub(r'\n +\n', "\n\n", context)
    # å»æ‰è¿‡å¤š\nçš„æƒ…å†µ
    context = re.sub("\n{2,}", "\n\n", context)
    context = re.sub(r'[ã€‚ï¼Œ\.](\s?[ã€‚ï¼Œ\.ï¼šï¼›]){1,5}',r'ã€‚',context)
    context = re.sub(r'[,\.](\s+[,\.]){1,5}',r'\.',context)
    # context = re.sub("[li][\.,]" , '1.' ,context)
    return context

def process_line(items, sp):
    item = json.loads(items.strip())
    context = clean_text(item,  sp)
    context = post_process(context)
    if len(context) < 100:
        return
    item["text"] = context
    item = json.dumps(item, ensure_ascii=False)
    return item

# fw = open("C:/pycharm/orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®/pdf/clean_json/reclean3_guidelines_ly_gptpdf.jsonl", "w", encoding="utf-8")
with open("C:\pycharm\orcè¯†åˆ«pdfæ¸…æ´—æ•°æ®\pdf\clean_json\original_data\guidelines_ly_gptpdf_preformat.jsonl", "r", encoding="utf-8") as fs:
    lines = fs.readlines()

    # éšæœºæŠ½å–5000æ¡è®°å½•
    sampled_lines = random.sample(lines, 2000)
    for items in tqdm(sampled_lines):
        item = json.loads(items.strip())
        # if item["seq_id"] == "9194c2a8-809d-4632-ab5a-6358c172cb02":
        # print(item)
        # print(detect(item['text']))
        # if detect(item['text']) == "zh-cn":
        #     lang = "zh"
        # else:
        lang = item['lang']

        page_num = item['attr']['page_num']
        text = item['text']
        text = clean_text(text,lang)
        text = post_process(text)
        # print(context)
        item["text"] = text
        item = json.dumps(item, ensure_ascii=False)
        print(item)
        # print("*" * 100)
        # fw.write(item + "\n")


