{"seq_id": "e7565e00-a134-4dc8-a2b5-2aeb82cf3d3a", "title": "The Most Distinctive Causes of Death by State, 2001-2010", "text": "【0】The Most Distinctive Causes of Death by State, 2001-2010\n========================================================\n\n【1】GIS SNAPSHOT — Volume 12 — May 14, 2015\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Francis P. Boscoe, PhD; Eva Pradhan, MPH\n\n【5】_Suggested citation for this article:_ Boscoe FP, Pradhan E. The Most Distinctive Causes of Death by State, 2001–2010. Prev Chronic Dis 2015;12:140395. DOI: http://dx.doi.org/10.5888/pcd12.140395 external icon .\n\n【6】PEER REVIEWED\n\n【7】PEER REVIEWED\n\n【8】On This Page\n\n【9】*   Background\n*   Methods\n*   Main Findings\n*   Action\n*   Acknowledgments\n*   Author Information\n*   References\n\n【10】\\[ High-resolution JPG for print image icon \\]\n\n【11】The most distinctive cause of death (defined as the location quotient) for each state and the District of Columbia, 2001–2010. The map shows the cause of death from the _International Classification of Diseases, 10th Revision_ (ICD-10), List of 113 Selected Causes of Death with the highest age-adjusted mortality rate ratio in each state. The causes are listed in the legend in the order of disease classification in ICD-10. This map highlights nonstandard cause-of-death certification practices within and between states that can potentially be addressed through education and training. \\[A text description of this map is also available.\\]\n\n【12】Top\n\n【13】Background\n----------\n\n【14】Maps of the most distinctive or characteristic value of some variable at the state or country level became popular on social media in 2014. Among the most widely shared examples have been maps of state-level birth name preferences, music-listening preferences, and mortality from among the top 10 causes of death (1). This form of data presentation has a long history in economic geography, where the mapped values are known as location quotients (2). We use the _International Classification of Diseases, 10th Revision_ (ICD-10), List of 113 Selected Causes of Death file published by the National Center for Health Statistics (3) to present a more nuanced view of mortality variation within the United States than what can be seen by using only the 10 most common causes of death.\n\n【15】Top\n\n【16】Methods\n-------\n\n【17】Counts for each cause of death included on the ICD-10 List of 113 Selected Causes of Death along with population sizes were obtained for each of the 50 states and the District of Columbia for 2001 through 2010 from the Underlying Cause of Death file accessible through the Centers for Disease Control and Prevention (CDC) WONDER (Wide-ranging Online Data for Epidemiologic Research) website (4). We also included subcauses of death contained in this file, such as specific types of cancer, which brought the total number of causes of death to 136. The standardized mortality rate ratio (ie, the ratio of the age-adjusted state-specific death rate for each cause of death relative to the national age-adjusted death rate for each cause of death, equivalent to a location quotient) was then calculated, and the maximum ratio for each state was mapped. That is, we mapped\n\n【18】where Max _<sub>j </sub>_ is the age-adjusted mortality rate for each state _i_ and SMR _<sub>ij </sub>_ is the age-adjusted mortality rate for the United States for each cause of death _j_ . Causes of death with fewer than 10 counts at the state level were suppressed and therefore not available for this analysis.\n\n【19】The map was produced in SAS software version 9.3 (SAS Institute, Inc) by using a single program that imported the output from CDC WONDER, calculated the mortality rate ratios, and generated the map using PROC MAPIMPORT and PROC GMAP. The program code is available from the authors. Minor cosmetic enhancements were made to the map using Adobe Illustrator (Adobe, Inc). Both colors and numeric labels were used on the map to facilitate black-and-white printing.\n\n【20】Top\n\n【21】Main Findings\n-------------\n\n【22】The resulting map depicts a variety of distinctive causes of death based on a wide range of number of deaths, from 15,000 deaths from HIV in Florida to 679 deaths from tuberculosis in Texas to 22 deaths from syphilis in Louisiana. The largest number of deaths mapped were the 37,292 deaths in Michigan from “atherosclerotic cardiovascular disease, so described”; the fewest, the 11 deaths in Montana from “acute and rapidly progressive nephritic and nephrotic syndrome.” The state-specific percentage of total deaths mapped ranged from 1.8% (Delaware; atherosclerotic cardiovascular disease, so described) to 0.0005% (Illinois, other disorders of kidney).\n\n【23】Some of the findings make intuitive sense (influenza in some northern states, pneumoconioses in coal-mining states, air and water accidents in Alaska and Idaho), while the explanations for others are less immediately apparent (septicemia in New Jersey, deaths by legal intervention in 3 Western states). The highly variable use of codes beginning with “other” between states is also apparent. For example, Oklahoma accounted for 24% of the deaths attributable to “other acute ischemic heart diseases” in the country despite having only slightly more than 1% of the population, resulting in a standardized mortality rate ratio of 19.4 for this cause of death, the highest on the map. The highest standardized mortality rate ratio after Oklahoma was 12.4 for pneumoconioses in West Virginia.\n\n【24】A limitation of this map is that it depicts only 1 distinctive cause of death for each state. All of these were significantly higher than the national rate, but there were many others also significantly higher than the national rate that were not mapped. The map is also predisposed to showing rare causes of death — for 22 of the states, the total number of deaths mapped was under 100. Using broader cause-of-death categories or requiring a higher threshold for the number of deaths would result in a different map. These limitations are characteristic of maps generally and are why these maps are best regarded as snapshots and not comprehensive statistical summaries (5).\n\n【25】Top\n\n【26】Action\n------\n\n【27】This map has been a robust conversation starter among those who have seen it before publication, generating hypotheses and inviting further exploration of the underlying data set, something that an equivalent tabular representation does not accomplish as well. Although chronic disease prevention efforts should continue to emphasize the most common conditions, an outlier map such as this one should also be of interest to public health professionals, particularly insofar as it highlights nonstandard cause-of-death certification practices within and between states that can potentially be addressed through education and training. This is especially true considering that most death certificates are completed by community physicians who receive little or no formal training in this area. For example, a study found that nearly half of the death certificates certified by physicians in a suburban Florida county contained major errors, often reflecting confusion between the underlying cause of death and the terminal mechanism of death (6). It would not take many systematic miscodes involving an unusual cause of death for it to appear on this type of map.\n\n【28】Top\n\n【29】Acknowledgments\n---------------\n\n【30】This project was supported in part by CDC’s National Program of Cancer Registries through cooperative agreement no. 5U58DP003879 awarded to the New York State Department of Health.\n\n【31】Top\n\n【32】Author Information\n------------------\n\n【33】Corresponding Author: Francis P. Boscoe, PhD, New York State Cancer Registry, 150 Broadway, Ste 361, Albany, NY 12204. Telephone: 518-474-2255. Email: francis.boscoe@health.ny.gov .\n\n【34】Author Affiliation: Eva Pradhan, MPH, New York State Department of Health, Albany, New York.\n\n【35】Top\n\n【36】References\n----------\n\n【37】1.  Blatt B. Bad latitude. Slate. April 30, 2014. http://www.slate.com/articles/arts/culturebox/2014/04/viral\\_maps\\_the\\_problem\\_with\\_all\\_those\\_fun\\_maps\\_of\\_the\\_u\\_s\\_plus\\_some\\_fun.html. Accessed June 20, 2014.\n2.  Smith JL. Examination of the relative importance of hospital employment in non-metropolitan counties using location quotients. Rural Remote Health 2013;13(3):2497 PubMed external icon\n3.  Centers for Disease Control and Prevention, National Center for Health Statistics. Instruction manual, part 9. ICD-10 cause-of-death lists for tabulating mortality statistics (updated October 2002 to include ICD codes for terrorism deaths for data year 2001 and WHO updates to ICD-10 for data year 2003). http://www.cdc.gov/nchs/data/dvs/im9\\_2002.pdf.pdf. Accessed October 28, 2014.\n4.  Centers for Disease Control and Prevention. About underlying cause of death, 1999–2010. http://wonder.cdc.gov/ucd-icd10.html. Accessed June 20, 2014.\n5.  Gelman A, Price PN. All maps of parameter estimates are misleading. Stat Med 1999;18(23):3221–34. CrossRef external icon PubMed external icon\n6.  Cambridge B, Cina SJ. The accuracy of death certificate completion in a suburban community. Am J Forensic Med Pathol 2010;31(3):232–5. CrossRef external icon PubMed external icon\n\n【38】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0befa867-7cca-4091-b6b3-cbf7493cd38f", "title": "Cross-Neutralization between Human and African Bat Mumps Viruses", "text": "【0】Cross-Neutralization between Human and African Bat Mumps Viruses\nMany batborne paramyxoviruses closely related to mammalian paramyxoviruses recently have been identified, suggesting a possible risk for transmission of batborne paramyxoviruses to humans ( _1_ ). Although no infectious virus has been isolated, the genome of a new paramyxovirus detected in an epauletted fruit bat ( _Epomophorus_ sp.) in the Democratic Republic of the Congo was closely related to the mumps virus (MuV, genus _Rubulavirus_ ) ( _2_ ).\n\n【1】Mumps is typically characterized by inflammation of the parotid glands but also can be accompanied by orchitis, aseptic meningitis, pancreatitis, and deafness ( _3_ ). Mumps vaccines have been used worldwide for >20 years. MuV is serologically monotypic ( _4_ ). The fusion (F) and hemagglutinin-neuraminidase (HN) proteins, but not the small hydrophobic (SH) membrane protein, are the major targets of neutralizing (NT) antibodies ( _5_ , _6_ ).\n\n【2】By using expression plasmids, in 2015, Kruger et al. determined that the envelope proteins of the new paramyxovirus African bat MuV (ABMuV) were serologically and functionally related to those of MuV ( _7_ ). We generated infectious recombinant MuVs (rMuVs) carrying either or both of the F and HN glycoproteins of ABMuV to analyze their functions and serologic cross-reactivities in the context of virus infection.\n\n【3】### The Study\n\n【4】Figure 1\n\n【5】Figure 1 . Construction of rMuVs expressing the ABMuV envelope proteins for study of their functions. A) Genome structures of the rMuVs. The 7 boxes indicate the N, V/P, M, F, SH, HN, and...\n\n【6】A full-length genomic cDNA of the MuV Odate strain (pMuV-Odate) ( _8_ ) was constructed, and the open reading frames of the F and HN genes were exchanged individually or together with those of ABMuV, which were obtained by an artificial composition (GenBank accession no. HQ660095) (pMuV-Odate/ABMuV-F, /ABMuV-HN, and /ABMuV-FHN) ( Figure 1 , panel A). All infectious viruses were rescued by transfecting the plasmids pMuV-Odate, /ABMuV-F, /ABMuV-HN, and /ABMuV-FHN, along with helper plasmids, into BHK/T7–9 cells. Rescued viruses were propagated in Vero cells.\n\n【7】First, we examined syncytium formation in the Vero cells infected with all of the rMuVs ( Figure 1 , panel B). Although rOdate/ABMuV-F produced a cytopathic effect similar to that of rOdate, larger syncytia and severe cell detachment were observed in the cells infected with rOdate/ABMuV-HN and -FHN. To understand the basis for the enhanced syncytium formation by rMuVs carrying the ABMuV HN protein, we expressed the F and HN proteins of the Odate and ABMuV strains using expression plasmids and examined for syncytium formation. Expression of the ABMuV HN protein did not enhance syncytium formation when used in expression plasmids ( Figure 1 , panel C). These data were consistent with a previous study ( _7_ ). The matrix protein was expressed together with the HN and F proteins because it can modulate syncytium formation. However, it did not affect the fusion activity. Therefore, the findings with infectious viruses appeared to differ from those obtained with expression plasmids ( _7_ ).\n\n【8】All rMuVs were propagated efficiently in Vero cells ( Figure 1 , panel D). Although at 24 and 48 h postinfection the titers of rOdate/ABMuV-F were lower than those of the other 3 viruses, they later increased to ≈10 <sup>8 </sup> PFU/mL, which was comparable to the titers of rOdate and rOdate/ABMuV-FHN. On the other hand, the peak titer of rOdate/ABMuV-HN was as low as 5 × 10 <sup>6 </sup> PFU/mL. To determine whether the envelope proteins affect the cell tropisms of MuV in vitro _,_ we evaluated the viral growth in human- and bat-derived cell lines. Human lung epithelial A549 and human monocytic THP-1 cells were used because epithelial cells and monocytes are the primary targets of MuV in vivo ( _9_ ). In A549 cells, rOdate/ABMuV-HN showed the highest titer by up to >10 <sup>6 </sup> PFU/mL at 96 h postinfection ( Figure 1 , panel E). The other 3 rMuVs also replicated well in A549 cells up to ≈10 <sup>5 </sup> PFU/mL, with rOdate/ABMuV-FHN showing much faster kinetics than the others. All 4 viruses grew to similar titers of up to 10 <sup>7 </sup> PFU/mL in THP-1 cells ( Figure 1 , panel F). Growth was also efficient in the fruit bat–derived FBKT1 cells, although the peak titers of rOdate were lowest ( Figure 1 , panel G). Collectively, these findings using culture cells suggested that the envelope proteins are not a critical determinant of host specificity between ABMuV and MuV.\n\n【9】Figure 2\n\n【10】Figure 2 . Comparison of the NT titer of rOdate versus rOdate/ABMuV-F (A), -HN (B), and -FHN (C) in a study of serologic cross-reactivities. r and p values, calculated by using the Pearson product-moment...\n\n【11】We conducted NT assays and ELISA using human serum obtained from 12 healthy adults (18–58 years of age) under approval by the Ethical Committees of National Institute of Infectious Diseases. Ten of 12 serum specimens (nos. 1–10) were seropositive or indeterminate (titer >2 <sup>1 </sup> ) and neutralized rOdate (NT titer \\> 4-fold) ( Table ). The MuV-NT serum samples showed cross-neutralization between rOdate and 3 chimeric MuVs ( Table ). Correlations of the NT titers were significant among rOdate and rOdate/ABMuV-F, -HN and –FHN of 0.67 (p<0.05), 0.77 (p<0.01), and 0.71 (p<0.05), respectively, by Pearson product-moment correlation ( Figure 2 ). In addition, serum from a rabbit vaccinated with a genotype B mumps vaccine strain also neutralized the rMuVs carrying the ABMuV envelope proteins (data not shown). All data demonstrated that MuV and ABMuV were serologically cross-reactive.\n\n【12】### Conclusions\n\n【13】To our knowledge, no infectious ABMuV has been isolated, although the entire genome sequence was detected in bats. To study the context of virus infection, we generated rMuVs carrying the ABMuV envelope proteins by reverse genetics. By using expression plasmids, Kruger et al. reported that the functions, such as fusion, hemadsorption, and neuraminidase activities, of the envelope proteins were conserved and compatible between MuV and ABMuV ( _7_ ). These findings agreed with our data using the recombinant viruses, but notable differences existed. For example, Kruger et al. reported that the ABMuV envelope proteins induce smaller syncytia than the MuV proteins, whereas we observed enhanced syncytium formation by rMuV carrying the ABMuV HN protein. However, the enhancement was not due simply to the functional difference between MuV and ABMuV HN proteins because the HN proteins showed similar fusion-supporting capacities when expressed using expression plasmids. Further investigation of the involvement of other viral proteins modulating the HN protein function could lead to elucidation of the mechanism underlying this difference. Moreover, although Kruger et al. mentioned that the fusion activity might restrict the viral species specificity, our data indicated that the envelope proteins of MuV are not critical determinants of the host specificity in cultured cells. Our study also demonstrated that a synthetic genome strategy, which has also been used for the study of a bat influenza virus ( _10_ , _11_ ), is useful for the characterization and risk assessment of emerging viruses, even when the authentic viruses have not been isolated.\n\n【14】Our data showed extensive cross-neutralization between MuV and ABMuV. Although NT antibodies might play an essential role for MuV protection, a definitive NT titer for MuV protection is still under debate ( _12_ ). Cortese et al. suggested that case-patients generally had lower preoutbreak mumps antibody levels than non–case-patients; however, no cutoff NT titer was defined in their study ( _12_ ). Our findings suggest that antibodies induced by either mumps vaccines or infection with wild-type MuV generally neutralize ABMuV efficiently. Because cell-mediated immunity might also contribute to MuV protection ( _13_ ), further investigations are needed to clarify the definitive parameters of MuV and ABMuV protection. Nonetheless, our data demonstrate that the current MuV vaccination program reduces the risk for an emerging infection of ABMuV in humans.\n\n【15】Dr. Katoh is a senior researcher in the Department of Virology III, National Institute of Infectious Diseases, Tokyo, Japan. His research interests are molecular biology, pathogenesis and pathology, epidemiology, and prevention, and control of mumps virus infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "38eae89b-bc7c-4d4d-9b85-b2b6f255f159", "title": "Early Results of States’ Efforts to Support, Scale, and Sustain the National Diabetes Prevention Program", "text": "【0】Early Results of States’ Efforts to Support, Scale, and Sustain the National Diabetes Prevention Program\n========================================================================================================\n\n【1】SPECIAL TOPIC — Volume 14 — December 7, 2017\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Yvonne Mensa-Wilmot, PhD, MPH <sup>1 </sup> ; Shelly-Ann Bowen, PhD, MS <sup>2 </sup> ; Stephanie Rutledge, PhD, MA <sup>1 </sup> ; Jennifer Murphy Morgan, MSPH <sup>1 </sup> ; Timethia Bonner, DPM, PhD <sup>3 </sup> ; Kimberly Farris, PhD, MPH, MSW <sup>1 </sup> ; Rachel Blacher, MPH <sup>1 </sup> ; Gia Rutledge, MPH <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Mensa-Wilmot Y, Bowen S, Rutledge S, Morgan JM, Bonner T, Farris K, et al. Early Results of States’ Efforts to Support, Scale, and Sustain the National Diabetes Prevention Program. Prev Chronic Dis 2017;14:170478. DOI: http://dx.doi.org/10.5888/pcd14.170478 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Abstract\n*   Introduction\n*   Implementation Framework To Scale And Sustain the National DPP\n*   State Health Department Progress on Key Activities\n*   Implications for the Future\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【9】Abstract\n--------\n\n【10】The Centers for Disease Control and Prevention (CDC) developed a cooperative agreement with health departments in all 50 states and the District of Columbia to strengthen chronic disease prevention and management efforts through the implementation of evidence-based strategies, such as CDC’s National Diabetes Prevention Program. The National Diabetes Prevention Program supports organizations to deliver the year-long lifestyle change program that has been proven to prevent or delay the onset of type 2 diabetes among those at high risk. This article describes activities, barriers, and facilitators reported by funded states during the first 3 years (2013–2015) of a 5-year funding cycle.\n\n【11】Top\n\n【12】Introduction\n------------\n\n【13】Prediabetes is clinically known as the stage between normal blood glucose and severe glucose intolerance (1) where blood glucose or glycated hemoglobin A1C levels are elevated but not high enough to be diagnosed as diabetes. The Centers for Disease Control and Prevention (CDC) estimates that 84 million adults aged 18 years or older in the United States have prediabetes, nearly 90% of whom are unaware of their condition (2). Prediabetes increases the risk of developing not only type 2 diabetes but cardiovascular disease as well (3). The progression of prediabetes to type 2 diabetes can be prevented or delayed by lifestyle behavior modification addressing diet, exercise, and stress reduction that results in a 5% to 7% weight loss (3,4). On the basis of findings from the Diabetes Prevention Program research study and subsequent translation studies (5,6), Congress authorized CDC in 2010 to establish the National Diabetes Prevention Program (National DPP), which provides a framework for type 2 diabetes prevention efforts in the United States.\n\n【14】A key component of the National DPP is a structured, evidence-based, year-long lifestyle change program (LCP) to prevent or delay onset of type 2 diabetes in people with prediabetes or at risk of developing type 2 diabetes (7). The LCP is group-based program that is facilitated by a trained lifestyle coach, and uses a CDC-approved curriculum. The curriculum uses regular opportunities for direct interaction between the lifestyle coach and participants, builds peer support, and focuses on behavior modification through healthy eating, increasing physical activity, and managing stress. The program may be delivered in person, online, or through a combination of both delivery modes (8,9,10,).\n\n【15】CDC’s Division of Diabetes Translation works collaboratively to scale and sustain the National DPP through partnerships with public and private organizations at state and local levels (7). The Division of Diabetes Translation also manages the Diabetes Prevention Recognition Program (DPRP), the quality assurance arm of the National DPP. Through the DPRP, CDC awards recognition to organizations delivering the LCP that are able to meet national quality standards and achieve the outcomes proven to prevent or delay onset of type 2 diabetes (11).\n\n【16】In 2013, in an effort to promote an integrated model of chronic disease prevention and management, CDC’s National Center for Chronic Disease Prevention and Health Promotion developed the State Public Health Actions to Prevent and Control Diabetes, Heart Disease, Obesity and Associated Risk Factors and to Promote School Health (SPHA-1305) cooperative agreement. Under this 5-year cooperative agreement, all 50 state health departments and the District of Columbia were funded to implement strategies to reinforce health promotion, epidemiology, and surveillance activities and implement targeted strategies that would have a significant impact on school health, nutrition, obesity, diabetes, heart disease, and stroke.\n\n【17】We present preliminary findings from a collaborative effort between CDC and state health departments designed to scale and sustain the National DPP. Findings from the first 3 years are described with the goal of providing an in-depth understanding of types of activities implemented along with barriers and facilitators experienced. Comments from grantee reports are included to augment findings presented. The information described in this article was exempt from ethical research approval because it involved only a secondary analysis of state program reports. Grantees did not report data in year 1; thus, findings reflect years 2 and 3 of the funding cycle (reports submitted in 2015 and 2016).\n\n【18】Top\n\n【19】Implementation Framework To Scale And Sustain the National DPP\n--------------------------------------------------------------\n\n【20】Through funding to state health departments, CDC works to promote awareness of prediabetes and the National DPP; increase prediabetes screening, testing, and referral; and increase program participation by facilitating relationships between government agencies, community-based organizations, insurance providers, private-sector employers, academia, and health care providers (8). State health departments also work to secure the program as a covered benefit for state employees and Medicaid beneficiaries at risk for type 2 diabetes.\n\n【21】The strategy for scaling and sustaining the National DPP is a set of recommended activities grouped into 4 drivers that are essential to long-term success: 1) support the efforts of partners to increase the availability of LCPs, 2) implement referral policies and mechanisms, 3) establish payers and payment mechanisms, and 4) identify and enroll people with prediabetes or at high risk for type 2 diabetes in LCPs ( Table 1 ). When targeted individually and collectively, these drivers are designed to improve availability of programs; expand reimbursement and insurance coverage; increase the use of practices and policies within health care systems to screen, test, and refer patients; and increase willingness of people at high risk of developing type 2 diabetes to enroll.\n\n【22】Top\n\n【23】State Health Department Progress on Key Activities\n--------------------------------------------------\n\n【24】State health departments provide an account of their progress for key activities and outcomes to CDC annually. Two CDC authors (Y.M. and S.R.) qualitatively analyzed data from grantee annual performance reports from years 2 and 3 to summarize the types of activities implemented. We developed structural codes from the drivers and analyzed 20 activities from each data set to ensure reliability of the codes. To ensure consistency in coding, authors then discussed their findings and refined codes to reach a consensus before independently analyzing the remaining data. During their discussion, authors added codes to capture activities that were inconsistent with the drivers. Activities that were dropped by state health departments were not included in the analysis.\n\n【25】### Barriers and facilitators encountered by state health departments\n\n【26】State health departments selected 1 intervention strategy to evaluate over the cooperative agreement period. In year 2, California, Colorado, Florida, Kentucky, Maine, Maryland, Minnesota, Missouri, Montana, New Mexico, Nebraska, New York, North Carolina, Oregon, and Rhode Island chose to evaluate the National DPP strategy. In year 3, Nebraska elected not to evaluate its National DPP work. Evaluation reports from 15 states were included in the year 2 analysis, and reports from 14 states were included in year 3. Two authors (S.B. and Y.M.) analyzed the 2 data sets by using a multistage iterative process to develop a hierarchy of codes for the data. Authors determined a priori to use the drivers as basic codes, then conducted a thematic analysis coding of reported facilitators and barriers to each driver. After an initial analysis of 2 reports, authors compared findings and refined and added subcodes before proceeding to code the remaining reports. An additional “other” category was added to capture information not classified within the codes and subcodes.\n\n【27】The number of activities implemented by state health departments across all 4 categories of drivers doubled from year 2 to year 3, from 148 to 295. State health departments engaged partners to support the scaling of the National DPP gradually and strategically through the funding cycle. A summary of key facilitators and barriers with representative comments is presented ( Table 2 ).\n\n【28】### Support the efforts of partners to increase availability of LCPs\n\n【29】Thirty-five state health departments supported the implementation of activities of their partners to increase availability of programs in year 3, an increase of 133% from 15 state health departments in year 2 (Figure). The most commonly reported activities for this driver were creating a network of partners to develop a strategic plan to scale and sustain the National DPP, convening key stakeholders to address barriers affecting programs, examining state data to prioritize the location of new programs, establishing mechanisms to increase the availability of LCPs, identifying organizations with infrastructure and capacity to deliver programs, and leveraging state resources to support their partners to start new programs. State health departments partnered with community organizations, health care organizations, employers, private insurers, and government agencies to increase the availability of LCPs in the community.  \n  \n\n【30】**Figure.**  \nNumber of state and District of Columbia health departments (n = 51) implementing activities within each of 4 drivers essential to increasing enrollment of people with prediabetes or at high risk of developing type 2 diabetes into National Diabetes Prevention Program (National DPP) lifestyle change programs (LCPs), 2015–2016. \\[A tabular version of this figure is also available.\\]\n\n【31】California reported “a marked increase in the number of in-person and online LCPs due, in part, to organizations and businesses that were able to host LCPs in multiple locations.” The ability of these organizations and businesses to obtain funding was reported as another facilitator that “removed key barriers to the start-up of new programs” (Minnesota) and “supported program uptake” (Maryland). Nebraska reported that “The complex nature of evidence-based programs made timely, clear, and adequate technical assistance important to our programs and enabled us to continue with our implementation efforts.”\n\n【32】Although partnerships were vital, some state health departments faced several challenges in their partnerships, including a lack of accountability, predetermined reporting structure, clarity in partnership roles, follow-through with strategic planning efforts, decision-making power, clear partner priorities, and interest. All of these challenges impeded efforts to increase program uptake.\n\n【33】### Implement referral policies and mechanisms\n\n【34】Forty-seven state health departments implemented activities to increase the number of provider referrals made to LCPs in year 3. This represented a 62% increase in number of state health departments from year 2 (Figure). Of the 11,385 participants enrolled in LCPs in year 3, 67.6% (7,700) were referred by a provider. The most common activities reported were promoting the adoption of the American Medical Association (AMA)/CDC provider tool kit (12), providing technical assistance on prediabetes screening and testing, integrating referrals into coordinated care models, and leveraging existing electronic health records (EHRs) as novel referral methods to increase participation in LCPs partnering with state and local medical and nonmedical associations to engage the clinical community. For example, in years 2 and 3, the Florida Department of Health, in partnership with the American Diabetes Association (ADA), provided mini-grants to 14 LCPs. These grantees were able to reach 503 health care practices and 955 physicians to discuss establishing processes or policies for referrals to LCPs, and 33 policies and 256 procedures were implemented to refer patients to LCPs. In the 14 LCPs, 336 participants achieved the desired weight loss outcome of 5% or more in year 3.\n\n【35】Facilitators to developing referral policies for LCPs were having the buy-in of hospital systems, partnering with providers to establish patient referral policies, delivering provider education through academic detailing (face-to-face education of providers by trained health care professionals), providing feedback to providers on referral status, and integrating or linking CDC-recognized lifestyle change programs to referring clinics. Having LCPs attached to primary health care settings was valuable. For example, “YMCAs that established referral policies with local hospitals or health care providers show greater success recruiting and filling workshop classes than those that did not” (New York State). Integration of prediabetes clinical measures into EHRs and providing prediabetes resources in patient waiting areas contributed to referral success. Lifestyle coaches and participants viewed health care providers and workplace health programs as effective referral mechanisms. Grantees also reported challenges to increasing referrals, such as low provider awareness, provider resistance to making referrals, and difficulty reaching providers to establish a feedback loop.\n\n【36】### Establish payers and payment mechanisms\n\n【37】Activities to establish coverage through payers and payment mechanisms included convening stakeholders to develop a state-specific business case, recruiting champions, and engaging stakeholders to discuss coverage for state employees and Medicaid beneficiaries. By year 3, 42 state health departments were implementing activities around this driver, a threefold increase from 14 state health departments in year 2.\n\n【38】Establishing partnerships to address lack of coverage was key to increasing LCP reimbursement and enrollment for state health departments: “State employees began having the National DPP offered to them as a covered benefit. Our diabetes program has been working \\[for a while\\] with the State Employee Group Insurance Program to promote the ‘Prevent’ program within our agency” (Minnesota); “The National DPP was added as a covered health benefit for state employees enrolled in Kaiser Permanente and United Healthcare plans” (Colorado).\n\n【39】Lack of insurance coverage for the National DPP was reported as a significant barrier. One state health department expressed concern that the lack of insurance coverage for the National DPP transferred the implementation costs to delivery sites that depended on reimbursement to be sustainable. In this state, low or no availability of coverage is reportedly driven by a complex payer landscape where Medicaid reimburses for the National DPP, but not all employers offer insurance coverage for prediabetes. Another state health department established a formal relationship with the state governor’s office, which resulted in coverage for state employees. The governor subsequently established National DPP enrollment as a leading health metric for the state.\n\n【40】### Identify and enroll people with prediabetes or at high risk for type 2 diabetes in LCPs\n\n【41】In year 3, 22 state health departments were involved in activities to increase enrollment of people at high risk of developing type 2 diabetes into LCPs (Figure). This was a 46% increase in the number of state health departments implementing activities to support this driver from year 2 to year 3. The most commonly reported activities were training lifestyle coaches, developing a marketing plan, and directing culturally appropriate marketing materials to people at high risk of developing type 2 diabetes.\n\n【42】State health departments reported that availability of culturally and linguistically aligned lifestyle coaches was a major facilitator for identification and enrollment of people with prediabetes or at high risk for type 2 diabetes into LCPs. Transportation, proximity to programs, awareness of programs, maintaining contact with program participants on a regular basis, and availability of low-cost or no-cost programs were also reported as facilitators to increasing enrollment. The Montana state health department reported increased enrollment and participation in the LCPs and concluded that incentives contributed significantly to participants’ weight loss outcomes. Barriers to participants’ willingness to enroll were transportation needs, cost, scheduling difficulties, nonadherence to care, LCP complexity and length, lack of perceived self-efficacy, lack of skills needed to track food intake and physical activity, and feelings of discomfort in group settings. Another state health department reported that “the challenge continues in assuring that both consumers and clinicians recognize that prediabetes is a considerable risk factor and one that can be reduced with participation in evidence-based programming” (Nebraska). Solutions reported were using Medicaid transportation assistance, adapting the curriculum without changing core elements, reiterating key session points, simplifying tracking tools, promoting coping skills, and providing ongoing support from multiple people beyond the coaches (case managers, doctors, therapists, family, and friends).\n\n【43】### Limitations of this early analysis\n\n【44】Reporting from state health departments on their implementation of activities to scale and sustain the National DPP had some limitations. Because the evaluation of activities implemented to scale and sustain the National DPP was optional, only 15 state health departments in year 2 and 14 in year 3 elected to evaluate the impact of their activities. Thus, the discussion of barriers and facilitators to implementation represents what was reported by state health departments that evaluated their National DPP work. If state health departments opted to evaluate a strategy on the basis of how well they were doing, the overall results would appear more favorable than what actually took place. In addition, the variation in the level of detail provided in the annual performance reports and evaluation reports is a limitation of this study. Some grantee reports provided detailed accounts of activities, successes, and barriers, whereas other reports provided brief responses.\n\n【45】Despite aforementioned limitations, this report reflects efforts to promote an integrated model of chronic disease prevention and provides insights into ways to evaluate activities to support and scale a complex, multisector national program designed to stem the current and projected growth in new cases of type 2 diabetes. Our findings identify unique and innovative approaches for real-world program adoption and implementation — specifically, approaches that inform new ways of encouraging people in various sectors to work together to improve health. Our findings provide real-time insight that can be used to refine universal program implementation and increase opportunities for people at risk to be exposed to evidence-based interventions and to have good health outcomes.\n\n【46】Top\n\n【47】Implications for the Future\n---------------------------\n\n【48】State health departments are effectively supporting evidence-based programs such as the National DPP to prevent or delay the onset of type 2 diabetes in people at high risk. Improving and sustaining collaborations between public health agencies and health systems is crucial to the success of this work. We present information on what works and also information for developing guidance on implementing activities that support and scale this evidence-based intervention in community settings. Understanding the activities being implemented, along with barriers and facilitators, has implications for technical assistance to support the expansion and sustainability of the National DPP. These results provide relevant data on state health departments’ progress and contribute to the identification of potential best practices. Furthermore, what is learned from states’ evaluations is critical to making adjustments midstream during implementation of activities to scale and sustain the National DPP. These early findings can inform the establishment of communities of practice, identify state health departments to lead peer-to-peer learning collaboratives, and shape guidance for scaling not just the National DPP but future public health practice.\n\n【49】Top\n\n【50】Acknowledgments\n---------------\n\n【51】The authors acknowledge the state health departments for providing their insights related to implementing activities and describing barriers and facilitators as part of the SPHA 1305 cooperative agreement. The authors received no funding for the research described in this article and have no conflicts of interest to declare. The findings and conclusions in this report are those of the authors and do not necessarily represent the views of CDC.\n\n【52】Top\n\n【53】Author Information\n------------------\n\n【54】Corresponding Author: Yvonne Mensa-Wilmot, PhD, MPH, Centers for Disease Control and Prevention, Division of Diabetes Translation, 4770 Buford Hwy NE, MS-F75, Atlanta, GA 30341-3717. Email: YMensaWilmot@cdc.gov .\n\n【55】Author Affiliations: <sup>1 </sup> Division of Diabetes Translation, Centers for Disease Control and Prevention, Atlanta, Georgia. <sup>2 </sup> ICF International, Atlanta, Georgia. <sup>3 </sup> Oak Ridge Institute for Science and Education, Oak Ridge, Tennessee.\n\n【56】Top\n\n【57】References\n----------\n\n【58】1.  McCain J. Prediabetes: pre- does not mean preordained. Manag Care 2016;25(5):35–41. PubMed external icon\n2.  Centers for Disease Control and Prevention. National diabetes statistics report, 2017. Atlanta (GA): US Department of Health and Human Services, Centers for Disease Control and Prevention; 2017.\n3.  Geiss LS, James C, Gregg EW, Albright A, Williamson DF, Cowie CC. Diabetes risk reduction behaviors among US adults with prediabetes. Am J Prev Med 2010;38(4):403–9. CrossRef external icon PubMed external icon\n4.  Perreault L, Pan Q, Mather KJ, Watson KE, Hamman RF, Kahn SE; Diabetes Prevention Program Research Group. Effect of regression from prediabetes to normal glucose regulation on long-term reduction in diabetes risk: results from the Diabetes Prevention Program Outcomes Study. Lancet 2012;379(9833):2243–51. CrossRef external icon PubMed external icon\n5.  Diabetes Prevention Program (DPP) Research Group. The Diabetes Prevention Program (DPP): description of lifestyle intervention. Diabetes Care 2002;25(12):2165–71. CrossRef external icon PubMed external icon\n6.  Knowler WC, Fowler SE, Hamman RF, Christophi CA, Hoffman HJ, Brenneman AT, et al. 10-Year follow-up of diabetes incidence and weight loss in the Diabetes Prevention Program Outcomes Study. Lancet 2009;374(9702):1677–86. Erratum in Lancet 2009;374(9707):2054. CrossRef external icon PubMed external icon\n7.  Centers for Disease Control and Prevention. The National Diabetes Prevention Program. https://www.cdc.gov/diabetes/prevention/index.html. Accessed September 24, 2017.\n8.  Albright AL, Gregg EW. Preventing type 2 diabetes in communities across the US: the National Diabetes Prevention Program. Am J Prev Med 2013;44(4, Suppl 4):S346–51. CrossRef external icon PubMed external icon\n9.  Sattin RW, Williams LB, Dias J, Garvin JT, Marion L, Joshua TV, et al. Community trial of a faith-based lifestyle intervention to prevent diabetes among African-Americans. J Community Health 2016;41(1):87–96. CrossRef external icon PubMed external icon\n10.  Zhuo X, Zhang P, Gregg EW, Barker L, Hoerger TJ, Tony Pearson-Clarke , et al. A nationwide community-based lifestyle program could delay or prevent type 2 diabetes cases and save $5.7 billion in 25 years. Health Aff (Millwood) 2012;31(1):50–60. CrossRef external icon PubMed external icon\n11.  Centers for Disease Control and Prevention. CDC Diabetes Prevention Recognition Program. https://www.cdc.gov/diabetes/prevention/lifestyle-program/requirements.html. Accessed September 24, 2017.\n12.  American Medical Association, Centers for Disease Control and Prevention. Preventing type 2 diabetes: a guide to refer your patients with prediabetes to an evidence-based diabetes prevention program. Atlanta (GA): US Department of Health and Human Services, Centers for Disease Control and Prevention; 2015.\n\n【59】Top\n\n【60】Tables\n------\n\n【61】#####  Table 1. Abbreviated List of Activities from the National Diabetes Prevention Program Technical Assistance Guide\n\n| Driver | Activities |\n| --- | --- |\n| Support the efforts of partners to increase the availability of LCPs | Integrate LCP planning and implementation with ongoing state/city diabetes coalition activities or state diabetes action plans.Explain readiness criteria to organizations interested in becoming LCPs.Use grant funds to help ADA/AADE DSME programs develop a strategic business plan to determine their capacity to offer a LCP. |\n| Implement referral policies and mechanisms | Distribute the AMA/CDC provider tool kit, and engage health care systems and providers in using it; partner with state and local medical associations in reaching the clinical community.Provide technical assistance, training, and academic detailing (face-to-face education of providers by trained health care professionals) on prediabetes screening, testing, and referrals to health care providers and care teams within existing LCP service areas.Support health care systems in building EHRs or other systems to facilitate and track referrals and enhance decision support. |\n| Establish payers and payment mechanisms | Develop a state-specific business case for the National Diabetes Prevention Program.Work with state employee health plans and the state Medicaid agency to secure or extend coverage where needed.Encourage LCP providers to connect with third-party administrators where necessary to facilitate billing and reimbursement. |\n| Identify and enroll people with prediabetes or at high risk for type 2 diabetes in LCPs | Use strategic communication strategies (eg, customized waiting room advertising) to reach people at high risk about the importance of National Diabetes Prevention Program benefits and coverage.Provide advanced training for lifestyle coaches (eg, motivational interviewing) to further strengthen group facilitation skills.Provide materials and other resources to support existing LCP providers’ marketing efforts to recruit participants. |\n\n【63】Abbreviations: AADE, American Association of Diabetes Educators; ADA, American Diabetes Association; AMA, American Medical Association; CDC, Centers for Disease Control and Prevention; DSME, diabetes self-management education; EHR, electronic health record; LCP, lifestyle change program.\n\n【64】#####  Table 2. Facilitators and Barriers to Implementing the National Diabetes Prevention Program, 2015–2016\n\n| Themes | Comments From State Health Department Representatives |\n| --- | --- |\n| **Facilitators** | **Facilitators** |\n| Reimbursement availability | “State employees began having the National DPP lifestyle change program offered to them as a covered benefit. Our diabetes program has been working with the State Employee Group Insurance Program to promote the ‘Prevent’ program within our agency.” (Minnesota)“A large employer and a large insurance company announced (2017) that the National DPP will become a covered benefit. Expansion in insurance coverage is due in part to California’s Department of Public Health’s PDSTAT statewide organization of stakeholders, which has been instrumental in educating payers and insurance companies about the need for and value of the National DPP. The US Preventive Services Task Force recommendations on diabetes screening, released in October 2015, were another factor in encouraging adoption of coverage for the National DPP.” (California) |\n| Practice/provider referral policies | “Based on CDC DPRP data, over 75% of participants in lifestyle change programs have enrolled based on a blood-based diagnostic test, which indicates that the majority of participants had a clinical test indicating prediabetes and were likely referred by a health care provider. YMCAs that established referral policies with local hospitals or health care providers show greater success in recruiting and filling classes than those that did not.” (New York State) |\n| Program curriculum | “Having standard curricula and referral policies helps facilitate dissemination of the National DPP lifestyle change program in community settings, particularly since coordinated care organizations want to implement evidence-based programs.” (Oregon) |\n| **Barriers** | **Barriers** |\n| CDC recognition process | “Paperwork and complicated processes, as well as the inability to use grant funds to support direct services, have been a challenge.” (Maryland) |\n| Limited program resources | “Several health systems, clinics, and community-based organizations are linked to lifestyle change programs for delivery and referral. However, many do not have formal policies and bidirectional networks in place. Staff and funding aimed at enhancing these policies and networks have been essential to carry this work forward.” (Nebraska)“These were the barriers to optimal National DPP implementation. There is a limited amount of wellness funding that has to be stretched across different priority areas.” (Colorado) |\n| Reimbursement availability | There is no standardized method of reimbursement, and confusion exists about who within the health system can apply for reimbursement: “Lack of insurance coverage for the program often shuts down conversations about referrals and is a constant barrier. Despite these obstacles, we do have some early adopters who are developing policies or willing to undergo practice change.” (Minnesota) |\n| Obtaining referrals | “Many lifestyle change programs report low enrollment and almost no referrals from physicians, even in cases where outreach was conducted to provider offices and larger health systems.” (California) |\n| Participant cost | “Lack of insurance coverage for lifestyle change programs statewide is most often cited as a reason for why providers are not diagnosing and referring patients and why patients are not attending (due to the high cost of the program). There are only a small handful of insurers in New York State that are covering the National DPP as a benefit for their members.” (New York State) |\n| Lack of data | “There is a lack of data on program completion rates, insurance information of enrollees, and measured health outcomes of program completers. Some insurers are aware of the benefit of the program but need more information on completers and outcomes to consider reimbursement.” (Rhode Island) |\n| Lack of awareness | “The majority of employees were not aware of the health and wellness policies in place in their departments.” (Colorado) |\n\n【66】Abbreviations: CDC, Centers for Disease Control and Prevention; DPRP, Diabetes Prevention Recognition Program; National DPP, National Diabetes Prevention Program; PDSTAT, Prevent Diabetes Screen Test Act Today.\n\n【67】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dac3372b-7782-4d18-a81e-d6d84e9f8d20", "title": "PCD 2021 Student Research Collection: Building Public Health Research Capacity in Real-World Settings and the 2022 Call for Papers", "text": "【0】PCD 2021 Student Research Collection: Building Public Health Research Capacity in Real-World Settings and the 2022 Call for Papers\n==================================================================================================================================\n\n【1】EDITORIAL — Volume 18 — July 8, 2021\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Leonard Jack Jr, PhD, MSc <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Jack L Jr. PCD 2021 Student Research Collection: Building Public Health Research Capacity in Real-World Settings and the 2022 Call for Papers. Prev Chronic Dis 2021;18:210214. DOI: http://dx.doi.org/10.5888/pcd18.210214 external icon .\n\n【6】NON–PEER REVIEWED\n\n【7】**Samuel F. Posner, PhD:  \nStudent Contest Visionary**\n\n【8】The PCD 2021 Student Research Collection is dedicated to Samuel Posner, PhD, the person who envisioned and led the journal’s student competition in 2011. Dr Posner provided 9 years of service to PCD as editor in chief, from 2008 through 2016. Dr Posner’s legacy is one of scientific excellence, technical innovation, and service.\n\n【9】Dr Posner is now the Acting Director of the National Center for Immunization and Respiratory Diseases (NCIRD) and the center’s deputy director for science. In this role, he oversees the center’s surveillance, epidemiology, laboratory, and data science programs to expand and refine CDC’s capacity to detect, prevent, and respond to vaccine-preventable and respiratory infectious disease threats.\n\n【10】Dr Posner came to CDC in 1998 from the University of California, San Francisco after completing doctoral work in quantitative psychology at the University of Southern California in 1996. He joined NCIRD in January of 2016 as the associate director for epidemiological science and director of the Office of Science and Integrated Programs. In this role, Dr Posner led the scientific review of several NCIRD programs, including the review and reframing of the Legionella program. Under his guidance, the Legionella program has grown in scope and reach and now funds 23 jurisdictions for both prevention and outbreak response.\n\n【11】In the COVID-19 response, Dr Posner co-led the development of a scientific agenda to guide implementation of scientific activities for better understanding of COVID-19 transmission dynamics, risk factors for mortality, and prevention strategies. He also worked closely on the data and informatics architecture for the COVID-19 national vaccine program.\n\n【12】Dr Posner is the author of more than 100 articles published in peer reviewed journals and has written more than 15 book chapters. He is an internationally recognized expert in the fields of preconception care and multiple chronic conditions, and he is an adjunct associate professor at both Emory University’s Rollins School of Public Health, Department of International Health, and the University of Alabama, Birmingham’s School of Public Health, Department of Health Behavior.\n\n【13】Early in his career, Dr Posner recognized the value of mentoring students in the value of publication, and his student research competition brought a new generation of public health researchers and practitioners to PCD. Since the inception of the competition, the journal has received nearly 500 student manuscripts for consideration. Dr Posner’s vision to promote academic research excellence for students around the world lives on today.\n\n【14】With this student collection, PCD celebrates the 10-year anniversary of our efforts to build scientific publishing skills and abilities among students. The primary aims of PCD’s student manuscripts have evolved over the years. Specifically, we aim to 1) provide an opportunity to become familiar with a journal’s manuscript submission requirements and peer review process; 2) foster connections among student knowledge and training, the conduct of quality research, and a journal’s publication expectations; 3) develop research and scientific writing skills to become producers of knowledge, rather than just consumers of knowledge; 4) provide an opportunity to become a first author on a peer-reviewed article; and 5) promote supportive, respectful, and mutually beneficial mentee relationships that strengthen students’ ability to generate and submit scholarly manuscripts throughout their professional careers (1). We believe that committing time, attention, and resources to providing student authors with valuable feedback (whether or not manuscripts are accepted) serves as a key capacity-building resource. Providing this feedback not only benefits PCD in the future but other peer-reviewed journals as well.\n\n【15】PCD published articles of winning student manuscripts from 2011 through 2015 and in 2017 and 2018. From 2011 through 2015, manuscripts were screened and reviewed by a panel of peer reviewers who identified an overall winner whose manuscript was ultimately published (2–11). Because of the tremendous response from students, we expanded submission screening, peer reviewing, and publishing to 5 student levels in 2017 and 2018, and winners were identified at the high school, undergraduate, graduate, doctoral, and postdoctoral levels (1,12). In addition to publishing student articles in each of the 5 levels, articles that successfully completed the peer-review process were also published. In this 2021 collection, we have continued to publish articles that successfully completed our rigorous peer-review process; however, we chose not to select a winner in each level. The COVID-19 pandemic rendered PCD unable to obtain the human resources necessary for facilitating a timely selection process. PCD will return to its usual selection process of identifying winners at all levels in future student competitions.\n\n【16】Over the years, PCD has refined eligibility requirements for students interested in submitting research manuscripts to the journal. Students must be a high school graduate, an undergraduate or graduate student, a medical resident, or a postdoctoral fellow. Authors must meet the journal’s criteria to be recognized as first author; that is, they must have prepared the first draft and conducted research on or practiced in the topic addressed in the manuscript. In addition, their work must have been completed within the previous 12 months. Manuscripts submitted for PCD consideration cannot be under consideration by another journal. PCD only considers Original Research or GIS Snapshots for student submissions. Most importantly, the author must serve in the combined roles of the manuscript’s first author and the corresponding author, which allows direct communication with the editor in chief and journal staff members. Students in this role receive critical instruction at every stage of scholarly publication, from submission, to peer review, to editorial revision, to production, and finally, to publication.\n\n【17】PCD’s student articles released to date address topics relevant to the prevention, screening, and surveillance of population-based interventions for chronic diseases, including but not limited to arthritis, cancer, diabetes, depression, obesity, and cardiovascular disease. Of the 38 manuscripts submitted for the PCD 2021 Student Research Collection, 10 successfully completed our rigorous internal and external peer-review process and multiple revisions. PCD sincerely appreciates our associate editors, the editorial board, members of the statistics review committee, and the many peer reviewers who provided detailed comments and suggestions to student authors. We congratulate each student author who developed and submitted a manuscript for consideration, whether it was accepted or not.\n\n【18】The current collection addresses a broad range of topics, including childhood obesity in secondary schools in Hong Kong (13); nutrition and physical activity among adults (14–16); the impact of inadequate sleep on mental health (17); the association between neighborhood built environments and depression in the rural South (18); colorectal cancer risk factors and screening among uninsured adults living in Tampa Bay, Florida (19); spatial accessibility to dental care among Alabama youth (20); identifying challenges to care for people living with hepatitis delta virus and their caretakers (21); and community resources to promote health among Chinese immigrants living in Philadelphia (22).\n\n【19】PCD is pleased to announce the Call for Student Papers: 2022 Publishing Opportunity for Students. Information about submission requirements are available on the PCD Announcements page at at https://www.cdc.gov/pcd/announcements.htm . The deadline for submissions is Monday, March 28, 2022. PCD looks forward to continuing its commitment to the development of scientific writing and publishing skills among students. For more information about the journal and previous collections of student articles, please visit the PCD website at https://www.cdc.gov/pcd/index.htm .\n\n【20】Top\n\n【21】Author Information\n------------------\n\n【22】Corresponding Author: Leonard Jack Jr, PhD, MSc, Editor in Chief, Preventing Chronic Disease: Public Health Research, Practice, and Policy, Office of Medicine and Science, National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, 4770 Buford Hwy NE, Mailstop S107-8, Atlanta, GA 30341. Email: ljack@cdc.gov .\n\n【23】Author Affiliations: <sup>1 </sup> Preventing Chronic Disease, Office of Medicine and Science, National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, Atlanta, Georgia.\n\n【24】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "22f0e9b3-2a92-4e22-bb7c-4e0c2230668f", "title": "Personal Protective Equipment and Risk for Avian Influenza (H7N3)", "text": "【0】Personal Protective Equipment and Risk for Avian Influenza (H7N3)\nIn April 2006, an outbreak of avian influenza occurred on 3 poultry farms in Norfolk, England ( _1_ ). Reverse transcription–PCR (RT-PCR) of poultry blood samples and cloacal swabs detected low-pathogenic avian influenza (H7N3) on 1 farm, and veterinary investigation confirmed influenza subtype H7N3 on the 2 adjacent farms. Surveillance and protection zones were established around all infected premises, and all birds were culled. Persons who had been exposed were offered oseltamivir prophylaxis; those with influenza symptoms were offered oseltamivir treatment and influenza vaccination. All persons at risk were orally instructed to wear personal protective equipment (PPE).\n\n【1】### The Study\n\n【2】We conducted a retrospective cohort study of all persons who had been potentially exposed to infectious material by handling live and dead poultry, poultry products, or litter derived from infected premises. Our objective was to measure associations between potential exposure to infectious material, completeness of use of PPE, and taking and timing of oseltamivir prophylaxis with having symptoms consistent with or confirmed as resulting from influenza virus A (H7N3) infection. We pretested and then administered a questionnaire by telephone after poultry culling ended (median 66 days, range 60–143 days). For persons who did not respond to the questionnaire (n = 39), we extracted data recorded in the outbreak records to describe their activities in relation to the outbreak, their use of oseltamivir prophylaxis, and their seasonal influenza vaccine status. Only persons who were interviewed and completed the questionnaire (n = 103) were included in the statistical analysis. Persons were invited to provide an acute-phase blood sample during the outbreak and a convalescent-phase sample 28 days after their last potential exposure. Exceptions were those at low risk; e.g., incinerator workers and lorry drivers.\n\n【3】Possible case-patients were those who reported conjunctivitis or influenza-like symptoms ( \\> 1 of the following: fever, sore throat, cough, shortness of breath, body/muscle pain, runny nose) in the 7 days after last potential exposure. Confirmed case-patients were those for whom virus was detected by culture and RT-PCR of material from the conjunctiva or respiratory tract and/or confirmed by serologic testing. Influenza virus (H7N3) from the conjunctiva of the index case-patient was prepared by growth in embryonated eggs. Serum samples were screened by using microneutralization (MN) and hemagglutination inhibition (HI) tests ( _2_ _,_ _3_ ). We defined MN >20 as evidence of seroreactivity. When either test gave a positive result, we performed confirmatory Western blot analysis, using purified influenza (H7N3) virus ( _4_ ).\n\n【4】We calculated odds ratios (ORs), 95% confidence intervals (CIs), and p values for being a possible or confirmed case-patient. Independent variables are shown in the Technical Appendix , Table A. All risk factors with p < 0.2 in the single-variable analysis were initially included in a logistic regression model and then removed, least significant first, until all had p < 0.1. Confounding variables (those that caused \\> 10% change in the ORs of covariates) were retained regardless of p value.\n\n【5】In total, 142 persons were potentially exposed. Questionnaires were completed for 103 (73%) persons (21 could not be contacted, 10 declined, 7 had no contact information, and 1 questionnaire was lost). Characteristics, potential exposures, and preventive measures differed little between persons who did or did not complete the questionnaire ( Table 1 ). Of 46 persons who reported symptoms, 19 reported conjunctivitis with influenza-like symptoms and 27 reported influenza-like symptoms only. PPE reported as “always used\" were protective coveralls (81%), protective footwear (82%), disposable gloves (67%), face-fitted mask (51%), other mask (24%), and protective goggles (19%) ( Technical Appendix , Table B).\n\n【6】Fifty-six (54%) persons reported complete use of PPE. Single-variable analysis indicated that working on an infected premise (OR 2.76, 95% CI 1.17–6.50) was significantly associated with being a possible or confirmed case-patient ( Technical Appendix , Table A). Higher levels of exposure to potentially infected poultry (OR 2. 20, 95% CI 0.96–5.04) and only partial use compared with full use of PPE (OR 2.16, 95% CI 0.97–4.83) were also associated with being a possible or confirmed case-patient, but 95% CIs were <1.0. Characteristics not associated with being a possible or confirmed case-patient were age >30 years; male sex; being a Department for Environment, Food and Rural Affairs employee; smoking; having had a prior influenza vaccination; timing of starting oseltamivir prophylaxis; and exposure to potentially infected poultry in the preceding months. Multivariable analysis showed the association with being a possible or confirmed case-patient to be statistically significant for incomplete use of PPE and weakly significant for working on an infected premise ( Table 2 ).\n\n【7】Serum samples were available from 91 persons: 33 acute- and convalescent-phase pairs, 49 acute-phase samples, and 9 convalescent-phase samples. Only the serum from the index case-patient showed reactivity in both the MN (titer 40) and HI (titer 32) tests and also showed reactivity in Western blot. No acute-phase sample from this person was available. All other acute- and convalescent-phase samples were negative in both tests. During the outbreak, eye, nose, and throat swabs were taken from 14 persons (1–8 days after symptom onset); 10 reported influenza-like symptoms (2 without eye involvement), 2 reported no symptoms, and 2 had no clinical information available. Comprehensive molecular diagnostic tests for common human viral respiratory pathogens (enteroviruses, rhinoviruses, adenoviruses, respiratory syncytial viruses, parainfluenza viruses) were also performed and did not provide evidence of alternative causes of infection. A vaccine strain of avian paramyxovirus (Newcastle disease virus) was recovered from 1 person with conjunctivitis, which suggests that at least 1 case of conjunctivitis was caused by avian paramyxovirus. Serologic testing for seasonal influenza infection (HI tests on all paired serum samples) did not indicate any recent human infections.\n\n【8】Our study had a number of limitations. Because workers were interviewed a minimum of 2 months after the outbreak, they may not have accurately recalled their exposures. In addition, we relied on self-reported data. Difficulties recalling symptoms were less likely as we actively followed up persons for 7 days after last exposure. In the absence of a control group, such as farmers from noninfected premises, whether the incidence of influenza-like illness and conjunctivitis in this cohort was different is unclear, although during the outbreak, influenza activity in the general population was low and no isolates of seasonal influenza were reported. We did not measure dust exposure as an alternative explanation for conjunctivitis in some or all persons, apart from the index case-patient who reported this symptom. The results from laboratory testing were limited because convalescent-phase serum was not available from all persons who reported influenza-like illness. However, a wide range of molecular diagnostic tests for human viral pathogens were performed on samples from persons who were not well at the time of the outbreak. Because the kinetics of appearance and disappearance of human antibodies to avian influenza are poorly understood, timing of the collection of samples may not have been optimal in this outbreak and we may have missed the opportunity to diagnose some infections. Moreover, because serologic tests for influenza virus A (H7N3) may not correlate well with infection ( _5_ ), we could not rule out influenza A virus (H7N3) infection among symptomatic persons, even in the presence of convalescent-phase serum that was negative for H7.\n\n【9】### Conclusions\n\n【10】Strict compliance with PPE use should be reinforced when outbreaks of avian influenza among poultry are being managed, as recommended in current guidance from the United Kingdom ( _6_ ) and the European Centre for Disease Prevention and Control ( _7_ ). Compliance tends to be suboptimal ( _8_ ), possibly because of low risk perception among poultry workers ( _9_ ). Understanding what obstacles prevent workers from wearing complete PPE is needed. Our study suggests that rigorous use of PPE by persons managing avian influenza outbreaks reduces influenza-like symptoms and conjunctivitis and potentially hazardous exposure to infected poultry materials.\n\n【11】Dr Morgan is an Epidemic Intelligence Service Officer at the Centers for Disease Control and Prevention, Atlanta, Georgia. His interests include infectious disease surveillance and acute incident epidemiology.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4d618fa0-ea32-4d65-8c75-a8e80f426e39", "title": "White-Nose Syndrome Fungus in a 1918 Bat Specimen from France", "text": "【0】White-Nose Syndrome Fungus in a 1918 Bat Specimen from France\nWe report the earliest known historical incidence of the fungus _Pseudogymnoascus_ (formerly _Geomyces_ ) _destructans_ , detected in a museum specimen of a bat ( _Myotis bechsteinii_ ) collected in France in 1918. This fungal pathogen causes white-nose syndrome (WNS) in bats ( _1_ ). Since its introduction into eastern North America around 2006, WNS has devastated bat populations across the continent ( _2_ ). _P. destructans_ has also been found across the Eurasian landmass ( _3_ _,_ _4_ ) without documented mass bat deaths. Epidemiologic evidence among bats and fungal genetics indicate that the fungus has been recently introduced into North American bat populations ( _5_ _–_ _7_ ).\n\n【1】To clarify the epidemiologic history of WNS and to investigate physical evidence of its presence in specific locations in the past, we screened 138 19th- and 20th-century bat specimens (housed at the National Museum of Natural History \\[USNM\\], Washington, DC) from North America (n = 41), Europe (n = 83), and East Asia (n = 14) for _P. destructans_ DNA ( Technical Appendix ). We sampled dry museum skins and intact bodies stored in 70% ethanol; some were originally fixed in formalin. We swabbed bat rostra and wings to collect potentially preserved _P. destructans_ biomolecules and stored swabs in 100% ethanol until DNA extraction.\n\n【2】We extracted DNA in a dedicated ancient DNA laboratory at the National Zoological Park (Washington, DC) by using stringent protocols to prevent false positive results from modern DNA contamination ( _8_ ). Before extraction, we removed swabs from the ethanol and let them air dry. We then let swabs digest overnight at 55°C in 600 μL extraction buffer (1× Tris-EDTA buffer, pH 8.0, 0.019 mmol/L EDTA, 0.01 mmol/L NaCl, 1% SDS, 10 mg/mL DTT, and 1 mg/mL proteinase K) ( _8_ ). Later extractions omitted DTT. We extracted digested samples twice in 600 μL phenol and once in 600 μL chloroform. We removed and concentrated the aqueous phase by using Amicon Ultra-4 30 kDA molecular weight cutoff columns (Millipore Sigma, Merck, Billerica, MA, USA) to a final volume of ≈250 μL. We included 1 extraction blank for every 10–11 historical samples.\n\n【3】We screened extracts for _P. destructans_ by using a previously described species-specific quantitative PCR targeting 103 bp (including primers) of the intergenic spacer region ( _9_ ). Each extract was amplified in 2–8 replicate PCRs. Multiple, no-template controls ( _2_ , _3_ ) were included in each PCR setup. Positive products from experiments in which quantifiable contamination (>0.1 genome equivalents/μL sample) was observed in \\> 1 negative control were discarded; these experiments were repeated with fresh reagents.\n\n【4】One sample (USNM 231170) tested positive in 2 of 3 PCRs. We performed a second independent extraction on this sample. The replicate extraction tested positive in 4 of 5 PCRs. Two of the USNM 231170–positive PCR products were confirmed by using Sanger sequencing and comparison to publicly available _P. destructans_ sequences in GenBank. These sequences were 100% identical to _P. destructans_ sequences from North America (GenBank accession nos. JX270192.1 and JX415267.1). In addition, 2 samples (USNM 15513 and 154222) yielded positive products in a single PCR each, but we were unable to replicate these results. No usable sequence was obtained from the USNM 15513 amplicon; the USNM 154222 sequence differed from the North American sequence by a single thymine deletion. DNA sequences were deposited in GenBank (accession nos. MF370925–6).\n\n【5】The 1 confirmed case of _P. destructans_ infection among the museum samples we studied (USNM 231170, male, skin and skull) was in a Bechstein’s bat ( _Myotis bechsteinii_ ) collected on May 9, 1918, at Forêt de Russy, Centre-Val de Loire, France. This sequence is unlikely to represent _P. destructans_ from a recently collected infected bat from North America because recently collected specimens have been purposefully stored with care in a separate room within the USNM mammal department, away from the historical bat collection. Furthermore, none of the historical samples of bats collected in North America, which are more likely to be cross-contaminated with potentially infected specimens compared with European specimens, tested positive for _P. destructans._\n\n【6】We provide evidence of the presence of _P. destructans_ ≈100 years ago in Europe. In addition, we found no evidence of _P. destructans_ in bats collected in eastern North America during 1861–1971. Although false negatives are highly likely because of the age, preparation, and storage of these specimens, these results are consistent with a Eurasian origin of the current WNS epidemic and strong association of the fungus with Eurasian bats of the genus _Myotis_ ( _3_ _,_ _6_ _,_ _10_ ). Bats across Eurasia have adapted to _P_ . _destructans_ over more than a century, but the fungus was initially detected in North America during the early 21st century, and the bats on this continent have no immunity. This result extends the documented temporal occurrence of _P. destructans_ as a bat-associated fungus to the early 20th century and highlights the value of archived museum specimens for epidemiologic study of emerging fungal diseases, including WNS.\n\n【7】Dr. Campana is a computational genomics scientist at the Smithsonian Institution Conservation Biology Institute’s Centers for Conservation Genomics. His research interests include analysis of animal and pathogen population genomics by using a combination of ancient DNA and computational genomics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "661ba51f-3822-409e-8cb6-a11ec387b2da", "title": "All Injuries", "text": "【0】All Injuries\nData are for the U.S.\n\n【1】Morbidity\n\n【2】*   Percent of adults aged 18 and over who had an activity-limiting injury in the past 3 months: 5.9%\n\n【3】Source: Activity-limiting Injury in Adults: United States, 2020−2021\n\n【4】Physician office visits\n\n【5】*   Number of visits to physician offices for injuries: 57.5 million (includes visits for adverse effects of medical treatment)\n\n【6】Source: National Ambulatory Medical Care Survey: 2018 National Summary Tables, table 16 \\[PDF – 704 KB\\]\n\n【7】Emergency departments visits\n\n【8】*   Number of emergency department visits for injuries: 38.0 million (includes visits for adverse effects of medical treatment)\n\n【9】Source: National Hospital Ambulatory Medical Care Survey: 2020 National Summary Tables, table 16 \\[PDF – 1.4 MB\\]\n\n【10】Mortality\n\n【11】#### All injury deaths\n\n【12】*   Number of deaths: 306,086\n*   Deaths per 100,000 population: 92.2\n\n【13】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【14】#### All poisoning deaths\n\n【15】*   Number of deaths: 111,830\n*   Deaths per 100,000 population: 33.7\n\n【16】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【17】#### Motor vehicle traffic deaths\n\n【18】*   Number of deaths: 45,404\n*   Deaths per 100,000 population: 13.7\n\n【19】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【20】#### All firearm deaths\n\n【21】*   Number of deaths: 48,830\n*   Deaths per 100,000 population: 14.7\n\n【22】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【23】Mortality: Drug overdose/drug poisoning deaths\n\n【24】*   Number of deaths: 106,699\n*   Deaths per 100,000 population: 32.1\n\n【25】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【26】*   Number of drug overdose deaths involving any opioid: 80,411\n*   Drug overdose deaths involving any opioid per 100,000 population: 24.2\n\n【27】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【28】*   Number of drug overdose deaths involving heroin: 9,173\n*   Drug overdose deaths involving heroin per 100,000 population: 2.8\n\n【29】Source: National Vital Statistics System – Mortality Data (2021) via CDC WONDER\n\n【30】Related FastStats\n-----------------\n\n【31】*   Accidents/Unintentional Injuries\n*   Assault/Homicide\n*   Drug Overdoses\n*   Suicide/Self-Inflicted Injury\n\n【32】More data: reports and tables\n-----------------------------\n\n【33】*   Trends in Injury from Health, United States\n*   Health, United States — Topic Page: Injury visits\n*   Deaths: Leading Causes for 2019 \\[PDF – 3 MB\\]\n*   Drug Overdose Deaths in the United States, 2001-2021\n*   Unintentional Fall Deaths Among Adults Aged 65 and Over: United States, 2020\n*   Concussions and Brain Injuries in Children: United States, 2020\n*   Emergency Department Visit Rates for Motor Vehicle Crashes by Selected Characteristics in the United States, 2019–2020\n*   Drug Poisoning Mortality, by State and by Race and Ethnicity: United States, 2019\n*   Urban-rural Differences in Unintentional Injury Death Rates Among Children Aged 0-17 Years: United States, 2018-2019\n*   Motor Vehicle Traffic Death Rates, by Sex, Age Group, and Road User Type: United States, 1999-2019\n*   Unintentional Drowning Deaths Among Children Aged 0-17 Years: United States, 1999-2019\n\n【34】More data: query tools\n----------------------\n\n【35】*   CDC WONDER – Multiple Cause-of-Death Data\n*   CDC WONDER – Underlying Cause of Death\n*   NCHS Mortality Dashboard\n*   Web-based Injury Statistics Query and Reporting System (WISQARS)\n*   NCHS Data Visualization on County-level Drug Overdose Mortality in the United States, 2003-2021\n\n【36】Related Links\n\n【37】*   Ambulatory Health Care Data\n*   Mortality Statistics\n*   National Center for Health Statistics: Injury Data and Resources\n*   National Health Interview Survey\n*   NHIS: Injury and Poisoning Information\n*   Injury Definitions and Methods \\[PDF – 2.5 MB\\]\n*   National Center for Injury Prevention and Control\n*   National Institute for Occupational Safety and Health", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e819c25b-5147-4aec-a249-4671e5bccab8", "title": "Hepatitis Virus in Long-Fingered Bats, Myanmar", "text": "【0】Hepatitis Virus in Long-Fingered Bats, Myanmar\nThe family _Hepadnaviridae_ comprises 2 genera ( _Orthohepadnavirus_ and _Avihepadnavirus_ ), and viruses classified within these genera have a narrow host range. The genus _Orthohepadnavirus_ consists of pathogens that infect mammals, and it currently contains 4 species: _hepatitis B virus_ , _woodchuck hepatitis virus_ , _ground squirrel hepatitis virus_ , and _woolly monkey hepatitis B virus_ . The genus _Avihepadnavirus_ contains 2 avian species: _duck hepatitis B virus_ and _heron hepatitis B virus_ ( _1_ ). Hepadnaviruses mainly infect the liver cells of their hosts and, in humans, cause hepatitis B, cirrhosis, and hepatocellular carcinoma ( _2_ ). Approximately 2 billion persons worldwide are infected with hepatitis B virus (HBV), and 600,000 persons die every year from the consequences of hepatitis B ( _3_ ).\n\n【1】Bats are associated with an increasing number of emerging and reemerging viruses, many of which pose major threats to public health ( _4_ ). We conducted a viral metagenomic analysis of 6 species of bats from Myanmar. The analysis revealed a large number of viral contigs annotated to orthohepadnavirus with <70% nt identity (B. He, unpub. data), suggesting the presence of orthohepadnaviruses in these animals. We describe the virus by full genomic analysis and morphologic observation.\n\n【2】### The Study\n\n【3】We purchased 853 freshly killed insectivorous bats in Sedon and Wutao Counties in southeastern Kachin State, Myanmar; the counties are adjacent to Yunnan Province, People’s Republic of China. The bats covered 6 species: _Miniopterus fuliginosus_ (n = 640), _Hipposideros armiger_ (n = 8), _Rhinolophus ferrumequinum_ (n = 176), _Myotis chinensis_ (n = 11), _Megaderma lyra_ (n = 6), and _Hipposideros fulvus_ (n = 12). All bat tissue samples were subjected to viral metagenomic analysis (unpublished data). The sampling of bats for this study was approved by the Administrative Committee on Animal Welfare of the Institute of Military Veterinary, Academy of Military Medical Sciences, China.\n\n【4】We used PCR to further study the prevalence of orthohepadnavirus in the 6 bat species; the condition of the samples made serologic assay and pathology impracticable. Viral DNA was extracted from liver tissue of each of the 853 bats by using the QIAamp DNA Mini Kit (QIAGEN, Hilden, Germany). To detect virus in the samples, we conducted PCR by using the TaKaRa PCR Kit (TaKaRa, Dalian, China) with a pair of degenerate pan-orthohepadnavirus primers (sequences available upon request). The PCR reaction was as follows: 45 cycles of denaturation at 94°C for 30 s, annealing at 54°C for 30 s, extension at 72°C for 40 s, and a final extension at 72°C for 7 min. Positive results were obtained for 22 long-fingered bats ( _Miniopterus fuliginosus_ ). Of these bats, 2.19% (7/320) were from Sedon County and 4.69% (15/320) from Wutao County; the viruses they harbored shared >98% nt identity. No other species had positive amplification results, indicating that _M. fuliginosus_ was the most likely species to harbor orthohepadnaviruses.\n\n【5】Figure 1\n\n【6】Figure 1 . . Predicted schematic representation of the bat hepatitis virus (BtHV) genome and its phylogenetic relationship with other hepadnaviruses. A) Genomic structural map of BtHV. Boxes and arrows represent the open reading...\n\n【7】Of the 22 positive samples, 3 were randomly selected for full genome amplification: M086 from Sedon County and 776 and M005 from Wutao County. PCR was conducted by using the PCR protocol defined above with high-fidelity _Pfu_ DNA polymerase (Promega, Madison, WI, USA) and 4 pairs of specific primers (sequences available upon request). Four overlapping amplicons were obtained, sequenced in both directions, and assembled into the full genomic sequence by using SeqMan, version 7.1.0 (DNASTAR, Madison, WI, USA). All 3 full genomes (GenBank accession nos. JX941466– JX941468) were 3,230 nt in length, which is close to the size of primate hepatitis viruses (≈3,200 nt) but smaller than rodent hepatitis viruses (≈3,300 nt). We analyzed the genome structure by using Vector NTI Advance 10 (Invitrogen, Carlsbad, CA). The results showed that the bat hepatitis viruses (BtHVs) contained the same circular and compact genomic structure as other orthohepadnaviruses, comprising 4 open reading frames encoding the multifunctional Pol, preS1/preS2/S, preC/C, and X proteins in the same direction ( Figure 1 , panel A).\n\n【8】Genomic sequence comparison and phylogenetic analysis based on amino acids of the _pol_ gene (2,562 bp) were constructed with ClustalW version 2.0 ( www.clustal.org/ ) and MEGA5 ( _5_ ). Phylogenetic tree analysis showed that previously described orthohepadnaviruses formed 2 clusters, primate hepatitis viruses and rodent hepatitis viruses, whereas the 3 newly identified BtHVs formed an independent cluster within the _Orthohepadnavirus_ genus ( Figure 1 , panel B). Sequence comparison showed that the full genomes of the BtHVs were 63.1%–65.3% and 33.9%–34.8% identical to members of the _Orthohepadnavirus_ and _Avihepadnavirus_ genera, respectively. Similar low identities were also observed separately in the 4 genes of the BtHVs ( Table ). These results support the classification of the BtHVs within the _Orthohepadnavirus_ genus, being distantly related to current species and likely to form a new species designated as BtHV.\n\n【9】Figure 2\n\n【10】Figure 2 . . Electron microscopy of negative-stained orthohepadnavirus particles from a bat (arrow). Clumps of Australia antigen–like particles are seen.\n\n【11】Hepadnaviruses have not been grown in any available in vitro cell system; thus, we did not attempt to isolate BtHV in cell culture. To detect the presence of virus particles, we used pooled liver tissues from the 3 bats that were randomly selected for full genome amplification. We homogenized the pooled tissues in SM buffer (50 mM Tris, 10 mM MgSO <sub>4 </sub> , 0.1M NaCl; pH7.5), followed by clarification by low-speed centrifugation to remove cell debris. We then passed the pooled sample through a 0.22-μm syringe filter (Millipore, Carrigtwohill, Ireland). Polyethylene glycol 6000 was added, and the resulting precipitate was sedimented at 12,000 × _g_ in a desktop centrifuge (Eppendorf, Hamburg, Germany) for 40 min at 4°C. The pellet was resuspended and examined after negative staining in a JEM-1200 EXII transmission electron microscope (JEOL, Tokyo, Japan). Numerous spherical particles of ≈20 nm diameter were observed ( Figure 2 ). The particles were morphologically similar to the Australia antigens of HBV, the most abundant viral component found in HBV-infected humans and animals and also known as surface protein or S antigen ( _6_ , _7_ ). PCR amplification of DNA extracted from the virus pellet revealed the full genome of the BtHV, with the expected size of ≈3200 bp (image not shown).\n\n【12】### Conclusions\n\n【13】Our observations provide strong evidence for the circulation of orthohepadnaviruses in at least 1 species of bats, _M. fuliginosus_ , in Myanmar. These bats have a wide distribution ( _8_ ), and increasing numbers of viruses, including coronaviruses and betaherpesviruses, are being isolated from them ( _9_ , _10_ ). Of the 6 bat species we sampled, only _M. fuliginosus_ was positive for BtHV. The prevalence of BtHV-positive bats in the 2 counties from which we obtained bats, was 2.2% and 4.7%, respectively, indicating that this species is likely a natural reservoir host of BtHV. The lack of detection of BtHV in bats from the other 5 species may be due to the limited numbers of bats sampled (although no evidence of hepadnavirus was found in any of the 176 _R. ferrumequinum_ bats) or to a narrow host range of the virus. Further study is required to determine the tropism and prevalence of BtHVs in other bat species.\n\n【14】Mr He is a doctoral candidate at Institute of Military Veterinary, Academy of Military Medical Sciences. He is majoring in animal virology, with research interests focusing on the discovery of bat emerging viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eab072a2-2418-4215-9bc1-6b7d11363e6e", "title": "Geometric Abstract Art and Public Health Data", "text": "【0】Geometric Abstract Art and Public Health Data\nPiet Mondrian (1872–1944), Composition in White, Red, and Yellow (1936). Oil on canvas, 31 1/2 in × 24 1/2 in /80.01 cm × 62.23 cm. Public domain digital image courtesy of Los Angeles County Museum of Art, 5905 Wilshire Boulevard, Los Angeles, CA 90036.\n\n【1】In his “Composition in White, Red, and Yellow,” Dutch artist Piet Mondrian illustrated geometric abstraction. He accomplished this effect via two primary-colored units embedded in a grid of vertical and horizontal black lines that draw viewers to a flat-structured polished web. Mondrian distilled direction to perpendicular columns and rows, color to bright red and yellow and neutral white and black, shape to squares and rectangles, and form to intersecting lines and outlined units. He methodically pared down form to indispensable lines and stacked rich colors in a few blocks. Influenced by analytical cubism, which is renowned for breaking objects and images into components, Mondrian pioneered reductive form and color, through an abbreviated pictorial vernacular. In creating this seemingly simplified painting he championed a distinctive relationship between color and blocks, rendering asymmetric balance and harmony.\n\n【2】Black, bordering pigmented zones, creates a maze reflecting mathematical accuracy and distinct complexity. Moving a single line or changing one color might disrupt subtle accord and structural delineations. The painting portrays minimal, yet essential, information through blatant intersections, geometric shapes, and interlocking planes. While thick black lines separate blocks, the arrangement and colors radiate energy that is both kinetic and serene, stimulating and restful, dynamic and static. Mondrian imparted a delicate balance between unequal and equivalent counterparts by using indispensable and contrasting positive and negative energies of solid and void, horizontal and vertical.\n\n【3】Similar to the balance in Mondrian’s painting, corresponding energies of time and space, art and science, and medicine and public health create momentum and energy in our compartmentalized and integrated personal and professional experiences. As public health specialists, we analyze microscopic images as well as gigantic amounts of clinical, public health, and geospatial data to develop interventions that enhance individual and population health. Through intricate scientific and geostatistical methods, scientists and healthcare providers assess relationships between risk factors and disease and between behaviors and well-being. We frame prevention and control of health conditions—infectious or chronic, environmental or hazardous—in units and blocks of person, place, and time.\n\n【4】By pioneering geoanalysis of data and using a dot map, Dr. John Snow traced the 1854 cholera outbreak in Soho, London, England, to a public water pump on Broad Street (now Broadwick Street). Disabling a well pump by removing its handle, a reductive and a seemingly simple public health intervention, credited Snow with ending an outbreak, providing advocacy for public health changes in London water and waste systems, and contributing to modern epidemiology. Then and now, visual representations of public health data, including graphs and maps, help in understanding disease causation by showing locations of cases and highlighting similarities or differences between blocks, regions, and countries with and without infection. The web of disease causation, a core public health concept, pinpoints theories on population patterns of health and disease and embodies multiple and complex factors influencing disease dynamics and differentials between and within populations.\n\n【5】Well into the twenty-second century, public health professionals continue to track infection, direct prevention, advance treatment, and predict trends by culling information on disease transmission. By scrutinizing mazes and grids of public health data and leveraging digital technologies, we provide insights into causes and manifestations of disease, reduce data to strategies crucial for prevention and treatment, and strive to translate science into public health. Through distilling complex epidemiologic, molecular, and geospatial, data we address also disease syndemics–two or more conditions that interact synergistically and increase morbidity and mortality. Through the media, the public receives crucial information and graphics about affected populations and locations and about health conditions that are physiologically, socially, or clinically linked.\n\n【6】Geospatial data, virtual grid meta-databases, grid computing concepts, spatial analytical methods, visualization or data-display techniques, and color-coded geographic visualizations: these all enhance our understanding of public health threats and facilitate control of outbreaks, endemic diseases, epidemics, and pandemics. Such methods inform research and programs on the effectiveness of vaccination programs, whole-genome sequencing analysis, and cluster detection of infections and diseases. Similar to how artists craft and reveal their expertise in color, form, and perspective—in cubist and abstract art—public health professionals leverage their expertise in epidemiology, statistics, and communication and in behavioral, social, and laboratory sciences to integrate clinical care and public health. Mondrian’s network of black lines and colored blocks focuses on essence, equilibrates white-painted blocks with primary colored blocks, enunciates focal and peripheral information, and juxtaposes structural and dynamic equilibrium. We perceive similar reductive and essential processes and pictorials in public health. Art and science balance our health and enhance our lives.\n\n【7】Dr. Semaan is a behavioral scientist in the Division of HIV/AIDS Prevention, National Center for HIV/AIDS, Viral Hepatitis, STD, and TB Prevention, Centers for Disease Control and Prevention, Atlanta, GA. She has served as volunteer docent at the Philadelphia Museum of Art and at the Atlanta High Museum. As a volunteer, she also serves on the board of directors of the Oglethorpe University Museum of Art, Atlanta, GA.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ea64e70e-c94a-4c4a-a65d-0d60193e6c95", "title": "HIV Infection among Illegal Migrants, Italy, 2004–2007", "text": "【0】HIV Infection among Illegal Migrants, Italy, 2004–2007\nDuring the past 2 decades, Italy has had an uncontrolled increase in number of migrants. As many as 4 million (720,000 undocumented) foreign-born persons (≈7% of the population in Italy) are living in Italy (62.5% in the northern region, 25% in the central region, and 12.5% in the southern region) ( _1_ ).\n\n【1】Sexually transmitted infections (STIs) are present in migrants, especially early after migration. Migrants are usually men (single or married) who live alone. Migrant women are often forced into prostitution. These factors may expose them to risky sexual contacts and STIs. Preventive educational campaigns rarely reach migrant communities because of logistic, cultural, and language barriers.\n\n【2】As of December 2008, a total of 60,346 AIDS cases were reported in Italy, where the opt-in strategy (persons need to accept testing) of HIV testing is applied ( _2_ ). The proportion of migrants among persons with cases of AIDS in Italy has progressively increased from 2.5% before 1993 to 20.7% in 2007–2008 ( _2_ ). This finding reflects the increasing number of migrants and their delayed access to screening and medical care ( _3_ ). The proportion of migrants among persons with new cases of HIV infection has also increased from 11% in 1992 to 32% in 2007 in Italy ( _2_ ), a finding that confirms previous data ( _4_ ). Estimated HIV incidence rates among migrants are 64.0 cases (for men) and 52.5 cases (for women)/100,000 persons in 2007. These rates are 11× higher than the HIV incidence rate for native-born Italians ( _5_ ). No reliable information is available on the prevalence of HIV infection in the illegal migrant population in Italy. Data are available only for selected risk groups such as female commercial sex workers ( _6_ ), transsexuals ( _7_ ), and patients with STIs ( _8_ ). Furthermore, no reliable data are available for likely place of infection for persons recently screened and found to be HIV infected. To address these issues, we determined the prevalence and likely place of infection with HIV for an illegal migrant population in Italy.\n\n【3】### The Study\n\n【4】The study protocol was reviewed and approved by the ethical boards of all participating centers. The study was conducted during January 2004–December 2007 in clinical centers that offered primary healthcare to illegal migrants in northern (Brescia), central (Rome), and southern (Palermo) Italy. All adult migrants from a non–European Union country who registered for a visit at each center were asked to participate in the study. Participants were divided into 4 groups according to their risk for HIV infection: 1) commercial sex workers, 2) persons reporting unsafe sex (occasional not-for-money homosexual or heterosexual contacts with persons other than their regular partner), 3) persons with other risk factors (intravenous drug use or blood transfusion in their country of origin, and 4) persons with no risk factors identified.\n\n【5】Participants were tested for antibodies against HIV types 1 and 2 and for p24 antigen by using a commercial microparticle enzyme immunoassay (AxSYM HIV Ag/Ab Combo; Abbott Laboratories, Abbott Park, IL, USA). HIV-positive serum samples were further tested for HIV antibody avidity by using the AxSYM HIV 1/2gO assay (Abbott Laboratories) as reported ( _9_ ), to ascertain likely time of infection. An avidity index < 0.80 indicated that infection was acquired recently (within the past 6 months); a higher index indicated that infection was acquired earlier (>6 months ago). A cutoff value of 0.80 was used and validated as having 93.0% sensitivity, 98.5% specificity ( _10_ ), and >90.0% reproducibility. Avidity results were cross-checked with reported time of migration to assess likely place of exposure.\n\n【6】A total of 4,078 persons were invited to participate in the study. Of 3,976 (97.5%) who agreed to participate, 3,003 (73.6%) underwent HIV testing (2,815 in Brescia, 48 in Rome, and 140 in Palermo). In terms of HIV risk for 2,853 respondents, 191 were commercial sex workers, 1,246 practiced unsafe sex, 47 reported other risks, and 1,494 reported no risk factors (total = 2,978 because multiple risk factors were reported by some persons). Demographic characteristics for the participants are shown in Table 1 .\n\n【7】HIV-1 infection was detected for 29 (0.97%) of 3,003 participants (95% confidence interval \\[CI\\] 0.90%–1.2%); no participants were infected with HIV-2. Avidity index results were obtained for 27 of the 29 HIV-positive participants. Univariate analysis showed that sociodemographic and behavior factors associated with HIV infection were Christian religion (p = 0.029, odds ratio \\[OR\\] 3.07, 95% CI 1.06–8.83), migration from sub-Saharan Africa (p = 0.001, OR 11.94, 95% CI 1.61–88.81), commercial sex (p = 0.0001, OR 18.2, 95% CI 6.25–52.97), and unsafe sex (p = 0.016, OR 3.43, 95% CI 1.22–9.66). Multiple logistic regression showed that factors independently associated with increased risk for HIV infection were migration from sub-Saharan Africa (p = 0.0001, OR 3.7, 95% CI 2.2–9.4), commercial sex (p = 0.025, OR 18.4, 95% CI 4.9–48.5), and unsafe sex (p = 0.01, OR 2.3, 95% CI 1.7–8.6).\n\n【8】Figure\n\n【9】Figure . Antibody avidity indices for 27 HIV-infected migrants, Italy, 2004–2007. Horizontal line indicates the cutoff value. ID, identification.\n\n【10】Place of infection could not be determined for 17 (63.0%) of 27 persons; 6 (22.2%) of 27 were presumably recently infected in Italy, and 4 (14.8%) of 27 presumably acquired their infection in their country of origin before emigration. Results of avidity index determinations are shown in the Figure and Table 2 . Sociodemographic characteristics of our population did not differ from those reported nationwide ( _1_ ).\n\n【11】### Conclusions\n\n【12】Our data confirm that many illegal migrants practice unsafe sex (low rate of condom use). These findings are worrisome if one considers poor knowledge of HIV transmission reported in our study population ( _11_ ). Consequently, migrants are particularly vulnerable to STIs, as shown by the high prevalence rate (0.97%) for the adult migrant population, which is higher than the estimated 0.4% prevalence rate for the national population in Italy ( _12_ ). The higher HIV prevalence rate for persons from an area (sub-Saharan Africa) in which HIV is highly endemic might reflect exposure in the country of origin or new infections in the host country. A total of 6 (22.2%) of the 27 HIV infections for which avidity index data were available were probably acquired in Italy by migrants from sub-Saharan Africa (n = 3), eastern Europe (n = 2), and Latin America (n = 1). Conversely, all 4 (14.8%) persons who acquired infection before migration to Italy were originally from sub-Saharan Africa.\n\n【13】Our study had 2 limitations. First, recruitment was not evenly balanced between centers. However, migrants are unevenly distributed in Italy, with the largest communities in northern Italy. Second, the study acceptance rate was only 73.6%. However, this acceptability rate is similar to rates in other studies in Europe ( _13_ _,_ _14_ ). Furthermore, sociodemographic characteristics of our sample were homogeneous with those of other studies from the same centers and nationwide ( _1_ , _15_ ), and we did not anticipate any differences between participants and those who did not participate.\n\n【14】Our data show high HIV prevalence for an illegal migrant population in Italy. For persons originally from sub-Saharan Africa and, as for the native population in Italy, practicing commercial or unsafe sex were independently associated with HIV infection. A large proportion of persons had presumably acquired infections after migration. These data indicate the prominent role of social determinants of HIV infection, including marital status and living conditions. Health education and free access to HIV testing and care for the illegal migrant population in western countries is needed, particularly for persons from sub-Saharan Africa.\n\n【15】Dr Pezzoli is consulting physician at the Center for International Health, Local Health Authority, Brescia, Italy. Her primary research interests are migrant health, treatment of patients infected with HIV, and tuberculosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e59fdbb3-c888-4e4a-a9fb-e71bb6ab8865", "title": "Neighborhood Food Outlets, Diet, and Obesity Among California Adults, 2007 and 2009", "text": "【0】Volume 10 — March 14, 2013\n\n【1】### Article Tools\n\n【2】*   PDF - 261 KB\n*   Email\n*   Print\n*   Download citation\n*   Este resumen en español\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Neighborhood Food Outlets, Diet, and Obesity Among California Adults, 2007 and 2009\n===================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Aiko Hattori, PhD; Ruopeng An, MPP, MPhil; Roland Sturm, PhD\n\n【8】_Suggested citation for this article:_ Hattori A, An R, Sturm R. Neighborhood Food Outlets, Diet, and Obesity Among California Adults, 2007 and 2009. Prev Chronic Dis 2013;10:120123. DOI: http://dx.doi.org/10.5888/pcd10.120123 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Introduction**  \nVarying neighborhood definitions may affect research on the association between food environments and diet and weight status. The objective of this study was to examine the association between number and type of neighborhood food outlets and dietary intake and body mass index (BMI) measures among California adults according to the geographic size of a neighborhood or food environment.\n\n【12】**Methods**  \nWe analyzed data from 97,678 respondents aged 18 years or older from the 2007 and 2009 California Health Interview Survey through multivariable regression models. Outcome variables were BMI, weight status of a BMI of 25.0 or more and a BMI of 30.0 or more, and the number of times per week the following were consumed: fruits, vegetables, sugar-sweetened soft drinks, fried potatoes, and fast food. Explanatory variables were the number of fast-food restaurants, full-service restaurants, convenience stores, small food stores, grocery stores, and large supermarkets within varying distances (0.25 to 3.0 miles) from the survey respondent’s residence. We adopted as a measure of walking distance a Euclidean distance within 1 mile. Control variables included sociodemographic and economic characteristics of respondents and neighborhoods.\n\n【13】**Results**  \nFood outlets within walking distance (≤1.0 mile) were not strongly associated with dietary intake, BMI, or probabilities of a BMI of 25.0 or more or a BMI of 30.0 or more. We found significant associations between fast-food outlets and dietary intake and between supermarkets and BMI and probabilities of a BMI of 25.0 or more and a BMI of 30.0 or more for food environments beyond walking distance (>1.0 mile).\n\n【14】**Conclusion**  \nWe found no strong evidence that food outlets near homes are associated with dietary intake or BMI. We replicated some associations reported previously but only for areas that are larger than what typically is considered a neighborhood. A likely reason for the null finding is that shopping patterns are weakly related, if at all, to neighborhoods in the United States because of access to motorized transportation.\n\n【15】Top of Page\n\n【16】Introduction\n------------\n\n【17】The relationship between neighborhood food environments and obesity occupies a central role in policy debates (1–4). A recurring theme is the notion of “food deserts,” where access to healthful and affordable food is limited (5,6). Food deserts are often identified by the absence of supermarkets or full-range grocery stores and by the presence of fast-food restaurants and convenience stores (6). Hypotheses that link the food environment with obesity claim that proximity to fast-food outlets, convenience stores, or small grocery stores undermines diet quality, whereas proximity to supermarkets or full-range grocery stores enhances it by providing healthful products, mainly fruits and vegetables (7). However, evidence is more tentative than often presented in the news media and in policy arguments (7,8). Associations between food environment measures and obesity are not reliably replicated, and dietary behaviors are not consistent with these associations (9). Varying definitions of “neighborhood” could also contribute to the inconsistency of results and lack of comparability of data. Few studies have examined the association between varying neighborhood sizes and dietary intake and weight status (10–12). The objective of this study was to examine the associations of number and type of neighborhood food environments with dietary intake and body mass index (BMI) measures by using definitions of “neighborhood” based on various geographic sizes. We analyzed data from the California Health Interview Survey.\n\n【18】Top of Page\n\n【19】Methods\n-------\n\n【20】We used data from the 2007 and 2009 waves of the California Health Interview Survey (CHIS), a random-digit–dial telephone survey of California’s noninstitutionalized population (13). In 2007 and 2009, the CHIS included interviews of 98,662 adults aged 18 years or older. The sampling weights provided by CHIS account for unequal sampling probabilities and nonresponse to allow generalization to the study population. We limited our analysis to 97,678 adults aged 18 years or older, excluding 533 (0.5%) pregnant women and 451 (0.5%) respondents whose information was provided through a proxy interview.\n\n【21】Self-reported measures on dietary intake included the number of times per week the following were consumed during the month before the interview: fruits (excluding juices), vegetables (excluding fried potatoes), sugar-sweetened soft drinks (excluding diet soft drinks), and fried potatoes (including French fries, home fries, and hash browns). The measures also included the number of times fast food had been consumed in the week before the interview. The information was collected through questions in the following format: “During the past month \\[‘or in the past 7 days’ for fast-food consumption\\], how often did you eat \\[food item name\\]?” The question was often followed by clarification of the food items. The response was standardized to reflect mean per-week intake frequency.\n\n【22】Anthropometric measures included BMI, calculated as self-reported weight in kilograms divided by self-reported height in meters squared. We defined “overweight or obese” as a BMI of 25.0 or more and “obese” as a BMI of 30.0 or more, according to World Health Organization classifications for adults (14).\n\n【23】Whereas most large survey studies use a predefined administrative unit such as census tract or zip code to define neighborhood, we defined neighborhood on the basis of geographic distance from a respondent’s residence, and we defined neighborhood food environment by counting the number of different types of food outlets within those distances. We drew circular buffers of varying radii (0.25, 0.5, 1.0, 1.5, and 3.0 miles) centered on each respondent’s residential address. Because 1 mile is often used as a threshold for walkable distance (6), we considered distances of 0.25, 0.5, and 1.0 miles to be within walking distance. We measured Euclidean distance (straight line distance between 2 points) using ArcMap 9.1 (ESRI, Redlands, California).\n\n【24】We used food outlet data from the 2008 release of InfoUSA (15). We overlaid food outlet locations on the buffers around respondents’ residences and counted the number of different types of food outlets in each buffer. We classified fast-food restaurants, full-service restaurants, convenience stores, small food stores, mid-size grocery stores, and large supermarkets by using the North American Industry Classification System (NAICS) (16). NAICS does not have a code for fast-food restaurants; we identified 63 major fast-food franchises that have main menus that include items such as hot dogs, hamburgers, pizza, fried chicken, submarine sandwiches, or tacos by NAICS codes 72221105–6. Full-service restaurants were identified by NAICS codes 72211001–20; convenience stores, code 44512001; and small food stores (annual sales <$1 million), mid-size grocery stores (annual sales of $1–$5 million), and large supermarkets (annual sales >$5 million), codes 44511001–3, respectively.\n\n【25】To examine the association between neighborhood food environment and dietary intake, we performed negative binomial regression analysis. The dietary intake measures were the dependent variables, and the numbers of different types of food outlets in the buffers were the explanatory variables. Negative binomial regression is a generalization of the Poisson model in which the Poisson parameter has a random component (17). We performed separate regressions for each dietary intake measure and buffer size, calculated average marginal effects (AMEs), which measure an estimated change in the outcome in the observed unit associated with 1 unit change in the regressor of interest, and applied the Bonferroni adjustment for multiple comparisons. We controlled for potentially confounding individual and neighborhood factors. Individual-level control variables included sex, age (in years and age squared), race/ethnicity (white, African American, Hispanic, Asian or Pacific Islander, Native American, other race/multirace), household size, annual household income (in natural logarithm), education (not a high school graduate, high school graduate, high school graduate but not college graduate, college graduate, and more than college degree), marital status (married, divorced/separated/widowed, single), parental status (has a child or has no child), physical activity (regular activity, some activity, sedentary), and survey year. Although we were interested in vehicle ownership, this information was not collected in CHIS 2009 and therefore not included in our analysis. Proxies for neighborhood-level control variables, which were obtained from 2000 Census data (18), included population density, median household income, and proportion of non-Hispanic white residents of a respondent’s residential census tract. These factors did not match our definition of neighborhood for the explanatory variables, but they could be derived only from predefined administrative units.\n\n【26】To examine the association between the neighborhood food environment and BMI measures, we regressed BMI (through ordinary least squares \\[OLS\\]) and its dichotomous cutoffs of BMI of 25.0 or more and BMI of 30.0 or more (through logistic regression) on the same set of explanatory and control variables. Again, we performed separate regressions for each buffer size, calculated AMEs for logistic regression models, and applied the Bonferroni adjustment for multiple comparisons. We then compared the significant associations of food environments with dietary intakes and BMI measures to examine the hypothesis that the neighborhood food environment influences BMI through its influence on dietary intakes.\n\n【27】We performed several sensitivity analyses: stratified analysis by urbanicity (urban vs nonurban residents) and income level (federal poverty level \\[FPL\\] ≤130% vs FPL >130%) and analysis by density (instead of raw counts) of food outlets by census tract (the number of food outlets in census tract per 1,000 population and the number of food outlets in census tract per square mile). We defined “low income” as FPL of 130% or less. All analyses were performed in Stata 12.1 (StataCorp LP, College Station, Texas). We weighted the regression using sampling weights and estimated _P_ values based on heteroscedasticity-robust standard errors obtained by using the Eicker–Huber–White sandwich estimator.\n\n【28】Top of Page\n\n【29】Results\n-------\n\n【30】Of the dietary items surveyed, mean intake was greater for fruits (7.7 times per week) and vegetables (6.9 times per week) than for sugar-sweetened soft drinks (2.2 times per week), fast food (1.5 times per week), or fried potatoes (0.9 times per week) ( Table 1 ). Average BMI was 26.8; 57% of respondents were overweight or obese, and 22.7% were obese.\n\n【31】The number of food outlets per food outlet type in each buffer varied in the expected way: we found more small food stores and fast-food restaurants on average than other food outlet types ( Table 2 ); large supermarkets and mid-size grocery stores were less common. We found an average of approximately 7 fast-food restaurants, 3 full-service restaurants, 2 convenience stores, 7 small food stores, 1 mid-size grocery store, and 2 large supermarkets in the 1-mile buffer. Low-income respondents were surrounded by more food outlets of each type, including mid-size grocery stores and large supermarkets, than were their wealthier counterparts.\n\n【32】Among all 5 buffer sizes, most (79% \\[119/150\\]) of the effects of the number of food outlets on intake of food items were not significant. Eight effects were significant after applying Bonferroni’s adjustment; 7 of these were clustered around larger buffer sizes (1.5 miles and 3.0 miles) and fast-food restaurants and large supermarkets ( Table 3 ). The number of fast-food restaurants in the 3.0-mile buffer was positively associated with an increase in intake frequency of sugar-sweetened soft drinks, fast food, and fried potatoes, and less frequent consumption of fruits and vegetables. By contrast, the number of large supermarkets within 1.5- and 3.0-mile buffers was associated with less frequent consumption of sugar-sweetened soft drinks.\n\n【33】Among all 5 buffer sizes, most (83% \\[75/90\\]) of the effects of the number of food outlet types on BMI, and weight status of a BMI of 25.0 or more or a BMI or 30.0 or more were not significant. Seven effects were significant after applying Bonferroni’s adjustment; these were significant in the larger buffer sizes and large supermarkets and fast-food restaurants. The number of fast-food restaurants in the 3.0-mile buffer was positively associated with the probability of a BMI of 25.0 or more but not with BMI or the probability of a BMI of 30.0 or more ( Table 4 ). The number of large supermarkets in the 3.0-mile buffer was associated with lower BMI and lower probabilities of a BMI of 25.0 or more and a BMI of 30.0 or more. Some of these associations were also found in the 1.5- and 1.0-mile buffers.\n\n【34】More fast-food restaurants in the 3.0-mile buffer predicted increased frequency of consuming fried potatoes, sugar-sweetened soft drinks, and fast food, decreased frequency of consuming vegetables, and a lower probability of a BMI of 25.0 or more. By contrast, the number of supermarkets was largely not associated with dietary intake, whereas more supermarkets within 1.0-, 1.5-, and 3.0-mile buffers predicted lower BMI.\n\n【35】When we stratified the analysis by urbanicity and FPL, we did not find the significant associations between the number of food outlet types and dietary intake or BMI measures among nonurban respondents and low-income respondents that we observed in unstratified analysis, but we did find them among their urban and wealthier counterparts.\n\n【36】When we used census tract data, only the association between the number of mid-size grocery stores in census tract per 1,000 population and fast-food consumption was significant after adjustment for multiple comparisons; we found no significant associations for the other food outlet types or BMI measures.\n\n【37】Top of Page\n\n【38】Discussion\n----------\n\n【39】Our study did not find strong evidence to support the hypothesis that the food environment within walkable distance is associated with the dietary intake or BMI measures of residents. We found some evidence that the food environment for larger, nonwalkable areas (1.5- and 3.0-mile buffers) is associated with dietary intake and BMI measures. Moreover, significant results for BMI measures do not match dietary intakes, except for the number of fast-food restaurants within a 3.0-mile buffer, which was simultaneously associated with increased frequency of consuming fried potatoes, sugar-sweetened soft drinks, and fast food, decreased frequency of consuming vegetables, and greater probability of being overweight or obese (a BMI of 25.0 or more). We found an association between BMI measures and supermarkets only for the 1.5- and 3.0-mile buffers, but those associations did not parallel dietary intake. In particular, we found no association between supermarkets and vegetable and fruit consumption at any buffer size. When we compared the density of food outlets on the basis of residential census tract instead of raw counts, we observed only 1 significant association (between the number of mid-size grocery stores in census tract per 1,000 population and fast-food consumption) and there were no significant associations between other types of food outlets and either BMI measures or dietary intake. Therefore, our data do not confirm previous data, especially data on the association between fast-food restaurants or supermarkets and dietary intake or BMI. The primary strength of this study is that we explored a range of distances from respondents’ residential address, from areas smaller than census tracts to areas much larger, instead of using predefined units, such as census tracts or zip codes.\n\n【40】This study has several limitations. First, we measured food outlet type, not the availability of certain foods. We obtained food outlet data from InfoUSA; business listings are rarely up-to-date or without error. One study reported only fair agreement between commercial data and field observations for supermarkets, grocery stores, convenience stores, and full-service restaurants and poor agreement for fast-food restaurants (19). The dietary intake questions we used did not provide guidance on serving size, and they asked about a small number of food items; hence, they did not allow examination of overall diet quality. The questions also had a long recall period of 1 month. BMI was calculated from self-reported height and weight; self-report tends to underestimate BMI (20–22). The study used data from California, and the results may not apply to other geographic regions or populations. Finally, this study used cross-sectional data and was unable to establish causation.\n\n【41】The limitations of the data used in this study may have contributed to our null findings, but there are other, substantive reasons for finding a null relationship between of the number of food outlet types and dietary intake or BMI. A likely reason for our null findings is that food-shopping patterns are weakly related, if at all, to neighborhoods in the United States because of access to motorized transportation. Access to motorized transportation increases mobility and may limit the relevance of immediate food environments by allowing residents to travel beyond a walking distance. California has one of the highest rates of vehicle ownership (23); most of its population resides in metropolitan areas (24). Although the local food environment may be important for rural households (where residents are sparsely distributed) or households that lack access to motorized transportation, the sample size in our study may have been too small to detect associations affected by rural residence or car ownership. The most recent CARDIA study reports associations between fast-food restaurants in immediate neighborhoods and fast-food consumption among low-income men but not for higher-income groups or neighborhoods outside walking distance, suggesting greater reliance on immediate food environments among low-income people who are less likely to own a car (11).\n\n【42】Food environment research has often been based on predefined administrative units, typically census tracts, which correspond to our smaller buffer sizes. In California, the median census tract corresponds to a buffer with a 0.5-mile radius; nationwide, the median census tract corresponds to a buffer with a 0.79-mile radius, both within the walkable distance of 1 mile (6). However, the 1-mile definition may no longer be relevant in the motorized society of the United States and California. Further research on varying sizes of neighborhoods is warranted to understand people’s food-shopping travel patterns and how the patterns expose people to different food environments.\n\n【43】In the literature of food deserts, it is often argued that lower-income communities have reduced access to foods. However, these results are often based on density measures (eg, stores per capita in census tract) (25,26). When we compared the density of food outlets among low- and higher-income respondents by using data on density based on census tracts (the number of food outlets in census tract per 1,000 population), the densities of supermarkets and full-service restaurants were lower among low-income people. However, low-income areas are also much more densely populated, so the distance between a residence and a food outlet may be a more relevant criterion than density. In our definitions of neighborhoods, which used circular buffers centered on individuals’ homes, neighborhood sizes are identical for all individuals and the dependent variable (the number of food outlet types per buffer) has the same physical meaning. Using this uniformly defined measure, we found that low-income respondents had greater access to each type of food outlet. Our findings are consistent with a report by the US Department of Agriculture, which found a smaller median distance to the nearest supermarket among low-income individuals (compared with higher-income individuals) in urban areas (6). Although predefined administrative units (eg, census tracts) may be more feasible for monitoring purposes, measures reflecting the proximity of food outlets may more reliably identify disadvantaged communities.\n\n【44】The concept of neighborhood food environments has been the focus of the news media and policy makers, yet the evidence is not clear on whether promoting or discouraging a particular type of food outlet is an effective approach to promoting healthful dietary behaviors and weight status. Initial findings in a new area of research — such as food environments — may be qualified over time, and both exact replication and conceptual replication of previous findings using alternative data sources and methods is a central theme for advancing scientific knowledge and informing policies. No single study completely addresses a research question, and this study can only contribute another data point. However, at least in California, the relationship between neighborhood food outlets and dietary intake or BMI is subtler than the relationship presented in the news media. The relationship appears to exist in larger geographic areas rather than within walking distances or in a typical urban census tract, larger areas that are arguably outside what we consider to be a neighborhood.\n\n【45】Top of Page\n\n【46】Acknowledgments\n---------------\n\n【47】This project was supported by the following: grant no. T32 HS000046 from the Agency for Healthcare Research and Quality (A.H.), Jim Lovelace Foundation 2010–2011 Dissertation Award (R.A.), and grant no. R01HD057193 from the National Institute on Child Health and Human Development (R.S.). The content is solely the responsibility of the authors and does not necessarily represent the official views of the Agency for Healthcare Research and Quality or the National Institute on Child Health and Human Development.\n\n【48】Top of Page\n\n【49】Author Information\n------------------\n\n【50】Corresponding Author: Aiko Hattori, PhD, Carolina Population Center, University of North Carolina at Chapel Hill, CB no. 8120, University Square, 123 W Franklin St, Chapel Hill, NC 27516-2524. Telephone: 919-966-3873. E-mail: hattori@email.unc.edu . At the time of the study, Dr Hattori was at the University of California, Los Angeles.\n\n【51】Author Affiliations: Ruopeng An, Pardee RAND Graduate School, Santa Monica, California; Roland Sturm, RAND Corporation, Santa Monica, California.\n\n【52】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d3675a2a-2194-42e2-bec0-7265c626931b", "title": "Pandemic (H1N1) 2009 Virus in Swine Herds, People’s Republic of China", "text": "【0】Pandemic (H1N1) 2009 Virus in Swine Herds, People’s Republic of China\n**To the Editor:** During March and early April 2009, a new swine-origin influenza A (H1N1) virus emerged in Mexico and the United States; this virus subsequently spread across the globe by human-to-human transmission at an unprecedented rate. Pandemic (H1N1) 2009 virus also affected pigs. On May 2, 2009, the Canadian Food Inspection Agency notified the World Organisation for Animal Health that the novel influenza A virus had been confirmed on a pig farm in Alberta, Canada. Infection of pigs with pandemic (H1N1) 2009 virus has been observed in multiple countries ( _1_ ). In this study, we report transmission of pandemic (H1N1) 2009 virus from humans to pigs in the People’s Republic of China.\n\n【1】During August 2009–April 2010, swine influenza virus surveillance was conducted in the provinces of central and eastern China, including Henan, Hubei, Hunan, Jiangxi, and Anhui. A total of 1,021 samples, comprising tracheal mucus swabs and lungs, from pigs on 30 farms, were collected, and dozens of swine influenza viruses, H1N1, H1N2, H3N8, H9, and H10 subtypes, were isolated. Eight isolates were subtype H1N1, including 4 novel pandemic (H1N1) 2009 viruses: A/swine/Nanchang/3/2010 (H1N1) (GenBank accession nos. JF275917–24), A/swine/Nanchang/5/2010 (H1N1) (GenBank accession nos. JF275933–40) and A/swine/Nanchang/6/2010 (H1N1) (GenBank accession nos. JF275941–48), which were isolated from tracheal mucus, and A/swine/Nanchang/F9/2010 (H1N1) (GenBank accession nos. JF275925–32), isolated from the lung. Pigs from which the novel viruses were isolated showed mild respiratory signs, including depression, cough, and transient increase in body temperature. Compared with the sequence of A/California/04/2009 (H1N1), genomic sequencing of the 4 pandemic viruses showed 15 common point mutations, such as polymerase basic protein 2, T588I; polymerase acidic protein, A70V, P224S, D547E; hemagglutinin, P100S, D103E, S145P, T214A, S220T, I338V; nucleocapsid protein, V100I, H289Y; neuraminidase, V106I, N248D; and nonstructural protein, I123V.\n\n【2】Recent studies have shown that the novel pandemic (H1N1) 2009 human influenza viruses were almost avirulent for mice (50% mouse lethal dose \\> 10 <sup>6 </sup> PFU for A/CA/04/09 \\[ _2_ , _3_ \\]). In this study, mice were anesthetized with ketamine/xylazine, as described ( _4_ ) and 50 μL of phosphate-buffered saline containing the indicated doses (10 <sup>5 </sup> 50% egg infectious dose) of the 4 viruses were instilled into anesthetized mice through nostrils. Interestingly, the 4 pandemic (H1N1) 2009 viruses isolated from pigs could cause systemic infection on mice. All mice had extensive loss of body weight, and some of the infected mice died within 2 weeks postinfection (data not shown).\n\n【3】To detect whether the 4 isolated pandemic (H1N1) 2009 viruses could cause clinical diseases in pigs and what kinds of pathologic damage could be induced in infected pigs, pigs were intratracheally challenged with the 4 pandemic (H1N1) 2009 viruses at a dose of 10 <sup>7 </sup> 50% egg infectious dose and monitored for clinical signs and pathologic changes. Clinical signs in pigs were mild, and no deaths occurred. Except for slightly labored breathing, no infected pigs showed dominant signs, such as cough, nasal discharge, facial edema, and dyspnea. Body temperatures of infected pigs started to increase at 2 days postinfection (dpi), peaked ≈3 dpi at 41.5°C, and returned to the initial temperature by 5 dpi.\n\n【4】Figure\n\n【5】Figure . Hematoxylin and eosin–stained trachea and lung controls and samples from pigs infected with pandemic (H1N1) 2009 virus. A) Control trachea sample; B) mixed inflammatory cell infiltrate present throughout trachea sample from...\n\n【6】We observed necrosis of the tracheal wall and pneumonia foci in the 4 pandemic (H1N1) 2009 virus–infected pigs. Hematoxylin and eosin–stained sections of the trachea from the infected pigs showed mild necrotizing tracheitis. The necrotic epithelial cells were present in the lumen, and a mixed inflammatory cell infiltrate was present throughout ( Figure ). In the lung, we also observed alveolar septal edema and interstitial inflammatory cell infiltrates, as well as histologic changes, including alveolar epithelial hyperplasia, a mixed inflammatory infiltrate, and interstitium broadening ( Figure ). Additionally, immunohistochemical analysis for the distribution of viral antigens showed positive staining in bronchial epithelial cells and alveolar pneumocytes.\n\n【7】Previous studies showed that pandemic (H1N1) 2009 may have become established in swine populations in Canada, Norway, and Hong Kong ( _1_ _,_ _5_ _–_ _8_ ). The human-to-pig transmission of pandemic (H1N1) 2009 may substantially affect virus evolution and subsequent epidemiology. Although the pandemic was mild, the virus could develop further reassortment in swine and gain virulence. On the other hand, subtype H5N1 and H9N2 viruses have become established in pigs, so the introduction of pandemic (H1N1) 2009 virus to pigs has provided the possibility for the incorporation of avian virus genes into mammalian-adapted viruses. That transmission could occur from humans to pigs and vice versa is especially troublesome. Given the possible production of novel viruses of potential threat to public health, we should emphasize influenza surveillance in pigs and establishment of the genetic basis of the viral genome for rapidly identifying such reassortment events.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "76c89a7b-3cfd-4c93-9696-28e1e422411f", "title": "Natural Norovirus Infections in Rhesus Macaques", "text": "【0】Natural Norovirus Infections in Rhesus Macaques\nNoroviruses are a leading cause of acute gastroenteritis in humans and have also been described in several animal species. Although the pathologic role of most animal noroviruses is not clearly established, evidence of the close genetic relatedness of animal and human noroviruses and the detection of animal norovirus–specific antibodies in humans and of human norovirus specific–antibodies in different animal species strongly suggest zoonotic or interspecies transmission of noroviruses ( _1_ – _4_ ). However, zoonotic norovirus infections have not yet been reported, perhaps because zoonotic transmission of noroviruses is rare or the routine norovirus detection techniques are not designed to detect animal strains. On the other hand, experimental infection of several animal species has clearly shown that human norovirus strains are able to replicate in animals, including gnotobiotic pigs and calves and nonhuman primates ( _5_ _–_ _8_ ). In addition, molecular detection of GII.12 and GII.4 noroviruses, which are usually human strains, has been reported in swine, cattle, and pet dogs ( _9_ _,_ _10_ ).\n\n【1】### The Study\n\n【2】A previous study evaluated samples that were collected during 2008 from the rhesus macaque ( _Macaca mulatta_ ) colony housed at the Tulane National Primate Research Center (Covington, LA, USA) to determine the presence of caliciviruses ( _11_ ). Using a broadly reactive primer pair (P289/P290), this study detected diverse recovirus strains and 1 GII norovirus (FT244, GenBank accession no. HM035148) in 1 macaque ( _11_ ). Recently, a quantitative real-time reverse transcription PCR (RT-PCR) for the detection of GI, GII, and GIV noroviruses was developed ( _12_ ). In 2015, I used this highly specific and sensitive assay to retest the 500 rhesus macaque fecal samples from the previous study, including the sample that contained FT244, to determine the presence of noroviruses.\n\n【3】Figure 1\n\n【4】Figure 1 . Norovirus genome copies per gram of rhesus macaque fecal samples collected in 2008 ( _11_ ) and retested in 2015 by using a highly sensitive and specific real-time reverse transcription PCR....\n\n【5】Of the 500 samples, 41 (8.2%) showed a cycle threshold (C <sub>t </sub> ) lower than the assay’s cutoff value. These positive samples included 30 (6%) GI (C <sub>t </sub> 29.60–39.98), 8 (1.6%) GII (C <sub>t </sub> 32.96–38.76), and 3 (0.6%) GIV (C <sub>t </sub> 38.30–39.88) noroviruses. The viral load in the GI (mean 3.2 × 10 <sup>6 </sup> copies/g) and GII (mean 7.5 × 10 <sup>4 </sup> copies/g) norovirus-positive samples were in the range reported for human fecal samples ( _13_ ). All GIV-positive samples had viral loads close to the assay’s detection limit (1.3 × 10 <sup>3 </sup> copies/g) ( Figure 1 ). The sample that was positive for the GII norovirus (FT244) in the previous study had a viral load of 2.0 × 10 <sup>5 </sup> genome copies/g. Further analysis by sequencing of region D amplicons ( _14_ ) revealed that the GI-positive samples contained GI.1 noroviruses and the GII-positive samples contained GII.7 noroviruses. However, I could not obtain RT-PCR products and sequence information for the GIV-positive samples, even by nested RT-PCR. Consequently, I concluded that these samples either gave a false-positive result (C <sub>t </sub> values close to the detection limit) by real-time RT-PCR or contained a novel GIV norovirus that could not be amplified efficiently by primers designed on the basis of currently available GIV norovirus sequences.\n\n【6】Figure 2\n\n【7】Figure 2 . Phylogenetic trees showing GI and GII noroviruses from rhesus macaque fecal samples collected in 2008 ( _11_ ) and retested in 2015 by using a highly sensitive and specific real-time reverse...\n\n【8】In addition, I obtained full-length open reading frame (ORF) 2 sequences from 2 GI-positive and 4 GII-positive samples. The GI.1 norovirus ORF2 sequences (1,593 nt) had 100% nucleotide homology with each other and with the prototype Norwalk virus (M87661) ( Figure 2 , panel A). The GII.7 norovirus ORF2 sequences (1,623 nt) exhibited 99%–100% nucleotide homology with each other and 95% nucleotide homology with the closest GII.7 strain in the GenBank database (accession no. KJ196295) ( Figure 2 , panel B). I deposited 3 ORF2 sequences obtained in this study into the GenBank database under accession nos. KT943503–KT943505.\n\n【9】### Conclusions\n\n【10】A previous study reported the molecular detection of a GII norovirus in 1 of 500 rhesus macaques tested ( _11_ ). Although this detection rate was extremely low (0.2%), the finding indicated the occurrence of natural norovirus infections in colony macaques. In this study, retesting the 500 samples by a more sensitive and specific real-time RT-PCR confirmed the previously detected presence of the GII norovirus in 1 sample but also identified additional samples positive for GI, GII, and GIV noroviruses. Furthermore, additional amplification of the viral genome and sequencing confirmed the presence of GI.1 and GII.7 noroviruses but not GIV noroviruses.\n\n【11】The detection of a GI.1 norovirus in samples collected in 2008, with 100% homology to the prototype Norwalk virus, is somewhat surprising. The prototype virus was originally described in an outbreak occurring during 1968 ( _15_ ). According to outbreak surveillance data, GI.1 norovirus infections are extremely rare; consequently, establishing whether the Norwalk strain is still in circulation or has completely disappeared is difficult. Data on human norovirus strains circulating in the community or present in environmental samples at the time of the rhesus macaque fecal sample collection were not available from the local health department. The fecal samples were collected by rectal loops from individual macaques, so environmental contamination of samples, which might be an issue in other studies where manure is collected ( _9_ ), can be ruled out. Samples positive for Norwalk virus were not present in the laboratories involved in the study, and rhesus macaque samples were stored in a separate freezer under Biosafety Level 2+ protocol. This finding is supported by a report of recent circulation of noroviruses with 100% nt homology to the Norwalk virus (GenBank accession nos. JX455860, JX455863, JX455870, and JX455872) that were found in sewage samples in Tunisia during 2007–2009.\n\n【12】The observations from this study and the previous study ( _11_ ) indicate a strong plausibility for nonhuman primate reservoirs of human norovirus infections and the genetic mixing of animal and human caliciviruses under natural conditions from which new strains or emerging pathogens may arise. Additional studies are needed to establish the frequency, identity, and relevance of norovirus infections in nonhuman primates.\n\n【13】At the time of the study, Dr. Farkas was an assistant professor at the Division of Infectious Diseases, Cincinnati Children’s Hospital Medical Center. Currently, he is an assistant professor at the School of Veterinary Medicine, Louisiana State University, Baton Rouge, Louisiana, USA. His research interests include human and animal enteric viral diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7ec7353f-6281-400f-9034-998409146087", "title": "Etymologia: Elizabethkingia", "text": "【0】Etymologia: Elizabethkingia\n### _Elizabethkingia_ \\[e-lizʺə-beth-kingʹe-ə\\]\n\n【1】Figure\n\n【2】Figure . Six-day-old blood agar growth of _Elizabethkingia meningioseptica_ with 5 μg vancomycin (with zone of clearing) and 10 μg colistin disks. Source: Dr. Saptarshi via Wikimedia Commons\n\n【3】Named for Elizabeth O. King, a bacteriologist at the US Centers for Disease Control who studied meningitis in infants, _Elizabethkingia meningoseptica_ is a gram-negative, obligate aerobic bacterium in the family _Flavobacteriaceae_ ( Figure ). King named the bacterium _Flavobacterium_ (from the Latin _flavus_ , “yellow”) _meningosepticum_ , and in 1994 it was reclassified in the genus _Chryseobacterium_ (from the Greek _chryseos_ , “golden”). In 2005, it was placed in the new genus _Elizabethkingia_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "975e3a7c-f5df-425b-beb3-c7cecaa4f2c1", "title": "Toxigenic Corynebacterium diphtheriae–Associated Genital Ulceration", "text": "【0】Toxigenic Corynebacterium diphtheriae–Associated Genital Ulceration\nMembers of the genus _Corynebacterium_ colonize the upper respiratory tract and healthy skin and are frequently found in clinical specimens without clinical significance. Three species, _C. diphtheriae_ , _C. ulcerans_ , and _C. pseudotuberculosis_ , can cause serious infections by acquiring the ability to produce the diphtheria toxin manifesting clinically as cutaneous or respiratory diphtheria. Because vaccination is widely used in industrialized countries, diphtheria is rare, but it remains endemic to several countries of Africa, Asia, the Eastern Pacific region, and Eastern Europe. During 2001–2016, a total of 80 cases of toxigenic _Corynebacterium_ spp. infections were notified in Germany ( _1_ ).\n\n【1】### Case Report\n\n【2】In October 2016, a 16-year-old boy from Ethiopia sought care at the University Hospital of Cologne (Cologne, Germany) with 3 papules at the glans and a slightly bleeding, painful ulceration at the dorsal shaft of his penis. He complained about local pruritus and had swollen, painful inguinal nodules on the left side. On clinical examination, the ulcer appeared to have coalesced from a few smaller lesions with an erythematous base and a yellow to purulent exudate. The boy denied having lesions before the ulcer. Specific history could not be clarified satisfactorily: the boy had an undefined status of residence and lived in an accommodation facility for asylum seekers. Given that he was an unaccompanied minor refugee, the examiners considered traumatic sexual experiences when the boy was unwilling to share further details especially about his sexual history. General physical examination was unremarkable. Pubertal development was normal.\n\n【3】We presumed a sexually transmitted infection. Because we considered herpes simplex virus and chancroid as likely diagnoses, the boy started on empiric outpatient treatment with valaciclovir and azithromycin. Serologic screening for syphilis and herpes simplex virus PCR yielded negative results. Toxigenic _C. diphtheriae_ was cultured from the ulcer, and the boy was hospitalized 7 days after initial consultation because of cutaneous diphtheria. Toxigenicity of the isolate was verified by real-time PCR for _tox_ gene and the modified Elek test in the German National Consulting Laboratory for Diphtheria. On the day of hospitalization the lesions showed only minor signs of resolution. Blood test analysis did not detect any systemic inflammation. No other swab of the genital lesions or the uninflammed tonsils grew pathogens. Because the boy’s vaccination history was unclear, we used an enzyme immunoassay to determine the serum level of diphtheria toxin antibodies, and the results indicated seroprotection (4.69 IU/mL).\n\n【4】The patient was isolated and treated with intravenous penicillin (6 million U/d). When susceptibility testing revealed penicillin resistance, his antimicrobial drug regimen was changed to ciprofloxacin (1,200 mg/d). After 4 days of inpatient treatment, the patient was discharged. Follow-up visits confirmed full recovery; repeat swab sample testing of throat and penis were culture- and PCR-negative. The patient completed antimicrobial drug therapy 10 days after discharge.\n\n【5】In contact tracing, initiated by the local health department, 2 persons were determined to be at risk because they shared a room with the patient at the accommodation facility. Swab samples from tonsils and suspicious skin lesions (including 1 penile wound) of the contacts were negative for _C. diphtheriae_ . Consequently, we discussed public health actions with them; chemoprophylaxis or vaccination were not considered necessary.\n\n【6】### Conclusions\n\n【7】Only few cases of cutaneous diphtheria are reported in Germany, and they are strongly associated with defined risk factors, especially contact, during endemic circulation, associated with traveling ( _2_ ), migration ( _3_ ), zoonotic reservoirs ( _4_ ), or poor socioeconomic conditions ( _5_ ). In this case, the clinical manifestion of the genital ulcer with typical symptoms of sexually transmitted infections, such as painful unilateral inguinal nodules, supports the likelihood of sexually transmitted diphtheria, although we were not able to verify the patient’s medical or sexual history. We did not isolate any other pathogen, in contrast with another case of male urethritis in which toxigenic _C. diphtheriae_ was part of a polymicrobial infection and sexual transmission was assumed ( _6_ ). This case shows that unusual manifestations of diphtheria are conceivable on various body sites. Clinicians should consider this easily overlooked disease in the differential diagnosis for similar cases. This awareness might lead to an increase in confirmed cases, improve outcomes, and prevent disease through subsequent public health actions.\n\n【8】Dr. Fuchs is in his fourth year in clinical microbiology at the University Hospital of Cologne. His primary research interests are infectious diseases and antimicrobial resistance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "453b4f5a-67e9-4f07-b2be-753a43beb13b", "title": "Risk Factors for Norovirus, Sapporo-like Virus, and Group A Rotavirus Gastroenteritis", "text": "【0】Risk Factors for Norovirus, Sapporo-like Virus, and Group A Rotavirus Gastroenteritis\nRecent studies in the Netherlands and other countries have shown that viral infections, especially noroviruses (NV), are the most frequent cause of gastroenteritis in the community, both outbreak-related and endemic ( _1_ – _8_ ). The overall incidence of gastroenteritis in the Netherlands was estimated at 283 per 1,000 persons per year in a community-based study in 1999 ( _3_ ). NV was detected in 11% of cases, Sapporo-like viruses (SLV) in 2%, and rotavirus group A in 4%. The incidence at the general-practice level from 1996 to 1999 was estimated at 14 per 1,000 person-years; 5% was attributed to NV, 2% to SLV, and 5% to rotavirus ( _2_ , _9_ ). For rotavirus, preventive measures currently focus on developing vaccines to reduce hospitalizations in cases where the illness is complicated by dehydration. For caliciviruses, in spite of their high incidence, little effort has gone into prevention, and little is known about preventable routes of infection. Although possible transmission routes, such as food products or water supplies ( _10_ , _11_ ), were identified in outbreaks, sources in endemic cases are difficult to detect. Therefore, we conducted a case-control study to identify risk factors that could provide leads for preventing endemic cases of viral gastroenteritis attributable to caliciviruses and rotavirus.\n\n【1】### Methods\n\n【2】A community-based prospective cohort study with a nested case-control study was undertaken in the Netherlands in 1999 ( _3_ ). The cohort was followed to estimate the incidence of gastroenteritis. The nested case-control study was used to identify risk factors and determine etiology. The study was performed in cooperation with the sentinel general practice network of the Netherlands Institute of Primary Health Care. The cohort consisted of an age-stratified sample of persons registered at general practices in this network. Cases identified in the community-cohort with gastroenteritis were included in the case-control study, and a matched control was selected from the cohort members without gastroenteritis at that time. Case-patients and controls were matched by age, degree of urbanization, region, and date of inclusion. At the start of follow-up, all persons in the cohort completed a questionnaire on demographic characteristics and long-term risk factors (such as food-handling practices and presence of animals). Case-patients and controls included in the case-control study completed a questionnaire addressing short-term risk factors in the 7-day period before onset of symptoms and submitted stool samples. Case-patients submitted four stool samples (on days 1, 8, 14, and 21 of the episode), and controls submitted two stool samples (on days 1 and 8 from inclusion as a control). Samples were tested for NV and SLV by reverse transcription-polymerase chain reaction and for rotavirus group A by enzyme-linked immunosorbent assay, as described ( _3_ , _12_ – _14_ ).\n\n【3】##### Potential Risk Factors\n\n【4】Potential risk factors we studied were chronic gastrointestinal symptoms, being breastfed, having animals in the household (both pets and farm animals), food-handling hygiene index, method of keeping and heating up leftover food, presence of household equipment (blender, dishwasher, microwave, freezer), child in diapers in household, participant or other child in household attending a daycare center or primary school, size of household, being pregnant, being vegetarian, nationality, country of birth of participant and parents, being employed, type of house, income, educational level, age, and sex. The following factors were studied in the week before onset of illness or before inclusion as a control: contact with others with gastroenteritis (in and outside the household); swimming or other water-related sports; foreign travel; use of antimicrobial drugs, consumption of (raw or well-done) chicken, pork, beef, organ meat, meat in dough, fish, crab, shrimp, oysters, mussels, raw vegetables, salad, fruits, dried fruits, rice, raw milk, ice cream, soft cheeses, runny eggs, raw eggs, take-away fast-food, take-away bread rolls, take-away kebab, take-away Chinese food, meal services, food from canteen, food from reception, food from barbecue, eating out in a restaurant, and contact with farm animals (with or without diarrhea).\n\n【5】##### Statistical Analyses\n\n【6】All gastroenteritis case-patients who tested positive in the first or second stool samples and their matched controls were included in the analyses. Gastroenteritis was defined as one of the following: three or more loose stools in 24 hours; three or more vomiting episodes in 24 hours; diarrhea with at least two additional symptoms; or vomiting with at least two additional symptoms. Additional symptoms were abdominal pain, abdominal cramps, nausea, blood in stool, mucus in stool, fever, diarrhea, or vomiting.\n\n【7】Univariate analyses were completed by using McNemar and Bowkers test for symmetry for categorical variables and paired t tests and Wilcoxon signed rank test for continuous variables. A conditional logistic regression model was used to study the independent effects of risk factors with an association in the univariate analyses with a p value of <0.10. Selection of variables in the model was backwards manually, based on the log-likelihood ratio; a significance level of 0.05 was used.\n\n【8】All risk factors in the questionnaire were studied to have the possibility to generate hypotheses on transmission, in addition to confirming and clarifying existing theories. Since the specific variables on food handling in the questionnaires were mainly focused on possible risk factors for bacterial gastroenteritis, they were used as indicators of food-handling hygiene in these analyses. An index was made for food-handling hygiene on the basis of several indicator variables. Two different scores were developed: a basic score, calculated by adding up all factors and weighing them equally, and an optimized score, which used the β from a logistic model as the weight for each factor. This logistic model was fit on NV gastroenteritis as an outcome because this was the largest group. The following variables were included as indicators (factors marked with an asterisk were independent indicators in the optimized score): frequency of shopping, \\*checking the appearance of product in shop, checking the packaging for damage in shop, following the storage instructions, checking the expiration date, \\*duration of keeping eggs, \\*use of same cutting board for raw meat and other products, \\*washing of cutting board between use for raw meat and other products, and frequency of changing dish brush, \\*scourer, and dishcloth.\n\n【9】The effect of food-handling hygiene includes both the effect of poor food-handling hygiene in the household favoring indirect person-to-person transmission and foodborne infection by introduction of contaminated food into the household. To estimate the second effect separately, we estimated the proportion preventable by hygienic food handling among those not in contact with other persons with gastroenteritis in the last week. This estimate was made by calculating the incidence attributable to food-handling hygiene among those not exposed to other persons with gastroenteritis and dividing it by the total incidence of virus-specific gastroenteritis. We assumed that all persons who reportedly had contact with a person with gastroenteritis were infected by that person.\n\n【10】Because age was likely to interact with all variables, we constructed a separate model for the age groups <5 years and \\> 5 years. This stratification was possible for NV only because not enough adults were infected with rotavirus and SLV to make the analysis.\n\n【11】Population-attributable risk fractions (PAR) were calculated on the basis of multivariate odds ratios (OR) by estimating the incidence attributable to the risk factor and dividing it by the total incidence of virus-specific gastroenteritis. The total incidence of virus-specific gastroenteritis was calculated by multiplying the proportion positive and the overall incidence of gastroenteritis in the cohort. The incidence attributable to the risk factor was calculated as the total virus-specific incidence minus the estimated incidence if the risk factor was absent, which was estimated by weighing the cases according to their exposure status. Exposed cases were weighed as 1/OR of exposure, nonexposed cases as 1. All incidence estimates were standardized by age and cohort. Data from the case-control study were extrapolated to the entire cohort.\n\n【12】### Results\n\n【13】##### Norovirus (NV)\n\n【14】In total, 152 case-patients were positive for NV—57 in both stool samples, 57 only in the first sample, and 38 only in the second sample. Of the matched controls, seven were positive for NV but did not have gastroenteritis. The median age of case-patients was 2 years (age distribution: <1 year: 47 \\[31%\\]; 1–4 years: 60 \\[39%\\]; 5–9 years: 25 \\[16%\\]; 10–59 years: 12 \\[8%\\]; \\> 60 years: 8 \\[5%\\]).\n\n【15】Figure 1\n\n【16】Figure 1 . Distribution of basic food-handling hygiene score in norovirus gastroenteritis cases (n = 152) and controls (n = 152). (A higher score indicates less hygienic practices.)\n\n【17】NV gastroenteritis was independently associated with food-handling hygiene, having more than one household member with gastroenteritis (hereafter referred to as household gastroenteritis contact), and having contact with a person with gastroenteritis outside the household (hereafter referred to as outside gastroenteritis contact) in the week before onset of symptoms ( Table 1 , Figure 1 ). For the risk factor of household gastroenteritis contact, risk was slightly higher if the household member was a child rather than an adult (5.2 \\[95% confidence intervals (CI) 1.8 to 15.3\\] vs. 4.4 \\[95% CI 2.0 to 9.6\\]). Risks were comparable if the household member had diarrhea or vomiting. Because of the strong correlation of all variables on household contacts, only the number of household gastroenteritis contacts was included in the model. The contact of cases with symptomatic persons outside the household had taken place in the house of friends or family (31%), a daycare center (19%), school (18%), home (10%), or other places and work (22%). Cases and controls did not differ significantly. The association of attendance at daycare and primary school with NV gastroenteritis in the univariate analysis was no longer observed after correction for household gastroenteritis contact, especially if the sick household member was a child.\n\n【18】Food-handling hygiene and outside gastroenteritis contact were the factors with the highest impact, as measured by PAR ( Tables 1 and 2 ). PAR for all significant risk factors combined was 80%. PAR for outside gastroenteritis contact and household gastroenteritis contact combined accounted for 63% of cases. PAR for the two factors representing transmission in the household combined (hygiene and household gastroenteritis contact) was 56%. This figure is much lower than the sum of both PARs.\n\n【19】For persons \\> 5 years of age, the effect of household gastroenteritis contact was reduced (OR = 1.1, PAR = 4%) when controlling for food-handling hygiene. Both factors (food-handling hygiene and household gastroenteritis contact) combined showed a similar PAR in both age groups (60% vs. 65%). Use of an optimized food-handling hygiene score for NV gastroenteritis resulted in a higher estimate of PAR for food-handling hygiene (60%).\n\n【20】We estimated the effect of contaminated food’s entering the household, separate from transmission from symptomatic contact persons through food to the patient, as explained in Methods. (We assumed that all case-patients who had contact with a person with gastroenteritis in the week before onset of symptoms were infected by these contacts.) Thirty-four pairs remained for the calculation of OR. For food-handling hygiene, OR was slightly higher for those not having contact with someone with gastroenteritis (1.4, 95% CI 0.8 to 2.2). The estimate of PAR for contaminated food’s entering the household was 12% of all NV gastroenteritis cases and 16% when the optimized score was used.\n\n【21】##### Sapporo-like Viruses\n\n【22】In total, 48 cases were positive for SLV—21 in both samples, 22 only in the first sample, and 5 only in the second sample. Of the matched controls, two were positive for SLV but did not have gastroenteritis. The median age of SLV gastroenteritis case-patients was 1 year (age-distribution: <1 year: 19 \\[40%\\]; 1–4 years: 21 \\[44%\\]; 5–9 years: 4 \\[8%\\]; 10–68 years: 4 \\[8%\\]).\n\n【23】Outside gastroenteritis contact was the only independent risk factor for SLV gastroenteritis ( Table 3 ). The location of contacts of case-patients and controls did not differ significantly. For case-patients, 29% of contacts took place at daycare, 21% at homes of friends or family, 14% at home, and 7% at school.\n\n【24】##### Rotavirus\n\n【25】In total, 54 cases were positive for rotavirus—11 in both samples, 41 only in the first, and 2 only in the second. None of the matched controls was positive for rotavirus. The median age of rotavirus gastroenteritis case-patients was <1 year (age distribution of cases: <1 year: 28 \\[52%\\]; 1–4 years: 18 \\[33%\\]; 5–9 years: 3 \\[6%\\]; 10–72 years: 5 \\[9%\\]).\n\n【26】Figure 2\n\n【27】Figure 2 . Distribution of food-handling hygiene score in rotavirus gastroenteritis cases (n = 54) and controls (n = 54). (A higher score indicates less hygienic practices.)\n\n【28】Rotavirus gastroenteritis was independently associated with outside gastroenteritis contact and with food-handling hygiene ( Table 4 and Figure 2 ). A strong independent negative association was found with presence of a blender in the household. By univariate analysis, a high education level was a risk factor for rotavirus gastroenteritis, as was a household gastroenteritis contact. The risk was higher if the household gastroenteritis contact was a child (OR 5.8, 95% CI 1.3 to 25.6) than if the contact was an adult (OR 4.0, 95% CI 0.5 to 38.2). The association of a household gastroenteritis contact disappeared after correction for outside gastroenteritis contact. The association with educational level disappeared after correction for food-handling hygiene. Locations of outside gastroenteritis contacts did not differ significantly between cases and controls. For cases, 40% of contacts took place at homes of friends or family, 30% at daycare centers, and 20% at home. PAR for outside gastroenteritis contact was 86%; for food-handling hygiene, PAR was 46%. PAR for both factors combined was 92%. Use of the optimized food-handling hygiene score resulted in a PAR for food-handling hygiene of 64% and PAR for all factors combined of 96%.\n\n【29】We estimated the effect of contaminated food’s entering the household in the same way as we did for NV gastroenteritis. The case-patients in 10 pairs had not had any contact with a person with gastroenteritis. In this group, OR for food-handling hygiene was 1.8 (95% CI 0.8 to 3.9), which is higher than in the total group of rotavirus cases. The estimate of PAR for contaminated food’s entering the household, based on these assumptions, was 4% of all rotavirus gastroenteritis cases.\n\n【30】### Discussion\n\n【31】To our knowledge, this study is the first to describe risk factors for the three main viral pathogens causing gastroenteritis and to estimate the effect of these risk factors in the population. The main risk factor for NV, SLV, and rotavirus gastroenteritis was contact with persons with gastroenteritis, supporting the hypothesis that these viruses are mainly transmitted from person to person ( _13_ , _15_ ). The high PARs indicate that most of these infections can indeed be prevented by stopping the spread from symptomatic persons to others.\n\n【32】Food-handling hygiene in the household was also strongly associated with risk for NV gastroenteritis and with a high PAR. This association indicates that in a household setting these viruses do not necessarily transmit directly from one person to another but by means of food. Hygienic food-handling procedures can therefore further prevent the infection spreading from one person to another ( _16_ ).\n\n【33】The impact of food-handling hygiene can be partly explained by food contamination that occurs when a sick household member prepares meals. However, food contaminated at an earlier step in the food chain may also be a source. On the basis of our data, an estimated 12%-16% of NV gastroenteritis and 4% of rotavirus gastroenteritis cases are caused by introduction of contaminated food or water. This figure may be an overestimate, if infection through the shedding of other asymptomatic persons plays a major role, or if the knowledge of respondents about illness in their contacts is limited. Alternatively, the proportion of NV infections attributable to foodborne transmission might be an underestimate since we assumed that, if contact with symptomatic others had taken place, such contact was always the cause of illness. The 40% of NV infections that were foodborne, as presented by Mead ( _17_ ) on the basis of NV outbreak surveillance, is higher than our 12%-16% estimate related to contaminated food’s entering the household and comparable with the 47% related to food hygiene. Mead’s estimate was extrapolated to a community incidence from preliminary NV data from our community study (8,17). Clearly, the precise number of community cases of viral foodborne infection cannot be derived by either approach. However, we strongly support the conclusion that a considerable proportion of NV infections may be prevented by improving food hygiene.\n\n【34】In surveillance systems of outbreaks of NV, person-to-person spread and foodborne spread are reported to be the most common transmission routes ( _1_ , _8_ , _18_ ). The relative importance of each differs by country and is strongly influenced by the design of the surveillance system ( _19_ ). Outbreaks covered in a surveillance system do not necessarily represent all outbreaks. Our study shows that in sporadic cases, direct and indirect person-to-person transmission remains the most prominent mode of transmission, followed by food contaminated outside the household. Nevertheless, extrapolation of our estimate to the population of the Netherlands (16 million) suggests that, of the 650,000 NV gastroenteritis cases that occur annually, an estimated 80,000 cases are foodborne, which is more than the estimate for Salmonella (50,000 foodborne cases each year).\n\n【35】No specific food products were associated with NV gastroenteritis. This finding is not remarkable because NV can probably survive on almost all food products that are not cooked before consumption, and a very low infectious dose is required, as has been demonstrated for another naked single-stranded RNA virus, poliovirus ( _20_ ). Since NV cannot be grown in cell culture, little is known about its heat inactivation profiles. However, studies for another enteric single-stranded RNA virus with similar structure (hepatitis A virus) suggest that heating for 30 s at 90°C will completely inactivate viruses in any food ( _21_ ). Most published foodborne outbreaks could be traced back to infected food handlers at some point in the production chain, suggesting that this is by far the most common source of foodborne infections ( _8_ , _22_ – _24_ ). Our results show that, without applying extraordinary hygienic practices but by just following normal hygiene procedures, a substantial portion of sporadic NV infections could be prevented. Because of the high transmission rate in households, persons with a household member with gastroenteritis are at greater risk of being infected. Since several foodborne outbreaks have been reported in which the food handler who had most likely contaminated the food was not symptomatic (yet), making professional food handlers aware of their higher probability of being infected when living with a household member with gastroenteritis might be useful ( _24_ ).\n\n【36】For NV, for persons of \\> 5 years, PAR for food-handling hygiene and the combined PAR for food-handling hygiene and a household gastroenteritis contact were similar. The decrease in OR for a household gastroenteritis contact to almost 1, after hygiene was included in the model, suggests that transmission from one ill household member to another occurs almost entirely through food in persons of \\> 5 years. For children, only part of the transmission from one person to another in the household is through food-handling hygiene. Possibly, for young children, exposure is very common and better food-handling hygiene in the household only prevents a minority of exposure possibilities.\n\n【37】A large study in U.S. households showed that the proportion of rotavirus infections acquired in the household was higher for adults than for children, indicating that children introduce the infection into the household ( _25_ ). In contrast to rotavirus and SLV gastroenteritis, NV gastroenteritis is not limited to the youngest age groups. This finding could explain why food-handling hygiene and having a household gastroenteritis contact had a higher impact on NV gastroenteritis than on SLV gastroenteritis. For rotavirus and SLV, undetected asymptomatic infections (not included as cases in this study) may occur at older age through these routes.\n\n【38】Living in a household with a child attending a daycare center or primary school was univariately associated with NV gastroenteritis. However, when the data were corrected for a household gastroenteritis contact, especially with a child, this association disappeared. This finding suggests that daycare centers and primary schools are the settings in which the primary infection in the household was acquired. The fact that a participant’s daycare attendance had only borderline association with NV gastroenteritis indicates that gastroenteritis acquired at daycare centers was less common in our study than secondary transmission in the household.\n\n【39】We could not confirm the association between rotavirus gastroenteritis and daycare center attendance, as has been reported by others ( _26_ , _27_ ). A study with comparable methods in England also did not find this association ( _28_ ).\n\n【40】##### Methodologic Issues\n\n【41】The optimized score for food-handling hygiene was fitted in the same NV data for which it was used to estimate the effect of hygiene. This might have resulted in an overestimate for NV because noise is also modeled in the prediction. Although all the factors included in the food-handling hygiene index are related to food handling, we cannot exclude the possibility that the index might be a proxy for hygiene as a whole and not just related to food handling. Finally, we assumed that the relationship between the hygiene score and the risk for NV gastroenteritis was exponential. Although this assumption is not entirely true, the exponential model was a good approximation.\n\n【42】In addition, the uncertainty around the estimates will be wide, and PARs should be interpreted as indications of the magnitude of the effect of a risk factor. Many risk factors were tested for, and a type 1 error might have occurred, identifying risk factors that were in fact not associated to the disease. However, for most of the risk factors, plausible biological mechanisms exist. An exception is the association of a blender with rotavirus, on which, therefore, no conclusions are drawn.\n\n【43】By using a case-control design based on clinical gastroenteritis, differentiating risk factors for infection with the virus from risk factors for developing illness after infection is not possible. Factors that represent long-term exposure might have induced immunity earlier in life, and subsequent infections might not result in clinical disease ( _29_ , _30_ ). As a result, long-term risk factors might be more difficult to detect in case-control studies, or, if the proportion of individual persons with immunity is large, even cause a negative association. Especially for rotavirus, for which immunity is proven to exist, this factor might play a role. Whether relevant immunity to NV and SLV is induced is still under debate.\n\n【44】Dr. de Wit has worked as an epidemiologist on the epidemiology of gastroenteritis at the National Institute of Public Health and the Environment since 1996.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "33eca710-9b67-420e-ad62-f0bf89d4cfc7", "title": "Susceptibility of Domestic Swine to Experimental Infection with Severe Acute Respiratory Syndrome Coronavirus 2", "text": "【0】Susceptibility of Domestic Swine to Experimental Infection with Severe Acute Respiratory Syndrome Coronavirus 2\nSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease (COVID-19) in humans; symptoms can range from asymptomatic to mild or severe, including severe respiratory distress and sometimes death ( _1_ ). Rapidly spreading, the novel SARS-CoV-2 virus emerged in China in late 2019, and on March 11, 2020, the World Health Organization declared a pandemic ( _2_ ). SARS-CoV-2 is believed to have originated in bats, but its origins are still under intense investigation, and reports continue to identify the ability of the virus to infect additional animal species ( _3_ – _8_ ).\n\n【1】Detection of natural infections sheds light on knowledge gaps in SARS-CoV-2 transmission and raised concerns of amplifying or reservoir hosts. In turn, clarification of wild and domestic animal susceptibility can help us assess their potential roles in and risks for transmission to prevent future disease spread. Domestic swine, one of the most highly produced agricultural species, previously have impacted public health ( _9_ – _12_ ). Backyard, small stakeholder animal production has increased in both rural and urban environments and provides a source of high-quality protein and income in these areas, but the practice also can serve as a source for zoonotic disease; therefore, the potential role of pigs in the spread of SARS-CoV-2 should be investigated ( _13_ ). Recent evidence for involvement of production animals in SARS-CoV-2 transmission was highlighted in the Netherlands, where anthroponotic transmission from humans to farmed mink was proposed with subsequent zoonotic transmission to \\> 2 humans from mink ( _14_ ). That case further exemplifies the need to identify the potential role of production animals in disease transmission.\n\n【2】Angiotensin-converting enzyme 2 (ACE2) has been identified as the receptor for SARS-CoV-2 in human cells ( _15_ ). A BLAST ( https://blast.ncbi.nlm.nih.gov ) query of the protein database by using translated nucleotide (BLASTx) from the human ACE2 coding sequence predicts 98% coverage and 81% identity for the homologous receptor in swine. Of note, using the same search, mink ( _Mustela lutreola_ ) show 82% similar identity and domestic felines ( _Felis catus_ ) show 85% similar identity to the human ACE2 for their cognate receptors. Moreover, mink and cats both have been reported to be susceptible to SARS-CoV-2 and have shown transmission to other animals ( _5_ , _16_ ). Zhou et al. ( _17_ ) used in vitro infectivity studies testing ACE2 receptors from laboratory mice, horseshoe bats ( _Rhinolophus sinicus_ ), civets, and the domestic pig, and found all receptors except those from mice entered HeLa cells, indicating a functional target for SARS-CoV-2. Moreover, the authors used additional known coronavirus receptors, including both aminopeptidase N and dipeptidyl peptidase 4, and found neither are used for cell entry ( _17_ ), underlining the specificity for the ACE2 receptor.\n\n【3】We aimed to determine whether domestic swine are susceptible to SARS-CoV-2 infection by testing for live SARS-CoV-2 virus after experimental inoculation. After oronasal inoculation, we assessed swine for clinical signs and pathology, evidence of virus shedding, viral dissemination within tissues, and seroconversion.\n\n【4】### Methods\n\n【5】Experimental design, including housing conditions, sampling regimens, and humane endpoints, were approved by the Animal Care Committee of the Canadian Science Centre for Human and Animal Health (no. AUD #C-20-005). All procedures and housing conditions were in strict accordance with the Canadian Council on Animal Care guidelines. Group housing was in Biosecurity Level 3 (BSL-3) zoonotic large animal cubicles. Animals were provided commercial toys for enrichment and access to food and water ad libitum. All invasive procedures, including experimental inoculation and sample collection (nasal washes, rectal swabs, and blood collection), were performed under isoflurane gas anesthesia, and animals were euthanized by intravenous administration of a commercial sodium pentobarbital solution.\n\n【6】##### Study Design\n\n【7】We obtained 19 domestic, 8-week-old American Yorkshire crossbred pigs ( _Sus scrufa domesticus_ ), 6 castrated males and 13 females, locally sourced from a high health status farm in Manitoba, Canada. We obtained animals locally, rather than from a specific pathogen–free colony, to determine the risk to farmed pigs in Canada. We oronasally challenged 16 pigs with 1 × 10 <sup>6 </sup> PFU/animal in a total of 3 mL Dulbecco’s Modified Eagle’s Medium (DMEM; Wisent, http://www.wisentbioproducts.com ) under sedation with isoflurane. We distributed 1 mL per nostril and placed 1 mL in the distal pharynx by using a sterile, tomcat-style catheter. We confirmed the challenge dose by back-titration of the inoculum on Vero E6 cells.\n\n【8】We divided the 16 inoculated pigs into 2 groups of 8, and each group was housed in a separate BSL-3 cubicle. At day 10, we introduced 2 naive pigs, 1 into each cubicle with the inoculated pigs, to serve as in-room transmission controls. Animal numbers were not based on power analysis but on limitations of the containment animal room size and requirements of Canadian Council on Animal Care guidelines. Group assignments for day of euthanasia and necropsy were based on randomization at the time of permanent animal identification via ear tags.\n\n【9】At the time of inoculation (day 0) and every other day beginning at 3 days postinoculation (dpi) until day 15, we performed a physical examination, including collection of blood; rectal, oral, and nasal swabs; and nasal wash with sterile Delbecco’s phosphate-buffered saline (D-PBS). We began performing necropsies and post-mortem sampling starting at 3 dpi ( Table 1 ). We sampled and necropsied 1 additional uninoculated pig to serve as a farm control providing negative control tissues. We sampled the remaining pigs at 22 dpi and 29 dpi. We collected group oral fluids from rope chews daily.\n\n【10】##### Animal Sampling\n\n【11】Oral, rectal, and nasal swab specimens were taken from each pig under general anesthesia by using isoflurane. Samples were placed into sterile D-PBS containing streptomycin, vancomycin, nystatin, and gentamycin. Fluid was collected from a bilateral nasal wash by using sterile D-PBS. Blood was collected in serum, sodium citrate, sodium heparin, and K3 EDTA collection tubes via jugular venipuncture.\n\n【12】##### Hematology, Chemistry, and Blood Gas Analyses\n\n【13】Hematology was performed on an HM5 analyzer (Abaxis, https://www.abaxis.com ) by using K3 EDTA-treated whole blood. We evaluated erythrocytes, hemoglobin, hematocrit, mean corpuscular volume, mean corpuscular hemoglobin, mean corpuscular hemoglobin concentration, red cell distribution weight, platelets, mean platelet volume, leukocytes, and absolute and percent neutrophil count, lymphocyte count, monocyte count, eosinophil count, and basophil count. Blood chemistries were evaluated on a VetScan 2 (Abaxis) with Comprehensive Diagnostic Profile rotors (Abaxis) by using serum stored at −80°C until tested. We evaluated glucose, blood urea nitrogen, creatinine, calcium, albumin, total protein, alanine aminotransferase, aspartate aminotransferase, alkaline phosphatase, amylase, potassium, sodium, phosphate, chloride, globulin, and total bilirubin. We used sodium heparin-treated blood to analyze venous blood gases by using an iSTAT Alinity V (Abaxis) instrument with a CG4+ cartridge (Abaxis) to measure lactate, pH, total carbon dioxide, partial pressure carbon dioxide, partial pressure oxygen, soluble oxygen, bicarbonate, and base excess. We used age-specific values and the instrument reference intervals to establish normal ranges ( _18_ – _20_ ).\n\n【14】##### Necropsy\n\n【15】Necropsy was performed after euthanasia via sodium pentobarbital overdose, confirmation of death, and exsanguination by femoral artery laceration. We collected tissue samples from skeletal muscle, abdominal fat, liver, spleen, pancreas, duodenum, jejunum, ileum, spiral colon, kidney, gastrohepatic and mesenteric lymph nodes, right cranial lung lobe, right middle lung lobe, right caudal lung lobe, left cranial lung lobe, left caudal lung lobe, trachea, heart, tracheobronchial lymph nodes, cervical spinal cord, meninges, cerebrum, cerebellum, brainstem, olfactory bulb, nasal turbinates, submandibular lymph nodes, tonsils, trigeminal ganglion, and the entire eye. From female animals, we collected the uterus and ovaries of the reproductive tract en bloc. We collected epiglottis and laryngeal folds from some animals. We split tissue samples between 10% neutral-buffered formalin and fresh tissue. We also collected cerebrospinal fluid, urine (when possible), vitreous, and bronchoalveolar lavage by using DMEM.\n\n【16】We fixed tissues in 10% neutral phosphate-buffered formalin. We routinely processed and sectioned tissue at 5 µm and stained with hematoxylin and eosin (HE) for histopathologic examination. We performed in situ hybridization on 5 μm paraffin-embedded formalin-fixed tissue sections by using RNAscope 2.5HD Detection Reagent Red kit and V-nCoV2019-S probe (Advanced Cell Diagnostics, http://rna.acdbio.com ). Then, we counterstained sections with Gill’s 1 hematoxylin (Leica Biosystems, https://www.leicabiosystems.com ), dried, and coverslipped.\n\n【17】We propagated SARS-CoV-2 isolate hCoV-19/Canada/ON-VIDO-01/2020 (GISAID accession no. EPI\\_ISL\\_425177), on Vero E6 cells in DMEM supplemented with 1% fetal bovine serum. We titrated virus by plaque assay and performed viral isolation as previously described ( _21_ , _22_ ).\n\n【18】##### Tissue Homogenization and Virus Isolation\n\n【19】Weighed, frozen tissue sections in Precellys bead mill tubes (Bertin, https://en.esbe.com ) were thawed, and we added D-PBS to make 10% (w/v) tissue homogenates. We processed tubes by using a Minilys personal tissue homogenizer (Bertin, https://www.bertin-instruments.com ) and clarified by centrifugation at 2,000 × _g_ . We used TriPure Reagent (Roche, https://www.roche.com ) to inactivate clarified homogenates, swab specimens, and fluids collected from experimental animals and extracted RNA in duplicate. Samples positive by semiquantitative real-time RT-PCR (qRT-PCR) samples were tested for virus isolation through standard plaque assay on Vero E6 cells by using freshly prepared homogenates of frozen tissue.\n\n【20】##### RNA Extraction\n\n【21】We extracted total RNA from cell culture and experimental samples, including nasal, oral, and rectal swab specimens; nasal washes; oral fluids; whole blood in sodium citrate; and tissues by using MagMax CORE Nucleic Acid Purification Kits (ThermoFisher Scientific, https://www.thermofisher.com ) per manufacturer’s recommendation with the following modifications. In brief, we diluted samples in TriPure Reagent (Sigma-Aldrich, https://www.sigmaaldrich.com ) at a 1:9 ratio and used this in place of the manufacturer’s lysis buffer for inactivation. We used 650 µL of TriPure-inactivated sample, 30 µL of binding beads, and 350 µL of kit-provided CORE binding buffer spiked with ARM-ENTERO (Asuragen, https://asuragen.com ) enteroviral armored RNA, then single washes in both wash 1 and wash 2 buffers, and a final elution volume of 30 μL of kit-supplied elution buffer by using the automated MagMax Express 96 system running the KingFisher-96 Heated Script MaxMAX\\_CORE\\_KF-96 (ThermoFisher Scientific). The spiked enteroviral armored RNA was used as an exogenous extraction and reaction control.\n\n【22】##### Detection of SARS-CoV-2\n\n【23】We performed qRT-PCR on all extracted samples by using primers and a probe specific for SARS-CoV-2 envelope (E) gene ( _23_ ), including forward primer E\\_SARBECO\\_F1 (5′-ACAGGTACGTTAATAGTTAATAGCGT-3′); reverse primer E\\_SARBECO\\_R2 (5′-ATATTGCAGCAGTACGCACACA-3′); and probe E\\_SARBECO-P1 (5′-ACACTAGCCATCCTTACTGCGCTTCG-3′). We prepared master mix for qRT-PCR by using TaqMan Fast Virus 1-step Master Mix (ThermoFisher Scientific) according to manufacturer’s specifications by using 0.4 µmol of each E gene primer and 0.2 µmol of probe per reaction. Reaction conditions were 50°C for 5 min, 95°C for 20 s, and 40 cycles of 95°C for 3 s then 60°C for 30 s. Runs were performed by using a 7500 Fast Real-Time PCR System (Thermofisher, ABI), and semiquantitative results were calculated based on a gBlock (Integrated DNA Technologies, https://www.idtdna.com ) standard curve for SARS-CoV-2 E gene. For confirmation, we used SARS-CoV-2–specific primers targeting the spike (S) gene and the RNA-dependent RNA polymerase (RdRp) gene. For S, we used the forward primer SARS2\\_Spike\\_FOR (5′-TGATTGCCTTGGTGATATTGCT-3′); the reverse primer SARS2\\_Spike\\_REV (5′-CGCTAACAGTGCAGAAGTGTATTGA-3′); and the probe SARS2\\_Spike\\_Probe (5′-TGCCACCTTTGCTCACAGATGAAATGA-3′). For RdRp, we used forward primer RdRp\\_SARSr-F (5′-GTGARATGGTCATGTGTGGCGG-3′); reverse primer RdRp\\_SARSr-R (5′-CARATGTTAAASACACTATTAGCATA-3′); and probe RdRp\\_SARSr-P2 (5′-CAGGTGGAACCTCATCAGGAGATGC-3′). We tested all samples in duplicate and considered cycle threshold (C <sub>t </sub> ) <36 positive.\n\n【24】##### Genome Sequencing\n\n【25】We were able to extract SARS-CoV-2 RNA from the submandibular lymph node of 1 pig (20–06), which was processed for high-throughput sequencing by CFIA National Centre for Foreign Animal Disease (NCFAD) Genomics Unit with enrichment for sequences for vertebrate viruses according to previously published method ( _24_ , _25_ ) and sequenced on a MiSeq (Illumina, https://www.illumina.com ) using MiSeq Reagent Kit v3 (600-cycle; Illumina). Data analysis also were performed by the CFIA NCFAD Genomics Unit using nf-villumina version 2.0.0 ( https://github.com/peterk87/nf-villumina ), an in-house workflow developed by using Nextflow ( _26_ ), which performed read quality filtering with fastp ( _27_ ); Centrifuge version 1.0.4-beta ( _28_ ) and Kraken2 version 2.0.8 ( _29_ ) read taxonomic classification using an index of the National Center for Biotechnology Information (NCBI) nucleotide database (downloaded 2020 Feb 4); a Kraken2 index of NCBI RefSeq ( https://www.ncbi.nlm.nih.gov/refseq ) sequences of archaea, bacterial, viral, and human genomes GRCh38 (downloaded and built on 2019 Mar 22); removal of nonviral reads (i.e., not classified as belonging to superkingdom “Viruses” (NCBI taxonomic identification 10239) by using Kraken2 and Centrifuge taxonomic classification results; de novo metagenomics assembly of taxonomically filtered reads by Shovill version 1.0.9 ( _30_ ), Unicycler version 1.0.9 ( _31_ ), and Megahit version 1.2.9 ( _32_ ); and nucleotide BLAST+ version 2.9.0 ( _33_ , _34_ ) search of all assembled contigs against the NCBI nucleotide BLAST database (downloaded 2020 April 10) using the “update\\_blastb.pl” script as part of the blast Bioconda package ( _35_ ). We mapped nf-villumina taxonomically filtered reads against the top viral nucleotide BLAST match, SARS-CoV-2 isolate 2019-nCoV/USA-CA3/2020, MT027062.1, to generate a majority consensus sequence.\n\n【26】##### Serum Neutralization Assays\n\n【27】We determined neutralizing antibody titers in serum samples by using a plaque reduction neutralization test (PRNT) against SARS-CoV-2. Serial 5-fold dilutions of heat inactivated (30 min at 56°C) serum samples were incubated with virus for 1 h at 37°C. Each virus–serum mixture was then added to duplicate wells of Vero E6 cells in a 48-well format, incubated for 1 h at 37°C, and overlaid with 500 μL of 2.0% carboxymethylcellulose in DMEM per well. Plates were then incubated at 37°C for 72 h, fixed with 10% buffered formalin, and stained with 0.5% crystal violet. Serum dilutions with >70% reduction of plaque counts compared with virus controls were considered positive for virus neutralization. We used negative serum samples plus virus controls to estimate the percent reduction.\n\n【28】##### Surrogate Virus Neutralization Test\n\n【29】Detection and semiquantitation of neutralizing antibodies were determined by using SARS-CoV-2 Surrogate Virus Neutralization Test Kit (Genscript, https://www.genscript.com ) according to the manufacturer’s instructions. All samples from 7–29 dpi were assessed, including archived negative serum samples and kit-supplied negative controls. We considered values above the manufacturer’s recommended cutoff of 20% positive for neutralization.\n\n【30】### Results\n\n【31】Starting at 1 dpi, a mild, bilateral ocular discharge developed in the 16 experimentally inoculated pigs; in some cases, this discharge was accompanied by serous nasal secretion. Discharge was observed for only the first 3 dpi. Temperatures among pigs remained normal throughout the study ( Appendix Table 1). Overall, animals did not develop clinically observable respiratory distress; however, 1 animal (pig 20–06) had mild depression with a cough at 1 dpi, which continued through 4 dpi. This animal did not display additional clinical signs over the course of the study.\n\n【32】Viral shedding can occur through droplets from coughing, sneezing, oral fluids, or gastrointestinal involvement. Thus, we developed a sampling schedule to determine the incidence of viral shedding ( Table 1 ). Starting at 3 dpi, we sampled oral, nasal, and rectal swabs every other day up to 15 dpi, in case of delayed onset ( _1_ ). We extracted nucleic acid from swabs and performed qRT-PCR to identify SARS-CoV-2 by targeting the E gene, but we did not detect viral RNA in swabs from any animals over the course of the study ( Table 2 ).\n\n【33】Nasal washes are a sensitive method for detection of pathogens in swine, and we routinely sampled nasal washes by using sterile D-PBS to rinse nasal passages. Two pigs (20–10 and 20–11) displayed low levels of viral RNA by qRT-PCR at 3 dpi ( Table 2 ). We attempted recovery of live virus from PCR-positive nasal wash samples, but neither produced cytopathic effect or increased RNA detection via qRT-PCR of the cell culture supernatant.\n\n【34】We also used a noninvasive, group sampling method to evaluate viral shedding. A cotton rope was hung in animal pens before feeding; when pigs chewed on the rope, they deposited oral fluids. We processed fluids from ropes daily and samples from cubicle 1 had a weak positive signal for viral RNA at 3 dpi by qRT-PCR ( Tables 1 , 3 ). We attempted virus isolation from this sample but were not able to isolate the virus. Of note, the positive oral fluid did not come from the same room as the 2 positive nasal washes from pig 20–10 and pig 20–11, which were housed in cubicle 2. Therefore, at least 3 animals provide evidence of viral nucleic acid in oronasal secretions from 2 independent animal rooms. In addition, we did not detect viral infection at any point from the 2 naive transmission contact pigs introduced to the infected pigs at 10 dpi.\n\n【35】After collecting samples, we attempted SARS-CoV-2 detection from whole blood by qRT-PCR ( Table 1 ). Viremia, as indicated by the presence of viral RNA in the blood, was not detected in any animal during the study ( Table 2 ). We measured blood cell counts by using the VetScan HM5 (Abaxis), blood chemistries by using VetScan 2 (Abaxis), and blood gases by using i-STAT (Abbott, https://www.abbott.com ). Although some laboratory variation was observed during the study, changes were minimal and inconclusive, and profiles consistent with acute viral infection or subsequent organ damage were not observed.\n\n【36】To identify potential target tissues or gross lesions consistent with SARS-CoV-2, we performed necropsy on 2 animals every other day from 3 dpi through 15 dpi and necropsied an additional 2 pigs at both 22 dpi and 29 dpi ( Table 1 ). No clinically significant pathology was observed that could be attributed directly to a viral infection. We performed qRT-PCR across all tissues and samples collected at necropsy targeting the E gene of SARS-CoV-2 ( Table 4 ). One tissue sample, the submandibular lymph node from pig 20–06, necropsied at 13 dpi, was positive by qRT-PCR (C <sub>t </sub> \\= 32) for viral RNA. The tissue sample testing was repeated in triplicate, on independent days, and generated consistent results. Further, RNA was extracted from homogenized tissue, and we recovered the full genome sequence of SARS-CoV-2 from pig 20–06.\n\n【37】We generated a 10% homogenate from the submandibular lymph node of pig 20–06 and used it to infect Vero E6 cells. We took aliquots from the cell culture on days 2 and 3 postinfection to monitor viral replication as indicated by an increasing quantity of RNA. On day 3, we observed mild cytopathogenic effect in the first passage with an increase in viral RNA measured by qRT-PCR targeting the E, S, and RdRp genes. The first passage supernatant was clarified by centrifugation and a second passage performed in Vero E6 cells. At day 2 postinfection of the second passage, we observed substantial cytopathogenic effect and increasing copies of SARS-CoV-2 viral RNA, confirmed by qRT-PCR. Together, these findings demonstrated the presence of live, replication-competent SARS-CoV-2 virus isolated from the submandibular lymph node of pig 20–06 ( Table 2 ).\n\n【38】We monitored development of SARS-CoV-2 neutralizing antibodies over the course of study. Starting at 7 dpi, we obtained serum from individual animals for both virus neutralization test (VNT) and a surrogate VNT (sVNT) using cPass Neutralization Antibody Detection kit (Genscript, https://www.genscript.com ). Serum samples first were tested by using a traditional VNT; 1 pig (20–07) generated neutralizing antibody titers, albeit weak, at a 1:5 dilution with a 70% reduction of plaques at both 13 dpi and 15 dpi ( Table 5 ). Consequently, the sVNT assay identified the same animal, pig 20–07, as antibody-positive with 0.188 µg/mL antibody at 15 dpi. A second pig (20–14) generated antibodies at 11 dpi (0.113 µg/mL) and 13 dpi (0.224 µg/mL). We also used sVNT to identify secreted antibody in oral fluids. Of note, at 6 dpi we detected positive antibody (0.133 µg/mL) from group oral fluid collected from cubicle 1 ( Table 5 ).\n\n【39】### Discussion\n\n【40】Our study found that domestic swine are susceptible to low levels of SARS-CoV-2 viral infection. Among 16 experimentally inoculated animals, 5 (31.3%) displayed some level of exposure or elicited an immune response to the virus. Only 1 pig in our study retained live virus, but 2 other animals had detectible RNA measured in nasal wash, and another 2 developed antibodies. One pig (20–06) displayed mild, nonspecific clinical signs, including coughing and depression. Then, over the 9 days between cessation of clinical signs and postmortem evaluation, we found this pig maintained the virus in the submandibular lymph node, but virus was undetected in other samples from this animal. In addition, multiple pigs demonstrated mild ocular and nasal discharge that appeared during the immediate, postinfection period. Of note, among 5 animals with potential infection, we detected only low levels of viral RNA; no live viral shedding was identified.\n\n【41】After detection of viral RNA in group oral fluids collected by rope chews at 3 dpi, we detected secreted antibody by using sVNT; we detected viral RNA in the same sample type at 6 dpi. The amount of antibody measured in oral fluids from swine would be considered below a protective cutoff based on comparisons to classical neutralizing titers, however the discovery of secreted antibody in oral fluids might be useful for surveillance efforts. This finding also demonstrates the possibility that human saliva should be evaluated as a less invasive method to provide accompanying evidence with serosurveillance studies for exposure to SARS-CoV-2.\n\n【42】The results of this study contradict previous reports indicating swine are not susceptible to SARS-CoV-2 infection ( _4_ , _36_ ). Previous studies did not detect RNA in swabs or organ samples, and no seroconversion was measured. Infectious dose, viral isolate, age, and breed or colony of swine could affect study outcomes. Of note, we used a 10-fold higher viral dose for experimental infection than was used in previous studies. Moreover, we obtained animals from a high health status farm in Manitoba, rather than a specific pathogen–free colony, to determine the risk to farmed pigs in Canada. Altogether, these findings indicate that further investigations into the susceptibility of additional domestic livestock species should be conducted to assess their risk for infection and zoonoses. Finally, we emphasize that to date no SARS-CoV-2 cases among domestic livestock have been documented by natural infection; however, the results of this study support further investigations into the role that animals might play in the maintenance and spread of SARS-CoV-2.\n\n【43】This article was preprinted at https://biorxiv.org/cgi/content/short/2020.09.10.288548v1 .\n\n【44】Dr. Pickering is the head of the Special Pathogens Unit at the National Centre for Foreign Animal Disease with the Canadian Food Inspection Agency. His research focuses on high consequence pathogens including both emerging and reemerging zoonotic diseases of veterinary importance.\n\n【45】Author contributions: B.P. conceived the research. B.P, G.S., M.M.P., E.M, P.M., and C.E.L. performed the experiments. B.P, G.S., M.M.P., C.E.H., and C.E.L. analyzed the data. B.P. wrote the manuscript with input from B.P, G.S., M.M.P., E.M., and C.E.L. All authors discussed the results and reviewed the manuscript.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "69a88196-bd2a-41fa-9548-29018c0545cc", "title": "Erratum, Vol. 03, October 2006 Release", "text": "【0】Volume 10 — May 09, 2013\n\n【1】### Article Tools\n\n【2】*   PDF - 101 KB\n*   Email\n*   Print\n*   Send feedback to _PCD_\n\n【3】### ERRATUM  \n\n【4】Erratum, Vol. 03, October 2006 Release\n======================================\n\n【5】_Suggested citation for this article:_ Erratum, Vol. 03, October 2006 Release. Prev Chronic Dis 2013;06\\_0011e. DOI: http://dx.doi.org/10.5888/pcd10.060011e .\n\n【6】In the article “Identifying Supports and Barriers to Physical Activity in Patients at Risk for Diabetes,” formulas for calculating the mean of scales were incorrect. The formulas appear in the Appendix under the heading “Scoring for IPAI.” Brackets were omitted for the numerators.\n\n【7】In the individual domain, the corrected formulae are the following:\n\n【8】Low priority scale = \\[a(R) + c(R) + d(R) + g(R)\\]/ 4\n\n【9】Weight control scale = \\[e + f\\]/2\n\n【10】Injury concerns scale = \\[h(R) + j(R) + k(R)\\]/3\n\n【11】In the support domain, the corrected formula is “Little support scale = \\[l +m +n+o\\]/4.”\n\n【12】In the environmental domain, the corrected formulae are “No place for activity scale = \\[q + r + s(R)\\]/3” and “No time for activity scale = \\[t + u(R)\\]/2.”\n\n【13】Corrections were made to our website on April 29, 2013, and appear online at http://www.cdc.gov/pcd/issues/2006/oct/06\\_0011.htm . We regret any confusion or inconvenience this error may have caused.\n\n【14】Top of Page\n\n【15】* * *\n\n【16】  \nThe opinions expressed by authors contributing to this journal do not necessarily reflect the opinions of the U.S. Department of Health and Human Services, the Public Health Service, the Centers for Disease Control and Prevention, or the authors' affiliated institutions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3b1ecc19-88f5-420b-bb3b-d1568d122707", "title": "Skin Exposures and Effects", "text": "【0】Skin Exposures and Effects\n### Overview\n\n【1】Some chemicals can enter the body through the skin and injure individual organs or groups of organs. Absorbing workplace chemicals through the skin can cause diseases and disorders that include occupational skin diseases.\n\n【2】Most efforts to address chemical hazards have focused on breathing chemicals in rather than absorbing through skin. Because of this, researchers have developed many ways to assess the effects of chemicals that workers inhale. However, there are fewer methods to assess skin exposures.\n\n【3】On This Page\n\n【4】*   Occupations at Risk\n*   Dermal Absorption\n*   Contact Dermatitis\n\n【5】Skin Notation Profiles\n\n【6】NIOSH assigns skin notations . They describe what happens when chemicals contact the skin and directly damage organs or prompt the body’s immune response. The immune response causes skin conditions like allergic contact dermatitis or skin irritation.\n\n【7】Occupational skin diseases are one of the most common types of workplace diseases. They can take different forms, such as:\n\n【8】*   Rash (contact dermatitis) caused by skin irritation\n*   Rash caused by skin allergies\n*   Skin cancers\n*   Skin infections\n*   Skin injuries\n*   Other skin diseases\n\n【9】### Occupations at Risk\n\n【10】Workers in multiple industries and sectors may face risks of absorbing chemicals through their skin.\n\n【11】### Dermal Absorption\n\n【12】Dermal absorption happens when a chemical goes through the skin and travels into the body. Many chemicals used in the workplace can damage organs if they penetrate the skin and enter the bloodstream. Examples of these chemicals include pesticides and organic solvents.\n\n【13】How fast the skin absorbs chemicals depends largely on the outer layer of the skin called the stratum corneum _._ The stratum corneum provides a barrier by keeping molecules from passing into and out of the skin. This barrier protects the lower layers of skin. How much the skin absorbs chemicals depends on the following factors:\n\n【14】*   Skin integrity (damaged or intact)\n*   The place on the skin where the chemicals are absorbed. (This includes features like thickness and water content of stratum corneum and skin temperature.)\n*   Physical and chemical properties of the chemical\n*   Concentration of the chemical on the skin surface\n*   How long the skin absorbs the chemical\n*   The surface area of skin that absorbs the chemical\n\n【15】### Contact Dermatitis\n\n【16】Contact dermatitis, also called eczema, happens when the skin becomes inflamed from contact with a chemical. Contact dermatitis is the most common form of reported occupational skin disease. Research suggests that contact dermatitis makes up 90–95% of occupational skin disease cases in the United States. Common symptoms of dermatitis include the following:\n\n【17】*   Itching\n*   Pain\n*   Redness\n*   Swelling\n*   Small blisters or wheals (itchy, red circles with a white center) on the skin\n*   Dry, flaking, scaly skin that may crack\n\n【18】##### Occupational contact dermatitis is often placed into two categories:\n\n【19】1.  **Irritant contact dermatitis** involves a chemical directly damaging and inflaming skin. The damage often occurs only to the place where the skin absorbs the chemical. The following factors may cause irritant contact dermatitis:\n\n【20】*   Phototoxic responses (such as tar)\n*   Brief contact with highly irritating chemicals (such as acids, bases, or oxidizing/reducing agents)\n*   Mild irritants that cause damage over time (such as water, detergents, or weak cleaning agents).\n\n【21】1.  **Allergic contact dermatitis** happens when a chemical that the skin absorbs causes the body’s immune system to produce an allergic reaction. For this to happen, a worker must be sensitive or allergic to the allergen. When the skin absorbs the chemical allergen again, the chemical may cause an immune reaction that inflames the skin. The reaction can go beyond the place where the skin absorbed the chemical. The following factors may cause allergic contact dermatitis:\n\n【22】*   Industrial compounds (such as metals, epoxy and acrylic resins, or rubber additives)\n*   Agrochemicals (such as pesticides and fertilizers)\n*   Commercial chemicals\n\n【23】The symptoms and the appearance of irritant contact dermatitis and allergic contact dermatitis are similar. Clinical testing (such as patch testing) is often necessary to tell the difference between them. Contact dermatitis can be mild or severe, depending on many factors that include the following:\n\n【24】*   Characteristics of the chemical\n*   Concentration of the chemical\n*   How often the skin absorbs the chemical\n*   Environmental factors, such as temperature or humidity\n*   Condition of the skin, such as healthy or damaged, dry, or wet\n\n【25】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "21530d2a-4d05-4fda-9bc2-335a0c4e12eb", "title": "Survey of Treponemal Infections in Free-Ranging and Captive Macaques, 1999–2012", "text": "【0】Survey of Treponemal Infections in Free-Ranging and Captive Macaques, 1999–2012\nYaws, an endemic tropical disease distinguished by bone and skin lesions, is caused by infection with _Treponema pallidum_ subsp. _pertenue_ treponemes. Successful yaws treatment campaigns during 1950–1965 were followed by a resurgence of disease, and the World Health Organization (WHO) consequently mounted a yaws eradication campaign ( _1_ ). Although the agent of yaws is spread among humans via direct contact, research has shown that nonhuman primates (NHPs) may serve as mammalian host reservoirs with the potential for zoonotic transmission ( _2_ ). Successful eradication campaigns depend on there being no reservoir shielding the agent from eradication efforts; thus, the role that NHPs play in yaws among humans must be determined ( _3_ ).\n\n【1】African Old World primates (OWPs) can be infected by _T. pallidum_ and exhibit symptoms of yaws ( _2_ ). Of note, the _Treponema_ Fribourg-Blanc strain (isolated from a baboon in western Africa in 1966) exhibits remarkable genetic similarity to strains that cause yaws in humans ( _4_ ) and in experiments, was shown capable of infecting humans ( _5_ ). More recently, studies focusing on treponemal infections among NHPs in eastern Africa and the Republic of Congo showed that the NHP geographic range overlaps considerably with areas having a formerly high prevalence of yaws in humans ( _2_ ).\n\n【2】Macaques ( _Macaca_ spp.), OWPs native to Asia and northern Africa, are susceptible to and have been experimentally infected with _T. pallidum_ ( _6_ ). After the initial WHO eradication efforts, yaws was believed to be largely eliminated from countries of mainland Asia, although reporting and active case detection have not been uniform throughout the region ( _7_ ). Several island nations in Asia, however, continue to report active human yaws cases ( _8_ , _9_ ).\n\n【3】Macaques, the most widely distributed and numerous NHPs in the world, are sympatric with humans throughout Asia, thriving in human-altered environments and commonly kept as pets. To further characterize the role NHPs might play in the maintenance of _T. pallidum_ subspecies, we screened an extensive archive of serum samples collected from free-ranging and captive macaques.\n\n【4】### The Study\n\n【5】As part of a project characterizing the pathogen landscape among macaques and humans, we collected blood samples from NHPs during 1999–2012 and stored them at −80°C ( _10_ ). We retrospectively screened samples from 734 macaques representing 13 species distributed throughout the animal’s natural geographic range ( Table 1 ). Study protocols were approved by the University of Washington Institutional Animal Care and Use Committee (no. 4233–01) and adhered to the American Society of Primatologists Principles for the Ethical Treatment of NHPs ( https://www.asp.org/society/resolutions/EthicalTreatmentOfNonHumanPrimates.cfm ).\n\n【6】Figure\n\n【7】Figure . Individual sampling sites where macaques were tested for infection with _Treponema_ spp. during 1999–2012 and the number of human yaws cases during 2001–2011, Sulawesi, Indonesia. Numbers in parentheses indicate number nonhuman...\n\n【8】We used a Macro-Vue RPR Card Test Kit (BD, Franklin Lakes, NJ, USA) to screen the 734 blood samples; 11 (1.5%) were positive ( Table 2 ). The RPR (rapid plasma reagin) test, a lipoidal test (nontreponemal) for IgG and IgM typically associated with treponemal infection, can occasionally elicit nonspecific responses. To confirm RPR-positive samples, we used ESPLINE TP (Fujirebio, Tokyo, Japan), an enzyme immunoassay for measuring reactivity to 2 recombinant _T. pallidum_ antigens, Tp47 and Tp17. ESPLINE TP and RPR tests have been validated for use in OWPs ( _11_ ). Of the 11 RPR test–positive samples, 1 was from Singapore; 2 from Bali, Indonesia; and 8 from Sulawesi, Indonesia. Six samples (all from Sulawesi) yielded confirmatory positive results on the ESPLINE TP assay. Of note, in Sulawesi, the only positive macaques were pets sampled from South Sulawesi and West Sulawesi Provinces, which make up the island’s southwestern peninsula ( Figure ). We also used ESPLINE TP to test the 28 RPR test–negative samples from Sulawesi’s southwestern peninsula; none tested positive.\n\n【9】At the time of sampling, the macaques underwent a physical examination, including close inspection of head, trunk, extremities, oral cavity, and genitals. We conducted a retrospective review of the data and found that none of the macaques had lesions typical of treponemal infection ( _4_ ). Of the 734 macaques, 13, including 2 seropositive macaques from Sulawesi’s southwestern peninsula, had hypopigmentation on the palms of their hands, feet, or both. Hypopigmentation is rarely seen in yaws but is a common manifestation of pinta, which is caused by infection with _T. carateum_ , a close relative of _T. p._ _pertenue_ .\n\n【10】### Conclusions\n\n【11】Our findings show that pet macaques in Southeast Asia can be infected with _Treponema_ spp. related to those that infect humans. The overall prevalence of infection was low in our survey, but the pocket of infection detected among pets in Sulawesi’s southwestern peninsula is noteworthy. The demonstration of reactivity in the serologic tests provides unequivocal evidence that the macaques had been infected with _T. pallidum_ or a highly related pathogen. We had hoped to amplify a portion of _tp0548_ , a locus in the _T. pallidum_ genome used for molecular typing, but no amplifiable pathogen DNA was found in the whole-blood samples that had been held in storage for >10 years. Therefore, we could not determine whether the treponemal strains from NHPs in Sulawesi resembled strains that cause human yaws.\n\n【12】Sulawesi, the third largest island in the Indonesian archipelago, has a population of ≈17 million persons and 7 endemic macaque species. The seropositive samples from South Sulawesi and West Sulawesi Provinces were collected in July and August of 2000, immediately predating an active yaws outbreak among humans in the region that caused 241 documented cases in the neighboring southeastern peninsula during 2001–2011 (WHO, http://apps.who.int/iris/bitstream/10665/75528/1/WHO\\_HTM\\_NTD\\_IDM\\_2012.2\\_eng.pdf ) ( Figure ). During that outbreak, WHO characterized the South Sulawesi and West Sulawesi Provinces as “data deficient” regions in regard to the status of yaws among the human population. Most macaques whose samples were used in this study were free-ranging, but all of the macaques sampled in South Sulawesi and West Sulawesi Provinces had been captured at a young age for use as pets. The association between humans and pet macaques is often intimate, with the sharing of food; space; and physical contact through grooming, play, or aggression ( _12_ ). Two of the _Treponema_ spp.–infected pets were owned by the same person and housed together. Studies of pet macaques in Sulawesi and their owners have indicated that infectious agents can move between these populations ( _12_ , _13_ ). Although the treponemal serologic status of the pet owners in this study is unavailable, the fact that seropositive pet NHPs from a region neighboring an area with a high number of human yaws cases suggests that the NHP cases resulted from treponeme transmission from humans to pets.\n\n【13】All macaques in this study, with the exception of _M. sylvanus_ from Gibraltar, were from historically yaws-endemic areas where WHO conducted past yaws eradication campaigns. Much of Asia has a rich tradition of human–NHP commensalism, and macaques are common in villages, often as pets ( _10_ ). Moreover, we previously showed that macaques can harbor an array of mammalian picornaviruses, astroviruses, and mycobacteria ( _13_ – _15_ ), underscoring the role of macaques in the ecology of these pathogens. However, as with our current study of treponemal infections, definitive evidence for transmission and the direction of transmission have not been established for these pathogens.\n\n【14】Our findings of treponemal infections among macaques in Southeast Asia, along with published work showing infection in NHPs in Africa ( _4_ ), should encourage holistic and One-Health approaches to eradication and surveillance activities, including consideration of monitoring NHPs in yaws-endemic regions. Such approaches are particularly relevant for pet NHPs, which can easily be assessed and treated. The human–NHP interface is ancient and complex, and continued research, particularly in yaws-endemic regions, can help to ameliorate concerns as a second WHO yaws eradication campaign moves forward.\n\n【15】Dr. Klegarth is a postdoctoral research associate in the Evolutionary Emergence of Infectious Diseases Laboratory at the University of Washington. Her research is focused on human–wildlife conflict, with an emphasis on urban nonhuman primates, and the infectious agents that are transmitted at this interface.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "05f8a3a7-bb86-44ea-8f67-7c2248794c0e", "title": "PCD Disseminates Public Health Interventions Addressing Chronic Disease Prevention and Health Promotion", "text": "【0】PCD Disseminates Public Health Interventions Addressing Chronic Disease Prevention and Health Promotion\n=======================================================================================================\n\n【1】EDITOR IN CHIEF'S COLUMN — Volume 16 — May 9, 2019\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Leonard Jack Jr, PhD, MSc\n\n【5】_Suggested citation for this article:_ Jack L Jr. PCD Disseminates Public Health Interventions Addressing Chronic Disease Prevention and Health Promotion. Prev Chronic Dis 2019;16:190123. DOI: http://dx.doi.org/10.5888/pcd16.190123 external icon .\n\n【6】On This Page\n\n【7】*   Author Information\n*   References\n\n【8】Leonard Jack Jr, PhD, MSc\n\n【9】  \nWe were fortunate to have a productive, innovative, and successful year in 2018 (1–8). In 2019 we want to build on that success and explore new ways to bring the best in public health research, policy, and practice to our readers. Last year saw the introduction of 2 new manuscript types — Implementation Evaluation and Program Evaluation Briefs — to help us better identify submissions that provide careful, thoughtful, and rigorously conducted research and evaluation findings derived from the application of population-based approaches.\n\n【10】This year PCD is continuing to review and evaluate the best article types and formats for sharing important work by identifying quality manuscripts that report findings from researchers and practitioners working in various settings. From the beginning, PCD’s mission has been to recognize the insights that can be gained outside of traditional research and evaluation models. We understand and appreciate that there are myriad ways practitioners, researchers, evaluators, and policy makers create and implement promising practices, from strategies and programs to trainings to system changes. With all of this in mind, PCD introduced a new article type in February: Public Health Practice Briefs. These articles highlight public health practices outside of traditional research and evaluation. We believe that the inclusion of a practice-focused article type fills a niche in the public health literature and gives potential PCD authors maximum flexibility in choosing the best format to showcase their work.\n\n【11】Last year PCD set a publication record, publishing 5 collections addressing a diversity of topics, from state and local public health actions to childhood obesity to health disparities and implementation evaluation, in addition to our regular collection devoted to PCD’s Student Research Paper Contest. Many of these topics were a response to PCD’s expert panel recommendations to expand and explore new public health topic areas. In 2019 PCD expects to break the record set in 2018, with more collections planned that continue to address the recommendations of PCD’s external review panel recommendations. Here are some of the highlights for 2019:\n\n【12】**CDC’s Colorectal Cancer Control Program.** Colorectal cancer is the second most common cause of cancer death among cancers that affect both men and women (1). The collection, expected in August, shares findings covering 2009 through 2015 from 25 states and 4 tribal organizations that were funded to implement evidence-based interventions to increase colorectal cancer screening rates in adults. This collection will include articles that describe the Colorectal Cancer Control Program, outcomes from the direct screening portion of the program, use of evidence based-interventions, and economic evaluation outcomes.\n\n【13】**Population health, place, and space: spatial perspectives in chronic disease research and practice.** Expected in October is PCD’s first collection sharing articles that highlight the use of geospatial information systems to increase understanding of the role of place and space in shaping the distribution of chronic disease to aid in identifying appropriate public health responses for chronic disease prevention and treatment. This PCD special collection documents research, translation, case studies, and analytic tools that have a spatial perspective and highlights the innovative and effective incorporation of place and space into chronic disease surveillance, prevention, and treatment.\n\n【14】**National Center for Chronic Disease Prevention and Health Promotion: programmatic efforts to reduce risk factors for chronic diseases.** The National Center for Chronic Disease Prevention and Health Promotion (NCCDPHP) works to reduce the risk factors for chronic diseases, especially for groups affected by health disparities, that is, differences in health across geographic, racial, ethnic, and socioeconomic groups (2). NCCDPHP has a rich portfolio of public health efforts to reduce the potential of developing chronic disease and to improve the health and quality of life for those living with chronic disease. PCD plans to publish later this year a collection on program evaluation efforts that offer insights into the development, implementation, and evaluation of population-based interventions to prevent chronic diseases and to control their effects on quality of life, morbidity, and mortality. Articles in this collection will be generated by NCCDPHP, its partners, and its awardees.\n\n【15】**Health care systems, public health, and communities: population health improvements.** Reducing the burden of chronic disease is a global challenge requiring diverse collaborations and diffusion and adoption of effective interventions in multiple settings. The past decade has seen a range of innovative community-driven and clinically driven primary and secondary prevention strategies designed to prevent and reduce the burden of chronic conditions worldwide. PCD plans to publish a collection on health care systems, public health, and communities that provides research, evaluation, and other work describing innovative and effective ways to link health care and community health to improve population health.\n\n【16】**Reducing obesity in high-obesity areas.** Obesity is a major public health challenge. In 2014, the Division of Nutrition, Physical Activity, and Obesity at the Centers for Disease Control and Prevention supported 11 land grant institutions in states that had at least one county with an adult obesity rate over 40%. The goal of this program was to implement evidence-based strategies at the state and community levels to improve environments for good nutrition and increased physical activity. This collection will contain articles providing information on partnerships, community participatory action, and programmatic activities implemented in counties with high rates of obesity among its adult populations.\n\n【17】**Good health and wellness in Indian country.** American Indians and Alaska Natives have a high rate of chronic diseases, which requires sustained culturally appropriate approaches to ameliorate. This collection will feature articles describing various aspects of the Good Health and Wellness in Indian Health Program — a culturally appropriate and coordinated program designed to implement chronic disease prevention and health promotion in American Indian and Alaska Native communities.\n\n【18】**Public Health and pharmacy: collaborative approaches to improve population health.** Reducing the burden of chronic disease is a global challenge requiring diverse collaborations and dissemination and adoption of effective interventions in multiple settings. Over the past decade a range of innovative community and clinical prevention strategies have been used in public health and pharmacy to prevent and reduce the burden of chronic conditions. Early this year, PCD announced a call for papers that provide timely information on effective ways the disciplines of public health and pharmacy can collaborate to improve the nation’s health and improve population health globally (https://www.cdc.gov/pcd/announcement.htm). PCD welcomes articles that describe collaboration among public health agencies, schools of pharmacy, community pharmacies, health care partners, and others to implement clinical and nonclinical strategies; approaches that pharmacies and pharmacists can use to improve population health; and ways health care and public health data can guide the integration of clinical and public health approaches to chronic conditions. Manuscripts for this collection are due to PCD on or before Friday, October 31, 2019.\n\n【19】We are enthusiastic about these collections and the opportunities they give us to delve deeper into these public health areas. We will keep our readers up to date on the progress of these collections and welcome any suggestions you may have for future collections. On behalf of all PCD staff members, editorial board members, associate editors, and peer reviewers, we thank you for interest in the work of the journal. We also thank the many authors who contributed to collections in 2018 and made the year such a successful one for the journal. We look forward to an exciting 2019 and to working closely with authors to continue our mission to publish relevant and timely articles of the highest quality.\n\n【20】Top\n\n【21】Author Information\n------------------\n\n【22】Corresponding Author: Leonard Jack, PhD, MSc, Preventing Chronic Disease: Public Health Research, Practice, and Policy, Office of Medicine and Science, National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, 4770 Buford Hwy NE, Mailstop F-80, Atlanta, GA 30341. Email: ljack@cdc.gov .\n\n【23】.\n\n【24】Top\n\n【25】References\n----------\n\n【26】1.  Jack L Jr. Using PCD’s first-ever external review to enhance the journal’s worldwide usefulness to researchers, practitioners, and policy makers. Prev Chronic Dis 2018;15:E41. CrossRef external icon PubMed external icon\n2.  National Center for Chronic Disease Prevention and Health Promotion. https://www.cdc.gov/chronicdisease/index.htm. Accessed March 20, 2019.\n3.  Jack L Jr. PCD advances recommendations from first-ever external review. Prev Chronic Dis 2018;15:E111. CrossRef external icon PubMed external icon\n4.  Jack L Jr. PCD increases content about public health approaches being implemented to improve population health. Prev Chronic Dis 2018;15:E164. CrossRef external icon PubMed external icon\n5.  State and local public health actions to prevent and control chronic diseases. 2018. https://www.cdc.gov/pcd/collections/pdf/PCD\\_StateAndLocal\\_Collection\\_FINAL\\_3-27-18.pdf. Accessed March 20, 2019.\n6.  Student research paper contest. 2017. https://www.cdc.gov/pcd/collections/pdf/PCD\\_2017StudentPapersCombined.pdf. Accessed March 20, 2019.\n7.  Carrie D, Brook B, Heidi B. The Childhood Obesity Research Demonstration (CORD) Project. https://www.cdc.gov/pcd/collections/pdf/CORD\\_Collection.pdf. Accessed March 20, 2019.\n8.  Eliminating health disparities. 2018. https://www.cdc.gov/pcd/collections/Health\\_Disparities\\_Collection\\_2018.htm. Accessed March 20, 2019.\n9.  Promoting the science and practice of implementation evaluation in public health. 2018. https://www.cdc.gov/pcd/collections/pdf/PCD\\_IE\\_Collection\\_final\\_Dec-2018.pdf. Accessed March 20, 2019.\n\n【27】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8bf7168a-2d5f-40a5-822d-dc2c4976478c", "title": "Differentiation of Prions from L-type BSE versus Sporadic Creutzfeldt-Jakob Disease", "text": "【0】Differentiation of Prions from L-type BSE versus Sporadic Creutzfeldt-Jakob Disease\nAmong transmissible spongiform encephalopathies (TSEs), the L-type bovine spongiform encephalopathy (L-BSE) in cattle requires particular attention for public health. L-BSE is transmitted more efficiently than is classical BSE among primates ( _1_ – _3_ ) as well as among transgenic mice that express human prion protein (PrP) ( _4_ , _5_ ). We recently reported that L-BSE was readily transmissible by experimental oral inoculation in a nonhuman primate species, the grey mouse lemur ( _Microcebus murinus_ ) ( _3_ ). These findings raise the possibility that some human Creutzfeldt-Jakob disease (CJD) cases might result from exposure to the L-BSE agent; previous studies highlighted similarities between L-BSE and some human subtypes (type 2) of sporadic CJD (sCJD) ( _1_ , _6_ ).\n\n【1】To examine the possible relationship between L-BSE and sCJD, we evaluated a strain-typing strategy that relies on comparative transmission characteristics in the Syrian golden hamster and in a transgenic mouse line (TgOvPrP4) expressing ovine PrP (ARQ allele). Both of these species are susceptible to L-BSE prions from cattle ( _7_ , _8_ ). The transmission of L-BSE, including after a first passage in _Microcebus murinus_ lemurs ( _3_ ), was compared with that for the MM2-cortical subtype of sCJD ( _9_ ); this subtype was chosen on the basis of a study that indicated higher levels of molecular similarities of L-BSE with this sCJD subtype than with the MV2 subtype ( _1_ ).\n\n【2】### The Study\n\n【3】The TSE brain inocula used in this study, conducted during November 2010–December 2011, were derived from 2 natural L-BSE isolates from France (02-2528 and 08-0074); a lemur injected intracerebrally (i.c.) with the 02-2528 L-BSE cattle isolate ( _3_ ); and a human patient with MM2-cortical sCJD. Consent was obtained for using tissues from the human patient in research, including genetic analyses. Animal experiments were performed in the biohazard prevention area (A3) of the Anses-Lyon animal facilities, in accordance with the guidelines of the French Ethical Committee (decree 87-848) and European Community Directive 86/609/EEC.\n\n【4】Six-week-old TgOvPrP4 mice and 4-week-old Syrian golden hamsters were injected i.c. with 20 and 30 µL, respectively, of 10% (wt/vol) brain homogenates in 5% sterile glucose. Serial passages were performed in TgOvPrP4 mice by i.c. inoculation of 1% (wt/vol) homogenates from mice positive for protease-resistant PrP (PrP <sup>res </sup> ). At the terminal stage of the disease, animals were euthanized, and their brains and spleens were collected for PrP <sup>res </sup> analyses by Western blot and for histopathologic studies ( _8_ ).\n\n【5】Figure 1\n\n【6】Figure 1 . . . . Susceptibility of Syrian golden hamsters to MM2-cortical subtype sporadic Creutzfeldt-Jakob disease (sCJD) and L-type bovine spongiform encephalopathy (L-BSE) prions. Disease-associated prion protein (PrP <sup>d </sup> ) was analyzed in brains of...\n\n【7】In hamsters, transmission of the MM2-cortical sCJD agent was inefficient. Clinical signs were absent up to 876 days postinoculation (dpi) ( Table ), and disease-associated PrP (PrP <sup>d </sup> ) in brain samples was not detected by paraffin-embedded tissue blot (PET-blot) ( Figure 1 , panel A), immunohistochemical ( Figure 1 , panel C), or Western blot ( Figure 1 , panels E, F) analyses. PrP <sup>res </sup> was also undetectable in spleen tissues by Western blot ( Table ).\n\n【8】In contrast, the L-BSE agent passaged in a lemur was efficiently transmitted to hamsters, with a mean survival period of 529 ± 117 dpi, similar to that for L-BSE from cattle (622 ± 64 dpi) ( Table ). PET-blot analysis ( Figure 1 , panel B) showed widespread PrP <sup>res </sup> distribution in the brain; immunohistochemical analysis ( Figure 1 , panel D) showed a granular type of PrP <sup>d </sup> deposition that redefined the periphery of most of the blood vessels. Western blot analysis ( Figure 1 , panels E, F) showed PrP <sup>res </sup> in the brains of hamsters inoculated with L-BSE from cattle and lemur and in 1/4 spleens of hamsters injected with L-BSE passaged in lemur ( Table ). Brain PrP <sup>res </sup> was characterized by low apparent molecular mass (≈19 kDa for the unglycosylated band) associated with a lack of reactivity toward the N terminal 12B2 antibody, in contrast to that for the control animal with scrapie ( Figure 1 , panels E, F).\n\n【9】In TgOvPrP4 mice, all TSEs were efficiently transmitted, as confirmed by PrP <sup>d </sup> accumulation in the mouse brains ( Table ). After serial passages in additional TgOvPrP4 mice, the survival periods in each experiment became considerably shorter ( Table ; Technical Appendix Figure 1 ). No statistically significant differences in results were identified between the L-BSE sources (p>0.6). Mean survival period decreased to 111 ± 25 dpi at second passage in mice inoculated with the agent of MM2-cortical subtype sCJD, which differed significantly from that of mice inoculated with L-BSE (p<0.0001). A third passage of both cattle L-BSE and human sCJD did not reduce the survival periods in TgOvPrP4 mice (data not shown).\n\n【10】Figure 2\n\n【11】Figure 2 . . . . Western blot molecular typing of protease-resistant prion protein (PrP <sup>res </sup> ) in brain and spleen tissues of ovine prion protein–transgenic (TgOvPrP4) mice at second passage. PrP <sup>res </sup> from mice infected with...\n\n【12】Western blot analyses of PrP <sup>res </sup> from mouse brains showed partially similar features for MM2-cortical sCJD and L-BSE, including low molecular mass (≈19 kDa for the unglycosylated band) ( Figure 2 , panel A) and similar conformational stability of PrP <sup>d </sup> after treatment with guanidinium hydrochloride ( Technical Appendix Figure 2 ). However, the proportions of diglycosylated, monoglycosylated, and unglycosylated bands of brain PrP <sup>res </sup> differed between sCJD and L-BSE ( Figure 2 , panel C); higher proportions of diglycosylated PrP <sup>res </sup> were found in sCJD-infected mice (mean 67% of the total signal) compared with L-BSE–infected mice (≈18% lower; p<0.0001). PrP <sup>res </sup> was readily identified in the spleens of TgOvPrP4 mice at the second passage for sCJD and L-BSE from cattle and at the first passage for L-BSE from lemur ( Table ). No significant differences in the proportions of PrP <sup>res </sup> glycoforms for sCJD-infected versus L-BSE–infected mice were observed in the spleens ( Figure 2 , panel D), but PrP <sup>res </sup> was ≈0.5 kD higher in mice injected with sCJD ( Figure 2 , panel B, arrows).\n\n【13】Histopathologic analysis showed severe vacuolar lesions in TgOvPrP4 mice infected at second passage with sCJD and lemur-passaged L-BSE ( Technical Appendix Figure 3 ). However, in sCJD-infected mice, vacuolar lesions were mostly observed in the anterior parts of the brain (except the parietal cortex), whereas in mice infected with lemur-passaged L-BSE, the lesions were more widely distributed, involving the colliculi and the hypothalamus. In mice infected with sCJD and lemur-passaged L-BSE, PET-blot analyses showed that most of the PrP <sup>res </sup> occurred in the frontal parts of the brain, but the intensity and appearance of PrP <sup>res </sup> in the cortex, thalamus, and hippocampus were distinctly different. Immunohistochemical analyses of the hippocampus showed PrP <sup>d </sup> deposition in the dentate gyrus in sCJD-infected mice, in contrast to a lack of deposition in lemur-passaged L-BSE–infected mice.\n\n【14】### Conclusions\n\n【15】We report the isolation of 2 prion strains derived from L-BSE and MM2-cortical sCJD after transmission in Syrian hamsters and ovine PrP–transgenic mice. In hamsters, we did not transmit any disease with sCJD, but the transmission of L-BSE from lemur was efficient, as previously reported for L-BSE from cattle ( _7_ , _11_ ). This result suggests that L-BSE did not undergo major modifications after this cross-species transmission and could indicate a clear biologic difference between MM2-cortical sCJD and L-BSE. We also demonstrated the efficient transmission of both L-BSE and MM2-cortical sCJD in TgOvPrP4 mice, which enabled us to compare these diseases in a single model. Unexpectedly, during serial passages, we observed that the agent of MM2-cortical sCJD causes a much more rapidly fatal disease. Despite similar molecular features in sCJD and L-BSE, including the PrP <sup>res </sup> electrophoretic mobility and the conformational stability of PrP <sup>d </sup> , sCJD and L-BSE differed in PrP <sup>res </sup> glycosylation for the mouse brains and gel migrations for the mouse spleens. Mice infected with MM2-cortical sCJD versus those infected with L-BSE also showed distinct lesion profiles and PrP <sup>d </sup> distribution, which confirms clear biologic differences between these diseases.\n\n【16】Although only 1 case of sCJD of a unique molecular subtype was examined in our study, our observations do not support the hypothesis of a causal relationship between L-BSE and this human sCJD subtype. Our study thus encourages further investigations using the proposed bioassay approach for a more complete evaluation of possible relationships between L-BSE and human prion diseases.\n\n【17】Mr Nicot is a PhD student at the Agence Nationale de Sécurité Sanitaire in Lyon. His primary research interests include characterization of the infectious agents and prion protein during intra- and interspecies transmission of animal and human prion diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a26db2cb-04dd-4636-aaf6-07ecc933b65f", "title": "Aquatic Bird Bornavirus 1 in Wild Geese, Denmark", "text": "【0】Aquatic Bird Bornavirus 1 in Wild Geese, Denmark\nAvian bornaviruses were first identified as the probable causative agent of proventricular dilatation disease in parrots in 2008 ( _1_ , _2_ ). Eight psittacine viruses and 5 passerine viruses have since been described ( _3_ , _4_ ). In 2009, aquatic bird bornavirus 1 was detected in free-ranging Canada geese ( _Branta canadensis_ ) and trumpeter swans ( _Cygnus buccinator_ ) in Ontario, Canada ( _5_ ). Subsequently, this bornavirus has been detected across North America in at least 15 species of free-ranging wild birds ( _6_ , _7_ ). Initially designated as ABV-CG due to the high prevalence in Canada geese, the virus has been renamed aquatic bird bornavirus 1 (ABBV-1) in a reorganization of the taxonomy of the _Bornaviridae_ ( _4_ ). A second waterfowl-associated virus (ABBV-2) was isolated from ducks in North America in 2014 ( _8_ ). Despite the fact that North American and European waterfowl are known to share breeding grounds in the Arctic, avian bornaviruses had not been detected in wild birds outside North America. The purpose of this study was to investigate the presence of aquatic bird bornavirus 1 in wild waterfowl in Denmark.\n\n【1】### The Study\n\n【2】By using real-time reverse transcription PCR (RT-PCR), we screened brain tissue from 333 hunter-killed geese from 9 locations in Denmark ( Table ; Technical Appendix Figure), collected during November and December 2014. Each screened sample consisted of pooled tissue from 5 individual birds, of the same species, collected at the same location. Primers and probe targeting the matrix (M) gene specific for ABBV-1 ( _9_ ) were used. RNA was purified from 35 mg of pooled or individual brain samples with the QIAGEN RNeasy Mini Kit (QIAGEN, Copenhagen, Denmark), according to instructions from the supplier. Each PCR reaction contained 5 µL RNA; 1× RT-PCR buffer (AgPath-ID One-Step RT-PCR Kit; Life Technologies, Naerum, Denmark); 0.5 µmol/L of each primer, 0.25 µmol/L FAM-BHQ-1 labeled probe; and 1× RT-PCR enzyme mix in a total volume of 25 µL. The reactions were run on Rotor-Gene Q (QIAGEN) at 45°C, for 600 s, 95°C for 600 s, followed by 45 cycles of 94°C for 5 s and 60°C for 60 s. Data were analyzed with Rotor-Gene Q Series Software version 2.3.1 (QIAGEN). Parameters were adjusted as follows: dynamic tube, on; slope correct, on; ignore first cycle, 1; outlier removal, 10%; threshold fixed, 0.01. All other settings were default.\n\n【3】Samples from birds in positive pools were purified and tested individually by quantitative -PCR and confirmed by endpoint conventional RT-PCR with a 2.200-bp amplicon covering the nucleocapsid (N), X protein, phosphoprotein (P), and partial M genes. Primers were previously published ( _7_ ). Each reaction contained 1× buffer, 1.2 µmol/L of each primer, 0.4 µmol/L dNTP mix, 0.4 µmol/L enzyme mix (QIAGEN OneStep RT-PCR Kit; QIAGEN), and 5 µL purified RNA, in a total volume of 25 µL. Amplification was performed on a T3 PCR machine (Biometra, Fredensborg, Denmark) with cycling conditions 30 min at 50°C, 15 min at 95°C, and 120 s at 94°C, followed by 35 cycles of 30 s at 94°C, 30 s at 50°C, and 150 s at 68°C, and a final elongation at 68°C. Products were analyzed on 0.8% agarose E-Gels (Invitrogen, Naerum, Denmark) and verified by Sanger sequencing (LGC Genomics, GmbH, Berlin, Germany) with primers previously published ( _1_ ) and the following primers: 5′-CAGCTCCAGTAAGGTGAGTTG-3′, 5′-CGCCGACTAGTGGACAGCCC-3′, and 5′-CTGCGGCATTCTACTGGAG-3′. Sequence data were edited with CLC Main Workbench 7.0 (CLC bio, QIAGEN, Aarhus, Denmark).\n\n【4】Figure\n\n【5】Figure . Phylogenetic tree comparing aquatic bird bornavirus 1 sequences obtained from waterfowl in Europe with selected bornavirus sequences from GenBank. Bold indicates viruses isolated in this study. Numbers along branches indicate bootstrap...\n\n【6】Seven ABBV-1–positive brain samples from individual birds were identified ( Table ). These samples were from 3 species of geese originating in 6 of 9 locations sampled. The N, X, P, and partial M fragment sequences were aligned, and a neighbor-joining tree with 1,000 bootstrap replicates was constructed with the CLC software and edited by using FigTree version 1.4.2 ( http://tree.bio.ed.ac.uk/ ) ( Figure ). The European sequences from this study clustered together with North American ABBV-1, and a pairwise comparison in CLC software showed 98.2% to 99.8% identity among the 7 sequences and 97.4% to 98.1% identity to the reference strain of ABBV-1 (GenBank accession no. KF578398).\n\n【7】The 2.1% (95% CI 0.6%–3.6%) prevalence of ABBV-1 in wild geese in Denmark is considerably lower than the prevalences reported in North America, which average 10%–30% but in some studies have exceeded 50% ( _6_ , _7_ , _9_ , _10_ ). In 1 study, the prevalence of ABBV-1 was higher in stable nonmigrating populations of Canada geese than in migratory birds, suggesting that prevalence may vary with population density and intensity or duration of use of geographic locations. In the investigation described here, we surveyed only a limited number of species previously identified as positive for ABBV-1 but identified 3 novel host species. Thus, the low prevalence found in geese in Denmark might be due to the sampling of a high proportion of transient migratory and apparently healthy birds as well as a possible variation in species susceptibility.\n\n【8】The finding of ABBV-1 in migratory waterfowl in Denmark suggests that the virus is widespread in waterfowl populations in Europe, but further investigation is needed to verify this claim. Pink-footed geese ( _Anser brachyrhynchus_ ) and greylag geese ( _A. anser_ ) migrate through the western part of Europe from Svalbard, Norway, to Spain. Barnacle geese ( _A. leucopsis_ ) migrate from wintering grounds in the Wadden Sea area to northern Scandinavia and Russia. Because migrating waterfowl often gather in flocks of mixed species, transmission of pathogens between species is possible, and even likely, on the basis of our findings of nearly identical (99.7%) sequences in 1 pink-footed goose and in 1 greylag goose. The origin of ABBV-1 cannot be determined from this study, but the presence of highly homologous viruses in North America and Europe promotes speculation on possible transmission routes between these continents. Avian populations in Greenland could be the link between American and European flocks; the country is host to large breeding populations of geese that winter in both North America and in Europe.\n\n【9】The results here do not allow the clinical implications of ABBV-1 infections in waterfowl in Denmark to be determined, because none of the sampled geese were reported to be ill, and only goose heads were examined in the study. In North America, birds infected with ABBV-1 have exhibited nonsuppurative inflammation of the central, peripheral and autonomous nervous systems and associated neurologic and gastrointestinal clinical signs, including proventricular stasis.\n\n【10】### Conclusions\n\n【11】This study identifies ABBV-1 in wild geese in Europe; phylogenetic analyses demonstrated that the sequences from our investigation cluster with those from North America in the waterbird-1 cluster. The barnacle goose, greylag goose, and pink-footed goose were added to the list of waterfowl known to be hosts of ABBV-1. On the basis of the migration patterns of the affected species, we propose that the virus is distributed widely in Europe, but further investigation is needed to determine the validity of this hypothesis.\n\n【12】Drs. Thomsen and Nielsen recently received their doctor of veterinary medicine degrees from University of Copenhagen, Denmark. Their main interests are avian medicine and pathology, along with molecular detection of disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db2b031f-24d2-442f-b873-203a51bbd27d", "title": "SARS-CoV-2 in Nursing Homes after 3 Months of Serial, Facilitywide Point Prevalence Testing, Connecticut, USA", "text": "【0】SARS-CoV-2 in Nursing Homes after 3 Months of Serial, Facilitywide Point Prevalence Testing, Connecticut, USA\nNursing home residents represent a population highly vulnerable to the spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). In the midst of the coronavirus disease (COVID-19) pandemic, nursing homes account for a substantial proportion of total deaths attributed to the virus in the United States and globally ( _1_ – _3_ ). The high proportion of asymptomatic, presymptomatic, and atypical manifestations of COVID-19 in staff and elderly residents is a critical driver of widespread and rapid transmission of the virus ( _4_ – _6_ ). Facilitywide testing is a critical tool to identify such infections, particularly in lieu of effective vaccines or treatments early in a novel viral outbreak ( _7_ – _10_ ). Point prevalence surveys (PPSs) enable testing of populations at a specific point with the goal of isolating both infectious and exposed persons from unexposed, uninfected persons to prevent ongoing transmission.\n\n【1】Nursing homes in the state of Connecticut experienced a high burden of COVID-19 during the first surge of the pandemic. The first COVID-19 case was reported in a nursing home in Connecticut on March 15, 2020. Over the next 2 months, nursing homes accounted for 61.6% deaths in the state ( _6_ ). After an increase in testing resources and evidence of asymptomatic transmission, the Connecticut Department of Public Health (CT DPH) began PPS testing in early May, and PPS testing was formally recommended on May 11 and mandated weekly in staff effective June 14 ( _11_ , _12_ ). Facility staff were trained by public health practitioners to ensure proper separation (hereafter, cohorting) of infected, exposed, and uninfected unexposed persons after receiving PPS results and temporary exclusion of staff from the workplace ( _13_ , _14_ ). Because data were collected for public health surveillance, not research, institutional review board evaluation was not required.\n\n【2】We previously reported the results of the first round of PPS testing in a subset of Connecticut nursing homes, in which a high number and proportion of asymptomatic infections were detected ( _6_ ). We also discussed the rapid turnaround time from conduct of PPS and institution of cohorting in those initial PPSs, factors that probably contributed to the positive effect of PPSs in reducing transmission. In this observational study, we followed the same nursing homes as they conducted serial PPS testing. We describe 4 weeks of incidence data before initial PPSs and 12 weeks of follow-up data in which facilities underwent 1–11 additional PPSs. We also present the results of PPSs conducted in staff in the selected subset of nursing homes as well as from the first round of PPSs in nearly all (n = 196/212) nursing homes in the state.\n\n【3】### Methods\n\n【4】##### Nursing Home Selection\n\n【5】Due to limitations in testing resources at the start of PPS rollout, CT DPH prioritized specific nursing homes to receive test kits based on the size of their outbreaks and potential immediate effect of control measures. Of 212 nursing homes in the state, 34 conducted the first round of PPS testing on or before May 20, 2020, and were selected for extended follow-up in this study; 1 of these homes was COVID-19–naive and excluded from our previous study ( _6_ ). The homes selected for inclusion in this study were of average size and quality of nursing homes in the state, with an average of 135 licensed beds and quality rating of 3.58/5 stars ( _6_ ). By June 25, a total of 196 (92.5%) of 212 nursing homes throughout Connecticut had conducted \\> 1 round of resident PPS testing and were included for reporting of initial results.\n\n【6】##### PPS Testing, Cohorting, and Simultaneous Interventions\n\n【7】PPS involved molecular SARS-CoV-2 testing by nasopharyngeal swabs of all residents or staff in a facility within a short time period, in general 1 day ( _6_ ). The state of Connecticut mandated weekly PPS testing in staff to begin in the latter half of June. In mid-May, CT DPH recommended but did not mandate weekly PPS testing of residents after identification of a new nursing home–onset case until no new cases were detected in residents or staff for 14 days ( _11_ , _12_ ). These recommendations remained effective through the duration of the study period. Nursing homes were paired with affiliate hospitals or laboratories to help conduct PPS testing and ensure fast turnaround of results.\n\n【8】A primary goal of PPSs was to ensure rapid and comprehensive isolation and cohorting of infected persons and to enact other infection prevention and control (IPC) measures, such as contact tracing to identify exposures and temporary exclusion of infected staff from the workplace. We did not collect data on adherence to these measures in nursing homes.\n\n【9】COVID-19 cases were also detected between PPSs, primarily through selective screening of residents leaving or entering the facility, visiting healthcare settings, or experiencing relevant symptoms, and also through limited contact tracing. Many other IPC policies for nursing homes were enacted during the study period federally and in the state of Connecticut, which can be found in Appendix C of the CT DPH contracted report by Mathematica, Inc. ( _15_ ).\n\n【10】##### Data Extraction\n\n【11】Nursing home staff answered daily questionnaires in a web-based COVID-19 database maintained by CT DPH, through which we extracted data on daily case counts, deaths, and censuses. PPS results were confirmed with study investigators by telephone: nursing directors reported the results of tests given to residents or staff who did not have a prior diagnosis of COVID-19. Case dates correspond to the date of specimen collection. We were unable to follow up on how each lab and nursing home responded to inconclusive results: whether they repeated the test, acquired a new sample, or treated the result as positive. New cases excluded residents transferred in with a known SARS-CoV-2 infection. Case counts by town were obtained from the Connecticut COVID-19 portal ( _16_ ).\n\n【12】##### Incidence Rates Relative to First PPS\n\n【13】COVID-19 incidence rates were calculated for 3 time periods respective to each nursing home: 4 weeks before first PPS, day of first PPS (“day 0”), and 12 weeks after the first PPS. For each respective time period _X_ and nursing home, we used the following equation:\n\n【14】where person-days at risk on day _t_ was calculated as the resident census reported on day _t,_ subtracting the number of previous COVID-19 case-patients who had not died from complications of the disease by day _t._ The total number of cases and person-days at risk in nursing home _i_ was summed for all days within each time period _X_ . Because we could not follow individual persons over time, we used census data to account for the dynamic nature of nursing home populations. We compared the incidence rates in PPS in individual nursing homes in the 4 weeks prior and 12 weeks following first using the 2-sample Z-test for equality of proportions with Yates’ continuity correction.\n\n【15】##### Poisson Regression Model\n\n【16】We investigated the association between PPS and the trajectory of nursing home outbreaks while accounting for concomitant changes in community incidence and intrinsic variability between nursing homes. The number of new cases in nursing home on calendar day offset by person-days at risk on day _t_ was modeled as a Poisson regression:where _person\\_days <sub>it </sub>_ is as described previously;\n\n【17】_sum\\_community\\_IR <sub>it </sub>_ is the incidence rate per 100,000 population in the town in which nursing home _i_ is located over the past 14 days relative to day _t_ ( _17_ ); _day\\_of\\_first\\_PPS <sub>i </sub>_ is the date of the first PPS, included as a dummy variable to account for the substantial change in screening practices; and _time\\_interval\\_since\\_first\\_PPS <sub>i </sub>_ is treated as a categorical variable divided into 1–15, 16–30, 31–60, and 60–90 days. Categorical variables for day of the week and nursing home ID ( _α <sub>i </sub>_ were also included. The model did not exhibit evidence of overdispersion (deviance/degrees of freedom = 0.8), indicating that the Poisson model was appropriate. We conducted a sensitivity analysis to determine the impact of different lags of community incidence (0, 3, 7, 14, and 28 days) on model results; the sum of incidence over the previous 14 days was found to minimize the Akaike information criterion. Risk ratios were calculated by exponentiation of the relevant regression coefficients. Analyses and figures were executed in R version 3.5.1 ( https://www.r-project.org ).\n\n【18】### Results\n\n【19】##### PPS Implementation\n\n【20】In the 12 weeks of follow-up after initial PPSs, an average of 6.0 (range 1–10) follow-up PPSs in residents and 6.2 (range 2–10) total PPSs in staff were administered per nursing home, for a total of 198 follow-up surveys in residents and 205 surveys in staff in all 34 nursing homes ( Table 1 ). The average time between the first and second round of resident PPS testing was 30 days; average time between all subsequent PPSs was 9 days. Periods between staff PPSs were shorter than between resident PPSs ( Appendix Figure 1). The period between resident PPSs decreased over time, in part, because of additional state requirements and recommendations to conduct weekly resident testing in mid-July. Most (31/34) nursing homes in this study conducted \\> 1 PPS beyond the recommended threshold of 14 days after a positive case was detected. The total number of PPSs in residents and staff in each nursing home was not statistically associated with the nursing home quality rating.\n\n【21】##### Resident Cases Detected in Follow-Up Period\n\n【22】Before the first PPS, nursing homes had experienced an average of 36 COVID-19 cases (27.7% infected; range 0–81 cases, 0%–86.1% infected). A total of 601 cases were detected in these facilities during the first PPS, as previously described ( _6_ ). Approximately 1,775 (55.8%) of all residents in the study were assumed to be susceptible to infection after the first round of testing was complete.\n\n【23】Figure 1\n\n【24】Figure 1 . Coronavirus disease cases detected in consecutive PPSs in residents (A) and staff (B) in nursing homes, Connecticut, USA. The number of participating nursing homes for each survey is listed above...\n\n【25】After the initial round of PPS, a total of 44 resident cases were identified in all subsequent rounds of PPS testing, of which 9 (20.4%) were symptomatic at the time of testing ( Table 1 ). The probability of identifying additional cases through PPSs decreased significantly over subsequent PPSs: the second PPS identified 20 cases (n = 34 nursing homes), and subsequent PPSs identified an additional 8 (n = 33), 6 (n = 31), 4 (n = 28), 3 (n = 25), 2 (n = 22), 0 (n = 18), 0 (n = 9), 0 (n = 4), and 0 (n = 1) cases in residents ( Figure 1 ).\n\n【26】Figure 2\n\n【27】Figure 2 . Cumulative proportion of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infections in individual nursing homes over a 16-week study period relative to the first PPS, Connecticut, USA. Each colored line...\n\n【28】In between PPSs, 93 additional resident cases were also detected, of which 70 (75.3%) were symptomatic at the time of testing. Most (85, 90.3%) cases were identified during the longer period between the first and second round of PPS testing. More than half (60.2%) of cases were detected within 1 incubation period following the first PPS, when exposure in those persons had likely already occurred; that exposure rendered cohorting measures less effective. Further, there was a positive but nonsignificant correlation (p = 0.09) between the number of days between PPS and the number of cases identified in a nursing home ( Appendix Figure 2). Two nursing homes contained most of these cases, reporting 38 and 20 cases in the 44 days between their first and second PPS ( Figure 2 ; Appendix Figures 2, 3).\n\n【29】##### Temporal Patterns of Resident Infections\n\n【30】Nursing homes underwent initial PPS at different stages of outbreak severity ( Figure 2 ). After initial PPS, the proportion of residents infected in each nursing home plateaued for most facilities. In 41.2% of nursing homes, fewer than half of all residents were infected with SARS-CoV-2 by the end of the study period.\n\n【31】Figure 3\n\n【32】Figure 3 . Paired coronavirus disease incidence rate estimates relative to first PPS, Connecticut, USA. Dashed lines represent single nursing homes included in the study. Points represent the incidence in the 4 weeks...\n\n【33】The median incidence rates in nursing homes were 9.3 (95% CI 0.2–49.2) cases/1,000 at-risk person-days before the first PPS; 267.8 (95% CI 0–861.5) cases/1,000 person-days on the day of the first PPS, and 0.54 (95% CI 0–18.4) cases/1,000 person-days in the period after the first PPS. Incidence rates decreased (p<0.05) in 85% (29/34) of facilities following the implementation of PPSs ( Figure 2 ). Of the 4 nursing homes that experienced no significant change, 2 had <10 residents remaining susceptible to SARS-CoV-2 and 1 had not experienced any cases before the first PPS. Meanwhile, 2 nursing homes experienced large outbreaks of >10 cases after the first PPS, 1 of which experienced an increase in incidence rate of 8.3 cases/1,000 person-days ( Figure 3 ; Appendix Figure 3).\n\n【34】##### Accounting for Concurrent Changes in Community Incidence\n\n【35】Figure 4\n\n【36】Figure 4 . Coronavirus disease incidence rates in nursing homes (cases/1,000 person-days, red) and in towns and cities (cases/100,000 person-days, blue), Connecticut, USA. Incidence rates are aggregated for the 34 nursing homes in...\n\n【37】The population of the towns and cities in which the nursing homes were located experienced a contemporaneous decrease in community incidence during the study period ( Figure 4 ). Community incidence over the previous 2 weeks was associated with proportional changes in incidence in nursing homes (β <sub>1 </sub> \\= 0.98, 95% CI 0.84–1.11). After adjusting for community incidence and the change in screening practices, the implementation of serial PPSs was associated with a significant decrease in nursing home incidence rates of 77% (95% CI 71%–83%) in the first 15 days after the first PPS, 49% (95% CI 31%–63%) from days 16–30, 41% (95% CI 12%–60%) from days 31–60, and 80% (95% CI 64%–89%) reduction from days 61–90, compared with the pre-PPS period.\n\n【38】##### Staff Cases Detected in Follow-Up Period\n\n【39】Nursing homes identified 87 staff cases (6 inconclusive) or an average of 2.6 cases (SD 4.9) per facility in the follow-up period ( Table 1 ). The first PPS in 34 nursing homes identified 57 total staff cases, and subsequent PPSs (n = 33 nursing homes) identified an additional 15 (n = 34 nursing home’s staff tested), 5 (n = 33), 4 (n = 30), 5 (n = 27), 3 (n = 24), 1 (n = 17), 1 (n = 7), 0 (n = 5), and 0 (n = 2) staff cases ( Figure 1 ). Symptomatic status and cases counts identified outside of weekly PPSs were not ascertained. One nursing home was removed from staff testing results beyond the first PPS due to lack of verifiable data.\n\n【40】##### Statewide Initial PPS Testing\n\n【41】In the state of Connecticut, as of June 25, 2020, a total of 196 nursing homes had completed 1 round of PPS testing. In these initial single round of surveys, 12,336 residents were tested. A total of 1,733 tests (14.0%) were SARS-CoV-2 positive and an additional 70 tests were inconclusive. Of those with positive results, 1,537 (88.7%) were reported by facilities as having been asymptomatic at the time of testing. Follow-up for symptomatic status beyond the day of testing was not conducted.\n\n【42】### Discussion\n\n【43】We compiled a large dataset covering 16 weeks of public health surveillance data in nursing homes, documenting COVID-19 outbreaks in the 4 weeks before and 12 weeks after the start of repeated facility-wide PPSs. Several previous studies have also documented the successful implementation of PPS testing in multiple congregate living facilities in the context of COVID-19 outbreak control ( _4_ , _7_ , _8_ , _18_ – _26_ ). We describe a study of 34 facilities conducting 437 surveys in residents and staff and 35,133 nasopharyngeal swab tests, or an average of 13 PPSs per nursing home in residents and staff combined in a 12-week period. Selected nursing homes experienced a range of outbreak severities at the time of initial PPSs, yet all nursing homes experienced 1 or 0 cases in the final 4 weeks of follow-up. In addition, 29/34 (85%) nursing homes exhibited significant (p<0.05) decreases in incidence rates of SARS-CoV-2 infection in the 12-week follow-up period compared with the 4-week period before any PPS.\n\n【44】The initial round of PPS testing likely captured asymptomatic cases and residents with protracted viral shedding that had been missed in the pre-PPS period (and who may have been symptomatic at that time), as well as presymptomatic cases that would have been captured in the post-PPS period in lieu of PPSs ( _6_ , _27_ , _28_ ). To account for the change in screening practices, we compared trends in incidence rates before and after initial PPSs. The change in incidence rates of COVID-19 cases in nursing homes over the study period, especially in the period following the first round of PPS, coincided with a decrease in community cases. However, we found that, even after adjusting for community incidence and the change in screening practice, the decrease in incidence rates in nursing homes was significantly associated with the onset of PPSs (p<0.05 for all subsequent time divisions).\n\n【45】Most COVID-19 cases detected in the 12-week follow-up period were identified in the extended period, on average 30 days, between the first and second PPS. These cases were identified primarily through symptom screening; limited contact tracing; and other types of selective testing, including at the time of resident hospitalization or hemodialysis. We postulate that more frequent PPSs, especially between the first and second rounds of testing, may have improved outbreak control by enabling earlier cohorting and that the extended time period between PPS may have decreased the efficacy of this intervention overall. Our results also suggest that although introductions of the virus from staff, visitors, and patients undergoing outside procedures pose a substantial risk of seeding new outbreaks, nursing homes may be able to alter the trajectory of their outbreaks by rigorous case surveillance once an outbreak occurs, despite ongoing community transmission.\n\n【46】Our study’s limitations include that we were not able to incorporate a control group in this analysis because this work was done in the context of outbreak control, in which nearly all nursing homes in the state received PPSs over the follow-up period. Furthermore, we were not able to follow individual participants over time due to the dynamic nature of nursing home populations and limitations in public health surveillance capacity. Similarly, we were unable to collect data to track the implementation of cohorting and other behavioral and physical interventions after receiving test results from PPSs. Nonetheless, CT DPH staff called all facilities before the first PPS to assess knowledge of cohorting and train staff on appropriate cohorting; they also followed up on homes with continued transmission after the first PPS to evaluate adherence. Finally, we could not account for all concurrent interventions, including changes in visitation policies, staff cohorting practices, and PPE abundance, limiting the interpretability of the usefulness of repeated PPSs ( _15_ ).\n\n【47】We described the successful implementation of hundreds of repeated facilitywide PPSs in nursing homes. Although our findings cannot inform policies of asymptomatic testing of staff and residents as a preventive strategy, they suggest that PPSs is one of several effective tools in outbreak management, particularly in the context of low COVID-19 incidence in the general population. In addition to testing, outbreak control relied on use of PPE and other protective behaviors such as social distancing and limitations to visitation, successful cohorting of infected and exposed residents, exclusion of infected staff from the workplace, environmental modifications, and sustained IPC training ( _18_ ). Our work may motivate states to reserve financial resources for sustained, serial PPS testing in the context of outbreak control and other forms of IPC planning in long-term care. We urge policymakers to continue serial testing in congregate living facilities during the period of vaccine rollout because acquisition of immunity will take time and coverage rates may vary in facilities ( _9_ , _29_ , _30_ ). Optimal serial testing strategies in the post–vaccine rollout period will require additional study.\n\n【48】Hanna Y. Ehrlich is a PhD candidate at the Yale School of Public Health. Primary research interests include public health surveillance, public health policy, and the epidemiology of vectorborne and emerging diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "554d46d9-24ee-463a-846c-c6ccc57611a9", "title": "Dobrava-Belgrade Virus in Apodemus flavicollis and A. uralensis Mice, Turkey", "text": "【0】Dobrava-Belgrade Virus in Apodemus flavicollis and A. uralensis Mice, Turkey\nThe genus _Hantavirus,_ family _Bunyaviridae,_ contains human pathogenic viruses that cause hemorrhagic fever with renal syndrome (HFRS) and hantavirus cardiopulmonary syndrome ( _1_ ). HFRS in Europe is caused mainly by Puumala virus and different genotypes of Dobrava-Belgrade virus (DOBV) ( _2_ ). In Asia, Hantaan virus and Seoul virus cause most HFRS cases. Hantaviruses are enveloped viruses with a single-stranded 3-segmented RNA genome of negative polarity. The small (S), medium, and large genome segments encode the nucleocapsid protein, the glycoproteins Gn and Gc, and an RNA-dependent RNA polymerase, respectively.\n\n【1】Hantaviruses have been detected in various rodent, shrew, mole, and bat species ( _3_ ). They are transmitted to humans by inhalation of aerosols that are contaminated with urine, feces, and saliva of infected reservoir hosts. The human pathogenic DOBV was first isolated from a yellow-necked field mouse ( _Apodemus flavicollis_ ) and, subsequently, from a striped field mouse ( _A. agrarius_ ) and a Caucasian wood mouse ( _A. ponticus_ ) ( _1_ ). The association of DOBV with these different _Apodemus_ species seems to determine its human pathogenicity, with the _A. flavicollis–_ associated genotype Dobrava being the most life threatening ( _1_ ).\n\n【2】Figure 1\n\n【3】Figure 1 . . Regional map of Turkey showing the Bartin province (41°38′9′′N, 32°20′15′′E), where trapping of small mammals was conducted. Abbreviations indicate locations at which captures were performed: A, Akbas; B, Bogaz; H1,...\n\n【4】Few reports about hantavirus seroprevalence in human and rodent populations in Turkey occurred before 2009 ( _4_ , _5_ ). In February 2009, the first hantavirus outbreak among humans in this country was described in 2 provinces in the western Black Sea region ( _6_ ; Figure 1 ). DOBV-reactive antibodies were reported for 7 of 200 patients who had renal symptoms in a region near the Aegean Sea ( _7_ ). In 2010, DOBV RNA was detected by a nucleic acid test in urine from a person in Istanbul Province who was experiencing fatigue, diffuse pain, nausea, and vomiting ( _8_ ). The reservoir host(s) and virus strain(s) causing human infections on the Black Sea coast of Turkey remained unknown.\n\n【5】### The Study\n\n【6】After the hantavirus outbreak in 2009, a National Hantavirus Study Group was founded in Turkey. Coordinated by this group, in June 2009, a total of 173 rodents and 2 shrews were collected from 7 sites in the rural area of Bartin Province in the western Black Sea Region, where suspected human hantavirus cases had been reported ( Figure 1 ).\n\n【7】To determine the presence of DOBV in rodent tissue samples, we developed a single tube reverse transcription quantitative PCR (RT-qPCR) using the QuantiTect Probe RT-PCR kit (QIAGEN, Hilden, Germany) and novel primers and probe ( Technical Appendix Table). For that purpose, spleen, liver, and lung samples from each animal were pooled and homogenized in sterile phosphate-buffered saline (pH 7.0). RNA extraction was performed by using a commercial viral nucleic acid isolation kit (Vivantis, Selangor Darul Ehsan, Malaysia). The reaction mixture (total 25 μL) of the RT-qPCR assay consisted of 1.5 μmol/L of each primer, 0.8 μmol/L of the probe, and 5 μL of RNA. Positive (100 copies of DOBV genotype Dobrava isolate(s) segment containing control plasmid) and negative control (ultrapure water) reactions were also included in each test. Thermal cycling was carried out in a RotorGene 6000 (QIAGEN) at 50°C for 30 min, 95°C for 15 min, then 45 cycles of 15 s at 94°C, 30 s at 60°C, and 1 min at 72°C.\n\n【8】The RT-qPCR revealed 23 DOBV RNA-positive rodents, including 10 _A. flavicollis mice_ , 12 _A. uralensis_ mice, and 1 _Rattus rattus_ rat ( Tables 1 , 2 ). The viral RNA load varied from 7.2 × 10 <sup>2 </sup> to 4.6 × 10 <sup>6 </sup> copies/mL. Most DOBV-positive rodents were from neighboring trapping sites in Akbas, Bogaz, and Kumluca Districts of Bartin Province ( Figure 1 ). The reason hantavirus RNA could not be detected in bank voles ( Table 1 ) might be because a DOBV-specific RT-qPCR was used that is unable to detect bank vole–associated PUUV.\n\n【9】Serum samples from all 173 rodents were screened by a commercial enzyme immunoassay (Hantavirus DxSelect; Focus Diagnostic, Cypress, CA, USA) that used anti-mouse IgG and IgM conjugates (Sigma-Aldrich Chemie, Taufkirchen, Germany). The assay found that 11 (6.4%) and 36 (20.8%) animals contained hantavirus-reactive IgM and IgG, respectively ( Table 1 ). Eight (4.6%) serum specimens were positive for both IgM and IgG, whereas 3 (1.7%) and 28 (16.2%) samples were positive for either IgM or IgG, respectively. The hantavirus seroprevalence varied between the different trapping sites and rodent species investigated ( Table 1 ). Serologic status of the 23 RT-qPCR positive animals was proven by indirect immunofluorescence assay (IFA) as described by manufacturer (Euroimmune, Lübeck, Germany) except using fluorescein isothiocyanate (FITC)–labeled anti-mouse IgG conjugate (Sigma-Aldrich Chemie). The IgG ELISA results were confirmed for 5 samples, whereas 5 additional samples demonstrated a different reactivity in ELISA and IFA ( Table 2 ).\n\n【10】All DOBV RT-qPCR positive samples were subjected to conventional S-segment RT-PCR, which produced an amplification product of 790 bp for sequence analysis. After denaturation of RNA at 70°C for 5 min, cDNA was synthesized by using Moloney murine leukemia virus RT and random hexamers (MBI Fermentas, Vilnius, Lithuania), by incubating at 25°C for 10 min and thereafter at 37°C for 1 h. PCR amplification was carried out by adding 3 μL of cDNA into a reaction mix containing 75 mmol/L Tris-HCl (pH 8.8), 20 mmol/L NH <sub>4 </sub> (SO <sub>4 </sub> ) <sub>2 </sub> , 2.4 mmol/L MgCl <sub>2 </sub> , 10 pmol of each primer, 0.2 mmol/L dNTP, and 5U Taq DNA polymerase (MBI Fermentas). The initial denaturation for 6 min at 94°C, was followed by 40 cycles each at 94°C for 1 min, 52°C for 1 min, 72°C for 2 min, and a final extension at 50°C for 1 min and 72°C for 10 min. Analysis by agarose gel electrophoresis revealed amplification products of the expected size for 21 samples. The PCR products were purified by using PCR/Gel purification kit (GeneMark Technology Co., Taichung City, Taiwan) and cloned into pJet1.2 blunt (MBI Fermentas). Plasmid DNA was directly subjected to sequencing in CEQ 8000 Genetic Analyzer (Beckmann Coulter, Brea, CA, USA) using the Dye Termination Cycle Sequencing Kit (Beckmann Coulter).\n\n【11】Figure 2\n\n【12】Figure 2 . . Bayesian phylogenetic tree, based on an alignment of 450-nt long region of the small segment from various Dobrava-Belgrade virus lineages and other Murinae-associated hantaviruses. Posterior probabilities for Bayesian analysis are...\n\n【13】The nucleotide sequence identity among the obtained 21 S-segment sequences (GenBank accession nos. KF615886–KF615906) was very high, reaching 99.2 %. For phylogenetic analysis 2 novel DOBV strains found in _A. flavicollis_ ( Table 1 , number 81/09; accession no. HQ406826) and _A. uralensis_ ( Table 1 , number 80/09; accession no. HQ406825) were selected that showed a nucleotide sequence identity of 99.1%, whereas the amino acid sequence identity of the corresponding part of the nucleocapsid protein was 98.7%. A phylogenetic analysis of these 2 _Apodemus_ –derived DOBV sequences from Turkey confirmed their strong similarity to _A. flavicollis_ –associated DOBV genotype Dobrava sequences from Greece, Slovenia, and Slovakia ( Figure 2 ). The sequence divergence of the 2 sequences from Turkey to other genotype Dobrava sequences was found to be 3.8% at nucleotide level (0.7% at amino acid level). The sequence divergence to DOBV sequences from _A. ponticus_ and _A. agrarius_ mice was found to be much higher, up to 15.2% and 6%, respectively.\n\n【14】### Conclusions\n\n【15】Human infections with members of the _A. flavicollis_ –associated DOBV lineage have been described in Slovenia, Greece, Serbia, Montenegro, the Czech Republic, and Hungary ( _2_ ). The current study of rodents in Turkey identified DOBV genotype Dobrava (DOBV-Af) as a potential causative agent of the hantavirus outbreak on the Black Sea coast of Turkey. This conclusion is in agreement with previous findings in patients from this region ( _6_ , _8_ , _10_ ).\n\n【16】Notably, this study demonstrated closely related DOBV sequences in _A. flavicollis_ and _A. uralensis_ mice and a high DOBV prevalence in both species. This observation might indicate multiple DOBV spillover to _A. uralensis_ mice. In addition, we found a single spillover infection here in rats ( _R. rattus_ ). Similarly, spillover infections of DOBV genotype Dobrava (DOBV-Af) in _Mus musculus_ and _A. sylvaticus_ mice and of DOBV genotype Kurkino (DOBV-Aa) in _A. flavicollis_ mice have been reported ( _11_ , _12_ ). Alternatively, the frequent detection of DOBV-specific nucleic acid in _A. uralensis_ mice may indicate that this rodent species functions as a reservoir. Previously, in a study in Czech Republic _A. uralensis_ mice were found to contain hantavirus antigen ( _13_ ). These findings underline the current problem of identifying hantavirus reservoirs ( _14_ ) as was recently also observed for European Tula virus ( _15_ ). The detection of hantavirus-reactive antibodies and the absence of hantavirus RNA in _Myodes_ species might be explained by DOBV spillover infections or, alternatively, by the presence of another hantavirus not detected by the RT-qPCR assay used here. Therefore, future studies on sympatrically occurring _Apodemus_ species at multiple sites in Turkey along the distribution range of _A. uralensis_ mice need to confirm whether these rodents have a role as a potential reservoir host of DOBV. In addition, a comprehensive study in different rodent and other small mammal species should determine whether hantaviruses other than DOBV are present.\n\n【17】Dr Oktem is an associate professor at Dokuz Eylul University in Izmir, Turkey, and a microbiologist in the Department of Medical Microbiology, Faculty of Medicine. His major research interest is virus diseases of medical importance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c75e3e9-6431-4da8-9dde-31baf665ad66", "title": "Postbooster Antibodies from Humans as Source of Diphtheria Antitoxin", "text": "【0】Postbooster Antibodies from Humans as Source of Diphtheria Antitoxin\nAlthough diphtheria is an almost forgotten disease in industrialized countries, sporadic cases still occur. Possible reasons for these cases include partial failure of vaccine compliance, antivaccine campaigns, inadequate booster regimens, and immunosenescence. Health authority interest in this disease was rekindled after a nonvaccinated boy in Spain died of systemic diphtheria in June 2015 and 9 cases of cutaneous diphtheria among refugees were notified by Denmark, Sweden, and Germany in 2015 ( _1_ ). According to the World Health Organization, 7,321 cases of diphtheria were reported worldwide in 2014. In the early 1890s, Emil von Behring used serum from a hyperimmune horse (challenged with sublethal dose of _Corynebacterium diphtheriae_ ) to develop equine diphtheria antitoxin (DAT), which seemed to confer passive immunity to patients with diphtheria ( _2_ ). Subsequently, use of equine DAT to treat this disease became common. Uncontrolled but large studies of mortality rates from that time suggested effectiveness of equine DAT use; however, double-blinded randomized studies conducted by Adolf Bingel in 1918 concluded that equine DAT offered no benefit over serum from nonhyperimmune horses (not challenged with _C. diphtheriae_ ) ( _2_ ). Although modern efficacy studies are lacking, equine DAT is still the recommended treatment for diphtheria, listed among the World Health Organization essential medicines ( _3_ ). When administered early in the clinical course of disease, treatment with DAT can be lifesaving for patients with toxin-induced systemic symptoms.\n\n【1】A large proportion of European countries do not stockpile DAT, and many countries have experienced difficulties replacing expired stockpiles ( _3_ , _4_ ). As highlighted by the European Centre for Disease Prevention and Control ( _1_ ), the current lack of DAT in the European Union is a concern. DAT is not produced or licensed in the United States or in most European countries; it is imported from Brazil under an Investigational New Drug protocol ( _5_ ).\n\n【2】Equine DAT can induce anaphylactic reactions (a test for sensitivity to DAT should be conducted before each administration) ( _5_ ). The European Centre for Disease Prevention and Control and the US Centers for Disease Control and Prevention encourage searching for new providers of equine DAT and promote the development of alternative antitoxins of human origin. The definitive solution will probably come from monoclonal antibodies ( _4_ ) or synthetic molecules such as nucleic acid aptamers. These new molecules could constitute an unlimited source of DAT, with a low risk for hypersensitivity reactions. Unfortunately, these alternatives are not yet available and will need to undergo thorough regulatory processes before being approved for use in humans. We therefore describe the potential role of human plasma from vaccinated volunteers as a source of DAT.\n\n【3】Plasma from vaccinated persons is used to produce Anthrasil (Cangene Corporation, Winnipeg, Manitoba, Canada), a fully human polyclonal antianthrax intravenous immunoglobulin (IVIG) licensed in the United States. Antitetanus immunoglobulin is produced from plasma of young volunteers who received a booster dose of the tetanus–diphtheria vaccine.\n\n【4】The successful implementation of vaccination programs in industrialized and many developing countries indicates that most of these populations have antibodies against the diphtheria toxin. Nonetheless, the geometric mean concentration of IgG against diphtheria toxin in plasma of vaccinated adults who received the last dose of tetanus–diphtheria vaccine in their adolescence is not much over 0.3 IU/mL ( _6_ ). For diphtheria treatment, 20,000–100,000 IU of DAT is needed; the dose depends on disease severity ( _5_ ). In consequence, producing DAT from plasma obtained from the general population could not be cost-effective because large volumes would be needed to obtain a dose of DAT with enough potency for clinical use.\n\n【5】An alternative could be to obtain plasma from adult donors who recently received a booster dose of vaccine. Researchers have observed that during the diphtheria epidemic that emerged in the newly independent states of the former Soviet Union from 1991 through 1994, booster vaccination of convalescent patients led to enhanced antidiphtheria toxin titers ( _3_ , _7_ ). Seroepidemiologic studies evaluating the effect of booster vaccination of adults against diphtheria support this finding. Booster vaccination of adults induces up to 10 IU/mL of IgG against diphtheria toxin in plasma 4 weeks after vaccination ( _8_ – _13_ ) ( Table ). The use of conjugate vaccines, or a high vaccine dose, could yield the highest plasma concentrations of DAT after a booster dose of vaccine ( _9_ ).\n\n【6】Assuming use of revaccinated donor plasma with the highest titer, IVIG with a DAT potency up to 60–100 IU/mL could be obtained by using the standard methods for producing IVIG ( _3_ ). This concentration could be enough to treat moderate forms of diphtheria (those with skin lesions only, laryngeal disease, or nasopharyngeal disease) ( _5_ ). The European Pharmacopoeia recommends that the potency of equine-derived DAT be no less than 1,000 IU/mL ( _3_ ). To treat severe diphtheria, a dose of 100,000 IU, obtained by using a 5% IVIG solution with potency of 100 IU/mL, would require 1.6 liters of product, a substantially high volume that would be very difficult to administer to a child.\n\n【7】This major drawback could be solved by using antigen-specific antibody purification. The process is simple: the antigen is immobilized in a solid phase so that the antibodies that bind specifically to it are retained during addition of plasma. Bound antibody can be recovered by acid elution ( _14_ ). This method has been successfully used to purify specific antibodies from plasma or normal IVIG for research and development purposes ( _15_ ). In 1988, also in an experimental context, M. Sutjita et al. demonstrated that this approach was useful for concentrating DAT from human serum; they used a diphtheria toxoid-Sepharose 4B (Sigma Aldrich, St. Louis, MO, USA) affinity column ( _14_ ). In consequence, this approach could be used to purify DAT from plasma of revaccinated persons or from commercial immunoglobulins (i.e., the antitetanus immunoglobulin itself or nonspecific IVIG), which contains variable concentrations of DAT ( Technical Appendix ). This concentrated DAT could be useful for treating diphtheria of any severity in adults and children, with very low risk of inducing hypersensitivity reactions.\n\n【8】A potential drawback of affinity purification is that the obtained DAT could be denatured by acid elution. This risk could be minimized by immediately neutralizing pH by adding 1 mol/L Tris, followed by dialysis with phosphate-buffered saline. The obtained product should undergo the same biological agent removal processes as those used for standard IVIG (i.e., chemical inactivation, heat inactivation, nanofiltration, and precipitations). Neutralization potency of DAT obtained from human plasma should be assigned according to the Vero cell cytotoxicity assay and the guinea pig lethality model; the 1st International Standard for Diphtheria Antitoxin Human should be used as the reference antitoxin (National Institute for Biological Standards and Control code 10/262).\n\n【9】A limitation of using DAT obtained from human plasma is the potential cost. Some developing countries, where most cases of diphtheria occur, could not afford it. Production costs and the price of each dose of human DAT could be reduced by using as source the same plasma obtained from the donors recruited to produce the antitetanus immunoglobulin. Industrialized countries could also donate doses of this human DAT to developing countries.\n\n【10】Plasma from young adults receiving a booster dose of vaccine could represent a potential source of human DAT. Antigen-affinity antibody purification could help to produce a highly concentrated DAT from this plasma, useful for treating even the most severe forms of diphtheria. This approach could help mitigate the limited access to this essential medicine.\n\n【11】Dr. Bermejo-Martin is head of the Infection & Immunity Medical Investigation Unit at the Hospital Clínico Universitario, Instituto de Estudios de Ciencias de la Salud de Castilla y León, in Valladolid, Spain. His research interests include immunity in severe infections, such as those caused by emerging viruses and sepsis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7973916d-5330-4a19-99ae-8faaf03d4ef0", "title": "Errata—Vol. 17, No. 3", "text": "【0】Errata—Vol. 17, No. 3\nReferences were misnumbered in the Appendix Table of the article _Mycobacterium lentiflavum_ in Drinking Water Supplies, Australia (H.M. Marshall et al.). The article has been corrected online ( www.cdc.gov/eid/content/17/3/395.htm ).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6057cd40-028f-461c-8993-ae11bce30fe4", "title": "Astrovirus MLB2 Viremia in Febrile Child", "text": "【0】Astrovirus MLB2 Viremia in Febrile Child\nApproximately 10% of nonbacterial, sporadic diarrhea is caused by infection with astroviruses ( _1_ ). Until 2008, astroviruses that infect humans were thought to be limited to 8 closely related serotypes. However, 5 highly divergent astroviruses (MLB1, MLB2, VA1, VA2, and VA3) were discovered recently in stool samples from patients with ( _2_ _–_ _4_ ) and without ( _5_ ) diarrhea. No definitive disease association has been established for these 5 astroviruses.\n\n【1】A few reports have described enteric viruses in blood and other parts of the body, but none have described astroviruses in human plasma. For example, rotavirus RNA has been detected in serum ( _6_ ), cerebrospinal fluid ( _7_ ), and throat swab specimens ( _7_ ); and rotavirus viral protein 6 and nonstructural protein 4 also have been detected in serum ( _6_ _,_ _8_ ). Norovirus RNA also has been reported in human serum ( _9_ ) and in cerebrospinal fluid ( _10_ ), and enterovirus RNA has been reported in human serum ( _11_ ). We report a case of astrovirus MLB2 viremia.\n\n【2】### The Study\n\n【3】As part of a broad effort to define the human virome, we performed high-throughput sequencing (Genome Analyzer IIX; Illumina Inc., San Diego, CA, USA) on several plasma samples from children with febrile illness (K.M. Wylie et al., unpub. data). The Human Research Protection Office, Washington University (St. Louis, MO, USA) approved this study. The case report in this article describes results generated from study of a 20-month-old boy with a history of transient and resolved neutropenia. The child was evaluated in the emergency department of St. Louis Children’s Hospital for petechial rash (3-day history), fever < 40°C (1-day history), cough, and nasal congestion. He did not have vomiting or diarrhea.\n\n【4】The evaluation included a leukocyte count, with results (7.8 × 10 <sup>3 </sup> cells/mm <sup>3 </sup> ) within the reference values and with a differential count of 26% bands, 59% neutrophils, 8% lymphocytes, 6% monocytes, and 1% atypical lymphocyte; blood culture results were negative. Nasopharyngeal swab specimen was negative for respiratory syncytial virus, influenza types A and B, parainfluenza, and adenovirus by fluorescent antibody testing, and culture results were negative for respiratory viruses. Chest radiograph was interpreted as showing mild peribronchial thickening, which may represent a viral process. In addition, plasma or blood samples from the patient were subjected to a battery of PCR screenings for the following viruses, the results of which were all negative: adenovirus; enteroviruses (Enterovirus ASR; Cepheid Inc., Sunnyvale, CA, USA); human herpesvirus 6 and 7; parvovirus B19 (RealStar Parvovirus B19 PCR Kit 1.0; Altona Diagnostics, Hamburg, Germany); human bocavirus; cytomegalovirus (whole blood); Epstein-Barr virus (whole blood); and JC, BK, WU, and KI polyomaviruses.\n\n【5】Total nucleic acid was extracted from 100 μL of the patient’s plasma by using the Roche (Indianapolis, IN, USA) MagNa Pure System and randomly amplified by using a sequence-independent PCR strategy as described ( _12_ ). Amplicons were sheared and, following standard library construction, were sequenced by using the Genome Analyzer IIX (Illumina Inc.) according to the manufacturer’s protocol. Sequencing resulted in 6,394,424 sequence reads of 100 nt. When present in the sheared amplicons, the primer used for random amplification was removed, resulting in 83-nt sequences. From all reads, 238 had >80% nt identity to the partial MLB2 sequence in GenBank (GQ502192.1) when aligned by using Cross\\_match software ( www.phrap.org/phredphrapconsed.html#block\\_phrap ). In addition, 374 sequence reads with similarity to anelloviruses were detected in this sample by alignment of the reads to the GenBank NT and NR databases, using Cross\\_match and Blastx ( http://blast.ncbi.nlm.nih.gov/Blast.cgi ), respectively. Anelloviruses are commonly detected in human blood ( _13_ ) and have no known disease association. No reads aligned with any other viruses, except endogenous human retrovirus sequences.\n\n【6】Given the number of sequence reads from the plasma sample that could be aligned with the 3,280-nt sequence of MLB2 (accession no. GQ502192.1) in GenBank, we reasoned that additional reads were likely to be present from parts of the MLB2 genome that had not yet been sequenced. To provide a complete reference genome for such an analysis, we sequenced the complete MLB2 genome from a previously described isolate (GenBank accession no. GQ502192.1) ( _3_ ) from a stool sample by using a combination of reverse transcription PCR (RT-PCR), 3′ and 5′ rapid amplification of cDNA ends, and pyrosequencing on a genome sequencer (Roche) as described ( _4_ ). The complete MLB2 genome of 6,119 nt, excluding the polyA tail, was confirmed by Sanger sequencing of overlapping RT-PCR amplicons and has been deposited in GenBank (accession no. JF742759).\n\n【7】Figure\n\n【8】Figure . Map of 10 plasma-derived astrovirus MLB2 strain contigs generated by high-throughput sequencing (Genome Analyzer IIX; Illumina Inc., San Diego, CA, USA). ORF, open reading frame.\n\n【9】Comparison of the high-throughput sequencing reads from the plasma to the complete genome yielded an additional 199 reads with >80% nt identity. Assembly of all reads yielded 10 contigs, with an average length of 305 bp, which aligned throughout the MLB2 genome ( Figure ). Conventional RT-PCR and quantitative TaqMan RT-PCR independently confirmed the presence of MLB2 in the plasma sample. The complete sequence of the capsid (open reading frame 2) of this plasma-derived MLB2 strain was obtained by RT-PCR (GenBank JF742760) by using primers designed from the stool-derived MLB2 strain. The capsid of the plasma-derived MLB2 strain has 99% nt identity with the stool-derived MLB2 strain. Of the 27 nt substitutions, 25 were synonymous. Because of limited quantities of the plasma sample, we were unable to sequence the complete genome of the plasma-derived MLB2.\n\n【10】To quantify the MLB2 virus load in the plasma specimen, we developed a quantitative RT-PCR TaqMan assay targeting the capsid (forward primer LG0169 5′-ACAACTGGCCCTACATTGAATTC-3′, reverse primer LG0170 5′-CCGACACGCACATCTCGAT-3′, and probe FAM-TCGGGTCTTGGCGCGCGAT-tam). We used the MAXIscipt Kit (Ambion, Austin, TX, USA) to generate in vitro–transcribed RNA from a plasmid containing the region of interest to establish a standard curve for the assay. On the basis of the results of this assay, this sample has 4.5 × 10 <sup>5 </sup> copies of MLB2 per mL of plasma.\n\n【11】To evaluate how frequently astroviruses may be present in human plasma, we screened archived plasma samples from 90 children with fever and 98 afebrile controls by using an astrovirus consensus RT-PCR ( _3_ ). Total nucleic acid was extracted as described above. All 188 plasma samples were negative, which suggests that astrovirus MLB2 viremia is relatively rare, at least in the cohort analyzed.\n\n【12】### Conclusions\n\n【13】The role of novel astrovirus MLB2 in human health and disease and the clinical consequence of MLB2 viremia are not yet known. This case report raises the possibility that astrovirus MLB2 may be a cause of febrile illness. In addition, the finding of MLB2 viremia suggests that astrovirus MLB2 may have effects outside the enteric system. These data, combined with the recent detection of an astrovirus in brain tissue of an immunocompromised patient ( _14_ ) and the brain tissue of mink with shaking mink syndrome ( _15_ ), demonstrate a broader distribution of astroviruses in the body than previously recognized. The possibility that additional disease states may be linked to astrovirus infection is intriguing. For example, no other known pathogen was detected in the patient in this case report, and he had mostly upper respiratory signs, raising the possibility that MLB2 may play a role in respiratory illness. These new hypotheses warrant further investigation.\n\n【14】Dr Holtz is a pediatric gastroenterologist at Washington University School of Medicine in St. Louis. Her research focuses on the discovery and characterization of novel viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "29c93eb9-e2ab-4681-a4d5-44062b738876", "title": "Mapping Census Tract Clusters of Type 2 Diabetes in a Primary Care Population", "text": "【0】Mapping Census Tract Clusters of Type 2 Diabetes in a Primary Care Population\n=============================================================================\n\n【1】GIS SNAPSHOT — Volume 16 — May 16, 2019\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Marynia Kolak, PhD <sup>1 </sup> ; Geethi Abraham, MD, MPH <sup>2 </sup> <sup>,3 </sup> ; Mary R. Talen, PhD <sup>2 </sup> <sup>,3 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Kolak M, Abraham G, Talen MR. Mapping Census Tract Clusters of Type 2 Diabetes in a Primary Care Population. Prev Chronic Dis 2019;16:180502. DOI: http://dx.doi.org/10.5888/pcd16.180502 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Background\n*   Date Sources and Map Logistics\n*   Highlights\n*   Action\n*   Acknowledgments\n*   Author Information\n*   References\n*   Table\n\n【9】High-resolution JPG for print image icon\n\n【10】This map shows areas with significantly high and significantly low prevalence of diabetes and prediabetes in a health center population in Chicago. Prevalence was determined by ICD ( _International Classification of Diseases_ ) codes and measured hemoglobin A1 <sub>c </sub> (HbA <sub>1c </sub> ). The map highlights regional clusters and isolated areas of diabetes prevalence that could be targeted with interventions to improve health outcomes. Diagnoses determined by ICD codes are shown in colors as hot and cold spot cluster cores corresponding to “high–high” (HH) and “low–low” (LL) LISA (local indicator of spatial autocorrelation) statistics, where selected census tracts and neighboring tracts both have high rates (HH) or both have low rates (LL) of diabetes. Hot spot outliers have high diabetes rates compared with neighboring tracts (“high–low” \\[HL\\]), whereas cold spot outliers have low diabetes rates compared with neighboring tracts (“low–high” \\[LH\\]). Hot spots of prediabetes and diabetes determined by measured HbA <sub>1c </sub> levels are also shown. LISA significance set at _P_ < .05. Supermarket data are from Kolak et al (1). Census tract and community area boundary data are from the Chicago Data Portal (2,3). Basemap imagery is from OpenStreetMap, Leaflet, and Carto.  \\[A text description of this figure is available.\\]\n\n【11】Top\n\n【12】Background\n----------\n\n【13】Although much effort has been made by public health agencies to geographically plot chronic diseases at the national, state, and city levels, information is limited on how disease is distributed in smaller geographic areas or populations, such as a health center population (4). Our objective was to identify patterns of type 2 diabetes in a patient population of a large urban federally qualified health center by using census tracts as a proxy for neighborhoods. This approach expands on other longitudinal studies that found associations between rates of type 2 diabetes and neighborhood social and physical environment characteristics such as access to healthy foods (5–7).\n\n【14】As public health data have become more available, the spatial analyses of disease prevalence at local levels using clinical records has emerged as a powerful population-based health tool (8,9). Adapting these best methodological practices to an individual health center may provide additional strategies for identifying localized areas of health risk for targeting interventions and improving care in a health center’s population. Ultimately, merging GIS (geographic information systems) capabilities with primary care patient panels is a novel approach to guide disease management strategies and community outreach in a local health care system.\n\n【15】Top\n\n【16】Date Sources and Map Logistics\n------------------------------\n\n【17】We generated maps from data extracted from the electronic medical records (N = 10,523) of a primary care patient population seen from August 1, 2015, to September 30, 2017, at a health center in Chicago. Residential addresses of patients were geocoded and converted to spatial data points. Points were then joined and aggregated to Chicago census tract boundaries (2). We created a subset of census tracts as the core service area; 31% of all patient-residing census tracts (140 of 455 tracts) included most of the health center’s patients (n = 9,126) and were located within 5 miles of the health center. This selection process reduced the number of spurious census tract outliers (ie, those with few health center patients). The study was approved by the Northwestern University Institutional Review Board.\n\n【18】We identified 1,246 patients with a type 2 diabetes diagnosis using the _International Classification of Diseases (ICD), Ninth Revision, Clinical Modification_ codes 250.xx and _ICD, Tenth Revision_ , _Clinical Modification_ codes E11.XX; 854 of these patients resided in the core service area. We also classified patients by measured hemoglobin A1 <sub>c </sub> (HbA <sub>1c </sub> ) levels; patients with levels from 5.4% to 6.4% were classified as having prediabetes, and patients with levels of 6.5% or higher were classified as having diabetes. Fifteen percent of patients in the core service area had HbA <sub>1c </sub> levels ranging from 5.4% to more than 14%; in the wider population, 25% of patients were classified as such. On the basis of HbA <sub>1c </sub> levels, 959 patients (of the total population) had type 2 diabetes, of whom 49 did not have an ICD code for diabetes. We used 5-year averages from the 2016 American Community Survey, as prepared by the Centers for Disease Control and Prevention’s Social Vulnerability Index database (10), for the following social determinants of health: percentage of persons living in poverty, rate of unemployment, per capita income, percentage of persons with no high school diploma, percentage of single parents, percentage limited in speaking English, percentage living in crowded housing, percentage having no vehicles, and percentage uninsured. Data for these covariates were extracted and joined to data for census tract areas.\n\n【19】We used exploratory spatial data analysis techniques on raw and population-adjusted data to analyze variation in type 2 diabetes distribution by census tract. We conducted an empirical Bayes smoothed univariate cluster and outlier detection analysis using the local indicator of spatial autocorrelation (LISA) statistic to determine type 2 diabetes prevalence (11). Empirical Bayes smoothing analysis uses a prior distribution, in this case, the average value of the sample, corrected for the variance instability associated with rates that have a small population base. We determined hot and cold spot clusters of type 2 diabetes prevalence. Hot spot clusters refer to areas that are significantly similar to their neighbors in high disease prevalence (“high–high” \\[HH\\] statistic), and cold spot clusters refer to areas that are significantly similar to their neighbors in low disease prevalence (“low–low” \\[LL\\] statistic). Clusters are composed of both cluster cores and nearby neighbors. Hot and cold spot clusters were visually identified by their cluster core in our analysis; these cores are surrounded by areas with proportionally high or low values. We also determined hot and cold spot outliers (“high–low” \\[HL\\] and “low–high” \\[LH\\]), which refer to areas that are significantly different (at _P_ < .05) from their neighbors.\n\n【20】We compared the mean values of social determinants of health indicators in diabetes hot and cold spot tracts clusters with the mean values in all other core-service–area tracts using analysis of variance. We selected cluster cores and their neighboring tracts to represent complete clusters in this descriptive analysis. We added supermarket locations to the map as a proxy for access to healthy foods to demonstrate how incorporating environmental features may be useful in evaluating disease distribution (6). Although features other than supermarkets may be associated with disease occurrence, our analysis explored how clusters of type 2 diabetes may intersect with physical locations for food access. We used free and open-source software (R \\[The R Foundation\\] and GeoDa version 1.12.1.59 \\[Center for Spatial Data Science\\], a mapping and spatial statistics software) for all data management and analysis (12,13). We generated final maps by using R and Adobe Illustrator version 22.1.\n\n【21】Top\n\n【22】Highlights\n----------\n\n【23】Among 140 census tracts, we found 31 hot spot clusters and 25 cold spot clusters of patients with type 2 diabetes, along with 4 hot outliers and 3 cold outliers. Compared with all census tracts in the core service area, the census tracts in the hot spot clusters had significantly lower income levels and high school graduation rates and a significantly greater percentage of households with vehicles, single parents, residents with limited English proficiency, crowded housing, and persons without health insurance ( Table ). The raw type 2 diabetes rate in hot spot census tracts was significantly higher than in all other tracts (0.14 in 36 hot spots vs 0.10 in 104 non–hot spots; _P_ \\= .005) and double the rate in cold spot census tracts (0.14 vs 0.07). We also found consistent overlap between hot spot census tracts and higher HbA <sub>1c </sub> ranges; conversely, some tracts with high HbA <sub>1c </sub> rates were not identified as hot spot tracts.\n\n【24】Top\n\n【25】Action\n------\n\n【26】Our analysis demonstrates that stable calculation at the census tract level of a health center’s patient population can facilitate spatial analysis and identify patient groups with health risks in neighborhood clusters. This research can set a foundation for developing targeted interventions in a health center population at the neighborhood level to improve health outcomes. Our maps also identified neighborhoods at lower risk for type 2 diabetes and create opportunities to explore possible neighborhood resiliency factors.\n\n【27】Our data and findings represent a patient population and are not meant to serve as a true sample of the actual population. However, comparing the differences between hot and cold spot clusters can open avenues for mobilizing outreach at community health centers or federally qualified health centers and partnering among local resources to support interventions for the clinical population. Findings can be shared with policy makers and community advocates to influence what resources are needed and where they are needed most. Although geographically plotting of chronic diseases at the national, state, and city levels provides important information on the distribution of disease among populations, our map illustrates the use of exploratory data analysis techniques to expand the use of patient panels or registries and identify and address the health needs of vulnerable patient populations within a health system at the local level. This methodology can build bridges and partnerships between community health centers, federally qualified health centers, public health officials, and community organizations to develop neighborhood initiatives and outreach.\n\n【28】Top\n\n【29】Acknowledgments\n---------------\n\n【30】We thank our colleagues from Northwestern McGaw Medical Center, Erie Family Health Center, and University of Chicago Center for Spatial Data Science, who provided insight and expertise that greatly assisted the research. We are also grateful to the anonymous reviewers and editor for their comments and insights on an earlier version of the manuscript.\n\n【31】Top\n\n【32】Author Information\n------------------\n\n【33】Corresponding Author: Geethi Abraham, MD, MPH, McGaw Medical Center of Northwestern University, 710 N Lake Shore Dr, Abbott Hall, 4th Floor, Chicago, IL 60611-2909. Telephone: 773-793 7699. Email: gabraham@eriefamilyhealth.org .\n\n【34】Author Affiliations: <sup>1 </sup> Center for Spatial Data Science, Chicago, Illinois. <sup>2 </sup> McGaw Medical Center of Northwestern University, Chicago, Illinois. <sup>3 </sup> Erie Family Health Center, Chicago, Illinois.\n\n【35】Top\n\n【36】References\n----------\n\n【37】1.  Kolak M, Bradley M, Block D, Pool L, Garg G, Toman CK, et al. Chicago supermarket data and food access analytics in census tract shapefiles for 2007–2014. Data Brief 2018;21:2482–8. CrossRef external icon PubMed external icon\n2.  Chicago Data Portal. Boundaries — census tracts — 2010. https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Census-Tracts-2010/5jrd-6zik. Accessed May 11, 2018.\n3.  Chicago Data Portal. Boundaries — community areas (current). https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Community-Areas-current-/cauq-8yn6. Accessed May 11, 2018.\n4.  Wilkinson RG, Pickett KE. Income inequality and socioeconomic gradients in mortality. Am J Public Health 2008;98(4):699–704. CrossRef external icon PubMed external icon\n5.  Hunt BR, Whitman S, Henry CA. Age-adjusted diabetes mortality rates vary in local communities in a metropolitan area: racial and spatial disparities and correlates. Diabetes Care 2014;37(5):1279–86. CrossRef external icon PubMed external icon\n6.  Christine PJ, Auchincloss AH, Bertoni AG, Carnethon MR, Sánchez BN, Moore K, et al. Longitudinal associations between neighborhood physical and social environments and incident type 2 diabetes mellitus: the Multi-Ethnic Study of Atherosclerosis (MESA). JAMA Intern Med 2015;175(8):1311–20. CrossRef external icon PubMed external icon\n7.  Gebreab SY, Hickson DA, Sims M, Wyatt SB, Davis SK, Correa A, et al. Neighborhood social and physical environments and type 2 diabetes mellitus in African Americans: The Jackson Heart Study. Health Place 2017;43:128–37. CrossRef external icon PubMed external icon\n8.  Green C, Hoppa RD, Young TK, Blanchard JF. Geographic analysis of diabetes prevalence in an urban area. Soc Sci Med 2003;57(3):551–60. CrossRef external icon PubMed external icon\n9.  Dijkstra A, Janssen F, De Bakker M, Bos J, Lub R, Van Wissen LJ, et al. Using spatial analysis to predict health care use at the local level: a case study of type 2 diabetes medication use and its association with demographic change and socioeconomic status. PLoS One 2013;8(8):e72730. CrossRef external icon PubMed external icon\n10.  Centers for Disease Control and Prevention, Agency for Toxic Substances and Disease Registry, Geospatial Research, Analysis, and Services Program. Social vulnerability index 2016 database Illinois. https://svi.cdc.gov/data-and-tools-download.html. Accessed May 1, 2018.\n11.  Anselin L. Local indicators of spatial association — LISA. Geogr Anal 1995;27(2):93–115. CrossRef external icon\n12.  Anselin L, Syabri I, Kho Y. GeoDa: an introduction to spatial data analysis. Geogr Anal 2006;38(1):5–22. CrossRef external icon\n13.  Ihaka R, Gentleman RR. a language for data analysis and graphics. J Comput Graph Stat 1996;5(3):299–314.\n\n【38】Top\n\n【39】Table\n-----\n\n【40】#####  Table. Descriptive Statistics of Social Determinant of Health Covariates for Hot and Cold Spot Clusters of Diabetes in a Patient Population of a Large Health Center in Chicago, 2015–2017 <sup>a</sup>  \n\n| Determinant b | Hot Spot Cluster of Diabetes (n = 31) | Cold Spot Cluster of Diabetes (n = 25) | All Census Tracts in Primary Service Area (n = 140) |\n| --- | --- | --- | --- |\n| Poverty, % | 22.5 (.64) | 25.1 (.39) | 23.3 |\n| Unemployment, % | 13.4 (.73) | 17.4 (<.001) | 13.0 |\n| Per capita income, $ | 17,014 (.003) | 16,727 (.006) | 21,192 |\n| No high school diploma, % | 34.7 (<.001) | 28.3 (.12) | 25.1 |\n| Single parent, % | 16.4 (.16) | 17.1 (.08) | 14.6 |\n| Limited in speaking English, % | 22.4 (<.001) | 11.9 (.17) | 14.3 |\n| Crowded housing, % | 8.9 (<.001) | 5.0 (.23) | 6.1 |\n| No vehicles, % | 18.1 (.02) | 21.2 (.57) | 22.4 |\n| Uninsured, % | 26.6 (.005) | 23.0 (.59) | 23.7 |\n\n【42】<sup>a </sup> Diabetes data extracted from the electronic medical records (n = 10,523) of a primary care patient population seen from August 1, 2015, to September 30, 2017. _P_ values, determined by analysis of variance, are for difference between means for the cluster (hot spot or cold spot) and means for all other tracts in the sample. An average for all tracts in the service area is provided for additional context.  \n<sup>b </sup> 5-year averages from the 2016 American Community Survey (10).\n\n【43】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f46be1ae-97e5-4232-b610-dce383f8202b", "title": "The Oral Health in America Report: A Public Health Research Perspective", "text": "【0】The _Oral Health in America_ Report: A Public Health Research Perspective\n=========================================================================\n\n【1】ESSAY — Volume 19 — September 8, 2022\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Jane A. Weintraub, DDS, MPH <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Weintraub JA. The Oral Health in America Report: A Public Health Research Perspective. Prev Chronic Dis 2022;19:220067. DOI: http://dx.doi.org/10.5888/pcd19.220067 .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Introduction\n*   Data Needed\n*   Health Disparities and Social Determinants of Health\n*   Individual and Community Relationships\n*   Scientific Advances and Equitable Distribution\n*   Educational Opportunities\n*   Summary\n*   Acknowledgments\n*   Author Information\n*   References\n\n【9】Introduction\n------------\n\n【10】In December 2021, the National Institutes of Health, National Institute of Dental and Craniofacial Research, released its landmark 790-page report, _Oral Health in America: Advances and Challenges_ (1). This is the first publication of its kind since the agency’s first _Oral Health in America: A Report of the Surgeon General_ described the silent epidemic of oral diseases in 2000 (2). This new, in-depth report, an outstanding resource, had more than 400 expert contributors. Its broad scope is exemplified by its 6 sections ( Box ), each of which includes 4 chapters: 1) Status of Knowledge, Practice, and Perspectives; 2) Advances and Challenges; 3) Promising New Directions; and 4) Summary. In this essay, I provide a public health research perspective for viewing the report, identify some advances and gaps in our knowledge, and raise research questions for future consideration.\n\n【11】### **Box. Section Titles, Oral Health in America: Advances and Challenges (1)**\n\n【12】1\\. Effect of Oral Health on the Community, Overall Well-Being, and the Economy\n\n【13】2A. Oral Health Across the Lifespan: Children\n\n【14】2B. Oral Health Across the Lifespan: Adolescents\n\n【15】3A. Oral Health Across the Lifespan: Working-Age Adults\n\n【16】3B. Oral Health Across the Lifespan: Older Adults\n\n【17】4\\. Oral Health Workforce, Education, Practice, and Integration\n\n【18】5\\. Pain, Mental Illness, Substance Use, and Oral Health\n\n【19】6\\. Emerging Science and Promising Technologies to Transform Oral Health\n\n【20】Top\n\n【21】Data Needed\n-----------\n\n【22】A recurring theme in the report is the need for many types of data, from microdata — the molecular, nanoparticle level — to macrodata — the population and global level. Data are needed to guide public health policies and programs at the federal, state, and local levels. Future research using big data from multiple sources (eg, community health needs assessments, surveillance systems, GIS mapping, electronic health records, practice-based research networks) will provide timely, population-based information to evaluate and drive changes to policy and delivery systems and oral health advocacy efforts.\n\n【23】This new report includes descriptive national data from 3 cycles of the National Health and Nutrition Examination Survey (NHANES). To continue monitoring national oral health surveillance data and trends, oral health data need to be included routinely in NHANES and in other large national studies. Too often, questions about oral health are missing from surveys, or clinical oral health data are not collected. For example, very little about oral health was included as part of the planned data collection protocol for the National Institutes of Health _All of Us_ Research Program. This program aims to collect health information from 1 million people (3). Local and state data are often outdated, incomplete, or unavailable. Most oral health data are cross-sectional and are useful for studying trends and associations, but population-based longitudinal data to study causality and the effectiveness of interventions and policies are sparse.\n\n【24】How does oral health care improve other health conditions? Proprietary claims data from insurance companies (4) show the inter-relationship between treatment of periodontal disease and systemic conditions, but secondary data analysis has many limitations and confounding factors. Clinical trials show that periodontal treatment improves glycemic control among people with diabetes (5), but long-term outcome assessments are lacking. We need more answers to convince policy makers and payers about the importance of including comprehensive adult oral health services in publicly financed programs such as Medicaid, which is currently lacking in many states, and Medicare, where those services are missing altogether.\n\n【25】Top\n\n【26】Health Disparities and Social Determinants of Health\n----------------------------------------------------\n\n【27】Many examples of substantial oral health disparities and inequities are presented in Section 1 of the report. For some conditions and population groups, little improvement has been made, especially among adults and seniors. Section 1 also describes the adverse social, economic, and national security effects of poor oral health, barriers to care, social and commercial determinants of oral health, and related common risk factors. More than the clinical data collected in a typical dental history is needed to understand social determinants and employ local and upstream interventions. The report suggests obtaining social histories from patients to get information about where people live, learn, work, and play. For example, to learn about socioeconomic status, diet, and medications, we want to know not only “What’s in your wallet,” (as touted in a frequent television advertisement) but what’s in your refrigerator? What’s in your medicine cabinet? Telehealth has given clinicians a look inside patients’ homes. Collaboration with social workers, home health aides, and visiting nurses could inform us even more about the home environment. With integrated electronic medical and dental patient records, oral health professionals and medical colleagues can share information. Barriers to integration and assessment of population health outcomes affect many dentists who still use paper records or software specific to dental care that lacks diagnostic codes and interoperability with other health care records systems (6).\n\n【28】The report highlights the need for more information about adolescents and older adults and other understudied population groups. Section 1 describes many diverse, vulnerable populations (eg, people with special health care needs, low health literacy, mental illness, substance abuse disorders; victims of structural racism) who all need to be included in oral health research. Non-English speakers and hard-to-reach populations that have physical and/or financial barriers to traditional dental care are less likely to be recruited and represented in clinical trials, making results less generalizable and interventions less applicable. The applied research agenda being developed by the American Association of Public Health Dentistry (7) and the “Consensus Statement on Future Directions for the Behavioral and Social Sciences in Oral Health,” which is based on an international summit (8), are helpful in setting research and methodologic priorities, including qualitative, implementation, and health systems research.\n\n【29】Top\n\n【30】Individual and Community Relationships\n--------------------------------------\n\n【31】Knowledge about the interrelationships between oral and systemic health has greatly expanded since the 2000 report. About 60 adverse health conditions have now been shown to be associated with oral health (1), which is part of the rationale for the integration of oral health and primary care. Research will advance our understanding of the mechanisms by which oral and systemic conditions are affected by upstream environmental and social factors, epigenetic factors, and the aging process, both individually and communally. For example, how do external exposures change our microbiomes? Our oral microbiome may be exposed to air containing Sars-CoV-2, water containing protective fluoride, or many kinds of food, beverages, medications, illicit substances, smoked products, and sometimes the biome of close personal contacts. How does the health of a community’s high caries risk groups change with policies such as a tax on sugar-sweetened beverages, Medicaid reimbursement changes, or health promotion efforts to improve oral health literacy and dietary behaviors? To what extent will increased application of value-based health care reimbursement with emphasis on disease prevention, early detection, and minimally invasive care improve oral health? Will the World Health Organization’s addition of dental products (eg, fluoride toothpaste, low-cost silver diamine fluoride, glass ionomer cement) to its Model List of Essential Medicines (9) increase their use to prevent and treat dental caries for under-resourced populations without access to conventional high-cost dental care?\n\n【32】Top\n\n【33】Scientific Advances and Equitable Distribution\n----------------------------------------------\n\n【34】The report’s Section 6 describes many exciting advances in biology, biomimetic dental materials, and technology. Rapid advances in salivary diagnostics are providing information about early, abnormal changes in remote organ systems in the body. Advanced imaging techniques and artificial intelligence can be used for early diagnosis of oral lesions before they are visible to the human eye. The validity and accuracy of these techniques need careful evaluation. Can these earlier clinical end points be used to shorten the length of expensive clinical trials? Guide new preventive strategies? At what point do providers intervene with early preventive or therapeutic strategies instead of letting the body heal itself?\n\n【35】Will populations at greatest risk for disease and the greatest barriers to accessing dental care be able to benefit from early intervention? Every intervention has a cost. If access to new prevention and therapeutic discoveries is not equitable, will health disparities worsen? We need community engagement in the research process and the tools from many disciplines to measure and facilitate the best outcomes. The national Oral Health Progress and Equity Network’s blueprint for improving oral health for all includes 5 levers to advance oral health equity: “amplify consumer voices, advance oral health policy, integrate dental and medical \\[care\\], emphasize prevention and bring care to the people” (10).\n\n【36】Top\n\n【37】Educational Opportunities\n-------------------------\n\n【38】Who will analyze all these data mined from many micro and macro sources, and who will interpret the data? Health learning systems and complex software algorithms are being developed to provide automated diagnostic information. Data analysts with knowledge of these and other sophisticated tools and modeling approaches are needed.\n\n【39】The dental, oral, and craniofacial research and practice communities increasingly need to be part of interdisciplinary research and educational programs with opportunities for collaboration and learning. Federally qualified health centers and look-alikes are good sites for medical–dental integration, but many of these facilities do not provide dental care.\n\n【40】More positions are needed for dental public health specialists who can lead advocacy efforts, interdisciplinary teams of researchers, clinicians, and community partners and conduct research. For example, the new Dental Public Health Research Fellowship at the National Institute of Dental and Craniofacial Research will provide more intensive research training to further advance dental public health and population-based research. Mechanisms are needed to promote, facilitate, and reward sharing of research and training resources across disciplines in our competitive environment.\n\n【41】Top\n\n【42】Summary\n-------\n\n【43】Public health perspectives are an important part of interdisciplinary approaches to guide, conduct, and apply research and implement policies to improve oral health. Preventive approaches exist as do barriers to their dissemination and implementation. To prevent disease and improve population oral and overall health, systems change and policy reform are needed along with scientific advances across the research spectrum, more population-level data and analysis, and community participatory engagement. I am optimistic that the next _Oral Health in America_ report will describe fewer inequities and more progress toward oral health for all.\n\n【44】Top\n\n【45】Acknowledgments\n---------------\n\n【46】This article is based on a presentation made in the webinar, _Oral Health in America — Advances and Challenges: Reading the Report through a Research Lens_ , sponsored by the American Association for Dental, Oral, and Craniofacial Research. The author received no financial support for this work and has no conflicts of interest to declare. The statements made are those of the author. No copyrighted materials were used in this article.\n\n【47】Top\n\n【48】Author Information\n------------------\n\n【49】Corresponding Author: Jane A. Weintraub, DDS, MPH, R. Gary Rozier and Chester W. Douglass Distinguished Professor, University of North Carolina at Chapel Hill Adams School of Dentistry, Department of Pediatric and Public Health, Koury Oral Health Sciences Building, Suite 4508, Chapel Hill, NC 27599-7450. Telephone: (919) 537-3240. Email: Jane\\_Weintraub@unc.edu .\n\n【50】Author Affiliations: <sup>1 </sup> University of North Carolina at Chapel Hill Adams School of Dentistry and Gillings School of Global Public Health, Chapel Hill, North Carolina.\n\n【51】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eea8c289-f54a-435f-a9ab-64fa0aa9c3f9", "title": "Extreme Drug Resistance in Acinetobacter baumannii Infections in Intensive Care Units, South Korea", "text": "【0】Extreme Drug Resistance in Acinetobacter baumannii Infections in Intensive Care Units, South Korea\n**To the Editor:** _Acinetobacter_ spp. have emerged as a cause of nosocomial infections, especially in intensive care units (ICUs). In South Korea, _Acinetobacter_ spp. was ranked as the third most frequently found pathogen in ICUs ( _1_ ). With the emergence of multidrug-resistant (MDR) or pandrug-resistant (PDR) isolates, few drugs are now available to treat MDR or PDR _Acinetobacter_ infections; polymyxins are the only therapeutic option in many cases ( _2_ ). Current polymyxin resistance rates among _Acinetobacter_ isolates are low worldwide ( _3_ ). We report the emergence of extreme drug resistance (XDR) in _A. baumannii_ isolates from patients in ICUs of Samsung Medical Center in Seoul, South Korea. These isolates were resistant to all tested antimicrobial drugs, including polymyxin B and colistin, to which PDR isolates are normally susceptible.\n\n【1】Sixty-three nonduplicate _Acinetobacter_ spp. isolates were collected from the ICUs from April through November 2007. Species identification was performed based on partial RNA polymerase β-subunit gene sequences, amplified rDNA restriction analysis, and the gyrase B gene–based multiplex PCR method ( _3_ ). Forty-four isolates were identified as _A. baumannii_ : 9 as genomic species 3, six as genomic species 13TU, 2 as _A. baumannii_ \\-like species, and 1 each as _A. junnii_ and genomic species 10.\n\n【2】In vitro susceptibility testing was performed and interpreted by using the broth microdilution method according to the Clinical and Laboratory Standards Institute guidelines ( _4_ ). Colistin and polymyxin B resistances were defined as MIC \\> 4 mg/L ( _4_ ). MDR was defined as characterized by resistance to \\> 3 classes of antimicrobial drugs, and PDR was defined as characterized by resistance to all antimicrobial drugs, regardless of colistin and polymyxin B susceptibility. XDR was defined as resistance to all antimicrobial drugs. Multilocus sequence typing (MLST) and pulsed-field gel electrophoresis (PFGE) were performed for all PDR isolates according to previously described methods ( _5_ , _6_ ). Genes encoding oxacillinases, such as those classified as OXA-23-like, OXA-24/40-like, OXA-51-like, and OXA-58-like, were detected as previously described ( _7_ ). PCR and sequence analyses were performed to detect and characterize the other antimicrobial resistance genes, according to methods reported ( _8_ ).\n\n【3】Of 63 _Acinetobacter_ isolates, 31.7% and 34.9% were resistant to imipenem and meropenem, respectively. Of the 63 isolates, 27.0% and 30.2% were resistant to polymyxin B and colistin, respectively. For the other antimicrobial drugs, _Acinetobacter_ spp. isolates showed antimicrobial resistance rates >50%. Nineteen isolates (30.2%), all belonging to _A. baumannii_ , were PDR. Most of these PDR isolates (16/19, 84.2%) were collected from endotracheal aspirate, and others were from peritoneal fluid and sputum. When characterized by PFGE and MLST, all PDR isolates belonged to a single clone, ST22, and all contained the _bla_ <sub>OXA-23 </sub> and _bla_ <sub>OXA-66 </sub> genes. IS _Aba1_ was detected upstream of _bla_ <sub>OXA-23 </sub> and _bla_ <sub>OXA-66 </sub> in all PDR isolates. In addition, most PDR isolates contained _bla_ <sub>TEM-116 </sub> , _bla_ <sub>PER-1 </sub> , and _bla_ <sub>ADC-29 </sub> genes. TEM-116 is a point mutant derivative of TEM-1, Val84→Ile. All β-lactamase genes were located on a plasmid. Also, IS _Aba1_ was located at the upstream of all the _bla_ <sub>ADC </sub> , which was shown by PCR. However, none of the isolates had _bla_ <sub>CTX-M </sub> , _bla_ <sub>VEB </sub> , _bla_ <sub>IMP </sub> , _bla_ <sub>VIM </sub> , or _bla_ <sub>GIM </sub> .\n\n【4】Of the PDR isolates, 8 were resistant even to colistin and polymyxin B. These 8 isolates also showed resistance to tigecycline (MICs 4 mg/L). Thus, they were resistant to all antimicrobial drugs tested in this study and were considered to have XDR. The underlying diseases of the patients whose isolates were examined varied ( Table ). Although 2 isolates with XDR were colonizers, 6 caused infections. All but 1 patient was treated with mechanical ventilation before isolation of the pathogen. Number of hospital days before isolation of _A. baumannii_ was 13–256 days, and the number of ICU days before isolation was 2–38 days. Four patients were immunocompromised, and 3 had bacteremia. Among the patients with infections characterized by XDR, the overall 30-day mortality rate was 66.7%, and the infection-related 30-day mortality rate was 50.0%. All 8 isolates with XDR showed common characteristics: ST22 containing OXA-23, OXA-66, TEM-116, PER-1, and ADC-29.\n\n【5】We report the emergence of XDR in PDR _A. baumannii_ isolates in South Korea. Characteristics of PDR _A. baumannii_ isolates suggest that they spread from a single clone. A single _A. baumannii_ strain with XDR might evolve from the prevailing PDR _A. baumannii_ and could disseminate in the ICU, probably after contamination of the hospital environment and by nosocomial transmission. In South Korea, a high resistance rate to imipenem and meropenem in _Acinetobacter_ spp. isolates may lead to extensive use of polymyxins ( _3_ ). Thus, we can hypothesize that the most prevalent carbapenem-resistant, or MDR _A. baumannii_ clone, became PDR and then evolved into clones with XDR by acquisition of polymyxin resistance caused by antimicrobial pressure. Our investigation showed a simultaneous emergence of resistance to all antimicrobial agents available, including colistin, polymyxin B, and tigecycline. XDR poses serious problems in the treatment of patients with _A. baumannii_ infections, especially given the slow development of new antimicrobial agents.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "725c45ad-c53c-4c7a-98c8-44d1bf2e61a8", "title": "Evidence-based Tool for Triggering School Closures during Influenza Outbreaks, Japan", "text": "【0】Evidence-based Tool for Triggering School Closures during Influenza Outbreaks, Japan\nInfluenza pandemic preparedness and seasonal influenza control programs have focused on vaccine development and antiviral drugs, which are only partially effective and not always available to all persons at risk ( _1_ _–_ _3_ ). Nonpharmaceutical interventions, such as social distancing, represent additional key tools for mitigating the impact of outbreaks. Because children are a major factor in the transmission of influenza within communities and among households, school closure may be a valuable social distancing method ( _4_ _,_ _5_ ).\n\n【1】Japan has a unique system of monitoring school absenteeism and of instituting school closures during influenza outbreaks. Individual classes, specific grade levels, or the entire school may be closed; final decision-making authority is given to school principals. However, as in the United States and other countries, there are no regulations to support these decisions ( _6_ ). Our study suggests a simple system to help determine when schools should be closed; daily influenza-related absentee thresholds are measured to predict outbreaks.\n\n【2】### The Study\n\n【3】We used data on absenteeism caused by influenza from the 54 elementary schools in Joetsu City, Niigata Prefecture, Japan during the 4 influenza seasons during 2005–2008. Data was obtained between the second week of January to the third week of March for each influenza season. Average school size was 221 students. Current public health policy prevents influenza-infected children from attending school until 2 days after fever has disappeared. An illness requires 2 physician visits: 1 for the initial diagnosis and 1 to obtain written permission from the treating physician to return to school. Diagnoses are usually made by using a rapid antigen test and patients are treated with the antiviral drugs, oseltamivir or zanamivir.\n\n【4】Figure 1\n\n【5】Figure 1 . Four-year surveillance of influenza-related absentee rates in 54 elementary schools in Joetsu City and national surveillance of influenza-like illness (ILI) reported by sentinel physicians in Japan. Data were collected from the...\n\n【6】Based on elementary school daily influenza-related absentee surveillance, the most intense influenza seasons were 2005 and 2007 ( Figure 1 ). The number of schools reporting outbreaks during the 4 influenza seasons was 34 (63%, 2005), 13 (24%, 2006), 35 (65%, 2007) and 18 (33%, 2008), respectively. Rates of absenteeism caused by confirmed influenza infection in the 54 elementary schools in Joetsu City were well correlated with national reports of influenza-like illness by 5,000 sentinel physicians, who reported 322, 205, 226, and 142 cumulative cases of infection per sentinel in each season ( Technical Appendix ).\n\n【7】We evaluated the optimal influenza-related absentee rate for predicting outbreaks of influenza. For this study, we defined an influenza outbreak in a school as a daily influenza-related absentee rate of >10%, on the basis of the 95th percentile of daily absentee rates (10.7%) in 54 elementary schools during 4 influenza seasons ( Technical Appendix ).\n\n【8】Next, we considered 9 different daily influenza-related absentee threshold levels for initiating early school closures: 1%, 2%, 3%, … , 9%. In addition, for each threshold level, we considered 3 scenarios: 1) a single-day scenario, in which daily influenza-related absentee rates are observed for the first time above a given threshold for 1 day; 2) a double-day scenario, in which rates reached a given threshold for the first time for 2 consecutive days; the rate for the second day was the same or higher than for the first day; and 3) a triple-day scenario, in which rates reached a given threshold for the first time for 3 consecutive days; rates for the second and third days were the same or higher than the rate for the first day. The double-day and triple-day scenarios did not include weekends. To evaluate the performance of prediction for each threshold, we determined the school’s outbreak status in the 7-day period starting on the first day of each scenario ( Technical Appendix ) JMP7.0.1 (SAS Institute, Inc., Cary, NC, USA) was used for statistical analysis.\n\n【9】Figure 2\n\n【10】Figure 2 . The receiver operating characteristic (ROC) curve for detection of influenza outbreak by 1%–9% thresholds under single-day, double-day, triple-day scenarios. ROC space is defined on the x axis as 1 – specificity...\n\n【11】We calculated the sensitivity and specificity of each scenario at all 9 threshold levels, and presented these data as a plot in Figure 2 . The area under the curve for the single-, double-, and triple-day scenarios was 0.80 (95% confidence interval \\[CI\\] 0.77–0.83), 0.85 (95% CI 0.82–0.89) and 0.87 (95% CI 0.83–0.91), respectively.\n\n【12】We used the Youden index for calculating optimal thresholds ( _7_ ). The Youden index = (sensitivity) + (specificity) – 1. A perfect test result would have a Youden index of 1. For the single-day scenario, the optimal threshold was 5%, with a sensitivity of 0.77 and specificity of 0.73. For the double-day scenario, the optimal threshold was 4%, with a sensitivity of 0.84 and specificity of 0.77. For the triple-day scenario, the optimal threshold was 3%, with a sensitivity of 0.90 and specificity of 0.72.\n\n【13】### Conclusions\n\n【14】We have demonstrated the predictive value of a simple and practical detection method for triggering school closures early after influenza outbreaks. Our analysis suggests that a single-day at a threshold influenza-related absentee rate of 5%, double-days \\> 4%, or triple-days \\> 3% are optimal levels for alerting school administrators to consider school closure. The double- and triple-day scenarios performed similarly, and gave better results than the single-day. Thus, the double-day scenario might be the preferred early warning trigger.\n\n【15】Our study had the advantage of reliable empirical data on influenza-related absenteeism in schools. Data were based on physician and laboratory diagnosis and a strong absentee surveillance program. However, there are limitations to our approach. We did not have available vaccination or medication histories of patients. Also, our results are based on data from only 1 city’s school district; validation in a broader area will be required. Although separate analyses may be required for other geographic regions, we present a simple approach that can be easily reapplied.\n\n【16】Influenza outbreak detection from surveillance data typically relies on relatively complex time series analysis or smoothing ( _8_ _,_ _9_ ). The noisiness of school surveillance data makes detection of outbreaks difficult ( _10_ ). However, complex statistical analyses are not practical to use in the context of daily decision-making in schools. Despite the limitations of our study, we have presented a method that provides a basis for empirical data-supported decision-making by school administrators that is intuitive and practical.\n\n【17】School closure could be an effective method of social distancing, although evidence supporting its effectiveness is incomplete. Some studies suggest that though child-to-child transmission might decrease, transmission might increase in other age groups ( _11_ _,_ _12_ ). During school closures, children may need to forgo participation in external activities that could increase contact rates. Additionally, working parents staying home to care for their children ( _13_ ) could result in a decrease in household income, causing loss of productivity and economic losses ( _14_ ). Decision-makers will need to consider these factors when considering school closures.\n\n【18】During the early days of the outbreak of influenza A pandemic (H1N1) 2009 virus, the US Centers for Disease Control and Prevention (Atlanta, GA, USA) released 2 different recommendations for school dismissal after the appearance of the first suspected case: dismiss for 7 days (as of April 26) and then for 14 days (as of May 1). Later, to reflect new knowledge about the extent of community spread and disease severity, the recommendation was revised to advise against school closure unless absentee rates interfered with school function ( _15_ ). The pandemic (H1N1) 2009 influenza outbreak highlights the need for a flexible national policy that can be quickly adapted to reflect current situations. The evidence-based strategy for predicting outbreaks based on influenza-related absentee rates that we present here provides local administrators, who may need to consider school closure, with a simple and practical tool to aid in their decisions.\n\n【19】Dr Sasaki is an associate professor at the Department of Health and Nutrition, University of Niigata Prefecture. Her research focuses on the epidemiology of infectious diseases and health surveillance systems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "58a641cc-206a-43af-8618-e2cefc022b14", "title": "Unique Perspectives", "text": "【0】Unique Perspectives\nPaul Cezanne (b. 1839) Still Life with Apples (1893–1894). Oil on canvas (25 3/4 × 32 1/8 in/65.4 × 81.6 cm). Digital image courtesy of the Getty's Open Content Program, The J. Paul Getty Museum, Los Angeles, CA.\n\n【1】Paul Cezanne was a visionary who bridged 19th century impressionism with 20th century cubism and other modern styles. Born in Aix-en-Provence, France, into the family of a successful banker, Cezanne had a comfortable childhood, culminating in studies at the College Bourbon. There he met lifelong friends Emile Zola and Baptistin Baille, who would become prominent scholars in literature and acoustics, respectively.\n\n【2】Cezanne enrolled in the Free Municipal School of Drawing in Aix-en-Provence, but his father pressured him to enter law school to focus on more “practical” pursuits. Cezanne continued taking drawing lessons. Defying his father’s wishes, he moved to Paris in 1861 to join Zola and pursue becoming an artist. His father eventually softened to Cezanne’s career choice and granted him a large inheritance that freed Cezanne from financial uncertainty. Cezanne focused his early art on the whimsical landscapes of his childhood. As his style matured, he became captivated with defining paintings by simple shapes, exceptional lighting, and uncommon perspectives. By the 1890s, he had achieved artistic and financial success in Paris but chose to return to his native Provence where he preferred to paint.\n\n【3】_Still Life with Apples_ (1893–1894) is a deconstruction of the impressionist art of the era. In this still life, the displayed apples monopolize the imagery, but their position on the table draws the gaze to the right. Cezanne overlaps and blends the vases and bottle, in a style that is in contrast to the full and complete forms championed by impressionists. The haphazard table cloths invoke the chaotic turbulence of ocean water, adding motion to the stillness of the painting. The red, green, and yellow hues of the apples emphasize their prominence against the softer colors of the table and vases. The perspective of the painting is also unique to Cezanne’s style: each piece of fruit, the bottle, the pots, and even the plate, retains an individual presence, giving them an independence from one another.\n\n【4】Cezanne’s interest in still life paintings was about the form, shade, and color of the presented objects. Cezanne stresses the shape and position of the apples over their functionality and appeal as a source for nourishment. The emphasis on shape allowed Cezanne to subvert the tropes of classical impressionism by demonstrating a unique viewpoint on how the apple’s form occupies its given space.\n\n【5】Cezanne asks people to look at the apples of his painting from a different perspective, and this may serve as a lesson for those who work to promote food safety. The food we eat may be touched by many people and rests on many surfaces before it comes to our plates. Our food may contact a multitude of microbes during its journey from farm or field to table. The capacity to detect, investigate, and prevent food-borne diseases varies across the world. Adapting surveillance processes and prevention techniques may require new approaches and unique perspectives, in much the way Cezanne implemented innovations to impressionism when it grew too limiting.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db34445a-cba4-4d79-8a60-4d7f7b7ad74e", "title": "Use of a Diagonal Approach to Health System Strengthening and Measles Elimination after a Large Nationwide Outbreak in Mongolia", "text": "【0】Use of a Diagonal Approach to Health System Strengthening and Measles Elimination after a Large Nationwide Outbreak in Mongolia\nMeasles, a highly transmissible infectious disease that causes serious illness and death worldwide, is often referred to as a public health “canary in the coalmine” because it can be used as both a signal of weak health systems and a driver for strategies and policies to strengthen health systems ( _1_ ). When programmatic weaknesses in immunization systems occur, measles is frequently the first vaccine-preventable disease (VPD) detected ( _2_ – _5_ ). Moreover, because of the high transmissibility of measles virus, the recognizable clinical presentation of nearly all cases in high-incidence settings, the high efficacy of the vaccine for prevention, and lifelong immunity after vaccination or acute infection, measles epidemiology generally reflects population susceptibility and indicates vulnerable communities, areas with lack of response capacity, and weaknesses in the health system ( _6_ , _7_ ). Measles elimination, therefore, becomes a useful vehicle to achieve broad strengthening of the overall health system ( _8_ ). The “canary in the coalmine” approach to measles elimination efforts takes advantage of vertical strategies that focus on using surveillance data for action and to identify areas missed by vaccination, and of horizontal strategies that build systems and health services to sustain the gains and achieve broader objectives. The combination of these approaches has been described as a diagonal approach ( _9_ ).\n\n【1】The Global Vaccine Action Plan (GVAP), approved by the World Health Assembly in 2012, set targets for vaccination coverage and a goal to achieve measles and rubella elimination in 5 of the 6 World Health Organization (WHO) regions by 2020 ( _10_ ). In 2012, the Measles & Rubella Initiative partners launched the Global Measles and Rubella Strategic Plan 2012–2020 with targets aligned to the GVAP ( _11_ ). Measles-driven policies and elimination strategies can provide opportunities for improving immunization service delivery performance, as well as strengthening health systems to help achieve the United Nations Sustainable Development Goals and Universal Health Coverage ( _8_ , _9_ , _12_ ). The Global Health Security Agenda (GHSA) is a partnership between governments, multilateral organizations, and civil society launched in 2014 to promote global health security against infectious disease threats and drive full implementation of the WHO International Health Regulations (IHR 2005) ( _13_ ), organized within a framework of Prevent-Detect-Respond ( _14_ ). Recognizing that immunization is a key requirement to advancing global health security ( _12_ ), the framework includes monitoring of measles vaccination coverage as a GHSA performance indicator, dovetailing with ongoing efforts to increase vaccination coverage and achieve measles elimination ( _11_ , _15_ ).\n\n【2】Mongolia, a WHO member state in the Western Pacific Region (WPR), participates in the GHSA ( _16_ ) and has received support to strengthen IHR 2005 capabilities and response capacity for public health events of international concern. In 2009, Mongolia established an Early Warning, Alert, and Response Network (EWARN) ( _17_ ) to supplement existing disease-specific, case-based surveillance systems by collecting syndromic event-based data from public primary health facilities. With GHSA support, a national public health Emergency Operations Center (EOC) and corresponding Incident Management System (IMS) were established in 2015 to coordinate response activities, particularly during outbreaks. In addition, satellite emergency response hubs termed Emergency Operations Points (EOPs) were established at national public health agencies.\n\n【3】In March 2014, the WHO WPR Verification Commission for Measles Elimination verified that measles elimination, which is defined as no measles case reported for 36 months in a country meeting required program performance indicators ( _18_ ), had been achieved in Mongolia. However, in March 2015, multiple laboratory-confirmed measles cases were detected in the capital city, Ulaanbaatar; by June 5, 2015, a total of 11,181 suspected cases had been reported nationwide from all 21 provinces ( _19_ ). The government of Mongolia requested that WHO and the US Centers for Disease Control and Prevention, in collaboration with the Ministry of Health and Sports (MOHS), conduct an outbreak investigation to assess factors contributing to ongoing transmission and provide recommendations for outbreak response and elimination strategies. In addition to identifying risk factors for transmission and evaluating the response vaccination activities and strategies, we used the measles outbreak as an opportunity to conduct a broader evaluation of the health system and emergency response strategies, following the GHSA framework, to prevent future outbreaks in Mongolia. Because nosocomial transmission of measles virus was identified early in the investigation as being a possible contributor to the outbreak, we conducted an assessment of infection prevention and control (IPC) practices in select healthcare facilities (HCFs). We also reviewed surveillance data, standard operating procedures (SOPs), and practices, and evaluated national emergency preparedness activities and response processes during the outbreak.\n\n【4】### Methods\n\n【5】##### Outbreak Investigation\n\n【6】To better describe the epidemiology of healthcare-associated measles and to identify and recommend prevention measures, we reviewed data from case-based surveillance for March 1, 2015–June 26, 2016. Confirmed cases were either laboratory confirmed by positive test result for measles-specific IgM ELISA or PCR or clinically confirmed by meeting criteria of rash plus fever and \\> 1 of the following: cough, coryza, or conjunctivitis. We also reviewed National Center for Communicable Diseases (NCCD) measles surveillance data for cases with onset during December 1, 2015–June 27, 2016, during which period-specific healthcare exposures were collected for case-patients. We defined healthcare-associated cases as laboratory-confirmed measles virus infection in a patient who was a healthcare worker (HCW) or who was hospitalized (non-HCW) during the 7–21 days (measles incubation period) preceding onset of signs or symptoms and who had an epidemiologic link to a hospitalized case-patient or lacked a known community source.\n\n【7】##### Assessment of IPC Policies and Practices (Prevent)\n\n【8】We assessed IPC practices at 3 hospitals in Ulaanbaatar with a large number of reported outbreak cases in surveillance data: 2 national referral tertiary care hospitals (1 of which was NCCD, the national HCF for infectious diseases) and 1 district hospital. We also assessed 1 primary care facility. At the 4 selected HCFs, we conducted structured interviews of facility staff and directly observed IPC practices and compliance with MOHS guidance and recommendations from previously published IPC documents ( _20_ – _25_ ). We reviewed MOHS occupational health policy, MOHS bulletins to HCFs, and HCF occupational health policies to evaluate vaccination and furlough policies.\n\n【9】##### Assessment of Surveillance (Detect)\n\n【10】We reviewed policies, SOPs, and protocols, conducted key informant interviews, and analyzed data for January 1, 2014–June 27, 2016. We used this information to assess national laboratory-supported measles case-based surveillance and EWARN surveillance for fever and rash syndrome.\n\n【11】##### Assessment of Emergency Preparedness and Outbreak Response (Respond)\n\n【12】We conducted interviews with key stakeholders at national and subnational levels of the emergency response system and reviewed EOC, EOP, and IMS SOPs. We identified and mapped roles, responsibilities, and mechanisms and verified them with stakeholders. After the investigation, we held a consultative training workshop with MOHS, NCCD, and Mongolia Field Epidemiology Training Program (FETP) staff to formulate specific recommendations on the basis of evidence from the investigation findings.\n\n【13】### Findings and Recommendations\n\n【14】##### Outbreak Investigation\n\n【15】Figure 1\n\n【16】Figure 1 . Confirmed measles cases in Mongolia, March 1, 2015–Jun 27, 2016. A) Confirmed cases by epidemiologic week of rash onset and reported exposure to a healthcare facility during the 7–21 days (measles...\n\n【17】Of 33,947 confirmed case-patients with rash onset during March 1, 2015–June 27, 2016, a total of 14,407 (42%) were hospitalized and 2,222 (7%) reported visiting an HCF during the incubation period before rash onset, particularly during the initial phase of each of the 2 waves of intense transmission in 2015 and 2016, when ≈25% of cases had HCF exposure ( Figure 1 ). During December 1, 2015–June 27, 2016, we identified 603 total healthcare-associated measles cases. Of these, 55 (9%) occurred in HCWs; 220 (36%) occurred in infants \\> 9 months of age who were eligible for routine measles vaccination; and 448 (74%) occurred in infants \\> 6 months of age who were therefore eligible for postexposure or outbreak response measles vaccination.\n\n【18】##### Prevent: IPC Assessment\n\n【19】Some IPC policies were available, but lack of corresponding infrastructure limited proper infection control to prevent measles virus transmission in hospitals. For example, we found inconsistent implementation of appropriate procedures for isolation or cohorting of confirmed measles cases; in addition, no negative pressure isolation rooms existed in any of the HCFs visited, and only 1 airborne isolation room existed in the country.\n\n【20】Policies and SOPs for measles contact tracing and postexposure prophylaxis (PEP) in HCFs existed; however, these recommendations were generally not practiced during the outbreak. The National Standard on Measles Surveillance Guidelines from 2003 recommended routine contact tracing of measles cases and, where appropriate, administration of measles-containing vaccine (MCV) or immunoglobulin as PEP ( _26_ ). However, we found that contact tracing efforts in HCFs became quickly overwhelmed by the increasing case counts, primarily because of limited financial and human resources. Specific guidance for measles PEP in HCFs was not provided during the outbreak, and MCV and immunoglobulin supplies were not made available for PEP.\n\n【21】Occupational health safeguards to prevent measles generally were not present. Proof of measles vaccination was not a mandatory condition of employment in HCFs; records of measles immunity status were not routinely kept at HCFs. Although MCV was reportedly offered to HCWs during the outbreak, we found inconsistent provision of the vaccine for HCWs, and records of staff vaccination during the outbreak were not available to review. Nonimmune HCWs were not furloughed or temporarily reassigned from patient care activities after measles exposure, unless and until febrile rash illness developed. In addition, HCWs who were furloughed did not receive a salary during the furlough period. Therefore, HCWs likely worked providing care to patients during the highly contagious period that begins 4 days before rash onset and lasts until 4 days after rash onset.\n\n【22】##### Detect: Disease Surveillance Assessment\n\n【23】According to surveillance protocols, cases detected by EWARN meeting the syndromic case definition of fever with maculopapular rash are investigated and also reported through the measles case-based surveillance system, using an individual case investigation form and collecting a specimen for laboratory testing for case confirmation. Surveillance protocols did not distinguish between appropriate procedures for routine surveillance and enhancements to surveillance that are needed during outbreaks and did not include parameters on when to scale back specimen collection or how to perform epidemiologic linkage for case confirmation.\n\n【24】Cases from epidemiologic and laboratory surveillance databases were not linked by using the standard practice of assigning unique identifiers to each case and specimen. More than 14,000 specimens were collected and tested during this outbreak, overwhelming the national reference laboratory and leading to delays in case confirmation. Epidemiologic linkage was not performed uniformly or according to the WHO WPR recommended case classification algorithm ( _27_ ). Trends in EWARN and case-based surveillance were not routinely compared, and compatible cases detected by EWARN were not consistently reported and investigated through the case-based system. EWARN data indicated an initial increase in fever and rash cases beginning in epidemiologic week 17 of 2014. However, we found discrepancies between EWARN and case-based data in 2014, with much lower sensitivity in the case-based system, possibly leading to delayed detection of initial cases as many suspected cases were not investigated and tested. The first confirmed cases were detected in epidemiologic week 9 of 2015, in Ulaanbaatar and in Umnogovi Province, bordering China.\n\n【25】##### Respond: Emergency Preparedness and Outbreak Response Assessment\n\n【26】The IMS SOPs and staffing needs for the national EOC and HCF EOPs were still under development at the time of the outbreak, which limited the coordination capacity of the IMS during the outbreak. The EOC was not staffed until May 2016, as the outbreak was winding down, and even once staffed, it was never activated. Relationships between and roles of the EOC and EOPs were not clearly delineated. There was limited preallocation of resources and funding to the EOC and EOPs in the event of a public health emergency, delaying and constraining response activities. The response lead, termed the Event Manager in Mongolia, did not have the authority to release funds or resources without substantial review by supervisors, also delaying response activities. Frequent reassessments and/or risk assessments of the outbreak and response activities to ensure that needs matched the available resources were not performed. Finally, no national outbreak preparedness and response plan existed that identified the basic needs for measles outbreaks (i.e., vaccination, airborne precautions, laboratory support) or SOPs outlining airborne disease outbreak response activities.\n\n【27】Figure 2\n\n【28】Figure 2 . Flowcharts for organization of the Incident Management System in Mongolia during (A) and after (B) the 2015–2016 measles outbreak. Restructuring of the system after the outbreak was designed to better align...\n\n【29】The NCCD EOP was formally activated in December 2015 to lead the measles outbreak response. The NCCD EOP used a draft IMS proposal, and although the draft covered basic sections required in a public health emergency response (logistics and finance sections), the structure ( Figure 2 , panel A) did not mirror standard IMS structure as recommended by WHO ( _28_ ). In addition, critical organizational subdivisions required for a successful measles outbreak response were not delineated in the structure, such as the inclusion of operations teams to support epidemiologic investigation (case investigation and contact tracing) and IPC activities ( Figure 2 , panel B).\n\n【30】Response demands exceeded the capacity of available NCCD EOP staff and resources, especially at the outbreak peak. No staff roster or surge capacity were available to mobilize staff from other national agencies that had applicable skill sets (e.g., epidemiologists, intensivists, logisticians, laboratorians, FETP) to address this deficit.\n\n【31】##### Selected Recommendations\n\n【32】As a result of our investigation, we developed several recommendations. These recommendations addressed the gaps in policy, practice, and infrastructure identified as likely contributing causes of the outbreak and sustained virus transmission.\n\n【33】##### Prevent\n\n【34】Recommendations for long-term systems strengthening included improving physical building infrastructure necessary for proper IPC of measles and other contagious respiratory diseases. In the short term, measles contact tracing and PEP in HCFs should be implemented according to existing national guidelines. MCV and immunoglobulin for PEP should be stockpiled and mechanisms developed for rapid mobilization and delivery once a measles outbreak is confirmed. To limit healthcare-acquired transmission during measles outbreaks, only staff who have 2 documented MCV doses or evidence of immunity through serologic testing should be allowed to interact with patients ( _29_ ). HCFs should maintain records of staff measles immunity status, proactively identify staff without immunity, and provide MCV. HCWs should be encouraged to remain at home when they feel ill and should not suffer financial losses for doing so.\n\n【35】##### Detect\n\n【36】A comprehensive surveillance review should be conducted to identify gaps in surveillance performance and improve data flow to decision makers for prompt, effective action. One of our key recommendations was to establish coordination mechanisms to align EWARN and case-based VPD surveillance systems so that cases are adequately and promptly investigated and so that trends in one surveillance system trigger enhanced surveillance mechanisms in the other surveillance system. In addition, we recommended improved linkage between epidemiologic investigation and laboratory testing, appropriate use of a unique identifier variable, and case classification including epidemiologic linkage for case confirmation.\n\n【37】##### Respond\n\n【38】We recommended that a national outbreak preparedness and response plan for measles and other airborne infectious diseases be established and agreed upon by all relevant stakeholders. Emergency response SOPs should be finalized as an urgent preparedness activity to map out the organizational structure per WHO recommendations, define the interaction of the EOC and EOPs, delineate procedures for activation and deactivation, define roles and responsibilities of positions in the IMS structure, and outline data flow and communication mechanisms with national and subnational staff and partner agencies ( _28_ ). Emergency response SOPs should incorporate lessons learned from previous outbreaks and should be distributed to all stakeholders at each level of the public health system, including primary health clinics. The IMS could be strengthened by mapping out further subdivisions that are required for an effective outbreak response for airborne diseases ( Figure 2 , panel B). The EOC and EOPs should implement training for staff members regarding their specific roles in emergency response and run periodic exercises, using a mock measles outbreak scenario, to test the SOPs and emergency response capacity and coordination with relevant public health stakeholders outside of the EOC and EOPs. Systems breakdowns identified through these activities should lead to the refinement of emergency preparedness and response guidelines and other relevant SOPs, such as those for IPC and surveillance.\n\n【39】### Conclusions\n\n【40】Until measles is eradicated worldwide, the risk for measles virus importations and subsequent outbreaks will remain in countries such as Mongolia that have achieved measles elimination. Prevention of large measles outbreaks that may occur after virus importations can be achieved by implementing measles elimination strategies, maintaining high 2-dose measles vaccination coverage, and developing robust capacity for rapid response. Measles outbreaks in postelimination settings can provide valuable lessons on how to prevent and overcome hurdles on the road to eradication and can reveal weaknesses in health systems that might undermine control efforts for other infectious diseases.\n\n【41】Preventing and controlling measles outbreaks require established policies and procedures that pay special attention to specific settings where measles virus introduction and sustained transmission may occur. For example, HCFs can serve as amplification points for outbreaks of measles and other infectious diseases ( _30_ – _33_ ). Given the universal challenges of efforts to quickly identify cases of infectious diseases and appropriately triage patients in busy HCFs, vaccination of all HCWs and use of PEP should be prioritized because these methods are likely the most effective strategies to prevent and reduce healthcare-associated measles.\n\n【42】Rapid detection and response to measles outbreaks is essential for elimination efforts and can prevent infections and reduce the number of deaths ( _34_ , _35_ ). The IHR 2005 and GHSA frameworks outline guidance on surveillance system strengthening to ensure countries have the capacity to detect and respond to outbreaks of VPDs such as measles, as well as new and emerging pathogens ( _13_ , _36_ ). As a part of this guidance, syndromic surveillance systems such as EWARN should be used to provide sensitive signals of major public health events but must be linked to systems for immediate case investigation, confirmation, and coordinated response activities, ideally through an incident management system or its equivalent. EWARN should be tightly linked with case-based surveillance through routine data sharing. Case-based surveillance is a key requirement for achieving measles and rubella elimination and provides the added benefit of being a standard against which signals from parallel syndromic surveillance systems such as EWARN can be checked and calibrated.\n\n【43】Achieving successful control of measles outbreaks requires a multifaceted strategy involving surveillance, laboratory capacity, contact tracing, vaccination, and hospital IPC measures; thus, maintaining a capacity for coordination of activities is critical for an effective, cohesive outbreak response ( _37_ ). During measles outbreaks, the speed and completeness of response measures are critical and dictate the extent of measles transmission and burden of disease ( _38_ ). Delaying or poorly implementing response efforts such as contact tracing and targeted vaccination can lead to an exponential increase in additional exposures, infectious cases, hospitalizations, and substantial geographic spread, which can quickly overwhelm existing healthcare infrastructure, leading to further amplification of the outbreak. Examining overall national emergency response capacity is essential not only to evaluate how the country will react to another measles outbreak but also to identify gaps that are applicable to other potential epidemic-prone diseases. Our multidisciplinary assessment resulted in specific, actionable recommendations for strengthening the structure and effectiveness of emergency response planning, which, if properly implemented, will have a wide-reaching effect on the reduction of illness and death during public health emergencies.\n\n【44】The broad health systems assessment we conducted, following the Prevent-Detect-Respond framework of the GHSA, is an example of one tactical element of a comprehensive diagonal approach to link measles elimination with immunization program and health system strengthening. Other proposed tactical elements included reaching the chronically unreached by using measles risk assessments and campaigns to identify and target underserved populations and geographies; introducing routine use of a second dose of measles vaccine to create new opportunities to receive vaccines and other child health interventions in the second year of life and beyond; and advocating for measles elimination to support institutions, policies, and practices needed to sustain high-quality immunization programs ( _9_ ). The diagonal approach has successfully leveraged activities aimed at specific disease elimination or eradication efforts to strengthen health systems and overall immunization service delivery performance. For example, in several settings, including South Korea, Sri Lanka, the United States, and some provinces in China, school entry vaccination check laws have had a broad effect on overall coverage and equity of immunizations ( _39_ – _44_ ). Similarly, strengthening laboratory-supported surveillance systems and outbreak response capacity (including local epidemiologic capacity through FETP programs) to achieve elimination enables improved capacity to monitor surveillance performance and to detect other VPDs, such as yellow fever, Japanese encephalitis, and emerging diseases such as Ebola and Zika. For example, the existing polio eradication infrastructure in West Africa was a critical platform that was leveraged to enable rapid case detection, investigation, confirmation, and contact tracing as part of the Ebola outbreak response during 2014–2015 ( _45_ ). In addition, established case-based surveillance systems for measles or dengue have been used to detect cases of Zika in settings where that disease is an emerging epidemic ( _46_ ).\n\n【45】When measles outbreaks occur because of gaps in the confluence of multiple sectors of health systems that include immunization, IPC, surveillance, and emergency response, the GHSA framework provides useful tools to leverage outbreak investigations to strengthen the overall health system and prevent future outbreaks of measles and other infectious disease threats. In this way, GHSA investments to Prevent-Detect-Respond reduce rates of illness and death. Even relatively small measles outbreaks can have substantial cost implications ( _7_ ); investments in measles vaccination in low- and middle-income countries yield a positive economic return on investment of 27–67 times the cost ( _47_ ). By using the substantial multilateral investments by countries and donors to global health partnerships including GHSA, GVAP, and the Measles & Rubella Initiative to strategically strengthen health systems with a diagonal approach to measles elimination, this positive return on investment could become exponentially higher.\n\n【46】Dr. Hagan is a medical epidemiologist with the Accelerated Disease Control and Vaccine Preventable Diseases Branch, Global Immunization Division, Center for Global Health, Centers for Disease Control and Prevention. He is on the Measles Elimination Team and focuses on global measles epidemiology, measles elimination policy and programmatic issues, and outbreak response.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7281d560-a387-4dd5-a665-82d0cb505832", "title": "Independent Origin of Plasmodium falciparum Antifolate Super-Resistance, Uganda, Tanzania, and Ethiopia", "text": "【0】Independent Origin of Plasmodium falciparum Antifolate Super-Resistance, Uganda, Tanzania, and Ethiopia\nControlling and reducing malaria requires a combination of vector control measures and administration of antimalarial drugs as prophylaxis or treatment ( _1_ ). The widespread use of antimalarial drugs has resulted in the emergence of resistant _Plasmodium falciparum_ , recurrently exposing persons in malaria-endemic regions to an unacceptably high risk for treatment failures ( _2_ ).\n\n【1】Highly chloroquine-resistant parasites spread from Asia in the 1960s and led to devastating rates of malaria-related death in Africa starting in the late 1980s, gradually forcing affected countries to replace chloroquine with sulfadoxine–pyrimethamine (SP) ( _3_ – _5_ ). The effectiveness of SP did not last long. In fact, retrospective analysis indicated that pyrimethamine-resistant parasites were present in sub-Saharan Africa before SP was implemented as first-line treatment, probably because pyrimethamine as monotherapy had been used in Asia during the 1960’s and 1970’s ( _6_ – _8_ ). Resistance to sulfadoxine also soon emerged ( _9_ ), and the combination of pyrimethamine- and sulfadoxine-resistant parasites led to severe and widespread SP treatment failure ( _10_ – _12_ ). As a consequence, affected countries were once again forced to change their drug policies ( _13_ ) and have now adopted artemisinin-based combination therapies as first-line treatment for uncomplicated malaria. Yet, SP is still recommended for use as intermittent preventive treatment in pregnant women (SP-IPTp) and infants (SP-IPTi) ( _14_ , _15_ ). Also, seasonal malaria chemoprevention applies SP in combination with amodiaquine ( _16_ ). Use of SP for prevention in many countries of sub-Saharan Africa, where clinical failure after SP treatment has been reported, underscores the need for effective surveillance of its protective efficacy and for monitoring of the development and spread of SP resistance in _P. falciparum_ populations.\n\n【2】The molecular basis of SP resistance is a combination of single-nucleotide polymorphisms (SNPs) in 2 distinct genes coding for the target enzymes of SP. The enzymes dihydrofolate reductase (DHFR) and dihydropteroate synthetase (DHPS) are targeted by pyrimethamine and sulfadoxine, respectively ( _17_ ). High-level pyrimethamine resistance is generally encoded by 3 mutations in the _Pfdhfr_ gene, coding for substitutions: N51I, C59R, and S108N ( _18_ ); the molecular basis of sulfadoxine resistance is caused by substitutions S/A436F, A437G, K540E, A581G, and A613S/T in a variety of combinations in DHPS ( _19_ ).\n\n【3】The most prevalent genotype in eastern Africa is a combination of the _Pfdhfr_ triple mutant (51I, 59R, and 108N, denoted as IRN) combined with the _Pfdhps_ double mutant (S436, 437G, 540E, A581, and A613, denoted as SGEAA). Together, this combination of SNPs is referred to as the “quintuple” mutant _Pfdhfr_ / _Pfdhps_ genotype and is associated with high risk for SP treatment failure ( _17_ ) and results in limited protective value of SP-IPTi ( _20_ ). Accordingly, the World Health Organization (WHO) recommends that SP-IPTi should be implemented only when the prevalence of the K540E mutation (and thus the quintuple mutant) is <50% ( _14_ ).\n\n【4】More recently, an alanine to glycine mutation at codon position 581 in _Pfdhps_ has emerged that, in combination with the _Pfdhfr_ triple-mutant allele IRN, was shown to confer higher level resistance ( _21_ ). This combination, referred to as the “sextuple _Pfdhfr_ / _Pfdhps_ mutant genotype” or the “super-resistant genotype” ( _22_ ), is associated with reduced SP-IPTp efficacy by 1) a reduction in the protection period of SP-IPTp from 4 weeks to 2 weeks ( _23_ ); 2) increased parasitemia attributed to competitive facilitation ( _23_ ); 3) increased risk for severe malaria in the offspring ( _24_ ); and 4) low birthweight in newborns from mothers undergoing SP-IPTp in Tanzania ( _25_ ). Consequently, WHO recommendations concerning the use of SP-IPTp base the threshold on 2 mutations: SP-IPTp should be discontinued if the prevalence of the K540E mutation is >95% and the A581G mutation is >10% ( _20_ ). No threshold in the prevalence of molecular markers of resistance has been set with regard to seasonal malaria chemoprevention ( _15_ , _16_ ).\n\n【5】Maps collating all published data from molecular surveillance of _Pfdhfr_ and _Pfdhps_ mutations ( _22_ ) indicate 3 main foci of super-resistant parasites: 1 in northern Tanzania ( _26_ ); a second in southwestern Uganda, Rwanda, and bordering areas of Democratic Republic of Congo ( _27_ – _29_ ); and a third in western Kenya ( _30_ ). Prevalence of A581G also is high in Ethiopia and northern Sudan, where it again occurs as the _Pfdhps_ triple-mutant allele SGEGA but in combination with a _Pfdhfr_ double-mutant allele 51I-108N.\n\n【6】Assessments of microsatellite variation linked to _Pfdhps_ have shown that limited microsatellite diversity flanking the SGEAA double mutants compared with the SAKAA wild types. Two SGEAA lineages were discovered in eastern Africa: 1 prevailing in northeastern Africa (Ethiopia and Sudan) and the other throughout southeastern Africa. Both lineages derived from independent ancestry ( _10_ ). Here we apply the same approach, using the same microsatellite loci, to determine the ancestry and possible relationship between the double SGEAA and triple SGEGA alleles in Ethiopia, Uganda, and Tanzania. By focusing on microsatellite variation linked to _Pfdhps_ , we can explore whether the emergence of the SP-IPT-threatening SGEGA triple mutants in Ethiopia, Uganda, and Tanzania derive from local SGEAA alleles or are being imported.\n\n【7】### Materials and Methods\n\n【8】##### Study Sites\n\n【9】Samples for the study were collected during 2004–2008. Study sites were in Uganda (2 sites), Tanzania (3 sites), and Ethiopia (1 site) ( Table ).\n\n【10】##### Sample Collection\n\n【11】##### Bufundi and Rikungiri, Uganda\n\n【12】Uganda implemented SP-IPTp in 2000 and has not implemented SP-IPTi or seasonal malaria chemoprevention. Finger-prick blood-spot samples were obtained from symptomatic patients of all ages after _P. falciparum_ infection was confirmed by a Paracheck rapid test (Orchid Biomedical Systems, Chennai, India) during May–December 2005 at reference health facilities in Bufundi (Kabale District) (38 samples) and Kebisoni (Rukungiri District) (41 samples). Blood spots were air dried on Whatman no. 3 filter paper (VWR–Bie & Berntsen, Herlev, Denmark), sealed in plastic bags with a desiccant, and stored at room temperature for molecular genotyping ( _27_ ). The Uganda National Council for Science and Technology (UNSCT HS 35) and the ethics committee of the London School of Hygiene and Tropical Medicine (London, UK) gave scientific and ethical permission. Consent was obtained from all persons or their guardians before sample collection.\n\n【13】##### Hale, Korogwe, and Magoda, Tanzania\n\n【14】Tanzania implemented SP-IPTp in 2001 and has not implemented SP-IPTi or seasonal malaria chemoprevention. Samples were obtained from 3 different settings in Tanga region. From Hale (36 samples), finger-prick blood-spot samples were taken from symptomatic children 6–59 months of age who attended Hale Health Centre during July–August 2006 as previously described ( _21_ ). The study protocol was approved by the Ethics Review Committees of the National Institute for Medical Research, Tanzania, and the London School of Hygiene and Tropical Medicine and was registered as a clinical trial with the National Institutes of Health ( http://www.clinicaltrials.gov , identifier NCT00361114). From Korogwe (83 samples), finger-prick or venous blood samples were obtained on filter paper from children and adolescents <20 years of age from Mkokola and Kwamasimba villages. Samples were collected in March 2004, May 2006, and May 2007, as described ( _26_ ). The Medical Research Coordinating Committee of the National Institute for Medical Research and Ministry of Health, Tanzania, granted ethical clearance for the study. All participants or their parents or guardians provided informed consent. Samples from Magoda villages (22 samples) were collected from children <5 years of age in June 2008 as part of a cross-sectional assessment of malaria prevalence ( _31_ ).\n\n【15】##### Humera, Ethiopia\n\n【16】Ethiopia has adopted neither of the WHO recommendations regarding use of SP as prophylaxis. Samples were collected from patients of all ages who attended Kahsay Abera Hospital in Humera during January–April 2004 and who had symptomatic uncomplicated malaria ( _10_ ). The patients were enrolled in an in vivo efficacy trial, comparing artemether–lumefantrine therapy with SP therapy, which was conducted by staff of Kahsay Abera Hospital and the Mekele Regional Health Bureau. Finger-prick blood-spot samples were taken from patients before treatment after they gave written informed consent to participate in the study, and genetic analysis was conducted in support of the drug efficacy evaluation. The Ethical Clearance Committee of the Tigray Health Research Council and the external Ethics Review Board used by Médecins sans Frontières gave ethical permissions for the study.\n\n【17】##### Genotyping\n\n【18】Sample collection at the different study sites was not standardized because the samples derived from independent studies. However, all samples consisted of finger-prick blood spots stored on filter paper, and parasite DNA was extracted by using the Chelex method ( _32_ ).\n\n【19】Point mutations in samples from Hale, Tanzania, were determined by direct sequencing ( _21_ ). Sequencing was performed by using the ABI-3730 automatic sequencer (Applied Biosystems, Foster City, CA, USA), and samples were analyzed with Applied Biosystems BigDye V. 3.1 (Applied Biosystems).\n\n【20】For all other samples, the polymorphic region of _Pfdhps_ was PCR-amplified before sequence-specific oligonucleotide probing (SSOP) for mutations at codons 436, 437, 540, 581, and 613 by using primers and PCR conditions described elsewhere ( _33_ ). SSOP-genotyping of samples from Uganda and Ethiopia was conducted according to an SSOP–dot-blot method ( _10_ ); genotyping of samples from Korogwe and Magoda in Tanzania was conducted according to an SSOP-ELISA method ( _33_ ).\n\n【21】Only samples containing the _Pfdhps_ SGEAA or SGEGA alleles were included for further analysis; other alleles, such as wild-type or single-mutant alleles, were excluded. In general, only a single sequence was detected at every codon, but if the sequence analysis detected a mixture, these samples were handled as mixed infections. Mixed infections, in turn, were further analyzed only if 1 allele was substantially in the majority (i.e., a 2:1 signal ratio between the dominant genotype and the minor genotype) and a majority SNP could be confidently determined at all codon positions ( _33_ ).\n\n【22】##### Microsatellite Analysis\n\n【23】Analysis was performed on 3 _Pfdhps-_ linked microsatellites located 0.8 kb (marker \\[m.\\] 0.8), 4.3 kb (m. 4.3), and 7.7 kb (m. 7.7) downstream of the coding position 437 of _Pfdhps_ , located on chromosome 8 ( _34_ ). Microsatellites were amplified by seminested PCR as described previously ( _34_ ), and products were run with GeneScan-500 LIZ Size Standards (Applied Biosystems) in an ABI 3730 DNA analyzer (Applied Biosystems) and analyzed by using Genemapper software (Applied Biosystems). If >1 microsatellite allele was detected in any given sample, the peak height ratio was used to determine the majority allele for that locus. If the major allele did not have a peak height of at least double the height of the minor allele, the sample was excluded from further analysis.\n\n【24】Microsatellite haplotypes were constructed by combining alleles detected in each of the 3 microsatellite loci. Samples with missing data were not included.\n\n【25】### Results\n\n【26】A total of 300 samples with either _Pfdhps_ double-mutant (SGEAA) or triple-mutant (SGEGA) alleles were subjected to microsatellite analysis, and 277 (92.3%) of these gave conclusive results. Microsatellite haplotypes associated with the _Pfdhps_ double- and triple-mutant alleles are listed in full ( Technical Appendix Table), where the haplotypes are ranked hierarchically according to allele size, first at the 0.8-kb locus, then at 4.3-kb locus, and finally at the 7.7-kb locus, and each unique haplotype was assigned a number.\n\n【27】##### Diversity of Microsatellite Composition among SGEAA Samples\n\n【28】Figure\n\n【29】Figure . Proportion of microsatellite haplotypes linked to SGEAA and SGEGA, eastern AfricaMicrosatellite haplotypes associated with the _Pfdhps_ double-mutant allele SGEAA (A) and _Pfdhps_ triple-mutant allele SGEGA (B) in Ethiopia, Tanzania, and UgandaHaplotype...\n\n【30】SGEAA alleles from Ethiopia were associated with 4 different microsatellite haplotypes ( Figure , panel A; Table ). Haplotype 4 (notation 121–114–98, refers to fragment size 121 bp at the 0.8-kb locus, 114 bp at the 4.3-kb locus, and 98 bp at the 7.7-kb locus) predominated and was found in 30 (85.7%) of the 35 SGEAA alleles sampled. Of the remaining 3 microsatellite haplotypes, haplotypes 7 and 21 were each found twice; 11 was found once. Haplotype 11 (131–104–107) was most common in the samples from Uganda and Tanzania. No samples from Tanzania or Uganda were Ethiopia haplotype 4. The Uganda SGEAA alleles were associated with 8 different microsatellite haplotypes ( Figure , panel A; online Technical Appendix Table, 5 haplotypes found at each site); the most common haplotype, haplotype 11 (131–104–107), was found in 82.4%, (42/51) of samples. The 92 SGEAA samples collected from 3 study sites in Tanzania exhibited 24 different microsatellite haplotypes ( Figure, panel A; online Technical Appendix Table). Korogwe exhibited the greatest diversity by having 21 haplotypes among 64 SGEAA sample. As in Uganda, most of the Tanzania SGEAA alleles were associated with haplotype 11 (53 \\[57.6%\\]). Among the remaining 39 samples were 23 alternative but related haplotypes.\n\n【31】##### Diversity of Microsatellite Composition among SGEGA Samples\n\n【32】Of 41 SGEGA samples from Ethiopia, only 1 microsatellite haplotype was present: haplotype 4 (121–114–98) ( Figure, panel B; Technical Appendix Table). The 28 SGEGA samples from Uganda were associated with 3 microsatellite haplotypes; all 3 combinations were represented in the 2 sites ( Figure , panel B; Appendix Table). Of these SGEGA samples, 24 (85.7%) were haplotype 11 (131–104–107). The 2 less common haplotypes, haplotypes 18 (131–104–125) and 14 (131–104–113), are evidently related to haplotype 11, differing by 1 allele at the 7.7-kb locus. Of 49 SGEGA samples collected from Tanzania, we found 7 microsatellite haplotypes ( Figure , panel B; Technical Appendix Table). This finding indicates less diversity than was associated with SGEAA alleles: Hale (3 haplotypes), Korogwe (3 haplotypes), and Magoda (6 haplotypes). Of the Tanzania SGEGA samples, 18 (36.7%) were haplotype 11, the same haplotype common among Tanzania and Uganda SGEAA samples and among Uganda SGEGA samples. Of the remaining 28 SGEGA samples from Tanzania, 24 (49.0% of total Tanzania SGEGA samples) were haplotype 18 (131–104–125), a haplotype found only twice in association with SGEAA samples from Tanzania. This haplotype was found twice among Uganda SGEGA samples and never among Uganda SGEAA samples.\n\n【33】### Discussion\n\n【34】Although SP is no longer recommended as first-line treatment for _P. falciparum_ infection, it is widely recommended for prevention and possibly still available over the counter for self-treatment. Presumably, therefore, SP has continued to exert selective pressure on already resistant parasites, which might explain the continuing emergence of the triple-mutant _Pfdhps_ allele (SGEGA), which is currently being described for certain regions of eastern Africa. An increased prevalence of the A581G mutation has been well documented in eastern Africa in recent years; it increased from 12% in 2003 to 56% in 2007 at study sites in Korogwe, Tanzania ( _26_ ). In Kabale and Rukungiri, Uganda, samples from 2005 showed a high prevalence of the A581G mutation at 45% and 46%, respectively ( _27_ ); studies during 2005–2006 in Rukara and Mahesha, Rwanda, observed prevalences of 60% and 29%, respectively ( _28_ ). A study in 2010 in Huye District, Southern Province, Rwanda, reported a prevalence of 63% ( _35_ ). In eastern Sudan, a study found an increase in the prevalence of the A581G mutation from 14% in 2003 to 34% in 2012 ( _36_ ). In Kenya, Kisumu, a study showed an increase in the A581G mutation from 0% in 1999–2000 to 85% in 2003–2005 ( _30_ ). More recently, in Nyanza Province, western Kenya, the prevalence of the A581G increased from 0% to 5.3% from 2008–2009 ( _37_ ).\n\n【35】In this study, we investigated the origins of triple-mutant _Pfdhps_ alleles by analyzing the microsatellite diversity flanking _Pfdhps_ . We sampled both SGEAA double mutants and SGEGA triple mutants in 3 populations at the key moment: when the SGEGA triple mutant had emerged but had not yet replaced the SGEAA double mutant. At this time, double- and triple-mutant alleles were present in similar numbers in the areas, but SP-sensitive alleles were very rare.\n\n【36】In Ethiopia, both SGEAA and SGEGA alleles were associated with haplotype 4 (121–114–98), indicating a shared ancestry that has evolved independently from SGEAA and SGEGA alleles from Uganda and Tanzania. The SGEAA in these 2 countries were associated with lineage 11 (131–104–107), the same microsatellite haplotype previously shown to be associated with the double-mutant alleles throughout Tanzania, Kenya, Uganda, Mozambique, and Zambia ( _10_ ).\n\n【37】In Ethiopia and Uganda, we found evidence that the most prevalent SGEAA haplotype locally had given rise to SGEGA haplotypes in the same area; the most common SGEAA haplotype was also the most common SGEGA haplotype in both countries (haplotypes 4 and 11, respectively). However, in Tanzania, the microsatellite haplotype most commonly associated with SGEGA (haplotype 18 \\[131–104–125\\], 25/49 samples) was not identical to that most commonly associated with SGEAA (haplotype 11 \\[131–104–107\\]), because only 2 SGEAA samples were haplotype 18. This finding leads us to speculate that the A581G mutation has emerged on at least 2 occasions in Tanzania.\n\n【38】We found that the microsatellite diversity associated with both SGEAA and SGEGA haplotypes in Ethiopia samples was less than in the SGEAA and SGEGA samples from sites in Uganda and Tanzania. The high level of homozygosity among microsatellite haplotypes in Ethiopia might be due to a high degree of selective pressure, which in turn might be assisted by population bottlenecks brought about by a narrow malaria transmission season and limited exchange of parasites, with neighboring regions resulting from limited migration. The Ethiopia samples originate from the Tigray District near the Eritrean border. Despite some migration of refugees from Eritrea to Ethiopia, a substantial spread to and from the Tigray District in Ethiopia during the years before sample collection is doubtful because of the continued presence of forces at the border during the cease fire succeeding the Eritrean–Ethiopian war initiated in 2000. Double- and triple-mutant alleles from northeastern and eastern Sudan also are associated with haplotype 4 (121–114–98) ( _10_ ) and represent greater diversity than what we present from Ethiopia, which supports the view that the parasite populations in these 2 countries are linked ( _10_ , _38_ ).\n\n【39】Tanzania is a capital of trade and emigration for sub-Saharan Africa. The Tanzam highway (running from Tanzania through Zambia) is one of the most trafficked roads on the African continent, and higher diversity and sharing of common microsatellite haplotypes among the Ugandan and Tanzanian populations were therefore expected. A recent publication about the correlation between human population movement and malaria movement in Uganda, Tanzania, and Kenya ( _39_ ) illustrates that the major human population movement and malaria movement in Tanzania originates from central Dodoma and directs northward and westward. In this regard, an early selection of a haplotype in the Tanga region (northeast) compared with other areas of Tanzania, can be speculated to be plausible because of the larger levels of parasite migration to northern and western parts of the country, diluting to some extent the newly selected haplotypes in these areas.\n\n【40】In conclusion, we provide evidence that the A581G mutation can arise on various SGEAA ancestral backgrounds, of which we have shown 3 different cases (haplotypes 4, 11, and 18), from areas previously known to represent 2 distinct parasite lineages. Our microsatellite analysis is consistent with reports that the SGEGA triple-mutant alleles are undergoing rapid expansion, and we found evidence of spread of the Tanzania SGEGA haplotype (haplotype 18) as far as southwestern Uganda, which illustrates the potential for dispersal of super-resistant _P. falciparum_ malaria throughout the region. Given the rate of increase and the ability of double-mutant allele lineages to acquire the super resistance–conferring A581G mutation independently, it is vital for the continuing effectiveness of prophylaxis with SP that more comprehensive surveillance for the A581G mutation be used to track emerging super-resistant malaria in Africa.\n\n【41】Dr Alifrangis is an associate professor at the faculty of Health and Medical Sciences of the University of Copenhagen, His primary research interest is the use of molecular markers as tools to monitor and possibly hinder emergence and spread of drug resistance in malaria-endemic regions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ae69ddd8-92db-4476-9697-3d951fca8ea4", "title": "Invasive Meningococcal X Disease during the COVID-19 Pandemic, Brazil", "text": "【0】Invasive Meningococcal X Disease during the COVID-19 Pandemic, Brazil\nInvasive meningococcal disease (IMD) is a severe disorder that is associated with high rates of morbidity and mortality worldwide ( _1_ ). Although 12 serogroups of _Neisseria meningitidis_ have been characterized based on their capsular polysaccharides, most IMD cases are caused by serogroups A, B, C, W, Y, and, more rarely, X ( _1_ ).\n\n【1】_N. meningitidis_ serogroup X (MenX) has been responsible for limited IMD cases in the United States and Europe, but since 1990, MenX isolates have emerged in some countries within the meningitis belt of Africa, causing outbreaks and epidemics in Burkina Faso, Togo, Niger, Kenya, and Uganda ( _2_ ). In Brazil, only 6 cases of MenX IMD cases were reported in the last 15 years; the last one was isolated in the city of São Paulo in 2017 ( http://tabnet.datasus.gov.br/cgi/tabcgi.exe?sinannet/cnv/meninbr.def ). We report 2 cases of MenX IMD that occurred in São Paulo in 2021 and 2022, during the COVID-19 pandemic. These isolates were identified during routine laboratory-based public health surveillance in the National Reference Laboratory at the Adolfo Lutz Institute in São Paulo.\n\n【2】Case-patient 1 was a 7-month-old boy who, in November 2021, was admitted to a São Paulo emergency department with fever, vomiting, bulging anterior fontanelle, stiff neck, and seizure. Cerebrospinal fluid collected at that time revealed a leukocyte count of 1,440 cells/mm <sup>3 </sup> with a 73% proportion of neutrophils; protein level was 263 mg/dL, glucose 19 mg/dL, and lactate 80.5 mg/dL, and results of bacterioscopy and culture were negative. The patient was treated with ceftriaxone (100 mg/kg every 12 h) for 10 days, with a favorable outcome.\n\n【3】Case-patient 2 was a 6-year old boy who, in January 2022, was admitted to a São Paulo City emergency department with fever, headache, and vomiting. A sample of cerebrospinal fluid revealed a leukocyte count of 4920/mm <sup>3 </sup> with a 96% proportion of neutrophils; protein level was 207 mg/dL, glucose 48 mg/dL, and lactate 82.3 mg/dL, and results for bacterioscopy and culture were negative. The patient was treated with ceftriaxone (100 mg/kg every 12 h), the recommended antibiotic, with a favorable outcome.\n\n【4】Because both patients were diagnosed with meningitis, chemoprophylaxis with rifampin was administered to all persons characterized as close contacts. Both patients resided in the city of São Paulo but in different regions, 40 km away from each other. Despite the short period between their illnesses, epidemiologic surveillance could not establish an obvious relationship between the 2 patients. Neither patient had traveled to countries with reported MenX disease, nor had they had known contact with other persons diagnosed with meningitis.\n\n【5】We extracted DNA from the cerebrospinal fluid samples obtained from the 2 patients using the Roche MagNa Pure LC 2.0 platform ( https://lifescience.roche.com ) according to the manufacturer’s instructions. Both DNA samples were positive for _N. meningitidis_ ( _ctrA_ gene) by multiplex real-time PCR ( _3_ ), and these results were confirmed using another real-time PCR targeting the meningococcal _sodC_ gene ( _4_ ). Both samples were positive for genogroup X ( _xcbB_ gene) by multiplex real-time PCR and negative for genogroups A, B, C, W, and Y ( _5_ ).\n\n【6】A previous study demonstrated that the 413 bp fragment of the _rplF_ gene, which encodes the 50S ribosomal protein L6, is a suitable genetic target for differentiating species within the genus _Neisseria_ ( _6_ ). A sequence analysis of the _rplF_ gene of both of our patient samples revealed an _rplF_ fragment assigned to allele 1, confirming the genospecies _N. meningitidis_ ( _6_ ). We conducted multilocus sequence typing according to standard protocols using the the PubMLST database ( https://pubmlst.org/neisseria ) and identified Nm400, the isolate from case-patient 2, as sequence type 2888, which is not assigned to a clonal complex. Because of the unavailability of clinical material for Nm111, the isolate from case-patient 1, multilocus sequence typing could not be performed for that isolate.\n\n【7】According to the PubMLST database (as of March 12, 2022), there are 4 records of ST2888: 1 is serogroup X, 2 are serogroup B, and 1 isolate did not have a serogroup recorded. Only 1 case of IMD caused by MenX ST2888 was reported, in Italy in 2009, in a patient who had not traveled abroad and who had a favorable outcome ( _7_ ).\n\n【8】A large study in sub-Saharan Africa showed that the current population structure reveals MenX to be of a predominantly single lineage—ST181 of MenX belongs to a single main lineage, ST181 (clonal complex 181)—which is unlike the diversified MenX population found in Europe ( _8_ ). A recent study in Italy described 4 cases of serogroup X IMD among refugees: 3 were immigrants from Africa, and the other was an immigrant from Bangladesh who had been in contact with refugees from Africa for several months. This highlights the potential threat of new lineages being introduced into vulnerable populations in times of humanitarian crises and conflicts ( _9_ ).\n\n【9】Given the COVID-19 pandemic scenario that has brought an interruption in nonpharmacologic measures to control SARS-CoV-2 transmission, and considering the globally observed reduction of meningococcal vaccination coverage ( _10_ ), a resurgence in cases of IMD will be likely, and cases of non–vaccine-preventable serogroup X should be monitored. Ongoing surveillance of IMD, as well as addition of surveillance initiatives in some regions, will be needed to ensure a successful public health response in terms of both prevention and control.\n\n【10】Dr. Fukasawa is a pharmacist and scientific researcher whose primary research interests include molecular surveillance of bacterial diseases caused by _Neisseria meningitidis_ , _Streptococcus pneumoniae_ , and _Haemophilus influenzae_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1e11f60b-c986-4769-9b94-0c17238be265", "title": "Adoption of Sodium Reduction Strategies in Small and Rural Hospitals, Illinois, 2012", "text": "【0】Volume 11 — March 20, 2014\n\n【1】### Article Tools\n\n【2】*   PDF - 232 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### BRIEF  \n\n【4】Adoption of Sodium Reduction Strategies in Small and Rural Hospitals, Illinois, 2012\n====================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Objective\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Nancy Amerson, MPH; Marguerite Nelson, MS, MPH; Abigail Radcliffe, MA; Conny Moody, MBA; Lori Williams, BA; Cheryl Miles, MA\n\n【8】_Suggested citation for this article:_ Amerson N, Nelson M, Radcliffe A, Moody C, Williams L, Miles C. Adoption of Sodium Reduction Strategies in Small and Rural Hospitals, Illinois, 2012. Prev Chronic Dis 2014;11:130261. DOI: http://dx.doi.org/10.5888/pcd11.130261 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】Sodium reduction strategies on a population-based level are promoted as a public health intervention. Small and rural hospitals in Illinois were funded to adopt sodium reduction strategies as an intervention, have their hospital cafeteria and vending machines assessed via an environmental scan, and participate in an evaluation. Intervention strategies to identify and to label lower-sodium foods were implemented most, and pricing strategies were implemented least among funded hospitals. The sodium reduction strategies implemented could be replicated in other small and rural hospitals, because they were done with minimal funding and with minimal barriers.\n\n【12】Top of Page\n\n【13】Objective\n---------\n\n【14】On a population-based level, policies to reduce sodium in foods served in institutional settings (eg, hospital cafeterias) are recommended to address high blood pressure (1). Approximately 65% of sodium consumed in 10 food categories comes from food obtained at a retail store (2). The hospital food environment affects patients, work site employees, and the larger community as families and visitors make food purchases in hospital facilities. With this project, we wanted to increase the adoption of sodium reduction in small and rural hospitals and to determine the factors associated with adoption of sodium reduction strategies.\n\n【15】Top of Page\n\n【16】Methods\n-------\n\n【17】Using guidance from the Centers for Disease Control and Prevention (CDC), 9 of 88 small and rural Illinois hospitals received funding to increase adoption of sodium reduction strategies in the hospital setting. A Web-based survey was deployed at 2 points during the grant funding period to identify the sodium reduction strategies in place in small and rural hospitals. The first survey was deployed to funded hospitals to identify sodium reduction strategies in place at funded hospitals as of March 1, 2012. Funded hospitals were required to form a multidisciplinary team and select at least 1 intervention strategy from _Under Pressure: Strategies for Sodium Reduction in the Hospital Environment_ (1) to implement during a 4-month period. A postintervention survey was deployed to identify sodium reduction strategies in place after the intervention among funded hospitals. The survey was also sent to all unfunded hospitals to assess what, if any, sodium reduction strategies were being used.\n\n【18】Illinois Cardiovascular Program staff worked with staff from CDC’s Division of Nutrition, Physical Activity, and Obesity to modify a Healthy Hospitals Environmental Scan (scan) for use in Illinois’s funded small and rural hospitals. The scan was originally modified by CDC from the Nutrition Environment Measures Survey (3,4). The scan consists of approximately 200 questions and 2 areas for assessment: the cafeteria and the vending machines. Additional questions were added to the scan to collect more sodium-specific data. The Illinois Hospital Association, with permission of the funded hospitals, conducted the prescan in April 2012 before the initiative and a postscan in August 2012 at the conclusion of the grant funding.\n\n【19】Top of Page\n\n【20】Results\n-------\n\n【21】All 9 of the funded hospitals completed both surveys. An additional 21 unfunded hospitals completed a survey regarding sodium reduction activities (response rate: 21/79, 26.6%). Before the sodium reduction initiative, a combined total of 144 sodium reduction strategies were already in place at funded hospitals. Grant funding contributed to an additional 95 sodium reduction strategies being implemented among funded hospitals. The number of strategies added by each hospital ranged from 2 to 19. Table 1 highlights the differences in the number of sodium reduction strategies implemented before and after the initiative in funded hospitals. Each hospital implemented at least 1 intervention strategy from the required list.\n\n【22】On average, the funded hospitals were implementing 16 sodium reduction strategies per hospital before the initiative. At the conclusion, the average number of strategies per hospital increased to 27. Unfunded hospitals are implementing an average of 16 sodium reduction activities per hospital, identical to the funded sites pre-assessment. Table 2 highlights the strategies in place most often among funded hospitals after the intervention. Support from dietary councils and wellness programs from within and outside the hospital organization was cited most often as a facilitator for successful implementation. Administrative obstacles were cited most often as a barrier and included issues with contracts and timelines.\n\n【23】The food environment of the funded small and rural hospitals changed over the course of the intervention. The scan results showed an increase in the availability of sodium information and sodium messages in the hospitals. The number of hospitals that provide sodium information in the cafeteria, in printed brochures, and vending machines increased. Easy access to discretionary salt use (eg, salt shakers or packets on tables) remained high in funded hospitals. However, we found little to no change in pricing strategies to promote healthful food items.\n\n【24】Top of Page\n\n【25】Discussion\n----------\n\n【26】Most strategies that were implemented were from the _Under Pressure_ guidance document. A few strategies were developed by the hospitals. Lower-sodium food options were promoted more in funded hospitals at postintervention. Funding allowed for an increase in the number of sodium reduction strategies addressed and allotted for time and effort to be directed toward increasing implementation. Messaging about healthful items increased in the cafeteria and vending machine environments.\n\n【27】Changing the food environment in the hospital cafeteria and vending machines by promoting lower-sodium foods can occur quickly. More effort may be needed to use pricing strategies to promote more healthful foods. This systems-level change may involve more stakeholders and require more time for it to be implemented.\n\n【28】The timeframe in which the intervention was implemented may have been a limitation. Funded sites were awarded in March 2012 and the intervention was completed in June 2012. The 4-month period may have limited the types of interventions attempted. Other project limitations include not examining the sodium content of meals, tracking sales of newly offered food products to monitor consumption, or evaluating whether hospitals added more lower-sodium products as choices rather than replace high-sodium products.\n\n【29】As a result of the sodium reduction initiative in small and rural hospitals, funded sites were able to make policy and system-level changes to affect their patients, hospital employees, and the community at large. The sodium reduction activities implemented through this initiative were done with minimal funding and with minimal barriers. These factors make replication of these activities promising in other small and rural hospitals.\n\n【30】Top of Page\n\n【31】Acknowledgments\n---------------\n\n【32】The project was supported by funding from the CDC Heart Disease and Stroke Prevention Program (CDC-DP-70403-CONT12, grant no. 5U50DP000747-04, Addressing Illinois Heart Disease and Stroke Prevention).\n\n【33】We acknowledge the public health assistance of the following people from CDC in developing the intervention and program plan: Brook Belay, MD, MPH, Division of Nutrition, Physical Activity and Obesity, for content expertise on the hospital environmental scan; Kristy Mugavero, RN, MSN, MPH, Division for Heart Disease and Stroke Prevention, for context expertise on sodium. We also acknowledge the evaluation guidance of Alberta Mirambeau, MPH, Jan Losby, PhD, and Joanna Elmi, MPH, with the Division for Heart Disease and Stroke Prevention, CDC.\n\n【34】Top of Page\n\n【35】Author Information\n------------------\n\n【36】Corresponding Author: Nancy Amerson, MPH, Division of Chronic Disease Prevention and Control, Office of Health Promotion, Illinois Department of Public Health, 535 W Jefferson St, 2nd Floor, Springfield, IL 62761. Telephone: 217-558-2637. E-mail: nancy.amerson@illinois.gov .\n\n【37】Author Affiliations: Marguerite Nelson, Conny Moody, Cheryl Miles, Illinois Department of Public Health, Springfield, Illinois; Abigail Radcliffe, Lori Williams, Illinois Hospital Association, Springfield, Illinois.\n\n【38】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8b032940-7626-4fab-a243-ab3556577d6a", "title": "NDM-1–producing Klebsiella pneumoniae, Croatia", "text": "【0】NDM-1–producing Klebsiella pneumoniae, Croatia\n**To the Editor:** The novel metallo-β-lactamase named New Delhi metallo-β-lactamase (NDM-1) was identified from _Klebsiella pneumoniae_ and _Escherichia coli_ isolates in Sweden from a patient previously hospitalized in India ( _1_ ). NDM-1 is spreading rapidly worldwide to nonclonally related isolates, many of which are directly or indirectly tracked to the Indian subcontinent ( _2_ ). A carbapenem-resistant _K. pneumoniae_ strain, KLZA, was isolated in May 2009 from the culture of a blood sample from of a 40-year-old man on the day after his admission to a surgical intensive care unit of the Clinical Hospital Center in Zagreb, Croatia. The patient had been transferred after 5 days of hospitalization in Bosnia and Herzegovina following a car accident. The clinical history mentioned antimicrobial drug treatment that did not include carbapenems (gentamicin, metronidazole, and ceftriaxone) and no link to the Indian subcontinent. Antimicrobial drug susceptibility testing was performed by Vitek2 (bioMérieux, Marcy-l’Etoile, France) and broth microdilution and interpreted according to the latest documents from the European Committee on Antimicrobial Susceptibility Testing ( www.eucast.org/clinical\\_breakpoints/ , version 1.1).\n\n【1】The strain proved resistant to imipenem and meropenem, to all broad-spectrum cephalosporins, and to aminoglycosides and susceptible to ciprofloxacin and tigecycline ( Table ). We checked for _bla_ <sub>VIM </sub> , _bla_ <sub>IMP </sub> , _bla_ <sub>SPM </sub> , _bla_ <sub>GIM </sub> , _bla_ <sub>SIM </sub> , and _bla_ <sub>NDM </sub> resistance genes by using PCR. A PCR product was obtained only with the NDM primers, after being purified (QIAquick PCR Purification Kit, QIAGEN, Hilden, Germany), its sequence showed 100% identity with _bla_ <sub>NDM-1 </sub> .\n\n【2】Strain genotyping was performed by multilocus sequence typing to determine the sequence type (ST) of the isolate and to establish a comparison with previously reported NDM-1–producing isolates. Allelic numbers were obtained on the basis of sequences of 7 housekeeping genes at www.pasteur.fr/recherche/genopole/PF8/mlst/Kpneumoniae.html . Multilocus sequence typing identified _K. pneumoniae_ KLZA as an ST25 strain, which significantly differs from the ST14 type found in the index NDM-1–producing strain and from other isolates originating from India ( _1_ ) and then in other countries. ST25 _K. pneumoniae_ was also found in _K. pneumoniae_ isolates in Geneva ( _3_ ). Other _K. pneumoniae_ STs harboring NDM-1 were ST15, ST16, and ST147 ( _4_ – _7_ ).\n\n【3】Resistance was transferred by conjugation to _E. coli_ J53, with selection based on growth on agar in the presence of ceftazidime (10 mg/L) and azide (100 mg/L). The conjugant T1 showed resistance to β-lactams, including all carbapenems, as well as decreased susceptibility to ciprofloxacin.\n\n【4】The KLZA strain and its transconjugant harbored other determinant of resistance, namely _bla <sub>CTX-M-15 </sub>_ , _bla <sub>CMY-16 </sub>_ , and _qnrA6_ . Plasmid incompatibility groups, determined by a PCR-based replicon typing method, belonged to the incA/C replicon type.\n\n【5】This report of an NDM-1–producing _K. pneumoniae_ in Croatia adds to those of other cases in patients from patients hospitalized in the Balkan area. The patient in this report had no apparent link to the Indian subcontinent.\n\n【6】In a survey conducted by the European Centre for Disease Prevention and Control to gather information about the spread of NDM-1–producing _Enterobacteriaceae_ in Europe and reporting cases from 13 countries during 2008–2010, five of the 55 persons with known travel histories had traveled to the Balkan region during the month before diagnosis of their infection: 2 to Kosovo and 1 each to Serbia, Montenegro, and Bosnia and Herzegovina. All had received hospital care in Balkan countries because of an illness or accident that occurred during the journey ( _7_ ). Two of the latter cases ( _4_ , _8_ ) and a case from Germany ( _9_ ) were subsequently published. No patient had any apparent link to the Indian subcontinent.\n\n【7】Although the way NDM-1 isolates might have been imported to western Europe not only from the Indian subcontinent but also from Balkan countries ( _10_ ) has been highlighted, awareness of western Europe as a possible area of endemicity remains limited. The aforementioned report from Germany, although recognizing that the patient had been repatriated after hospitalization in Serbia, declared “no evidence about contact with people from regions where NDM-enterobacteria are endemic” ( _9_ ). This limited awareness shows the threat of neglecting to screen patients who are transferred from countries thought not to be at risk for NDM-1. Furthermore, it means that specimen are not sent to the local reference laboratories and recognized as positive for NDM-1, thus permitting wide dissemination of NDM-1–producing enterobacteria in the community ( _4_ ). The accumulating evidence of NDM-1 from the Balkan area could suggest a possible multifocal spread of this enzyme, with the Balkans as a possible second area of endemicity, in addition to the Indian subcontinent, and prompts for widespread epidemiologic surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51bdae33-b256-483d-b5a2-895a7dc67096", "title": "Multiplex PCR−Based Next-Generation Sequencing and Global Diversity of Seoul Virus in Humans and Rats", "text": "【0】Multiplex PCR−Based Next-Generation Sequencing and Global Diversity of Seoul Virus in Humans and Rats\nHantaviruses (order _Bunyavirales_ , family _Hantaviridae_ , genus _Orthohantavirus_ ) pose a worldwide public health threat and are the causative agents of hemorrhagic fever with renal syndrome (HFRS) in Eurasia and hantavirus pulmonary syndrome in the Americas ( _1_ ). HFRS is caused mainly by Old World hantaviruses, such as Hantaan virus (HTNV), Seoul virus (SEOV), Dobrava–Belgrade virus, and Puumala virus, that are transmitted to humans by inhalation of dust contaminated with rodent excreta (saliva, urine, and feces) or bite by an infected rodent. Annually, 150,000 cases of HFRS are reported (case-fatality rate range <1%–15%) ( _2_ ). Clinical signs and symptoms include headache, myalgia, abdominal and back pain, nausea, vomiting, diarrhea, proteinuria, thrombocytopenia, hemorrhage, and renal failure ( _3_ ). The typical disease course consists of 5 phases: febrile, hypotensive, oliguric, diuretic, and convalescent: the phases vary in length from several hours to several days. A difficulty in diagnosis is the extensive incubation period from the time of exposure to the onset of symptoms, which might be as long as 50 days. There are no effective vaccines or antiviral agents against hantavirus infection.\n\n【1】SEOV has a negative-sense, single-stranded, tripartite RNA genome ( _4_ ). The large (L) segment encodes an RNA-dependent RNA polymerase, the medium (M) segment encodes 2 membrane glycoproteins (Gn and Gc), and the small (S) segment encodes a nucleoprotein. Brown rats ( _Rattus norvegicus_ ) and black rats ( _R. rattus_ ) are the primary reservoir hosts of SEOV and have a worldwide distribution ( _5_ , _6_ ).\n\n【2】SEOV infections have been reported in Asia, Europe, the Americas, and Africa ( _7_ – _12_ ). HFRS caused by SEOV is responsible for 25% of clinical cases and is a mild form with a case-fatality rate of <1% in Asia ( _13_ ). Recently, an outbreak of SEOV-induced HFRS was reported in the United Kingdom among rat owners, breeders, and distributors of the pet animal market ( _14_ ). In the United States, outbreaks of SEOV infections occurred in 11 states in 2017; there were 17 confirmed SEOV-infected patients ( _15_ , _16_ ). SEOV was identified in New York, New York, and is considered an urban public health threat ( _17_ ).\n\n【3】Whole-genome sequencing of SEOV is a prerequisite for tracking SEOV infections and evaluating disease risks for development and implementation of preventive and therapeutic strategies. Acquisition of viral genome sequences plays a critical role in surveillance, identification, and risk mitigation of outbreaks of virus infection ( _18_ ). Next-generation sequencing (NGS) is a potent tool for defining virus genome sequences. However, an obstacle for obtaining virus genomic information is ultra-low virus RNA loads in the clinical specimens. To enrich the low amount of viral RNA, we developed a multiplex PCR–based NGS that showed high coverage of HTNV genome sequences from HFRS patients ( _19_ ).\n\n【4】In this study, we collected 1,269 _R. norvegicus_ rats in an urban HFRS-endemic area in South Korea during 2000–2016. We report a robust strategy for whole-genome sequencing of SEOV and provide useful insights into epidemiologic characteristics and phylogeographic diversity of a unique worldwide hantavirus.\n\n【5】### Materials and Methods\n\n【6】##### Ethics\n\n【7】Human samples were provided after informed consent was obtained. The study was approved and conducted in accordance with ethicals guidelines for the Korea University Institutional Animal Care and Use Committee. Live trapping of rats at US military training sites and installations was approved by US Forces Korea in accordance with regulation 40–1 (Prevention, Surveillance, and Treatment of Hemorrhagic Fever with Renal Syndrome). Rats were humanely killed by cardiac puncture, and tissues were collected under isoflurane anesthesia in accordance with procedures approved by Korea University Institutional Animal Care and Use Committee protocol #2010–212.\n\n【8】##### Sample Collection\n\n【9】We tested retrospective HFRS patient serum samples obtained from the Korea Bank for Pathogenic Viruses (Seoul, South Korea). We collected _R. norvegicus_ rats during 2000–2016 by using collapsible live-capture traps (Tomahawk Live Trap Co., Hazelhurst, WI, USA, and H.B. Sherman, Tallahassee, FL, USA). Traps were set at intervals of 1–2 m and examined early the next morning over a 1–2-day period at US Army training sites. For the US Army Garrison in Seoul, we used baited live capture traps (Tomahawk Live Trap Co.) or glue boards. Captured rats were submitted to the 5th Medical Detachment/Medical Command Activity–Korea, US Army Garrison (Yongsan, Seoul), and then transported to the College of Medicine, Korea University (Seoul), where they were held in a Biosafety Level 3 laboratory until processing. Live rats were humanely killed by cardiac puncture under isoflurane anesthesia and identified to species by using morphologic criteria and PCR, when required. Serum, lung, spleen, kidney, and liver tissues were collected aseptically and frozen at −70°C until used.\n\n【10】##### Indirect Immunofluorescence Antibody Test\n\n【11】We used an indirect immunofluorescence antibody (IFA) test for serum samples from HFRS patients and live rats. We initially diluted samples 1:32 in phosphate-buffered saline and then tested them for IgG against SEOV. We applied diluted serum samples to slides containing SEOV-infected Vero E6 cells fixed with acetone and incubated wells at 37°C for 30 min. The slides were washed, fluorescein isothiocyanate–conjugated goat antibody to human and rat IgG (ICN Pharmaceuticals, Laval, Quebec, Canada) was added, and slides were incubated at 37°C for 30 min. We then washed the slides again and examined them for virus-specific fluorescence by using a fluorescent microscope (Axio Scope; Zeiss, Berlin, Germany).\n\n【12】##### Real-Time Quantitative PCR\n\n【13】We performed real-time quantitative PCR (qPCR) for total RNA by using the high-capacity RNA-to-cDNA Kit (Applied Biosystems, Carlsbad, CA, USA) in a 10-μL reaction mixture containing 1 µg of total RNA. We used an SYBR Green PCR Master Mix (Applied Biosystems) in a StepOne Real-Time PCR System (Applied Biosystems). We performed reactions at 95°C for 10 min, followed by 45 cycles at 95°C for 15 s, and then 1 cycle at 60°C for 1 min. Primer sequences specific for SEOV S segments were SEOV-S719F: 5′-TGGCACTAGCAAAAGACTGG-3′ and SEOV-S814R: 5′-CAGATAAACTCCCAGCAATAGGA-3′.\n\n【14】##### Reverse Transcription and Rapid Amplification of cDNA Ends PCR\n\n【15】We extracted total RNA from serum or lung tissues of seropositive samples by using TRI Reagent Solution (Ambion Inc., Austin, TX, USA). We synthesized cDNA by using the High Capacity RNA-to-cDNA Kit (Applied Biosystems) and random hexamer or OSM55 (5′-TAGTAGTAGACTCC-3′). For initial identification, we used oligonucleotide primers for SEOV L segment as described ( _20_ ). To obtain the 3′ and 5′ termini genome sequences of SEOV, we performed rapid amplification of cDNA ends (RACE) PCR by using a 3′-Full RACE Core Set and a 5′-Full RACE Core Set (Takara Bio Inc., Kusatsu, Shiga, Japan), according to the manufacturer’s specifications. We purified PCR products by using the LaboPass PCR Purification Kit (Cosmo Genetech, Seoul, South Korea). We performed sequencing in both directions of each PCR product by using the BigDye Terminator v3.1 Cycle Sequencing Kit (Applied Biosystems) on an automated sequencer (Applied Biosystems).\n\n【16】##### Multiplex PCR–Based NGS\n\n【17】We designed multiplex PCR primers for SEOV L, M, and S segments and amplified cDNA by using primers ( Technical Appendix 1 ) and primer mixtures and Solg 2× Uh-Taq PCR Smart Mix (Solgent, Daejeon, South Korea), according to the manufacturer’s instructions. We performed the first and second enrichments in a 25-μL reaction mixture containing 12.5 μL of 2× Uh pre-mix, 1 μL of cDNA template, 10 μL of primer mixture, and 1.5 μL of distilled water. Initial denaturation was at 95°C for 15 min, followed by 40 cycles or 25 cycles at 95°C for 20 s, 50°C for 40 s, and 72°C for 1 min, and a final elongation at 72°C for 3 min.\n\n【18】We prepared multiplex PCR products by using the TruSeq Nano DNA LT Sample Preparation Kit (Illumina, San Diego, CA, USA) according to the manufacturer’s instructions. We mechanically sheared samples by using an M220 focused ultrasonicator (Covaris, Woburn, MA, USA). The cDNA amplicon was size-selected, A-tailed, ligated with indexes and adaptors, and enriched. We sequenced libraries by using the MiSeq benchtop sequencer (Illumina) with 2 × 150 bp and a MiSeq reagent V2 (Illumina). We imported and analyzed Illumina FASTQ files by using EDGE ( _21_ ).\n\n【19】##### Phylogenetic Analysis\n\n【20】We aligned and edited virus genome sequences by using the multiple sequence alignment with high accuracy and high throughput algorithm ( _22_ ). We generated phylogenetic trees by using the maximum-likelihood method in MEGA version 6.0 ( _23_ ) and models for analysis according to the best fit substitution model (TN93 + gamma + invariate for L segments, general time reversible + gamma + invariant for M segments, and T92 + gamma for S segments). We assessed support for topologies by bootstrapping for 1,000 iterations. The prototype strain used, SEOV 80-39, was isolated from _R. norvegicus_ rats captured in Seoul in 1980.\n\n【21】### Results\n\n【22】##### Retrospective Analysis of HFRS Patient Specimens\n\n【23】We found that specimens collected in 2002 from 6 HFRS patients were positive for SEOV by ELISA (J.-W. Song and H. Kariwa, unpub. data). We confirmed that the HFRS specimens were serologically positive for SEOV by IFA ( Table 1 ). Titers of SEOV-specific antibody ranged from 1:128 to 1:4,096. Reverse transcription PCR detected the partial sequence of L segment (nt 2946–3335) from 2 HFRS patients (Hu02-180 and Hu02-258).\n\n【24】##### Epidemiologic Surveillance of _R. norvegicus_ Rats\n\n【25】We collected 1,269 _R. norvegicus_ rats in urban HFRS-endemic areas in South Korea, including the city of Seoul (1,226/1,269) and Gyeonggi (40/1,269), Gangwon (1/1,269), and Jeollanam (2/1,269) Provinces ( Table 2 ). A total of 76 (6.2%) of 1,226 rats collected in Seoul were serologically positive for SEOV. However, we found IgG against SEOV in only 1 (2.3%) of 43 rats collected from the other areas, including Gyeonggi, Gangwon, and Jeollanam Provinces. We detected SEOV RNA in 13 (16.9%) of 77 seropositive _R. norvegicus_ rats. Serologic prevalence of SEOV in male rats (7.5%, 43/576) was not significantly different from that in female rats (5.0%, 34/684) (p = 0.0763 by χ <sup>2 </sup> test). Serologic prevalence of SEOV in rats by weight (age) was 6.1% (21/342) in those weighing < 50 g, 5.9% (22/374) in those weighing 51–100 g, 5.8% (30/521) in those weighing 101–200 g, and 13.8% (4/29) in those weighing 201–300 g. Seasonal prevalence of SEOV infection in rats was 5.8% (11 of 190) in spring (March–May), 4.4% (19/433) in summer (June–August), 6.4% (27/420) in fall (September–November), and 10.0% (19/190) in winter (December–February).\n\n【26】##### SEOV RNA Loads in Tissues from Seropositive _R. norvegicus_ Rats\n\n【27】Figure 1\n\n【28】Figure 1 . Measurement of SEOV RNA loads in different tissues of _Rattus norvegicus_ rats, South Korea, 2000–2016. C <sub>t </sub> values were determined for SEOV small segment RNA in lung, liver, kidney, and spleen tissues...\n\n【29】To measure viral load of SEOV RNA in _R. norvegicus_ rats, we performed real-time qPCR for seropositive samples from lungs, livers, kidneys, and spleens ( Figure 1 ). Viral load for SEOV showed ranges from tissues of 5 rats (Rn02-15, Rn10-134, Rn10-145, Rn11-44, and Rn11-53) that were positive by serologic and molecular screening (IFA+ PCR+). Rat Rn10-145 showed the highest amount of SEOV RNA in all tissues, followed by rats Rn02-15 and Rn10-134. Rat Rn11-44 showed the highest amount of SEOV RNA in all tissues except liver. Rat Rn11-53 showed the highest amount of SEOV RNA load in lung tissues, but virus RNA was not detectable in liver, kidney, and spleen tissues.\n\n【30】##### Multiplex PCR–Based NGS for Retrospective HFRS Patient and _R. norvegicus_ Rat Specimens\n\n【31】We determined viral loads for HFRS patient specimens by using real-time qPCR. Cycle threshold (C <sub>t </sub> ) values ranged from 27.5 to 36.8 ( Table 3 ). To perform multiplex PCR–based NGS for SEOV, we designed multiplex PCR primers to amplify every 150-bp sequence for the entire SEOV tripartite genome. We recovered genomic sequences of SEOV from 6 SEOV-positive patient samples. We sequenced human sample Hu02-258, which showed the highest viral load (lowest C <sub>t </sub> value), for 99.6% of the L segment, 99.7% of the M segment, and 91.6% of the S segment. Recovery rates for SEOV genomic sequences from samples Hu02-180 and Hu02-529 showed a correlation with viral loads. Samples Hu02-112, Hu02-294, and Hu02-668 showed high recovery rates of SEOV S and M segments despite lower viral loads (highest C <sub>t </sub> values). However, the L segment showed relatively low coverages (85.0% for Hu02-180, 68.2% for Hu02-294, and 72.7% for Hu02-668).\n\n【32】Using total RNA extracted from rat lung tissues, we determined viral loads by using real-time qPCR. C <sub>t </sub> values ranged from 16.1 to 27.6. We applied multiplex PCR–based NGS for whole-genome sequencing of 4 SEOV strains in the IFA+ PCR+ rats captured in South Korea during 2000–2016. Coverage of genomic sequences of SEOV was 99.1%–99.7% for L segments, 99.2%–99.7% for M segments, and 98.3%–99.4% for S segments. We observed a correlation between C <sub>t </sub> values and multiplex PCR–based NGS coverages ( Technical Appendix 2 ). Whole-genome sequences from Rn10-134, Rn10-145, Rn11-44, and Rn11-53 were obtained with termini sequences of 3′ and 5′ ends. SEOV sequences were deposited in GenBank (accession nos. MF149938–MF149957).\n\n【33】##### Global Diversity of SEOV\n\n【34】We generated phylogenetic trees by using nearly complete genome sequences of SEOV and the maximum-likelihood method. Phylogenetic analysis demonstrated distinct phylogenetic groups (groups A–F). Group A contained SEOV strains from northeastern and southeastern China and an SEOV strain from North Korea. Group B contained SEOV strains from Southeast Asia (Singapore and Vietnam) and France. Group C contained SEOV strains from South Korea and Japan and SEOV strains Tchoupitoulas from Louisiana in the United States. Group D contained an EOV strain from Jiangxi and Hubei Provinces in southeastern China. Group E contained strains from the United Kingdom and the United States (New York, NY, and Baltimore, MD). Group F contained SEOV strains from mountainous areas in southeastern China.\n\n【35】Figure 2\n\n【36】Figure 2 . Phylogenetic analysis of SEOV small RNA segments, South Korea, 2000–2016, and reference strains. A phylogenetic tree was generated by using the maximum-likelihood method with the T92 + gamma distribution model of...\n\n【37】We obtained 9 genome sequences of SEOV S segments from HFRS patients and _R. norvegicus_ rats. Phylogenetic analysis of SEOV S segments showed that group A formed a monophyletic lineage with group D ( Figure 2 ). Group C genetically clustered with group E. The phylogeny of group B was distinct from those of groups A, C, D, and E. Group F from mountainous areas in China formed a lineage that was independent from the other groups obtained from rats collected in urban areas.\n\n【38】Figure 3\n\n【39】Figure 3 . Phylogenetic analysis of SEOV medium RNA segments, South Korea, 2000–2016, and reference strains. A phylogenetic tree was generated by using the maximum-likelihood method with the general time reversible + gamma +...\n\n【40】We obtained 6 genome sequences of SEOV M segments from HFRS patients and _R. norvegicus_ rats ( Figure 3 ). Phylogenetic analysis of SEOV M segments showed distinct phylogenetic clusters (groups A–F). These phylogenetic patterns showed that M segments of SEOV had genetic heterogeneity when compared with S segments.\n\n【41】Figure 4\n\n【42】Figure 4 . Phylogenetic analysis of SEOV large RNA segments, South Korea, 2000–2016, and reference strains. A phylogenetic tree was generated by using the maximum-likelihood method with the TN93 + gamma + invariant model...\n\n【43】We obtained and phylogenetically analyzed 5 SEOV L segments ( Figure 4 ). The SEOV L segment from an HFRS patient and _R. norvegicus_ rats captured in South Korea clustered to form a monophyletic group with SEOV 80-39. SEOV strains from China belonged to a genetic lineage with SEOV DPRK08 from North Korea. SEOV strains from the United Kingdom and Baltimore formed a close phylogenetic group. SEOV IR33 and IR473 obtained from laboratory outbreaks in the United Kingdom were independent from other SEOV strains.\n\n【44】### Discussion\n\n【45】NGS is a robust tool for obtaining extensive genetic information and completing whole-genome sequences ( _18_ ). However, molecular enrichment plays a critical role in amplification of pathogen genome sequences from clinical or animal specimens. Our previous study showed recovery of nearly whole-genomic sequences of HTNV from HFRS military patients by using virus-targeted molecular enrichment ( _19_ ).\n\n【46】In this study, whole-genomic sequencing of SEOV, an etiologic agent of mild HFRS worldwide, was applied to samples from retrospective HFRS patients and seropositive _R. norvegicus_ rats by using multiplex PCR–based NGS. Nearly whole-genome sequences of SEOV tripartite RNA, on the basis of SEOV 80-39 (prototype strain), corresponded to viral loads of patient serum samples and rat lung tissues. Phylogenetic analyses of the genome sequence of SEOV tripartite RNA supported worldwide distributions of SEOV and identified 6 genetic lineage groups. Group A contained SEOV strains from northeastern and southeastern China and North Korea. Group B contained SEOV strains from Singapore and Vietnam in Southeast Asia and Lyon in France. Group C contained SEOV strains originating primarily in South Korea and Japan and an SEOV strain from Louisiana in the United States. Group D consisted of SEOV strains from southeastern China, including Jiangxi and Hubei Provinces. Group E contained SEOV strains from the United Kingdom and eastern United States (New York and Baltimore) and formed a monophyletic lineage. Group F contained SEOV strains from mountainous areas in southeastern China ( _24_ ).\n\n【47】SEOV originated in China and spread worldwide during movement of rats coincidently with human activities (e.g., commercial trade, travel, and migration by railways and through seaports) ( _24_ ). The close genetic relationship of SEOV in South Korea and Japan was probably caused by geographic distance and historical activities (e.g., commerce and occupation by Japanese forces). The genetic lineage containing strains from Southeast Asia and France might have originated during colonization or on trade routes that extended distribution of SEOV-infected rats ( _25_ ). Recently, SEOV outbreaks have been reported in the United Kingdom and United States. Clinical cases showed that SEOV infections were identified among pet owners, breeders, and distributors ( _26_ ). The genetic relationship of SEOV between counties probably reflects movement of rats associated with the animal pet market.\n\n【48】The prevalence of hantaviruses (e.g., HTNV and Imjin virus \\[MJNV\\]) in natural reservoir hosts has showed sex- and weight (age)–specific differences ( _27_ , _28_ ). However, in our study, the incidence of SEOV in _R. norvegicus_ rats was not dependent on sex and weight (age). Epidemiologic differences in hantavirus infections between _A. agrarius_ and _R. norvegicus_ rats might be, in part, caused by ecologic differences, reservoir host distributions, and behavior (e.g., association with humans) ( _29_ ). Seasonal circulation of SEOV infection was maintained over 1 year, suggesting an enzootic infectious cycle. These observations might suggest that preventive strategies for disease risk mitigation focus on limits of rat populations all year.\n\n【49】Our previous study demonstrated differential amounts of HTNV RNA in lung, kidney, liver, and spleen tissues of rodents collected in areas in which HFRS is prevalent ( _30_ ). In addition, the genomic RNA load of MJNV, a shrewborne hantavirus, showed various patterns in different tissues in nature ( _28_ ). IFA+ PCR+ shrews showed high and various loads of MJNV RNA in all tissues. MJNV RNA from IFA– PCR+ shrews was detected in lung but not in kidney, liver, or spleen tissues, indicating an early phase of infection before MJNV-specific IgG was produced ( _31_ , _32_ ). In our study, rats Rn02-15, Rn10-134, and Rn10-145 showed various amounts of SEOV RNA in all tissues. Rat Rn11-44 had high levels of SEOV RNA in all tissues except the liver. Virus RNA in rat Rn11-53 might reflect the early phase of SEOV infections because of highest viral load in lung tissues but not other tissues. Patterns of SEOV RNA loads might indicate systemic infections in nature and active circulation of virus among rat populations in urban HFRS-endemic areas.\n\n【50】Diversity of virus genomes results from genomic variation or exchanges ( _33_ ). RNA viruses show high mutation rates caused by deficiencies in proofreading by virus polymerases. Genomic variation also results from a mechanism of host immune evasion ( _34_ , _35_ ). Genetic exchanges, such as reassortment and recombination, lead to the generation of divergent virus progeny ( _36_ ). Our previous studies identified reassortment and recombination of hantaviruses, including HTNV and MJNV, in nature ( _19_ , _28_ , _37_ ). Using nearly complete sequences of SEOV S, M, and L segments, phylogenetic analyses demonstrated that S segments of group A SEOVs formed a cluster with those of group D SEOVs and that L and M segments of group A SEOVs showed a close phylogenetic relationship with those of group B SEOVs. The S segment of group C SEOVs grouped phylogenetically with group E SEOVs. However, L and M segments of group C SEOVs formed a distant genetic cluster from those of group E SEOVs. Phylogenetic analysis of SEOV S segments showed a differential pattern from that of SEOV M segments, indicating a genome organization compatible with genetic exchanges in nature. To clarify genetic events among SEOV worldwide, whole-genome sequences of the SEOV L segment need to be investigated. Application of multiplex PCR–based NGS will be useful in elucidating phylogenetic patterns of the SEOV L segment.\n\n【51】In conclusion, this epidemiologic survey of _R. norvegicus_ rats in urban HFRS-endemic areas of South Korea identified the prevalence and distribution of SEOV. We applied multiplex PCR–based NGS to whole-genome sequencing of SEOV tripartite RNA from retrospective serum samples from HFRS patients and rat tissues. Phylogenetic analyses demonstrated the global distribution and genetic diversity of SEOV on the basis of nearly complete genome sequences. This study provides useful information for SEOV-based surveillance, disease risk assessment, and mitigation against hantavirus outbreaks.\n\n【52】Dr. Won-Keun Kim is a virologist and research instructor at the College of Medicine, Korea University, Seoul, South Korea. His research interests include next-generation sequencing, hantaviruses, and virus−host interactions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2360cc17-a710-44f2-b0ce-22d0f966088e", "title": "Endemic Norovirus Infections in Children, Ho Chi Minh City, Vietnam, 2009–2010", "text": "【0】Endemic Norovirus Infections in Children, Ho Chi Minh City, Vietnam, 2009–2010\nNorovirus (NoV) is a leading cause of acute gastroenteritis in children <5 years of age ( _1_ ). The epidemiology of NoV in industrialized countries has been intensively investigated, yet the contribution of this pathogen to the effects of diarrheal disease in low- and middle–income countries is not well characterized ( _1_ _,_ _2_ ). Gaining insight into the epidemiology of NoV infections of children in such countries is essential for disease control, particularly considering that several vaccine candidates are in advanced-stage clinical trials ( _3_ ). To address the lack of data on risk factors for endemic NoV infections in low-income countries, we conducted a prospective case–control study among hospitalized children in a major urban location in southern Vietnam.\n\n【1】### The Study\n\n【2】This study was conducted in 3 hospitals (Children’s Hospital 1, Children’s Hospital 2, and the Hospital for Tropical Diseases) in Ho Chi Minh City, Vietnam, during May 2009–December 2010. Written informed consent from a parent or legal guardian was mandatory for participation. Children <5 years of age who resided in Ho Chi Minh City, who had acute diarrhea on admission (≥3 loose stools or ≥1 bloody loose stool within a 24-hour period), and were given no antimicrobial drug treatment 3 days before hospitalization, were invited to participate during May 2009–April 2010. To collect control data, during March–December 2010, we enrolled children who were attending outpatient and inpatient clinics in the nutrition or gastroenterology departments for routine health checks or conditions unrelated to gastroenteritis. Children in this control group met the same demographic criteria, did not have diarrhea, and had not received antimicrobial drugs during the preceding 3 weeks.\n\n【3】Stool specimens were collected from case-patients on the day of admission (n = 1,419) and from control participants while they were attending the clinic (n = 609). All stool samples were cultured by using classic microbiologic methods to detect _Shigella, Salmonella_ , _Campylobacter,_ and _Yersinia_ spp. and were microscopically examined for _Entamoeba, Cryptosporidium,_ and _Giardia_ spp. Methods are described in the Technical Appendix. Conventional reverse transcription PCR was performed on RNA extracted from stool samples to detect rotavirus ( _4_ ) and NoV genogroups I (GI) and II (GII) ( _5_ ), followed by direct sequencing of the amplicons for genotyping.\n\n【4】After rotavirus (46.6%; 661/1,419), NoV was the second most common pathogen detected in symptomatic case-patients (20.6%; 293/1,419); diarrheal bacteria and parasites were cumulatively found in 14.5% ( Technical Appendix ). The prevalence of NoV was higher than in a pooled international estimate ( _1_ ) and than in previous studies performed in Ho Chi Minh City ( _6_ _–_ _8_ ), yet was lower than that found in a study conducted in northern Vietnam ( _9_ ). The frequency of NoV detected in control participants was 2.8% (17/609), similar to a pooled international estimate ( _1_ ). The majority of NoV-positive case-patients experienced nonbloody, nonmucoid watery diarrhea, vomiting, and fever. These symptoms were comparable to those in previous studies of diarrheal infections in children in Vietnam ( _7_ _,_ _9_ ).\n\n【5】NoV was detected throughout the study period ( Technical Appendix Figure). There was a positive linear correlation between NoV infections and monthly rainfall (R = 0.550, p = 0.029), but no similar correlation with temperature (range 22.1°C–37.8°C) (R = 0.308, p = 0.330). This association of NoV infections with the tropical rainy season may reflect differential transmission between different climatic regions because NoV infections are typically associated with the winter season in industrialized countries in temperate regions ( _10_ ).\n\n【6】GII NoV was detected in 239 (99.1%) of 241 and 11 (73.3%) of 15 NoV-positive stool samples from the symptomatic and asymptomatic enrollees, respectively. The remaining children were infected with NoV GI (GI.3, GI.4, GI.5); 1 enrolled case-patient was infected with 2 genotypes: NoV GI.3 and GII.4. Of the GII strains, GII.4 was the most prevalent genotype, comprising 201 (84.1%) of the 239 samples. The next most prevalent was GII.3: 24 (10.0%) were identified in the symptomatic and asymptomatic groups. Other GII genotypes (GII.2, GII.6, GII.7, GII.9, GII.12, and GII.13) were found in <3% of NoV-positive samples.\n\n【7】Socioeconomic and behavioral data were obtained from all enrollees by using a questionnaire and analyzed by using Stata Version v9.2 (StataCorp LP, www.stata.com ) ( Table 1 ). We used χ <sup>2 </sup> and Fisher exact tests to compare proportions between groups and Mann-Whitney U tests for nonparametric data. Univariate analyses were performed to assess factors associated with symptomatic NoV infections. Factors found to be significantly associated with infection in the univariate analysis, in addition to a-priori factors of age, sex, and income level, were then included in a multivariate logistic regression model to simultaneously control for confounding effects. Two-sided p values ≤0.05 were considered significant throughout ( Table 2 ).\n\n【8】NoV infections are commonly associated with outbreaks in enclosed environments ( _2_ ), yet we found attendance in daycare centers and nursery schools was not common; the majority of children remained at home during the day. However, several factors were significantly and independently associated with symptomatic NoV infections. Demographic risk factors included younger age (in months) (adjusted odds ratio \\[aOR\\] 0.96, 95% CI 0.94–0.98, p<0.001) and household crowding (≥3 children in the house) (aOR 1.70, 95% CI 1.0–2.9, p = 0.052). Living in a household where food was regularly purchased from outdoor markets added a significant risk (aOR 4.99, 95% CI 3.1–7.9, p<0.001). Unpredictably, we found that consuming bottled water, rather than pipeline water (aOR 2.18, 95% CI 1.4–3.4, p< 0.001), was a risk factor and did not correlate with household income. However, those drinking municipal water also reported boiling or filtering water before consumption, and those drinking bottled water did not. This association suggests that bottled water in this location may be of poor quality. A further unexpected finding was the protective nature of outdoor toilets (aOR 0.22, 95% CI 0.1–0.4, p<0.001), which may be a result of the sterilizing capabilities of sunlight or of containing fecal contamination outside the residence, possibly protecting children during the period of infancy before they can use toilets. We found that the greatest risk factor for symptomatic NoV infections (aOR 26.14, 95% CI 10.4–65.9, p<0.001) was contact with a person who recently had a diarrheal infection. This finding is consistent with previous investigations showing that person-to-person transmission is predominant during sporadic outbreaks ( _11_ _–_ _14_ ).\n\n【9】This study has several limitations. First, passive case detection limits generalizability because health care–seeking behavior may depend on disease severity and income in this setting. Second, the control participants may not be entirely representative of the population from which the case-patients arose because a large proportion of the control participants were visiting the hospital for nutritional advice, which may have an effect on diarrheal disease risk ( _15_ ). Yet, a limited sensitivity analysis comparing NoV-positive case-patients to NoV-negative control participants and NoV-negative case-patients to NoV-negative control participants demonstrated several differences in risk factors, suggesting that the identified risk factors are associated with NoV rather than health care–seeking behavior (Appendix Table 2 ).\n\n【10】### Conclusions\n\n【11】This epidemiologic investigation showed that 20.6% of hospitalized children with acute diarrhea in Ho Chi Minh City tested positive for NoV, compared with 2.8% of diarrhea-free control participants. We conclude that young age, residential crowding, use of bottled water, and recent contact with a symptomatic individual are key risk factors for symptomatic NoV infection in this location. Because most children did not attend day care, potential preventative measures for NoV infection in Ho Chi Minh City should be focused on improving local hygiene standards to prevent person-to-person transmission within the home.\n\n【12】Ms Tra My is registered in the Tropical Medicine PhD program at the University of Oxford (Oxford, UK). Her research focuses on aspects of acute gastroenteritis in young children in Vietnam.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "88ff99c2-ab40-4b35-8f55-55eb8b832dd0", "title": "Classic Scrapie in Sheep with the ARR/ARR Prion Genotype in Germany and France", "text": "【0】Classic Scrapie in Sheep with the ARR/ARR Prion Genotype in Germany and France\nTransmissible spongiform encephalopathies (TSEs) are fatal neurodegenerative diseases in sheep and goats (scrapie), cattle (bovine spongiform encephalopathy \\[BSE\\]), and humans (Creutzfeldt-Jakob disease \\[CJD\\]). A variant form of CJD ( _1_ ) was discovered in 1996 and was linked to the BSE epidemic in the United Kingdom and elsewhere. Classic scrapie is caused by a variety of prion strains that can be distinguished from one another by their biologic and biochemical features ( _2_ ). Recently, so-called atypical scrapie strains that have remarkably different biochemical and transmission characteristics have been discovered ( _3_ , _4_ ). Although the transmissibility of a particular sheep scrapie isolate to nonhuman primates has been demonstrated ( _5_ ), no epidemiologic data have linked scrapie in small ruminants to human CJD cases ( _6_ ). TSE susceptibility in sheep is controlled mainly by polymorphisms in the monocistronic _PRNP_ gene that encodes for normal cellular protein (PrP <sup>C </sup> ). Three major mutations are associated with sheep susceptibility or resistance to classic scrapie and BSE: at codons 136 (A or V), 154 (R or H), and 171 (R, Q, or H) ( _7_ ). Animals with genotypes V <sup>136 </sup> R <sup>154 </sup> Q <sup>171 </sup> /VRQ, ARQ/VRQ, ARQ/ARQ , and VRQ/ARH PrP are most susceptible to scrapie ( _8_ ). In the past 20 years, no TSE cases have been found in ARR/ARR sheep in Europe, although thousands of scrapie-diseased animals have been genotyped. However, 1 report, albeit heavily questioned, has been made in the literature of a possible case in an ARR/ARR sheep in Japan ( _9_ ). Therefore, this genotype was considered to confer full resistance to BSE and scrapie ( _7_ ) (for a full review see \\[ _10_ \\]). To minimize the risk of humans acquiring TSE by consuming animal products, massive breeding programs involving PrP-genotyping of millions of sheep were initiated in the European Union (EU).\n\n【1】However, the successful transmission of BSE prions to ARR/ARR sheep showed that the resistance of this genotype toward the TSE agent was not absolute ( _11_ ). Recently, the identification of previously unrecognized so-called atypical scrapie in sheep of various genotypes, including ARR/ARR, has reinforced this statement ( _4_ ). We report here the identification and characterization of 2 natural classic scrapie cases in sheep of the ARR/ARR genotype, which are clearly different from BSE and atypical scrapie.\n\n【2】### Methods\n\n【3】##### ELISA, Scrapie-associated Fibril, and Conventional Immunoblots\n\n【4】For ELISA detection of PrP <sup>Sc </sup> , commercial TSE rapid tests, TeSeE Sheep/Goat (Bio-Rad, Marnes-la-Coquette, France) , were used according to the manufacturer’s recommendations. Scrapie-associated fibrils from the brain stem of infected ovines were purified and immunoblotted, according to the protocol by the World Organization for Animal Health ( _12_ ). In this assay, we used the monoclonal antibody L42, which binds the 145–150 sequence of PrP (YEDRYY). Visualization was achieved by using the chemiluminescence substrate CDP-Star (Tropix, Bedford, MA, USA) and the Bio-Rad VersaDoc imaging system, and signals were analyized by using the Quantity One quantification software (Bio-Rad).\n\n【5】The TeSeE Western Blot Kit (Bio-Rad) was used according to the manufacturer’s recommendations. This kit uses the monoclonal antibody SHa-31, which binds the 145–152 sequence of PrP(YEDRYYRE).\n\n【6】##### Proteinase K (PK) Resistance Assay\n\n【7】Ten percent of brain homogenates from the brain of a 5-year-old sheep with scrapie in France, designated S83, were analyzed by the TeSeE Sheep/Goat ELISA as recommended by the manufacturer. Each sample was diluted in PrP <sup>Sc </sup> \\-negative ARR/ARR sheep 10% brain homogenate until an optical density (OD) of 1.5 to 1.7 was obtained in the ELISA. Equilibrated homogenate aliquots were submitted to PK digestion with a concentration ranging from 50 to 500 µg/mg. PrP <sup>Sc </sup> was precipitated (as in the basic TeSeE Sheep/Goat ELISA) and the pellet dissolved in 25 µL buffer C1, incubated for 5 min at 100°C, and diluted 12-fold in R6 reagent. Samples were run in triplicate and detected by using the TeSeE Sheep/Goat ELISA.\n\n【8】##### Bioassay\n\n【9】Twenty microliters of 10% brain homogenates were intracerebrally inoculated into C57Bl6, RIII, Vm, and Tgshp XI mice that overexpress the ovine PrP <sup>ARQ </sup> , into Tg338 mice that overexpress the ovine PrP <sup>VRQ </sup> , and into Tgbov XV mice that overexpress the bovine PrP. Incubation times were recorded, and tissue samples from clinically affected mice were collected and preserved.\n\n【10】##### Lesion Profiling and Paraffin-Embedded Tissue (PET) Blot\n\n【11】Lesion profiles were established by following the standard method of Fraser and Dickinson ( _13_ ) and using 6 brains per isolate. For the PET blots, we used sections from positive transgenic mice ( _14_ ).\n\n【12】##### PRNP Sequencing\n\n【13】DNA was extracted from brain tissue of the scrapie-positive sheep with the QIAamp DNA Mini Kit (QIAGEN, Hilden, Germany). PCR-amplification and sequencing of a 970-bp PCR fragment, including the complete coding region of the _PRNP,_ were done as described previously ( _15_ ). Additionally, the PCR fragment was cloned with the pGEM-T Easy Vector System (Promega GmbH, Mannheim, Germany). Plasmid DNA of 12 recombinant clones was sequenced, and these sequences were compared with GenBank _PRNP_ sequence U67922.\n\n【14】### Results\n\n【15】##### Histories of 2 ARR Homozygous Sheep with Scrapie\n\n【16】The case in an ARR homozygous sheep in Germany was initially diagnosed during 2004 in the course of the intensified EU-wide testing of fallen stock and slaughter animals. A recent retrospective genotyping study on all 230 small ruminant TSE cases diagnosed in Germany between 2000 and 2006 led to the identification of this ARR/ARR case.\n\n【17】Figure 1\n\n【18】Figure 1 . Antibody-binding patterns of the prion protein (PrP <sup>Sc </sup> ) associated with cases of ARR/ARR scrapie in France and Germany. A) and B) Western blots showing the differences in monoclonal antibody (MAb) P4 binding...\n\n【19】This animal showed borderline reactivity in the Bio-Rad TeSeE rapid test on the brain stem (initial result, OD 0.279; cut-off 0.217; test repetition in duplicate OD 0.259/0.256; cut-off 0.219) and was positive in the Western blot confirmation test ( Figure 1 ). The animal (S115/04) was a 2- to 4-year-old ewe of the black-headed German mutton breed. Apart from a general loss of condition, the animal did not show any clinical signs suggestive of scrapie. After the official confirmation of the case, all animals in the herd of origin (n = 1,297) were genotyped regarding PrP codons 136, 154, and 171. The frequencies of PrP genotypes in sheep >1 year of age (n = 871) were 46.7% (ARR/ARQ), 35.4% (ARR/ARR), 16.9% (ARQ/ARQ), 0.7% (ARR/VRQ), and 0.1% for each of the genotypes ARQ/VRQ, ARR/AHQ, and AHQ/ARQ. In accordance with the EU regulations, all sheep not carrying at least 1 ARR haplotype were slaughtered, and rapid tests for TSE were conducted. No further TSE case was detected among these sheep. The origin of the infection in this outbreak remains unknown, but rams originating from another flock in Germany with classic scrapie were apparently used in this flock in 1999.\n\n【20】We also examined brain, tonsil, or retropharyngeal lymph node samples from >1,700 sheep with clinically suspected scrapie, collected in France from 1993 through 2001, and identified a single case in a sheep with the ARR/ARR genotype. This animal, S83, was a 5-year-old sheep born in 1995 that had some clinical symptoms of scrapie but for which no detailed symptoms are described. The brain stem sample of S83 was positive by ELISA in the Bio-Rad TeSeE Sheep/Goat rapid test (OD 2.4; cut-off 0.212) and gave a clearly positive signal on Western blot ( Figure 1 ).\n\n【21】Tonsil and retropharyngeal lymph nodes from this animal were negative for scrapie by ELISA and Western blot. In addition, conventional histologic examination found a severe _Listeria_ infection, but no vacuoles, in the brain stem. No detailed information was available on the flock of origin and the genetic structure of that flock. No further scrapie cases were reported until 2001; the farm was then closed.\n\n【22】##### Genetic Analysis\n\n【23】In addition to the routine genotyping at codons 136–154 and 171, _PRNP_ gene sequences from the 2 scrapie cases were determined by sequencing of PCR amplificates from brain-derived DNA to compensate for the possibility of a genetic chimerism in blood. For both scrapie cases S115/04 and S83, _PRNP_ sequence analysis unanimously showed the homozygous _PRNP_ genotype ARR/ARR. There were no differences between the _PRNP_ sequences determined by direct sequencing of the PCR products and by sequencing of 12 cloned PCR fragments for each case, which excludes the possibility that _PRNP_ haplotypes other than ARR were present in the analyzed tissue.\n\n【24】##### Biochemical Properties of PrP <sup>Sc</sup>\n\n【25】Figure 2\n\n【26】Figure 2 . Biochemical characterization of the prion protein (PrP <sup>Sc </sup> ) associated with ARR/ARR cases in France and Germany. A) Western blot (stained by monoclonal antibody L42) illustrating that protein kinase (PK)– treated ovine bovine...\n\n【27】The Western blot of S115/04 and S83 showed the 3-banded PrP <sup>Sc </sup> pattern that is typically associated with scrapie. The molecular masses of the nonglycosylated PrP <sup>Sc </sup> bands were ≈21 kDa, and both animals lacked the 12-kDa PrP <sup>Sc </sup> band seen in atypical scrapie ( Figure 1 ). Additionally, the nonglycosylated PK-treated PrP <sup>Sc </sup> bands of S115/04 and S83 showed lower electrophoretic mobility than PrP <sup>Sc </sup> derived from BSE-infected sheep (ARQ/ARQ). The monoclonal antibody P4, which recognizes PrP <sup>Sc </sup> derived from scrapie-affected sheep, but has a lower affinity to BSE PrP <sup>Sc </sup> , clearly detected both S115/04- and S83-derived PrP <sup>Sc </sup> and, to a substantially lesser extent, PrP <sup>Sc </sup> from a BSE-infected sheep. These results strongly support the contention that both isolates differ from those that cause BSE and atypical scrapie ( Figure 1 ; Figure 2 , panel A).\n\n【28】The limited amount of sample material available from case S115/04 prevented any further biochemical characterization. For case S83, however, it was possible to compare the PK resistance of PrP <sup>Sc </sup> with that of 20 randomly selected classic scrapie cases, 1 case of BSE in an ARR/ARR sheep, and 1 atypical case in an ARR/ARR sheep. S83 PrP <sup>Sc </sup> was found to have a significantly lower level of PK resistance than that found in classic scrapie PrP <sup>Sc </sup> . However, resistance was similar to that of PrP <sup>Sc </sup> in experimentally BSE-infected ARR/ARR sheep and PrP <sup>Sc </sup> levels of animals with atypical scrapie ( Figure 2 , panel B).\n\n【29】##### Bioassay in Transgenic Mice\n\n【30】Samples from both infected sheep were inoculated into a panel of conventional (C57Bl6, RIII, and VM) mice, transgenic bovinized (Tgbov XV) mice, and ovinized (ARQ Tgshp XI \\[unpub. data\\], VRQ Tg 338) mice. These transmission experiments are ongoing. However, for the isolate S83 from France, results in transgenic ovinized VRQ mice (Tg338) are available.\n\n【31】In intracerebrally inoculated Tg338 mice (20 µL of a 10% brain homogenate), nervous symptoms consistent with TSE developed after a mean incubation period of 309 ± 35 days. In all mice, PrP <sup>Sc </sup> was detected by using Western blot. For Tg 338 mice, the mean incubation period was 239 ± 32 days for the atypical scrapie isolate, and the incubation time for BSE prion previously passaged through ARQ/ARQ sheep was 534 ± 25 days.\n\n【32】Figure 3\n\n【33】Figure 3 . Lesion profiling (A, B, C) and paraffin-embedded tissue blot characterization of prion protein (PrP <sup>Sc </sup> ) deposition at thalamic level (D, E, F). Tests were performed by using formalin-fixed brain from Tg338 mice...\n\n【34】Both lesion profile and PET blot PrP <sup>Sc </sup> distribution in Tg 338 mice enabled a clear differentiation between the S83 isolate, atypical scrapie, and BSE in sheep ( Figure 3 ). The lesion profile observed in Tg338 ( Figure 3 , panel A) inoculated with atypical scrapie was similar to that described in published data for 11 atypical scrapie cases in France ( _16_ ).\n\n【35】Figure 4\n\n【36】Figure 4 . Biochemical properties of prion protein (PrP <sup>Sc </sup> ) associated with the ARR/ARR scrapie case S83 from France after passage in Tg 338 VRQ mice. A) Western blot mobility of the original S83 ARR/ARR...\n\n【37】S83-isolate–infected Tg338 mice PrP <sup>Sc </sup> reproduced a Western blot signature similar to that of the initial cases (electromobility and glycotype) and was distinct from that of PrP <sup>Sc </sup> purified from Tg338 mice infected with either BSE in sheep or atypical scrapie ( Figure 4 , panel A). After passage of the 1) S83 isolate, 2) isolates from the atypical scrapie case and BSE in sheep, and 3) 3 independent classic scrapie isolates passaged in Tg338 mice, the PK resistance of the PrP <sup>Sc </sup> was measured ( Figure 4 , panel B). Because Tg338 mice overexpress the VRQ sheep allele and, consequently, PrP <sup>Sc </sup> produced in this mouse model derives from the conversion of VRQ-PrP <sup>C </sup> , the finding that PrP <sup>Sc </sup> PK resistance in these mice was similar to that of the original isolate was surprising. Thus, the lower PrP <sup>Sc </sup> PK resistance observed in S83 isolate seems not to depend on the _PRNP_ sheep genotype but is rather an intrinsic property of the scrapie strain involved.\n\n【38】### Discussion\n\n【39】Epidemiologic and genetic data collected in recent decades have indicated that sheep carrying the PrP <sup>C </sup> \\-encoding ARR haplotype only are resistant to natural TSE infections, whereas those carrying the homozygous VRQ or ARQ haplotypes are highly susceptible. This assumption has been supported by genotyping data for thousands of sheep with scrapie worldwide: only 1 homozygous ARR carrier has been found. A case of classic scrapie in an ARR/ARR sheep in Japan was reported more than a decade ago ( _7_ ). However, the validity of this diagnosis has been heavily challenged when it could not be reconfirmed independently, because no suitable frozen or formalin-fixed material was available. Our current report shows that classic scrapie cases can occur in homozygous ARR sheep and suggests that even the former report may have been an imperfect first demonstration of such a case.\n\n【40】The apparent resistance of sheep of the ARR/ARR genotype to both natural scrapie and experimental BSE has triggered national authorities since the late 1990s (e.g., United Kingdom, the Netherlands, France), and since 2001 the EU, to implement a genetic selection and culling policy in sheep to protect the human food chain from small ruminant TSEs and to eradicate TSE from affected flocks.\n\n【41】This global approach, even if valuable, considered scrapie as a single entity and did not take into account any kind of TSE agent biodiversity. Sheep TSE agents have strikingly different abilities to replicate in hosts expressing a spectrum of PrP variants. Sheep with genotype ARQ/ARQ in scrapie flocks are commonly affected by the disease ( _8_ ). However, a historical sheep scrapie brain pool (SSBP-1) transmits easily to VRQ homozygous or heterozygous sheep but not to ARQ/ARQ animals ( _17_ ). Similarly atypical scrapie cases (including the “Nor98” type) are more frequently found in AHQ and AF <sub>141 </sub> RQ haplotype carriers than in sheep carrying exclusively other haplotypes. However, atypical cases have also been found in ARR/ARR animals ( _18_ , _19_ ). Similarly, the ability of BSE to develop in ARR/ARR sheep was observed after experimental parenteral inoculation. However, in this case, higher transmission rates and shorter incubation periods were observed in sheep of the other genotypes, such as ARQ/ARQ, and AHQ/AHQ ( _20_ , _21_ ). The susceptibility of ARR/ARR sheep to an oral BSE challenge was reported most recently ( _22_ ).\n\n【42】Both BSE and atypical scrapie PrP <sup>Sc </sup> have a characteristic molecular signature, which allows a rapid and reliable biochemical discrimination from each other and from classic scrapie PrP <sup>Sc </sup> ( _3_ , _23_ ). In both cases of scrapie in ARR/ARR sheep in France and Germany, abnormal PrP <sup>Sc </sup> harbored features (apparent molecular mass and glycotype) that were similar to those observed in classic scrapie. However, at least in the S83 case, PrP <sup>Sc </sup> seemed to have a remarkably lower PK resistance than that observed in a panel of scrapie isolates. This observation sustains the idea that the involved agent could belong to a particular scrapie agent group that cannot be directly identified by using the current biochemical criteria for TSE agent discrimination.\n\n【43】The successful propagation of the S83 isolate in Tg338 mice that express the ovine VRQ haplotype and the persistence of its original biochemical signature (including the low PK resistance) allow the inference that this scrapie agent could also be present and could naturally propagate in sheep that harbor genotypes other than ARR/ARR. The transmissibility and contagiousness of the S83 isolate are currently under investigation in experimentally challenged sheep. These experiments should produce a better understanding of the susceptibility of each genotype to this agent and its capacity to spread efficiently in sheep flocks.\n\n【44】The discovery of these 2 cases clearly indicates that the genetic resistance of ARR/ARR sheep to the so-called classic scrapie agent is not absolute. It also provides evidence that, rather than being a single entity, scrapie is a mosaic of infectious agents harboring different biologic properties in its natural host. Finally, although many thousands of cases of classic scrapie have been reported in sheep of other PrP genotypes and hundreds of thousands of rapid tests have been performed in Europe since the implementation of active TSE surveillance in small ruminants began in 2001, the discovery of these 2 ARR/ARR cases supports the idea that such infections are extremely rare.\n\n【45】Dr Groschup is head of the Institute for Novel and Emerging Infectious Diseases at the Friedrich-Loeffler-Institut, Insel Riems, Germany, and extraordinary professor at the Veterinary University of Hannover, Germany. His interests focus on prion infections, including their diagnosis, transgenic detection, and molecular determinants of infectivity and transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3efdd723-fd7e-447b-bbb3-382735e443b4", "title": "Key Public Health Messages", "text": "【0】Key Public Health Messages\nCDC’s Arthritis Management and Wellbeing Program promotes five key messages to help adults affected by arthritis be in control of their condition and their lives. These five key messages describe simple techniques that are easy to implement:\n\n【1】### 1\\. Learn Arthritis Management Strategies.\n\n【2】*   Learning arthritis management strategies can provide adults with arthritis with the skills and confidence to effectively manage their condition.\n*   Interactive workshops teach self-management skills, are low-cost (about $25 – $35), and available in communities across the country. Attending one of these programs can help a person learn ways to manage pain, exercise safely, and gain control of arthritis.\n\n【3】Learn more about self-management education workshops .\n\n【4】Top of Page\n\n【5】### 2\\. Be Active.\n\n【6】*   Research shows physical activity decreases pain, improves function and delays disability for adults with arthritis.\n*   Adults with arthritis should strive to get at least 150 minutes of moderate physical activity each week for substantial health benefits. This can be broken up into shorter periods. However, some physical activity is better than none.\n*   There are group and personal exercise programs, such as EnhanceFitness and Walk with Ease , that can help adults with arthritis increase their physical activity safely and comfortably. Learn about CDC-recognized arthritis-appropriate physical activity programs .\n\n【7】Learn more about physical activity for arthritis .\n\n【8】Top of Page\n\n【9】### 3\\. Watch Your Weight.\n\n【10】*   Research confirms that maintaining a healthy weight can limit arthritis progression and activity limitation.\n*   For every pound lost, there is a 4 pound reduction in the load exerted on the knee.\n*   For adults who are overweight, a modest weight loss (for example, 5% or 12 pounds for a 250 pound person) can help reduce arthritis-related pain and disability.\n\n【11】Learn more about managing your arthritis .\n\n【12】Top of Page\n\n【13】### 4\\. See Your Doctor.\n\n【14】*   Early diagnosis and professionally guided management is critical to maintaining a good quality of life, particularly for adults with inflammatory arthritis.\n*   Essential disease modifying drugs are beneficial in rheumatoid arthritis and other inflammatory arthritis conditions and are available only by prescription.\n\n【15】Top of Page\n\n【16】### 5\\. Protect Your Joints.\n\n【17】*   Injuries to joints, such as those related to sports or work, can increase the likelihood of developing osteoarthritis. Jobs that have repetitive motions, for example repeated knee bending, place individuals at higher risk for this type of arthritis.\n*   Avoiding injuries to joints can reduce the likelihood of developing or worsening osteoarthritis.\n\n【18】Top of Page\n\n【19】Related Pages\n\n【20】*   About CDC’s Arthritis Management and Wellbeing Program\n*   Arthritis Types\n*   Physical Activity for Arthritis\n*   Managing Arthritis\n*   Arthritis-Related Statistics", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "893edfbb-ef77-406a-a1bb-22fca9fb30d6", "title": "Trends in Hospitalizations for Peptic Ulcer Disease, United States, 1998–2005", "text": "【0】Trends in Hospitalizations for Peptic Ulcer Disease, United States, 1998–2005\nPeptic ulcer disease (PUD) is a common illness that affects >6 million persons in the United States each year, causing considerable illness and a large economic cost to the healthcare system ( _1_ ). Infection with _Helicobacter pylori_ substantially increases the risk for PUD and its complications ( _2_ ). Appropriate antimicrobial drug regimens to eradicate the infection and cure ulcers have been available since Marshall and Warren discovered _H. pylori_ as an etiologic agent of ulcers in the early 1980s ( _3_ ). Eradicating these infections prevents recurrence and ulcer complications such as bleeding or perforation ( _4_ _–_ _6_ ). Therefore, a decline in hospitalizations for PUD and its complications could be expected since treatment for _H. pylori_ infection became available.\n\n【1】Although rates of hospitalization for PUD declined in the United States during the 1980s and 1990s, rates remained high ( _7_ , _8_ ). One reason was the lack of knowledge among the general public and clinicians about the link between _H. pylori_ and PUD ( _9_ _–_ _11_ ). The Centers for Disease Control and Prevention, in collaboration with partners from other federal agencies, academic institutions, and private industry, initiated an educational campaign in 1997 to increase awareness of the relationship ( _9_ ). The goals of the campaign were to promote the increased use of appropriate antimicrobial drug treatment to eradicate _H. pylori,_ which would thus lead to a further decline in rates of hospitalization for PUD and its complications. Accordingly, reducing hospitalizations for PUD 35% from the 1998 baseline rate of 71/100,000 population to 46/100,000 population by the year 2010 was included in the Healthy People 2010 objectives that were developed in 1998 by the US Department of Health and Human Services ( _12_ ).\n\n【2】The prevalence of _H. pylori_ infections and their associated conditions can vary considerably among population groups within the same country. Racial and ethnic differences have been noted, with blacks more affected than whites and Mexican-Americans more affected than non-Hispanic whites and non-Hispanic blacks ( _12_ _,_ _13_ ). A recent meta-analysis in which researchers adjusted for age and socioeconomic status, showed that _H. pylori_ infection was significantly associated with male sex in 18 adult populations ( _14_ ). In addition, male patients were hospitalized more often for duodenal ulcers than were female patients ( _15_ _,_ _16_ ). The prevalence of _H. pylori_ infection and PUD can also vary by geographic location, socioeconomic status, and age ( _13_ _,_ _17_ ).\n\n【3】Although recent studies have suggested that rates of PUD have declined in European countries and in non-European countries outside the United States ( _16_ _,_ _18_ _–_ _20_ ), the overall recent national trends of PUD in the United States have not been described. To determine whether rates of hospitalization due to PUD and its complications have decreased and to describe the demographic characteristics of hospitalized persons with PUD, we conducted a retrospective analysis of hospital discharge data for PUD in the United States from 1998 through 2005.\n\n【4】### Methods\n\n【5】Using the Nationwide Inpatient Sample (NIS) ( _21_ ), we analyzed hospital discharge data during 1998–2005 for the general US population. The NIS is the largest all-payer, inpatient-care database in the United States; it is produced by the Healthcare Cost and Utilization Project (HCUP), sponsored by the Agency for Healthcare Research and Quality in partnership with public and private statewide data organizations ( _21_ _,_ _22_ ). The NIS is a stratified probability sample of hospitals in participating states designed to approximate a 20% sample of all US community hospitals. Hospitals in the sampling frame include short-term, nonfederal general and specialty hospitals from as many as 37 states (in 2005). We calculated national hospitalization estimates using discharge weights developed by HCUP.\n\n【6】Diagnosis codes from the International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM), were used to define PUD in the following terms: peptic ulcer (533), gastric ulcer (531), gastrojejunal ulcer (534), and duodenal ulcer (532) ( _23_ _)_ . Since the first-listed diagnosis is the condition chiefly responsible for the hospitalization, hospitalizations in which 1 of these codes was listed as the first diagnosis were considered to be more specific than hospitalizations in which 1 of these codes was listed as 1 of as many as 15 diagnoses. We limited the analysis to rates of hospitalization for first-listed PUD diagnoses, unless otherwise stated. In a separate analysis, we selected records with _H. pylori_ infection as 1 of as many as 15 diagnoses (ICD-9-CM code 041.86), without regard to PUD on the record.\n\n【7】The PUD and _H. pylori_ hospitalizations were examined by age group (<20, 20–44, 45–64, and \\> 65 years of age), sex, race/ethnicity (white, black, Hispanic, and Asian or Pacific Islander), geographic region (Northeast, Midwest, South, and West), designated ulcer type (peptic, gastric, gastrojejunal, and duodenal), other diagnoses listed along a PUD diagnosis, and procedures. Race/ethnicity was missing in the record for 26.0% of hospitalizations.\n\n【8】We selected first-listed hospitalizations for gastritis/duodenitis (ICD-9-CM code 535) as a comparison group to ensure that a change in the PUD hospitalization rate was not attributable to changes in diagnoses resulting from the increased specificity associated with endoscopy. Hospitalizations for all diagnoses were also examined as a comparison group to ensure that a change in hospitalizations for PUD was not merely a reflection of a change in the total number of hospitalizations for all diagnoses.\n\n【9】Annual average hospitalization rates were expressed as the number of hospitalizations per 100,000 population. The hospitalization rates were calculated by using the weighted number of hospitalizations and the census population for each year of the study period from HCUP ( _24_ _,_ _25_ ). SEs for the hospitalization estimates were calculated by using SUDAAN software and discharge weights provided by HCUP to account for the sampling design; SEs were used to calculate 95% confidence intervals (CIs) for the rates ( _25_ ). If the relative SE of national estimates exceeded 0.30, or if the number of unweighted outpatient visits or hospitalizations in a strata was <30, the estimates were considered unreliable and were not shown here ( _26_ ).\n\n【10】A weighted least-squares technique was used to assess a linear trend in the annual hospitalization rate for ulcer types during the 8-year study period of 1998–2005; the independent variable was year and the dependent variable was the rate. A modification to the classical regression technique was necessary to account for the changing NIS survey design over the study period, as described by Gillum et al. ( _27_ ). In this method, a regression line is fit to the data, and the resulting slope is tested for difference from zero by using a Wald test for significance; autocorrelation was not assessed. A p value <0.05 was considered significant in this study.\n\n【11】Age-adjusted rates were calculated by using the direct method, taking into account the survey design, and using the projected 2000 US Census population as the reference population ( _28_ ). In the direct method, the age-adjusted rate represents what the rate would be if the study population had the same age distribution as a reference population (i.e., the projected US Census 2000 age distribution). This method is used to remove confounding by age when rates are compared over time or across populations with different age distributions. To calculate age-adjusted rates, the age group–specific rates (hospitalizations/population) were multiplied by weights representing the proportion of the reference population belonging in the corresponding age group; the resulting quantities were summed to obtain the age-adjusted rate ( _28_ ). We report the age-adjusted PUD hospitalization rates; the overall age-adjusted rate did not differ from the unadjusted rate, although the rates for some groups did differ.\n\n【12】### Results\n\n【13】##### Overall PUD Hospitalization Rates\n\n【14】A total of 1,453,892 first-listed PUD hospitalizations were estimated for 1998–2005, with an average annual age-adjusted hospitalization rate of 63.6/100,000 population (95% CI 62.9–64.3) ( Table 1 ). The hospitalization rate was highest for adults \\> 65 years of age (299.8/100,000 population) and decreased with decreasing age group. Overall, age-adjusted hospitalization rates were significantly higher for male patients than for female patients (71.9/100,000 population \\[95% CI 71.0–72.7\\] and 56.3/100,000 population \\[95% CI 55.6–57.0\\], respectively). The rates were significantly higher for male patients of all age groups and of all race/ethnicity groups.\n\n【15】Overall, age-adjusted hospitalization rates were significantly lower for whites (44.2/100,000 population; 95% CI 43.3–45.0) than for each of the other racial/ethnic groups ( Table 1 ). This rate difference for male patients was similar across groups of various races and ethnicities. Among female patients, the rate was significantly higher for blacks than those for each of the other racial/ethnic groups.\n\n【16】The average annual age-adjusted hospitalization rate was higher for patients with ulcers designated gastric (33.7/100,000 population; 95% CI 33.3–34.2) than for all patients with other ulcer designations ( Table 1 ). However, a gender-specific comparison showed that for male patients, the hospitalization rate for ulcers designated gastric was comparable to that for ulcers designated duodenal (33.5/100,000 population \\[95% CI 33.1–34.0\\] and 33.1/100,000 population \\[95% CI 32.9–33.5\\]). Among female patients, the hospitalization rate for ulcers designated gastric (33.6/100,000 population; 95% CI 33.1–34.0) was almost double that for ulcers designated duodenal (17.1/100,000 population; 95% CI 16.8–17.4).\n\n【17】##### Trends over Time in PUD Hospitalization Rates\n\n【18】Figure 1\n\n【19】Figure 1 . Age-adjusted hospitalization rates for first-listed discharge diagnoses of peptic ulcer disease (diagnosis codes 531–534 from the International Classification of Diseases, 9th Revision, Clinical Modification), United States, 1998–2005. A) Overall age-adjusted hospitalization...\n\n【20】The overall age-adjusted hospitalization rate for PUD decreased 21%, from 71.1/100,000 population (95% CI 68.9–73.4) in 1998 to 56.5/100,000 population (95% CI 54.6–58.3) in 2005 ( Table 2 ; Figure 1 ). The hospitalization rate appeared to decline for all age groups (19%–22%), except children <20 years of age, for whom no significant change occurred during the study period. Although the hospitalization rate was higher for male patients than for female patients in 1998 (83.1/100,000 and 60.8/100,000 population, respectively) and in 2005 (62.2/100,000 and 51.3/100,000 population, respectively), the difference decreased because of a greater decline for male patients (25%) than for female patients (16%). The hospitalization rate was significantly lower in 2005 than in 1998 for all racial/ethnic groups, except for Hispanics; the greatest decline was found for blacks (40%) and the least decline was observed for whites (22%). The hospitalization rate was also significantly lower in 2005 than in 1998 in all regions.\n\n【21】The age-adjusted rate for hospitalizations for gastritis/duodenitis (selected as a comparison group to ensure that a change in the PUD hospitalization rate was not attributable to changes in diagnosis coding practices) decreased 16%, from 55.0/100,000 population (95% CI 52.6–57.4) in 1998 to 46.0/100,000 population (95% CI 44.4–47.7) in 2005. The rate of hospitalization for all diagnoses (included as a comparison group to ensure that a change in hospitalizations for PUD was not merely a reflection of a change in the total number of hospitalizations for all diagnoses) did not change significantly during the study period.\n\n【22】##### Procedures and Other Listed Diagnoses\n\n【23】Figure 2\n\n【24】Figure 2 . Hospital procedures most frequently listed with first-listed discharge diagnoses of peptic ulcer disease (diagnosis codes 531–534 from the International Classification of Diseases, 9th Revision, Clinical Modification \\[ICD-9-CM\\]), United States, 1998–2005. Source:...\n\n【25】Figure 3\n\n【26】Figure 3 . Other diagnoses most frequently listed with first-listed discharge diagnoses of peptic ulcer disease (PUD) (diagnosis codes 531–534 from the International Classification of Diseases, 9th Revision, Clinical Modification \\[ICD-9-CM\\]), United States, 1998–2005....\n\n【27】Figure 4\n\n【28】Figure 4 . Proportion of first-listed ulcer diagnoses with a co-diagnosis of _Helicobacter pylori_ infection (diagnosis codes 531–534 from the International Classification of Diseases, 9th Revision, Clinical Modification \\[ICD-9-CM\\]), by ulcer type, United States,...\n\n【29】Esophago-gastroduodenoscopy (EGD) with closed biopsy of ≥1 sites involving the esophagus, stomach, or duodenum was the most common procedure performed in patients with PUD listed as first reason for hospitalization ( Figure 2 ). Transfusion of packed red blood cells, endoscopic control of gastric or duodenal bleeding and flexible fiberoptic colonoscopy were also common. Many diagnoses frequently were listed with PUD hospitalizations. Among these, unspecified essential hypertension, acute posthemorrhagic anemia, iron deficiency anemia secondary to blood loss, diaphragmatic hernia, and _H. pylori_ infection were the most common ( Figure 3 ). A greater proportion of ulcers designated duodenal were listed with an _H. pylori_ co-diagnosis than any other ulcer designation considered ( Figure 4 ).\n\n【30】##### Hospitalization Rates for _H. pylori_ Infections\n\n【31】The overall age-adjusted rate of hospitalization that included any discharge diagnosis of _H. pylori_ infection decreased 47%, from 35.9/100,000 population (95% CI 34.3–37.5) in 1998 to 19.2/100,000 population (95% CI 18.3–20.1) in 2005 ( Table 3 ). The hospitalization rate increased with age and declined during 1998–2005 for all age groups except children <20 years of age. The greatest percentage rate decrease was observed for adults \\> 65 years of age, for whom the hospitalization rate decreased 54%, from 163.5/100,000 population (95% CI 152.9–174.0) in 1998 to 75.4/100,000 population (95% CI 70.0–80.8) in 2005. The hospitalization rate for male patients was only slightly higher than that for female patients, and the decline in rates was similar for both groups. In 1998, the hospitalization rate was higher for blacks (44.1/100,000 population; 95% CI 39.7–48.6) and Hispanics (41.8/100,000 population; 95% CI 34.0–49.6) than for whites (23.2/100,000 population; 95% CI. 21.7–27.9) and Asian/Pacific Islanders (34.0/100,000 population; 95% CI 26.2–41.9). In 2005, the same pattern was observed; hospitalization rate was significantly higher for blacks (23.6/100,000 population; 95% CI 21.1–26.1) and Hispanics (24.5/100,000 population; 95% CI 21.1–27.9) than for whites (10.3/100,000 population; 95% CI 9.6–11.0) and Asian/Pacific Islanders (15.8/100,000 population; 95% CI 12.0–19.6). Rates declined significantly for all racial/ethnic groups, except for Hispanics; the greatest percentage rate decrease was observed for whites (56%). The hospitalization rates were significantly different in 1998 and 2005 in all geographic regions. In 1998, the hospitalization rate was higher in the South (39.3/100,000 population; 95% CI 36.4–42.1) than in the West and Northeast regions (33.6/100,000 and 31.4/100,000 population, respectively; 95% CI 30.8–36.3 and 27.8–35.1, respectively). By 2005, the hospitalization rate was higher in the South (21.6/100,000 population; 95% CI 19.8–23.4) and in the Northeast (19.3/100,000 population; 95% CI. 17.6–21.0) than in the West (16.5/100,000 population; 95% CI 14.8–18.1) because the decline apparently occurred more slowly in the Northeast (39%) than in all other regions.\n\n【32】### Discussion\n\n【33】Our analysis of hospital discharge records from a nationally representative sample of US hospitals indicates that the overall age-adjusted rate of hospitalization for PUD declined during 1998–2005. This finding is consistent with decreases previously observed for PUD hospitalizations in the United States in several studies from the 1970s through the 1990s ( _7_ _,_ _8_ _,_ _14_ _,_ _17_ _,_ _29_ _)_ . One study by Manuel et al. did not observe a downward trend in PUD hospitalizations during 1996–2005; however, that study included only 5 hospitals ( _30_ ). We analyzed data for first-listed PUD hospitalizations to limit data to hospitalizations for care specifically for ulcer-related issues. In addition, >50% of patients in our study had an EGD with closed biopsy of ≥1 sites involving the esophagus, stomach, or duodenum ( Figure 2 ). EGD with biopsy is a reliable technique to differentiate between PUD and other causes of abdominal pain such as gastritis ( _17_ ). Thus, it appears that the data in this study are reflective of patients who were truly hospitalized primarily for PUD or its complications.\n\n【34】Decreases in PUD hospitalizations are likely attributable to an underlying decline in _H. pylori_ prevalence ( _7_ _,_ _15_ ). However, declines in PUD hospitalizations have also been attributed to changes in diagnosis coding because of improved diagnostic specificity associated with endoscopy ( _31_ ). If the decrease in PUD hospitalizations was attributable to changes in diagnosis coding, the decrease in PUD hospitalizations would be inversely related to a rise in hospitalizations for gastritis/duodenitis. We found that hospitalization rates declined for gastritis/duodenitis and for PUD, which indicates that the results cannot be attributable to changes in diagnosis coding practices. Furthermore, because the hospitalization rate for all diagnoses did not change significantly from 1998 to 2005, the decline in the PUD hospitalization rate observed would not merely reflect general trends in hospitalization. The overall rate for any listed _H. pylori_ diagnosis declined significantly during the study period, which suggests that a decrease in rates of _H. pylori_ infections may be partially responsible for the decrease in hospitalizations for PUD.\n\n【35】In our study, the overall rate of hospitalizations for PUD differed according to the patient’s age, sex, race/ethnicity, and region. The highest rates of hospitalization for those with both PUD and _H. pylori_ infection were for adults \\> 65 years of age and decreased with each subsequent age group. This finding may result from an underlying birth cohort effect, in this case a decrease in _H. pylori_ incidence for younger generations because of improved sanitation and fewer risk factors for transmission ( _18_ _,_ _32_ _,_ _33_ ). A similar percentage change in rate of PUD hospitalizations was observed for all age groups \\> 20 years. The comparable declines for these age groups may be partially attributable to increased use of _H. pylori_ eradication therapy during 1998–2005, perhaps because of increased awareness among clinicians and patients of the association between _H. pylori_ and PUD ( _9_ ).\n\n【36】In this study, the overall rate of hospitalization for PUD in 1998 was higher for male patients than for female patients. However, by 2005 this difference had narrowed considerably because of a greater decrease in rates for male patients than for female patients. A 1985 study that examined data from the National Center for Health Statistics also recognized a trend toward comparable rates of hospitalization for both sexes ( _34_ ). Our study also found differences in hospitalization rates between sexes by designated ulcer type; duodenal ulcer hospitalization rates were higher for male patients than for female patients. This finding is consistent with hospital admission data from the United Kingdom ( _15_ , _16_ ).\n\n【37】A study that used a national sample of US hospital discharge records noted differences in the hospitalization rate for PUD between racial/ethnic groups; blacks were more frequently hospitalized for PUD than whites in 1998 ( _13_ ). Although we also found that rate of hospitalization for PUD was higher for blacks than for whites, the rate appears to be declining more rapidly for blacks than for whites. In addition, although rates were significantly lower for whites than for those in other racial/ethnic categories in 1998, by 2005 this rate difference was no longer significant because for whites, the decline apparently occurred more slowly than it did for all other racial/ethnic groups. Differences also varied by sex, as well as race/ethnicity, and suggest that hospitalizations for PUD among nonwhite men may merit further investigation. Race/ethnicity information was missing for patients in 26% of hospitalization records, possibly making comparisons between racial/ethnic groups inaccurate. A study of underreporting of race/ethnicity information in the National Hospital Discharge Survey suggests that hospitals that do not report race/ethnicity information may have a higher proportion of discharges for whites and a lower proportion of discharges for blacks than hospitals that do report race/ethnicity information ( _35_ ). Our study did not examine PUD hospitalizations for American Indians and Alaska Natives because the survey’s sample size was not large enough and did not include visits to Indian Health Service or tribal facilities. However, a previous study showed that among American Indians and Alaska Natives, the prevalence of ulcer-associated conditions was high during 1996–2005, which indicates that hospitalizations for PUD among this group may warrant further study ( _36_ ).\n\n【38】Our study showed similar trends for hospitalizations for PUD and _H. pylori_ infection, although we noted some differences. For both PUD and _H. pylori_ infections, the rate of hospitalization increased with age, and the age-adjusted hospitalization rate was lower for whites than for persons in any other racial/ethnic group category. In addition, the overall age-adjusted rate of both PUD and _H. pylori_ hospitalizations was higher for male patients than for female patients. However, although the age-adjusted PUD hospitalization rate appears to be declining more rapidly among male patients than among female patients, the age-adjusted _H. pylori_ infection hospitalization rate appears to be declining at a similar pace for female patients and male patients. The age-adjusted PUD hospitalization rate for Hispanics did not decline significantly, and a decline in the age-adjusted _H. pylori_ hospitalization rate for this group was only borderline significant, which suggests that rates among this group may deserve special attention. However, this finding may be biased because of missing information on race/ethnicity.\n\n【39】Our findings in this study show a continued downward trend in the rate of hospitalizations for PUD in the United States. Differences in the rate of decline for PUD hospitalization rates between sexes, racial/ethnic groups, and regions warrant further study. The overall downward trend observed in this study does not seem to be attributable to increases in gastritis/duodenitis hospitalizations or to a decline in total hospitalizations. The decline in the PUD hospitalization rate may be attributable to a birth cohort effect with subsequent declines in _H. pylori_ infection prevalence and increased use of successful antibiotic treatments to eradicate _H. pylori_ infections. Other factors possibly contributed to the decline in PUD hospitalizations observed in this study, including trends in use of nonsteroidal anti-inflammatory drugs and the availability of over-the-counter H2 antagonists and proton pump inhibitors. Studies on the relationship between PUD hospitalizations and nonsteroidal anti-inflammatory drug use, the possibility of undercoding for _H. pylori_ on hospitalization discharge records, and subpopulation analyses would help further guide recommendations and show how to focus interventions. To facilitate further declines in hospitalizations for PUD, patients and clinicians should continue to be educated about the association between _H. pylori_ and PUD.\n\n【40】Ms Feinstein previously worked in the area of foodborne epidemiology at the Centers for Disease Control and Prevention. She is now a PhD candidate at the University of North Carolina, Chapel Hill, focusing on the epidemiology of infectious diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2cf96b5e-a074-49d9-915a-7f7b8036e992", "title": "Using Community Health Workers to Prevent Infectious Diseases in Women", "text": "【0】Using Community Health Workers to Prevent Infectious Diseases in Women\nCommunity health workers have a long history of promoting comprehensive health services, but using them to deliver infectious disease prevention services in the United States has not been well studied recently. When to use community health workers and the skills required vary depending on the environment, culture, and context in which they are used. In some environments, where healthcare services may be difficult to access, community health workers may be involved in various stages of infectious disease prevention: general health promotion, specific prophylaxis, early diagnosis and treatment, limiting disability, and rehabilitation. In this context, community health workers may be junior level health officers or volunteer healthcare workers who receive appropriate training for functioning at the required stage.\n\n【1】In other environments, community health workers function as _promotoras_ or \"health promoters.\" In this context, promotoras typically emerge from naturally occurring networks and focus on fostering behavior change through role modeling, group activities, skill building, and goal setting. Promotoras work in small groups and build strong collaborations with schools, churches, clinics, and local health departments. Through these activities, promotoras facilitate links between patients and the healthcare system and minimize dropouts from required treatment regimens. Promotoras and community healthcare workers are trusted by the communities they serve and provide cost-effective services to persons whose infectious diseases might otherwise go untreated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "21af4b22-f25d-4269-b922-c46c152afab0", "title": "Serologic Evidence of Orientia Infection among Rural Population, Cauca Department, Colombia", "text": "【0】Serologic Evidence of Orientia Infection among Rural Population, Cauca Department, Colombia\nScrub typhus, caused by species in the genus _Orientia_ , is a reemerging mite-borne rickettsiosis and a major cause of acute undifferentiated febrile illness (AUFI) ( _1_ ). Classically, scrub typhus was believed to be strictly endemic to the so-called tsutsugamushi triangle, which ranges from southeastern Siberia in the North to the Kamchatka Peninsula in the East, northern Australia in the South, and Pakistan in the West ( _1_ ). However, scrub typhus outside the tsutsugamushi triangle was suggested 70 years ago because seropositivity to _O. tsutsugamushi_ was found among persons from several countries in Africa ( _1_ ).\n\n【1】In 2006, a confirmed case of scrub typhus outside the classical disease-endemic region was described in a traveler who visited Dubai (United Arab Emirates); the case was caused by a novel species, _Candidatus_ Orientia chuto ( _2_ ). More recently, in Latin America, autochthonous cases of scrub typhus have been reported in Chile and attributed to a new _Orientia_ species, _Candidatu_ s Orientia chiloensis, associated with _Herpetacarus_ spp. mites, which have been recognized as novel vectors of scrub typhus in this newly recognized disease-endemic region ( _3_ , _4_ ).\n\n【2】In addition to the aforementioned studies in Chile, serologic surveys have been performed among military personnel stationed in the Peruvian Amazon and Honduras, demonstrating exposure to _Orientia_ , as well as a possible role for this bacterium in the etiology of AUFI ( _5_ , _6_ ). Thus, to explore new geographic regions that might have _Orientia_ circulation, we report serologic evidence of this rickettsial agent in Colombia.\n\n【3】We screened 486 serum samples collected in 4 municipalities in the Cauca Department, Colombia, during 2017 and used in a previous study ( _7_ ) for _Orientia_ seroreactivity. Cauca Department is located in southwestern Colombia, mainly in the Andean and Pacific regions plus a tiny part in the Amazonian region. The climate is warm and humid on the western slope; warm and semiarid toward the Patía basin; and temperate humid to semihumid centrally, with cold climates recorded on both sides of the Popayán (capital city) Plateau.\n\n【4】We tested for antibodies reactive to _O. tsutsugamushi_ by indirect immunofluorescence antibody (IFA) assay and ELISA ( Appendix ). To further confirm the specificity of the reactive samples, we performed a Western blot assay ( Appendix ).\n\n【5】Figure\n\n【6】Figure . Serologic evidence of _Orientia_ infection among rural population, Cauca Department, Colombia. Western blot analysis was performed using Vero cell extracts and _O. tsutsugamushi_ Karp strain whole-cell antigens....\n\n【7】Of 486 serum samples, 6 (1%) were seropositive for _Orientia_ by IFA with an endpoint titer of 1:128 to 1:1,024, and 62 (13%) yielded positive results by ELISA (optical density cutoff >0.37). Only 1 sample was positive by both methods. Overall, 67 (13.8%) serum samples were positive by \\> 1 screening method. Testing of 67 of the seropositive samples by Western blot showed that 20 (30%) were reactive with \\> 1 _O. tsutsugamushi_ band. A total of 8 samples were reactive to 56-kD, 5 to 47-kD, and 7 to both ( Table ; Figure ).\n\n【8】Our results show serologic evidence of _Orientia_ exposure among the rural population of 4 municipalities from Cauca Department, Colombia. This evidence suggested circulation of this rickettsial agent in this region.\n\n【9】Using 2 serologic techniques (IFA and ELISA), we found different seropositivity rates (1% for IFA and 13% for ELISA) for the same serum samples. Similar discordance has been observed for persons from Sao Tome and Principe ( _8_ ). The difference in seropositivity rates between methods might be caused by the antigens used because our IFA used only antigens of the Karp strain, which could fail to detect antibodies stimulated by organisms of other serotypes and species. Moreover, some autochthonous cases of scrub typhus in Chile have also shown discordant serologic results (reactive IgG by IFA/negative IgG by ELISA, and vice versa) ( _9_ , _10_ ).\n\n【10】As briefly mentioned, the use of ELISA to detect IgG reactive against _Orientia_ showed reactivity in 5.3% of convalescent-phase serum samples from febrile patients in the region of Iquitos (Peruvian Amazon) ( _5_ ) and in 5.6% of US military personnel stationed at least 6 months in Honduras ( _6_ ). Our study found 13% seropositivity by using the same serologic approach (IgG ELISA), which is greater than results obtained in Perú and Honduras. In addition to ELISA and IFA screening methods, we used a confirmatory test (Western blot), which demonstrated reactivity to \\> 1 immunodominant protein (56 kD and 47 kD) of _Orientia_ in 30% of the tested samples, which were positive by \\> 1 screening method.\n\n【11】In conclusion, we report serologic evidence of _Orientia_ in Colombia, supporting that these bacteria are probably widely distributed in Latin America. Our study also supports emerging evidence that _Orientia_ spp. is no longer restricted to the tsutsugamushi triangle. Prospective studies to examine acute-phase and convalescent-phase serum samples and isolation of the agent in those with AUFI in Colombia and other countries in Latin America should be undertaken. In addition, scrub typhus should be considered as a potential diagnosis by physicians and public health officials when an eschar-associated rickettsiosis is suspected in this region.\n\n【12】Dr. Faccini-Martínez is a physician‒scientist at Fundación Universitaria de Ciencias de la Salud, Bogotá, Colombia. His primary research interests include zoonotic and vector-borne diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "53ab3861-6e9b-4e31-9a83-b5ae80915c93", "title": "IgG Against Dengue Virus in Healthy Blood Donors, Zanzibar, Tanzania", "text": "【0】IgG Against Dengue Virus in Healthy Blood Donors, Zanzibar, Tanzania\nIn eastern Africa, the available evidence indicates that dengue virus serotypes 1, 2, and 3 (DENV-1, -2, -3) are common causes of acute fever ( _1_ ). A recent map of DENV transmission has shown that the virus could be transmitted in most eastern African countries, including mainland Tanzania and the Zanzibar Archipelago ( _2_ ).\n\n【1】In 2010, a PROMED report raised concerns about DENV infections in Tanzania ( _3_ ). That same year, travelers from Europe and Japan were found to be infected with DENV-3 after they returned from mainland Tanzania or Zanzibar ( _4_ – _6_ ). In Tanzania, seroprevalence rates for febrile outpatients in Tosomaganga (Iringa Region) and Pemba Island (Zanzibar) in 2007 ( _7_ ) and in Moshi (Arusha Region) in 2007–2008 ( _8_ ) were 1.8%, 7.7%, and 10.7%, respectively. To determine DENV circulation in the Zanzibar Archipelago, we assessed the seroprevalence of DENV among adult blood donors at the Zanzibar National Blood Transfusion Services (ZNBTS).\n\n【2】### The Study\n\n【3】We conducted a cross-sectional seroprevalence survey at ZNBTS from September 20 to December 10, 2011. ZNBTS is located in Stone Town, the principal city of the Zanzibar Archipelago. Ethics approval was obtained from the Zanzibar Medical Research Ethical Committee. The sample size was calculated by using methods for proportion. The estimated prevalence was set at 50% because data were not available regarding the true prevalence of the infection in the area. Considering a population of ≈1,000,000 inhabitants and a confidence level of 95%, the sample size was set at 384 donors. Sample size was then increased to 500 donors to account for those lost to follow-up.\n\n【4】During the study period, all consecutive adult donors attending ZNBTS, who had been screened and selected for blood donation, were enrolled in the study. Donors were screened by serologic tests for hepatitis B virus, hepatitis C virus, HIV, and _Trepomena pallidum_ ; they were selected for blood donation if results of all screening tests were negative. A structured interview was conducted by using a close-ended questionnaire after the donor signed the informed consent form and before the screening.\n\n【5】From each enrolled person, 10 mL of venous blood was collected. After the screening tests, the remaining serum was divided into 2 aliquots: 1 was stored at −20°C at the sample processing site for performance of the IgG ELISA at Mnazi Mmoja Hospital in Unguja, Zanzibar, and 1 was dispatched to the “L. Spallanzani” National Institute for Infectious Diseases in Rome, Italy, for testing by immunofluorescence assay (IFA) for IgG. At the end of the collection phase, samples were tested by Panbio Dengue IgG Indirect ELISA kit (Inverness Medical Innovations Australia Pty Ltd, Sinnamon Park, Queensland, Australia) according to the manufacturer’s instructions. A positive ELISA result was defined as having an index value >1.1. To compensate for the low specificity of the ELISA, we tested samples by IFA with homemade slides and a mix of uninfected and DENV-2 (New Guinea C strain)–infected Vero E6 cells. The diagnostic accuracy of the IFA has been described ( _9_ ).\n\n【6】Donors were considered positive for IgG against DENV if results of both tests were positive. Discordant results were considered negative. All districts except the urban district were considered rural areas. Univariate association between DENV IgG positivity and donor characteristics was assessed by means of odds ratios (ORs) and 95% CIs, by χ <sup>2 </sup> for categorical values, and Student _t_ \\-test for continuous variables. A multiple logistic regression model using a backward procedure was used. All variables were entered in the backward selection model, and a cutoff level of p = 0.10 was used for subsequent selections. Data management and analysis were performed by using STATA version 11 (StataCorp, College Station, TX, USA).\n\n【7】Five hundred persons consecutively attending ZNBTS were selected for blood donation and, therefore, were eligible to be enrolled in the study. Demographic characteristics of the participants are shown in Table 1 . The mean age was 32 years; 97.2% of the participants were male. Most donors had a water storage container (71.2%) and/or resting water near their home (88.0%). Bed nets were used by 59.4% of the donors, but only 40% reported insecticide spraying at home.\n\n【8】Of the 500 blood samples, 253 (50.6%) were positive by both tests, 77 (15.4%) were positive by ELISA and negative by IFA, and 170 (34.0%) were negative by both tests. DENV IgG prevalence was 50.6% (95% CI, 46.2–54.9). Considering IFA as the reference standard, ELISA sensitivity and specificity were 100% and 68.8%, respectively. According to univariate analysis, DENV IgG–positive donors were significantly more likely to be older (OR 1.32, 95% CI 1.19–1.47) and live in urban districts (OR 3.18 95% CI 2.21–4.69) compared with DENV IgG-negative donors ( Table 1 ). According to multivariate analysis, older age (adjusted OR \\[AOR\\] 1.42, 95% CI 1.27–1.61) and living in an urban district (AOR 4.09, 95% CI 2.72–6.17) were independently associated with DENV IgG positivity ( Table 2 ). Moreover, borderline evidence indicated an association of positivity with the presence of resting water near the home (AOR 1.66, 95% CI 0.99–2.76).\n\n【9】### Conclusions\n\n【10】We found high DENV IgG seroprevalence (50.6%) in adult blood donors residing in the urban district. The presence of DENV IgG is independently associated with age and urban district residence. Our results, compared with the previous low prevalence rate (7.7%) detected on Pemba island and the Zanzibar Archipelagos in 2007 ( _7_ ), suggest an endemic pattern of transmission of DENV infection in Zanzibar, similar to the situation in other African countries ( _10_ ). Considering the progressive reduction of laboratory-confirmed malaria cases in Zanzibar and the nonspecific influenza-like symptoms of DENV primary infections, this wide DENV circulation in Zanzibar appears to be largely underdiagnosed. Patients with primary DENV infection are likely to be mistakenly treated with antimalarial drugs on the basis of clinical symptoms, as we observed on Pemba ( _11_ ).\n\n【11】We cannot determine when the donors in this study were infected. The strong association with age could be explained by the progressively longer exposure of the older donors to the risk for infection. This association could be also explained by the successful malaria vector control initiative (use of long-lasting insecticidal nets, indoor residual spraying and biolarviciding at mosquito breeding sites, and environmental management), which could account for the lower DENV prevalence in the younger population ( _12_ ). Of note, in our study the proportion of persons who used bed nets and those who did not keep water storage containers near home was quite low.\n\n【12】However, before drawing firm conclusions, a few limitations must be described. First, a high proportion of participants were male. This may have been because of the low proportion of women who donate blood, a consequence of the cultural belief that women are weaker because of blood losses during menstrual periods and pregnancies. This explanation was reported by the workers at ZNBTS and is in accordance with results from a previous study on African immigrants ( _13_ ). Our sample population was thus less representative of the general population and could have affected the results, either by underestimating or overestimating the inferred prevalence. Second, we did not attempt to detect DENV circulating serotypes in this study; nevertheless, there is evidence of DENV-3 circulation in previous reports about imported DENV cases in Europe and Japan ( _4_ – _6_ ). Third, no antibody neutralizing assay has been performed to rule out cross-reactions with other circulating flaviviruses, but epidemiologic data from Zanzibar do not indicate outbreaks of other flavivirus infections. Our data constitute the first step toward better defining the circulation of DENV in the Archipelago and toward building up a preparedness and response plan to fight DENV infection.\n\n【13】Dr Vairo is chief of the Department of the “L. Spallanzani” Institute in Tanzania. His research activities are mainly focused on HIV, TB, and emerging infectious diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d4708cb4-a486-41c7-9cd6-058836c634d1", "title": "Retinal Hemorrhages in 4 Patients with Dengue Fever", "text": "【0】Retinal Hemorrhages in 4 Patients with Dengue Fever\nDengue is usually a self-limiting viral fever spread by the _Aedes aegypti_ mosquito. Minor bleeding frequently complicates dengue fever. Hemorrhagic complications are usually mild and limited to gum bleeding, epistaxis, hematuria, or menorrhagia ( 1 ). However, cases of severe and life-threatening bleeding have been reported in the medical literature. Rarely, retinal hemorrhages affecting patients with dengue fever are reported. We report 4 patients with dengue fever complicated by retinal hemorrhages who were hospitalized in our institution in June and July 2004.\n\n【1】### The Study\n\n【2】In June and July 2004, a total of 1,620 cases of dengue fever and dengue hemorrhagic fever were reported in Singapore; 621 case-patients were admitted to our hospital. In the same period, retinal hemorrhages were diagnosed in 4 dengue fever patients in our hospital.\n\n【3】As part of our protocol, we use polymerase chain reaction (PCR) (while a patient is still febrile) and serology (after resolution of fever) to confirm clinical diagnoses of dengue fever. Real time-PCR is performed daily with commercial reagents (Artus GmbH, Hamburg, Germany) on a Light Cycler (Roche Diagnostics, Penzberg, Germany). Serology is performed with the Dengue Duo IgM and IgG Rapid Strip Test (PanBio, Brisbane, Queensland, Australia). Dengue serotypes were determined by immunofluorescence, with type-specific monoclonal antibodies, on virus cultured from serum taken early in the disease. The diagnosis of retinal hemorrhage was made by an ophthalmologist after dilated fundoscopic examination. Fundal photography was also performed for all patients for baseline recording.\n\n【4】Four patients (3 women and l man) had visual symptoms, which developed during hospitalization for dengue fever. The mean age of the patients was 34 years (range 21–49). Four patients had reduced visual acuity. Two patients noticed the visual problems in the morning upon waking up. One patient complained of metamorphopsia. All denied any associated eye pain, photophobia, or increased tearing. Except mild myopia in 1 patient, no patient had a history of medical or ocular problems. The average duration of fever before visual symptoms developed was 6.25 days (range 5–8). The onset of visual symptoms was usually observed within 1 day from the resolution of fever and the nadir of the thrombocytopenia. The average platelet count on the day of onset of symptoms was 36 × 10 <sup>9 </sup> /L (range 33–44). The lowest platelet count was recorded 1 day before the onset of symptoms in 1 patient, on the day of onset of symptoms in 2 patients, and 1 day after the onset of symptoms in 1 patient. The average platelet count at the nadir of thrombocytopenia was 33 × 10 <sup>9 </sup> /L (range 26–40).\n\n【5】Fundoscopic examination showed bilateral blot hemorrhages within the vascular arcades in all 4 patients. All patients received standard supportive care, and 2 patients received platelet transfusion.\n\n【6】The clinical diagnosis of dengue fever was confirmed by PCR in 2 cases, by immunoglobulin (Ig)G and IgM in 1 patient, and by IgM in the remaining patient. Dengue virus was isolated from all 4 patients. All were serotype 1. In 3 patients, visual symptoms resolved completely within 2 days, with full recovery of the patients' visual acuities. Improvement in visual symptoms was delayed and incomplete in patient 4, and she had reduced visual acuity and metamorphopsia even after 2 months.\n\n【7】### Conclusions\n\n【8】Singapore has experienced a steady rise in the incidence of dengue in recent years. In 2002, a total of 3,937 cases of dengue were reported, followed by 4,785 cases in 2003. As of September 11, 2004, a total of 4,985 cases were reported ( 2 ).\n\n【9】Hemorrhagic diathesis is a common complication of dengue fever. Such complications typically occur toward the end of the febrile period, when the symptoms of classic dengue fever resolve. Clinically, bleeding is usually mild and manifests itself as epistaxis, gum bleeding, hematuria, and menorrhagia. Only occasional cases of severe gastrointestinal or retroperitoneal bleeding are reported. However, retinal hemorrhages are exceedingly rare as a complication of dengue fever.\n\n【10】We performed a Medline search using dengue and retinal hemorrhage as key words. It showed only 4 case reports describing retinal hemorrhages associated with dengue fever ( 3 – 6 ). Only 1 case was reported in the English medical literature. Haritoglou et al. ( 3 ) described a patient with dengue fever who had bilateral loss of vision. Fundoscopic examination indicated exudative maculopathy and small hemorrhages located in the nerve fiber layer. Initially, this patient had a visual acuity of 20/250 in both eyes. It increased to 20/100 in the right eye and 20/32 in the left eye 8 weeks after the patient was initially seen. The residual deficit was assumed to be due to intraretinal lipid deposits. Wen et al. ( 4 ) described 24 patients with visual disturbances complicating dengue fever. The mean age of these patients was 32 (range 16–45); 19 of them had blurring of vision, while others complained of central scotoma, floaters, photophobia, or halo vision. Five patients reported unilateral symptoms. Visual symptoms developed in most patients 5–8 days (range 2–15) after the onset of fever. Fundoscopic examination indicated retinal hemorrhages in 15 patients. Seven of the remaining 9 patients were seen a long time after the onset of symptoms, and fundoscopic examination showed only mild, atypical maculopathy. Optic nerve atrophy developed in 1 patient, and another patient was diagnosed with optic neuritis.\n\n【11】Our observations are similar to those described in the above reports. Our patients were also young, and visual symptoms developed 5–8 days after the onset of fever, consistent with current theories on the immunopathogenesis of dengue ( 7 ). All our patients complained of decreased visual acuity, and fundoscopic examination showed macular hemorrhages and exudative maculopathy. Careful review of clinical records allowed us to make additional observations, which have not been previously reported. In all our patients, visual symptoms developed shortly (within 1 day) before or after resolution of fever. The onset of symptoms was also associated with the nadir of thrombocytopenia.\n\n【12】All patients were evaluated by an ophthalmologist. They were treated conservatively, and 2 received platelet transfusion. Although 3 patients improved symptomatically within 2 days, recovery was slower and incomplete in the fourth patient. Although we see a considerable number of patients with dengue in our hospital, these patients are the first we have seen with dengue fever complicated by retinal hemorrhages. We postulate 2 explanations for this new clinical manifestation. First, the recent, sharp rise in the incidence of dengue fever may have allowed us to observe this otherwise rare manifestation. Or, their infection could have been caused by a particularly virulent type of dengue virus.\n\n【13】We cultured virus from acute-phase serum samples from all 4 patients; all were serotype 1. In 2003, 72% of 68 viruses isolated from sequential sera were serotype 2, and 21% were serotype 1. This proportion has changed dramatically over the last year. In 2004, 70% of 37 sequential isolates belonged to serotype 1 and only 22% to serotype 2 (Barkham T., unpub. data). Although it is tempting to presume that the recent increase in incidence of retinal hemorrhages is associated with the unique virulence of serotype 1 virus, the fact that the dengue virus in all 4 patients belonged to serotype 1 may have been pure chance and merely reflect the fact that serotype 1 became predominant in 2004. We are planning to clarify this issue by conducting a prospective study and would be pleased to hear from other centers with similar experiences.\n\n【14】Only a small minority of patients admitted with dengue fever reported ocular symptoms. All of them had retinal hemorrhages involving the macula. Hemorrhage involving peripheral parts of the retina is probably much more common. However, such patients may remain asymptomatic or have only minimal symptoms. The true incidence of retinal hemorrhage in dengue fever may be substantially higher. Further studies will be necessary to determine the true incidence of retinal hemorrhage in patients with dengue fever.\n\n【15】Retinal hemorrhage is a rare but potentially serious complication of dengue fever. The onset of symptoms appears to coincide with the resolution of fever and the nadir of thrombocytopenia. Patients with dengue fever who report visual symptoms should be evaluated promptly. Although there is no specific therapy, retinal hemorrhage may be an indication for early and aggressive correction of thrombocytopenia.\n\n【16】Dr. Chlebicki is the registrar in the Department of Infectious Diseases at Tan Tock Seng Hospital in Singapore. His research interests include dengue, melioidosis, and interventions to improve infection control practice in the hospital setting.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ad170ad8-4b0e-4cf3-ba5c-62444ab69acf", "title": "Nontuberculous Mycobacterial Disease Following Hot Tub Exposure", "text": "【0】Nontuberculous Mycobacterial Disease Following Hot Tub Exposure\nNontuberculous mycobacteria (NTM) are an important cause of disease in the United States, with the number of NTM isolates exceeding those of Mycobacterium tuberculosis ( 1 ). Pulmonary disease, the most commonly reported localized manifestation of NTM, is often associated with the M. avium complex (MAC) ( 2 ). Other NTM species, such as M. kansasii, M. fortuitum, M. xenopi, and M. abscessus, have also been associated with pulmonary disease ( 2 , 3 ). Although NTM-associated pulmonary disease has been described primarily among immunocompromised persons ( 4 , 5 ), it is being recognized with increasing frequency among those without predisposing conditions ( 2 , 6 , 7 ).\n\n【1】Unlike MTB, NTM are not known to be transmitted person to person. Most NTM have been isolated from water or soil ( 8 \\- 14 ). Species such as MAC are thermophilic ( 12 ), resistant to chemical germicides ( 14 ), and readily aerosolized ( 13 ). For several NTM species, environmental sources have been linked epidemiologically to cases of disease ( 15 \\- 20 ). In 1991, Burns investigated an outbreak of respiratory tract colonization in which epidemiologic and pulsed-field gel electrophoresis (PFGE) findings implicated a contaminated showerhead as the source of M. fortuitum ( 21 ). Subsequently, von Reyn used PFGE to link MAC infection in five AIDS patients to hot water sources in two hospitals ( 22 ).\n\n【2】Recently, Embil et al. ( 23 ) described five persons who became ill with pulmonary disease following exposure to hot tubs. MAC was isolated from all five patients and the two tubs. When MAC isolates were examined by multilocus enzyme electrophoresis (MEE), however, the hot tub and patient isolates had different MEE patterns. Kahana et al. ( 24 ) reported one patient diagnosed with MAC disease associated with a hot tub. In this case, the organisms isolated from the patient and the tub were identical by MEE.\n\n【3】### The Study\n\n【4】In October 1998, the Boulder County Health Department and the Tuberculosis Program, Colorado Department of Public Health and Environment, began an investigation into an apparent cluster of tuberculosis cases among a previously healthy family of five.\n\n【5】Case 1: This patient, a 46-year-old woman, was in excellent health until early June, when she noted the onset of shortness of breath and a dry cough. A chest radiograph at that time was consistent with early right lower lobe bronchial pneumonia. The patient was treated over the subsequent weeks with a series of antibiotics, including amoxacillin and azithromycin, without improvement. During early July, she had fever as high as 104°F approximately every 4 days, accompanied by night sweats and preceded by chills. A chest radiograph showed increased markings in the right lower lobe medially with associated peribronchial thickening and some faint air bronchograms consistent with an early bronchial pneumonia. She was begun on a course of tetracycline and prednisone (50 mg four times a day for 3 days, decreased to 20 mg four times a day for 11 days). During this time the patient traveled to Disneyland and while away felt much improved. Her symptoms recurred with the cessation of steroids and return home, however, and by the end of August, her shortness of breath, night sweats, and malaise had worsened, and she had an 18-pound weight loss.\n\n【6】Before she became ill, the patient had exercised regularly. In late August, her shortness of breath worsened so that she was unable to walk across a room, and she visited the hospital. A chest radiograph showed increased interstitial markings in both lungs. A computed tomographic (CT) scan of the chest showed a diffuse increase in pulmonary interstitial markings, with a ground-glass background. On September 10 the patient underwent fiberoptic bronchoscopy. A right lower lobe biopsy showed an occasional noncaseating granuloma consistent with sarcoidosis. Stains for acid-fast bacteria (AFB) and fungi were negative.\n\n【7】A chest radiograph obtained September 10 following bronchoscopy showed a fine reticular interstitial pattern involving the mid to lower lung. The patient was begun on a second course of prednisone and showed some improvement. A chest radiograph obtained on September 22, however, showed an interstitial lung process involving all areas, although both lungs appeared radiographically improved. Prednisone was discontinued on September 23 in anticipation of an open lung biopsy. Oxygen saturation (SaO <sub>2 </sub> ) measurements before surgery on 4 liters of 0 <sub>2 </sub> ranged from 88% to 92%.\n\n【8】On September 24, the patient had an open lung biopsy of the lingula and left lower lobe. The lingula showed moderate to severe granulomatous inflammation with AFB, numerous granulomas with focal caseation and necrosis, interstitial chronic inflammation, and mild and interstitial immature fibrosis with focal pattern. The left lower lobe of the lung showed moderate granulomatous inflammation, multiple granulomas with focal caseation and necrosis, and mild interstitial chronic inflammation and immature fibrosis. Aerobic tissue cultures showed rare gram-positive cocci. Stains and cultures for fungi and Legionella were negative, as were a shell vial assay and culture for cytomegalovirus. An intradermal test with 5 tuberculin units of purified protein derivative S (PPD) placed on September 24 was negative. Serologic HIV test results were also negative. A chest radiograph obtained September 25 showed bilateral bibasilar consolidation consistent with atelectasis or pneumonia with some infiltrate in the upper lobes.\n\n【9】On September 25, the patient was begun on oral isoniazid, 300 mg; rifampin, 600 mg; ethambutol, 400 mg; and pyrazinamide, 500 mg daily for suspected miliary tuberculosis. This regimen was discontinued 7 days later.\n\n【10】On subsequent evaluation at National Jewish Hospital in early October, the patient had pO <sub>2 </sub> 40, pCO <sub>2 </sub> 36, pH 7.43, and SaO <sub>2 </sub> 77% on room air. High-resolution CT scan showed fine central lobular nodularity without bronchiectasis. Pulmonary function tests were remarkable ( Table ).\n\n【11】Case 2: The 44-year-old husband of Patient 1 was well until mid-August when he had onset of productive cough, fever, and night sweats. Over the next month, he had an 8-pound weight loss. His past medical history is remarkable only for a history of heavy smoking (1/2 to 1 pack per day) until 7 years previously, when he stopped smoking entirely.\n\n【12】Cases 3, 4, and 5: The 14-, 12-, and 9-year-old sons of Patients 1 and 2 became ill in mid-September with influenza-like symptoms, including fever as high as 104°F, nausea, vomiting, and shortness of breath. The 12-year-old (Patient 4) was hospitalized for dehydration. His chest radiograph showed diffusely abnormal lungs with mildly increased nodular makings more prominent in the lower than upper lobe. Pulmonary function tests were performed for Patient 4 in early October ( Table ).\n\n【13】A chest radiograph for Patient 3 showed ground-glass alveolar infiltration with tiny opacities, suggesting a miliary pattern consistent with tuberculosis. Patient 5's chest radiograph showed mild increased streaking bilaterally.\n\n【14】All family members had negative skin tests with PPD at 48 and 72 hours. Although none described potential exposures to tuberculosis, the prospect of a family with tuberculosis prompted retrieval of specimens and pending cultures from the local hospital. Probe tests at the state public health laboratory on these specimens and sediments of centrifuged culture medium were all negative for M. tuberculosis but positive for MAC for two family members. Because of these laboratory findings and the unusual occurrence of five cases of suspected pulmonary tuberculosis among family members, the cases were presented to a physician at National Jewish Hospital, who noted the possibility of NTM-related disease secondary to hot tub exposure.\n\n【15】##### Environmental Investigation\n\n【16】Approximately a year earlier, the family had installed a hot tub in an enclosed sunroom next to the kitchen. The source of water for the tub was surface water from the Boulder municipal system, transported via tanker truck. Drinking and bathing water come from an alluvial aquifer well shared with several neighbors.\n\n【17】The hot tub water was changed only two or three times from January to October. The tub was equipped with an ozonator. On occasion a chlorine/bromine float or a cup of bleach would be added just before the tub was used. Disinfectant levels or pH were not checked.\n\n【18】Patient 1 used the hot tub rarely, most recently once in June and a second time in July. Because the tub water irritated her skin, she showered immediately after using the tub. However, when one of her sons was in the tub, she generally stood nearby. The 12-year-old, Patient 4, was the most frequent user. The three children often entered the tub after having been outside, without having showered first. Patients 1 and 4 had the greatest exposure to the hot tub aerosols. In retrospect, they described a clear relationship between hot tub exposure and worsening of symptoms, i.e., recurrence of night sweats, chills, and fever.\n\n【19】Following identification of MAC and M. fortuitum from clinical specimens ( Table ) and further consultation, Patients 1 and 4 were begun on a regimen of rifampin, ethambutol, amikacin, clarithromycin, ciprofloxacin, and prednisone. Patients 2, 3, and 5 were treated with clarithromycin and ciprofloxacin. After 6 months, pulmonary function tests for Patients 1 and 4 had improved ( Table ), signs and symptoms had resolved, and chest radiographs were normal for all family members.\n\n【20】The results of sputum evaluation on smear and culture for AFB are summarized ( Table ). Patient 4 was smear and culture positive for AFB, but negative on probe for MTB or MAC. The organism was subsequently identified as M. fortuitum.\n\n【21】Figure\n\n【22】Figure . Restriction fragment-length polymorphism analysis for isolates from patients and hot tub. Lane 1: Mycobacterium avium CDC #91-9282, serotype 4. Lane 2: M. avium CDC #91-9285, serotype 10. Lane 3: M. avium...\n\n【23】Patient and water isolates were initially identified as MAC and typed by MEE, with identical enzyme profiles ( 25 ). Restriction fragment-length polymorphism (RFLP) analysis with an insertion sequence specific for M. avium (IS 1245) ( 26 ) confirmed that all isolates were the identical strain of M. avium ( Figure ). Isolation of M. fortuitum from this hot tub has been described ( 27 ).\n\n【24】### Conclusions\n\n【25】NTM organisms isolated from the hot tub are likely responsible for this family's illness for the following reasons: exposure to the hot tub was temporally related to onset of symptoms; MAC was isolated from the lung biopsy and sputum of one patient and the sputum of two others, as well as from the hot tub; and MAC isolates from patient specimens and the hot tub were identical by RFLP. In addition, M. fortuitum was isolated from both the hot tub and a fourth hot tub-exposed person.\n\n【26】The source of the MAC and M. fortuitum is unclear. Our inability to isolate either organism from samples from the tanker truck used to supply water for the hot tub does not rule this out as a source. NTM have been isolated from municipal water supplies in the past ( 22 ). Alternatively, the users may have introduced the organisms, as the children often used the tub without showering first.\n\n【27】Proliferation of these organisms in a hot tub is not surprising, as both MAC and M. fortuitum are thermophilic ( 12 ). Moreover, at temperatures >84°F, chlorine loses much of its efficacy as a disinfectant ( 15 ).\n\n【28】Controversy exists as to whether persons with pulmonary disease secondary to NTM are experiencing a hypersensitivity reaction to the organisms or symptoms secondary to true infection ( 28 , 29 ). Murphy concludes that in the presence of dyspnea, nodular infiltrates seen on CT, response to steroids, and absence of predisposing factors such as chronic lung disease, a patient with MAC-related lung disease has hypersensitivity pneumonitis. Pathologic findings, including palisaded and multinucleated histiocytes and granulomatous inflammation, however, suggest infection ( 28 ). In a recent case presentation, symptoms and radiographic findings in a patient from whose lung tissue MAC was cultured are consistent with both the diagnoses of hypersensitivity pneumonitis and atypical mycobacterial infection, a conclusion substantiated by pathologic findings ( 29 ). In cases evaluated at National Jewish Medical and Research Center, most patients required treatment with both steroids and antimycobacterial medications ( 30 ). This experience suggests that NTM disease represents a spectrum of disease with components of both hypersensitivity pneumonitis and infection.\n\n【29】Our cases had characteristics of both hypersensitivity pneumonitis and true infection. The short interval between hot tub use and exacerbation of symptoms and the patchy ground-glass appearance of the lungs, with centrilobular nodules on CT, suggest hypersensitivity pneumonitis ( 31 ). The granulomas seen on pathologic examination and the response to treatment with antimycobacterial medications, however, suggest true infection. The temporary improvement in Patient 1's condition after she received prednisone may represent either appropriate treatment of hypersensitivity pneumonitis or a decrease in granulomatous inflammation in the bronchioles, secondary to infection ( 28 ).\n\n【30】Little data exist to explain the mechanism of disease caused by NTM in healthy persons. Exposure to sufficiently large and repeated inocula of the organism in droplets of readily respirable size appears to be sufficient to overwhelm normal host defenses.\n\n【31】Hot tubs should be maintained according to manufacturers' recommendations, which include both frequent water changes and adequate use of disinfectants. In addition, placing a hot tub in an enclosed environment should be strongly discouraged. Patients with atypical pneumonia should be questioned about similar illnesses among family member and others who have had similar exposures, including exposure to a hot tub. As hot tubs become increasingly popular (pers. comm., John J. Cergol, Jr.), hot tub-related illness associated with NTM may become an emerging infectious disease challenge.\n\n【32】Dr. Mangione is director of the Disease Control and Environmental Epidemiology Division of the Colorado Department of Public Health and Environment. She holds appointments in the Department of Biometrics and Preventive Medicine and the Division of Infectious Diseases, University of Colorado Health Sciences Center.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4b90f368-e7df-407e-ae3f-601d1ea1fccd", "title": "Social Determinants of Health–Related Needs During COVID-19 Among Low-Income Households With Children", "text": "【0】Social Determinants of Health–Related Needs During COVID-19 Among Low-Income Households With Children\n=====================================================================================================\n\n【1】ORIGINAL RESEARCH — Volume 17 — October 1, 2020\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Shreela V. Sharma, PhD, RDN, LD <sup>1 </sup> ; Ru-Jye Chuang, DrPH <sup>1 </sup> ; Melinda Rushing, MSW <sup>1 </sup> ; Brittni Naylor, MPH <sup>1 </sup> ; Nalini Ranjit, PhD <sup>2 </sup> ; Mike Pomeroy, MPH <sup>3 </sup> ; Christine Markham, PhD <sup>4 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Sharma SV, Chuang R, Rushing M, Naylor B, Ranjit N, Pomeroy M, et al. Social Determinants of Health–Related Needs During COVID-19 Among Low-Income Households With Children. Prev Chronic Dis 2020;17:200322. DOI: http://dx.doi.org/10.5888/pcd17.200322 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【9】**Summary**\n\n【10】**What is already known on this topic?**\n\n【11】Little is known about the social needs of low-income households with children during the coronavirus-2019 (COVID-19) pandemic.\n\n【12】**What is added by this report?**\n\n【13】We conducted a rapid-response survey on social needs, COVID-19–related concerns, and diet-related behaviors during the shelter-in-place phase of the US pandemic among low-income households with children enrolled in a nutrition program. We found high levels of financial instability; concern about unemployment, food insecurity, and COVID-19; and reduced frequency of eating out and grocery shopping.\n\n【14】**What are the implications for public health practice?**\n\n【15】Government and community agencies must have a role in developing short- and long-term strategies to address the needs of our most vulnerable and underserved populations.\n\n【16】Abstract\n--------\n\n【17】**Introduction**\n\n【18】Little is known about the social needs of low-income households with children during the coronavirus-2019 (COVID-19) pandemic. Our objective was to conduct a cross-sectional quantitative and qualitative descriptive analysis of a rapid-response survey among low-income households with children on social needs, COVID-19–related concerns, and diet-related behaviors.\n\n【19】**Methods**\n\n【20】We distributed an electronic survey in April 2020 to 16,435 families in 4 geographic areas, and 1,048 responded. The survey asked families enrolled in a coordinated school-based nutrition program about their social needs, COVID-19–related concerns, food insecurity, and diet-related behaviors during the pandemic. An open-ended question asked about their greatest concern. We calculated descriptive statistics stratified by location and race/ethnicity. We used thematic analysis and an inductive approach to examine the open-ended comments.\n\n【21】**Results**\n\n【22】More than 80% of survey respondents were familiar with COVID-19 and were concerned about infection. Overall, 76.3% reported concerns about financial stability, 42.5% about employment, 69.4% about food availability, 31.0% about housing stability, and 35.9% about health care access. Overall, 93.5% of respondents reported being food insecure, a 22-percentage-point increase since fall 2019. Also, 41.4% reported a decrease in fruit and vegetable intake because of COVID-19. Frequency of grocery shopping decreased and food pantry usage increased. Qualitative assessment identified 4 main themes: 1) fear of contracting COVID-19, 2) disruption of employment status, 3) financial hardship, and 4) exacerbated food insecurity.\n\n【23】**Conclusion**\n\n【24】Our study highlights the compounding effect of the COVID-19 pandemic on households with children across the spectrum of social needs.\n\n【25】Top\n\n【26】Introduction\n------------\n\n【27】Coronavirus disease 2019 (COVID-19), the disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is a worldwide pandemic (1). As of September 2020, the United States is the epicenter of the pandemic, with more than 6.3 million confirmed cases and more than 189,000 fatalities (2). Starting in early and mid-March, many US states began implementing social distancing measures and lockdowns, which prompted retail outlets, restaurants, schools, universities, businesses, and other entities to close, and implemented a work-from-home policy where possible. As a result of the pandemic, unemployment rose to an all-time high of 14.7% in April 2020, and more than 10% of Americans filed for unemployment from April through July 2020 (3). Furthermore, thousands of people diagnosed and hospitalized with COVID-19 are experiencing employment challenges in conjunction with large medical bills (4–6). For most Americans, access to health care is directly related to employment, so that loss of employment means loss of health care. A secondary effect of school closures is the lack of access to national school lunch and/or breakfast programs (free or reduced-price meals) for children, thus increasing the risk of food insecurity among low-income families. By April 2020, approximately 38% of American households were experiencing food insecurity (7). With the continued threat of COVID-19, unemployment and financial instability may rise, pushing families into making trade-offs between basic needs such as food and shelter (ie, paying rent this week instead of buying food) (8). Basic human needs, also called the social determinants of health, include employment, food security, housing security, access to health care, and transportation; the lack of these is linked to poor health outcomes (9).\n\n【28】Brighter Bites is a nonprofit, evidence-based school health program that distributes fresh produce weekly to low-income families and provides nutrition education in school and for parents (10,11). Only schools that have more than 75% of children enrolled in a free or reduced-price meals program are eligible to participate in Brighter Bites. The program, which uses a school co-op concept, is disseminated in Southwest Florida and 5 US cities: Houston, Austin, Dallas, New York City, and Washington, DC. More than 100 schools participated during the 2019–2020 school year. Brighter Bites has improved dietary behaviors and the home nutrition environment among participating families (10). However, as a result of COVID-19–related school closures, program implementation has paused. To better understand the ongoing needs of families and provide critical services during the pandemic, Brighter Bites conducted a rapid-response survey in April 2020 in 4 locations (Houston, Dallas, Washington, DC, and Southwest Florida) among participating families. The survey asked about financial stability, employment status, food security, housing security, transportation needs, access to child care, and access to health care. At the time the survey was conducted, all 4 locations were under shelter-in-place orders, which included social distancing, school closures, and suspension of all nonessential services. The objective of our study was to quantitatively and qualitatively describe the data obtained from this rapid-response survey.\n\n【29】Top\n\n【30】Methods\n-------\n\n【31】Brighter Bites distributed the survey electronically in April 2020 in English and Spanish via Formsite (Vroman Systems, Inc) to 16,436 Brighter Bites families (56.9% in Houston, 29.2% in Dallas, 7.8% in the District of Columbia, and 6.1% in Southwest Florida) who were enrolled in the program during the 2019–2020 school year and provided their telephone number in fall 2019. Program staff members distributed survey links through text messages. The overall survey response rate was 6.4% (1,048 of 16,436). Response rates by location were 7.7%, Houston; 3.8%, Dallas; 6.4%, Washington, DC; and 6.9%, Southwest Florida. Survey completion was voluntary. We obtained informed consent from all respondents, and one parent or adult per family completed the survey. Brighter Bites collected all data and shared de-identified data with the University of Texas Health Science Center for analysis as part of a data-sharing agreement. All procedures were approved by the University of Texas Health Science Center’s Committee for Protection of Human Subjects.\n\n【32】### Data collection\n\n【33】The 30-item self-report survey took approximately 10 minutes to complete.\n\n【34】**Sociodemographic characteristics.** The survey collected data on respondent’s sex, respondent’s relationship to the child, respondent’s and child’s race/ethnicity, respondent’s employment status, respondent’s education level, and enrollment in government assistance programs. Program options were the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC), Supplemental Nutrition Assistance Program (SNAP), Double Dollars, Medicaid/Texas Health Steps, Medicare, national school lunch and/or breakfast programs (free or reduced-price meals), and Children’s Health Insurance Program (CHIP).\n\n【35】**COVID-19–related concerns.** The survey asked respondents how much they had seen or heard about COVID-19. Four response options ranged from “nothing at all” to “a great deal.” The survey also asked about respondents’ perceived concern for themselves and their children in contracting COVID-19. Response options were on a 4-point Likert-type scale ranging from strongly disagree to strongly agree. These questions were adapted from previously administered surveys (12).\n\n【36】**Social determinants of health.** The survey asked respondents about their concerns during the pandemic about financial stability, employment status, availability of food, affordability of food, availability or affordability of housing, access to reliable transportation, access to childcare, and access to a clinic or physician. The survey provided a checklist for these 8 items, and respondents could check all that applied. The survey also invited respondents to write in any other concerns by using this statement: “Please share your greatest concern at this time, or any other thoughts you would like to share with us.”\n\n【37】**Food insecurity.** The parent or another adult in the family used the 2-item Hunger Vital Sign screening questionnaire developed and validated by Hager et al (13) to report household food security status during the COVID-19 pandemic. If the respondent indicated “often true” or “sometimes true” to either of the 2 questions, we considered the household food insecure. If a respondent answered “never true” to both questions, we considered the household food secure.\n\n【38】**Food shopping frequency and behavior.** The survey used items adapted from the National Cancer Institute’s 2007 Food Attitudes and Behavior Survey (14). Respondents reported the frequency (times per month) and type of store at which their household shopped for fruits and vegetables. Response options were a large chain grocery store or supermarket, a small local store or corner store, a farmers market/food co-op/farm stand, and a food bank/food pantry or other food distributions. Additionally, the survey asked respondents about their store-shopping practices: whether they physically shopped inside the store, shopped online with curbside pick-up, or shopped online with goods delivered to home. Respondents could check all that applied.\n\n【39】**Dietary habits.** The survey asked respondents about their frequency of eating out at restaurants in the previous 7 days and whether their frequency of eating out had changed because of the pandemic (15). The survey also asked if, because of COVID-19, their consumption of fruits and vegetables had increased, decreased, or stayed the same.\n\n【40】### Statistical analysis\n\n【41】**Quantitative data analysis.** We performed all analyses on de-identified data using Stata version 15.0 (StataCorp LLC). We computed means, standard deviations, and frequency distributions overall and stratified by race/ethnicity. We used the χ <sup>2 </sup> test or Fisher exact test and 1-way analysis of variance to compute differences in responses by city and race/ethnicity. For 3 variables (food insecurity, frequency of eating out, and frequency of shopping for produce) we compared responses on similar items in data collected from 3,880 families in the same 4 locations in fall 2019, before the start of the Brighter Bites program; we used the χ <sup>2 </sup> test to assess changes. Significance was set at _P_ <.05 for all tests. The fall 2019 survey and April 2020 survey used similar questions for the 3 variables, except for the question on type of store. Both surveys provided at least 1 example of each type of store, but the baseline survey did not list “other food distributions” in the same category as food bank/food pantry.\n\n【42】**Qualitative data.** We collated and analyzed responses to the open-ended question, “Please share your greatest concern at this time, or any other thoughts you would like to share with us.” We used thematic analysis to analyze the survey data by using an inductive approach in which we derived codes and themes on the basis of the content from the survey data (16,17). Two trained coders independently coded the comments by using an open coding method in Microsoft Word to establish initial coding themes and subthemes. Next, the 2 coders collectively assessed the themes and subthemes for agreement and created a codebook with definitions for each theme. Finally, the coders used this codebook to reconfirm the coding of the themes and subthemes in qualitative comments.\n\n【43】Top\n\n【44】Results\n-------\n\n【45】Overall, the demographic characteristics of respondents did not vary substantially by site ( Table 1 ). Most (68.2%) respondents resided in Houston; 97.0% were female; 85.9% were Hispanic, 7.1% were African American, and 50.5% were mostly or only Spanish speaking; the mean age was 36.7 years. On average, 25.1% and 28.2% of families reported receiving WIC or SNAP, respectively. Also, 46.7% of families reported receiving Medicaid, and 74.4% of the children participated in free or reduced-price meals programs. A greater proportion in Houston and Washington, DC, than in the other 2 locations received Medicaid or CHIP ( _P_ < .001), whereas a smaller proportion of children in Dallas than in the other 3 locations participated in free or reduced-price meals programs ( _P_ \\= .04). These differences, however, were small.\n\n【46】Because demographic characteristics did not vary by site, our examination of COVID-19–related knowledge, risks, attitudes, and behaviors across racial/ethnic groups did not adjust for site differences. Most (81.8%) respondents reported knowing a great deal about COVID-19 and were concerned about being infected themselves (87.3%) or their children being infected (87.0%) ( Table 2 ). A smaller proportion of African American respondents than respondents in other racial/ethnic groups were concerned about being infected with COVID-19 themselves (38.7%) and about their child being infected (39.3%) ( _P_ < .001).\n\n【47】Overall, 76.3% of respondents reported concerns about financial stability, 42.5% were concerned about their employment, 69.4% about availability of food, 49.5% about affordability of food, 31.0% about housing stability, and 35.9% about access to a clinic or physician. A small proportion of respondents expressed concern about reliable transportation (6.4%) and childcare (8.2%). When we stratified results by race/ethnicity, a significantly greater proportion of African American (64.9%) and Non-Hispanic White (63.2%) respondents, compared with Hispanic respondents or in the “other” racial/ethnic group, was concerned about affordability of food ( _P_ \\= .009). Moreover, a smaller proportion of Hispanic respondents (6.3%) than African American (23.0%), Non-Hispanic White (15.8%), or “other” (17.1%) racial ethnic groups was concerned with access to childcare ( _P_ < .001) ( Table 2 ).\n\n【48】Overall, diagnosis of COVID-19 and prevalence of pre-existing conditions, such as diabetes, heart disease, and being immune compromised, were less than 5% among respondents at the time of survey completion. When asked about their general health status, 25.0% of respondents reported their health status to be fair or poor, with no significant differences by race/ethnicity.\n\n【49】Overall, 93.5% of respondents reported being food insecure in April 2020. In fall 2019, before Brighter Bites, 71.5% of respondents reported being food insecure ( _P_ < .001) ( Figure ). A significantly greater proportion of Hispanic respondents (94.7%) than respondents in other racial/ethnic groups reported food insecurity ( _P_ < .001) ( Table 2 ). Overall, 61.6% of respondents reported never eating food from a restaurant during the previous 7 days (including take-out), with a smaller proportion of non-Hispanic White families (34.2%) than other racial/ethnic groups never eating out ( _P_ < .001). In fall 2019, 12.7% of families reported never eating out ( _P_ < .001). In addition, 83.5% of respondents reported decreasing the frequency of eating out because of COVID-19, with a greater proportion of Hispanic families (84.9%) than families in other racial/ethnic groups reporting decreased frequency ( _P_ \\= .003).  \n\n【50】**Figure.** COVID-19-related changes in food insecurity, eating out, and food shopping behavior among low-income households with children (N = 1,048), Brighter Bites. All changes were significant at _P_ < .001, except for shopping at a farmers market/food co-op/farm stand ≥1 time per week ( _P_ \\= .31). \\[A tabular version of this figure is also available.\\]\n\n【51】Most (97.1%) Brighter Bites families reported shopping at a large grocery store during the pandemic but at varying frequencies. Almost half (49.1%) reported in April 2020 shopping at least once per week, whereas 62.4% reported shopping at least once per week in fall 2019 ( _P_ < .001) ( Figure ). Moreover, in fall 2019 only 3.2% reported shopping at a food bank or food pantry or at other distributions at least once per week, where 13.4% reported shopping at these food outlets at least once per week in April 2020 ( _P_ < .001). When asked about the effect of COVID-19 on fruit and vegetable consumption, 41.4% of families reported a decrease in intake of fruits and vegetables because of COVID-19; we found no significant differences by race/ethnicity in this response ( Table 2 ).\n\n【52】### Qualitative analysis\n\n【53】Four themes emerged from the analysis of write-in responses to the question about immediate concerns during the COVID-19 pandemic ( Table 3 ): 1) fear of contracting COVID-19, 2) disruption of employment status, 3) financial hardship, and 4) exacerbated food insecurity. Collectively, all themes stemmed from fear of contracting COVID-19. As indicated in subthemes and comments, many respondents were concerned about contracting COVID-19 themselves or concerned about a family member contracting COVID-19. The concerns were due to various reasons, such as lack of personal protective equipment and working in frontline jobs. Respondents also expressed concern about an inability to pay bills, rent, and for other basic needs because of closure-related unemployment. Respondents were also uncertain about how they were going to access food as social distancing and the pandemic continued. In addition, respondents were concerned about limited produce at various food stores, spiked produce prices, and the finances required to purchase food for their families.\n\n【54】Top\n\n【55】Discussion\n----------\n\n【56】The COVID-19 pandemic is occurring in the context of a global economic crisis, both of which highlight health and social challenges for the most vulnerable people in our communities. To our knowledge, our study is the first to use a mixed-methods qualitative and quantitative assessment to understand the social needs of low-income families with children during the pandemic. Moreover, our study provides insight into these needs during the pandemic’s acute phase, when all 4 cities in the study sample were under shelter-in-place orders.\n\n【57】Understanding the social needs of our most vulnerable families with children is critical because of the health disparities associated with COVID-19 prevention and treatment (18). Low-income racial/ethnic minority populations, predominantly Hispanic and African American populations, are struggling with an increased risk of COVID-19 infection and COVID-19–related complications and mortality (19–22). These increases in risk are likely due to health inequities, which result in a disproportionately greater prevalence of obesity, diabetes, respiratory disorders and other predisposing conditions among Hispanic and African American populations (23). Inequities exist in social and structural factors, such as access to food, employment, transportation, housing, poverty, education, and health literacy (24). An overwhelming majority of our study families were food insecure, were experiencing financial instability, and had concerns about their employment status. Furthermore, about one-third of respondents expressed concern about access to health care, which would increase their risk of illness of death, not just from COVID-19, but from other health conditions as well.\n\n【58】We obtained further insights into the needs and concerns of survey respondents in the qualitative comments, which identified 4 themes: fear of contracting COVID-19, unemployment, financial hardship, and food insecurity. Interestingly, a smaller proportion of African American respondents than respondents in other racial/ethnic groups were concerned with being infected with COVID-19. This difference may be due to a disparity in health literacy and should be explored in future studies, given the high proportion of COVID-19–related complications and death in the African American population. Our qualitative assessment also demonstrated that although most respondents reported following safety guidelines, some expressed fear of becoming infected in public spaces. One subtheme was lack of personal protective equipment among family members working in a high-risk environment. In light of these concerns, Brighter Bites has partnered with other nonprofit organizations to provide masks for families at many of their food distribution locations. Similarly, concerns about disruption of employment and financial hardship identified such acute problems as inability to pay rent or bills. These results underscore how COVID-19 and the related economic crisis have not only caused physical harm but have further destabilized people who were already struggling. The number of unemployed people in the United States increased by 11.5 million from February to September 2020 (25), and this number may increase. Brighter Bites developed a set of comprehensive resources to disseminate to their families via text, emailed newsletters, and the Brighter Bites website (www.brighterbites.org). These resources include information on where to get tested for COVID-19, how to register for government assistance programs, COVID-19 prevention practices, and maintaining healthy eating, physical activity, and mental health.\n\n【59】Our study demonstrated a significant increase in food insecurity during the pandemic and a decrease in intake of healthy food such as fruits and vegetables. In addition, 41% of respondents said that their intake of fruits and vegetables decreased because of COVID-19, and 83% said their frequency of eating food from restaurants decreased because COVID-19. Qualitative comments provided additional insight, with many respondents expressing a concern about a general scarcity of food. This general scarcity was attributed to a lack of availability and affordability and less frequent grocery shopping because of presumed store closures or fear of contracting COVID-19. Seligman et al posited a conceptual framework for the relationships among financial stress, food insecurity, and health (26). They suggested that the combination of stress (resulting from unemployment and financial hardship) and poor nutrition can challenge disease prevention and management because these stressors strain the household budget, leaving little money for healthy foods and resulting in disordered eating (ie, consumption of unhealthy processed, nutrient-deficient foods), and increased medical care resulting in spending trade-offs (26). Furthermore, school and childcare closures, which halt school meals for children, have further pushed families into stress and food insecurity. Strategies to support meals for children, such as expanding and supporting enrollment in government assistance programs (eg, Pandemic-EBT), distribution of food for pick-up through schools, and other policy strategies should be considered (27,28).\n\n【60】The strengths of our study include a sample drawn from 4 US locations and a survey administered when all 4 areas were experiencing similar COVID-19 mitigation strategies. In addition, the study fills a gap in knowledge on the social needs of low-income households with children during COVID-19. Moreover, we conducted the qualitative analysis to help contextualize the quantitative findings, and these analyses indicated COVID-19–related shifts in food-related practices and behaviors among respondents.\n\n【61】Our study has several limitations. Our sample is not representative of the general population in the 4 locations surveyed or Brighter Bites members but is a biased sample of families who likely needed help during this time. Furthermore, the sample size differed across the locations on the basis of the proportion of families enrolled in the Brighter Bites program. Also, the changes we observed in food insecurity, eating out, and food shopping practices could be attributed to other policy, systems, or environmental factors unrelated to COVID-19. Finally, our cross-sectional descriptive study does not allow us to draw conclusions on cause and effect; our study is a snapshot at an early moment in the pandemic and serves as a baseline for examining patterns of change and causal associations in future studies.\n\n【62】Our results highlight the compounding effect of the COVID-19 pandemic across the spectrum of basic social needs among low-income households with children. Our study is a call to action for government and community agencies to deploy short- and long-term strategies to address the needs of our most vulnerable populations.\n\n【63】Top\n\n【64】Acknowledgments\n---------------\n\n【65】Funding for the study was provided by Brighter Bites through Feeding Texas and the Texas Health and Human Services Commission. The funder/sponsor did not participate in the work. We acknowledge the Michael & Susan Dell Center for Healthy Living for their support of the project. The authors also acknowledge the families who participated in this study. Finally, we thank Brighter Bites staff members, participating schools, and parents for their support of the project. Dr Sharma is on the board of directors (an unpaid, advisory board position) of Brighter Bites, the goal of which is to improve access to fresh fruits and vegetables and nutrition education among underserved communities. The other authors have no conflicts of interest relevant to this article to disclose. No copyrighted materials were used in this article.\n\n【66】Top\n\n【67】Author Information\n------------------\n\n【68】Corresponding Author: Shreela Sharma, PhD, RDN, LD, Professor, Department of Epidemiology, Human Genetics and Environmental Sciences, The University of Texas Health Science Center at Houston School of Public Health, 1200 Pressler St, Houston, TX 77030. Telephone: 713-500-9344. Email: Shreela.V.Sharma@uth.tmc.edu .\n\n【69】Author Affiliations: <sup>1 </sup> Department of Epidemiology, Human Genetics and Environmental Sciences, Michael & Susan Dell Center for Healthy Living, The University of Texas Health Science Center at Houston School of Public Health, Houston, Texas. <sup>2 </sup> Department of Health Promotion and Behavioral Sciences, Michael & Susan Dell Center for Healthy Living, The University of Texas Health Science Center, Austin Regional Campus, Austin, Texas. <sup>3 </sup> Brighter Bites, Houston, Texas. <sup>4 </sup> Department of Health Promotion and Behavioral Sciences. The University of Texas Health Science Center at Houston School of Public Health, Houston, Texas.\n\n【70】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ee5df14f-590d-40b8-8d3e-40839a8f2008", "title": "Three Rickettsioses, Darnley Island, Australia", "text": "【0】Three Rickettsioses, Darnley Island, Australia\nThe Torres Strait islands are scattered between Cape York in northeastern Australia and Papua New Guinea. Darnley (Erub) Island is the largest island (5.7 km <sup>2 </sup> ) in the eastern Torres Strait and has a mostly indigenous population of 360.\n\n【1】Rickettsioses in northeastern Australia include Queensland tick typhus ( _Rickettsia australis_ ) ( _1_ , _2_ ), murine typhus ( _R. typhi_ ), scrub typhus ( _Orientia tsutsugamushi_ ) ( _3_ ), and Flinders Island spotted fever ( _R. honei_ strain “marmionii”) ( _4_ ). The latter 2 diseases are endemic to the Torres Strait islands ( _4_ , _5_ ). Because all 4 diseases have similar clinical manifestations, which may include maculopapular rash, fever, headache, rigor, myalgia, and arthralgia ( _1_ _–_ _4_ ), laboratory investigation is needed to identify the rickettsial etiologic agent. We describe the northernmost case of Queensland tick typhus and 2 cases of scrub typhus, along with their molecular identifications, from patients examined at the Darnley Island Health Clinic.\n\n【2】### The Cases\n\n【3】In March 2003, a 23-year-old man (patient 1) sought treatment for fever (39.3°C), headache, and an eschar on his right thigh. He had no rash. No diagnosis was made, but he was given penicillin V. Seven days later he returned with worsening symptoms of fever (39.9°C), headache, cough, arthralgia, and lethargy. A provisional diagnosis of scrub typhus was made, and he was given doxycycline. By the next day he was afebrile, and his condition was much improved. The diagnosis was confirmed by a positive serologic result for scrub typhus rickettsiae (antibody titer 512) and the presence of _O. tsutsugamushi_ in blood detected on day 7 by PCR and culture ( Table ).\n\n【4】In May 2004, an 11-year-old boy (patient 2) was examined because of a 3-day history of fever (39.0°C), headache, nausea, abdominal pain, and a boil surrounded by cellulitis behind his left ear. He had no rash. He was given flucloxacillin for the boil. On day 10 he was still ill and febrile. His blood was tested for malaria, dengue, and scrub typhus, but he was given no antimicrobial therapy. He was not seen again until day 18, when he was prescribed doxycycline after PCR detected _O. tsutsugamushi_ (and it was later isolated) in his day-10 blood sample. Follow-up serologic testing of samples collected on days 10 and 33 displayed seroconversion to antibodies against scrub typhus (titers <128 to >1,024; Table).\n\n【5】In April 2003, a 29-year-old man (patient 3) sought treatment for fever (38.4°C), cough, nausea, lethargy, and cellulitis of both feet. His left thigh had 2 boils, and his inguinal nodes were palpable. Three days later, he was evacuated by air and admitted to the Thursday Island hospital, at which time he was still febrile (39.2°C) but had no cellulitis. He also had a macular rash, which he reported as originally having been pustular; the thigh lesions were recognized as eschars. A tentative diagnosis of scrub typhus was made, and the patient was given doxycycline. Within 48 hours he was afebrile and discharged. Serologic testing for rickettsiae was positive for the spotted fever group (titer 256) on day 3, but the titers remained unchanged 4 months later. A spotted fever group rickettsial organism was grown from the day-3 blood sample; however, PCR was negative for rickettsiae ( Table ).\n\n【6】Rickettsial serologic testing was performed on patients’ paired serum specimens by using an indirect immunofluorescence assay ( _6_ ). Titers \\> 128 were deemed positive. Patient 1 had an increase in titer to _O. tsutsugamushi;_ patient 2 exhibited seroconversion to _O. tsutsugamushi_ antibodies; and patient 3 had stationary positive titers to spotted fever group rickettsiae ( Table ).\n\n【7】Rickettsial isolation was performed according to previously described methods ( _7_ ). A spotted fever group rickettsial organism was isolated from patient 3, and an _Orientia_ organism was isolated from patients 1 and 2 ( Table ). Only 1 _Orientia_ organism could be adapted to continuous culture (patient 2).\n\n【8】Figure\n\n【9】Figure . Phylogenetic tree obtained by a neighbor-joining analysis of the 56-kDa gene of _Orientia tsutsugamushi._ Bootstrap values from 100 analyses are shown at the node of each branch.\n\n【10】We extracted DNA from enriched buffy coat and rickettsial cultures by using the QIAamp Blood Mini Kit (QIAGEN, Hilden, Germany) and following the manufacturer’s protocols. Scrub typhus was diagnosed by 56-kDa gene PCR, which used the primers A (5′-TACATTAGCTGCAGGTATGACA-3′) and B (5′-CCAGCATAATTCTTTAACCAAG-3′) (Invitrogen, Mount Waverley, Victoria, Australia) as previously described, without the nested procedure and with a 51°C annealing temperature ( _8_ ). Buffy coats and cultures from patients 1 and 2 were PCR positive for _O. tsutsugamushi_ ( Table ). The 320-bp product (patient 2) was sequenced (Newcastle DNA, University of Newcastle, Australia; GenBank accession no. AY860955) and shared 89.8% homology with the Taiwanese strains TW381 and TW521 (GenBank accession nos. AY222635 and AY222630, respectively). A phylogenetic tree of the 56-kDa antigen gene was constructed by using the SEQBOOT and CONSENSE programs of the PHYLIP software package ( Figure ).\n\n【11】Spotted fever group rickettsemia was identified by 17-kDa antigen gene PCR that used the primers MTO-1 (5′-GCTCTTGCAACTCTATGTT-3′) and MTO-2 (5′-CATTGTTCGTCAGGTTGGCG-3′) (Invitrogen) as previously described, with an annealing temperature of 51°C and 45 cycles ( _9_ ). The 17-kDa buffy coat PCR result was negative for patient 3, but the culture gave a 413-bp sequence that was 100% homologous with _R. australis_ (GenBank accession no. M74042; Table). The patient’s buffy coat DNA extract was not tested for PCR inhibitors, and no attempt was made to use the _R. australis_ isolate in a heterologous serologic reaction because the strain could not be established in continuous culture.\n\n【12】### Conclusions\n\n【13】Isolation of _R. australis_ from a patient on Darnley Island redefines the northern limit of distribution of Queensland tick typhus in Australia. Previously, Queensland tick typhus had been thought to extend from Wilson’s Promontory (the tip of southeastern Australia) ( _10_ ) to the Atherton Tableland (north Queensland) ( _2_ ). This more northern finding of Queensland tick typhus was not unexpected because distribution of the vector of Queensland tick typhus in northeastern Australia, _Ixodes holocyclus_ , is likely to include the Torres Strait islands and Papua New Guinea ( _11_ ).\n\n【14】Rickettsial diseases that differ clinically from scrub typhus have been reported in Papua New Guinea ( _12_ ). A recent serologic survey found 7 (3.7%) of 191 Papua New Guineans were seropositive to scrub typhus or spotted fever group rickettsiae ( _13_ ). Another survey found that at least 19 (17%) of 113 Papua New Guineans had an antibody titer >256 against spotted fever group rickettsiae (A.G. Faa, unpub. data). A spotted fever group rickettsial disease such as Queensland tick typhus, Flinders Island spotted fever, or another undescribed rickettsiosis could explain these findings.\n\n【15】Low antibody titers (128) to the spotted fever group in the scrub typhus patients ( Table ) are consistent with previous exposure to spotted fever group or typhus group rickettsiae. Because 20% of each enriched buffy coat specimen was examined by rickettsial PCR and 80% by culture, results were skewed in favor of isolation rather than DNA detection.\n\n【16】Since 1935, numerous cases of scrub typhus have been reported in Australia and Papua New Guinea ( _5_ , _12_ ). Scrub typhus is known to be endemic to Darnley Island; however, strains have not been typed ( _5_ ). Strains from northeastern Australia were serologically determined to be Karp or Karp-related ( _14_ ). However, this new strain is 10.2% divergent from any other described strain, including Karp ( Figure ). Hence, we designated it as the Darnley strain, after the island from which it was isolated. The phylogenetic relationship of the Darnley strain to other Australian strains, including Litchfield, needs to be elucidated. The presence of 3 rickettsial diseases on this small island demonstrates the complexity of rickettsial epidemiology in Australia.\n\n【17】Dr Unsworth is a postdoctoral research associate at Texas A&M University, College Station, Texas, USA. His interests include the epidemiology of Australian rickettsiae and Q fever pathogenesis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9bf23610-a4cb-4424-8b7f-ab5aff004bff", "title": "Bagaza Virus in Partridges and Pheasants, Spain, 2010", "text": "【0】Bagaza Virus in Partridges and Pheasants, Spain, 2010\nAn essential feature of certain emerging pathogens is their ability to expand their geographic ranges. Several arboviral diseases, particularly those caused by flaviviruses, have been found in new areas beyond their usual ranges. The best example of this phenomenon was introduction of West Nile virus into the Americas in 1999 ( _1_ ). Other recent expansions of flaviviruses were the introductions of Japanese encephalitis virus into Australia in 1995–1998 ( _2_ ) and Usutu virus into Europe in 2001 ( _3_ ).\n\n【1】We report an outbreak of disease in wild birds (partridges and pheasants) in Spain that was caused by a flavivirus, Bagaza virus (BAGV). This virus was first isolated in Bagaza, Central African Republic, in 1966, from a pool of mixed-species female _Culex_ spp. mosquitoes ( _4_ ). It has subsequently been found in mosquitoes in other countries in western Africa ( _5_ _,_ _6_ ) and in India, where serologic evidence suggests that this virus may infect humans ( _7_ ), although its pathogenicity in humans is uncertain. BAGV has been shown to be synonymous with Israel turkey meningoencephalitis virus, a pathogen affecting poultry (turkeys) and reported only in Israel and South Africa ( _8_ ).\n\n【2】### The Study\n\n【3】In September, 2010, an unusually high number of red-legged partridges ( _Alectoris rufa_ ) died on several hunting properties in southwestern Cádiz, the southernmost province in Andalusia, Spain. Clinical signs included weakness, prostration, lack of motor coordination, weight loss, and white diarrhea. Some common pheasants ( _Phasianus colchicus_ ) were also affected. These findings coincided in time with the first cases of West Nile virus (WNV) infection in horses detected in Spain ( _9_ ).\n\n【4】Because red-legged partridges have recently been shown to be susceptible to WNV disease ( _10_ ), it immediately raised suspicions that WNV could be responsible for these deaths. Therefore, samples were obtained from dead partridges (n = 11) and pheasants (n = 2) and sent to the Spanish Central Veterinary Laboratory in Algete for diagnostic analysis. The carcass of 1 of the affected partridges was also subjected to necroscopic analysis of different tissues, including heart, intestine, lung, liver, kidney, brain, and feathers ( Table ).\n\n【5】Analysis by real-time reverse transcription PCR (RT-PCR) specific for lineage 1 and lineage 2 WNV ( _11_ ) showed negative results for all samples tested. However, a heminested RT-PCR specific for a broad range of flaviviruses ( _12_ ) showed positive results for all samples ( Table ), indicating that a non-WNV flavivirus was present in samples from different tissues of diseased birds.\n\n【6】cDNA obtained after real-time RT-PCR (252-bp fragment) or heminested RT-PCR (214-bp fragment) amplification of samples was subjected to nucleotide sequencing. Resulting nucleotide sequences were compared with those in GenBank by using BLAST analysis ( www.ncbi.nlm.nih.gov ). Our isolates had >90% homology with 2 BAGV strains in GenBank (strain DakAr B209 from Africa, GenBank accession no. AY632545, and strain 96363 from India, GenBank accession no. EU684972).\n\n【7】Virus isolation was performed by infection of embyonated chicken eggs with tissue homogenates from an infected partridge and confirmed by pan-flaviviral RT-PCR and nucleotide sequencing ( Table ). Virus was detected more frequently in allantoic fluid of infected eggs than in other egg tissues ( Table ). Further propagation was accomplished in BSR cells, a clone of baby hamster kidney-21 cells.\n\n【8】Genomic characterization of virus was performed by bidirectional sequencing of real-time RT-PCR fragments amplified from heart and brain samples from 1 of the affected partridges. We used 27 primer sets designed for this study on the basis of sequences for BAGV in GenBank. Full-length genome sequences were obtained by assembling overlapping nucleotide sequences and using the SeqScape program (Applied Biosystems, Foster City, CA, USA). Two full-length genome sequences obtained from heart (BAGV Spain H/2010, GenBank accession no. HQ644143) and brain (BAGV Spain B/2010, accession no. HQ644144), were identical.\n\n【9】Figure 1\n\n【10】Figure 1 . Phylogenetic relationships between a full-length genomic sequence for Bagaza virus identified in Cádiz, Spain, 2010 (solid circle) and 32 full-length flavivirus sequences, including 2 Bagaza virus isolates from GenBank. The phylogenetic...\n\n【11】Figure 2\n\n【12】Figure 2 . Phylogenetic relationships between partial envelope protein–coding gene sequence for Bagaza virus identified in Cádiz, Spain, 2010 (solid circle) and 38 equivalent flavivirus nucleotide sequences, including those for 3 Bagaza virus isolates,...\n\n【13】To assess phylogenetic relationships between these new BAGVs and other flaviviruses, the complete BAGV sequence obtained in this study (accession no. HQ644143) or a partial envelope (E) protein–coding gene subregion were aligned with other complete flaviviral genomes in GenBank by using the ClustalW algorithm in MEGA5 ( _13_ ). Phylogenetic trees were constructed by using the complete BAGV genomic sequence or the partial E region (the E region is a well-known variable region in flaviviruses, and additional relevant nucleotide sequences are available in GenBank). Maximum-likelihood, neighbor-joining, or maximum-parsimony algorithms were used, and bootstrap tests with 500 replicates were performed to support each tree grouping. All generated trees showed similar topology and clustered the genomic sequence obtained in this study within the BAGV branch. Maximum likelihood trees are shown in Figures 1 and 2 .\n\n【14】Sequence of BAGV from Spain was closely related to the 2 unique full-length BAGV sequences available in GenBank and showed greater similarity with the strain from Africa (94.1% nt identity) than with the strain from India (92.8% nt identity). Identity between the isolates from Africa and India was 95.0%, indicating that they are more related to each other than to BAGV from Spain. A total of 742 nt and 637 nt differences (70 nt and 47 aa differences) were observed between BAGV from Spain and the isolates from India and Africa, respectively. Genetic relatedness between all 3 viruses was high (>92%), which indicates that they belong to the same _Flavivirus_ species ( _Bagaza virus_ ). The tree based on the E region grouped Israel turkey meningoencephalitis virus and BAGV within the same cluster and showed that both viruses are closely related to Ntaya virus ( Figure 2 ).\n\n【15】### Conclusions\n\n【16】BAGV has been detected and isolated in Cádiz, Spain. It appeared to seriously affect partridges and, to a lesser extent, pheasants, and caused an unusually high number of deaths in these birds. No signs of infection and no deaths were observed for other bird species. However, whether other bird species are susceptible to disease caused by BAGV should be determined because this virus is similar to Israel turkey meningoencephalitis virus, a relevant pathogen for turkeys. Also, other vertebrates could be at risk for infection with this virus. Thus, experimental studies on the pathogenicity of this virus in specific vertebrates should be conducted.\n\n【17】Transcontinental spread of flaviviruses has been often associated with bird migrations ( _14_ _,_ _15_ ). Thus, infected birds migrating between Africa and Europe could have introduced BAGV into Spain. However, there is no evidence of transmission of this virus by migratory birds, and alternative explanations (poultry industry or trading of exotic birds for commercial or hunting purposes) should not be overlooked.\n\n【18】Dr Agüero is a research scientist at and deputy director of the Central Veterinary Laboratory, Algete, Spain. Her research interests are virology, veterinary diagnosis, and animal health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bbe3d9b1-32de-479a-932d-8ae911574b45", "title": "Diversity of Bartonella spp. in Bats, Southern Vietnam", "text": "【0】Diversity of Bartonella spp. in Bats, Southern Vietnam\n**To the Editor:** To investigate bats as potential reservoirs for _Bartonella_ spp. in Vietnam, we screened a range of bat species to determine the prevalence and genetic diversity of _Bartonella_ spp. in bat populations in southern Vietnam. In a study of bat biodiversity in southern Vietnam, 60 bats were trapped at 6 sites in Dong Nai Culture and Nature Reserve and Cat Tien National Park, Vietnam, in May 2013. Bats were trapped by using mist nets and harp traps set at ground level, and were euthanized by using isoflurane ( http://www.avma.org/KB/Policies/Documents/euthanasia.pdf ) for cataloguing at the Vietnam Academy of Sciences and Technology, Hanoi. Blood specimens were collected by cardiac puncture, and external measurements were recorded. Bats were speciated according to morphology ( _1_ , _2_ ); trapped bats represented 10 species belonging to 5 genera. All species have been given a conservation status of least concern ( http://www.iucnredlist.org/ ).\n\n【1】Total nucleic acid was extracted from blood samples by using the MagNApure automated nucleic acid extraction system (Roche, Basel, Switzerland). Extracted nucleic acid was subjected to conventional PCR to detect _Bartonella_ spp. DNA by using primers specific for the citrate synthase A gene ( _3_ ). A positive PCR result was determined by amplification of a 729-bp fragment. Twenty-one (35.0%) of 60 bat blood specimens had a result consistent with presence of _Bartonella_ spp. ( Table ). Among insectivorous or carnivorous bats, _Bartonella_ prevalence was 20 (45.5%) of 44 compared with 1 (6.2%) of 16 fruit-eating bats (χ <sup>2 </sup> \\= 6.3, p = 0.01). The prevalence of _Bartonella_ spp. did not differ between sampling locations ( Table ) or by estimated age of the bat (determined by deviation above or below the median tibial length of each species); prevalence was 33.3% (9/27) in younger bats and 36.4% (12/33) in older bats.\n\n【2】DNA sequences from the 21 PCR citrate synthase A gene amplicons (GenBank accession nos. KP100340–KP100360) were subjected to BLAST analysis ( http://blast.ncbi.nlm.nih.gov/Blast.cgi ) to assess sequence similarity. Potentially novel _Bartonella_ phylogroups were identified as having <96% sequence similarity with all publicly available sequences in GenBank ( _4_ ). The sequences were then manually aligned with those of a representative sample of _Bartonella_ spp. and trimmed to the 327-nt region (positions 801–1127) commonly used for taxonomic classification ( _4_ ). A neighbor-joining tree was constructed by using the Hasegawa–Kishino–Yano plus gamma model of nucleotide substitution in Geneious version 7.1.7 with 1,000 bootstrap replications ( _5_ ).\n\n【3】Sequence analysis identified 10 distinct _Bartonella_ phylogroups (I–X) among 21 _Bartonella-_ positive blood samples from bats in Vietnam (online Technical Appendix, http://wwwnc.cdc.gov/EID/article/21/7/14-1760-Techapp1.pdf ). Nine of these phylogroups showed <96% sequence similarity to all previously identified _Bartonella_ sequences, suggesting they might belong to new _Bartonella_ species. _Bartonella_ spp. in _Rhinolophus_ spp. bats were classified into phylogroups I, III, VIII, IX, and X. Phylogroups II and VII were detected in samples from _Hipposideros_ spp. bats, and phylogroups IV and V were detected in _Megaderma_ spp. bats. Phylogroup VI was detected only in a _Megaerops_ spp. bat.\n\n【4】Although 9 lineages (I, III–X) were novel, phylogroup II was identified in 4 _Hipposideros_ spp. bats and showed 96.3%–97.2% similarity to _Bartonella_ spp. isolated from a bat fly found on a _Hipposideros_ spp. host in Malaysia (GenBank accession no. JX416238). This similarity might suggest widespread distribution of this _Bartonella_ spp. lineage in _Hipposideros_ spp. bats or their ectoparasites in Southeast Asia. Additional genetic characterization of strains is needed to determine whether any of these novel phylogroups represent new species and to investigate their evolutionary and ecological relationships with other _Bartonella_ spp. identified in Vietnam and elsewhere.\n\n【5】The primary observation in this study was detection of _Bartonella_ spp. (by DNA amplification) in bats in southern Vietnam at a prevalence of 35.0%, which is comparable with that reported in Kenya (30.2%) and Guatemala (33.0%) ( Table ) ( _6_ , _7_ ). However, the use of conventional PCR in this study might underestimate the true prevalence.\n\n【6】Although high prevalences have been proposed to be caused by persistent infection of bats with _Bartonella_ spp., our findings indicate no increase in prevalence by age of bat, which would be expected if persistent infection were common. This finding, and detection of multiple lineages infecting individual bat species, may instead reflect high levels of transmission within and between bat species caused by crowded roosting areas and sharing of roosts by multiple species. This behavior provides opportunities for transmission of _Bartonella_ bacteria or exchange of infected ectoparasites, such as _Cyclopodia_ spp. ( _8_ ), although the precise roles of these 2 processes are unknown.\n\n【7】Although no human cases of _Bartonella_ spp. infection have been reported in Vietnam, _Bartonella_ spp. have been identified in febrile humans elsewhere in Southeast Asia ( _9_ ) and are also common in rats in southern Vietnam ( _10_ ). Because close contact with bats (i.e., through manure farming and consumption of bat meat) and potential arthropod vectors (i.e., through handling and consumption of fruit) is common in parts of Vietnam, targeted screening of bats and their human contacts might improve our understanding of the zoonotic potential of these bacteria and their potential effect on public health.\n\n【8】Ms. Pham is a research assistant at the Oxford University Clinical Research Unit in Ho Chi Minh City, Vietnam. Her primary research interests focus on characterizing the diversity and spread of potential agents of zoonotic disease in domestic and wild animal populations across Vietnam.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c782d7f-c018-463d-b25c-a61e9883f430", "title": "Porcine Epidemic Diarrhea Virus and Discovery of a Recombinant Swine Enteric Coronavirus, Italy", "text": "【0】Porcine Epidemic Diarrhea Virus and Discovery of a Recombinant Swine Enteric Coronavirus, Italy\n_Porcine epidemic diarrhea virus_ (PEDV) and _Transmissible gastroenteritis virus_ (TGEV) (family Coronaviridae, genus _Alphacoronavirus_ ) are enveloped viruses that contain a single-stranded, positive-sense RNA genome of ≈28 kb. Infection with these viruses causes watery diarrhea, dehydration, and a high mortality rate among suckling pigs. Coronaviruses (CoVs) are prone to genetic evolution through accumulation of point mutations and homologous recombination among members of the same genus ( _1_ ). Porcine respiratory coronavirus (PRCV), a mutant of TGEV, appeared in pigs in the 1980s ( _2_ ). The spread of PRCV, which conserved most of the antigenic sites and causes cross-protection against TGEV ( _3_ ), led to the gradual disappearance of TGEV. Newly emerging CoVs pose a potential threat to human and animal health because multiple human CoV infections have been derived from animal hosts. Emerging swine coronaviruses are of great concern to swine health because of the potential increase in viral pathogenesis.\n\n【1】In 1978, PEDV was first identified in Europe; subsequent reports occurred in many countries in Asia, including China, Japan, Korea, and Thailand. In 2010–2012, genetically different PEDV strains emerged, causing severe outbreaks in China ( _4_ ). PEDV spread to the United States, Canada, and Mexico in 2013–2014, and genetically related strains were detected in South Korea and Taiwan ( _5_ _–_ _7_ _)._ The PEDV outbreak caused large global economic losses to the swine industry. In Europe, a severe PEDV epidemic occurred in Italy during 2005–2006 ( _8_ ), and in 2014–2015, PEDV was detected in Germany, France, and Belgium. These strains have a high nucleotide identity to PEDV strains that contain distinct insertions and deletions (INDELs) in the S gene (S-INDELs) from the United States ( _9_ _–_ _11_ ). We report the detection and genetic characterization of swine enteric CoVs circulating in Italy during 2007–2014. We also report a recombinant TGEV and PEDV strain (identified as the species _Swine enteric coronavirus_ \\[SeCoV\\]) circulating from June 2009 through 2012. Finally, we describe the phylogenetic relationship of the 2014 PEDV S-INDELs to the recent PEDV strains circulating in Europe.\n\n【2】### The Study\n\n【3】During 2007–2014, we collected 27 fecal and 24 intestinal samples from pigs with suspected PEDV or TGEV infections; the pigs came from swine farms in northern Italy (Po Valley), which contains the regions of Piemonte, Lombardia, Emilia Romagna, and Veneto ( Technical Appendix Figure 1). The Po Valley contains 70% of Italy’s swine. Clinical signs included watery diarrhea in sows and a death rate in piglets of 5%–10%, lower than is typical with PEDV or TGEV infections. Samples were submitted for testing by electron microscopy, PEDV ELISA, viral isolation, pan-CoV reverse transcription PCR (RT-PCR), and RT-PCR for PRCV and TGEV; selected positive pan-CoV samples were sequenced ( _12_ _–_ _14_ ) ( Technical Appendix ).\n\n【4】Results of electron microscopy showed that 25 (49%) of the 51 samples contained CoV-like particles, but all samples were negative for viral isolation. Although only 38 samples (74%) were positive by pan-CoV RT-PCR, 47 (92%) were positive by the PEDV ELISA ( Table 1 ) ( _12_ _,_ _13_ ). Of the 38 pan-CoV–positive samples, 18 were selected for partial RNA-dependent RNA polymerase (RdRp), spike (S1) ( _14_ ), and membrane (M) sequencing ( Table 1 ). All samples were negative for PRCV and TGEV by RT-PCR, ruling out co-infection with PEDV and TGEV or PRCV ( _15_ ).\n\n【5】Figure 1\n\n【6】Figure 1 . Phylogenetic analyses of swine enteric coronaviruses in Italy. A) Analysis performed on the basis of the nucleotide sequence of the complete spike (S1) gene of 4 representative strains from the 3...\n\n【7】On the basis of the partial sequences from RdRp and the S1 and M genes, the strains from Italy clustered into 3 temporally divided groups, suggesting 3 independent virus entries. Cluster I represents strains circulating from 2007 through mid-2009; cluster II represents strains circulating from mid-2009 through 2012; and cluster III represents strains circulating since 2014 ( Technical Appendix Figure 2, panels A–C). Cluster I was identified in Emilia Romagna (n = 1), Lombardia (n = 5), and Veneto (n = 1). Cluster II was identified in Emilia Romagna (n = 1) and Lombardia (n = 8). Cluster III was identified in Emilia Romagna at 2 swine farms. To help explain the temporal clustering, a single S1 gene segment was sequenced from clusters I and II (PEDV/Italy/7239/2009 and SeCoV/Italy/213306/2009, respectively). Because of the recent outbreak of PEDV in Europe, the 2 positive samples from cluster III (PEDV/Italy/178509/2014 and PEDV/Italy/200885/2014) were sequenced ( Figure 1 , panel A).\n\n【8】Figure 2\n\n【9】Figure 2 . Potential recombination points in the SeCoV strains in study of swine enteric coronaviruses in Italy. The potential parent strains H16 (TGEV) and CV777 (PEDV) are shown in teal and purple, respectively....\n\n【10】One strain from each cluster was selected for whole genome sequencing ( Technical Appendix ). Unfortunately, the whole genome was obtained from only clusters I and II (PEDV/Italy/7239/2009 and SeCoV/Italy/213306/2009, respectively; Figure 1 , panel B). Recombination analysis was conducted on the 2 whole genomes and was not detected in PEDV/Italy/7239/2009. Recombination was detected in SeCoV/Italy/213306/2009 at position 20636 and 24867 of PEDV CV777 and at position 20366 and 24996 of TGEV H16 ( Figure 2 ), suggesting the occurrence of a recombination event between a PEDV and a TGEV. The complete S gene of SeCoV/Italy/213306/2009 shared 92% and 90% nt identity with the prototype European strain PEDV CV777 and the original highly virulent North American strain Colorado 2013, respectively, and the remaining genome shared a 97% nt identity with the virulent strains TGEV H16 and TGEV Miller M6. Whole-genome analysis of PEDV/Italy/7239/2009 showed that it grouped with the global PEDV strains ( _6_ ) and shared ≈97% nt identity with PEDV strains CV777, DR13 virulent, and North American S-INDEL strain OH851 ( Table 2 ).\n\n【11】### Conclusions\n\n【12】During 2007–2014, most (92%) samples collected from the Po Valley in Italy were positive for PEDV by ELISA; only 72% were positive by pan-CoV PCR. However, because we were investigating the presence of PEDV or TGEV in samples with clinical signs of diarrhea, the high occurrence of PEDV may not reflect the actual prevalence of PEDV in Italy. The increased percentage of PEDV found in samples tested by ELISA, compared with the proportion found by PCR, may be explained by the number of ambiguous bases in the pan-CoV primers; the ambiguous bases severely reduce the efficiency of the reaction. The swine enteric CoV strains from Italy in our study, including the recombinant strain, were reported in pigs with mild clinical signs, indicating that PEDV and SeCoV have been circulating in Italy and likely throughout Europe for multiple years but were underestimated as a mild form of diarrhea.\n\n【13】To understand the evolution of PEDV in Italy, the partial RdRp, S, and M genes were sequenced from 18 samples and grouped in 3 different temporal clusters. Cluster I (2007–mid 2009) resembles the oldest PEDV strains; cluster II resembles a new TGEV and PEDV recombinant variant; and cluster III, identified from 2 pig farms in northern Italy in 2014, resembles the PEDV S-INDEL strains identified in Germany, France, Belgium, and the United States. The >99.3% nt identity of the S1 gene within cluster III and in previously identified strains could suggest the spread of the S-INDEL strain into Europe. However, directionality of spread cannot be determined because of a lack of global and temporal PEDV sequences.\n\n【14】Although our findings could indicate 3 introductions of PEDV in Italy, the results more likely suggest the high ability of natural recombination among CoVs and the continued emergence of novel CoVs with distinct pathogenic properties. Further investigation is needed to determine the ancestor of the SeCoV strain or to verify whether the recombinant virus was introduced in Italy. Recombinant SeCoV was probably generated in a country in which both PEDV and TGEV are endemic, but because the presence of these viruses in Europe is unclear and SeCoV has not been previously described, it is difficult to determine the parental strains and geographic spread of SeCoV. Future studies are required to describe the pathogenesis of SeCoV and its prevalence in other countries.\n\n【15】Dr. Boniotti is a scientist at the Istituto Zooprofilattico Sperimentale della Lombardia e Emilia Romagna; her primary research interests include molecular diagnosis and epidemiology of viral and bacterial infectious diseases. Dr. Papetti is a research scientist at the Istituto Zooprofilattico Sperimentale della Lombardia e Emilia Romagna; her major research interests are molecular diagnostic development and phylogenetic analysis of infectious diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1907e6d9-36ed-4bda-aa7f-2f42450e150b", "title": "Institutional Review Boards: Developing Countries Consideration", "text": "【0】Institutional Review Boards: Developing Countries Consideration\nInstitutional review boards (IRBs) play an essential role in protecting the rights of volunteers involved in research projects. Their function has become more complex, particularly concerning projects conducted in developing countries. But can IRBs in the United States guarantee the protection of human subjects involved in research projects in developing countries?\n\n【1】IRBs have no effective way of controlling what goes on in the field. The complex ethical clearance process does not determine whether persons engaged in research projects in developing countries are fully aware of the major aspects of the studies they participate in. The clearance process includes the IRB approval and consent forms. Required U.S. consent forms are too long and the language too complicated to be certain all participants have a full understanding of the study. The forms also appear to be intended more to offer legal protection to sponsoring agencies than to protect the welfare of the volunteer. Most importantly, the forms do not guarantee that volunteers have fully understood the objectives, risks, and benefits of the study, and the extent of their voluntary participation. To protect volunteers as well as all persons and institutions involved, these forms must not only communicate necessary information concerning the study to be conducted but also evaluate volunteers' knowledge and their desire to participate. To achieve this goal, we propose to use a simple questionnaire administered by a team not involved in the volunteer recruitment process. We have used such a questionnaire to evaluate potential volunteers for a phase II HIV vaccine trial. Although volunteers had three intensive, 2-hour counseling sessions, only half responded correctly to all 21 questions. The others were referred for additional counseling and reevaluation.\n\n【2】The IRB process requires that collaborative projects with U.S. institutions have clearance from multiple IRBs. Each IRB meets generally once a month and uses its own consent forms. Each has its own set of rules. Each will respond with different concerns that must be addressed. The approval process may create a lag time of 3 to 12 months to obtain ethical clearances for a project lasting 12 to 24 months.\n\n【3】The ethical clearance process can be simplified in several ways: 1) All studies supported by NIH should have a unique IRB application form and a unique IRB consent form. 2) A certain percentage of the research grant should be allocated to support the ethical clearance process. Ethical support should be available at the grant's initiation. 3) While waiting for the formal ethical clearance and final consent, potential volunteers could be counseled and evaluated. 4) The primary responsibility of local and national IRBs should be clearly determined. IRBs must share responsibilities to achieve the greatest benefit for volunteers. 5) A mechanism must be developed to resolve conflicts between IRBs from developed and developing countries. Yearly meetings of IRBs from host and sponsoring institutions should take place to facilitate the exchange of documents and other information.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "758e0a30-7f7b-462d-a37f-e7441f27c61f", "title": "Cost Effectiveness of a Potential Vaccine for Human papillomavirus", "text": "【0】Cost Effectiveness of a Potential Vaccine for Human papillomavirus\nCervical cancer is one of the most common malignancies in women: this year in the United States, approximately 13,000 new cases will be diagnosed, and >4,000 women will die of the disease. Fortunately, cervical cancer is highly preventable with regular Papanicolaou (Pap) testing. Between 1973 and 1995, the Surveillance, Epidemiology, and End Results (SEER) Program (sponsored by the National Cancer Institute) documented a 43% decrease in incidence and a 46% decrease in death from cervical cancer. Such reductions, however, have not been observed in locations or countries where cytologic testing is not widely available. Epidemiologic research strongly implicates _Human papillomavirus_ (HPV) as the major risk factor for cervical cancer. Therefore, methods of prevention, diagnosis, and treatment of HPV infection have been pinpointed as a means of reducing the incidence of cervical cancer.\n\n【1】HPV comprises >100 different types of viruses; approximately 40 of these are transmitted sexually. Although most HPV infections proceed and resolve without symptoms, some types of HPV (such as 6 and 11) may cause genital warts, whereas other types (such as 16 and 18) are associated with certain types of cancer. HPV infections are recognized as the major cause of cervical cancer: >90% of women who have cervical cancer also have been infected with HPV ( _1_ – _7_ ). HPV types that are correlated with the development of cancer are referred to as high-risk. Although no medical means currently exist to eliminate HPV infection, precancerous lesions and warts caused by these viruses can be treated.\n\n【2】Given the substantial disease and death associated with HPV and cervical cancer, research to develop a prophylactic HPV vaccine is ongoing ( _8_ ). Vaccines for HPV-16 and HPV-18 are currently being studied in clinical trials; the Phase I trial results are encouraging ( _9_ , _10_ ). The cost effectiveness of such vaccines, however, has not been studied sufficiently. Therefore, we evaluated the effectiveness and cost effectiveness of a prophylactic vaccine.\n\n【3】### Data and Methods\n\n【4】Figure 1\n\n【5】Figure 1 . Schematic representation of the decision model. In panel A, the square node at the left represents the vaccination decision. The woman’s health thereafter is simulated by a Markov model. Each month,...\n\n【6】We used a decision model to estimate the length of life and expenditures for vaccination of adolescent girls for high-risk HPV types ( Figure 1 ). We adhered to the recommendations of the Panel on Cost Effectiveness in Health and Medicine ( _11_ ) for conducting and reporting a reference-case analysis. We expressed our results in terms of costs, life-years, quality-adjusted life-years (QALYs), and incremental cost-effectiveness (ICE) ratios. We performed one-way sensitivity analyses on all model variables, as well as multi-way sensitivity analyses on selected variables.\n\n【7】##### Patient Population\n\n【8】The target population for this analysis was all adolescent girls in the United States. Our base-case analysis considered a hypothetical cohort of 12-year-old girls. A recent study by the Centers for Disease Control and Prevention (CDC), indicated that although 3% of girls have had sexual intercourse before reaching age 13, 18.6% are sexually active by age 15, and 59.2% by age 18 ( _12_ ). We therefore believed that vaccinating 12-year-old girls would capture most girls before they are sexually active and are at risk for HPV infection. We examined the optimal vaccination age in sensitivity analyses.\n\n【9】Our analysis assumes a universal vaccination strategy for adolescent girls. Although risk factors for HPV infection are identifiable, we chose to evaluate a universal vaccination program for several reasons. Previous vaccination programs aimed to reduce incidence of _Hepatitis B_ _virus_ (HBV) infection have tried to target the risk groups that account for most cases ( _13_ ). These high-risk groups, however, are difficult to vaccinate for a variety of reasons, including inaccessibility, noncompliance, and the inability to identify people at risk. Also, because >30% of HBV-infected persons show no identifiable risk factor for infection ( _13_ , _14_ ), they would not be included in such a targeted immunization strategy. Similarly for HPV infection, the broad range of risk factors and the difficulty identifying these behaviors inhibit targeting such risk groups. We evaluated the cost-effectiveness of targeting high-risk girls (assuming a reduced compliance) in sensitivity analyses.\n\n【10】##### Decision Model\n\n【11】We used Decision Maker software (Pratt Medical Group, Boston, MA, v2002.07.2) to develop a Markov model that followed the girls over their lifetimes. For each strategy, our model included probabilities of occurrence and progression of HPV, of squamous intraepithelial lesions (SIL), and of cervical cancer, as well as the probability of death, costs, and quality of life associated with the various health states. Whenever possible, we based our probability estimates ( Appendix ) on large, high-quality studies reported in the literature.\n\n【12】Our model ( Figure 1A ) tracks a cohort of girls who are either vaccinated against specific HPV types or who receive the current standard of care. Based on hepatitis B vaccination completion rates among U.S. adolescents, we assumed that 70% of the targeted girls would be vaccinated successfully ( Appendix ). We assumed that girls who were not vaccinated would receive the current standard of care.\n\n【13】Every month, each girl is at risk of developing high- or low-risk HPV, SIL, or cervical cancer. Over time, an infected woman’s HPV infection can regress, persist, or progress to either low- or high-grade SIL. SIL can also exist independent of an HPV infection. High-grade SIL can progress to cervical cancer. The diagnosis, treatment, and natural history of cervical cancer are modeled in Figure 1B .\n\n【14】We assumed that the current standard of care included routine Pap tests for compliant patients every 2 years starting at age 16. Throughout a woman’s lifetime, her HPV, SIL, or cervical cancer status can be discovered and treated either because symptoms have developed or through routine Pap tests ( Figure 1C ). We assumed that 10% of woman diagnosed with low-grade SIL would undergo cryotherapy and that all would receive a 6-week reexamination, and Pap tests at 3, 6, 12, and 18 months after cryotherapy. Treatment of high-grade SIL was assumed to include loop electrosurgical excision procedure (LEEP), and subsequent reexamination and Pap tests ( _15_ , _16_ ).\n\n【15】A woman may also choose to have a benign hysterectomy reducing her risk of cervical cancer. In addition to being at risk for death because of cervical cancer, all women are at risk for age-specific death unrelated to HPV or cervical cancer.\n\n【16】##### Data and Base-Case Assumptions\n\n【17】##### HPV Infection\n\n【18】Incidence of HPV infection was based on Myers’ mathematical model of HPV infection ( Appendix ) ( _17_ ). In our base-case analysis, annual incidence began at age 15 (10%), peaked at age 19 (18%), and dropped off quickly after age 29 (1%). We assumed that no prevalent HPV infections existed in the initial cohort of 12-year-old girls but varied this assumption in sensitivity analyses. Given HPV infection, regression rates were highest for women <25 years (46%/yr) and lowest for women >30 years (7%/yr), reflecting a preponderance of more persistent infections in the older age group ( Appendix ).\n\n【19】##### Low- Versus High-Risk HPV\n\n【20】Because of a lack of significant HPV genotype cross-immunity, any vaccine developed probably will be effective against a limited number of HPV types ( _18_ , _19_ ). HPV types 16, 18, 45, and 31 together are the most commonly associated with cervical cancer, with evidence of these four types apparent in >75% of women who have cervical cancer ( _20_ ). In our model, HPV types 16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, and 68 were considered high risk; all other HPV types were categorized as low risk ( _20_ _–_ _22_ ). Based upon this classification in the general population, 59% of HPV infections are caused by high-risk types ( Appendix ).\n\n【21】##### Low- and High-Risk Rates of HPV Progression\n\n【22】To evaluate potential vaccination strategies, we modeled different disease-progression rates in women infected with low- and high-risk HPV. By combining data on overall progression rates of HPV infection to cancer, with prevalence data on women infected with low- and high-risk HPV who had low- or high-grade SIL or cervical cancer, we estimated separate progression rates for low- and high-risk HPV infections. Data from seven articles were considered of high enough quality to be included in our analysis ( _1_ – _7_ ).\n\n【23】High-risk HPV infections were significantly more common in women who had cervical cancer than in women who had precursor lesions. Based on the results of seven studies (N=1609), high-risk HPV infection was detected in 56% of women who had low-grade SIL, in 83% of women who had high-grade SIL, and in 90% of women who had cervical cancer. Low-risk HPV infection was detected in 22%, 8%, and 3% of these women, respectively. No evidence of HPV infection was found in 22%, 9%, and 7% of these women, respectively ( _1_ – _7_ ). We then calculated relative progression rates for transition from high-risk, low-risk, or no HPV infection to low-grade SIL; from low-grade to high-grade SIL; and from high-grade SIL to cervical cancer ( Appendix ).\n\n【24】##### Cancer Surveillance, Treatment, and Progression\n\n【25】We estimated that 71% of the adult female population received biennial Pap testing. Pap test sensitivity and specificity results were based on a meta-analysis conducted by the Duke Evidence-Based Practice Center ( _17_ , _23_ ). Diagnosis of asymptomatic cervical lesions depended on a woman’s likelihood of having a Pap test and on the sensitivity and specificity of this test.\n\n【26】Assessment of treatment effectiveness for cervical lesions was based on a review of 13 studies that detailed treatment effectiveness by lesion stage ( Appendix ). Initial treatment effectiveness was estimated at 97% and 94% for low-grade and high-grade SIL, respectively. Unsuccessful initial treatment of cervical lesions was followed with a repeat treatment (cryotherapy or LEEP) (77%), cone biopsy (18%), or hysterectomy (5%) ( _24_ ), increasing treatment success. We based cancer progression rates, annual patient survival rates, and probability of symptoms by cancer stage on an analysis by Myers et al. ( Appendix ) ( _17_ ). Myers et al. validated their data by comparing predicted distribution of cancer by stage for an unscreened population with data from studies of women who had had no prior screening.\n\n【27】##### Benign Hysterectomy\n\n【28】We considered women who did not have cervical cancer but who had hysterectomies to be fully protected from cervical cancer. We tested this assumption in sensitivity analyses. Age-specific hysterectomy rates were based on data from the Hospital Discharge Survey of the National Center for Health Statistics ( Appendix ).\n\n【29】##### HPV Vaccine Characteristics\n\n【30】In our model the HPV vaccine was administered by using a series of three injections in a school-based immunization program. Because vaccine longevity is uncertain, we assumed that successful vaccination conferred immunity for 10 years but that repeated booster shots every 10 years were required to maintain the vaccine’s efficacy. We evaluated the need for more frequent booster shots or a vaccine that conferred lifetime immunity in sensitivity analysis. For our base-case analysis, vaccine efficacy against high-risk HPV types was estimated at 75%. We tested the complete range of vaccine effectiveness (from 0% to 100%) because of the absence of efficacy data from Phase III clinical trials and because future marketed vaccines may target only a subset of the high-risk HPV types.\n\n【31】##### Quality of Life\n\n【32】HPV infection and cervical cancer can markedly affect quality of life and therefore can affect a woman’s quality-adjusted life expectancy. Accordingly, we incorporated adjustments for quality of life associated with current health, HPV, SIL, and with cervical cancer and its treatment.\n\n【33】Utilities for health states were based on a report by the Institute of Medicine on Vaccines for the 21st Century, which used committee-consensus Health Utility Indices levels for relevant health states ( Appendix ). Undiagnosed HPV and cervical lesions were considered to be asymptomatic and to have no utility decrement. Diagnosed and treated low- and high-grade SIL were assigned lower utilities (0.97) for a 1-year duration. Treatment for locally invasive cancer was assigned a low utility (0.79–0.80) during 4 months of initial treatment, with a moderate utility (0.90–0.97) during a 2- or 3-year follow-up. For more advanced cancer, a woman’s utility was decreased to 0.62 during both treatment and follow-up to reflect the severity of her disease and its effects on quality of life. We based current health utilities on the gender- and age-specific data from the Beaver Dam study ( _25_ ).\n\n【34】##### Costs\n\n【35】We converted all costs to 2001 U.S. dollars by using the gross-domestic-product deflator. Pap-testing costs were $81 per test, including a 10% rescreen rate. We estimated the cost of the vaccine materials, personnel, and administration at $300, based on school-based HBV vaccination programs ( Appendix ). We assumed a three-injection protocol with a booster shot ($100) required every 10 years.\n\n【36】Treatment costs of low- and high-grade SIL were based on Medicare average reimbursements and resource-based cost estimates. We estimated the cost of treatment of low-grade SIL from the cost of an initial colposcopy and biopsy, cryotherapy (in 10% of patients), a 6-week reexamination, and Pap tests at 3, 6, 12, and 18 months after treatment. The cost of treatment of high-grade SIL was based on cost of initial colposcopy and biopsy, LEEP, and subsequent reexamination and Pap tests. Cost of cancer treatment varied, depending on the stage at which cancer was diagnosed. Costs were based on Medicare average reimbursement rates ( _26_ ) and cross-checked with a 1999 HMO case-control full-cost analysis ( _27_ ) ( Appendix ).\n\n【37】##### Sensitivity Analysis\n\n【38】We performed one-way and multi-way sensitivity analyses to account for important model uncertainties. For clinical variables, our ranges for sensitivity analyses represent our judgment of the variation likely to be encountered in clinical practice, based on the literature and on discussion with experts. The ranges for costs represent variation by 25% above and below the base-case estimate. To determine ranges for utilities, we used clinical judgment.\n\n【39】### Results\n\n【40】#### Model Validation\n\n【41】We evaluated outcomes in the current practice arm of the model to ensure that they reflected the frequency of events from the Surveillance, Epidemiology and End Results (SEER) registry. Our model’s annual rates of cervical-cancer cases and cervical-cancer-related deaths match 2001 SEER estimates, as well as those calculated by the Myers model ( _17_ ) (data available from the authors).\n\n【42】##### Base-Case Analysis\n\n【43】A prophylactic vaccine against high-risk HPV types is more expensive than current practice but results in greater quality-adjusted life expectancy ( Table 1 ). HPV vaccination of 12-year-old girls improves their life expectancy by 2.8 days or 4.0 quality-adjusted life days at a cost of $246 relative to current practice (ICE of $22,755/QALY).\n\n【44】Vaccinating the present U.S. cohort of 12-year-old girls (population approximately 1,988,600) averts >224,255 cases of HPV, 112,710 cases of SIL, 3,317 cases of cervical cancer, and 1,340 cervical-cancer deaths over the cohort’s lifetime. Prevention of one case of cervical cancer would require vaccination of 600 girls ( Table 2 ).\n\n【45】##### Sensitivity Analyses\n\n【46】Figure 2\n\n【47】Figure 2 . Sensitivity analysis. Tornado diagram representing the incremental cost-effectiveness ratios of one-way sensitivity analysis on the vaccination strategy compared to current practice. The vertical line represents the incremental cost-effectiveness ratio under base-case...\n\n【48】Figure 2 shows the ICE ratios of one-way sensitivity analyses of the vaccination strategy compared to current practice. We explored those variables with the greatest effect on the ICE ratio by running more extensive sensitivity analyses. Given the uncertainty surrounding the vaccine efficacy, pricing, and mechanism, we performed extensive sensitivity analyses using vaccine-related variables.\n\n【49】Figure 3\n\n【50】Figure 3 . Sensitivity analysis: Vaccine efficacy. Effect of a change in _Human papillomavirus_ (HPV) vaccine efficacy on the cost effectiveness of vaccination compared with current practice under varying assumptions of vaccine immunity. The...\n\n【51】Figure 4\n\n【52】Figure 4 . Sensitivity analysis: Vaccine cost. Effect of a change in _Human papillomavirus_ (HPV) vaccine cost on the cost effectiveness of vaccination compared with current practice under varying assumptions of vaccine immunity. The...\n\n【53】In our base-case analysis, we estimated that an HPV vaccine would provide immunity against high-risk HPV types in 75% of the girls vaccinated. At early stages of vaccine development or given a vaccine that targets only selected high-risk HPV types, the efficacy may prove to be lower. Sensitivity analyses on the vaccine efficacy and cost showed that even if the efficacy was reduced to 40% or the vaccine cost was increased to $600, vaccination costs <$50,000/QALY, relative to current practice ( Figures 3 and 4 ).\n\n【54】We assumed that vaccination required a one-shot booster every 10 years. We also considered that vaccination could provide lifetime immunity, in which case the ICE improved to $12,682/QALY. Vaccinating the present U.S. cohort of 12-year-old girls with such a lifetime vaccine would avert >272,740 cases of HPV, 174,208 cases of SIL, 7,992 cases of cervical cancer, and 3,093 cervical-cancer deaths over the cohort’s lifetime. Prevention of one case of cervical cancer would require vaccination of 250 girls. Even if a booster shot is required every 3 years, the vaccine compared to current practice remained fairly cost effective ($45,599/QALY) ( Figures 3 and 4 ). Our model assumes that a vaccination program would target 12-year-old girls for vaccination. Waiting until girls are 15 years old to provide vaccination results in a slightly lower life expectancy (reducing quality-adjusted life expectancy by 0.2 days) though at a reduced cost ($20). Vaccination of 12-year-old girls as compared to 15-year-old girls costs $40,440 per additional quality-adjusted life year gained.\n\n【55】Figure 5\n\n【56】Figure 5 . Sensitivity analysis: Frequency of Pap tests in vaccinated women. Effect of changing the frequency with which vaccinated women receive a Pap test. The diamonds represent Pap testing vaccinated women annually, every...\n\n【57】Although the estimates used in our analysis reflect current Pap-test characteristics and compliance, if every woman obtained a Pap test every 2 years (base-case estimate is 71% compliance every 2 years), the ICE of vaccination increases to $33,218/QALY. Our base-case analysis assumes that vaccinated women would continue to receive Pap tests at the same frequency as unvaccinated women. HPV vaccination and the resulting reduction in cervical-cancer risk, however, might decrease frequency of Pap testing. Figure 5 shows how the costs and quality-adjusted life expectancy are influenced by the frequency of Pap tests in the vaccinated cohort. A strategy in which vaccinated women have Pap testing every 4 years increases life expectancy while reducing costs compared to current practice. While providing more frequent Pap tests to vaccinated women does increase a woman’s quality-adjusted life expectancy, it also increases costs. The cost-effectiveness ratios of more frequent testing are shown in Figure 5 .\n\n【58】Our results were sensitive to several of our base-case assumptions ( Figure 2 ). Vaccination saved 11.4 quality-adjusted life days and cost $290 over current practice when costs and benefits were not discounted (ICE of $9,286/QALY). At a discount rate of 5%, vaccination cost $37,752/QALY. Some women may be quite alarmed by being diagnosed with high-grade SIL; decreasing the utility of high-grade SIL to 0.8 lowers the cost-effectiveness ratio to $16,927/QALY. Varying the underlying incidence of HPV from 0.5 to 2 times our base-case values resulted in cost-effectiveness ratios ranging from $43,088 to $12,664 per QALY, respectively. Sensitivity analyses with other variables did not change our results substantially ( Figure 2 ).\n\n【59】### Discussion\n\n【60】We evaluated the usefulness of a potential vaccine against high-risk HPV types administered to adolescent girls and found it to be cost effective as compared to current practice ($22,755/QALY). Although the increase in quality-adjusted life expectancy from a vaccination program is modest for the individual, the increase aggregates to substantial numbers of HPV infections, cases of cervical cancer, and prevented cancer-related deaths ( Table 2 ). Furthermore, the life-expectancy gains are similar to those realized by current vaccination programs. Vaccination against high-risk HPV saved 2.8 life days and 4.0 quality-adjusted life days per person. In comparison, vaccinations against measles, mumps, rubella, and pertussis each save 2.7, 3.0, 0.3, and 3.3 life days, respectively ( _28_ , _29_ ). Sensitivity analyses found that the HPV vaccine would be cost effective, even assuming vaccine efficacy as low as 40% or that booster shots would be required every 3 years.\n\n【61】The only previous analysis of the cost effectiveness of a vaccine against HPV was published by the Institute of Medicine (IOM) ( _30_ ). That analysis also showed an HPV vaccine to be cost effective. Our analysis differs from the IOM’s, however, in that we modeled a vaccine specific to high-risk types of HPV because such vaccines are under development and in clinical trials. In addition, our progression and recurrence rates are HPV-type specific.\n\n【62】Our analysis does have limitations. We analyzed the benefits and costs of vaccinating only adolescent girls against HPV. Because HPV is sexually transmitted, reducing the prevalence of HPV in the population will also affect the prevalence of HPV in women’s sexual partners. Although HPV is most commonly associated with cervical cancer, it may also play a role in cancers of the anus, vulva, vagina, and penis. The benefits of HPV vaccination associated with reductions in these types of cancers are not included in our analysis. Including them should make HPV vaccination even more favorable. The decision whether to vaccinate adolescent boys as well is more complex; therefore, in future work we plan to extend our analysis to incorporate such costs and benefits. In addition, the costs and benefits used in this analysis are tailored to the population and health-care environment of the United States. As Figure 5 demonstrates, the availability of HPV vaccines may justify less frequent Pap tests. This effect may be particularly relevant in developing countries that must decide how best to allocate their limited health-care resources.\n\n【63】We make several assumptions about the target vaccination population and program implementation that need discussion in terms of their political and social feasibility. First, we propose a school-based vaccination program rather than a clinic-based one. School-based immunization programs address several challenges encountered when vaccinating adolescents. First, school-based programs provide an infrastructure in which to vaccinate adolescents. Adolescent health-care visits are often not routine, and given scheduled visits, adolescents are often noncompliant with appointments. In addition, we believe that fitting the three-dose HPV vaccination regimen into the academic year will increase compliance while containing costs. Several school-based programs have documented completion rates of >90%. In contrast, lower rates of completion (11% to 87%) have been found in more traditional health-care settings ( _31_ – _34_ ). Second, we propose providing universal vaccination rather than targeting specific high-risk groups. Certain groups of women are at higher risk for HPV infection, and the cost effectiveness of vaccinating such target groups may be more favorable than a universal vaccination program. Experience with _Hepatitis B_ vaccination in adolescents, however, has demonstrated how such groups may be those that are hardest to reach ( _13_ , _14_ , _35_ ), and that many risk factors for infection (such as number of partners) may not be readily identifiable ( _13_ , _14_ ). Finally, we propose vaccinating girls at an early adolescent age (12 years). Although the lifetime cost of vaccinating 12-year-old girls is slightly greater than that of vaccinating 15-year-old girls, earlier vaccination costs <$50,000 per QALY when compared to costs of vaccinating older adolescents. A significant proportion of adolescents are sexually active by 15 years of age; therefore, vaccination at 12 years of age aims to include as many girls as possible before sexual activity begins and HPV infection risk increases. In addition, studies using _Hepatitis B_ vaccines as a proxy have found better immune responses in younger persons and have shown that younger children require lower doses ( _36_ , _37_ ). Finally, we believe a 3-dose school-based vaccination program aimed at 12-year olds will result in greater compliance because adolescents of this age have more consistent school attendance ( _13_ , _38_ – _40_ ). Before a HPV vaccination program is successfully implemented, social and political issues will need to be addressed and agreed upon by stakeholder groups, including pediatricians, public health officers, parents, adolescents, school administrators, and community leaders.\n\n【64】Several institutions, including Merck Research Laboratories, MedImmune Inc., GlaxoSmithKline, and the National Cancer Institute (NCI), are developing and testing prophylactic HPV vaccines. Researchers at NCI and Johns Hopkins have developed a virus-like particle vaccine with promising initial results ( _9_ , _10_ ). If the results of the recently completed Phase II study in the United States and Costa Rica confirm the Phase I results, a Phase III trial in Costa Rica involving 10,000 women will begin. Nonetheless, a vaccine probably will not be approved for widespread use for 3–5 years. Meanwhile, the need for continued cervical-cancer screening and treatment programs remains high.\n\n【65】Our study suggests that vaccination of girls with a HPV vaccine is cost effective when compared to many other generally acceptable health interventions. Although HPV vaccines are still under development, our assessment of the cost effectiveness, however, is robust across a wide range of vaccine mechanisms and efficacies. Although several hurdles to an HPV vaccine must be overcome before it is widely disseminated, our analysis suggests that a vaccine against high-risk HPV would have substantial public health benefit and emphasizes the importance of ongoing vaccine research and development.\n\n【66】Dr. Sanders is an assistant professor of medicine in the Center for Primary Care and Outcomes Research at Stanford University. Her research expertise lies in medical decision making, cost-effectiveness analysis, medical informatics, and guideline development.\n\n【67】Mr. Taira is a third-year medical student in Stanford University’s School of Medicine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9d6642ba-de30-46c7-b61b-e19be1befca1", "title": "Quinine-Resistant Malaria in Traveler Returning from French Guiana, 2010", "text": "【0】Quinine-Resistant Malaria in Traveler Returning from French Guiana, 2010\n**To the Editor:** Resistance of _Plasmodium falciparum_ to antimalarial drugs is one of the most worrying problems in tropical medicine. For _P. falciparum_ malaria acquired in French Guiana, the combination of quinine and doxycycline is one of the first-line recommended treatments ( _1_ ). Since 1996, only 2 treatment failures with quinine have been reported from that country ( _2_ ). An elevated 50% inhibitory concentration (IC <sub>50 </sub> ), classified as in vitro quinine resistance, was reported for 17% of 32 _P. falciparum_ isolates obtained during 1983–1987 in French Guiana ( _3_ ). Throughout 1994–2005, isolates were susceptible to quinine, with a mean IC <sub>50 </sub> <200 nmol/L ( _4_ ).\n\n【1】We report quinine treatment failure in a 35-year-old man who was infected during a 3-month stay in Saül, a rural area of French Guiana. The patient did not use antivectorial or antimalarial prophylaxis. The patient sought treatment with fever 4 days after returning to France on June 22, 2010 (day 0), and a diagnosis of _P. falciparum_ malaria was made on the basis of results of a rapid diagnostic test performed by a private medical laboratory. The man, who weighed 58 kg, was treated as an outpatient with 500 mg of quinine to be taken orally 3×/d for 7 days; he did not receive doxycycline. He was admitted to the Laveran Military Teaching Hospital in Marseille on July 15 (day 24 and first day of recrudescence) for uncomplicated malaria with a _P. falciparum_ parasitemia level of 4%. He was given artemether, 80 mg/d, by intramuscular injection for 3 days. Blood samples taken on day 27 (third day of recrudescence) and day 52 (4 weeks of recrudescence) were negative for _P. falciparum_ .\n\n【2】In vitro testing of drug susceptibility was performed by the standard 42-hour <sup>3 </sup> H-hypoxanthine uptake inhibition method ( _5_ ). We assessed susceptibility to 11 antimalarial drugs on the fresh isolate and after culture adaptation ( Table ). The laboratory-adapted strain 3D7, tested 3× on the same batch of plates, was used as reference. The strain isolated from the blood sample on day 24 (first day of the recrudescence) showed reduced susceptibility to quinine (1,019 nmol/L), chloroquine (427 nmol/L), and monodesethylamodiaquine (157 nmol/L). The isolate was susceptible to all other antimalarial drugs tested. We assessed gene polymorphisms of _pfcrt_ ( _P. falciparum_ chloroquine resistance transporter), _pfmdr1_ ( _P. falciparum_ multidrug resistance 1 protein), and _pfnhe-1_ ( _P. falciparum_ Na <sup>+ </sup> /H <sup>+ </sup> exporter 1); the copy number of _pfmdr1_ involved in quinoline resistance; and gene polymorphisms of _dhfr_ (dihydrofolate reductase, involved in proguanil or pyrimethamine resistance), _dhps_ (dihydropteroate synthetase, involved in sulfadoxine resistance), and _cytB_ (cytochrome B, involved in atovaquone resistance) ( _6_ ).\n\n【3】The _pfnhe_ ms4760 microsatellite showed a profile 3, with 1 repeat of DNNND and 2 repeats of DDDNHNDNHNN. Studies of the _pfnhe-1_ polymorphism of worldwide culture-adapted isolates showed that increased numbers of DNNND were associated with decreased quinine susceptibility ( _7_ ). Association of 2 repeats of DNNND and a high quinine IC <sub>50 </sub> value was found in a case of clinical failure of quinine in a traveler returning from Senegal ( _8_ ).\n\n【4】Reinfection was excluded because the patient had stayed in mainland France since his return. The patient reported that he took the quinine as instructed. We report here a clinical and parasitologic failure of quinine treatment associated with high IC <sub>50 </sub> but not linked with the ms4760 _pfnhe-1_ profile involved with quinine in vitro reduced susceptibility. A hypothesis that may explain our data is that unlike previous studies ( _7_ ), in which the less susceptible strains originated from Asia, this isolate came from South America. The profile associated with quinine resistance for chloroquine resistance and _pfcrt_ genotypes could be different in the 3 malaria-endemic continents. There are no data on ms4760 _pfnhe-1_ of _P. falciparum_ isolates from South America. In another recent study ( _9_ ), a multivariate analysis performed on 83 clinical isolates from Madagascar and 13 African countries did not confirm this association between quinine susceptibility and _pfnhe-1_ microsatellite polymorphisms.\n\n【5】The _pfcrt_ gene had a point mutation on codon 76 (76T) and _pfmdr1_ on codons 184F, 1034C, 1042D, and 1246Y. These data are in agreement with those from previous studies that showed the mutation 76T in the _pfcrt_ gene led to decreased susceptibility to chloroquine, amodiaquine, and quinine. The isolate had only 1 copy of _pfmdr1_ . The data on mutations and copy number of _pfmdr1_ are consistent with data in Brazil ( _10_ ). Nevertheless, the lack of gene amplification and specific point mutations in _pfmdr1_ were not associated with decreased in vitro susceptibility of quinine. _dhfr_ and _dhps_ genes had a 5-mutation haplotype, 51I C59 108N I164-S436 437G 540E 581G A613, which suggested in vitro resistance to proguanil, pyrimethamine, and sulfadoxine. This case confirms the need to always add doxycycline to quinine for treatment of _P. falciparum_ malaria acquired in French Guiana as well as other parts of South America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "17c4330b-4547-4bdb-8385-08068fd14364", "title": "Impact of San Francisco’s Toy Ordinance on Restaurants and Children’s Food Purchases, 2011–2012", "text": "【0】Volume 11 — July 17, 2014\n\n【1】### Article Tools\n\n【2】*   PDF - 629 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Impact of San Francisco’s Toy Ordinance on Restaurants and Children’s Food Purchases, 2011–2012\n===============================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Jennifer J. Otten, PhD, RD; Brian E. Saelens, PhD; Kristopher I. Kapphahn, MS; Eric B. Hekler, PhD; Matthew P. Buman, PhD; Benjamin A. Goldstein, PhD; Rebecca A. Krukowski, PhD; Laura S. O’Donohue, BS; Christopher D. Gardner, PhD; Abby C. King, PhD\n\n【8】_Suggested citation for this article:_ Otten JJ, Saelens BE, Kapphahn KI, Hekler EB, Buman MP, Goldstein BA, et al. Impact of San Francisco’s Toy Ordinance on Restaurants and Children’s Food Purchases, 2011–2012. Prev Chronic Dis 2014;11:140026. DOI: http://dx.doi.org/10.5888/pcd11.140026 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Introduction**  \nIn 2011, San Francisco passed the first citywide ordinance to improve the nutritional standards of children’s meals sold at restaurants by preventing the giving away of free toys or other incentives with meals unless nutritional criteria were met. This study examined the impact of the Healthy Food Incentives Ordinance at ordinance-affected restaurants on restaurant response (eg, toy-distribution practices, change in children’s menus), and the energy and nutrient content of all orders and children’s-meal–only orders purchased for children aged 0 through 12 years.\n\n【12】**Methods**  \nRestaurant responses were examined from January 2010 through March 2012. Parent–caregiver/child dyads (n = 762) who were restaurant customers were surveyed at 2 points before and 1 seasonally matched point after ordinance enactment at Chain A and B restaurants (n = 30) in 2011 and 2012.\n\n【13】**Results**  \nBoth restaurant chains responded to the ordinance by selling toys separately from children’s meals, but neither changed their menus to meet ordinance-specified nutrition criteria. Among children for whom children’s meals were purchased, significant decreases in kilocalories, sodium, and fat per order were likely due to changes in children’s side dishes and beverages at Chain A.\n\n【14】**Conclusion**  \nAlthough the changes at Chain A did not appear to be directly in response to the ordinance, the transition to a more healthful beverage and default side dish was consistent with the intent of the ordinance. Study results underscore the importance of policy wording, support the concept that more healthful defaults may be a powerful approach for improving dietary intake, and suggest that public policies may contribute to positive restaurant changes.\n\n【15】Top of Page\n\n【16】Introduction\n------------\n\n【17】The marketing of unhealthful foods and beverages through toys and other incentives contributes to the development of unhealthy eating patterns and behaviors that lead to child obesity (1,2). In 2009, 10 of the top fast food restaurant chains spent $341 million to acquire toys to distribute with children’s meals (a combination of entrée, side dish, and beverage that is targeted to children through marketing or age restrictions) (3). During the same year, these restaurants sold slightly more than 1 billion meals with toys to children aged 0 through 12 (3). A study evaluating the content of children’s meals at the top 50 restaurant chains found that 97% of children’s meal combinations did not meet evidence-based child nutrition standards and that 56% did not have a single children’s meal meeting these standards (4).\n\n【18】Public health leaders have called for policy-driven strategies aimed at improving food environments and marketing practices (1,5,6). On December 1, 2011, San Francisco, California, became the first US city to enact an ordinance prohibiting restaurants from giving away free toys or other incentives with children’s meals or with foods and beverages not meeting minimal nutritional criteria (Box) (7). The intent of the ordinance was “to improve the health of children and adolescents in San Francisco by setting healthy nutritional standards for children’s meals sold at restaurants in combination with free toys or other incentive items” (8).\n\n【19】### Box. Summary of the Healthy Food Incentives Ordinance (No. 290–10), San Francisco, California.\n\n【20】  \nThe San Francisco County ordinance prohibits restaurants in the county from giving away free toys or other incentive items, such as games, trading cards, admission tickets, or other consumer products —whether physical or digital — with children’s meals (ie, any combination of food items offered together for a single price) that exceed the following criteria for each meal:\n\n【21】*   600 calories\n*   35% of total calories from fat, 10% of total calories from saturated fat (with some exceptions for sources of total and saturated fat), and/or 0.5 grams of trans fat\n*   640 mg of sodium\n\n【22】In addition, the meal must include at least 0.5 cups of fruit and 0.75 cups of vegetables (unless breakfast, which must include at least 0.5 cups of fruit or 0.5 cups of vegetables)\n\n【23】Single food and beverage items must not exceed\n\n【24】*   35% of total calories from fat\n*   10% of total calories from added sweeteners\n\n【25】The ordinance allows free toys or other incentives with foods, beverages, and meals that meet these criteria. The criteria were based on nutrient recommendations and standards for children provided by the Institute of Medicine of The National Academies and the US National School Lunch Standards (K–6) (9,10).\n\n【26】Few ordinances of this type have been enacted or evaluated. The first US county-level ordinance of this type was implemented in Santa Clara County (no. NS-300–820) (11). Although this ordinance applied to only a few restaurants in unincorporated areas of the county, research suggested it improved restaurant food environments and practices (12). Another study found that children were significantly more likely to choose a more healthful meal among an array of healthful and less healthful meals when only more healthful meals were offered with a toy (13).\n\n【27】The objective of this evaluation was to examine the impact of the San Francisco Healthy Food Incentives Ordinance at ordinance-affected fast food chain restaurants on restaurant response and the energy and nutrient content of all orders and children’s-meal–only orders purchased for children.\n\n【28】Top of Page\n\n【29】Methods\n-------\n\n【30】### Study design\n\n【31】The San Francisco Healthy Food Incentives Ordinance (also referred to as the “toy ordinance”) was adopted on November 23, 2010, and enacted on December 1, 2011. Our study used a pre–post design to evaluate this ordinance, and data were collected on local and national restaurant responses (eg, marketing environment, menu changes) and from caregiver/child dyads who were interviewed at two points before ordinance enactment and at one point after enactment. Pre-ordinance time point 1 was from January through March 2011, immediately after adoption and 9 to 12 months before enactment; pre-ordinance time point 2 was from October through November 2011, one or two months before enactment; and post-ordinance was from January through March 2012, one to three months after enactment and a seasonal match with pre-ordinance time point 1.\n\n【32】### Study setting and population\n\n【33】The ordinance applied to all restaurants in San Francisco that sold children’s meals with toys or other incentives (n = 98). We narrowed the sample to 2 global restaurant chains (Chain A \\[n = 19\\]; Chain B \\[n = 11\\]) that operated in San Francisco during the study period. This decision was based on budget constraints and research (12,14) showing that caregiver/child dyads felt that toys affected purchases most strongly at major restaurant chains, a finding supported by research showing that these two chains market toys with children’s meals to children more frequently than do other chains (15). We then excluded two Chain A restaurants because of lack of public space in which to conduct the street-intercept surveys.\n\n【34】In 2009, San Francisco was the 12th largest city (population, ~1 million); the population was 58% white, 31% Asian, 7% black; and 14% Hispanic (16). In 2010, 32% of children in San Francisco were obese or overweight (17). Our target study population was parents or caregivers with children (aged 0–12 y) who were customers of the restaurants.\n\n【35】### Data collection\n\n【36】**Changes in children’s meals at the local and national level.** Because there were many ways a restaurant could comply with the ordinance and it was unknown how restaurants would comply, we tracked local changes in menus and restaurant practices by reviewing local press releases, by direct observation, and by test purchases by trained staff. We tracked menus and restaurant practices at the national level by reviewing items in the news media and press releases, announcements, and menus posted on corporate websites. All children’s meal combinations (each combination of entrée, side dish, and beverage) were evaluated for nutritional content and whether they met ordinance criteria by using information from corporate websites.\n\n【37】**Street-intercept survey.** During 8 weekends of each of the three study time points, a team of 4 to 6 trained research personnel collected data during the lunch hour (generally 11:00 am to 3:00 pm) through street-intercept surveys of parent–caregiver/child dyads. Data collection occurred only on weekends to reduce the potential effect of school being in or out of session.\n\n【38】Research personnel were randomly assigned a set of restaurants and instructed to collect data at each location until they collected 10 surveys or 5 hours of data, whichever occurred first; restaurants where 10 surveys had been collected in less than 5 hours were revisited until 5 hours of data were accumulated.\n\n【39】We used a street-intercept survey method to invite parents or caregivers to participate in the study, a method used in studies on menu-labeling legislation (18,19). Parents or caregivers with children were approached as they entered the restaurant on foot and were offered a $5 restaurant gift card to complete a 10- to15-minute interviewer-administered survey after they completed their purchases. Participants were informed that the research topic was child food purchases and were asked to provide their receipt; no mention was made of the ordinance at interview initiation. The study protocol was approved by the Stanford University institutional review board; parents and caregivers provided verbal consent.\n\n【40】The street-intercept survey collected data on\n\n【41】*   Eating-out habits and purchase priorities (eg, frequency of fast food visits by month, reason for choosing this restaurant);\n*   Habits, priorities, and requests related to children’s meals and toys (eg, frequency a children’s meal is chosen, importance of and request for toys);\n*   Awareness of the toy ordinance and calorie labels; and\n*   Foods and beverages ordered and demographic and anthropometric data (ie, birthdate, sex, race/ethnicity, and height and weight) for both parent/caregiver and child.\n\n【42】We collected surveys from 863 dyads at three time points (pre-ordinance 1 \\[n = 296\\]; pre ordinance 2 \\[n = 286\\]; post-ordinance \\[n = 281\\]). We excluded 101 surveys for the following reasons: survey was missing information on food orders or restaurant location (n = 58), food orders could not be verified (n = 14), or the children surveyed were older than 12, the parent or caregiver was younger than 18, or data on age were missing (n = 29). The final survey sample consisted of 762 dyads.\n\n【43】### Nutritional analysis of food and beverage purchases\n\n【44】A nutritional database was created for menu items available during each time point using data provided by corporate websites. For special or limited-time items, data were often unavailable. When data were unavailable, items were verified for existence, and nutritional information was often found in an announcement or press release. If an item could not be verified or adequate nutrition information found, we excluded the survey that mentioned the item. If the size of an item was not clear, we assumed it was the smallest size available. If details of a customization (eg, extra cheese, sauce, salad dressing, or beverage type) were not clear, we used average energy and nutrient values for all customization items in the same category. The following variables were calculated for each item purchased and aggregated by order per person, including customizations: calories, fat, saturated fat, trans fat, sodium, sugar, fiber, and protein. As an a priori decision, we based data for orders on self-report from parents or caregivers rather than receipts because our pilot work at Santa Clara County fast food restaurants found that receipts were incorrect (eg, inaccurate sandwich types) or contained insufficient information (eg, type of sauce or beverage) in 50% of test purchases conducted by trained staff.\n\n【45】### Statistical analysis\n\n【46】All analyses were conducted with R 3.1 \\[R Foundation for Statistical Computing, Vienna, Austria\\]. The main outcomes of interest were differences across time in mean energy content (kcal) per purchase for all children surveyed and for children ordering only children’s meals. Outliers were verified for plausibility and validity by cross-checking with surveys; all were plausible and valid. Because energy data were skewed, we used a nonparametric Kruskal–Wallis test to analyze the main outcomes overall and stratified by race/ethnicity, restaurant, and meal components (ie, entrée, side dish, and beverage). Multivariable regression analyses were performed but did not change inferences. We used Mann–Whitney _U_ tests to test comparisons between time points, descriptive statistics to characterize study participants and survey responses, and χ <sup>2 </sup> tests to test for differences in categorical variables across time points.\n\n【47】Top of Page\n\n【48】Results\n-------\n\n【49】Both restaurant chains responded to enactment of the toy ordinance by using the same compliance strategy: offering toys for an additional 10 cents with the purchase of a children’s meal. When restaurants began doing so, 88% of those in our sample purchasing children’s meals purchased the toy.\n\n【50】### Changes in children’s meals\n\n【51】No children’s meals at either chain met the ordinance nutritional criteria at any time during the study. However, changes at both chains during the study period affected the nutritional content of children’s meals.\n\n【52】**Chain A.** Beginning in September 2011 (after pre-ordinance time point 1 but before pre-ordinance time point 2) and only in California markets (20), Chain A did the following: one, revised the default side dish of its smaller child’s meal from 2.4 ounces of french fries and no fruit to 1.1 ounces of french fries and 1.2 ounces of apple slices (a reduction of 110 kcal, 6 g fat, and 130 mg sodium); two, revised the default side dish of its larger child’s meal from 2.4 ounces of french fries and no fruit to 2.4 ounces of french fries and 1.2 ounces of apple slices (an addition of 15 kcal); and three, stopped serving caramel sauce with its apple slices (a reduction of 70 kcal, 0.5 g fat, 9 g sugar, and 35 mg sodium). In addition, Chain A started offering fat-free chocolate milk in lieu of 1% chocolate milk (as a beverage option, not a default; a reduction of 40 kcal and 3 g fat and addition of 20 mg sodium) (20). These changes were confirmed through direct observation or test purchases. It was unclear whether the new default side dishes affected consumer purchases. Post-hoc analyses examining the proportion who received the default side dish among those ordering children’s meals showed a decrease from pre-ordinance time point 1 to pre-ordinance time point 2 coincident with Chain A menu changes (from 89% to 67%, _P_ \\= .003), although no difference was found between seasonally matched pre-ordinance time point 1 and post-ordinance (from 89% to 83%, _P_ \\= .49). (20).\n\n【53】**Chain B.** On July 31, 2011 (after pre-ordinance time point1 but before pre-ordinance time point 2), Chain B announced it would join the National Restaurant Association’s Kids LiveWell program (21). As part of the program, Chain B announced that soda and french fries would no longer be default in children’s meals but that employees would verbally offer all beverage and side dish options (22). In test purchases during pre-ordinance time point 2, alternative beverages and side dishes were not offered and apple slices with caramel continued to be sold for an extra cost (which varied by location) compared with french fries. However, in post-ordinance test purchases, Chain B employees offered beverage and side alternatives for children’s meals. We also observed at post-ordinance and validated by reviewing corporate menus that Chain B stopped serving caramel sauce with its children’s apple slices (a reduction of 45 kcal, 1g fat, 5g sugar, and 35 mg sodium).\n\n【54】### Street-intercept surveys\n\n【55】We found no significant demographic differences across time points, except for race/ethnicity ( Table 1 ). Because we found no significant difference in calories per order by race/ethnicity between or among time points, we did not control for race/ethnicity in the primary analyses. Significantly fewer children’s meals were purchased at pre-ordinance time point 2 than at pre-ordinance time point 1 or at post-ordinance ( _P_ \\= .01), but the number of children meals purchased during the seasonally matched time points were not different ( _P_ \\= .79).\n\n【56】Other exploratory factors potentially affected purchases and toy-related behaviors. Awareness of the ordinance among dyads was moderately high at pre-ordinance time point 1 (45.2%) and decreased thereafter (29.4% at time point 2 and 17.7% at post-ordinance, _P_ < .001) (Table 1). The mean frequency of eating at fast food restaurants varied significantly across time (pre-ordinance time point 1, 5.4 times per month; pre-ordinance time point 2, 4.5 times per month; post-ordinance, 4.8 times/month; _P_ \\= .02). Caregivers reported “taste” as a factor influencing purchase of a children’s meal less often over time ( _P_ \\= .06) and “child-sized portions” more often over time ( _P_ < .001) ( Table 2 ).\n\n【57】Because both restaurant chains responded the same way to the toy ordinance — by charging an additional 10 cents for a toy but not by changing menus to meet ordinance criteria — data were aggregated across restaurants in the primary analyses. The difference in calories per order over time for all children’s orders was not significant ( _P_ \\= .20) ( Table 3 ). However, we found a significant decrease in calories per order over time for children’s meals only ( _P_ < .001). Further analysis (Figure) showed a significant decrease from pre-ordinance time point 1 to pre-ordinance time point 2 and from pre-ordinance time point 1 to post-ordinance (both _P_ < .001) but no significant difference between pre-ordinance time point 2 and post-ordinance ( _P_ \\= .82).\n\n【58】**Figure.** Total calories per order (n = 335) before and after enactment of the San Francisco Healthy Food Incentives ordinance, for children ordering a children’s meal at 2 national restaurant chains in San Francisco, 2011–2012. Mann–Whitney _U_ tests were used to test for comparisons between time points. The horizontal line in the middle of each box indicates the median, and the top and bottom borders of the box mark the 75th and 25th percentiles, respectively. The upper whisker extends to the largest data point within 1.5 IQR of the upper quartile while the lower quartile extends to the smallest data point within 1.5 IQR of the lower quartile. Points beyond the whiskers are shown as dots. Abbreviations: Pre 1, pre-ordinance time point 1; Pre 2, pre-ordinance time point 2; Post, post-ordinance time point. Abbreviation: IQR, interquartile range. \\[A tabular version of this figure is also available.\\]\n\n【59】Because of significant main effects, children’s meal–only orders were disaggregated first by restaurant chain and then by entrée, side dish, beverage, and dessert and tested for differences ( Table 4 ). Post-hoc analyses showed a significant decrease in calories per order over time for Chain A ( _P_ < .001) but no change for Chain B ( _P_ \\= .43). Further analysis of Chain A entrées, side dishes, and beverages showed no difference over time in calories, total fat, or sodium for children’s entrées and no difference in calories or sodium for beverages. However, we found a significant decrease over time in calories, total fat, and sodium for side dishes and in total fat for beverages (due to a substitution of a nonfat beverage for a low-fat beverage).\n\n【60】Top of Page\n\n【61】Discussion\n----------\n\n【62】This study examined the impact of the San Francisco Healthy Food Incentives Ordinance on restaurant response and child food and beverage orders in a natural experiment at global Chain A and B restaurants before and after ordinance enactment. Both restaurant chains used the compliance strategy of offering toys for an additional 10 cents with the purchase of a children’s meal, and neither changed their menus to meet ordinance criteria (24). This compliance strategy did not significantly reduce the proportion of children receiving toys with children’s meals. The restaurants were able to comply in this way because of ordinance language which prohibited only the giving away of _free_ toys or other incentives with foods, beverages, and meals not meeting nutritional criteria (8). This language allowed toys to be sold separately.\n\n【63】Restaurants in Santa Clara County responded differently to a similar ordinance, which did not contain the word “free” (12). Santa Clara County restaurants changed their toy distribution and marketing practices in positive ways, including improving the marketing of more healthful children’s meals and discontinuing the giveaway of toys with children’s meals not meeting ordinance criteria or altogether (12). Ordinance wording is a critical element to the outcomes of public health policies (25).\n\n【64】We found a significant decrease in calories per order for children purchasing children’s meals just before ordinance enactment. This decrease appeared to result from menu changes at Chain A from a less healthful to a more healthful default side dish. Additionally, because of the more healthful default side dish and the substitution of a fat-free beverage for a low-fat beverage, we found a significant decrease in total fat and sodium over time in children’s-meal–only orders. These results support research suggesting that more healthful default options may be a powerful approach for improving individual behavior (26).\n\n【65】This study contributes to the limited research on the effects of public health policies aimed at improving restaurant food environments. Strengths of the study are a quasi-experimental design in which restaurants were surveyed before and after enactment of an ordinance at two global fast food chains; the real-world setting; the use of multiple methods of evaluation; and the seasonally matched time points. Additionally, San Francisco has a racially/ethnically diverse population and a large proportion of overweight children. Limitations of the study include the use of self-reported data on food and beverages, height, weight, and other variables that are sensitive to bias (27). Measuring height and weight was not feasible in this setting. Although self-report of food and beverage purchases may introduce response bias, the use of receipts can introduce other inaccuracies or uncertainty. To reduce bias, interviewers were trained to systematically collect food and beverage data, and collection was monitored weekly. Another limitation was the short duration of the study (1 year) and small sample size. The street-intercept method also may have introduced response bias; those willing to be surveyed may have differed from those not willing. Finally, the data are limited to San Francisco, and the external validity of the findings is not known.\n\n【66】This study illustrates two major findings: one, the wording of regulations and policies can have significant impacts on compliance, and two, more healthful default menu options can have a rapid and positive impact on the healthfulness of food and beverage purchases. Moreover, although Chain A menu changes may not have been caused by the ordinance, the transition to a more healthful default side dish and fat-free beverage option were consistent with the ordinance intent. Finally, a valuable aspect of these types of changes is their potential reach: Chain A’s transition to a more healthful default offering, which began in September 2011 only in California, was reported by Chain A to be diffused to all US restaurants by the spring of 2012 (20).\n\n【67】Top of Page\n\n【68】Acknowledgments\n---------------\n\n【69】This project was supported by The Obesity Society Early-Career Research Grant awarded to Dr Otten and by Robert Wood Johnson Foundation grant no. 68301 awarded to Dr King.\n\n【70】Top of Page\n\n【71】Author Information\n------------------\n\n【72】Corresponding Author: Jennifer J. Otten, PhD, RD, Assistant Professor, University of Washington School of Public Health, Nutritional Sciences Program, Box 353410, Seattle, WA 98115. Telephone: 206-221-8233. E-mail: jotten@uw.edu .\n\n【73】Author Affiliations: Brian E. Saelens, University of Washington and Seattle Children’s Research Institute, Seattle, Washington; Kristopher I. Kapphahn, Benjamin A. Goldstein, Laura S. O’Donohue, Christopher D. Gardner, Abby C. King, Stanford University School of Medicine, Stanford, California; Eric B. Hekler, Matthew P. Buman, School of Nutrition and Health Promotion, Arizona State University, Phoenix, Arizona; Rebecca A. Krukowski, University of Tennessee Health Science Center, Memphis, Tennessee.\n\n【74】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71bfd44e-4568-4e93-a0ca-1cfa7c8723dd", "title": "The Association Between Insurance Status and Cervical Cancer Screening in Community Health Centers: Exploring the Potential of Electronic Health Records for Population-Level Surveillance, 2008–2010", "text": "【0】Volume 10 — October 24, 2013\n\n【1】### Article Tools\n\n【2】*   PDF - 542 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】The Association Between Insurance Status and Cervical Cancer Screening in Community Health Centers: Exploring the Potential of Electronic Health Records for Population-Level Surveillance, 2008–2010\n=====================================================================================================================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Stuart Cowburn, MPH, MS; Matthew J. Carlson, PhD; Jodi A. Lapidus, PhD; Jennifer E. DeVoe, MD, DPhil\n\n【8】_Suggested citation for this article:_ Cowburn S, Carlson MJ, Lapidus JA, DeVoe JE. The Association Between Insurance Status and Cervical Cancer Screening in Community Health Centers: Exploring the Potential of Electronic Health Records for Population-Level Surveillance, 2008–2010. Prev Chronic Dis 2013;10:130034. DOI: http://dx.doi.org/10.5888/pcd10.130034 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Introduction**  \nCervical cancer incidence and mortality rates in the United States have decreased 67% over the past 3 decades, a reduction mainly attributed to widespread use of the Papanicolaou (Pap) test for cervical cancer screening. In the general population, receipt of cervical cancer screening is positively associated with having health insurance. Less is known about the role insurance plays among women seeking care in community health centers, where screening services are available regardless of insurance status. The objective of our study was to assess the association between cervical cancer screening and insurance status in Oregon and California community health centers by using data from electronic health records.\n\n【12】**Methods**  \nWe used bilevel log-binomial regression models to estimate prevalence ratios and 95% confidence intervals for receipt of a Pap test by insurance status, adjusted for patient-level demographic factors and a clinic-level random effect.\n\n【13】**Results**  \nInsurance status was a significant predictor of cervical cancer screening, but the effect varied by race/ethnicity and age. In our study uninsured non-Hispanic white women were less likely to receive a Pap test than were uninsured women of other races. Young, uninsured Hispanic women were more likely to receive a Pap test than were young, fully insured Hispanic women, a finding not previously reported.\n\n【14】**Conclusion**  \nElectronic health records enable population-level surveillance in community health centers and can reveal factors influencing use of preventive services. Although community health centers provide cervical cancer screening regardless of insurance status, disparities persist in the association between insurance status and receipt of Pap tests. In our study, after adjusting for demographic factors, being continuously insured throughout the study period improved the likelihood of receiving a Pap test for many women.\n\n【15】Top of Page\n\n【16】Introduction\n------------\n\n【17】Cervical cancer is a significant public health challenge in the United States. Approximately 12,000 women were expected to receive a diagnosis of cervical cancer in 2012, and 4,220 were expected to die of the disease (1). Cervical cancer incidence and mortality rates decreased 67% during the past 3 decades, and most of the decrease is attributed to timely screening and early detection through use of the Papanicolaou (Pap) test. However, early detection efforts are hindered by disparities in cervical cancer screening (2,3). In the general population, 1 factor consistently associated with such disparities is health insurance. Lower rates of cervical cancer screening among uninsured women than among insured women persist across racial, age, and economic groups (4–12).\n\n【18】Community health centers (CHCs) play an important role in addressing disparities in cervical cancer screening. CHCs serve the primary health care needs of over 20 million people in the United States, 38% of whom are uninsured (13). Cervical cancer screening is available to women seeking care in CHCs regardless of their insurance status or ability to pay. As a consequence, disparities in cervical cancer screening among CHC patients related to insurance status should be diminished. However, few studies have investigated the association between insurance status and use of preventive health care services in CHC populations, in part because of a lack of available data. The adoption of electronic health records (EHRs) by some CHCs has now made such research feasible.\n\n【19】We used data from EHRs to assess the association between cervical cancer screening and insurance status in a population of CHC patients. Our objective was to acquire a better understanding of factors affecting use of preventive health care services by CHC patients to inform efforts to reduce health disparities in underserved communities.\n\n【20】Top of Page\n\n【21】Methods\n-------\n\n【22】This cross-sectional study involved secondary analysis of EHR data for a clinic-based population sample. Data were supplied by OCHIN, a nonprofit organization that hosts networked EHRs for CHCs in 13 states. Patient records for women attending 17 clinics located in Oregon and California were retrieved by electronic query from the OCHIN EHR database. These clinics, operated by 5 CHC organizations, were selected because they had both administrative and clinical records available for the entire study period, 2008 through 2010. The study protocol was approved both by the research review committee of OCHIN’s Practice-Based Research Network, which includes CHC representatives, and by the Oregon Health and Science University Institutional Review Board.\n\n【23】### Subject selection\n\n【24】Eligible subjects were women who were aged 24 to 64 years in 2010, who had made 1 or more medical visits at a study clinic during 2010, and, to control for the effects of having a usual source of care (12), at least 1 visit in or before 2008. Patients with a documented history of hysterectomy were excluded. Pregnancy was considered a potential confounder because Pap tests are often administered as part of routine prenatal care and because pregnancy can facilitate access to health insurance. Consequently, subjects who were pregnant during the study period were also excluded.\n\n【25】The outcome of interest was receipt of cervical cancer screening from 2008 through 2010. Receipt of screening was defined by evidence of a completed order for a Pap test in the patients’ EHR. Pap tests performed within 9 months of a prior Pap smear abnormality or related diagnosis of a cervical abnormality were considered diagnostic rather than screening tests, and excluded from the analysis (14). The primary independent variable was health insurance coverage as a percentage of time covered during the period 2008 through 2010. Percentage of time covered was quantified by identifying periods of insurance coverage from the OCHIN database, summing the total number of days with coverage, and dividing by 1,094 days (3 years). To ensure adequate sample sizes for statistical analysis, subjects were classified into only 3 categories: “continuously insured” if they had coverage for 100% of the study period, “continuously uninsured” if they had no coverage during the study period, and “partially insured” if they had coverage for 1% to 99% of the study period. Most insurance records included start and end dates for coverage periods. If the end date was missing, coverage was assumed to have lasted 3 months (15). Insurance coverage that would not pay for cervical cancer screening (eg, workers compensation) was excluded. Periods of coverage totaling less than 7 days were considered administrative errors and excluded.\n\n【26】Covariates included age, race/ethnicity, and household income as a percentage of the federal poverty level (FPL). Selection and categorization of covariates were based on availability of data and informed by previous studies examining use of cervical cancer prevention services (3,5,9,16). The woman’s age, calculated on January 1, 2008, was dichotomized as 21 to 39 years and 40 to 64 years. A combined race/ethnicity variable was generated as follows: women who ever identified as Hispanic or primarily Spanish-speaking were considered Hispanic; among non-Hispanic patients, women who always identified as non-Hispanic white were considered non-Hispanic white; women who met neither of these criteria were classified as non-Hispanic other. Household income as a percentage of FPL was averaged for each participant over the study period, excluding missing records and records with values over 1000%, which were considered erroneous. Categories of 0% to 99% of FPL and 100% or more of FPL were used in analyses.\n\n【27】### Statistical methods\n\n【28】Analysis was restricted to subjects with complete information for all covariates. Descriptive statistics were generated and χ <sup>2 </sup> tests performed to examine differences in distribution of sociodemographic covariates by insurance group. A series of univariable and multivariable bilevel log-binomial regression models was used to estimate unadjusted prevalence ratios (PRs) and adjusted prevalence ratios (APRs) and 95% confidence intervals (CIs) for receipt of cervical cancer screening by insurance status. Log-binomial models were preferred over logistic models, as the latter can strongly overestimate relative risk when the outcome of interest is relatively common (prevalence ≥10%) (17). Patient-level factors were modeled as fixed effects at level 1. A clinic-level variable was entered as a random intercept at level 2 to account for the possible interclass correlation of subjects within clinics (18,19).\n\n【29】Variables associated with the outcome at the _P_ < .10 level in univariable models were entered into the multivariable model. Pairwise and 3-way interactions between independent variables were assessed. Backward stepwise selection with an exclusion level of _P_ < .05 was used to identify main effects and interaction terms included in the final model. All analyses were conducted in SAS Enterprise Guide version 9.4 (SAS Institute, Inc, Cary, North Carolina).\n\n【30】Top of Page\n\n【31】Results\n-------\n\n【32】Our final study sample included 11,560 women from the base population of 24,382. We excluded a total of 12,822 women, 1,680 who had a history of hysterectomy, 7,943 who had not had an office visit both in 2010 and during or before 2008, 2,683 who were pregnant during the study period, and 516 whose race/ethnicity or FPL data were missing. Six percent (n = 1,217) of Pap tests identified in the EHR were ordered for diagnostic rather than screening purposes and excluded from the analysis.\n\n【33】Within the study sample, 22.9% of women had no insurance coverage from 2008 through 2010, 33.4% had partial coverage, and 44.8% were continuously covered ( Table 1 ). On average, the partially insured were covered for 55% of the study period (range 7%–99%; data not shown). Slightly more than half of subjects (53.9%) were aged 40 years or older. The sample consisted of 46.9% non-Hispanic white women, 38.3% Hispanic women, and 14.8% women classified as non-Hispanic other. Average household income was less than 100% of FPL for 69% of subjects. A total of 63.5% of women received 1 or more Pap tests for cervical cancer screening from 2008 through 2010, but the proportion differed significantly across insurance groups ( _P_ < .001). Uninsured women were most likely to receive a Pap test (67.6%), followed by the continuously insured (63.9%) and partially insured (60.2%).\n\n【34】Each independent variable was significantly associated with cervical cancer screening in univariable regression models ( Table 2 ). Hispanics were 1.39 (95% CI, 1.34–1.44) times as likely to receive a Pap test as were non-Hispanic whites; women of other races/ethnicities were 1.16 (95% CI, 1.11–1.22) times as likely as non-Hispanic whites _._ Women aged 40 to 64 years were less likely to receive a Pap test than were women aged 21 to 39 years (PR, 0.93; 95% CI, 0.91–0.96). Cervical cancer screening was more common among the uninsured than among the continuously insured (PR, 1.08; 95% CI, 1.04–1.12). There was no significant difference in receipt of Pap tests between partially insured women and continuously insured women. Women with household incomes less than 100% of the FPL were less likely to be screened than were those with household incomes above 100% FPL (PR, 0.95; 95% CI, 0.92–0.98).\n\n【35】In addition to main effects, the final multivariable model for estimating prevalence of cervical cancer screening included 3 pairwise interactions: insurance coverage by age ( _P_ \\= .013), insurance coverage by race/ethnicity ( _P_ \\= .001), and age by race/ethnicity ( _P_ \\= .007), plus one 3-way interaction, age by race/ethnicity by insurance coverage ( _P_ <.0001). Given these interactions, APRs for receipt of a Pap test by insurance status were reported in subgroups defined by race/ethnicity and age (Figure). Among non-Hispanic white women, the uninsured were less likely to receive a Pap test than were the fully insured (APR, 0.80; 95% CI, 0.71–0.91 for ages 21–39 and APR 0.88; 95% CI, 0.79–0.97 for ages 40–64). Younger non-Hispanic white women who were partially insured were also less likely to receive a Pap test than were their peers with continuous insurance (APR, 0.91; 95% CI, 0.84–0.99). Among Hispanic women aged 21 to 39 years, the partially insured were less likely to receive a Pap test than were the continuously insured (APR, 0.91; 95% CI, 0.84–0.99), whereas the uninsured were more likely to receive a Pap test (APR, 1.11; 95% CI, 1.05–1.18) than were the fully insured. Among Hispanic women aged 40 to 64 years, having no insurance lowered likelihood of receiving a Pap test (APR 0.80; 95% CI: 0.75–0.86). There was no significant association between insurance status and cervical cancer screening among women in the non-Hispanic-other category. Household income was an independent predictor of Pap test screening. Women with household income less than 100% of FPL were less likely to be screened than women with income 100% or more of the FPL (APR, 0.95; 95% CI, 0.92–0.97).\n\n【36】**Figure.** Adjusted prevalence ratios (APRs) and 95% confidence intervals (CIs) for receipt of cervical cancer screening, by insurance status and stratified by race/ethnicity and age, for women in selected Oregon and California OCHIN-affiliated community health centers, from 2008 through 2010 (N = 11,560). Adjusted prevalence ratios are significant at the α = .05 level if the 95% confidence interval does not contain 1.00. \\[A tabular version of this figure is also available.\\]\n\n【37】Top of Page\n\n【38】Discussion\n----------\n\n【39】Our study demonstrates how EHRs enable population-level surveillance among the uninsured and underinsured without resorting to cost- and time-intensive patient surveys. Over 2,600 women who had no health insurance from 2008 through 2010 were included in this study, along with 3,856 women sporadically insured during the same period. The analyses performed here could not have been conducted by using claims data, which misses services received during periods without insurance coverage. Other methods of medical chart abstraction would be impractical given the size of the analytic samples.\n\n【40】Overall, 64% of the women in the study population received a Pap test, a lower proportion than reported in the 2010 National Health Interview Survey (83%) (20), the 2005 Health Information National Trends Survey (90%) (21), and 2010 Behavioral Risk Factor Surveillance System survey results for Oregon (80%) or California (87%) (2). Although the results of our study are not directly comparable with survey data, they add to a growing body of evidence indicating that use of patient self-report data can lead to overestimates of cancer screening (14,22–24). In contrast, screening estimates reported here are higher within equivalent age and race/ethnicity categories than described in the only other known example of a large-scale EHR-based cervical cancer screening study focused exclusively on CHCs (16). Data from 10 Florida CHCs showed that 36% of non-Hispanic white women and 64% of Hispanic women received a routine Pap test from 2005 through 2007 (16). The equivalent proportions reported in our study were 53% of non-Hispanic white women and 75% of Hispanic women. Interestingly, no difference in prevalence of Pap testing was observed between insured and uninsured women in Florida (53% in both groups received a Pap test), whereas being uninsured was a positive predictor of cervical cancer screening in unadjusted estimates reported here. The disparate findings may reflect the use of different inclusion criteria in the 2 studies. For example, the Florida study did not limit subjects to established patients. In addition, clinics participating in the Florida study were in different stages of EHR implementation. Therefore, clinical data were not available for the entire study population (16). Such factors may have resulted in less complete data capture and underreporting of Pap tests in Florida, leading to lower screening estimates than reported here. Insurance status was also defined differently in the 2 studies, potentially contributing to the difference in findings.\n\n【41】In adjusted models, being uninsured lowered the likelihood of receiving a Pap test more for non-Hispanic white women than for Hispanic women and those of other races/ethnicities. Similar results have been reported previously (25), and other studies found lower risks of advanced-stage cervical cancer among uninsured and Medicaid-insured Hispanic women than among similarly insured non-Hispanic white women (26). Such findings have been interpreted to suggest that minority women are more skilled than non-Hispanic white women at accessing free or subsidized screening services offered in safety-net clinics (25,26). Alternatively, the clinics studied here might have cervical cancer screening programs that are particularly effective at reaching underinsured minorities. For example, some or all of the clinics in this study may have participated in the National Breast and Cervical Cancer Early Detection Program (NBCCEDP) from 2008 through 2010, which offers Pap tests to underserved and underinsured women. Although it was not possible to assess the proportion of the study population who were screened through such programs, it is conceivable that initiatives such as the NBCCEDP reduced or even reversed disparities in screening between insured and uninsured or underinsured women, and between women in different age and race/ethnicity groups, that might otherwise have been more evident. Lower rates of Pap test screening among uninsured non-Hispanic white women in this study population may also reflect particular access-to-care barriers specific to this group. For example, non-Hispanic white women in the study include many recent immigrants from Eastern Europe who have diverse linguistic and cultural backgrounds. Linguistic and cultural factors have been previously observed to influence use of preventive health services in primary care settings (27).\n\n【42】The most intriguing finding in this analysis was that uninsured Latinas aged 21 to 39 had a higher likelihood of cervical cancer screening than their peers who were continuously insured throughout this study. While the effect was modest (APR, 1.11; 95% CI, 1.05–1.18), no other examples were found in the literature that document higher Pap test rates for the uninsured than for the fully insured, when controlling for other demographic factors. The result is unlikely to be explained by NBCCEDP participation, because the program is aimed almost exclusively at women aged 40 years or older. It is possible, however, that the study clinics may have concurrently participated in state, county, or CHC-led programs that emphasized cervical cancer screening for younger uninsured Hispanic women, thus contributing to the observed findings. A more detailed analysis of screening rates by clinic may provide useful insight into factors underlying this result. Alternatively, the elevated screening rate among uninsured younger Latinas may be linked to the women’s country of birth. Rodríguez et al (5) found that after adjusting for confounding variables, foreign-born status was positively associated with receiving a Pap test and mammography among Latinas in California. Moreover, foreign-born Latinas were found to have disproportionately lower rates of insurance compared with US-born Latinas and non-Hispanic whites (5). If the uninsured younger Latinas we studied were more likely to be foreign-born than insured Latinas, such a relationship might explain the findings. Additional research on subpopulations within the analytic sample would be required to identify associations between country of birth and screening use. Although beyond the scope of this study, such work could inform public health strategies to improve cervical cancer screening rates among the diverse population of women seen in CHCs.\n\n【43】This study has several limitations. First, data were limited to selected CHCs in Oregon and California, so the results may not be generalizable. Second, receipt of Pap tests was identified via search algorithms with commonly used procedure and diagnostic codes, and a small percentage of services may have been missed if coded differently. The potential for this type of error is probably minor because search algorithms were based on validated scripts developed for federal reporting to the Uniform Data System (28). Automated data extraction from EHRs has also demonstrated good-to-excellent agreement with manually extracted data, with electronic methods having notably larger case capture (29).\n\n【44】Third, no information was available regarding Pap tests received outside the OCHIN CHC network. Consequently, Pap test screening in the study population may be underestimated. The results may also be biased if patients more likely to seek care outside the study clinics were disproportionally distributed among the insurance groups. This could explain some of the differences observed. To minimize the potential for underreporting, the study sample was restricted to established patients. Prior research indicates that women having a usual source of care may be less likely to seek routine cervical and breast cancer screenings elsewhere (6,9,12,30,31). The completeness of OCHIN’s preventive service utilization data has also been validated in other studies (32), and fewer services could be expected to be missing from the OCHIN data among the uninsured because persons without coverage have limited options as to where they can access care (32).\n\n【45】Fourth, because of inconsistencies in data collection at clinic sites, duration of coverage by private insurance may have been over- or underestimated, potentially resulting in misclassification of patients by insurance category. To test this possibility, the data were reanalyzed limiting insured patients to Medicaid and Medicare recipients only. Resulting APRs for receipt of Pap tests by insurance coverage were essentially the same, suggesting that misclassification by insurance status is not a major concern. The use of a single category to define partial insurance may also be considered a limitation, potentially masking dose–response associations between time covered and receipt of cervical cancer screening among partially insured subjects. Although beyond the scope of this study, a subanalysis of partially insured subjects using narrower bands of coverage would allow assessment of such trends.\n\n【46】Fifth, regression models did not explicitly account for organizational differences among the CHCs that could influence receipt of cervical cancer screening, such as provider demographics, clinic participation in prevention programs, and scope of services offered. Finally, this report does not address overuse of Pap tests, a topic of emerging interest as the US health care system struggles to contain costs. EHR data have been successfully used to identify Pap tests performed sooner than recommended for women at low risk of cervical cancer (33). With modification to include new screening recommendations and revised eligibility criteria, the algorithms used here could be adapted to identify such Pap tests, informing efforts to maximize the use of CHCs’ limited resources for provision of appropriate services.\n\n【47】Despite these limitations, this study highlights the utility of EHRs for population-level surveillance among the uninsured and underinsured, demonstrating how EHRs can be leveraged to reveal patterns of preventive health care service use in CHC patients. The results indicate that in this population insurance-related disparities in receipt of cervical cancer screening persist even though Pap tests are available regardless of insurance status. After adjusting for patient demographics, having continuous insurance improved the likelihood of cervical cancer screening for many women studied here. Additional research is warranted to further understand the role of insurance in preventive service uptake among CHC patients.\n\n【48】Top of Page\n\n【49】Acknowledgments\n---------------\n\n【50】The authors acknowledge the support of OCHIN, Inc, and its member organizations that generously allowed access to the data that made this study possible.\n\n【51】Top of Page\n\n【52】Author Information\n------------------\n\n【53】Corresponding Author: Stuart Cowburn, MPH, MS, OCHIN, Inc, 1881 SW Naito Parkway, Portland, Oregon 97201. Telephone: 503-943-2638. E-mail: cowburns@ochin.org .\n\n【54】Author Affiliations: Matthew J. Carlson, Department of Sociology, Portland State University, Portland, Oregon; Jodi A. Lapidus, Jennifer E. DeVoe, Oregon Health & Science University, Portland, Oregon.\n\n【55】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f975c5a1-031b-45d7-9f67-e79fd109cf91", "title": "Norovirus and Gastroenteritis in Hospitalized Children, Italy", "text": "【0】Norovirus and Gastroenteritis in Hospitalized Children, Italy\nNoroviruses (NoVs) were the first viruses to be clearly associated with acute gastroenteritis, but for many years, knowledge of their role in infection and disease has been limited ( _1_ ). The introduction of the reverse transcription–PCR (RT-PCR) method defined the relevant role of these agents in outbreaks and sporadic cases of gastroenteritis throughout the world and showed the broad heterogeneity and rapid evolution of NoV strains ( _2_ _–_ _5_ ).\n\n【1】Italy has no surveillance system for nonbacterial gastroenteritis. Recently 2 outbreaks of confirmed NoV gastroenteritis were reported ( _6_ _,_ _7_ ). With regard to sporadic enteritis in Italian children, the few studies performed have reported prevalence rates from 2.1% to 18.6% ( _8_ _–_ _11_ ).\n\n【2】### The Study\n\n【3】This report aims to describe the clinical and epidemiologic features of NoV infection in children hospitalized for enteritis and to compare the severity score related to the different viral agents and NoV genotypes. From January to December 2004, 390 fecal specimens were obtained from 365 children with acute gastroenteritis within 24 hours of admission to the Department of Infectious Diseases at the G. Di Cristina Children’s Hospital in Palermo, Italy. Gastroenteritis was defined as \\> 3 stools that were looser than normal stools per day or 1 episode of vomiting. Demographic and clinical data were collected for most patients. A 14-point scoring system was used to summarize the clinical severity of the cases ( Table 1 ). All the specimens were examined for the presence of _Salmonella_ spp., _Shigella_ spp., _Campylobacter_ spp., and _Yersinia_ spp. One hundred ninety-nine samples negative for bacteria and from 192 children (100 boys and 92 girls; median age 11.75 months) were examined for NoVs, group A rotaviruses (HRVs), adenoviruses (AdVs), and astroviruses (HAstVs). NoV detection was carried out by single-step or nested RT-PCR ( _12_ ). Positive and negative controls were included in all amplification reactions, and contamination of reactions by PCR products was avoided by strict separation of working areas and the use of filter-plugged pipette tips. The genotyping of NoV strains was obtained by sequence analysis performed on the RNA-dependent RNA polymerase gene and the sequences were aligned and compared with a selection of representative sequences from the various NoV genotypes available in online databases ( _12_ ).\n\n【4】HRV, AdV, and HAstV were detected by enzyme immunoassays (EIAs) (IDEIA Rotavirus, IDEIA Adenovirus, and Amplified IDEIA Astrovirus; DakoCytomation, Angel Drove, UK). AdV-positive specimens were tested for enteric subgenus F serotypes 40 and 41 with the Premier Adenoclone-type 40–41 EIA (EIA Cambridge Bioscence, Worcester, MA, USA). HAstV-positive samples were confirmed by RT-PCR ( _10_ ). Statistical analysis was carried out by using the χ <sup>2 </sup> test, and a significance level of 5% was adopted.\n\n【5】NoVs were detected in 93 (48.4%) of 192 patients, and at least 1 of the gastroenteric viruses tested was found in 148 (77.1%) patients. Among 148 patients positive for enteric viruses, NoVs were the only viruses detected in 58 (39.2%), while 1 or 2 more viruses were present in 35 (23.6%) ( Table 2 ). Of the 93 NoVs-positive patients, 74 (79.6%) were detected after the first PCR step, and 19 (20.4%) after the nested-PCR step. A total of 36 RT-PCR amplicons, 28/74 first step–positive and 8/19 nested-positive, were submitted for sequencing.\n\n【6】HRVs were identified in 80 (41.6%) of 192 patients, AdVs in 8 (4.2%) patients, with 1 (0.5%) strain belonging to serotype 40/41, and HAstVs in 5 (2.6%) patients. Overall, single viral infections were found in 112 (58.3%) of 192 patients; double viral infections were detected in 34 (17.7%) patients, and 2 (1%) patients were infected with 3 viral agents.\n\n【7】All the NoV strains sequenced were characterized as GGII NoVs and could be attributed to a defined genotype. The 2 predominant strains were GGIIb/Hilversum (44.4%) and GGII.4 Hunter (52.8%); a single strain belonged to the GGII.4 Farmington Hills cluster. Both the GGIIb/Hilversum- and the GGII.4-positive patients were also infected with another virus in 37.5% and 35% of cases, respectively.\n\n【8】NoV infections were detected in almost every month of the year; the highest incidence was recorded from February through May. Most of the GGIIb/Hilversum strains were isolated from February through April; the GGII.4 strains were detected at a higher frequency from January through March and again from October through December.\n\n【9】Children infected by NoVs comprised 46 (49.5%) boys and 47 (50.5%) girls. The median age was 12 months. The median duration of diarrhea was 4 days (range 1–17 days) with a median number of bowel movements per day of 7.5 (range 1–21). Vomiting and fever were present in 46 (49.5%) and 48 (51.6%) children, respectively. Thirty nine (41.9%) children showed signs of dehydration. There was no clinical difference in the median age and in the severity of illness caused by each of the 2 prevalent NoV genotypes (p>0.05).\n\n【10】The severity score in all the groups of infected children is shown in Table 2 . Though not statistically significant (p>0.05), HRVs were associated with the highest severity score among single infections. The severity score for NoV co-infections was higher than that for NoV single infections, except for double infections with HAstV. In the last few years many studies have confirmed the growing importance of NoVs as agents of sporadic enteritis. In Italy, the NoV detection rate in pediatric enteritis appears to be increasing. In 2002 in northern Italy, RT-PCR found NoVs to be the second most common viral agents of enteritis after HRVs, with a rate of 10.4% of single infections ( _11_ ). In our study, NoVs emerged as the principal cause of viral enteritis (p<0.05) responsible for 39.2% of cases of diarrhea positive for at least 1 viral agent.\n\n【11】The 2 predominant NoV genotypes circulating in southern Italy in 2004 were GGII.4 Hunter and GGIIb/Hilversum. The first was identified in Australia during 2002–2004 and was then related to an increase in gastroenteritis outbreaks in the Netherlands; the second appeared in France in 2000 and soon became prevalent in Europe ( _3_ _,_ _4_ _,_ _13_ ). GGIIb/Hilversum has been described as a strain highly prone to recombinational events, and it may play a peculiar role in children ( _3_ _,_ _14_ ). Detection of both GGII.4 and GGIIb/Hilversum NoV genotypes in sporadic cases of gastroenteritis occurring throughout the year in our area demonstrates that 2 distinct NoV strains can be introduced in a local population and be maintained over a long period. Emergence of new genetic variants may be the cause of increasing NoV infections ( _5_ ).\n\n【12】### Conclusions\n\n【13】Our results highlight the need to apply molecular diagnostic tools widely to determine the actual etiology of acute childhood enteritis when the causative agent is not known. This procedure enabled us to define the real prevalence of NoV infection and its frequent occurrence in association with another etiologic agent (18.2%), as reported ( _2_ _,_ _11_ ). The protracted duration of virus shedding reported both in HRV and NoV infections makes it difficult to attribute the main clinical role to one or the other as either may well represent asymptomatic shedding after an early infection ( _15_ ). The lower severity score observed in NoV single infections with regard to both NoV-HRV mixed infections and HRV single infections, suggests that the clinical picture might be dominated by HRVs. However, both viruses could be acting in synergy, and this option might also be contemplated for NoV-AdV co-infections because their score was higher than that of single infections with each of the 2 viruses. To our knowledge, this is the first report relating the clinical picture to NoV genotypes. This study did not show any statistically significant difference in the clinical parameters evaluated in patients infected with GGII.4 or GGIIb/Hilversum types (p>0.05).\n\n【14】In conclusion, NoVs emerged in our area as the main cause of sporadic viral gastroenteritis in hospitalized children during 2004, reaching epidemiologic effects of HRV. Analysis of the genetic variability of NoV permitted confirmation of the changing epidemiologic features of these emerging pathogens.\n\n【15】Dr Colomba is a medical researcher at the Infectious Diseases Department, University of Palermo. Her primary research interests include enteric pathogens and zoonotic diseases such as leishmaniasis and rickettsioses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5fa9a7fd-43a5-461a-8ed6-fcf3be919a9d", "title": "Genomic Characterization of Nipah Virus, West Bengal, India", "text": "【0】Genomic Characterization of Nipah Virus, West Bengal, India\nNipah virus (NiV) causes encephalitis or respiratory signs and symptoms in humans, with high death rates ( _1_ _–_ _4_ ). NiV outbreaks have been reported from Malaysia, Bangladesh, Singapore, and India ( _1_ _,_ _5_ _–_ _13_ ). Cases in humans have been attributed to zoonotic transmission from pigs and bats ( _1_ _,_ _14_ ). We describe a full genome sequence of NiV from an outbreak in India.\n\n【1】### The Study\n\n【2】During April 9–28, 2007, five persons became ill and died within a few days at village Belechuapara, Nadia district, West Bengal, India, which borders Bangladesh. The index case-patient (case-patient 1) was a 35-year-old male farmer addicted to country liquor derived from palm juice. Hundreds of bats were observed hanging from the trees around his residence, which strongly suggested association with the infection of the index case-patient and possibility of contamination of the liquor with bat excreta or secretions.\n\n【3】Three patients were close relatives of case-patient 1: his 25-year-old brother (case-patient 2), his 30-year-old wife (case-patient 3), and his 39-year-old brother-in-law (case-patient 4). They became ill 12, 14, and 14 days, respectively, after contact with case-patient 1. In another person, a 28-year-old man (case-patient 5) who collected blood samples from and performed a computed tomography scan of the brain of case-patient 1, symptoms developed 12 days after contact.\n\n【4】No samples were available from the first 2 case-patients. Brain and lung tissues from case-patient 3, cerebrospinal fluid (CSF) from case-patient 4, and urine and CSF from case-patient 5 were collected. Blood samples were obtained from case-patients 4 and 5 and from 34 asymptomatic contacts from the village. Serum samples from these persons were tested for immunoglobulin (Ig) M and IgG antibodies to NiV (IgM/IgG anti-NiV) with ELISA by using reagents provided by the Centers for Disease Control and Prevention (Atlanta, GA, USA). To detect NiV RNA, urine (250 µL), CSF (100 μL), or autopsied brain or lung tissue (100 mg) were used, and RNA was extracted by using TRIzol LS and TRIzol reagents (Invitrogen Life Technologies, Carlsbad, CA, USA). Nested reverse transcription–PCR (RT-PCR) was conducted by using nucleocapsid (N) gene–based primers ( _8_ ).\n\n【5】Attempts to isolate NiV in Vero E6 cell lines or infant mice were unsuccessful. The full-length genomic sequence was obtained from the lung of case-patient 3 by using 36 sets of primers ( Table A1 ), Superscript II RNase reverse transcriptase for reverse transcription (Invitrogen), and Pfx polymerase for amplification (Invitrogen). The PCR products of predicted molecular size were gel eluted (QIAquick PCR Purification Kit; QIAGEN, Hilden, Germany) and sequenced by using BigDye Terminator cycle sequencing Ready Reaction Kit (Applied Biosystems, Foster City, CA, USA) and an automatic Sequencer (ABI Prism 3100 Genetic Analyzer; Applied Biosystems). PCR products were sequenced in both directions. To determine the genotypic status, phylogenetic analysis was conducted by using partial N gene and full NiV genome sequences with the Kimura 2-parameter distance model and neighbor-joining method available in MEGA version 3.1 software ( www.megasoftware.net ). The reliability of phylogenetic groupings was evaluated by the bootstrap test with 1,000 bootstrap replications.\n\n【6】Patients’ signs and symptoms included high fever (103°F–105°F \\[39.4°C–49.6°C\\]) with and without chills, severe occipital headache, nausea, vomiting, respiratory distress, pain in calf muscles, slurred speech, twitching of facial muscles, altered sensorium, (focal) convulsions, unconsciousness, coma, and death. The first 3 case-patients died within 2–3 days after symptom onset; case-patients 4 and 5 died after 5 and 6 days, respectively. Clinical investigations could be conducted for case-patients 4 and 5. Results of serologic tests for malaria parasite, typhoid, anti-dengue IgM, HIV, and hepatitis B surface antigen were negative. Peripheral blood profiles were within reference limits. Alanine aminotransferase, aspartate aminotransferase, creatine phosphokinase, and C-reactive protein were elevated. Blood gas analysis in case-patient 4 (case-patient 5 values in parentheses) showed oxygen saturation 49.4% (71%), pO <sub>2 </sub> 36.1 mm Hg (44.8 mm Hg), pCO <sub>2 </sub> 44.4mm Hg (44.1 mm Hg), HCO <sub>3 </sub> 15.7 mmol/L (19.1 mmol/L), and pH 7.166 (7.255). Lumbar puncture of case-patient 5 showed opening pressure within reference range (1 drop/second), 2 cells/cm; all cells observed were lymphocytes. Chest radiograph indicated pulmonary edema, which suggested acute respiratory distress syndrome. At the time of admission, computed tomography and magnetic resonance imaging scans of the brain showed no abnormality.\n\n【7】Serum samples from case-patients 4 and 5 were positive for IgM anti-NiV. Of the clinical samples screened, brain and lung tissues of case-patient 3, CSF of case-patient 4, and urine of case-patient 5 were NiV RNA positive. Of the 34 asymptomatic contacts, 1 was positive for IgG anti-NiV and negative for IgM anti-NiV and did not report any major illness in the past. This positivity may reflect a previous subclinical infection or cross-reactivity in ELISA needing further follow-up.\n\n【8】Figure\n\n【9】Figure . A) Phylogenetic analysis based on partial nucleocapsid (N) gene nucleotide sequences (159 nt, according to Nipah virus \\[NiV\\] Bangladesh sequence, GenBank accession no. AY988601, 168–327 nt) of the 4 NiVs sequenced...\n\n【10】Before this report, similar cases had not been reported from the village or the surrounding area. Partial N gene sequences confirmed NiV in the clinical specimens from all 3 case-patients. Phylogenetic analysis showed that similar to findings from the 2001 outbreak study ( _8_ ), viruses from Bangladesh and India clustered and diverged from the viruses from Malaysia. ( Figure , panel A). The length of the full genome of the isolate from India was 18,252 nt. The sequence of this virus (INDNipah-07–1, GenBank accession no. FJ513078) was closer to the virus from Bangladesh ( Figure , panel B), with 99.2% (151 nt substitutions) and 99.80% (17 aa substitutions) identity at nucleotide and amino acid levels respectively. Of the 151 nt substitutions, 9 occurred in the N open reading frame (ORF),11 in the phosphoprotein ORF, 8 in the matrix ORF, 11 in the fusion glycoprotein ORF, 7 in the attachment protein ORF, and 47 in the large polymerase ORF. Fifty-eight substitutions occurred in nontranslated regions at the beginning and the end of each ORF. The intergenic sequences between gene boundaries were highly conserved in the isolate from India, compared with the isolate from Bangladesh, which showed 1 change (GAA to UAA) between the attachment protein and large polymerase genes. No change was observed in the leader and the trailer sequences.\n\n【11】The Table compares amino acid substitutions in the different regions of the genome of the isolate from India with those of the viruses from Bangladesh and Malaysia. Of the 17 aa substitutions, 7 were unique to the isolate from India, and 10 were similar to the isolates from Malaysia. Overall, however, the isolate from India was closer to the isolate from Bangladesh, although distinct differences were observed.\n\n【12】To our knowledge, this is the second report of an NiV outbreak in India, identified within 1 week of the investigation. The first outbreak affected mainly hospital staff or persons visiting hospitalized patients; the 74% case-fatality rate strongly suggested person-to-person transmission ( _8_ ). Both outbreaks (2001 and 2007) occurred in the state of West Bengal bordering Bangladesh wherein several outbreaks of the disease have been reported ( _7_ _,_ _9_ _–_ _13_ ). However, fruit bats from West Bengal have not been screened for evidence of NiV infection. This state needs to create awareness about NiV and obligatory testing of suspected case-patients.\n\n【13】### Conclusions\n\n【14】NiV caused an intrafamilial outbreak with a 100% case-fatality rate, which confirmed person-to-person transmission. The NiV strains from India and Bangladesh were closer than the Malaysian viruses. Although the outbreaks occurred in neighboring geographic areas, NiV outbreaks in Bangladesh and India were not caused by the same virus strain or by spillover.\n\n【15】Dr Arankalle is a scientist at the National Institute of Virology in Pune, India. Her current research interests include hepatitis and emerging and reemerging viruses, such as chandipuravirus, chikungunya virus, avian influenza virus, Nipah virus, pandemic influenza (H1N1) 2009 virus, and undiagnosed viruses causing epidemics in India", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9244f2ca-ad0e-4022-8d50-9947d9c1b79a", "title": "Rare Norovirus GIV Foodborne Outbreak, Wisconsin, USA", "text": "【0】Rare Norovirus GIV Foodborne Outbreak, Wisconsin, USA\nNorovirus is the leading cause of epidemic and endemic acute gastroenteritis globally. The virus can be transmitted through person-to-person contact, consumption of fecally contaminated food or water, or self-contamination after touching contaminated environmental surfaces ( _1_ , _2_ ). Noroviruses are divided into at least 10 genogroups (G), and viruses in GI, GII, GIV, GVIII, and GIX cause illness in humans ( _3_ ). More than 99% of all norovirus outbreaks are caused by GI and GII viruses in the United States ( _4_ ). GVIII includes 2 strains that have been detected in Japan during 2004 and 2011 ( _3_ ), and GIX has caused 11 reported outbreaks in the United States since 2013 ( https://www.cdc.gov/norovirus/reporting/calicinet/data.html ).\n\n【1】GIV is divided into 2 recognized genotypes: GIV.1, which infects humans ( _5_ ), and GIV.2, which infects canines and felines ( _6_ ). GIV viruses were reported in humans in the Netherlands during 1998 and the United States during 1999 ( _7_ , _8_ ) and have since been sporadically reported in clinical and environmental samples ( _5_ , _9_ – _11_ ). An outbreak linked to a GIV norovirus in the United States has not been reported since 2001 ( _4_ , _8_ ). In this article, we describe a 2016 foodborne norovirus outbreak associated with a novel GIV strain (tentatively GIV.NA).\n\n【2】### The Study\n\n【3】Figure 1\n\n【4】Figure 1 . Epidemiologic curve of attendees and food handlers by date of illness onset for event during rare norovirus GIV foodborne outbreak, Wisconsin, USA. Arrow indicates event date and time period event...\n\n【5】On May 6, 2016, the Wisconsin Department of Public Health was notified of a possible norovirus outbreak. The outbreak occurred at a breakfast event held at a restaurant on May 3, 2016, for local business owners ( Figure 1 ). According to interviews of the affected group, 49 attendees and 4 food handlers reported being ill, and the first case was reported on May 4, 2016. The peak of illness occurred 48 hours after the breakfast event. Symptoms included diarrhea, vomiting, and nausea ( Table 1 ). Duration of illness was 1–5 days (median 2 days), and incubation time range was 15–57 hours (median 38 hours).\n\n【6】On the basis of the epidemiologic investigation, pathogen transmission occurred through foodborne exposure, and the highest risk ratio was linked to individually eaten fruit from a fruit salad served at the breakfast (risk ratio 2.17–3.29) ( Table 2 ). The epidemiologic curve and risk ratios were calculated by using Microsoft Excel ( https://www.microsoft.com ) and R software ( http://www.r-project.org ). The food handlers prepared the fruit, which was served as a fruit salad during the meal on May 2, the day before the breakfast. Reportedly, the strawberries and grapes were washed, whereas the melons were not. There was no leftover fruit available for laboratory testing.\n\n【7】Nucleic acid was extracted from stool samples collected from 6 ill persons and tested for norovirus GI/GII by real-time quantitative reverse transcription PCR (qRT-PCR), and 5 GII-positive samples (from 3 attendees and 2 food handlers) with high cycle threshold values (range 28–37) were amplified by conventional RT-PCR targeting a partial region of the 5′ end of open reading frame 2 ( _4_ ). The sequences did not cluster with any GI or GII norovirus reference sequences and closely matched GIV viruses ( _4_ ).\n\n【8】Figure 2\n\n【9】Figure 2 . Maximum-likelihood phylogenetic analysis of rare norovirus GIV isolated during foodborne outbreak, Wisconsin, USA (red text), and reference strains. A) Partial polymerase gene (762 nt); B) complete capsid (VP1) gene (554...\n\n【10】Next-generation sequencing (NGS) was performed on 3 samples and verification of final consensus sequences was performed by using Geneious version 11.1.2 (Biomatters Inc., https://www.newjerseybids.us ) ( _4_ ). Sequences for all 3 near complete genomes (≈7,490 nt) were identical (GenBank accession no. NC\\_044855). The closest polymerase gene sequence in GenBank had a 79% nt similarity ( Figure 2 , panel A), and the capsid sequence matched partial capsid sequences derived from wastewater in Brazil, Japan, and the United States with a 98% nt identity. The closest complete major capsid sequence in GenBank (accession no. AF414426) had a 76% aa identity ( Figure 2 , panel B).\n\n【11】### Conclusions\n\n【12】We report a novel norovirus GIV genotype as the causative agent of a foodborne norovirus outbreak. Norovirus GIV outbreaks are rare and were reported in the Netherlands during 1998, the United States during 1999, and in Australia during ­­­­­2010 ( _5_ , _7_ , _8_ ) and have since been detected sporadically in clinical samples ( _9_ – _11_ ). However, seroprevalence studies in Italy, the Netherlands, and the United States have shown that 19%–31% of these populations have antibodies against GIV ( _12_ – _14_ ). Possible explanations include that most laboratories do not test for norovirus GIV or most infections are asymptomatic or do not lead to a visit to a physician. However, 3 young children who were positive for GIV in a study in Italy had severe endemic acute gastroenteritis symptoms ( _10_ ).\n\n【13】Several studies have detected norovirus GIV in rivers and wastewater ( _9_ – _11_ , _15_ ); the number of positive samples ranged from 8.2% to 34%, further supporting that norovirus GIV is circulating in the general population. Several short sequences detected in wastewater collected in Brazil, Japan, and the United States match the capsid sequence in our study ( _9_ , _15_ ). However, new norovirus genotypes require \\> 2 nonidentical complete capsid sequences from different geographic locations that form a separate phylogenetic cluster ( _3_ ) Therefore, the virus detected in this outbreak cannot officially be assigned as GIV.3 yet but is assigned GIV.NA1\\[PNA1\\].\n\n【14】Detection of a norovirus GIV strain associated with a foodborne outbreak shows that despite the absence of reported GIV norovirus outbreaks over the past 15 years in the United States, these viruses continue to circulate in the human population. Because samples from endemic acute gastroenteritis outbreaks are typically tested only for noroviruses GI and GII, including testing of norovirus-negative samples for GIV might improve determining endemic acute gastroenteritis outbreaks of unknown etiology.\n\n【15】Ms. Barclay is a microbiologist at the National Calicivirus Laboratory, Division of Viral Diseases, National Center for Immunization and Respiratory Diseases, Centers for Disease Control and Prevention, Atlanta, GA. Her primary research interest is surveillance of noroviruses and other viral gastroenteritis viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "108cab04-87b0-446b-afa5-7e7d0a215f5b", "title": "COVID-19 Booster Dose Vaccination Coverage and Factors Associated with Booster Vaccination among Adults, United States, March 2022", "text": "【0】COVID-19 Booster Dose Vaccination Coverage and Factors Associated with Booster Vaccination among Adults, United States, March 2022\nA COVID-19 vaccine booster dose is intended to boost the immune system for better, long-lasting protection when the primary vaccine response decreases over time. Studies have shown that a booster increased the immune response in trial participants who completed a Pfizer-BioNTech ( https://www.pfizer.com ) or Moderna ( https://www.modernatx.com ) primary series 6 months earlier or who received a Johnson & Johnson/Janssen ( https://www.jnj.com ) single-dose vaccine 2 months earlier ( _1_ _,_ _2_ ).\n\n【1】With an increased immune response, booster doses provide additional protection against both Delta and Omicron variants for clinical COVID-19 emergency department visits and hospitalization even for those persons who have received an initial vaccine series ( _1_ _,_ _3_ ). For example, the mRNA vaccine effectiveness (VE) against emergency room visits during the period of Delta predominance was 76%–86% after the second initial dose and 94% after a booster dose; estimates of VE during Omicron variant predominance were 38%–52% after the second initial dose and 82% after a booster dose. VE against hospitalizations during the period of Delta predominance was 81%–90% after the second initial dose and 94% after a booster dose, and estimates of VE for during Omicron variant predominance were 57%–81% after the second initial dose and 90% after a booster dose ( _3_ ).\n\n【2】The Centers for Diseases Control and Prevention (CDC) first recommended booster doses for select populations in September 2021 and on November 29, 2021, recommended that all persons \\> 18 years of age should get a booster dose when eligible ( _1_ _,_ _2_ ). By March 2022, approximately 84% of American adults were fully vaccinated with the COVID-19 primary vaccine series; primary vaccine series completion rates varied by some social‒demographic characteristics ( _4_ ). Receiving a COVID-19 booster dose is useful both to prevent COVID-19-related illness and death and slow the spread of COVID-19 in the United States. The objective of this study was to assess COVID-19 booster dose vaccination coverage by demographics and behaviors and experiences toward vaccination among fully vaccinated adults by using data from the National Immunization Survey–Adult COVID Module (NIS-ACM) ( _5_ ).\n\n【3】### Methods\n\n【4】We collected the NIS-ACM data used in this report by telephone interview among adults \\> 18 years of age by using a random-digit‒dialed sample of cell telephone numbers.\n\n【5】Data were collected during February 27–March 26, 2022. Trend analysis was based on data collected during October 31, 2021–March 26, 2022. Booster dose was defined as receipt of a third dose of COVID-19 vaccine after completion of a 2-dose primary mRNA COVID-19 vaccine series for adults who are not immunocompromised or a fourth dose of COVID-19 vaccine after completion of a 3-dose mRNA COVID-19 vaccine series for adults who reported being immunocompromised. For respondents whose initial vaccine was a Janssen/Johnson & Johnson vaccine, booster dose was defined as receipt of a second dose of the vaccine after completion of a single-dose primary vaccine series for adults who are not immunocompromised or a third dose of Janssen vaccine after completion of 2-dose series for adults who reported being immunocompromised ( _1_ _,_ _2_ ). Receipt of a booster dose of COVID-19 vaccine was based on responses to the questions, “Have you received at least one dose of a COVID-19 vaccine?,” “Which brand of COVID-19 vaccine did you receive for your first dose?,” “How many doses of a COVID-19 vaccine have you received?,” and self-reported health conditions that may put respondents at higher risk for COVID-19 (including immunocompromised status).\n\n【6】Survey questions also collected information on vaccine confidence, behaviors, and experiences, such as being concerned about getting COVID-19, thinking COVID-19 vaccines are safe, believing COVID-19 vaccines are useful for protection from COVID-19, whether friends or family were vaccinated, and whether the respondent had difficulty getting a COVID-19 vaccine (e.g., difficulty getting an appointment online, knowing where to get vaccinated, getting to vaccination sites). Information on demographic characteristics, health insurance status, reported medical conditions, previous diagnosis of COVID-19, disability status, frontline/essential work status, provider recommendation of a COVID-19 vaccine, and work/school COVID-19 vaccination requirement were also collected ( _6_ ). Questions regarding vaccine confidence, behaviors, and experiences did not specifically address booster doses. Analytic datasets were created for approximate months of data collection, and we used data from 5 data collection periods (November 2021, collected during October 31–November 27; December 2021, collected during November 28–December 31; January 2022, collected during January 2–January 29; February 2022, collected during January 30–February 26; and March 2022, collected during February 27–March 26) for these analyses. The response rates for the 5 monthly datasets ranged from 21.4% to 22.0%, and the total sample sizes for the 5 periods were 39,508, 68,612, 62,693, 58,488, and 63,072, respectively.\n\n【7】We stratified COVID-19 booster dose vaccination coverage by using demographic characteristics and vaccine confidence, behaviors, and experiences. Race/ethnicity was classified as non-Hispanic White, non-Hispanic Black, Hispanic, non-Hispanic Asian, non-Hispanic American Indian/Alaska Native, non-Hispanic Native Hawaiian/Pacific Islander, or other/multiple races. Urbanicity status was derived based on the centroid of the postal code of residence, categorized as metropolitan statistical area (MSA) principal city, MSA nonprincipal city, or non-MSA. Social vulnerability index (SVI) was categorized as low, moderate, or high based on county of residence (CDC/Agency for Toxic Substances and Disease Registry) by using tertiles of SVI score ( _7_ ).\n\n【8】We analyzed data by using SAS version 9.4 ( https://www.sas.com ) and SUDAAN version 11.0.1 ( https://www.rti.org ). We weighted all percentages to represent the noninstitutionalized US adult population and calibrated survey weights by age and sex to state-level vaccine administration data reported to CDC as of the middle of the monthly data collection period ( _6_ ). We conducted multivariable logistic regression analysis and predictive marginals to assess factors associated with receipt of a booster dose among adults and generated the unadjusted prevalence ratio (PR) and the adjusted prevalence ratio (aPR) from regression models. We used PR to assess association instead of odds ratio \\[OR\\] in our analysis because PR is a more direct measure of effect than OR, and when outcomes are not rare, as with most of vaccination coverage analysis, the OR tends to present an exaggerated measure of effect compared with the PR. We used t-tests to determine differences between groups with statistical significance at p<0.05 and for linear trends over months. This activity was reviewed by CDC and was conducted consistent with applicable federal law and CDC policy (45 C.F.R. part 46.102(l)(2), 21 C.F.R. part 56; 42 U.S.C. §241(d); 5 U.S.C. §552a; 44 U.S.C. §3501).\n\n【9】### Results\n\n【10】Figure\n\n【11】Figure . Trends in COVID-19 booster dose vaccination coverage among fully vaccinated adults, by age group, National Immunization Survey–Adult COVID Module, United States, November 2021–March 2022.\n\n【12】COVID-19 booster dose coverage among fully vaccinated adults \\> 18 years of age increased from 25.7% in November 2021 to 63.4% in March 2022 (p<0.05 by test for trend) ( Figure ). Coverage in mid-March 2022 among those 50–64 years of age (66.4%) and \\> 65 years of age (79.5%) was higher than among those 18–49 years of age (53.6%) ( Table 1 ). By mid-March 2022, booster dose coverage was 52.8% among all adults \\> 18 years of age (including unvaccinated adults in the denominator), and coverage among those 50–64 years of age (58.5%) and \\> 65 years of age (77.0%) was higher than among those 18–49 years of age (40.6%).\n\n【13】Among fully vaccinated adults \\> 18 years of age, booster dose coverage in mid-March 2022 was lower among Native Hawaiian/Pacific Islander (45.4%), Black (52.7%), Other/multiple races (54.1%), Hispanic (55.5%), and American Indian/Alaska Native (56.6%) than among White adults (67.7%); Asian adults had the highest coverage (74.6%) (p<0.05) ( Table 1 ). Booster dose coverage was higher among all healthcare personnel (HCP) \\> 18 years of age and among school and childcare workers 18–49 years of age than for other essential workers ( Table 1 ).\n\n【14】Coverage was higher for adults who had reported medical conditions (69.9%) than in adults who did not have these conditions (60.4%). In addition, women and those who lived above the poverty level, had some college or higher education, had health insurance, and had received a vaccine other than COVID-19 in the past 2 years had higher booster vaccination coverage than did the respective reference groups ( Table 1 ). Adults living in a moderate or high SVI county and those who had a previous COVID-19 infection had lower booster dose vaccination coverage than did the respective reference groups. Adults with disability had lower booster dose vaccination coverage than did adults without disability across age groups (18–49, 50–64, and \\> 65 years of age) ( Table 1 ). Furthermore, compared with the respective reference groups, booster dose coverage was higher among adults who reported they were concerned about getting COVID-19 (70.1% vs. 58.1%), thought the vaccine was safe (71.8% vs. 39.8%), and thought the vaccine was useful for protection from COVID-19 (67.8% vs. 22.8%). In addition, reporting little or no difficulty getting a COVID-19 vaccine was associated with decreased booster vaccination ( Table 1 ).\n\n【15】In the multivariable model adjusted for demographic variables, characteristics independently associated with increased booster vaccination were older age, Asian race, household income \\> $75,000, some college or higher education, being insured, having received any vaccine that was not a COVID-19 vaccine in the past 2 years, and having reported medical conditions ( Table 2 ). In addition, for occupational categories, being an HCP, school/childcare worker, or other frontline worker, or not being an essential worker, was associated with increased booster vaccination compared with being in the category of other essential worker. Non-Hispanic Black adults, those living in a high SVI county, those living in non-MSAs, those with a disability, and those with a previous COVID-19 infection had decreased booster vaccination. For the multivariable model including demographic and behavioral variables, demographic characteristics independently associated with booster vaccination were similar to those for the model adjusted for demographic variables only. In addition, being concerned about getting COVID-19, believing the vaccine is safe, believing the vaccine is useful for protection, and having many or almost all friends and family vaccinated were independently associated with increased booster vaccination. Reporting a little or no difficulty getting a COVID-19 vaccine was independently associated with decreased booster vaccination ( Table 2 ).\n\n【16】Among adults \\> 18 years of age who were fully vaccinated but did not receive a booster dose, prevalence of provider recommendation of COVID-19 vaccine was 47.9%, and prevalence was higher among adults 50–64 years of age (50.7%) than among adults 18–49 years (46.5%). Overall, 36.8% reported “being concerned about getting COVID-19,” 60.6% reported “thinking the vaccine is safe,” 80.5% reported “believing COVID-19 vaccine is important for protection from infection,” 77.6% reported “most or almost all friends or family were vaccinated,” and 31.9% reported “work or school requires COVID-19 vaccine” ( Table 3 ). Among fully vaccinated adults who did not receive a booster dose, ≈4%–10% of adults reported difficulties in getting a COVID-19 vaccine (e.g., difficulty getting vaccinated \\[9.9%\\], difficulty getting an appointment online \\[9.5%\\], difficulty knowing where to get vaccinated \\[5.3%\\], and difficulty getting to vaccination sites \\[4.2%\\]) ( Table 3 ).\n\n【17】### Discussion\n\n【18】By March 2022, a total of 84% of American adults were fully vaccinated with the COVID-19 primary vaccine series, according to the NIS-ACM ( _7_ ). Booster dose coverage among fully vaccinated adults \\> 18 years of age was 63.4% in March 2022. Overall, ≈53% of the adult population have both received the primary series and \\> 1 booster vaccination. Disparities by race/ethnicity and other factors are apparent in booster dose uptake. Healthcare providers can educate and encourage everyone to receive a booster dose when they are eligible. Targeted strategies are needed to reduce disparities in COVID-19 vaccination coverage toward reducing disparities in COVID-19.\n\n【19】Booster dose vaccine uptake was most strongly associated with confidence in the need for getting vaccinated, confidence in vaccine safety, and concern about getting COVID-19. Although persons were presumably amenable to getting the primary vaccine series, ≈39% of fully vaccinated adults who did not receive a booster dose did not believe COVID-19 vaccines were completely safe, and 20% did not believe they were useful for protection against COVID-19. Most fully vaccinated adults who did not receive a booster (63%) were not concerned about getting COVID-19, especially younger adults. Higher levels of concern about COVID-19 and positive attitudes toward vaccination among adults might contribute to uptake of booster dose vaccination. To further improve vaccine uptake, more innovative approaches are needed to improve vaccine confidence.\n\n【20】In addition, we found that reporting of family and friends being vaccinated was associated with booster dose vaccination uptake. This finding indicated the useful role of social processes for increasing vaccination ( _8_ _,_ _9_ ). Community healthcare workers can educate the community about the vaccines, text persons to let them know of vaccine eligibility, use public media or social media, and encourage vaccinated community members to share their own vaccination experiences with their unvaccinated friends and family as a means for improving COVID-19 vaccination coverage ( _8_ _–_ _10_ ).\n\n【21】Our analysis did not find an association between increasing levels of difficulty accessing vaccine and lower booster dose vaccination coverage. This finding might be attributable to extensive efforts to reduce access barriers, including mobile vaccination sites, removing an insurance or identification requirement, and substantial community-led outreach ( _11_ _–_ _13_ ). We found that persons who reported difficulty getting a COVID vaccine had higher booster dose coverage, a finding that might seem counterintuitive. However, our study assessed barriers to COVID-19 vaccination overall and not specifically for booster vaccines. Early adopters who sought vaccine at the beginning of the COVID-19 vaccination program when supply was scarce might have been more likely to experience barriers to vaccination. The finding that factors such as lower income and education were associated with lower booster uptake, even after controlling for attitudinal factors, suggests that barriers to access might remain. Among fully vaccinated adults who did not receive booster dose vaccination, ≈4%–10% of adults did report difficulties in getting a COVID-19 vaccine (e.g., difficulty getting vaccinated, getting an appointment online, knowing where to get vaccinated, or getting to vaccination sites). Understanding the barriers to vaccination can help identify strategies most likely to increase vaccine uptake. Many of these barriers could be further reduced by providing vaccination in the office of their usual medical provider ( _14_ , _15_ ). Reducing barriers to COVID-19 vaccination could further improve vaccination coverage among adults.\n\n【22】COVID-19 has disproportionately impacted racial and ethnic minority populations by illness, hospitalizations, and death in the United States ( _16_ , _17_ ). Although most disparities in primary COVID-19 vaccination had been eliminated by March 2022 ( _7_ ), disparities in booster dose vaccination remain. Booster coverage was lower among all racial/ethnic groups except Asian compared with non-Hispanic White adults. Equitable vaccination can help to reduce illness-related disparities in minority groups. One study found that COVID-19 vaccine hesitancy decreased more rapidly among non-Hispanic Black than non-Hispanic White adults during December 2020‒June 2021, indicating that lower coverage might be less likely the result of vaccine hesitancy than other factors ( _18_ ). Several factors, including knowledge, attitudes, and beliefs about vaccines and barriers related to accessing vaccines and healthcare services, contribute to lower vaccination coverage in non-Hispanic Black adults and other minority groups ( _18_ _–_ _21_ ). Tailored and community-led interventions, including postal code‒level vaccination access planning and community engagement, have been shown to reduce inequities in COVID-19 vaccination by race and ethnicity ( _10_ , _22_ , _23_ ). Vaccination programs could implement culturally and linguistically appropriate focused interventions among communities with lower vaccination coverage to reduce vaccination disparities.\n\n【23】COVID-19 booster dose vaccination coverage was particularly lower among adults living in poverty, with lower education, or without health insurance, and continued efforts are needed to reach these groups and reduce inequities ( _24_ _–_ _26_ ). In addition, having a previous COVID-19 infection was independently associated with decreased booster vaccination. We did not assess when persons had COVID-19 in relation to the timing of the initial vaccination series or booster vaccination, but this finding might suggest that persons who have already had COVID-19 might believe that they are protected and do not need a booster. However, COVID-19 vaccines including booster doses have been shown to provide additional protection to persons who had previous infections ( _27_ ), and all adults are recommended to receive a booster dose, regardless of previous infection with COVID-19.\n\n【24】Although we were unable to assess provider recommendation specifically for booster vaccination, studies have shown that a provider recommendation is highly associated with vaccine uptake ( _8_ _,_ _25_ _,_ _28_ ). Findings from our study indicated that, among those who have not received a booster dose, >50% have not received a provider recommendation for any COVID-19 vaccine. Underuse of primary care services during the COVID-19 pandemic and some adults not having a primary physician for sick or preventive care ( _29_ ) might have limited opportunities for providers to convey recommendations and communicate with patients the benefits of primary and booster dose vaccination and information on the safety and effectiveness of COVID-19 vaccination. Clinicians and healthcare providers, including pharmacists and allied health professionals, can get coaching, practice, and support from the broader healthcare organizations in which they are embedded, follow the Advisory Committee on Immunization Practices (ACIP) recommendations ( _1_ _,_ _2_ ), recommend needed vaccinations, and encourage eligible persons to receive COVID-19 booster dose vaccination.\n\n【25】Findings from this study showed that booster dose vaccination coverage among adults 50–64 and \\> 65 years of age was higher than among those 18–49 years of age, and this pattern remained the same after controlling for demographic and behavioral variables. The risk for severe illness from COVID-19 increases with age ( _30_ , _31_ ). Higher COVID-19 booster dose vaccination coverage among older adults might also have been caused by early recommendation from ACIP for this population ( _1_ , _2_ ). Booster dose coverage was higher among persons who had reported medical conditions but lower among persons who had disabilities. Higher COVID-19 booster dose vaccination coverage among adults who had reported medical conditions might also be caused by the early recommendation in September 2021 from ACIP and recognition of increased risk for severe COVID-19 in this population ( _1_ , _2_ ). One recent study indicated that adults who had disabilities experienced more difficulty in obtaining a COVID-19 vaccination than persons who did not have a disability ( _26_ ). Healthcare providers can try to ensure that all persons receive COVID-19 vaccination if they are eligible, regardless of age. Reducing barriers to scheduling and making vaccination sites more accessible might help improve vaccination coverage among adults who have disabilities ( _26_ ).\n\n【26】Results from our study showed that booster dose coverage was much higher among essential HCP than among other essential workers. Although primary series vaccination was >90% among HCP and those in the education sector and ≈80% for other frontline and essential workers ( _7_ ), booster dose coverage was much lower, leaving populations that have frequent COVID-19 exposure possibly susceptible to disease. This finding could be caused by lack of requirements for vaccination and removal of programs such as onsite vaccination that were available for the initial vaccine series. In addition, other access programs for different populations were probably available for initial vaccines but not for boosters. Reinstating these more intensive access programs or putting vaccines in the hands of primary care providers could help increase booster dose coverage among this population.\n\n【27】Four limitations might be considered when interpreting these findings. First, NIS-ACM has a low response rate (≈22%). However, survey weights were calibrated to COVID-19 vaccine administration data to mitigate possible bias from incomplete sample frame, nonresponse, and misclassification of vaccination status. Second, COVID-19 vaccination was self-reported and might be subject to recall or social desirability bias. However, reliability of self-reported COVID-19 vaccination might be comparable with that of self-report of influenza vaccination, which has been shown to have a relatively high agreement with vaccination status ascertained from medical records ( _32_ _,_ _33_ ). Third, provider recommendations or work/school requirements for COVID-19 vaccination, and vaccination access barriers were not specifically assessed for booster doses.\n\n【28】Booster dose coverage was not optimal, and disparities by race/ethnicity and other factors are apparent in booster dose uptake. Continual monitoring of booster dose vaccination will be helpful for developing tailored strategies to improve vaccination coverage, especially with the recent recommendation for adults to receive the updated COVID-19 boosters ( _34_ ). The updated COVID-19 boosters are formulated to better protect against the most recently circulating COVID-19 variant. The immune response of the updated COVID-19 booster vaccine was superior to that of the previous booster vaccine, and the safety was similar as that of the previous booster vaccine ( _35_ ). To maximize protection against COVID-19, both an increase in persons initiating and completing the primary series, and getting recommended boosters, is needed.\n\n【29】Targeted strategies are needed to improve booster dose vaccination coverage among adults. These strategies should include healthcare providers educating and encouraging everyone to receive a booster dose when they are eligible regardless of age and previous infection with COVID-19 ( _8_ _–_ _10_ ) and provisions for consistent access to vaccines, vaccination incentives, onsite vaccination, and reminders ( _14_ _,_ _15_ ). More innovative approaches should include improving confidence in vaccines, community healthcare workers encouraging vaccinated community members to share their own vaccination experiences with their unvaccinated friends and family ( _14_ _,_ _15_ ), understanding barriers and reducing barriers to vaccination ( _14_ _,_ _15_ ), and providing vaccination in the office of their usual medical provider ( _14_ _,_ _15_ ). Vaccination programs are needed that implement culturally and linguistically appropriate focused interventions among communities with lower vaccination coverage ( _8_ _–_ _10_ , _14_ _,_ _15_ , _26_ , _36_ ) and encourage national, state, and local health departments, and community-based and faith-based organizations to implement a combination of strategies, which have been shown to be effective in improving booster dose coverage ( _26_ , _36_ ).\n\n【30】Dr. Lu is a research scientist at the National Center for Immunization and Respiratory Diseases, Centers for Disease Control and Prevention, Atlanta, GA. His primary research interest is child and adult vaccination assessment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "80f2f8c6-f74a-44cd-bfb6-7c20a0b64bef", "title": "Legionnaires’ Disease in South Africa, 2012–2014", "text": "【0】Legionnaires’ Disease in South Africa, 2012–2014\nData are limited regarding prevalence of _Legionella_ spp. bacteria that cause community-acquired pneumonia (CAP) in Africa ( _1_ ), despite the high prevalence of HIV-infected adults in many African countries, including South Africa ( _2_ ). Legionellosis is a notifiable disease in South Africa but is rarely reported. We sought to determine the prevalence of _Legionella_ spp. infections in South Africa and describe epidemiologic characteristics of patients with Legionnaires’ disease (LD).\n\n【1】### The Study\n\n【2】During June 2012–September 2014, we conducted a prospective, hospital-based, observational study as part of the severe respiratory illness (SRI) surveillance at 2 sites in South Africa: Klerksdorp-Tshepong Hospital Complex, Klerksdorp, North West Province; and Edendale Hospital, Pietermaritzburg, KwaZulu-Natal Province. A patient with SRI was defined as a person hospitalized with lower respiratory tract infection of any duration. We used a standardized questionnaire to collect demographic and clinical information. Nasopharyngeal specimens and induced sputum samples were tested for _Legionella_ spp. infections by using a real-time PCR assay, as previously described ( _3_ ). Specimens that were _Legionella_ positive were also tested by real-time PCR assays to identify _L. pneumophila_ and _L. longbeachae_ . In addition, patients’ specimens were tested for other respiratory pathogens and for HIV. Of the 22 _Legionella_ \\-positive patients, we could trace 17 with whom we conducted a retrospective epidemiologic investigation, which included interviews (detailed study methods in the Technical Appendix ).\n\n【3】During June 2012–September 2014, a total of 4,525 SRI patients were enrolled; induced sputum specimens, the recommended specimen type for _Legionella_ spp. detection, were collected from 1,805 (40%). Of 1,803 patients with sputum specimens for which data were available, 885 (49%) were male, and 324 (18%) were children <5 years of age. HIV prevalence was 64% (1,025 of 1,594 patients with sputum specimens and known HIV status), and prevalence of active tuberculosis (TB) infection was 24% (421 of 1,758 patients with sputum specimens and known TB status). Of 1,720 patients with sputum specimens and known survival status, 142 (8%) patients died.\n\n【4】Figure 1\n\n【5】Figure 1 . Number of case-patients and detection rate for _Legionella_ spp. infections, by age group, South Africa, June 2012–September 2014 (N = 1,803).\n\n【6】Among the 1,805 patients with sputum samples, 21 (1.2%, 95% CI 0.7%–1.7%) tested positive for _Legionella_ spp. by real-time PCR. For 1 patient (designated E1 in the online Technical Appendix Table) from whom sputum could not be collected, _Legionella_ spp. infection was detected in the nasopharyngeal specimen, so 22 patients with _Legionella_ spp. infections were detected in total. Among the 21 patients whose sputum tested positive for _Legionella_ spp. infections, median age was 40 years (range 19–59 years; Figure 1 ), and 11 (52%) were males.\n\n【7】Figure 2\n\n【8】Figure 2 . Number of case-patients and detection rate of _Legionella_ spp. infections, by month and year, for Edendale Hospital and Klerksdorp-Tshepong Hospital Complex, South Africa, June 2012–September 2014 (N = 1,805). \\\\\n\n【9】A cluster of case-patients (15/21 \\[71%\\]) was observed during July–December 2012 ( Figure 2 ), including all 6 from Edendale Hospital and 10 (10/16, 63%) from Klerksdorp-Tshepong Hospital Complex. These sites are geographically distant (≈600 km) from one another, so the respective clusters or outbreaks are unlikely to be related. We did not culture samples with _Legionella_ spp. infection, so we were unable to perform strain typing to confirm whether the clusters were caused by related strains. The remaining 6 patients from Klerksdorp-Tshepong Hospital Complex appeared to have sporadic infections.\n\n【10】_Legionella_ patients resided in different areas or communities within the cities of Pietermaritzburg and Klerksdorp. Epidemiologic investigation revealed exposure to several potential sources of infection, such as waste management, air conditioners, plumbing, mining, and swimming pools; however, no common exposure could be identified, so environmental sampling and testing were not performed.\n\n【11】Fifteen (75%) of 20 _Legionella_ patients with known HIV status were infected with HIV, and 9 (43%) of the 21 patients tested positive for TB at the admission during which _Legionella_ infection was detected. HIV or TB infection, or both, was detected in 18 (90%) of 20 patients with known HIV and TB status. A history of active TB before the admission during which _Legionella_ was detected was reported for 14 (82%) of 17 patients. For 17 _Legionella_ spp.–infected patients for whom information was available, additional LD-associated factors included regular alcohol consumption (10 \\[59%\\]), cigarette smoking (9 \\[53%\\]), asthma (2 \\[12%\\]), and heart disease (2 \\[12%\\]).\n\n【12】Eighteen (86%) of 21 patients had symptoms >7 days before hospital admission, a delay possibly occurring because many patients were chronically ill (75% were HIV infected and \\> 43% had TB). Median duration of hospitalization for _Legionella_ patients was 4 days (range 1–35 days), and 1 (9%) patient was admitted to intensive care and survived the illness; 4 (20%) patients died. Antimicrobial drug treatment (in-hospital and discharge medication) was known for 21 patients and included amoxicillin/clavulanic acid (16 \\[76%\\]), anti-TB medications (15 \\[71%\\]), cotrimoxazole (7 \\[33%\\]), cefuroxime/ceftriaxone (5 \\[24%\\]), and erythromycin (5 \\[24%\\]).\n\n【13】_Legionella_ spp. isolates were identified for 2 patients as _L. pneumophila_ serogroup 1 and _L. longbeachae_ . Species could not be determined for 19 patients because of low bacterial loads in their specimens. Of the 21 patients with _Legionella-_ positive sputum specimens, co-infections were detected in 14 (67%). Co-infecting pathogens were _Mycobacterium tuberculosis_ (9 \\[43%\\]), rhinovirus (6 \\[29%\\]), respiratory syncytial virus (2 \\[10%\\]), adenovirus (2 \\[10%\\]), _Bordetella pertussis_ (1 \\[5%\\]), and _Streptococcus pneumoniae_ (1 \\[5%\\]).\n\n【14】_Legionella_ spp. detection rates in this study were similar to those described in other countries ( _4_ ). However, age distribution tended toward younger adults, not the elderly, the population previously reported as most affected ( _4_ ). Men and women were evenly distributed in our study, although a substantial male predominance is common for LD ( _2_ _,_ _4_ ). Differences in age and gender distributions, compared with distributions in other studies, likely result from high HIV and TB prevalence among younger adults in our study population. LD is typically associated with summer because warm and wet conditions promote bacterial replication ( _2_ _,_ _4_ ). Longer periods of surveillance are needed to establish seasonality of LD in South Africa.\n\n【15】Clinically, patients with LD in this study were likely to be HIV-infected, chronically ill persons with suspected or confirmed TB and were therefore usually treated for TB infection and discharged. HIV-induced immune suppression and lung damage because of biologic or chemical agents likely increased their susceptibility to _Legionella_ infections. Cases of LD and TB occurring simultaneously have been previously described ( _5_ _–_ _7_ ). _Legionella_ infection in populations with HIV or TB co-infections may cause acute exacerbation of respiratory symptoms, prompting patients to seek hospital care.\n\n【16】In South Africa, treatment for CAP is usually penicillin or ampicillin for adults <65 years of age and amoxicillin/clavunate or cefuroxime for elderly or HIV-infected adults ( _8_ ). However, treatment for LD should include a macrolide or fluoroquinolone ( _4_ ). Only one fourth of _Legionella_ patients in this study received appropriate treatment, likely because of clinical inability to distinguish LD from other forms of pneumonia and because clinicians rarely consider _Legionella_ when they lack access to diagnostic testing and local prevalence data. This problem is further compounded by the high prevalence of HIV and TB in South Africa. Anti-TB treatment, which was administered to more than two thirds of the _Legionella_ patients, would have had therapeutic benefits; rifampin has been shown to have activity against _Legionella_ spp. ( _9_ _,_ _10_ ). However, suboptimal treatment of _Legionella_ patients with co-infections likely contributed to a case-fatality ratio (20%) more than twice that for all SRI patients (8%) ( _4_ _,_ _11_ ). Lack of appropriate treatment of patients with CAP in South Africa for atypical pathogens has been described ( _12_ ).\n\n【17】### Conclusions\n\n【18】In South Africa, patients with LD often have chronic illness caused by co-infections such as HIV and TB at time of admission. _Legionella_ infections in most patients were undiagnosed, and patients were suboptimally treated for TB or more typical causes of CAP. Increased awareness and improved diagnostic testing could result in earlier diagnosis, appropriate treatment, and improved outcomes for these patients. In addition to routine diagnostics, surveillance for LD should be performed on an ongoing basis for rapid identification and response to outbreaks.\n\n【19】Dr. Wolter is a senior medical scientist in the Centre for Respiratory Diseases and Meningitis at the National Institute for Communicable Diseases in Johannesburg and is a specialist in molecular microbiology. Her research interests include the diagnosis and epidemiology of respiratory pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c94c6654-d59e-46da-80d3-2cf486b60fe1", "title": "Cluster of SARS-CoV-2 Gamma Variant Infections, Parintins, Brazil, March 2021", "text": "【0】Cluster of SARS-CoV-2 Gamma Variant Infections, Parintins, Brazil, March 2021\nIn Parintins, Brazil, an increased rate of coronavirus disease (COVID-19)–associated hospitalization, from 75.5 cases/100,000 persons in November 2020 to 397 cases/100,000 persons in February 2021, led to an unprecedent health crisis on this island. The outbreak coincided with emergence of the Gamma (P.1) variant of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), raising concern that the variant was causing infection even in persons who previously had COVID-19 ( _1_ ). In March 2021, the Municipal Health Department of Parintins, in collaboration with the US Centers for Disease Control and Prevention (CDC), investigated recently infected persons and their household contacts to identify circulating SARS-CoV-2 variants, assess epidemiologic and laboratory evidence of previous SARS-CoV-2 infection in infected persons, and assess intrahousehold transmission.\n\n【1】We used the COVID-19 surveillance database in Parintins to identify persons \\> 18 years of age who had a positive SARS-CoV-2 antigen test result (Panbio COVID-19; Abbott, https://www.abbott.com ) in the previous 3 days. On March 4 and 5, 2021, the 22 case-patients identified were visited at home, and all adults able to provide written consent were invited to participate; 90 persons (22 index patients, 68 household contacts) agreed. An index case-patient was defined as the person with the earliest symptom onset date in the household; for all but 1 household, index case-patients were the same persons initially identified in the surveillance database. All participants responded to a questionnaire and provided nasopharyngeal swab and dried blood spot samples; nasal swab samples for antigen testing (BINAXNow; Abbott) were obtained from household contacts only.\n\n【2】We tested nasopharyngeal swabs by reverse transcription PCR (RT-PCR) (Allplex 2019-nCov Assay; Seegene, https//www.seegene.com ) and by a variant-of-concern–specific RT–PCR protocol ( _2_ ). Samples with cycle threshold (C <sub>t </sub> ) values <30 underwent whole-genome sequencing (COVIDSEQ; Illumina, https://www.illumina.com ) ( _1_ ) to confirm screening results. We tested dried blood spot samples for SARS-CoV-2 IgG (FlexImmArray; Tetracore, https://tetracore.com ).\n\n【3】We analyzed data by using RStudio version 1.4.1106 ( https://www.rstudio.com ). We assessed differences in proportions and odds ratios by using Fisher exact tests. The study was conducted in accordance with applicable federal law and CDC policy and approved by local health authorities in Brazil.\n\n【4】At the time of the investigation, diagnosis in the region was based on clinical findings or SARS-CoV-2 point-of-care IgG test results. A previous COVID-19 diagnosis was defined as illness diagnosed by a physician as COVID-19 or history of fever and \\> 1 other COVID-associated sign/symptom \\> 3 months earlier.\n\n【5】Of 90 participants, results of SARS-CoV-2 antigen testing, RT-PCR, or both were positive for 54 (60%). Among 45 persons tested with variant-of-concern RT-PCR, whole-genome sequencing, or both, the Gamma variant was detected for 68.9%. We found no significant difference in the proportion of symptomatic infections caused by the Gamma variant (87.0%) and other variants (92.8%), (odds ratio \\[OR\\] 0.52, 95% CI 0.01–6.05; p = 0.56). When we excluded persons for whom the virus genotype differed from that of the household index case-patient, the proportion of infected household contacts was 45% (14/31) in households with a Gamma variant index case-patient and 25% (3/12) with a non–Gamma variant index case-patient (OR 2.42, 95% CI 0.47–16.59; p = 0.22).\n\n【6】Figure\n\n【7】Figure . Network showing relationship between Gamma variant severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) sequences from household members involved in investigation of cluster of SARS-CoV-2 Gamma (P.1) variant infections, Parintins, Brazil,...\n\n【8】Because all 14 non-gamma samples had C <sub>t </sub> values >32, none was successfully sequenced; in contrast, 27/31 Gamma specimens were sequenced. Gamma variant sequences were substantially diverse; overall samples differed by < 17 single-nucleotide polymorphisms. However, samples from persons in the same household largely clustered; of 21 samples from households with >1 sequence available, 17 (81%) differed by <2 nt ( Figure ).\n\n【9】Among 31 patients with Gamma variant infection, none met our definition of having a previous diagnosis of COVID-19 ( Table ). In contrast, 3 (21.4%) of 14 persons with a non–Gamma variant infection and 16 (44.4%) of 36 persons tested by RT-PCR but found to be uninfected had evidence of previous infection or illness (p = 0.026 and p<0.0001, respectively).\n\n【10】We determined that the Gamma variant was responsible for a high proportion of infections in this cluster; we did not find evidence of Gamma variant infection in persons with previous COVID-19. The clustering of sequences by household suggests intrahousehold transmission, which might reflect the difficulty of isolating case-patients at home, especially in large households.\n\n【11】Our findings are limited because the participants were not selected randomly and results may not be generalizable. For some participants, IgG could reflect response to current rather than previous infection, and the use of serologic and clinical criteria might overestimate the number of previous infections. In addition, sequencing success among Gamma and non–Gamma variants might be affected by different viral loads or times of infection relative to time of sampling.\n\n【12】According to the limited epidemiologic and laboratory data available, vaccines currently deployed in Brazil seem to protect against the Gamma variant ( _3_ , _4_ ). Since the investigation, Parintins has accelerated its immunization campaign, resulting in an ≈8-fold increase in the percentage of persons receiving the first vaccine dose during January–June 2021. Although Gamma variant infections did not increase among persons with a history of prior COVID-19, vaccination of all eligible persons and implementation of measures to mitigate intrahousehold transmission will help reduce the spread of SARS-CoV-2.\n\n【13】Dr. da Silva is a medical epidemiologist at CDC in Atlanta and an adjunct assistant professor at Emory University School of Medicine. Her research interests include COVID-19 epidemiology and clinical manifestations as well as HIV drug resistance and HIV treatment optimization in low- and mid-income countries\n\n【14】Members of the CDC Brazil Investigation Team: Fernanda Lessa, Danielle Fernandez, Paola Rullan-Oliver, and Antonio Vieira.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bff763ce-2fde-4e38-9f55-18f5914377f5", "title": "Public Support for Smoke-Free Air Strategies Among Smokers and Nonsmokers, New York City, 2010–2012", "text": "【0】Volume 11 — January 30, 2014\n\n【1】### Article Tools\n\n【2】*   PDF - 425 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Public Support for Smoke-Free Air Strategies Among Smokers and Nonsmokers, New York City, 2010–2012\n===================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】This page was updated on March 06, 2014, to incorporate the corrections in Vol. 11\n\n【8】#### Elizabeth Needham Waddell, PhD; Shannon M. Farley, MPH; Jenna Mandel-Ricci, MPA, MPH; Susan M. Kansagra, MD, MBA\n\n【9】_Suggested citation for this article:_ Waddell EN, Farley SM, Mandel-Ricci J, Kansagra SM. Public Support for Smoke-Free Air Strategies Among Smokers and Nonsmokers, New York City, 2010–2012. \\[Erratum appears in Prev Chronic Dis 2014;11. http://www.cdc.gov/pcd/issues/2014/13\\_0263e.htm .\\] Prev Chronic Dis 2014;11:130263. DOI: http://dx.doi.org/10.5888/pcd11.130263 .\n\n【10】PEER REVIEWED\n\n【11】Abstract\n--------\n\n【12】**Introduction**  \nFrom 2010 through 2012, the New York City Department of Health and Mental Hygiene engaged in multiple smoke-free-air activities in collaboration with community, institution, and government partners. These included implementing a law prohibiting smoking in all parks and beaches as well as working to increase compliance with existing Smoke-free Air Act provisions.\n\n【13】**Methods**  \nWe investigated trends in awareness of existing smoke-free rules publicized with new signage and public support for new smoke-free air strategies by using 3 waves of survey data from population-based samples of smoking and nonsmoking adults in New York City (2010–2012). Analyses adjusted for the influence of sociodemographic characteristics.\n\n【14】**Results**  \nAmong both smokers and nonsmokers, we observed increased awareness of smoke-free regulations in outdoor areas around hospital entrances and grounds and in lines in outdoor waiting areas for buses and taxis. Regardless of smoking status, women, racial/ethnic minorities, and adults aged 25 to 44 years were more likely than men, non-Hispanic whites, and adults aged 65 years or older to support smoke-free air strategies.\n\n【15】**Conclusion**  \nNew signage was successful in increasing population-wide awareness of rules. Our analysis of the association between demographic characteristics and support for tobacco control over time provide important contextual information for community education efforts on secondhand smoke and smoke-free air strategies.\n\n【16】Top of Page\n\n【17】Introduction\n------------\n\n【18】Data from population surveys have demonstrated that support for tobacco control has increased substantially over the past 3 decades as smoking rates have declined and public health awareness and the number of antismoking policies have grown (1–6). After a cigarette sales tax increase and passage of a comprehensive indoor air law in New York City (NYC) called the Smoke-Free Air Act (SFAA) in 2002, smoking prevalence declined from 21.5% in 2002 to 19.2% in 2003, and continued the decline to 14.8% in 2011 (7,8). Nearly half of New Yorkers surveyed after passage of smoke-free air legislation reported less exposure to environmental tobacco smoke (7). Since passage of the SFAA of 2002, additional interventions and policies have expanded smoke-free air in NYC. These include legislative expansion of the SFAA to include hospital grounds, public parks, and beaches; voluntary policy changes by institutions; and educational activities to increase awareness of and compliance with smoke-free air rules. However, public awareness and support for smoke-free air strategies has not been measured at the citywide level. Between March 2010 and March 2012, the NYC Department of Health and Mental Hygiene (DOHMH) implemented multiple activities to promote smoke-free air. These activities were made possible through the Communities Putting Prevention to Work (CPPW) grant from the Centers for Disease Control and Prevention, which enabled NYC to develop partnerships with community institutions and implement community-level educational initiatives to promote smoke-free air strategies aimed at decreasing tobacco use and secondhand smoke exposure (9). Activities were as follows:\n\n【19】*   DOHMH created institutional partnerships with NYC hospitals to improve compliance with existing smoke-free air laws for hospital grounds, including outdoor areas, and to improve cessation services for employees, patients, and visitors. The first hospital partnership was formed in April 2011, and 6 more hospitals joined the program by November 2011. Updating all hospital campus signage to designate tobacco-free areas was a first step in the partnership, including signage in multiple languages to meet community needs.\n*   DOHMH partnered with the Port Authority of New York and New Jersey to post signs prohibiting smoking in outdoor queues in April of 2011, protecting the millions of travelers who pass through its 2 airports and 2 bus terminals each year. A total of 71 signs were installed at all taxi lines at LaGuardia Airport, John F. Kennedy International Airport, Port Authority Bus Terminal, and George Washington Bridge Bus Terminal.\n*   DOHMH developed a citywide print and television media campaign that aired from May 26, 2011, to June 12, 2011, to increase public awareness of the 2010 legislation that prohibited smoking in all of NYC’s 1,700 public parks and 14 miles of beaches, as well as all pedestrian plazas. The media campaign included TV ads in English and Spanish, as well as subway and bus ads. Additionally, “no smoking” signage was posted at entrances to all parks and beaches.\n\n【20】We used data from the NYC Tobacco Behavior and Public Opinion Survey (TBPOS), a cross-sectional survey administered 3 times over 2 years, to assess New Yorkers’ exposure to environmental tobacco smoke and to evaluate changes in awareness of and support for smoke-free air rules as a result of these efforts. We also determined sociodemographic characteristics associated with support for smoke-free air rules, which could inform educational efforts on smoke-free air strategies in other jurisdictions. Similar surveys have assessed public opinion around smoke-free environments at the state and national levels (3,5,10–14). We examined changes in local support for tobacco control over a 2-year period.\n\n【21】Top of Page\n\n【22】Methods\n-------\n\n【23】DOHMH conducted 3 waves of the TBPOS (wave 1 in August 2010, wave 2 in April 2011, and wave 3 in February 2012). The TBPOS was a citywide landline and cellular-phone cross-sectional survey, which was implemented by a survey research firm and approved by the DOHMH’s institutional review board. Participants were identified through landline (90% of sample) and cell-phone (10% of sample) numbers. Participants in the landline component did not receive any compensation. Participants in the cell-phone sample were mailed a $4.50 MetroCard (public transit pass) to cover the expense of the call.\n\n【24】Interviewers called each telephone number up to 15 times, with a goal of enrolling 1,440 participants over a 1-month period, including 720 current smokers and 720 nonsmokers. Smoking status was obtained at the time of the interview as part of eligibility screening.\n\n【25】Surveys were administered in English and Spanish. Current smoking was determined by 2 questions, “Have you smoked at least 100 cigarettes in your entire life?” and “Do you now smoke cigarettes every day, some days, or not at all?” To assess perceptions of secondhand smoke exposure in the areas targeted by CPPW initiatives, participants were asked if they thought secondhand smoke was a problem 1) in outdoor public places, within a few feet of them; 2) in the areas around hospital entrances or on hospital grounds; and 3) in lines in outdoor waiting areas for a bus or taxi at NYC airports or Port Authority and George Washington Bridge bus terminals. To assess knowledge of existing no-smoking regulations, participants were asked a series of questions focused on 1) hospital entrances or on hospital grounds and 2) outdoor waiting areas for a bus or taxi. For each location, participants indicated whether they thought smoking was allowed or prohibited and whether they had noticed in the last 3 months any signs prohibiting smoking. To assess public opinion regarding smoke-free air strategies, participants were asked if they favored or opposed prohibiting smoking 1) in all public parks and 2) on all public beaches. Additional survey questions assessed sex (male/female), age in years, race/ethnicity (non-Hispanic black, non-Hispanic white, Hispanic/Latino, Asian-American/Other), language spoken most often at home, and education level. Participants were also specifically asked if they considered themselves “a Hispanic, Latino, or Spanish-speaking American.”\n\n【26】During wave 1 (pre-intervention), 685 smokers and 762 nonsmokers were surveyed; in wave 2 (pre-intervention), 715 smokers and 721 nonsmokers were surveyed; and in wave 3 (post-intervention), 718 smokers and 727 nonsmokers were surveyed. We calculated response rates and cooperation rates using the American Association for Public Opinion Research’s third definition, which does not consider partial interviews complete (15). To assess the overall representativeness of the TBPOS, we compared the demographic composition of the weighted data set (all years combined) to the 2011 Community Health Survey (CHS) population (8). We used SAS 9.2 (SAS Institute Inc, Cary, North Carolina) survey procedures to estimate weighted percentages and adjust for clustering by survey wave. Combined smoker and nonsmoker estimates were obtained by weighting observations to match smoking prevalence data that were available for NYC at the time of the survey (wave 1, 15.8%; waves 2 and 3, 14.0%) (8). Because smoking behavior was our key variable of interest, we weighted combined survey data to smoking prevalence only. We did not post-stratify the data based on demographic characteristics or telephone type.\n\n【27】We assessed postintervention changes using a set of orthogonal contrasts. The first contrast (wave 1 = .5, wave 2 = .5, wave 3 = −1) tests if the wave 3 (February 2012) outcome differed from waves 1 (August 2010) and 2 (April 2011). The second contrast (wave 1 = 1; wave 2 = −1; wave 3 = 0) tests if waves 1 and 2 differ; including the second contrast helps account for any variance between these waves. Each outcome was regressed on the 2 contrasts simultaneously in logistic regression models. Change from pre- to postintervention was indicated by a statistically significant ( _P_ < .05) coefficient on the first contrast. To adjust for potential demographic differences across waves, we ran additional regression models that controlled for age group, education, race/ethnicity, sex, and language spoken most often at home (English vs Spanish/other). We assessed change for smokers and nonsmokers separately and combined.\n\n【28】To test for demographic differences in public opinion on smoke-free air strategies, controlling for smoking status, we fit multivariable logistic regression models for support for smoke-free parks and beaches. We initially included main effects only and removed variables one at a time when the _P_ value was less than .10. We then added all possible 2-way interaction terms between smoking and significant demographic characteristics and interactions when the _P_ value was less than .10.\n\n【29】Top of Page\n\n【30】Results\n-------\n\n【31】Response rates in wave 1 were 21.3% for landlines and 8.6% for cellular phones, with cooperation rates of 50.6% for landlines and 10.1% for cellular phones; in wave 2 the response rates were 23.7% for landlines and 3.3% for cellular phones, with cooperation rates of 59.0% for landlines and 3.9% for cellular phones; and in wave 3 the response rates were 21.1% for landlines and 2.9% for cellular phones, with cooperation rates of 54.1% for landlines and 3.4% for cellular phones.\n\n【32】Compared with the CHS population, TBPOS participants were more likely to be female (63%; 95% CI, 61–65 vs 54%; 95% CI, 52–56), white/non-Hispanic (55%; 95% CI, 53–57 vs 35%; 95% CI, 34–37), and college graduates (53%; 95% CI, 51–55 vs 32%; 95% CI, 30–34) and less likely to be in the group aged 25 to 44 years (27%; 95% CI, 26–29 vs 40%; 95% CI, 38–42). These patterns were similar among smokers and nonsmokers. At wave 3 of the TBPOS, the majority of respondents overall reported that secondhand smoke was a problem in outdoor public places in NYC within a few feet of them (59%), in the areas around hospital entrances or on hospital grounds (76%), and in lines in outdoor waiting areas for a bus or taxi at NYC airports or Port Authority and George Washington Bridge bus terminals (65%). Among smokers and nonsmokers combined, there was no significant difference in these areas between pre- and postintervention surveys ( Table 1 )\n\n【33】More than half (55%) of respondents at wave 3 were aware that smoking is prohibited the areas around hospital entrances and on hospital grounds in New York City, which was a significant improvement ( _P_ \\= .04) over wave 1 (48%) and wave 2 (53%). This improvement was no longer significant ( _P_ \\= .07) after controlling for demographic characteristics. Fewer than half of respondents overall (41%) had noticed signs that prohibited smoking in areas around hospital entrances or on hospital grounds by wave 3, similar to waves 1 and 2.\n\n【34】A significantly ( _P_ < .001) greater proportion of respondents at wave 3 (32%) were aware that smoking is prohibited in bus and taxi lines at NYC airports and bus terminals than at waves 1 and 2 (26% and 27%, respectively). Only 15% of respondents overall had noticed signs prohibiting smoking in these areas by wave 3, which represented a significant ( _P_ \\= .04) increase from waves 1 and 2 (15% and 11%, respectively), even after adjusting for demographics ( _P_ \\= .007).\n\n【35】Among the combined population of smokers and nonsmokers, support for prohibiting smoking in all public parks (47% at wave 3) and on all public beaches (50% at wave 3) was consistent over the 3 survey waves ( Table 2 ).\n\n【36】Between pre- (30% and 33%, respectively) and post- (24%) waves, the proportion of smokers reporting that exposure to nearby secondhand smoke in outdoor public spaces was a problem declined significantly ( _P_ < .001), even after adjusting for demographics ( _P_ \\= .001).\n\n【37】At wave 3, 67% of smokers were aware that smoking was prohibited on hospital grounds compared with 60% at wave 1 and 61% at wave 2 ( _P_ \\= .006); the improvement remained significant after controlling for demographics ( _P_ \\= .005). Similarly, 62% of smokers had noticed signs prohibiting smoking around hospitals at wave 3, compared with 59% at wave 1 and 55% at wave 2 ( _P_ \\= .02; _P_ \\= .007 after controlling for demographics). Awareness of no-smoking regulations in outdoor taxi and bus lines also increased among smokers (31% in wave 1 and 26% in wave 2 vs 38% in wave 3, _P_ < .001 after controlling for demographics), as did smokers’ awareness of signs prohibiting smoking in outdoor lines for taxis and buses (20% in wave 1 and 15% in wave 2 vs 25% in wave 3, _P_ < .001 after controlling for demographics).\n\n【38】Among nonsmokers, there were no significant trends in reports of secondhand smoke across survey waves and no significant changes in awareness of no-smoking rules around hospitals. Awareness that smoking is prohibited in outdoor lines for taxis and buses improved somewhat ( _P_ \\= .005, _P_ < .001 after controlling for demographics) among nonsmokers between waves 1 and 2 (25% and 27%, respectively) and wave 3 (32%).\n\n【39】Although public support for smoke-free parks and beaches was stable among smokers, there was a slight decline in support for smoke-free beaches among nonsmokers ( _P_ \\= .03), with 56% and 59% in favor at waves 1 and 2, respectively, compared with 53% at wave 3. This trend was not statistically significant ( _P_ \\= .17) after controlling for demographics.\n\n【40】Table 3 displays demographic variation in support for smoke-free parks and smoke-free beaches among the overall survey population, all years combined.\n\n【41】About one third of smokers favored smoke-free parks and beaches compared with more than half of nonsmokers (Table 3). Women were more likely than men (56% vs 46% for parks and 56% vs 47% for beaches) to favor both strategies. By age group, we observed that participants aged 25 to 44 years were more likely than those 65 years and older to favor smoke-free parks (57% vs 49%) and beaches (58% vs 50%).\n\n【42】More blacks (54%), Hispanics/Latinos (62%), and Asians/other (race/ethnicities) (61%) favored smoke-free parks compared with non-Hispanic whites (47%). Similarly, more Hispanics (61%) and Asians/other (60%) favored smoke-free beaches than whites (50%). Finally, participants whose principal language at home was other than English were more likely than usual English speakers to support smoke-free parks (65% vs 50%) and beaches (62% vs 51%). With 1 exception, all of these differences persisted in the multivariable regression models that simultaneously adjusted for smoking status and demographic characteristics. Education fell out of both stepwise models, and language usually spoken at home was not significantly associated with support for smoke-free beaches. There were no significant interactions observed between smoking behavior and sex, age group, race/ethnicity, or language.\n\n【43】Top of Page\n\n【44】Discussion\n----------\n\n【45】We used trend data from the 2010–2012 TBPOS to assess the impact of public education activities on smoke-free air rules on public perception of secondhand smoke exposure, awareness of existing smoke-free air strategies, and support for smoke-free strategies that have been implemented in NYC. Among both smokers and nonsmokers, there were increases in awareness of smoke-free rules in outdoor areas around hospital entrances and grounds and in outdoor bus and taxi lines. Support among nonsmokers for new smoke-free air strategies was consistently high and stable across waves.\n\n【46】Adult New Yorkers’ survey responses were generally consistent with prior research that found that members of traditionally underrepresented groups (eg, women, minorities, immigrants, working poor) may be more likely to support smoke-free air strategies than their counterparts (16–20). Contrary to prior research results, younger adults in NYC displayed more support for prohibiting smoking in all public parks and on beaches than those aged 65 years or older. We posit that younger New Yorkers may be more accustomed to smoke-free air regulations, given changes in NYC since 2002.\n\n【47】The survey’s low response rate is its greatest limitation with regard to generalizability to NYC adults overall. NYC TBPOS response rates were somewhat lower than the New York State response to the 2009–2010 National Adult Tobacco Survey, which was 38.9% for landlines and 20.0% for cellular phones (21). Response rates for the NYC TBPOS were closer to the 2011 NYC CHS landline sample (23.7% vs 33.2%, respectively), but far lower than the cellular phone sample (3.3% vs 44.2%, respectively) (22). Future TBPOS surveys will need to improve recruitment and retention of cellular phone participants to better represent younger New Yorkers and racial/ethnic minorities.\n\n【48】The survey’s low cellular phone response rate likely contributed to the underrepresentation of minority respondents and respondents in the group aged 25 to 44 years. While our trend analyses controlled for demographic characteristics, our annual estimates may be skewed. Our analysis of the demographic associations with support from smoke-free air strategies suggested that women are far more likely than men to support these policies, potentially inflating the estimates. However this effect was likely counteracted by the overrepresentation of whites and underrepresentation of adults in the 25 to 44 years age group.\n\n【49】Our analysis does not account for variation in exposure to interventions or for variation in public opinion among smokers that could be due to current or past quit attempts or desire to quit smoking (10). Nevertheless, our observations of change in attitudes toward and awareness of smoke-free air efforts in NYC provide evidence of normative change in a climate of increased support for tobacco control.\n\n【50】These limitations do not affect our findings regarding the associations between demographic characteristics and support for smoke-free air strategies. The demographic variation that we observed provides important contextual information for community education efforts on secondhand smoke and smoke-free air strategies. We documented a pattern of increased support for smoke-free air regulations among women, racial/ethnic minorities, immigrants, and smokers with lower levels of education. These populations may, in fact, be the most vulnerable to secondhand smoke.\n\n【51】Furthermore, our results show that local efforts to increase signage and educate the public were successful in increasing population-wide knowledge about smoke-free air rules in particular areas, especially among smokers. As of 2013 over 3,000 hospitals, health care systems, or clinics in the United States had adopted smoke-free grounds rules (23), and over 600 airports were smoke-free (24). Appropriate signage and education on these rules is important for increasing compliance and access to smoke-free air. Also, over 800 jurisdictions had smoke-free parks, and over 170 jurisdictions had smoke-free beach rules across the country (25,26). Although we did not see an overall increase in support for smoke-free park and beach rules, the educational campaigns may have increased compliance and awareness in these areas as well. As jurisdictions examine existing and new smoke-free-air rules, our results provide information on ways to increase awareness of these rules.\n\n【52】Top of Page\n\n【53】Acknowledgments\n---------------\n\n【54】This article was supported in part by the New York City Department of Health and Mental Hygiene and by Cooperative Agreement number 1U58DP002419–01 from the Centers for Disease Control and Prevention – Communities Putting Prevention to Work.\n\n【55】Top of Page\n\n【56】Author Information\n------------------\n\n【57】Corresponding Author: Shannon M. Farley, Bureau of Chronic Disease Prevention and Tobacco Control, New York City Department of Health and Mental Hygiene, 42-09 28th Street, 9th Floor, CN-46, Queens, NY 11101-4312. Telephone: 347-396-4557. E-mail: sfarley@health.nyc.gov .\n\n【58】Author Affiliations: Elizabeth Needham Waddell, New York City Department of Health and Mental Hygiene and Waddell Research, Portland, Oregon; Jenna Mandel-Ricci, Susan M. Kansagra, Bureau of Chronic Disease Prevention and Tobacco Control, New York City Department of Health and Mental Hygiene, New York, New York.\n\n【59】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b8baef95-d912-411f-a741-21b2046a08e2", "title": "A Community Health Worker–Based Program for Elderly People With Hypertension in Indonesia: A Qualitative Study, 2013", "text": "【0】A Community Health Worker–Based Program for Elderly People With Hypertension in Indonesia: A Qualitative Study, 2013\n====================================================================================================================\n\n【1】ORIGINAL RESEARCH — Volume 12 — October 15, 2015\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Riana Rahmawati, MH; Beata Bajorek, PhD\n\n【5】_Suggested citation for this article:_ Rahmawati R, Bajorek B. A Community Health Worker–Based Program for Elderly People With Hypertension in Indonesia: A Qualitative Study, 2013. Prev Chronic Dis 2015;12:140530. DOI: http://dx.doi.org/10.5888/pcd12.140530 external icon .\n\n【6】PEER REVIEWED\n\n【7】PEER REVIEWED\n\n【8】On This Page\n\n【9】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【10】Abstract\n--------\n\n【11】**Introduction**\n\n【12】Hypertension is prevalent in the elderly, but treatment is often inadequate, particularly in developing countries. The objective of this study was to explore the role of a community-based program in supporting patients with hypertension in an Indonesian rural community.\n\n【13】**Methods**\n\n【14】A qualitative study comprising observation and in-depth interviews was conducted in an Integrated Health Service Post for the Elderly (IHSP-Elderly) program in Bantul district (Yogyakarta province). Eleven members of IHSP-Elderly program (ie, hypertensive patients), 3 community health workers (CHWs), and 1 district health staff member were interviewed to obtain their views about the role of the IHSP-Elderly program in hypertension management. Data were analyzed using thematic analysis.\n\n【15】**Results**\n\n【16】CHWs played a prominent role as the gatekeepers of health care in the rural community. In supporting hypertension management, CHWs served members of the IHSP-Elderly program by facilitating blood pressure checks and physical exercise and providing health education. Members reported various benefits, such as a healthier feeling overall, peer support, and access to affordable health care. Members felt that IHSP-Elderly program could do more to provide routine blood pressure screening and improve the process of referral to other health care services.\n\n【17】**Conclusion**\n\n【18】CHWs have the potential to liaise between rural communities and the wider health care system. Their role needs to be strengthened through targeted organizational support that aims to improve delivery of, and referral to, care. Further study is needed to identify the key factors for effective CHW-based programs in rural communities and the incorporation of these programs into the health care system.\n\n【19】Top\n\n【20】Introduction\n------------\n\n【21】Hypertension, a risk factor for cardiovascular disease, stroke, and premature death, is prevalent in the elderly (1). However, treatment of hypertension in the elderly is often inadequate, particularly in developing countries, where only 10% of cases are controlled (2). In Indonesia, a developing country in Southeast Asia, the prevalence of hypertension among those aged 65 or older is 60%, making it the most common chronic disease (3).\n\n【22】One key strategy to help scale up hypertension management is to involve the community; patient empowerment is the cornerstone of primary health care and chronic disease management (4). Programs involving community health workers (CHWs) in the United States have increased the proportion of patients with controlled blood pressure (5,6). The role of CHWs in facilitating access to health care services for patients with particular diseases has been studied in some developing countries (7). For example, a study in Uganda described the role of CHWs in supporting the management of pneumonia and malaria (8). Few studies describe CHWs’ role managing hypertension in developing countries (9).\n\n【23】In Indonesia, the Integrated Health Service Post for the Elderly (IHSP-Elderly), a national CHW-based program, has been promoting healthy aging since 1997; the program focuses on screening for and managing chronic diseases in low socioeconomic population subgroups (10). This study explored how the IHSP-Elderly programs could play a role in improving hypertension management for patients in rural communities in the Bantul district, Yogyakarta. In this district, hypertension is one of the most common diseases treated in primary health care facilities (11). The perspectives of CHWs, members of the IHSP-Elderly program (ie, patients), and local government health policy advisors were examined.\n\n【24】Top\n\n【25】Methods\n-------\n\n【26】### Study setting and design\n\n【27】This qualitative study took place in a village in the rural Bantul district of Yogyakarta province in Indonesia (Box) in March 2013. Yogyakarta province (population, 3.5 million) is in Java, the country’s most populous island; almost 52% of 238 million Indonesians live there (14). The life expectancy in Yogyakarta province is 74.1 years (15).\n\n【28】Box. Bantul District, Yogyakarta Province, Indonesia\n\n【29】Population, 930,276 (12)\n\n【30】Elderly population (aged ≥60 y), 110,729 (12)\n\n【31】Economy based on subsistence agriculture (12)\n\n【32】Muslim majority (91%) (12)\n\n【33】861 of 933 subvillages have an IHSP-Elderly program (13)\n\n【34】Sociocultural climate related to the implementation of IHSP-Elderly program:\n\n【35】• Both men and women are members\n\n【36】• Most (90%) community health workers are women (13)\n\n【37】• Freedom of conversation between sexes is accepted\n\n【38】• Freedom of expression is accepted (ie, people can freely exchange their views)\n\n【39】• Music and physical exercise in the IHSP-Elderly program are welcomed by members (12)\n\n【40】• Home visitation by CHWs can be undertaken without security concerns\n\n【41】This study used observation and in-depth face-to-face interviews to explore perspectives on the role of the IHSP-Elderly program in the management of hypertension. A semistructured approach used a purpose-designed interview guide ( Table 1 ), developed after reviewing the relevant literature (16–18). A separate set of questions was used for each group of participants. The questions were pilot tested for clarity and relevance. The validity of findings was confirmed through independent analysis of data by the researchers and through respondent validation.\n\n【42】All participants were informed about the study via a purpose-designed information sheet and were required to provide written consent before participation. For 3 participants who were unable to read fluently, a researcher provided a comprehensive verbal explanation of study requirements before obtaining their consent. All participants were literate enough to write their name and provide a written signature. No participants received a financial reward for their interview. The study was approved by the Bantul district health office responsible for authorizing the conduct of research studies. The study protocol and other supporting documents were reviewed. Recruitment of participants began after approval was obtained to ensure no conflict of interest.\n\n【43】### Recruitment of participants\n\n【44】Nationally, there are 69,500 IHSP-Elderly programs (19). Our study’s sampling frame comprised 861 IHSP-Elderly programs in 27 community health centers in the Bantul District. First, an online random number generator was used to select 1 community health center and then 1 IHSP-Elderly program. In the village selected for participation, the elderly population consisted of 117 men and women; the local IHSP-Elderly program provides care for up to 70 members, and it has 3 CHWs.\n\n【45】The study had 3 types of participants: members of the IHSP-Elderly program (patients), CHWs serving the IHSP-Elderly program, and a district health staff member. Patients were purposely sampled according to the following inclusion criteria: aged 60 years or older, active in the IHSP-Elderly program for at least 1 year, and with diagnosed hypertension (confirmed by IHSP-Elderly record, systolic blood pressure ≥140 mm Hg and ≥diastolic blood pressure 90 mm Hg). The appropriate sample size for members was initially determined to be 10 (20,21), but we recruited an additional patient because theme saturation was not reached until we interviewed 11 members. All 3 CHWs serving the IHSP-Elderly program were invited to participate in the study and participated. These CHWs were local residents who were trained to serve the IHSP-Elderly program, and each had done so for at least 3 years. The head of the district health office designated the staff member in charge of the IHSP-Elderly program as the participant representing the district health office.\n\n【46】### Data collection and analysis\n\n【47】A typical IHSP-Elderly event (a weekly meeting) was observed to collect descriptive information on activities and interactions (verbal and nonverbal behavior) among members and CHWs. Each member pays 1,000 Indonesian rupiah (IDR) (<USD $0.10) to attend a meeting. In comparison, the standard patient fee for a consultation in the community health center is 9,000 IDR (22) and about 50,000 IDR in a private clinic. Relative to the average income for Bantul residents (gross regional domestic product in 2013 was 12.1 million IDR per capita \\[12\\]), the service fee for the IHSP-Elderly program is affordable.\n\n【48】Demographic data for IHSP-Elderly members were extracted from IHSP-Elderly register books and interviews, then tabulated and descriptively analyzed. IHSP-Elderly members were interviewed until theme saturation was reached. Interviews were audio-recorded and transcribed in a local language (Javanese or Bahasa). Each transcript was analyzed by using thematic analysis via manual inductive coding. The first author (R.R.) translated the transcript from Javanese or Bahasa to English, and external translators, who were not involved in the study, verified the translation. The word use in the translation reflected the language capability of the participants. The coding framework was generated by the first author and subsequently reviewed and validated by the second author (B.B.) To ensure the validity of the analysis, the emergent themes were compared with the transcribed notes and verified by 5 participants (3 patients, 1 CHW, and 1 district health staff member) who volunteered to provide feedback.\n\n【49】Top\n\n【50】Results\n-------\n\n【51】Of the 15 participants, 11 were women; the mean age of the patients was 69.8 (standard deviation, 9.2) years ( Table 2 ).\n\n【52】### Field observation of a weekly meeting of the IHSP-Elderly program\n\n【53】The IHSP-Elderly meeting was attended by 25 members (18 women and 7 men). The 2-hour session was conducted in the afternoon in the front yard of a CHW’s house. The meeting began with a blood pressure check for each member, followed by group physical exercise. Most of the IHSP-Elderly members queued to get their blood pressure checked. The participants were jovial and appeared to have a good time; they shared jokes with CHWs and there was a lot of laughter. They made comments and suggestions freely each time the CHWs explained the blood pressure readings.\n\n【54】The physical exercise, featuring traditional music and simple verbal instructions, was facilitated by an instructor. The exercise was guided by appropriate imagery such as looking at the moon, water paddling, fist clenching, flying, and wave making. Most members followed the instructions with ease. A few had difficulty, but they tried to mimic the instructor’s gestures. Every now and then there was a ripple of their laughter. After the exercise finished, the CHWs provided light refreshments for all members.\n\n【55】### Thematic analysis of interviews\n\n【56】Three key themes emerged from the interviews: the central role of CHWs, multiple benefits for the elderly, and current limitations to providing optimal services.\n\n【57】#### Central role of CHWs in the community\n\n【58】The district health staff member emphasized the critical role of the IHSP-Elderly program in addressing hypertension in the community. She stressed that the program originated in the community to serve the community: “IHSP-Elderly plays a strategic role in our healthy aging program. It is a program by community and for community; as you know there are currently many elderly people. We can say that CHWs are health gatekeepers for the rural community.”\n\n【59】CHWs organized weekly physical exercise and monthly meetings, provided the venue for meetings, organized equipment and refreshments, planned materials and speakers for public education, and reported the IHSP-Elderly programs to the community health center. They seemed to be highly dedicated to their roles and to volunteering their time to serving the community in this way. Being unpaid volunteers, they were motivated by intangible benefits, such as the appreciation of community leaders, the trust of the community health center and members of IHSP-Elderly program, and the ability to contribute to the wellness and happiness of the community. “I love to share what I know with the elderly. Giving benefits to others makes me happy” (CHW).\n\n【60】> I have been active as a CHW since 1997, with the trust of the community health center and village leader. They believe that I am still the best . . . despite my infirmity as I am getting older. The IHSP-Elderly members also ask me to remain \\[as a CHW coordinator\\]. For me personally, managing the IHSP-Elderly is lovely work.\n\n【61】In addition, the CHW prepared invitations and arranged home visits to increase participation of IHSP-Elderly members: “Elderly people tend to be forgetful. Giving invitations will remind them to come” (CHW). “In case there were inactive members, we would come to their house, greet them, and wish them all good health” (CHW).\n\n【62】CHWs also arranged public education programs on the physical, emotional, and spiritual aspects of healthy aging. These monthly programs featured both internal speakers (CHWs) and external speakers. The value of using CHW as speakers was their creativity in facilitating interactive discussions and encouraging elderly members to participate.\n\n【63】#### Multiple benefits for the elderly\n\n【64】**Being healthier.** The desire of older members to be healthier was the most motivating factor for participating in the IHSP-Elderly program. In particular, they perceived immediate health benefits from attending the physical exercise sessions, such as sleeping better and feeling more relaxed. “I feel better after the physical exercise. It’s quite easy for me. It makes me healthier” (Member). “It makes me healthier, relaxes my breathing, and I can sleep well after exercise. It is also helpful for my knee pain” (Member). “Members who feel healthier will be active in the IHSP-Elderly program” (CHW).\n\n【65】**Having peer support.** Peer support was an important motivator for participating in the IHSP-Elderly program: “I joined IHSP-Elderly a long time ago. It is hard for me now to follow physical exercise, getting older limits me from doing exercise, but CHWs motivated me just to come, and I enjoy the meet-and-greet with the peers” (Member). “One member, with a walker, always comes and joins IHSP-Elderly activities. She is glad to meet her peers and gets new information” (CHW). “The benefits come from the joyful meetings. They greet each other weekly. We can say that the active members are healthier than other elderly” (CHW).\n\n【66】**Accessing free health care.** Participation in the IHSP-Elderly program was also driven by the free access to health care. Members of IHSP-Elderly program were highly enthusiastic about having access to free blood pressure checks, regardless of who measured their blood pressure (the CHWs, health staff, or others). “Being examined is exciting, you know, as an elderly people, what a great opportunity we have to get an examination for free” (Member). “I am very glad. I always try to come if a blood-pressure check is announced” (Member). “I just put on the invitation that there will be blood pressure checks. They would be enthusiastic to come” (CHW). “They \\[CHWs\\] can do it, but sometimes it is done by CHC \\[community health center\\] staff. Three months ago there were medical students who also offered a blood-pressure checking program for free. That is okay, \\[whoever\\]” (Member).\n\n【67】#### Current limitations to providing optimal services\n\n【68】Although the members were enthusiastic about the CHW-led activities of the IHSP-Elderly program, they felt that there were some inconsistencies in what was offered; for instance, blood pressure checks were not done routinely. IHSP-Elderly meetings were conducted weekly, but only physical exercise could be offered weekly. The members suggested that a routine follow-up of blood pressure readings should also be arranged at least once a week. “It is not a routine program. If I am not mistaken, the last blood pressure check other than the one today was done last month, or two months ago. It would be lovely if they could make it weekly” (Member). “In some IHSP-Elderly, members can check their blood pressure. But as far as I know, not all of them have a capable CHW to do that” (District health staff member).\n\n【69】Additionally, the IHSP-Elderly program had only 1 properly functioning aneroid sphygmomanometer to measure blood pressure. This device was purchased in 2007 and had not been recalibrated in 6 years. There was no funding to purchase a new device. In this IHSP-Elderly program, only 1 CHW (female, 40 years old) was properly trained and had enough confidence to measure blood pressure; the other 2 CHWs did not feel as capable. “Actually I have been trained, but I felt nervous when people queued around me. I was shaking” (CHW). “She \\[the other CHW\\] did not come for a while because she fractured her hand. During that time, I could not take over her task \\[to measure blood pressure\\] as I am not skillful yet” (CHW).\n\n【70】CHWs recognized that their role in managing hypertension was restricted to early screening and monitoring, followed by referring patients to other health service providers. However, this study identified a gap between what elderly members wanted and what the IHSP-Elderly program could provide. Members preferred a simpler care pathway in which the CHWs could provide easier access to hypertension treatment: “I wish I could receive medication here \\[in IHSP-Elderly program\\] so that I needn’t go anywhere else” (Member).\n\n【71】Although the CHC provides a mobile health service for patient follow-up (including further examination and provision of medication), most IHSP-Elderly members did not use this service. Access to the mobile service was difficult; members had to travel approximately 3 kilometers to reach it. Furthermore, because hypertension is symptomless, some members were not motivated to travel. The following quotes illustrate the barriers to using the mobile health service:\n\n【72】> When we identified hypertension cases, we would ask the patients to visit mobile health services, but most of them had difficulties accessing that service. I think it was related to transportation. It might also be because they did not feel any serious symptoms (CHW).\n\n【73】“I couldn’t go there. No one could take me over there” (Member). “I have told CHC staff that the members might not visit the mobile service, they need to ask their children or grandchildren to take them, and it is not easy in the working hours” (CHW).\n\n【74】Top\n\n【75】Discussion\n----------\n\n【76】This study highlights the potential benefits of involving CHWs in improving hypertension management in a developing country (Indonesia), supporting the findings of previous studies focusing on the role of CHWs in hypertension management (7,8,23). CHWs serve as a bridge between the community and the health care system, addressing community beliefs about diseases and supporting patients’ adherence to therapies, which is difficult to achieve in traditional health care settings (24,25). For example, nonpharmacological approaches play an important role in hypertension management, and CHW-based programs can offer exercise and health education activities. Some studies report that nonpharmacological approaches are not so well addressed as pharmacological approaches are in doctor–patient consultations (26,27). A CHW-based program may be well positioned to support hypertension management by providing information about lifestyle choices.\n\n【77】Self-monitoring blood pressure improves blood pressure control (28), but self-monitoring is difficult to facilitate in a low-resource population. Providing free blood pressure checks in a CHW-based program presents an opportunity to monitor patients between scheduled health service visits. Moreover, the patients’ enthusiasm for having their blood pressures checked in an IHSP-Elderly program is a good base on which to establish follow-up activities or to enhance self-management.\n\n【78】The limited capability of CHWs and a lack of resources (eg, calibrated sphygmomanometers) preclude a greater role for CHWs in providing regular blood pressure monitoring for IHSP-Elderly members. Research on the problems related to levels of infrastructure and CHW skill highlights the need for adequate training and supervision for CHWs (29–31). Training could be focused on how to 1) differentiate grades of hypertension, 2) help IHSP-Elderly members develop care plans for self-management, and 3) refer patients to other health care services. So that CHWs can provide accurate and up-to-date information to patients about medication for hypertension, training on the key messages about antihypertensive agents (their types, roles, and side effects) is needed, particularly in geographic areas with a high prevalence of hypertension and poor access to information. In such areas, demand for services from CHWs is high because of the constraints faced by the elderly in their ability to access existing follow-up services.\n\n【79】This study is the first of its kind from Indonesia to highlight the perspectives of patients, CHWs, and health policy advisors about a CHW-based program for hypertension management. Because of the methodology of qualitative studies and the fact that this study took place in 1 rural district and had a small sample, the results of this study cannot be deemed to represent the entire Indonesian population. However, because Bantul District is a typical rural district in Indonesia and the IHSP-Elderly program is a national program, our results support the likelihood that IHSP-Elderly program could play a larger role in CHW-based management of hypertension in other rural areas, even considering the barriers that may be encountered. Moreover, the findings of this study provide insights for the development of CHW-based programs in other settings with a similar social context.\n\n【80】This study highlighted how CHWs have the potential to liaise between rural communities and the wider health care system. The role of CHWs needs to be strengthened through targeted organizational support that aims to improve delivery of health care services and, when needed, referral to additional health care services. Further study is needed to identify the key factors for effective CHW-based programs in rural communities and the incorporation of these programs into the health care system.\n\n【81】Top\n\n【82】Acknowledgments\n---------------\n\n【83】This study received no specific grant from any funding agency in the public, commercial, or nonprofit sectors. We are grateful for contributions from all study participants. We also acknowledge the Head of Bantul Health District for permitting this study.\n\n【84】Top\n\n【85】Author Information\n------------------\n\n【86】Corresponding Author: Riana Rahmawati, MH, Graduate School of Health, School of Pharmacy, University of Technology Sydney, CB 01.13, PO Box 123, Broadway NSW 2007, Australia. Telephone: +61 2 9514 9224. Email: Riana.Rahmawati@student.uts.edu.au .\n\n【87】Author Affiliations: Beata Bajorek, Graduate School of Health, School of Pharmacy, University of Technology Sydney, Australia, and Department of Pharmacy, Royal North Shore Hospital, Sydney, Australia. Dr Rahmawati is also affiliated with Pharmacology Department, Islamic University of Indonesia, Sleman, Yogyakarta, Indonesia.\n\n【88】Top\n\n【89】References\n----------\n\n【90】1.  World Health Organization. Global health risks: mortality and burden of disease attributable to selected major risks: Geneva (CH): World Health Organization; 2009. p. 16.\n2.  Lloyd-Sherlock P, Beard J, Minicuci N, Ebrahim S, Chatterji S. Hypertension among older adults in low- and middle-income countries: prevalence, awareness and control. Int J Epidemiol 2014;43(1):116–28. CrossRef external icon PubMed external icon\n3.  Indonesia Ministry of Health. Basic health research \\[Riset kesehatan dasar\\]. Jakarta (ID): National Institute of Health Research and Development, Ministry of Health; 2013.\n4.  Brownstein JN, Bone LR, Dennison CR, Hill MN, Kim MT, Levine DM. Community health workers as interventionists in the prevention and control of heart disease and stroke. Am J Prev Med 2005;29(5 Suppl 1):128–33. CrossRef external icon PubMed external icon\n5.  Brownstein JN, Chowdhury FM, Norris SL, Horsley T, Jack L Jr, Zhang X, et al. Effectiveness of community health workers in the care of people with hypertension. Am J Prev Med 2007;32(5):435–47. CrossRef external icon PubMed external icon\n6.  Krantz MJ, Coronel SM, Whitley EM, Dale R, Yost J, Estacio RO. Effectiveness of a community health worker cardiovascular risk reduction program in public health and health care settings. Am J Public Health 2013;103(1):e19–27. CrossRef external icon PubMed external icon\n7.  Pallas SW, Minhas D, Pérez-Escamilla R, Taylor L, Curry L, Bradley EH. Community health workers in low- and middle-income countries: what do we know about scaling up and sustainability? Am J Public Health 2013;103(7):e74–82. CrossRef external icon PubMed external icon\n8.  Rutebemberwa E, Kadobera D, Katureebe S, Kalyango JN, Mworozi E, Pariyo G. Use of community health workers for management of malaria and pneumonia in urban and rural areas in eastern Uganda. Am J Trop Med Hyg 2012;87(5 Suppl):30–5. CrossRef external icon PubMed external icon\n9.  Reidpath DD, Ling ML, Yasin S, Rajagobal K, Allotey P. Community-based blood pressure measurement by non-health workers using electronic devices: a validation study. Glob Health Action 2012;5(0):14876. CrossRef external icon PubMed external icon\n10.  Pedoman pelaksanaan posyandu lanjut usia \\[Implementation guideline of integrated health service post for the elderly\\]. Jakarta (ID): Komite Nasional Lanjut Usia \\[National Commission for Older Persons\\]; 2010.\n11.  Dinas Kesehatan Kabupaten Bantul \\[Bantul District Health Office\\]. Yogyakarta (ID): Profil Kesehatan Kabupaten Bantul \\[Bantul District Health Profile\\]; 2014. p. 14–6.\n12.  Pemerintah Kabupaten Bantul \\[Government of the Bantul District\\]. The harmony of nature and culture. Yogyakarta (ID): Database Profil Kabupaten Bantul \\[Database Profile of the Bantul District 2013\\]; 2013.\n13.  Keputusan Bupati Bantul tentang Daftar Penerima dan Besaran Penerimaan Bantuan Sosial Posyandu Balita, Posyandu Lansia dan Poskestren \\[Decree of the head of Bantul district about recipient list of government subsidy for Integrated health service post, Integrated health service post for the elderly and Pesantren health post\\]. Appendix II. Bantul. Yogyakarta (ID): Bantul District; 2012.\n14.  Statistics Indonesia. Population of Indonesia by province, regency/city, and subregency/subcity. The result of the 2010 population census. http://sp2010.bps.go.id/files/ebook/pop%20indo\\_kab\\_kota\\_kec\\_rev20101224/index.html. Accessed September 02, 2015.\n15.  Statistics Indonesia. Infant mortality rate and life expectancy population Indonesia. The result of 2010 population census. http://sp2010.bps.go.id/files/ebook/angka%20kematian%20bayi%20sp2010/index.html. Accessed June 25, 2014.\n16.  Javanparast S, Baum F, Labonte R, Sanders D. Community health workers’ perspectives on their contribution to rural health and well-being in Iran. Am J Public Health 2011;101(12):2287–92. CrossRef external icon PubMed external icon\n17.  Suri A, Gan K, Carpenter S. Voices from the field: perspectives from community health workers on health care delivery in rural KwaZulu-Natal, South Africa. J Infect Dis 2007;196(Suppl 3):S505–11. CrossRef external icon PubMed external icon\n18.  South J, Kinsella K, Meah A. Lay perspectives on lay health worker roles, boundaries and participation within three UK community-based health promotion projects. Health Educ Res 2012;27(4):656–70. CrossRef external icon PubMed external icon\n19.  Ministry of Health, Republic of Indonesia. \\[The importance of community and family to improve quality of life of hypertensive patient\\]. http://www.depkes.go.id/index.php?vw=2&id=2313. Updated May 31, 2013. Accessed June 20, 2014.\n20.  Morse JM. Determining sample size. Qual Health Res 2000;10(1):3–5. CrossRef external icon\n21.  Sandelowski M. Sample size in qualitative research. Res Nurs Health 1995;18(2):179–83. CrossRef external icon PubMed external icon\n22.  Peraturan Daerah Kabupaten Bantul No. 09 Tahun 2011. Retribusi Jasa Umum. \\[Regional Regulation of the Bantul District No. 09 Year 2011\\]. \\[General Service Retribution\\]. Yogyakarta (ID): Bantul District; 2011.\n23.  Balcazar HG, Byrd TL, Ortiz M, Tondapu SR, Chavez M. A randomized community intervention to improve hypertension control among Mexican Americans: using the promotoras de salud community outreach model. J Health Care Poor Underserved 2009;20(4):1079–94. CrossRef external icon PubMed external icon\n24.  Rosenthal EL, Brownstein JN, Rush CH, Hirsch GR, Willaert AM, Scott JR, et al. Community health workers: part of the solution. Health Aff (Millwood) 2010;29(7):1338–42. CrossRef external icon PubMed external icon\n25.  Viswanathan M, Kraschnewski J, Nishikawa B, Morgan LC, Thieda P, Honeycutt A, et al. ; RTI International-University of North Carolina Evidence-based Practice Center. Outcomes of community health worker interventions. Evid Rep Technol Assess (Full Rep) 2009;(181):1–144, A1–2, B1–14 passim. PubMed external icon\n26.  Bezreh T, Laws MB, Taubin T, Rifkin DE, Wilson IB. Challenges to physician-patient communication about medication use: a window into the skeptical patient’s world. Patient Prefer Adherence 2012;6:11–8. PubMed external icon\n27.  Heymann AD, Gross R, Tabenkin H, Porter B, Porath A. Factors associated with hypertensive patients’ compliance with recommended lifestyle behaviors. Isr Med Assoc J 2011;13(9):553–7. PubMed external icon\n28.  Agarwal R, Bills JE, Hecht TJ, Light RP. Role of home blood pressure monitoring in overcoming therapeutic inertia and improving hypertension control: a systematic review and meta-analysis. Hypertension 2011;57(1):29–38. CrossRef external icon PubMed external icon\n29.  Jackson EJ, Parks CP. Recruitment and training issues from selected lay health advisor programs among African Americans: a 20-year perspective. Health Educ Behav 1997;24(4):418–31. CrossRef external icon PubMed external icon\n30.  Standing H, Chowdhury AM. Producing effective knowledge agents in a pluralistic environment: what future for community health workers? Soc Sci Med 2008;66(10):2096–107. CrossRef external icon PubMed external icon\n31.  WHO Study Group on Community Health Workers. Strengthening the performance of community health workers in primary health care. World Health Organization technical report series; no. 780. Geneva (CH): World Health Organization; 1989; 46 p.\n\n【91】Top\n\n【92】Tables\n------\n\n【93】#####  Table 1. Interview Guide for Participants, Study of the Role of a Community-Based Program for Elderly People With Hypertension, Indonesia, 2013\n\n| Questions for members of Integrated Health Service Post (IHSP) for the Elderly |\n| --- |\n| 1\\. How long have you had hypertension (since early diagnosis)? |\n| 2\\. What kind of symptoms do you have relating to hypertension? |\n| 3\\. What kind of medication have you taken? |\n| 4\\. What is your opinion about IHSP-Elderly activities? |\n| 5\\. Why do you think the activities of the IHSP-Elderly are useful? |\n| 6\\. What do you think about blood pressure examination by CHWs in the IHSP-Elderly? |\n| 7\\. What is your opinion about the role of CHWs in the IHSP-Elderly to support hypertension treatment? |\n| 8\\. How can the IHSP-Elderly improve its service in hypertension management? |\n| **Questions for community health workers** |\n| 1\\. What kind of activities does the IHSP-Elderly organize for healthy aging? |\n| 2\\. Since you are a volunteer, what do you expect to be actively involved in the IHSP-Elderly? |\n| 3\\. What do you usually do after diagnosing members with high blood pressure? |\n| 4\\. How do you get the necessary information and skills relating to hypertension? |\n| 5\\. What are the barriers to implementing the hypertension program? |\n| 6\\. What are your suggestions for future programs? |\n| **Questions for district health staff member** |\n| 1\\. Why is the IHSP-Elderly program important for communities in the district? |\n| 2\\. What has the government done to support the IHSP-Elderly program? |\n| 3\\. What are the expectations of the district health office for the IHSP-Elderly in regard to the hypertension program? |\n\n【95】#####  Table 2. Characteristics of Participants (N = 15), Study of the Role of a Community-Based Program for Elderly People with Hypertension, Indonesia, 2013\n\n| Characteristic | Members of IHSP-Elderly Program (n = 11) | Community Health Workers (n = 3) | District Health Staff (n = 1) |\n| --- | --- | --- | --- |\n| **Age, mean (SD), y** | 69.8 (9.2) | 55.7 (14.5) | 41 |\n| **Women, n** | 8 | 2 | 1 |\n| **Education** | **Education** | **Education** | **Education** |\n| Below primary school | 6 | 0 | 0 |\n| Primary school | 2 | 0 | 0 |\n| High school | 3 | 3 | 0 |\n| University | 0 | 0 | 1 |\n| **Blood pressure, mean (SD), mm Hg** | **Blood pressure, mean (SD), mm Hg** | **Blood pressure, mean (SD), mm Hg** | **Blood pressure, mean (SD), mm Hg** |\n| Systolic | 165.5 (15.1) | NA | NA |\n| Diastolic | 94.5 (5.2) | NA | NA |\n\n【97】Abbreviations: IHSP-Elderly, Integrated Health Service Post for the Elderly; NA, not available; SD, standard deviation.\n\n【98】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c379bd9b-5ae6-498d-a8ef-1f7b30aa597d", "title": "Sorbitol-fermenting Escherichia coli O157, Scotland", "text": "【0】Sorbitol-fermenting Escherichia coli O157, Scotland\n**To the Editor** : Verotoxin-producing _Escherichia coli_ (VTEC) of serogroup O157 causes severe gastrointestinal and renal illness; clinical signs may be mild diarrhea, hemorrhagic colitis, or hemolytic uremic syndrome (HUS). Typically, 10%–15% of reported VTEC infections quickly progress to HUS ( _1_ ). Sorbitol-fermenting (SF)–O157 strains have emerged in continental Europe ( _2_ , _3_ ). Some evidence suggests that SF-O157 is more frequently associated with HUS than are non-sorbitol–fermenting strains ( _3_ – _6_ ). SF-O157 shows increased adherence to colonic epithelial cells and may in turn cause a more potent inflammatory host response, resulting in a higher risk for HUS ( _4_ ). The potentially greater virulence of SF-O157 requires urgent identification of its reservoir(s) and vehicle(s) of infection, as well as determination of genetic or other predisposing factors for infection with this strain. To understand whether the host pathophysiologic responses to SF-O157 and non–SF-O157 strains differ, we analyzed a cohort of children with HUS who were infected with _E. coli_ O157.\n\n【1】During April and May 2006, Health Protection Scotland (HPS) identified 18 cases of verotoxin-producing SF-O157 infection in Scotland, 13 of which were associated with a nursery. HUS developed in 8 of the 18 patients; those with thrombotic microangiopathy were admitted to the renal unit of a specialist pediatric hospital, which immediately reports cases of HUS to HPS as part of national surveillance ( _7_ ). To test the hypothesis that SF-O157 was more virulent than non–SF-O157, we performed an age-matched, nested case–case study of HUS case-patients and analyzed host clinical markers, treatment, and outcomes from SF-O157 and non–SF-O157 cases in 2006. Clinical questionnaires, patient information sheets, and consent forms were completed by clinicians for each case-patient and returned to HPS; data were entered into a database in Epi Info version 6 (Centers for Disease Control and Prevention, Atlanta, GA, USA).\n\n【2】Statistical analysis by _t_ test showed that nadirs for serum albumin were significantly higher for children with SF-O157 HUS (p = 0.03; Table ) than for children with non–SF-O157 HUS and that children with SF-O157 HUS had significantly more sessions of hemodialysis than did children with non–SF-O157 HUS (p = 0.01; Table ). All case-patients were oligoanuric; the 2 groups did not differ with respect to this parameter. Initial signs and symptoms were similar for both sets of patients, i.e., classic VTEC symptoms of bloody diarrhea and abdominal pain. This finding is in acccordance with those of other studies of SF-O157 outbreaks, which also noted signs and symptoms compatible with VTEC-associated gastroenteritis ( _5_ , _6_ ).\n\n【3】Our study highlights a number of lessons. Medical practitioners rarely have the opportunity to recognize patients at such an appreciable and predictable risk of progressing rapidly to anuric renal failure as they do when they see children with early O157 infection. Failure to appreciate the potential gravity of O157 infection and the possible development of HUS may result in avoidable illness and even death. Our investigation of the prehospital management of SF-O157 and non–SF-O157 in this cohort found no difference in pharmacologic intervention or duration of delay in admission to hospital.\n\n【4】Our study has limitations. A number of patients in the cohort were prescribed antimicrobial drugs and/or antimotility drugs or were sent home from the local hospital without hospital admission or further monitoring; such actions potentially exacerbate clinical outcomes ( _1_ , _8_ ). We recognize that comparison of the SF-O157 outbreak strain with non–SF-O157 strains (some of which caused sporadic cases) may be a potential confounding factor in the analysis. However, recently published work has indicated no statistically significant differences in the verotoxin proteins encoded by SF-O157 or non–SF-O157 strains or in their level of toxicity ( _9_ ). Other virulence factors may contribute to increased likelihood of HUS ( _4_ ).\n\n【5】Our data suggest that infection with SF-O157 results in less severe colitis than does the more common non–SF-O157 infection. Less severe colitis could result in a lower risk for renal disease because less verotoxin would be translocated into the bloodstream and bound to the kidneys. However, patients infected with SF-O157 had anuria for longer periods and consequently had longer sessions of peritoneal and hemodialysis. Although unknown bacterial or host inflammatory cytokines may contribute to enhanced disease progression, this observation is surprising and requires further investigation. Additional research is needed to learn more about the virulence of SF-O157 strains and establish other host factors that contribute to disease progression.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b1e77585-0c37-4d26-b5cc-b93ce5479093", "title": "Psittacosis", "text": "【0】_Chlamydia psittaci_ is a type of bacteria that often infects birds. Less commonly, these bacteria can infect people and cause a disease called psittacosis. Psittacosis can cause mild illness or pneumonia (lung infection). To help prevent this illness, follow good precautions when handling and cleaning birds and cages.\n\n【1】About psittacosis\n\n【2】For clinicians and laboratorians\n\n【3】Surveillance and reporting\n\n【4】2017 compendium\n\n【5】The National Association of State Public Health Veterinarians released an updated Compendium of Measures to Control _Chlamydia psittaci_ Infection Among Humans (Psittacosis) and Pet Birds (Avian Chlamydiosis). View the Compendium and related resources .\n\n【6】How do you say that?\n\n【7】Find pronunciation guides below for common words associated with psittacosis.\n\n【8】*   Chlamydia — klah-MID-e-a\n*   Pneumonia — noo-MOAN-yah\n*   Psittaci — SIT-ah-see\n*   Psittacosis — sit-ah-KOH-sis", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "46a71c38-5606-438c-81aa-7fb9d18fd209", "title": "Primary Care Surveillance for Acute Bloody Diarrhea, Wales", "text": "【0】Primary Care Surveillance for Acute Bloody Diarrhea, Wales\nThe number of cases of acute bloody diarrhea in the United Kingdom, and elsewhere, is unknown. In addition, while some established etiologic agents of acute bloody diarrhea (Salmonella, Campylobacter, Shigella and Shiga toxin-producing Escherichia coli \\[STEC\\] O157) are routinely detected in local diagnostic laboratories, others, such as non-O157 STEC, are not and therefore would not be ascertained by current surveillance. In the United Kingdom, all residents are registered with a general practitioner (a primary-care physician). Since blood in the stool is disturbing for patients, they likely would report it to their general practitioners. We therefore used surveillance by a sentinel group of volunteer general practitioners to estimate cases of acute bloody diarrhea in Wales, detect any clusters, and identify the etiologic agents.\n\n【1】An established network of 34 volunteer general practitioners' offices in Wales (combined registered practice population 223,465 in 1998, representing 8% of the 2,933,324 population) routinely reports cases of measles, mumps, rubella, shingles, chickenpox, influenza, whooping cough, and infective gastroenteritis to the Public Health Laboratory Service Communicable Disease Surveillance Centre (CDSC) (Wales) ( 1 , 2 ). Acute bloody diarrhea was introduced as a reporting category in February 1997, with a clinical case definition of \"diarrhea and visible blood of acute onset (first episode only).\" Reporting was weekly by paper; data items recorded were age, sex, practice name, and report date. From February 1997 to December 31, 1998, stool samples, requested from the patient by the general practitioner, were submitted to the local clinical diagnostic laboratory. Bacteriologic tests were performed for Salmonella, Campylobacter, Shigella, and STEC O157 by culture; ova and parasites were examined by microscopy. Laboratories were contacted for results.\n\n【2】Data were analyzed by person, time, and place. Clusters were defined as three or more cases of acute bloody diarrhea reported in any 2 consecutive weeks by the same practice. (A practice is required, contractually, to specify to the health authority a defined geographic area where all its registered patients should reside.) We used the Poisson distribution to calculate 95% confidence intervals (CIs) for the number of cases; rates were calculated by using the combined practice population of 223,465 as the denominator.\n\n【3】A total of 81 cases of acute bloody diarrhea were reported during the 23-month study period (mean annual rate, 18 per 100,000); 31 cases (95% CI = 21 to 44) were reported from February through December 1997, and 50 cases (95% CI = 37 to 65) during 1998. The age-sex distribution was similar each year, with the highest incidence in young children ages ≤4 years, particularly girls, and the lowest in adults over 65 years. The age distribution differed by sex: rates did not vary in male patients >5 years of age but were high in 25- to 44-year-old women ( Table 1 ). No seasonal distribution pattern was noted.\n\n【4】In 1997, three clusters were identified, one with four cases, and two with three cases each. Within the first cluster, Campylobacter was isolated in two cases, but in the other two clusters both Salmonella and Campylobacter were isolated. General practitioners' records did not show links between cases within these clusters. In 1998, two clusters were identified. In the first, five persons had Salmonella; three patients belonged to the same family, but the other two were not linked to the family or to each other. The second cluster was composed of two sisters with Salmonella and two unlinked cases, one Salmonella and one Campylobacter.\n\n【5】Stool specimens were obtained from 74 (91%) of 81 patients with acute bloody diarrhea. Salmonella organisms were isolated from 30 (41%) patients, 11 male and 19 female, ages 8 months to 74 years. Campylobacter was isolated from 29 (39%) patients, 13 male and 16 female, ages 1 month to 65 years. All five cases of acute bloody diarrhea in men ages 25-34 years were Campylobacter. Shigella was isolated in one case, a woman in the 35- to 44-year age-group. STEC O157 was not detected (upper 95% CI = 3.7 cases, equivalent to 1.6 per 100,000 total population in Wales). Cryptosporidium was detected in one case. A total of 13 (18%) cases (7 in male, 6 in female patients) were undiagnosed by the local laboratory. Stool specimens in 3 of these cases were sent to the Laboratory of Enteric Pathogens, Central Public Health Laboratory, Colindale, London, for testing for other STEC serogroups; none was detected.\n\n【6】This is the first measurement of cases of acute bloody diarrhea in Wales. Since the study sample is broadly representative of the Welsh population (2) , the mean annual rate of 18 per 100,000 practice population would equate to 530 cases in Wales as a whole. While a sentinel practice-based scheme cannot measure the true prevalence, it is an economical and efficient way to estimate the order of magnitude of the disease. The low frequency of acute bloody diarrhea and the rarity with which it is encountered by any individual practitioner limit greater precision. The range calculated from the 95% CIs in the study extrapolates to 275 to 850 cases per year in Wales. The difference between the number of cases reported during 11 months in 1997 and the whole of 1998 is not statistically significant since the 95% CIs overlap, but some increase may have occurred as reporting became more efficient.\n\n【7】A high proportion of patients (91%) supplied stool specimens, compared with 67% in a general practitioner-based survey of patients with nonbloody diarrhea (3) . Campylobacter and Salmonella, known causes of bloody diarrhea, were isolated from 80% of specimens, and the epidemiologic characteristics of cases of acute bloody diarrhea frequently reflected those of these organisms (4) . For example, the higher annual incidence of acute bloody diarrhea in children <5 years of age (63 per 100,000, compared with 18 per 100,000 for the entire study population) is reflected in laboratory reports to the CDSC (Wales) during the study period (i.e., for Campylobacter, an annual incidence of 130 per 100,000, with an incidence in children <5 years of age of 234 per 100,000; for Salmonella, an incidence of 68 per 100,000, with an incidence in children <5 years of age of 156 per 100,000 (CDSC \\[Wales\\], unpub. data).\n\n【8】One anomaly, however, is the higher rates of acute bloody diarrhea in 25- to 44-year-old women. This is largely accounted for by higher rates of salmonellae ( Table 2 ), something not seen in routine surveillance of Salmonella isolates in this period (CDSC \\[Wales\\], unpub. data). If this finding is true, women in this age-group may be more likely than men to exhibit acute bloody diarrhea when infected with salmonellae. No cases of STEC O157 were detected through this surveillance system; however, this organism is rare in Wales. The mean annual rate between 1990 and 1998 was 1.6 cases per 100,000; in 46.3% of cases, blood was detected in the stool (5) .\n\n【9】Thirteen (18%) cases for which stool specimens were submitted were of unknown etiology. Although noninfectious causes such as inflammatory bowel disease may also be involved, more diagnoses could almost certainly be made if specimens were sent to a reference laboratory. This could provide useful information about the emergence of, for example, non-O157 STECs. Although difficult to diagnose, such organisms are emerging causes of acute bloody diarrhea and the serious sequelae of hemolytic uremic syndrome worldwide (6) .\n\n【10】Dr. Chalmers directs the Public Health Laboratory Service Cryptosporidia Reference Laboratory in Swansea, Wales, United Kingdom. She has a research background in veterinary microbiology and protozoology and current interests in the epidemiology of occupational and food- and waterborne diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "262103bf-e0e5-45b8-8297-f11cdf65ad21", "title": "Lyme Disease, Virginia, USA, 2000–2011", "text": "【0】Lyme Disease, Virginia, USA, 2000–2011\nLyme disease (LD), caused by the bacterium _Borrelia burgdorferi_ and transmitted in the eastern United States by the black-legged tick ( _Ixodes scapularis_ ), is the most common vector-transmitted disease in North America ( _1_ ). Maintained in an enzootic cycle comprising competent vertebrate reservoir host species, _B. burgdorferi_ is transmitted to humans by the bite of an _I. scapularis_ nymph or adult that acquired infection during a blood feeding as a nymph or larva ( _2_ ). Although the principal reservoir host for this pathogen, the white-footed deer mouse, _Peromyscus leucopus_ , is wildly distributed throughout North America, LD is generally confined to 2 geographic foci in the eastern United States: 1 in the upper Midwest and 1 in the Northeast ( _2_ – _5_ ). Densities of host-seeking _I. scapularis_ nymphs correlate significantly with cases of human LD ( _3_ ), but this species has been reported throughout much of eastern North America ( _6_ – _9_ ). Nationally, LD incidence increased during 1992–2002, but overall numbers of confirmed cases have since remained relatively stable ( _1_ , _10_ ).\n\n【1】In some locations, LD incidence recently has increased dramatically; in Virginia, the number of confirmed cases nearly tripled from 2006 to 2007 ( http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm ) to ≈12.4 cases per 100,000 residents, well above the 1998–2006 average of 2.2 per 100,000 ( _1_ ). A 1990 report of LD cases in Virginia noted that the disease was rare in the early 1980s but apparently increased in incidence and geographic distribution through the late 1980s, leading the authors to conclude that the disease was expanding southward ( _11_ ). Before 2006, most studies of _I. scapularis_ ticks in Virginia focused on the eastern and southeastern parts of the state and found that densities of _I. scapularis_ ticks declined, as did their rate of infection with _B. burgdorferi_ , with distance from the coast ( _12_ , _13_ ). Several early surveys for _I. scapularis_ ticks in Virginia’s neighboring states of North Carolina and Maryland also found them to be most abundant on the Coastal Plain but absent or less common in the Piedmont and Appalachian Mountains. During 1983–1987, Apperson et al. surveyed 1,629 hunter-killed deer from the Coastal Plain, Piedmont, and Appalachian Mountain regions of North Carolina and found _I. scapularis_ ticks only on deer from the Coastal Plain ( _14_ ). Amerasinghe et al. surveyed 1,281, and 922 hunter-killed deer in 1989 and 1991, respectively, at sites from the Coastal Plain to the Appalachian Mountains of Maryland and found _I. scapularis_ ticks on 59%–70% of deer on the Coastal Plain, fewer on deer in the Piedmont Region, and on only 1%–5% of deer in the Appalachian Mountains ( _15_ , _16_ ).\n\n【2】Although _I. scapularis_ ticks exist in the southeastern United States ( _6_ – _9_ ), they are most easily detected by drag sampling, a method used as a proxy for risk to tick exposure ( _5_ ), in areas associated with highest LD incidence, i.e., the Northeast (New Jersey through Massachusetts) and upper Midwest (Wisconsin and Minnesota) ( _3_ , _5_ , _17_ ). The difference in apparent abundance of _I. scapularis_ ticks and risk for LD between the northern and southeastern United States has been the subject of much discussion and debate ( _18_ ) and might be related, either through behavioral or physiologic mechanisms, to genetic differences between _I. scapularis_ populations in these regions ( _7_ , _19_ – _22_ ). Population genetic structure of _I. scapularis_ ticks has shown that dynamic range shifts are likely to have occurred in recent evolutionary history ( _19_ – _22_ ) and that 2 distinct lineages within this species can be identified; a relatively genetically uniform “American clade” exists in the northern United States (although this lineage has also been detected in the South), and a genetically diverse “southern clade,” members of which have been found only in the South ( _20_ ). Although other nomenclatures have been proposed for these 2 lineages (e.g., clades A and B for northern and southern lineages, respectively \\[ _19_ \\]), we follow the terminology established by Norris et al.: “American” describes the widely distributed yet less diverse clade and “southern” describes the geographically restricted yet more diverse mtDNA clade of _I. scapularis_ ticks ( _20_ ).\n\n【3】Range expansion of _I. scapularis_ ticks over relatively short periods has been observed ( _23_ , _24_ ). Moreover, recent environmental modeling, based on extensive field collections of host-seeking _I. scapularis_ ticks, suggests that this species suggests that the range of this species is expanding widely and its occurrence in a given area depends on the lack of abiotic drivers, vapor pressure deficit and elevation ( _5_ , _25_ ). In Virginia, studies found that _I. scapularis_ ticks were concentrated in in northern sites; very few ticks were reported in other parts of the state ( _5_ _,_ _17_ _,_ _25_ ). In contrast, human LD cases at inland, higher-elevation locations have increased in recent years in Virginia ( http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm ). The incongruity between human case and vector abundance datasets might be explained by recent (i.e., since 2007) spatial and/or numerical expansion of _I. scapularis_ populations. We hypothesized that density of _B. burgdorferi_ –infected ticks would be highest in counties associated with high incidence of human disease if epidemiologic data represent cases in tick-endemic areas. In contrast, low numbers of infected ticks in areas of high human disease might indicate either misdiagnosis or allochthonous exposure.\n\n【4】### Methods\n\n【5】##### Data from Cases in Humans\n\n【6】We compiled LD cases reported to the Virginia Department of Health ( http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm ) directly by physicians or identified through follow-up of positive laboratory results by county public health department personnel during 2000–2011. All LD cases counted in Virginia State Reportable Disease Reports met clinical and laboratory criteria specified in the National Surveillance Case Definition (SCD) for LD. We assessed cases counted in Virginia during 2000–2007 using criteria in the 1996 SCD and cases counted during 2008–2011 using criteria in the 2008 SCD. The 1996 SCD enabled states to liberally interpret what constituted laboratory evidence of infection; an IgM-positive Western immunoblot (WB IgM) test result could be counted as laboratory evidence of infection even though a more specific 2-tier test that used an enzyme-linked immunoassay (EIA) and the WB IgM was recommended. The Virginia Department of Health used the less restrictive interpretation of laboratory evidence in its LD surveillance from 1996 through the end of 2007. However, given that single-tier positive results from either the EIA or the WB IgM are less specific than a positive 2-tier result from both the EIA and the WB IgM ( _26_ , _27_ ), laboratory evidence of infection in the 2008 SCD required, at a minimum, a positive 2-tier test result on blood collected during the acute phase of illness (i.e., within 30 days after illness onset). The more stringent laboratory criteria adopted in the 2008 SCD were designed to minimize the number of false cases counted by state surveillance programs.\n\n【7】We analyzed all data at the county level, which required us to reclassify cases reported in cities to the counties in which they are situated because cities and counties in Virginia are often separate administrative entities. We estimated LD incidence per county for each year during 2000–2011 by dividing the annual number of counted cases by the estimated population size in 2007 ( _28_ ). To characterize annual change in incidence per county, we calculated the difference in cases between successive years and then averaged these values across years. We analyzed the spatial distribution of human LD cases at the state level by identifying the centroid, or geometric center, of county-level LD incidence for each year, starting in 2000 using ArcMAP 10.0 (ESRI, Redlands, CA, USA). We then used weighted linear regression to determine the effect of year on latitude and longitude of that year’s centroid position weighted by annual number of cases.\n\n【8】##### Study Sites and Field Collections\n\n【9】Figure 1\n\n【10】Figure 1 . Locations of 4 field sites at which ticks were sampled, Virginia, May-July 2011. Circles indicate sampling areas. LE, Lesesne State Forest; AB, Appomattox-Buckingham State Forest; GR, University of Richmond–owned field site; CR, Crawfords State Forest....\n\n【11】In May and June 2011, we sampled ticks at 4 closed-canopy deciduous forest sites along an east-west elevational gradient: Crawfords State Forest (CR) (30 m), a University of Richmond–owned tract in Goochland County (GR) (80 m), Appomattox-Buckingham State Forest (AB) (170–200 m), and Lesesne State Forest (LE) (380–450 m) ( Figure 1 ). We collected ticks at all sites by drag sampling ( _29_ ) whereby a 1-m <sup>2 </sup> piece of corduroy was dragged along both sides of 5 haphazardly selected 100-m transects (1,000 m <sup>2 </sup> total), stopping every 20 m to remove ticks ( _17_ , _25_ ). We visited each site 4 times during May–July 2011 with at least 10 days separating visits. All ticks were speciated by light microscopy using dichotomous keys ( _30_ ), and density of _I. scapularis_ ticks was calculated as the average number of ticks collected per transect. Difference in density of _I. scapualris_ nymphs among sites and visits was determined by analysis of variance of square root–transformed count data. We compared infection prevalence in ticks among sites by Gtest and by creating log-likelihood estimates of 95% CIs with a binomial probability function ( _31_ ).\n\n【12】##### Molecular and Phylogenetic Methods\n\n【13】To extract total DNA, individual ticks were dried and flash-frozen by using liquid nitrogen, crushed by using a sterilized pestle, and processed with Qiagen DNeasy Blood and Animal Tissue Kit (QIAGEN, Valencia, CA, USA) by using manufacturer’s protocols. We tested for _B. burgdorferi_ DNA by PCR amplification of the outer surface protein C ( _osp_ C) gene and the intergenic spacer region of 16S–23S rRNA genes ( _32_ ). Presence of amplified DNA was determined by gel electrophoresis, and samples that produced amplicons were purified with a QIAquick PCR Purification Kit (QIAGEN) and submitted for sequencing at the Nucleic Acids Research Facility at Virginia Commonwealth University (Richmond, VA, USA). We also performed PCR to amplify and subsequently sequence an ≈460-bp portion of the _I. scapularis_ 16S rRNA gene using primers 16S +1 and 16S –1 ( _20_ ). Bidirectional chromatograms from all sequence data were assembled and initially analyzed with Sequencher 4.10.1 (Gene Codes, Ann Arbor, MI, USA). _B. burgdorferi_ sequences were blasted by using GenBank ( http://blast.ncbi.nlm.nih.gov/Blast.cgi ) to confirm species identification. Sequence data from _I. scapularis_ 16S samples were aligned with reference sequences ( _33_ ) by using ClustalW ( http://www.clustal.org ) implemented in MEGA 5.0 ( http://www.megasoftware.net/ ), which was also used to select among models of evolution and to reconstruct phylogeny.\n\n【14】### Results\n\n【15】Figure 2\n\n【16】Figure 2 . Progressive geographic spread of human Lyme disease across Virginia, 2001–2011. Data were reported by the Virginia Department of Health http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm . Cases per 100,000 population were calculated by county or city...\n\n【17】During 1995–1998, the Virginia Department of Health counted 55–73 LD cases per year. The number increased to 122 cases in 1999, and cases continued to increase through the early 2000s. Although Virginia’s LD activity during 2000–2005 was focused primarily on northern Virginia and the Eastern Shore of Virginia (a peninsula extending south from Maryland on the eastern side of the Chesapeake Bay), small numbers of LD cases were recorded in counties across Virginia, including counties in the most southern and southwestern parts ( Figure 2 ). During 2006–2007 the incidence of LD increased substantially in counties throughout the Appalachian Mountains ( Figure 2 ). After the change in the SCD in 2008, many of the most southern and southwestern counties that had recorded LD cases before 2008 ceased to report cases, and the geographic progression of LD appeared as a compact front that progressed from county to county from northeast to southwest. LD cases were not observed again in any of the far southwestern counties until 2011, by which time LD was considered endemic to many of the counties immediately to their northwest ( Figure 2 ).\n\n【18】Figure 3\n\n【19】Figure 3 . Variation in estimated prevalence of _Borrelia burgdorferi_ infection in _Ixodes scapularis_ nymphs at 4 field sites in Virginia. Sites are arranged west to east from left to right. LE, Lesesne State...\n\n【20】We collected 2,549 ticks from the field: 2,192 _Amblyomma americanum_ (1 larva, 1,917 nymphs, 274 adults), 306 _I. scapularis_ (304 nymphs, 2 adults), 50 _Dermacentor variabilis_ (all adults), and 1 _I. dentatus_ (nymph). Sampling site was a major determinant of _I. scapularis_ density ( _F_ \\= 71.07, p<0.0001, degree of freedom \\[df\\] = 3), as was sampling date ( _F_ \\= 6.85, p=0.024, df = 1). Post hoc comparisons indicated that tick density at the highest elevation site (9.55 nymphs/200 m <sup>2 </sup> ) was significantly greater than at any other site and that tick density at GR (1.66 nymphs/200 m <sup>2 </sup> ) was significantly higher than at site AB (0.25 nymphs/200 m <sup>2 </sup> ) ( Table ). We detected _B. burgdorferi_ DNA in 48 _I. scapularis_ nymphs, 45 of which produced unambiguous sequence reads for at least 1 locus ( _osp_ C or intergenic spacer region. Infection prevalence varied significantly among sites (likelihood ratio test, G = 16.3, p<0.0001, df = 3); the prevalence of infection was significantly higher at site LE (0.2) than at sites CR (0.00) and GR (0.04). Because of low sample size, site AB did not yield a reliable estimate of infection prevalence ( Figure 3 ).\n\n【21】Figure 4\n\n【22】Figure 4 . Maximum-likelihood phylogenetic reconstruction of _Ixodes scapularis_ lineages based on 16S rRNA gene sequences using Tamura 3-parameter model ( _35_ ). All samples beginning with IS were collected during this study; reference...\n\n【23】Figure 5\n\n【24】Figure 5 . Centroids of annual incidence, by county, Virginia, 2000–2011. The size of each circle represents the annual number of cases reported by the Virginia Department of Health and is proportional to annual...\n\n【25】Analysis of _I. scapularis_ 16S sequences yielded 17 haplotypes (GenBank accession nos. KF146631–47) from 85 individual nymphs (14 haplotypes from 44 ticks at LE, 1 from 2 ticks at AB, 6 from 21 ticks at GR, and 4 from 18 ticks at CR). Maximum-likelihood phylogenetic reconstruction using Tamura 3-parameter model ( _34_ ) indicated that all haplotypes detected fall within the American clade; none of the ticks we sampled were phylogenetically identified as southern clade _I. scapularis_ ( Figure 4 ). In addition to an overall increase in human LD cases (from 136 in 2000 to an average of >1,000 in 2010 and 2011), we observed a significant spatial shift of the geometric center of LD incidence in Virginia. The longitude value associated with the centroid of each year’s LD incidence depended significantly on year from 2000 to 2011 ( _F_ \\= 12.48, p = 0.005, r <sup>2 </sup> \\= 0.56) ( Figure 5 ). Latitude values did not change significantly over time ( _F_ \\= 0.14, p = 0.71, r <sup>2 </sup> \\= 0.01). We also calculated the average LD incidence per county for 2000–2006 (before the dramatic spike in cases in Virginia) and for 2007–2011 to identify counties in which the largest increases in cases occurred ( Table ).\n\n【26】### Discussion\n\n【27】Our results indicate that 1) human LD incidence in Virginia has increased since 2000 and that the spatial distribution of cases has changed significantly, 2) abundance of _I. scapularis_ nymphs and prevalence of _B. burgdorferi_ infection are consistent with recent changes in human disease data, and 3) _I. scapularis_ populations detected in central and western Virginia are dominated by American-clade haplotypes. Taken together, these results suggest recent spatial and/or demographic expansion of _I. scapularis_ ticks in Virginia, resulting in increased human exposure to _B. burgdorferi_ ; the most notable increases in ticks and disease risk are at higher elevations in the western part of Virginia. More generally, our results indicate a dynamic pattern of LD risk. The spatial trends we identified through acarologic sampling are consistent with observed changes in disease incidence and are of paramount public health importance; the observed changes LD epidemiology in Virginia most likely reflect a spatial increase in disease endemicity ( Table ). We propose that the increase in LD in Virginia is caused by either increasing abundance of _I. scapularis_ ticks, increasing prevalence of _B. burgdorferi_ infection in the vector, or both. Our data suggest that this vector species may be more abundant than it was before 2007; during widespread collections during 2004–2007, _I. scapularis_ ticks existed throughout most of Virginia, and no infected _I. scapularis_ ticks were detected in central or western Virginia ( _17_ , _25_ ). Similar range expansion of _I. scapularis_ ticks has been described in Wisconsin and Michigan ( _23_ , _24_ ).\n\n【28】The extent to which the spatial distribution of LD cases in Virginia will continue to change is unclear. Environmental variables previously identified as important drivers of _I. scapularis_ abundance may not have uniform effects throughout the range of this species. For example, on the basis of extensive sampling in the eastern United States over several years, Diuk-Wasser et al. estimated an elevational threshold of 510 m for this species ( _25_ ), and Rosen et al. detected more _I. scapularis_ on deer at low elevation than high elevation sites in Tennessee ( _35_ ). However, our sampling showed the highest density of host-seeking _I. scapularis_ nymphs at elevations approaching this threshold, and we have subsequently collected host-seeking nymphs at >1,000 m in Nelson County in west-central Virginia (R.J. Brinkerhoff, unpub. data). In 2007, a growing focus of LD incidence was observed in southwestern Virginia in Pulaski, Floyd, and Montgomery Counties. These counties have continued to have that region’s highest incidence of LD through 2011 ( http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm ) and mostly occupy high mountain valleys with average elevations of 584–762 m. An elevational threshold that limits tick populations at northern latitudes, where high elevation sites experience extreme cold during winter months, would not be expected where equivalent elevations are associated with more moderate climatic conditions.\n\n【29】Our analysis of LD data from humans indicates that the largest increases in LD incidence since 2007 has occurred in higher-elevation counties in western Virginia; the correspondence between these data and acarologic sampling suggests that the cases reported in these locations most likely are locally acquired and indicate recent spatial and/or numerical expansion of human disease. Our results are notably inconsistent with the findings of surveys of _I. scapularis_ ticks on hunter-killed deer in North Carolina and Maryland during 1987–1992, which indicated that _I. scapularis_ ticks were most abundant on the Coastal Plain and absent or uncommon in the Appalachian Mountains ( _14_ – _16_ ). When human LD surveillance began in Virginia in 1989, the highest incidence was on the state’s Eastern Shore ( http://www.vdh.virginia.gov/epidemiology/surveillance/surveillancedata/index.htm ). This finding was consistent with early surveys of ticks indicating that _I. scapularis_ was the most common species in the Coastal Plain and much less common at higher elevations to the west ( _14_ ). A logical conclusion at that time was that LD would continue to spread southward along the state’s Coastal Plain. However, during 2000–2011, LD became more prevalent in Virginia’s upper Piedmont and Appalachian Mountain zones than in the lower Piedmont and Coastal Plain. The results of older surveys of ticks and recent environmental models are not consistent with the current geographic incidence of LD or our field data. This discrepancy suggests a southwestward spatial expansion of northern tick populations into the upper Piedmont and mountain regions of Virginia or demographic expansion of persons into areas of previously low tick density in western localities. We do not have acarologic data from each county in which LD incidence has increased, nor do we have long-term systematic sampling data, and thus we cannot directly attribute local changes in LD to changes in tick densities.\n\n【30】Analysis of single-nucleotide polymorphisms in the _I. scapularis_ genome reinforces the hypothesis that these ticks recolonized northern North America after the most recent glaciation event and that northern populations are genetically less diverse than southern populations ( _21_ ). Moreover, analyses of single-nucleotide polymorphism data are consistent with south-to-north postglaciation gene flow, whereby northern American-clade populations are a subset of the genetic variation found in southern-clade populations ( _21_ ) resulting from founder effects when ticks recolonized northern latitudes ( _22_ ). Tick populations within both LD-endemic foci show signs of genetic isolation from one another and from southern populations ( _22_ ), and evidence exists for similar lack of gene flow among populations within regions ( _19_ ). Identification of American-clade _I. scapularis_ ticks in the southeastern United States ( _19_ , _33_ ) might reflect remnant American-clade lineages in the South or might indicate southward dispersal of American-clade ticks. Qiu et al. noted that coastal sites in southern states were associated with strictly American-clade populations, whereas a mix of American- and southern-clade ticks was detected at inland sites ( _19_ ). With respect to our study, we point to the recent lack of detection of _I. scapularis_ ticks at high-elevation sites in western or central Virginia ( _17_ , _25_ ) and the presence of exclusively American-clade _I. scapularis_ ticks in the current study as possible evidence consistent with the population expansion of American-clade ticks from northern population foci. However, we cannot exclude the possibility that the distribution of endemic American-clade ticks simply has expanded in Virginia.\n\n【31】Although American- and southern-clade _I. scapularis_ ticks are now considered 1 species, apparent differences exist in host-seeking behavior, biting behavior, and duration of attachment to different host types ( _9_ , _36_ , _37_ ). Genetic differences between the major _I. scapularis_ lineages have been well documented ( _7_ , _19_ – _22_ ), and if American-clade ticks are more likely to feed on humans, the emergence of LD in Virginia would be consistent with increased relative abundance of this variant. In the South, immature _I. scapularis_ ticks feed predominantly on low-competence or noncompetent lizard species and are relatively uncommon on rodents ( _8_ , _36_ – _38_ ). Southern-clade nymphs may have questing behavior that makes them unlikely to be collected on cloth drags or to bite humans ( _9_ ); thus, nymphal ticks are difficult to collect, even in places where adult ticks are common. LD risk should be very low in areas where _I. scapularis_ nymphs are unlikely to bite humans, and immature ticks are more likely to feed on reptiles than on competent vertebrate reservoirs. However, data from a single mitochondrial gene, albeit one that has been widely characterized for this species, do not necessarily reflect patterns of differentiation found in nuclear markers ( _21_ ) and probably are not useful for delineating among behavioral phenotypes. Moreover, we sampled in daytime hours during the presumed peak period of nymphal activity (late spring, early summer) and thus would not have detected ticks exhibiting different host-seeking behaviors. It is possible that multilocus genomic analysis or year-round sampling would yield different insights from those reached in this study.\n\n【32】The latitudinal gradient in LD risk in the eastern United States is not easily explained and probably is driven by demographic and environmental factors ( _5_ , _26_ , _39_ ). However, our data suggest that the boundary between regions to which _I. scapularis_ ticks are and are not endemic is moving and that _B. burgdorferi_ –infected ticks might be expanding in or into areas from which they historically have been absent. As a result, clinicians and epidemiologists need to be vigilant in the face of changing spatial distributions of risk, especially in transition zones where patterns of disease are rapidly changing ( _40_ ).\n\n【33】Dr Brinkerhoff is an assistant professor at the University of Richmond and holds an honorary senior lectureship in the School of Life Sciences, University of KwaZulu-Natal, Pietermaritzburg, South Africa. His research focuses on the ecology, evolution, and epidemiology of bacterial pathogens transmitted by arthropod vectors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5cc4a24f-6cf0-49f2-ab61-96a859d8021d", "title": "Disparate Effects of Invasive Group A Streptococcus on Native Americans", "text": "【0】Disparate Effects of Invasive Group A Streptococcus on Native Americans\n_Streptococcus pyogenes_ (group A _Streptococcus_ \\[GAS\\]) causes a range of pyogenic, toxigenic, and immunologic diseases with varying degrees of severity, from pharyngitis and impetigo to necrotizing fasciitis and streptococcal toxic shock syndrome ( _1_ ). Improved sanitation and increased availability of antimicrobial drugs have led to a decline in GAS incidence and virulence ( _2_ ), but concern about invasive GAS (iGAS) has increased in recent decades ( _3_ ).\n\n【1】Each year, ≈500,000 deaths occur worldwide from GAS; 160,000 of these are from invasive infections, of which 97% occur in resource-limited countries ( _4_ ). Some of the highest rates of iGAS occur in Indigenous populations within industrialized countries ( _5_ ), most likely as a consequence of health disparities between general populations and marginalized peoples, in addition to a poorly understood host–pathogen relationship. In the United States, American Indians and Alaska Natives (AI/AN) suffer from high rates of iGAS, but epidemiologic and clinical data on GAS among these Indigenous populations is more sparse than it is in other industrialized nations ( _6_ ). We highlight discordance between the substantial effect of iGAS on AI/AN and understanding of factors that influence iGAS disease in these vulnerable US populations and in other Indigenous persons.\n\n【2】### iGAS in the United States\n\n【3】During the 1980s, the first reports of streptococcal toxic shock–like syndrome resulted in a resurgence in concern about GAS disease. A population-based study of iGAS in the United States found a stable annual incidence rate of 4.3 cases/100,000 population during 1985–1990 ( _7_ ). Despite this stable rate, GAS disease increased, and multiorgan dysfunction and hypotension signaled an increase in GAS severity.\n\n【4】This study prompted the Centers for Disease Control and Prevention (CDC) to add iGAS to the Active Bacterial Core surveillance (ABCs) program. The first survey, which assessed iGAS in the United States during 1995–1999, found an annual incidence of 2.2–4.8 iGAS cases/100,000 population ( _3_ ). Rates were stable in 2 follow-up epidemiologic studies that showed iGAS incidence of 3.5 cases/100,000 population for 2000–2004 and 3.8 cases/100,000 population for 2005–2012 ( _1_ , _8_ ).\n\n【5】The CDC reports suggested stability with unchanging rates and disease distribution. Each report concluded that iGAS incidence had been stable not only intrastudy but also across decades, starting with the work of Hoge et al. ( _7_ ). The researchers referenced higher rates of iGAS among Indigenous populations of other countries, and ABCs data indicated that AI/AN make up <2% of the country but contributed 4% of iGAS cases during 2000–2012 ( _1_ , _8_ ). This finding provides at least inferential evidence of higher rates of iGAS among AI/AN that warrants further investigation.\n\n【6】Incidence varies by region. For example, Utah reported an iGAS incidence significantly higher than country averages for 2002–2010: 6.3 cases/100,000 Utah residents. The study noted an increased relative risk among Native Hawaiians/Pacific Islanders (3.3% vs. 1.2%), but this observation was not explored further ( _9_ ). More recent data from the ABCs program indicated an increase in iGAS incidence in the general population; rates rose in 2014 to 4.8 cases/100,000 population and in 2016 to 5.8 cases/100,000 population ( _10_ , _11_ ). Although this recent increase is notable, it is marginal, and overall rates remain substantially lower than those reported among AI/AN.\n\n【7】These studies highlight the stable pattern of iGAS in the general US population. However, they fail to highlight the limited but important evidence that suggests higher iGAS rates among AI/AN.\n\n【8】### iGAS among AI/AN\n\n【9】Hoge et al. reported an overall iGAS incidence of 4.3 cases/100,000 population but found an age-adjusted incidence of 46.0 cases/100,000 population for AI ( _7_ ). Heightened concern for iGAS among AI led Benjamin et al. to review all cultures positive for GAS from sterile sites at a hospital serving Zuni Indians during the same period (1982–1991) ( _12_ ). The annualized incidence rate for iGAS of 13.3 cases/100,000 population (95% CI 8.3–33.2) for Zuni Indians contrasted with a rate of 1.7 cases/100,000 population in a neighboring county of predominantly Hispanic and non-Hispanic White residents. The authors noted that during a time when the medical community was focused on suspected rising rates in the general population that remained in single digits, the rate of iGAS among Zuni AI patients was, and had been, substantially higher.\n\n【10】The findings are consistent with more recent studies and case series that described iGAS among AI/AN. Rudolph et al. evaluated iGAS in Alaska during 2001–2013, and although only 20% of the state’s population was AN, this group represented 46% of the state’s 516 iGAS cases. The rates of iGAS were 13.7 for AN and 3.9 for non-AN persons per 100,000 population ( _6_ ). The Arizona Department of Health Services reported iGAS incidence rates for AI/AN that were 2–5 times higher than those for the general population and an increase in rates in recent years ( _13_ ). However, this information has not reached the broader public health or medical literature that mostly comprises smaller, local reports of iGAS clusters involving AI communities ( _14_ , _15_ ).\n\n【11】### A Disease of Displaced Indigenous Populations\n\n【12】Other high-resource countries with more recent iGAS population-based studies among Indigenous populations can provide insight for an explanatory model for iGAS among Indigenous persons in the United States. Although evidence is limited on the true incidence of iGAS globally, Indigenous populations of industrialized nations have the highest recorded rates of both epidemic and endemic iGAS disease ( _4_ ).\n\n【13】##### Canada\n\n【14】After an iGAS outbreak swept through Canada, researchers conducted an epidemiologic review of iGAS cases in northwestern Ontario for 2001–2013 at the health center, metropolitan, and provincial levels ( _16_ ). First Nations Canadians accounted for 41% of iGAS cases despite making up only 8% of the population. The rate of iGAS at the health center where most of this population obtains care ranged from 9.8 to 18.1 cases/100,000 population, a rate 2–7 times higher than the largely nonnative metropolitan and provincial populations. In a more focused follow-up study of 22,000 First Nations Canadians among 26 communities in the same region, Bocking et al. reported an iGAS annualized rate of 56.0 cases/100,000 population for 2009–2014, >10 times higher the rate for the general population during the same period ( _17_ ).\n\n【15】##### Australia\n\n【16】The rates of GAS and poststreptococcal sequelae are substantially higher for Aboriginal Australians than for the general population of Australia, for which GAS disease has declined ( _18_ ). In a retrospective 6-year study of all cases of GAS bacteremia in the Northern Territory, Carapetis et al. noted that >50% of cases occurred in Aboriginal Australians, who make up only one quarter of the population ( _19_ ). Two separate sequential studies in neighboring Queensland spanning 1996–2009 supported these findings; rates of iGAS for Aboriginal Australians were 3–8 times higher than for non-Indigenous Australians ( Appendix ).\n\n【17】##### New Zealand\n\n【18】In nearby New Zealand, Safar et al. conducted a population-based study of iGAS to anticipate coverage of potential GAS vaccines in development ( _20_ ). In the diverse city of Auckland (population ≈1.3 million), 11% were Indigenous Māori. Yet, this group accounted for 31% of all iGAS infections over a 2-year period, and the incidence rate for Māori was 5 times higher than for New Zealand residents of European and Asian descent. The disparity was more dramatic at the ends of the age spectrum for Indigenous Māori. iGAS incidence rates per 100,000 population were 40.9 for children <1 year of age and 146.8 for persons \\> 65 years of age.\n\n【19】The disparity in iGAS between Indigenous and non-Indigenous populations spans geography and cultures (Table). However, the extent and reasons for that disparity remain poorly understood.\n\n【20】### Influencing Factors in iGAS\n\n【21】The relationship between GAS and Indigenous populations most likely involves complex interactions between pathogens and humans. Numerous studies have contributed to a long list of influencers of GAS incidence but a lack of rigorous analyses has yielded few prevention strategies ( _21_ ). The high rates of iGAS among Indigenous persons may represent unique circumstances that combine pathogen characteristics, population dynamics, and individual risk factors that all contribute to varying degrees.\n\n【22】A high incidence of noninvasive GAS disease and postinfectious sequelae is well described in the Indigenous populations of industrialized countries and coincides with elevated rates of iGAS ( _5_ ). Among Indigenous Australian children, the prevalence of pyoderma was reported as high as 90% ( _18_ ), and incidence rates were 4–10 times higher than in other resource-limited settings ( _22_ ). Rates of acute rheumatic fever for Indigenous Australians are as high as 194 cases/100,000 population and for Māori in New Zealand, 78 cases/100,000 population ( _5_ ). These rates are down from estimated rates of 374–508 cases/100,000 population during the 1980s, but remain higher than rates for local non-Indigenous groups (1.1–7.2/100,000) ( _5_ ). Finally, Indigenous Australians have the highest reported rates of acute poststreptococcal glomerulonephritis worldwide (239 cases/100,000 population) ( _23_ ).\n\n【23】Less is known about noninvasive GAS and post-GAS sequelae among AI/AN in recent decades. Studies among AI children in Minnesota during the 1960s and 1970s showed high rates of poststreptococcal glomerulonephritis in addition to a >80% prevalence of pyoderma ( _24_ , _25_ ). Despite national declines in rheumatic heart disease, limited reports reveal rates of rheumatic heart disease in AI nearly 5 times national estimates (4.6 vs. <1.0 cases/1,000 population) ( _26_ ) along with higher rates of rheumatic heart disease–related death ( _27_ ). These observations require a broader appreciation of GAS among Indigenous persons, including AI/AN.\n\n【24】##### Socioeconomic Status\n\n【25】Socioeconomic status (SES) is a commonly asserted but poorly studied explanation for the high rates of iGAS among Indigenous persons. Few studies have directly measured SES when comparing iGAS rates between Indigenous and non-Indigenous populations, and the evidence is conflicting. In 1 study from Australia, half of iGAS cases occurred in the lowest socioeconomic quintile, in which Indigenous persons were overrepresented ( _20_ ). Yet, a study in Fiji that specifically looked for a relationship with SES found that Indigenous Fijians had higher rates of iGAS despite a higher SES than did Indo-Fijians ( _28_ ). Although this study was limited by small sample sizes, it is an example of the difficulty in understanding how SES indicators relate to iGAS incidence. Yet, of all the potential influencers, SES seems to be the most obvious commonality between displaced Indigenous persons of different industrialized nations.\n\n【26】According to US national health statistics, 23.9% of AI/AN lived below the poverty level during 2014–2016 ( _29_ ), whereas 12.7% of all persons and 11.0% of White persons lived below the poverty level ( _29_ ). According to Akee et al., AI/AN experience substantial income inequality and immobility. Along with Blacks and Hispanic persons, AI/AN are consistently at the lower end of income distribution in the United States and are the least likely to move percentiles in their lifetime ( _30_ ). Of the top 10% of US income earners, 84% were White, whereas only 0.3% were AI/AN ( _30_ ).\n\n【27】For numerous health indicators, AI/AN are worse off as well and have some of the highest age-adjusted death rates secondary to chronic liver disease and cirrhosis, diabetes, unintentional injuries, septicemia, and alcohol-related deaths ( _29_ ). Although the overall age-adjusted death rate for AI/AN is lower than that for all races (591 vs. 729/100,000 population), the years of potential life lost before age 75 years is 10% higher for AI/AN than for White persons (i.e., AI/AN lose 645 more years of life per 100,000 persons before age 75 years) ( _29_ ). Infant mortality is a “fundamental indicator of … community health status, and the availability and use of appropriate health care,” and from 2005 to 2015, infant mortality rates decreased >10% for every racial group except AI/AN, for whom infant mortality rates did not improve ( _29_ ).\n\n【28】For some conditions, however, AI/AN have lower death rates than the US general population and for White Americans. These conditions include heart disease, cerebrovascular disease, malignancies, and chronic lower respiratory disease ( _29_ ). Misclassification and incomplete data for racial groups other than White and Black explains some of the reason for these lower death rates.\n\n【29】Although AI/AN experience socioeconomic and health disparities, linking the 2 causally is complicated. The relationship between SES overall, specific SES indicators (e.g., income or education), and health outcomes varies by race ( _31_ ). Furthermore, Black Americans have near equivalent rates of poverty (22.0% in 2016) to AI/AN and have some of the largest health disparities of any racial group ( _29_ ). The all-cause age-adjusted death rate is highest for Black persons (857/100,000 population), and Black persons have higher death rates for heart disease, cerebrovascular disease, malignancies, diabetes, and homicides than persons of all other races ( _29_ ). Although Black Americans have worse health indicators than AI/AN for many conditions, the rates of iGAS among Blacks are more similar to those among Whites than for AI/AN. In the most recent comprehensive report by CDC, the rate of iGAS among Blacks was 4.7 cases/100,000 population and increased to 6.2 cases/100,000 population in the 2016 update ( _1_ , _11_ ).\n\n【30】For these reasons, we caution against attributing substantial disparities in iGAS among AI/AN compared with the general population to SES alone. Although SES undoubtedly plays a major role, more information is needed on which SES indicators most strongly predict iGAS in AI/AN and the general population. Such information will best guide public health interventions beyond oversimplified suggestions to improve hygiene and ameliorate poverty.\n\n【31】Perhaps the reason SES fails to accurately capture which, and how, certain SES indicators most affect Indigenous persons is that the colonization, displacement, assimilation practices, and subsequent intergenerational historical trauma experienced by Indigenous populations fundamentally differentiate them from other impoverished groups. The health effects of centuries of denying and denigrating Indigenous culture have only recently been appreciated, but much needs to be done to promote healing ( _32_ ). The displacement and discrimination experienced by AI/AN is difficult to measure and correlate with health outcomes. Furthermore, the experiences of Indigenous persons have shaped migration, their interactions with non-Indigenous communities, and relationships with institutions such as the education and healthcare systems. These factors have led to social, economic, and political disparities not captured in typical epidemiologic studies and may contribute to the confusion in how SES is linked to health disparities among AI/AN. Recognizing these historical differences for AI/AN is critical for better understanding health disparities, such as iGAS, and might be the most important and least understood aspect of disparities in Indigenous health.\n\n【32】##### Microbiological Factors\n\n【33】Although strain novelty and virulence play important roles in iGAS outbreaks, their differential contribution remains unclear. Some studies have postulated that strain prevalence and novelty, rather than innate virulence potential, was the predominant reason a specific strain would emerge as a cause of iGAS ( _33_ ). That is, upsurges in both iGAS and noninvasive GAS disease could occur when a predominant strain is supplanted with a new _emm_ \\-type to which the population lacks sufficient immunity ( _21_ , _34_ ). Outbreaks can occur within specific risk groups that may implicate interaction between strain-type and host irrespective of virulence (e.g., the spread of severe GAS infections among intravenous drug users in the United Kingdom) ( _35_ ).\n\n【34】Yet, hypervirulent strains also have contributed to outbreaks of iGAS ( _36_ , _37_ ). A well-described outbreak of iGAS in Canada resulted from a single, recently emerged, hypervirulent strain of _emm59_ , which then migrated to the United States; genetically diversified; and led to _emm59_ outbreaks in Montana, Wyoming, and Arizona ( _15_ , _38_ , _39_ ). During the outbreak in Canada, Tyrrel et al. noted that a relatively modest proportion of iGAS cases (5%) in Ontario was attributable to _emm59_ . Yet, they observed that 56 (83%) of Ontario’s 68 _emm59_ cases occurred in an area with a high proportion of First Nations Canadians ( _37_ ). This observation supported by the findings of Athey et al. that reported a disproportionate number of iGAS cases among Indigenous Canadians in Ontario, particularly in the Thunder Bay region, where _emm59_ caused 44% of iGAS in 2008 ( _16_ ). An _emm59_ outbreak in a northern Arizona hospital cited Native American race as a primary risk factor for infection. Eighteen (62%) of 29 iGAS cases were identified as _emm59_ , 15 (83%) of which occurred in AI ( _15_ ). A phylogenetic analysis of 67 _emm59_ strains known to cause outbreaks of iGAS in the United States revealed that AIs had the highest proportion of invasive _emm59_ infections ( _40_ ). However, without more detailed community epidemiologic data, the relative contribution of novelty versus virulence was difficult to discern. Genetic studies of invasive streptococcal outbreaks in the United States suggest that a progenitor pathogen makes its way into a population (e.g., AI/AN) and then spreads quickly from person to person if the dynamics are favorable ( _15_ , _16_ , _39_ , _40_ ). Novelty and virulence most likely work together to intensify iGAS epidemics in Indigenous communities. Where a new pathogen might lead to an increase in iGAS overall because of population susceptibility, a particularly virulent new pathogen leads to dramatically increased rates of disease. It is entirely plausible that population-specific dynamics and pathogen virulence work together in amplifying outbreaks.\n\n【35】##### Host Factors: Concurrent Conditions and Immunologic Vulnerability\n\n【36】Evidence suggests a strong relationship between iGAS and the prevalence of concurrent conditions ( _19_ ). Diabetes, skin disease, chronic kidney disease, heart disease, and alcoholism are consistently associated with iGAS ( _1_ , _17_ , _20_ , _28_ ). Although some studies found Indigenous persons with iGAS were more likely than non-Indigenous persons to have diabetes, chronic kidney disease, or both ( _7_ , _19_ ), evidence demonstrating no association in the number or type of concurrent conditions was equally limited ( _20_ , _28_ ). This observation reflects the limitations of the research and the poor understanding of how concurrent conditions contribute to iGAS. Indigenous populations clearly have more such conditions and acquire them at a younger age ( _41_ , _42_ ). The quantity of concurrent conditions, in addition to the specific conditions (e.g., diabetes), might explain why Indigenous persons, including AI/AN, are overrepresented among iGAS cases and is supported by the higher rates of all-cause infectious disease–related death, including from iGAS, in AI/AN ( _43_ ). However, this approach would be an oversimplification that neither accounts for the role of pathogen virulence and host factors nor provides insight into how such conditions specifically contribute to iGAS. Furthermore, attributing markedly elevated rates of iGAS to overall vulnerability cannot explain the variability in findings on the effects of concurrent conditions among surveys. Concurrent conditions appear to play an integral but incomplete role in the incidence of iGAS among Indigenous populations. Prospective studies to interpret the effects of specific conditions on the incidence of iGAS are needed to better identify persons at highest risk within vulnerable populations.\n\n【37】The potential contribution of host genetics is the least understood of variables. Evidence suggests genetic conservation among AI/AN ( _44_ ), and a genetic basis for several conditions prevalent among AI/AN has been explored or established, including asthma ( _45_ ), diabetes ( _46_ ), and rheumatoid arthritis ( _47_ ). Evolutionary bottlenecks and founder effects have led to an overall decrease in allele diversity, and tribal structure of reservation life has led to semi-independent gene pools ( _44_ , _48_ ). Such gene pools provide a potentially more homogenous widespread susceptibility that could partly explain the vulnerability of all AI/AN populations to invasive infections caused by encapsulated organisms ( _Haemophilus influenzae, Streptococcus pneumoniae_ ) that is well described but incompletely understood ( _49_ ). A similar mechanism could be involved with GAS disease. The application of more recently developed techniques that measure host immune response to bacterial infections may offer additional insights, which can guide future efforts at GAS vaccine development and control ( _50_ ).\n\n【38】Given the disparity in iGAS among AI/AN, it is reasonable to consider a host genetic predisposition to invasive infections. In a world of rapidly evolving techniques for genetic and epigenetic testing and the emerging potential for gene-targeted therapies, exploring the possibility of a genetic predisposition seems prudent to guide potential therapies for this marginalized population.\n\n【39】### Conclusions\n\n【40】The medical and public health communities need to address the effects of iGAS among the ≈3 million AI/AN in the United States. The first step should address the lack of population-based studies of Indigenous Americans. The Indian Health Service needs to support further investigations into iGAS to fulfill its mission “\\[t\\]o raise the physical, mental, social, and spiritual health of American Indians and Alaska Natives to the highest level” ( https://www.ihs.gov/aboutihs ).\n\n【41】We recommend mandatory reporting of iGAS in regions with substantial numbers of AI/AN. This reporting would encourage smaller healthcare facilities to monitor iGAS outbreaks and improve surveillance in smaller, vulnerable populations. Greater capture of isolates is integral to using population-based epidemiologic studies to better identify risk factors for iGAS specific to AI/AN. We also propose large-scale sequencing and phylogenetic studies of GAS _emm_ types to clarify the migration of strains within and among AI/AN and determination of unique host immune responses to GAS among AI/AN to guide control efforts. A tailored approach might be necessary for AI/AN or individual tribes, but better evidence would inform community-based interventions to reduce the incidence of iGAS and lay the foundation for multivalent GAS vaccines to protect communities from the most harmful GAS strains.\n\n【42】Dr. Close is an internist and pediatrician working as a clinician and public health officer with the Indian Health Service at Whiteriver Hospital in eastern Arizona. His research interests are anemia in children, the underutilization of vaccines in complex emergencies, and the effects of GAS and poststreptococcal sequelae in Native American populations.\n\n【43】Dr. McAuley is an internist and pediatrician and an adult and pediatric infectious disease specialist serving the Clinical Director of the Indian Health Service Whiteriver Indian Hospital in Arizona. His research interests include parasitology, tuberculosis, and health disparities in vulnerable populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6a5b47dd-c15d-4da9-8237-be8c98d7710e", "title": "Correction, Vol. 8, No. 12", "text": "【0】Correction, Vol. 8, No. 12\nIn the article, “Antimicrobial Resistance of _Escherichia coli_ O26, O103, O111, O128, and O145 from Animals and Humans” by Carl M. Schroeder et al., errors occurred in Figure 2 on page 1412. The corrected figure appears online at http://www.cdc.gov/ncidod/eid/vol8no12/02-0070.htm .\n\n【1】We regret any confusion these errors may have caused.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e0d1d824-47f9-44ac-84ab-584fcda76ff2", "title": "Disability, Health, and Multiple Chronic Conditions Among People Eligible for Both Medicare and Medicaid, 2005–2010", "text": "【0】Volume 10 — September 19, 2013\n\n【1】### Article Tools\n\n【2】*   PDF - 664 KB\n*   Email\n*   Print\n*   Download citation\n*   Este resumen en español\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Disability, Health, and Multiple Chronic Conditions Among People Eligible for Both Medicare and Medicaid, 2005–2010\n===================================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】This page was updated on July 10, 2014, to incorporate the corrections in Vol. 11\n\n【8】#### Michael H. Fox, ScD; Amanda Reichard, PhD\n\n【9】_Suggested citation for this article:_ Fox MH, Reichard A. Disability, Health, and Multiple Chronic Conditions Among People Eligible for Both Medicare and Medicaid, 2005–2010. \\[Erratum appears in Prev Chronic Dis 2014;11. http://www.cdc.gov/pcd/issues/2014/13\\_0064e.htm .\\] Prev Chronic Dis 2013;10:130064. DOI: http://dx.doi.org/10.5888/pcd10.130064 .\n\n【10】PEER REVIEWED\n\n【11】Abstract\n--------\n\n【12】**Introduction**  \nPeople who are eligible for both Medicare and Medicaid (dual eligibles) and who have disabilities and multiple chronic conditions (MCC) present challenges for treatment, preventive services, and cost-effective access to care within the US health system. We sought to better understand dual eligibles and their association with MCC, accounting for sociodemographic factors inclusive of functional disability category.\n\n【13】**Methods**  \nMedical Expenditure Panel Survey (MEPS) data for 2005 through 2010 were stratified by ages 18 to 64 and 65 or older to account for unique subsets of dual eligibles. Prevalence of MCC was calculated for those with physical disabilities, physical plus cognitive disabilities, and all others, accounting for sociodemographic and health-related factors. Adjusted odds for having MCC were calculated by using logistic regression.\n\n【14】**Results**  \nOf dual eligibles aged 18 to 64, 53% had MCC compared with 73.5% of those aged 65 or older. Sixty-five percent of all dual eligibles had 2 or more chronic conditions, and among dual eligibles aged 65 or older with physical disabilities and cognitive limitations, 35% had 4 or more, with hypertension and arthritis the most common conditions. Dual eligibles aged 18 to 64 who had a usual source of medical care had a 127% increased likelihood of having MCC compared with those who did not have a usual source of care.\n\n【15】**Conclusion**  \nAttention to disability can be a component to helping further understand the relationship between health and chronic conditions for dual eligible populations and other segments of our society with complex health and medical needs.\n\n【16】Top of Page\n\n【17】Introduction\n------------\n\n【18】A segment of the population drawing scrutiny in recent years is the “dual eligibles,” the approximately 9 million low-income senior citizens and low-income people with disabilities as determined by the Social Security Administration (SSA) who are eligible for both Medicare and Medicaid coverage. Medicare enrollment provides primary insurance coverage for acute care medical services, and concurrent Medicaid enrollment helps them pay for premiums and services not fully covered by Medicare (1–13). Although the scope of their joint coverage varies according to their Medicaid eligibility, about 74% of dual eligibles under the age of 65 years receive full “wrap around” Medicaid coverage, as do 81% of dual eligibles aged 65 or older (6). The costs incurred to both Medicare and Medicaid by dual eligibles is disproportionate to their enrollment in these programs (1,7), leading to efforts to understand their use of health care to better inform policy and public health practice (8–13).\n\n【19】The health care associated with the high prevalence of chronic illness among dual eligibles accounts for much of these high costs (6,14). Seventy-two percent of dual eligibles have 2 or more chronic conditions, and 14% of dual eligibles with 6 or more chronic conditions account for 46% of Medicare spending (14). Having more than 1 chronic condition, however, frequently linked in some way to disability, does not mean that people cannot maintain their independence or lead quality lives (15–18). These findings have led to research examining the relationship between multiple chronic conditions (MCC) among dual eligibles and how these co-existing conditions are influenced by or contribute to functional disability limitations (19–21). The SSA definition of disability focuses on persons aged less than 65 with certain physical or mental conditions severe enough to create an inability to work (“substantial gainful activity”) in an ongoing capacity for at least a year or until death (22). Disability can be defined by using population-based data to describe functional limitations not directly related to work among this population also (19,20,23). Among working-age (18–64 years) adults with disabilities identified through self-reported limitations, more than half (an estimated 13 million adults) have more than 1 chronic condition. Compared with people without disabilities, when adjusted for MCC, people with disabilities have much higher health care use even as they experience greater difficulties in accessing care such as preventive services (20,24) or treatment that may be related to the conditions themselves (19,23). People with disabilities experience disparities in health status or receipt of preventive services that could be associated with 1 or more chronic conditions. These disparities may be related to work among younger cohorts of dual eligibles and to increased independence among older cohorts of dual eligibles (21). For some, these disparities exist because certain disabilities predispose them to chronic illnesses. For many, however, a likely explanation is that these disparities result from social, environmental, or behavioral determinants of health that can affect people with disabilities disproportionate to their distribution in the general population. In work commissioned by the US Department of Health and Human Services Strategic Initiative to identify the connection between functional disability limitations and MCC, 63% of dual eligibles living in community settings were found to have both chronic conditions and functional limitations, and dual eligibles with chronic conditions and functional limitations represented 1.5% of the US community-based population while accounting for 6% of national health care expenditures ($60.5 billion) (19).\n\n【20】The objective of our study was to build upon this earlier work by using a public health surveillance tool, the Medical Expenditure Panel Survey (MEPS), to better understand the relationship between functional disability limitations and MCC within a high-cost segment of the US publicly insured population, the dual eligibles. Our study is driven by 2 research questions: 1) what is the prevalence of MCC for discrete socioeconomic and health characteristics among dual eligibles in the United States overall and stratified by age category and type of disability limitation? and 2) what are the adjusted odds of having MCC among dual eligibles given sociodemographic and health characteristics, stratified by age category and type of disability limitation?\n\n【21】Top of Page\n\n【22】Methods\n-------\n\n【23】The MEPS is an annual panel survey that collects demographic and health care expenditure and use information on individuals and families, including their health care providers, their employers, and their health care coverage (25). It is used to track health care services among subsets of the population, including people with disabilities (26–29). Multiple studies have demonstrated the use of the MEPS for analyzing health disparities among and between disability groups (16,18,24,26–30). Broad disability type is identified, allowing for grouping people with physical disabilities or cognitive limitations or both while accommodating for people who may not be able to respond themselves because of their cognitive limitation through proxy response. Accuracy of Medicare and Medicaid eligibility using the MEPS is optimized by first characterizing the difference between Medicare, Medicaid, and other public insurance for respondents and then showing respondents an actual Medicare or Medicaid card from that person’s state of residence.\n\n【24】To achieve sufficient sample size for our analyses, we pooled data for continuous years 2005 through 2010. Noninstitutionalized adults aged 18 years or older were identified as dual eligibles if they had “coverage at any time” for both Medicare and Medicaid during the year in which they were included (25). This resulted in an unweighted sample of 4,579 (weighted sample, 5,166,380).\n\n【25】Because of the unique characteristics associated with dual eligibles below age 65 and those 65 or older, we stratified our analyses by these age categories, further constructing 3 mutually exclusive groups for subsamples based on their having 1) physical disabilities, 2) physical disabilities and cognitive limitations, or 3) other disabilities. A physical disability was flagged if, at any time in the 2 years that a person was followed, the person reported walking limitations, using an assistive device, or needing assistance with activities of daily living (ADLs). People with cognitive limitations were those reporting confusion or memory loss, having problems making decisions, or requiring supervision for their own safety. Although too small a subgroup to study on its own because of insufficient cell sizes, we created a distinct subgroup of those with both physical and cognitive disabilities to account for people with multiple disabilities. Those not reporting a physical disability or having only a cognitive limitation or other disability comprised the final group, “other disabilities.” This group included people with disabilities such as mental health or cognitive limitations only. MCC were 2 or more chronic conditions of those included in the MEPS: heart attack, coronary heart disease, angina, angina pectoris, other heart disease (these 5 grouped as cardiovascular disease), stroke, emphysema, hypertension, diabetes, arthritis, and asthma. This is a subset of conditions used elsewhere (21,30). Overall, 65.8% of dual eligibles had MCC, an unweighted sample of 3,094 (weighted sample, 3,398,939).\n\n【26】Descriptive analyses determined prevalence of MCC and sociodemographic and health characteristics of the MCC and dual eligible population, identified by disability type. Logistic regression analyses weighted by population were used to assess odds ratios for having MCC within each age and disability category for all dual eligibles with MCC, adjusted for sociodemographic and health characteristics.\n\n【27】Top of Page\n\n【28】Results\n-------\n\n【29】Fifty-three percent of dual eligibles aged 18 to 64 had MCC compared with 73.5% of those 65 or older ( Table 1 ). Among these dual eligibles with MCC, mean ages and total annual expenditures were 51 and $18,137 for those aged 18 to 64 and 75 and $14,364 for those aged 65 or older, respectively. Among dual eligibles with MCC who had a physical disability alone, 68% had MCC among those aged 18 to 64 and 86% had MCC among those 65 or older, comparable to prevalence for those with both physical disabilities and cognitive limitations but higher than prevalence for these age groups for those not reporting a physical disability or having only a cognitive limitation or other disability (30% and 52%, respectively). The number of chronic conditions included in our definition of MCC ranged from 2 (24%) to 7 (<1%), with variation by disability and age categories (Figure). Sixty-six percent of all dual eligibles had 2 or more chronic conditions. Among dual eligibles aged 65 or older with physical disabilities and cognitive limitations, 35% had 4 or more chronic conditions. Dual eligibles aged 18 to 64 in each disability category had higher rates of 1 or fewer chronic conditions than did older dual eligibles.\n\n【30】**Figure.** Multiple chronic conditions among Medicare- and Medicaid-eligible noninstitutionalized adults, by disability category and age, Medical Expenditure Panel Survey (MEPS), 2005–2010 (N = 3,398,940 \\[weighted\\]). Abbreviations: PD, physical disability; CL, cognitive limitation. The PD category includes those with functional limitations or those who use assistive devices. It includes anyone who reported having long-term walking limitations, long-term need for assistive device, or long-term need for assistance with activities of daily living but did not report cognitive limitation. CL includes those who reported confusion or memory loss, having problems making decisions, or requiring supervision for their own safety. The category “no PD or CL” includes those who did not identify as having either a PD or CL. For chronic conditions, the full-year consolidated files for MEPS include only questions related to heart attack, coronary heart disease, angina, angina pectoris, other heart disease (these 5 grouped as cardiovascular disease), stroke, emphysema, hypertension, diabetes, arthritis and asthma. \\[A tabular version of this figure is also available.\\]\n\n【31】For dual eligibles aged 18 to 64 with MCC, 59% were female, 60% non-Hispanic white, and 10% Hispanic; 76% lived in an urban area, 50% lived below federal poverty guidelines, 31% lacked a high school diploma, and 29% reported poor health status ( Table 2 ). Additionally, 59% were obese, 94% reported having a usual source of medical care, 74% were not married, and a greater percentage lived in the South (32%) than in other parts of the country. For dual eligibles with MCC who were aged 65 or older, 71% were female, 45% were non-Hispanic white, 22% were Hispanic, 81% lived in urban area, 35% lived below federal poverty guidelines, 61% lacked a high school diploma, and 17% reported poor health status. Thirty-nine percent were obese, with usual sources of care, married status, and residence in the South comparable to those aged less than 65 years.\n\n【32】The leading chronic conditions among those measured for both those aged 18 to 64 and those 65 or older were hypertension and arthritis, respectively. Among each disability category and the total, the greatest differences between those aged 18 to 64 years and those aged 65 or older were for having asthma and cardiovascular disease, with 36% of all those aged 18 to 64 reporting asthma compared with 20% of those 65 or older and 41% of those 18 to 64 reporting cardiovascular conditions compared with 51% of those 65 or older.\n\n【33】Variation in prevalence was evident between subsets of this population. Hispanics comprised a larger proportion of dual eligibles with MCC among those aged 65 or older compared with those aged 18 to 64, regardless of disability category. Alternately, non-Hispanic whites comprised a substantially larger proportion of those aged 18 to 64 than of those aged 65 or older. The proportion of women was higher than the proportion of men among those with MCC, regardless of age, but this differential was larger for those aged 65 or older. In education, twice the proportion of dual eligibles with MCC aged 65 or older had less than a high school education than seen among those aged 18 to 64. For dual eligibles with a high school education, there were higher proportions in the younger group than in the older group. Dual eligibles aged 18 to 64 in each disability category had high obesity rates, as high as 60% among those with physical disabilities. High percentages of poor health status were reported by those aged 18 to 64, with the highest rate (39%) among dual eligibles with both physical disabilities and cognitive limitations; among those aged 65 or older, poor health status was highest among those with both physical disabilities and cognitive limitations. A greater proportion of dual eligibles with MCC aged 65 or older lived in the South than in other regions of the country.\n\n【34】Several factors were significantly associated with MCC. Among all dual eligibles aged 18 to 64 having MCC, men aged 18 to 64 were 25% less likely to have MCC than women ( Table 3 ). With each year of age within this age category, the odds of MCC increased by 9%, and Hispanics were significantly less likely than non-Hispanic whites to have MCC. In this age group for all dual eligibles, there was a 59% or 69% greater likelihood of having MCC among those living in the Midwest or South, respectively, compared with those in the West, a 31% greater likelihood among nonurban dwellers, and a 127% increased odds among those with a usual source of medical care. Fair or poor health status and obesity or underweight were associated with MCC in this age category, as they were for those aged 65 or older. Significant odds ratios for dual eligibles among those aged 65 or older suggested a greater likelihood of MCC for age, being non-Hispanic black, having a high school or lower education, having a usual source of medical care, obesity or underweight, and fair or poor health status.\n\n【35】Among dual eligibles with only physical disabilities, significant odds ratios associated with having MCC were found among those aged 18 to 64 for age, being married, living in the Midwest compared with the West, reporting fair or poor health, or not having a healthy weight. Except for health status and healthy weight, all of these significant associations disappeared for dual eligibles with physical disabilities who were 65 or older, with only non-Hispanic ethnicity being significant. Among people with both physical and cognitive disabilities, significant odds of having MCC were found, again among those aged 18 to 64 years first, for age, being black, being a high school graduate, being married, living in the South, reporting fair or poor health status, or unhealthy weight. For those 65 or older with both physical and cognitive limitations, significant odds were found for women, other race/ethnicity (this includes Asian, American Indian, and persons from the Pacific Rim), reporting fair or poor health status, or unhealthy weight. For all other dual eligibles (“other disabilities” category), age, fair or poor health status, and unhealthy weight were significant among those aged 18 to 64 years, with Midwest residence replacing age as significant also among those 65 or older.\n\n【36】Top of Page\n\n【37】Discussion\n----------\n\n【38】Most attention to dual eligibles has been tied in some way to controlling their cost to the health care system, leading to discussions of targeted care management strategies, early screening for potential high cost or high risk conditions to apply preventive strategies, and any number of risk-adjustment schema that help provide more equitable compensation for high-end and high-resource services. Attention has also focused on better understanding the scope and range of chronic conditions among dual eligibles, with the implications that greater attention to controlling 1 or more of the conditions through a multidisciplinary delivery model may prove beneficial. Our work amplifies a third factor that can be considered: disability, or more broadly, functionality as reflected by disability. Our findings show that sociodemographic or health-related characteristics may influence the likelihood of an association with MCC among all dual eligibles. One implication of this finding is the potential to more fully address the social determinants of health affecting those with certain types of physical disabilities in ways that may help control the effects of MCC through improved access to services, not just in health care settings but in communities at large, realizing the importance of targeting some of these strategies to work-related environments, especially for those under age 65. Regardless of whether the disability preceded or followed the chronic conditions identified in this research, greater environmental accessibility to physical activities, adequate nutrition, health care services, disease management education, and other components that directly address the ability of dual eligibles to function more freely within communities could improve outcomes. Given our findings that suggest high rates of usual sources of medical care for all disability categories among dual eligibles, provider education may also play a role.\n\n【39】MEPS data are limited in ways that influence the ability to extrapolate findings. Responses are self-reported, which can result in errors related to recall or poor understanding. Lack of understanding is made more likely by the representation of people with functional disabilities in the dual eligible population, many of whom have cognitive disabilities that may influence responses. Although MEPS routinely uses proxies as respondents for those with cognitive limitations, this methodology can increase bias because the person reporting may not have accurate knowledge of experiences solicited by the questions. In addition, the questions used to identify cognitive limitations are broad, including a wide range of disability from intellectual disability to dementia. Because MEPS does not oversample on the basis of disability, some less common disabilities may not be fully represented within the dual eligible population.\n\n【40】Our sample only includes dual eligibles from the noninstitutionalized population, while many, estimated at 17% (7), of dual eligibles live in institutional settings. This limitation makes comparisons to studies using other data sources that include people who live in large congregate care facilities somewhat problematic, and we assume that for this reason some of our prevalence rates may be underestimated. In addition, our selection of the 7 most frequently identified chronic conditions is a subset of others identified elsewhere (14,17,18). Although we thought the 7 conditions represented a meaningful subset of chronic conditions pervasive in this population, direct comparisons with other research that included conditions such as, for example, depression, identified through data sources such as the Medicare Current Beneficiary Data, are not possible.\n\n【41】Limitations notwithstanding, our analysis shows that within the dual eligible population, disability subgroups have unique associations with MCC. People with physical disabilities, especially if accompanied by a cognitive limitation, are among the highest costing and sickest of our noninstitutionalized dual eligible population. The potential for more targeted interventions aligning with the goals of the US Department of Health and Human Services Strategic Initiative is promising. Such interventions would target improvement of overall health status by taking into account ongoing efforts to understand and address the challenges faced by the dual eligible population in accessing treatment of chronic conditions and heightened attention to identifying and targeting disability subgroups for whom strategies can be identified. A better understanding of dual eligibles that includes disability can help policy makers more effectively design cost-containment strategies, case managers more effectively steer consumer-directed health plans, and people enrolled in both Medicaid and Medicare identify practices among targeted and coordinated services that will lead to their improved health at substantially reduced costs to themselves and the emerging systems in which they will be a part.\n\n【42】Top of Page\n\n【43】Acknowledgments\n---------------\n\n【44】The authors acknowledge statistical support provided by Ms Kandace Fleming and the extensive editorial support provided by Ms Mallory Murphy of the University of Kansas in the development of this manuscript.\n\n【45】Top of Page\n\n【46】Author Information\n------------------\n\n【47】Corresponding Author: Michael H. Fox, ScD, Associate Director for Science, National Center on Birth Defects and Developmental Disabilities, Division of Human Development and Disabilities, Centers for Disease Control and Prevention, 1600 Clifton Rd, Mailstop E-88, Atlanta, GA 30333. Telephone: 404-498-3806. E-mail: mhfox@cdc.gov .\n\n【48】Author Affiliations: Amanda Reichard, University of Kansas, Lawrence, Kansas.\n\n【49】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "28007f5b-ea69-4d81-a1db-8cc95cf4c4df", "title": "Resistance to Dihydroartemisinin", "text": "【0】Resistance to Dihydroartemisinin\n**To the Editor:** The title of the letter by Cojean et al. ( _1_ ) is misleading. The data presented essentially point to an absence of in vitro resistance to dihydroartemisinin (dhART) in the panel of African isolates studied, with 1 of 397 isolates having an elevated 50% inhibitory concentration (IC <sub>50 </sub> ) for dhART. The S769N _PfATPase6_ mutation associated with in vitro resistance to artemether ( _2_ ) was observed in 1 isolate. This mutant isolate had a low IC <sub>50 </sub> for dhART, but its IC <sub>50 </sub> for artemether has not been tested. Since the relationship between in vitro susceptibility to artemether and dhART is still uncertain ( _3_ ), these data do not disprove the association of a _PfATPase6_ S769N polymorphism with elevated IC <sub>50 </sub> for artemether that was observed in isolates from French Guiana ( _2_ ).\n\n【1】Worth noting is that the association of the S769N _PfATPase6_ polymorphism with elevated IC <sub>50 </sub> for artemether was confirmed in an isolate collected in French Guiana in 2005; that isolate had an IC <sub>50 </sub> for artemether of 127 nmol/L. Molecular typing identified 2 clonal types, 1 with a wild-type _PfATPase6_ allele and 1 with a S769N single mutant. After 3 weeks of in vitro cultivation without drug, the mutant allele was no longer detected and the IC <sub>50 </sub> for artemether was 8.2 nmol/L. This finding suggests poor fitness of the mutant allele under standard culture conditions.\n\n【2】The observation of an additional case of in vitro resistance to artemether in French Guiana 3 years after the first cases is of concern. Reinforcement of surveillance is needed as is clarification of the relationship of in vitro susceptibility to artemether and artesunate, the derivatives currently included in artemisinin-based combination therapies (ACTs). Surveillance and clarification would be particularly timely since emerging clinical or parasitologic failures to some ACTs have been reported ( _4_ , _5_ ).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "064494ed-7d21-45eb-814d-a785789dc921", "title": "STRESS AT WORK", "text": "【0】STRESS AT WORK\nOn This Page\n\n【1】*   Overview\n*   Job Stress and NORA\n*   NIOSHTIC-2 Search\n*   NIOSH Books, Reports, and Videos\n*   Other NIOSH Pages on Stress\n\n【2】### Overview\n\n【3】Job Stress\n\n【4】Job stress can be defined as the harmful physical and emotional responses that occur when the requirements of a job do not match the capabilities, resources, or needs of the worker. Job stress can lead to poor health and even injury.\n\n【5】Primary themes in job stress research at NIOSH include:\n\n【6】*   To better understand the influence of what are commonly-termed “work organization” or “psychosocial” factors on stress, illness, and injury\n*   To identify ways to redesign jobs to create safer and healthier workplaces\n\n【7】Examples of research topics at NIOSH within these two broad themes include:\n\n【8】*   Characteristics of healthy work organizations\n*   Work organization interventions to promote safe and healthy working conditions\n*   Surveillance of the changing nature of work\n*   Work organization interventions to reduce musculoskeletal disorders among office operators\n*   Work schedule designs to protect the health and well-being of workers\n*   The effects of new organizational policies and practices on worker health and safety\n*   Changing worker demographics (race/ethnicity, gender, and age) and worker safety and health\n*   Work organization, cardiovascular disease, and depression\n*   Psychological violence in the workplace\n*   Publishes educational documents on work, stress, and health\n\n【9】### Job Stress and NORA\n\n【10】In 1996, NIOSH established an interdisciplinary team of researchers and practitioners from industry, labor, and academia to develop a national research agenda on the “organization of work.” Work organization refers to management and supervisory practices, to production processes, and to their influence on the way work is performed. (In this sense, the study of work organization and health subsumes the field of job stress.) This initiative is part of a broader, collaborative effort by NIOSH external partners to spearhead a “National Occupational Research Agenda” (NORA) to guide occupational safety and health research into the future, not only for NIOSH, but for the entire U.S. occupational safety and health community. During its tenure, the organization of work team has conferred with academic, industry, and labor stakeholders to identify essential research and other requirements to better understand how work organization is changing, the safety and health implications of these changes, and prevention measures. This effort culminated in the NIOSH report “ The Changing Organization of Work and the Safety and Health of Working People .”\n\nIn 2016, at the beginning of the third decade of NORA, NIOSH established the Healthy Work Design and Well-Being (HWD) Cross-Sector Program ( Healthy Work Design and Well-Being Program | NIOSH | CDC ). Its mission is to protect and advance worker safety, health, and well-being by improving the design of work, management practices, and the physical and psychosocial work environment. Job stress is identified as a priority area of the program.  The HWD Research Agenda, developed by the HWD external council, identifies occupational risk factors, including job stress, that can affect the health, safety, and well-being of workers ( CDC – NORA – Healthy Work Design and Well-Being Sector Agenda ).\n\n【12】### NIOSHTIC-2 Search\n\n【13】NIOSHTIC-2 is a searchable bibliographic database of occupational safety and health publications, documents, grant reports, and journal articles supported in whole or in part by NIOSH.  \nNIOSHTIC-2 search results on job stress\n\n【14】### NIOSH Books, Reports, and Videos about Stress\n\n【15】Working with Stress Video\n\n【16】DHHS (NIOSH) Publication No. 2003-114d  \nA brief introduction to work stress issues for the worker and manager. Topics include the causes of job stress, physical and psychological effects, and what can be done to minimize job stress. The video is available in both DVD and VHS formats, and can also be viewed online. (17 minutes)  \nNIOSH Working with Stress Part 1 of 2  \nNIOSH Working with Stress Part 2 of 2\n\n【17】Stress… At Work Booklet\n\n【18】DHHS (NIOSH) Publication No. 99-101 (1999)  \nThis booklet highlights knowledge about the causes of stress at work and outlines steps that can be taken to prevent job stress.\n\n【19】Worker Health Chartbook 2004: Anxiety, Stress, and Neurotic Disorders\n\n【20】DHHS (NIOSH) Publication No. 2004-146 (2004)  \nProvides data for anxiety and stress disorders based on magnitude and trend, age, sex race/ethnicity, severity, occupation, and industry.\n\n【21】### Other NIOSH Pages on Stress\n\n【22】*   NIOSH Stress Blogs\n*   NIOSH Occupational Health Psychology\n*   NIOSH Quality of Worklife Questionnaire\n*   Work Schedules: Shift Work and Long Work Hours\n*   Traumatic Incident Stress\n*   Cardiovascular Disease and Occupational Factors\n*   Aircrew Safety & Health – Job Stress\n\n【23】<div> <a> </a> </div>\n\n【24】Low Resolution Video\n\n【25】<div> <a> </a> </div>\n\n【26】Low Resolution Video\n\n【27】<div> <a> Stress and Cardiovascular Disease among First Responders: Data from the BCOPS Study </a> </div>", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "517bee1a-34f7-4c39-8ba1-c976e7f2b3ad", "title": "Syndromic Surveillance", "text": "【0】**To the Editor:** As public health practitioners directly involved in constructing, maintaining, and interpreting syndromic disease surveillance systems, we offer the following comments on the Buehler et al. article, \"Syndromic Surveillance and Bioterrorism-related Epidemics\" ( 1 ). In general, this article was well-crafted. It reviewed the potential for syndromic surveillance to detect various diseases of bioterrorism, specifically an anthrax event based on the inhalational anthrax cases of 2001. However, the reader may conclude that hospital-based syndromic surveillance is potentially ineffective and unproven.\n\n【1】Buehler et al. describe how, within 18 hours, a presumptive diagnosis of anthrax would prompt a full-scale response. We think that functional syndromic surveillance can respond to the rapid onset of hospital-based disease. To isolate and positively identify _Bacillus anthracis_ from a blood culture would take ≈48 hours. Syndromic surveillance should detect a large number of cases within 24 hours. A fully functional hospital syndromic surveillance system that uses automated analysis (such as the daily emergency department–based surveillance with SaTScan in New York City) should identify a substantial increase in a relevant syndrome within 12 to 24 hours after data submission ( 2 ). A continued daily rise in any disease category would most certainly set off alarms in a syndromic surveillance network. If active statewide laboratory surveillance is included in syndromic surveillance, such as the gram-positive rod surveillance conducted in Connecticut ( 3 ), this surveillance should rapidly detect even single cases of anthrax concurrent with the presumptive diagnosis within the hospital.\n\n【2】The authors also state that syndromic surveillance would not detect outbreaks too small to trigger statistical alarms. The combination of active and passive surveillance in the hospital admissions–based syndromic surveillance in Connecticut allows a number of syndromes to be tracked immediately upon notification; these syndromes include pneumonia and acute respiratory disease in healthcare workers admitted to a hospital, all disease clusters, and fever with rash illness. This system is very flexible, and active surveillance of other syndromes can be quickly instituted as required. This active surveillance component has been proven useful. The first 2 of Connecticut's 17 confirmed human cases of West Nile virus during 2002 were discovered in August when a health director, who regularly monitored the syndromic admissions data for the hospital in his municipality, requested immediate West Nile virus testing from the hospital's infection-control department when he received two late summer reports of neurologic illness.\n\n【3】Buehler et al. state that specificity for distinguishing bioterrorism-related epidemics from more ordinary illness may be low because the early symptoms of bioterrorism-related illness overlap with those of many common infections. Illness specificity can be modulated within a syndromic surveillance system by making changes in the definition of the information requested, the method of analysis used, or by incorporating varying amounts of active surveillance into a passive reporting system. In Connecticut, annual rates of hospital admissions for pneumonia and respiratory illness have significantly increased (>3 standard deviations) during winter months. These increases have corresponded temporally with peaks in laboratory-confirmed influenza reports and in our state-based and the national sentinel physician influenzalike illness reports. Similarly, in the military-based syndromic surveillance system, respiratory outbreaks are detected by monitoring routine outpatient visits and pharmacy prescriptions. Absolute numbers of visits, as well as percentage of visits, to primary care clinics for influenzalike illness provide up-to-date information on respiratory disease conditions at military installations in both active-duty personnel and family members.\n\n【4】Connecticut has added additional active surveillance categories to its syndromic surveillance for potential SARS cases by gathering extensive data on all healthcare providers hospitalized with respiratory illness. In the absence of an identified pathogen, the entire United States was conducting syndromic surveillance for SARS during the spring of 2003.\n\n【5】What are existing alternatives to rapid, patient-based reporting through syndromic surveillance for bioterrorism and emerging illness? Will individual physicians (i.e., the \"astute clinicians\") truly recognize an increase of nonspecific symptoms among their patients in time to warn public health authorities of an impending bioterrorism event? During the past 4 years in the U.S. military population, unless disease was extremely severe with high rates of hospitalization, virtually no outbreaks of infectious diseases detected by syndromic surveillance were reported to public health officials, even when effective preventive measures existed. Our experience leads us to encourage states and municipalities to develop functional, patient-based syndromic surveillance systems and discover both their limitations and their possibilities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "08a48518-e578-4b12-8f0b-71d7ee64198f", "title": "Risk Factors for Influenza among Health Care Workers during 2009 Pandemic, Toronto, Ontario, Canada", "text": "【0】Risk Factors for Influenza among Health Care Workers during 2009 Pandemic, Toronto, Ontario, Canada\nThe numerous outbreaks of influenza described in acute care hospitals indicate that influenza transmission in this setting is of major concern ( _1_ _–_ _3_ ). Nonetheless, it remains unclear whether health care workers (HCWs) are at higher risk for infection than are adults working in nonclinical settings (non-HCWs). Vaccination recommendations for HCWs are intended primarily to protect patients from hospital-acquired influenza and influenza-associated death ( _4_ , _5_ ). Although working in hospitals has been proposed as a risk factor for influenza ( _6_ ), findings that support that working in health care settings poses an occupational risk ( _7_ ), or that performing particular activities or working in specific health care disciplines are associated with an increased risk for influenza infection, are sparse.\n\n【1】Better understanding of risk factors for infection among HCWs would support decision-making regarding priorities for seasonal influenza vaccination, antiviral treatment or prophylaxis programs, implementation of other measures to reduce influenza transmission in hospitals, and planning for pandemics. Therefore, we aimed to assess risk factors for influenza among HCWs and to determine whether, during the first 2 waves of influenza A(H1N1)pdm09, HCWs working in acute care hospitals were at higher risk than non-HCWs for symptomatic influenza.\n\n【2】### Materials and Methods\n\n【3】##### Participants and Setting\n\n【4】The Influenza Cohort Study, initiated by the Working Adult Influenza Cohort Study Group, a research team based in Toronto, Ontario, Canada, was started in May 2009. The purpose of the study was to examine incidence, clinical features, and epidemiology of infection caused by A(H1N1)pdm09 among HCWs and other working adults in Canada. For this analysis, participants were enrolled during May 29–September 27, 2009. Participants were eligible if they were 18–75 years of age and either worked \\> 8 hours per week in 1 of 5 acute care hospitals (HCW) or in an office-based setting in Toronto (non-HCW). Non-HCWs were intended to provide a sample of working adults at low occupational risk for influenza, so as to bias the study toward the ability to identify an occupational risk in health care. Details of the recruitment of these control participants are included in the Technical Appendix . The study was approved by the Research Ethics Boards of all participating hospitals and universities and by the human resources departments of participating employers.\n\n【5】Upon enrollment, participants received a collection kit, an illustrated guide, and instruction from a nurse for mid-turbinate nasal swab sample self-collection. They also completed a Web-based questionnaire detailing influenza vaccination history, underlying medical conditions, demographic data, potential work- or school-related risk factors for respiratory virus infection, and potential community risk factors. Blood samples were taken from consenting participants at enrollment and again in April or May of 2010.\n\n【6】Participants were asked to complete weekly Web-based diaries from enrollment until March 31, 2010, detailing respiratory symptoms and acute respiratory illness (ARI) or febrile illnesses and documenting time-dependent risk factors (e.g., contact with persons with ARI symptoms). Per the study protocol, if any signs or symptoms suggestive of an ARI developed, participants provided a self-collected mid-turbinate nasal swab sample as soon as possible after onset to be tested for influenza by using PCR. ARI was defined as 1) fever without another obvious source; or 2) new symptoms, including \\> 2 of the following: runny or stuffy nose, sneezing, sore or scratchy throat, hoarseness, or cough; or 3) one local (runny/stuffy nose, sneezing, sore/scratchy throat, hoarseness, or cough) and 1 systemic symptom (fever, malaise, myalgia, headache, or fatigue).\n\n【7】Participants whose specimens tested positive for influenza were offered treatment in accordance with public health recommendations ( _8_ ). All participants with undetermined A(H1N1)pdm09 vaccine status as of March 31, 2010, were contacted again to confirm whether they had received it and, if so, when. For logistical reasons, participants with unconfirmed 2009–2010 seasonal influenza vaccine status could not be contacted again; instead, these participants were assumed not to have received it. In Canada, vaccine for A(H1N1)pdm09 became available for HCWs and patients at high risk for complications of influenza during calendar week 43 (starting October 25, 2009) and was available for healthy adults during calendar week 47 (starting November 22).\n\n【8】##### Definitions\n\n【9】For this study, HCW were defined as persons working in an acute care hospital. A non-HCW was defined as a person working in an office-type environment not associated with the provision of health care. The first and second waves of the influenza pandemic in Ontario were defined as the periods for which the weekly proportion of respiratory specimens that were positive for A(H1N1)pdm09 was >5%, as reported by the Ontario Agency for Health Protection and Promotion. Similarly, seasonal influenza waves were defined as periods for which >5% of weekly specimens tested positive for seasonal influenza. By this definition, the first pandemic wave occurred during calendar weeks 21–31 of 2009 (May 17–August 8); the second wave occurred during calendar weeks 39–48 (September 27–December 5). Peak weeks were defined as weeks during which positivity rates were >15% and comprised calendar weeks 21–27 (May 17–July 11) during wave 1 and calendar weeks 41–46 (October 11–November 21) during wave 2. As expected, few cases of seasonal influenza were identified during the study period.\n\n【10】Aerosol-generating medical procedures were defined as any of the following: administration of nebulized therapy or humidified oxygen at >40%, use of bag-valve mask, manual ventilation, noninvasive ventilation, open airway suctioning, bronchoscopy or other upper airway endoscopy, tracheostomy, endotracheal intubation, cardiopulmonary resuscitation, oscillatory ventilation, or any procedure that involved manipulation of open ventilator tubing in a mechanically ventilated patient or sputum induction or other deliberate induction of coughing.\n\n【11】Adherence to hand hygiene and facial protection recommendations was defined as the self-reported proportion of situations during which hand hygiene and facial protection were performed according to infection control recommendations ( _9_ ). Symptomatic influenza infection was defined as influenza-positive PCR results for a participant-collected mid-turbinate nasal swab sample.\n\n【12】##### Antibody Assays and Interpretation\n\n【13】Serum specimens were extracted from blood samples and 1 mL aliquots frozen at −70°C. Aliquots were tested by hemagglutination-inhibition (HAI) assay to determine antibody titers against the A(H1N1)pdm09 strain (A/California/07/2009-like) and the 2008–09 seasonal A(H1N1) strain (A/Brisbane/59/07) to identify potential cross-reactivity by using a protocol adapted from World Health Organization methods ( _10_ ). Two HAI assays were performed per aliquot by using 0.5% turkey erythrocytes and 4 hemagglutination units per 25 µL of virus. For discordant pairs, the higher of the 2 geometric mean titers was used. Serum specimens were tested at the Queen Elizabeth II Health Sciences Centre, Halifax, Nova Scotia, Canada. Seroprotection was defined as having HAI antibody titers of \\> 40\\. Seroconversion was specifically defined as a prevaccination HAI titer of <10 and a postvaccination titer of \\> 40 or a 4-fold change in titers for participants with a prevaccination titer of \\> 10 ( _11_ , _12_ ).\n\n【14】##### Data Management and Statistical Analyses\n\n【15】Data were entered online by the participants, then cleaned and manually inspected for errors and outlying values. Differences in group proportions were assessed by the χ <sup>2 </sup> or Fisher exact test, as appropriate, and differences in means (for normally distributed data, on the basis of the Shapiro-Wilk test for normality) and medians (for non-normally distributed data) were calculated by using Student _t_ test and Wilcoxon rank-sum test, respectively.\n\n【16】The analysis for the primary objective (i.e., to determine whether the risk for laboratory-confirmed symptomatic influenza was higher in HCWs than in non-HCWs) included all participants who were enrolled by the start of the second wave of the 2009 H1N1 influenza pandemic (calendar week 39, starting September 27, 2009). Multivariable generalized estimating equation logistic regression analysis was used to determine adjusted odds ratios with 2-sided 95% CIs for constant and time-dependent risk factors for symptomatic influenza infection on the basis of information from baseline questionnaires and weekly diaries. Model construction was performed on the basis of the method proposed by Harrell ( _13_ ) including A(H1N1)pdm09 vaccination status and changing risk for influenza infection over time (community influenza activity). Our a priori approaches to adjust for changing risk for influenza infection over time were to 1) adjust for weekly percentage of specimens positive for influenza reported to the Ontario Agency for Health Protection and Promotion (continuous variable) and 2) adjust for peak weeks (defined as weeks during which >15% of specimens were positive for influenza; \\[dichotomous variable\\]). Vaccine failure among participants was defined as acquiring A(H1N1)pdm09 infection after receipt of A(H1N1)pdm09 vaccine >7 days before symptom onset. Participants who acquired A(H1N1)pdm09 within 7 days after vaccination were considered not fully protected. To evaluate the validity of this assumption, we performed sensitivity analyses by calculating lags of 0 days and 14 days, respectively. The same criteria were used in the analysis of the secondary objective (i.e., to determine risk factors for laboratory-confirmed symptomatic influenza among HCWs). The models with the lowest quasi-likelihood under the independence model criterion were preferred.\n\n【17】Data were analyzed in SAS, version 9.1 for PC (SAS Institute, Cary, NC, USA). We considered p values <0.05 as statistically significant.\n\n【18】##### Sample Size\n\n【19】This study was initiated at the onset of the 2009 influenza pandemic; because the expected incidence of infection was unknown, a formal sample size was not established. Details of the sample size estimate for the planned seasonal study can be found in the online Technical Appendix.\n\n【20】### Results\n\n【21】##### Study Population, Symptomatic Influenza Case-patients and Community Influenza Activity\n\n【22】Figure\n\n【23】Figure . . . . Flowchart of 732 persons enrolled in the Influenza Cohort Study, Toronto, Ontario, Canada\n\n【24】The first participant was enrolled in the study on May 28, 2009 (calendar week 21). By October 11 (calendar week 41), at the start of the second wave of the pandemic, 732 participants were enrolled in the Influenza Cohort Study: 563 (76.9%) were HCWs who worked in 1 of 5 community and teaching acute care hospitals in the Toronto area and 169 (23.1%) were non-HCWs who worked in an office environment not associated with the provision of health care ( Table 1 ; Figure ). Of the 2 cohorts, HCWs were younger and were more likely to have been vaccinated against seasonal and pandemic influenza, to work with children, to have children <5 years of age in their households, and to use public transportation \\> 8 times per week. Of 422 HCWs who were vaccinated against A(H1N1)pdm09, 403 (95.5%) received vaccine within 2 weeks after its availability; of 61 non-HCWs, 28 (45.9%) were vaccinated during the same time period (p<0.001).\n\n【25】A total of 334 (45.6%) study participants submitted 436 nasal swab samples. More than half (52.1%) of these samples were collected on the day of symptom onset (day 1), 19.4% on day 2, 9.9% on day 3, and 12.1% on or after day 4. Among the 20 (4.6%) specimens yielding influenza, 12 (60.0%) were collected on day 1, four (20.0%) on day 2, three (15.0%) on day 3, and one (5.0%) on day 4 of illness. Thirteen (2.2%) of 563 HCWs and 7 (4.1%) of 169 non-HCWs submitted samples that tested positive for influenza. A(H1N1)pdm09 was detected in 19 (95%) of the 20 positive participants: 1 case during each of calendar weeks 24, 25, 31, 39, 40, and 47; two cases during each of calendar weeks 42, 44, and 45; and 7 cases during calendar week 43. Thus, 16 of 19 cases occurred during weeks of peak A(H1N1)pdm09 activity. Seasonal influenza A(H3N2) virus was isolated in a sample from 1 participant during calendar week 43.\n\n【26】##### Risk Factors for Symptomatic Influenza Infection\n\n【27】The probability of symptomatic influenza infection did not differ between HCWs and non-HCWs (p = 0.28) ( Table 2 ). Study participants who had a child <18 years of age living in the household (36.2% of influenza negative/untested participants vs. 65.0% of influenza positive participants; p = 0.009), a child who attended day care living in the same household (12.5% vs. 30.0%; p = 0.03), and who were not vaccinated against A(H1N1)pdm09 >7 days before onset of infection (76.3% vs. 10.0%; p<0.001) were more likely to have respiratory illness with positive test results for influenza.\n\n【28】After adjusting for A(H1N1)pdm09 vaccination history and community influenza activity, we found no difference in the risk for influenza infection between persons working in an acute care hospital (HCWs) and other healthy adults (non-HCWs) ( Table 3 ). Rather, contact with a family member with an ARI in the previous week was the main risk factor for symptomatic influenza infection, irrespective of the method of adjusting for changing risk over time. In general, quasi-likelihood under the independence model criterion statistics were lower in models adjusting for weekly percentage of specimens yielding influenza than in those adjusting for weeks of peak influenza activity (results not shown). A sensitivity analysis calculating lags of 0 and 14 days (vs. 7 days) from the time of receipt of A(H1N1)pdm09 vaccine did not alter these results.\n\n【29】Analyses restricted to HCWs and including potential occupational risk factors in health care are shown in Table 4 . During the study period, 49.6% of HCWs worked in emergency departments, medical inpatient wards, intensive care units, or pediatric wards; 12.9% were present during >1 and 9.4% performed >1 aerosol-generating medical procedure per week. Approximately one quarter (26.5%) of HCWs reported providing direct care for >1 patient per week who had ARI. The analysis of risk factors for infection indicates that, similar to the combined study population, HCWs with symptomatic influenza infection confirmed by positive nasal swab sample were more likely to have children <18 years of age in their households (69.2% of HCWs who tested positive vs. 36.9% who tested negative or were untested; p = 0.02) and less likely to have been vaccinated against A(H1N1)pdm09 >7 days before onset of infection (15.4% vs. 86.3%; p<0.001) ( Table 4 ). Compared with other HCWs, those with symptomatic influenza infection were more likely to be present during aerosol-generating medical procedures >1× per week (38.5% vs. 12.7%; p = 0.02) and reported lower adherence to hand hygiene recommendations (77.5% vs. 95%; p = 0.02). After adjustment for changing risks for influenza infection over time, risk factors for influenza infection among HCWs were: contact with a family member with ARI in the previous week, performing or assisting with aerosol-generating medical procedures, and lower adherence to hand hygiene recommendations ( Table 5 ).\n\n【30】##### HAI Antibody Assays\n\n【31】Among the combined study population, 450 (61.5%) of 732 participants provided pre- and post-influenza season blood samples. Among those, 3.6% had protective HAI titers against A(H1N1)pdm09 at baseline. There was no association with workplace and baseline HAI titers. Of the 142 (31.6%) participants who tested positive after enrollment, 137 (96.5%) had received the A(H1N1)pdm09 vaccine, 2 (1.4%) submitted a nasal swab that tested positive by PCR, and 3 (2.2%) did not submit a swab for testing or report an ARI (consistent with asymptomatic infection).\n\n【32】Analysis of data collected during the period after vaccine became available for unvaccinated participants without known previous A(H1N1)pdm09 infection showed that 8 (16.3%) of 49 HCW and 3 (5.3%) of 57 non-HCWs seroconverted or had a positive mid-turbinate nasal swab sample. Although persons working in an acute care hospital were 3.1× as likely as other working adults to be infected with influenza, the results were not significant in this small unvaccinated group (95% CI 0.9–11.1). Influenza among unvaccinated participants was not associated with age, sex, or any of the other characteristics listed in Table 1 .\n\n【33】### Discussion\n\n【34】In this prospective cohort study conducted in Canada during the 2009 influenza A(H1N1) pandemic, we found no association between working in an acute care hospital and risk for influenza infection. Our findings are similar to those of Williams et al., who assessed serologically confirmed influenza during the 2007–08 influenza season in Berlin, Germany ( _14_ ). They found no association between HCW status and influenza but demonstrated that the presence of children in the household and ownership of a car among participants with no children in the household were risk factors, whereas receipt of seasonal influenza vaccine was found to be protective. Similarly, Marshall et al. found no overall difference in influenza infection rates between hospital workers who did and did not have patient contact during the 2009 pandemic in Australia, but the authors identified exposure to children as a risk for influenza ( _15_ ).\n\n【35】The results of this cohort study also add insight into occupational risk factors for influenza among persons who work in acute care hospitals. In contrast to a finding by Kawana et al. ( _16_ ), neither our study nor those of Marshall et al. and Seto et al. detected an increased risk for influenza among workers who had direct patient care responsibilities ( _17_ ). However, Marshall et al. indicated that working in an intensive care unit of a hospital was a risk factor for influenza, and wearing gloves while caring for patients who were on droplet precaution was protective. These findings are similar to ours in that exposure to aerosol-generating medical procedures, which are most often performed in intensive care units, was a risk factor for influenza, and adherence to hand hygiene, which may have an effect similar to appropriate glove use, was protective. Although collinearity of both putative risk and protective factors may continue to make it difficult to accurately identify risk factors for acquisition of influenza in health care settings, our data highlight the role of hand hygiene in the control of influenza infection ( _18_ ), and of protective equipment use by persons who perform or assist with aerosol-generating medical procedures.\n\n【36】The mode of transmission of influenza remains a matter of ongoing debate. Although most experts believe that droplet and aerosol transmission are the most common modes of spread of influenza, our finding and that of Marshall et al. ( _15_ ), as well as the evidence from the elementary school–based study by Talaat et al. that increasing hand hygiene adherence reduces the risk for infection with influenza, suggest that transmission by direct or indirect contact contributes substantially to influenza transmission ( _18_ ). Appropriate hand hygiene practice should continue to be recommended to prevent influenza transmission.\n\n【37】Pandemic influenza vaccine became available in Canada at the peak of the second wave of the pandemic. This complicated our analysis in that the risk for influenza infection depended on differing times of receipt of influenza vaccine and on timing of the pandemic waves. We addressed these issues by using multivariable generalized estimating equation logistic regression for the analysis, which facilitated adjustment for timing of receipt of vaccine, and we accounted for the dynamics of the pandemic waves by incorporating weekly percentages of laboratory specimens that tested positive for influenza virus. We believe that our results are robust because 2 different approaches to adjust for changing risk over time led to the same results. Nevertheless, whether the relative percentage of positive specimens reflects the relative number of influenza cases in the community remains a matter of debate.\n\n【38】Our study has several limitations. It has a lack of power related to the small number of cases of symptomatic influenza during the second wave of the pandemic in this population of working adults. We attempted to minimize selection bias by using broad inclusion and limited exclusion criteria; nevertheless, the possibility of having access to rapid diagnosis and treatment during the second pandemic wave might have resulted in biased enrollment of participants who had a higher self-perceived risk for influenza infection, and perception of risk might differ between persons working in acute care hospitals and persons working in nonclinical settings. Similarly, generalizability may be hampered because participants in studies of influenza could differ from others in their attitudes toward vaccine acceptance and infection prevention practices. We tried to reduce the possibility of measurement bias in nasal swab collection by having a broad interpretation of respiratory illness because the interpretation of more detailed criteria for signs or symptoms of influenza infection (e.g., influenza-like illness) might differ between HCWs and non-HCWs, but differences might have remained. Although the self-collection of swab specimens occurred over 1–4 days after illness onset, it is unlikely that any cases would have been missed because previous studies have shown that A(H1N1)pdm09 remains readily detectable within this period ( _19_ – _21_ ). The study encompasses a selective sample of persons working in a limited number of acute care hospitals and other working adults with Internet access in a single geographic area during the 2009 influenza A(H1N1) pandemic. Although we deliberately selected controls likely to be at low risk for occupational exposure to influenza (e.g., not working in an occupation exposed to numerous children) in an effort not to miss an effect of the health care work environment, unmeasured biases in our control selection could have been present. In addition, our results may not be generalizable to seasonal influenza or to geopolitical areas where infection control practices in hospitals are different.\n\n【39】The yield of self- or parent-collected nasal swab specimens has been shown to be comparable to health care provider–collected nasopharyngeal aspirates from children and adults ( _22_ – _24_ ), but whether the yield of self-collected nasal swabs differs between HCWs and non-HCWs has not been assessed. There is evidence that microneutralization of antibody assays may demonstrate a greater sensitivity than HAI ( _25_ ); as a result, we may have missed seroconversion by using the latter. Further seroconversions might have been missed by the delay between the first (upon enrollment) and the second (April or May 2010) blood sampling caused by declining antibody titers over time. Recall bias might have played a role in that ill participants might have reported risk factors such as contact with sick people in the previous week more accurately than people who did not develop an illness. Finally, participating in the study may have reinforced awareness of the risk for influenza infection and thus may have raised adherence to protective measures.\n\n【40】We did not identify an increase in the risk for influenza among workers in acute care hospitals compared to office-based workers during the 2009 pandemic. However, our findings are limited by lack of power. Within an HCW group, we were able to identify activities that could help focus prevention. Increasing efforts to improve hand hygiene and the use of protective equipment during aerosol-generating medical procedures would further reduce the risk for influenza infection among HCWs.\n\n【41】Dr Kuster is an infectious diseases specialist and clinical epidemiologist at the University Hospital, Zurich, Division of Infectious Diseases and Hospital Epidemiology, Zurich, Switzerland. His research interests focus on epidemiology of influenza infection and antibiotic stewardship.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2d514787-4e54-4290-bceb-c53699ab86c6", "title": "Estimation of Severe Middle East Respiratory Syndrome Cases in the Middle East, 2012–2016", "text": "【0】Estimation of Severe Middle East Respiratory Syndrome Cases in the Middle East, 2012–2016\nMiddle East respiratory syndrome (MERS), caused by MERS coronavirus (MERS-CoV), was first recognized in September 2012 ( _1_ ). From that time until January 2016, >1,600 cases were laboratory-confirmed, and ≈600 deaths have been attributed to the virus ( _2_ ). Cases have been detected among persons who traveled from the Middle East to 16 countries, and a MERS-CoV outbreak in South Korea introduced by a traveler caused >100 cases ( _3_ ).\n\n【1】Estimates of the epidemic size in the Middle East are required to understand the level of MERS-CoV circulation and the likelihood of MERS-CoV exportations. However, these estimates have not been calculated for >2 years, during which time the number of recorded cases has increased by >15 times ( _2_ , _4_ ). We used data from travelers to this region to update estimates of severe MERS cases in the Middle East.\n\n【2】### The Study\n\n【3】We estimated the cumulative number of severe MERS cases in source countries from which laboratory-confirmed MERS-CoV infections were reported by the World Health Organization for nonresident travelers during September 2012–January 2016. The source countries were Saudi Arabia, United Arab Emirates, Jordan, and Qatar ( _1_ ); only the emirates of Abu Dhabi and Dubai were included in the United Arab Emirates calculations due to a lack of traveler data on the other 5 emirates, but no MERS cases have been reported from these other emirates.\n\n【4】All MERS traveler case-patients whose data were used in the analysis were hospitalized because of respiratory symptoms and therefore likely experienced severe disease. However, 30% of reported MERS case-patients were listed as mildly symptomatic or asymptomatic and were likely missed by the passive surveillance systems that detected severe travel-associated cases ( _1_ ). Consequently, we used the term “severe” to indicate that our estimates of case numbers are for those with more serious disease.\n\n【5】To estimate the cumulative number of severe cases, we used data for 1) the number of travelers to the 4 source countries, 2) average trip lengths, 3) number of confirmed MERS-CoV infections among travelers to the source countries, and 4) population sizes of source countries. Our estimates are for the period September 2012, when MERS-CoV was first identified, through January 2016.\n\n【6】We estimated the number of severe cases in each source country using methods used previously by Cauchemez et al., which assume that travelers and local residents have similar per-day risk of infection ( _4_ ). The infection rate among travelers to source countries was multiplied by the total person-time at risk for the population of that country using the following formula:\n\n【7】cumulative number of severe MERS-CoV cases in country X =\n\n【8】severe case rate among travelers to source countries × country X person-time =\n\n【9】×country X population size × 365 × epidemic periodwhere the epidemic period = 3.33 years (September 2012–January 2016). We estimated CIs using profile-likelihoods ( _4_ ). This approach can estimate the cumulative incidence of disease regardless of seasonality in infection rates ( _5_ ).\n\n【10】We analyzed 11 travel-associated MERS cases, including 6 case-patients from high-income countries ( Technical Appendix ). It has been suggested that MERS-CoV surveillance may be better in high-income countries than in lower-income countries ( _6_ ). We tested this hypothesis by comparing the frequency of case detection among travelers returning to high-income countries, as defined by the Organization for Economic Cooperation and Development ( http://www/oecd.org/about/membershipandpartners/ ), versus lower-income countries (all other non–Middle Eastern countries worldwide). We found that significantly more cases have been identified in high-income settings (p<0.001 by Fisher exact test; Technical Appendix ). Consequently, we produced 2 sets of calculations, 1 using data only from high-income countries and 1 that combined data from all non–Middle Eastern countries. The high-income country analysis does not assume that no cases occurred in lower-income countries but shows different case detection rates across travelers’ home countries.\n\n【11】Using data for 32 high-income countries, we estimated ≈3,263 severe cases (95% CI 1,297–6,613; Table 1 ) for all source countries during September 2012–January 2016. We calculated that Saudi Arabia had the largest number of cases (2,269, 95% CI 902–4,599). We estimated ≈1,431 severe cases (95% CI 743–2,452) when data from high-income and lower-income countries were combined.\n\n【12】We conducted sensitivity analyses in which we included 1) laboratory-confirmed cases among travelers for whom it was unclear in which country they had been infected or if they had been infected by another travel-associated case-patient and 2) probable but non–laboratory-confirmed MERS-CoV cases reported in travelers. These analyses indicated there could have been up to 4,895 severe cases across source countries (95% CI 2,352–8,824). We also conducted sensitivity analyses to assess the effect of uncertainty of travelers’ average length of stay in source countries ( Table 2 ; Technical Appendix Tables 2, 3). Increases in travelers’ assumed lengths of stay produced lower cumulative incidence estimates related to lower estimated infection rates, and decreases in travelers’ assumed lengths of stay produced higher case estimates. For example, using data for travelers from high-income nations, a 2-day increase in average length of stay produced estimates of 2,326 severe cases across source countries (95% CI 924–4,714; Technical Appendix Table 2), and a 2-day decrease in lengths of stay produced estimates of 5,463 severe cases (95% CI 2,171–11,071).\n\n【13】### Conclusions\n\n【14】We used data on the incidence of MERS among travelers returning from the Middle East to better estimate the occurrence of severe disease in the most affected countries. We estimated that there were ≈3,300 cases of severe disease in the 4 source countries during September 2012–January 2016. This estimate was 2.3-fold higher than the total number of laboratory-confirmed cases across source countries from September 2012–January 2016.\n\n【15】Using data up to August 2013, Cauchemez et al. estimated the total case count to be 11-fold higher than the number of laboratory-confirmed cases reported across source countries ( _4_ ). The closer agreement between observed and estimated cases in our analysis is consistent with improvements in surveillance practices across source countries during 2014 ( _6_ _–_ _8_ ). Our results are also complementary to a serologic study from Saudi Arabia that reported antibodies to MERS-CoV were found in 0.15% of the population ( _10_ ). Our study adds information by focusing on severe infections (which are of greatest clinical concern) and providing more up-to-date information by including data from the 2-year period after the serologic samples were collected.\n\n【16】Our estimates were based on a small sample size (11 travel-associated cases) and assumed that travelers and residents of the Middle East had similar infection risks. Our sensitivity analyses demonstrated that results are sensitive to travelers’ estimated lengths of stay and also showed that estimates of the epidemic size that incorporated data from lower-income countries were 60% lower than estimates obtained by using data from high-income countries alone. This finding implies different levels of case detection across travelers’ home countries or different MERS-CoV exposure between visitors of different nationalities. Additional data (e.g., larger sample size, travel volume, and lengths of stay, stratified by age and immigration status, frequencies of testing, and contact with camels) could provide further estimates.\n\n【17】Public health officials are concerned about MERS-CoV, both in the source countries and from exported cases in persons who can seed outbreaks elsewhere ( _9_ , _11_ ). By better estimating the epidemic size in the Middle East, our results can help guide public health preparedness efforts in source countries and contribute to projections of the number of cases that could occur among travelers ( _9_ , _11_ – _13_ ).\n\n【18】Dr. O’Hagan is an epidemiologist and transmission modeler at Centers for Disease Control and Prevention. His interests include the combined use of mathematical modeling and epidemiologic studies to better understand the effect of infectious disease risk factors and interventions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a1942418-b95a-4d8f-a3fb-cc15349a422d", "title": "KPC and NDM-1 Genes in Related Enterobacteriaceae Strains and Plasmids from Pakistan and the United States", "text": "【0】KPC and NDM-1 Genes in Related Enterobacteriaceae Strains and Plasmids from Pakistan and the United States\nPathogenic _Enterobacteriaceae_ , including _Escherichia coli_ and _Klebsiella pneumoniae_ , are major causes of multidrug-resistant (MDR) infections in hospitals worldwide. These pathogens have recently been shown to have acquired resistance to carbapenems, and the US Centers for Disease Control and Prevention identified carbapenem-resistant _Enterobacteriaceae_ as of the 3 most urgent MDR threats ( _1_ ). Among the _Enterobacteriaceae_ , β-lactam resistance, including carbapenem resistance, is primarily caused by enzymatic degradation by β-lactamases. Two carbapenemase subclasses are especially problematic: _Klebsiella pneumoniae_ carbapenemase (KPC) and New Delhi metallo-β-lactamase-1 (NDM-1). KPC, identified in 2001 ( _2_ ), has become endemic to several noncontiguous areas of the world, including the United States, Israel, Greece, South America, and China ( _3_ ). NDM-1 was first described in 2008, although retrospective studies identified NDM-1 from 2006 ( _4_ ) and is abundant in New Delhi water samples ( _5_ ). Most patients from whom NDM-1 is isolated have an epidemiologic link to the Indian subcontinent, but NDM-1 has also recently become endemic to the Balkans and Middle East ( _6_ ).\n\n【1】The spread of antibiotic resistance genes such as NDM-1 and KPC is facilitated by horizontal gene transfer (HGT) between bacteria ( _7_ ). Among globally disseminated pathogens, HGT facilitates combination of the most effective antibiotic resistance genes from diverse geographies into multidrug resistance plasmids that spread between strains. Recombination and transposition have created populations of these plasmids that have related architectures but vary in their composition of antibiotic drug resistance cassettes ( _8_ ). This effect has enabled both KPC and NDM-1 to rapidly expand within the _Enterobacteriaceae_ and other proteobacterial pathogens, such as _Acinetobacter baumanii_ ( _9_ , _10_ ). Antibiotic resistance genes can also spread through clonal expansion in successful pathogenic strains, for example, KPC in _K. pneumoniae_ sequence type (ST) 258 ( _11_ ), and the extended-spectrum β-lactamase CTX-M-15 in _E. coli_ ST131 ( _12_ ). Both HGT and clonal expansion have enabled KPC and NDM-1 to rapidly spread to distant locations after their emergence ( _6_ , _8_ ).\n\n【2】The similarities in the spread and resistance spectra of KPC and NDM-1 (both provide resistance to nearly all β-lactam antimicrobial drugs) leads to the hypothesis that similar mobile elements will make both genes available to similar pathogen populations. We tested this hypothesis by examining clinical _Enterobacteriaceae_ isolates from Pakistan and the United States encoding NDM-1, KPC, or no carbapenemase.\n\n【3】### The Study\n\n【4】We collected 450 bacterial isolates (including 195 _Enterobacteriaceae_ ) in Pakistan during February 2012–March 2013 from Pakistan Railway General Hospital in Rawalpindi and the Pakistan Institute of Medical Sciences in Islamabad. From this collection, we randomly selected 55 _Enterobacteriaceae_ isolates for whole-genome sequencing. We then selected 23 isolates from samples collected in the United States during January 2010–June 2013 from patients in Barnes Jewish Hospital in St. Louis, Missouri, that had similar proportions of β-lactam susceptibility and resistance to the isolates collected in Pakistan for sequencing. All isolates were de-identified and retrieved from existing strain banks. The combined set included 33 _E. coli_ , 30 _K. pneumoniae_ , 9 _Enterobacter cloacae_ , and 6 _Enterobacter aerogenes_ ( Technical Appendix Table 1). We extracted plasmid DNA from 9 isolates encoding NDM-1, 11 isolates encoding KPC, and 3 isolates encoding CTX-M-15 and performed shotgun sequencing on those plasmid preparations. Detailed methods are described in the Technical Appendix .\n\n【5】Figure 1\n\n【6】Figure 1 . Distribution of antimicrobial drug resistance genotypes of KPC and NDM-1 genes in related _Enterobacteriaceae_ strains and plasmids in Pakistan and the United States. Phylogenetic trees have been annotated with the specific...\n\n【7】Using antibiotic resistance gene predictions from the Resfams database ( _13_ ) and core genome alignment, we constructed a phylogenetic tree for each species in our set, overlaid by the β-lactamases encoded by each isolate ( Figure 1 ). Isolates from both locations were found to be members of the same subspecies clades ( Technical Appendix Figure 1) and to contain similar repertoires of β-lactamases ( Figure 1 ), indicating that geography is not a discriminating variable for these isolates. Many of these isolates were also MDR: resistance to ciprofloxacin, trimethoprim/sulfamethoxazole, gentamicin, doxycycline, and chloramphenicol occurred in 63%, 65%, 45%, 54%, and 56% of isolates, respectively. As expected from results of previous work ( _8_ ), _E. coli_ ST131 isolates had high rates of CTX-M carriage (82%; Figure 1 , panel A) and ciprofloxacin resistance (100%).\n\n【8】The variety of strains that we discovered encoding KPC and NDM-1 is consistent with existing evidence that HGT is a major factor in their spread. All KPC genes were proximal to Tn4401 and all NDM-1 genes were carried on ISAba125, mobile elements with which each gene has respectively been previously associated ( _14_ ). We observed multiple examples of NDM-1 within the _K. pneumoniae_ ST11 clade ( _15_ ) ( Figure 1 , panel B; Technical Appendix Figure 1, panel B), a close relative of ST258. This association could be caused by clonal expansion or multiple HGT events and emphasizes that lineages known to encode KPC are now also acquiring NDM-1. We also observed high rates of NDM-1 carriage in _Enterobacter_ isolates ( Figure 1 , panels C and D), which in general showed a high number (maximum 8) and wide variety of β-lactamases. These isolates were also MDR: 57% of the _Enterobacter_ isolates were resistant to all or all but 1 of the antimicrobial drugs tested. At best, these _Enterobacter_ strains are a reservoir for resistance in Pakistan; at worst, they are the vanguard of an expansion of carbapenem-resistant _Enterobacter_ infections.\n\n【9】Figure 2\n\n【10】Figure 2 . Pairwise BLAST identity ( http://blast.ncbi.nlm.nih.gov/Blast.cgi ) of all CTX-M genes, _Klebsiella pneumoniae_ carbapenemase (KPC), and New Delhi metallo-β-Lacatamase-1 (NDM-1) plasmids from isolates collected in Pakistan and the United States plasmid preparations, and...\n\n【11】Previous observations have predominantly found KPC and NDM-1 to be expressed from plasmids ( _6_ , _11_ ). To characterize the sequence similarity of plasmids within the NDM- and KPC-carrying plasmid populations, we purified and sequenced plasmid DNA from 9 isolates encoding NDM-1, 11 encoding KPC, and 3 encoding CTX-M-15. Sequencing showed that these plasmids include representatives from IncHI2, IncY, IncN, IncFIA, IncFIB, IncFIC, and IncI1 incompatibility groups. Using reciprocal BLAST ( http://blast.ncbi.nlm.nih.gov/Blast.cgi ) alignment between each pair of plasmid preparations, we calculated the percentage of each plasmid shared using a 99% identity threshold. We performed this same analysis for all sequenced plasmids containing NDM-1, KPC, or CTX-M available in the National Center for Biotechnology Information database ( http://www.ncbi.nlm.nih.gov ) together with our set ( Figure 2 ) and separately ( Technical Appendix Figure 2). Certain components, primarily mobile elements, were abundant within these plasmids: the average plasmid shared 500 contiguous bases with 58 of the other plasmids; however, median BLAST identity for this pairwise comparison was <12%, even when considering plasmids with the same β-lactamase, suggesting that both carbapenemases exist within a variety of plasmid configurations.\n\n【12】To visualize this comparison of carbapenemase plasmids, we generated a network diagram in which each node represented a plasmid and each line represented shared sequence between 2 plasmids ( Figure 2 , panel B). Node size and line width correlate to the number of nucleotides contained in the plasmid or sharing interaction. This visualization shows the abundant small, shared regions that exist between most plasmid pairs, represented as thin background lines. This visualization also highlights the larger shared regions that indicate highly similar plasmids, represented by the few wide lines. These outliers were often between pairs of plasmids encoding the same β-lactamase but were also observed between NDM-1 and KPC containing plasmids (maximum 79% of smaller plasmid length).\n\n【13】### Conclusions\n\n【14】Together, this evidence supports our hypothesis that strains and plasmids known to carry either carbapenemase also have access to the other. Given the similarity of carbapenemase-negative strains to those carrying KPC or NDM-1 and the high diversity of plasmids in which they can be found, we anticipate that global carbapenem usage will encourage HGT of both of these carbapenemases into additional strain and plasmid backgrounds. Because KPC and NDM-1 are poised to cross genetic and geographic boundaries, we recommend that hospitals routinely screen _Enterobacteriaceae_ strains for both genes, even in regions where they are not yet endemic. We further advocate reduced carbapenem use to limit the selection for resistance against this vital antibiotic class.\n\n【15】Dr. Pesesky received his PhD from Washington University in St. Louis, Missouri, USA, in 2015 and is currently a postdoctoral fellow at the University of Washington, Seattle, Washington, USA. His research focuses on molecular and genomic investigations of high interest functions in bacteria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6359d027-0456-41ec-9a60-f667f52eec2e", "title": "West Nile Virus Epidemic, Northeast Ohio, 2002", "text": "【0】West Nile Virus Epidemic, Northeast Ohio, 2002\nSince its 1999 North American introduction, West Nile virus (WNV) has emerged as an important cause of illness and death. Although several at-risk populations have been identified, older age remains the major risk factor for developing encephalitis after infection ( _1_ _–_ _4_ ).\n\n【1】WNV rapidly spread across the United States, resulting in intense epidemic activity in Louisiana, Illinois, Michigan, and Ohio in 2002; Colorado in 2003; and Arizona and California in 2004 ( _5_ _,_ _6_ ). In Ohio, WNV infections were first recognized in animals in 2001. In 2002, Ohio reported 341 human cases of WNV encephalitis or meningitis (West Nile neuroinvasive disease \\[WNND\\], incidence: 28 cases/million population) with 31 deaths. In 2002, Cleveland and surrounding Cuyahoga County (2000 population 1,393,978 of whom 1,302,982 were >5 years of age) reported 221 laboratory-confirmed cases of WNV illness, including 155 WNND cases (111 cases/million population) with 11 deaths from July 30 to October 3. All reported WNND patients (median age 61 years, range 11–98 years) were hospitalized (CDC ArboNET Surveillance Network, unpub. data).\n\n【2】Since most WNV infections are asymptomatic ( _7_ _,_ _8_ ), the true rate of WNV infection can best be estimated by measuring the prevalence of WNV-specific antibody in a recently exposed population. In December 2002, the Cuyahoga County public health community conducted a household-based seroprevalence survey to estimate neighborhood and countywide WNV infection rates.\n\n【3】### The Study\n\n【4】The survey was conducted December 5–12, 2002. Stratified multistage cluster sampling was used to estimate countywide and subpopulation prevalence rates. The county was divided into 3 risk strata ( Table 1 ). Census tracts were sampled within strata with probability proportional to population. Within each census tract, clusters of ≈50 households were formed. At random points, residents were approached for recruitment until 10 participating households were enrolled from each cluster.\n\n【5】Residents >5 years of age who had lived in the household since July 1, 2002, were asked to participate by providing a blood sample and responding to a questionnaire. One person from each household completed a questionnaire about the home environment. Questionnaires developed by the Centers for Disease Control and Prevention (CDC) were used ( _10_ ). Informed consent was obtained from all participants or their legal guardian. Assent was obtained from minors >8 years of age. Residents were offered a US $10 gift certificate and test results as compensation. Persons who were pregnant, mentally handicapped, or taking anticoagulants were not enrolled. Institutional review board approval was obtained from University Hospitals of Cleveland.\n\n【6】Serum samples were screened with a WNV-specific immunoglobulin M (IgM) antibody-capture (MAC) enzyme-linked immunosorbent assay (ELISA) ( 11 ) and indirect IgG ELISA at Focus Laboratories (Cypress, CA, USA). Positive IgM and IgG were defined as an antibody index >2.0 and >0.9, respectively. All IgM- and IgG-positive samples were sent to the Viral and Rickettsial Laboratory, California Department of Health Services (Richmond, CA, USA) for confirmatory plaque reduction neutralization tests to identify WNV and St. Louis encephalitis virus (SLEV)–specific neutralizing antibody. At the second laboratory, WNV MAC-ELISAs ( _12_ ) were repeated and IgG ELISAs for WNV, SLEV, and dengue were performed ( _13_ ). Laboratory-based case definitions were developed ( Table 2 ).\n\n【7】SPSS version 11.5 (SPSS Inc, Chicago, IL, USA) and SUDAAN version 8.0 (Research Triangle Institute, Research Triangle Park, NC, USA) were used for preliminary analyses and to assess differences in demographics, behavior, and clinical characteristics between seropositive and seronegative persons. Since SUDAAN variance estimation did not accommodate our complex sample design, we developed formulas to provide better estimates of variance and confidence intervals (CIs) using an α = 0.05. Unless noted, all analyses were weighted. Individual weights were derived by taking the inverse of the probability of selection.\n\n【8】The standard Horvitz-Thompson estimator was used for point estimation ( _14_ ). For variance estimates, all sources of variation that resulted from the selection process were included by using standard Taylor series approximations. To calculate the confidence interval for the true prevalence ratio (PR), we approximated the variance of the logarithm of the sample PR by using standard Taylor series method. The end points of this interval were exponentiated to obtain the interval for PR.\n\n【9】### Conclusions\n\n【10】Participants were recruited from 13 Cuyahoga County municipalities and 9 Cleveland neighborhoods. Of 4,676 households visited, 2,318 households had an eligible adult present; of these eligible households, 819 households (35.3%) agreed to participate. Of 1,747 eligible residents in 819 households, 1,251 (71.6%) consented to participate; 42 participants in 13 households had insufficient serum samples and were excluded. The study sample consisted of 1,209 participants from 806 households; they had a mean age of 43.2 years (range 5–94 years) and included 168 (12.4%) children 5–17 years of age. Compared to 2000 Cuyahoga County census demographics, our study sample contained a significantly larger proportion of adults 18–64 years of age (75.7% vs. 63.5%), female participants (57.8% vs. 52.8%), and African Americans (31.8% vs. 27.4%).\n\n【11】Initial screening identified WNV IgM and IgG antibody in 4 serum samples, IgG only in 90 serum samples, and IgM only in 2 specimens. Based on criteria listed in the Table 2 , confirmatory testing of the 96 samples identified 27 confirmed and 7 probable WNV-infected persons. The countywide seroprevalence rate was 1.9% (95% CI 0.8–4.6) ( Table 1 ), which suggests that 10,400–59,900 residents were infected. Based on 155 WNND cases reported from Cuyahoga County, ≈1 WNND case occurred for every 160 infected persons (95% CI 1:67–1:386).\n\n【12】Figure\n\n【13】Figure . Comparison of age-stratified seroprevalence rates (gray bars) to the age-stratified incidence of West Nile neuroinvasive disease (WNND) (black line). Seroprevalence rates were measured in the 2002 seroprevalence study. The incidence of...\n\n【14】Seroprevalence varied significantly between age groups (p<0.05) ( Table 1 ). Based on reported WNND cases and age-stratified seroprevalence rates, we estimate that 1 case of WNND occurred per 4,167 infected children 5–17 years of age, per 154 infected adults 18–64 years of age, and per 38 infected persons >65 years of age ( Figure ). Strata-specific seroprevalence values ranged from 1.5% to 3.3% but were not statistically different ( Table 1 ).\n\n【15】In 2002, Cuyahoga County experienced its largest epidemic of arboviral encephalitis and meningitis, yet only 1.9% of the county's population became infected during this first WNV transmission season. In the 733-km <sup>2 </sup> area of Cuyahoga County, 155 cases of encephalitis and meningitis (WNND incidence: 111 cases/million population) occurred; the seroprevalence was 1.9% countywide and 2.5% in the selected highest risk survey stratum.\n\n【16】Little is known about WNV infection rates in children ( _15_ ). In contrast to a previous study ( _8_ ), our study demonstrated an age-dependent risk for WNV infection. The antibody prevalence in the 5- to 17-year age group was significantly greater than in older age groups. These data suggest that children were 4.5 times more likely to be infected than older persons. In this study, children reported spending more time outdoors and using less personal protective measures, which likely contributed to their higher seroprevalence rate. In 2002, only 4 cases of WNND were reported in the 5- to 17-year age group, resulting in a WNND:infection ratio of 1:4,200 compared to a 1:38 ratio among persons >65 years of age. Thus, the risk for WNND after infection may be as much as 110× greater in adults >65 years of age, as compared to children. Inclusion of a larger number of children in this study compared to previous studies allowed these age-stratified analyses to be completed.\n\n【17】Although WNV seroprevalence was similar to those measured in previous outbreaks (7,8), our study was the first to demonstrate that the risk for WNV infection can be age-dependent. Children had a higher rate of infection than adults, but serious neurologic disease developed in few of them. This finding has implications for public health practice and emphasizes the need for children to use protective measures to prevent mosquito bites to further lower their risk for infection with WNV and other mosquitoborne viruses.\n\n【18】Dr Mandalakas is an assistant professor of pediatrics and global health at Rainbow Babies and Children's Hospital, Case Western Reserve University, and medical director of the Cuyahoga County Board of Health. Her research interests include infectious disease epidemiology and childhood tuberculosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1156b42d-4243-4086-b1c3-e08748507258", "title": "Clostridium difficile Infection in Outpatients, Maryland and Connecticut, USA, 2002–2007", "text": "【0】Clostridium difficile Infection in Outpatients, Maryland and Connecticut, USA, 2002–2007\nIn the United States, ≈375 million episodes of acute diarrhea occur annually ( _1_ ). Among hospitalized persons, toxin-producing _Clostridium difficile_ is a primary diarrheagenic pathogen, usually as a consequence of normal bowel flora distortion caused by antimicrobial drug therapy ( _2_ , _3_ ). _C. difficile_ infection (CDI) complicates and prolongs hospital stays, leading to increases in health care costs, illness, and death. Recent reports suggest increases in community-onset CDI among persons without recent antimicrobial drug treatment or hospitalization. We describe a prospective evaluation of CDI in persons with diarrhea who visited emergency departments (EDs) and ambulatory primary care clinics in Baltimore, Maryland, and New Haven, Connecticut, and identify microbiologic causes and epidemiologic characteristics of diarrhea. This report highlights cases of outpatient CDI, identifies factors associated with infection, and describes molecular strain characterization.\n\n【1】Patients seeking medical attention for community-onset diarrheal illnesses were enrolled from May 2002 through September 2004 in the EDs and ambulatory clinics at Yale–New Haven Hospital (New Haven, CT, USA) and from May 2002 through July 2007 at EDs and clinics affiliated with the University of Maryland (Baltimore, MD, USA) ( _4_ ).\n\n【2】Informed consent for stool sample collection, initial and follow-up patient interviews, and medical records review was obtained from primarily urban and suburban residents, or parents/guardians for minors, who sought treatment for self-identified primary or secondary diarrhea. This research was approved by the institutional review boards at all participating institutions.\n\n【3】Participants were interviewed at outpatient clinics to assess health status, symptoms, and potential exposures to enteric pathogens, and at follow-up to determine the duration of diarrhea, whether treatment was administered, or whether hospitalization resulted from the initial visit. Stool samples collected during the visit or provided within 48 h and kept cool were homogenized and transferred into multiple vials for storage at –80°C.\n\n【4】An outpatient CDI case was defined as in outpatient with diarrhea whose stool was positive for _C. difficile_ toxins by enzyme immunoassay (TOX A/B II ELISA; TechLab, Blacksburg, VA, USA). Presumptive non–health care–associated (NHA) CDI was defined by the absence of an overnight stay at an inpatient healthcare facility over the previous month.\n\n【5】Traditional risk factors for CDI that were investigated included antimicrobial drug use within the past month, age \\> 65 years, serious underlying illness/weakened immune system, history of bowel or ulcer surgery, colon disease, previous CDI, and recent hospitalization. Statistical analysis was done by using SAS version 9.2 (SAS Institute, Inc., Cary, NC, USA). All p values reported are 2-sided, with no correction for multiple comparisons; p<0.05 was considered significant.\n\n【6】_C. difficile_ toxin–positive stool specimens, in 1-mL aliquots, were shipped frozen to the Centers for Disease Control and Prevention (Atlanta, GA, USA) anaerobe laboratory for culturing by direct inoculation onto cycloserine cefoxitin fructose agar (CCFA) or ethanol shock, followed by CCFA inoculation. Cultures were incubated for 48–72 h at 35°C under anaerobic conditions and examined for characteristic yellow-green fluorescence under long-wave ultraviolet light and CCFA _p_ \\-cresol odor. _C. difficile_ colonies were confirmed with indole (negative) and PRO disk (positive; Remel, Lenexa, KS, USA) tests.\n\n【7】Pulsed-field gel electrophoresis was performed on _C. difficile_ genomic DNA digested with _Sma_ I, and toxinotyping was performed ( _5_ ). Binary toxin was assayed by PCR for _cdtB_ ( _6_ ). Deletions in _tcdC_ were detected ( _7_ ).\n\n【8】_C. difficile_ toxin tests were performed on 1,091/1,197 stool specimens; 43 (3.9%) of these case-patients met the case definition for outpatient CDI. The mean age of these case-patients was 43.7 years (range 4 months–88 years). Outpatient CDI case-patients were younger at Yale because a significantly greater proportion of toxin-positive children were recruited at Yale (45.5%) than at the University of Maryland (15.6%) (p = 0.04). The 43 outpatient CDI case-patients included 5 infants <1 year of age, 5 children 1–18 years of age, 23 adults 19–64 years of age, and 10 adults \\> 65 years of age; 21 were Caucasian, 18 were African American, and 4 were of other or unknown race/ethnicity (22 male and 21 female case-patients).\n\n【9】Most case-patients (36/43, 83.7%) had a recognized underlying risk factor. Twenty-seven (62.8%) had received systemic antimicrobial drugs, including ciprofloxacin, gaitifloxacin, amoxicillin, ampicillin/sulbactam, piperacillin/tazobactam, cefpodoxime, vancomycin, clindamycin, metronidazole, erythromycin, or trimethoprim/sulfamethoxazole within the preceding month; 14 (32.6%) had been hospitalized; and 15 (34.9%) had chronic illnesses or had undergone bowel surgery that potentially affect immune status or gastrointestinal function ( Table 1 ). Two persons, 1 with AIDS and 1 who underwent a previous bowel resection for diverticulitis, had been treated in the past month for CDI. Only 7 (16.3%) patients had NHA-CDI infections without identified risk factors; 3 were infants (<1 year), 1 was a child (1–18 years), 3 were adults (19–64 years), and none were elderly ( \\> 65 years) ( Table 2 ).\n\n【10】The 43 outpatient CDI case-patients were compared with the other 1,048 persons in which _C. difficile_ toxin had not been detected. Persons with CDI were, on average, significantly older than others with diarrhea, 43.7 years vs. 29.2 years, respectively (p<0.01). Outpatient CDI case-patients were more likely than _C. difficile_ –negative patients to have medical or surgical conditions (34.9% vs.16.8%, p<0.001), been recently hospitalized (32.6% vs. 8.8%, p<0.001), or to have used antimicrobial drugs (62.8% vs. 22.0%, p<0.001).\n\n【11】Co-infections with other enteric pathogens were common among CDI case-patients, including _C. perfringens_ (3), rotavirus (5), norovirus (3), sapovirus (2), and 1 each with hookworm, _Bacillus cereus_ , astrovirus, and adenovirus. The likelihood of co-infection was similar in patients with (12/36 \\[33.3%\\]) and without (3/7 \\[42.9%\\]) risk factors (p>0.1).\n\n【12】Figure\n\n【13】Figure . Characteristics of isolates obtained from patients with _Clostridium difficile_ infection, Maryland and Connecticut, USA, May 2002–July 2007. PFGE, pulsed-field gel electrophoresis.\n\n【14】Of 43 _C. difficile_ toxin–positive stools initially tested, 39 stool samples were submitted to the Centers for Disease Control and Prevention for anaerobic culture and _C. difficile_ was isolated from 31 samples. Binary toxin was identified in 12 (38.7%). Pulsed-field gel electrophoresis identified 15 different types ( Figure ). No associations were found between risk factors, including age, and strain or toxinotype (data not shown).\n\n【15】NHA-CDI has been recognized for >12 years, and recent reports suggest that disease occurs without patient’s known exposure to antimicrobial drugs or other previously identified risk factors ( _2_ , _8_ – _13_ ). Although we found a proportion of _C. difficile_ –positive diarrheal stools similar to that of 2 other recent prospective studies that used confirmatory culture (i.e., 1.5%–3.9%) for outpatient CDI ( _7_ , _13_ , _14_ ), we also found a lower proportion of outpatient CDI cases without recognized risk factors of recent hospitalization, chronic medical conditions, recent antimicrobial drug exposure, or co-infection than did those studies.\n\n【16】One limitation of our study was using retrospective self-reporting for assessment of hospitalizations or antimicrobial drug use in the previous month, which potentially can result in recall bias. Also, antimicrobial drug therapy was assessed for only 4 weeks before diarrhea onset; exposure to antimicrobial drugs for a period longer than 1 month before patient seeks treatment may present a risk for CDI. In addition, this study was conducted at 2 urban centers in the eastern United States and may not be generalizable to other locations or clinical settings. Finally, although enzyme immunoassay detection for _C. difficile_ was the standard of care at the time of the study, it is now considered too insensitive to be used as a stand-alone diagnostic test ( _15_ ).\n\n【17】In summary, we detected toxigenic _C. difficile_ in a similar proportion of patients to those reported in other studies of CDI. However, all but 3 patients had either known risk factors for CDI or other pathogens potentially responsible for their illness; 1 was <1 year of age. _C. difficile_ isolates responsible for outpatient CDI are genetically diverse. An evolving picture of widespread, frequent CDI among outpatients without risk factors should be tempered by these findings.\n\n【18】Dr Hirshon is an associate professor in the Departments of Emergency Medicine and Epidemiology and Public Health at the University of Maryland School of Medicine. His research interests include developing emergency departments as sites for surveillance and hypothesis- driven research in public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "815e63ef-ae58-4630-8941-e7d05edf5525", "title": "Factors Predicting Glycemic Control in Middle-Aged and Older Adults With Type 2 Diabetes", "text": "|  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n\n| \n<table><tbody><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>View Current Issue</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>Issue Archive</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><span><a>Archivo de nmeros en espaol</a></span></th></tr></tbody></table></th></tr><tr><th><img></th></tr></tbody></table>\n\n【2】<table><tbody><tr><th><img><br></th></tr><tr><th><form><label>Search <i>PCD</i></label><br><img><br><input> <input> <input><br><img><br><input><br><img><br></form></th></tr></tbody></table><table><tbody><tr><th><b><a>Emerging Infectious Diseases Journal</a></b></th></tr><tr><th><b><a>MMWR</a></b></th></tr></tbody></table>\n\n |  | <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Volume 7: No. 1, January 2010 \n\n【4】ORIGINAL RESEARCHFactors Predicting Glycemic Control in Middle-Aged and Older Adults With Type 2 Diabetes\n=========================================================================================================\n\n【5】<table><tbody><tr><th><img><br></th><th><table><tbody><tr><th><a>TABLE OF CONTENTS</a></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><img></th><th><span><a>Este resumen en espaol</a></span></th></tr><tr><th><img></th><th><a>Print this article</a></th></tr><tr><th><img></th><th>E-mail this article:<br><img><br><input><br><img><br><input> </th></tr><tr><th><img></th><th><a>Send feedback to editors</a></th></tr><tr><th><img></th><th><a>Download this article as a PDF </a>(542K)</th></tr><tr><th><hr>You will need <a>Adobe Acrobat Reader </a>to view PDF files.</th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><b>Navigate This Article</b></th></tr><tr><th></th><th><a>Abstract</a></th></tr><tr><th></th><th><a>Introduction</a></th></tr><tr><th></th><th><a>Methods</a></th></tr><tr><th></th><th><a>Results</a></th></tr><tr><th></th><th><a>Discussion</a></th></tr><tr><th></th><th><a>Acknowledgments</a></th></tr><tr><th></th><th><a>Author Information</a></th></tr><tr><th></th><th><a>References</a></th></tr><tr><th></th><th><a>Tables</a></th></tr><tr><th><img><br></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr></tbody></table>\n\n【6】#### Ching-Ju Chiu, PhD; Linda A. Wray, PhD\n\n【7】_Suggested citation for this article:_ Chiu C-J, Wray LA. Factors predicting glycemic control in middle-aged and older adults with type 2 diabetes. Prev Chronic Dis 2010;7(1):A08. http://www.cdc.gov/pcd/issues/2010/jan/08\\_0250.htm Accessed \\[ _date_ \\]. PEER REVIEWED \n\n【8】Abstract\n--------\n\n【9】**Introduction**Few studies have prospectively assessed the explanatory effects of demographics, clinical conditions, treatment modality, and general lifestyle behaviors on glycemic control in large heterogeneous samples of middle-aged and older adults with type 2 diabetes. We hierarchically examined these factors, focused especially on the effects of modifiable factors (ie, general lifestyle behaviors), and compared predictive patterns between middle-aged and older adults. **Methods**We used nationally representative data from the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study. We analyzed data from 379 middle-aged adults (aged 51-64 y) and 430 older adults (aged ≥65 y) who self-reported having type 2 diabetes at baseline. **Results**Among middle-aged adults, demographic factors and clinical conditions were the strongest predictors of hemoglobin A1c (HbA1c) levels. However, among older adults, treatment modality (diet only, oral medication, or insulin only or in combination with other regimens) significantly affected HbA1c levels. Lifestyle (physical activity, smoking, drinking, and body weight control), independent of the effects of demographics, clinical conditions, and treatment modality, significantly affected HbA1c levels. An increase of 1 healthy behavior was associated with a decrease in HbA1c levels of more than 1 percentage point. **Conclusion**Our findings provide support for current diabetes guidelines that recommend a lifestyle regimen across the entire span of diabetes care and highlight the need to help both sociodemographically and clinically disadvantaged middle-aged adults with type 2 diabetes as well as older adults who exhibit poor adherence to medication recommendations to achieve better glycemic control. Back to top \n\n【10】Introduction\n------------\n\n【11】Type 2 diabetes affects a large number of middle-aged and older adults in the United States. More than 16 million adults in the United States and 200 million people worldwide have the disease (1). One of the primary goals of diabetes management is to lower blood glucose levels because it is well established that improved glycemic control delays the onset and retards the progression of microvascular and macrovascular complications (2). However, many US adults (40%-60%) with type 2 diabetes have poor glycemic control (3,4). Understanding the factors that contribute to glycemic control is key to developing more effective treatment and identifying the needs of adults with type 2 diabetes. Lifestyle behaviors are postulated to play a role in glycemic control, and their effects on glycemic control have been given increasing attention in the past decade. However, most studies in this area have focused only on the effects of diabetes-specific self-management behaviors on glycemic control (5-7). Furthermore, other studies that have demonstrated the effects of general health behaviors on hemoglobin A1c (HbA1c) levels have focused on a single lifestyle behavior, such as exercise or weight control (8,9). Although findings from these studies have been encouraging, most studies have examined changes in HbA1c levels among homogeneous samples within a short period after the strictly controlled intervention. The effectiveness of general lifestyle behaviors on long-term glycemic control among adults from heterogeneous backgrounds, with differing clinical conditions, and who use different forms of treatment remains uncertain. The influences of demographics, clinical conditions, and treatment adherence on glycemic control have also been documented. Previous studies have suggested that minority groups (eg, African Americans, Hispanics, American Indians, Pacific Islanders) (10,11) and adults who have had diabetes for a long time, who have comorbidities (12,13), or who use insulin or multiple oral agents have high HbA1c levels (12,14). However, longitudinal evidence regarding the explanatory effects of these variables on glycemic control from large heterogeneous national samples is sparse. Because these factors have not been examined comprehensively, whether clinical conditions explain the racial/ethnic disparities or whether the effect of clinical conditions on glycemic control differs among people from different demographic backgrounds is unclear. Furthermore, there has been little investigation of the predictive factors of glycemic control in midlife and older age. As suggested by cumulative advantage/disadvantage theory, age has a modifying effect on many explanatory factors in health outcomes (15). Therefore, we hypothesized that distinct predictive patterns of glycemic control exist between middle-aged and older adults. The effects of dietary therapy on glycemic control in clinical settings generally deteriorate over time (14). However, current literature does not answer the question of whether older adults are less sensitive to other modifiable factors, such as health behaviors. We prospectively examined the effects of general lifestyle behaviors (physical activity, smoking, drinking, and weight control) on HbA1c levels, controlling for documented correlates, including demographic characteristics (4,16,17), clinical conditions (12,13), and treatment modalities (14,18). We studied a nationally representative sample of middle-aged and older adults with type 2 diabetes. We demonstrate the complex explanatory effects of each factor on glycemic control by using a heterogeneous sample, longitudinal design, and hierarchical regression analysis. We tested the hierarchical models among middle-aged adults and older adults separately to examine whether distinct predictive patterns between middle-aged and older adults exist. Back to top \n\n【12】Methods\n-------\n\n【13】### Participants\n\n【14】Our study sample was 379 middle-aged adults (aged 51-64 y) and 430 older adults (aged ≥65 y) who 1) in the 1998 Health and Retirement Study (HRS) interview self-reported having diagnosed type 2 diabetes after age 34 and 2) returned valid blood spot tests for HbA1c in the 2003 HRS Diabetes Study. The HRS is an ongoing biennial survey that was initiated in 1992 to track the health status and retirement plans of community-dwelling middle-aged and older US adults and that oversamples Hispanic Americans and African Americans. Details about recruitment procedures and characteristics of the participants in the HRS interview are described elsewhere (19). The HRS 2003 Diabetes Study followed adults who self-reported diagnosed diabetes in the 2002 or earlier HRS interviews and collected data on various diabetes-related psychological, behavioral, and clinical measures. We weighted data to make the study sample representative of US middle-aged and older adults with type 2 diabetes who were born in 1947 or earlier (ie, aged 51 or older in 1998). \n\n【15】### Measures\n\n【16】Demographic characteristics (age, sex, race/ethnicity, education, and marital status) were obtained from 1998 HRS data. Age and education were measured as continuous variables. Race/ethnicity was categorized as non-Hispanic white, non-Hispanic black, and Hispanic/other. Marital status was measured as a dichotomous variable (married/partnered or other). Measures of clinical conditions, including the number of self-reported physical or psychological chronic conditions and the duration of diabetes, were obtained from 2000 HRS data. The number of physical or psychological chronic diseases was the sum of indicators for whether a participant had ever been informed by a doctor that he or she ever had high blood pressure, diabetes, cancer, lung disease, heart disease, stroke, psychiatric problems, or arthritis. The duration of diabetes was calculated as the number of years between individuals’ diagnosis of diabetes and the year 2000. Information about the age of diagnosis of diabetes was obtained from the question, “At what age were you told by a doctor that you had diabetes?” Information about treatment modality was obtained by participants’ answers to 3 questions from the 2000 HRS: “Are you following a special diet?,” “In order to treat or control your diabetes, are you now taking medication that you swallow?,” and “Are you now using insulin shots or a pump?” Participants who answered yes to the diet question but did not report using oral medications or insulin were placed in the “diet only” treatment group, participants who reported using oral medications only or a combination of oral medication and diet but did not report using insulin were placed in the “oral medication” treatment group, and participants who reported using insulin only or in combination with other regimens were placed in the “insulin only or combination” treatment group. Self-reported information obtained in the 2000 HRS about lifestyle behaviors related to physical activity, substance use, and control of body weight were used. To report physical activity, participants were asked the yes/no question, “On average over the last 12 months have you participated in vigorous activity or exercise 3 times a week or more?” Substance use was assessed by asking, “Do you smoke cigarettes now?,” “Have you ever felt that you should cut down on drinking?,” “Have people ever annoyed you by criticizing your drinking?,” “Have you ever felt bad or guilty about drinking?,” or “Have you ever taken a drink first thing in the morning to steady your nerves or get rid of a hangover?” Participants who answered no to all questions were defined as “no substance use”; all others were defined as “substance use.” Body mass index (BMI) was used to determine control of body weight. Participants were categorized as either having body weight control (BMI <30 kg/m 2 ) or not (BMI ≥30 kg/m 2 ), on the basis of National Heart, Lung, and Blood Institute guidelines (20). We recoded the responses to the above behaviors into 3 categories for lifestyle: poor (0 positive health behaviors), fair (1 positive health behavior), and good (≥2 positive health behaviors). Glycemic control was determined by HbA1c levels. This biomarker was available through blood spot assays collected in the HRS 2003 Diabetes Study. \n\n【17】### Data analysis\n\n【18】SAS version 9.1 (SAS Institute, Inc, Cary, North Carolina) was used to analyze the data. To characterize the sample across major study variables and to test their association with HbA1c levels, we performed _t_ tests, analysis of variance (ANOVA), and simple regression. Hierarchical multiple regression analyses were conducted to answer the study’s specific research questions. Significance was set at _P_ < .05. In model 1, only demographic variables (age and race/ethnicity) were entered as explanatory variables; in model 2, we added clinical condition variables (number of chronic diseases, duration of diabetes); in model 3, we added treatment modality variables (diet only, oral medication, or insulin use); and in model 4, we added a lifestyle variable (fair, good, or poor). Significance tests for _R_ 2 increment were calculated to provide information on how well the sequential variables accounted for the variance in HbA1c levels beyond that accounted for by variables in the preceding model. To explore possible differences in the predictive patterns between middle-aged and older adults, the models were tested for the entire sample and separately for the middle-aged and older adult samples. Back to top \n\n【19】Results\n-------\n\n【20】### Sample characteristics and bivariate analysis with HbA1c levels\n\n【21】Glycemic control was significantly associated with age, race/ethnicity, number of chronic diseases, duration of diabetes, treatment modality, and lifestyle ( Table 1 ). Age was negatively correlated with HbA1c values. Participants who were non-Hispanic white, reported fewer chronic diseases, reported a shorter duration of diabetes, reported using diet-only treatment, or reported having good lifestyle behaviors had lower HbA1c levels. Sex, education, and marital status were not significantly associated with HbA1c levels and were excluded from the hierarchical regression analyses. \n\n【22】### Effects of demographics, clinical conditions, treatment modality, and lifestyle on glycemic control\n\n【23】Model 1, which included only age and race/ethnicity as variables, explained 4.8% of the variance in HbA1c levels ( Table 2 ). Age was negatively associated with HbA1c levels, and non-Hispanic black and Hispanic participants had higher HbA1c levels than non-Hispanic white participants. Model 2, which added duration of diabetes and the number of chronic diseases as variables, explained 7.6% of the variance in HbA1c levels. A significance test of _R_ 2 increment from model 1 to model 2 suggests that the clinical conditions, over and above the demographic determinants, explained a significant proportion of the variance in HbA1c levels. Participants who reported having more chronic diseases or a longer duration of diabetes had higher HbA1c levels than participants who reported having fewer chronic diseases or a shorter duration of diabetes, independent of the demographic determinants. Model 3, which added treatment modality as a variable, explained 14.1% of the variance in HbA1c levels. The change in _R_ 2 from model 2 to model 3 was significant, suggesting that treatment modality has a substantial effect in predicting HbA1c levels, independent of demographic and clinical characteristics. Compared with participants who used insulin only or in combination with other regimens, participants who were treated with diet only or oral medications had lower HbA1c levels. Although demographic characteristics, clinical conditions, and treatment modality explained an appreciable amount of the variance in HbA1c levels, an additional 2.1% of the variance was explained independently and significantly by lifestyle behaviors. An increase of 1 healthy behavior was associated with a decrease in HbA1c levels of more than 1 percentage point. \n\n【24】### Comparison of predictive patterns in middle-aged and older adults\n\n【25】In middle-aged adults, our selected predictors explained 21.3% of the total variance in HbA1c levels ( Table 3 ). Demographics and clinical conditions accounted for the largest explained variance. Treatment modality also explained a significant proportion of the variance in HbA1c levels, as did lifestyle, which was added to model 4. The result suggests that, among middle-aged adults with type 2 diabetes, lifestyle behaviors are strongly associated with HbA1c levels, independent of the effects of demographic background, clinical conditions, and treatment modality. In the older adult group, our selected predictors explained 10.2% of the variance in HbA1c levels, approximately half that in the middle-aged adult sample ( Table 4 ). Demographics, clinical conditions, and lifestyle variables did not significantly predict HbA1c levels in older adults, but treatment modality did. The patterns of the 4 predictors on glycemic control in the entire sample, as well as in middle-aged adults and older adults separately, are presented in the Figure.  **Figure.** Effects of demographic factors, clinical conditions, treatment modality, and lifestyle on HbA1c levels among middle-aged adults (n = 379), older adults (n = 430), and the entire sample (N = 809), the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study. \\[A tabular version of this figure is also available.\\] Back to top \n\n【26】Discussion\n----------\n\n【27】Our results suggest that distinct predictive patterns of glycemic control exist for middle-aged and older adults and confirm the long-term beneficial link between general lifestyle behaviors and HbA1c levels, especially in middle-aged adults. The results also highlight the crucial role of medication treatment in older adults with type 2 diabetes. The result that nonwhite adults with type 2 diabetes were less likely to achieve adequate glycemic control than were their white counterparts is in accord with other studies (4,10,11,21) and confirms the previous finding that mechanisms beyond our model covariates of clinical conditions, treatment modality, and general lifestyle underlie racial/ethnic disparities in glycemic outcomes. Our finding that younger age was associated with worse glycemic control is congruent with a previous study (17) and echoes other studies that suggest that early-onset type 2 diabetes is associated with worse glycemic control outcomes than is later-onset type 2 diabetes (22). Our finding that the number of chronic diseases and duration of diabetes, independent of demographics, predicted glycemic control is consistent with at least 1 study (12). Although our results showed that diet-only therapy was associated with lower HbA1c levels and that using insulin only or in combination with other regimens is associated with higher HbA1c levels, these relationships more likely represent a marker of diabetes severity than of medication effects themselves. Of particular interest is the fact that demographic factors, clinical conditions and treatment modality explained a substantial proportion of the variance in HbA1c levels and that lifestyle behaviors independently explained 2.1% of the variance in HbA1c levels in the entire sample. The effect did not vary across sex and racial/ethnic groups, diabetes duration, number of chronic conditions, and diabetes treatment modality. The finding that an increase of 1 healthy behavior is associated with a decrease in HbA1c levels of more than 1 percentage point is not only significant but also clinically relevant. Comparison of the predictive patterns between middle-aged adults and older adults reveals that in midlife, demographic factors and clinical conditions are the most substantial predictors for HbA1c levels. This finding has practical implications for diabetes care. Early interventions may be needed to help socioeconomically and clinically disadvantaged middle-aged adults with type 2 diabetes to achieve satisfactory glycemic control. Conversely, our finding that glycemic control in older age is especially sensitive to treatment modality suggests that diabetes care should also focus on older adults who exhibit poor adherence to medication recommendations. Furthermore, the lower explanatory power of our model among older adults compared with that among middle-aged adults implies that in older age, glycemic control may be affected by more complex factors than during midlife. This finding supports the increasing body of literature advocating special considerations for older adults with diabetes (23-26). Our study has several limitations. First, although previous studies have found that the validity of self-reported exercise behavior is high among older adults (27), the validity of self-report measures of substance use and weight control among older adults remains uncertain. Rates of performing healthy behaviors may have been overestimated because of social desirability bias (28,29). Furthermore, past research with the HRS has shown that, on average, survey respondents are healthier than nonrespondents (30); therefore, the relationship between modifiable predictors and glycemic control may appear stronger than it actually is. Second, although our prospective design allowed us to investigate predictors of HbA1c levels, the lack of baseline HbA1c data limited our ability to infer causal effects of our predictors of glycemic control. Third, we may have underestimated the explanatory effects of general lifestyle behaviors because other general lifestyle factors, such as psychologically related behaviors (eg, stress management, self-control) were not included. Fourth, the comparison of the predictive effects of some explanatory variables on glycemic control in the 2 age groups may be confounded with a “survival effect.” For example, less healthy members of minority groups may have died at earlier ages, leaving those surviving to age 65 or older as relatively healthier, thus potentially underestimating the effects of race/ethnicity on glycemic control in older age (30). Overall, our results suggest that general lifestyle behaviors have a beneficial effect on glycemic control, beyond effects accounted for by demographic factors, clinical conditions, and treatment modality, especially in middle-aged adults. Our findings provide support for current diabetes care guidelines that recommend a lifestyle regimen across the entire span of diabetes care and highlight the need to help sociodemographically or clinically disadvantaged middle-aged adults with type 2 diabetes and older adults who exhibit poor adherence to medication recommendations to achieve glycemic control. Back to top \n\n【28】Acknowledgments\n---------------\n\n【29】This research was supported by a level 2 grant from the Social Science Research Institute at The Pennsylvania State University for “Marriage and Diabetes Management in Middle-Aged and Older Couples” (L. A. Wray, principal investigator) and National Institute of Diabetes and Digestive and Kidney Diseases grant no. DK078894-01 (L. A. Wray, principal investigator). We also acknowledge the valuable comments provided by Warner K. Schaie and Jeffrey B. Halter on a symposium presentation of this study at the 60th Annual Meeting of the Gerontological Society of America, San Diego, California, November 2007. Back to top \n\n【30】Author Information\n------------------\n\n【31】Corresponding Author: Ching-Ju Chiu, PhD, Department of Biobehavioral Health, 315 HHD East, The Pennsylvania State University, University Park, PA 16802. Telephone: 814-865-0773. E-mail: cuc197@psu.edu . Author Affilation: Linda A. Wray, The Pennsylvania State University, University Park, Pennsylvania. Back to top \n\n【32】References\n----------\n\nZimmet P. The burden of type 2 diabetes: are we doing enough? Diabetes Metab 2003;29(4 Pt 2):6S9-18.UK Prospective Diabetes Study Group. Intensive blood-glucose control with sulphonylureas or insulin compared with conventional treatment and risk of complications in patients with type 2 diabetes (UKPDS 33). Lancet 1998;352(9131):837-53.van den Arend IJ, Stolk RP, Krans HM, Grobbee DE, Schrijvers AJ. Management of type 2 diabetes: a challenge for patient and physician. Patient Educ Couns 2000;40(2):187-94.Shorr RI, Franse LV, Resnick HE, Di Bari M, Johnson KC, Pahor M. Glycemic control of older adults with type 2 diabetes: findings from the Third National Health and Nutrition Examination Survey, 1988-1994. J Am Geriatr Soc 2000;48(3):264-7.Rothman RL, Mulvaney S, Elasy TA, VanderWoude A, Gebretsadik T, Shintani A, et al. Self-management behaviors, racial disparities, and glycemic control among adolescents with type 2 diabetes. Pediatrics 2008;121(4):e912-9.Egede LE, Michel Y. Medical mistrust, diabetes self-management, and glycemic control in an indigent population with type 2 diabetes. Diabetes Care 2006;29(1):131-2.Williams GC, McGregor H, Zeldman A, Freedman ZR, Deci EL, Elder D. Promoting glycemic control through diabetes self-management: evaluating a patient activation intervention. Patient Educ Couns 2005;56(1):28-34.Dunstan DW, Daly RM, Owen N, Jolley D, De Courten M, Shaw J, et al. High-intensity resistance training improves glycemic control in older patients with type 2 diabetes. Diabetes Care 2002;25(10):1729-36.Boule NG, Haddad E, Kenny GP, Wells GA, Sigal RJ. Effects of exercise on glycemic control and body mass in type 2 diabetes mellitus: a meta-analysis of controlled clinical trials. JAMA 2001;286(10):1218-27.Chou AF, Brown AF, Jensen RE, Shih S, Pawlson G, Scholle SH. Gender and racial disparities in the management of diabetes mellitus among Medicare patients. Womens Health Issues 2007;17(3):150-61.LeMaster JW, Chanetsa F, Kapp JM, Waterman BM. Racial disparities in diabetes-related preventive care: results from the Missouri Behavioral Risk Factor Surveillance System. Prev Chronic Dis 2006;3(3):A86.Benoit SR, Fleming R, Philis-Tsimikas A, Ji M. Predictors of glycemic control among patients with type 2 diabetes: a longitudinal study. BMC Public Health 2005;5:36.Blaum CS, Velez L, Hiss RG, Halter JB. Characteristics related to poor glycemic control in NIDDM patients in community practice. Diabetes Care 1997;20(1):7-11.Turner RC, Cull CA, Frighi V, Holman RR. Glycemic control with diet, sulfonylurea, metformin, or insulin in patients with type 2 diabetes mellitus: progressive requirement for multiple therapies (UKPDS 49). UK Prospective Diabetes Study (UKPDS) Group. JAMA 1999;281(21):2005-12.Dannefer D. Cumulative advantage/disadvantage and the life course: cross-fertilizing age and social science theory. J Gerontol B Psychol Sci Soc Sci 2003;58(6):S327-37.Heisler M, Faul JD, Hayward RA, Langa KM, Blaum C, Weir D. Mechanisms for racial and ethnic disparities in glycemic control in middle-aged and older Americans in the health and retirement study. Arch Intern Med 2007;167(17):1853-60.Nichols GA, Hillier TA, Javor K, Brown JB. Predictors of glycemic control in insulin-using adults with type 2 diabetes. Diabetes Care 2000;23(3):273-7.Hensrud DD. Dietary treatment and long-term weight loss and maintenance in type 2 diabetes. Obes Res 2001;9 Suppl 4:348S-353S.Juster FT, Suzman R. An overview of the Health and Retirement Study. J Hum Resour 1995;30(Special Issue on the Health and Retirement Study: Data Quality and Early Results):S7-S56.Clinical guidelines on the identification, evaluation, and treatment of overweight and obesity in adults. Bethesda (MD): National Heart, Lung, and Blood Institute; 1998.Gary TL, McGuire M, McCauley J, Brancati FL. Racial comparisons of health care and glycemic control for African American and white diabetic adults in an urban managed care organization. Dis Manag 2004;7(1):25-34.Chuang LM, Soegondo S, Soewondo P, Young-Seol K, Mohamed M, Dalisay E, et al. Comparisons of the outcomes on control, type of management and complications status in early onset and late onset type 2 diabetes in Asia. Diabetes Res Clin Pract 2006;71(2):146-55.Hainer TA. Managing older adults with diabetes. J Am Acad Nurse Pract 2006;18(7):309-17.Brown AF, Mangione CM, Saliba D, Sarkisian CA; California Healthcare Foundation/American Geriatrics Society Panel on Improving Care for Elders With Diabetes. Guidelines for improving the care of the older person with diabetes mellitus. J Am Geriatr Soc 2003;51(5 Suppl Guidelines):S265-80.Blaum CS. Management of diabetes mellitus in older adults: are national guidelines appropriate? J Am Geriatr Soc 2002;50(3):581-3.Wray LA, Alwin DF, McCammon RJ, Manning T, Best LE. Social status, risky health behaviors, and diabetes in middle-aged and older adults. J Gerontol B Psychol Sci Soc Sci 2006;61(6):S290-8.Harada ND, Chiu V, King AC, Stewart AL. An evaluation of three self-report physical activity instruments for older adults. Med Sci Sports Exerc 2001;33(6):962-70.Adams SA, Matthews CE, Ebbeling CB, Moore CG, Cunningham JE, Fulton J, et al. The effect of social desirability and social approval on self-reports of physical activity. Am J Epidemiol 2005;161(4):389-98.Hébert JR, Peterson KE, Hurley TG, Stoddard AM, Cohen N, Field AE, et al. The effect of social desirability trait on self-reported dietary measures among multi-ethnic female health center employees. Ann Epidemiol 2001;11(6):417-27.Cao H, Hill DH. Active versus passive sample attrition: The Health and Retirement Study. Institute for Social Research. Ann Arbor (MI): University of Michigan; 2005.Back to top  |  |\n| --- | --- | --- | --- |\n\n|  |  | \nTables\n------\n\n【35】#####  Table 1. Sample Characteristics and Bivariate Association With HbA1c Levels, Middle-Aged (51-64 y) and Older Adults (≥65 y) With Type 2 Diabetes (N = 809), the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study\n\n【36】<table><tbody><tr><th>Characteristics <sup>a</sup></th><th>% <sup>b</sup></th><th>Mean HbA1c Level, <sup>c </sup>%</th><th><i>P </i>Value <sup>d</sup></th></tr><tr><th><b>Mean age (range, 51-89 y)</b></th><th>65.3</th><th>NA</th><th>&lt;.001</th></tr><tr><th><b>Sex</b></th></tr><tr><th>Male</th><th>50.0</th><th>7.27</th><th>.84</th></tr><tr><th>Female</th><th>50.0</th><th>7.29</th></tr><tr><th><b>Race/ethnicity</b></th></tr><tr><th>Non-Hispanic white</th><th>81.1</th><th>7.15</th><th>&lt;.001</th></tr><tr><th>Non-Hispanic black</th><th>10.8</th><th>7.82</th></tr><tr><th>Hispanic/other</th><th>8.2</th><th>7.72</th></tr><tr><th><b>Mean years of education (range, 0-17 y)</b></th><th>11.8</th><th>NA</th><th>.91</th></tr><tr><th><b>Marital status</b></th></tr><tr><th>Married/partnered</th><th>69.2</th><th>7.25</th><th>.41</th></tr><tr><th>Other</th><th>30.8</th><th>7.34</th></tr><tr><th><b>Mean no. of chronic diseases <sup>e</sup></b></th><th>3.0</th><th>NA</th><th>&lt;.001</th></tr><tr><th><b>Mean duration of diabetes (range, 1-42 y)</b></th><th>9.9</th><th>NA</th><th>&lt;.001</th></tr><tr><th><b>Treatment modality</b></th></tr><tr><th>Diet only</th><th>28.1</th><th>6.75</th><th>&lt;.001</th></tr><tr><th>Oral medication</th><th>50.9</th><th>7.33</th></tr><tr><th>Insulin only or combination</th><th>21.0</th><th>7.86</th></tr><tr><th><b>Lifestyle <sup>f</sup></b></th></tr><tr><th>Poor</th><th>3.2</th><th>8.54</th><th>&lt;.001</th></tr><tr><th>Fair</th><th>34.4</th><th>7.33</th></tr><tr><th>Good</th><th>62.3</th><th>7.18</th></tr></tbody></table>Abbreviations: HbA1c, hemoglobin A1c; NA, not applicable.a Data for mean age, sex, race/ethnicity, mean years of education, and marital status are from the 1998 HRS. Data for all other characteristics are from the 2000 HRS.b All values are percentages unless otherwise indicated.c Data from the 2003 Health and Retirement Study Diabetes Survey.d _P_ values calculated using _t_ test, analysis of variance (ANOVA), and simple regression; significance set at α = .05.e The number of chronic diseases was the sum of indicators for whether a participant had ever been informed by a doctor that he or she ever had high blood pressure, diabetes, cancer, lung disease, heart disease, stroke, psychiatric problems, or arthritis.f Poor = 0 positive health behaviors, fair = 1 positive health behavior, good ≥2 positive health behaviors. \n\n【37】#####  Table 2. Hierarchical Regression Analyses Predicting HbA1c Levels in Middle-Aged (51-64 y) and Older Adults (≥65 y) With Type 2 Diabetes (N = 809), the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study a\n\n【38】<table><tbody><tr><th>Variable</th><th>Model 1 <sup>b</sup></th><th>Model 2 <sup>c</sup></th><th>Model 3 <sup>d</sup></th><th>Model 4 <sup>e</sup></th></tr><tr><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th></tr><tr><th><b>Demographics</b></th></tr><tr><th>Age</th><th>−.02</th><th>&lt;.001</th><th>−.03</th><th>&lt;.001</th><th>−.02</th><th>&lt;.001</th><th>−.02</th><th>&lt;.001</th></tr><tr><th>Race/ethnicity</th></tr><tr><th>Non-Hispanic black</th><th>.67</th><th>&lt;.001</th><th>.57</th><th>&lt;.001</th><th>.42</th><th>.008</th><th>.45</th><th>.005</th></tr><tr><th>Hispanic</th><th>.56</th><th>.002</th><th>.55</th><th>.002</th><th>.46</th><th>.001</th><th>.50</th><th>.004</th></tr><tr><th>Non-Hispanic white</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Clinical Conditions</b></th></tr><tr><th>No. of chronic diseases</th><th>—</th><th>—</th><th>.09</th><th>.03</th><th>.04</th><th>.32</th><th>.04</th><th>.32</th></tr><tr><th>Duration of diabetes</th><th>—</th><th>—</th><th>.02</th><th>&lt;.001</th><th>.01</th><th>.32</th><th>.01</th><th>.32</th></tr><tr><th><b>Treatment Modality</b></th></tr><tr><th>Diet only</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.17</th><th>&lt;.001</th><th>−1.22</th><th>&lt;.001</th></tr><tr><th>Oral medication</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.50</th><th>&lt;.001</th><th>−.55</th><th>&lt;.001</th></tr><tr><th>Insulin only or combination</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Lifestyle <sup>f</sup></b></th></tr><tr><th>Fair</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.21</th><th>&lt;.001</th></tr><tr><th>Good</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.02</th><th>&lt;.001</th></tr><tr><th>Poor</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th></tr><tr><th><b>F <sup>g</sup></b></th><th>13.16</th><th>&lt;.001</th><th>12.42</th><th>&lt;.001</th><th>17.59</th><th>&lt;.001</th><th>16.06</th><th>&lt;.001</th></tr><tr><th><b><i>R </i><sup>2 </sup>, %</b></th><th>4.80</th><th>—</th><th>7.62</th><th>—</th><th>14.09</th><th>—</th><th>16.18</th><th>—</th></tr><tr><th><b>∆ <i>R </i><sup>2 </sup>, %</b></th><th>—</th><th>—</th><th>2.82</th><th>—</th><th>6.47</th><th>—</th><th>2.09</th><th>—</th></tr><tr><th><b>F <sup>h</sup></b></th><th>—</th><th>—</th><th>34.89</th><th>&lt;.001</th><th>30.12</th><th>&lt;.001</th><th>9.95</th><th>&lt;.001</th></tr></tbody></table>Abbreviation: HbA1c, hemoglobin A1c.a _P_ values calculated using _t_ test, analysis of variance (ANOVA), and simple regression; significance set at α = .05.b Model 1: demographic variables only.c Model 2: variables are demographics and clinical conditions.d Model 3: variables are demographics, clinical conditions, and treatment modality.e Model 4: variables are demographics, clinical conditions, treatment modality, and lifestyle.f Poor = 0 positive health behaviors, fair = 1 positive health behavior, good ≥2 positive health behaviors.g F test for model.h F test for change in _R_ 2 . \n\n【39】#####  Table 3. Hierarchical Regression Analyses Predicting HbA1c Levels in Middle-Aged Adults (51-64 y) With Type 2 Diabetes (N = 379), the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study a\n\n【40】<table><tbody><tr><th>Variable</th><th>Model 1 <sup>b</sup></th><th>Model 2 <sup>c</sup></th><th>Model 3 <sup>d</sup></th><th>Model 4 <sup>e</sup></th></tr><tr><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th></tr><tr><th><b>Demographics (Race/Ethnicity)</b></th></tr><tr><th>Non-Hispanic black</th><th>1.13</th><th>&lt;.001</th><th>.96</th><th>&lt;.001</th><th>.82</th><th>.001</th><th>.80</th><th>.002</th></tr><tr><th>Hispanic</th><th>.88</th><th>&lt;.001</th><th>.87</th><th>.004</th><th>.74</th><th>.01</th><th>.79</th><th>.007</th></tr><tr><th>Non-Hispanic white</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Clinical Conditions</b></th></tr><tr><th>No. of chronic diseases</th><th>—</th><th>—</th><th>.24</th><th>&lt;.001</th><th>.19</th><th>.002</th><th>.18</th><th>.003</th></tr><tr><th>Duration of diabetes</th><th>—</th><th>—</th><th>.04</th><th>&lt;.001</th><th>.01</th><th>.32</th><th>.02</th><th>.046</th></tr><tr><th><b>Treatment Modality</b></th></tr><tr><th>Diet only</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.07</th><th>&lt;.001</th><th>−1.18</th><th>&lt;.001</th></tr><tr><th>Oral medication</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.35</th><th>.11</th><th>−.46</th><th>.04</th></tr><tr><th>Insulin only or combination</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Lifestyle <sup>f</sup></b></th></tr><tr><th>Fair</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.22</th><th>&lt;.001</th></tr><tr><th>Good</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.96</th><th>.008</th></tr><tr><th>Poor</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th></tr><tr><th><b>F <sup>g</sup></b></th><th>12.90</th><th>&lt;.001</th><th>14.38</th><th>&lt;.001</th><th>12.82</th><th>&lt;.001</th><th>11.40</th><th>&lt;.001</th></tr><tr><th><b><i>R </i><sup>2 </sup>, %</b></th><th>6.67</th><th>—</th><th>14.44</th><th>—</th><th>18.49</th><th>—</th><th>21.30</th><th>—</th></tr><tr><th><b>∆ <i>R </i><sup>2 </sup>, %</b></th><th>—</th><th>—</th><th>7.77</th><th>—</th><th>4.05</th><th>—</th><th>2.81</th><th>—</th></tr><tr><th><b>F <sup>h</sup></b></th><th>—</th><th>—</th><th>16.98</th><th>&lt;.001</th><th>9.24</th><th>&lt;.001</th><th>6.61</th><th>.001</th></tr></tbody></table>Abbreviation: HbA1c, hemoglobin A1c.a _P_ values calculated using _t_ test, analysis of variance (ANOVA), and simple regression; significance set at α = .05.b Model 1: demographic variables only.c Model 2: variables are demographics and clinical conditions.d Model 3: variables are demographics, clinical conditions, and treatment modality.e Model 4: variables are demographics, clinical conditions, treatment modality, and lifestyle.f Poor = 0 positive health behaviors, fair = 1 positive health behavior, good ≥2 positive health behaviors.g F test for model.h F test for change in _R_ 2 . \n\n【41】#####  Table 4. Hierarchical Regression Analyses Predicting HbA1c Levels in Older Adults (≥65 y) With Type 2 Diabetes (N = 430), the 1998 and 2000 Health and Retirement Study (HRS) and the HRS 2003 Diabetes Study a\n\n<table><tbody><tr><th>Variable</th><th>Model 1 <sup>b</sup></th><th>Model 2 <sup>c</sup></th><th>Model 3 <sup>d</sup></th><th>Model 4 <sup>e</sup></th></tr><tr><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th><th>ß</th><th><i>P </i>Value</th></tr><tr><th><b>Demographics</b></th></tr><tr><th><b>Race/ethnicity</b></th></tr><tr><th>Non-Hispanic black</th><th>0.13</th><th>.49</th><th>.17</th><th>.37</th><th>.09</th><th>.32</th><th>.11</th><th>.56</th></tr><tr><th>Hispanic</th><th>.24</th><th>.25</th><th>.27</th><th>.20</th><th>.24</th><th>.23</th><th>.25</th><th>.21</th></tr><tr><th>Non-Hispanic white</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Clinical Conditions</b></th></tr><tr><th>No. of chronic diseases</th><th>—</th><th>—</th><th>−.03</th><th>.45</th><th>−.07</th><th>.08</th><th>−.06</th><th>.13</th></tr><tr><th>Duration of diabetes</th><th>—</th><th>—</th><th>.01</th><th>.32</th><th>−.01</th><th>.32</th><th>−.01</th><th>.32</th></tr><tr><th><b>Treatment Modality</b></th></tr><tr><th>Diet only</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−1.01</th><th>&lt;.001</th><th>−1.07</th><th>&lt;.001</th></tr><tr><th>Oral medication</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.46</th><th>.001</th><th>−.46</th><th>.001</th></tr><tr><th>Insulin only or combination</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th><th>1 [Reference]</th></tr><tr><th><b>Lifestyle <sup>f</sup></b></th></tr><tr><th>Fair</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.90</th><th>.13</th></tr><tr><th>Good</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>−.80</th><th>.18</th></tr><tr><th>Poor</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>—</th><th>1 [Reference]</th></tr><tr><th><b>F <sup>g</sup></b></th><th>.80</th><th>.45</th><th>1.38</th><th>.25</th><th>7.18</th><th>&lt;.001</th><th>5.71</th><th>&lt;.001</th></tr><tr><th><b><i>R </i><sup>2 </sup>, %</b></th><th>.38</th><th>—</th><th>1.34</th><th>—</th><th>9.59</th><th>—</th><th>10.17</th><th>—</th></tr><tr><th><b>∆ <i>R </i><sup>2 </sup>, %</b></th><th>—</th><th>—</th><th>.96</th><th>—</th><th>8.25</th><th>—</th><th>0.58</th><th>—</th></tr><tr><th><b>F <sup>h</sup></b></th><th>—</th><th>—</th><th>2.07</th><th>.13</th><th>19.30</th><th>&lt;.001</th><th>1.36</th><th>.26</th></tr></tbody></table>Abbreviation: HbA1c, hemoglobin A1c.a _P_ values calculated using _t_ test, analysis of variance (ANOVA), and simple regression; significance set at α = .05.b Model 1: demographic variables only.c Model 2: variables are demographics and clinical conditions.d Model 3: variables are demographics, clinical conditions, and treatment modality.e Model 4: variables are demographics, clinical conditions, treatment modality, and lifestyle.f Poor = 0 positive health behaviors, fair = 1 positive health behavior, good ≥2 positive health behaviors.g F test for model.h F test for change in _R_ 2 . Back to top  |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Privacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 <table><tbody><tr><th><a>Centers for Disease Control and Prevention</a><br><a>National Center for Chronic Disease Prevention and Health Promotion</a></th><th><img></th><th><img> <a>United States Department of<br>Health and Human Services</a></th></tr></tbody></table>  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "73342670-538d-4443-9c4f-75a92c386cf3", "title": "Culture Matters in Communicating the Global Response to COVID-19", "text": "【0】Culture Matters in Communicating the Global Response to COVID-19\n================================================================\n\n【1】COMMENTARY — Volume 17 — July 9, 2020\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### C.O. Airhihenbuwa, PhD <sup>1 </sup> ; J. Iwelunmor, PhD <sup>2 </sup> ; D. Munodawafa, EdD <sup>3 </sup> ; C.L. Ford, PhD <sup>4 </sup> ; T. Oni, MD <sup>5 </sup> ; C. Agyemang, PhD <sup>6 </sup> ; C. Mota, PhD <sup>7 </sup> ; O.B. Ikuomola, BS <sup>1 </sup> ; L. Simbayi, DPhil <sup>8 </sup> ; M.P. Fallah, PhD <sup>9 </sup> ; Z. Qian, MD <sup>2 </sup> ; B. Makinwa, MPA <sup>10 </sup> ; C. Niang, PhD <sup>11 </sup> ; I. Okosun, PhD <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Airhihenbuwa C, Iwelunmor J, Munodawafa D, Ford C, Oni T, Agyemang C, et al. Culture Matters in Communicating the Global Response to COVID-19. Prev Chronic Dis 2020;17:200245. DOI: http://dx.doi.org/10.5888/pcd17.200245 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Abstract\n*   Introduction\n*   Culture and Communication for Health\n*   PEN-3 Model and Communication Response to COVID-19\n*   Preexisting Chronic Conditions and Preexisting Structural Contexts: Cultural Empowerment\n*   Individualist Versus Collectivist: Cultural Identity\n*   Noncommunicable Diseases and COVID-19: Relationship and Expectation\n*   Implications for Public Health\n*   Acknowledgments\n*   Author Information\n*   References\n*   Table\n\n【9】**Summary**\n\n【10】**What is already known on this topic?**\n\n【11】The World Health Organization developed risk communication and community engagement (RCCE) to facilitate global response to COVID-19. RCCE communicates about individual risks but communicates little about community risks.\n\n【12】**What is added by this report?**\n\n【13】Community engagement requires knowledge of culture in framing COVID-19 communication and messaging. The PEN-3 cultural model was used to frame community engagement for collective actions.\n\n【14】**What are the implications for public health practice?**\n\n【15】COVID-19 reveals existing structural inequity in black and brown communities nationally and globally. PEN-3 offers a cultural framework for community-engaged communication and messaging for COVID-19.\n\n【16】Abstract\n--------\n\n【17】Current communication messages in the COVID-19 pandemic tend to focus more on individual risks than community risks resulting from existing inequities. Culture is central to an effective community-engaged public health communication to reduce collective risks. In this commentary, we discuss the importance of culture in unpacking messages that may be the same globally (physical/social distancing) yet different across cultures and communities (individualist versus collectivist). Structural inequity continues to fuel the disproportionate impact of COVID-19 on black and brown communities nationally and globally. PEN-3 offers a cultural framework for a community-engaged global communication response to COVID-19.\n\n【18】Top\n\n【19】Introduction\n------------\n\n【20】Our primary aim in this commentary is to offer a community-engaged communication strategy that focuses on coronavirus disease 2019 (COVID-19) messages in cultural context. COVID-19, the disease caused by the novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), was declared a global pandemic on March 11, 2020. Since that time, messages of prevention have focused primarily on preventing individual risks, particularly for those with preexisting chronic conditions, including hypertension, diabetes, stroke, and asthma. As infection and death rates grow, communication about response to the pandemic has increasingly focused on individual behavior choices, which assumes that prevention is largely in an individual’s control. In efforts to promote uniform messaging for COVID-19, the World Health Organization developed a multilevel risk communication and community engagement (RCCE) response strategy for health care workers, the wider public, and national governments (1,2).\n\n【21】Well intentioned as RCCE may be, the strategy ends up focusing more on individual risk and less on community engagement. By community engagement, we mean creating spaces and opportunities for those who live in the community to have their voices heard in naming the problem and offering solutions to the problems they face (3). The process of such engagement also includes identifying community resilience and ways to build on values that are important to the community. Communication about individual risk is important, but prevention and control messaging is more likely to be achieved when we engage the voices of those who live in the communities, particularly communities that bear the heaviest burden of the pandemic.\n\n【22】Vulnerability to the COVID-19 pandemic cannot be fully explained by individual risks alone but rather by broader social and structural determinants of health that result in inequities in communities where vulnerable populations live, work, play, pray, and learn (4–6). Moreover, a disproportionate burden of COVID-19 mortality is among racial and ethnic populations in communities that have had historical inequities in health (7–9). With increasing global mortality, a deep concern remains about the alarming levels of general spread, disease severity, and inaction for these communities (10). Research on health disparities, particularly on antiracism (11), demands a focus on risk environment and risk situation rather than the conventional epidemiologic focus on risk factor, which tends to place the burden of behavior change on individuals rather than the context and structure that define and confine their vulnerability (12–14). Thus, community-engaged communication is crucial for acknowledging the voices of those in the community with culturally relevant solutions that are more likely to be sustained beyond the pandemic. Communities that are the most affected experience historical, structural inequities that create not only their preexisting chronic health conditions but also their preexisting vulnerable living and working conditions (15). To understand these communities, the role of culture matters if any communication strategy is to be adopted or sustained.\n\n【23】Top\n\n【24】Culture and Communication for Health\n------------------------------------\n\n【25】Culture is central to effective COVID-19 messaging for community engagement. We define culture as a collective sense of consciousness that influences and conditions perception, behaviors, and power and how these are shared and communicated (3). Culture may appear neutral, but its power to define identity and communities as a collective is based on values expressed through institutions such as health care, education, and families (3). Culture shapes language, which in turn shapes communication both in message delivery and reception. In response to COVID-19 in Europe, for example, cultural sensitivity to racial and ethnic minority group experiences is believed to be critical if messages for mitigation are to have broader impact (16).\n\n【26】Framing communication messaging that engages the most affected communities can draw some lessons from the multilevel strategies employed in HIV communication, which identify relevant structural factors of institutional policy, economic status, gender, and spirituality while grounded in the force of culture (17,18). For example, as part of HIV communication strategy, the concept of “zero grazing” was introduced in Uganda as a prevention message for multipartner marriages by encouraging that sexual activities be kept within the circle of those in the marriage only. This message was a community collective response to the conventional individualist message of one-to-one sexual relations.\n\n【27】For COVID-19, some black and brown communities have initiated collective communication for mitigation so that messages have cultural meanings for those with whom they share common cultural values. For example, although heavily affected by COVID-19, some indigenous communities in the United States have sought their own solutions to this pandemic by using traditional knowledge and language to promote voluntary isolation at the individual level and sealing off their territories at the community level (19) while still being able to continue aspects of their spiritual well-being (20). Thus, to rapidly improve our communication messages in response to COVID-19, we need an effective global response that invites community-engaged solutions with culture as a connecting space.\n\n【28】Culture is key to the global response to community engagement. COVID-19 unveils a pattern of cultural insensitivity that has also been evident in communication about Ebola. In the early stages of the Ebola outbreak in 2014–2015, conventional messages did more harm than good because they did not value the cultural roles associated with death. Two examples of these messages were, “When you get Ebola, you will die” or “If someone is sick, don’t touch him.” In Liberia, the high death rate from malaria and other diseases among the poor blunted messages for urgency to heed prevention and treatment of Ebola (21). In the West Point slum of Monrovia, Liberia, for example, adhering to physical distancing for Ebola and now COVID-19 is made difficult by sea erosion from the past 10 years, which reduced the land mass by 50%, even though the same number of people remain. Structural inequities often reveal the limit of individual choices in the absence of corrective actions to address contextual constraints over which the community has no control. These constraints are the preexisting contexts of inequities in many black and brown communities globally (5,22).\n\n【29】We believe that COVID-19 mitigation efforts that focus on individual behavior such as handwashing and physical distancing must be balanced with structural mitigation efforts such as clean water, access to housing, unemployment, and for those with jobs, ability (type of job) and tools (access to computer and internet) to work from home. These are the daily realities of racial/ethnic and economically disadvantaged populations that bear the heaviest burden of the pandemic (22). Yet as has been learned from HIV (23) and Ebola (21), culture offers communication messaging that ranges from positive aspects of lived experience that should be promoted to negative practices that should be overcome within the context of communities. To frame approaches to communications and community engagement for COVID-19, we use the PEN-3 cultural model ( Figure ). We believe that this model offers a roadmap for engaging communities in communication about COVID-19 mitigation efforts.  \n\n【30】**  \nFigure.**  \nThe PEN-3 Model. The model has 3 primary components: cultural identity, cultural empowerment, and relationships and expectations, and each of the 3 components has 3 domains. \\[A text version of this figure is available.\\]\n\n【31】Top\n\n【32】PEN-3 Model and Communication Response to COVID-19\n--------------------------------------------------\n\n【33】PEN-3 is a cultural model that was developed and first published in 1989 (24). The PEN-3 cultural model consists of 3 primary domains: 1) cultural identity, 2) relationships and expectations, and 3) cultural empowerment. Each domain includes 3 factors that form the acronym PEN; person, extended family, neighborhood (cultural identity domain); perceptions, enablers, and nurturers (relationship and expectation domain); positive, existential and negative (cultural empowerment domain). The domains are described in detail elsewhere (3). A key outcome of using PEN-3 is learning to first identity the positive aspects of behavior and culture such that negative behavior is not the only focus of intervention, as shown in a systematic review (25). At the height of the global HIV stigma and racism against the cultures of black and brown identities, PEN-3 was developed to offer a space for voices to be heard that are otherwise silenced. The model was designed to guide researchers and practitioners to listen to those voices, and in so doing, to ask for not only what these communities were doing wrong but to begin with what they are doing correctly. Culture exists where we live, work, play, pray, and learn. In PEN-3, the focus on cultural logic of decision making about a pandemic is less about who is right or wrong than about what societal reasoning and rationale are at the foundation of the message. Even more important is which populations and communities are the intended audience for messages meant to be solutions. Thus, the importance of the positive aspects of a community and people, their collective resilience, and their cultural logic must not be overshadowed by the presence of diseases, as we have learned from the work on HIV and Ebola and now COVID-19. Therefore, reframing COVID-19 communication messages globally must respond not only to individuals but to the community as a collective. Individuals must not be privileged over the collective or community.\n\n【34】Science also has culture. The application of the PEN-3 model to COVID-19 communication also applies to the scientific community whose task it is to solve the disparities unveiled by COVID-19. To acknowledge that the scientific community exists within 1 or more cultures is to remove it from the pedestal on which it has rested for so long in ways that are well beyond any reproach and critique of the notion that science is inherently value-free (26). Indeed, questions about the effectiveness of social distancing have contrasting beliefs between a country like Sweden (which does not believe in social distancing) and the United States (which does); yet both are based on scientific claims, confirming that science is itself a production of culture and politics. In focusing on the PEN-3 domain of cultural empowerment, for example, the positive and existential dimensions of scientific culture are eagerly and frequently promoted by the scientific community. However, the negative dimensions evident in contrasting recommendations must also be examined, because they create communication challenges. To remedy the challenges requires messaging that promotes cultural inclusivity in the responses to the COVID-19 pandemic.\n\n【35】For years, science ignored the role of structural racism in explaining and predicting disease burdens. Yet it is structural racism that created and maintains communities in which preexisting chronic health conditions such as hypertension and diabetes exist. Therefore our communication should address actions we take at the individual level, risks we face at the collective and community level, and the role science plays in promoting or hindering mitigation efforts. Thus, for COVID-19, PEN-3 offers the importance of cultural empowerment anchored in community-engaged mitigation efforts. We need to focus on both individual risks and community engagement and in so doing address 3 binarisms that must be coalesced to advance global communication for COVID-19. To illuminate the power of culture in community engagement, each of the PEN-3 domains is paired with a binary that needs to be understood and coupled in communication about COVID-19.\n\n【36】Top\n\n【37】Preexisting Chronic Conditions and Preexisting Structural Contexts: Cultural Empowerment\n----------------------------------------------------------------------------------------\n\n【38】Whereas the language of risk factors focuses on individual preexisting chronic conditions such as diabetes, hypertension, and asthma, the language of health disparities and risk environments focuses on preexisting community contexts. These include unhealthy food structures, unemployment environments, poor housing (eg, intergenerational cohabitation), and job types that define and confine vulnerability to COVID-19. The language of individual risk has been used to frame the prevention message of social distancing and wearing a mask. Yet, a recent commentary concluded that physical distancing is a privilege for populations with preexisting contexts that reinforce not only vulnerability to conditions like diabetes but also living conditions that make it impossible to adhere to physical distancing (27). Several recent publications have emerged in which scholars have lamented the heavy racial burden of COVID-19 on African American, Latino, and Native American populations in the United States (8,9,28). Similar alarm has been raised in Europe, particularly among immigrant populations (16) and in Brazil, which has one of the highest number of cases in the world. In Brazil, nearly 6% of the population, which is mostly black, live in favelas (slums or shantytowns located within or on the outskirts of the country’s large cities) and are exposed to social and environmental vulnerability with poor access to water and employment, among other needs (29). Socio-spatial inequality determines the patterns of Brazilian cities and the disposition of housing conditions, which limit adherence to the health policy of social isolation. This accumulation of disadvantages represents structural risks for any health condition, which has resulted in high prevalence of many neglected diseases in these vulnerable areas in Brazil. In South Africa, particularly in the absence of official data based on race/ethnicity, the government downplayed racial/ethnic vulnerability until the premier of the Province of Gauteng, which includes Johannesburg, revealed that the hotspots of COVID-19 in his province were shifting from the suburbs, where most whites live, to townships, where most blacks and people of mixed race (known as coloreds) live (30). In many Nigerian cultures, certain cosmological viewpoints suggest that fate determines diseases and ill health and that these are independent of science and human actions (31). The cultural empowerment domain of the PEN-3 model allows COVID-19 interventionists to look at the total context, including how people construct their lived experience within their resilience and the hurdles in their communities. COVID-19 communication should begin with positive factors, such as persistence and resilience, to achieve solutions that nurture and revive the community. To better understand the role of culture in a pandemic we can draw lessons from 2 pandemics that remain with us today, HIV and Ebola ( Table ).\n\n【39】Top\n\n【40】Individualist Versus Collectivist: Cultural Identity\n----------------------------------------------------\n\n【41】Every society has a social contract that frames the ways we act and prioritize decisions and choices: as individuals, such as in the United States, as the collective as in China, or some mix of those forms as in Canada and France. One of the key lessons for a global response to a pandemic is that the cultural logic of different societies shapes and influences their prevention strategies. In the United States, individual vulnerability to risk is culturally privileged over community risk, when both should be addressed equally. Such coalescing of dual logics is embodied in the cultural messages from the yin and yang (coexistence and balancing of opposite forces) that may inform messaging in China; Ubuntu (I am because we are) in South Africa; and the expression “Nit nittay garabam” (The person is the remedy of the person) in Wolof in Senegal (32). These cultural expressions are different, neither better nor worse than individualist cultural logic that typically informs messaging in the United States. In China, for example, quarantine was implemented in Wuhan as a collective action to varying degrees and scopes. At the individual level, everyone was mandated to stay at home, and a permit to leave home could be obtained only from a community committee made up of volunteers. At the city level, all city entries and exits were screened; all public transport was discontinued including public bus, subway, ferry, and taxi. This response reflected the collectivist social and cultural contract of Chinese society (33). Thus, when a message of response in one country is communicated in another as draconian, for example, we need to unpack the different rather than competing cultural logics that inform these messages, particularly in a pandemic. Given the virulence of COVID-19, communication messages must be inclusive of multiple cultural logics whereby the word “and” is preferred over the word “or”. In the book entitled _Built to Last_ (34), the authors debunked the competing binarism of and/or in their study of the characteristics of successful and enduring visionary companies. In advancing the phrases, the “tyranny of the or” and the “genius of the and,” the authors made the case for why duality is a strength and not a competition in which one side has to win. COVID-19 messaging globally should embrace cultures and communities with the genius of the “and” by not privileging any one culture over another. The late Chinua Achebe, a Nigerian novelist, once noted that for collective cultures, wherever one idea stands, it is absolutely necessary to expect another idea to stand next to it (35). Thus, instead of thinking in single cultural logic, we have to embrace multicentric logics – individual, collective, and everything in between.\n\n【42】Top\n\n【43】Noncommunicable Diseases and COVID-19: Relationship and Expectation\n-------------------------------------------------------------------\n\n【44】As the world is consumed with the COVID-19 pandemic, there remains a silent pandemic of noncommunicable diseases (NCDs) that now coexist in the same communities most affected by COVID-19. The response to NCDs in the context of COVID-19 should remain a top priority as part of structural solutions to inequities. To promote equity, we must address the structural determinants of health by first addressing structural racism, which is inscribed in institutional policies and practices that have created and sustain the disproportionate burden of hypertension, diabetes, and other NCDs in the black and brown communities (5). Thus, structural racism is a key determinant of such NCDs as hypertension, diabetes, stroke, and asthma (6). NCDs are the leading cause of death worldwide, with the most significant burden placed on low-income and middle-income populations in terms of premature deaths. In the United States, racial minorities, specifically black, Latino, and Native American populations, are the most burdened by NCDs (36). Indeed, the leading causes of death in these populations are heart disease, cancer, unintentional injuries, chronic lower respiratory disease, stroke, and cerebrovascular diseases, which together account for approximately 65% of total deaths (37). Thus, the NCD burden exists in the same population where COVID-19 exists. Our communication messaging, therefore, should erase a binarism of competition that leads to a pandemic _or_ NCDs rather than COVID-19 _and_ NCDs. The behaviors and context that favor one condition are likely to favor the others. Indeed, where NCD stands, infectious diseases like COVID-19 are likely to stand next to it. The messages of COVID-19 prevention in social and physical distancing and wearing masks are important solutions, but their sustainability depends on adequate response to disparities in the burden of diabetes, asthma, and other NCDs that are preexisting chronic conditions. Structurally, social distancing is problematic in South African townships, Brazilian favelas, and Nigerian slums where people share with one another basic essentials, such as sugar or salt when they run out of stock. The situation is further exacerbated by the lack of access to potable water in many of these communities including the _quartiers_ of Senegal, the town of Khayelitsha in South Africa, favelas in Brazil, slums of Nigeria, and Flint, Michigan, in the United States. Communication and messaging for COVID-19 should also focus on us as health scientists and professionals by looking to ourselves for the same needed cultural transformation that we expect from communities responding to NCD pandemics as we do for infectious pandemics. Similar to Ebola (38) and HIV, COVID-19 revealed the falsehood in the separation of disease burdens by how they come to inhabit our bodies. This is the time for communication and messaging to focus not only outward to the community but also inward toward public health experts who frame the messages. How we respond now to COVID-19 is how we must respond to NCDs like hypertension, diabetes, obesity, cholesterol management, and asthma, because these disorders are constant reminders of persistent inequities in our communities.\n\n【45】Top\n\n【46】Implications for Public Health\n------------------------------\n\n【47】COVID-19 communication and messaging should address community risks at least as much as individual risks. PEN-3 offers a communication framework that engages the community by promoting positive factors, acknowledging unique factors, and preventing negative factors. There is a limit to the culture(s) of science, and scientists should reexamine the negative dimensions of scientific cultural solutions to the pandemic. Research and evaluation are also needed to embrace alternative perspectives and the culture of policy and politics that influence the choice of architecture for communication and messaging strategies. Such research and evaluation, for example, on communicating risk mitigation, should democratize scientific research and empower communities to advance solutions to the root causes of health inequities and strategies to improve their own well-being (39). By offering a model for effectively engaging communities, PEN-3 also focuses on mutual community-centered strategies, highlighting not only the perceptions that matter but also the enablers or resources and nurturers or collective roles that foster community agency and voice in mitigating the COVID-19 pandemic. Moreover, to the extent these strategies center equity, they enable culturally grounded approaches to scientific inquiry and challenge the field from within itself to honor community agency and resilience. These alternative perspectives can accelerate efforts in health equity by identifying and addressing the underlying structural determinants of inequities, such as structural racism, that lead to the disproportionate burden of COVID-19 cases and deaths among racial/ethnic minority groups. Ultimately, the goal of COVID-19 communication and messaging within culture is to mitigate increase in new cases and deaths, address preexisting structural contexts, and ultimately advance global communication messaging that promotes health and social justice for this pandemic now and others in the future.\n\n【48】Top\n\n【49】Acknowledgments\n---------------\n\n【50】No copyrighted material was used in the commentary.\n\n【51】Top\n\n【52】Author Information\n------------------\n\n【53】Corresponding Author: Collins O Airhihenbuwa, PhD, MPH, Professor of Health Policy and Behavioral Sciences, School of Public Health, Georgia State University, 140 Decatur St, Atlanta, GA 30303. Telephone: 404-413-9326. Email: Cairhihenbuwa@gsu.edu .\n\n【54】Author Affiliations: <sup>1 </sup> School of Public Health, Georgia State University, Atlanta, Georgia. <sup>2 </sup> College for Public Health and Social Justice, Saint Louis University, Saint Louis, Missouri. <sup>3 </sup> Department of Community Medicine, Midlands State University, Gweru, Zimbabwe. <sup>4 </sup> Center for the Study of Racism, Social Justice & Health, UCLA Fielding School of Public Health, Los Angeles, California. <sup>5 </sup> MRC Epidemiology Unit, University of Cambridge, Cambridge, United Kingdom, and School of Public Health and Family Medicine, University of Cape Town, South Africa. <sup>6 </sup> Department of Public Health, Amsterdam UMC, University of Amsterdam, Amsterdam Public Health Research Institute, Amsterdam, Netherlands. <sup>7 </sup> Department of Public Health, Institute of Collective Health, Federal University of Bahia, Salvador, Brazil. <sup>8 </sup> Human Sciences Research Council, Cape Town, South Africa. <sup>9 </sup> National Public Health Institute of Liberia, Office of the Director–Monrovia, Greater Montrovia, Liberia . <sup>10 </sup> AUNIQUEI, Office of the Director and Chief Executive Officer–Lagos, Lagos, Nigeria; Former Director of African Region of United Nations Population Fund. <sup>11 </sup> Institute of Environmental Sciences, Cheikh Anta Diop University, Dakar, Senegal.\n\n【55】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f95fb856-db1a-4ce3-bf02-08dd193450a2", "title": "Antimicrobial Drug Resistance among Refugees from Syria, Jordan", "text": "【0】Antimicrobial Drug Resistance among Refugees from Syria, Jordan\n**To the Editor:** The Kassem et al. article regarding high rates of multidrug-resistant (MDR) bacteria colonizing Syrian children highlights the challenge of choosing empiric antimicrobial drugs to treat war-injured refugees from Syria ( _1_ ). The findings mirror other reports ( _2_ _–_ _3_ ) and our own experience in a charitable hospital in Amman, Jordan, which manages war-injured refugees from Syria. As part of a program of antimicrobial drug stewardship and infection prevention and control, empiric antimicrobial drug protocols were introduced. For antimicrobial drug–naive patients, the first-line choice for prophylaxis and treatment of skin and soft-tissue infections, including those involving open fractures, was a narrow-spectrum cephalosporin, as recommended by the Infectious Diseases Society of America guidelines ( _4_ ); however, clinical failure was common.\n\n【1】We retrospectively reviewed the clinical microbiology data of 75 patients admitted in January 2015 with a history of suspected post-trauma infection. All these patients were first treated in field hospitals in Syria; 82.7% were male, and 33% were <16 years old. Twenty-four percent had multiple injuries, 20% had osteomyelitis, and 53% had metal prosthetic implants.\n\n【2】Thirty bacterial isolates were identified, mostly from deep wound swabs of 21 (28%) injured patients; 9/21 were infected with 2 isolates. Twenty-nine (97%) isolates were gram-negative bacteria: 10 _Proteus_ spp., 10 _Escherichia coli_ , 5 _Pseudomonas_ spp., and 4 _Klebsiella_ spp. Disk diffusion susceptibility testing showed that 20 (66%) isolates were MDR and 11 (36.7%) were carbapenem resistant.\n\n【3】The hospital laboratory did not have the capacity to perform further testing and confirmation of the resistant strains in line with international quality standards because they lacked suitable equipment and financial resources. Preventing further dissemination of MDR organisms among war-injured refugees from Syria at hosting healthcare facilities requires an effective surveillance system, investment in infection prevention and control, appropriate antimicrobial drug stewardship, and urgent laboratory capacity building inside Syria and in the refugee-host countries.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ea5f5562-d1e5-410f-b83a-d0695cb84328", "title": "The Soot that Falls from Chimneys", "text": "【0】The Soot that Falls from Chimneys\nThomas Hart Benton (1889–1975), Interior of a Farm House (1936) Tempera on board (45.7 cm × 76.2 cm), Courtesy of The John and Mable Ringling Museum of Art, The State Art Museum of Florida, a division of Florida State University, USA\n\n【1】“The best damned painter in America,” is how Harry S. Truman described Thomas Hart Benton, his choice to create a mural for the Truman Library in Independence, Missouri. The President’s fellow Missourian lived in Kansas City at the time, his artistic career in a slump, though his reputation as a fine muralist still intact. “I picked him,” Mr. Truman told the crowd at the mural’s dedication, “because he was the best, and this is the finest work by the best.”\n\n【2】During work on the Library mural, the President noted that he got along with the painter, even though, he joked, “That’s hard for anyone to do.” A gifted musician, writer, and prolific lithographer, Benton was also direct, impatient, radical, and often tactless. He hated museums, professing that art works belonged in clubs and barrooms, “Anywhere anybody had time to look at ’em.” In his autobiography, he mused, “A few people have, at times, expressed a belief that I was not the most desirable kind of fellow to have around. But, all in all, my differences with the home folks, when looked at in perspective, have not amounted to much.”\n\n【3】Born in a fiercely political family, the artist was named after his uncle Thomas Hart Benton, the first and longest serving U.S. Senator of Missouri. His father, a populist, was a member of the U.S. House of Representatives. Benton bucked family tradition and entered the Art Institute of Chicago, in 1907, aspiring to become a cartoonist. His longtime love of painting affirmed in the fine arts environment, he traveled to Paris, where he attended the Académie Julien and Académie Colarossi. He met Mexican muralist Diego Rivera and studied the masters at the Louvre, among them El Greco, whose exaggerated forms found their way into his mature style.\n\n【4】Back in New York for the lack of funds, he explored the local scene and such prevailing styles as impressionism and synchromism, especially in the work of Stanton MacDonald-Wright, who became his friend. His career took a different turn when he joined the Navy. “The most important thing, so far, I had ever done for myself as an artist.” This work, which involved extensive documentary drafts and drawing, had an enduring effect on his style. “When I came out of the Navy after the First World War, I made up my mind that I wasn’t going to be just a studio painter, a pattern maker in the fashion then dominating the art world―as it still does. I began to think of returning to the painting of subjects, subjects with meanings, which people in general might be interested in.”\n\n【5】Though by this time well-versed in modernism, Benton turned against it unable to embrace its “colored cubes and classic attenuations,” rejecting it as “divorced from the common ways of the day.” He moved toward naturalistic and representational work focused on the American scene. He taught at the Art Students League, instructing many who went on to become famous in their own right, not the least of them abstract expressionism icon Jackson Pollock. “Even after I had castigated his innovations and he had replied by saying I had been of value to him only as someone to react against, he kept in personal touch with me ….”\n\n【6】Benton’s style, a blend of modern and academic elements, came to be known as regionalism. Others in this movement were Grant Wood, most famous for his _American Gothic_ , and John Steuart Curry, who painted life in his native Kansas. Many criticized their choice of the local over the cosmopolitan, but the regionalists, particularly Benton, struggled with the notion of an authentic American voice long before New York became an art center. “You just can’t think of art in terms of progress,” Benton explained in an interview, “It is not progressive. It is just different from age to age. One age gets used to a certain kind of art form and thinks that is better, but the next age will deny that thought and go back to some older form. So I wouldn’t compare the animal paintings of the cavemen with those of our times or any other times …. We were as good, as artists, when we began our history as we are now―sometimes better.”\n\n【7】The 1930s, a decade of unprecedented economic hardship in the United States, witnessed renewed interest in history reflected in all aspects of culture. Murals, among them _The Social History of Missouri_ in Jefferson City, which Benton considered his masterpiece, were part of this resurgence, recording as they were milestones in the country’s development. Benton captured his era’s transformation from rural and agrarian to urban and industrial. He painted the growth of business and technology, and the consequent changes in the lives of the common people, in paintings of steamboats and trains, factories, logging and mining operations, offices and farmhouses. “History was not a scholarly study for me but a drama.” He was innovative, bold, outspoken, and unafraid of controversy, allowing myth to blend with observation, casting ordinary people as heroes and pioneers. “I wasn’t so much interested in famous characters as I was in Missouri and the ordinary run of Missourians that I’d known in my life.”\n\n【8】Benton made a habit of gallivanting around the countryside, meeting people and sketching them and their surroundings. Later he would lay out his designs from these pencil sketches, using pen and ink to define and preserve them. These and three-dimensional clay models he created served as prototypes for oil and tempera studies and for larger compositions. While many critics objected to his subjects, bold colors, manipulated forms, or muscular style, few found fault with his compositional and architectural skills. He had a talent for incorporating multiple themes in limited space and still maintaining cohesiveness.\n\n【9】_Interior of a Farmhouse_ on this month’s cover offers a glimpse of the brilliant color, energy, and movement that characterize Benton’s art and the complexity and richness of his murals. The title understates this intricate composition. The farmhouse at center stage anchors a community of scenes connected by a fence here, a doorway there, an angle, a partial wall, and contains his favorite people: workers doing what they do in the kitchen, the barn, the fields, at rest. On the periphery, steamboat navigation and the wheels of industry are rolling, their ubiquitous smokestacks belching above the Missouri River. Court is in progress; a worker reads the daily news; another washes up; animals wander in and outdoors. The painter reviews American industry in the 1930s, which pulsates, as if it were a live, breathing organism itself.\n\n【10】The values of honest living and hard labor, at the heart of Benton’s work, went hand in hand with the belief that harmony between humans and nature resided on the farm, the interior of which in this painting is not altogether filled with agrarian bliss. Despite the energy emanating from the vibrant community, there are tensions, political and ecologic undertones, part and parcel of industrialization. Benton the social historian sensed the dark side of factories and increased transportation, which he noted in palpable terms, a cloud so menacing against the pristine horizon it unfolded half way across the painting.\n\n【11】“The yellow smoke that rubs its muzzle on the window-panes/…Let fall upon its back the soot that falls from chimneys/,” Benton’s fellow Missourian T.S. Eliot wrote prophetically in “The Love Song of J. Alfred Prufrock.” As they settle, the dark plumes from smokestacks, a fixture in the artist’s work signaling the machine’s intrusion, cause havoc in the farmhouse. “The harmony man had with his environment has broken down,” he wrote. “Now men build and operate machines they don’t understand and whose inner workings they can’t even see.”\n\n【12】Choked by industrial and other pollution, we have come to resemble Benton’s farmhouse, an organism under stress, because “man doesn’t escape his environment.” The human lung, at the center of the body’s complex internal operations is also affected by external factors in the environment: pollution, infectious agents, allergens. These factors, along with causing many other local and global adverse effects, complicate and aggravate a host of respiratory problems from rhinovirus infection, influenza, and pneumonia, to pneumococcal disease, tuberculosis, and legionellosis, now found to spread around the community from the water tank of a paving machine. Once again, the farmhouse is threatened by what’s lurking in “The yellow fog that rubs its back upon the window-panes.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8e7069c0-62a4-4f04-91dc-9e7a37db2034", "title": "In Memoriam: Frank John Fenner (1914–2010)", "text": "【0】In Memoriam: Frank John Fenner (1914–2010)\nFigure\n\n【1】Figure . Frank Fenner at the John Curtin School of Medical Research, Canberra, Australia, inoculating embryonating eggs with myxoma virus, 1950. Used with permission of the John Curtin School of Medical Research.\n\n【2】Frank John Fenner ( Figure ), one of the world’s most distinguished virologists and a dear friend of many colleagues around the world, died in Canberra, Australia, on November 22, 2010, at the age of 95. This In Memoriam must be different from those usually published here. After all, quite detailed pieces are anticipated from the Australian Academy of Sciences, the Royal Society, et al., and Frank had published a comprehensive autobiography ( _1_ ). Thus, there is opportunity to present personal memories, hoping to provide more of a sense of the man, the colleague, and the friend of so many members of the global virology community. This tribute seems to fit in with the closing paragraphs of Frank’s autobiography, in which he reflects on friendship and special friends.\n\n【3】Frank was born in Ballarat, Victoria, Australia, but his family moved to Adelaide, South Australia, when he was 2 years old. He received a bachelor of medicine and surgery degree (1938) and a doctor of medicine degree (1942) at the University of Adelaide. During 1940–1946, he was an officer (Captain, Major) in the Australian Army Medical Corps and served in Australia, Palestine, Egypt, New Guinea, and Borneo. At various times, he worked as a medical officer in a field ambulance and casualty clearing station, a pathologist in a general hospital, and, most prominently, as a malariologist. For his work in combating malaria in Papua New Guinea, he was made Member of The Order of the British Empire. As Frank noted later, the highlight of his military service was meeting and marrying a nurse, his wonderful wife, Ellen, known to all as Bobbie. Bobbie died in 1995, but ever after Frank said that his marriage was the most important part of his life. Frank is survived by his daughter, Marilyn, grandson, Simon, granddaughter Sally, and great-grandson Jasper.\n\n【4】After his war-time service, Frank was recruited to work at The Walter and Eliza Hall Institute of Medical Research in Melbourne by Sir Frank Macfarlane Burnet. Initially, they worked on ectromelia virus infection in mice (to explain his work, Frank coined the term mousepox). In 1949, he received a fellowship to study at the Rockefeller Institute for Medical Research in New York City, where he worked in the laboratory of René Dubos. While Dubos worked on _Mycobacterium tuberculosis_ , Frank worked on _M_ . _ulcerans_ , the etiologic agent of Buruli ulcer, the third most common mycobacterial disease worldwide after tuberculosis and leprosy.\n\n【5】Returning to Australia in 1949, Frank was appointed Professor of Microbiology at the new John Curtin School of Medical Research (JCSMR), a unit of the Australian National University (ANU), in Canberra. He then began studying myxoma virus infection in rabbits. Throughout the 1940s and 1950s, Australia had severe rabbit plagues. Frank’s classic work on myxomatosis culminated in his classic 1948 paper ( _2_ ). This paper includes a description of the progression of mousepox infection in mice, from the time/site of virus entry, to the time/sites of infection of major target organs, to the time/sites essential for virus transmission. It is still featured in all virology texts, and it marks the beginning of modern research in viral pathogenesis. At the same time, this work provided the scientific basis for release of myxoma virus for biologic control of rabbits. This work became the foundation for understanding the parallel evolution of the virus toward lesser virulence and the evolution of the rabbit toward more resistance to infection; it is another classic concept featured in virology texts.\n\n【6】Frank became director of the JCSMR in 1967 and served in this role until 1973. He made major changes in the school across broad areas of biomedical science, developing and changing the departments of genetics, medical chemistry, clinical science, human biology, immunology, and pharmacology, and guided the microbiology, immunology, and virology faculties toward world prominence. Imagine one building housing (not all at the same time) such greats as Gordon Ada, Alan Bellett, Bob Blanden, Stephen Boyden, John Cairns, Peter Cooper, Colin Courtice, Peter Doherty, Jack Eccles, Stephen Fazekas de St. Groth, Adrian Gibbs, Alfred Gottschalk, Royal Hawkes, Dick Johnson, Bill Joklik, Kevin Lafferty, Graeme Laver, Fritz Lehmann-Grube, George Mackaness, Barrie Marmion, Ian Marshall, Brian McAuslan, Peter McCullagh, Cedric Mims, Bede Morris, Chris Parrish, Ian Ramshaw, Rob Webster, David White, and Gwen Woodroofe.\n\n【7】And on the side, Frank published 23 books, of which 4 have become symbols of the march of virology through one of its most expansive eras: The Biology of Animal Viruses (2 editions) ( _3_ ), written with Brian McAuslan, Joe Sambrook, David White, and Cedric Mims; Myxomatosis (recently republished) ( _4_ ), written with Francis Ratcliffe; Medical Virology (4 editions) ( _5_ ), written with David White; and Veterinary Virology (3 editions) ( _6_ ), written with Paul Gibbs, Michael Studdert, Peter Bachmann, David White, Rudi Rott, Marian Horzinek, and Fred Murphy. All of Frank’s books are incredibly readable; he cited the influence of his father in developing his writing style: “Always generalize.” Somehow, in books so full of hard experimental data and proven facts, he always seamlessly added his magic, his interpretative skills, and his clear use of the King’s English.\n\n【8】In 1977, the World Health Organization (WHO) named Frank the chairman of the Global Commission for the Certification of Smallpox Eradication. The last known case of naturally transmitted smallpox occurred in Somalia in 1977, and WHO set in place a comprehensive global program to make sure that pockets of infection had not been overlooked. Eradication of the disease has been regarded as one of the greatest achievements in human history, and it was Frank’s great honor to officially pronounce global eradication to the World Health Assembly in May 1980. He later said that making this pronouncement stood out most among his many achievements: “\\[The pronouncement\\] was accepted unanimously by the World Health Assembly on that day.”\n\n【9】Frank had an abiding interest in the environment, and in 1973 was appointed foundation director of the Centre for Resources and Environmental Studies at ANU; he held this position until his retirement in 1979. The Centre became part of the Fenner School of Environment and Society in 2007. It was in this role, late in his career, that Frank publicly expressed a gloomy prospect for the future of humankind: environmental degradation, global warming, overpopulation. In retrospect, this view now seems at odds with Frank’s otherwise positive view of life, especially in regard to the grand prospects of advances in medical science in support of human welfare.\n\n【10】Frank played many national and international leadership roles ( _1_ ), but 2 in particular were of great interest to him and central in the advance of science in Australia and virology internationally. The first role was in the Australian Academy of Sciences. Frank was in the first group of scientists to become fellows of the Australian Academy of Sciences, and over the years served in several roles. He served as secretary for biologic sciences, where he spearheaded studies of fauna in Australia and became a leader and financial supporter of environmental conservation programs. Later, this grew into the Fenner Conferences on the Environment and the Fenner Medal for Plant and Animal Sciences.\n\n【11】The second role was in the International Committee on the Taxonomy of Viruses. Frank had been interested in virus taxonomy from his first days working in the field. He became a charter member of the International Committee on Nomenclature of Viruses (changed to the International Committee on the Taxonomy of Viruses in 1973) as it was established by Sir Christopher Andrewes, André Lwoff, and Peter Wildy at the International Congress of Microbiology in Moscow in 1966. Frank was unable to attend the next Congress in Mexico City in 1970, but in his absence was elected the second president of the committee (the first president was Peter Wildy). These were formative days; the basic structure of virus taxonomy as we know it today emerged from the work of the committee, under the presidencies of Peter Wildy and Frank Fenner, as the committee met at the first 3 International Congresses of Virology (Helsinki, 1968; Budapest, 1971; and Madrid, 1974) ( _7_ ).\n\n【12】Over the years Frank received many honors ( _1_ ). However, some are listed to illustrate his preeminent place in the world of science, virology, and public service: Companion of the Order of St. Michael and St. George, Companion of the Order of Australia, Foreign Associate of the US National Academy of Sciences, the Australian Prime Minister’s Prize for Science, the Japan Prize, the Copley Medal of the Royal Society, the WHO Medal, and the Albert Einstein World Award for Science. Frank was also proud that the Frank Fenner Building, which houses the ANU Medical School and Faculty of Science, and a residential college, Fenner Hall, are named in his honor.\n\n【13】Turning to a more personal viewpoint, I was fortunate to spend 1970–1971 at the JCSMR as an honorary fellow in the laboratory of Cedric Mims. At the time, Frank was director of the School. It was an amazing time, with the outstanding virology faculty well in place, most having been recruited by Frank in preceding years. It was an institution where conceptualization preceded experimentation, that is, where everyone thought a lot about the anticipated results and meaning of his or her experimental work beforehand.\n\n【14】One of the keys to the intellectual energy level was morning coffee, a lost art. At ≈10:30 am , small groups formed in the departmental lounge, and discussion began, often leading to cryptic notes written on napkins, in some instances leading to collaborative papers in the Journal of Experimental Medicine, the Journal of Infectious Diseases, or such. On occasion, small meetings were held in Frank’s office, in some instances with guests, with collaborative projects involving virologists from other institutions and other countries following. I recall one such meeting in Frank’s office where the director of the Commonwealth Scientific and Industrial Research Organisation had dropped by to bring up a new wrinkle in the use of myxoma virus for rabbit control across Australia. In all this discussion, there was an infectious verve, a sense that the edges of the virologic and immunologic sciences were about to be breached, again. Upon returning to the Centers for Disease Control, I tried to transplant this scientific lifestyle, and although successful within our small unit, I realized that by achieving such a productive and satisfying lifestyle across the entirety of the JCSMR, Frank had again used his magic.\n\n【15】Later, another of Frank’s characteristics was made clear to me: his prodigious capacity for work and for the highest ethical standard in dealing with data, his colleagues, and his students. He wrote ( _1_ ), “From childhood, I have been an early riser… getting up when I woke at about 5 am … usually arriving at the School between 6 and 7 am .” I recall that Frank’s car was always in the first parking space; if it was not, it meant he was out of town. This work ethic extended to the writing/editing of various editions of the books Veterinary Virology ( _6_ ) and Medical Virology ( _5_ ). Frank would come to Atlanta for a week at a time and stay at our house. With thousands of marginal notes made beforehand, we would spend endless hours going over chapters. Usually I thought the effect was chaotic, but then a few weeks later I would be sent the most lucid, organized drafts, all in Frank’s uniquely smooth, clear, expansive style. I recall times when my wife would call that dinner was ready and I would crumple in anticipation of a drink and a bit of a rest, only to be rejoined after a wonderful dinner and family conversation by Frank’s gentle voice, “time to get back to work.” I recall that in all this writing/editing, Frank held to a high level of objectivity and honesty in the prose; this was rather like the concept of “evidence-based medicine” of today. I attribute this skill to Frank’s sense that the written word must reflect an underlying ethical standard, one that we all should emulate.\n\n【16】I could go on, but perhaps there is place for one more personal memory (I hope my memories prompt others to recall their own). This memory can start with Frank’s vignette in his autobiography ( _1_ ): “Bozeman and Yellowstone National Park, 11–25 July 1997: I had been invited to give the Edwin H. Lennette Memorial Lecture at the annual meeting of the American Society for Virology, in Bozeman, Montana, in July 1997. Fred Murphy got in touch with me well before the meeting and suggested that I accompany his family on a week’s trip through Yellowstone and Grand Teton National Parks. Fred had an RV (recreational vehicle), which had beds, shower, toilet, stove, and refrigerator. Fred, Irene, son Rick and his 2 young boys, son Tim and his wife, son Terence and his wife, and I travelled from Bozeman and throughout the parks in the RV; his sons had also brought their bicycles. We had a wonderful trip all around Yellowstone, which was not only the first national park in the USA, but one of the most wonderful in the world, with entrancing hot springs, geysers and waterfalls. We also had a great float trip down the Snake River beneath the Tetons. Then we went to Bozeman, where the meeting was very interesting. I gave my lecture and saw a lot of old friends, including Joe Esposito, Grant McFadden, Olin Kew, Dick Moyer and Mary Estes.”\n\n【17】This trip was one of the most memorable camping trips the Murphy family had ever taken, and Frank became family immediately. I recall seeing Frank walking down a wide trail with 2 of my daughters-in-law, 1 on each arm, all chatting and laughing; I have always wondered what they were talking about. I recall finding a bottle of scotch in a cupboard of the RV, which Frank and I used pharmaceutically at the end of each day of adventure as we sat before dinner in the Murphy perfectly matched lawn chairs (from yard sales; no 2 alike). The bottle eventually was emptied and we lamented its passing, but the next evening as I perchance opened the cupboard, there was a new bottle. Frank had mysteriously found a place to replenish the elixir, but of course he never explained his action. I must admit that as we sat and sipped that scotch the topic of conversation was often virology. Afterward, my son Rick sent Frank a copy of his video of the trip, so there was much email traffic back-and-forth reliving each adventure. Life at its fullest.\n\n【18】Does all this capture Frank Fenner, the man, the friend who has died? If not, then words cannot serve what memories can. Whenever we think of the rise of virology from its roots in pathology and the infectious disease sciences, to its flowering in the first laboratory experiments in viral biology and pathogenesis, and to the beginnings of molecular virology, the name Frank Fenner will be remembered. Whenever we who knew him as a friend think of the emergence of virology as a distinct discipline, at the time of the first International Congresses of Virology (1968 forward), his role will be remembered with great fondness. Whenever we extend these memories to the more personal aspects of life, Frank’s quiet, unassuming character, a character with a steel spine and great insight and understanding of people, will be recalled, again with fondness mixed with great respect. He will never be forgotten.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9213f945-83a7-4b9f-9454-811ae05a5fa3", "title": "Clonal Genotype of Geomyces destructans among Bats with White Nose Syndrome, New York, USA", "text": "【0】Clonal Genotype of Geomyces destructans among Bats with White Nose Syndrome, New York, USA\nGeomycosis, or white nose syndrome, is a newly recognized fungal infection of hibernating bats. The etiologic agent, the psychrophilic fungus _Geomyces destructans_ , was first recognized in caves and mines around Albany, New York, USA ( 1 , 2 ). The disease has spread rapidly in New York and other states in the northeastern United States. At least 1 affected bat species is predicted to face regional extinction in the near future ( 3 ). Much remains unknown about this fungus, including its ecology and geographic distribution. For example, although hibernacula are high on the list of suspected sites, where the bats acquire this infection is not known. Similarly, although strongly suspected, the role of humans and other animals in the dispersal of _G. destructans_ and the effect of such dispersals in bat infections have not been confirmed. We recently showed that 6 _G. destructans_ strains from sites near Albany were genetically similar ( 2 ), raising the possibility of a common source for the spread of this infection. Corollary to this observation and other opinions ( 3 , 4 ), the US Fish & Wildlife Service has made an administrative decision to bar human access to caves as a precautionary measure ( www.fws.gov/whitenosesyndrome/pdf/NWRS\\_WNS\\_Guidance\\_Final1.pdf ). Thus, an understanding of the dispersal mechanism of _G. destructans_ is urgently needed to formulate effective strategies to control bat geomycosis.\n\n【1】### The Study\n\n【2】We applied multiple gene genealogic analyses in studying _G. destructans_ isolates; this approach yields robust results that are easily reproduced by other laboratories ( 5 ). Sixteen _G. destructans_ isolates recovered from infected bats during 2008–2010 were analyzed. These isolates originated from 7 counties in New York and an adjoining county in Vermont, all within a 500-mile radius ( Table 1 ). The details of isolation and identification of _G. destructans_ from bat samples have been described ( 2 ). One isolate of a closely related fungus _G. pannorum_ M1372 (University of Alberta Mold Herbarium, Edmonton, Alberta, Canada) was included as a reference control. To generate molecular markers, 1 isolate, _G. destructans_ (M1379), was grown in yeast extract peptone dextrose broth at 15°C, and high molecular weight genomic DNA was prepared according to Moller et al. ( 6 ). A cosmid DNA library was constructed by using pWEB kit (Epicenter Biotechnologies, Madison, WI, USA) by following protocols described elsewhere ( 7 ). One hundred cosmid clones, each with ≈40-Kb DNA insert, were partially sequenced in both directions by using primers M13 and T7. The nucleotide sequences were assembled with Sequencher 4.6 (Gene Codes Corp., Ann Arbor, MI, USA) and BLAST ( www.ncbi.nlm.nih.gov/BLAST ) homology searches identified 37 putative genes. Sequences of 10 genes, including open reading frames, 3′ and/or 5′ untranslated regions, and introns, were evaluated as potential markers for analyzing _G. pannorum_ and _G. destructans._ Our screening approach indicated that 8 gene targets could be amplified from both _G. destructans_ and _G. pannorum_ by PCR ( Table 2 ).\n\n【3】To obtain DNA sequences from 1 _G. pannorum_ and 16 _G. destructans_ isolates, we prepared genomic DNA from mycelia grown in yeast extract peptone dextrose broth through conventional glass bead treatment and phenol-chloroform extraction and then ethanol precipitation ( 7 ). AccuTaq LA DNA Polymerase (Sigma-Aldrich, St. Louis, MO, USA) was used for PCR: 3 min initial denaturation at 94°C, 35 amplification cycles with a 15-sec denaturation at 94°C, 30-sec annealing at 55°C, and 1-min extension at 68°C and a 5-min final extension at 68°C. PCR products were treated with ExoSAP-IT (USB Corp., Cleveland, OH, USA) before sequencing. Both strands of amplicons were sequenced by the same primers used for PCR amplification ( Table 2 ). A database was created by using Microsoft Access (Microsoft, Redmond, WA, USA) to deposit and analyze the sequences. Nucleotide sequences were aligned with ClustalW version 1.4 ( www.clustal.org ) and edited with MacVector 7.1.1 software (Accelrys, San Diego, CA, USA). Phylogenetic analyses were done by using PAUP 4.0 ( 8 ) and MEGA 4 ( 9 ).\n\n【4】Figure 1\n\n【5】Figure 1 . Consensus maximum-parsimony tree derived from analyzing 8 concatenated gene fragments including a total of 4,470 aligned nucleotides by using PAUP\\* 4.0 ( _8_ ). The number 545 on the branch indicates the total...\n\n【6】We cloned and sequenced ≈200 Kb of the _G. destructans_ genome and identified genes involved in a variety of cellular processes and metabolic pathways ( Table 2 ). DNA sequence typing by using 8 gene fragments showed that all 16 _G. destructans_ isolates had identical nucleotide sequences at all 8 sequenced gene fragments but were distinct from _G. pannorum_ sequences. A maximum-parsimony tree generated from the 8 concatenated gene fragments indicated a single, clonal genotype for the 16 _G. destructans_ strains ( Figure 1 ). This consensus tree included 4,470 aligned nucleotides from all targeted gene sequences with 545 variable sites that separate the _G. destructans_ clonal genotype from _G. pannorum_ . Further analyses of the same concatenated gene fragments with exclusion of 50 insertions and deletions between _G. destructans_ and _G. pannorum_ yielded a tree with a shorter length (495 steps instead of 545 steps) but an identical topology ( Technical Appendix Figure 1 ). This pattern remained unchanged when different phylogenetics models were used for analysis ( Technical Appendix Figure 2 ). The lack of polymorphism among the 16 _G. destructans_ isolates was unlikely because of evolutionary constraint at the sequenced gene fragments. We found many synonymous and nonsynonymous substitutions in target genes among a diversity of fungal species, including between _G. destructans_ and _G. pannorum_ ( 10 ) ( Technical Appendix Figure 3 ).\n\n【7】### Conclusions\n\n【8】Figure 2\n\n【9】Figure 2 . Collection sites in New York counties (A) are color-matched with respective _Geomyces destructans_ isolates in maximum-parsimony tree based on nucleotide sequence of the VPS13 gene (B). The tree was constructed with...\n\n【10】Our finding of a single clonal genotype in _G. destructans_ population fits well with the rapid spread of geomycosis in New York ( Figure 2 ). Our sampling population covered both spatial and temporal dimensions, and the numbers of isolates analyzed were adequate in view of difficulties encountered in obtaining pure isolations of _G. destructans_ ( 11 ). Although the affected New York sites are separated by sizable distances and include geographic barriers, a role for the natural dissemination of the fungus through air, soil, and water cannot be ruled out. Indeed, several fungi with geographic distributions similar to that in our study have shown major genetic variation among strains ( 12 , 13 ). It is also possible that humans and/or animals contributed to the rapid clonal dispersal. In such a scenario, the diseased or asymptomatic bats might act as carriers of the fungus by their migration into new hibernation sites where new animals get infected and the dissemination cycle continues ( 4 ). Similarly, the likely roles played by humans and/or other animals in the transfer of the fungal propagules from an affected site to a clean one cannot be ruled out from our data.\n\n【11】Virulent clones of human and plant pathogenic fungi that spread rapidly among affected populations have been recognized with increasing frequency in recent years ( 12 , 14 ). However, other pathogens, such as the frog-killing fungus _Batrachochytrium dendrobatidis_ , have emerged with both clonal and recombining populations ( 13 ). Our data do not eliminate the possibility that the _G. destructans_ population undergoes recombination in nature. This process to generate genetic variability would require some form of sexual reproduction, which remains unknown in _G. destructans._ In addition, the fungus might have both asexual and sexual modes in its saprobic life elsewhere in nature, but it exists only in asexual mode on bats ( 15 ).\n\n【12】In conclusion, our data suggest that a single clonal genotype of _G. destructans_ has spread among affected bats in New York. This finding might be helpful for the professionals involved in devising control measures. Many outstanding questions remain about the origin of _G. destructans_ , its migration, and reproduction, all of which will require concerted efforts if we are to save bats from predicted extinction ( _3_ ).\n\n【13】Dr Rajkumar is a postdoctoral research affiliate in the Mycology Laboratory at the Wadsworth Center, New York State Department of Health, Albany, New York, USA. His research interests are molecular genetics, genomics, and antifungal drugs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "34cfbf64-9887-4dc5-88ce-0f01c1d8a336", "title": "Use and Effectiveness of Quitlines for Smokers With Diabetes: Cessation and Weight Outcomes, Washington State Tobacco Quit Line, 2008", "text": "【0】Volume 10 — June 27, 2013\n\n【1】### Article Tools\n\n【2】*   PDF - 270 KB\n*   Email\n*   Print\n*   Download citation\n*   Este resumen en español\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Use and Effectiveness of Quitlines for Smokers With Diabetes: Cessation and Weight Outcomes, Washington State Tobacco Quit Line, 2008\n=====================================================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Gillian L. Schauer, MPH; Terry Bush, PhD; Barbara Cerutti; Lisa Mahoney, MS; Juliet R. Thompson; Susan M. Zbikowski, PhD\n\n【8】_Suggested citation for this article:_ Schauer GL, Bush T, Cerutti B, Mahoney L, Thompson JR, Zbikowski SM. Use and Effectiveness of Quitlines for Smokers With Diabetes: Cessation and Weight Outcomes, Washington State Tobacco Quit Line, 2008. Prev Chronic Dis 2013;10:120324. DOI: http://dx.doi.org/10.5888/pcd10.120324 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Introduction**  \nHaving diabetes and smoking increases the risk of morbidity and mortality. However, cessation-related weight gain, a common side effect during quitting, can further complicate diabetes. Evidence-based telephone quitlines can support quitting but have not been studied adequately in populations with chronic diseases such as diabetes. The purpose of this study was to evaluate the use and effectiveness of a tobacco quitline among tobacco users with diabetes. Cessation-related weight concerns and weight gain were also assessed.\n\n【12】**Methods**  \nWe administered a telephone-based follow-up survey to tobacco users with and without diabetes 7 months after their enrollment in a quitline. We collected and analyzed data on demographics, tobacco use, dieting, weight concern, quitting success (7- and 30-day point prevalence), and weight gain. We computed summary statistics for descriptive data, χ <sup>2 </sup> and _t_ tests for bivariate comparisons, and multivariable analyses to determine correlates of cessation.\n\n【13】**Results**  \nTobacco users with diabetes used the quitline in a greater proportion than they were represented in the general population. Quit rates for those with and without diabetes did not differ significantly (24.3% vs 22.5%). No significant differences existed between groups for weight gain at follow-up, regardless of quit status. However, participants with diabetes reported more weight gain in previous quit attempts (34.2% vs 22.4% gained >20 lbs, _P_ \\= .03). Weight concern was a significant correlate of continued smoking, regardless of diabetes status.\n\n【14】**Conclusions**  \nResults suggest that quitlines are effective for participants with diabetes, but tailored interventions that address weight concerns during cessation are needed.\n\n【15】Top of Page\n\n【16】Introduction\n------------\n\n【17】People with diabetes smoke at the same rate as people in the general population, despite excess health morbidity and mortality (1,2). In people with diabetes, cigarette smoking increases the risk of macrovascular complications, including circulatory, cardiovascular, and coronary heart disease (3,4), and microvascular complications, including kidney disease (5). Quitting smoking is essential to reduce the onset and exacerbation of diabetes (6).\n\n【18】However, weight gain is a potential barrier to successful quitting. It is common for people to gain 10 to 15 pounds after quitting (7,8) and those who are heavy smokers or are already overweight can gain considerably more (7,8). People with diabetes are more likely than those without diabetes to be overweight or obese (9), so excessive weight gain during cessation can present a substantial health hazard (10). In addition to actual weight gain, concern about gaining weight after quitting is common (11,12), and both hinder cessation (13,14). Although weight gain and weight concern are important factors in the initiation and continuation of smoking, few data exist on the effect of weight-related issues on cessation in tobacco users with diabetes.\n\n【19】Numerous evidence-based resources exist to support successful quitting, including pharmacotherapy and counseling provided through state tobacco quitlines (15). Toll-free, telephone-based tobacco quitlines are 1 of the most cost-effective treatment resources, yet even among the general population, they typically reach only 1% to 3% of smokers nationwide (16). Formative research suggests that people with diabetes have low awareness and use of effective cessation treatments including medication and quitlines (17). Although most cessation research has focused on the general population or related disease groups such as cardiovascular or lung disease (4), no published research has evaluated the use of a tobacco cessation quitline in a population of smokers with diabetes.\n\n【20】The primary purpose of this study was to describe the reach and effectiveness of a tobacco quitline among a sample of people with diabetes compared with those without the disease. A secondary aim was to assess the impact that concerns about weight gain have on quitting success.\n\n【21】Top of Page\n\n【22】Methods\n-------\n\n【23】### Sample\n\n【24】Participants for this study were tobacco users aged 18 or older who called the Washington State Tobacco Quit Line (the quitline) from May through September 2008. For this cross-sectional study, we surveyed a census of tobacco users who reported a diagnosis of diabetes at the time of quitline registration and met other eligibility criteria and compared them with a matched group who registered for quitline services during the same period but did not report having diabetes. To control for differences that might affect quit rates (the dependent variable), we matched participants by sex, insurance status, and smoking dependence. Only 1 participant per household was included in the study; pregnant women were excluded because of the possibility of a pregnancy-related diabetes diagnosis.\n\n【25】At the time of data collection, the quitline offered free cessation services to any state resident aged 18 or older, including an initial counseling call of up to 30 minutes, self-management materials mailed to the caller, provision of nicotine patch or gum medication (if indicated), and referral to community-based cessation resources. The number of follow-up counseling calls (1 call vs up to 5 calls) and the amount of medication (2 vs 8 weeks) varied by population; uninsured, Medicaid-insured, and those referred from Veterans Affairs and Indian Health Services received the most intensive treatment options. Data on the Medicaid-insured population were available only for May and June because the state began offering separate Medicaid-only quitline services in July 2008. However, the separate Medicaid-only quitline services were available only for Medicaid fee-for-service participants. Other Medicaid participants may have been eligible for quitline services (and thus study recruitment) during the study period. This study was conducted by Alere Wellbeing in collaboration with the Washington State Department of Health with approval from the Western Institutional Review Board.\n\n【26】Data came from 1) information collected via telephone at the time of quitline registration, 2) a 7-month follow-up telephone survey, 3) automated process data collected at Alere Wellbeing (eg, number of counseling calls completed) and 4) comparison data from the 2008 Washington State Behavioral Risk Factor Surveillance System (BRFSS) survey. The 7-month follow-up telephone survey was administered from December 2008 through April 2009 by trained survey staff. To increase the survey response rate, a prenotification letter was mailed to participants about 10 days before survey administration. Participants were also offered a $20 gift card for completing the survey. If the interviewer could not reach a participant after 11 attempts, the survey was considered unanswered.\n\n【27】### Measures and definition of concepts\n\n【28】Quitline registration data included participant demographics, tobacco use and cessation history, stage of readiness to quit, and prior use of pharmacotherapy. Chronic disease status was assessed by asking “Have you been diagnosed with any of the following chronic conditions: Asthma? Chronic obstructive pulmonary disease or emphysema? Diabetes? Heart disease?” Diabetes status was further clarified by asking participants if they had ever been told by a doctor that they had diabetes, a core question from the Centers for Disease Control and Prevention’s (CDC’s) 2007 Behavioral Risk Factor Surveillance System Survey (http://www.cdc.gov/brfss/). Participants who were prediabetic or were diagnosed with diabetes only during pregnancy were excluded from analyses.\n\n【29】Seven-day and 30-day tobacco point-prevalence quit rates were based on a respondent’s self-report of being tobacco-free for the last 7 days or more, or 30 days or more at the time of the 7-month survey (18). Abstinence rates were computed by using both the responder and intent-to-treat methodology. Among continued smokers, intention to quit and reduction in amount of cigarettes smoked were also assessed.\n\n【30】Quitline use among tobacco users with diabetes was measured by a “reach effect ratio” based on the proportion of smokers aged ≥18) with diabetes who enrolled in quitline services in 2008 divided by the proportion of smokers with diabetes in the state (19). A reach ratio of 1.0 indicates that the quitline reaches the subgroup proportionally to its distribution in the smoking population in that state; ratios less than 1 indicate lower reach and greater than 1 indicate higher reach.\n\n【31】Self-reported height and weight, physical activity, dieting, level of concern about gaining weight, perceived risk for relapse if they were to gain weight, prior weight gain due to quitting, perceived weight and change in weight, and postcessation weight change (among those who quit) were also measured.\n\n【32】Self-reported depression was assessed by using the Patient Health Questionnaire-2 (20): “Over the last 2 weeks how often have you been bothered by the following problems? 1- Having little interest or pleasure in doing things? 2- Feeling down, depressed or hopeless?” A mean score on the 2 items was computed, with the recommended cut point of 3 or more describing clinically significant depression (20). The mean score on 1 item was used to determine self-reported anxiety symptoms: “Over the last 2 weeks how often have you been bothered by: feeling nervous, anxious, or on the edge?” Panic was assessed with a yes/no question: “During the past 2 weeks did you have any episodes of panic or fear?”\n\n【33】### Statistical analysis\n\n【34】Summary statistics were computed for descriptive data. Those who completed the survey were compared with those who did not to assess differences in individual characteristics. Chi-square and _t_ tests were used for bivariate comparisons between groups. We conducted multiple logistic regression analyses by using SAS version 9.2 (SAS Institute Inc, Cary, North Carolina) to test for independent associations between the 2 groups in quit rates after controlling for demographic and tobacco use characteristics. Correlates of quitting were identified for the total sample and for the subsample with diabetes. Despite matching, sex was included in the multivariable analyses to control for possible selection bias (21), since both diabetes status (the key independent variable) and tobacco abstinence (the outcome variable) have been found to differ by sex (22). Less than 10% of responses were “do not know” or missing. These were omitted from the analyses. Significance was set at α =.05 for all analyses.\n\n【35】Top of Page\n\n【36】Results\n-------\n\n【37】### Quitline use\n\n【38】According to the weighted prevalence rates from the 2008 BRFSS in Washington State, 15.7% (95% confidence interval \\[CI\\] = 15.0–16.4) of residents aged 18 or older smoked. No statistical difference in diabetes prevalence was found between adults who smoked and the general adult population in Washington (6.1% vs 6.0%, excluding those diagnosed only during pregnancy). Quitline registrations from 2008 showed that 8.3% (n = 1,077) of all adults who registered for quitline services reported a diagnosis of diabetes. This represents a quitline reach effect ratio of 1.36 (8.3%/6.1%), indicating that in Washington State, smokers with diabetes used the quitline in a higher proportion than they were represented in the general population of smokers.\n\n【39】### Sample characteristics\n\n【40】Six hundred eligible tobacco users who registered for services with the quitline between May 1, 2008, and September 30, 2008, were eligible to participate in the 7-month survey (261 participants with diabetes and 339 participants without diabetes) (Figure). After launching the survey, we learned that approximately 19% of identified participants had either disconnected or wrong telephone numbers. The survey response rate was 40.3%, yielding a final sample of 242 participants (111 with diabetes, 131 without diabetes). Few differences existed between respondents and nonrespondents. Participants with diabetes were just as likely to complete the 7-month survey as those without diabetes ( _P_ \\= .34). Compared to nonrespondents, survey respondents were older (47.5 years vs 42.6 years; _P_ < .001), less likely to be uninsured (10.7% vs 20.4%; _P_ < .003), and more likely to have smoked for 20 years or more (75.6% vs 64.5%, _P_ < .01).\n\n【41】**Figure.** Recruitment process for survey of eligible tobacco users who sought help from the Washington State Quit Line, May 2008–September 30, 2008. \\[A text description of this figure is also available.\\]\n\n【42】Respondents with diabetes were similar to those without diabetes in sex, race/ethnicity, education, insurance status, and current level of anxiety or panic ( Table 1 ). Those with diabetes were older and more likely to report having significant depressive symptoms. Of the smokers with diabetes, 13% reported a diagnosis of type 1 diabetes, 78.9% reported a diagnosis of type 2 diabetes, and 8.2% reported they did not know. Both groups reported that the primary reason for calling the quitline was their desire or need to quit smoking, followed by advice to quit by family or health professionals.\n\n【43】Participants with diabetes did not differ from those without diabetes on the type of counseling call program (one call vs multiple calls); 41% of participants with diabetes and 45% of those without the disease enrolled in the multiple proactive counseling call program (up to 5 proactive counseling calls). No significant difference was found between groups in their satisfaction with quitline services or with their use of or satisfaction with quitline-provided nicotine replacement therapy (NRT). Approximately 68% of quitline users with diabetes used NRT compared with about 76% quitline users without diabetes.\n\n【44】### Tobacco use characteristics\n\n【45】Most participants reported smoking cigarettes rather than other forms of tobacco (Table 1). Most were heavy, established smokers and more than half reported having their first cigarette of the day within 5 minutes of waking. Many also reported having other chronic diseases such as asthma, chronic obstructive pulmonary disease, or heart disease. Participants with diabetes reported smoking for significantly more years than those in the comparison group (86.2% vs 66.0% reported ≥20 years of smoking, _P_ < .001) (Table 1).\n\n【46】### Weight concern characteristics\n\n【47】Compared with participants without diabetes, participants with the disease were significantly more likely to be obese (56.9% vs 35.8% with BMI >30, _P_ < .001) and to rate themselves as being very overweight (29.6% vs 12.7%, _P_ < .001) (Table 1). Participants with diabetes did not differ from those without diabetes in dieting behaviors or physical activity levels. Although no differences in weight concern or perceived risk of relapse due to weight gain after quitting existed between participants with and without diabetes, nearly two-thirds of participants with diabetes were worried about possible weight gain, although less than one-third thought they would return to smoking if they gained excessive weight after cessation.\n\n【48】### Tobacco cessation outcomes\n\n【49】In bivariate analyses, participants with diabetes reported quit rates that were similar to those of participants without diabetes (24.3% vs 22.5%, 30-day respondent quit rate; _P_ \\= .73) ( Table 2 ). A separate analysis that further segmented those without diabetes by other chronic disease status (with vs without other chronic disease) found no significant difference in quit rates (diabetes group, 24.3%; group with other chronic disease, 26.5%; group without chronic disease, 20.0%; _P_ \\= .84).\n\n【50】However, participants with diabetes reported more weight gain in prior quit attempts than those without diabetes (34.2% vs 22.2% reported gaining >20 lbs; _P_ < .05). Participants with diabetes who were successful in quitting smoking through the quitline were more likely to report weight gain than those who were successful and did not have diabetes, although results were not significant ( _P_ \\= .43). Amount of weight gained was also higher but not significantly so (23.2 lbs vs 14.7 lbs; _P_ \\= .20) (Table 2).\n\n【51】Consistent with the bivariate results, multivariable results revealed that participants with diabetes had similar odds of reporting tobacco abstinence at either 7 or 30 days compared with those without diabetes (OR, 0.8; 95% CI, 0.38–1.55 and OR, 0.7, 95% CI, 0.36–1.55, respectively) after adjusting for BMI, age, sex, NRT use, weight concern, and mean depression score ( Table 3 ).\n\n【52】The only correlate of 7-day and 30-day tobacco abstinence among the total sample was weight concern; those who expressed greater perceived risk of relapse due to weight gain were less likely to report abstinence (7-day OR, 0.8; 95% CI, 0.65–0.88; 30-day OR, 0.8; 95% CI, 0.67–0.91). However, when analysis was restricted to the subsample with diabetes, weight concern was not a significant predictor of quitting (OR, 0.8; 95% CI, 0.66–1.03; data not shown).\n\n【53】Top of Page\n\n【54】Discussion\n----------\n\n【55】Although tobacco quitline use is low among the general population (16), results from this study indicate that the proportion of tobacco users with diabetes who used the quitline was greater than their representation in the general statewide population. This finding is important, given that diabetes prevalence is increasing nationally and smoking is a behavioral risk factor that contributes both to causing and complicating the disease (23). Higher proportional use of the quitline among those with diabetes as compared with the general population may be the result of a concerted effort by the Washington State Department of Health to integrate cessation treatment referrals into state chronic disease systems (24). Another factor may be that diagnosis of a chronic condition, especially one related to smoking (eg, diabetes), has been shown to create an impetus to quit smoking (25).\n\n【56】In addition to use, a major finding of this study is that having diabetes did not affect quit rates, even after adjusting for individual differences such as age, sex, weight concern, depression, use of NRT, and BMI. Furthermore, participants with diabetes had similar odds of reporting tobacco abstinence compared with those without diabetes after adjusting for the existence of other chronic diseases, supporting evidence that chronic disease does not hinder successful cessation (25).\n\n【57】Although participants with diabetes were older, had smoked longer, and were more likely to be depressed than those without diabetes, these factors did not decrease their likelihood of quitting. This may be because people with diabetes typically have high health care use, which could mean increased contact with the health care system (26). It may also be due to increased pressure from providers to quit.\n\n【58】The perceived risk of relapse due to weight gain was associated with lower odds of reporting tobacco abstinence, even after controlling for differences in incidence of depression. Findings from this study indicated that participants with diabetes were no more likely than the comparison group to gain weight over time. However, among participants who reported successfully quitting smoking, those with diabetes were more likely to report that they gained weight than were quitters who did not have diabetes. This weight gain may be attributable to the increased prevalence of obesity and depression in the diabetes arm. People with diabetes have been shown to have higher rates of depression (27), and depression and diabetes have both been associated with reduced self-care, including nonadherence to diet, exercise, and medication (28). Adding smoking cessation to the equation may further increase weight gain.\n\n【59】Although depression can impede successful quitting (29), those with diabetes were able to quit smoking at same level as the comparison group, despite having elevated levels of depression. These findings suggest that depression may not directly impact successful quitting in those with diabetes. However, since those with and without diabetes reported significantly different weight gain during prior quit attempts, the relationship between depression and quitting may be moderated by weight changes. Given that lower levels of weight concern were a significant predictor of successful quitting in this study, a full mediation and moderation analysis is warranted to clarify the relationship between depression, weight gain, weight concern, and quitting. When taken together, these weight-related findings suggest that greater efforts are needed to address weight gain in cessation treatment.\n\n【60】Numerous limitations should be considered when interpreting these results. First, results may not be generalizable to people outside of Washington State or to people who attempt to quit smoking through other treatment resources. Second, because the purpose of the study was to evaluate differences in use and quit rates, we matched by sex, insurance status, and cigarettes per day, thus limiting our ability to assess differences in these variables. Third, although only 6% to 10% of the initial sample refused to participate in the survey, the additional 26% to 33% who were located but not reached after 11 attempts might be considered passive refusals. Because of the low contact rate, it should also be noted that responder quit rates are tentative. Although the response rate for this study is lower than that of many clinical trials, it is comparable to completion rates from other telephone-based studies of quitline participants (12). Fourth, Medicaid fee-for-service data were not available for the entire study period. Although there is no reason to believe that the inclusion of additional Medicaid data would have changed these results, this is a limitation of the current study. Finally, diabetes diagnoses were self-reported and may underestimate the true population of quitline participants who have diabetes.\n\n【61】Despite these potential limitations, these data make an important contribution to the literature, suggesting that people with diabetes use the quitline and succeed at quitting at rates equal to those without diabetes, but that quit rates may be further enhanced by addressing weight concern and weight gain.\n\n【62】Top of Page\n\n【63】Acknowledgments\n---------------\n\n【64】We thank Marcelle Thurston and Jeanne Harmon from the Washington State Diabetes Prevention and Control Program for their assistance in the survey design. None of the authors (Ms Schauer, Dr Bush, Ms Cerutti, Ms Mahoney, Ms Thompson, and Dr Zbikowski) have any conflicts of interest to disclose or cite. This study was approved by the Western Institutional Review Board and was funded by the Washington State Department of Health Tobacco Prevention and Control Program.\n\n【65】Top of Page\n\n【66】Author Information\n------------------\n\n【67】Corresponding Author: Gillian L. Schauer, MPH, Department of Behavioral Sciences and Health Education, Emory University, 1518 Clifton Rd NE, GCR 430, Atlanta, GA 30322. Telephone: 206-819-9391. E-mail: gillian.schauer@emory.edu .\n\n【68】Author Affiliations: Terry Bush, Lisa Mahoney, Susan M. Zbikowski, Alere Wellbeing, Clinical and Behavioral Sciences, Seattle, Washington; Barbara Cerutti, Imperial College London and Imperial College Healthcare National Health Service Trust, London, UK; Juliet R. Thompson, Washington State Department of Health, Tobacco Prevention and Control Program, Tumwater, Washington.\n\n【69】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8cd89ab1-6f6e-44a0-b51e-1158869f827d", "title": "Pulmonary Nocardia ignorata Infection in Gardener, Iran, 2017", "text": "【0】Pulmonary Nocardia ignorata Infection in Gardener, Iran, 2017\nSince the description of _Nocardia ignorata_ in 2001 ( _1_ ), 4 respiratory isolates in Europe and 3 corneal isolates in India have been reported ( _2_ , _3_ ). Among the respiratory isolates, only 2 were confirmed as the cause of disease ( _2_ ). We report a case of pulmonary nocardiosis that was caused by _Nocardia ignorata_ in a person from Iran.\n\n【1】The patient, a 55-year-old man, smoked, had multiple myeloma, and had a history of opium use for 3 years. He was consuming methadone syrup daily at the time of hospitalization but had no history of corticosteroid consumption. He had frequent exposure to dust because of working in a garden.\n\n【2】The recent course of disease began ≈4 months earlier, initially as a feeling of heaviness and pain in the anterior chest (no clear distribution) and shortness of breath (functional class II level). However, coronary disease was rejected as a possible diagnosis during examination. The patient had night sweats and a weight loss of 5% in the past 4 months.\n\n【3】Two months after initial symptoms began, he had had fever, chills, and gradually productive coughs. During a visit to a different hospital, he was given a diagnosis of bacterial pneumonia and received azithromycin and cefixime, which initially reduced the symptoms but did not completely resolve them. When he sought care at Imam Khomeini Hospital (Tehran, Iran) because he had symptoms including cramping abdominal pain, fever, and shortness of breath 4–7 days before the visit, he was first admitted to the emergency department and then transferred to the lung unit.\n\n【4】Vital signs of the patient at the time of hospitalization were temperature 37.6°C, blood pressure 135/75 mm Hg, heart rate 80 beats/min, respiratory rate 18 breaths/min, and O <sub>2 </sub> saturation 90%. Laboratory findings were increased leukocyte count (12,210 cells/mm <sup>3 </sup> with 68.8% neutrophils) and C-reactive protein level 65 mg/L (reference value <5 mg/L); thyroid function test results were within reference ranges. Examination of abdominal organs did not show organomegaly or any other indications of disease; prostate size and consistency were unremarkable by testicular and digital rectal examinations. Heart sounds were unremarkable, and cervical, axillary, and extracorporeal lymph nodes were also unremarkable. Examination of the lungs showed fine end inspiratory crackles in both lungs.\n\n【5】Chest radiograph showed a mediastinal mass from the periphery of the right main bronchus to the medial and lower branches and an extension to the posterior part (vertebral trunk and adjacent to the esophagus). The maximum size of the mass was 31 × 74 mm, and an air bronchogram showed opacities near the mass, suggesting postobstructive pneumonia.\n\n【6】The patient was given empirical pneumonia treatment with levofloxacin (750 mg/d) and ceftriaxone (1 g, 2×/d) for 8 days. For the mediastinal mass, the patient underwent endoscopic ultrasonography and bronchoscopy, but his symptoms did not resolve. In a spiral computed tomography scan of the thorax, we found patchy ground glass and nodular opacities that were spread in both lungs, especially in posterior and lateral segments of the left lower lobe, the posterior segment of the right lower lobe, lingular, and the right upper lobe. Thus, nocardiosis and tuberculosis examinations were recommended.\n\n【7】We performed a tuberculin skin test, and the intraderm response to tuberculin was absent. Moreover, mucosal erythema with inflammation were seen on a right lung bronchoscopic examination. We found abundant mucoid secretions, no endobronchial ulcers, and a white discharge without endobronchial ulcer in the left lung.\n\n【8】We then subjected a bronchoalveolar lavage specimen for microbiological cultures. No _Mycobacterium_ growth was seen, but colonies suspected of being _Nocardia_ species appeared on blood agar plates after 4 days of incubation. A modified Kinyoun acid-fast stain confirmed the partial acid-fast staining feature of the colonies.\n\n【9】We performed DNA extraction by using the modified Pitcher method ( _4_ _–_ _6_ ) and a _Nocardia_ \\-specific PCR with primers NG1 (5′-ACCGACCACAAGGGGG-3′) and NG2 (5′-GGTTGTAAACCTCTTTCGA-3′). We obtained positive results with this PCR and a resistance-to-lysozyme test, and confirmed the presence of _Nocardia_ species. For accurate identification, we PCR amplified 16S rRNA and partial DNA gyrase B ( _gyrB_ ) genes ( _4_ ) and subjected PCR products to direct sequencing (GenBank accession nos. KY817987.1 for 16S rRNA and MN159177 for _gyrB_ ).\n\n【10】We performed a BLAST analysis ( https://blast.ncbi.nlm.nih.gov/Blast.cgi ) by using the megablast algorithm. This analysis confirmed the identity as _N. ignorata_ (16S rRNA coverage 100% and 99% identity with GenBank accession no. KM113026.1; _gyrB_ coverage 100% and 100% identity with GenBank accession no. GQ496109.1).\n\n【11】Antimicrobial drug susceptibility testing (broth microdilution method) showed that the isolate was susceptible to trimethoprim/sulfamethoxazole, imipenem, amikacin, doxycycline, and linezolid and resistant to erythromycin, ceftriaxone, and ciprofloxacin. At this point, we gave the patient cotrimoxazole and imipenem for 20 days and then trimethoprim/sulfamethoxazole for 6 months. The clinical, biological, and radiologic outcomes of the treatment were optimal, and no recurrence occurred after 8 months.\n\n【12】_N. ignorata_ is a rare human pathogen. However, soil samples have been described as a possible reservoir ( _2_ , _7_ , _8_ ). Accordingly, this patient, who had frequent exposure to soil through his work as a gardener, might have acquired the infection from that source.\n\n【13】Dr. Rahdar is an assistant professor in the Department of Microbiology, School of Medicine, Iranshahr University of Medical Sciences, Iranshahr, Iran. His research interests are rare bacterial pathogens and antimicrobial resistance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "17f63da7-0657-43eb-aac1-2a7582022b1e", "title": "PCD’s First Annual Student Research Contest: Lui and Wallace Examine Hospitalization Rates for At-Risk Populations", "text": "PCD’s First Annual Student Research Contest: Lui and Wallace Examine Hospitalization Rates for At-Risk Populations\n|  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n\n| \n<table><tbody><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>View Current Issue</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>Issue Archive</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><span><a>Archivo de nmeros en espaol</a></span></th></tr></tbody></table></th></tr><tr><th><img></th></tr></tbody></table>\n\n【2】<table><tbody><tr><th><img><br></th></tr><tr><th><form><label>Search <i>PCD</i></label><br><img><br><input> <input> <input><br><img><br><input><br><img><br></form></th></tr></tbody></table><table><tbody><tr><th><b><a>Emerging Infectious Diseases Journal</a></b></th></tr><tr><th><b><a>MMWR</a></b></th></tr></tbody></table>\n\n |  | <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Volume 8: No. 5, September 2011 \n\n【4】COMMENTARY_PCD_ s First Annual Student Research Contest: Lui and Wallace Examine Hospitalization Rates for At-Risk Populations\n==============================================================================================================================\n\n【5】<table><tbody><tr><th><img><br></th><th><table><tbody><tr><th><a>TABLE OF CONTENTS</a></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><img></th><th><a>Print this article</a></th></tr><tr><th><img></th><th>E-mail this article:<br><img><br><input><br><img><br><input> </th></tr><tr><th><img></th><th><a>Send feedback to editors</a></th></tr><tr><th><img></th><th><a>Syndicate this content</a></th></tr><tr><th><img></th><th><a>Download this article as a PDF </a>(240K)</th></tr><tr><th><hr>You will need <a>Adobe Acrobat Reader </a>to view PDF files.</th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><b>Navigate This Article</b></th></tr><tr><th></th><th><a>Author Information</a></th></tr><tr><th><img><br></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr></tbody></table>\n\n【6】#### Samuel F. Posner, PhD\n\n【7】_Suggested citation for this article:_ Posner SF. _PCD_ s first annual Student Research Contest: Lui and Wallace examine hospitalization rates for at-risk populations. Prev Chronic Dis 2011;8(5):A103. http://www.cdc.gov/pcd/issues/2011/sep/11\\_0172.htm . Accessed \\[ _date_ \\]. I am pleased to announce that A Common Denominator: Calculating Hospitalization Rates for Ambulatory CareSensitive Conditions in California by Camillia K. Lui and Steven P. Wallace is the winner of the first annual _Preventing Chronic Disease (PCD)_ Student Research Contest. Ms Lui is a fourth-year doctoral student at the University of California, Los Angeles, in the Department of Community Health Sciences. Her advisor is Dr Steven Wallace. _PCD_ is dedicated to being the venue for sharing advances in public health research, practice, and policy, and we are committed to the development of young public health professionals as part of this effort. To this end, we have instituted the Student Research Contest as a way to engage students in the publication process and recognize the outstanding work of the next generation of the public health workforce. In July 2010, we announced our first annual call for student papers and reached out to multiple partners to distribute the call and encourage students to submit their work to _PCD_ . Papers were due in January 2011, and we received submissions on a range of topics from institutions throughout the United States. In February and March, a small team of editorial board members (Drs Bowman, Brownson, Lengerich, and Remington), _PCD_ s founding editor (Dr Lynne Wilcox), and I reviewed the submitted manuscripts. In March we selected the paper by Ms Lui and Dr Wallace as the winner. Lui and Wallace examined the prevalence, hospitalization rates, and geographic variability of hypertension and congestive heart failure, 2 chronic health conditions that are considered to be manageable with effective outpatient treatment (ie, ambulatory caresensitive conditions), in California. Their analysis makes use of 2 large datasets, the California Health Interview Survey (www.chis.ucla.edu/) and hospital patient discharge files of the California Office of Statewide Health Planning and Development (www.oshpd.ca.gov/). This analysis is important to health care resource planning because it uses the population at risk rather than the total population in calculating hospitalization rates. With these 2 common conditions, 2 different scenarios emerged. In the case of hypertension, approximately 74% of the geographic areas did not change in ranked quintile when comparing age and age/disease prevalence rates, which suggests that resources are meeting the needs of the at-risk population. However, in the case of congestive heart failure, 31 of the 55 geographic areas in California changed quintile rank — approximately 72% by 2 or more ranks. In this case, the geographic distribution of the population at risk does not mirror that of the general population. This finding has implications for planning and targeting public health programs and health care services in areas where the population at risk resides. As with any analysis that uses administrative and self-reported data, this study has limitations, and the results do not identify the one area that should be changed to inform policy and programs to reduce health care spending and rates of illness and death. This analysis is thoughtful and identifies several issues that need further investigation. What are the reasons for increased rates of age/risk-adjusted hospitalizations in some geographic areas? Are these observed differences a function of demographic, economic, geographic, or access factors? What are the interventions needed to ensure that people receive adequate and appropriate care? This analysis demonstrates the need to consider disease prevalence when examining hospitalization rates and that, depending on the condition, resources may need to be redistributed. <table><tbody><tr><th><img> <b><a>Listen to an interview with Camillia Lui </a></b>, winner of the inaugural Preventing Chronic Disease Student Paper Competition. (MP3 1.07Mb)<br><span><b>Description: </b>Windows Media Player is a program for viewing and listening to audio and video files (including live broadcasts).<br><b>File extensions: </b>.asf, .asx, .avi, .wma, .wmv, .wm, .wpl, .mpg, .mpeg, .mp3, .mp4<br><b>Viewing/listening: </b>If you do not already have Windows Media Player, you can <a>download Windows Media Player for free </a>*.</span><br>A <a>transcript of this interview </a>is also available.</th></tr></tbody></table>Congratulations to Ms Lui on winning the first annual _PCD_ Student Research Contest. Several outstanding papers were submitted, and the decision was not an easy one. We thank all of the students who submitted papers for this contest and recognize the hard work that goes into publishing scientific work. Please listen to the short podcast with Ms Lui to hear her discuss her article. Manuscripts for the second annual _PCD_ Student Research Contest are now being accepted, and the winning manuscript will be published in 2012. Back to top \n\n【8】Author Information\n------------------\n\nSamuel F. Posner, PhD, Editor in Chief, _Preventing Chronic Disease_ , National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, Mailstop K-85, 4770 Buford Hwy, NE, Atlanta, GA 30341-3717. Telephone: 770-488-6398. E-mail: sposner@cdc.gov . Back to top  |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Privacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 <table><tbody><tr><th><a>Centers for Disease Control and Prevention</a><br><a>National Center for Chronic Disease Prevention and Health Promotion</a></th><th><img></th><th><img> <a>United States Department of<br>Health and Human Services</a></th></tr></tbody></table>  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "58a19603-0f67-4362-a279-f60a7fab59ab", "title": "In Vivo Observation of Cutaneous Larva Migrans by Fluorescence-Advanced Videodermatoscopy", "text": "【0】In Vivo Observation of Cutaneous Larva Migrans by Fluorescence-Advanced Videodermatoscopy\nDermoscopy alone is not useful for identifying cases of cutaneous larva migrans. Therefore, we conducted a study to introduce a simple and useful noninvasive method for clinical practice. Although still relatively unknown, this method provides more information than clinical procedures and simple dermatoscopy.\n\n【1】### The Study\n\n【2】We report the case of a 34-year-old man who came to our attention at the Dermatological Clinic of the University of Turin (Turin, Italy) after the appearance of an intensely itchy, erythematous-papular serpiginous lesion that was gradually increasing in size. It was located near the right groin. Our suspicion that it was cutaneous larva migrans was validated because the patient had returned from Thailand, a country to which this infestation is endemic, where he had gone for a seaside holiday.\n\n【3】Dermoscopy alone was not useful for identifying the cause in this case. Although the diagnosis of larva migrans is usually clinical, to obtain a diagnosis of certainty by using a biopsy specimen of suspected larva, we used fluorescence-advanced videodermatoscopy to identify its precise position ( _1_ ).\n\n【4】Fluorescence-advanced videodermatoscopy is an optical electronic system that uses a monochromatic light-emitting source with an a mean ± SD λ of 405 ± 5 nm and a field of view of 340 μm to examine the skin. This system uses the ability of endogenous molecules to absorb specific wavelengths and emit fluorescence. The examination is conducted in vivo, and the optical device is directly applied to the skin by using liquid paraffin oil as an interface. The images are visualized in real time by using greyscale to indicate the levels of light absorption (i.e., black indicates no fluorescence and white indicates the highest fluorescence) ( _2_ ).\n\n【5】Figure\n\n【6】Figure . Imaging and biopsy results for patient with cutaneous larva migrans, Turin, Italy. A) Fluorescence-advanced videodermatoscopy showed larva with a diameter of 70–80 μm, located intraepidermally, ≈0.5 cm to the right...\n\n【7】In this instance, by positioning the probe on the surrounding skin downstream of the distal end of the serpiginous path, within an area of 0.5 cm, we were able to recognize an oval-shaped figure with a rounded tip, a darker gray color than the adjacent tissue (undamaged skin), and a white linear outline of the larva. ( Figure , panel A). Once the actual position of the larva was recognized, a 4-mm punch biopsy specimen was obtained and analyzed microscopically; we obtained subsequent histologic confirmation. ( Figure , panel B).\n\n【8】We prescribed a 7-day therapy regimen with topical ivermectin, under occlusion. Complete resolution of the cutaneous lesion was obtained ( _3_ ). It was not essential to perform a biopsy for successful treatment; however, although oral ivermectin is the most recommended treatment, because of its side effects and the difficulty in obtaining this drug, the topical formulation is preferable and is equally effective as other localized forms of this drug.\n\n【9】### Conclusions\n\n【10】Cutaneous larva migrans (creeping eruption) is a cutaneous disease that manifests as an erythematous migrating linear or serpiginous tract because of penetration of a hookworm larva into the epidermis. The hookworms most frequently responsible for cutaneous larva migrans are the dog hookworms _Ancylostoma braziliense_ or _An. caninum._ These hookworms are found worldwide, predominantly in tropical and subtropical countries, such as those in Southeast Asia, Africa, South America, the Caribbean, Australia, and southeastern parts of the United States. A similar condition known as larva currens is caused by _Strongyloides stercoralis_ roundworms and should be considered in the differential diagnosis.\n\n【11】The life cycle of these nematodes includes male and female adult stages in the intestines of dogs and cats; eggs are passed in the feces of the host and are deposited in the soil (sandy beaches, sand boxes, under dwellings). Under favorable conditions, these eggs hatch and give rise to the infective larval phase. Humans, defined as dead-end hosts, might become infected when infective filariform larvae in soil penetrate the skin. Within a few days, an increased erythematous or vesiculobullous serpiginous track will appear, accompanied by intense pruritis at the site of larval penetration. Larvae migrate at a rate of several millimeters per day ( _4_ ), and lesions are ≈3 mm wide and might be up to 15–20 mm in length. The larva is usually located 1–2 cm ahead of the eruption ( _5_ ). Vesiculobullous lesions develop in ≈10% of cases ( _6_ ). In comparison, the rash of larva currens is typically pink, evanescent, and urticarial and might be linear, serpiginous, annular, arcuate, or plaque-like ( _7_ ). This rash usually appears on the buttocks and abdomen during the chronic autoinfective stage of strongyloidiasis.\n\n【12】The diagnosis of cutaneous larva migrans is based on clinical history and physical findings. Infected patients typically have a history of exposure to contaminated soil or sand (walking barefoot or lying on sand) and the characteristic serpiginous lesion on the skin. Nevertheless, a definitive diagnosis by biopsy specimen sampling is difficult to obtain because the precise position of the larva is unpredictable. To overcome this problem, flight theory was applied by identification of an uncertainty circle in which the possible position of the larva would be obtained by means of a mathematical formula. The distance D between the last observation point and the possible actual point would be obtained by multiplying 3 parameters: D = V × T × R, in which V is the speed of the larva (mm/d), T is the time elapsed between the last and possible actual observations, and R is a multiplying factor that takes into account the characteristics of the path ( _8_ ).\n\n【13】It would be useful to have a procedure or instrument that can identify in vivo the precise position of the larva to enable accurate biopsy sampling. Methods to identify the position of the larva downstream from the distal end of the serpiginous tract have used dermoscopy; features identified, including translucent, brown, structureless areas corresponding to larval bodies and red-dotted vessels corresponding to an empty burrow, have been reported ( _9_ ).\n\n【14】In conclusion, fluorescence-advanced videodermatoscopy, a simple-to-use method of noninvasive diagnosis, is not a widely used procedure. However, our results show that it appears useful for examining of skin that is apparently healthy or does not have specific clinical–dermoscopic parameters, especially in the context of parasitology, which enables immediate recognition of the etiologic agent ( _10_ ).\n\n【15】Dr. Ramondetta is a fourth-year resident in dermatology at the Dermatology Clinic, University of Turin, Turin, Italy. Her primary research interests are pediatric dermatology and digital dermoscopy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "011e062d-e1f4-4b3b-8048-f77e0f79da6b", "title": "Methicillin-Resistant Staphylococcus aureus at Canoe Camp", "text": "【0】Methicillin-Resistant Staphylococcus aureus at Canoe Camp\nIn 2000, the Minnesota Department of Health (MDH) initiated prospective active surveillance at 12 sentinel hospitals for community-associated methicillin-resistant Staphylococcus aureus (CA-MRSA) infections. Cases are classified as CA-MRSA if the patient lacks traditional healthcare-associated risk factors (hospitalization, surgery, dialysis, residence in a long-term care facility during the year before culture, or having an indwelling device at the time of culture). Although >80% of CA-MRSA infections involve skin or soft tissue, invasive disease that involves bones, joints, and sepsis can occur ( _1_ _,_ _2_ ). During 2000–2004, 2% of CA-MRSA cases reported to MDH were joint infections.\n\n【1】Outbreaks of CA-MRSA have been associated with sports that require physical contact and result in frequent damage to skin ( _3_ _,_ _4_ ) and with crowded settings (e.g., correctional facilities, military settings), where access to hygiene measures is limited ( _5_ _,_ _6_ ). In response to a case cluster of CA-MRSA infections at a wilderness canoe camp, MDH performed an investigation to identify additional cases; MRSA colonization among staff, campers, and household members; and risk factors for infection.\n\n【2】### The Study\n\n【3】In August 2004, MDH was notified of 2 previously healthy 15-year-old male patients who were hospitalized for septic arthritis caused by MRSA; 1 had required treatment in an intensive care unit. The patients had been in the same group that participated in a 21-day wilderness canoe trip in northern Minnesota, USA. A third case-patient, a previously healthy 17-year-old female camper who had participated in an earlier 21-day trip and had had a skin infection without complications, was identified during the investigation ( Table ).\n\n【4】A case was defined as a clinically relevant, culture-confirmed, MRSA infection that occurred between June 1 and September 30, 2004, in a staff member, camper, or member of a camper's household. Colonization was defined as MRSA identified from the nares of these persons in the absence of MRSA infection.\n\n【5】MDH and local public health staff investigated the base camp. During the 21-day canoeing trip, participants spent 1 day at base camp at the beginning and end of the trip; the remaining time was spent canoeing, portaging, and camping on the trail. Camping groups were composed of 5–6 campers and a guide. Campers were of the same sex and were 15–17 years old. Campers and guides shared a common tent and did all activities together, but each had his or her own sleeping bag and towels. Campers and guides did not have access to running water or soap outside of base camp; the information on the wilderness area permit discourages use of soap, including biodegradable soap, within 150 feet of water.\n\n【6】Campers who had participated in the same, concurrent, or preceding canoe trips with the same itinerary and their household members were interviewed regarding health history, skin infections and injury, hygiene, clothing, camping behavior, and contact with other campers. Swabs of anterior nares were obtained from consenting staff, campers, and members of campers' households and were submitted to the MDH Public Health Laboratory. Isolates were identified by standard methods. Susceptibility testing was conducted by broth microdilution (PML Microbiology, Wilsonville, OR, USA) and interpreted according to Clinical and Laboratory Standards Institute (formerly NCCLS) criteria. Pulsed-field gel electrophoresis (PFGE) was performed by digestion with the restriction enzyme SmaI ( _7_ ). Patterns were compared visually and with Bionumeric software (Applied Maths, Kortrijk, Belgium) to reference strains of S. aureus ( _8_ ). Sequences encoding toxin genes (TSST-1, Panton-Valentine leukocidin, sea, seb, sec, and sed) were detected by PCR ( _9_ _,_ _10_ ). Univariate analysis was conducted with EpiInfo version 6.04 (Centers for Disease Control and Prevention, Atlanta, GA, USA) using a significance level of p<0.05.\n\n【7】Of 21 campers, 19 (90%) were interviewed. The 2 case-patients had occasionally shared the same canoe. All campers reported having had frequent skin injuries, including insect bites, cuts, burns, blisters, and abrasions, during their trips. Campers reported having worn shorts most of the time and frequently having had wet skin, clothing, and shoes. No differences were identified between campers with and without CA-MRSA or between camping groups with and without CA-MRSA in terms of health history, use of antimicrobial drugs during the past year, skin infections and injury, hygiene, clothing, or camping behavior. Interviews were also conducted with 55 camper household members, representing 19 households. Members of 2 households could not be reached, and no information was obtained on the number of persons in these households. Interview responses of household members of case-patients did not differ from those of household members of the other campers and staff. No case-patient or colonized person reported a history of MRSA infection or colonization. One camper who was not a case-patient reported having had a positive blood culture for CA-MRSA in the prior year; this camper had been hospitalized with an infection that began as cellulitis and resulted in septic shock and was found to be not colonized at the time of the investigation. Also, the camp director informed MDH that in the prior year, 2 counselors had been treated for apparent spider bites at a local clinic; however, cultures had not been obtained.\n\n【8】Nares swabs were obtained for case-patients 2 and 3 but not case-patient 1 because he was receiving nasal mupirocin treatment. None of the case-patients was colonized with MRSA. Nares swabs were also obtained for 62% of campers without MRSA infection; of these, 1 female camper who had participated in a preceding trip was found to be colonized with MRSA. Swabs obtained from the anterior nares of 16 camp guides and base camp staff members showed MRSA colonization in 1. This staff member had not gone on any camping trips with the case-patients. Of nares swabs from 40 household members (including all 8 household members of case-patients), none was positive for MRSA.\n\n【9】All 5 MRSA isolates (from the 3 case-patients and the 2 colonized persons) were susceptible to clindamycin, ciprofloxacin, erythromycin, gentamicin, linezolid, quinopristin/dalfopristin, rifampin, tetracycline, trimethoprim/sulfamethoxazole, and vancomycin and had an MIC for mupirocin of <4 μg/mL. The isolates were USA400 pulsed-field type and were indistinguishable by PFGE ( _8_ ). The isolate from the camper who had had an MRSA infection in 2003 was also indistinguishable by PFGE. All 2004 isolates contained genes encoding Panton-Valentine leukocidin, sea, and sec ( _10_ ).\n\n【10】To prevent further infections, camp staff were given information about S. aureus, MRSA, and infection-prevention measures. Campers were instructed to wear long pants when possible, protect skin from injury (e.g., use insect repellent), report all suspected skin infections promptly, keep skin as clean as possible, use alcohol-based hand sanitizers, and not share personal items like towels and clothing. Frequently touched surfaces were cleaned with a bleach solution, and life vests were assigned to individual campers and disinfected between campers.\n\n【11】### Conclusions\n\n【12】CA-MRSA infections are increasingly reported and can be severe. Despite multiple outbreaks attributed to USA300, only a few reported case clusters have been caused by USA400 ( _11_ _,_ _12_ ). USA400 does contain virulence factors, and although most cases have been mild skin and soft tissue infections, some have been severe and fatal ( _13_ ). Furthermore, although the colonization rate was low for staff, campers, and household members (2.7%), it was higher than CA-MRSA colonization rates for the general population ( _14_ ), as would be expected for case-patient contacts. S. aureus (including CA-MRSA) spreads easily among people in crowded settings, particularly when adequate hygiene cannot be maintained, both of which occurred in this camping setting.\n\n【13】Although anecdotal, suspected, and confirmed cases during the prior year are consistent with ongoing transmission associated with the camp, that spider bites were reported is noteworthy because CA-MRSA infections are often misdiagnosed as spider bites ( _15_ ). Injuries and breaks in skin should be closely monitored in settings where medical care is not readily available. Methods for skin protection and hygiene should be developed and implemented for populations in settings where CA-MRSA transmission and infection can be anticipated.\n\n【14】Ms Como-Sabetti is a senior epidemiologist at the Minnesota Department of Health in the Infection Control and Antibiotic Resistance Unit. Since 1997, she has worked at the Minnesota Department of Health in the areas of vaccine-preventable diseases, judicious antimicrobial drug use, methicillin-resistant S. aureus, and infection control.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6069b463-0334-4e6f-8e19-609f02ef21db", "title": "Nature Isn’t What It Used To Be", "text": "【0】Nature Isn’t What It Used To Be\nAlexis Rockman (b. 1962) The Farm (2000) Oil and acrylic on wood (243.84 cm × 304.8 cm) Collection of JGS, Inc. Courtesy of the artist\n\n【1】“There are also cocks, which are extraordinary size, and have their crests not red as elsewhere, or at least in our country, but have the flower-like coronals of which the crest is formed variously colored,” wrote traveler and geographer Megasthenes 17 hundred years ago. “Their rump feathers are neither curved nor wreathed but are of great breadth and they trail them in the way peacocks trail their tails, when they neither straighten nor erect them: the feathers of these Indian cocks are in color golden and also dark-blue like the smaragdus.” These impressive cocks, and other fantastic creatures, populated _Ta Indica_ , the author’s account of India. Megasthenes’ flamboyant beasts, some of them sporting nonstandard digits and extra heads, may have come from the stories of people he met in his travels and not his own observations. Nonetheless, they could be describing the creatures of Alexis Rockman, artist, naturalist, author, educator, activist.\n\n【2】“I try to use all of the ways we depict nature and natural history as content,” Rockman says, explaining the natural sciences bias of his subjects. “I’m interested in credibility.” His love for science and art flourished during childhood in his native New York. “I grew up in the American Museum of Natural History. My mother was an assistant to Margaret Meade … it shaped my perspective. ... Charles R. Knight and Chesley Bonestell were my heroes.” His education at the School of Visual Arts complemented long studies of nature in the United States and abroad. “I started out thinking that I would be a scientist. Eventually, over the years, I ended up becoming interested in other types of practices, like certain genres of filmmaking, animation. I think what I ended up doing was really a combination of all those different interests. I’ve always been interested in the history of the representation of nature.”\n\n【3】Rockman’s style, now realistic now abstract, has eluded traditional descriptions, “I try to make it as credible as possible without making it boring.” Some have seen surrealism in his unusual depictions of plants, animals, and humans, “What I am after … is the disturbing part or the transformative part.” Embracing popular culture, he flaunts it with great precision in a manner called hyperrealism in the tradition of Grant Wood, who captured the rural Midwest of the 20th century, especially in his American Gothic. Like pop art icon Andy Warhol, he is comfortable with modern technology and skillful with its agility and interactivity, “computer-manipulated, archetypal images that we’ve all seen, or if we haven’t seen them we feel that we know them.” The Farm, on this month’s cover, was commissioned by Creative Time, a public arts organization, as a New York City billboard. “I like to put people off-kilter by breaking up expected visual patterns.”\n\n【4】The constant struggles between nature and its creatures engage all of Rockman’s interests from evolution, climate change, and genetic engineering to the failures of technology. Like his idol H.G. Wells, the artist portrays humans as, “the unnatural animal, the rebel child of nature,” that, “more and more … turns himself against the harsh and fitful hand that reared him.”\n\n【5】The Farm portrays this favorite theme. Developing it follows the artist’s usual path of discovery, which involves learning from an expert, in this case a molecular biologist, about genetics and artificial selection. The subject drives the medium. The story unfolds with the clarity of a high-definition screen. A field of soybeans extends as far as the eye can see; against it, an allegorical tableau, in Rockman’s words, “the way humans have altered their landscape.”\n\n【6】“The way I constructed it is that, as in a lot of Western culture, we read things from left to right,” he explained in an interview. “On the left side of the image are the ancestral species of the chicken, the pig, the cow, and the mouse”; on the right, their contemporary versions. Farther to the right are “permutations of what things might look like in the future.” The transition from wild cow and boar to familiar barnyard beasts to grotesque technologically engineered models is precise and tactical, and the animals still maintain some original species characteristics. A fruit fly, a strand of DNA, an overmanipulated dog inside a prize-winning blue rosette compete for attention with the cocks placed conspicuously on the fence, straddling the horizon, challenging Megasthenes. Tomatoes created to fit the shipping crate, loaf-shaped watermelons, and multicolor corn complete the picture, occupying several layers of time and genetic activity in the permissive context the artist calls “democratic space.”\n\n【7】Rockman’s vision of biotechnology is riddled with clues and inside jokes rooted in economic, social, ethical, and other concerns. It’s a vision he wants to popularize, “It has to be decipherable to a six-year-old child. I try to construct it as an onion with different layers of meaning and iconography.” The Farm succeeds in this regard, perhaps even beyond the artist’s intentions. This icon of biotechnology is also the stage for foodborne disease emergence. The soybean farm is much too close to the farm animals, whose fertile waste deposits seep into the nearby water used to irrigate the plants. The fence is useless for keeping out rodents or birds and their microbial deposits. And human manipulation, intended to make larger more efficient food animals, may have unintended consequences. If the DNA strands were animated, they would be turning wildly.\n\n【8】A 2006 multistate outbreak of _E_ . _coli_ O157:H7 infection was associated with salad. Samples taken from a stream, cattle manure, and feces from wild pigs on ranches in Salinas Valley, California, implicated spinach, but after this outbreak, leafy greens were seen as subject to this type of contamination. Rockman’s iconic overlap of urban, agricultural, and cattle-raising elements in ecosystems containing sigmodontine rodents, which are reservoirs of hantaviruses, is a recipe for hantavirus pulmonary syndrome. The syndrome, now found throughout the United States, is rare but deadly. Mad cow disease emerged in Great Britain, possibly as an interspecies transfer of scrapie from sheep to cattle and moved around the globe through trade. Spread from human to human is now a threat through contaminated hospital equipment and blood transfusions. People who consume antler velvet as a nutritional supplement may also be at risk for exposure to prions.\n\n【9】“Nature isn’t what it used to be,” Rockman wrote in one of his books. And of course it never was or ever will be. But the irony of that statement awaits future developments. Because, as H.G. Wells put it, “The only true measure of success is the ratio between what we might have done and what we might have been on the one hand, and the thing we have made and the things we have made of ourselves on the other.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5307425c-95c7-4457-be35-f4c6d2261ef6", "title": "Correction: Vol. 22, No. 4", "text": "【0】Correction: Vol. 22, No. 4\nAn affiliation for Mitsuru Toda was missing from the article Effectiveness of a Mobile Short-Message-Service–Based Disease Outbreak Alert System in Kenya (M. Toda et al.). The article has been corrected online ( http://wwwnc.cdc.gov/eid/article/22/4/15-1459\\_article ).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b9b30a70-daa1-44e1-9fc8-5c9d5ad35c8d", "title": "OCCUPATIONAL VIOLENCE", "text": "【0】OCCUPATIONAL VIOLENCE\nWorkplace violence is the act or threat of violence, ranging from verbal abuse to physical assaults directed toward persons at work or on duty. The impact of workplace violence can range from psychological issues to physical injury, or even death. Violence can occur in any workplace and among any type of worker, but the risk for fatal violence is greater for workers in sales, protective services, and transportation, while the risk for nonfatal violence resulting in days away from work is greatest for healthcare and social assistance workers.\n\n【1】NIOSH funds, conducts, and publishes research focused on risk factors and prevention strategies for workplace violence. Employers, occupational safety and health professionals, and workers can use the following resources to reduce occupational violence.\n\n【2】Online Workplace Violence Prevention Course for Nurses\n\n【3】This award-winning course completed by more than 45,000 healthcare workers, educates nurses on the scope and nature of violence and how to prevent it in the workplace.\n\n【4】Take the course now to earn your free continuing education units!\n\n【5】Workplace Violence During the COVID-19 Pandemic\n\n【6】This publication used a novel method (media scraping) to obtain timely, recent information on workplace violence events (WVEs) during the COVID-19 pandemic. Various publicly available online media sources were scanned to identify and describe WVE characteristics related to COVID-19 in the US during the early part of the pandemic (March to October 2020). View the report.\n\n【7】Indicators of Workplace Violence Report\n\n【8】This report from the Bureau of Justice Statistics, Bureau of Labor Statistics, and NIOSH provides indicators of the current problem of workplace violence in the U.S.\n\n【9】View the report.\n\n【10】Reducing workplace violence in gasoline stations and convenience stores\n\n【11】This document describes strategies to prevent and reduce workplace violence incidents in gas stations and convenience stores.\n\n【12】Within the re­tail sector, workers in gas sta­tions (often involving sales of conve­nience store items) are at a higher risk of violence compared with workers in other types of retail and the overall work­force.\n\n【13】Fast Facts\n\n【14】Training\n\n【15】Resources\n\n【16】In the News", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "79af107e-f889-4794-ae0b-22b7f1890b24", "title": "Public Response to Community Mitigation Measures for Pandemic Influenza", "text": "【0】Public Response to Community Mitigation Measures for Pandemic Influenza\nScientists and policymakers are concerned about the emergence of an influenza pandemic for which we will have neither a strain-specific vaccine nor sufficient antiviral medications at the onset of the outbreak. In February 2007, the Community Strategy for Pandemic Influenza Mitigation was issued; it describes the early, targeted, and layered use of nonpharmaceutical interventions, coupled with specific uses of antiviral influenza medications, to reduce transmission of pandemic influenza and mitigate the disease ( _1_ ).\n\n【1】Researchers differ over the potential effectiveness of such community mitigation measures. Evidence to determine the best strategies for protecting persons during a pandemic is limited. Several studies based on findings from mathematical models and historical analyses suggest that early implementation of multiple measures, such as social distancing, school closures, and isolation of sick persons, may be effective in reducing the transmission of the virus ( _2_ – _6_ ). Other researchers cite uncertainty ( _7_ ) or believe such measures may not be effective ( _8_ , _9_ ).\n\n【2】Community mitigation interventions include 1) isolation and treatment with influenza antiviral medications of all persons with confirmed or probable pandemic influenza; 2) voluntary home quarantine of and provision of antiviral medications as prophylaxis to members of households with persons with confirmed or probable influenza (if sufficient quantities of antiviral medications exist and a feasible means of distribution is in place); 3) dismissal of students from schools and closure of childcare facilities along with preventing the recongregation of children and teenagers in community settings; and 4) social distancing of adults in the community and workplace, which may include cancellation of large public gatherings and possible alteration of workplace environments and schedules to decrease social density. A great deal of cooperation from the public would be required to successfully implement community mitigation measures during a pandemic. Public reaction to an unfamiliar crisis is obviously difficult to predict. However, by using surveys that describe hypothetical scenarios, we can elicit potential responses of persons in these situations. Public opinion and input can help inform policy decisions and provide information about realistic expectations for mitigation measures before a public health emergency arises ( _10_ ). This survey was conducted to gauge public reaction to social distancing and other nonpharmaceutical interventions that may be used during a severe pandemic.\n\n【3】### Methods\n\n【4】Data reported here are derived from a survey by the Harvard School of Public Health Project on the Public and Biological Security. The survey was ≈20 minutes long and consisted of 85 questions. International Communications Research conducted the survey from September 28 through October 5, 2006.\n\n【5】The survey was conducted in English and Spanish with a representative national sample of 1,697 adults \\> 18 years of age, including an over-sample of adults who had children <18 years of age in their households. Altogether, 821 such adults with children were interviewed. In the overall results, this group was weighted to its actual proportion of the total US adult population (cooperation rate was 75%; response rate was 36% \\[ _11_ \\]). Common methods for media and preelection surveys were used, and relied on weighting of the data to ensure representativeness. More information about the survey methods and complete question wordings is available in the Technical Appendix .\n\n【6】Surveys like this one, and others that would be conducted as part of a series in the event of a pandemic influenza, can provide technical assistance to public health officials by monitoring the response of the public to the evolving health threat posed by such an outbreak. In a public health emergency, surveys would have to be conducted with short field periods to enable rapid measurement of how the public reacts to a particular set of circumstances. These rapid cycle surveys would make it possible to provide timely information to public health officials and to ensure a quick response.\n\n【7】Getting survey results to public health officials in real time creates a situation similar to that of preelection polling (i.e., specific events can change the behavior and beliefs of many persons in a relatively short timeframe). National polling organizations that engage in preelection surveys use shorter field periods, which provide more up-to-date information but yield lower response rates than surveys conducted over longer time periods ( _12_ ). Forecasts of voters’ choices in preelection polls have shown that outdated information may introduce more errors into predictions of results than low response rates do ( _13_ ).\n\n【8】Independent studies have shown that the results of statistically weighted data from shorter duration surveys are similar to those based on the higher response rate in surveys of long duration and can be used without an unacceptable risk for bias ( _14_ – _18_ ). Nonresponse in telephone surveys produces some known biases in survey-derived estimates because participation tends to vary for different subgroups of the population. To compensate for these known biases, sample data are weighted to the most recent Census data available from the Current Population Survey for sex, age, race, region, and education ( _19_ ). Other techniques, including random-digit dialing, replicate subsamples, callbacks staggered over times of day and days of the week, and systematic respondent selection within households, are used to ensure that the sample is representative.\n\n【9】Possible sources of nonsampling errors for this survey include nonresponse bias, as well as specific wording of questions and the order in which questions are asked. The margin of error for the total sample was ±2.4%. To examine differences among subgroups, we compared responses by testing for differences in proportions, taking into account the effect of the study’s design ( _20_ ).\n\n【10】Because many of the respondents may not have been familiar with pandemic influenza, they were first presented with a descriptive hypothetical scenario: “Now I want to ask you some questions about a possible outbreak in the United States of pandemic flu, a new type of flu that spreads rapidly among humans and causes severe illness. Currently there have not been any cases of pandemic flu in the United States. However, imagine that there was a severe outbreak in the United States, possibly in your community. A lot of persons were getting sick from the flu and the flu was spreading rapidly from person to person.” This scenario was intentionally designed to describe a severe situation without being overly alarming. Respondents were then asked how they would respond to and be affected by the circumstances that would arise from such an outbreak. The small proportion of the respondents who said they would be unable to cooperate with public health authorities could be translated into millions of persons who would have difficulty.\n\n【11】### Results\n\n【12】##### Familiarity with Pandemic Influenza\n\n【13】To determine whether respondents understood what was meant by pandemic influenza, the survey asked how familiar the respondents were with the term (it is unfamiliar to most Americans). Forty-one percent said they knew what the term meant. Thirty-three percent reported that they had heard of the term but did not know what it meant, and 25% had never heard of pandemic flu ( Technical Appendix ).\n\n【14】##### Ability to Stay Home\n\n【15】Respondents were asked about their ability to comply with public health recommendations during an influenza pandemic; 94% said they would stay at home, away from others, for 7–10 days if they had pandemic flu. In addition, 85% said all members of their household would stay at home for the same period if a member of their household were sick ( Table 1 ; Technical Appendix ).\n\n【16】Eighty-five percent said they would be able to take care of sick household members at home for 7–10 days. However, 76% of respondents worried about getting sick if they cared for a sick household member.\n\n【17】Seventy-three percent said that they would have someone available to take care of them at home if they became sick with pandemic flu and had to remain at home for 7–10 days. However, 24% said they would not have someone available to take care of them. Persons living in households with only 1 adult are far more likely not to have someone available to take care of them (45%) compared with persons from households with >1 adult (17%). Approximately one third of low-income (36%), African-American (34%), disabled (33%), and chronically ill (32%) adults said that they would not have anyone who could take care of them. A substantial proportion of the respondents (from 48% to 71%, depending on the measure) believed that they or a household member would likely experience problems if they had to stay at home for 7–10 days and avoid contact with anyone outside their household ( Table 1 ).\n\n【18】##### School Closings\n\n【19】Thirty-nine percent of respondents reported having children <18 years of age living in their household ( _21_ ), including 16% with children 13–17 years of age in the household, 22% with children 5–12 years of age, and 14% with children <5 years of age. Of adults in households that had children <18 years of age, 91% said that they have major responsibility for the children in their household ( Technical Appendix ).\n\n【20】Respondents were told that to keep pandemic influenza from spreading and to protect the safety of children, some communities might close schools and daycare facilities for some period of time. Although the Community Strategy for Pandemic Influenza Mitigation used the term dismissal from school, the survey used the term school closure. Respondents were also told that the length of time schools and daycares would remain closed would probably be tied to the severity of the pandemic influenza outbreak.\n\n【21】If schools and daycare were closed for 1 month, 93% of adults who have major responsibility for children <5 years of age in daycare or children 5–17 years of age and have at least 1 employed adult in the household thought they could arrange care so that at least 1 employed adult in the household could go to work. Eighty-six percent thought they would be able to do so for 3 months ( Table 2 ). Of those who said they could arrange care for 1 month so that at least 1 adult would be able to work, 87% said they or another family member would be the primary caretakers for children if schools and daycares had to be closed. Of these adults, 64% said they would need little or no help even if children had to be kept at home for a long time. Of those who said they would need a lot or some help, 50% said they would rely most on help from family, 11% on friends or neighbors, and 34% on outside agencies (including government agencies, church and community groups, or voluntary agencies).\n\n【22】However, 60% of adults who have major responsibility for children <18 years of age said that at least 1 employed person in the household would have to stay home from work. Of employed persons, 25% who have major responsibility for children <18 years of age in their household said that if schools and daycares closed for 1 month, they would be able to work from home and take care of the children.\n\n【23】If schools were closed for 3 months, 95% of adults with major responsibility for children 5–17 years of age said they would be willing to give school lessons at home. Of those who were willing to do so, 47% thought they would need a lot or some help, although 53% said they would need little or no help.\n\n【24】Among adults with major responsibility for children 5–17 years of age, 85% thought that if schools were closed for 3 months, they would be able to keep their children and teenagers from taking public transportation, going to public events, and gathering outside home while schools were closed. Of adults who have major responsibility for children <5 years of age in daycare or children 5–17 years of age in their household, 25% reported that a child in their household gets free breakfast or lunch at school or daycare. Asked specifically about an outbreak of pandemic influenza, 34% of those whose children get free meals at school (8% of the total who have responsibility for children in this age group) said that if schools and daycare were closed for 3 months, not getting the free meals would be a problem.\n\n【25】##### Ability to Stay Home from Work\n\n【26】Sixty-three percent of the US adult population was employed at the time of the survey ( _22_ ). Employed respondents were asked about the problems they might face being out of work for various lengths of time. Most employed persons (74%) believed they could miss 7–10 days of work without having serious financial problems; 25% said they would face such problems. Fifty-seven percent thought they would have serious financial problems if they stayed home for 1 month. Of those surveyed, 76% believed they would have such problems if they stayed home from work for 3 months ( Table 3 ; Technical Appendix ).\n\n【27】Of employed respondents, 29% said that they would be able to work from home if they were asked to stay home for 1 month because of a serious outbreak of pandemic flu. Of the low-income workers (<$25,000/y), 13% believe that they would be able to work from home for that long, compared with 44% of high-income workers ( \\> $75,000/y).\n\n【28】Employed respondents were also asked about their employers’ plans and policies for dealing with an outbreak of pandemic flu. Few working persons (19%) were aware of any workplace plan to respond to a serious outbreak of pandemic flu.\n\n【29】Of employed adults, 57% said they would stay home from work if public officials said they should; 35% said they would go to work if their employers told them to report to their jobs. Of employed adults, 22% were worried that, in the event of a serious outbreak of pandemic flu in their community, their employer would make them go to work even if they were sick.\n\n【30】Of employed respondents, 50% believed that their workplace would stay open if there was a serious outbreak of pandemic flu, even if public health officials recommended that some businesses in the community should shut down. Forty-three percent thought that their workplace would shut down.\n\n【31】Of employed respondents, 35% thought that if they stayed home from work, they would still get paid; 42% thought that they would not get paid, and 22% did not know whether they would get paid. Low-income respondents (from households <$25,000/y) were significantly less likely than high-income respondents (from households \\> $50,000/y) to believe they would still get paid ( Table 4 ).\n\n【32】##### Ability to Cooperate with Other Recommendations\n\n【33】Respondents were given a scenario about an outbreak of pandemic influenza and asked if they would cooperate if public health officials recommended that for 1 month they curtail various activities of their daily lives. The initial response between 79% and 93% (depending on the measure) was that they would cooperate ( Table 5 ; Technical Appendix ).\n\n【34】##### Problems Responding to Recommendations\n\n【35】On several measures, more low-income Americans (those who come from households with an annual income <$25,000/y) than high-income Americans believed they would experience problems responding to public health recommendations. Similarly, on many of these measures a higher proportion of African Americans and Hispanic Americans than whites believed they would experience problems ( Table 4 ). The same holds true for persons who described their own health status as fair or poor ( Table 6 ; Technical Appendix ).\n\n【36】### Conclusions\n\n【37】If community mitigation measures were instituted for a severe influenza pandemic, most respondents would comply with recommendations but would be challenged to do so if their income or job was severely compromised. Results from this survey were useful in shaping the Community Mitigation Guidance because important information was obtained about public acceptability and key public concerns and challenges.\n\n【38】During a severe pandemic, public health authorities are likely to recommend that all but the sickest persons remain home while ill. Strategic planning by home-health, faith-based, and community organizations; medical providers; and public health agencies about how to coordinate care for those who would have to stay home ill during a pandemic will be essential, particularly for those who live alone.\n\n【39】The resiliency of those who would need to stay home during a pandemic will depend on their level of preparedness. Previous studies on personal preparedness at home have shown that respondents have concerns about having sufficient supplies if asked to stay quarantined at home for a prolonged period of time ( _23_ ). Two recent surveys indicate that many Americans have made no preparations for a public health emergency and most have prepared less than they think they should ( _24_ , _25_ ). Careful community planning, including public education and engagement, will be needed to encourage the public to be prepared for an emergency like a pandemic.\n\n【40】Survey results also indicated that most persons were concerned about getting sick themselves if they had to stay at home to care for a household member who was ill with pandemic flu. The public must be given accurate information before and during a pandemic about how to provide at-home care along with precautions that caretakers should follow to protect their own health.\n\n【41】Employers can enable employees to comply with public health recommendations during a pandemic ( _26_ , _27_ ). Sick leave and other policies (such as telecommuting, staggered shifts, and other strategies) should promote and create incentives for workers to stay home if they or a household member becomes sick during a severe pandemic or if well, to report to work. Well employees should report to work (especially those in health care and other critical infrastructure jobs) to ensure business continuity and the ability to provide care as needed ( _28_ ). Workers should be aware of their employer’s pandemic preparedness plans and other strategies that will promote social distancing at the workplace during a pandemic. Implementing these measures will help to ensure a safer workplace during a pandemic and will mitigate transmission of disease.\n\n【42】Among the key interventions for potentially reducing transmission of the influenza virus during a pandemic will be to dismiss students from schools, close childcare facilities, and keep children from re-congregating in the community. Depending on the severity of the pandemic, the duration of school dismissal could range from a few weeks up to 3 months. How families would cope with the cascading effects from prolonged cancellation of school classes is a concern. Families could face the problem of serious income loss. Most respondents said that at least 1 employed person would have to stay home from work during a pandemic to care for children. Therefore, employers can identify employees who may need to stay home to care for children and determine in advance if those employees could work from home, work staggered shifts, or be trained to take on other responsibilities, or if other employees can be cross-trained to take on some of those job functions. Employers must be prepared for increased absenteeism related to childcare responsibilities.\n\n【43】Community mitigation measures could cause particular problems for persons from low-income families and for racial and ethnic minorities. With these problems in mind, communities should plan for the needs of vulnerable populations who may be adversely affected during a pandemic. Workers who do not have sick or other leave time available will need support if they have to stay home during a pandemic. Communities should explore alternative ways of replacing school-based services, such as free meals, if schools are unable to provide those services.\n\n【44】These findings can inform planners about what the public may do if a pandemic occurs. However, the public might react differently when the event actually occurs. These results should be interpreted with caution in advance of a severe pandemic that could cause prolonged disruption of daily life and widespread illness in a community. Adherence rates to recommendations might be high during the early stages of a pandemic but results may not be as predictive over the course of several months. We have more confidence in the predictive ability of the survey in areas in which the public has a greater amount of personal experience, e.g., workplace issues, income, and the need for assistance at home.\n\n【45】Willingness to adhere to community mitigation measures may be influenced by the severity of illness persons observe in the community relative to their need for income and the level of community, individual, and family disruption. In addition, public response is likely to be affected by the perceived effectiveness of government and voluntary agencies in dealing with crisis situations. Planning for implementation of community mitigation measures, as well as actions to reduce secondary consequences, are important steps in enhancing adherence to public health recommendations.\n\n【46】The communication resources of government can be scarce during a crisis. Such resources can be used most effectively if there are recent data about what the public needs to learn. This was seen in the cases of severe acute respiratory syndrome and anthrax ( _29_ ). During a pandemic, short-duration rapid-turnaround public surveys can provide timely information to public health officials about the acceptability of recommendations and needed communication to the public if problems are found ( _15_ ). Although the challenge is formidable, our best chances of protecting health and maintaining functioning communities during a pandemic rely on optimal adherence to public health measures and a coordinated response within and between communities.\n\n【47】Dr Blendon is professor of health policy and political analysis at the Harvard School of Public Health (HSPH), Boston. He directs the HSPH Project on the Public and Biological Security, which conducts surveys to assess public knowledge, attitudes, and behavior in response to emerging infectious diseases, threats of bioterrorism, and disasters.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dd0b0ef1-2167-4ffc-8775-2d0ff5e4b1d1", "title": "Using the Community Readiness Model to Examine the Built and Social Environment: A Case Study of the High Point Neighborhood, Seattle, Washington, 2000–2010", "text": "【0】Volume 11 — November 06, 2014\n\n【1】### Article Tools\n\n【2】*   PDF - 665 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### COMMUNITY CASE STUDY  \n\n【4】Using the Community Readiness Model to Examine the Built and Social Environment: A Case Study of the High Point Neighborhood, Seattle, Washington, 2000–2010\n============================================================================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Background\n*   Community Context\n*   Methods\n*   Outcome\n*   Interpretation\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【7】#### Joyce Buckner-Brown, PhD, MHS, RRT; Denise Tung Sharify, BA; Bonita Blake; Tom Phillips, BS; Kathleen Whitten, PhD\n\n【8】_Suggested citation for this article:_ Buckner-Brown J, Sharify DT, Blake B, Phillips T, Whitten K. Using the Community Readiness Model to Examine the Built and Social Environment: A Case Study of the High Point Neighborhood, Seattle, Washington, 2000–2010. Prev Chronic Dis 2014;11:140235. DOI: http://dx.doi.org/10.5888/pcd11.140235 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Background**  \nResidents of many cities lack affordable, quality housing. Economically disadvantaged neighborhoods often have high rates of poverty and crime, few institutions that enhance the quality of its residents’ lives, and unsafe environments for walking and other physical activity. Deteriorating housing contributes to asthma-related illness. We describe the redevelopment of High Point, a West Seattle neighborhood, to improve its built environment, increase neighborhood physical activity, and reduce indoor asthma triggers.\n\n【12】**Community Context**  \nHigh Point is one of Seattle’s most demographically diverse neighborhoods. Prior to redevelopment, it had a distressed infrastructure, rising crime rates, and indoor environments that increased asthma-related illness in children and adolescents. High Point residents and partners developed and implemented a comprehensive redevelopment plan to create a sustainable built environment to increase outdoor physical activity and improve indoor environments.\n\n【13】**Methods**  \nWe conducted a retrospective analysis of the High Point redevelopment, organized by the different stages of change in the Community Readiness Model. We also examined the multisector partnerships among government and community groups that contributed to the success of the High Point project.\n\n【14】**Outcome**  \nOverall quality of life for residents improved as a result of neighborhood redevelopment. Physical activity increased, residents reported fewer days of poor physical or mental health, and social connectedness between neighbors grew. Asthma-friendly homes significantly decreased asthma-related illness among children and adolescents.\n\n【15】**Interpretation**  \nProviding affordable, quality housing to low-income families improved individual and neighborhood quality of life. Efforts to create social change and improve the health outcomes for entire populations are more effective when multiple organizations work together to improve neighborhood health.\n\n【16】Top of Page\n\n【17】Background\n----------\n\n【18】The relationship between health disparities and the built environment has been the focus of recent research examining new approaches to the ongoing challenge of providing affordable, quality housing in low-income neighborhoods (1–8). Housing is one of the 11 social determinants of health (9) because the built environment has measurable effects on both physical and mental health and can encourage physical activity, improve access to healthful foods, and reduce crime through neighborhood design and regulations (1–8).\n\n【19】According to the US Surgeon General, healthy homes are those “sited, designed, built, renovated, and maintained in ways that support the health of residents. Specific features that constitute healthy housing include structural and safety aspects of the home (ie, how the home is designed, constructed, and maintained; its physical characteristics; and the presence or absence of safety devices), quality of indoor air and water, and the presence or absence of chemicals” (8). In contrast, poor housing can expose people to dangerous physical conditions. Environmental health science studies include not only the effects of pollutants, but also other factors that affect population health, such as housing quality and other indicators of social distress (2). Differences in neighborhoods and housing quality among people of different socioeconomic status add to the disproportionate burden of illness and injury among minority racial/ethnic populations and low-income communities (2). For example, asthma has become a major public health issue in urban areas because of its increasing prevalence and disproportionate effect on children and adolescents in low-income and minority urban communities. Dilapidated housing is associated with asthma triggers such as mold, moisture, dust mites, and rodents, and with mental health stressors such as violence and social isolation (2). This article describes how a Seattle, Washington, community implemented a comprehensive redevelopment plan that created a sustainable built environment with improved indoor environmental quality.\n\n【20】Top of Page\n\n【21】Community Context\n-----------------\n\n【22】High Point, in the Delridge district of West Seattle, is one of the city’s most demographically diverse neighborhoods and has a substantial immigrant population from Southeast Asia and East Africa (10). Before community redevelopment, High Point was a culturally diverse community with many racial/ethnic groups: 36% of residents were African or African-American, 29% were Asian/Pacific Islander, 18% were non-Hispanic white, and 17% were other races/ethnicities. Most household heads (61%) were born outside the United States. The neighborhood’s cultural diversity was also reflected in language: only 37% of residents spoke English as their preferred language; 26% spoke Vietnamese, 12% spoke Cambodian, 8% spoke Somali, and the remaining 17% spoke 1 of 10 other languages (10). The High Point community was developed in 1942 to provide temporary government housing to defense workers during World War II (10). The Seattle Housing Authority assumed administrative oversight in 1953 and converted High Point into public housing (10). By the late 1960s, High Point had a distressed infrastructure and rising crime rates. The indoor environments of its residences placed children at substantial risk of exposure to asthma triggers. Many families with children experienced repeated visits to emergency departments for asthma-related illness.\n\n【23】The High Point redevelopment initiative was undertaken with the overall objective of improving neighborhood quality of life with community involvement. The aim of community involvement was to promote collaboration among all stakeholders in the redevelopment effort, including residents, federal and local government, and private funders. We describe the collaboration that made the redevelopment possible, the built environment that increased the walkability of the neighborhood, and pediatric asthma outcomes.\n\n【24】Top of Page\n\n【25】Methods\n-------\n\n【26】From the start, the Seattle Housing Authority engaged High Point residents and community organizations in the redevelopment planning process. Through meetings and collaboratively designed workshops, residents and planners focused on making High Point a home for people of all ages and cultures. Residents were involved in every aspect of planning, including problem identification, strategy development and implementation, and program evaluation. We conducted a retrospective analysis of the project and used the 9-stage Community Readiness Model developed by the Tri-Ethnic Center for Prevention Research at Colorado State University (www.triethniccenter.colostate.edu) to describe the mobilization of High Point’s collaborative efforts (11,12) ( Table 1 ).\n\n【27】### Awareness of the problem\n\n【28】By summer 2000, High Point was more than 60 years old and in varying stages of deterioration. The original development consisted of 1,300 units. By the 1970s, 550 units had been demolished because of poor condition or because they were located in landslide-prone areas. By 2000, 716 of the remaining units were occupied by public housing residents; the rest housed social service providers (10). Community members reported an increase in gang activity and a crack cocaine epidemic. Asthma was so commonplace it was considered normal (13). Mold developed from leaking windows that had soaked the plasterboards over the course of years. Dust mites and roaches, common asthma triggers, were prevalent (14). Residents were aware of the problems but lacked the resources to make changes.\n\n【29】### Preplanning\n\n【30】In the 1990s the Seattle Housing Authority began applying for funding from the Housing Opportunities for People Everywhere VI (HOPE VI) (10), a US Department of Housing and Urban Development (HUD) program to revitalize severely distressed public housing. In 2000 the Seattle Housing Authority was successful in obtaining $35 million for redevelopment of High Point under HOPE VI. The High Point coalition, a group of 30 to 40 High Point residents, was formed to advise the Seattle Housing Authority on the redevelopment of High Point. In addition to community members, other members of the coalition were the City of Seattle, the Seattle Health Department, Rotary Club, the High Point Community Council, Sunrise Heights Neighborhood Association, Morgan Community Association, the Delridge and Southwest District Council, High Point Neighbors, Highland Park Action Committee, West Seattle Chamber of Commerce, West Seattle Kiwanis, and the King County school district. Partners met monthly to determine project goals, design strategies, oversee implementation and evaluation methods, and review evaluation findings (10). High Point residents received stipends for participating and represented the community at meetings.\n\n【31】Seattle’s mayor and city council members were champions for the High Point redevelopment initiative. Community leaders in West Seattle circulated a petition and contacted civic and government officials to obtain their support for the project.\n\n【32】A local business partner provided tax credit syndicators. (A syndicator acquires low-income housing tax credits from affordable housing developers by investing equity in their housing developments in exchange for their tax credits.) Builders purchased property at High Point, which provided an additional $165 million to the project budget, bringing the total project budget to $200 million. Three consultants from University of Washington with expertise in social and built environments assisted the High Point coalition in structural design.\n\n【33】Seattle Housing Authority conducted a needs assessment in the summer of 2001. Of 609 households contacted, slightly more than a third (34%) said that people at High Point had problems with alcohol, and 41% said they thought that High Point residents had a problem with tobacco (10). Most residents (77%) said they thought that High Point was a safe place. Among minority residents who said that High Point was unsafe, the most frequently problems cited were drug dealing or drug use (80%), outsiders causing trouble (77%), gangs (74%), noise (61%), and car vandalism (60%) (10).\n\n【34】### Preparation\n\n【35】The High Point revitalization project was developed and implemented by a partnership of residents, community organizations, Seattle Housing Authority staff, public health practitioners, university faculty, and other public agencies. The coalition intentionally used community-based participatory approaches to guide its formation and operation (15). The values that guided its work included development of community capacity and equitable partnerships, linguistic and ethnic inclusivity, and community ownership. Equitable partnerships require sharing knowledge, power, and resources and providing reciprocal appreciation of what each group offers the partnership. For example, High Point’s Community Council identified asthma as a problem in the community and, in collaboration with the structural design experts, developed the residences as “Breathe Easy” units (http://www.seattlehousing.org/redevelopment/high-point/breathe-easy/).\n\n【36】The coalition supported the development of youth and adult community action teams. Each team consisted of 8 to 10 community members who convened to assess community conditions, discuss community concerns, build leadership and social capital, and develop activities to address changes to the built and social environment. More than 450 High Point residents and their West Seattle neighbors participated in a design survey and hands-on workshop that considered what the redeveloped High Point should look like. The Seattle Housing Authority Board of Commissioners approved the High Point redevelopment plan to serve residents whose incomes were below 30% of HUD’s area median income for King County.\n\n【37】### Initiation\n\n【38】The redevelopment of High Point occurred in 2 phases. Phase I began in 2003 and was completed in December 2007. Phase I consisted of construction of new public housing units. Phase II, infrastructure construction, began in 2006 and was completed in 2010 ( Table 2 ). An integrated team of architects, interior designers, landscape architects, urban designers and planners, High Point residents, West Seattle community members, and Seattle Housing Authority staff worked together to create High Point’s redevelopment plan. The plan included 3 major components: quality design, a healthy environment, and an engaged community. The redevelopment plan created a safe, high-quality, and healthy residential environment with a range of housing types, each constructed to Built Green standards (http://www.epa.gov/greenbuilding/index.htm) and fully integrated with the surrounding community (13). New streets were realigned and reconnected with the West Seattle grid, and new neighborhood facilities and community services were operating at more inviting locations. The mix of housing types and resident income levels became more compatible with the greater neighborhood. Principles of New Urbanism guided the redevelopment process (16). New Urbanism is a revitalized urban design system that promotes social interaction, creates walkable spaces, and supports physical safety (eg, sidewalk-facing porches, windows facing streets to allow observation) (17,18).\n\n【39】Families began moving into rental and for-sale housing in Phase II of the neighborhood redevelopment, which began in 2006 and was completed in 2010. Seven hundred and sixteen units were ultimately replaced. Three hundred fifty of these are operated by Seattle Housing Authority and serve extremely low-income households. Private home development will continue until the nearly 1,700-unit capacity of the site is reached. In addition, Providence Health Systems built and leased Elizabeth House, a 75-unit supportive housing program for low-income elderly residents. The community engagement process created plans for a new, mixed-income community with health as its focus. High Point residents wanted the kind of healthy living conditions that wealthier neighborhoods usually take for granted.\n\n【40】### Stabilization\n\n【41】The renovation of High Point resulted in a green and sustainable community (http://www.seattlehousing.org/redevelopment/high-point/photos/). The neighborhood has energy-efficient housing, a community clinic, a public library, rebuilt community-raised garden beds, a community center that offers child care and employment placement services, an assisted living facility (Elizabeth House), lush green parks, and walkable sidewalks that enhance social interaction and physical activity. The redevelopment incorporated many sustainable development principles. The site and rental housing are certified at the highest Built Green levels. Nearly all rental housing and homeowner units are Energy Star (http://www.energystar.gov/) rated. The site features porous sidewalks and parking areas and the only porous pavement street in Washington State. The site is situated on 120 acres with an engineered natural drainage system, which uses the ground to filter rainwater instead channeling water into a traditional conveyance system. The rebuilt homes have ventilation systems designed to bring in fresh air from the outside. According to Takaro et al, “Even small particles that might be bad for health, such as diesel particulate or any pollen in the case of an asthmatic, will be filtered out so the air in the home is actually healthier than the air outside” (14).\n\n【42】The High Point neighborhood redevelopment has been recognized with prestigious land use and development awards (Figure 1) and has gained national attention for its progressive approach to redevelopment (10). In 2004, High Point was awarded the Pacific Coast Builders Conference’s prestigious Gold Nugget Award for “Best Plans on the Boards.” More recently, the American Institute of Architects honored the project with a coveted “Show You’re Green” award, 1 of only 8 in the nation awarded for sustainable innovations and affordability. High Point is also featured in 2 PBS documentaries, _Edens Lost and Found_ and _Hidden Epidemics_ . The former profiles redevelopment activities in Seattle, Los Angeles, Chicago, and Philadelphia, and the latter examines socioeconomic and racial disparities in health.\n\n【43】**Figure 1.** Illustration of the awards received by the High Point community. Abbreviations: AIA, American Institute of Architects; NAHB, National Association of Home Builders; HUD, US Department of Housing and Urban Development; PCBC, Pacific Coast Builders Conference; EPA, US Environmental Protection Agency. \\[A text description of this figure is also available.\\]\n\n【44】### Confirmation and expansion\n\n【45】By the end of the decade, High Point had nearly 1,700 new affordable and market-rate units. High Point includes housing units for residents with very low incomes (50% of area median income or below) and low incomes (80% of area median income or below) in addition to market-rate rental and for-sale housing. Most houses have private yards and porches. They sit on safe streets with controlled traffic and show great variety in architecture, character, and styles on each block. Proceeds from the land and home sales have helped fund low-income housing in the neighborhood and elsewhere. In addition, the land has been returned to the city’s property tax rolls, where it can generate revenues to help keep the neighborhood economically self-sufficient (10).\n\n【46】### Community ownership\n\n【47】High Point’s redevelopment success resulted from close cooperation among planners, residents, and other community stakeholders. All landscaping is well maintained, housing is uniform in style, and an observer would not detect differences between properties owned and properties rented. However, shrubbery is sometimes not properly trimmed; if it grows to sufficient heights, criminal activity may increase. Thus, continuous accountability and oversight are crucial, and all residents are responsible for making recommendations as needed relating to strategic planning, property management, and ongoing evaluation of customer services and needs to ensure the stability of the social and built environment.\n\n【48】Top of Page\n\n【49】Outcome\n-------\n\n【50】### Improving air quality\n\n【51】Social science research has demonstrated a strong link between the social and built environments and their roles in contributing to healthy outcomes. For example, Takaro et al (14) examined the asthma-related clinical outcomes of living in High Point’s Breathe Easy Homes. A quasi-experimental design was used to compare the asthma outcomes of 2 groups of low-income children and adolescents with asthma. One group of 34 participants who moved into a Breathe Easy Home was matched with a second group of 68 local residents who had received a previous asthma-control intervention. Both groups received in-home asthma education. The group in the Breathe Easy Homes had less asthma-related illness than the comparison group, but most differences in improvements were not significant. The researchers reported that “Breathe Easy Home residents’ asthma-symptom–free days increased from a mean of 8.6 per 2 weeks in their old home to 12.4 after 1 year in the Breathe Easy Home.” Furthermore, the proportion of Breathe Easy Home residents with an urgent asthma-related clinical visit in the previous 3 months decreased from 62% to 21%. Breathe Easy Home caretakers’ quality of life improved significantly. Residents in the Breathe Easy Homes group improved more than did those in the comparison group, but most differences in improvements were not significant. However, exposures to mold, rodents, and moisture were reduced significantly. Children and adolescents with asthma who moved into an asthma-friendly home experienced large decreases in asthma morbidity and trigger exposure.\n\n【52】### Improving the built environment\n\n【53】As stated previously, High Point’s redevelopment plan was based on features of New Urbanism. These features, which contribute to a healthy community, include pedestrian friendly street designs (porches, hidden parking lots, wider sidewalks, tree-lined and slow-speed streets), a pond, multiple parks, walking trails, and separation of sidewalks from traffic by swales designed to manage storm water runoff and increase rainwater infiltration (14).\n\n【54】Using the Centers for Disease Control and Prevention’s (CDC’s) _Guide to Community Preventive Services_ as a resource (19), community action teams developed multiple interventions to promote walking, including establishing walking groups, improving walking routes, providing information about walking options, and advocating for pedestrian safety. Self-reported walking increased among walking group participants from 65 to 108 minutes per day (13). The proportion of walking participants who reported being at least moderately active for at least 150 minutes per week increased from 62% to 81% in a 3-month period (13). General health improved, and walking group participants reported fewer days when physical health and mental health were not good.\n\n【55】### Improving the social environment\n\n【56】The social environment affects physical activity through community safety, social support, and access to recreation and activity programs (13). Community action teams introduced the concept of walking groups as an effective social environmental strategy to promote physical activity. Teams modified the Walk Kit from the California Center for Physical Activity (http://www.caactivecommunities.org/resources/walk-kit/). High Point’s walkers were encouraged to meet _Healthy People 2010_ recommendations of at least 30 minutes of moderate physical activity **(** ie, 30 minutes of exercise or 10,000 walking steps) on 5 or more days per week (13). The walking groups met 5 times per week at weekday, evening, and weekend sessions. Walking appears to have collateral benefits. Participants reported walking more for errands, and social interactions with their neighbors increased. Walkers received T-shirts, pedometers, and prizes as incentives for meeting individual walking goals. The walking sessions promoted walking among other community members who saw their neighbors walking together in a group identified by the T-shirts or bright yellow rain ponchos they were wearing (Figure 2).\n\n【57】**Figure 2.** High Point’s narrow streets, short blocks, and wide planting strips promote walking. Front yards, porches located close to sidewalks, and the overall design of the community encourage social interaction. \\[A text description of this figure is also available.\\]\n\n【58】Top of Page\n\n【59】Interpretation\n--------------\n\n【60】Housing matters for health (1–8), and quality housing is associated with positive physical and mental well-being (20). There is broad consensus that residents of socially and economically disadvantaged communities experience, on average, worse health outcomes than those living in more prosperous areas. The HOPE VI Urban Revitalization Demonstration Program’s housing initiative, with its explicit goal of decentralizing poverty, has demonstrated the benefits of reducing neighborhood-level poverty by providing affordable housing to low-income families in mixed-income residential developments. Relocation of residents may reduce the poverty level of an entire neighborhood but not necessarily the poverty level of individuals and particular families. As we move forward, more focus is needed on supporting collaborative programs that increase health equity across a range of health outcomes.\n\n【61】Efforts to create social change and improve health outcomes for entire populations have greater impact when many organizations work together to promote health causes. Consensus and collaboration among the affected population, with attention to its cultures, beliefs, and resources, should be included in planning strategies to improve and sustain living conditions related to resident health and public health issues. When the community is engaged, planning efforts are more likely to be based on concepts and ideas that are culturally appropriate for that unique community. To be successful, interventions should be tailored to the stage of readiness of the affected population to tackle complex public health issues. This case study of the High Point redevelopment project illustrates how the involvement and commitment of local residents in the planning and implementation of a local housing improvement effort can contribute to its success.\n\n【62】Top of Page\n\n【63】Acknowledgments\n---------------\n\n【64】The authors thank members of the High Point community, the High Point Community Council, and the Seattle Housing Authority for their collaboration and contributions to this article. This work could not have been possible without their partnership and support. Authors received no financial support for this article. Opinions expressed by authors do not necessarily reflect the opinions of the US Department of Health and Human Services, CDC, or the authors’ affiliated institutions.\n\n【65】Top of Page\n\n【66】Author Information\n------------------\n\n【67】Corresponding author: Joyce Buckner-Brown, Health Scientist, Centers for Disease Control and Prevention, 4770 Buford Hwy, NE, Mailstop K81, Atlanta, GA. Telephone: 770-488-5427. E-mail: jbucknerbrown@cdc.gov .\n\n【68】Author Affiliations: Denise Tung Sharify, Neighborhood House Community Health Program, Seattle, Washington; Bonita Blake, High Point Community Council, Seattle, Washington; Tom Phillips, Developer, High Point Community, Seattle Washington; Kathleen Whitten, ICF International, Atlanta, Georgia.\n\n【69】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "50db02bc-62fa-4522-b673-f19217d54f6d", "title": "Abrasive Blasting", "text": "【0】On This Page\n\n【1】*   NIOSHTIC-2 Search\n*   NIOSH Resources\n*   U.S. Governmental Resources\n*   Non-U.S. Governmental\n\n【2】Abrasive blasting may have several hazards associated with it at any given time. Abrasive blasting is more commonly known as sandblasting since silica sand has been a commonly used material as the abrasive, although not the only one always used. Abrasive blasting entails accelerating a grit of sand sized particles with compressed air to provide a stream of high velocity particles used to clean metal objects such as steel structures or provide a texture to poured concrete. This process typically produces a large amount of dust from the abrasive, anything on the substrate being abraded, and/or the substrate itself.\n\n【3】If the process is not completely isolated from the operator, abrasive blasting dusts are a very great health risk. Respirable dust from silica sand and other abrasive materials pose a risk to the lungs. Where abrasive blasting is used to remove lead-based paint on the steel infrastructure of bridges, it can generate particles of lead that pose a risk to the nervous system. In addition to potential health hazards, abrasive blasting can pose safety risks as well. Cleaning steel while working from scaffolding introduces a fall risk and from within industrial tanks a confined space risk. The abrasive stream itself can cause physical harm to the operator or anyone close by. There are NIOSH guidelines and OSHA regulations addressing many aspects of abrasive blasting including such things as proper airline length, and quality of breathing air provided to the abrasive blasting respirator. There is much to know about abrasive blasting and the associated hazards in order to consistently perform the task safely.\n\n【4】### NIOSHTIC-2 Search\n\n【5】NIOSHTIC-2 search results on Abrasive Blasting  \nNIOSHTIC-2 is a searchable bibliographic database of occupational safety and health publications, documents, grant reports, and journal articles supported in whole or in part by NIOSH.\n\n【6】### NIOSH Resources\n\n【7】_The links to external web sites included below are provided for informational purposes only. Citation should not be taken as endorsement by NIOSH of the web site content nor of the sponsoring organization._\n\n【8】**Carbon Monoxide Poisoning**  \nCarbon Monoxide Hazards from Small Gasoline Powered Engines\n\n【9】*   Laborer Dies of Carbon Monoxide Poisoning During Sandblasting Operations in Virginia  \n    In-house FACE Investigative Report No. 1991-31\n\n【10】**Criteria for Abrasive Blast Cleaning Operations**\n\n【11】*   Industrial Health and Safety Criteria for Abrasive Blast Cleaning Operations  \n    HEW Publication No. (NIOSH) 75-122 (1975)\n\n【12】**Engineering Control**\n\n【13】*   Abrasive Blasting Operations, Engineering Control and Work Practices Manual  \n    HEW Publication No. (NIOSH) 76-179 (1976)\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Chemical Stripping \\[PDF – 328 KB\\]  \n    Survey Report No. ECTB 183-17a, June 1999\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Abrasive Blasting using Staurite XL in Containment \\[PDF – 531 KB\\]  \n    Survey Report No. ECTB 183-13a, July 1993\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Abrasive Blasting using Steel Grit with Recycling \\[PDF – 1,001 KB\\]  \n    Survey Report No. ECTB 183-12a, June 1993\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Chemical Stripping using Caustic (Peel Away ST-1) \\[PDF – 518 KB\\]  \n    Survey Report No. ECTB 183-15a, November 1994\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Abrasive Blasting Inside Two Ventilated Containment Systems \\[PDF – 202 KB\\]  \n    Survey Report No. ECTB 183-14a, December 1994\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: Power Tool Cleaning \\[PDF – 303 KB\\]  \n    Survey Report No. ECTB 183-16a, November 1995\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: \\[PDF – 423 KB\\]  \n    Survey Report No. ECTB 183-22, May 1999\n*   Control Technology for Removing Lead-Based Paint from Steel Structures: \\[PDF – 534 KB\\]  \n    Survey Report No. ECTB 247-11, December 1999\n\n【14】**Falls From Elevations**\n\n【15】*   Painter/Sandblaster Dies Following a 30-foot Fall from Scaffolding Inside a Water Tank-South Carolina  \n    In-house FACE Investigative Report No. 1993-15\n*   Preventing Worker Injuries and Deaths Caused by Falls From Suspension Scaffolds  \n    DHHS NIOSH Publication No. 92-108 (August 1992)\n\n【16】**Lead Poisoning**\n\n【17】*   Preventing Lead Poisoning in Construction Workers  \n    DHHS (NIOSH) Publication No. 91-116a (April 1992)\n*   Protecting Workers Exposed to Lead-Based Paint Hazards, A Report to Congress  \n    DHHS (NIOSH) Publication No. 98-112 (January 1997)\n\n【18】**Noise Induced Hearing Loss**\n\n【19】*   Choosing Hearing Protection\n\n【20】**Respiratory (Lung) Disease**\n\n【21】*   Silicosis\n    *   Construction Workers: It’s Not Just Dust!… Prevent Silicosis  \n        DHHS (NIOSH) Publication No. 97-101\n    *   Preventing Silicosis and Deaths in Construction Workers  \n        DHHS (NIOSH) Publication No. 96-112 (1996)  \n        en Español\n    *   Preventing Silicosis and Deaths From Sandblasting  \n        DHHS NIOSH Publication No. 92-102 (August 1992)  \n        en Español\n    *   Silicosis in Abrasive Blasting\n    *   Silicosis: Learn the Facts!  \n        DHHS (NIOSH) Publication No. 2004-108  \n        en Español\n*   Tuberculosis (TB\n\n【22】**Respiratory Protection**\n\n【23】*   Abrasive Blasting Respiratory Protective Practices  \n    HEW Publication No. (NIOSH) 74-104 (1974)\n*   Respiratory Protection Programs\n    *   NIOSH Respiratory Protection Certification Program: National Personal Protective Technology Laboratory\n\n【24】**Substitutes for Silica Sand Use**\n\n【25】*   Comparative Pulmonary Toxicity of Blasting Sand and Five Substitute Abrasive Blasting Agents  \n    Porter DW, Hubbs AF, Robinson VA, Battelli LA, Greskevitch M, Barger M, Landsittel D, Jones W, Castranova V. 2002. COMPARATIVE PULMONARY TOXICITY OF BLASTING SAND AND FIVE SUBSTITUTE ABRASIVE BLASTING AGENTS. Journal of Toxicology and Environmental Health, Part A 65(16):1121-1140.\n*   Chemical Composition of Coal and Other Mineral Slags  \n    Stettler LE, Donaldson HM, Grant GC. 1982. Chemical composition of coal and other mineral slags. American Industrial Hygiene Association Journal 43(4):235-238.\n*   Comparison of Occupational Exposures Among Painters Using Three Alternative Blasting Abrasives  \n    Meeker JD, Susi P, Pellegrino A. 2006. Case Study. Journal of Occupational and Environmental Hygiene 3(9):80-84.\n*   Comparative Pulmonary Toxicity of 6 Abrasive Blasting Agents  \n    Hubbs AF, Minhas NS, Jones W, Greskevitch M, Battelli LA, Porter DW, Goldsmith WT, Frazer D, Landsittel DP, Ma JYC, Barger M, Hill K, Schwegler-Berry D, Robinson VA, and Castranova V.  \n    Comparative Pulmonary Toxicity of 6 Abrasive Blasting Agents. Toxicol. Sci. 2001 61: 135-143.\n*   Abrasive Blasting Agents: Designing Studies to Evaluate Relative Risk  \n    Hubbs A, Greskevitch M, Kuempel E, Suarez F, Toraason M. 2005. Abrasive Blasting Agents: Designing Studies to Evaluate Relative Risk. Journal of Toxicology and Environmental Health, Part A 68(11):999-1016.\n*   Substitute Materials for Silica Sand, Evaluation of Substitute Materials for Silica Sand in Abrasive Blasting  \n    This document contains the results from a contract that directed KTA-Tator, Inc. to conduct a three-phase study for the purpose of investigating relative levels of 30 different health-related agents and other attributes of surface preparation of the alternative abrasives to silica sand.\n\n【26】### U.S. Governmental Resources\n\n【27】_The links to external web sites included below are provided for informational purposes only. Citation should not be taken as endorsement by NIOSH of the web site content nor of the sponsoring organization._\n\n【28】Abrasive Blaster Dies of Carbon Monoxide Poisoning\n\n【29】Bureau of Labor Statistics – Injuries, Illnesses, and Fatalities (IIF) program\n\n【30】Electronic Library of Construction Occupational Safety & Health\n\n【31】Inadvertent Connection of Air-line Respirators to Inert Gas Supplies: OSHA Safety and Information Bulletin\n\n【32】OSHA Abrasive Blasting in Shipyard Employment\n\n【33】OSHA Respiratory Protection\n\n【34】OSHA Safety and Health Topic, Lead\n\n【35】OSHA Technical Link on Confined Spaces\n\n【36】OSHA Technical Link on Falls\n\n【37】OSHA Training Materials for Silicosis\n\n【38】U.S. Department of Transportation Federal Highway Administration: Safety and Health on Bridge Repair, Renovation, and Demolition Projects\n\n【39】### Non-U.S. Governmental Resources\n\n【40】_The links to external web sites included below are provided for informational purposes only. Citation should not be taken as endorsement by NIOSH of the web site content nor of the sponsoring organization._\n\n【41】A Review of Engineering Control Technology for Exposures Generated During Abrasive Blasting Operations  \nFlynn MR, Susi P. 2004. A Review of Engineering Control Technology for Exposures Generated During Abrasive Blasting Operations. Journal of Occupational and Environmental Hygiene 1(10):680-687.\n\n【42】American National Standards Institute\n\n【43】The Center for Construction Research and Training (formerly the Center to Protect Workers’ Rights)\n\n【44】Control Technology for Crystalline Silica Exposures in Construction: Wet Abrasive Blasting  \nMazzuckelli L, Golla V, Heitbrink W. 2004. Case Studies. Journal of Occupational and Environmental Hygiene 1(3):26-32.\n\n【45】Ergonomics of Abrasive Blasting: A Comparison of High Pressure Water and Steel Shot  \nRosenberg B, Yuan L, Fulmer S. Ergonomics of Abrasive Blasting: A comparison of high pressure water and steel shot, Applied ErgonomicsVolume 37, Issue 5, September 2006, Pages 659-667.\n\n【46】Health and Safety Executive – COSHH Essentials in Construction: Silica  \nA series of informative guides that describe various processes and tasks which may generate respirable crystalline silica. These guides examine work tasks in 8 different industries, and describe areas to reduce exposure to workers. The HSE-UK developed the guides, which were then translated into Spanish by NIOSH.  \nen Español\n\n【47】Laborer’s Health and Safety Fund of North America\n\n【48】Michigan State University – Abrasive Blasting, Preventing Silicosis\n\n【49】Mount Sinai-Irving J. Selikoff Center for Occupational & Environmental Medicine – Guides for Managing Lead and Silica Control Programs in Construction\n\n【50】The National Safety Council\n\n【51】Stop Silicosis in Sandblasters Use Silica Substitutes  \nNew Jersey Occupational Health and Surveillance Program\n\n【52】WorkSafe Health & Safety Centre for Construction", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8d279c56-3dd8-44b6-8bea-59500c421621", "title": "Meteorologic Influences on Plasmodium falciparum Malaria in the Highland Tea Estates of Kericho, Western Kenya", "text": "【0】Meteorologic Influences on Plasmodium falciparum Malaria in the Highland Tea Estates of Kericho, Western Kenya\nHighland malaria has returned to the tea estates of western Kenya after an absence of nearly 30 years ( 1 – 3 ). Altitude and weather influence malaria epidemiology in highland areas because of the slowing of parasite development within the anopheline vectors at lower temperatures ( 4 ). Increased malaria incidence in unstable transmission areas has been variously attributed to changes in land-use patterns ( 5 ); population migration ( 6 , 7 ); changes in mosquito vector populations ( 8 ); breakdown in provision of health services ( 9 ), especially insecticide spraying ( 10 , 11 ); drug resistance ( 12 – 16 ); and meteorologic changes ( 17 , 18 ), particularly global warming ( 19 – 25 ).\n\n【1】We investigated whether climate changes could be implicated in the reemergence of malaria in a unique 30-year malaria and meteorologic time series, collected from the health-care system on a tea plantation in the western highlands of Kenya. Our detailed substudy included site-specific meteorologic and malariometric data from a larger analysis of trends in meteorologic conditions across East Africa from 1911 to 1995 ( 26 – 28 ). Our previous studies have also examined various aspects of the epidemiology of malaria in the Kenyan highlands ( 29 , 30 ).\n\n【2】### Methods\n\n【3】##### Study Site and Clinical Data\n\n【4】Long-term malaria illness and total hospital admissions data (January 1966–December 1995) exist from a large tea plantation in Kericho, Kenya, which is operated by Brooke Bond Kenya Ltd. ( 3 , 31 , 32 ). The plantation, located in the western Rift Valley highlands, covers an area of approximately 141 km <sup>2 </sup> and ranges from 1,780 to 2,225 m above mean sea level. Epidemic malaria was first recorded on the Kericho tea estates during World War II and was eventually controlled by a combination of mass administration of proguanil and residual insecticide spraying during the late 1940s ( 2 ). Currently, the Brooke Bond Kenya Ltd. plantation consists of approximately 20 separate tea estates with a total of 50,000 employees and dependents, who receive their medical care from the company-operated health systems. The company hospital maintains a 24-hour, 7-day clinical admission service for patients who need intensive clinical management. Stained blood smears from patients with suspected malaria are examined; this procedure, in combination with further supportive clinical and laboratory procedures, is used to confirm a primary malaria diagnosis. Case numbers in Kericho can be treated as incidence figures since the population eligible for health care remained at approximately 50,000 during the recording period ( 32 ). No centralized preventive chemoprophylaxis, vector control, or bed-net distribution has been implemented since the late 1950s. A substantial minority of the tea estate workers originate from the holoendemic Lake Victoria area and travel back and forth intermittently to their home areas; however, this travel pattern has been occurring since the road was surfaced in the 1950s and has not changed recently. This study was conducted under a protocol approved by the Kenyan National Ethical Review Committee (SSC 484) and the U.S. Army Office of the Surgeon General (WRAIR 682).\n\n【5】### Meteorologic Data\n\n【6】Two meteorologic datasets were compiled. Point locality measurements of mean monthly temperature (°C) and monthly total rainfall (mm) were obtained from the Tea Research Foundation meteorologic station on the Kericho tea estates for the 1966–1995 period. Climate data were also obtained from a global 0.5 x 0.5° (approximately 55 x 55 km \\[3,025 km <sup>2 </sup> \\] at the equator) gridded dataset of monthly terrestrial surface climate for the 1966–1995 period (33,34) (available from: URL: http://www.cru.uea.ac.uk/link ). The dataset was used to ensure that results from the single meteorologic station were in agreement with data from a wider geographic area; this procedure also allowed a wider range of climate variables, including temperature extremes, to be tested. Primary variables of precipitation (mm), mean temperature (°C), and diurnal temperature range (°C) were available and interpolated from extensive meteorologic station data by using angular distance-weighted averaging of anomaly fields. The secondary variable of vapor pressure was also provided, interpolated where available, and calculated from primary variables, when the coverage of meteorologic stations was insufficient. Minimum and maximum monthly temperature estimates were created by subtracting or adding, respectively, half the diurnal temperature range from mean monthly temperature. Time series were derived by using an extraction routine developed in ENVI (Research Systems Inc., Boulder, CO) with georeferencing information for Kericho (0.33°S, 35.37°E), obtained from Encarta (Microsoft, Seattle, WA).\n\n【7】To investigate whether a combination of meteorologic conditions was changing and thus facilitating the resurgence of malaria, we also categorized months as suitable for _Plasmodium falciparum_ transmission if they had a mean monthly temperature exceeding 15°C (since temperatures experienced by the indoor resting _Anopheles gambiae_ vectors are likely to be 3°C–5°C higher) and monthly rainfall totals exceeding 152 mm ( 1 , 4 ) by using the gridded climatology data. The numbers of suitable months for transmission were summed, totaled for each year, and tested for the 1966–1995 period.\n\n【8】##### Statistical Analyses\n\n【9】To test for trends in the climate and malaria suitability time series, we estimated the following regression equation:\n\n【10】Δy <sub>t </sub> \\= α + β t + γ y <sub>t-1 </sub> \\+  δ <sub>i </sub> Δy <sub>t-1 </sub> \\+  μ <sub>j </sub> d <sub>j </sub> \\+ ε <sub>t </sub> (1)\n\n【11】where y is the variable of interest; α, β, γ, and μ <sub>j </sub> are regression parameters; ε <sub>t </sub> is a normally distributed error term with mean zero; and t is a deterministic time trend. The centered dummy variables d <sub>j </sub> model the monthly seasonal variations in climate. The coefficients μ <sub>j </sub> sum to zero. Δ is the first difference operator. The lagged values of the dependent variable model the serial correlation in the dependent variable. We chose the number of lags, p, using the adjusted R-square statistic. The maximal number of lags p considered was 24.\n\n【12】If the time series y can be characterized as the sum of a stationary stochastic process and a linear time trend, then the appropriate test for the trend is a t test on β in ( 1 ). If the series is a random walk, however, or a more complex stochastically trending process, the critical levels for the distribution of the t score in this regression are much greater than usual ( 35 ), and alternative tests should be employed. Since many climate time series contain a stochastically trending component ( 36 ), the nature of the series must be explored before testing for climate change. This methodologic issue complicates the evaluation of the significance of trends established with standard regression procedures often used in such studies.\n\n【13】If γ =0 (a unit root in the autoregressive process) and β =0, then y is a random walk. The random walk may also have a deterministic drift term (α≠0). In either case, however, the series is nonstationary, and classical regression inference does not apply. The nonstandard distributions of α, β, and γ have been tabulated by Dickey and Fuller ( 37 , 38 ). We first tested for the presence of a unit root by evaluating the t statistic for γ against its nonstandard distribution. The critical value for this so-called Augmented Dickey-Fuller at the 5% level is -3.45. Values of the t statistic for γ more negative than this critical value indicate that the series is not a random walk and vice versa. If the null hypothesis is rejected, then the t statistics associated with α and β are normally distributed. If the unit root hypothesis is accepted, then these statistics also have nonstandard distributions. The correct test for a trend is then the t test on α in ( 1 ) with the omission of the linear trend. The test’s critical value at the 5% significance level is 2.54. The results of these tests are presented in the Table .\n\n【14】We also regressed temperature and rainfall data from the meteorologic station at Kericho on the same variables from the interpolated climatology ( 33 , 34 ) by using a variety of formulations including levels, logarithms, and a regression adjusted for heteroscedasticity. We then tested whether the slope coefficients were significantly different from unity, which should not be the case if the gridded dataset is a good proxy for the climate at Kericho.\n\n【15】### Results\n\n【16】Figure 1\n\n【17】Figure 1 . Malaria, hospital admissions, and meteorologic station data, Kericho tea estate, 1966–1995. Malaria incidence (a) total hospital admissions (b) mean monthly temperature (c) and total monthly rainfall (d) are all plotted with...\n\n【18】Figure 2\n\n【19】Figure 2 . Climate and malaria suitability data for the Kericho area from the global gridded climatology data, including meteorologic and malaria suitability time series. Minimum (bottom), mean (middle) and maximum (top) monthly temperature...\n\n【20】During the period 1966–1995, malaria incidence increased significantly (p=0.0133) while total (i.e., malarial and other) admissions to the tea estate hospital showed no significant change ( Table and Figure 1a,b ). Measurements of mean monthly temperature and total monthly rainfall also showed no significant changes ( Table and Figure 1c,d ). Similar results were shown by the climatology data interpolated from a wider area. Mean, maximum, and minimum monthly temperatures; precipitation; and vapor pressure all demonstrated no significant trends ( Table ; Figure 2a,b,c ). Moreover, the interpolated climatology data, when transformed into month of malaria transmission suitability ( 1 , 4 ), again showed no significant changes ( Table ; and Figure 2d ).\n\n【21】Results were very similar, though significance levels varied, between the three formulations of the regression model that compared the local meteorologic station data and those from the interpolated climatology data ( 33 , 34 ). The coefficient for the regression of the meteorologic station rainfall data on the interpolated climatology precipitation data is in every case not significantly different from unity. Significance levels are 10% for the model in levels, 18% for the heteroscedasticity-adjusted model, and 96% for the logarithmic model. In the regression of the two temperature series, however, the coefficient is significantly different from unity in every case, as is a joint test statistic for the two slope coefficients.\n\n【22】### Discussion\n\n【23】The resurgence of _P. falciparum_ malaria in the East African highlands ( 3 , 8 , 18 , 26 , 40 – 44 ) has led several researchers to speculate that climate change is a predominant cause ( 23 , 45 – 50 ). On the basis of these studies, which have been disputed by experts in vector-borne disease biology ( 10 , 27 – 29 , 51 , 52 ), and some biological modeling, which has been robustly criticized ( 53 ), the International Panel on Climate Change has recently concluded with “medium-to-high confidence” that there will be a net increase in the range and incidence of malaria ( 49 ) **;** the results of our work do not support these conclusions.\n\n【24】Malaria incidence increased significantly (p=0.0133) during the 1966–1995 period, while total admissions remained unchanged. Besides an increase in local malaria transmission, two other factors may have influenced the increase in malaria hospitalizations. An increase in malaria severity indicated by an increased case-fatality rate (from 1.3% in the 1960s to 6% in the 1990s) is most likely linked to chloroquine resistance, which we believe to be the probable cause of much of the overall increase in malaria transmission ( 32 ). Travel to and from the Lake Victoria region by a minority of the tea estate workers also exerts an upward influence on malaria transmission in Kericho since such travel increases the numbers of workers asymptomatically carrying gametocytes, which infect mosquitoes for further human infection. This complex topic is the subject of a future publication.\n\n【25】All climate variables, whether from the Kericho tea estate meteorologic station or the pixel covering Kericho in the global climatology dataset showed no significant trends, despite the fact that equivalence tests showed some significant differences between the temperature time series—findings that are in agreement with a broader geographic analysis of East African data from 1911 to 1995 ( 26 ) and lend support to the appropriateness of interpolated climate data for use in these investigations. We also think that, when examining trends in meteorologic phenomena, epidemiologists should use more robust statistical techniques for the reasons outlined in the methods. The results of this detailed examination of coincident empirical data do not support the widespread, recent speculation regarding malaria resurgences in response to climate change. No aspect of climate has changed significantly—neither the temperature extremes (maximum and minimum) nor the periods when meteorologic data were transformed into months when malaria transmission is possible. Further study has also shown that variability in these meteorologic variables, independent of any longer term trends, has decreased ( 54 ). We must therefore look elsewhere for the causes of these resurgences ( 27 , 28 , 32 ). These factors are likely to vary. In Kericho, however, increased chloroquine resistance has been strongly argued to be the cause, since all other relevant environmental and sociologic factors are unchanged ( 32 ).\n\n【26】The attraction of the global warming hypothesis as an explanation of highland malaria is the existence of a continental trend toward global warming coincident with a trend toward increasing malaria incidence in several parts of Africa, ranging from Senegal ( 13 , 14 ) to Madagascar ( 10 ). Where such malaria increases have been examined in detail, however, alternative explanations such as discontinuation of anti-vector measures in Madagascar ( 10 ) or chloroquine resistance in Senegal appear to be more likely causes ( 13 , 14 ). Malaria epidemiology is greatly influenced by a range of local factors, making a consistent continent-wide explanation seem unlikely ( 28 , 52 ).\n\n【27】We do not argue that meteorologic conditions have no immediate impact on the seasonal dynamics and incidence of malaria or that climate change is probably not an important future concern in public health. Rather we urge some caution in the interpretation of synonymous changes in climate over wider areas and local changes in malaria incidence.\n\n【28】Col. G. Dennis Shanks is the former director of the U.S. Army Component of the Armed Forces Institute of Medical Research in Bangkok, Thailand, which is a part of the Walter Reed Army Institute of Research. He is a physician trained in pediatrics and tropical medicine whose main professional interests are malaria chemotherapy, malaria epidemiology, and clinical trials in developing countries.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51ae2836-ed4a-495d-a200-bd1199b6abd6", "title": "A Physical Activity Intervention and Changes in Body Mass Index at a Middle School With a Large American Indian Population, Oklahoma, 2004–2009", "text": "【0】A Physical Activity Intervention and Changes in Body Mass Index at a Middle School With a Large American Indian Population, Oklahoma, 2004–2009\n===============================================================================================================================================\n\n【1】RESEARCH BRIEF — Volume 13 — December 1, 2016\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### June E. Eichner, PhD <sup>1 </sup> ; Olakunle A. Folorunso, MPH <sup>1 </sup> ; William E. Moore, PhD <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Eichner JE, Folorunso OA, Moore WE. A Physical Activity Intervention and Changes in Body Mass Index at a Middle School With a Large American Indian Population, Oklahoma, 2004–2009. Prev Chronic Dis 2016;13:150495. DOI: http://dx.doi.org/10.5888/pcd13.150495 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Abstract\n*   Objective\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n\n【9】Abstract\n--------\n\n【10】School-based interventions can reach children and adolescents and aid in reducing the prevalence of childhood obesity. A physical education class that engaged middle school students in a daily 1-mile walk or run and other team sports was developed in a rural school in southwestern Oklahoma with a large American Indian population. Body mass index _z_ scores decreased among boys and were stable among girls in the intervention group compared with students who did not participate in the intervention. A daily required walk or run may help to establish a physical activity habit with all of its associated benefits.\n\n【11】Top\n\n【12】Objective\n---------\n\n【13】Even with the recent reports of stabilizing rates, the prevalence of childhood overweight and obesity is at an all-time high (1). American Indian children have disproportionately high rates of overweight and obesity (2,3), but because these populations of children are small and isolated in rural areas, intervention studies rarely include them. The objective of this study was to compare changes in body mass index (BMI) _z_ scores of participants enrolled in a physical activity intervention at a middle school in southwestern Oklahoma with the BMI z scores of the nonparticipating school population.\n\n【14】Top\n\n【15】Methods\n-------\n\n【16】The Anadarko Public School System, with approximately 2,000 students in grades kindergarten through 12, serves a largely American Indian population from various tribes, including Apache, Fort Sill Apache, Caddo, Comanche, Delaware, Kiowa and Wichita. Sixty-five percent of the students are American Indian, 25% are white, 5% are African American, and 5% are Latino. The poverty rate in the district population is about twice the state average, the unemployment rate is 3 times higher, and the average household income is 82% of the state average (4). Study participants were sixth-, seventh-, and eighth-grade students aged 12 to 15 years attending Anadarko Middle School, which has an average fall enrollment of 442 (4).\n\n【17】We developed the program, Middle School Opportunity for Vigorous Exercise (MOVE), with assistance from the school’s physical education teacher and principal. The program was based on an exercise prescription suggested by the 1996 Report of the Surgeon General, _Physical Activity and Health_ (5). The program’s objective was to habituate middle school students to daily walking or running. Participants walked or ran 1 mile each school day and then engaged in a team activity such as basketball, soccer, football, dodge ball, or volleyball. Class size was approximately 20 students. The intervention took place during 5 school years beginning in August 2004 and ending in May 2009.\n\n【18】We measured changes in BMI among participants and nonparticipants. To be included in this analysis, data had to be for first-time MOVE participants who completed 2 consecutive semesters of the class (fall and spring of 1 school year) and had both pre-intervention and post-intervention BMI measurements (weight \\[kg\\]/height \\[m <sup>2 </sup> \\]). Some students attended the MOVE class for only 1 semester or several nonconsecutive semesters. At the study’s midpoint (2006–2007), we measured height and weight of all available middle school students at the beginning and end of the school year. We measured changes from pre-intervention to post-intervention in BMI _z_ score and BMI percentile according to the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (6). We analyzed differences in changes between the intervention and comparison groups using repeated measures analyses of variance (ANOVA) and Statistica software (StatSoft \\[www.statsoft.com\\]); an α of less than .05 determined significance. Approval was obtained from the University of Oklahoma Health Sciences Center institutional review board.\n\n【19】Top\n\n【20】Results\n-------\n\n【21】During the study period, 182 unique students enrolled in the MOVE class; of these, 50 students transferred out of the school or district or joined a sports group or band. Sixty-six students (46 boys, 20 girls) met the criterion of 2-semester consecutive attendance in the MOVE class ( Table 1 ). Of these, 57.6% were American Indian; 19.7% were white, 15.2% were Latino, 6.1% were African American, and 1.5% were Asian. At pre-intervention assessment, 10 of 66 (15.2%) students were overweight, and 19 (28.8%) were obese. Mean BMI _z_ scores remained the same among girls participating in MOVE (from 0.7 to 0.7) and increased for nonparticipating girls (from 1.1 to 1.2). Mean BMI _z_ score decreased among boys participating in MOVE (from 0.8 to 0.7) and increased among nonparticipating boys (from 1.1 to 1.2).\n\n【22】MOVE participants overall (boys and girls) had a significantly smaller increase in BMI _z_ score than did nonparticipants (BMI _z_ score × treatment, _P_ \\= .01) ( Table 2 ). We found no significant difference between sexes when we compared changes in BMI z scores of participants with those of nonparticipants (BMI _z_ score × treatment × sex; _P_ \\= .49).\n\n【23】Top\n\n【24】Discussion\n----------\n\n【25】Overall, middle school students participating in the MOVE class for 1 year had better BMI _z_ scores than nonparticipating students. Among MOVE participants, mean BMI _z_ scores decreased for boys and were stable for girls. Among nonparticipants, mean BMI _z_ scores increased for both sexes. The large number of students not completing the 1-year MOVE sequence was due to competing requirements, transfers out of the school system, and sports or band practice. MOVE was not a required course; students with the greatest need for the program may have dropped out. Although our program was small compared with large, multicenter trials, it was structured to achieve a predetermined amount of movement each day. Health education and a curriculum that encourages a healthy lifestyle are important, but a minimal amount of daily walking or running may also be required to maintain or decrease BMI. Our physical activity program showed evidence of impact on the primary outcome of BMI _z_ score. Conducted under real-world circumstances, our program demonstrated that improvements in BMI can be achieved by providing a well-regimented program of moderate to vigorous physical activity.\n\n【26】Top\n\n【27】Acknowledgments\n---------------\n\n【28】We thank the students, staff, faculty and administrators of the Anadarko Public School System for their cooperation, collaboration, and support, and we acknowledge the contributions of Terry Wilson and Wesley Wilson for their skill and enthusiasm for working with young people. This article and the research it describes were funded through the CDC Prevention Research Centers’ cooperative agreements with University of Oklahoma Prevention Research Center, no. U48/CCU610817 and no. U48-DP-000026, and National Institutes of Health grant no. P20 MD000528. The contents are solely the responsibility of the authors and do not necessarily represent the official views of CDC, the National Institutes of Health, the Anadarko School District, or the Anadarko community. The authors declare no conflict of interest.\n\n【29】Top\n\n【30】Author Information\n------------------\n\n【31】Corresponding Author: June E. Eichner, PhD, University of Oklahoma Health Sciences Center, CHB Room 354, 801 NE 13th St, Oklahoma City, OK 73104. Telephone: 405-271-2229. Email: june-eichner@ouhsc.edu .\n\n【32】Author Affiliations: <sup>1 </sup> University of Oklahoma Health Sciences Center, Oklahoma City, Oklahoma.\n\n【33】Top\n\n【34】References\n----------\n\n【35】1.  Ogden CL, Carroll MD, Kit BK, Flegal KM. Prevalence of childhood and adult obesity in the United States, 2011–2012. JAMA 2014;311(8):806–14. CrossRef external icon PubMed external icon\n2.  Eichner JE, Moore WE, Perveen G, Kobza CE, Abbott KE, Stephens AL. Overweight and obesity in an ethnically diverse rural school district: the Healthy Kids Project. Obesity (Silver Spring) 2008;16(2):501–4. CrossRef external icon PubMed external icon\n3.  Moore WE, Stephens A, Wilson T, Wilson W, Eichner JE. Body mass index and blood pressure screening in a rural public school system: the Healthy Kids Project. Prev Chronic Dis 2006;3(4):A114. PubMed external icon\n4.  Oklahoma School Report Card, 2005, 2006, 2007, 2008, 2009. Education Oversight Board/Office of Accountability. http://www.SchoolReportCard.org. Accessed July 1, 2016.\n5.  Centers for Disease Control and Prevention (CDC). Physical activity and health: a report of the Surgeon General. US Department of Health and Human Services, CDC; 1996.\n6.  CDC growth charts: United States, 2000. http://www.cdc.gov/growthcharts/. Accessed January 15, 2010.\n\n【36】Top\n\n【37】Tables\n------\n\n【38】#####  Table 1. Characteristics at Pre-Intervention Assessment of Participants in MOVE Physical Education Class and Nonparticipants, Anadarko, Oklahoma, 2004–2009\n\n| Variable | Nonparticipants (n = 287) | MOVE Participants (n = 66) | _P_ Value |\n| --- | --- | --- | --- |\n| **Age (SD), y** | 12.8 (1.0) | 13.4 (0.8) | <.01 a |\n| **Sex, n (%)** | **Sex, n (%)** | **Sex, n (%)** | **Sex, n (%)** |\n| Female | 158 (55.1) | 20 (30.3) | <.01 b |\n| Male | 129 (44.9) | 46 (69.7) | <.01 b |\n| **Race, n (%)** | **Race, n (%)** | **Race, n (%)** | **Race, n (%)** |\n| American Indian | 178 (62.0) | 38 (57.6) | .58 b |\n| Other | 109 (38.0) | 28 (42.4) | .58 b |\n| **BMI status, n (%)** |  |  |  |\n| Underweight | 4 (1.4) | 2 (3.0) | .23 b |\n| Healthy weight | 121 (42.2) | 35 (53.0) | .23 b |\n| Overweight | 60 (20.9) | 10 (15.2) | .23 b |\n| Obese | 102 (35.5) | 19 (28.8) | .23 b |\n| **BMI percentile mean (SD)** | 78.6 (24.7) | 69.6 (31.1) | .01 a |\n| **BMI _z_ score c mean (SD)** | 1.1 (1.0) | 0.8 (1.3) | .03 a |\n| **BMI _z_ score c mean converted to percentile (SD)** | 86.9 (84.6) | 77.6 (89.6) | — |\n\n【40】Abbreviations: BMI, body mass index; MOVE, Middle School Opportunity for Vigorous Exercise.  \n<sup>a </sup> Determined by _t_ test.  \n<sup>b </sup> Determined by Fisher exact test.  \n<sup>c </sup> BMI _z_ score is based on Centers for Disease Control and Prevention 2000 growth charts (5).\n\n【41】#####  Table 2. Repeated Measures Analysis of Variance of BMI _z_ Score <sup>a </sup> : MOVE Participants vs Nonparticipants, Anadarko, Oklahoma, 2004–2009\n\n| Measure | Sum of Squares | Degrees of Freedom | Mean Square | _F_ | _P_ Value |\n| --- | --- | --- | --- | --- | --- |\n| **Between participants** | **Between participants** | **Between participants** | **Between participants** | **Between participants** | **Between participants** |\n| Intercept | 326.48 | 1 | 326.47 | 150.05 | <.001 |\n| Sex | 0.01 | 1 | 0.01 | 0.01 | .93 |\n| Treatment | 16.75 | 1 | 16.75 | 7.70 | .01 |\n| Sex × Treatment | 0.16 | 1 | 0.16 | 0.08 | .78 |\n| Error | 759.32 | 349 | 2.18 | — | — |\n| **Within participants** | **Within participants** | **Within participants** | **Within participants** | **Within participants** | **Within participants** |\n| BMI _z_ score | 0.02 | 1 | 0.02 | 0.64 | .42 |\n| BMI _z_ score × sex | 0.09 | 1 | 0.09 | 3.15 | .08 |\n| BMI _z_ score × treatment | 0.18 | 1 | 0.18 | 6.62 | .01 |\n| BMI _z_ score × sex × treatment | 0.01 | 1 | 0.01 | 0.49 | .49 |\n| Error | 9.52 | 349 | 0.03 | — | — |\n\n【43】Abbreviation: BMI, body mass index; MOVE, Middle School Opportunity for Vigorous Exercise.  \n<sup>a </sup> BMI _z_ score is based on Centers for Disease Control and Prevention 2000 growth charts (5).\n\n【44】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "57c784cb-8cd5-4e1b-9ad3-ec8809cb8071", "title": "Street Cleaning Trucks as Potential Sources of Legionella pneumophila", "text": "【0】Street Cleaning Trucks as Potential Sources of Legionella pneumophila\nLegionnaires’ disease (LD) is an acute pneumonia caused by inhalation or aspiration of aerosols contaminated with _Legionella_ bacteria. _Legionella_ spp. are ubiquitous in freshwater aquatic habitats. Multiplication of _Legionella_ in artificial water systems is facilitated by temperatures around 35°C and factors such as lack of disinfection, water stagnation, and poor maintenance ( _1_ ).\n\n【1】In Spain, 925 cases of LD were reported in 2014, of which 82% were sporadic cases, not related to outbreaks ( _2_ ). LD outbreaks have been related to cooling towers, spa pools, and water distribution systems ( _2_ _–_ _4_ ), which are considered by regulations in Spain to have a high probability of proliferation of _Legionella_ ( _5_ ).\n\n【2】The identification of exposure in sporadic cases provides a good opportunity to enhance understanding of reservoirs for _Legionella_ in relatively rare sources ( _6_ _,_ _7_ ). This case report summarizes the environmental study conducted to identify the possible source of exposure of a street cleaning worker who contracted LD.\n\n【3】### The Study\n\n【4】A 58-year-old man in Barcelona who smoked began to show symptoms compatible with legionellosis on July 27, 2015. On August 11, LD was diagnosed in the patient; this diagnosis was confirmed by a urinary antigen test (UAT). On August 14, the case was reported to the Public Health Agency of Barcelona.\n\n【5】A trained public health practitioner interviewed the affected patient by using a standardized questionnaire to obtain information on demographic data, personal risk factors, and activities during the 15 days before the onset of illness. The only exposure to sources of aerosolized water highlighted by the epidemiologic survey was an occupational exposure to high-pressure water hoses from street cleaning trucks during the incubation period. The trucks used had 2-m <sup>3 </sup> water tanks with an internal foam lining ≈15 cm thick. The purpose of the foam was to help steady the truck when in motion. The truck tanks were filled with groundwater untreated with chlorine or with drinking water from the town’s water supply network, depending on their availability during the journey. At the end of the day, the tanks were emptied but the internal foam remained impregnated with water. Once a year, the tanks were disinfected with a 20-ppm chlorine solution for 2 hours; hoses were replaced relatively often because of deterioration. However, the interior walls of the tanks were never cleaned, and the foam lining was never replaced.\n\n【6】Three days after the interview, we took samples from the 4 trucks that the worker used in the 15 days before symptom onset. We took water samples and smears from the hoses and water tanks to test them for _Legionella_ . We also took a water sample from the groundwater tank where the trucks were usually filled.\n\n【7】We analyzed the samples in accordance with the ISO 11731 method for counting _Legionella_ spp. ( _8_ ). In addition, we obtained counts of _L. pneumophila_ serogroup 1 and of serogroups 2–15. We performed molecular typing of the strains of _L. pneumophila_ serogroup 1 isolated in the different samples via DNA macrorestriction and subsequent pulsed field electrophoresis. We calculated the similarity coefficient and displayed the results of molecular typing as a dendrogram generated by the FPQuest software package (Bio-Rad Laboratories, Hercules, CA, USA).\n\n【8】Water sample temperatures ranged from 26°C to 28°C. We detected _L. pneumophila_ serogroup 1 in the water from 2 of the 4 trucks sampled. _L. pneumophila_ serogroup 1 counts were 150 CFU/L for truck 1 and 1,000 CFU/L for truck 2 ( Table ). We did not detect _Legionella_ in the sample of the groundwater tank or in the hose smears of either truck.\n\n【9】After _Legionella_ detection, the tanks were disinfected and cleaned following the cleaning procedure described by law for hot water systems ( _5_ ). After 15 days, we drew new water samples to assess the effectiveness of the disinfection, and after 23 days, we took samples of the foam and from the internal surfaces of the tanks of both trucks. We obtained samples of foam by cutting out pieces (4 × 4 × 15 mm) with a sterile scalpel and placed them in sterile containers with Ringer’s solution for subsequent _Legionella_ analysis. The results indicated the presence of _L. pneumophila_ serogroup 1 in the foam and water tanks of both trucks and in the hose water sample and the smear from the tank of truck 1 ( Table ).\n\n【10】Figure\n\n【11】Figure . Dendrogram generated by FPQuest software showing the pattern of SfiI bands for isolates of _Legionella pneumophila_ from 2 street cleaning trucks, Barcelona, Spain, 2015 _._ Strains are identified with code of truck...\n\n【12】The results of molecular typing indicated that clones were detected in truck 1 (Dice similarity coefficient 100%) belonging to the same strain found in the hose water, in the tank water, and in the foam. We also detected another clone, genetically related to the first (Dice similarity coefficient 89%), in the tank water, in the smears of the surface of the tank, and in the foam. We identified 2 different molecular patterns in the strains isolated in the tank water and in the foam in truck 2; both patterns appeared in both tank water and foam ( Figure ). Regarding the foam, we obtained similar results in the foam underneath decorative stones in an ornamental fountain that caused an LD outbreak in a hospital ( _9_ ).\n\n【13】Weather conditions were warm in July 2015, with an average temperature >25°C even at night ( _10_ ). These temperatures, along with water stagnation, would have favored proliferation of the bacteria in the water tanks.\n\n【14】Although the mode of transmission is not clear, the high-pressure hose used by the worker probably discharged aerosols containing bacteria that could be inhaled ( _7_ ). The worker did not use a protective facemask and was the second employee in the same company who was working with cleaning trucks to contract LD. Four years earlier, another case of LD had occurred in similar conditions (J. Cayla, Public Health Agency of Barcelona, pers. comm., 2011 July 22).\n\n【15】Published criteria ( _11_ ) indicate that cleaning trucks are a potential source of _Legionella_ . The level of evidence of this study corresponds to level III, because _L. pneumophila_ subgroup 1 was isolated in the trucks used by the worker but clinical and environmental strains could not be compared because he was diagnosed by a UAT. Furthermore, because UAT detects only _L. pneumophila_ serogroup 1, lack of respiratory culture can lead to missed diagnoses of other species and serogroups; fortunately, this limitation does not apply here.\n\n【16】A report describing similar exposure concerned an asphalt paving machine causing an outbreak of _Legionella_ in 2009 ( _6_ ). Like the trucks in this study, this was a mobile source, and the use of untreated water had also contributed to tank pollution. Although some studies have associated LD with occupations such as gardening ( _12_ _,_ _13_ ) and professional driving ( _14_ ), no studies have related LD to street cleaning.\n\n【17】### Conclusions\n\n【18】We detected _L. pneumophila_ serogroup 1 in 2 of the 4 trucks used by the affected worker during the incubation period; either truck could have been the source of exposure. The results of this study show that the internal foam of the tanks could act as a reservoir for _L. pneumophila_ and that maintenance and routine cleaning (without removing the foam) did not prevent _Legionella_ proliferation.\n\n【19】After the occurrence of this case, the internal foam in the truck tanks was removed, and the water tanks were cleaned again. The company responsible for the street cleaning trucks adopted a water management plan, with the agreement of the Public Health Agency of Barcelona, based on Hazard Analysis and Critical Control Points methodology ( _15_ ) and stricter control measures. Workers are now required to wear personal protective equipment during work-related exposure.\n\n【20】Ms. Valero is a public health technician in the Barcelona Public Health Agency in Barcelona, Spain. Her main research interests are environmental health protection research and the study of potential sources of _Legionella._", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3a6cdd00-cf62-49f8-9f1b-d498cef6d3c3", "title": "High Infection Attack Rate after SARS-CoV-2 Delta Surge, Chattogram, Bangladesh", "text": "【0】High Infection Attack Rate after SARS-CoV-2 Delta Surge, Chattogram, Bangladesh\nTo the Editor: After an initial serosurvey ( _1_ ) to understand the prevalence of total antibodies to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) in residents of the Sitakunda subdistrict was completed, a large epidemic wave hit the area, and nearly all publicly available samples genotyped via GISAID ( https://www.gisaid.org ) were the SARS-CoV-2 Delta variant ( _2_ , _3_ ). Of the total confirmed infections during the entire pandemic from the Chattogram District, 48.4% (48,253) were reported June 14–August 31, 2021. During September 21–October 9, 2021, we revisited all enrolled households and collected blood from 84% (1,938/2,307) of those tested in our initial serosurvey ( Appendix Figure).\n\n【1】We tested 721 of the initially seronegative participants who agreed to a second blood draw using the same Wantai total Ab receptor-binding domain assay and found that 68% (492/721) had seroconverted in the approximately 3-month period between survey rounds ( Appendix Table 1). Participation in the second round was not associated with serostatus in the first round. Among seropositive participants, 87 (18%) had received \\> 1 dose of SARS-CoV-2 vaccine, and 28.3% (140/492) of those who seroconverted reported having had a sudden onset of \\> 1 coronavirus disease–related symptom since the first serosurvey. Assuming no seroreversion between rounds, 88.2% (1,709/1,938) of participants providing blood in both rounds were seropositive by the second serosurvey. Using our previous methods ( _1_ ), we estimated an adjusted seroprevalence after the Delta wave of 88.2% (95% CrI 85.4%–90.8%) for all participants and 87.9% (95% CrI 85.2%–90.6%) when including only unvaccinated participants ( Appendix Table 2). Seroprevalence among children 1–9 years of age remained significantly lower when compared with 25–34 year olds (28% reduced risk for 1–4 and 16% for 5–9 year age groups; p<0.00001), unlike other age groups ( Appendix Table 2). Mirroring evidence from around the world, the Delta variant led to a significant increase in SARS-CoV-2 transmission in Bangladesh, leaving the vast majority of people with detectable serum antibodies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "79578e3d-5086-4cb0-8c35-37eaaeee2f57", "title": "Etiology of Severe Acute Respiratory Infections, Bangladesh, 2017", "text": "【0】Etiology of Severe Acute Respiratory Infections, Bangladesh, 2017\nIn April 2017, the Institute of Epidemiology Disease Control and Research (IEDCR) in Bangladesh noted an 89% increase in severe acute respiratory infections (SARI) compared with April 2016 through the National Influenza Surveillance Bangladesh (NISB) at 10 tertiary-care hospitals. During April 10–June 21, 2017, we conducted a case–control study to ascertain the cause of the outbreak and its associated risk factors.\n\n【1】We defined a SARI case as acute respiratory illness in a patient within 10 days of onset, with history of fever and cough, and requiring hospitalization ( _1_ ). We sought to enroll all adults \\> 18 years of age who were admitted to NISB hospitals with SARI. Staff screened patients for eligibility, obtained written informed consent, surveyed participants about demographics, and took combined nasal and throat swab samples. Patients who died in hospital wards before enrollment were ineligible. Within 2 days of case-patient enrollment, staff enrolled 2 asymptomatic controls, identified by convenience from the same hospitals’ outpatient clinics, surveyed them, and took combined nasal and throat swab samples. Patients who had fever or respiratory symptoms in the previous 14 days were ineligible to serve as controls. Swab specimens from case-patients and controls were tested for viral, bacterial, and fungal nucleic acids using FTD Respiratory pathogens 33 real-time reverse transcription PCR (Fast Track Diagnostics, http://www.fast-trackdiagnostics.com )( _2_ ).\n\n【2】We examined the association between SARI case-status and sociodemographic characteristics, preexisting conditions, and pathogens detected through multivariate logistic regressions. The investigation was judged to be public health action by the institutional review boards of the IEDCR and US Centers for Disease Control and Prevention (approval no. IEDCR/IRB/2016/17).\n\n【3】Figure\n\n【4】Figure . Participant enrollment in study of etiology of severe acute respiratory infections and influenza activities in Bangladesh, April–June, 2017. A) Enrollment of adults with severe acute respiratory infections (SARI) and their...\n\n【5】We identified 79 eligible SARI case-patients and 158 eligible controls from 5 NISB hospitals ( Figure ). Of these, 73 (92%) eligible patients and 146 (92%) eligible controls consented to participate ( Table ). Case-patients were more likely than controls to be male (89% vs. 77%; p = 0.041 by Fisher exact test) and have preexisting underlying chronic conditions (47% vs. 25%; p = 0.001), including asthma (36% vs. 12%; p = 0.000) and allergies (27% vs. 10%; p = 0.003) ( Table ). Although 34 (47%) of case-patients were treated with antibiotics, none were treated with antiviral drugs.\n\n【6】Fifty-three (73%) case-patients and 92 (63%) controls tested positive for \\> 1 pathogens ( Table ). Among 53 test-positive case-patients, 18 (25%) tested positive for influenza viruses ( Figure ), including 12 (67%) for influenza A(H1N1)pdm09, 5 (28%) for unsubtyped influenza A, and 1 (6%) for influenza C. Among 92 test-positive controls, 8 (5%) tested positive for influenza viruses ( Figure ), including 3 (38%) for influenza A(H1N1), 4 (50%) for unsubtyped influenza A, and 2 (25%) for influenza C, 1 of whom had a codetection of an unsubtyped influenza A virus.\n\n【7】Male sex (odds ratio \\[OR\\] 2.4, 95% CI 1.0−5.4), \\> 1 preexisting conditions (OR 2.7, 95% CI 1.5−4.8), asthma (OR 4.2, 95% CI 2.1−8.4), and history of allergies (OR 3.1, 95% CI 1.5−6.6) were more common among SARI case-patients than controls ( Table ). Any influenza virus (OR 5.7, 95% CI 2.3−13.7), and influenza A(H1N1) specifically (OR 9.4, 95% CI 2.6−34.4), was significantly associated with SARI status. Only influenza A(H1N1) (OR 11.4, 95% CI 2.7−47.4) remained associated with case status.\n\n【8】The surge in SARI during April 2017 was attributable to influenza viruses. Although influenza epidemics in Bangladesh typically occur during May–September ( _6_ ), our investigation suggests the 2017 season started a month early. The government of Bangladesh has purchased Northern Hemisphere formulation influenza vaccines for Hajjis (i.e., for persons entering Mecca) ( _7_ , _8_ ) but does not otherwise have a vaccination policy because its possible benefit has not been shown in Bangladesh. Influenza vaccines are sporadically available in Bangladesh’s private market but rarely used. Our findings suggest the potential utility of estimating the cost-benefit ratio of influenza vaccination among persons at high risk for SARI using Southern Hemisphere formulations, which incorporate the most current vaccine strains recommended by the World Health Organization ( _7_ ) before the April–May start of the Bangladesh influenza season.\n\n【9】Patients with SARI were frequently prescribed antibiotics but not antiviral drugs. Whether antibiotics benefit SARI patients or solely drive antibiotic resistance is unclear; however, observational data suggest the benefit of empiric antiviral drugs to prevent influenza complications during epidemics ( _9_ ). IEDCR recommended empiric treatment of patients with SARI with antiviral drugs during the 2009 pandemic but does not currently ( _10_ ). Quantifiying the cost-effectiveness of empiric antiviral treatment for SARI patients during influenza epidemics could be valuable, particularly when used within 48 hours of symptom onset, when antivirals are most effective ( _6_ , _9_ ).\n\n【10】This investigation had limitations. Controls were not randomly selected, and age and sex were not matched with case-patients. We limited our sample collection to the upper respiratory tract, did not have wells to identify influenza A(H3N2) RNA, and did not collect blood or other samples that would have more accurately detected bacterial infections.\n\n【11】Our investigation of a surge in SARI cases during April 2017 in Bangladesh suggests the value of surveillance and rapid response in identifying the etiology of outbreaks. Our findings also suggest the potential value of estimating the cost-effectiveness of influenza vaccination campaigns among groups at high risk for SARI administered as early as late March to early April and of empiric antivirals during Bangladesh’s influenza epidemics.\n\n【12】Dr. Rahaman, a current PhD candidate at the University of Adelaide, is a medical doctor with specialist training in field epidemiology, international public health, and health management. His research interests include infectious disease epidemiology, particularly influenza; zoonoses, such as Q fever prevention; environmental epidemiology; and a One Health approach.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d77fbf83-4476-4ed9-8b39-c3369d26bcff", "title": "Enterovirus D68 Outbreak in Children, Finland, August–September 2022", "text": "【0】Enterovirus D68 Outbreak in Children, Finland, August–September 2022\nWidespread enterovirus D68 (EV-D68) epidemics occurred in the United States and in other parts of the world in 2014, 2016, and 2018 ( _1_ , _2_ ). Low-level EV-D68 detection was reported in the United States during the COVID-19 pandemic in 2020 ( _3_ ). EV-D68 re-emerged in autumn 2021, prominently in Europe but also to some extent in the United States, and again in 2022, when reports emerged from the United States and Europe about a substantially increased circulation of EV-D68 associated with acute respiratory illnesses in children ( _4_ – _6_ ). Previous EV-D68 epidemics in Europe and the United States were associated with severe lower respiratory tract infections, wheezing illnesses, and, more rarely, acute flaccid myelitis or other neurologic presentations in children ( _7_ , _8_ ). The clinical manifestations during the outbreaks in 2022 have not been described in detail in the literature.\n\n【1】In Finland, EV-D68 cases were detected in 2014 and sporadically before and after that, but circulation of the virus has not been seen during recent years. All laboratory findings of enterovirus in Finland are reported in the Finnish National Infectious Diseases Register ,and some enteroviruses are typed. Hospitalizations of children because of respiratory illnesses increased rapidly in southwest Finland in August 2022. We screened children (≤16 years of age) admitted to Turku University Hospital, Turku, Finland, for respiratory or central nervous system illness associated with enterovirus (confirmed by reverse transcription PCR \\[RT-PCR\\]) and identified EV-D68 by type-specific RT-PCR and sequencing. We describe the outbreak and report the clinical features of 57 children hospitalized with EV-D68 in August–September 2022.\n\n【2】### The Study\n\n【3】Turku University Hospital is the only hospital providing inpatient care for children in the Southwest Finland Hospital District, covering a population of 470,000 people, including 77,000 children and adolescents ≤16 years of age. The hospital also serves as a tertiary facility for a larger area, but our study did not include patients referred from other hospitals. Hospital policy requires that all children and adolescents with symptoms of infection at admittance be screened by a rapid RT-PCR test for SARS-CoV-2, influenza A and B, and respiratory syncytial virus (RSV). A laboratory-developed triplex RT-PCR test for enterovirus, rhinovirus, and RSV ( _9_ ) or a 16-plex RT-PCR test for respiratory viruses including enterovirus (Allplex Respiratory Panels 1–3; Seegene, https://www.seegene.com ) is performed by using nasopharyngeal swab or cerebrospinal fluid specimens for patients with a respiratory tract or central nervous system illness and negative screening test ( Appendix ).\n\n【4】Figure 1\n\n【5】Figure 1 . Flowchart of the enterovirus D68 (EV-D68) testing process of children hospitalized with respiratory illness or suspected central nervous system (CNS) infection at Turku University Hospital, Turku, Finland, during August 1–September...\n\n【6】When we noticed an enterovirus outbreak, we aimed to genotype viruses in all detected cases. First, we subjected specimens positive for enterovirus to typing based on partial enterovirus viral protein (VP) 1 gene sequencing after seminested PCR amplification ( _10_ ). We observed a high prevalence of EV-D68, but the amplification succeeded only in specimens with high virus load (low cycle threshold value). Therefore, we applied an RT-PCR assay with EV-D68 VP1 gene-specific primers ( _11_ ). Because a portion of the specimens remained untyped, we subjected them to a nested amplification and sequencing for the VP4/2 gene region ( _9_ _,_ _12_ ) ( Figure 1 ; Appendix)\n\n【7】Figure 2\n\n【8】Figure 2 . Weekly number of enterovirus D68, coxsackievirus A6, and nontyped enteroviruses in children hospitalized with enteroviral illness in southwest Finland during July 25–October 2, 2022.\n\n【9】In August 2022, hospitalization and intensive care unit admittances of children for wheezing illnesses and pneumonia increased rapidly at our hospital. EV-D68 was documented in most children who tested positive for enterovirus ( Figure 2 ). During the study period of August 1–September 30, 2022, a total of 57 children were hospitalized with a confirmed EV-D68 infection. The weekly number of new EV-D68 cases ranged from 7 to 20 during the 4-week period of August 22–September 18, 2022, and decreased thereafter.\n\n【10】The median age of the 56 children hospitalized with EV-D68 respiratory illness was 4.23 years (interquartile range 2.42–6.15 years), and most (66.1%) were male ( Table ). Before their most recent illness, 26.8% of the children had confirmed asthma or had been prescribed inhaled corticosteroids for suspected asthma. The primary reason for hospitalization was wheezing illness for 78.6% and pneumonia for 16.1%. The mean length of hospital stay among the children was 2.75 days (SD 2.05 days). More than half needed supplemental oxygen or other respiratory support, and 7 (12.5%) were admitted to the intensive care unit. The study population included 1 case of encephalitis and no cases of acute flaccid myelitis. The child with encephalitis manifested ataxia as the predominant symptom and did not need intensive care; EV-D68 was detected in the nasopharyngeal specimen but not in cerebrospinal fluid. All children recovered fully, and only 1 child was re-admitted to hospital within 14 days after discharge.\n\n【11】We detected another respiratory virus, in addition to EV-D68, in 8 (14.3%) children. Because other respiratory viruses can affect the clinical picture, we performed a sensitivity analysis of the clinical characteristics, excluding children with a co-detected virus (Appendix Table). We determined the clinical characteristics of 48 children with EV-D68 as the only detected virus to be similar to all study children.\n\n【12】### Conclusions\n\n【13】The EV-D68 outbreak we studied started rapidly and was exceptionally intense but was of short duration in our area. Other parts of Finland also reported an increased number of children hospitalized with wheezing illnesses during the same timeframe. As in earlier reports, we found that the clinical picture of EV-D68 infection in hospitalized children was most often a respiratory illness characterized by wheezing and respiratory distress. In that respect, EV-D68 resembles rhinoviruses more than it does other enteroviruses ( _13_ ).\n\n【14】Compared with wheezing illnesses caused by other viruses, the median age of children with EV-D68–associated illness was somewhat high (4.2 years), and a large proportion of children had a severe illness treated at the intensive care unit. Some children were simultaneously positive for another respiratory virus, but clinical manifestations were similar among those with EV-D68 only and those with co-detected viruses. Co-detection of \\> 2 respiratory viruses is common in children. Although cases in this outbreak included 1 child diagnosed with encephalitis and no cases of acute flaccid myelitis, EV-D68 can cause acute flaccid myelitis and other neurologic illnesses. A study of a larger population would be needed to determine the occurrence of rare manifestations.\n\n【15】Active surveillance for EV-D68 is important because of its epidemic occurrence, continuous antigenic evolution ( _14_ ), and potential to cause severe respiratory illnesses and serious neurologic conditions. Rapid multiplex RT-PCR methods are increasingly used for microbiologic diagnostics of respiratory tract infections, but these methods do not provide virus type, and some methods do not discriminate between rhinoviruses and enteroviruses. Our findings support virologic laboratory testing, especially in severe clinical cases, to determine the enterovirus type.\n\n【16】Our study included only 1 large pediatric hospital in Finland, and the number of EV-D68 cases is probably underestimated because of the short study period; enterovirus testing of only children negative in the screening test for SARS-CoV-2, influenza A and B, and RSV; and use of either triplex or 16-plex RT-PCR as the primary test for enterovirus. If we had used both tests or the highly sensitive triplex test for all cases, sensitivity might have been higher. We were not able to determine the lineage of genotyped EV-D68 viruses based on partial sequencing of VP1 only.\n\n【17】In summary, we observed a rapid onset of an EV-D68 epidemic in southwest Finland in August 2022. Most children who needed hospital treatment had a respiratory illness characterized by acute wheezing and respiratory distress. Given the virus’ ability to cause acute flaccid myelitis and other neurologic illnesses, continuing surveillance for EV-D68 is needed.\n\n【18】Dr. Peltola is a professor of pediatric infectious diseases at the University of Turku, Turku, Finland. His primary research interests include epidemiology, diagnostics, treatment, and prevention of respiratory tract infections in children.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e788e0b9-5d29-41fd-b8ed-dfa2469c7437", "title": "Tribal Practices for Wellness in Indian Country", "text": "【0】Tribal Practices for Wellness in Indian Country\n===============================================\n\n【1】ESSAY — Volume 16 — July 25, 2019\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Nancy S. Andrade, MPA <sup>1 </sup> ; Marita Jones, MPH <sup>2 </sup> ; Shelley M. Frazier, MPH <sup>2 </sup> ; Chris Percy, MD <sup>2 </sup> ; Miguel Flores Jr, LISAC, CSOTS <sup>3 </sup> ; Ursula E. Bauer, PhD, MPH <sup>1 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Andrade NS, Jones M, Frazier SM, Percy C, Flores M Jr, Bauer UE. Tribal Practices for Wellness in Indian Country. Prev Chronic Dis 2019;16:180660. DOI: http://dx.doi.org/10.5888/pcd16.180660 external icon .\n\n【6】On This Page\n\n【7】*   Expanding Understanding of Tribal Practices that Keep People Well\n*   Convening Cultural Advisors\n*   Funding Tribal Practices\n*   Acknowledgments\n*   Author Information\n*   Table\n\n【8】Expanding Understanding of Tribal Practices that Keep People Well\n-----------------------------------------------------------------\n\n【9】For American Indian and Alaska Native tribes and communities, cultural and traditional teachings and practices are important protective factors that provide their people with strength and resilience to lead healthful lives. Tribal leaders have expressed that these practices are not widely understood by federal agencies, and often are not supported with financial and technical resources. Tribes may choose not to apply for government funding opportunities because the practices that work best for their populations are not described in the funding announcement. In February 2015, the Tribal Advisory Committee (TAC) of the Centers for Disease Control and Prevention/Agency for Toxic Substances and Disease Registry (CDC/ATSDR) recommended that CDC convene a group of knowledgeable cultural advisors to increase understanding of the role of tribal practices to support physical, emotional, and spiritual well-being. The purpose was to craft specific language to include in CDC’s funding opportunities to support implementation of these practices.\n\n【10】Top\n\n【11】Convening Cultural Advisors\n---------------------------\n\n【12】CDC’s National Center for Chronic Disease Prevention and Health Promotion (NCCDPHP) hosted 3 convenings in 2015 and 2016, in Indian Country and at CDC headquarters. Participants were nominated by CDC/ATSDR TAC members, 1 from each of the 12 Indian Health Service regions. Healthy Native Communities Partnership, Inc (HNCP), a native nonprofit organization with extensive experience providing culturally appropriate technical assistance and consultation to tribes and Native communities, facilitated the convenings.\n\n【13】HNCP’s facilitation approach is uniquely designed for each group — integrating the needs and gifts of participants, the aims of the hosts, and appropriate cultural and traditional foundations for the purpose and location. TAC members, CDC staff, and HNCP clarified the rational and experiential aims for the convenings. Cultural advisors, nominated by the TAC, received personal invitations that included the need being addressed, purpose and expectations, how they were nominated, and meeting logistics.\n\n【14】In collaboration with CDC, the HNCP team designed a flow for each convening that built relationships and understanding among participants while integrating culture and tradition. A flexible agenda, without time constraints, showed the path the group would follow for each day. HNCP used visual and participatory processes to facilitate discussions, build trust, and create a welcoming, comfortable, and fun environment that encouraged candid discussion through an inclusive and respectful facilitation approach promoting connection to culture.\n\n【15】The first convening on Tribal Practices That Promote Health and Well-Being was hosted at a site operated by the Kalispel Tribe of Indians in Washington State. Cultural advisors from 10 Indian Health Service regions, 3 TAC members, 3 staff members from CDC and 1 from SAMHSA (the Substance Abuse and Mental Health Services Administration), and 3 American Indian facilitators from HNCP participated. Seated in a circle, participants were welcomed to the meeting by the host, the NCCDPHP’s director at the time. The meeting began with a prayer song to help set positive intentions. One participant, who also chaired the CDC/ATSDR TAC at the time, shared the purpose and vision of the meeting. Through a series of trust-building activities, including one in which pairs exchanged traditional gifts and introduced their partners, participants began to connect with each other on multiple levels.\n\n【16】The group then shared their perspectives on how culture and community contribute to health, strength, and resilience for their people. They described the many forms this takes in terms of understandings, teachings, activities, gatherings, and practices at the local level by creating a simultaneous large group drawing titled _To be strong and resilient, how and when are people connected to culture and community?_ Each person explained their contributions to the drawing that served as the focus for subsequent discussions and development of understanding.\n\n【17】As facilitators led a conversation to organize categories of health-promoting practices, several members of the group expressed concerns about what kinds of cultural knowledge and practices were appropriate to share and which are sacred and must be protected. Trust issues with CDC as a US government agency were raised. The participant who chaired the CDC/ATSDR TAC at the time shared the background for this gathering, mentioned how it resulted from a request from the TAC, and discussed the importance to tribes of being able to include cultural and traditional approaches to wellness. He explained that they were not being asked to divulge their sacred traditional wisdom and thanked the group members for bringing up these important issues.\n\n【18】The first convening concluded with conversations and reflections on the day. In small groups, participants explored principles CDC should keep in mind while working together with tribes and Native communities throughout the funding process — proposal, review, implementation, and evaluation. The small groups shared their conversations, developed key themes, and identified next steps. Each participant agreed on the importance of meeting again. The group reflected on the day by expressing what went well with the convening and what could be improved for subsequent meetings, and closed with a prayer.\n\n【19】The second convening was held at a site operated by the Gila River Indian Community in Arizona. The convening began with a welcome, sharing of intentions, trust-building activities, establishment of group agreements, and sharing of hopes and expectations for the meeting. A visual storyboard helped to put the group’s efforts in historical perspective and to clarify the roles of tribes, CDC, the TAC, HNCP, and the cultural advisors in this work. The participants then reviewed the examples of wellness-promoting activities, practices, and teachings that had been shared during the first convening and organized these into 7 themes and strategies. The convening closed with reflections, next steps for developing Notice of Funding Opportunity (NOFO) language at another meeting, and a prayer.\n\n【20】The third and final convening was held at CDC headquarters in Atlanta, Georgia. The group started with a prayer and a welcome, and then honored with words of remembrance a participant who had died. After a trust-building activity, participants reviewed their work, using a Four Directions Model ( Figure ), focused on Listening, Dialogue, Action, and Reflection.\n\n【21】**Figure.** Cultural advisors consultation process. Abbreviations: CDC/ATSDR, Centers for Disease Control and Prevention/Agency for Toxic Substances and Disease Registry; HNCP, Healthy Native Communities Partnership; TAC, Tribal Advisory Committee. \\[A text version of this figure is also available.\\]\n\n【22】Participants reviewed and revised the draft activity language under each of the 7 strategies and proposed short-term and long-term outcomes. Small work groups reviewed each of these and presented proposed changes to the full group, which came to a consensus on the revisions. After reflections on the day, the TAC chairman closed the meeting by providing a prayer for the group’s work of creating language on cultural wellness practices and blessed the documents. The Table describes the 7 strategies and corresponding activities.\n\n【23】The results of these convenings go beyond the specific advice and work products delivered to the TAC. At the individual level, each of the participants made significant contributions to the group and participants learned from each other, shared stories and cultural knowledge, and were heard respectfully. Participants appreciated the opportunity for honest dialogue; sensitivity toward cultural knowledge, customs, and traditions; respect and understanding for boundaries; and the ability of the group to find common ground. As the group moved through the series of convenings, participants noted that they felt positively about the teamwork, how they were able to regroup and take a different direction, the levels of intensity and passion, being reminded of what’s important, and having a vision of sustainability that reflects sovereignty. With persistence, respect, and a willingness to listen to each other, this group was able to define its own roles and complete important work together.\n\n【24】Results at the group and organizational level included 1) a large federal bureaucracy followed through respectfully on recommendations made by its TAC; 2) Tribal cultural advisors from different regions respectfully recognized their commonalities, differences, and boundaries; 3) the perception of federal agencies when working with Tribal cultures shifted to better understand their practices, strengths, and needs; and 4) small steps were taken toward developing trust and respect between tribes and federal agencies. Most importantly, at the level of tribes and Native communities, the work of the people who supported and participated in this important effort has the potential to revitalize cultural and traditional wellness practices leading to improvements in health, well-being, and resilience for the indigenous peoples of North America.\n\n【25】Top\n\n【26】Funding Tribal Practices\n------------------------\n\n【27】The CDC/ATSDR TAC concurred with the language developed during the convenings. The language was included in a NOFO that was awarded by NCCDPHP in 2018. The 3-year funding opportunity will support the tribal practices identified by the convening group and build resiliency and connections to community and culture, which over time will reduce risks for chronic disease among American Indians and Alaska Natives. The long-term goals are to reduce morbidity and mortality attributable to heart disease, stroke, cancer, and diabetes. NCCDPHP is committed to including cultural and traditional practices as fundamental elements of future programs designed to improve American Indian and Alaska Native health.\n\n【28】Top\n\n【29】Acknowledgments\n---------------\n\n【30】We thank Chester Antone (Tohono O’odham), Tohono O’odham Tribal Council Member and former Chairman of CDC/ATSDR’s Tribal Advisory Committee, for requesting and leading these convenings. We also thank all the individuals who participated in the convenings: Cathy Abramson (Sault Ste Marie), John Beaver (Muscogee \\[Creek\\]), Rachel Carroll Pethers (Northern Cheyenne), Capt Carmen Clelland (Cheyenne and Arapaho), Juana Majel Dixon (Pauma Band of Luiseño Indians), David Dickinson, Moke Eagle Feathers (Northern Cheyenne), Miguel Flores (Yaqui and Tohono O’odham), Robert Flying Hawk (Yankton Sioux), Kelly Francis (Navajo), Cheryle Kennedy (Navajo), Ophelia Spencer (Navajo), Charlie Strickland (Mohegan), Michele Suina (Cochiti Pueblo), Tim Thompson (Muscogee \\[Creek\\]), and Graydon Yatabe. This work was possible because of their commitment to keeping tribal communities healthy and well.\n\n【31】The authors declare no competing financial interests. The findings and conclusions in this report are those of the authors and do not necessarily represent the official position of CDC.\n\n【32】Top\n\n【33】Author Information\n------------------\n\n【34】Corresponding Author: Nancy S. Andrade, MPA, National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, 4770 Buford Hwy, NE, MS S107-1, Atlanta, GA 30341. Telephone: 770-488-0896. Email: nandrade@cdc.gov .\n\n【35】Author Affiliations: <sup>1 </sup> National Center for Chronic Disease Prevention and Health Promotion, Centers for Disease Control and Prevention, Atlanta, Georgia. <sup>2 </sup> Healthy Native Communities Partnership, Inc, Shiprock, New Mexico. <sup>3 </sup> Holistic Wellness Counseling and Consultant Services, Tucson, Arizona.\n\n【36】Top\n\n【37】Table\n-----\n\n【38】Table. Strategies and Activities to Promote Tribal Health and Wellness\n\n| Strategies | Activities |\n| --- | --- |\n| **1\\. Family and community activities that connect cultural teachings to health and wellness** | Implement family-centered community activities and events working with community members and partners that teach, build upon, celebrate, and strengthen cultural and traditional practices and teachings. |\n| **1\\. Family and community activities that connect cultural teachings to health and wellness** | Establish or develop Native language activities for health education to promote and connect community health and Native language. |\n| **1\\. Family and community activities that connect cultural teachings to health and wellness** | Implement a culturally based, community-chosen activity that supports strategy 1. |\n| **2\\. Seasonal cultural and traditional practices that support health and wellness** | Establish an annual community calendar of seasonal cultural and traditional events, celebrations, and activities that support and reinforce healthy practices. |\n| **2\\. Seasonal cultural and traditional practices that support health and wellness** | Support implementation of 1 or more seasonal and traditional cultural events, celebrations, or traditional harvest activities and engage community members and partners to make the event even healthier. |\n| **2\\. Seasonal cultural and traditional practices that support health and wellness** | Implement a culturally based, community-chosen activity that supports strategy 2. |\n| **3\\. Social and cultural activities that promote community wellness** | Establish and/or strengthen community social and cultural activities focused on sharing cultural knowledge and practices and honoring the future through our people and youths, especially teachings of historical events, for mental and emotional well-being. |\n| **3\\. Social and cultural activities that promote community wellness** | Implement social and/or Tribal cultural activities incorporating opportunities to learn about traditional healthy food, physical activities, and lifestyle practices to enhance mental and emotional well-being. |\n| **3\\. Social and cultural activities that promote community wellness** | Implement a culturally based, community-chosen activity that supports strategy 3. |\n| **4\\. Tribal, inter-tribal, governmental, and nongovernmental collaborations that strengthen well-being** | Partner with area tribes and Inter-Tribal Councils to strengthen opportunities to engage in healthy traditional, cultural, and educational activities. |\n| **4\\. Tribal, inter-tribal, governmental, and nongovernmental collaborations that strengthen well-being** | Collaborate on projects such as partnerships with community development financial institutions and other partners and sectors to increase culturally relevant economic and other opportunities. |\n| **4\\. Tribal, inter-tribal, governmental, and nongovernmental collaborations that strengthen well-being** | Implement a culturally based, community-chosen activity that supports strategy 4. |\n| **5\\. Intergenerational learning opportunities that support well-being and resilience** | Establish or strengthen opportunities to encourage 2-way sharing and connect youths, adults, and elders to share knowledge about food, language, ceremonies, stories, places, technology, crafts, and play. |\n| **5\\. Intergenerational learning opportunities that support well-being and resilience** | Establish or strengthen opportunities for adults and elders to pass on Tribal, cultural, and other knowledge to children and young people and to other adults and elders. |\n| **5\\. Intergenerational learning opportunities that support well-being and resilience** | Establish and strengthen intergenerational programs that address historical trauma and that promote and enhance healing and resilience. |\n| **5\\. Intergenerational learning opportunities that support well-being and resilience** | Implement a culturally based, community-chosen activity that supports strategy 5. |\n| **6\\. Cultural teachings and practices about traditional healthy foods to promote health, sustenance, and sustainability** | Establish or strengthen sustainable programs to gather, raise, harvest, produce, or preserve traditional healthy foods and provide those foods and beverages to individuals, families, schools, institutions, and others. |\n| **6\\. Cultural teachings and practices about traditional healthy foods to promote health, sustenance, and sustainability** | Partner with Tribal, Inter-Tribal, governmental, and nongovernmental entities to produce and promote traditional diets, including foods and drinks to sustain health. |\n| **6\\. Cultural teachings and practices about traditional healthy foods to promote health, sustenance, and sustainability** | Implement a culturally based, community-chosen activity that supports strategy 6. |\n| **7\\. Traditional and contemporary physical activities that strengthen well-being** | Enhance, strengthen, or increase opportunities and supports for traditional and contemporary physical activity at schools, work sites, cultural and community events, and other venues. |\n| **7\\. Traditional and contemporary physical activities that strengthen well-being** | Enhance, strengthen, or increase traditional knowledge and history that supports traditional and contemporary physical activities at home, school, work sites, and cultural and community events. |\n| **7\\. Traditional and contemporary physical activities that strengthen well-being** | Build traditional or contemporary physical activity into strategies 1 through 6. |\n| **7\\. Traditional and contemporary physical activities that strengthen well-being** | Implement a culturally based, community-chosen activity that supports strategy 7. |\n\n【40】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6bae314-d4c7-4a20-8001-f23e0827f330", "title": "Enterotoxin-producing Escherichia coli O169:H41, United States", "text": "【0】Enterotoxin-producing Escherichia coli O169:H41, United States\nEnterotoxigenic _Escherichia coli_ (ETEC) is an important cause of diarrhea in the developing world and among travelers and is increasingly recognized as a cause of outbreaks in the United States ( _1_ ). Dalton et al. reviewed confirmed ETEC outbreaks that occurred both in the United States and among passengers on cruise ships that docked in U.S. ports from 1975 through 1995 ( _2_ ). Twenty-one such outbreaks caused by 17 different ETEC serotypes occurred during this period; 7 (33%) occurred among cruise ship passengers ( _2_ ). Because laboratory tests for the identification of ETEC are not widely available, outbreaks caused by ETEC may escape recognition, and healthcare workers may miss opportunities for treatment and prevention. To improve recognition of ETEC outbreaks, Dalton proposed that specimens from outbreaks of gastroenteritis that meet certain criteria be referred for ETEC testing at a public health reference laboratory. These outbreaks include those for which routine stool cultures have not yielded an etiologic agent and those which are characterized by an incubation period of 24 to 48 hours, a duration of illness \\> 60 hours, and a diarrhea-to-vomiting prevalence ratio of \\> 2.5 ( _2_ ).\n\n【1】### The Study\n\n【2】For the 8-year period 1996 through 2003, we reviewed all suspected ETEC outbreaks solely or jointly investigated by the Foodborne and Diarrheal Diseases Branch at the Centers for Disease Control and Prevention (CDC). In accordance with Dalton et al., we defined a confirmed ETEC outbreak as one in which ETEC isolates of the same serotype were isolated from \\> 3 ill persons and no other viral or bacterial pathogens were identified, or one in which ETEC isolates of the same serotype were isolated from \\> 10 ill persons and no more than one other bacterial or viral pathogen was identified in a single stool specimen. Stool specimens collected during these investigations were routinely cultured for _Salmonella_ , _Shigella_ , _Campylobacter_ , _E. coli_ O157:H7, _Yersinia_ , and _Vibrio_ spp. In many instances, these specimens were also tested for Noroviruses **.**\n\n【3】To identify ETEC, patient specimens were plated to MacConkey agar, and individual colonies or sweeps of confluent growth were tested by polymerase chain reaction (PCR) for heat-labile (LT) and heat-stable (ST) enterotoxin genes ( _3_ ). Using standard methods, we serotyped LT- or ST-positive isolates for O and H antigens. ETEC isolates were tested by the disk-diffusion method for susceptibility to ampicillin, amoxicillin/clavulanic acid, ceftriaxone, chloramphenicol, ciprofloxacin, gentamicin, kanamycin, nalidixic acid, streptomycin, sulfisoxazole, tetracycline, and trimethoprim-sulfamethoxazole ( _4_ ). A PCR–restriction fragment length polymorphism test was used to identify the H41 gene in selected nonmotile _E. coli_ O169 isolates ( _5_ ).\n\n【4】### Conclusions\n\n【5】During the 8-year study period, CDC received isolates from 59 outbreaks for ETEC testing. Sixteen met the criteria for our definition of a confirmed ETEC outbreak; three occurred on international cruise ships that docked in U.S. ports and 12 occurred in the United States ( Table ). We identified ETEC in specimens from six other outbreaks that did not meet these criteria, either because ETEC isolates of the same serotype were isolated from only two persons (n = 2) or because we isolated additional bacterial pathogens from the specimens (n = 4).\n\n【6】The 16 ETEC outbreaks had a median of 41 ill persons per outbreak (range 5−916), for a total of 2,865 patients. From 81% to 100% of ill persons in each outbreak reported diarrhea; less frequently reported symptoms included abdominal cramps (66%–90%), fever (0%–73%), nausea (44%–70%), and vomiting (0%–33%). In the 15 outbreaks in which diarrhea and vomiting were reported, the median “diarrhea-to-vomiting prevalence ratio” (the percentage of patients who reported diarrhea divided by the percentage of patients who reported vomiting) was 7.7 (range 2.9–undefined: The upper limit of the range is undefined because in two outbreaks all ill persons interviewed denied vomiting). In nine outbreaks with sufficient data, incubation periods among individual patients were 5–158 hours. The median incubation period was 24–48 hours for 10 of the 12 outbreaks for which it could be calculated. The duration of illness, reported in 11 outbreaks, was 0.3–21 days. The median duration of illness was \\> 60 hours in 11 of the 13 outbreaks for which it could be calculated.\n\n【7】A vehicle was implicated in 11 (69%) outbreaks. Unbottled ship’s water or beverages containing ice prepared on board the ship were implicated in the two outbreaks on cruise ships that had docked in foreign ports. Although no problems with chlorination of bunkered water were documented in these two outbreaks, this problem has been seen in previous waterborne ETEC outbreaks aboard cruise ships ( _6_ ). Basil served on board ship was implicated as the source of ETEC in the remaining cruise ship outbreak (on a ship that docked only in U.S. ports). One other outbreak was attributed to a fresh herb (parsley) served raw ( _7_ ). Salads made with raw vegetables were implicated in four other domestic outbreaks.\n\n【8】If we define a strain as each ETEC serotype identified during an outbreak that has a unique antimicrobial resistance pattern, we identified a total of 30 strains representing eight different serotypes in specimens from the 16 outbreaks ( Table ). In five outbreaks, we isolated more than one ETEC serotype. Heat-stable toxin (ST)-producing _E. coli_ O169:H41 was the most commonly identified serotype. This serotype was identified as the only pathogen in specimens from six outbreaks in the United States and was identified along with other ETEC serotypes in four additional outbreaks. Three of these four outbreaks occurred on cruise ships. In 21 previously reported ETEC outbreaks, _E. coli_ O169:H41 had been isolated only once, from an outbreak that occurred among international cruise ship passengers in 1995, in which another ETEC serotype predominated ( _2_ ). By all of the basic epidemiologic and clinical characteristics that we analyzed, outbreaks in which _E. coli_ O169:H41 was identified alone, or in combination with other serotypes, did not appear to differ from outbreaks in which this emerging strain was not identified.\n\n【9】Resistance to antimicrobial agents remained common among ETEC isolates ( Table ). Twenty-four (80%) of the 30 strains were resistant to tetracycline, 11 (38%) were resistant to sulfisoxazole, 4 (13%) were resistant to ampicillin, and 2 (7%) were resistant to trimethoprim-sulfamethoxazole. All strains of O169:H41 were resistant to tetracycline, and two were also resistant to at least one additional antimicrobial drug. Only two outbreaks were caused exclusively by pan-sensitive ETEC strains (7%).\n\n【10】A comparison of ETEC outbreaks reported to CDC from 1996 through 2003 with those from previous years shows that outbreaks on cruise ships and in the United States continue to occur and that antimicrobial resistance among ETEC isolates remains common. Raw vegetables and herbs have been increasingly implicated as the vehicles for ETEC outbreaks in recent years. This finding is in keeping with an increase in produce-associated outbreaks among other foodborne bacterial pathogens ( _8_ ). Finally, a new ETEC serotype, ST-producing O169:H41, has become predominant.\n\n【11】The first report of O169:H41 was in association with a foodborne outbreak in 1991 in Japan, where this serotype continues to be isolated ( _9_ _–_ _11_ ). Hamada reported four outbreaks that occurred from June 1997 to August 1998; the largest of these outbreaks had a 57% attack rate and resulted in approximately 2,800 cases. All four outbreaks occurred at either restaurants or catered events ( _11_ ). Contaminated wakame seaweed was implicated in one of the outbreaks and considered the likely cause in another ( _11_ ). Nishakawa et al. report that O169:H41 has become the most prevalent ETEC serotype in Japan ( _12_ ).\n\n【12】In 1995, CDC detected this serotype for the first time during an outbreak on a Caribbean cruise ship. It was identified during two additional Caribbean cruise ship outbreaks before causing a domestic outbreak in Minnesota in 1997. In addition to being identified in 10 of the 16 ETEC outbreaks that met our criteria, O169:H41 was also a predominant serotype in 4 of the 6 ETEC outbreaks that did not meet our criteria for a confirmed outbreak, either because the serotype was isolated from two persons only or because an additional pathogen was also isolated in the outbreak.\n\n【13】The emergence and eventual predominance of O169:H41 in the United States and Japan may have important implications for ETEC vaccine producers. Nishikawa et al. characterized strains of O169:H41 from Japan and reported that they are not clonal and that they possess a novel colony-forming factor ( _12_ ).\n\n【14】From 1996 through 1999, laboratory-confirmed ETEC outbreaks represented 0.2% of all foodborne outbreaks reported to CDC ( _13_ ). This number is likely to be an underestimate because special diagnostic tests are required to confirm ETEC. Seventy-one percent of outbreaks of foodborne illness reported to CDC during this period were of unknown cause. In a previous study, Hall et al. demonstrated that the epidemiologic and clinical syndrome in 1.1% of outbreaks of unknown cause reported to CDC from 1982 through 1989 was compatible with infections caused by ETEC or STEC ( _14_ ). ETEC is also responsible for some episodes of sporadic diarrheal disease. When researchers systematically looked for it, they isolated ETEC from 1.4% of stool samples from patients visiting urban and rural health maintenance organization clinics in Minnesota for diarrhea ( _15_ ). Our data suggest that the clinical criteria proposed by Dalton et al. for suspecting ETEC as a cause of an outbreak of unknown cause (median incubation 24–48 hours, mean or median duration >60 hours, and diarrhea-to-vomiting prevalence ratio \\> 2.5) remain valid.\n\n【15】The epidemiology of ETEC outbreaks in the United States is changing, but the incidence of these outbreaks does not appear to be decreasing. Researchers cannot use routine stool cultures to detect ETEC, and delays in stool sample collection for \\> 7 days greatly reduces yield ( _16_ , _17_ ). For outbreaks that meet the clinical profile and for which routine stool diagnostic tests have not yielded an enteric pathogen, physicians and public health authorities should send _E. coli_ isolates to reference laboratories, such as CDC, for ETEC testing.\n\n【16】Dr. Beatty, a pediatrician, is a preventive medicine resident at the Centers for Disease Control and Prevention. His research interests include enteric pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "01706ca9-ccfb-41f2-8f53-9797eea6da35", "title": "Self-Reported Physical Activity Among Middle-Aged Cancer Survivors in the United States: Behavioral Risk Factor Surveillance System Survey, 2009", "text": "【0】Volume 11 — September 11, 2014\n\n【1】### Article Tools\n\n【2】*   PDF - 381 KB\n*   Email\n*   Print\n*   Download citation\n*   Send feedback to _PCD_\n*   Display this article on your Web site\n\n【3】### ORIGINAL RESEARCH  \n\n【4】Self-Reported Physical Activity Among Middle-Aged Cancer Survivors in the United States: Behavioral Risk Factor Surveillance System Survey, 2009\n================================================================================================================================================\n\n【5】#### Navigate This Article\n\n【6】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\n*   Appendix\n\n【7】#### Pratibha Nayak, MPH; Holly M. Holmes, MD; Hoang T. Nguyen, PhD; Linda S. Elting, DrPH\n\n【8】_Suggested citation for this article:_ Nayak P, Holmes HM, Nguyen HT, Elting LS. Self-Reported Physical Activity Among Middle-Aged Cancer Survivors in the United States: Behavioral Risk Factor Surveillance System Survey, 2009. Prev Chronic Dis 2014;11:140067. DOI: http://dx.doi.org/10.5888/pcd11.140067 .\n\n【9】PEER REVIEWED\n\n【10】Abstract\n--------\n\n【11】**Introduction**  \nRegular physical activity (PA) can improve health outcomes in cancer survivors, but the rate of adherence to PA recommendations among middle-aged survivors is unclear. We examined adherence to PA recommendations among cancer survivors and controls. We sought to identify correlates of adherence to PA and to determine whether PA adherence is associated with health-related quality of life (HRQOL) among cancer survivors.\n\n【12】**Methods**  \nWe examined PA adherence among 8,655 cancer survivors and 144,213 control subjects aged 45–64 years who were respondents to the 2009 Behavior Risk Factor Surveillance System survey. We used multinomial logistic regression to assess associations between PA adherence and demographic, psychosocial, and clinical factors, and multivariable linear regression to assess the relationship between PA adherence and HRQOL of cancer survivors.\n\n【13】**Results**  \nCancer survivors and control subjects had similar rates of PA adherence. Of the survivors, 47% met the recommendations of 150 minutes of moderate-intensity PA or 120 minutes of vigorous-intensity PA per week, 41% were somewhat active, and 12% were sedentary. Compared with cancer survivors who were sedentary, survivors who were somewhat active were less likely to be obese (odds ratio \\[OR\\], 0.65; _P_ < .007), and those who met PA recommendations were less likely to be overweight (OR, 0.61; _P_ < .002) or obese (OR, 0.33, _P_ < .001). Regression analysis indicated that PA adherence was positively correlated with HRQOL ( _P_ < .001).\n\n【14】**Conclusion**  \nMost cancer survivors did not meet PA recommendations, but those who are active seem to have improved HRQOL. Therefore, targeted interventions to improve adherence to PA among cancer survivors are needed.\n\n【15】Top of Page\n\n【16】Introduction\n------------\n\n【17】The number of cancer survivors living in the United States today is estimated to be more than 13.7 million (1). Among cancer survivors, 35% are middle-aged adults, most commonly with a diagnosis of melanoma or of cancers of the breast, colon, prostate, or cervix. Advances in cancer treatment have improved 5-year survival rates beyond 69% for individuals whose disease was diagnosed at middle age (2); however, these cancer survivors can experience late-onset and long-term effects of treatment, such as radiation injury or cardiovascular disease, 10 or more years after their initial cancer treatment (3). Cancer survivors have increased comorbidity burden, activity limitations, and varied practice of health behaviors (4,5).\n\n【18】Meta-analysis has associated regular physical activity (PA) after cancer diagnosis with reduced adverse effects of cancer treatment (6). Some benefits of PA reported among breast cancer survivors are improved quality of life (7) and decreased anxiety, depression (8), fatigue (9), and risk of developing other chronic diseases (10). Two reports from meta-analyses urged researchers to examine the range and magnitude of positive effects of PA among the diverse population of cancer survivors (7,11).\n\n【19】Most patients with cancer will survive at least 5 years (2), and the positive effects of PA ameliorate the negative consequences of cancer treatment. Therefore, determining the rates of PA adherence and correlates to such adherence is important. This study describes adherence to PA recommendations among cancer survivors compared with control subjects without cancer and the association between survivor adherence to PA guidelines and demographic, psychosocial, and clinical factors, as well as health-related quality of life (HRQOL). Our findings may help public health practitioners design PA interventions for this growing population and ultimately improve quality of life among survivors.\n\n【20】Top of Page\n\n【21】Methods\n-------\n\n【22】We used data from the 2009 Behavioral Risk Factors Surveillance System (BRFSS) survey (12), which is conducted annually among a large, representative sample of noninstitutionalized adults in the United States and US Territories (District of Columbia, Guam, Puerto Rico, and the Virgin Islands). The participants were selected using random-digit–dialing. The core questionnaire consisted of questions on demographics, health behaviors, and chronic conditions.\n\n【23】Respondents were asked about their cancer history as part of the core questionnaire. We used this item to determine cancer survivor and control groups. Cancer survivors were defined as respondents who said yes when asked if they had ever been told by a doctor, nurse, or other health professional that they had cancer or a cancer history. Respondents who reported not having had any type of cancer were considered control subjects. Cancer survivors were asked the number of types of cancers diagnosed, age at diagnosis, and type of cancer.\n\n【24】We restricted the sample for both cancer survivors and control subjects to respondents aged 45–64 years so we could perform an age-matched comparison. We restricted cancer survivors to those who had the most common types of cancer (breast, bladder, cervical, colon, prostate cancer, or melanoma), which allowed for a comparison across PA levels. We excluded cancer survivors who were within 1 year of their cancer diagnoses to eliminate the effect of active treatment on PA.\n\n【25】### Measures\n\n【26】**PA recommendations.** The core questionnaire contains 6 items on PA, including the duration, frequency, and intensity of moderate to vigorous PA per week ( Appendix ). The survey defines vigorous-intensity PA as activity that causes large increases in breathing or heart rate, and moderate-intensity PA as activity that causes small increases in breathing or heart rate. We used information collected via these survey items to determine adherence to PA guidelines.\n\n【27】We defined adherence to PA guidelines as 150 minutes of moderate-intensity PA or 120 minutes of vigorous-intensity PA per week, as stated by professional medical organizations (13). We categorized PA adherence into 3 groups: meeting guidelines, somewhat active, or sedentary. We defined “meeting guidelines” as engaging in moderate-intensity PA for 30 or more minutes per day on 5 or more days per week, or doing vigorous-intensity PA for 20 or more minutes per day on 3 or more days per week. We categorized as “somewhat active” respondents who reported doing moderate-intensity PA for less than 30 (but more than 10) minutes per day on fewer than 5 days per week or doing vigorous-intensity PA for less than 20 (but more than 10) minutes per day on fewer than 3 days per week. Respondents who reported doing no moderate or vigorous PA were categorized as sedentary.\n\n【28】**Health-related quality of life (HRQOL).** The HRQOL was measured using 3 separate items: poor physical health, poor mental health, and poor overall health. We measured poor physical health with a question about “the number of days during the past 30 days when one’s physical health was not good.” We measured poor mental health with a question about “the number of days during the past 30 days when one’s mental health was not good.” We measured poor overall health with a question about “the number of days during the past 30 days when one’s physical or mental health kept one from doing one’s usual activities.” For each of these items, respondents indicated the total number of days in the previous 30 days when they felt that their physical or mental health was poor or activities were limited. The higher the score, the poorer that person’s HRQOL.\n\n【29】**Demographic, psychosocial, and clinical factors.** Self-reported information was included on sex, race/ethnicity (white, African American, Hispanic, or other), education (more than high school, high school, or less than high school), emotional support (never or always/sometimes), type of cancer (breast, bladder, cervical, colon, prostate, or melanoma), and time since diagnosis (1–2, 2–5, 5–10, or >10 years). Body mass index (BMI) was stratified into 3 categories: normal weight (BMI 18–24.9 kg/m <sup>2 </sup> ), overweight (BMI 25–29.9 kg/m <sup>2 </sup> ), or obese (BMI >30 kg/m <sup>2 </sup> ). We derived the comorbidity count using the single-item questions on comorbid conditions. Respondents were asked whether a health care professional had ever told them they had diabetes, hypertension, arthritis, hyperlipidemia, heart attack, angina, or stroke. We summed the response to heart attack, angina, and stroke into a single binary variable of “cardiovascular diseases.” We coded a response of yes as 1 and a response of no as zero. We summed the scores for the 5 items to create a count of comorbid conditions (0–5). We then categorized the comorbidity count into 3 levels: 0, 1–2, and >2.\n\n【30】To account for the complex survey design of the BRFSS, we used SAS version 9.3 (SAS Institute, Inc) survey procedure with weighted analyses. Cross tabs and chi-squared statistics were used to examine differences in demographic characteristics for categorical variables, and means were used to examine continuous variables. To estimate the adjusted odds ratios and 95% confidence intervals (CIs) for the PA adherence category, we used a multinomial logistic regression, controlling for sex, race or ethnicity, education, emotional support, cancer type, time since diagnosis, BMI, and comorbidity count. We used multivariable linear regression to examine the differences in HRQOL domains (ie, mean difference in number of days of poor physical and mental health) among cancer survivors who met PA guidelines or were somewhat active compared with those who were sedentary, while adjusting for sex, race, education, emotional support, cancer type, time since diagnosis, BMI, and comorbidity count. Significance was set at .05 a priori for all analyses.\n\n【31】Top of Page\n\n【32】Results\n-------\n\n【33】The cancer survivors were older, mostly women, and predominantly white compared with noncancer control subjects ( Table 1 ). The population estimates for comorbidity burden (≥1 comorbidity) were higher among cancer survivors (79%) than among control subjects (69%). No meaningful difference ( _P_ \\> .419) in adherence to PA guidelines was noted between cancer survivors and control subjects.\n\n【34】On the basis of population estimates, nearly 60% of cancer survivors were aged 55 to 64 years, and most (66%) were female, white (82%), and had a high school diploma or less education (59%). Breast cancer survivors made up 35% of the sample population, and more than 10 years had passed since the cancer diagnosis for nearly 40% of the cancer survivors. Most (79%) self-reported having at least 1 comorbid condition, and 68% reported being overweight or obese. Adherence to PA guidelines was suboptimal among middle-aged cancer survivors in the United States; only 47% met PA recommendations ( Table 2 ).\n\n【35】After adjusting for demographic and medical factors, we found many covariates to be independently associated with a sedentary lifestyle in the multinomial regression model ( Table 3 ). Male survivors were more likely to meet PA recommendations than female survivors. Sedentary activity was significantly associated with race; African Americans and “other” racial categories were less likely to meet PA recommendations than whites. Having a college degree was significantly associated with meeting PA guidelines. A lack of emotional support, being obese, and having more than 2 comorbidities all significantly ( _P_ values <.05) increased the odds of being sedentary (Table 3). Cancer survivors whose cancer had been diagnosed 2 to 5 years previously were twice as likely to meet PA recommendations as survivors whose cancer was diagnosed 1 to 2 years previously.\n\n【36】HRQOL was associated with adherence to PA recommendations. The mean number of days with poor physical health was 3.95 days (standard error \\[SE\\], 0.22) for cancer survivors who met PA guidelines, 5.48 days (SE, 0.29) for somewhat active survivors, and 13.51 days (SE, 0.64) for sedentary survivors (Figure). We observed a similar trend for the number of days with poor mental health: 3.55 days (SE, 0.25) for cancer survivors who met PA guidelines, 4.29 days (SE, 0.24) for those who were somewhat active, and 7.96 days (SE, 0.55) for sedentary cancer survivors. We observed a dose–response pattern across levels of PA adherence and HRQOL. Results from the multivariable linear regression indicated that increasing levels of PA adherence were significantly associated with improved HRQOL when controlling for demographic, psychosocial, and medical factors ( _P_ < .001).\n\n【37】**Figure.** Association between health-related quality of life and physical activity levels among cancer survivors in the United States, Behavioral Risk Factor Surveillance System Survey, 2009. The higher the score, the poorer a person’s health-related quality of life. The error bar represents the 95% confidence interval of the mean number of days per month. Among cancer survivors, the higher the level of physical activity adherence, the fewer days with poor health-related quality of life across all 3 items (poor overall health, poor physical health, or poor mental health). \\[A tabular version of this figure is also available\\].\n\n【38】Top of Page\n\n【39】Discussion\n----------\n\n【40】Using data from the nationally representative BRFSS 2009 survey, we found suboptimal PA adherence among middle-aged cancer survivors, which was similar to a related finding for the general population (5,14); however, because of their increased risk of comorbidity (8) and cancer recurrence (15), it is more important for cancer survivors to adhere to PA recommendations.\n\n【41】Some population-based studies indicated that cancer survivors perform less PA compared to the general population (16,17). Our findings concur with those of 2 other studies that compared cancer survivors and controls (18,19). These differences may result from variations in populations, study design, or the ways in which PA is assessed. Coups et al and Bellizi et al respectively reported that 25% and 33% of cancer survivors aged 40–64 years met PA guidelines (5,16). Richardson et al found that 46% of such cancer survivors reported leisure PA similar to our study results (17). Previous studies included people diagnosed with cancer, irrespective of cancer type, whereas we restricted our sample to respondents diagnosed with melanoma, or breast, prostate, cervical, bladder, or colon cancer (18,19). Kwon et al indicated that PA among cancer survivors varied across cancer type (14), which may be due to treatment variation, predisposition to obesity, and other comorbid conditions.\n\n【42】### Correlates for adherence to physical activity guidelines\n\n【43】Approximately half of the cancer survivors aged 45–64 years in the BRFSS 2009 survey met PA guidelines, and 41% reported a lower level of PA. Our study describes the characteristics of this large group of survivors with inadequate PA who may benefit from targeted interventions.\n\n【44】Cancer survivors who are sedentary were more likely to be female, African American, have lower education levels, or have low levels of emotional support (19,20). Having others who motivate the survivor or offer support in dealing with ongoing psychological distress lessens barriers to participation in PA (20). A weight loss intervention called “Moving Forward,” which was designed for African American breast cancer survivors, found that positive social support from friends and family encouraged survivors to maintain high levels of participation and retention in the weight loss program (21). The intervention included the participation of friends and family and promoted the interdependence of culture and kinship networks to support personal health decisions. The success of this intervention demonstrates the role of social support in promoting PA participation and how tailored programs can encourage cancer survivors who are overweight or obese to adhere to PA guidelines.\n\n【45】Our study found that cancer survivors who were sedentary had an increased burden of comorbidity and obesity. These findings concur with those of previous studies: being overweight or obese was significantly associated with not meeting PA guidelines (22,23). Along with targeted interventions to increase PA among sedentary cancer survivors, clinical management of obesity or other comorbidities may be necessary before initiating such programs.\n\n【46】### Health-related quality of life and physical activity\n\n【47】High levels of HRQOL was associated with high levels of adherence to PA guidelines, and cancer survivors who were somewhat active reported better HRQOL than did sedentary cancer survivors, which agreed with previous findings (24). Cancer survivors who practiced positive lifestyle behaviors reported high HRQOL, particularly those who met the PA guidelines (24).\n\n【48】Sedentary survivors reported experiencing more recent days of poor health compared with cancer survivors who reported being somewhat active or who met PA guidelines. Previous studies examined this relationship only among individuals with colorectal cancer and non-Hodgkin’s lymphoma (25,26). PA promotion should motivate sedentary cancer survivors to become more active so they can improve their HRQOL. Many cancer survivors may be more willing to initiate a less intense exercise routine than to immediately try to meet PA guidelines, and our study findings indicate that small doses of PA are associated with improved HRQOL. Cancer survivors should be encouraged to incorporate some form of PA into their daily life.\n\n【49】This study has strengths and limitations. First, the 2009 BRFSS collected information using landline telephone interviews of noninstitutionalized individuals; therefore, it did not represent people who are without landline telephones, use cellular telephones exclusively, or are in institutions. However, the BRFSS survey is representative of noninstitutionalized adults living in the United States (covers all US states and territories) and thus enables researchers to estimate the national prevalence of health-related behaviors such as PA. Second, the information collected is self-reported by the respondents and thus is subject to recall bias or inaccuracy. Furthermore, BRFSS does not collect information on the type of medical treatments received or their duration. To address bias arising from limitations in participation caused by aggressive treatment effects, we restricted the study sample to participants who were at least 1 year beyond their cancer diagnosis. Third, we did not include income in the adjusted model because of the large number of nonresponses. Socioeconomic status plays an important role in education level, access to recreational facilities, and availability of leisure time to engage in PA. To reduce this bias, we included surrogate variables of socioeconomic status such as education and race. Finally, because BRFSS is a cross-sectional survey, we can assess only association and not causality.\n\n【50】Our study has several strengths. We examined adherence to PA among cancer survivors of middle age (an understudied group) across diverse cancer types and using a population-based sample. Most previous studies examined adherence to PA as meeting versus not meeting guidelines. Such broad categories do not allow for an examination of the influences faced by cancer survivors who are somewhat active but do not meet the guidelines. We examined PA adherence on 3 levels: meeting guidelines, somewhat active, and sedentary. This approach allowed us to examine the relationship between varying levels of PA adherence and HRQOL. Finally, most previous studies focused on short-term rather than long-term cancer survivors. We studied cancer survivors who received their diagnosis at least 1 year before the study, and 40% of the survivors were more than 10 years post diagnosis.\n\n【51】This study highlights the subgroup of cancer survivors who are active but are not meeting PA guidelines and who may benefit from an intervention that helps them incorporate more PA into their daily life. Cancer survivors who are obese and have more than 2 comorbidities are likely to be sedentary. This subgroup of cancer survivors needs to be targeted and encouraged to incorporate some PA into their daily life. Previous research indicated that primary care providers have a positive influence on a patient’s PA behavior and that provider-based counseling improves adherence to PA guidelines among sedentary individuals (27). Despite this, few physicians recommend PA to the cancer survivors among their patients (28). Some barriers reported by physicians were concerns about safety and a lack of time during office visits. The American College of Sports Medicine’s roundtable on exercise guidelines for cancer survivors indicated that PA is safe for this population and should be undertaken even during active disease and treatment (29). Primary care physicians are in the best position to address obesity and coexisting medical conditions that may limit their patients’ ability to meet recommended PA guidelines. Because HRQOL improves dramatically with PA, even levels not sufficient to meet the guidelines can be helpful for cancer survivors. A one-size-fits-all approach may not work with this group, and based on their current activity levels and health status, such patients may need a PA intervention program that helps them meet their unique needs.\n\n【52】Top of Page\n\n【53】Acknowledgments\n---------------\n\n【54】This research was supported by funds from the University Cancer Foundation and the Duncan Family Institute for Cancer Prevention and Risk Assessment via the Cancer Survivorship Research Seed Money Grants at The University of Texas MD Anderson Cancer Center, and from the Cancer Prevention and Research Institute of Texas through the CERCIT grant (grant no. RP101207 P04 02- L. Elting, PI) to the University of Texas Medical Branch at Galveston. Some of this information was presented as a poster at the 33rd Annual Meeting and Scientific Sessions of the Society of Behavioral Medicine in New Orleans, Louisiana, April 11–14, 2012. We acknowledge the editorial assistance of Ms Lee Ann Chastain and statistical advice from Ms Chiew Kwei Kaw.\n\n【55】Top of Page\n\n【56】Author Information\n------------------\n\n【57】Corresponding Author: Pratibha Nayak, MPH, Department of General Internal Medicine, The University of Texas MD Anderson Cancer Center, 1400 Herman Pressler St, Houston, TX 77030. Telephone: 713-792-0711. E-mail: pnayak@mdanderson.org .\n\n【58】Author Affiliations: Holly M. Holmes, MD, Hoang T. Nguyen, Linda S. Elting, The University of Texas MD Anderson Cancer Center, Houston, Texas.\n\n【59】Top of Page", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a0b2aeb3-5b6f-4ff4-9de9-9fa6ca87f16d", "title": "Human Infection with Highly Pathogenic Avian Influenza Virus (H5N1) in Northern Vietnam, 2004–2005", "text": "【0】Human Infection with Highly Pathogenic Avian Influenza Virus (H5N1) in Northern Vietnam, 2004–2005\nHuman infection with the highly pathogenic avian influenza A virus (H5N1) was discovered in Hong Kong Special Administrative Region, People’s Republic of China, in 1997 ( _1_ – _3_ ). It has since been identified in other countries, primarily in Southeast Asia. Among 100 confirmed infected patients, 46 have died in Vietnam since 2003 ( _4_ , _5_ ).\n\n【1】Severe viral pneumonia accompanied by diffuse alveolar damage develops in patients infected with influenza virus (H5N1) ( _6_ ). High viral load causes intense cytokine reactions and inflammation ( _7_ ). Clinical factors that might be associated with severity include age, delayed consultation, lower respiratory tract lesions, and leukopenia ( _4_ , _8_ – _10_ ). However, few cases have reported which factors, including patient management, affect outcomes. Our study reviews the clinical courses of patients treated in Hanoi, Vietnam, and investigates the association between clinical findings and survival.\n\n【2】The effects of oseltamivir and other neuraminidase inhibitors have been demonstrated in experimental models ( _11_ – _13_ ), but their outcomes in humans have not been verified. Randomized controlled trials would be optimal for investigating the effectiveness of oseltamivir compared with placebo, but they are not an option because of ethical issues. Therefore, this issue can only be addressed through observational studies. Despite limited empirical evidence, the World Health Organization (WHO) reported that oseltamivir improved survival ( _14_ ) and recommended treatment with oseltamivir because of high mortality rates associated with influenza A virus ( _14_ , _15_ ). Patients from northern Vietnam are described in detail.\n\n【3】### Methods\n\n【4】We investigated patients infected with influenza A virus (H5N1) who were referred to the National Institute of Infectious and Tropical Diseases in Hanoi, Vietnam, from other local hospitals from January 2004 through July 2005. Pediatric patients were admitted to another institution in Hanoi and were excluded from the present study. A WHO inspection team at the National Institute for Hygiene and Epidemiology in Hanoi virologically confirmed H5N1 subtype infection in the patients by using a reverse transcription–PCR (RT-PCR) for influenza A/H5. We investigated only patients with H5N1 subtype infection determined from symptoms of acute respiratory tract infection, a history of high-risk exposure, or chest radiographic findings such as pneumonia. All patients were reported to WHO as having confirmed infection with avian influenza virus (H5N1). We excluded other patients with positive RT-PCR results because of reasons described below. The study was reviewed and approved by the ethics committees at the International Medical Center of Japan and the National Institute of Infectious and Tropical Diseases in Vietnam.\n\n【5】Data were obtained for general characteristics, history of high-risk exposure, medical history, symptoms, signs, microbiologic and biochemical test results, chest radiographic findings, treatment strategies, and outcomes from medical records from April through October 2006.\n\n【6】We investigated associations between clinical findings and survival by using univariate analysis. Initial laboratory and chest radiographic findings after hospitalization were recorded in the medical charts and used. The relationship between survival and treatment with oseltamivir or methylprednisolone was investigated by adjusting for factors related to severity in an exact logistic regression analysis, which is appropriate for small amounts or unbalanced binary data ( _16_ ). Because the study cohort was small, deaths were few and overfitting was possible ( _17_ , _18_ ); only 1 covariate could be added for adjustment into the logistic regression model. Therefore, we used leukocyte counts, platelet counts, aspartate aminotransferase (AST) levels, and urea nitrogen levels as an adjustment for severity because these values are associated with reported outcomes ( _14_ ). Also, many missing observations prevented adjustment using albumin levels. Data were analyzed by using the Wilcoxon test, χ <sup>2 </sup> test, and Fisher exact test when appropriate and the statistical package SAS version 8.2 (SAS Institute, Cary, NC, USA).\n\n【7】### Results\n\n【8】Among 41 patients who were hospitalized from January 2004 through July 2005 and had positive RT-PCR results, 12 were excluded from the study (3 patients whose medical records were unavailable; 2 patients related to persons with confirmed H5N1 subtype pneumonia who were asymptomatic, positive for viral RNA, and treated with prophylactic oseltamivir; and 7 patients who had some illnesses, particularly respiratory diseases, which complicated interpretation of the clinical course or chest radiographic findings). We therefore studied 29 patients with clinically and virologically confirmed influenza A (H5N1) infection.\n\n【9】Figure\n\n【10】Figure . Clinical course of 29 patients infected with highly pathogenic avian influenza virus (H5N1), northern Vietnam, 2004–2005. Zero days on horizontal axis represent days of hospitalization at the National Institute of Infectious...\n\n【11】Table 1 shows the general characteristics of the patients, and the Figure shows the clinical course from onset of disease to hospitalization and discharge. Patients ranged in age from 14 to 67 years and with a mean age of 35.1 years. A total of 25 patients were given 150 mg/day of oseltamivir, and 15 were treated with methylprednisolone (initial dose 40–160 mg/day, median dose 80 mg/day). Seven (24.1%) of the 29 patients died. No significant associations were found between mortality rates and age (p = 0.57), sex (p = 0.68), history of high-risk exposure (contact with poultry \\[p = 1.00\\], contact with sick poultry \\[p = 1.00\\], and contact with sick poultry or persons \\[p = 1.00\\]). Three of 6 patients from a family infected with H5N1 subtype died, and 4 of 23 patients without such an association died (p = 0.13). Duration between onset of disease and hospitalization was not associated with higher mortality rates (p = 0.98).\n\n【12】Table 2 shows initial laboratory findings at hospitalization. Leukopenia (neutropenia), thrombocytopenia, hypoalbuminemia, and increased AST and urea nitrogen levels were associated with increased deaths.\n\n【13】Five (20.0%) of the 25 patients treated with oseltamivir died, as did 2 (50.0%) of 4 who were not treated (odds ratio 0.25, 95% confidence interval \\[CI\\] 0.03–2.24, p = 0.24). To adjust for variation in disease severity among patients, exact logistic regression was performed by using leukocyte counts, platelet counts, AST levels, and urea nitrogen levels. Adjusted odds ratios for deaths among patients treated with oseltamivir were 0.15 (95% CI 0.00–2.57, p = 0.19), 0.16 (95% CI 0.00–2.23, p = 0.17), 0.54 (95% CI 0.02–11.85, p = 1.00), and 0.28 (95% CI 0.01–5.16, p = 0.55), respectively, for the 4 adjustments for disease severity.\n\n【14】The time between the onset of symptoms and initiation of treatment with oseltamivir varied ( Table 1 , Figure ). The mortality rates were 20% (3/15) and 20% (2/10) when treatment with oseltamivir was started within and after 7 days of disease onset.\n\n【15】Methylprednisolone was given to 15 of 29 patients. Five (33.3%) of these 15 patients died, and 2 (14.3%) of 14 patients who were not given this drug died (odds ratio 3.0, 95% CI 0.48–18.93, p = 0.39). Exact logistic regression after adjustment for severity by using leukocyte counts, platelet counts, AST levels, or urea nitrogen levels showed odds ratios for deaths among patients treated with methylprednisolone of 0.74 (95% CI 0.00–9.57, p = 0.82), 1.82 (95% CI 0.18–25.48, p = 0.89), 1.14 (95% CI 0.07–18.92, p = 1.00), and 2.43 (95% CI 0.28–31.69, p = 0.61), respectively.\n\n【16】Thirteen patients were treated with oseltamivir and methylprednisolone. The regression model that included these 2 drugs and interactions did not show effectiveness of either drug.\n\n【17】### Discussion\n\n【18】The overall mortality rate of 24.1% in this study was lower than rates in previous studies and WHO reports. Table 3 summarizes the characteristics of patients from previous studies. Patients in the present study were older because pediatric patients were excluded because of treatment elsewhere. WHO has reported that the mortality rate of 73% for infection with H5NI subtype is highest in persons 10–19 years of age, and that patients 20–39 years of age account for >60% of the deaths ( _22_ ). The expected mortality rate would be 51.8% if our case-patients had the same age-specific mortality rate as in a WHO report ( _14_ ). The lower mortality rate in our study could not be explained by an age difference. The relatively high leukocyte count and factors related to outcomes suggest that a reasonably large number of mildly infected patients might have been included, although chest radiographs showed variable progression in lesions from mild to severe.\n\n【19】Persons who died were concentrated in the early period of the study, especially in 2004. Virus genotype and load data could provide useful information on pathogenesis and outcome. However, these data were not available.\n\n【20】Factors affecting outcome were leukocyte and platelet counts, and albumin, ALT, and urea nitrogen levels. Results were consistent with previous findings ( _14_ ) and suggested that outcome is related to lesions in several organs.\n\n【21】Although the mortality rate was lower among patients treated with oseltamivir, differences were not significant. Exact logistic regression after adjustments for laboratory results yielded an odds ratio of 0.15–0.54 for death. The small number of patients prevented valid adjustment, and confounding factors might not have been sufficiently eliminated. A larger patient cohort should be able to adjust for severity of disease.\n\n【22】If one considers the possibility of confounding factors, the reason oseltamivir was not prescribed should be investigated. If oseltamivir was withheld from patients with severe infections and administered only to those with milder symptoms, the drug would apparently be more effective. Among 4 patients who were not prescribed oseltamivir, initial RT-PCR results were negative for 1 patient, who subsequently died. Oseltamivir was unavailable for treatment of another patient who died. The other 2 surviving patients were not prescribed oseltamivir because their chest radiographs showed only minimal lesions. Therefore, withholding oseltamivir was not associated with disease severity.\n\n【23】Higher doses of oseltamivir or longer drug administration have improved outcomes in animal models ( _23_ , _24_ ). Because all patients in our study were given oseltamivir at a dose of 150 mg/day, we could not investigate the effect of a higher dose.\n\n【24】Mortality rates were higher in patients treated with methylprednisolone than in those not treated with this drug. This finding can be explained by disease severity because severely ill patients were more likely to be given methylprednisolone. However, even after we adjusted for this confounding effect, no beneficial effect of methylprednisolone was observed. Further, an experimental model has recently raised doubt about the effect of cytokine suppression ( _25_ ).\n\n【25】Our study described patients infected with influenza A virus (H5N1) in Hanoi, Vietnam. These patients had lower mortality rates than those reported in other studies. The reason for the low mortality rate could not be investigated thoroughly without virologic information. Oseltamivir was prescribed in 25 of 29 patients, and their mortality rate was apparently decreased, although the patient cohort was too small to generate sufficient statistical power. In addition, since our study was an observational study, these findings might have been influenced by confounding factors. Further detailed observations from a larger number of patients are required.\n\n【26】Dr Hien is the director general at the National Institute of Infectious and Tropical Diseases in Hanoi, Vietnam. His research interests are clinical practice and research of infectious and tropical diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "631c0d66-5133-4408-be00-079126218f55", "title": "Cooperative Recognition of Internationally Disseminated Ceftriaxone-Resistant Neisseria gonorrhoeae Strain", "text": "【0】Cooperative Recognition of Internationally Disseminated Ceftriaxone-Resistant Neisseria gonorrhoeae Strain\nCeftriaxone is among the last remaining recommended therapies for treating _Neisseria gonorrhoeae_ infections and is used in many countries around the world as part of a dual therapy with azithromycin. Cephalosporin resistance in _N. gonorrhoeae_ has been associated with modifications of the _penA_ gene, which encodes penicillin-binding protein 2 (PBP2), a target for β-lactam antimicrobial drugs ( _1_ ). During 2009–2015, several ceftriaxone-resistant (MIC 0.5–4 mg/L) _N. gonorrhoeae_ strains were reported: in 2009, H041 in Japan ( _2_ ); in 2010, F89 in France ( _3_ ); in 2011, F89 in Spain ( _4_ ); in 2013, A8806 in Australia ( _5_ ); in 2014, GU140106 in Japan ( _6_ ); and in 2015, FC428 and FC460 in Japan ( _7_ ). However, until 2017, all of these strains were considered to have occurred sporadically because, except for limited transmission of F89 among persons in France and Spain during 2010–2011, there had been no reports of sustained transmission of these strains identified nationally or internationally. In 2017, this changed, substantiated by independent reports from Canada ( _8_ ) and Denmark ( _9_ ) of gonococcal isolates that had substantive similarity to the previously described FC428 strain in Japan.\n\n【1】The first reported case of the FC428 ceftriaxone-resistant _N_ . _gonorrhoeae_ strain was in Japan during January 2015 in a heterosexual man in his twenties who had urethritis ( _7_ ). The FC428 isolate was resistant to ceftriaxone (MIC 0.5 mg/L), cefixime (MIC 1 mg/L), and ciprofloxacin (MIC >32 mg/L); susceptible to spectinomycin (MIC 8 mg/L) and azithromycin (MIC 0.25 mg/L); and, unlike all previously described ceftriaxone-resistant strains, a penicillinase-producing _N. gonorrhoeae_ (PPNG; MIC ≥32 mg/L) bacterium. The patient was treated successfully with a single dose of spectinomycin 2 g intramuscularly (IM); however, a second isolate with an identical susceptibility profile (FC460) was subsequently cultured from the same patient 3 months later, suggesting reinfection by a separate contact.\n\n【2】In Canada, during January 2017, a gonococcal isolate (47707) ( _8_ ) of similar susceptibility to the first reported case (including ceftriaxone-resistant MIC 1 mg/L and PPNG; Table 1 \\[ _10_ \\]) was isolated from a sample collected from a 23-year-old woman. This patient had no history of travel, but her male partner, who had been treated empirically and had no culture results available, reported sexual contact during travel in China and Thailand during the fall of 2016. She was successfully treated with combination therapy of a single dose each of cefixime (800 mg orally) and azithromycin (1 g orally) and an additional dose 13 days later of azithromycin (2 g orally). The strain from Denmark (GK124) was also isolated in January 2017, had a similar susceptibility profile to FC428, and was obtained from a heterosexual man in his twenties who had reported unprotected sexual contact with women from Denmark, China, and Australia ( _9_ ). The patient was successfully treated with single doses of ceftriaxone (0.5 g IM) and azithromycin (2 g orally). Here, we report additional FC-428-like cases among persons in Australia, providing further evidence of the sustained international transmission of a ceftriaxone-resistant _N. gonorrhoeae_ strain.\n\n【3】### Methods\n\n【4】We confirmed _N. gonorrhoeae_ isolates by using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (Bruker Daltonics, Melbourne, Victoria, Australia; bioMérieux, Brisbane, Queensland, Australia). We determined antimicrobial susceptibilities of _N. gonorrhoeae_ to ceftriaxone, penicillin, tetracycline, azithromycin, gentamicin, and ciprofloxacin by using Etest (bioMérieux) and spectinomycin by using the agar dilution method ( _11_ ). We interpreted MIC on the basis of interpretive criteria from the Clinical and Laboratory Standards Institute ( _12_ ): penicillin resistance (MIC ≥2.0 mg/L); tetracycline resistance (MIC ≥2.0 mg/L); ciprofloxacin resistance (MIC ≥1.0 mg/L); and spectinomycin resistance (MIC ≥128.0 mg/L). Because the Clinical and Laboratory Standards Institute does not have an azithromycin breakpoint, and ceftriaxone breakpoints only state susceptibility (≤0.25 mg/L), we used the European Committee on Antimicrobial Susceptibility Testing ( _13_ ) breakpoints for ceftriaxone resistance (MIC>0.12 mg/L) and azithromycin resistance (MIC>0.5 mg/L). β-lactamase production was analyzed by using nitrocefin (Thermo-Fisher Scientific, Melbourne, Victoria, Australia). We subcultured isolates on GC agar base with Vitox Supplement (Thermo-Fisher Scientific) and incubated for 24 h at 35°C in a 5% CO <sub>2 </sub> atmosphere with or without antimicrobial drugs and stored in Tryptone (Thermo-Fisher Scientific) soya broth with 10% glyercol at −80°C.\n\n【5】##### Genomic Analyses\n\n【6】We put each isolate from Japan and Australia through DNA extraction, library preparation, and sequencing (Illumina, San Diego, CA, USA). From the strains from Japan, FC428 and FC460, we extracted DNA samples with the DNeasy Blood & Tissue Kit (QIAGEN, Tokyo, Japan). We created multiplexed libraries with Nextera XT DNA sample prep kit (Illumina) and generated paired-end 300-bp indexed reads on the Illumina MiSeq platform (Illumina) yielding 6,121,575 reads/genome and genome coverage of 845× for FC428 and 1,272,909 reads/genome and genome coverage of 845× for FC460.\n\n【7】To analyze the strains from Australia, A7536 and A7846, we extracted DNA on the QIAsymphony SP (QIAGEN) by using the DSP DNA Mini Kit (QIAGEN). We prepared the libraries according to manufacturer instructions for the Nextera XT library preparation kit (Illumina) and sequenced on the NextSeq 500 (Illumina) by using the NextSeq 500 Mid Output V2 kit (Illumina). Sequencing generated 6,763,774 reads and genome coverage of 361× for A7536 and 3,672,072 reads and genome coverage of 202× for A7846.\n\n【8】Figure\n\n【9】Figure . Core single-nucleotide variation (SNV) phylogenetic tree of ceftriaxone-resistant _Neisseria gonorrhoeae_ isolates _._ The maximum-likelihood phylogenetic tree is rooted on the reference genome of _N. gonorrhoeae_ FA1090 (GenBank accession no. NC\\_002946.2). Isolates are...\n\n【10】We then provided sequencing data to the Canadian National Microbiology Laboratory, where bioinformatic analyses were performed as previously described ( _14_ ). Quality reads were assembled by using SPAdes ( _15_ ) ( http://bioinf.spbau.ru/spades ) and annotated with Prokka ( _16_ ) ( https://github.com/tseemann/prokka ), and produced an average of 86 contigs per isolate, an average contig length of 26,276 nt, and an average N50 length of 68,884 nt. Quality metrics for whole-genome sequencing (WGS) are shown in Technical Appendix Table 1. A core single-nucleotide variation (SNV) phylogeny was created by mapping reads to FA1090 (GenBank accession no. NC\\_002946.2) by using a custom Galaxy SNVPhyl workflow ( _17_ ). Repetitive and highly recombinant regions with >2 SNVs per 500 nt were removed from the analysis. The percentage of valid and included positions in the core genome was 97.6%; 567 sites were used to generate the phylogeny. We used a meta-alignment of informative core SNV positions to create a maximum-likelihood phylogenetic tree for A7536, A7846, FC428, FC460, and 47707 ( Figure ). The H041, F89, and A8806 ceftriaxone-resistant strains (available in the World Health Organization \\[WHO\\] reference panel as WHO-X, WHO-Y, and WHO-Z, respectively) ( _10_ ) were included for comparison. WGS read data for A7536, A7846, FC428, and FC460 are available under BioProject PRJNA416507, and previously reported 47707 was submitted under BioProject PRJNA415047 ( _8_ ) .\n\n【11】We implemented _N. gonorrhoeae_ multiantigen sequence typing (NG-MAST) ( _18_ ), multilocus sequence typing (MLST) ( _19_ ), and _N. gonorrhoeae_ sequence typing for antimicrobial resistance (NG-STAR) ( _20_ ) by using gene sequences extracted in silico from WGS data. We submitted the sequences to the NG-MAST ( http://www.ng-mast.net/ ), _Neisseria_ MLST ( http://pubmlst.org/neisseria/ ), and NG-STAR ( https://ngstar.canada.ca ) databases to determine respective sequence types. Sequence data for the GK124 strain ( _9_ ) were not available for these analyses; however, a summary of the documented susceptibility and MLST and NG-MAST data is provided ( Table 1 ).\n\n【12】### Results\n\n【13】##### Case Histories and Isolate Details\n\n【14】The first documented case-patient in Australia was a man in his forties who was visiting from the Philippines. He went to a sexual health clinic in Adelaide in April 2017 reporting urethral discharge and dysuria. He reported recent heterosexual contact with multiple female sex workers in Cambodia and the Philippines; it was unclear where the infection was acquired. An _N. gonorrhoeae_ isolate (A7846) of similar susceptibility to FC428 (showing the characteristic ceftriaxone resistance and PPNG; Table 1 ) was cultured. The patient was treated with a 1-time dose combination therapy of ceftriaxone (500 mg IM) and azithromycin (1 g orally). A test result 7 days after treatment was negative for _N. gonorrhoeae_ .\n\n【15】A second case-patient in Australia was a man visiting from China. He was in his early 40s and described symptoms of urethral discharge and dysuria to a general practitioner in Sydney in August 2017. He reported heterosexual contact in China, but none in Australia. An isolate (A7536) of similar susceptibility to FC428 (ceftriaxone-resistant and PPNG; Table 1 ) was cultured. The patient was treated with a 1-time dose combination therapy of ceftriaxone (500 mg IM) and azithromycin (1 g orally); he returned to China shortly thereafter. Attending physicians advised him to return to follow up for test of cure and to trace contacts, but follow-up was not confirmed.\n\n【16】Core SNV phylogenetic analysis results ( Figure ) showed a close genetic relatedness among the FC428, FC460, 47707, A7536, and A7846 isolates. These isolates were distinct from the other previously described F89, A8806, and H041 ceftriaxone-resistant strains; the 2 groups of isolates were separated from each other by an average of 292 core SNVs. We detected no SNVs in the 2 isolates from Japan (FC428, FC460 collected from the same patient 3 months apart). For other isolates, 12 SNVs separated FC428 from both 47707 and A7536; 17 SNVs separated FC428 and A7846 (47707, A7536, and A7846 shared 8 identical SNVs); 8 SNVs separated 47707 and A7536; 5 SNVs separated 47707 and A7846; and 11 SNVs separated A7536 and A7846 ( Technical Appendix Table 2).\n\n【17】Molecular typing of FC428, FC460, 47707, A7536, and A7846 from the WGS showed an identical MLST of ST1903, which was also reported for GK124 from Denmark ( _9_ ) ( Table 1 ). We observed different NG-MAST: ST3435 for FC428 and FC460; ST1614 for 47707, A7846, and GK124; and ST15925 for A7536. FC428, FC460, 47707, A7536, and A7846 were of the same NG-STAR, ST233, which was characterized by a mosaic _penA_ \\-60.001 allele that had only been reported previously for FC428 and 47707 ( _8_ ). This allele encodes key alterations A311V and T483S in PBP2 ( Table 2 ; also observed for GK124) that are linked to ceftriaxone resistance, some of which are also present among the previously described ceftriaxone-resistant strains ( Table 2 \\[ _1_ \\]). We observed additional resistance mutations for FC428, FC460, 47707, A7536, and A7846 by using the NG-STAR designations, including the previously described alleles _mtrR_ \\-1 (promoter−35A deletion); _porB_ \\-8 (PorB G120K, PorB A121D); _ponA_ \\-1 (PonA L421P); _gyrA_ \\-7 (GyrA S91F, GyrA D95A); and _parC_ \\-3 (ParC S87R).\n\n【18】### Discussion\n\n【19】The recent reports of the _N. gonorrhoeae_ FC428 clonal strain in Denmark, Canada, and now Australia provide new evidence that there is sustained international transmission of a ceftriaxone-resistant _N. gonorrhoeae_ strain. This strain appears to have been circulating globally for \\> 2 years. Thus, it is highly likely this strain is prevalent elsewhere, possibly in Asia, but undetected. There are serious gaps in _N. gonorrhoeae_ antimicrobial resistance surveillance worldwide ( _21_ ), and we estimate that samples from as few as 0.1% of the estimated 80 million cases of _N_ . _gonorrhoeae_ reported globally each year ( _22_ ) are tested for antimicrobial resistance. Therefore, there are many opportunities for such strains to avoid detection.\n\n【20】Fortunately, the ceftriaxone MICs of the FC428 clonal strain remain lower than the H041 strain from Japan (MIC 2 mg/L) ( _2_ ), and further, the FC428 strain does not exhibit resistance to azithromycin ( Table 1 ). Therefore, treatment failure is arguably less likely against FC428 infections than in H041 and F89 infections, particularly when using ceftriaxone and azithromycin dual therapy; treatment failure was not observed in our study. Nevertheless, previous pharmacodynamic analyses indicate that ceftriaxone MICs of 0.5–1.0 mg/L can result in treatment failures with ceftriaxone 250 mg monotherapy and even (albeit to a lesser extent) when 1.0 g doses are used ( _23_ ). As such, a dissemination of the FC428 clone could offset dual therapy guidelines because azithromycin resistance is being increasingly reported ( _24_ _,_ _25_ ).\n\n【21】The cases of _N_ . _gonorrhoeae_ described here and the circumstances under which these analyses took place are also a timely reminder of the need for international collaboration in addressing the overall _N_ . _gonorrhoeae_ problem and highlight the benefits of rapid access to genomic data by using electronic communications. In fact, in the absence of WGS data, it would have been very difficult to identify the links between these isolates. Not only have we been able to use these tools to readily identify the problem but we also arguably achieved identification in a sufficiently timely manner as to enable countries to put in place interventions that can limit further the spread of this strain, including intensifying follow-up and contact tracing.\n\n【22】Differences in extraction and sequencing procedures among the 3 countries could introduce variations in DNA concentrations that might affect the quality of the sequencing, such as number of reads and depth of coverage. This limitation was minimized because downstream processing of the data, such as assembly and reference mapping software algorithms, standardizes input data before detailed analyses of the genomes are conducted. Laboratory and epidemiologic findings are critical for surveillance that closely tracks the dissemination and emergence of epidemic antimicrobial-resistant strains and for rapid recognition and implementation of control measures to limit the expansion of clones through sexual networks. We recommend that health departments in all countries be made aware of this spreading resistant strain and strengthen _N_ . _gonorrhoeae_ antimicrobial-resistance monitoring, including treatment failure identification, adequate follow-up and contact tracing of cases, and STI prevention programs.\n\n【23】In conclusion, international collaboration based on WGS typing methods revealed the dissemination of a ceftriaxone-resistant _N. gonorrhoeae_ in Japan, Canada, and Australia. Sustained transmission spanning 2 years suggests unidentified cases are likely present in other locations. These findings warrant the intensification of surveillance strategies and establishment of collaborations with other countries to monitor spread and inform national and global policies and actions.\n\n【24】Prof. Lahra is Medical Director, Division of Bacteriology, and Director of the World Health Organization Collaborating Centre for STD, Sydney, based in the Department of Microbiology, New South Wales Health Pathology, The Prince of Wales Hospital, Sydney. Her research interests include public health and antimicrobial resistance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "77a720ca-3e12-4b79-bbb8-b7c3ab643522", "title": "Flotsam of Never-Ending Respiratory Pathogens", "text": "【0】Flotsam of Never-Ending Respiratory Pathogens\n**Neil Welliver (1929−2005), _Flotsam Allagash_ , 1988.** Oil on canvas. 48 in x 48 in/122 cm x 122 cm. © Neil Welliver, Courtesy Alexandre Gallery, New York.\n\n【1】Noted art critic Robert Hughes wrote that Neil Welliver’s “huge paintings of the Maine woods—usually shown in winter or the early thaws of spring, seen in the remarkable and rigorous clarity of cold light, painted with an almost brusque directness—are among the strongest images in modern American art.”\n\n【2】Described as “a gruff, muscular man who chewed tobacco and somewhat resembled Ernest Hemingway in both appearance and machismo” in an obituary penned by Matt Schudel, Welliver developed a lifelong appreciation of nature while growing up in Millville, Pennsylvania. At age 19, he enrolled at what was then the Philadelphia Museum College of Art (now part of the University of the Arts). In 1955, he received his MFA from Yale, where he studied painting and color theory with Josef Albers.\n\n【3】Art historian Bruce Weber recounts that Albers was Welliver’s “greatest influence, the mentor who provided him with the necessary skills to pursue his personal lines of inquiry.” Subsequently, Albers hired him to teach at Yale, where Welliver remained until 1966. That year Welliver was appointed to develop the Graduate School of Fine Arts at the University of Pennsylvania, where he served as chair until he retired from academia in 1989.\n\n【4】Welliver’s earlier paintings―most of which were lost in a 1975 fire that destroyed his home and studio―were watercolors depicting domestic scenes and people in outdoor settings. His switch from watercolors to oils largely coincided with his focus on Maine landscapes for which he is recognized and appreciated.\n\n【5】This month’s cover image _Flotsam Allagash_ is an example of one of those landscapes. The painting situates the viewer on the bank of the wild, scenic Allagash River traversing Maine’s northwestern forest, once used by a flourishing lumber industry as a commercial waterway. “Flotsam,” defined as waste or debris regarded as worthless, describes the old, abandoned logging and lumber equipment scattered throughout the woods and on river’s shore. A twisted, broken stump looms like a dormant volcano, roots splayed and twisted, heaved up on the mud.\n\n【6】Piles of branches and other flotsam are strewn along the flanks of the shoreline. Though the river has ebbed, water still covers partially submerged branches and trunks. As the winding river disappears in the upper right, one notices that Welliver has rendered the distant mountain ridges, sky, and river with pale blues and grays that seem to merge, in contrast to his thick, rippling brush strokes and more saturated colors in the foreground.\n\n【7】When painting landscapes, Welliver would hike for miles, laden with a heavy backpack jammed with an array of equipment, canvases, paints, and supplies, to scout locations where he would compose _plein-air_ oil sketches, enjoying the crystal quality of the air and luminosity of light reflecting off snow. According to Weber, “His belief was that ‘If you give yourself to a place, you begin to feel its power.’”\n\n【8】But, in an often-quoted interview, Welliver acknowledged that the process was not easy: “To paint outside in the winter is painful. It hurts your hands, it hurts your feet, it hurts your ears. Painting is difficult. The paint is rigid, it’s stiff, it doesn’t move easily. But sometimes there are things you want and that’s the only way you get them.” Weber explains Welliver would return to his studio where he “meticulously plotted his works on large canvases, beginning in the upper left-hand corner and finishing in the lower right. He never revised his paintings once they were complete.” Welliver used a palette of 8 colors of oil paint―specifically ivory black, cadmium red scarlet, manganese blue, ultramarine blue, lemon yellow, cadmium yellow, and talens green light—blending pigments as he worked.\n\n【9】Today Welliver’s works are found in galleries and major museums, including the Hirschhorn Museum and Sculpture Garden, the Metropolitan Museum of Art and Museum of Modern Art in New York, and Boston’s Museum of Fine Arts. Welliver “was generally regarded as the dean of American landscape painting” when he died from pneumonia in 2005, notes Weber.\n\n【10】Pneumonia, which can be caused by viruses, bacteria, and fungi, continues to remain a leading cause of death worldwide. In the wake of the debris resulting from the immunological and inflammatory response initiated by the human host as a result of the injury created by these microorganisms, one could describe the process as flotsam.\n\n【11】Remote and isolated areas, such as the Allagash Wilderness, might offer some protection against respiratory infections caused by human contact. However, one cannot escape the ever present One Health ecological connections. The Allagash and other waterways offer refuge to migrating birds that can harbor highly pathogenic avian influenza strains that have potentially high consequences for wildlife, agriculture, and human health.\n\n【12】In 1930, the year after Welliver was born, the second leading cause of death in the United States was pneumonia and influenza, responsible for 155.9 deaths per 100,000 people. By 2005, that number had dropped to 21.2 deaths per 100,000 people. Vaccines, diagnostic testing, surveillance, antibiotics, clinical treatment, and improved access to care are among the factors responsible for this substantial decline. The spike in deaths from respiratory infections driven by the COVID-19 pandemic, increased case reports of Legionnaires’ disease, and the persistence of influenza starkly remind us of the continuous serious, life-threatening risk posed by respiratory diseases. Much like painting in the winter, progress in pneumonia treatment and prevention is not easy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "26f3bed6-1b8c-4a74-bfd5-4f2d732a383a", "title": "Identification of Patients With Diabetes Who Benefit Most From a Health Coaching Program in Chronic Disease Management, Sydney, Australia, 2013", "text": "【0】Identification of Patients With Diabetes Who Benefit Most From a Health Coaching Program in Chronic Disease Management, Sydney, Australia, 2013\n===============================================================================================================================================\n\n【1】ORIGINAL RESEARCH — Volume 14 — March 2, 2017\n\n【2】Minus\n\n【3】Related Pages\n\n【4】#### Grace Delaney, RN <sup>1 </sup> ; Neroli Newlyn, MN <sup>1 </sup> ; Elline Pamplona, RN <sup>1 </sup> ; Samantha L. Hocking, PhD <sup>1 </sup> <sup>,2 </sup> <sup>,3 </sup> ; Sarah J. Glastras, PhD <sup>1 </sup> <sup>,4 </sup> ; Rachel T. McGrath, PhD <sup>1 </sup> <sup>,2 </sup> <sup>,4 </sup> ; Gregory R. Fulcher, MD <sup>1 </sup> <sup>,2 </sup> ( View author affiliations )\n\n【5】_Suggested citation for this article:_ Delaney G, Newlyn N, Pamplona E, Hocking SL, Glastras SJ, McGrath RT, et al. Identification of Patients With Diabetes Who Benefit Most From a Health Coaching Program in Chronic Disease Management, Sydney, Australia, 2013. Prev Chronic Dis 2017;14:160504. DOI: http://dx.doi.org/10.5888/pcd14.160504 external icon .\n\n【6】PEER REVIEWED\n\n【7】On This Page\n\n【8】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Table\n\n【9】Abstract\n--------\n\n【10】**Introduction**\n\n【11】Chronic disease management programs (CDMPs) that include health coaching can facilitate and coordinate diabetes management. The aim of this study was to assess changes in patients’ general knowledge of diabetes, self-reported health status, diabetes distress, body mass index (BMI), and glycemic control after enrollment in a face-to-face CDMP group health coaching session (with telephone follow-up) compared with participation in telephone-only health coaching, during a 12-month period.\n\n【12】**Methods**\n\n【13】Patients with diabetes were enrolled in a health coaching program at Royal North Shore Hospital, Sydney, Australia, in 2013. Questionnaires were administered at baseline and at 3, 6, and 12 months, and the results were compared with baseline. Glycemic control, measured with glycated hemoglobin A <sub>1c </sub> (HbA <sub>1c </sub> ) and BMI, were measured at baseline and 12 months.\n\n【14】**Results**\n\n【15】Overall, 238 patients attended a face-to-face CDMP session with telephone follow-up (n = 178) or participated in telephone-only health coaching (n = 60). We found no change in BMI in either group; however, HbA <sub>1c </sub> levels in patients with baseline above the current recommended target (>7%) decreased significantly from 8.5% (standard deviation \\[SD\\], 1.0%) to 7.9% (SD, 1.0%) ( _P_ \\= .03). Patients with the lowest self-reported health status at baseline improved from 4.4 (SD, 0.5) to 3.7 (SD, 0.9) ( _P_ \\= .001). Diabetes knowledge improved in all patients (24.4 \\[SD, 2.4\\] to 25.2 \\[SD, 2.4\\]; _P_ < .001), and diabetes distress decreased among those with the highest levels of distress at baseline (3.0 \\[SD, 0.4\\] vs 3.8 \\[SD, 0.6\\]; _P_ \\= .003).\n\n【16】**Conclusion**\n\n【17】Diabetes health coaching programs can improve glycemic control and reduce diabetes distress in patients with high levels of these at baseline.\n\n【18】Top\n\n【19】Introduction\n------------\n\n【20】Diabetes is a chronic disease associated with illness and premature death (1). As a consequence, poor quality of life and diabetes distress are frequently observed (2,3). Diabetes distress (defined as the psychological stress related to living with diabetes), a poor quality of life, and low levels of diabetes knowledge, may have a detrimental effect on diabetes self-management and glycemic control, which in turn can lead to an increased risk of complications (4,5). Although the effect of diabetes on patient quality of life and feelings of well-being is not routinely assessed in clinical practice, such information can assist in developing an individual approach to diabetes management.\n\n【21】Education programs tailored toward diabetes self-management are effective ways to engage patients in management decisions and improve health outcomes (6,7). In one study, attendance at a diabetes group education session for 1 year improved knowledge of diabetes and problem-solving ability (8). Similarly, diabetes self-management programs were found to improve quality of life and diabetes distress in people with both type 1 and type 2 diabetes (9,10). Health coaching, an individualized educational approach to self-management through problem solving, and goal setting may be a useful adjunct to regular diabetes education to improve glycemic control and self-reported health status (11,12).\n\n【22】A chronic disease management program (CDMP) incorporating health coaching was introduced at Royal North Shore Hospital (RNSH) in Sydney in 2013. This approach differed from a traditional approach to group diabetes education because it was tailored to patient-set goals and targets. The aim of this study was to determine whether this education program led to a change in patients’ self-reported health status, diabetes distress, diabetes knowledge, body mass index (BMI), and glycemic control over a 12-month period of follow-up, regardless of whether health coaching was provided as a face-to-face session with additional telephone support or as telephone health coaching alone. We sought to identify the types of patients that might experience the greatest improvements in these measures and hypothesized that this information could lead to a more directed approach to enrollment in such programs.\n\n【23】Top\n\n【24】Methods\n-------\n\n【25】### Diabetes chronic disease management program\n\n【26】The health coaching program facilitated through the Department of Diabetes, Endocrinology and Metabolism at RNSH is funded through the Northern Sydney Local Health District CDMP. The CDMP is designed for people with specific chronic diseases (diabetes, coronary artery disease, chronic obstructive pulmonary disease, congestive heart failure, hypertension) who are at increased risk of hospital admission (13). The services provided through the program could include access to, and coordination of, health care appointments and additional support to enable people to more effectively self-manage diabetes to potentially reduce complications, improve health, and prevent hospitalization (14). All patients enrolled in the CDMP at RNSH were asked if they would like to participate in the health coaching program; just over half (53%) of invited patients took part in the program in 2013. The eligibility criteria for participation in the health coaching program were that patients be English-speaking, aged 16 years or older, and have type 1 or type 2 diabetes.\n\n【27】### Diabetes health coaching\n\n【28】The diabetes health coaching program at RNSH consisted of a face-to-face group education session, entitled the “Empowerment Program,” and telephone calls from a health coach (diabetes nurse educator) for 12 months after the group session. Alternatively, patients unable to participate in the face-to-face session could participate in telephone coaching in which they received an initial telephone call with educational information and follow-up telephone calls from the health coach for 12 months. The frequency of telephone calls depended on patient preference and ranged from weekly to 3 times a month. For most patients, telephone calls were made once per month.\n\n【29】The diabetes health coaching program used a diabetes conversation map to guide the initial face-to-face discussion or telephone call, which focused mainly on management of diabetes complications and risk-factors. Patients were provided with information and guidance on healthy eating, recommended levels of physical activity, and prevention of diabetes complications. In addition, information was provided on diabetes-specific health targets and recommendations for timing of visits to a primary care provider for review of progress toward targets and testing of blood glucose levels. Patients were encouraged to set goals related to management of their diabetes: for example, minutes of exercise per day, daily healthy eating aims, or medication adherence. The health coach provided guidance on how to develop strategies that would enable patients to achieve their goals. In subsequent telephone calls with the health coach, patients reported on their progress and evaluated their strategies for achieving their goals.\n\n【30】### CDMP health coaching patient assessments\n\n【31】To examine patients’ general knowledge of diabetes we used the Diabetes, Hypertension and Hyperlipidemia (DHL) knowledge instrument (15). In brief, this instrument was evaluated and validated in a population of patients with diabetes in Malaysia (15) and contains questions on patients’ understanding of the importance of glucose, blood pressure, and lipid control in minimizing the risk of diabetes complications. The DHL questionnaire consists of 28 questions, and each question that is answered correctly is given a score of 1; the maximum score is 28. Self-reported health status was assessed by using the first question of the Short-Form 36 Quality of Life Instrument (SF-1) (16,17); a score of 4 or higher (maximum score, 6) on the SF-1 indicates poor health-status (17). Diabetes distress was assessed by using the Diabetes Distress Scale (DDS) (18,19). An overall score (maximum score, 6) is determined by aggregating answers to 17 questions that each have a scale of 1 to 6; a score of 3 or higher on the DDS demonstrates moderate to severe diabetes distress (20). The DHL, SF-1, and DDS questionnaires were administered at 4 time points throughout the program — at baseline and at 3, 6, and 12 months. These questionnaires have been validated for use in the population of people with diabetes (16–20).\n\n【32】### Glycemic control, BMI, and medical history\n\n【33】All patients recruited into the Diabetes CDMP at RNSH had a baseline HbA <sub>1c </sub> measurement (immediately before being referred for health coaching) and subsequently at their routine appointment for diabetes complications screening at 12 months. In addition, weight and height were measured to calculate BMI (calculated as weight in kilograms \\[kg\\] divided by height in meters \\[m\\] squared) at baseline and 12 months. Data were obtained from patient medical records to determine the presence or absence of dyslipidemia, hypertension or diabetes complications (retinopathy, neuropathy or chronic kidney disease (defined clinically by an estimated glomerular filtration rate \\[eGFR\\] <60ml/min/1.73m <sup>2 </sup> ).\n\n【34】### Study design\n\n【35】An audit of longitudinal, prospectively collected data was carried out to investigate the impact of a diabetes health coaching program on patient outcomes during a 12-month period of follow-up. All patients participating in the health coaching program were living in the catchment area of RNSH, which is within the Northern Sydney Local Health District, one of Sydney’s areas of highest socioeconomic status (21). The study was approved by the local institutional review board, the Northern Sydney Local Health District Human Research Ethics Committee (reference no. RESP/16/58).\n\n【36】### Statistical analyses\n\n【37】The differences in patient-reported diabetes knowledge, health status, and diabetes distress over time were assessed by using Student’s paired _t_ test for parametric data and Wilcoxon matched-pairs signed-rank test for nonparametric data. For repeated measures, 1-way ANOVA was used to analyze parametric data, and the Friedman test was used for nonparametric data. Differences between groups were analyzed using Student’s unpaired _t_ test or the Mann–Whitney test, where appropriate. The change in HbA <sub>1c </sub> and BMI was assessed by paired _t_ test. Statistical analysis was carried out using GraphPad Prism Version 6 (GraphPad Software, Inc) and _P_ < .05 was considered significant.\n\n【38】Top\n\n【39】Results\n-------\n\n【40】### Patient demographics and characteristics\n\n【41】From January through December 2013, 178 patients participated in the face-to-face health coaching session followed by telephone support, and 60 patients participated in telephone-only health coaching ( Table ). Most patients (97.1%) in the overall cohort had type 2 diabetes, and most of the cohort was older than 65 years. Approximately two-thirds of the participants in both groups were women.\n\n【42】The incidence of comorbidities was similar between groups, and the most frequent comorbid conditions were hypertension (69.3%) and dyslipidemia (91.2%). We found no difference in age or duration of diabetes between groups (Table). Conversely, patients in the telephone-only health coaching group had a higher HbA <sub>1c </sub> (7.3%; _P_ \\= .03) and greater BMI (31.1; _P_ \\= .04) at baseline and were more likely to take insulin (46.7%; _P_ \\= .001) (Table).\n\n【43】The proportion of patients who completed assessments at all time points was 31.1% (74 of 238). Patients who did not complete all assessments were significantly more likely to be younger (mean, 67.0 y; standard deviation \\[SD\\], 10.0 y) than those who completed all assessments (71.3 y \\[SD, 8.7 y\\]) ( _P_ \\= .003) and to have a higher BMI at baseline (30.2 \\[SD, 5.3\\]) than those who completed all assessments (28.8 \\[SD, 5.6\\]) ( _P_ \\= .02) but were similar otherwise. Patients who did not complete all assessments exhibited no difference in diabetes knowledge, self-reported health status, or diabetes distress at baseline compared to patients who completed all assessments. Evaluable data (ie, baseline and 12-month data) were available for 50.4% of patients (120/238).\n\n【44】### General knowledge of diabetes\n\n【45】The mean baseline score for general knowledge of diabetes for all participants who completed the baseline assessment (n = 212) was 24.4 (SD, 2.4). Diabetes knowledge improved significantly among participants who also completed the assessment at 12 months, from a mean score of 24.4 to a mean score of 25.2 (SD, 2.2) ( _P_ < .001) (Figure 1A). In addition, 74 patients (31.1%) completed the diabetes knowledge assessment at all time points; of these, 56 patients attended the face-to-face session, and 18 patients participated in the telephone-only coaching. General knowledge of diabetes improved over time whether or not participants completed all assessments (Figure 1A and Figure 1B). Diabetes knowledge improved in 3 months, and this improvement was sustained at one year post enrollment.\n\n【46】**Figure 1.** Change in patients’ general knowledge of diabetes over time, measured with the Diabetes, Hypertension and Hyperlipidemia (DHL) knowledge instrument (15), for patients participating in the health coaching program. The change in score (possible range, 0–28) was assessed over time in A) all patients (n = 238), and in B) patients who completed the assessment at all time points. Scores for A at each time point after baseline were compared with baseline scores by using the Wilcoxon matched-pairs signed-rank test. Scores for B at each time point after baseline were compared with baseline scores by using the Friedman test. Error bars indicate standard deviation. \\[A tabular version of this figure is also available.\\]\n\n【47】We found no difference in diabetes knowledge at baseline between patients who attended the face-to-face health coaching session and patients who participated in telephone-only coaching (mean score, 24.3 \\[SD, 2.5\\] vs 25.1 \\[SD, 1.8\\]; _P_ \\= .10); however, patients who attended the face-to-face session improved significantly in diabetes knowledge at 12 months (mean score, 24.3 \\[SD, 2.5\\] vs 25.4 \\[SD, 2.4\\]; _P_ < .001) compared with patients who participated in telephone-only coaching (mean score, 25.1 \\[SD, 1.8\\] vs 24.7 \\[SD, 2.4\\]); _P_ \\= .66).\n\n【48】### Self-reported health status\n\n【49】The proportion of patients who reported poor health status at baseline (ie, patients with a score of ≥4 on the SF-1 scale) was 27.3%. Moreover, patients with diabetes complications (retinopathy, neuropathy, chronic kidney disease) were more likely to report a lower health status at baseline than patients with no diabetes complications (3.2 \\[SD, 1.0\\] vs 2.9 \\[SD, 1.0\\]; _P_ \\= .01). Among patients with poor health status at baseline (n = 65) who completed a minimum of 2 assessments (n = 51), health status improved significantly for 36 patients from 4.4 \\[SD, 0.6\\] at baseline to 3.6 \\[SD, 1.1\\] at 6 months ( _P_ < .001) (Figure 2A) and for 28 patients from 4.4 \\[SD, 0.6\\] at baseline to 3.7 \\[SD, 0.9\\] at 12 months ( _P_ \\= .001) (Figure 2A). Patients taking insulin (n = 68) had the same health status score at baseline as patients taking an oral antihyperglycemic medication (3.2 \\[SD, 1.0\\] vs 3.0 \\[SD 1.0\\]). We found no difference in self-reported health status at any time point between patients who attended the face-to-face health coaching and patients who participated in telephone-only health coaching.\n\n【50】**Figure 2.** Change in general knowledge of diabetes over time in self-reported health status and levels of diabetes distress in patients with diabetes. A. Self-reported health status (measured by the Short-Form 36 Quality of Life Instrument \\[16,17\\]) among respondents who had a high baseline score (score of 4, 5, or 6) and who had completed a minimum of 2 assessments (n = 51, 21.4%). B. Diabetes distress among respondents who had moderate to high distress at baseline (Diabetes Distress Scale score >3) and who had completed a minimum of 2 assessments (n = 17, 7.1%); _P_ \\= .04 for difference between 6 months and baseline; _P_ \\= .003 for difference between 12 months and baseline. _P_ values were calculated by using Wilcoxon matched-pairs signed rank test. Error bars indicate standard deviation. \\[A tabular version of this figure is also available.\\]\n\n【51】### Diabetes distress\n\n【52】Most patients who participated in the overall health coaching program (91.6%) had low levels of diabetes distress at baseline (DDS score <3). Patients with diabetes complications had distress levels similar to those without complications (DDS score, 1.9 \\[SD, 0.8\\] vs 1.9 \\[SD, 0.9\\]). Use of insulin was not associated with higher levels of distress (DDS score, 2.0 \\[SD, 0.9\\] vs 1.8 \\[SD, 0.8\\]; _P_ \\= .13). Among patients who had moderate to high diabetes distress at baseline (DDS score >3) and who completed a minimum of 2 assessments (n = 17), we found a small yet significant decrease in diabetes distress at 6 months for 15 patients (DDS score, 3.2 \\[SD, 0.7\\] vs 3.8 \\[SD, 0.6\\]; _P_ \\= .04) (Figure 2B) and 12 months for 10 patients (3.0 \\[SD, 0.4\\] vs 3.8 \\[SD, 0.6\\]; _P_ \\= .003) (Figure 2B) compared with baseline. Similar levels of diabetes distress were found at each time point irrespective of health coaching method (face-to-face or telephone only).\n\n【53】### Glycemic control and body mass index\n\n【54】The average HbA <sub>1c </sub> at baseline was 7.0% (SD, 1.1%), and 194 (81.5%) patients were considered to be at target HbA <sub>1c </sub> level (<7%). For patients above target, that is, those with poor glycemic control at baseline (HbA <sub>1c </sub> \\>7%; n = 44), we observed a significant improvement at 12 months (8.5% \\[SD, 1.0%\\] vs 7.9% \\[SD, 1.0%\\]; _P_ \\= .03). The proportion of patients with poor glycemic control at baseline (HbA <sub>1c </sub> \\>7%; n = 44) who achieved their target HbA <sub>1c </sub> level at 12 months was 20.05%. We found no significant difference in the improvement in HbA <sub>1c </sub> for patients based on health coaching method (Table).\n\n【55】We found no difference in BMI at 12 months compared with baseline in either the face-to-face or telephone-only health coaching group. Similarly, for patients with significant obesity at baseline (BMI ≥35; n = 29) we found no change in BMI at 12 months. Finally, health coaching method did not affect the change in BMI over time (Table).\n\n【56】Top\n\n【57】Discussion\n----------\n\n【58】The results of the present analysis identify patients who demonstrated the most benefit in diabetes distress and glycemic control after participation in a 12-month diabetes health coaching CDMP. Patients with a high level of diabetes distress, poor self-reported health status, and a low level of diabetes knowledge at baseline had the most improvement in these measures at both 6 and 12 months post CDMP initiation. The face-to-face health coaching session in the context of a CDMP resulted in significantly improved general knowledge of diabetes, whereas telephone health coaching only did not. Furthermore, patients with HbA <sub>1c </sub> levels above target at baseline demonstrated a significant improvement in glycemic control.\n\n【59】Identification of patients who are likely to receive the most benefit from participating in a CDMP or health coaching is of clinical significance, because programs can be tailored toward this patient population, and screening can be initiated before their enrollment so that outcomes can be maximized and costs to the health care provider minimized. This identification will provide a direct benefit for patients and will also ensure that health care services are used efficiently. Moreover, additional measures could be put in place to target the areas in which patients report they are most distressed or require support and education for diabetes. Therefore, we focused our analysis on patients with low levels of self-reported health status, elevated diabetes distress, or HbA <sub>1c </sub> levels above target at baseline. Health coaching is an innovative approach to chronic disease management that allows patients to set their own health goals, rather than the traditional approach of the provider setting the goals for the patient. This approach allows patients to feel more empowered and in control of their own health and may contribute to the improvement in psychological stress associated with diabetes, including perceived health-status and diabetes distress.\n\n【60】Our results are similar to those of Beverley et al, who found that older patients (aged 60–75 y) demonstrated positive changes in quality of life and diabetes distress after diabetes education in behavioral interventions (22). In addition, a recent study showed that health coaching for patients with type 2 diabetes resulted in improved satisfaction with life and a reduction in symptoms of depression (23). A meta-analysis determined that self-management and education interventions for diabetes were most effective in patients with depression symptoms or high levels of stress at baseline (24), which is similar to our findings. In contrast to other studies (7,25), we did not observe an improvement in glycemic control for the whole cohort after participation in the CDMP. The most likely explanation is that in our cohort, most participants (81.5%) had achieved target HbA <sub>1c </sub> levels before commencing the program and maintained this level of glycemic control over the subsequent 12 months, despite an anticipated decline in β-cell function during follow-up. For patients with suboptimal glycemic control at baseline, a significant improvement was observed after health coaching, suggesting that this mode of diabetes education is of substantial benefit to patients with inadequately controlled blood glucose levels.\n\n【61】Diabetes knowledge and attitudes toward diabetes can directly affect self-management, which in turn influences quality of life (26). Consistent with this, we found improvements in both diabetes knowledge and diabetes distress, suggesting that greater understanding of diabetes may reduce the stress that accompanies it and may make dealing with diabetes easier for patients. Furthermore, patients provided positive feedback regarding the program and stated that a health-coaching approach to patient follow-up was beneficial.\n\n【62】Our study population had high levels of diabetes education at entry into the health coaching program; therefore, only a small proportion of patients had high levels of diabetes distress or poor glycemic control at baseline. As a result, the improvement that occurred was observed in a subset of the entire cohort of participants. However, the change in distress observed for this group was significant, at both 6 and 12 months after beginning health coaching, suggesting that a sustained improvement took place. Conversely, the change in self-reported health status over time may be due to response bias, because patients completed the same questionnaire at several time points throughout their participation in the health coaching program. Furthermore, because the frequency of follow-up telephone calls as part of health coaching was determined by patient preference, more regular contact with the health coach possibly resulted in improved outcomes. However we were unable to assess this in the present study.\n\n【63】The strengths of this study are the large number of participants and the longitudinal data capture as part of the health coaching program’s patient assessments. Our study had several limitations. A high attrition rate in questionnaire completion may have led to bias in the responses obtained. Furthermore, there was no control group in which diabetes health coaching did not take place, and the study was not randomized to health coaching compared with standard diabetes education. Nonetheless, each participant acted as his or her own control as the change in his or her questionnaire results, BMI, and glycemic control was measured over time and compared with baseline. There is also a potential limitation in the sampling method used, and the study participants may not be representative of the larger population of patients living with chronic conditions. However, we consecutively recruited agreeable patients into the health coaching program after their enrollment in the CDMP, and patient characteristics were representative of those in other cohorts of patients with diabetes in Australia (27,28).\n\n【64】In summary, patients with high levels of diabetes distress or poor glycemic control at entry into health coaching were the ones who benefitted most from taking part in the program. On the basis of our results, we propose that such patients be routinely offered additional health coaching or more specialized chronic disease management, rather than providing all patient populations with interventions that may not be relevant or efficacious. Personalized medicine is increasingly being recognized as an effective way of delivering health care, and identification of patients with diabetes most responsive to particular education strategies will contribute toward the optimization of health care resources.\n\n【65】Top\n\n【66】Acknowledgments\n---------------\n\n【67】Grace Delaney and Neroli Newlyn contributed equally to this article. The authors received no funding for the research described in this article and have no conflicts of interest to declare.\n\n【68】Top\n\n【69】Author Information\n------------------\n\n【70】Corresponding Author: Rachel McGrath, Department of Diabetes, Endocrinology and Metabolism, Level 3, Acute Services Building, Royal North Shore Hospital, St Leonards, Sydney, NSW 2065, Australia. Telephone: +61 (2) 9463 1470. Email: rachel.mcgrath@sydney.edu.au .\n\n【71】Author Affiliations: <sup>1 </sup> Department of Diabetes, Endocrinology & Metabolism, Royal North Shore Hospital, St Leonards, Sydney, NSW 2065, Australia. <sup>2 </sup> University of Sydney, Northern Clinical School, Royal North Shore Hospital, St Leonards, NSW 2065, Australia. <sup>3 </sup> Charles Perkins Centre, University of Sydney, Australia. <sup>4 </sup> Kolling Institute of Medical Research, University of Sydney, Royal North Shore Hospital, St Leonards, NSW 2065, Australia.\n\n【72】Top\n\n【73】References\n----------\n\n【74】1.  Alberti KG, Zimmet PZ. Definition, diagnosis and classification of diabetes mellitus and its complications. Part 1: diagnosis and classification of diabetes mellitus provisional report of a WHO consultation. Diabet Med 1998;15(7):539–53. CrossRef external icon PubMed external icon\n2.  Rubin RR, Peyrot M. Quality of life and diabetes. Diabetes Metab Res Rev 1999;15(3):205–18. CrossRef external icon PubMed external icon\n3.  Snoek FJ, Bremmer MA, Hermanns N. Constructs of depression and distress in diabetes: time for an appraisal. Lancet Diabetes Endocrinol 2015;3(6):450–60. CrossRef external icon PubMed external icon\n4.  Glasgow RE, Toobert DJ, Gillette CD. Psychosocial barriers to diabetes self-management and quality of life. Diabetes Spectr 2001;14(1):33–41. CrossRef external icon\n5.  Fisher L, Mullan JT, Arean P, Glasgow RE, Hessler D, Masharani U. Diabetes distress but not clinical depression or depressive symptoms is associated with glycemic control in both cross-sectional and longitudinal analyses. Diabetes Care 2010;33(1):23–8. CrossRef external icon PubMed external icon\n6.  Anderson RM, Funnell MM, Butler PM, Arnold MS, Fitzgerald JT, Feste CC. Patient empowerment. Results of a randomized controlled trial. Diabetes Care 1995;18(7):943–9. CrossRef external icon PubMed external icon\n7.  Norris SL, Lau J, Smith SJ, Schmid CH, Engelgau MM. Self-management education for adults with type 2 diabetes: a meta-analysis of the effect on glycemic control. Diabetes Care 2002;25(7):1159–71. CrossRef external icon PubMed external icon\n8.  Trento M, Passera P, Borgo E, Tomalino M, Bajardi M, Cavallo F, et al. A 5-year randomized controlled study of learning, problem solving ability, and quality of life modifications in people with type 2 diabetes managed by group care. Diabetes Care 2004;27(3):670–5. CrossRef external icon PubMed external icon\n9.  Wattana C, Srisuphan W, Pothiban L, Upchurch SL. Effects of a diabetes self-management program on glycemic control, coronary heart disease risk, and quality of life among Thai patients with type 2 diabetes. Nurs Health Sci 2007;9(2):135–41. CrossRef external icon PubMed external icon\n10.  Forlani G, Zannoni C, Tarrini G, Melchionda N, Marchesini G. An empowerment-based educational program improves psychological well-being and health-related quality of life in Type 1 diabetes. J Endocrinol Invest 2006;29(5):405–12. CrossRef external icon PubMed external icon\n11.  Wong-Rieger D, Rieger FP. Health coaching in diabetes: empowering patients to self-manage. Can J Diabetes 2013;37(1):41–4. CrossRef external icon PubMed external icon\n12.  Dennis SM, Harris M, Lloyd J, Powell Davies G, Faruqi N, Zwar N. Do people with existing chronic conditions benefit from telephone coaching? A rapid review. Aust Health Rev 2013;37(3):381–8. PubMed external icon\n13.  The Australian Government Department of Health. Chronic disease management; 2014. http://www.health.gov.au/internet/main/publishing.nsf/content/mbsprimarycare-chronicdiseasemanagement. Accessed June 22, 2016.\n14.  NSW Agency for Clinical Innovation. NSW Chronic disease management program — connecting care in the community. http://www.aci.health.nsw.gov.au/resources/chronic-care/cdmp/nsw-cdmp. Accessed June 22, 2016.\n15.  Lai PS, Chua SS, Tan CH, Chan SP. Validation of the diabetes, hypertension and hyperlipidemia (DHL) knowledge instrument in Malaysia. BMC Med Res Methodol 2012;12(1):18. CrossRef external icon PubMed external icon\n16.  Hill-Briggs F, Gary TL, Baptiste-Roberts K, Brancati FL. Thirty-six-item short-form outcomes following a randomized controlled trial in type 2 diabetes. Diabetes Care 2005;28(2):443–4. CrossRef external icon PubMed external icon\n17.  University of Adelaide. North West Adelaide health study. Health outcomes overall health status as measured by the SF-1; 2011. https://health.adelaide.edu.au/pros/docs/reports/24\\_nw123\\_sf1.pdf. Accessed June 22, 2016.\n18.  Polonsky WH, Fisher L, Earles J, Dudl RJ, Lees J, Mullan J, et al. Assessing psychosocial distress in diabetes: development of the diabetes distress scale. Diabetes Care 2005;28(3):626–31. CrossRef external icon PubMed external icon\n19.  Schmitt A, Reimer A, Kulzer B, Haak T, Ehrmann D, Hermanns N. How to assess diabetes distress: comparison of the problem areas in diabetes scale (PAID) and the Diabetes Distress Scale (DDS). Diabet Med 2016;33(6):835–43. CrossRef external icon PubMed external icon\n20.  Fisher L, Hessler DM, Polonsky WH, Mullan J. When is diabetes distress clinically meaningful: establishing cut points for the Diabetes Distress Scale. Diabetes Care 2012;35(2):259–64. CrossRef external icon PubMed external icon\n21.  HealthStats NSW. Socioeconomic indexes for areas (SEIFA) by local government area HealthStats NSW. http://www.healthstats.nsw.gov.au/Indicator/soc\\_seifa\\_lgamap/soc\\_seifa\\_lgamap. Accessed November 30, 2016.\n22.  Beverly EA, Fitzgerald S, Sitnikov L, Ganda OP, Caballero AE, Weinger K. Do older adults aged 60–75 years benefit from diabetes behavioral interventions? Diabetes Care 2013;36(6):1501–6. CrossRef external icon PubMed external icon\n23.  Wayne N, Perez DF, Kaplan DM, Ritvo P. Health coaching reduces HbA1c in type 2 diabetic patients from a lower-socioeconomic status community: a randomized controlled trial. J Med Internet Res 2015;17(10):e224. CrossRef external icon PubMed external icon\n24.  Steed L, Cooke D, Newman S. A systematic review of psychosocial outcomes following education, self-management and psychological interventions in diabetes mellitus. Patient Educ Couns 2003;51(1):5–15. CrossRef external icon PubMed external icon\n25.  Pibernik-Okanovic M, Prasek M, Poljicanin-Filipovic T, Pavlic-Renar I, Metelko Z. Effects of an empowerment-based psychosocial intervention on quality of life and metabolic control in type 2 diabetic patients. Patient Educ Couns 2004;52(2):193–9. CrossRef external icon PubMed external icon\n26.  Kueh YC, Morris T, Borkoles E, Shee H. Modelling of diabetes knowledge, attitudes, self-management, and quality of life: a cross-sectional study with an Australian sample. Health Qual Life Outcomes 2015;13(1):129. CrossRef external icon PubMed external icon\n27.  Keech A, Simes RJ, Barter P, Best J, Scott R, Taskinen MR, et al. ; FIELD study investigators. Effects of long-term fenofibrate therapy on cardiovascular events in 9795 people with type 2 diabetes mellitus (the FIELD study): randomised controlled trial. Lancet 2005;366(9500):1849–61. CrossRef external icon PubMed external icon\n28.  Davis TM, Hunt K, McAullay D, Chubb SA, Sillars BA, Bruce DG, et al. Continuing disparities in cardiovascular risk factors and complications between aboriginal and Anglo-Celt Australians with type 2 diabetes: the Fremantle Diabetes Study. Diabetes Care 2012;35(10):2005–11. CrossRef external icon PubMed external icon\n\n【75】Top\n\n【76】Table\n-----\n\n【77】#####  Table. Characteristics of Patients Participating in Health Coaching, Diabetes Chronic Disease Management Program, Royal North Shore Hospital, Sydney, Australia, 2013\n\n| Characteristic a,b | Face-to-Face Health Coaching c (n = 178) | Telephone-Only Health Coaching d (n = 60) | All Patients (n = 238) |\n| --- | --- | --- | --- |\n| Age, mean (SD), y | 69.0 (9.6) | 66.6 (10.1) | 68.4 (9.8) |\n| Female | 119 (66.9) | 38 (63.3) | 157 (66.0) |\n| Type 1 diabetes | 6 (3.4) | 1 (1.7) | 7 (2.9) |\n| Type 2 diabetes | 172 (96.6) | 59 (98.3) | 231 (97.1) |\n| Duration of diabetes, mean (SD), y | 12.4 (6.9) | 13.5 (7.1) | 12.7 (6.9) |\n| Takes insulin | 40 (22.6) | 28 (46.7) | 68 (28.6) |\n| Takes oral antihyperglycemic medication | 149 (83.7) | 50 (83.3) | 199 (83.6) |\n| HbA 1c at baseline, % (SD) | 6.9 (1.0) | 7.3 (1.2) | 7.0 (1.1) |\n| HbA 1c at 12 months, % (SD) | 6.9 (1.0) | 7.0 (1.1) | 6.9 (1.0) |\n| BMI at baseline, kg/m 2 (SD) | 29.3 (5.7) | 31.1 (5.5) | 29.8 (5.4) |\n| BMI at 12 months, kg/m 2 (SD) | 29.2 (5.1) | 30.1 (5.6) | 29.5 (5.3) |\n| **Comorbidities e** | **Comorbidities e** | **Comorbidities e** | **Comorbidities e** |\n| Hypertension | 123 (69.1) | 42 (70.0) | 165 (69.3) |\n| Dyslipidemia | 163 (91.0) | 54 (90.0) | 217 (91.2) |\n| **Diabetes complications** | **Diabetes complications** | **Diabetes complications** | **Diabetes complications** |\n| Retinopathy | 20 (11.2) | 10 (16.7) | 30 (12.6) |\n| Chronic kidney disease | 57 (32.0) | 20 (33.3) | 77 (32.4) |\n| Peripheral neuropathy | 39 (21.9) | 11 (18.3) | 50 (21.0) |\n\n【79】Abbreviation: BMI, body mass index; HbA <sub>1c </sub> , glycated hemoglobin A <sub>1c </sub> ; SD, standard deviation.  \n<sup>a </sup> Data are number (percentage) unless otherwise indicated.  \n<sup>b </sup> Data obtained from patient medical records.  \n<sup>c </sup> Patients who received one in-person coaching session.  \n<sup>d </sup> Patients who received coaching by telephone.  \n<sup>e </sup> Data recorded at baseline from patient medical records.\n\n【80】Top", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "856d6456-5129-46a0-bf2c-4b1e1d8916cf", "title": "Contagious Period for Pandemic (H1N1) 2009", "text": "【0】Contagious Period for Pandemic (H1N1) 2009\nSince April 2009, an influenza A virus, pandemic (H1N1) 2009, has spread to most countries of the world. Widespread susceptibility of persons <60 years of age may have facilitated rapid dissemination ( _1_ ). Transmissibility of influenza viruses depends on duration of shedding, amount of virus shed, and other factors that may facilitate projection of virus into the environment, such as coughing or sneezing. Challenge studies in healthy volunteers inoculated with seasonal influenza viruses have shown that shedding generally coincides with symptom onset starting 1 day after inoculation, peaks on the second day, and generally ends 1 week after disease onset, on day 8 ( _2_ ). Duration of shedding is greatly affected by age, and for seasonal influenza viruses is longer in young children than in adults ( _3_ – _5_ ). Since the emergence of pandemic (H1N1) 2009, the recommended duration of self-isolation has varied from complete resolution of symptoms to 1 day after fever has subsided ( _6_ , _7_ ). The objective of this study was to estimate the proportion of pandemic (H1N1) 2009–infected persons shedding infectious virus 1 week after illness onset.\n\n【1】### Methods\n\n【2】##### Study Setting\n\n【3】Figure 1\n\n【4】Figure 1 . Flowchart of characteristics of 173 participants in study of shedding of pandemic (H1N1) 2009 virus, Quebec City, Quebec, Canada, May 27–July 10, 2009.\n\n【5】The research ethics committee of the Centre Hospitalier Universitaire de Québec approved the study. The prospective study was conducted during May 27–July 10, 2009, in Quebec City, Quebec, Canada. All participants were community based. Eligible persons were members of households in which at least 1 person with pandemic (H1N1) 2009 confirmed by reverse transcription–PCR (RT-PCR). Primary case-patients were either referred by their treating physicians or identified among community contacts of persons with laboratory-confirmed cases ( Figure 1 ). At the initial home visit, a nurse obtained written informed consent from all household members and collected data for each household participant by using a standardized questionnaire. The questionnaire asked about basic sociodemographic characteristics (e.g., age, sex, occupation), presence of underlying medical conditions, presence of various symptoms or signs (i.e., fever, chills, cough, sore throat, rhinorrhea, arthralgia/myalgia, fatigue, dyspnea, headache, diarrhea, or vomiting), seasonal influenza vaccination history, and social and healthcare impact of their illness (i.e., missed workdays or schooldays, days spent in bed, medical consultations, emergency department visits, or hospital admissions).\n\n【6】##### Nasopharyngeal Swabs and Laboratory Procedures\n\n【7】Nasopharyngeal (NP) secretions were collected from all household members, both symptomatic and asymptomatic, with NP swabs (Nylon Flocked Swabs; Copan Innovation, Brescia, Italy) that were inserted into 3 mL of universal transport medium (Copan Innovation). Patients with results positive by RT-PCR for pandemic (H1N1) 2009 before day 7 since disease onset had a second NP swab repeated on day 8. Persons still positive on day 8 were retested on day 11.\n\n【8】Nucleic acids were extracted from 200 µL of specimen by using the QIAGEN Viral RNA Mini Kit (QIAGEN, Mississauga, Ontario, Canada). Initially, pandemic (H1N1) 2009 was detected by using a conventional RT-PCR (pH1N1 PCR) specific for the hemagglutinin gene ( _8_ ). RNA extracts from pH1N1 PCR–negative specimens were frozen at –80°C and, once specimen collection was completed, were retested by using a conventional RT-PCR assay for all influenza A viruses (matrix PCR), which has a higher sensitivity ( _9_ ).\n\n【9】All samples with RT-PCR–positive results were cultured on MDCK cells in shell vials containing 1 mL of media to detect replicating viruses. Vials were observed daily for 7 days to detect cytopathic effects. Virus cultures were conducted on fresh specimens if they were positive by the pH1N1 PCR, whereas samples positive by the conventional matrix PCR had undergone 1 freeze-thaw cycle. All cultures showing cytopathic effects were sent to the Québec provincial reference laboratory for RT-PCR confirmation by using the previously described pH1N1 PCR.\n\n【10】##### Statistical Analyses\n\n【11】We compared proportions and distributions using the χ <sup>2 </sup> test or Fisher exact test when appropriate. All statistical analyses were 2-tailed, and p values < 0.05 were considered significant.\n\n【12】### Results\n\n【13】Figure 2\n\n【14】Figure 2 . Positive results by PCR and culture for influenza A and pandemic (H1N1) 2009 virus (pH1N1) in 47 household contacts with laboratory-confirmed influenza, by delay between day of symptom onset and day...\n\n【15】Of the 173 persons from 47 participating households, 35 with pH1N1 PCR–confirmed persons (index case-patients) were referred by their treating physicians. Among the 138 other participants (household or community contacts), 73 had respiratory symptoms for <7 days at the time of enrolment and 32 (44%) of 73 were positive by pH1N1 PCR ( Figure 1 , Table 1 ). Cell culture was also positive for 97% (31/32) of these pH1N1 PCR–confirmed cases. Of the 32 pH1N1 PCR–confirmed cases, 78% (25/32) had fever at some point since illness onset; at the time of specimen collection, 94% (30/32) had cough and 34% (11/32) were still febrile. Specimens were retested with a matrix PCR. All results from the pH1N1 PCR–positive participants were also positive by matrix PCR; of those who were negative by pH1N1 PCR, 37% (15/41) were positive. Of the 15 participants positive by matrix PCR, 13% (2/15) were positive by cell culture. Of the 47 confirmed cases (32 by pH1N1 PCR and 12 by matrix PCR), cell culture positivity varied from 69% to 87% for specimens obtained within 4 days after symptom onset and dropped to 33%–40% for specimens obtained days 5 and 6 after symptom onset ( Figure 2 ).\n\n【16】Of the 67 patients whose results were positive by pH1N1 PCR (35 case-patients initially referred by their physicians plus 32 detected among contacts), 62 were identified < 7 days after symptom onset and 43 had a second swab collected on day 8 ( Figure 1 ). Of these 43 cases (core participants), 47% were children <10 years of age, 26% were 10–17 years of age, and 28% were adults ( \\> 18 years of age) ( Table 1 ). On day 8, 18 (42%) were still positive by pH1N1 PCR, and 14 others were positive only by matrix PCR, for a total of 74% (32/43) positive by any PCR method on day 8. ( Table 2 ) Virus culture was positive for 19% (8/43) of the patients: culture was positive only for pH1N1 PCR–positive cases (8/18, 44%) but for none of the cases positive only by matrix PCR. PCR and virus culture positivity rates did not differ among age groups ( Table 2 ). Only 5% (2/43) of case-patients were still febrile on day 8, but 91% (39/43) were still coughing. None of the 8 patients who had a positive virus culture on day 8 were febrile, but 7 (88%) were still coughing. Another swab was repeated on day 11 for 16 of the 18 case-patients who were positive by pH1N1 PCR on day 8 and 14 (88%) were still positive by at least 1 PCR (4 by pH1N1 PCR and 10 by matrix PCR). However, no specimen was positive by cell culture. On day 11, a total of 12 (86%) of the 14 PCR–positive case-patients were still coughing.\n\n【17】Of the 73 symptomatic participants tested within 7 days after symptom onset, 15 of the 47 PCR–positive case-patients (in gray in Figure 1 ) were detected only by matrix PCR (2/15 cell culture positive) ( Table 1 ). Of specimens positive by pH1N1 PCR, 97% (31/32) were culture positive at diagnosis, 44% (8/18) on day 8, and none on day 11 of illness. Of those whose results were positive by matrix PCR, 13% (2/15) were culture positive at diagnosis, and none were positive on days 8 or 11 of illness. Because virus culture was much less frequently positive for specimens positive only by matrix PCR (2/39, 5%) than for specimens positive by pH1N1 PCR (39/54, 72%), the 19% virus culture positivity on day 8 among core participants (all pH1N1 PCR positive) overestimated the true proportion positive at day 8. Assuming that none of the 15 case-patients with matrix PCR–positive results and 19% of the 32 case-patients with pH1N1 PCR–positive results would shed live virus on day 8, we can estimate that 6 \\[(19% × 32 pH1N1 PCR positive) + (0 × 15 matrix PCR positive)\\] of the 47 (13%) case-patients would still be positive by virus culture on day 8. If, instead, we assume that all 73 symptomatic participants were infected by pandemic (H1N1) 2009 and that positive cell culture on day 8 would be found only in patients positive by pH1N1 PCR, then 8% (6/73) still would be shedding live virus 1 week after illness onset. The real cell culture positivity rate on day 8 for all pandemic (H1N1) 2009–infected patients thus probably ranges from 8% to 13%.\n\n【18】### Discussion\n\n【19】Human challenge studies with seasonal influenza have shown that virus shedding after day 7 is rare ( _2_ ), but clinical studies have shown that shedding may persist beyond that period in some populations, such as elderly persons, immunocompromised patients, and children ( _3_ – _5_ , _10_ – _12_ ). In a study among hospitalized persons infected with seasonal influenza A viruses, 54% remained positive by PCR beyond 7 days after symptom onset, and 29% were positive by cell culture ( _13_ ). In another study, elderly hospitalized patients infected by influenza A (H3N2) viruses had higher virus loads than did outpatients, and their PCR positivity rate 1 week after disease onset was still high (57%) ( _10_ ).\n\n【20】In this prospective study, the proportion of pandemic (H1N1) 2009–infected persons still shedding replicating virus on day 8 varied from 8% to 13%, with no difference between children and adults. None were still shedding infectious virus on day 11. With seasonal influenza, virus shedding may be longer in children because they have less preexisting immunity that would limit replication than in adults. However, children and adults <50 years of age appear equally susceptible to infection with pandemic (H1N1) 2009 virus, which would support our findings of comparable virus replication and shedding across age groups studied ( _1_ ).\n\n【21】Our study had some limitations. First, our small sample size and study design may have limited our ability to directly measure culture positivity on day 8. Only a small number of patients had a specimen collected on day 8 and even fewer on day 11. In retrospect, a better design would have been to collect specimens from all 73 symptomatic household members on day 8, irrespective of the initial pH1N1 PCR result. That design would have enabled a more direct estimate of the proportion of patients who were culture positive on day 8, rather than the indirect approach we used. However, our extreme scenario (which assumes that all 73 symptomatic contacts were infected) provides the minimal positivity rate on day 8, and testing of all 73 on day 8 could only have found a proportion equal to or greater than our 8% estimate.\n\n【22】Second, our sampling methods could have influenced positivity rates. Although collection of NP specimens with a flocked swab is one of the best methods for obtaining specimens to detect influenza, those specimens might have been improperly collected by the nurses. Suboptimal collection of swabs would have yielded false-negative PCR or cell culture results, which in turn would have underestimated the proportion of patients shedding virus on day 8.\n\n【23】Third, PCR testing with the matrix PCR was conducted retrospectively on frozen specimens, and only 5% of those were positive by virus culture. A greater proportion of virus culture specimens might have been positive if those specimens had been processed immediately instead of going through a freeze-thaw cycle ( _14_ ). Moreover, our study included only ambulatory patients ,whereas studies of seasonal influenza that include hospitalized or immunocompromised persons show prolonged shedding, contributing to the impression that our findings most likely underestimate the true proportion of case-patients still shedding virus on day 8. The strengths of our study include its prospective design in a family setting and its use of various methods, including 2 PCR assays and virus culture, to detect pandemic (H1N1) 2009.\n\n【24】Our results are consistent with other reports of virus shedding in pandemic (H1N1) 2009–infected patients. In Singapore, among 70 pandemic (H1N1) 2009–infected patients treated with oseltamivir and swabbed daily until virus clearance, 37% were PCR positive on day 7 of their illness and 9% on day 10 ( _15_ ). No virus culture was performed in that study, so we cannot estimate the proportion of patients shedding infectious virus at these time points. However, even with oseltamivir treatment, the positivity rate by pH1N1 PCR on day 7 was similar to our own (42%) on day 8, and we can thus infer that the cell culture positivity rates also would be similar. In China, among 421 patients with serial swabs tested by real-time PCR but not cell culture, the median time from onset of disease to negative test result by real-time PCR was 6 days (range 1–17 days), indicating that 50% of patients were shedding virus \\> 6 days ( _16_ ).\n\n【25】A study conducted by Witkop et al. during a pandemic (H1N1) 2009 outbreak at the US Air Force Academy showed that 29% (31/106) of afebrile patients and 19% (11/58) of patients who had been symptom-free for 24 hours still shed viable pandemic (H1N1) 2009 virus. In their study, 24% of 29 swabs collected on day 7 and 13% of the 16 swabs collected on day 8 of illness were culture positive, despite the large proportion of patients prescribed antiviral drugs ( _17_ ).\n\n【26】No definitive test is available for assessing the real contagiousness of a patient. The presence of replicating, and therefore infectious, influenza virus is an absolute prerequisite for contagiousness, but it does not necessarily imply it. Contagiousness depends on many factors, including viral load and presence of clinical characteristics contributing to spread of droplets (such as coughing, rhinorrea, or sneezing) and is affected by the number and proximity of contacts between a case-patient and a susceptible person. Nevertheless, our study raises concerns about current recommendations for self-isolation until only 24 hours after fever has subsided ( _6_ ). With pandemic (H1N1) 2009, fever generally persists 1–4 days and may be absent in 6%–11% of patients ( _1_ , _15_ ). In our study, of the 32 pH1N1 PCR–positive household members who had been symptomatic for <7 days, 78% had fever at any time since onset of their illness, but only 34% were still febrile on the day they tested positive. Nonetheless, 97% of specimens obtained from these patients were positive by cell culture. Our sample size was insufficient to directly compare PCR or culture positivity by fever status or other symptom or severity indicator at specimen collection or as a component of the overall illness.\n\n【27】Before policy implications can directly follow from these findings, the association of self-isolation with substantial social impact needs to be carefully weighed against the possible benefits of reducing community transmission. In the general population, a 1-week self-isolation period seems more likely to prevent transmission than does isolation until fever has resolved. However, given that 8%–13% of patients may still shed infectious virus on day 8, longer periods of self-isolation for persons expected to come into contact with vulnerable persons (e.g., pregnant women, newborns, or immunocompromised persons) also may be prudent.\n\n【28】Dr De Serres is a medical epidemiologist at the Institut National de Santé Publique du Québec and professor of epidemiology at Laval University, Quebec, Canada. He is a member of the Canadian Pandemic Vaccine Task Group and of the Quebec Immunization Committee. His research interests include the epidemiology of vaccine preventable diseases, vaccine effectiveness, and vaccine safety.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d9eba0de-1285-4ca5-a30c-06af08e346fb", "title": "Detecting Human-to-Human Transmission of Avian Influenza A (H5N1)", "text": "【0】Detecting Human-to-Human Transmission of Avian Influenza A (H5N1)\nHighly pathogenic avian influenza A (HPAI) subtype H5N1 is repeatedly crossing the species barrier to humans. Since December 2003, a total of 291 cases of HPAI (H5N1) have been reported in humans, resulting in 172 deaths (i.e _._ , 59% case-fatality ratio) in 12 countries, mostly in Southeast Asia ( _1_ ). Among these cases, 31 family clusters have been documented, ranging in size from 2 to 8 family members. How many of these clusters are due to a common avian source and how many are due to human-to-human transmission are important facts to determine. Should one of these HPAI (H5N1) strains gain the capacity for sustained human-to-human transmission, the resulting outbreak, if not contained, would spread worldwide through the global transportation network more rapidly than adequate supplies of vaccine matched to the new variant could be manufactured and distributed ( _2_ , _3_ ). We analyzed data from 2 of the largest of the familial clusters to ascertain if human-to-human transmission took place, and if so, how transmissible the strain was.\n\n【1】### Methods\n\n【2】##### May 2006 Human Avian Influenza Family Cluster, Indonesia\n\n【3】During late April and early May 2006, a cluster of 8 cases of HPAI (H5N1) was detected and investigated by the Indonesian public health surveillance system in northern Sumatra ( _4_ – _6_ ). All case-patients were members of the same extended family. Seven of them resided within 3 adjacent houses in the village of Kubu Sembilang. The remaining patient resided with his immediate family in the village of Kabanjahe (≈10 km away).\n\n【4】The index patient was a 37-year-old woman, thought to have been exposed to dead poultry and chicken fecal material before onset of illness. She also reportedly maintained a market stall that sold live chickens. Although her illness was not confirmed to have been caused by the (H5N1) avian influenza virus, her death on May 5, 2006, is suspected to be the result of HPAI (H5N1) infection because of her reported symptoms, illness progression, and prior contact with diseased or dead poultry.\n\n【5】Appendix Figure 1\n\n【6】Appendix Figure 1 . Exposure and disease events for each member of the family cluster in northern Sumatra, Indonesia. Dark boxes, duration of illness; white boxes without text, recovery period; thick dark vertical line, death;...\n\n【7】Twenty members of her extended family are suspected to have been in contact with her, many during a family gathering on April 29, 2006 ( _7_ ). At that time, she was manifesting symptoms (i.e., she had a heavy cough, was severely ill, and was prostrate). That night, 9 of these members slept in the same small room as she did (indicated by a black triangle in Appendix Figure 1 ). Of these 9 family members, 2 of her sons (15 and 17 years of age) and her 25-year-old brother, who lived in Kabanjhe, became ill in the next 3 weeks. The sons died. The brother was the only person from this family cluster to recover.\n\n【8】Of the remaining 11 family members, 4 became ill and died. The 29-year-old sister of the index patient, who lived in an adjacent house, became ill after she provided direct personal care to her ill sister ( _7_ ). The 18-month-old daughter of this sister also became ill after she was in the presence of the index patient with her mother. The 10-year-old nephew of the index patient, who lived in the other house adjacent to hers, became ill after he attended the family gathering and frequently visited his aunt’s house. The nephew’s father became ill after he personally cared for his son. The possibility that HPAI (H5N1) was transmitted from the nephew to his father is also supported by genetic sequencing data ( _4_ ). Though symptoms did not develop in the mother of the nephew, she was directly exposed to her husband during his illness. All case-patients, except for the index patient, were confirmed as influenza (H5N1) positive by PCR. The nephew’s mother was confirmed as influenza (H5N1) negative. As an intervention, 54 surviving relatives and close contacts were identified and placed under voluntary quarantine ( _7_ ). All of these persons, except for pregnant women and infants, received oseltamivir prophylactically.\n\n【9】##### December 2005 Human Avian Influenza Family Cluster, Eastern Turkey\n\n【10】Appendix Figure 2\n\n【11】Appendix Figure 2 . Exposure and disease events for each member of the family cluster in Eastern Turkey. Dark boxes, period of illness; white boxes without text, recovery period; thick dark vertical line, death; \\*Exposed...\n\n【12】From December 18, 2005, ( _8_ ) to January 15, 2006 ( _9_ ), a cluster of 8 confirmed (H5N1) influenza cases was detected in Dogubayazit District in eastern Turkey ( Appendix Figure 2 ) ( _10_ – _13_ ). These case-patients were among 21 members of 3 households located within 1.5 km of each other ( _14_ ). All confirmed case-patients were hospitalized after onset of symptoms ( _9_ ). Four of the confirmed case-patients died; the other 4 recovered ( _9_ ). Ten of the remaining 14 household residents were hospitalized with avian influenza-like symptoms but were never confirmed to be (H5N1) infected ( _9_ ). All but one of the hospitalized residents were children (6–15 years of age) ( _9_ ).\n\n【13】Before onset of symptoms, 4 children from 1 household, 3 of whom had confirmed cases (including the index patient), were reported to have had close contact with the dead bodies of sick chickens ( _15_ ). The 2 confirmed case-patients in the second household reportedly slaughtered a duck together on January 1, 2006, at the beginning of a die-off in the household’s flock ( _14_ ). Two of the remaining confirmed case-patients lived in the third household and had no history of contact with sick or dying poultry. The remaining confirmed case occurred in a fourth residence located near the first household ( _10_ ), but because we lacked information on the number of household members and the case-patient’s exposure history, we excluded it from these analyses. Most, if not all, of the 21 residents attended a dinner hosted by the family of the index patient on December 24, 2006, while he was symptomatic ( _8_ ).\n\n【14】##### Statistical Methods\n\n【15】We used a previously developed statistical transmission model ( _16_ , _17_ ) to test whether human-to-human transmission occurred, and if it did, to estimate transmission parameters. In the model, persons mix with one another in households and between households. In addition, we include a common source of infection due to zoonotic exposure. Mathematical and statistical details are given in the Technical Appendix .\n\n【16】##### Model of Probability of Transmission\n\n【17】Figure\n\n【18】Figure . Schematic of estimation method. An infectious person (in red) infects a susceptible person (in green) in the same household with probability of household secondary attack rate (SAR <sub>1 </sub> ) and infects a susceptible...\n\n【19】We define _p_ <sub>1 </sub> as the probability that an infectious household member infects another household member in 1 day. If the distribution of the infectious period is known, we can obtain the household secondary attack rate (SAR <sub>1 </sub> ) from _p_ <sub>1 </sub> , defined as the probability that an infectious household member infects another household member over his or her infectious period. Similarly, we define the daily transmission probability ( _p_ <sub>2 </sub> ) and the community SAR (SAR <sub>2 </sub> ) for between household spread. Finally, we define the daily probability ( _b_ ) that any person is infected from a zoonotic source. The contact structure used for parameter estimation is shown in the Figure . We assume that the distributions of the incubation and infectious periods are predetermined by the investigator.\n\n【20】We establish the likelihood function for each person and then for the whole population for statistical inference. The likelihood function for a person is equivalent to the probability of observing the realized data on that person throughout the outbreak. The likelihood function for a person labeled _i_ is built with the following steps: 1) Obtain the probability that person _i_ is infected by an infectious source labeled _j_ on day _t_ , given person _i_ is not infected up to day _t_ – 1. If source _j_ is a person, this probability is _p_ <sub>1 </sub> , for the same household, or _p_ <sub>2 </sub> for exposure in the community, multiplied by the probability of person _j_ being infectious on day _t_ . The probability of person _j_ being infectious on day _t_ is derived from the symptom-onset day of person _j_ and the distribution of the infectious period. If source _j_ is zoonotic, the infection probability is _b_ . The probability of escaping infection is simply 1 minus the corresponding probability of infection. 2) Take the product of the probabilities obtained in step 1 over all humans and zoonotic sources _j_ to obtain the probability of person _i_ escaping infection by any infectious source on day _t_ . 3) Take the product of the probabilities obtained in step 2 over all days before and including day _t_ to obtain the probability of person _i_ escaping infection up to day _t_ . 4) If person _i_ is not infected by the end of the outbreak, the likelihood function for person _i_ is the product of the probabilities of person _i_ escaping infection up to the last day of observation. 5) If person _i_ is observed to have symptom onset on day and the infection time is known to be _t_ , the probability of the data regarding person _i_ is the product of 3 pieces of information: a) the probability of person _i_ escaping infection up to day _t –_ 1, b) the probability that person _i_ is infected on day _t,_ and c) the probability that the duration of the incubation period is – _t_ . Because we do not observe the infection time, the likelihood function for person _i_ is obtained by summing the above product, a – c, over all potential values of _t_ .\n\n【21】The likelihood function for the whole population is the product of all the individual likelihood functions. In the event that human-to-human transmission occurs, SAR estimates are used to estimate the local basic reproductive number ( _R <sub>0 </sub>_ ), which is defined as the average number of secondary cases infected by a typical index case-patient in the beginning of the outbreak ( Technical Appendix ). There is potential for sustained transmission if _R <sub>0 </sub>_ is >1. If human-to-human transmission is determined to be occurring, then the above parameters are estimated from the symptom dates and contact information from the population under study. Data on exposed persons who do not become ill form an important component of the inference procedure.\n\n【22】##### Statistical Test\n\n【23】We set up a statistical test with the null hypothesis being that no human-to-human transmission occurs, that is, _p_ <sub>1 </sub> \\= _p_ <sub>2 </sub> \\= 0. The alternative hypothesis is either _p_ <sub>1 </sub> or _p_ <sub>2 </sub> is not equal to 0, or both are not equal to zero. The test statistic we use is proportional to the ratio of the maximum value of the likelihood function assuming the null hypothesis is true (null likelihood) and the maximum value of the likelihood function at the estimated parameter values (full likelihood).\n\n【24】Specifically, we define the likelihood ratio test statistic as –2 log (the null likelihood function divided by the full likelihood function). If no human-to-human transmission occurs, the 2 likelihood functions would be roughly equal, and we expect to see a likelihood ratio close to 1, and, thus, a likelihood ratio statistic close to 0. A large value of the likelihood ratio statistic is evidence of deviation from the null hypothesis. The question is how to obtain a reference set of the likelihood ratio statistic values that we would see under the null hypothesis. Given no human-to-human transmission, all the observed case-patients must have been infected by the zoonotic source. Since the exposure to the zoonotic source is assumed constant for each person on each day, the null likelihood function will not change if we reassign the infection and symptom status of the observed case-patients to a different group of people in the population. By performing such reassignment many times, we obtained a collection of datasets that were each equally likely to have been observed had there been no human-to-human transmission. The values of the likelihood ratio statistic calculated from these datasets form the null distribution for statistical testing. This method is referred to as a permutation test. The p value is given by the proportion of the reference values that are equal to or larger than the observed likelihood ratio statistic value. More technical details are given in the online appendix.\n\n【25】The probability of infection by the zoonotic source may not be estimable together with SAR <sub>1 </sub> or SAR <sub>2 </sub> from an observed cluster. In such a situation, a statistical test of the occurrence of human-to-human transmission is still meaningful because the likelihood ratio test statistic is still estimable from the permuted datasets.\n\n【26】##### Data Required\n\n【27】A list of the inputs that are required for estimation and statistical testing are listed in the Table . Three categories of input parameters are required for this estimation model: outbreak-wide, individual level, and analysis parameters. The duration of the outbreak, the duration of the incubation period for the pathogen, and the minimum and maximum durations of the infectious period for the pathogen are the required outbreak-wide inputs. For each person, their residential location (neighborhood and household), their demographic characteristics (sex and age), and whether they were a case-patient or not are required input parameters. Case-patients require additional input of their illness-onset dates, types of outcome, outcome dates, and whether or not they are the index patient in the outbreak. Hospitalization and treatment dates (considered prophylactic for nonpatients) are optional input parameters for each person. For each person who visits another residence during the outbreak period, his or her identifiers, the neighborhood and household visited, and the start and end dates of the visit are required inputs. Analysis-related inputs include the last date of community exposure to potential common sources of infection, the last date of observation, and inputs for _R <sub>0 </sub>_ estimation (mean number of residents per household and mean number of out-of-residence contacts per person per day). An expanded version of the model will require the input of other exposure information such as from schools or hospitals.\n\n【28】### Results\n\n【29】For the outbreak in Indonesia, Figure shows that the incubation period had a probable range of 3–7 days and the infectious period, a probable range of 5–13 days. Thus, we let the incubation period have a uniform distribution of 3–7 days (mean 5 days) and the infectious period a uniform distribution of 5–13 days (mean 9 days). For the data shown in Figure , only the household SAR (SAR <sub>1 </sub> ) can be estimated. We determined that human-to-human spread did occur by rejecting the null hypothesis of no human-to-human transmission (p = 0.009). The estimated household SAR is 0.29 (95% confidence interval \\[CI\\] 0.15–0.51). Thus, a single infected person in a household infected another household member with the probability of 0.29. The average household size for rural Indonesia is ≈5 people. Because we do not have an estimate of the community SAR, we have an estimate of the lower limit of the local _R <sub>0 </sub>_ , _<sub>, </sub>_ i.e., 1.14 with a 95% CI of 0.61–2.14. A sensitivity analysis on the distribution of the incubation and infectious period shows that the test and estimates for SAR <sub>1 </sub> and _R <sub>0 </sub>_ are insensitive to uncertainty about these distributions within plausible ranges.\n\n【30】For the outbreak in Turkey, all the parameters are estimable, but we do not reject the null hypothesis of no human-to-human transmission (p = 0.114). Our estimate of the daily probability of infection from the common source is 0.011 (95% CI 0.005–0.025).\n\n【31】### Discussion\n\n【32】We have presented statistical evidence that the strain of HPAI (H5N1) that caused the family cluster of human cases in northern Sumatra was spread from human to human and that the household SAR was 29%. This household SAR is similar to statistical estimates for interpandemic influenza A in the United States (12.7%–30.6%) ( _18_ , _19_ ). The mean incubation period of this strain appears to have been ≈5 days, nearly twice as long as for past pandemic strains and current interpandemic strains of influenza. The CI for the estimated lower bound for the local _R <sub>0 </sub>_ covers 1. Therefore, even though we determined that human-to-human transmission probably occurred, whether the virus was capable of sustained human-to-human transmission is not clear. This virus may have required very close human contact to be transmitted. Even with no intervention, the finding that _R <sub>0 </sub>_ \\= 1.14 indicates that the chance that a single introduction would result in any further spread is ≈12%. In addition, the reported prophylactic use of oseltamivir may have played some role in limiting further spread. We did not find statistical evidence of human-to-human spread for the outbreak in eastern Turkey. This does not mean that no low-level human-to-human spread occurred in this outbreak, only that we lack statistical evidence of such spread. The power would be too low to detect such spread for an outbreak with 7 total cases and small SARs ( _17_ ).\n\n【33】We did not consider the role of heterogeneity—such as age, sex, treatment status, or quarantine—in transmission. The parameters could be made to be functions of time-dependent covariates, as we have done with similar models ( _16_ , _19_ , _20_ ). We can easily extend the model used here for covariates; however, we must have sufficient data to support such models.\n\n【34】Computer simulations have shown that the targeted use of influenza antiviral agents could be effective in containing a potential pandemic strain of influenza at the source ( _21_ , _22_ ), if initiated within 3 weeks of the initial case in the community, and if the _R <sub>0 </sub>_ is <1.8. This strategy, known as targeted antiviral prophylaxis, involves treating identified index patients in a mixing group and offering a single course of prophylaxis to the contacts of these index patients in predefined close contact groups, i.e., households at a minimum but also possibly neighborhood clusters, preschool groups, schools, and workplaces. In addition, the voluntary household quarantine of suspected close contacts of case-patients was recommended. Targeted antiviral prophylaxis at the household and neighborhood cluster level was carried out for the outbreak in Sumatra.\n\n【35】Ascertaining whether a potential pandemic strain of influenza is capable of sustained human-to-human transmission and estimating key transmission parameters are important. To estimate more than the household SAR, more detailed community data need to be collected. This would include a complete census of potentially exposed households and persons in the area where immediate transmission could occur from both potential zoonotic and human sources. Such data would enable estimation of important parameters and a more complete estimate of the _R <sub>0 </sub>_ rather than just the lower limit.\n\n【36】We have developed a software application, TRANSTAT, for implementing these analyses. This application provides a stand-alone environment for the entry, storage, and analysis of data from outbreaks of acute infectious diseases. A partial list of the input information is given in the Table. The statistical methods presented here can be applied to the data along with several standard epidemiologic tools. This information system would allow for real-time analysis and evaluation of control measures for an outbreak. We would encourage outbreak investigators to use this tool, taking care to input data on the exposed nonpatients as well as case-patients. The authors will provide a link to this software upon request.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7cc97210-afc5-4322-a76a-27689ee12fa6", "title": "High Pathogenicity of Nipah Virus from Pteropus lylei Fruit Bats, Cambodia", "text": "【0】High Pathogenicity of Nipah Virus from Pteropus lylei Fruit Bats, Cambodia\nNipah virus (NiV) is a zoonotic paramyxovirus that was first identified as the cause of an outbreak of encephalitis in humans in Malaysia and Singapore during 1998–1999 ( _1_ ). Although NiV infection remains rare in humans, this virus has captured the attention of the public health community and scientists because of its high case-fatality rate, ranging from 40% in Malaysia to >90% in Bangladesh and India ( _2_ , _3_ ). Its high pathogenicity, potential for interspecies transmission, and lack of validated medical countermeasures led to the classification of NiV as a Biosafety Level 4 (BSL-4) pathogen. In 2015, the World Health Organization (WHO) listed NiV as a priority pathogen because of its probability of causing severe outbreaks and subsequently placed NiV on the WHO Blueprint list of priority diseases ( _4_ ). This designation was strengthened by the NiV outbreak in Kerala, India, where the virus had not previously been reported ( _3_ ).\n\n【1】NiV is a member of the _Henipavirus_ genus, together with Hendra virus, which first emerged in Brisbane, Queensland, Australia, in 1994 ( _5_ ), and the nonpathogenic Cedar virus, which was discovered in Australia in 2009 ( _6_ ). In addition, full-length henipa-like viral sequences were found in fruit bats in Africa ( _7_ ) and in rats in China (Moijang virus) ( _8_ ). As part of the Mononegavirales, NiV has a nonsegmented, negative-sense, single-stranded RNA genome. _Pteropus_ fruit bats, commonly known as flying foxes, are considered the henipavirus natural reservoir; these bats, when infected with NiV, do not seem to display any apparent clinical signs of disease ( _9_ ).\n\n【2】Only 2 NiV lineages are known to circulate in Asia and cause disease in humans: NiV-Malaysia and NiV-Bangladesh. In India and Bangladesh, NiV transmission from bats to humans was shown to occur through the consumption of raw date palm juice or fruits contaminated with the saliva or urine from fruit bats ( _10_ ). In addition, as observed during the outbreaks in Malaysia in 1998 and 1999, transmission can occur via contact with infected domestic animals, such as pigs, that act as amplifying hosts of the virus ( _11_ ). Interhuman transmission has been reported in Bangladesh and India in >50% of NiV outbreaks ( _12_ ).\n\n【3】Clinical manifestations of NiV infection in humans can range from asymptomatic to acute respiratory syndrome, generalized vasculitis, and fatal encephalitis. Among the few survivors of NiV outbreaks, long-term neurologic problems have been reported; 20% of patients have residual neurologic sequelae ( _13_ ), and NiV-Malaysia–infected patients experienced relapse and late-onset encephalitis ( _14_ ).\n\n【4】NiV and henipa-like viruses have been detected molecularly or serologically in _Pteropus_ bats in different countries of Asia ( _15_ ) and Africa ( _7_ ), Australia ( _16_ ), and Brazil ( _17_ ), and the worldwide distribution of fruit bats poses a continuous threat to another spillover with possible pandemic potential ( _18_ ). However, since 1998, all NiV cases in humans have been identified in Malaysia, India, Bangladesh, and the Philippines ( _19_ ). Human cases of NiV have not been reported in Cambodia or neighboring countries since the first serologic detection of NiV in Cambodia and isolation of CSUR381 in _Pteropus lylei_ bats in Cambodia in 2003 ( _20_ , _21_ ). Initial phylogenetic analyses of the nucleoprotein and attachment glycoprotein of CSUR381 suggested the virus was part of the NiV-Malaysia genotype ( _21_ ). However, a full-genome characterization and phylogenetic analysis have not been performed. In addition, the growth dynamics and virulence of this virus have not been analyzed, thus limiting more comprehensive evaluation of this virus’s pathogenic potential. In this study, we performed an in-depth characterization of CSUR381, including its pathogenicity both in vitro and in vivo, ultimately to assess the outbreak risk that isolates circulating in Cambodia pose in Southeast Asia.\n\n【5】### Materials and Methods\n\n【6】##### Viruses\n\n【7】In this study, we used 3 different NiV isolates: the NiV isolate CSUR381 from Cambodia (GenBank accession no. MK801755), NiV-Malaysia isolate UMMC1 (GenBank accession no. AY029767), and NiV-Bangladesh isolate SPB200401066 (GenBank accession no. AY988601). CSUR381 was isolated from _P. lylei_ bat urine at the Pasteur Institut in Battambang, Cambodia, in 2003 ( _21_ ), and the other 2 isolates were obtained from infected patients. We produced and titrated all viruses on Vero E6 cells.\n\n【8】##### Full-Genome Sequencing\n\n【9】We amplified and titrated the Cambodia NiV isolate on Vero cells and, after the second cell passage, extracted viral RNA from supernatant using the QIAamp Viral RNA Mini Kit (QIAGEN, https://www.qiagen.com ) according to the manufacturer’s instructions. We treated samples with DNase, purified and quantified RNA using the QuantiFluor RNA System (Promega, https://www.promega.com ), and analyzed using the AATI High Sensitivity Genomic DNA Analysis Fragment Analyzer (Advanced Analytical Technologies Inc., https://www.agilent.com ). Then, we amplified viral RNA using the Single Primer Isothermal Amplification Kit (NuGEN, https://www.nugen.com ). We prepared a library using Ovation Ultra Low (NuGen), which gave us average DNA fragment sizes of 382–426 bp. We then sequenced the whole genome of CSUR381 using MiSeq Nano v2 (Illumina, https://www.illumina.com ), which produced read lengths of 2 × 150 nt.\n\n【10】We carried out genome assembly de novo using SPAdes ( http://cab.spbu.ru/software/spades ) and predicted open reading frames with Prodigal ( https://omictools.com/prodigal-tool ). We reconstructed the missing 5′ and 3′ extremities using the 5′ RACE System for Rapid Amplification of cDNA Ends (Invitrogen, https://www.thermofisher.com ) according to the manufacturer’s recommendations. We performed reverse transcription with SuperScript II Reverse Transcriptase (Invitrogen) using the primer GSP1-leader (5′-GACCATTGATCCAACATC-3′) to recover the viral leader sequence and GSP-trailer (5′-AAAGTGATTGTCTACTCACT-3′) to recover the trailer sequence. After column purification, we tailed cDNA sequences with cytidine triphosphate and terminal deoxynucleotidyl transferase. Last, we amplified dC-tailed cDNA using the Abridged Anchor Primer provided in the 5′ RACE System for Rapid Amplification of cDNA Ends Kit and primers nested-GSP2-leader (5′-TACAGCTTCAATGTCTGGGTCATT-3′) to amplify the viral leader sequence, and nested-GSP2-trailer (5′-CAAGTTCAAGGACACCAAAAGT-3′) to amplify the viral trailer sequence. We sequenced PCR products using Sanger technology and submitted the complete genome sequence of CSUR381 to GenBank (accession no. MK801755).\n\n【11】##### Phylogenetic Analyses\n\n【12】Using the ClustalW algorithm ( http://www.clustal.org ), we performed multiple alignments for complete genomes and individual gene sequences. We implemented and manually checked the quality of alignments using BioEdit version 7.2.6 ( _22_ ) and conducted genomic characterization and evolutionary analyses in MEGA version 7.0.26 ( _23_ ). After determining the best DNA model to use for each alignment, we constructed maximum-likelihood phylogenetic trees for complete NiV genomes and all virus coding sequences. For statistical support, we used 500 bootstrap replicates for the analysis of the complete genome and 1,000 replicates for analyses of each gene.\n\n【13】##### Cell Lines and Infection\n\n【14】We cultured NCI-H358 (human bronchioalveolar carcinoma) and Vero E6 (African green monkey kidney) cells in Dulbecco’s modified Eagle medium (DMEM) with GlutaMAX (Thermo Fisher Scientific, https://www.thermofisher.com ) supplemented with 1% penicillin-streptomycin (10,000 U/mL), 1% L-glutamine, and 10% heat-inactivated (56°C for 30 minutes) fetal calf serum (FCS). We cultured human pulmonary microvascular endothelial cells (HPMECs) ( _24_ ) in endothelial cell growth medium (Growth Medium MV 2 Kit; PromoCell, https://www.promocell.com ). We incubated all these cell lines at 37°C with 5% carbon dioxide; all cell lines, including the _Pteropus_ cell line described in the next paragraph, tested negative for _Mycoplasma_ spp. by the MycoAlert kit (Lonza, https://www.lonza.com ).\n\n【15】We generated a _Pteropus_ flying fox cell line using a skin biopsy from the wing membrane of a female _P. giganteus_ (also known as _P. medius_ and flying fox) bat ( _25_ ) of the order Yinpterochiroptera. Biopsies were collected from bats by Tiergarten Schönbrunn (Vienna, Austria) staff during regular veterinary checkups following appropriate guidelines to minimize animal stress. The biopsies were washed with sterile phosphate-buffered saline and transferred into Freezing Medium Cryo-SFM (PromoCell), and sample vials were put on dry ice for shipment to Centre International de Recherche en Infectiologie in Lyon, France. To obtain primary cell cultures, we fractionated the biopsies into petri dishes, harvested the homogenates, and incubated them at 37°C with 5% carbon dioxide in DMEM/F-12 (Gibco, https://www.thermofisher.com ) supplemented with 10% fetal calf serum FCS, 1% L-glutamine (200 mM), 1,000 U/mL of penicillin, 1,000 U/mL of streptomycin, and 2.50 µg/mL amphotericin B (Gibco). We subsequently immortalized primary cells using the lentiviral vector SV40 large T-antigen produced at Genetic Analysis and Vectorology Platform (AniRA, École Normale Supérieure de Lyon, Lyon). We evaluated different clones on the basis of their morphologic stability and transfectability using jetPRIME kit (Polyplus, https://www.polyplus-transfection.com ). We confirmed immortalization of clones by detecting large T-antigen inserts by reverse transcription PCR (RT-PCR). We cultured the final _Pteropus_ cell line, which we designated PATGV1.12, in DMEM GlutaMAX supplemented with 10% heat-inactivated FCS. We additionally confirmed that this cell line was derived from _P. giganteus_ bats by sequencing the mitochondrial region D-loop ( _26_ ) and nuclear introns ACOX2, COPS7A, BGN, ROGD1, and STAT5A, which has been suggested to be pertinent for distinguishing among closely related bat species ( _27_ ).\n\n【16】We infected cells in 12-well plates at 80% confluence with a multiplicity of infection (MOI) of 0.3. For virus replication kinetics studies, we took 4 time points postinfection into consideration: 0 h, 24 h, 48 h, and 72 h. We performed infections in BLS-4 facility Jean Mérieux (Lyon). For each time point, we collected cell lysates according to validated BSL-4 procedures. We collected supernatants and kept them at −80°C until titration by plaque assay on Vero E6 cells.\n\n【17】##### Pseudotyping of Vesicular Stomatitis Virus and Evaluation of Cell Permissiveness\n\n【18】We used rVSVΔG-RFP (a recombinant vesicular stomatitis virus \\[VSV\\] in which the envelope glycoprotein G gene is replaced with the red fluorescent protein gene) ( _28_ , _29_ ) to generate pseudotyped VSVs harboring different combinations of NiV envelope glycoprotein G (attachment protein) and F (fusion protein) on their surfaces. Complementing rVSVΔG-RFP–infected cells with NiV glycoproteins expressed in trans, we were able to produce stocks of pseudotyped VSVs identical in their genetic background and differing only in the nature of their surface glycoproteins. Because the infectivity of rVSVΔG-RFP pseudotypes is restricted to a single round of replication, this tool is largely used for studying viral entry for a broad range of highly pathogenic viruses ( _30_ ).\n\n【19】To create the pseudotypes, we cloned the NiV glycoprotein G and F genes from RNA isolated from CSUR381, UMMC1, and SPB200401066 into 6 separate pCAGGS plasmid vectors. We transfected these 3 plasmid pairs separately into BSR-T7 cells using TransIT-LT1 Transfection Reagent (Mirus Bio, https://www.mirusbio.com ). We infected cells with rVSVΔG-RFP 16 h after transfection to produce a pseudotyped VSV for each NiV isolate. We collected supernatants at 24 h postinfection and concentrated pseudotyped VSVs by ultracentrifugation (28,000 rpm for 2 h at 4°C). We titrated these viruses on Vero cells. To evaluate viral entry into different cell lines, we performed infections in 24-well plates using 80% confluent, adherent cells and a 1-h contact between virus and cells. We determined the percentage of cells infected 6 h postinfection by quantifying cells expressing RFP via flow cytometry on a BD LSRFortessa ( https://www.bd.com ).\n\n【20】##### RNA Extraction and Real-Time RT-PCR\n\n【21】At the indicated time points, we collected cells and extracted RNA using the NucleoSpin RNA Kit (Macherey-Nagel, https://www.mn-net.com ) according to the manufacturer’s instructions. We assessed the yield and purity of extracted RNA using the DS-11-FX spectrophotometer (DeNovix, https://www.denovix.com ). We reverse transcribed extracted RNA using the iScript Select cDNA Synthesis Kit (Bio-Rad, https://www.bio-rad.com ) and performed real-time PCR using Platinum SYBR Green qPCR SuperMix-UDG (Invitrogen) on a StepOnePlus Real-Time PCR System (Applied Biosystems, https://www.thermofisher.com ). As previously described ( _31_ ), we amplified the NiV nucleoprotein gene and the _Pteropus_ glyceraldehyde 3-phosphate dehydrogenase housekeeping gene using forward primer 5′-ATCATCCCTGCTTCTACT-3′ and reverse primer 3′AGGTCAGATCCACAACT-5′. We analyzed quantitative RT-PCR results using StepOne version 2.3 (Applied Biosystems).\n\n【22】##### Experimental Infection of Hamsters\n\n【23】We obtained 2-month-old male golden hamsters ( _Mesocricetus auratus_ ) from Janvier Labs ( https://www.janvier-labs.com ). We housed hamsters in a BSL-4 containment facility (INSERM P4, Jean Mérieux, Lyon) and handled them according to the regulations for animal maintenance of France. We treated hamsters with isoflurane anesthesia before manipulations. We subcutaneously infected 2 groups of 6 hamsters with a high dose (13,500 PFU/animal) of either the NiV-Malaysia or Cambodia NiV isolate and followed hamsters daily to record their body temperature and weight. The regional ethics committee for animal experimentation (Lyon) approved these animal experiments.\n\n【24】### Results\n\n【25】##### Full-Genome Characterization and Phylogenetic Analyses\n\n【26】Analysis of the assembled viral sequence of CSUR381 showed a total genome length of 18,246 nt, similar to the lengths of NiV isolates reported in Malaysia. The nucleotide composition was 27.8% T or U, 18.4% C, 33.6% A, and 20.2% G; total GC content was 38.6%. To investigate genetic relationships between CSUR381 and other henipaviruses, we constructed distance matrices for the complete genome and for each gene using the p-distance method. When we compared the sequence of CSUR381 with those of other NiVs available in GenBank, the most similar sequences (with 97.7% nucleotide identity) were from NiV-Malaysia human isolates (GenBank accession nos. NC002728.1 and AY029768.1; Table 1 ). We also calculated nucleotide identity and amino acid homology for each of the 6 structural genes ( Table 2 ). Genetic pairwise comparisons with other NiV isolates showed the lowest nucleotide identity and amino acid homology for phosphoprotein (087.1/82.7%) and highest for matrix protein (98.9/99.4%).\n\n【27】Figure 1\n\n【28】Figure 1 . Maximum-likelihood phylogenetic analysis of NiV CSUR381, Cambodia, 2003 (red triangle), compared with other henipaviruses and NiVs. A) Phylogenetic tree constructed with complete genome sequences. A general time-reversible model was calculated as...\n\n【29】Using the maximum-likelihood method, we constructed phylogenetic trees on the basis of the complete genome ( Figure 1 , panel A) and the nucleocapsid gene ( Figure 1 , panel B). The general time-reversible model for the complete genome and the Kimura 2-parameter model for the nucleocapsid gene were predicted to be the best for performing those particular phylogenetic analyses. CSUR381 clustered with the monophyletic group of the NiV-Malaysia genotype for both the whole genome and nucleocapsid gene; bootstrap support was >98% in all cases, confirming the previous partial genomic characterization of CSUR381 ( _24_ ). We then generated phylogenetic trees for each of the coding sequences of the 6 NiV structural proteins, which gave equivalent results ( Appendix Figure 1).\n\n【30】Figure 2\n\n【31】Figure 2 . Multiple alignment of the phosphoprotein gene of Nipah virus CSUR381, Cambodia, 2003, and other henipavirus isolates. The highly conserved editing site (5′-AAAAAGGG-3′, red outline) is present in all Nipah and Hendra...\n\n【32】Multiple alignment of the henipavirus phosphoprotein gene ( Figure 2 ) revealed high conservation of the editing site (5′-AAAAAGGG-3′) in CSUR381, similar to other NiV and Hendra virus isolates and different from Cedar virus, a nonpathogenic virus isolated from a _P. alecto_ bat in Australia ( _6_ ). This finding suggests that CSUR381 might produce the nonstructural proteins V and W, capable of interacting with the host innate cellular immune response ( _32_ ). Comparisons of the deduced V, W, and C amino acid homologies between CSUR381 and other known NiVs showed a variation of 88%–100% ( Appendix Table).\n\n【33】##### Evaluation of Virus Entry\n\n【34】Figure 3\n\n【35】Figure 3 . Evaluation of entry of VSVΔG-RFPs (vesicular stomatitis virus in which the envelope glycoprotein G gene is replaced with the red fluorescent protein gene) pseudotyped with the surface glycoproteins of NiVs CSUR381...\n\n【36】We next determined the cellular permissiveness of a human endothelial cell line (HPMEC), a human respiratory epithelial cell line (NCI-H358), the newly generated _Pteropus_ bat cell line (PATGV1.12), and Vero cells to CSUR381 compared with the NiV-Malaysia (UMMC1) and NiV-Bangladesh (SPB200401066) isolate using pseudotyped rVSVΔG-RFP viruses. Cell lines were infected for 1 h at an MOI of 0.3. The percentages of cells infected were analyzed by flow cytometry 6 h after infection ( Figure 3 ), and results from HPMEC, NCI-H358, and PATGV1.12 were normalized to the findings from Vero. All tested cell lines were permissive to infection with all 3 viruses tested. Entry of NiV pseudotypes into the bat cell line PATGV1.12 and human respiratory epithelial cell line was similar. Compared with the NiV-Malaysia and NiV-Bangladesh pseudotypes, the CSUR381 pseudotyped virus showed higher but not significantly increased entry into the 3 tested cell lines (1-way analysis of variance).\n\n【37】We further analyzed the amino acid sequences of the F and G proteins of the 3 viruses by multiple alignment. The glycosylation site (N529/Q530/T531) ( _33_ ) and ephrin-B2 and ephrin-B3 binding sites ( _34_ ) in the G attachment protein were preserved ( Appendix Figure 2). In addition, multiple alignments showed that the F cleavage site was preserved among all analyzed NiV isolates ( Appendix Figure 3). Last, an analysis of the predicted N-terminal and C-terminal heptad-repeat regions within the F protein, which are needed for NiV fusion ( _35_ ), showed high conservation, and compared with NiV-Malaysia and NiV-Bangladesh, only 1 aa difference (V159→I) was detected in CSUR381 ( Appendix Figure 3). Altogether, the high conservation of the NiV glycoproteins and results from pseudotype virus studies suggest that CSUR381 can enter target cells at least as well as NiV-Malaysia and NiV-Bangladesh.\n\n【38】##### Replication of NiV Isolates in Different Cell Types\n\n【39】Figure 4\n\n【40】Figure 4 . Replication of NiVs CSUR381 (Cambodia 2003 isolate), UMMC1 (NiV-Malaysia isolate), and SPB200401066 (NiV-Bangladesh isolate) in Vero, HPMEC, NCI-H358 (human bronchioalveolar cells), and PATGV1.12 (bat cells). A) Real-time reverse transcription PCR titer....\n\n【41】To further evaluate the virulence of CSUR381, we compared the replication kinetics of this virus with those of the NiV-Malaysia and NiV-Bangladesh isolates. We infected cell types known to be primary targets of NiV in humans, pulmonary endothelial (HPMEC) and bronchioalveolar epithelial (NCI-H358) cells, and the bat cell line PATGV1.12 at an MOI of 0.3 ( Figure 4 ). NiV RNA synthesis was highest in HPMEC, where NiV-Bangladesh replicated the best, although a similar level of RNA and infectious virus particle production was observed for all 3 viruses ( Figure 4 , panel A). In accordance with virus entry studies ( Figure 3 ), virus replication was also observed in PATGV1.12 ( Figure 4 , panels A and B). Differences among the 3 tested NiV isolates were observed only in NCI-H358, where NiV-Malaysia RNA synthesis was significantly increased (p<0.001 by 2-way analysis of variance) compared with NiV-Bangladesh, provoking remarkable cytopathic effects ( Figure 4 , panel C). The formation of giant multinucleated cells, a hallmark of NiV infection, were already visible at 24 hours postinfection in all cell types and further developed during the course of the infection ( Figure 4 , panels C–E). Vero cells showed the most visible cytopathic effects, probably because of their interferon incompetence ( _36_ ).\n\n【42】##### Experimental Infection of Hamsters\n\n【43】We compared the pathogenicities of CSUR381 and the NiV-Malaysia isolate using the golden hamster animal model ( _37_ ). We infected 6 hamsters with either CSUR381 or the NiV-Malaysia isolate and followed them for clinical signs of infection. At 6 days postinfection, the first neurologic signs (which included paralysis and trembling limbs) were observed in both groups; their presentation rapidly evolved toward breathing difficulties and prostration. Weight reductions were evident in several animals in the late stages of infection (Figure 5, panel A), and decreases in body temperature were found in a few hamsters (Figure 5, panel B). At 7 days postinfection, 100% lethality was observed in the CSUR381 group. In the Malaysia group, 1 animal survived until 10 days postinfection (Figure 5, panel C); however, the difference between the 2 groups was not significant. These results demonstrate similar lethality of the 2 analyzed NiV isolates, supporting our other data and suggesting CSUR381 has a high pathogenic potential.\n\n【44】### Discussion\n\n【45】In this study, we performed a molecular and genetic characterization of CSUR381, a NiV isolated from _P. lylei_ bats in Cambodia. Furthermore, we analyzed its pathogenicity compared with those of 2 other NiV isolates derived from human patients from the Malaysia and Bangladesh outbreaks. Our results highly suggest that CSUR381 is part of the NiV-Malaysia genotype. Further phylogenetic comparisons with other NiV isolates demonstrated 83%–99% amino acid homology for each of the 6 structural proteins. In addition, the editing site of the phosphoprotein gene was preserved, suggesting possible production of the nonstructural V and W proteins known to be involved in counteracting the host innate immune system and thus contributing to pathogenicity of CSUR381 ( _32_ ).\n\n【46】Our virus entry studies showed highly similar results among the NiVs tested. All isolates entered _Pteropus_ bat and human cell lines at similar levels; high conservation of the NiV entry receptors (ephrin-B2 and ephrin-B3) ( _38_ ) might be responsible for the observed results. Our data also indicate that CSUR381 enters all tested cell types as well as the other 2 NiV isolates tested, suggesting that virus entry is not a limiting factor preventing CSUR381 spillover from bats to humans. In addition, all 3 tested NiV isolates infected cells and replicated in bat and human cell lines at similar levels. Results of infections with CSUR381 in hamsters additionally strengthened the notion that CSUR381 is possibly similar pathogenically to the tested NiV-Malaysia strain, which caused fatal outbreaks in Malaysia ( _1_ ).\n\n【47】Although NiV has been shown to circulate in Cambodia ( _20_ , _21_ ), Thailand ( _39_ ), and Vietnam ( _40_ ), transmission to humans or domestic animals has not been reported in these countries. According to our results, the absence of detected outbreaks in this region cannot be attributed to lower pathogenicity of the circulating NiVs; our results suggest that other factors probably contribute. However, the NiV isolate presented in this report has been the only live NiV isolated in this region, and the existence of other NiVs with different pathogenic potentials cannot be excluded.\n\n【48】In Cambodia, _P. lylei_ bats were found to often forage in residential areas and visit palm trees used in the region as a source of date palm sap; thus, opportunities abound for bats to interact with humans and livestock in this country ( _41_ ). Bat colony migration toward urban sites is further enhanced by the presence of hunters in rural areas ( _42_ ) and deforestation (causing consequent damage to roosting trees and food sources) ( _43_ ). Contamination of palm sap, which is consumed raw by persons in the region, with bat urine, saliva, or feces was found to be a major route of NiV transmission to humans during annual outbreaks in Bangladesh ( _10_ ).\n\n【49】Diverse agricultural practices in Southeast Asia could also play a role in NiV regional ecodynamics, potentially favoring easier NiV spillover in some countries over others. High-intensity pig farming was recognized as a major risk factor for outbreaks in Malaysia during 1998–1999; because of the low-scale pig production ongoing in Cambodia ( _44_ ), the risk for NiV transmission from _Pteropus_ spp. to domestic animals and humans in this country might be reduced.\n\n【50】Unrecognized NiV outbreaks might have occurred in Cambodia and neighboring countries; hospital-based surveillance in Bangladesh was shown to have missed nearly half of the NiV outbreaks in that country since the first reported virus emergence ( _45_ ). Interdisciplinary approaches are certainly required to identify these outbreaks and the drivers of NiV emergence ( _46_ ), and regular testing of patients with encephalitis in Cambodia and neighboring countries could provide additional insight. Our study contributes to the assessment of the risk for NiV outbreaks in Asia. Our findings can be used to help target adequate preventive measures, which could ultimately help reduce the risk for NiV emergence.\n\n【51】Ms. Gaudino is a graduate student at the International Centre for Infectiology Research in Lyon, France. Her main research interests include the study of mechanisms of pathogenicity and epidemiology of viral zoonoses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d614b5a4-2fb6-4d72-b657-8e9dd526009c", "title": "West Nile Virus Neurologic Disease in Humans, South Africa, September 2008–May 2009", "text": "【0】West Nile Virus Neurologic Disease in Humans, South Africa, September 2008–May 2009\nWest Nile virus (WNV), a mosquito-borne flavivirus ( 1 ), is a reemerging pathogen of global concern ( 2 ). Febrile illness occurs in ≈20% of WNV-infected persons; neurologic complications (e.g., meningitis, encephalitis, flaccid paralysis) occur in <1% ( 3 ).\n\n【1】Detection of IgM in serum or cerebrospinal fluid (CSF) is the preferred method for diagnosing WNV infection; however, because of cross-reactivity between flaviviruses, positive results should be confirmed by virus neutralization assay. Early WNV infection can be diagnosed by PCR and virus isolation ( 4 ), but success has been limited in diagnosing more advanced disease with these techniques.\n\n【2】WNV is endemic to southern Africa. In 1947, one of the largest WNV epidemics recorded occurred in the Karoo region of South Africa ( 5 ), and another occurred in combination with a Sindbis virus epidemic in 1983–84 in the Witwatersrand–Pretoria region of South Africa. The most recent seroprevalence data for WNV in the Pretoria region of South Africa is from the 1970s (reviewed in \\[ 6 \\]). To update that information, we determined whether WNV is being overlooked as a possible cause of disease in persons hospitalized in South Africa. The University of Pretoria Research Ethics Committee approved this study.\n\n【3】### The Study\n\n【4】Serum and CSF samples were obtained from the National Health Laboratory Service, Thswane Academic Division, Thswane, South Africa, which serves public sector hospitals in northern South Africa. To select samples for testing, we reviewed laboratory submission requests for patients with clinical conditions consistent with WNV infection: fever, headache, rash, or neurologic signs ( 7 , 8 ). A total of 206 patient samples (15 CSF and 191 serum) were selected. During September 2008–May 2009, we screened samples for the presence of WNV by using real-time, reverse-transcription PCR (rRT-PCR) ( 9 ), virus neutralization assay, and IgM ELISA.\n\n【5】We detected WNV neutralizing antibodies in serum and CSF samples by using a modified method ( 10 ). In brief, we mixed 50% tissue culture infective doses of Kunjin virus strain MRM61C (100 U/mL) in 2% fetal bovine serum (Invitrogen, Carlsbad, CA, USA) with 2-fold dilutions of heat-inactivated patient serum (1:10–1:640) in equal volumes and incubated the mixture for 1 h at 37°C in 5% CO <sub>2 </sub> . We then added 1 volume of Vero cells (1 × 10 <sup>5 </sup> cells/mL) in 2% fetal bovine serum, 100 U/mL of penicillin, and 100 μg/mL of streptomycin (Lonza, Basel, Switzerland) and incubated the mixture for 72 h at 37°C in 5% CO <sub>2 </sub> . Samples were considered positive for WNV neutralizing antibodies if <25% of the cells/well displayed cytopathic effect. Comparative testing with Wesselsbron virus, a closely related flavivirus, did not show cross-reactivity within the parameters of what we considered positive.\n\n【6】Of the 206 specimens, 40 (19.42%, 95% CI 14.02%–24.82%) were positive for neutralizing antibodies. Of these, 36/191 serum samples had antibody titers of <160. The positive CSF samples (4/15) had antibody titers of 4 ( Table 1 ). Positive serum samples were also tested by WNV IgM capture ELISA (Panbio; Alere, Sinnamon Park, QLD, Australia); 2 had positive results ( Table 1 ).\n\n【7】Figure\n\n【8】Figure . . Maximum-likelihood tree of an ≈200-bp fragment of the nonstructural 5 gene of a reverse transcription PCR–positive West Nile virus (WNV) specimen SAH5238/08 (GenBank accession no. JX974605; black diamond) isolated from...\n\n【9】Of the 206 specimens, 190 were of sufficient quantity to be subjected to RNA extraction (QIAamp Viral RNA Mini Kit; QIAGEN, Valencia, CA, USA) and subsequent WNV nested rRT-PCR ( 9 ). The presence of lineage 2 WNV RNA was identified in 1 CSF specimen and confirmed by sequencing (GenBank accession no. JX974605) ( Figure ).\n\n【10】To evaluate the sensitivity of these molecular and serologic tests for diagnosing WNV in humans, we used the same methods to test 9 archived sequential serum samples in parallel (Table 2 ). The samples were from a patient with WNV encephalitis who became infected with neuroinvasive lineage 2 WNV strain in 2003 after a needlestick injury ( 11 ). Samples were collected 0–30 days after exposure. Initial symptoms developed on postexposure day 7 and persisted for 19 days; the patient completely recovered by day 26 ( 11 ).\n\n【11】### Conclusions\n\n【12】We conducted a retrospective investigation of patients hospitalized with febrile illness or neurologic disease of unknown etiology in the Pretoria region of South Africa to determine whether some of the cases could be ascribed to WNV infection. Evidence of acute WNV infection was identified in samples for 7 patients ( Table 1 ). For 2 of the patients, WNV infection was identified by the presence of IgM and neutralizing antibodies in serum samples; these patients had been hospitalized for febrile illness. For the other 5 patients, infection was identified by a WNV–positive (by PCR) CSF sample (1 patient) and by the presence of neutralizing antibodies in CSF samples (4 patients); these patients had been hospitalized for neurologic signs and symptoms.\n\n【13】The 4 patients with neutralizing antibodies in CSF all had severe neurologic complications ( Table 1 ). Samples from these patients were insufficient for performing IgM testing; thus, WNV infection cannot be definitively determined. However, the presence of WNV neutralizing antibody in CSF samples plus acute clinical signs and symptoms of WNV infection provide a high index of suspicion for WNV infection in these patients. Factors such as increased blood–brain barrier permeability and the persistence of WNV antibodies long after infection may also serve as explanations for the presence of neutralizing antibodies in their CSF.\n\n【14】Nested rRT-PCR results and phylogenetic analysis confirmed the presence of lineage 2 WNV in the CSF sample from 1 patient ( Figure ); sequencing showed that the virus is closely related to 2 neuroinvasive WNV lineage 2 strains identified in South Africa ( 11 , 12 ) ( Table 1 ). The low rate of PCR-positive cases was not entirely unexpected and may be explained by 2 factors: 1) PCR has limited success for detecting arboviruses because CSF contains low levels of virus and arbovirus-associated viremia is brief ( 13 ), and 2) false-negative test results may occur if samples are not properly stored to protect the integrity of potential viral RNA.\n\n【15】To evaluate the sensitivity of the 3 diagnostic methods used in our study, we conducted a time-trial experiment by using retrospective serum samples from a patient in whom WNV meningoencephalitis developed after a needlestick injury ( 11 ). Early samples (postexposure days 8 and 9) were positive for WNV by rRT-PCR only. Results for samples obtained ≥13 and ≥16 days after exposure were positive by neutralization assays and IgM ELISA, respectively ( Table 2 ). Experiments with horses have indicated that WNV neutralizing antibody assays show a positive result earlier than IgM ELISAs; the reasons for this are undetermined ( 14 ). Although our time-trial experiment reflects findings from only 1 patient and should ideally be performed on a cohort, the results, considered with those from the studies in horses ( 14 ), may imply that some cases of WNV infection in humans and animals may be missed if IgM ELISA is the only serologic test used.\n\n【16】Using virus neutralization assays, we identified the presence of WNV antibodies in 36/204 serum samples from patients with febrile and neurologic illness in South Africa. This finding indicates that the patients were exposed to WNV. In addition, results were negative for the patients in our study who were tested for herpes simplex virus types 1 and 2, measles, mumps, and enteroviruses, and no other etiologic agent was found. Thus, infection with WNV should be included in the differential diagnosis of patients in this region with neurologic disease, especially considering the frequent detection of severe neurologic disease in horses in the region ( 15 ).\n\n【17】Our findings confirm that WNV is being overlooked as a cause of severe neurologic disease in South Africa, and they suggest a need for increased clinical awareness, enhanced prospective surveillance, and a more current serosurvey of WNV infection in humans. PCR may be a useful diagnostic method during early infection, but after seroconversion has taken place, serologic tests (e.g., IgM ELISA in conjunction with virus neutralization) are more likely to yield accurate results.\n\n【18】Mr Zaayman is pursuing a PhD degree in the Zoonosis Unit, Department of Medical Virology, University of Pretoria. He is conducting research on the development and application of molecular and immunological tools for the differential diagnosis of West Nile virus in South Africa.\n\n【19】Prof Venter is head of the Zoonoses Research Unit in the Department of Medical Virology, University of Pretoria, Pretoria, South Africa, and head of the Center of Respiratory Diseases and Meningitis, National Institute for Communicable Diseases, Sandringham, South Africa. Her primary research interests are arboviruses and their association with neurological diseases in humans and animals in Africa and viral causes of severe acute respiratory infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0f49f6c8-0430-43c5-95b9-80bb47cefd59", "title": "Widespread Oseltamivir Resistance in Influenza A Viruses (H1N1), South Africa", "text": "【0】Widespread Oseltamivir Resistance in Influenza A Viruses (H1N1), South Africa\n**To the Editor:** Oseltamivir is the most widely used antiviral drug for influenza; it is a potent inhibitor of influenza virus neuraminidase (NA) protein ( _1_ ). Until recently, oseltamivir resistance occurred in <1% of circulating viruses globally. An increased number of influenza A viruses (H1N1) with resistance to oseltamivir was first reported to the World Health Organization (WHO) by Norway in late January 2008. The viruses carried a specific histidine-to-tyrosine mutation at position 274 (H274Y; H275Y in N1 numbering system) in the NA protein that confers high-level resistance to oseltamivir ( _2_ ). Further surveillance by the European Surveillance Network for Vigilance against Viral Resistance and the WHO Global Influenza Surveillance Network (GISN) showed that 16% of community isolates (0%–67% by country) of influenza A viruses (H1N1) circulating in the 2007–08 season in several other countries were also oseltamivir resistant ( _3_ ). The predominant influenza subtype circulating in South Africa this winter season is H1N1. To determine whether oseltamivir-resistant viruses have spread to South Africa, we examined influenza A (H1N1) isolated during the 2008 winter season for resistance to this antiviral compound.\n\n【1】Specimens were obtained mainly from the National Institute of Communicable Diseases (NICD) active sentinel surveillance program in all 9 provinces. Throat or nasopharyngeal swabs were taken from patients within 48–72 hours of onset of symptoms and sent to NICD laboratories for virus isolation as described ( _4_ ).\n\n【2】Of the H1N1 subtype viruses isolated in May and June, 23 were sent to the WHO Collaborating Centers for Reference and Research on Influenza in London and Melbourne for resistance testing ( _5_ , _6_ ). Forty-five of the viruses, which included viruses isolated in July, were tested at NICD by using a modified amplification refractory mutation system PCR (ARMS-PCR) ( _7_ ). This method can simultaneously detect wild-type or mutant virus with the 274 mutation in a single PCR. Partial sequencing of the NA and hemagglutinin (HA) genes was performed to confirm the NA H274Y resistance mutation and to determine genetic drift in HA from the A/Brisbane/59/2007 virus recommended for the Northern Hemisphere 2007–08 vaccine.\n\n【3】At the time of resistance testing, 92 H1N1 subtype viruses had been isolated. The 23 virus isolates sent to the WHO Collaborating Centers were highly resistant to oseltamivir by the NA inhibition enzyme assay, with 50% inhibitory concentration values of 554 nM to 1,485 nM (A. Hay, I. Barr, pers. comm.). All 45 isolates tested locally were positive by ARMS-PCR for oseltamivir resistance at position 274. The H274Y mutation was confirmed by sequence analysis of the N1 genes. The N1 sequences were closely related to those isolated in Europe and elsewhere in the 2007–08 winter season. However, the presence of 1 or 2 aa mutations in viruses from South Africa (M23L and N73K in the stalk region) compared with resistant European isolates indicated that some genetic drift of N1 from the older strains had occurred. Although most 2008 isolates were closely related to the A/Brisbane/59/2007 strain, several of the isolates from South Africa had mutations in an additional 2 or 3 aa residues at positions 183, 185, and 189, which mapped close to the receptor binding site of HA. (GenBank accession numbers for nucleotide sequences obtained in this study are EU914901–EU914916.)\n\n【4】Before the 2007–08 Northern Hemisphere winter, surveillance by GISN laboratories showed that oseltamivir-resistant H1N1 subtype viruses were extremely rare. Low numbers of drug-resistant viruses carrying the H274Y mutation usually followed oseltamivir treatment and showed reduced fitness with poor transmission ( _8_ ). Consequently, fitter nonresistant viruses appear to have predominated. In contrast, no evidence indicated that persons from whom resistant viruses were isolated during the European 2007–08 winter season had either been treated or been in close contact with another person who had been treated with oseltamivir ( _2_ , _8_ ).\n\n【5】We report oseltamivir-resistant H1N1 subtype viruses in Africa and the Southern Hemisphere. It appears that resistant viruses have spread from the Northern Hemisphere and have undergone extensive transmission within the population. These viruses may soon appear in other countries in the Southern Hemisphere. Ongoing monitoring is needed to understand the further evolution of oseltamivir resistance.\n\n【6】Clinical symptoms of all patients in this study suggest an illness similar to that generally associated with seasonal influenza A virus (H1N1); no complications were reported. This finding is not unexpected because all isolates tested in the study were from outpatients. The policy in South Africa for use of oseltamivir for treatment of severe influenza remains unchanged because 2008 H3N2 subtype viruses are still drug sensitive.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9774c5b7-edb8-4dca-b53a-b17e7a0a7cae", "title": "Chikungunya in the Caribbean—Threat for Europe", "text": "【0】Chikungunya in the Caribbean—Threat for Europe\n**To the Editor:** The first evidence of chikungunya virus in the Western Hemisphere was its detection in December 2013 in the French West Indies ( _1_ ). One month later, the virus spread to other Caribbean islands.\n\n【1】Two cases of chikungunya in siblings (an 8-year old girl and a 10-year old boy) were recently identified at Toulouse University Hospital in southwestern France. Two days after these children had returned to France from the island of Martinique (French West Indies), acute fever associated with an arthromyalgic syndrome developed in these children. The children had maculopapular, nonpruriginous rashes on their arms and legs and endobuccal petechiae. The boy had bilateral knee effusions, and the girl had a measles-like rash that became more extended. Both children also had many mosquito bites that they scratched. They were discharged on the day of their admission. These 2 cases reported in metropolitan France after the patients visited Martinique indicate rapid spread of chikungunya virus.\n\n【2】Figure\n\n【3】Figure . Phylogenetic tree constructed by using the neighbor-joining method and based on a partial (205 nt) sequence of the envelope protein 1 gene of chikungunya virus that was imported to metropolitan France...\n\n【4】We identified the virus by sequencing a 205-nt fragment within the envelope protein E1 gene of chikungunya virus ( _2_ ) and performing phylogenetic analyses on the basis of reference sequences. This virus was a strain from Asia ( Figure ), whereas virus detected in 2 children in southeastern France in September 2010 had been imported from Rajasthan, India, and was an East/Central/South Africa strain ( _3_ ). All of these strains did not show the single amino acid substitution in the envelope protein gene (E1-A226V) that favors adaptation for dissemination by _Aedes albopictus_ mosquitoes ( _4_ ) and would affect the potential magnitude of this outbreak.\n\n【5】_Ae_ . _aegypti_ mosquitoes are common in the Western Hemisphere, where they are the major vector of urban dengue and yellow fever, and will facilitate spread of chikungunya in this region. _Ae_ . _albopictus_ (Asian tiger mosquito) is also an efficient vector of chikungunya virus and is found in many areas, including southern Europe. This mosquito species was responsible for the extensive chikungunya outbreak on La Réunion Island in the Indian Ocean ( _5_ ) and was involved in the first chikungunya outbreak in Italy in 2007 ( _6_ ). In these 2 outbreaks, human and mosquito virus strains contained mutation A226V in the envelope protein gene.\n\n【6】_Ae. albopictus_ mosquitoes became established in a large area (91,150 km <sup>2 </sup> ) of southern France in 2013, where ≈13 million persons live. This mosquito, which is highly efficient in transmitting chikungunya virus ( _7_ ), has been present in the study area for 2 years. For these reasons, a chikungunya/dengue national control program for continental France was established in 2006. The program involves rapid virologic diagnosis of imported or suspected autochthonous cases and vector control measures. This program operates during May–November, the period when _Ae. albopictus_ mosquitoes circulate, and is based on entomologic surveillance data. The area covered by the program in 2013 was >10 times larger than that covered in 2006.\n\n【7】The presence of an effective vector, its progressive spread, and the outbreak of chikungunya in the Western Hemisphere increase concerns of a chikungunya outbreak in Europe ( _8_ ). The greatest challenge is to find a way of interrupting the transmission chain of the virus as soon as possible. This challenge requires an effective policy of informing travelers at risk, early screening based on rapid virologic diagnosis, and effective vector control. Such control measures need an educated population to ensure emptying standing water from flowerpots, gutters, buckets, pool covers, pet water dishes, and discarded tires. They also need global antivector measures (eradication of eggs, larvae, and adults of _Aedes s_ pp. mosquitoes).\n\n【8】These measures must be extremely efficient because an outbreak of chikungunya in the Western Hemisphere could spread rapidly. All countries in southern Europe are concerned by this public health challenge, and the battle against chikungunya requires rapid establishment of a supranational organization that should be able in real time to collect and return epidemiologic, virologic, and entomologic data. Although the usual movements of tourists around southern Europe during the summer will increase the number of persons at risk in this area, an even greater threat is the international movement of >600,000 persons expected to attend the next Soccer World Cup in Brazil in 2014 ( _9_ ).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5a06a4bb-8f90-4c88-bc29-d2ae98f58d2c", "title": "Screening for Obesity in Reproductive-Aged Women", "text": "|  |  |  |  |  |  |\n| --- | --- | --- | --- | --- | --- |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n| <table><tbody><tr><td><a><img></a></td></tr></tbody></table><table><tbody><tr><td><a><img></a></td></tr></tbody></table> |  |  |  |  |  |\n\n| \n<table><tbody><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>View Current Issue</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><a>Issue Archive</a></th></tr></tbody></table></th></tr><tr><th><img></th></tr><tr><th><table><tbody><tr><th><span><a>Archivo de nmeros en espaol</a></span></th></tr></tbody></table></th></tr><tr><th><img></th></tr></tbody></table>\n\n【2】<table><tbody><tr><th><img><br></th></tr><tr><th><form><label>Search <i>PCD</i></label><br><img><br><input> <input> <input><br><img><br><input><br><img><br></form></th></tr></tbody></table><table><tbody><tr><th><b><a>Emerging Infectious Diseases Journal</a></b></th></tr><tr><th><b><a>MMWR</a></b></th></tr></tbody></table>\n\n |  | <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Volume 8: No. 6, November 2011 \n\n【4】SPECIAL TOPICScreening for Obesity in Reproductive-Aged Women\n=============================================================\n\n【5】<table><tbody><tr><th><img><br></th><th><table><tbody><tr><th><a>TABLE OF CONTENTS</a></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><img></th><th><span><a>Este resumen en espaol</a></span></th></tr><tr><th><img></th><th><a>Print this article</a></th></tr><tr><th><img></th><th>E-mail this article:<br><img><br><input><br><img><br><input> </th></tr><tr><th><img></th><th><a>Send feedback to editors</a></th></tr><tr><th><img></th><th><a>Syndicate this content</a></th></tr><tr><th><img></th><th><a>Download this article as a PDF </a>(338K)</th></tr><tr><th><hr>You will need <a>Adobe Acrobat Reader </a>to view PDF files.</th></tr></tbody></table></th></tr><tr><th><img><br></th></tr><tr><th><img><br></th><th><table><tbody><tr><th><b>Navigate This Article</b></th></tr><tr><th></th><th><a>Abstract</a></th></tr><tr><th></th><th><a>Introduction</a></th></tr><tr><th></th><th><a>Risks Associated With Obesity in Women of Childbearing Age</a></th></tr><tr><th></th><th><a>Preconception Interventions</a></th></tr><tr><th></th><th><a>Obesity Screening</a></th></tr><tr><th></th><th><a>Recommendations for Screening</a></th></tr><tr><th></th><th><a>Conclusions</a></th></tr><tr><th></th><th><a>Author Information</a></th></tr><tr><th></th><th><a>References</a></th></tr><tr><th><img><br></th></tr></tbody></table></th></tr><tr><th><img><br></th></tr></tbody></table>\n\n【6】#### Chloe Zera, MD, MPH; Susan McGirr; Emily Oken, MD, MPH\n\n【7】_Suggested citation for this article:_ Zera C, McGirr S, Oken E. Screening for obesity in reproductive-aged women. Prev Chronic Dis 2011;8(6):A125. http://www.cdc.gov/pcd/issues/2011/nov/11\\_0032.htm . Accessed \\[ _date_ \\]. PEER REVIEWED \n\n【8】Abstract\n--------\n\n【9】Although obesity screening and treatment are recommended by the US Preventive Services Task Force, 1 in 5 women are obese when they conceive. Women are at risk for complications of untreated obesity particularly during the reproductive years and may benefit from targeted screening. Risks of obesity and potential benefits of intervention in this population are well characterized. Rates of adverse pregnancy outcomes including gestational diabetes, preeclampsia, cesarean delivery, and stillbirth increase as maternal body mass index increases. Offspring risks include higher rates of congenital anomalies, abnormal intrauterine growth, and childhood obesity. Observational data suggest that weight loss may reduce risks of obesity-related pregnancy complications. Although obesity screening has not been studied in women of reproductive age, the effect of obesity and the potential for significant maternal and fetal benefits make screening of women during the childbearing years an essential part of the effort to reduce the impact of the obesity epidemic. Back to top \n\n【10】Introduction\n------------\n\n【11】In 2009, 26% of US adult women reported a body mass index (BMI) in the obese range (≥30 kg/m 2 ) (1). With rates of both adult and adolescent obesity increasing, the prevalence of obesity among women of childbearing age (aged 15-44 y) can also be expected to increase. The US Preventive Services Task Force (2) and the National Heart, Lung, and Blood Institute (NHLBI) (3) recommend screening all adults for obesity in clinical settings, but neither specifically mentions screening for women during their reproductive years. Although the Centers for Disease Control and Preventions Select Panel on Preconception Care released recommendations in 2006 to improve the preconception care of women, which included addressing prepregnancy obesity, obesity remains an undertreated chronic disease that affects at least 1 in 5 pregnancies (4). One factor contributing to the gap between recommendation and practice may be that for the 20% of women of childbearing age who are uninsured, preventive care services are often provided episodically in settings such as federally funded family planning programs rather than in traditional primary care settings. The effect of untreated obesity among women of childbearing age includes adverse reproductive outcomes as well as adverse outcomes for these womens offspring. Although women who are obese may be at increased risk for unplanned pregnancy and contraceptive failure (5), infertility rates are higher among obese women than among normal-weight women (6). Once a woman is pregnant, both maternal and fetal risks are increased by high maternal BMI. Pregnancy-associated morbidity and mortality are higher in obese women than in normal-weight women (7). The offspring of obese women face an increased risk of obesity and other chronic metabolic diseases (8). Women of childbearing age are therefore a uniquely at-risk population who may benefit from targeted screening. Our objective was to develop recommendations for screening in women of childbearing age by focusing on the efficacy, benefits, and potential harms of screening in this population. This article is part of a series of articles focused on screening women of reproductive age for chronic health conditions, particularly women seeking care at clinics that receive Title X Family Planning funding. We systematically reviewed the literature on obesity screening in women of reproductive age. We searched the MEDLINE database for articles published from January 1, 1998, to July 1, 2010. We used the search terms “obesity” or “BMI” and “screen” or “assessment” limited to humans aged 19 to 64 years to identify potentially relevant articles. Using these parameters, we were unable to identify any studies that focused on the feasibility, acceptability, risks, benefits, or costs of obesity screening in women of childbearing age. We therefore chose to frame our discussion of screening by summarizing the current evidence surrounding the risks of obesity before and during pregnancy, reviewing potential risks and benefits of obesity treatment as part of preconception care, highlighting gaps in current screening practices, and proposing recommendations for screening opportunities in the population of women who may become pregnant. Back to top \n\n【12】Risks Associated With Obesity in Women of Childbearing Age\n----------------------------------------------------------\n\n【13】Although unplanned pregnancy is of concern in a population at high risk for complications during pregnancy, obese women are less likely to use contraception than normal-weight women (9). Furthermore, the counseling of obese women regarding contraceptive options is more complex than for women of normal weight. Although contraceptive efficacy is generally high, there may be higher rates of failure of combined hormonal methods in obese women (10), and both intrauterine contraception placement and surgical sterilization are more technically challenging and more likely to result in complications (11) in women with a high BMI than in women of normal weight. Although the absolute risk of venous thromboembolism is small in obese women who use combined hormonal contraception, the risk is elevated relative to women who are not using combined contraception, and further study is needed to determine whether risks are higher in obese women than in normal-weight women (5). The risks of adverse pregnancy outcomes increase as maternal BMI increases beyond the normal range (BMI ≥25 kg/m 2 ). Achieving pregnancy can be more difficult for obese women because they are less likely to ovulate regularly, have decreased fecundity, and have increased risk of miscarriage (6). Gestational diabetes affects as many as 20% of pregnancies in women who are obese, a 4-fold increase compared with normal-weight women (12). Hypertension in pregnancy is more frequent in obese women, and they have a 2- to 3-fold increased odds of preeclampsia (12). The risk of preterm delivery in obese women is also elevated, likely in part because of their increased risk of preeclampsia (12). High maternal BMI is associated with intrapartum complications. Induction of labor is more common in women with high BMI than in normal-weight women (12). Rates of both primary and repeat cesarean delivery increase with increasing maternal BMI, with trial of labor after cesarean less likely to be successful than in normal-weight women (12,13). The peripartum complications faced by obese women include a greater likelihood of infection, need for blood transfusion, and venous thromboembolism, particularly with cesarean delivery (14). Prepregnancy obesity is also associated with a 3-fold increase in maternal death and near-miss morbidity (7). In addition to the maternal consequences of obesity, there are significant implications of excess maternal weight to offspring. The risk of fetal anomalies including cleft palate, neural tube defects, and congenital heart disease is increased (15), and detection of anomalies is made more difficult because of the technical challenges of ultrasound in obese women. The risk of stillbirth among obese women is approximately double that among normal-weight women (16). Obese women are more likely to give birth to infants who are large for gestational age or macrosomic (14); these infants are at higher risk than their normal birth weight counterparts for long-term cardiometabolic complications, including obesity (8). Even normal birth weight offspring of obese mothers are more likely to be obese than offspring of normal-weight mothers (8), further fueling the obesity epidemic. Back to top \n\n【14】Preconception Interventions\n---------------------------\n\n【15】The NHLBI has recommended treating all obese patients who are receptive to intervention, while acknowledging that a limitation of all therapies, including lifestyle intervention, medical therapy, and surgical therapy, is a low rate of long-term weight maintenance (3). Nonetheless, randomized controlled trials of multiple weight-loss strategies have demonstrated short-term efficacy. None of these trials has specifically focused on the treatment of obesity during a womans reproductive life, particularly before conception. Observational data suggest that interpregnancy weight loss is associated with a decreased risk of preeclampsia, large-for-gestational-age birth weight, and cesarean delivery (17), compared with the previous pregnancy. Few studies have examined the effect of lifestyle interventions or medical therapy before conception on pregnancy outcomes. Several case series have attempted to examine the effect of bariatric surgery by comparing outcomes in women who have had pregnancies both pre- and postoperatively, or by choosing normal-weight controls. There may be a decrease in infertility and spontaneous abortion in women who have undergone bariatric surgery (5). Consistent findings include lower rates of gestational diabetes and hypertensive complications (18) after surgical intervention, without increases in fetal or maternal morbidity. One study of children of mothers who had undergone bariatric surgery found that, compared with children born before their mothers had bariatric surgery, fewer children born after their mothers had bariatric surgery showed abnormalities in anthropometry and hormonal markers of cardiometabolic risk (19). While these data suggest that aggressive treatment of obesity may be warranted, prospective study is needed to quantify the risks and benefits of both medical and surgical management before pregnancy. Back to top \n\n【16】Obesity Screening\n-----------------\n\n【17】The rate of obesity screening in clinical practice is low. Obesity is underdiagnosed in primary care settings (20); consistently fewer than half of obese patients have a documented obesity diagnosis. Similarly, a minority of obese people report receiving weight-loss advice from their health care provider (21). Although some clinicians have argued that screening is unnecessary because patients have an adequate perception of body weight, misperception of weight status is in fact widespread, with 1 study reporting that only 14% of obese women recruited from urban health centers in Atlanta correctly identified their weight status (22). Theoretical risks of obesity screening include stigmatization or psychological effects associated with the diagnosis of obesity; however, we were unable to identify literature substantiating these risks. In fact, data suggest that awareness of weight status is beneficial. The 2003-2008 National Health and Nutrition Examination Survey found that 98% of the women who correctly perceived themselves as overweight desired to weigh less and 72% were actively attempting to lose weight, compared with only 37% of women who did not perceive themselves as overweight (23). Moreover, overweight and obese women who had received a diagnosis of overweight or obesity from their health care provider were twice as likely to endorse weight control behaviors (ie, diet, exercise) as women who did not have a formal diagnosis (23). The potential benefits of obesity screening as a part of preconception counseling remain unexplored. Back to top \n\n【18】Recommendations for Screening\n-----------------------------\n\n【19】Screening for obesity in women of reproductive age remains understudied as a population-level intervention. There are data, however, that support the basic tenets of screening for obesity in women during childbearing years. Obesity is a disease with measurable public health impact during the reproductive years, not just later in life. Screening for obesity by calculating BMI is a reproducible, simple, and inexpensive technique that reliably estimates adiposity without the time and expense of less practical methods such as measurement of total body water (8). Although there are theoretical risks, including misdiagnosis, stigma associated with diagnosis, and extra cost associated with treatment, these risks have not been demonstrated in current literature. Finally, the potential benefits of screening include appropriate treatment with resultant weight loss, which may improve outcomes for women and their children. In particular, weight loss before pregnancy may have immediate benefits including reduced maternal morbidity and decreased risk of long-term cardiometabolic consequences for children, even if weight loss is not sustained over time. At a minimum, identification of obesity before conception allows for appropriate preconception counseling. Current recommendations from the American Congress of Obstetricians and Gynecologists include discussion of specific maternal and fetal risks of prepregnancy obesity, counseling regarding the benefits of weight reduction, and management to identify early or minimize possible complications of obesity (24). We are unaware of prior reviews of obesity screening that have focused on women of reproductive age. Although we were not able to identify sufficient literature regarding obesity screening to perform a meta-analysis, evidence regarding the risks of untreated obesity in women of reproductive age is substantial. The limited data on pregnancy outcomes following the surgical treatment of obesity are promising and suggest there may be benefit to identifying and treating obesity in women who are planning pregnancy. The American Congress of Obstetricians and Gynecologists is the only national body that has issued specific guidelines for screening women of reproductive age for obesity, with the recommendation that BMI be calculated for all women (24). Despite these recommendations, current screening rates are low (20). Recognizing that many young women access the health care system sporadically and that nearly 50% of pregnancies are unplanned, we believe that screening should be performed in settings beyond primary care clinics, including community health centers, college health service centers, and family planning clinics. We suggest that women of childbearing age presenting for care in these settings should be told their BMI after a measured weight and height are obtained. The potential benefits of screening all women of childbearing age include identification of women at high risk and referral to appropriate treatment before pregnancy. The American Dietetic Association and the American Society for Nutrition have issued a joint statement supporting counseling all overweight and obese women of reproductive age on dietary modification and physical activity (25). Further data are needed to understand the effect that losing a small amount of weight has on pregnancy outcomes. A potential harm is stigma, although this risk has not been studied, and additional costs may be incurred with increased diagnosis of obesity. In resource-poor settings, follow-up care may be inadequate to ensure appropriate treatment of women identified in broad screening programs. Although we acknowledge these potential pitfalls of widespread screening, we argue that simply identifying obesity may itself be an intervention because women who accurately perceive that they are overweight may be more likely to change their diet or demonstrate healthy weight-related behaviors than women who misperceive their weight status (23). Back to top \n\n【20】Conclusions\n-----------\n\n【21】Although obesity screening has not been studied specifically in women of reproductive age, there are compelling reasons to prioritize screening in this population. Women of childbearing age who are obese are at increased risk of infertility, miscarriage, and other adverse pregnancy outcomes. Mounting evidence supports the intergenerational transmission of obesity as a result of an abnormal intrauterine environment. Although randomized controlled trials of intensive treatment of obesity before pregnancy have not yet been performed, observational data suggest the possibility that weight reduction may ameliorate these risks. Furthermore, clinicians who identify women as obese may be more likely to provide appropriate care and screening to identify and reduce risks for secondary complications of obesity. Screening of reproductive-aged women is an essential part of broadening efforts to reduce the effect of the obesity epidemic in this country. Back to top \n\n【22】Author Information\n------------------\n\nCorresponding Author: Chloe Zera, MD, MPH, Division of Maternal-Fetal Medicine, Department of Obstetrics, Gynecology and Reproductive Biology, Brigham and Womens Hospital, 75 Francis St, Boston MA 02115. Telephone: 617-732-5452. E-mail: czera@partners.org . Author Affiliations: Susan McGirr, Harvard Medical School, Boston, Massachusetts; Emily Oken, Obesity Prevention Program, Department of Population Medicine, Harvard Medical School and Harvard Pilgrim Health Care, Boston, Massachusetts. Back to top  |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. <table><tbody><tr><th><a>Home</a></th></tr></tbody></table>Privacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 <table><tbody><tr><th><a>Centers for Disease Control and Prevention</a><br><a>National Center for Chronic Disease Prevention and Health Promotion</a></th><th><img></th><th><img> <a>United States Department of<br>Health and Human Services</a></th></tr></tbody></table>  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "702c7aef-3405-4556-8bd3-145d6ee68251", "title": "Murine Typhus with Renal Involvement in Canary Islands, Spain", "text": "【0】Murine Typhus with Renal Involvement in Canary Islands, Spain\n### The Study\n\n【1】Murine or endemic typhus is caused by _Rickettsia typhi_ , formerly _R. mooseri_ ( _1_ ). Classic murine typhus is a zoonosis maintained in rats ( _Rattus rattus_ and _Rattus norvergicus_ ) and transmitted to humans through damaged skin by infected feces from the oriental rat flea ( _Xenopsylla cheopis_ ) ( _2_ ). New patterns of disease (“murine typhus-like” disease) have been described in recent years, and a new species of _Rickettsia_ ( _R_ . _felis_ ) that causes a similar clinical picture has been identified ( _1_ – _3_ ). New modes of infection have been identified, including infection through inhalation of flea feces and transmission by different types of fleas ( _Ctenophtephalis felis_ ) and from different reservoirs (e.g., dog, cat, and opossum).\n\n【2】Murine typhus occurs worldwide, particularly in warm and humid climates ( _1_ ). In Spain, two seroepidemiologic surveys, in Salamanca and Madrid (Central/Western Spain), yielded seroprevalence rates of 12.8% and 68%, respectively, in the general population ( _4_ , _5_ ). However, no clinical cases have been reported. In Seville (Southwestern Spain), murine typhus is an important cause of fever of intermediate duration ( _6_ ), and in Canary Islands, 10 autochthonous cases have been reported from Tenerife ( _7_ ). For this reason, we include serologic testing for _R. typhi_ in the evaluation of patients with fever of intermediate duration. We describe the clinical picture of murine typhus in the Canary Islands.\n\n【3】Adult ( \\> 14 years of age) in- and outpatients at the Hospital Universitario Insular of Las Palmas with a serologic diagnosis of murine typhus during December 1, 2000, through July 30, 2002, were included in our study. A case was defined by an immunoglobulin (Ig) M titer \\> 1: 40, or a fourfold or higher increase in IgG titers against _R. typhi_ by direct immunofluorescence test in 8 weeks (bioMeriéux, France), or both.\n\n【4】Figure 1\n\n【5】Figure 1 . Distribution of cases of murine typhus by season.\n\n【6】Figure 2\n\n【7】Figure 2 . Distribution of cases of murine typhus by geographic area.\n\n【8】Epidemiologic, clinical, and laboratory data were collected. Antibodies against other agents that may cause a fever of intermediate duration (e.g., _Coxiella burnetii_ , _R_ . _conorii_ , _Leptospira_ sp., Epstein-Barr virus, and cytomegalovirus) were systematically tested. Twenty-two patients (21 men, 1 woman), with a mean age of 28 years (range 14 to 76 years), were included. Murine typhus was more frequent in summer ( Figure 1 ). No case aggregation was observed. The geographic distribution is shown in Figure 2 . All patients reported contact with animals (13 with dogs, 6 with horses, 5 with goats, 2 with cats, and 1 with camels).\n\n【9】The main symptoms and signs recorded are shown in Table 1 . All patients had a high fever (mean 39.3°C) during a mean of 10 days (range 7 to 20 days). A light maculopapular, nonpurpuric rash, with rather centripetal distribution, was a frequent finding (68.2%). Up to one third of the patients had a dry cough. Hepatomegaly and, less frequently, splenomegaly were detected. Skin lesions suggestive of insect bites were found in 13.6% of the patients.\n\n【10】Four patients had a mild normocytic anemia. For most patients, leukocyte counts were normal, mild leukopenia was detected in two patients, and mild leukocytosis in four patients. Ten case-patients (45%) had thrombocytopenia. In most patients (89.5%), the erythrocyte sedimentation rate was high (11 mm to 83 mm), and the activated partial thromboplastin time (aPTT) was prolonged in six patients.\n\n【11】Aminotransferase elevation, usually four times above normal, was found frequently; two patients had normal values. Four patients had alanine aminotransferase values 10 times the normal value. Plasma bilirubin was normal for all patients.\n\n【12】In 36% of the patients, the plasma blood urea nitrogen was elevated; plasma creatinine was above normal in three cases (13%). In 19 cases (87%), alterations were found in the urinalysis. Fifteen patients had proteinuria and microhematuria with or without leukocyturia and granular casts, with a negative nitrite reaction. In two patients, isolated proteinuria occurred, and isolated microhematuria occurred in two other patients. All of these findings resolved quickly.\n\n【13】Eight patients fulfilled both diagnostic criteria (IgM \\> 1:40 plus seroconversion), eight patients had initial IgM elevation, and six seroconverted without IgM increase. Cross-reactivity between _R_ . _typhi_ and other microorganisms was not observed. Fifty percent of the patients had serologic evidence of past infection with _C_ . _burnetii_ (12/22) or _R_ . _conorii_ (3/22) and, in one case, of co-infection with _C_ . _burnetii_ .\n\n【14】Eight cases were not treated because of spontaneous recovery. The remaining patients received doxycycline (100 mg twice a day). Fever disappeared from 1 to 6 days (median 2 days).\n\n【15】Three patients had severe signs and symptoms. Patient 6 was admitted with acute respiratory failure, lung infiltrates, and acute renal failure (plasma creatinine 2.8% mg), microhematuria, and leukocyturia. Intravenous fluids, doxycycline, ciprofloxacin, and methylprednisolone (1 g) were administered, and the patient rapidly improved. Autoantibodies and cryoglobulins were negative. Patient 16 had a dry cough and acute renal failure (plasma creatinine 2.7% mg) and later become disoriented. A cranial contrast computed tomography scan was normal, and cerebrospinal fluid (CSF) showed mononuclear pleocytosis (90 cells/μL), protein 70 mg/dL, and normal glucose. Doxicycline was administered with rapid neurologic improvement. Conjunctivitis and rash appeared but waned shortly after. Finally, patient 21 had a progressive meningeal syndrome, CSF showed mononuclear pleocytosis (19 cells/μL), increased protein (49 mg/dL), and normal glucose. The patient completely recovered in 48 hours under doxycycline.\n\n【16】Fever of intermediate duration has been defined by others in Spain as fever of 7 to 28 days without localizing signs (i.e., respiratory, digestive, urinary, or neurologic), plus the absence of diagnostic clues after a complete evaluation ( _6_ ). A few diseases can account for most cases of this type of fever (mainly Q fever, brucellosis, boutonneuse fever, leptospirosis, mononucleosic syndromes, and murine typhus). In our area, autochthonous cases of boutonneuse fever or brucellosis have never been reported. Diagnosis is usually based on serology, which requires time for confirmation. Therefore, in the meantime, identifying clinical data for empirical treatment is important.\n\n【17】In our study, the number of cases per year is 12, higher than that in other areas of Spain ( _6_ ), Israel ( _8_ ), or the United States ( _9_ , _10_ ), with higher rate in summer. Most patients were male. All patients had direct contact with animals as reported by others ( _9_ , _10_ ); dogs were the most frequently cited animal ( _9_ , _10_ ).\n\n【18】The clinical features in our study are similar to those reported by others ( _6_ , _9_ – _13_ ) with respect to those most frequent symptoms (fever, headache, and arthromyalgia) ( Table 1 ). The incidence of rash is similar to that reported by Bernabeu ( _6_ ) and Whiteford ( _9_ ) and higher than that in other series. Reports of insect bites were more frequent in our study than studies from other areas (Bernabeu \\[ _6_ \\] and Silpapojakul \\[ _11_ \\]), but more insect bites were reported from a Texas study ( _9_ ).\n\n【19】The laboratory findings in our study are similar to findings in other studies, although its relative frequency is variable ( Table 2 ). The frequency of anemia varies from 1% to 69%, leukopenia from 4% to 40%, and thrombocytopenia from 3% to 60% ( _6_ , _9_ – _11_ ). While 80% of the patients with Q fever in our area have a prolonged aPTT, 27% of the patients with murine typhus displayed this abnormal coagulation test. An elevation of aminotransferases in the range of viral hepatitis was common, but hyperbilirubinemia is exceptional and usually associated with alcoholism, co-infection, or glucose-6-phosphate dehydrogenase deficiency.\n\n【20】Nephrologic alterations had a high frequency in our study. Three patients had acute renal failure, and 87% had some abnormality in the urinalysis, mainly microhematuria. These data are in sharp contrast with the low incidence of urinary alterations found in other studies. Some broad studies ( _6_ , _9_ , _10_ ) do not report urinary abnormalities in murine typhus, though Dumler et al. ( _13_ ) reported microhematuria or proteinuria in 28% of their patients. In a study specifically focused on renal involvement in murine typhus, Shaked et al. observed urinary abnormalities in 5 of 27 patients studied ( _8_ ). To the best of our knowledge, 11 cases of acute renal failure have been related to _R_ . _typhi_ ( _9_ , _11_ , _14_ , _15_ ).\n\n【21】In general, murine typhus is a mild disease. However, a number of miscellaneous complications have been described. Our severe cases accounted for 13% (one renopulmonary syndrome, one encephalitis, and one meningitis with renal failure).\n\n【22】### Conclusions\n\n【23】In summary, in Canary Islands, incidence of murine typhus seem to be higher, patients more frequently report contact with dogs, the frequency of complicated cases is higher, and the incidence of renal involvement is higher. These data define a clinical picture of murine typhus that is somewhat different for the Canary Islands. These differences could be attributed to age (infantile versus adult series), mode of transmission or infection, or different strains of _R. typhi_ . The diagnostic methodology was indirect, so cross-reaction with other rickettsiae cannot be excluded ( _11_ ). Moreover, in our area, dogs are frequently parasitized by the flea of cats, a well-known vector for _R. felis_ ( _3_ ). More studies with direct diagnostic methods are needed to better define these differences. Finally, detecting urinary abnormalities in the setting of fever of intermediate duration, especially if associated with skin rash, thrombocytopenia, or hypertransaminasemia, in our geographic area is strongly suggestive of murine typhus.\n\n【24】Dr. Hernández-Cabrera was an associate professor of Infectious Diseases and Tropical Medicine at Faculty of Medicine at University of Las Palmas de Gran Canaria. Spain. His research interests focus on rickettsial diseases and other causes of fever of intermediate duration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ae440027-10b7-4a4c-be8f-ce258a037a24", "title": "West Nile Virus in Horses, Guatemala", "text": "【0】West Nile Virus in Horses, Guatemala\n**To the Editor:** West Nile virus (WNV, _Flaviviridae_ : _Flavivirus_ ) is emerging as a public health and veterinary concern. Since its introduction into North America in 1999, it has spread rapidly, reaching the Caribbean Basin in 2001, Mexico in 2002, El Salvador in 2003, and Colombia in 2004 ( _1_ ). However, reports of equine illness and deaths in Latin America are inconclusive. With the exception of viral isolates from a dead bird, a human, and a mosquito pool in Mexico ( _2_ _,_ _3_ ), all reports of WNV presence in Latin America have relied on serologic evidence. WNV is a member of the Japanese encephalitis serocomplex, which in the Western Hemisphere includes St. Louis encephalitis virus (SLEV) ( _4_ ). Serologic investigations for WNV in Latin America must use highly specific assays to differentiate WNV infection from potentially cross-reactive viruses such as SLEV or possibly additional unknown viruses. In particular, SLEV is of concern since it was previously isolated from Guatemalan mosquitoes ( _5_ ).\n\n【1】Alerted by the findings of WNV transmission in the region ( _1_ ), we collected serum samples from horses from 19 departments of Guatemala from September 2003 to March 2004, to initially estimate the extent of WNV spread and its potential public health risk. Because no animals exhibited signs of neurologic illness at the time of the survey, only healthy horses were sampled. Before 2005, equine WNV vaccines were prohibited and unavailable in Guatemala (Unidad de Normas y Regulaciones, Ministerio de Agricultura Ganadería y Alimentación, Guatemala, pers. comm.); as such, cross-reactivity due to prior vaccination is highly unlikely. Samples were initially tested for WNV-reactive antibodies by using an epitope-blocking enzyme-linked immunosorbent assay (blocking ELISA) ( _6_ ). The ability of the test sera to block the binding of the monoclonal antibodies to WNV antigen was compared to the blocking ability of control horse serum without antibody to WNV. Data were expressed as relative percentages and inhibition values >30% were considered to indicate the presence of viral antibodies.\n\n【2】A subset of positive samples was further confirmed by plaque-reduction neutralization test ( _7_ ). Of 352 samples, 149 (42.3%) tested positive with the 3.1112G WNV-specific monoclonal antibody. Of 70 blocking ELISA–positive samples, the neutralization tests indicated the infecting agent was WNV, SLEV, and undifferentiated flavivirus in 9, 33, and 21 samples, respectively. Titers were expressed as the reciprocal of serum dilutions yielding >90% reduction in the number of plaques in a plaque-reduction neutralization test (PRNT <sub>90 </sub> ). PRNT <sub>90 </sub> titers of horses seropositive for WNV ranged from 80 to 320. PRNT <sub>90 </sub> titers of horses seropositive for SLEV ranged from 40 to 2,560. For the differential diagnosis of samples with neutralizing antibody titers against both WNV and SLEV in this test, a >4-fold titer difference was used to identify the etiologic agent. The undifferentiated flavivirus-reactive specimens had <4-fold difference in cross-neutralization titers. Likely possibilities for the inability to distinguish the infecting virus include previous infection with these or other flaviviruses (previously described or unknown) resulting in elevated cross-reactive titers. The remaining 10% of specimens that tested negative by PRNT probably represent nonneutralizing antibodies in the serum or false positivity in the blocking ELISA.\n\n【3】Figure\n\n【4】Figure . Geographic distribution in Guatemala of horses showing previous infections with West Nile virus (WNV), Saint Louis encephalitis virus (SLEV), or undifferentiated flavivirus as confirmed by plaque reduction neutralization test. Each location...\n\n【5】Our serologic results provide indirect evidence of past transmission of WNV, SLEV, and possibly other flaviviruses to horses in Guatemala. Although no confirmed cases of WNV-attributed disease have been reported in Central America to date, flavivirus transmission appears to be widely distributed in Guatemala ( Figure ). Efforts are under way to confirm WNV transmission by viral isolation and to evaluate the impact of WNV on human, horse, and wildlife populations. More information is needed to establish the public health threat of WNV and other zoonotic flaviviruses in the region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "542c4fdd-ff4d-4824-86c5-6b9fcd314893", "title": "Prisons as Reservoir for Community Transmission of Tuberculosis, Brazil", "text": "【0】Prisons as Reservoir for Community Transmission of Tuberculosis, Brazil\nBrazil has the fourth largest incarcerated population in the world and a tuberculosis (TB) incidence that is 20 times higher among prisoners than among the general population ( _1_ _,_ _2_ ). It has been hypothesized that prisons serve as institutional amplifiers for TB, wherein poorly controlled transmission among incarcerated persons is a driver of TB in the broader population ( _3_ _,_ _4_ ). However, few data show for linkages between prison and community epidemics of TB. To address this issue, we conducted a population-based study of TB cases in Dourados, a medium-size city in west–central Brazil, and used case–control and molecular methods to assess the relationship between incarceration and TB in the general population.\n\n【1】### The Study\n\n【2】Dourados has a population of ≈177,160 persons, of which 1,500 are inmates of a prison for men. We identified and recruited TB patients reported to the Sistema de Informação de Agravos de Notificação National (Notifiable Diseases Information System) and who resided in Dourados during June 2009–March 2013. We then conducted a case–control study in which 2 control persons without a TB diagnosis were identified and matched for each TB case-patient according to age group and place of residence.\n\n【3】We performed conditional logistic regression to identify significant (p<0.05) risk factors for active TB. Variables were included in a multivariable model if they reached a significance level of p<0.20 in univariate analysis. _Mycobacterium tuberculosis_ isolates were typed by IS _6110_ restriction fragment length polymorphism (RFLP) analysis ( _5_ ). RFLP patterns were analyzed by using an IS _6110_ RFLP database (RIVM–Bionumerics; Applied Maths, Sint-Martens-Latem, Belgium). A cluster was defined as a group of ≥2 isolates obtained from different patients for which the RFLP patterns were identical with respect to the number and size of bands.\n\n【4】Figure 1\n\n【5】Figure 1 . Flowchart for recruitment of patients with tuberculosis (TB) for case–control and molecular studies of prisons as reservoir for community transmission of tuberculosis, Brazil, June 2009–March 2013. \\*Eleven strains were not reactive...\n\n【6】A total of 240 TB cases were reported, of which 60 (25%) and 180 (75%) were in prisoners and community residents, respectively ( Figure 1 ). The annual incidence of TB in the prisoner population was ≈40 times higher than in the community population (1,044 cases/100,000 persons \\[95% CI 797–1,344 cases/100,000 persons\\] vs. 26 cases/100,000 persons \\[95% CI 23–31 cases/100,000 persons\\]). All 60 prisoners had pulmonary TB and it was confirmed bacteriologically for 54 (90%) persons. Among 180 persons with TB cases in the community population, 133 (74%) had pulmonary TB, 34 (19%) had extrapulmonary TB, and 13 (7%) had both forms; 107 (59%) of the TB cases were confirmed bacteriologically. During the study, 49 (82%) prisoners with TB completed treatment, 2 (3%) were not cured, 3 (5%) died, and 6 (10%) were transferred to other prisons. Prisoners with cases were incarcerated for an average duration of 26 months before diagnosis.\n\n【7】We recruited 61 persons with TB and 122 controls from the community to evaluate risk factors for TB acquisition ( Figure 1 ). Multivariable analysis showed that male sex (adjusted odds ratio \\[AOR\\] 6.6, 95% CI 2.4–18.1), monthly income ≤100 US dollars (AOR 3.4, 95% CI 1.1–10.6), alcohol use (AOR 11.5, 95% CI 2.0–67.0), known history of contact with a TB patient (AOR 5.6, 95% CI 1.4–22.0), and prior incarceration (AOR 24.5, 95% CI 2.4–254.6) were independent risk factors for TB ( Table ). A total of 23% (14/61) of the community cases were in persons previously incarcerated in the Dourados Prison.\n\n【8】Figure 2\n\n【9】Figure 2 . Temporal distribution of _Mycobacterium tuberculosis_ strains isolated from the urban population, prisoners, and ex-prisoners in Dourados, Brazil, clustered by IS _6110_ restriction fragment length polymorphism analysis, June 2009–March 2013, and stratified by...\n\n【10】We genotyped 97 (86%) of 113 strains isolated from persons with culture-positive _M._ _tuberculosis_ infection, of which 59 and 38 were isolated from community persons and prison patients, respectively ( Figure 1 ). Of these, 79 (81%) isolates were grouped into 17 clusters, and 18 isolates had unique RFLP patterns. Among the 17 cluster types, 10 types included 65 strains from community and prison settings, 6 types included 12 strains exclusively from the community setting, and 1 type included 2 strains exclusively from the prison setting. Cluster 10, the largest cluster (20 cases), was predominantly found in prisoners, but was also isolated from ex-prisoners and community members without a history of incarceration. Of the 12 community cases with cluster types exclusively found in the community, one was in an ex-prisoner ( Figure 2 ).\n\n【11】Overall, 54% (32/59) of community strains belonged to cluster types that also included prison strains. Among the 32 cluster strains that were circulating in the prison and community, 12 (37%) were isolated from ex-prisoners who were recently released from prison. Ten (83%) of 12 cases occurred within ≤2 years of the inmate’s release from prison ( Figure 2 ).\n\n【12】### Conclusions\n\n【13】Prisons have long been recognized as high-risk environments for TB ( _6_ _,_ _7_ ), but there are little data concerning the potential transmission of the disease into community settings. During a 4-year period in a medium-size city in Brazil, 25% of TB cases occurred among prisoners, who represented <1% of the population. Our case–control study showed that that ex-prisoners had 23% more cases of TB than the general population. Among cases in ex-prisoners, 83% (10/12) were diagnosed in the first 2 years after release from prison, which suggests recent infection acquired in the prison setting.\n\n【14】Although exposure to TB might occur after a prisoners’s release, we believe that this is less likely because most (71%) ex-prisoners had isolates with the same RFLP pattern as patterns found in prison isolates. Also, 83% (10/12) of ex-prisoners who had an isolate with a similar genetic profile were reported after a case of TB in the prison was reported ( Figure 2 ). Furthermore, we found that baseline tuberculin skin test positivity rates were low (7%) among newly incarcerated inmates, which further supports the assertion that the high rate of TB among prisoners was caused by transmission in the prison setting, rather than by exposure to the disease in the community before incarceration.\n\n【15】The presence of multiple clusters involving prisoners and the general population indicates that TB can spread between these 2 populations. Only 1 previous study reported a link between TB cases in a community setting and those in a jail or prison, but this finding was based on an outbreak that involved only 1 strain ( _8_ ). If one considers the genetic linkages observed in this study and the high rates of disease among prisoners and ex-prisoners, our findings suggest that prisons serve as major reservoirs of TB for the general population.\n\n【16】This study had several limitations. First, we did not evaluate the linkage between cases by investigating close contacts, which limited our ability to establish exact epidemiologic connections between patients. Future contact tracing studies might enhance our understanding of the chain of TB transmission in these settings ( _9_ _–_ _11_ ). Second, we used RFLP to assign clusters, which might overestimate the proximity of genetic or epidemiologic linkages ( _12_ ); future studies involving whole-genome sequencing might help clarify the timing and directionality of disease transmission ( _13_ ). Third, we assessed data for only 1 city and 1 prison. The prison had typical conditions in terms of layout, crowding, and diagnostic resources. However, other studies have found that TB incidence and transmission rates are even higher in other prisons in Brazil ( _3_ _,_ _14_ _,_ _15_ ). Thus, the contribution of spillover infections to the general community may be even greater.\n\n【17】Our data demonstrate that incarceration is a strong risk factor for acquiring TB and that the epidemic of TB in prisons is interlinked with that in the general population. Policies and programs aimed at reducing transmission in prisons and preventing TB among released prisoners should be considered to successfully control TB in the general population. Effective responses will require improving TB diagnostic capacity in prisons, implementing active case detection strategies, such as annual mass screening, testing for latent TB and provision of isoniazid preventive therapy, and developing transitional care and follow-up programs for prisoners released into the community.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a413be25-ae40-4556-bb7b-8aadc064b3b4", "title": "Oropouche Virus–Associated Aseptic Meningoencephalitis, Southeastern Brazil", "text": "【0】Oropouche Virus–Associated Aseptic Meningoencephalitis, Southeastern Brazil\nIn April 2016, a 28-year-old man (S.V.) in Ribeirão Preto, southeastern Brazil, with a history of Gilbert syndrome sought care for a sudden high fever, severe 1-sided headache, vomiting, intense photophobia, stiff neck, and confusion. Seventeen days earlier, he had returned from a trip to Mosqueiro Island in northern Brazil; he received yellow fever vaccine 5 days before traveling. His 1-week visit to the island included outdoor activities that involved entrances into the native Amazon rainforest. The patient returned to Ribeirão Preto and remained asymptomatic for 10 days before becoming acutely ill with fever, chills, myalgia, headache, and dizziness. Symptoms occurred intermittently for 3 days, which prompted him to seek medical care. After evaluation, a diagnosis of dengue fever was considered. Microhematocrit was 54%, and tourniquet test was negative. Nonstructural protein (NS) 1 antigen detection test was requested, and the patient was treated with medications for his symptoms and prescribed abundant oral hydration and bed rest. Dengue fever was ruled out because NS1 antigen detection and IgM serologic testing provided negative results 6 days after initial symptom onset. Therefore, he was referred to an infectious disease outpatient clinic at another healthcare facility for further investigation. Yellow fever vaccine reaction and malaria were initially the main hypotheses, but thick blood film examination results were negative.\n\n【1】Seven days after initial symptom onset, the patient’s headache worsened and became left-sided and pulsatile; intense photophobia, vomiting, and fever (105.8°F) developed. No hemorrhagic manifestations were observed. He was admitted to the emergency department at the Teaching Hospital in Ribeirão Preto with nuchal rigidity; ceftriaxone was initiated.\n\n【2】At admission, he was confused, including attention deficits and hallucinations. Physical examination found tachypnea (27 breaths/min), heart rate 76 beats/min, and blood pressure 130/80 mm Hg; cardiopulmonary examination results were unremarkable, and the liver was palpable at 3 cm under the costal margin (spleen was not palpable) with no pain at abdominal palpation. The patient exhibited mild spasticity of lower limbs with positive Babinski sign, without focalities at neurologic exam. Because of clinical examination findings, viral meningoencephalitis emerged as a possible diagnosis, and intravenous acyclovir was started.\n\n【3】Figure\n\n【4】Figure . Imaging and PCR results for a 28-year-old man with Oropouche virus infection, southeastern Brazil. A) Cerebral computed tomography showing a cortical edema on the left frontal lobe (white arrow). B) Agarose...\n\n【5】Laboratory examination showed complete blood counts within reference ranges (hematocrit 44%; leukocytes 10,300 cells/mm <sup>3 </sup> \\[no left shift; lymphocytes 2,300 cells/mm <sup>3 </sup> \\]; platelets 388,000/mm <sup>3 </sup> ), normal kidney function, and sodium and potassium concentrations within reference ranges. Liver enzymes were slightly elevated (aspartate aminotransferase 30 U/L \\[reference 15–46 U/L\\], alanine aminotransferase 71 U/L \\[reference 13–69 U/L\\], γ-glutamyl transferase 107 U/L \\[7–60 U/L\\]); total bilirubin was 0.73 mg/dL (reference 0.2–1.0 mg/dL). Serologic test results were nonreactive for hepatitis A, B, and C; HIV; syphilis; Epstein-Barr virus; _Toxoplasma gondii_ ; and _Trypanosoma cruzi_ . Thorax radiograph imaging was unremarkable; cerebral computed tomography scan showed cortical edema on the left frontal lobe ( Figure , panel A).\n\n【6】Results of a spinal tap showed a 23 cm H <sub>2 </sub> O opening pressure (reference 5–20 cm H <sub>2 </sub> O) with clear cerebrospinal fluid containing 45 cells/mm <sup>3 </sup> (reference <5 cells/mm <sup>3 </sup> ) (85% lymphocytes); protein concentration of 53 mg/dL (reference <45 mg/dL); and glucose concentration of 62 mg/dL (capillary 100 mg/dL \\[reference 2/3 of capillary glucose\\]). VDRL test results were negative, and bacterial and fungal antigens were not detected. PCR results were negative for enterovirus, herpes viruses 1 and 2, varicella zoster virus, cysticercosis, tuberculosis, and toxoplasmosis but positive for Oropouche virus (OROV) (fraction S) ( _1_ ). Blood sample PCR results were also positive for OROV ( Figure , panel B) ( _2_ ), and indirect immunofluorescence antibody testing ( Appendix ) ( _3_ ) confirmed OROV infection.\n\n【7】After 7 days of hospitalization, the patient recovered all neurologic function with no sequelae (posttreatment electromyography and magnetic resonance imaging findings were unremarkable). He was discharged and has had no additional relapses.\n\n【8】OROV, an RNA virus belonging to the genus _Orthobunyavirus_ , family Peribunyaviridae, is the causative agent of Oropouche fever in humans ( _4_ ). In a sylvatic cycle, OROV can be transmitted to some animals by mosquitoes and in an urban environment can be transmitted to humans by the midge _Culicoides paraensis_ ( _5_ ). Usually, humans become infected in forested areas and then can translocate the virus to an urban environment ( _6_ ).\n\n【9】After a 4- to 8-day incubation period, fever, headache, myalgia, arthralgia, chills, photophobia, dizziness, nausea, and vomiting develop ( _2_ , _6_ ). Less frequently, patients experience rash, anorexia, retro-orbital pain, and general malaise ( _2_ , _6_ , _7_ ). Hemorrhagic phenomena also have been described ( _8_ ). Most patients recover spontaneously after 7 days, although some experience symptoms for as long as 1 month ( _2_ , _6_ , _7_ ). Some cases can relapse after recovery; the clinical picture is similar to the initial onset or can be more severe, including aseptic meningitis ( _9_ , _10_ ). These patients may experience neck stiffness, dizziness, vomiting, lethargy, diplopia, and nystagmus, but prognosis without sequel is usually good ( _10_ ).\n\n【10】There has been an increased concern that Oropouche fever, endemic in northern Brazil, might spread across the country by contiguous urban cycles and by human movement. Physicians working worldwide in areas to which OROV is not endemic should include this neglected disease in the differential diagnosis of acute febrile syndrome, especially in patients visiting high-risk areas for OROV transmission.\n\n【11】Dr. Vernal, the patient in this report, is a Chilean medical doctor working with neglected diseases in Brazil since 2013 and a PhD student in clinical medicine and a medical resident of infectious disease at University of São Paulo. His primary research interests include tropical infectious diseases with special emphasis on leprosy and tegumentary leishmaniasis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1e399da2-9b45-435c-b671-07ac7f62f321", "title": "Reconstruction of Zika Virus Introduction in Brazil", "text": "【0】Reconstruction of Zika Virus Introduction in Brazil\nAutochthonous transmission of Zika virus has been confirmed in 67 countries worldwide and in 46 countries or territories in the Americas ( _1_ , _2_ ). It is believed that Zika virus was introduced into the Americas through Easter Island in 2014, after an outbreak in French Polynesia ( _3_ , _4_ ). Despite the rapid spread of Zika virus across the Americas and global concerns regarding its effects on fetuses, little is known about the pattern of spread. The risk for local transmission in unaffected regions is unknown but potentially serious where competent Zika virus vectors are present ( _5_ ) and also given the additional complexities of sexual transmission and population mobility ( _3_ , _6_ ).\n\n【1】Knowledge of the direction and speed of movement of a disease is invaluable for public health response planning, including timing and placement of interventions. We estimated the speed of Zika virus spread in Brazil by using data on confirmed cases of Zika virus disease at the municipal level and applying an approach used in estimating the speed of Ebola spread across parts of West Africa ( _7_ ).\n\n【2】### The Study\n\n【3】Confirmed cases of Zika virus disease were obtained from the Brazil Ministry of Health. Additional reports were also extracted from ProMED mail ( _8_ ) and HealthMap ( _9_ ). We performed the analysis by using 3 dates: 1) date of case registration in the surveillance system of the Brazilian Ministry of Health (model 1); 2) earliest of either date of symptom onset (if available) or registration date (model 2); and 3) earliest of either case registration date, date of symptom onset, or date of case report by other sources (model 3). Surface trend analysis was used to interpolated a continuous estimate of disease spread speed in magnitude and direction ( _10_ ) by using available spatial and temporal information. Time of dispersal was calculated from the start of the epidemic for each model ( Technical Appendix ).\n\n【4】Figure 1\n\n【5】Figure 1 . Contour surface trends and directional vectors for reconstructing Zika introduction in Brazil. A) Date of case registration (model 1); B) earliest date between date of symptom onset (if available) and date...\n\n【6】Data provided by the Brazilian Ministry of Health on May 31, 2016, indicated that Zika had been confirmed in 316 of 5,564 municipalities in 26 states; 6 additional municipalities were identified from other reporting sources. Contour maps of interpolated temporal trends ( Figure 1 ) indicate a trend of spread into southern and western Brazil, and initial outbreak reports originated from municipalities along the northeastern coast. On the basis of confirmed cases, the earliest location of spread was the northeastern coastal area between the states of Paraíba, Ceará, Bahía, Alagoas, and Rio Grande do Norte. There were also earlier dates of self-reported symptom onset in the northwestern state of Amazonas (January 1, 2015), the west-central state of Matto Grosso (January 4, 2015), and the southeastern coastal state of Rio de Janeiro (January 1, 2015).\n\n【7】Contour maps ( Figure 1 ) indicate slight differences in patterns of dispersion between the models. Model 1 indicates the strongest trend of a southward spread from the northeastern coast toward the populous southeastern coastal states of Rio de Janeiro, Espírito Santo, and São Paulo; the estimated time of dispersal was 22 weeks ( Figure 1 , panel A). In addition to west to east spread of Zika in southern Brazil, there was a pattern of movement west toward Bolivia.\n\n【8】The dispersal trend for model 2 was more varied but also indicated spread to the southeastern coastal states of Rio de Janeiro, Espírito Santo, and São Paulo ( Figure 1 , panel B). This model also suggests an initial spread north from the earliest reports in the northeastern region and a spread west toward Bolivia. The model estimates a north to south diffusion of ≈27 weeks. Model 3 suggests a strong southward spread originating from the northeastern coast toward the southeastern coastal states (approximate dispersal time of 29 weeks) and toward the western border and northwestern state of Amazonas ( Figure 1 , panel C).\n\n【9】Figure 2\n\n【10】Figure 2 . Speed or log speed (km/d) of Zika introduction into municipalities in Brazil. A) June 2015–May 2016; B) January 2015–May 2016; C) January 2015–May 2016. Municipalities are classified by region. Gray circles...\n\n【11】Overall, the average speed of diffusion was 42.1 km/day or 15,367 km/year. The minimum speed across all 3 models was 6.9 km/day, and the maximum speed was 634.1 km/day ( Figure 2 ). Municipalities in northeastern and northern regions had the slowest speeds, and municipalities in the west-central and southeastern regions had the highest speeds. This finding was caused by proximity of cases in time and space. More cases occurred closer in time and over larger areas in southern, southeastern, and west-central regions, which resulted in faster rates of case introduction.\n\n【12】All models were consistent in agreement that Zika dispersal in Brazil followed a general pattern of southward spread toward the populous coastal states (average speed of introduction of 42 km/day), which could be explained by multiple introductory cases into different areas probably caused by movement of viremic persons. We estimate that it took ≈5–6 months for Zika to spread from the northeastern coast to the southeastern coast and western border of Brazil. These findings are supported by the first report of local transmission of Zika virus in Paraguay in late November 2015 ( _11_ ) and in Bolivia in January 2016 ( _12_ ), 7 months after the first registered case in Brazil.\n\n【13】Limitations of this analysis include quality and timeliness of surveillance data that provided the basis for this study. Symptom onset date is subject to error because it is based on self-report, and earlier introductions of Zika in some municipalities might not have been captured by the Ministry of Health surveillance system and supplementary data sources, given the mild and generic nature of Zika symptoms and the high proportion of asymptomatic persons ( _3_ ). The northern region of Brazil had a major dengue outbreak in early 2015, and given symptom similarities between dengue and Zika, it is probable that some suspected dengue cases were in fact early cases of Zika.\n\n【14】Sporadic geographically disparate cases were recorded in various parts of Brazil, which increased the uncertainty associated with speed analysis. These cases, such as those in northwestern Brazil, increased uncertainty in direction and speed estimates, which are also related to edge effects. Edge effects occurred along the boundary of the study area, which in this study were constructed by using fewer data points and are therefore less stable. This effect is shown with directional arrows pointing toward earlier areas of spread versus toward later areas of spread ( Figure 1 , panels B, C).\n\n【15】### Conclusions\n\n【16】The arrival and rapid spread of Zika virus in the Americas resembles that of chikungunya virus, which was introduced into Saint Martin in the Caribbean in 2013 ( _13_ , _14_ ). Increased knowledge of the speed of spread and direction of Zika spread can help in understanding its possible future directions and pace at which it travels, which would be essential for targeted mosquito control interventions, public health messages, and travel advisories. Future work will investigate underlying causes for the southward and westward spread in Brazil by incorporating mobility data and seasonal events, such as movement of persons between northeastern and southeastern regions for vacations, which could have driven the spatial transmission pattern. Furthermore, multicountry analysis is needed to understand continental spatial and temporal patterns of dispersion of Zika virus and co-circulating viruses, such as chikungunya virus.\n\n【17】Dr. Zinszer is a postdoctoral fellow in the Informatics Program, Boston Children’s Hospital, Boston, Massachusetts. Her primary research interests include understanding patterns and determinants of emergence and spread of infectious diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71ca1c59-3e30-468d-8064-b912451937dd", "title": "Main Routes of Entry and Genomic Diversity of SARS-CoV-2, Uganda", "text": "【0】Main Routes of Entry and Genomic Diversity of SARS-CoV-2, Uganda\nSevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) ( _1_ , _2_ ), the cause of coronavirus disease (COVID-19), has been spreading globally since it was first reported in Wuhan, China, on December 30, 2019 ( _3_ , _4_ ), infecting >10 million persons and causing massive disruption of daily lives and substantial economic consequences ( _5_ ). Given the expanding pandemic and the absence of effective vaccines and antiviral drugs, the best strategy to control the spread of SARS-CoV-2 might be testing, contact tracing, and quarantining. Early implementation of diagnostic testing enables contact tracing and quarantining to reduce transmission in the community and can protect limited healthcare resources.\n\n【1】The importation of SARS-CoV-2 into Africa was inevitable given the volume of air travel and movement of tourists, traders, and workers between countries. We document COVID-19 outbreak preparedness and response in Uganda, a landlocked country in East Africa with entry by international flight or overland from bordering countries. The experience in Uganda provides a unique opportunity to follow virus transmission when early strong interventions are applied. We describe the importation of COVID-19 into Uganda and SARS-CoV-2 genomic data acquired from local sequencing efforts.\n\n【2】### The Study\n\n【3】Figure 1\n\n【4】Figure 1 . International flight routes of imported cases (colored lines) and the 4 main points of land entry into Uganda from Kenya, Tanzania, and South Sudan (colored dots).\n\n【5】Africa’s first case COVID-19 was recorded in Egypt on February 14, 2020 ( _6_ ), and as of June 30, a total of 52 countries in Africa had reported cases. In anticipation of COVID-19 entry into Africa, the Uganda Virus Research Institute (UVRI) established SARS-CoV-2 diagnostics capacity in early February. The screening of all international arrivals and quarantine of suspected case-patients began March 19. The first COVID-19 case was detected in a returning traveler on March 21. Immediately after this first case was identified, a ban on international passenger flights was implemented on March 22, followed by a ban on local travel and public gatherings on March 27. After public health officials recognized that international truck drivers arriving with cargo from neighboring countries (primarily Kenya and Tanzania) posed a risk for virus importation, testing of truck drivers was initiated on April 13 at main border entry points ( Figure 1 ) ( https://www.health.go.ug/category/events-and-updates/page/4 ), and as of May 18, entry into Uganda required a negative SARS-CoV-2 test. A timeline shows various measures of public health preparedness and response, including testing activity, the total number of cases in Uganda, cases among truck drivers, and important intervention dates ( Appendix Figure 1).\n\n【6】As of June 30, public health officials in Uganda had detected >1,500 cases in the country or at points of entry and had conducted >150,000 diagnostics tests. Approximately 2,000 tests per day have been performed at UVRI, which is designated as a Center of Excellence for Evaluation of COVID-19 Diagnostics by the Africa Centres for Disease Control and Prevention, by using real-time reverse transcription PCR assays on respiratory swabs samples from suspected case-patients ( _7_ ). To facilitate virus tracing, we established local sequencing capacity to determine full viral genome sequences from confirmed COVID-19 case-patients.\n\n【7】We report 20 SARS-CoV-2 genomic sequences from Uganda, obtained from 14 persons arriving from regions with circulating SARS-CoV-2 and 6 truck drivers screened at Uganda points-of-entry ( Table ; Figure 1 ). This study was approved by the UVRI Research and Ethics Committee (approval no. 00001354, study reference no. GC/127/20/04/771).\n\n【8】Figure 2\n\n【9】Figure 2 . Maximum-likelihood phylogenetic tree of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomes in Uganda. The full SARS-CoV-2 genomes used for phylogenetic lineage nomenclature (A. Rambaut et al., unpub. data, https://doi.org/10.1101/2020.04.17.046086 )...\n\n【10】We compared the 20 SARS-CoV-2 genomes detected in Uganda with genomes detected globally. The Uganda genomes belonged to phylogenetic lineages A, B, B.1, B.1.1, B.1.1.1, and B.4, among which lineage B.1 has the largest number of sequences that have spread to >20 countries in Europe, the Americas, Asia, and Australia ( https://github.com/hCoV-2019/lineages ). Genome UG001 (from a traveler arriving from the United States), genomes UG002 and UG012 (from travelers arriving from Dubai), and genome UG017 (from a truck driver from Tanzania) fall within SARS-CoV-2 lineage A (A. Rambaut et al., unpub. data, https://doi.org/10.1101/2020.04.17.046086 ), with the nearest known genomes occurring in Asia, Australia, Kenya, and the United States ( Figure 2 ). Genome UG011 was from a contact of a Uganda case-patient and is most related to USA/WA-UW-1948 and UnitedArabEmirates/L068 strains within lineage B.4 ( Figure 2 ). Genomes UG004, UG007, UG008, and UG010 were detected in a group of travelers returning from the United Kingdom; these genomes fall within lineage B.1.1.1, which included other United Kingdom–derived genomes ( Figure 2 ). Also in this lineage is genome UG014, detected in a traveler returning from Dubai. Additional sequences from a traveling group (UG005 and UG006) were assigned to lineage B, whereas UG003 (assigned to lineage B.1.1) and UG009 (assigned to lineage B.1.1.1) were closely related to the lineage B.1.1.1, containing genomes from the traveling group in whom genomes UG004, UG007, UG008, and UG010 were detected. Genome UG013 (from a traveler returning from Dubai) belonged to lineage B and was closely related to strains from Asia and Kenya. SARS-CoV-2 genomes identified from returning travelers from Dubai belonged to different lineages (UG002 and UG012 of lineage A, UG013 of lineage B, and UG014 of lineage B.1.1.1), suggesting these travelers contracted the virus from multiple sources despite sharing similar travel routes.\n\n【11】In addition to air traffic, another means of SARS-CoV-2 entry into Uganda is with drivers of cargo trucks entering the country through 4 main entry points from Kenya, Tanzania, and South Sudan ( Figure 1 ). All 4 genomes from truck drivers from Kenya belonged to lineage B.1, whereas genomes from truck drivers from Tanzania belonged to lineage A and B.1 ( Table ). The truck driver viral genomes did not cluster closely with any current local Uganda genomes, suggesting that these truck drivers contracted the virus outside Uganda, although the sample size is too small for firm conclusions. Careful monitoring and additional sequence data from truck driver and community cases will enable an estimate of the amount of transmission that might occur between truck drivers and the general population of Uganda.\n\n【12】An indication of the current SARS-CoV-2 genomic sequence diversity ( Appendix Figure 2) is the single nucleotide changes from the original Wuhan-1 strain (GenBank accession no. NC\\_045512). The Uganda strains differ at 5–20 positions across the ≈30 kb genome, including a small number of changes in the spike protein–coding region, which is a main target for vaccines. The spike protein showed 1 polymorphism with the lineage A viruses (including 4 Uganda virus sequences), encoding D614, whereas all other clades encoded G614 in the spike protein.\n\n【13】### Conclusions\n\n【14】We describe the initial SARS-CoV-2 genomes imported into Uganda. We observed 6 lineages among 20 genomes, which were imported through returning air travelers and truck drivers entering Uganda. We shared all sequences with the public health community by depositing in the GISAID public database ( https://www.gisaid.org , accession nos. EPI\\_ISL\\_451183–202) ( _8_ ).\n\n【15】Since the governmental ban on international flights was implemented in the last week of March, no further imported COVID-19 cases from international air travelers into Uganda have been reported, underscoring the effectiveness of these policy measures. However, the increasing detection of SARS-CoV-2 in apparently healthy truck drivers is concerning. The quantity of viral RNA levels in some truck driver samples is high (cycle threshold values 16–19), yet these persons were still capable of driving a truck, indicating mild symptoms. This combination of high viral levels and sufficient health to continue normal activities could lead to further spread of the virus within the community without effective quarantine measures. The current efforts to increase community testing and truck drivers contact tracing and quarantine are essential to identify new cases and prevent further spread of the virus in Uganda.\n\n【16】Mr. Lule Bugembe is a scientist at the UK Medical Research Council–Uganda Virus Research Institute and London School of Hygiene and Tropical Medicine Uganda Research Unit in Entebbe. His primary research interests include the use of bioinformatics and computational analysis of human host and pathogen genetic data to predict infectious disease trends and help with their control.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "942f5e59-f504-400e-9810-f1f23a805061", "title": "Acute Encephalitis Hospitalizations, California, 1990–1999: Unrecognized Arboviral Encephalitis?", "text": "【0】Acute Encephalitis Hospitalizations, California, 1990–1999: Unrecognized Arboviral Encephalitis?\nEncephalitis, or inflammation of the brain, is a serious clinical syndrome with many infectious, postinfectious, and postimmunization potential causes ( _1_ , _2_ ). Recognized causes of infectious encephalitis in humans include, but are not limited to, herpes simplex viruses, arboviruses, lymphocytic choriomeningitis, mumps, cytomegalovirus, Epstein-Barr virus, human herpesvirus 6, and enteroviruses. The epidemiology of encephalitis in the United States is characterized by the predominance of cases with unknown origin ( _3_ – _6_ ).\n\n【1】Historically, Western equine encephalomyelitis (WEE) and St. Louis encephalitis (SLE) viruses (WEEV and SLEV, respectively) were important causes of encephalitis in California residents, particularly in the Central Valley and southern California ( _7_ ). Since the 1960s, the incidence of WEE and SLE has decreased dramatically, although sporadic cases are still reported ( _7_ ). Most recently, two small SLE epidemics were reported in Los Angeles and the Central Valley in 1984 and 1989, respectively ( _8_ , _9_ ). Reports of arboviral encephalitis cases are uncommon in southern California, despite evidence of endemic WEEV and SLEV activity in birds and mosquito vectors in that area ( _10_ ). Sylvatic West Nile virus (WNV) activity was recently detected for the first time in California, which may contribute to changes in the epidemiology of central nervous system disease ( _11_ ). The 1999 appearance of WNV in New York and adjacent states produced illness and death among humans, horses, and several avian species ( _12_ ).\n\n【2】Healthcare providers and diagnostic laboratories in California are required to report human encephalitis cases to the California Department of Health Services (CDHS) under Title 17 of the California Code of Regulations. The code stipulates that the reporter identify the cause as viral, bacterial, fungal, or parasitic. Because this surveillance system is passive, human encephalitis cases may be underreported, even when the cause of encephalitis is identified ( _5_ ). In our study, hospital discharge data were used to estimate the incidence of acute encephalitis and to provide a basis for comparison with the number of reported cases of encephalitis. In addition, the encephalitis hospitalization rates of districts with differing levels of sylvatic arboviral activity were compared. These data may provide a useful baseline to evaluate trends in encephalitis hospitalizations in California, including unusual occurrences of arboviral encephalitis. Such baseline data may prove useful given the recent detection of WNV in California and the potential for introduction of other arboviral agents.\n\n【3】### Materials and Methods\n\n【4】##### Data Sources\n\n【5】Hospital discharge data (public use version A) include information on approximately 3.5 million yearly discharges from all California hospitals that serve the civilian population; federal facilities or state hospitals for patients with mental disorders or developmental disabilities are excluded ( _13_ ). The data do not contain patient names or other personal identifiers. Patients discharged from acute-care hospitals in California from 1990 through 1999 were the source population for the present study. Patients with acute infectious or unspecified encephalitis as the principal diagnosis or one of the 24 additional diagnoses were selected by using the International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM codes listed in Table 1 \\[ _14_ \\]). Only data on the first hospitalization of patients with more than one encephalitis-related hospitalization were included in the analysis. When available, the record linkage number, based on an encrypted Social Security number, was used to identify patients with multiple hospitalizations. Patients with no record linkage number were assumed to have been hospitalized only once.\n\n【6】The annual number of reported encephalitis cases from 1990 through 1999 was obtained from CDHS, Division of Communicable Disease Control. Arboviral surveillance data were obtained from published reports ( _15_ – _17_ ).\n\n【7】##### Data Analysis\n\n【8】The average age- and sex-specific incidence rates of encephalitis (cases per 10 <sup>5 </sup> person-years) were calculated by using the California population projections for 1995 ( _18_ ). These estimates were adjusted to account for the proportion of cases with missing demographic information by assuming that the proportion with missing data is the same within each subgroup. A 95% confidence interval was calculated for each group-specific rate. Such rates were compared by using a chi-square test for proportions. Annual incidence rates were calculated by using annual population estimates for California ( _19_ ). Linear trends in proportions were evaluated by using a chi-square test for trend ( _20_ ). Significance probabilities <0.05 (p values) were considered a strong indication of systematic influence (i.e., not chance variation).\n\n【9】Encephalitis hospitalization rates were evaluated separately for Sacramento and Yolo, Sutter and Yuba, and Riverside and Imperial Counties. These counties had a sufficient population size to allow calculation of meaningful rates and had reported sylvatic arboviral activity during the study period. The number of encephalitis cases was insufficient to calculate county-specific rates for each category of encephalitis and therefore the rates are for all encephalitis hospitalizations. Data were combined for Sacramento-Yolo and Sutter-Yuba to reflect the collection of arboviral surveillance data by a single bi-county agency for each of these districts, and for Riverside-Imperial to reflect their proximity and similarity in sylvatic arboviral activity. Temporal trends in encephalitis rates from 1991 through 1999 were examined in conjunction with arboviral surveillance data on sentinel chicken seroconversions to WEEV and SLEV and on virus-positive mosquito pools. The county-specific encephalitis rates for years with increased sylvatic arboviral activity were compared with the average rate for the remaining years by using a two-sample test for equality of proportions. The county of residence for patients hospitalized in 1990 was not available, precluding the inclusion of data from that year in the county-level analyses. Analyses were conducted with EpiInfo 6 (version 6.04d; Centers for Disease Control and Prevention, Atlanta, GA), SAS (version 8; SAS Institute, Cary, NC), and S-Plus 2000 (Professional Release 3; MathSoft, Inc., Seattle, WA).\n\n【10】### Results\n\n【11】Figure 1\n\n【12】Figure 1 . Cumulative incidence of encephalitis hospitalizations in California, 1990–1999 (n = 17,318).\n\n【13】From 1990 through 1999, a total of 17,318 patients were hospitalized with acute encephalitis; 3,511 (20.3%) had a concurrent diagnosis of AIDS. The proportion of encephalitis patients per year with AIDS decreased from 27% in 1990 to 9.5% in 1999 (chi-square test for trend = 475.9, p < 0.001). Annual rates for patients with and without AIDS are shown in Figure 1 . All subsequent analyses were limited to the 13,807 patients without a concurrent diagnosis of AIDS because of the distinct epidemiologic characteristics of these two populations.\n\n【14】Unspecified encephalitis made up most of the encephalitis diagnoses (55.7%), followed by specified viral encephalitis (not arboviral) (17.6%) and “other” causes of encephalitis (16.6%) ( Table 1 ). Arthropodborne viral disease constituted <1% of the encephalitis diagnoses, with a total of 83 diagnoses among 82 patients. Some patients had more than one ICD-9-CM code for encephalitis. Thus, the total number of diagnoses was greater than the number of patients.\n\n【15】The encephalitis rate was highest in infants (<1 year old), followed by persons \\> 65 years of age ( Table 2 ). The lowest rate was in persons 20–44 years of age. The chi-square test for proportions indicated that the difference in the rates between each age group was significant. Female patients had a rate that was significantly higher than that of male patients.\n\n【16】Figure 2\n\n【17】Figure 2 . Comparison of hospitalized versus reported encephalitis in California, 1990–1999. Hospitalized patients with a concurrent diagnosis of AIDS were excluded.\n\n【18】A comparison of the annual number of patients hospitalized with encephalitis with the number of encephalitis cases reported to CDHS is shown in Figure 2 . On average, the number of patients hospitalized with acute infectious or unspecified encephalitis was 10-fold higher than the number of encephalitis cases reported to CDHS.\n\n【19】The epidemiology of encephalitis in specific counties with sylvatic arboviral activity over the study period was further examined to evaluate the potential role of undiagnosed arboviral encephalitis. Most patients hospitalized with encephalitis in Sacramento-Yolo, Sutter-Yuba, and Imperial-Riverside Counties were diagnosed with unspecified encephalitis ( Table 3 ). Only 10 patients were diagnosed with arthropodborne viral encephalitis, 2 from Sacramento-Yolo, 1 from Sutter-Yuba, and 7 from Riverside-Imperial. The number of admissions per quarter was distributed fairly evenly for each bi-county area when the data from 1991 through 1999 were combined ( Table 3 ). In no case did the proportion of hospitalizations for any given quarter differ significantly from the null value of 25% (one-sample test for proportions: p > 0.05).\n\n【20】Figure 3\n\n【21】Figure 3 . Annual rate of encephalitis hospitalizations and annual number of sentinel chicken seroconversions to Western equine encephalomyelitis, Sacramento and Yolo Counties, California, 1991–1999. Hospitalized patients with a concurrent diagnosis of AIDS were...\n\n【22】Figure 4\n\n【23】Figure 4 . Annual rate of encephalitis hospitalizations and annual number of sentinel chicken seroconversions to Western equine encephalomyelitis, Sutter and Yuba Counties, California, 1991–1999. Hospitalized patients with a concurrent diagnosis of AIDS were...\n\n【24】Figure 5\n\n【25】Figure 5 . Annual rate of encephalitis hospitalizations and annual number of sentinel chicken seroconversions, Imperial and Riverside Counties, California, 1991–1999. Hospitalized patients with a concurrent diagnosis of AIDS were excluded. SLEV, St. Louis...\n\n【26】Annual encephalitis rates are shown for Sacramento-Yolo ( Figure 3 ), Sutter-Yuba ( Figure 4 ), and Imperial-Riverside ( Figure 5 ) Counties from 1991 through 1999. The encephalitis rates in these areas increased during some years when increased arboviral activity was detected in sentinel chickens flocks and mosquito pools ( Table 4 ). For instance, the encephalitis rate in Sutter-Yuba increased in 1997 (two-sample test for proportions: p = 0.041), when 41 sentinel chickens seroconverted to WEEV. A smaller increase in the encephalitis rate was observed in Sacramento-Yolo in 1996 and 1997 (two-sample test for proportions: p = 0.028), when 20 and 18 sentinel chickens seroconverted to WEEV, respectively. In contrast, the encephalitis rates did not increase in Sacramento-Yolo and Sutter-Yuba in 1993, when increased WEEV activity was detected in sentinel chickens and mosquito pools. In Riverside-Imperial, a small increase in the encephalitis rate in 1991 (two-sample test for proportions, p = 0.004) corresponded with an increase in detected WEEV and SLEV activity in sentinel chickens and mosquito pools. The proportion of encephalitis hospital admissions was higher than expected (>25%) during the summer (July–September) in Sutter-Yuba in 1997 (9/13 \\[69.2%\\]) and in Imperial-Riverside in 1991 (48/161 \\[29.8%\\]) (one-sample test for proportions: p = 0.008 and p = 0.187, respectively). No such increase occurred in Sacramento-Yolo in 1997 (18/76 \\[23.7%\\]).\n\n【27】### Discussion\n\n【28】The encephalitis rate showed an overall decrease during the study period. The high proportion of patients with unspecified encephalitis in this study ( Table 1 ) is consistent with findings from other studies and raises questions about potential causes of these encephalitis cases ( _3_ , _5_ ). Arboviral encephalitis was diagnosed in <1% of patients hospitalized with acute encephalitis from 1990 through 1999. This finding indicates that this type of encephalitis is either exceedingly rare in California or underdiagnosed. In the absence of public health alerts during periods of epizootic arboviral activity, clinicians may be disinclined to pursue laboratory testing for arboviral agents because of a low index of suspicion. Furthermore, outside of academic interest, clinicians may not have much incentive to request laboratory testing for specific agents for patients with viral encephalitis if a specific diagnosis will not change the course of treatment.\n\n【29】ICD-9-CM codes used in the hospital discharge database provide a standardized means of comparing data between hospitals. A previous study used ICD-9-CM codes in the National Hospital Discharge Survey to describe the epidemiology of encephalitis ( _4_ ). In both studies, the age-specific encephalitis rates were highest among infants (<1 year) and the elderly ( \\> 65 years) ( Table 2 ). This finding may be due in part to infection with herpes simplex, which is a common cause of nonepidemic, acute encephalitis that occurs most frequently in children and the elderly ( _6_ ). Patients with AIDS, 92.5% of whom were men, were excluded in the present study, which likely resulted in a higher proportion of females compared to the national study.\n\n【30】An advantage of using hospital discharge data to study the epidemiology of encephalitis is that most patients with encephalitis are likely to be hospitalized because of the severity of the illness. Accordingly, these findings are more readily generalized to the population of California, unlike the passive surveillance data, which are limited by underreporting ( _21_ ). In the present study, the annual number of hospitalized encephalitis patients was approximately 10-fold greater than the annual number of reported cases ( Figure 2 ). The actual degree of underreporting may be less, as not all of the hospitalizations for encephalitis may have been due to reportable causes. However, evidence exists that arboviral encephalitis was underreported in California from 1990 through 1999, with 82 patients hospitalized with arthropodborne viral encephalitis but only 7 arboviral encephalitis cases reported. Encephalitis cases with an unspecified cause may also be disproportionately unreported, since encephalitis cases reported under the current passive surveillance system request that the reporter specify the cause as viral, bacterial, fungal, or parasitic.\n\n【31】A disadvantage of relying on the ICD-9-CM codes to describe the epidemiology of encephalitis is the lack of patient identifiers. This fact may have resulted in multiple hospitalizations for individual patients being included, as evidenced by the number of hospitalizations for poliomyelitis and rabies ( Table 1 ). Another disadvantage is the lack of information on laboratory test results used to make diagnoses. While the high proportion of unspecified encephalitis cases in the present study possibly resulted from underuse of appropriate diagnostic tests, other studies do not support this hypothesis. For instance, from 1956 through 1958, a total of 1,595 encephalitis patients were identified in Kern County through active hospital-based surveillance and evaluated by using a standard battery of tests ( _5_ ). No cause was identified for 569 (36%) patients, and WEE and SLE accounted for < 5% of cases per year. When advanced diagnostic methods were used, the cause of encephalitis was identified for only 126 (38%) of 334 patients referred to the California Encephalitis Program from June 1998 through December 2000; no patients with arboviral encephalitis were identified ( _3_ ). These findings raise the possibility that current diagnostic tests may simply be inadequate for identifying all possible causes of encephalitis.\n\n【32】Given the limited number of encephalitis hospitalizations in any given county, all encephalitis diagnoses were combined to provide a meaningful examination of county-specific trends. Combining these diagnoses may have obscured trends in specific disease agents, although most of the patients in Sacramento-Yolo, Sutter-Yuba, and Riverside-Imperial had a diagnosis of unspecified encephalitis ( Table 3 ). In Sacramento-Yolo and Sutter-Yuba, concurrent increases in sylvatic WEEV transmission and in the rates of encephalitis hospitalizations in 1997 occurred ( Table 4 ; Figures 3 and 4 ), increasing the likelihood that a proportion of the unspecified encephalitis cases may have been due to arboviral encephalitis. A similar pattern was observed in Imperial and Riverside Counties in 1991, when levels of sylvatic WEEV and SLEV transmission were particularly high ( Table 4 and Figure 5 ). The proportion of hospital admissions for encephalitis in Sutter-Yuba in 1997 and Imperial-Riverside in 1991 increased during the summer months, when arboviral and enteroviral transmission most commonly occur ( _1_ ). In contrast, statewide hospital admissions for encephalitis were significantly higher during the winter months, which indicates that arboviral encephalitis is typically an unimportant contributor to encephalitis hospitalizations. A study of encephalitis patients in California from 1956 through 1958 also found an unexplained increase in the proportion of cases with undetermined origin during the winter ( _5_ ). One possible cause, lymphocytic choriomeningitis, occurs most commonly in the winter, although it is thought to be rare ( _1_ ).\n\n【33】The arboviral surveillance data used in the present study lacked denominator data on mosquito pools and sentinel chicken specimens tested for most years. In addition, mosquito pool and sentinel chicken surveillance is not uniform across mosquito control districts. In spite of these limitations, notable increases in arboviral activity in mosquito pools ( Table 4 ) and sentinel chickens ( Figures 3 – 5 ) were observed in some years. Many potential reasons exist for the lack of consistent correlation between sylvatic arboviral activity and encephalitis rates. One possibility is that mosquito population indices, sylvatic arboviral transmission levels, or both, were not always sufficient to increase the risk for human infection. For instance, _Culex tarsalis_ population indices are correlated with sentinel chickens seroconversion rates for WEEV and SLEV ( _22_ ). A retrospective study of a 1989 SLE epizootic in the Central Valley, with high _Cx. tarsalis_ abundance, 70 virus-positive mosquito pools, and seroconversion of 71% of sentinel chickens, identified 28 (43%) of 65 aseptic meningitis and encephalitis patients as SLE patients ( _9_ , _23_ ). In the present study, the encephalitis rate in Imperial-Riverside increased slightly in 1991, when WEEV and SLEV activity was detected in many mosquito pools and sentinel chickens, but remained relatively unchanged in 1994 and 1995, when many sentinel chickens seroconverted but few mosquito pools were virus-positive. Another possible contributor to the study findings is variation in the virulence of circulating arboviruses over the study period. Three phenotypes of WEEV, which differed in their virulence properties in adult mice, were isolated from mosquito pools collected in California from 1991 through 1995 ( _24_ ). Lastly, sylvatic arboviral activity and encephalitis rates may not be correlated. For instance, no encephalitis cases were detected during an intense WNV epizootic in Connecticut ( _25_ ). In fact, an aseptic meningitis epidemic attributable to enteroviruses was detected during an avian epizootic of WNV, while no WNV meningitis cases were detected ( _26_ ).\n\n【34】Many factors could explain the observed decrease in the incidence of clinical WEE and SLE cases since the 1960s. Mosquito control and water management programs have been effective at reducing mosquito vector populations ( _27_ ). Changes in human behavior may also have coincided with the decrease in the incidence of arboviral illness ( _28_ ). With the advent of television and air-conditioning, people are more likely to remain indoors during twilight hours, when peak feeding by vector species takes place. Earlier research showed rural residents to be at higher risk for arboviral illness than urban residents ( _29_ ). With changes in land use over the past century, a greater proportion of the human population now resides in urban and suburban settings. A 1995 California study of outpatients attending county health department clinics found a significantly higher seroprevalence of WEEV among residents of rural Imperial and Sutter Counties than Sacramento County residents; the seroprevalence for SLEV was significantly higher in Imperial than in both Sacramento and Sutter Counties ( _30_ ). However, although this study was conducted in areas with both sporadic and enzootic WEEV and SLEV transmission, the overall seroprevalence levels for both viruses were low.\n\n【35】The methods used in the present study are useful for evaluating trends in the incidence of emerging or potentially emerging diseases. The epidemiology of arboviral encephalitis will likely change with the establishment of WNV in California, making active hospital-based surveillance for arboviral disease an important supplement to traditional passive reporting. These findings suggest that unrecognized arboviral encephalitis has not constituted a large proportion of the unspecified encephalitis patients who were hospitalized from 1991 through 1999. However, the study results do indicate the potential utility of intensified surveillance efforts during periods of increased sylvatic arboviral activity. During such periods, implementing active hospital-based surveillance for encephalitis, acute flaccid paralysis, and aseptic meningitis, with collection and testing of diagnostic specimens, may result in detecting cases of arboviral disease that would otherwise go undiagnosed and unreported.\n\n【36】Dr. Trevejo recently completed her Ph.D. in epidemiology at the School of Public Health, University of California, Berkeley, where she evaluated multiple aspects of the arboviral surveillance system in California. Her research interests include the epidemiology of vector-borne diseases and the role of the human-animal interface in disease transmission. She is an assistant professor of epidemiology and public health with the Western University College of Veterinary Medicine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
