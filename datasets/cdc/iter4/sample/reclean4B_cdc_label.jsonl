{"seq_id": "d559b015-1981-4898-aea3-b6c5d3a479d1", "title": "Limited and Short-Lasting Virus Neutralizing Titers Induced by Inactivated SARS-CoV-2 Vaccine", "text": "【0】Limited and Short-Lasting Virus Neutralizing Titers Induced by Inactivated SARS-CoV-2 Vaccine\nCirculation of novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants capable of evading vaccine-derived protection is challenging the efficacy of coronavirus disease (COVID-19) vaccines . The inactivated SARS-CoV-2 vaccine CoronaVac , 1 of 2 COVID-19 vaccines licensed in Thailand, has been widely administered to health care workers. Clinical studies show CoronaVac efficacy against symptomatic COVID-19 ranging from 51% (Brazil) to 65.9% (Chile) and 100% against severe illness and illness requiring hospitalization . However, data on CoronaVac efficacy against variants of concern are very limited. Our study was approved by the Research Ethics Review Committee, Faculty of Medicine, Chulalongkorn University (Bangkok, Thailand) and recorded in the Thai Clinical Trial Registry **(** TCTR20210325003). Investigators adhered to U.S. Department of Defense AR 70–25 policies for protection of human subjects.\n\n【1】For this study, we enrolled 207 health care workers in Thailand who were fully vaccinated with 2 doses of CoronaVac (0.5 mL/dose, 2–4 wk between doses); all had received their first dose during February 22–March 12, 2021. Median age was 39 (interquartile range 30–51) years of age; 103 (49.6%) were men. Among study participants, 58 (28%) provided blood samples only at baseline (when the first dose was administered), 93 (44.0%) both at baseline and 2–3 weeks after the second dose, and 56 (27.0%) at baseline and at 2–3 weeks and 10–12 weeks after the second dose. Using an in vitro system , we evaluated the ability of the serum of CoronaVac recipients to neutralize SARS-CoV-2. We measured circulating serum neutralizing antibodies to the original wild-type strain by using a cPass receptor binding domain antigen-based surrogate virus neutralization test (sVNT) ELISA   and using a microneutralization assay (MNA)  for SARS-CoV-2 Wild-type strain and Alpha, Beta, and Delta neutralizing antibodies. Seroconversion rates for CoronaVac-vaccinated participants, determined by sVNT ELISA using 30% inhibition as cutoff, were 85.2% (78.2% mean inhibition level) at 2–3 weeks and 35% (25.4% mean inhibition level) at 10–12 weeks. The MNA seropositivity cutoff was set at ≥50%.\n\n【2】At 2–3 weeks after the second dose, 61.1% (91/149) of participants were seropositive against the Wild-type strain, 35.6% (53/149) against Alpha variant, 3.4% (5/149) against Beta, and 8.7% (13/149) against Delta . Mean neutralizing rate at 2–3 weeks was 49.3% (95% CI 44.9%–53.6%) against Wild-type strain, 40.9% (95% CI 37.8%–43.9%) against Alpha variant, 9.0% (95% CI 6.1%–11.8%) against Beta, and 10.8% (95% CI 7.1%–14.5%) against Delta. At 10–12 weeks after the second dose, the proportion of seropositive participants fell to 50% (28/56) against Wild-type strain and was significantly reduced (p < 0.001) to 17.9% (10/56) against Alpha variant, 1.8% (1/56) against Beta, and 1.8% (1/56) against Delta. Mean neutralizing rates at 10–12 weeks were 48.0% (95% CI 39.9%–56.1%) against Wild-type strain, 21.8% (95% CI 37.8%–43.9%) against Alpha variant, 1.2% (95% CI 3.5%–8.8%) against Beta, and 1.0% (95% CI 2.9%–7.5%) against Delta.\n\n【3】Comparing sVNT ELISA results between the 2 time points, Wild-type strain antibodies appear to have a half-life of 83.4 days (95% CI 76.6–90.3 days). However, when the MNA was used, neutralizing antibodies waned in a time- and variant-dependent manner. The half-life of neutralizing antibodies was as low as 47.2 days (95% CI 37.5–56.9 days) for Wild-type strain, 38.6 days (95% CI 31.2–45.9 days) for Alpha variant, 6.9 days (95% CI 3.2–10.6 days) for Beta, and 12.3 days (95% CI 6.8–17.8 days) for Delta . These data indicate the possibility that SARS-CoV-2 variants are able to escape humoral induced by wild-type prototype inactivated vaccines, which is consistent with results of other recent studies . Our findings support administering vaccine boosters, especially where these variants circulate.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2ad69ebd-2143-40e7-b8cf-4e7dd1a81036", "title": "Infections among Contacts of Patients with Nipah Virus, India", "text": "【0】Infections among Contacts of Patients with Nipah Virus, India\n**To the Editor:** Kumar et al. recently reported 3 asymptomatic, seropositive persons with Nipah virus (NiV) among 279 contacts of 18 NiV-infected patients . In the 1998–1999 NiV outbreak in Malaysia, asymptomatic, seropositive persons, most of whom were farm workers and soldiers involved in pig culling, also were identified . Furthermore, some additional cases might not have been detected among patients who had mild, nonencephalitic symptoms (e.g. fever, influenza-like illness). Up to 16% of asymptomatic, seropositive persons exhibited lesions in their brain magnetic resonance imaging (MRI) scans, albeit a slightly smaller number than in patients with acute NiV encephalitis . These discrete, small high-signal lesions in the cerebrum were best seen on fluid-attenuated inversion recovery images . In another study, a seropositive nurse who was exposed only to patients also had similar brain lesions visible on an MRI scan, suggesting person-to-person transmission . Whether late-onset NiV encephalitis (i.e. encephalitis in an asymptomatic or mildly symptomatic person) would develop in any of these persons, especially those with brain lesions visible on MRI scan, is unknown. In another cohort, approximately 5% of asymptomatic or mildly symptomatic persons  had febrile encephalitis characterized by headache, seizures, focal neurologic signs, and cerebrospinal fluid pleocytosis, a set of symptoms and signs distinct from acute NiV encephalitis. In patients who survived acute NiV encephalitis, a clinicopathologically similar relapsing encephalitis has also been reported . The death rate was ≈20% during the clinical course of late-onset/relapsing NiV encephalitis .\n\n【1】Physicians should consider using brain MRI to identify subclinical brain lesions in asymptomatic, seropositive Nipah patients. Moreover, physicians should advise patients of the possibility that late-onset encephalitis might occur months or even years after virus exposure . New treatments are in development or on trial, so patients with these complications may be offered access to effective treatments.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "620f7830-f8ae-4f02-b971-f5b6de92f8f2", "title": "Norovirus Outbreaks from Drinking Water", "text": "【0】Norovirus Outbreaks from Drinking Water\nWater can be a source of disease outbreaks . Contamination takes place almost exclusively by sewage that contains enteric pathogens, and enteric viruses that affect humans are mostly species-specific; their abundance may be explained by high concentrations in the stool of patients. Noroviruses (previously called Norwalk-like viruses) cause gastroenteritis in all age groups. Since noroviruses, unlike enteroviruses, do not easily grow in cell culture, their role became evident only in the 1990s, when specific diagnostic methods became available. Only in recent years has the vast genomic variety of noroviruses become apparent . A recent report  lists 5 genogroups and 22 genetic clusters that include mostly human but also porcine and murine viruses.\n\n【1】In addition to numerous community-based outbreaks, in which transmission is thought to take place from person to person, outbreaks caused by contaminated food have been frequent . The dominant role of noroviruses in foodborne and waterborne outbreaks has been estimated by Mead et al. Several waterborne outbreaks have been detected on the basis of epidemiologic evidence , and only in 1997 did the first report of noroviruses in well water appear . The genome-based diagnostic procedure, i.e. reverse transcriptase–polymerase chain reaction (RT-PCR), offers a sensitive and specific tool to identify these viruses. Sequence-based identification is effective for source-tracking outbreaks, especially those caused by noroviruses, which show a highly variable nucleotide sequence even within the short amplicon produced in the polymerase region of the virus .\n\n【2】Waterborne viral outbreaks are often difficult to recognize. Illness caused by norovirus is common, and if the contamination level is low, the number of cases remains low. A rather extensive outbreak is usually required for medical personnel and authorities to recognize water as a possible source of infection . This report includes virologic analyses of Finnish waterborne outbreaks during a 6-year period. We describe an improved procedure to identify water as the source of viral outbreaks.\n\n【3】### Methods\n\n【4】Reporting of foodborne and waterborne outbreaks in Finland was reorganized and intensified in 1997; new regulations emphasized that all suspected cases should be immediately reported to the National Public Health Institute (KTL). Recommendations were given for properly collecting both patient and environmental samples. The functions of local outbreak investigation teams were clarified and included training in conducting epidemiologic surveys. Laboratory performance was improved by including options for viral and protozoan diagnostics from both patient and environmental samples. All cases in which water was suspected as the source of the outbreak were reported to KTL. Sampling recommendations included 3–10 representative patient stool samples. Water samples, raw water, and when appropriate, tap water from different parts of the distribution network were collected immediately. Despite recommendations, not all outbreaks were investigated for viruses. The criteria for establishing an outbreak as waterborne were according to the English classification (grades A–D) .\n\n【5】In total, 271 patient samples from 25 outbreaks were analyzed for viruses. The range of fecal samples obtained from each outbreak was 2–69 (mean 11). A 10% fecal suspension in 0.05 mol/L Tris-HCl, 0.1 mol/L NaCl, 1 mmol/L CaCl 2  , pH 7.4, was used for RNA extraction.\n\n【6】A total of 73 water samples from 27 outbreaks were analyzed; 1- to 2-L water samples, collected in clean glass or plastic bottles, were concentrated as described by Gilgen et al. The 1-L samples were run through a positively charged disk membrane filter (diameter 47 mm, pore size 45 μm; AMF-Cuno, Zetapor, Meriden, CO, USA) with or without a fiberglass prefilter. After the elution step in 50 mmol/L glycine buffer, pH 9.5, containing 1% beef extract, the eluate was rapidly neutralized with HCl. The volume was further reduced to ≈100 μL with a microconcentrator (Centricon-100, Amicon, Beverly, MA, USA). This sample was used for RNA extraction and PCR as described .\n\n【7】RNA extraction and RT-PCR for the norovirus polymerase region were performed as described . Briefly, RNA was extracted by using phenol- and guanidine thiocyanate–containing Tripure reagent (Roche, Indianapolis, IN, USA) and precipitated with ethanol. Viral RNA was transcribed to cDNA, and DNA amplification was performed in separate tubes for norovirus genogroups I and II (GI and GII) by manual PCR with primers Nvp110  and N69 , and Nvp110 and NI , respectively. From 2002 on, the forward primers for the genogroups were modified as KA1 (5´-GANGGCCTSCCMTCWGGNTT-3´) and KA2 (5´-TGGAATTCNATHGCCCAYTGG-3´). The amplicons were visualized by electrophoresis in an agarose gel, hybridized by a probe panel, and used for nucleotide sequencing.\n\n【8】Sequencing was performed manually (Sequenase, version 2.0 DNA sequencing kit, USB, Cleveland, OH, USA) as described . Sequence analysis was performed by programs SeqApp and ClustalW. Our sequences were aligned with the following EMBL/GenBank noroviruses: Southampton/91/UK (L07418), Norwalk/68/US (M87661), Malta (AJ277616), Melksham/94/UK (X81879), Hawaii/76/US (U07611), Lordsdale/93/UK (X86557), GIIb (AY7732101), GIId (AF312728), and murine norovirus (AY228235). For nucleotide sequences for Hillingdon/94/UK and Grimsby/95/UK, see Vinje et al ; sequence of Lord Harris comes from the sequence database of the European network . GenBank accession numbers for nucleotide sequences of this study are AY958213–9 for GI and AY958204–12 for GII noroviruses.\n\n【9】### Results\n\n【10】##### Description of Outbreaks and Viral Findings\n\n【11】In total, 41 waterborne outbreaks (3–11 per year) were registered in Finland from 1998 to 2003. Of these, 28 (61%) were investigated for viruses. In 24 outbreaks both water and patient samples were available for analysis; in 3 outbreaks only water was available, and in 1 outbreak only patient samples were available for analysis. Samples for viral analysis were not obtained from the remaining 13 outbreaks. Analysis was performed by RT-PCR. Patient samples were also screened by electron microscopy for other enteric viruses and analyzed by RT-PCR for astroviruses. For water samples, a concentration method according to Gilgen et al. was established, starting from the volume of 1 L. In most cases, water samples were analyzed only for noroviruses. The most prominent viruses that caused waterborne outbreaks were noroviruses (18 outbreaks). Rotavirus caused 1 waterborne outbreak, and no viruses were found in 9 epidemics. Bacterial findings will be published elsewhere.\n\n【12】The 18 waterborne norovirus outbreaks are summarized in Table 1 . In every year except 2001, several norovirus outbreaks occurred in Finland. During the study period, 6 large norovirus epidemics with >200 cases were encountered. In the largest epidemics, >10,000 persons were exposed, and 2,000–5,500 cases occurred; in addition, 7 medium-sized (40–100 cases) and 5 small outbreaks (<20 cases) were caused by noroviruses.\n\n【13】Most norovirus contaminations occurred in groundwater systems, which are used most commonly in Finland. In 3 instances, surface, lake, or river water was used. Of the ground water epidemics, 8 occurred in public communal systems and 7 in private ground water wells. Typically rental cottages or different kinds of camping grounds with their own wells were affected.\n\n【14】The geographic distribution of the waterborne norovirus outbreaks is shown in Figure 1 . Outbreaks occurred all over the country, from the southern archipelago to the northernmost parts of Finland. Seasonal risk for waterborne norovirus outbreak seemed to be approximately equal . Half (20 of 41) of the waterborne epidemics occurred in summer, and norovirus outbreaks (11 of 15) were most common in late winter to spring (February–May). In fact most outbreaks in winter were caused by noroviruses, while in summer they were mainly caused by bacteria.\n\n【15】##### Detailed Analysis of Noroviruses\n\n【16】Noroviruses from 16 outbreaks (E1–E16) were further characterized by sequence analysis of amplicons, from which the genotype was also deduced . Noroviruses appeared in the patient samples in all 16 outbreaks and in water samples of 10 epidemics. Coliforms were also present in 9 epidemics, whereas in 7 outbreaks, no indication of microbiologic contamination was seen. Most outbreaks were caused by a single norovirus strain/genotype (11 epidemics); >1 virus was found more often in large outbreaks than in small ones. GII noroviruses were only slightly more common than GI (7 vs. 5 outbreaks). Of the 10 epidemics with positive water samples, equal numbers of GI and GII genotypes were detected.\n\n【17】In all but 1 of these outbreaks, the same norovirus genotype found in water samples also appeared in patient samples. The only exception was epidemic E11, in which 2 norovirus sequences, GII.1 and GII.4, were detected in the water sample, but only type GII.4 was detected in the patient samples. Not only the viral genotype but also the entire amplicon sequence were identical in each outbreak . Two norovirus genogroup I types, GI.3 (Birmingham) and GI.6 (Sindlesham, Hesse), were found; 1 GI sequence (outbreak E3) remained undetermined.\n\n【18】In the GII outbreaks, at least 4 different genotypes were found in patient or water samples. The most common genotype was GII.4 (Bristol, Lordsdale), found in water samples of 4 epidemics, and beginning in 2003, it was the new variant type . The established genotypes GII.1 (Hawaii) and GII.5 (Hillingdon) were also detected in some outbreaks, along with some potentially new genotypes or sequences that did not cluster well in any of the established genotypes, such as GIId (Upinniemi) and GIIb. As Figure 3 shows, in most outbreaks a virus with a unique amplicon sequence was recovered, even when it belonged to the same genotype as viruses in the other waterborne outbreaks. Norovirus genotype GII.4 was the only exception, and a longer nucleotide sequence likely would have shown some genetic differences (detected between sequences of epidemics E1 and E11; data not shown).\n\n【19】### Discussion\n\n【20】As part of the improved and intensified outbreak surveillance system in Finland, we have identified waterborne viral outbreaks since 1998. In a relatively brief period, during which norovirus diagnostics have been available for patient as well as environmental samples, a considerable number of waterborne norovirus outbreaks have been detected.\n\n【21】That Finland has >1,300 water treatment plants may in part explain the numerous outbreaks. Many of these plants still use surface water (lakes or rivers) as raw water. Inadequate disinfection is then the most common reason for waterborne epidemics, as was the case in outbreak E1 . At risk also are water plants that use groundwater and no disinfection. In Finland, snow melts in spring while the ground is still frozen, which leads to surface runoffs and flooding. Breaks in sewer lines in the vicinity of a well caused several large waterborne outbreaks. Poor sewage disposal also caused many small waterborne outbreaks in private homes or rental cottages.\n\n【22】The large number of genetically distinct norovirus genotypes has been advantageous in investigating waterborne epidemics. Although the short amplicon sequence does not definitively show that 2 viruses are identical, for the purpose of source tracing it seems adequate. In this study, a unique viral sequence appeared in most norovirus outbreaks, and viruses from patients and water in a particular outbreak showed identical sequences. The success in most outbreaks in identifying a norovirus with the same sequence from patients and water may be due to the fact that the outbreaks have taken place in small communities. In large waterborne outbreaks, usually >1 norovirus strain and often other viruses and microbes are causative agents.\n\n【23】Both norovirus genogroups occurred in waterborne epidemics. In a 5-year study  in Finland, GII outbreaks clearly outnumbered those caused by GI noroviruses (86.9% vs. 13.1%) . In waterborne outbreaks, however, nearly half were caused by GI viruses. Some differences may occur in stability as well as ability to spread from person to person among viruses representing different genotypes. Type GI.3, the most common GI genotype in water samples, was also the most frequent GI type in community outbreaks . Viruses of this genotype have caused waterborne outbreaks in the United States in 2001  and in the Netherlands .\n\n【24】As might be expected, keeping in mind its ubiquity , the GII.4 genotype was present in several waterborne outbreaks, and in Finland it has been the most frequent genotype in all outbreaks. The GII.4 new variant emerged in Finland in June 2002, and in the following year 2 waterborne outbreaks were caused by this new variant . Another emerging genotype, GIIb, found in Finland in 2001, a year later than in southern parts of Europe, was a causative agent in a waterborne outbreak in 2002. A waterborne outbreak in Sweden caused by this genotype has recently been reported .\n\n【25】Environmental virology of human pathogen detection has a rather limited history. A classic case is the monitoring of polioviruses in sewage . This method, based on a cell culture technique, is sensitive in detecting circulating wild poliovirus. Further efforts in environmental virology were lacking for many years, mainly because suitable methods were absent. Only after gene amplification techniques were introduced could a tool be developed to successfully detect norovirus in environmental samples . In recent years, an increasing number of reports have described waterborne norovirus outbreaks through contaminated drinking or recreational water .\n\n【26】National recommendations for volumes of water to be tested vary between tens and hundreds of liters. Such volumes pose a serious practical problem for the testing laboratory. For viral detection by RT-PCR, a smaller volume (1 L) is preferred, as suggested by Gilgen et al . Independent of the concentration method, the increase in RT-PCR inhibitors usually sets limits on the water concentration. Sensitive methods are needed to detect viruses in environmental samples. Recent reports on the applicability and sensitivity of real-time RT-PCR  for noroviruses also offer new possibilities to enhance its sensitivity. Another factor is that the test then becomes more rapid, which is essential in monitoring water quality, particularly in epidemic situations. The third advantage is that a quantitative estimate of the contamination level is obtained.\n\n【27】Microbial risks from water are recognized, with much emphasis on risk assessment . Assessment of water, however, depends on indicator organisms, such as coliforms or enterococci, whose survival in water is shorter than that of enteric viruses, especially norovirus and hepatitis A virus. Therefore, viruses can easily be harbored in \"microbiologically immaculate\" water . In situations in which a well is contaminated by sewage, coliforms are nearly always found. When sewage is released into lake water that serves as raw water downstream, indicator organisms may no longer be detectable, but noroviruses can still be present and cause illness. This sequence of events probably led to the first outbreak we examined (E1) .\n\n【28】When water plants use surface water, the contamination may be short-lived and may have vanished by the time the outbreak is detected. A \"rolling sample\" system might be used in which samples are collected in water plants at risk for contamination at regular intervals (e.g. daily, weekly) and stored at 4°C. Unless signs of an outbreak appear, the samples can be discarded at the same pace that new ones are collected. In case of contamination, water samples would be available for analyses.\n\n【29】The evidence presented here together with several recent reports mentioned above show the role of viruses as contaminants of drinking water. In Finland, the finding that noroviruses frequently cause waterborne outbreaks has led to authorities' increased awareness of viral risks. As a consequence, laboratory techniques have been improved, and the capacity for analyzing environmental samples, especially water, has increased. Legislative measures for viral monitoring as part of the microbial risk assessment in drinking water production should be seriously considered.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8cf4726f-9d25-4dc3-9f6c-50fee8c0600f", "title": "Susceptibilities of Nonhuman Primates to Chronic Wasting Disease", "text": "【0】Susceptibilities of Nonhuman Primates to Chronic Wasting Disease\nTransmissible spongiform encephalopathies (TSEs), or prion diseases, are neurodegenerative diseases that affect many mammalian species. Some examples include bovine spongiform encephalopathy (BSE) in cattle, scrapie in sheep and goats, Creutzfeldt-Jakob disease (CJD) in humans, and chronic wasting disease (CWD) in cervids. CWD was first found in captive deer in Colorado in 1967  and was later identified in several US states and Canadian provinces . Epidemiologic evidence suggests that CWD continues to spread among cervid populations in North America , creating concern that CWD may cross species barriers to infect humans or domestic animals that may be eaten by humans. Thus, the host range of CWD and the level of protection provided by species barriers should be determined.\n\n【1】Substantial progress has been made in testing species barriers for CWD by using transgenic mice expressing species-specific prion protein (PrP), by direct infection into new species, or by in vitro conversion assays. The most sensitive method for testing susceptibility to TSE agents is intracerebral injection. Unfortunately, this route does not mimic most natural situations and only enables assessment of whether the possibility of transmission exists. Hamir et al. infected cattle and sheep with CWD by the intracerebral route and found protease-resistant PrP (PrPres) in 5 of 13 cattle and 2 of 8 sheep, which indicated that these ruminant species can propagate CWD . However, oral exposure in these hosts apparently does not cause disease .\n\n【2】CWD cross-species transmission to nonagricultural and laboratory animals has shown variable levels of susceptibility depending on the route of transmission. For example, ferrets were 100% susceptible to CWD by intracerebral infection but were not susceptible to oral infection . Mink were only 25% susceptible to CWD by intracerebral infection and were not susceptible to oral infection . CWD has been successfully transmitted and adapted to laboratory rodents, including hamsters, transgenic mice expressing hamster PrP, and transgenic mice overexpressing mouse PrP . In contrast, transgenic mice expressing human PrP were not susceptible to CWD by intracerebral infection , a finding that provided evidence for a human species barrier against CWD infection. However, work started in 1980 and published in 2005 by Marsh et al. showed that 2 squirrel monkeys ( _Saimiri sciureus_ ) infected by the intracerebral route with brain homogenate from a single CWD-affected mule deer became clinically sick at 31 and 34 months postinfection, and both were positive for PrPres . This evidence that at least 1 species of nonhuman primate was susceptible to CWD weakened the conclusion that humans may be protected from CWD by a species barrier.\n\n【3】We addressed 4 questions raised by the original observation that squirrel monkeys are susceptible to CWD . First, we compared intracerebral and oral routes of infection. This comparison was of interest because the oral route is likely to be an important natural route of disease transmission, and susceptibility is known to be lower by this route in most models. Second, we compared 2 species of nonhuman primates, cynomolgus macaques ( _Macaca fascicularis_ ) and squirrel monkeys, each of which has previously shown susceptibility to various human prion diseases . However, humans are believed to be evolutionarily closer to cynomolgus macaques than to squirrel monkeys , and cynomolgus macaques may be a more accurate model for a human species barrier. Third, because only 1 CWD source was tested by Marsh et al. we studied 8 different pools of CWD representing wild and captive cervids, including mule deer, white-tailed deer, and elk, from separate regions in the United States. Fourth, we tested the species tropism of CWD agent passaged in squirrel monkeys.\n\n【4】### Materials and Methods\n\n【5】A description of the materials and methods used in this study follows. Additional details are available in the Technical Appendix\n\n【6】##### Animal Research\n\n【7】All monkeys and mice were housed at the Rocky Mountain Laboratories (Hamilton, MT, USA). Experimentation followed protocols approved by the National Institutes of Health Rocky Mountain Laboratories Animal Care and Use Committee.\n\n【8】##### CWD Pools for Infection of Primates\n\n【9】CWD-positive brain homogenates were provided by E.S.W. and M.W.M. Contents of each pool were as follows: MD-1, 6 free-ranging mule deer from Wyoming ; MD-2, 4 captive mule deer from Colorado; MD-3, 28 captive mule deer from Wyoming and Colorado ; WTD-1, 7 captive white-tailed deer from Wyoming and Colorado ; WTD-2, 1 wild white-tailed deer from Wyoming; Elk-1, 2 free-ranging elk from Wyoming ; Elk-2, 6 elk from a South Dakota game farm; and Elk-3, 10 captive elk from Wyoming and Colorado. Normal elk brain was a pool from 2 elk from Montana obtained from Lynn Creekmore of the US Department of Agriculture.\n\n【10】##### Inoculation of Monkeys\n\n【11】For intracerebral injections, squirrel monkeys received either 2 mg or 20 mg brain in a total volume of 200 μL, and cynomolgus macaques received 5 mg in a total volume of 500 μL. Oral doses of 200 mg brain/mL were given on 5 different days at 2–6 day intervals. Squirrel monkeys received 3-mL doses; most macaques received 4-mL doses. The inoculum was given to anesthetized animals through a rubber gastric tube.\n\n【12】##### Inoculation of Transgenic Mice\n\n【13】Brain homogenates diluted in phosphate buffered balanced solution containing 2% fetal bovine serum were inoculated intracerebrally into young adult mice. Volumes were 50 μL.\n\n【14】##### Generation of Transgenic Mice Expressing Human PrP\n\n【15】Mice expressing human PrP (tgRM and tg66) were generated by using a transgene, cosSHa.HumPrP, which was created by ligating the human PrP open reading frame into the cosSHa.Tet vector . The transgene was inoculated into eggs of FVBn–mouse PrP null mice in the laboratories of R.R. (tg66) and L.C. (tgRM). Each line of mice overexpressed human PrP as tested by Western blot with monoclonal antibody 3F4.\n\n【16】##### Analysis of Protease-sensitive PrP and PrPres by Immunoblot\n\n【17】Tissues were prepared by making a 20% (wt/vol) homogenate in 0.01 M Tris buffer, pH 7.4. Samples to be analyzed for protease-sensitive PrP (PrPsen) contained the following protease inhibitors: 10 μmol/L leupeptin, 1 μmol/L pepstatin A, and 1 μg/mL aprotinin. Samples were sonicated for 1 min and centrifuged at 5,000 rpm for 10 min. Supernatants were mixed 1:1 in 2× sample buffer and boiled for 3 min before electrophoresis.\n\n【18】Preparation of samples for PrPres analysis has been described . Removal of carbohydrate residues from PrPres was performed by digestion with peptide-N-glycosidase F .\n\n【19】After electrophoresis, proteins were transferred to Immobilon polyvinylidene difluoride–P membranes (Millipore, Billerica, MA, USA), and PrP bands were detected with antibodies 3F4 (residues 109–112) , D13 (residues 96–106)  (InPro Biotechnology, Inc. South San Francisco, CA, USA), or L42 (residues 145–163) (r-Biopharm, Darmstadt, Germany) . Bands were detected by using enhanced chemiluminescence substrate (GE Healthcare, Piscataway, NJ, USA).\n\n【20】##### Histopathologic and Immunohistochemical Analyses\n\n【21】Routine formalin fixation, embedding, and tissue-sectioning protocols were followed. Tissues were stained with hematoxylin and eosin and analyzed for pathologic changes. Immunohistochemical staining was performed by using an automated Nexus stainer (Ventana, Tucson, AZ, USA). Anti-PrP antibodies D13 and 3F4 were used for PrPres immunostaining as described .\n\n【22】##### Sequencing\n\n【23】Primate genomic DNA was purified from whole blood, and PCR products were amplified by using PuRe Taq Ready-To-Go PCR beads (GE Healthcare). Two primers from the extreme outer ends of the open reading frame, including the previously published forward primer HM-1  with mPrP-780R (5′-TCCCACTATCAGGAAGATGAGG-3′) or a combination of outer primers with internal primers mPrP-397F (5′-CCTTGGTGGCTACATGCTG-3′) and mPrP-416R (5′-CCAGCATGTAGCCACCAAG-3′), were used. Assembly comparisons were made against human, elk, mule deer, cynomolgus macaque, and squirrel monkey by using Sequencher version 4.6 (Gene Codes, Ann Arbor, MI, USA).\n\n【24】### Results\n\n【25】##### Infectivity Levels in CWD Pools\n\n【26】When the 8 pools of CWD (representing both wild and captive deer and elk) used as inocula were analyzed by immunoblot, PrPres in the 8 pools showed similar electrophoretic mobilities and glycoform patterns , but PrPres levels differed when quantitatively compared . To measure the level of infectivity in these pools, we titered each pool in transgenic mice expressing deer PrP (line 33; tgDeerPrP) . A typical endpoint dilution titration is shown in Figure 1 , panel B. The 8 pools had 50% infectious dose (ID 50  ) titers ranging from 6.3 × 10 7  to 5.0 × 10 8  ID 50  /g of brain homogenate . Comparison of titers with PrPres levels showed a partial correlation . For example, the CWD pool with the lowest infectivity titer (MD-2) was also the pool with the lowest PrPres level. However, for some pools, these tests showed discrepant values.\n\n【27】##### Intracerebral Infection of Squirrel Monkeys\n\n【28】To test susceptibility to CWD, we inoculated squirrel monkeys with each of the 8 CWD pools described above. Of 13 squirrel monkeys, 11 became symptomatic (33–53 mo postinfection \\[mpi\\]) . The most consistent and reliable clinical finding was a severe wasting syndrome. Weight loss (average decrease of 33%) was most pronounced in the final few months of infection. Affected monkeys also had rough, poor-quality coats despite continuing to eat and drink. In the final 3–5 weeks, monkeys became weak and less active and spent most of their time hunched at the bottom of their cage. When the monkeys were encouraged to move, they did so slowly and deliberately. In the terminal stage of disease, a few had muscle tremors, excessive salivation, and mild ataxia. Fine, coordinated movement such as eating food was rarely affected. Monkeys were euthanized when terminal-stage weakness and wasting compromised their mobility and ability to eat and drink.\n\n【29】No clear correlation between incubation period and amount of agent inoculated was noted . For example, 3 pairs of monkeys received the same inocula but in amounts that differed by 10-fold (Elk-1, Elk-3, and MD-1). Two pairs that received the lower dose became clinically sick first (Elk-1 and Elk-3). Both members of the third pair (MD-1) were euthanized after 36 months . Two animals received the same dose of WTD-1 pool, yet to date, only 1 animal has become clinically sick. Animals that received the CWD pool with the lowest titer (MD-2) had incubation periods similar to those receiving much higher titered inocula .\n\n【30】In all monkeys with clinical signs, CWD was confirmed by Western blot detection of PrPres in brain . The glycoform pattern of PrPres was similar for all affected monkeys inoculated with different CWD pools . Because PrPres deposition may also occur outside the central nervous system, we also tested peripheral lymphoid tissues. For 3 of 11 monkeys that had PrPres in brain, PrPres was also found in spleen and lymph nodes . In general, PrPres levels were much lower in lymphoid tissues than in brain and were often not detected by Western blot. All nonlymphatic tissues tested (cardiac muscle, skeletal muscle, duodenum, jejunum, ileum, colon, salivary gland, kidney, and lung) were negative for PrPres by immunoblot.\n\n【31】Tissues from squirrel monkeys euthanized after intracerebral injection with CWD  were also examined by histopathologic analysis, including staining with hematoxylin and eosin and immunohistochemical detection of PrPres. All monkeys examined had spongiosis in the cerebral cortex, caudate, putamen, and thalamus . In addition, PrPres deposition was observed in many brain regions with large PrPres-positive plaques in the thalamus, cerebellum, and spinal cord  and in smaller plaques spread out in the gray matter of the internal capsule and white matter of the corpus callosum . The most abundant and consistent location for PrPres staining was found in the frontal cortex and in the fiber tracts of the claustrum . The adjacent caudate had severe spongiosis and astrocytosis but minimal PrPres . PrPres was also detected in lymph nodes and spleen, within follicles, in areas resembling follicular dendritic cells . Immunohistochemical analysis showed no PrPres in heart, kidney, adrenal gland, skeletal muscle, salivary gland, tongue, pancreas, white fat, and all regions of the gastrointestinal tract.\n\n【32】##### Oral Infection of Squirrel Monkeys\n\n【33】To test a more natural route of infection, we exposed squirrel monkeys orally to CWD. Of the 15 exposed squirrel monkeys, 1  was found dead in its cage at 69 mpi; it had shown no neurologic signs or weakness. Western blot results indicated PrPres in brain, spleen, and lymph nodes . The level of PrPres in the brain of monkey 345 was comparable with that in end-stage intracerebrally inoculated monkeys; body weight at necropsy indicated a 33% decrease over the final 10 months. The high levels of PrPres and the severe wasting indicate that CWD infection could have been the cause of death. A second monkey, 303, was euthanized at 69 mpi because of suspicion of TSE after 2 weeks of progressive weakness, wasting, and eventual anorexia. PrPres analysis confirmed PrPres in brain , spleen, and lymph nodes. For monkeys 303 and 345, levels of PrPres in the lymph nodes and spleens were 10–100-fold lower than those in brain.\n\n【34】Two other orally infected monkeys were euthanized during the first 69 mpi . Monkey 301 was euthanized at 39 mpi, after rapid onset of lethargy and anorexia that led to severe dehydration. Results of Western blot analysis for PrPres were negative in brain , spleen, lymph nodes, heart, skeletal muscle, duodenum, jejunum, ileum, colon, salivary gland, kidney, lung, and tonsil. However, immunohistochemical analysis detected PrPres in the spleen and 1 mesenteric lymph node from this monkey, indicating a low level of infection . Monkey 614 was euthanized at 44 mpi because it did not recover from anesthesia related to routine tuberculosis screening. Neither Western blot nor immunohistochemical analysis detected PrPres in brain, spleen, or lymph nodes of this monkey.\n\n【35】##### Infection of Cynomolgus Macaques\n\n【36】We inoculated cynomolgus macaques both orally and intracerebrally with 3 CWD inocula representing elk, mule deer, and white-tailed deer . Of the cynomolgus macaques, 1  was euthanized at 48 mpi after it became aggressive. Brain , spinal cord, spleen, and lymph nodes were negative for PrPres by Western blot and immunohistochemical analysis. All remaining CWD-inoculated cynomolgus monkeys are currently (at 70 mpi) neurologically asymptomatic and have stable or increased body weights.\n\n【37】##### Sequences\n\n【38】Amino acid substitutions in PrP can alter susceptibility to TSE agents, including CWD . To determine whether the lack of susceptibility in several intracerebrally inoculated squirrel monkeys  was caused by PrP gene polymorphisms, we sequenced the PrP genes from 23 squirrel monkeys. When compared with published squirrel monkey sequences , variation was seen at residue 164, in the number of octapeptide repeats, and at residue 19 of the signal peptide . However, these genetic differences in PrP did not appear to account for the lack of susceptibility of monkey 310, which was genotype A, because this genotype was also found in 5 of the CWD-positive monkeys. Because we were not able to sequence PrP of monkey 628, we could not assess the role of PrP variation in the lack of disease.\n\n【39】##### Infectivity of CWD-infected Squirrel Monkey Tissues in PrP Transgenic Mice\n\n【40】To determine whether passage of CWD in squirrel monkeys altered the tropism of the infectious agent, we inoculated tgDeerPrP mice and tg mice expressing human PrP (lines 66 and RM) intracerebrally with tissue homogenates from 3 CWD-positive squirrel monkeys (nos. 322, 308, and 301) with PrPres and from an intracerebrally inoculated cynomolgus macaque . Clinical disease did not develop in any tgDeerPrP, tg66, or tgRM mice during 600–700 days . The lack of transmission to tgDeerPrP mice from the 3 squirrel monkeys with detectable CWD PrPres indicated that either the infectivity levels were low in these squirrel monkeys or that the original cervid species tropism was altered by the passage in squirrel monkeys. Similarly, the lack of transmission to tg mice expressing human PrP implied that passage through squirrel monkeys did not facilitate adaptation to an agent with increased tropism for humans.\n\n【41】### Discussion\n\n【42】As new CWD foci continue to emerge among cervid populations, the risk for CWD transmission to humans needs to be assessed. We used 2 monkey species and 2 routes of inoculation to test the susceptibility of primates to 8 different pools of CWD. To date, we have verified CWD in 11 of 13 intracerebrally inoculated squirrel monkeys; average incubation period was 41 months (range 33–53 months). Using a single CWD pool, Marsh et al. noted infection in 2 of 2 squirrel monkeys 31–34 months after intracerebral inoculation . Intracerebral inoculation of squirrel monkeys with other TSE agents, including agents of kuru, variant CJD, sporadic CJD, and sheep scrapie, had incubation periods of ≈24 months and attack rates of ≈100% . The extended incubation periods and lower attack rates for our squirrel monkeys may result from a partial species barrier to CWD.\n\n【43】The signs of wasting syndrome in CWD-infected monkeys were similar to those of CWD infection in cervids, in which loss of body condition is nearly always a major component of infection and neurologic deficits vary . The correlation of clinical signs between CWD in cervids and squirrel monkeys suggests that CWD might affect a common brain region in each species. We observed PrPres deposition in squirrel monkeys primarily in the frontal lobe of the cerebral cortex, claustrum, putamen, and thalamus. Cervids typically have the most abundant and predictable PrPres in the dorsal motor vagus nucleus (obex), olfactory cortex, and diencephalon (including thalamus, hypothalamus, metathalamus, and epithalamus) . A plausible hypothesis could be that disruption of regions within the hypothalamus and thalamus leads to a metabolic imbalance, resulting in a severe wasting syndrome.\n\n【44】We did not observe a strong correlation between infectivity titer inoculated and attack incidence or incubation period . One potential explanation is that the variation in speed of disease progression might not be relevant given the low number of animals in each group. A second possibility is that our squirrel monkeys varied at PrP alleles that may affect CWD susceptibility. However, analysis of 23 squirrel monkeys showed no PrP sequence differences correlating with susceptibility to CWD . A third possibility is that genes other than the gene for PrP might influence CWD susceptibility.\n\n【45】For humans, eating infected or contaminated tissue is a likely route of CWD exposure. In other animal models, oral transmission of TSE is generally 1,000-fold less effective than direct intracerebral challenge and results in longer incubation periods and lower efficiency of disease transmission. In our oral transmission experiments, we found evidence of CWD infection in 3 monkeys; 2 at 69 mpi had abundant PrPres in brain and lower levels in spleen and lymph nodes, and 1 euthanized at 39 mpi had PrPres in lymphatic tissues only. Thus, transmission seems to be slower by the oral route than by the intracerebral route, and other orally infected monkeys may be affected in the future.\n\n【46】Cynomolgus macaques are evolutionarily closer to humans than are squirrel monkeys . At nearly 6 years postinoculation, no macaques have shown clinical signs of CWD. Intracerebral inoculation of cynomolgus macaques with BSE causes disease in 3 years; human variant CJD requires 2–3 years, and human sporadic CJD requires 5 years . However, oral inoculation of cynomolgus macaques with BSE agent required a minimum of 5 years before clinical disease was observed . Therefore, we cannot rule out CWD transmission to cynomolgus macaques.\n\n【47】The PrP gene sequence can influence cross-species transmission of prion disease. Therefore, we compared squirrel monkey and cynomolgus macaque PrP gene sequences to look for differences that might account for different susceptibilities of these monkeys to CWD. In the PrP gene excluding the signal peptide, deer differed from squirrel monkeys at 17 residues and from cynomolgus macaques at 16 residues, but 14 of these differing residues were identical in squirrel monkeys and macaques . Therefore, there are only 2 residues in cynomolgus macaques (100 and 108) and 3 residues in squirrel monkeys (56, 159 and 182) at which these monkeys differ from deer and also from each other. These residues might play a role in susceptibility differences seen in our study.\n\n【48】Human exposure to CWD-infected cervids in past decades is likely. The highest levels of prion infectivity are present in the central nervous system and lymphatic tissues of CWD-infected cervids; contamination of knives, saws, and muscles with these tissues can easy occur when processing game. Despite the likelihood of exposures, epidemiologic studies of humans living in CWD-endemic areas of Colorado and Wyoming during 1979–2001 have not shown any increases in human TSE cases . Ongoing studies by the Colorado Department of Public Health and Environmental Human Prion Disease Surveillance Program, in conjunction with the University of Colorado School of Medicine, have also concluded that no convincing cases of CWD transmission to humans have been detected in Colorado . However, if CWD in humans appears like a wasting syndrome similar to that observed in the squirrel monkeys in our study, affected persons might receive a diagnosis of a metabolic disorder and never be tested for TSE. Fortunately, additional laboratory data are consistent with the epidemiologic data, and these results support the conclusion that a species barrier protects humans from CWD infection .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ef2c560b-fca9-4729-b0f8-d5f841bd487c", "title": "Investigation and Serologic Follow-Up of Contacts of an Early Confirmed Case-Patient with COVID-19, Washington, USA", "text": "【0】Investigation and Serologic Follow-Up of Contacts of an Early Confirmed Case-Patient with COVID-19, Washington, USA\nIn December 2019, a viral pneumonia outbreak caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emerged in Wuhan, China, before spreading rapidly to other provinces in China and then internationally . On January 20, 2020, the Centers for Disease Control and Prevention (CDC) confirmed a US case of coronavirus disease (COVID-19), the disease caused by SARS-CoV-2, in a traveler who had recently returned to Washington state from Wuhan . We investigated contacts of the confirmed case-patient to describe transmission to inform public health recommendations and control measures.\n\n【1】### Methods\n\n【2】Washington state and local health officials interviewed the case-patient to identify contacts and activities during time of symptom onset until appropriate isolation of the patient. Because contact investigations for COVID-19 had not been conducted in the United States, the Washington State Department of Health (WA DOH), in consultation with CDC, developed contact definitions based on the best evidence available at the time, which are not necessarily consistent with those currently in use . We tailored the contact definitions after the case-patient interview based on the known movement and activities of the case-patient. We categorized contacts into community or healthcare contacts. For this investigation, we defined community contact as any close contact (being within 6 feet of the case-patient) for a prolonged time (>10 minutes); being an office co-worker of the case-patient with close contact of any duration; contact with infectious secretions from the case-patient; or sharing a healthcare waiting room or area during the same time and up to 2 hours after the case-patient was present. Transient community interactions (e.g. grocery store cashiers) were not considered community contacts. Healthcare contact included any face-to-face interaction between healthcare personnel (HCP) and the case-patient without wearing the full personal protective equipment (PPE) that was recommended at the time of the investigation (i.e. gown, gloves, eye protection, and N95 respirator) or potential contact with the case-patient’s secretions by HCP without wearing full PPE. HCP who cared for the patient after patient isolation while wearing the full recommended PPE were monitored but not included in this contact investigation report.\n\n【3】The local health departments actively monitored identified contacts during the 14 days after the last exposure date to the case-patient (i.e. the monitoring period). Active monitoring consisted of daily telephone calls or text messages to ask the contacts whether they had measured fever ( \\> 100.4°F or \\> 38°C) or symptoms including cough, shortness of breath, chills, runny nose, body aches, sore throat, headache, diarrhea, nausea, or vomiting. The local health department supplied thermometers to contacts without a home thermometer. Contacts who developed signs or symptoms during the monitoring period were assessed as persons under investigation (PUIs) for SARS-CoV-2 infection, and nasopharyngeal (NP) and oropharyngeal (OP) swabs were obtained for testing.\n\n【4】Concurrently with mandatory active monitoring, CDC, WA DOH, and the local health departments conducted an enhanced contact investigation. This investigation was implemented after the urgent public health activities of identifying contacts and initiating active monitoring procedures were begun and was voluntary. The enhanced contact investigation included an in-depth interview using a standardized questionnaire and collection of NP and OP specimens and serum samples. We approached all identified community and healthcare contacts by telephone for participation. For contacts who agreed, we conducted the interview by telephone or during in-person household visits, depending on the contact availability at the time of the initial outreach. The questionnaire included demographic characteristics, medical history, and type and duration of the exposure to the case-patient. Exposure types assessed on the questionnaire were “physically within 6 feet of the case-patient,” “had face-to-face interaction,” “had direct physical contact,” and “traveled in the same vehicle, sitting within 6 feet of the case-patient.” For HCP, we also obtained data regarding healthcare-specific exposures and PPE use during the encounter. For all asymptomatic contacts who agreed, we collected NP and OP specimens and serum samples during the monitoring period to test for SARS-CoV-2 by molecular and serologic methods. We tested symptomatic contacts under PUI protocols. We approached participants in the initial enhanced investigation ≈6 weeks after last exposure to the case-patient for collection of follow-up serum samples.\n\n【5】NP and OP swabs were shipped to CDC and tested for SARS-CoV-2 using a real-time reverse transcription PCR (rRT-PCR) assay with 3 targets (N1, N2, and N3), as previously described . The human RNase P gene was used as an internal human gene to confirm RNA quality. Serum samples were tested at CDC using a SARS-CoV-2 ELISA with a recombinant SARS-CoV-2 spike protein (courtesy of Dr. Barney Graham, National Institutes of Health, Bethesda, MD, USA) as an antigen . Protein ELISA 96-well plates were coated with 0.15 μg/mL of recombinant SARS-CoV-2 spike protein and ELISA was carried out as previously described . An optimal cutoff optical density value of 0.4 was determined for >99% specificity and 96% sensitivity . Specimens with total SARS-CoV-2 antibody titers \\> 400 were considered seropositive. Serum samples from the case-patient were used as a positive control and commercially available serum collected before January 2020 from an uninfected person as a negative control.\n\n【6】This public health investigation was determined to be non-research by CDC and WA DOH. Thus, it was not subject to review by either institutional review board.\n\n【7】### Results\n\n【8】The case-patient, a 35-year-old man, was asymptomatic when he returned home to Washington from Wuhan on January 15, 2020 . During the next 2 days, he developed a cough (day 1 of illness) and chills (day 2) while continuing to work in an office setting. He reported feeling feverish on day 3, a weekend day. On day 4, he went to an urgent care clinic, where he was identified as meeting the PUI criteria at the time for COVID-19; NP and OP swabs and serum samples were collected and sent to CDC for SARS-CoV-2 testing . Upon laboratory confirmation as a COVID-19 case-patient (day 5), he was hospitalized for isolation and clinical observation .\n\n【9】The investigation identified 50 contacts of the case-patient while he was symptomatic, including 42 community contacts (11 office co-workers and 31 waiting room contacts at the urgent care clinic) and 8 healthcare contacts . The case-patient lived alone, so he had no household contacts. Among the 50 contacts, the median age was 44 years (range <1–86 years); 25 (50%) were male. All 50 contacts were actively monitored daily. Eight (16%) developed symptoms, including cough (n = 6), headache (n = 5), runny nose (n = 5), sore throat (n = 4), or fever (n = 1), during their monitoring period and were assessed as PUIs; none required hospitalization .\n\n【10】Of the 50 contacts, 38 (76%) participated in the voluntary enhanced contact investigation and completed the standardized questionnaire interview . Among these, 24 (63%) had \\> 1 underlying condition, including asthma (n = 10; 26%), hypertension (n = 7; 18%), type 2 diabetes mellitus (n = 4; 11%), or pregnancy/postpartum (n = 2; 5%) .\n\n【11】Eleven office co-workers were identified as having had close contact ( < 6 feet) with the case-patient. Four (36%) co-workers were exposed over the course of 1 day and 7 (64%) were exposed over the course of 2 days. The duration of close contact ranged from 2 to 90 minutes (<10 minutes, n = 2; 10–15 minutes, n = 3; 60–90 minutes, n = 6). All 11 co-workers reported face-to-face interaction with the case-patient, and 6 (55%) had direct physical contact (e.g. shaking hands, touching shoulder). Although the case-patient admitted symptoms of cough while at the office, no co-workers recalled being within 6 feet of the case-patient while he was coughing. On day 2 of illness, the case-patient attended a 2-hour lunch with 7 co-workers. Three co-workers traveled in the same vehicle as the symptomatic case-patient for a total of 30 minutes on the way to and from lunch.\n\n【12】All 8 HCP had interactions with the case-patient without wearing the full recommended PPE. One HCP was a public health employee who briefly visited the case-patient’s home and had a face-to-face conversation without wearing PPE; however, further details of this HCP’s exposure were not available because the contact did not participate in the enhanced contact investigation. The remaining 7 HCP worked at the urgent care clinic where the patient initially sought care: a receptionist, medical assistant, nurse, physician assistant, radiograph technician, and 2 environmental services (EVS) workers.\n\n【13】All 7 HCP at the urgent care clinic participated in the enhanced contact investigation. Of those, 5 (71%) had face-to-face interaction with the case-patient; the case-patient was wearing a facemask for most encounters. Duration of exposure among HCP ranged from 5 to 25 minutes (<10 minutes, n = 1; 10–15 minutes, n = 3; 15–30 minutes, n = 1). Three HCP had direct physical contact with the case-patient while the HCP was wearing a facemask and no gloves; HCP 1 positioned the case-patient for chest radiograph imaging, HCP 2 took the patient’s vital signs, and HCP 3 examined the case-patient and performed an OP exam. When obtaining the NP and OP swabs, HCP 3 wore an N95 respirator, face shield, and gloves but no gown. HCP 4 had direct physical contact while wearing an N95 respirator and gloves when drawing blood and processing specimens. Neither HCP wearing N95 respirators had been fit tested within the last year. The EVS workers cleaned the clinic >8 hours after the case-patient left the urgent care clinic. While cleaning, 1 EVS worker wore gloves consistently but the other did not.\n\n【14】The 31 waiting room contacts included patients and persons accompanying them who were likely to be in the waiting room at the same time as the case-patient or up to 2 hours after the case-patient was taken to a room. According to the sign-in sheet, ≈7 contacts overlapped in the waiting room with the case-patient. Because the case-patient was anonymous to the waiting room contacts, the 20 (65%) interviewed waiting room contacts were unable to describe their exposure type and duration. However, they consistently described being given a facemask if they had respiratory symptoms, staying >6 feet apart from other waiting room patients, and sitting <30 minutes in the waiting room. The case-patient said that he did not interact directly with any of the waiting room contacts.\n\n【15】Of the 50 contacts, 37 (74%) had NP and OP specimens collected and tested for SARS-CoV-2; 33 (66%) had specimens collected once, and 4 (8%) had specimens collected twice . Of the 8 (16%) contacts who were assessed as PUIs during the investigation, all were tested for SARS-CoV-2 infection while symptomatic. Among asymptomatic contacts, the NP and OP specimens were collected a median of 11 days (range 9–13 days) from the last date of exposure. All NP and OP specimens tested negative for SARS-CoV-2 by rRT-PCR, including those from PUIs. Serum samples were obtained from 32 (64%) of the 50 contacts . Initial serum was obtained from 28 (56%) contacts, and follow-up serum was obtained from 23 (46%) contacts; none had detectable antibodies to SARS-CoV-2.\n\n【16】### Discussion\n\n【17】This investigation identified no secondary cases among close contacts of this early US COVID-19 case-patient by molecular or serologic methods. Systematic contact tracing initiated soon after case confirmation identified office co-workers, HCP, and persons who overlapped in an urgent care clinic waiting area as contacts of the symptomatic case-patient with potential risk for infection. All 50 contacts were actively monitored daily for development of signs or symptoms consistent with COVID-19 and were assessed as PUIs if signs or symptoms developed. During the 14 days after last exposure of the identified contacts, testing of respiratory specimens by rRT-PCR of all symptomatic contacts (PUIs) and most asymptomatic contacts revealed no evidence of secondary transmission. Serum specimens collected ≈6 weeks after the last exposure for 23 (46%) contacts showed no evidence of SARS-CoV-2 antibodies, providing additional confirmation that secondary transmission did not occur among tested contacts.\n\n【18】The lack of transmission among contacts in this investigation is similar to findings reported in other early systematic contact investigations of case-patients with COVID-19 in January and February, in which only close household contacts were infected . One potential explanation for the lack of transmission among tested contacts may be the nature of the community exposures to the case-patient compared with the more intimate and continuous exposures that would typically be experienced by household contacts. Given the current situation of sustained community transmission of SARS-CoV-2, it is of interest that there was no evidence of transmission to co-workers, despite a 60–90 minute lunch together and travel together in a car while the case-patient was symptomatic, albeit with mild symptoms .\n\n【19】Similar to other reported COVID-19 case-patients, this case-patient had SARS-CoV-2 detected from NP and OP specimens at very low cycle threshold values (NP 18–19, OP 21–22) at the time of first testing at the urgent care clinic (day 4 of illness), indicating a high viral load . Of note, the case-patient wore a facemask in the urgent care clinic waiting room and during most of the healthcare encounters, except during examination and respiratory specimen collection. In addition, HCP who interacted with the case-patient all wore partial PPE, which included a facemask or an unfitted N95 respirator. Influenza studies found that patients or HCP wearing a facemask is associated with a reduced risk of nosocomial transmission . A COVID-19 contact investigation described 35 HCP who wore facemasks during a prolonged exposure to an aerosol-generating procedure for a patient with COVID-19; none acquired infection . These investigations suggest that facemasks worn by the case-patient or the HCP might have helped prevent secondary transmission of COVID-19 in the healthcare setting, although additional studies are needed.\n\n【20】A unique aspect of our contact investigation was the inclusion of serologic testing, which strengthened the conclusion that secondary transmission did not occur among tested contacts. Molecular testing detects viral RNA present in an active infection and is dependent on sampling location, technique, and the timing. Although the serologic assay used does not necessarily confirm current infection, it enables detection of seroconversion, indicating a history of SARS-CoV-2 infection. SARS-CoV-2 antibodies are detectable in most persons with COVID-19 within 1–3 weeks after illness onset, although more data are needed to determine whether all persons infected with SARS-CoV-2 develop detectable antibodies . Use of serologic testing complements molecular diagnostics and adds to the ability to detect asymptomatic infections or infections that occurred in persons who did not have testing performed during the acute phase of illness.\n\n【21】This contact investigation had limitations. The investigation involved a single case; thus, only transmissions related to specific interactions for a single case are assessed. The case-patient had no household contacts, and all HCP contacts reported using at least partial PPE. Therefore, these findings cannot be generalized for persons with other types of contacts. Furthermore, not all contacts were tested. Testing was biased toward contacts who knew the case-patient personally (office co-workers) or provided direct care for the case-patient (HCP). Most contacts were tested by rRT-PCR assay at one point during their monitoring period; we cannot exclude that timing of NP and OP swab collection could have affected the ability to capture asymptomatic infection. In addition, details of the exposure history, particularly exposure duration and frequency, are subject to recall bias. Contacts of the case-patient before symptom onset were not included in this investigation, including airplane contacts from the day before the case-patient’s symptom onset. Finally, contact identification could have been incomplete. We cannot rule out the possibility that certain interactions not captured in the contact investigation, including those classified as transient interactions, could have resulted in transmission, although unprotected prolonged exposures described in this report did not result in transmission.\n\n【22】This contact investigation provides detailed exposure information regarding prolonged close interactions among tested contacts that did not result in secondary transmission of SARS-CoV-2. Multiple factors likely influence transmissibility of a given COVID-19 case-patient, including viral load, symptom severity, aerosol generation, host factors in the case-patient and contact, use of protective equipment (e.g. facemask use by the case-patient), and exposure type, timing, duration, setting, and frequency. Further investigations are needed to determine host factors and exposures associated with increased transmission.\n\n【23】Contact investigations coupled with laboratory testing remain crucial public health tools for identifying, isolating, and preventing additional COVID-19 cases. Serologic methods, in addition to molecular detection, are a valuable tool in improving our understanding of the rate of asymptomatic infection. Understanding more about the occurrence of asymptomatic and presymptomatic infection and its contribution to SARS-CoV-2 transmission is critical for guiding community mitigation strategies and infection prevention and control recommendations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "76744747-6ce5-42f6-83a6-2abe76ee40f6", "title": "Effect of Culture-Independent Diagnostic Tests on Future Emerging Infections Program Surveillance", "text": "【0】Effect of Culture-Independent Diagnostic Tests on Future Emerging Infections Program Surveillance\nThe Centers for Disease Control and Prevention (CDC) Emerging Infections Program (EIP) network conducts population- and laboratory-based surveillance for foodborne, health care–associated, respiratory, and invasive bacterial pathogens of public health importance. The main objectives of surveillance are to 1) measure disease burden and monitor disease trends over time, 2) evaluate the impact of public health interventions, 3) track microbiological and molecular characteristics of pathogens, and 4) detect emerging infectious disease threats. EIP data are used for national projections of disease incidence and formulation of national public health policy for prevention and control of disease. Central to accomplishing these objectives is accurate laboratory detection of the pathogens under surveillance.\n\n【1】In the field of microbiology, culture remains the standard for detection of most organisms, but in clinical settings, detection of pathogens is increasingly reliant on culture-independent diagnostic tests (CIDTs). CIDTs include antigen-based tests and molecular tests. The most commonly used molecular tests are the nucleic acid amplification tests, which include PCR. In clinical settings, most CIDTs have several advantages over culture. Foremost, CIDT results can be obtained more rapidly than culture, a feature that can be critical for clinical decision-making. Additionally, CIDTs may require less technical expertise to perform. Although initial adoption of these newer technologies can be expensive, costs generally decline over time, particularly those associated with labor.\n\n【2】CIDTs have the potential to improve estimates of disease burden because 1) they may be more sensitive than culture, 2) their relative ease of use may increase the number of patients tested, 3) they may enable detection of organisms for which there are currently no practical laboratory tests, and 4) they may increase the ability to detect polymicrobial infections. However, incorporating CIDTs into public health surveillance presents several challenges. Interpreting trends in disease incidence can be difficult because of changes to testing practices and surveillance case definitions. Although also true for culture, detection of molecular material may not reflect the presence of a living microbe and true disease, especially when detected from nonsterile body sites. At least for now, it is generally more difficult to assess microbiological and molecular characteristics, such as pathogen subtypes and antimicrobial drug resistance and genotypes, without bacterial isolates. Addressing these and other factors that affect estimates of disease burden and the characterization of infectious pathogens is critical for public health surveillance systems and clinical decision-making. EIP sites have a long history of close collaboration between CDC, state and local public health departments, academia, and clinical laboratories, making them uniquely positioned to help chart the course in addressing these concerns. Because many infections are already being diagnosed by use of CIDTs and more CIDTs will probably be developed and used in the near future, a path for addressing these issues is urgently needed. This article provides an overview of current testing practices for pathogens under EIP surveillance and addresses how EIPs plan to advance their core objectives in the face of this dynamic diagnostic environment.\n\n【3】### Current Status of CIDTs in the EIP Network\n\n【4】CIDTs are either singleplex (i.e. they test for a single organism) or multiplex (i.e. they simultaneously test for multiple organisms). There has been rapid development of multiplex molecular tests that detect pathogens commonly associated with particular syndromes (e.g. respiratory, enteric, and bloodstream infections). CIDTs can be classified into commercial test kits that receive Food and Drug Administration (FDA) clearance or laboratory-developed tests (LDTs). FDA-cleared CIDTs undergo various levels of validation before they are made available for purchase in the United States, but postmarketing evaluations are generally not required . FDA defines LDTs as “in vitro diagnostic tests that are designed, manufactured, and used within a single laboratory.” Laboratories are required to establish test characteristics for LDTs, including accuracy and precision. Historically, FDA has not generally enforced premarket review and other applicable requirements because LDTs were relatively simple and available on a limited basis. Many LDTs are now more complex and are used nationwide. FDA guidance on additional oversight of LDTs is pending.\n\n【5】Many EIPs regularly conduct systematic surveys of clinical, commercial, and public health laboratories to monitor the use of CIDTs in laboratories that provide services to the population under surveillance. These surveys show that the availability and type of CIDTs used varies by pathogen . All or nearly all cases of influenza, _Clostridium difficile, Legionella_ spp. and _Bordetella pertussis_ infection reported through EIP are diagnosed by CIDTs. The percentage diagnosed by a particular type of CIDT has varied over the years. For instance, rapid antigen tests for influenza have been increasingly replaced by FDA-cleared molecular assays , including multiplex assays to detect viruses and bacteria from respiratory specimens  _._ Molecular tests are increasingly being used to detect _C. difficile_ infection. During 2011, ≈50% of _C. difficile_ infections were diagnosed by molecular assays performed at laboratories that serve the EIP population  _._ Also in 2011, for surveillance of _Legionella_ infections, 95% of cases were diagnosed by detection of urine antigen for _L. pneumophila_ serogroup 1 . During the early 1990s, culture and direct fluorescent antibody testing were the primary diagnostic methods used to identify _B. pertussis_ cases reported through the National Notifiable Disease Surveillance System . PCR, either alone or in combination with other diagnostic tests, diagnosed 89% of laboratory-confirmed pertussis cases reported through the EIP Enhanced Pertussis Surveillance system during 2011–2014 .\n\n【6】Culture remains the mainstay for diagnosis of invasive bacterial and fungal infections that cause predominantly bloodstream infections and meningitis, which are covered under EIP Active Bacterial Core surveillance (ABCs) and Healthcare-Associated Infections Community Interface programs . For these pathogens, fulfillment of the surveillance case definition still requires their isolation from a sterile site. Some FDA-cleared multiplex molecular tests for bacterial and fungal bloodstream pathogens are not truly culture independent because they require a positive blood culture from which an organism is identified by PCR  _._ In 2014, ≈10% of laboratories that participate in ABCs used one of these platforms to identify species from positive blood cultures . There are no FDA-cleared molecular tests for directly detecting bacteria from sterile site specimens (e.g. whole blood, cerebrospinal fluid \\[CSF\\]), but there are molecular LDTs that are used to directly detect bacterial pathogens from sterile sites. Less than 1% of ABCs laboratories offer these tests for at least 1 of the ABCs pathogens . There is an FDA-cleared molecular test to detect _Candida_ spp. directly from whole blood  _,_ but this test does not seem to be widely used by clinical laboratories . Nonetheless, multiplex PCR-based tests that detect organisms directly from blood and CSF are under development and may soon become more widely available in clinical settings .\n\n【7】For most pathogens covered under the surveillance system for foodborne pathogens , culture remains the primary means of diagnosis, but this predominance is changing . Antigen-based tests and molecular tests for _Campylobacter_ and Shiga toxin–producing _Escherichia coli_ have been increasingly adopted by EIP laboratories. Adding positive reports from CIDTs for _Campylobacter_ and Shiga-producing _E. coli_ results that are not culture confirmed could add an additional ≈13% and ≈8% cases to FoodNet surveillance, respectively . FDA recently cleared several molecular enteric syndrome panels , which are being rapidly adopted  _._\n\n【8】### Measuring Disease Burden Trends and Evaluating Public Health Interventions\n\n【9】To assess trends and the effect of population-based interventions over time, methods for measuring disease burden should remain relatively stable or adjustments should be made to account for changes in the use of diagnostic tests. The stability of disease burden estimates will be affected by differences in the performance characteristics of tests used, changes in clinical testing practices, and changes to case definitions.\n\n【10】##### Performance Characteristics and Use of Diagnostic Tests\n\n【11】Accurate tests give positive results when infection is present (i.e. the tests are sensitive) and negative results when infection is absent (i.e. the tests are specific). The predictive value of tests depends, in part, on the prevalence of infection in the population and on whether the organism may be present in the absence of disease (i.e. colonizing body sites). Molecular tests for influenza viruses, _C. difficile_ , and _B. pertussis_ have been found to be highly sensitive in clinical settings . The sensitivity of molecular tests for bacteria may be better than that for culture, particularly when antimicrobial drugs have been administered before specimen collection . Highly sensitive molecular tests may produce false-positive results, however, as has been shown in pseudo outbreaks of _B. pertussis_  _._ Molecular mutations in the organism may result in decreased sensitivity for antigen-based tests  and molecular tests . The specificity of CIDTs may be lower than that of culture because molecular targets may be nonspecific for the species of interest . The influenza and _C. difficile_ infection surveillance systems collect data on test method used and adjust national disease estimates on the basis of the sensitivity of the different test types .\n\n【12】The availability of tests; their speed, cost, and ease of use; and other factors (e.g. changes in testing guidelines) may result in changes to clinical testing practices, which may affect disease burden trends. These changes may especially be true for pathogens detected by multiplex panels for which clinical suspicion for an organism does not have to be as high as that for a specific organism. If more persons are tested for multiple organisms, more pathogens might be detected. To account for these potential changes, EIP influenza surveillance periodically collects data on the proportion of patients who are tested for influenza if they have an influenza-like illness and adjusts disease burden estimates on the basis of this information .\n\n【13】##### Case Definitions\n\n【14】The case definitions for EIP pathogens include specific requirements for the laboratory methods used and, for some pathogens, the site from which specimens were obtained . One consideration is whether clinical symptoms should be included in case definitions because detection of an organism may indicate asymptomatic carriage and not true disease . This consideration may especially be relevant for organisms that are detected after sample collection from nonsterile sites and that are known to colonize body sites. In EIP, the only activity that includes clinical symptoms as part of the surveillance case definition is Enhanced Pertussis Surveillance. Another consideration may be collection of specimens from negative controls to determine the likelihood of true infection.\n\n【15】In general, EIP case definitions have been characterized by high specificity and high positive clinical predictive value because most rely on culture from a normally sterile body site. Culturing of samples collected from nonsterile sites (e.g. stool samples) may also be more specific than testing for molecular material. EIP decisions about when and how to change case definitions will probably be specific for each pathogen. Advances in the quality of PCR diagnostics led the Council of State and Territorial Epidemiologists to include validated PCR results obtained from sterile site specimens in the Nationally Notifiable Disease Surveillance System for _Haemophilus influenzae_  and _Neisseria meningitidis_ starting in 2015 . Similarly, campylobacteriosis became nationally notifiable in 2015, and detection of _Campylobacter_ spp. by use of any CIDT would be classified as a “probable” case . Like the Council of State and Territorial Epidemiologists, EIP will need to consider what constitutes a valid test. FDA clearance may be a consideration, but FDA-cleared tests may not perform well in real-world clinical settings. LDTs may undergo rigorous validation that may justify including results from those tests. Presenting incidence data stratified by laboratory method (culture-confirmed and positive CIDT reports), as has been done for FoodNet, may be one way to highlight changes to case definitions .\n\n【16】### Detecting Other Emerging Pathogens\n\n【17】Increased availability and use of CIDTs may increase detection of certain pathogens that were previously hard to identify by culture (particularly those that are part of multiplex panels) or of bacterial pathogens that would otherwise be suppressed by antimicrobial drugs. This increased use may provide the opportunity to conduct surveillance for organisms for which the burden of disease may not have been measured or recognized as emerging infections (e.g. _Mycoplasma pneumoniae,_ respiratory syncytial virus, human metapneumovirus, enteroviruses, enterotoxigenic _E. coli_ ). Additionally, the detection of multiple infectious organisms by multiplex panels could provide insight into polymicrobial interactions and their effect on disease manifestations and severity.\n\n【18】### Analyzing Microbiological and Molecular Characteristics\n\n【19】One of the characteristics that has made EIP surveillance so useful for public health action has been the systematic collection of isolates that enable microbiological and molecular characterization. Serotyping and serogrouping data have been used for developing and evaluating vaccines and for measuring the effectiveness of prevention programs . Isolates collected through EIP have been used to identify outbreaks , monitor and raise awareness of the problem of antimicrobial drug resistance , identify mechanisms of resistance , detect the emergence of new strains  or mutations that may reduce vaccine effectiveness , and identify virulence factors . These isolates have been deposited in national repositories, and streptococcal isolates are widely available to the research communities .\n\n【20】Collection of isolates has been critical for strain characterization by serologic techniques and for in vitro determination of antimicrobial drug susceptibility in EIP reference laboratories. Over time, there has been a shift toward using molecular techniques for strain typing, but both typing and susceptibility testing still rely heavily on the availability of isolates. Although molecular techniques can identify genetic mutations that correlate with phenotypic antimicrobial drug resistance, new mutations may convey the emergence of phenotypes that are not apparent today. Through the CDC Advanced Molecular Detection initiative, EIPs have recently started whole-genome sequencing of EIP isolates . Whole-genome sequencing will be used for pathogen characterization for general surveillance and outbreak detection and for exploring genetic determinants of antimicrobial drug resistance, disease severity, and vaccine failure. Some molecular characterization has been performed directly for _N. meningitis_ in blood and CSF specimens and for _B. pertussis_ in respiratory tract specimens _._ For better characterization of strains without the use of culture, clinical specimens are now collected thorough EIP Enhanced Pertussis Surveillance, as will probably be done for other EIP pathogens in the future. However, much additional research is needed to determine whether and which molecular characteristics can be identified directly from clinical specimens. In the interim, collection of isolates remains essential, as demonstrated by the experience with the _C. difficile_ epidemic in the early 2000s, when CIDT use was widespread for this infection and the emergence of the NAP1 strain was not detected until 5 years after steady increases in _C. difficile_ incidence and severity .\n\n【21】### Future Considerations and Directions\n\n【22】To continue to impact public health programs and policies, EIPs will have to be forward-thinking in how disease burden trends are measured in light of the continued development and uptake of CIDTs . First, EIPs need to continue to systematically monitor the availability and use of these tests through periodic laboratory surveys, either within the EIP network or through coordination with outside organizations and to measure their use in clinical settings. Understanding the use of tests outside of EIP laboratories may also be relevant because some EIPs project estimates of national disease burden. EIPs will also need to develop and regularly evaluate criteria for incorporating CIDTs into case definitions, which will probably vary by pathogen. EIPs can and should contribute to the national discussion about changing case definitions for reportable diseases. Confirmatory testing at public health laboratories may also be necessary for pathogens detected by CIDTs that have questionable performance in real-world settings; however, doing so would require additional public health resources. Performance characteristics need to be determined on an ongoing basis because new variants of organisms that are not detected by the tests may arise. As is already being done for some EIP pathogens, data collection at EIP sites would need to expand to capture information on specific test types and to allow for the reporting of multiple test results. In the era of electronic laboratory reporting, the use of standard test codes that can be transmitted electronically will be essential, and data systems must be able to accommodate more complex data. It will also be critical to perform system checks to avoid counting cases multiple times because >1 testing method may be used for 1 patient. The type of test and the sensitivity and specificity of individual tests and adjustments for changes in testing practices could potentially be incorporated into incidence calculations. After CIDTs have been incorporated into case definitions, EIPs will need to highlight these changes and may consider reporting disease burden by testing method (e.g. cases by culture and molecular testing).\n\n【23】Because CIDTs may obviate the need for culture for making a clinical diagnosis, EIPs must consider short- and long-term strategies for assuring the continued availability of isolates. Isolates remain critical for molecular characterization and antimicrobial drug resistance testing. Resources or legal/regulatory approaches may be needed to give clinical or public health laboratories incentives to continue culturing specimens. It is unlikely that clinical laboratories will be paid by insurers for culture in addition to CIDTs. If providing resources to all laboratories is not possible, the EIP network may have a role in providing sentinel sites for collection of isolates. EIP may also have a role in the development and validation of culture-independent methods for serotyping, subtyping, virulence profiling, and antimicrobial drug resistance testing. EIPs have started using banked isolates for developing whole-genome sequence libraries, which will better characterize pathogens at the molecular level and may make characterization from patient specimens (e.g. whole blood, CSF) easier. In the clinical diagnostic setting, metagenomics (the study of genomes from mixed communities of organisms) may eventually replace organism identification, virulence profiling, and some resistance testing, and it may be possible to use this data stream for a variety of public health purposes, including surveillance. Although whole-genome sequencing and metagenomics hold great promise for characterizing pathogens for surveillance, outbreak detection, and detection of emergence of new pathogens, they also pose challenges for processing, analyzing, and interpreting large amounts of data. Resources are needed to develop and sustain the bioinformatics infrastructure and to make sequences available to genomics reference banks so that EIPs can play a broader role in advancing public health practice.\n\n【24】Perhaps the most challenges and opportunities for surveillance systems are presented by use of multiplex tests. They may enable better tracking organisms that are currently underrecognized because culturing is difficult or because they would not otherwise be considered in the differential diagnosis. It may also enable better tracking of polymicrobial infections. However, understanding when detection equates with true infection is a challenge. The EIP may play a unique role in helping to decipher true infections from mere detection of organisms and in describing true polymicrobial infections because laboratory results can be matched with epidemiologic and clinical data.\n\n【25】### Conclusions\n\n【26】The availability and use of CIDTs in clinical medicine present opportunities to rapidly characterize diseases currently covered under the EIP surveillance umbrella and to detect and monitor other emerging infectious diseases. Their use also presents challenges for maintaining the EIP ability to accurately describe disease burdens, the effect of interventions, and microbiological and molecular characteristics of pathogens over time. Because of the long-standing collaboration between the EIP, laboratories, and disease reporters and resources devoted to collecting highly detailed and comprehensive surveillance data, the EIP infrastructure lends itself to close examination of the effect of CIDTs. EIP hopes to work with other domestic and international public health entities, regulatory bodies, diagnostic manufacturers, and academic and clinical groups to chart an evidence-based course for continuing to incorporate CIDTs into public health surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5f91e075-8cca-40a9-8259-fdf2a85e3c7d", "title": "Associations of Anaplasma phagocytophilum Bacteria Variants in Ixodes scapularis Ticks and Humans, New York, USA", "text": "【0】Associations of Anaplasma phagocytophilum Bacteria Variants in Ixodes scapularis Ticks and Humans, New York, USA\n_Anaplasma phagocytophilum_ is the bacterium capable of causing anaplasmosis . Anaplasmosis symptoms include fever, headache, myalgia, and malaise . Severe illness is reported more commonly in older and immunocompromised patients but is also documented in immunocompetent persons and may result in hospitalization or death if appropriate treatment is not promptly administered . Patient condition generally improves markedly in the 24–48 hours after initiation of treatment with the antimicrobial drug doxycycline .\n\n【1】In eastern North America, _A. phagocytophilum_ is transmitted to humans through the bite of an infected blacklegged tick ( _Ixodes scapularis_ ) . _I. scapularis_ ticks have 3 active life stages (larva, nymph, and adult); nymphs and adults can carry multiple genetic variants of _A. phagocytophilum_ , including the pathogenic human-active variant (Ap-ha) and the nonpathogenic Variant 1 (Ap-V1) . Other genetic variants of _A. phagocytophilum_ have been documented in the northeastern United States; their prevalence in nature and pathogenicity remains understudied . _A. phagocytophilum_ is not transmitted transovarially from infected adult female _I. scapularis_ ticks to larvae and is maintained in the environment within various host species . The most common host reservoir of Ap-ha is the white-footed mouse ( _Peromyscus leucopus_ ) and of Ap-V1 is the white-tailed deer ( _Odocoileus virginianus_ ) .\n\n【2】In New York state, excluding the city of New York, 5,146 anaplasmosis cases were reported during 2010–2018; the median was 454 (range 220–1,112) cases/year. Anaplasmosis incidence increased statewide nearly 4-fold over a period of a decade, from 2.0 cases/100,000 persons in 2010 to 7.6 cases/100,000 persons during 2018; that increase was not spatially homogenous . Specifically, the largest increase in anaplasmosis incidence occurred in the Capital District region of New York, where incidence increased from 3.0/100,000 persons in 2008 to 5.3/100,000 persons in 2018 . Focal increases in incidence of anaplasmosis may result from a change in the abundance and spatial extent of Ap-ha–infected _I. scapularis_ ticks, potentially related to changes in the deer–tick–rodent cycle ; those changes may reflect the relative abundance of Ap-ha competent reservoir hosts, increased contact between host-seeking ticks (unfed ticks of any lifestage actively seeking a host bloodmeal) and small mammal reservoirs of Ap-ha, or a greater overlap between human residential or recreational areas and habitats conducive for the enzootic amplification of Ap-ha. Thus, further examination of the relationship between Ap-ha and Ap-V1 may broaden the understanding of anaplasmosis etiology and _A. phagocytophilum_ ecology, refining risk assessment for this emerging disease and enabling targeted prevention efforts to reduce anaplasmosis incidence.\n\n【3】We analyzed _A. phagocytophilum_ –infected host-seeking _I. scapularis_ tick specimens to elucidate the spatial differences in entomological risk for anaplasmosis in New York. We used a TaqMan single-nucleotide polymorphism (SNP) genotyping assay  to differentiate between the Ap-ha and Ap-V1 variants as previously described . Next, we calculated a measure of human-infection risk as a function of _I. scapularis_ tick density and _A. phagotycophilum_ genotype prevalence, known as the entomological risk index (ERI) for both Ap-ha and Ap-V1.We then tested for spatiotemporal differences in counts of Ap-ha– and Ap-V1–infected _I. scapularis_ ticks by using statistical modeling and tested for correlations between Ap-ha ERI and anaplasmosis incidence. We also used a scan statistic to search for spatiotemporal clusters of Ap-ha and Ap-V1 in New York tick populations for 2008–2020. We then compared spatiotemporal clusters of Ap-ha and Ap-V1 in _I. scapularis_ ticks to documented regions of increased anaplasmosis incidence in New York over the same timeframe . The results help to illuminate the relationship between the spatial ecology of each variant and the outcome of human disease.\n\n【4】### Methods\n\n【5】##### Active Tick Sampling\n\n【6】We collected host-seeking ticks primarily from publicly accessible lands across New York during 2008–2020 using standardized drag-cloth or flag surveys of vegetation and forest leaf litter, as previously described . Generally, we visited 1 site within each county twice annually; we visited additional sites as weather and resources permitted. Collection sites had suitable tick habitat and potential risk for human exposure to ticks. We typically sampled \\> 1,000 m 2  of suitable habitat per site during each collection event. In some instances, we did not find suitable habitat for nymphal and adult _I. scapularis_ tick sampling at the same collection site; as a result, we sampled some sites for 1 _I. scapularis_ lifestage, resulting in separate nymphal and adult _I. scapularis_ tick sampling sites . We stored ticks in 70%–100% ethanol at 4°C until they were sorted by developmental stage and identified to species using dichotomous keys , placed into sterile microcentrifuge tubes containing 100% ethanol, and stored at −20°C until DNA extraction .\n\n【7】##### Pathogen Detection and Ap-ha/Ap-V1 Differentiation\n\n【8】Individual _I. scapularis_ nymphs and adults underwent total genomic DNA extraction as previously described . Using a quadplex real-time PCR , we tested for the following pathogens and target genes: _A. phagocytophilum_ (msp 2), _Babesia microti_ (18S rDNA), _Borrelia burgdorferi_ (16S rDNA), and _Borrelia miyamotoi_ (16S rDNA). Samples testing positive for _A. phagocytophilum_ by quadplex PCR were further tested using a custom TaqMan SNP genotyping PCR to differentiate between the Ap-ha and Ap-V1 variants of _A. phagocytophilum_ as described previously , with the following modifications: each 25 uL reaction contained 0.625 uL of 80× Custom TaqMan SNP Genotyping Assay primer/probe mix, 12.5 uL TaqMan Universal Master Mix (ThermoFisher), 1.875 uL nuclease-free water, and 10 uL of gDNA template (or nuclease-free water for negative controls). We performed SNP assays and variant assignment and generated allelic discrimination plots using Applied Biosystems 7500 Real-time PCR System version 2.0.5 (ThermoFisher).\n\n【9】##### ERI Calculation\n\n【10】To estimate risk for human exposure to Ap-ha and Ap-V1 variants of _A. phagocytophilum_ , we calculated ERI  using the equation\n\n【11】for _I. scapularis_ nymphs and adults. A high ERI value denotes an increased risk for human exposure to a particular pathogen. Of note, Ap-V1 does not cause disease in humans, so the term entomologic risk index is used only to provide a metric for comparison between Ap-ha and Ap-V1 variants. For sites with multiple sampling events within the same season, we averaged ERIs for all events.\n\n【12】##### Bernoulli Space-Time Scan Statistic\n\n【13】We compared the spatiotemporal distribution of Ap-ha–infected and Ap-V1–infected _I. scapularis_ adults and nymphs by using the Bernoulli space-time scan statistic , implemented in SaTScan version 9.6  . SaTScan searched for statistically significant clusters of Ap-ha or Ap-V1; we considered a cluster to be a location at which the relative risk (RR) of the presence of a variant is >1.0 inside a given cluster compared to outside. We selected maximum spatial and temporal cluster sizes a priori for our analysis. We set maximum spatial cluster size as 10% of the collected ticks to allow for new clusters to form as time progressed and to show the movement of each variant. We selected maximum temporal cluster size as 90% of the study period to allow for the identification of established populations of either _A. phagocytophilum_ variant.\n\n【14】##### Statistical Analysis\n\n【15】We tested the Spearman rank correlation between mean Ap-ha ERI in _I. scapularis_ nymphs and adults and anaplasmosis incidence at the postal (ZIP) code tabulation area level gathered from the New York State Department of Health (NYSDOH) Communicable Disease Electronic Surveillance system as previously described . We assessed the correlation for each year, 2010–2018. We selected the Spearman rank test because of the underlying count data used to generate anaplasmosis incidence and Ap-ha ERI. We corrected the 18 correlation tests for multiple testing using the Bonferroni-Holm adjustment . We compared results of the Spearman rank tests with the results from a previous analysis using non–genotype-specific _A. phagocytophilum_ ERI .\n\n【16】We tested for spatiotemporal interaction in the number of Ap-ha– and Ap-V1–infected _I. scapularis_ nymphs and adults by year and across latitude and longitude categories using a generalized linear mixed model (GLMM) extension of zero-inflated negative binomial (ZINB) regression . ZINB regression accounted for overdispersion and excess zero-counts of Ap-ha–and Ap-V1–infected _I. scapularis_ ticks, whereas the GLMM extension handled the repeated nature of our sampling scheme by allowing sampling sites to have varying intercepts. We binned tick collection data and corresponding PCR results by year, latitude, and longitude to increase the number of observations within each combination of covariates to fit the model. We binned tick data by year into 4 categories: 2008–2011, 2012–2014, 2015–2017, and 2018–2020. We binned tick collection sites by latitude into 3 categories: sites south of 42°N, at 42°N to 43°N, and north of 43°N. We binned collection sites by longitude into 3 categories: sites east of 74°W, from 74°W to 76°W, and west of 78°W. We built 4 models to analyze Ap-ha and Ap-V1 in _I. scapularis_ nymphs and adults separately. We assessed interaction between year and latitude/longitude categories using the likelihood ratio test (LRT). We used the natural log of the total number of _I. scapularis_ ticks of the target developmental stage (nymphs during sampling events in late May, June, July, and August; adult ticks during April, early May, October, November, and December) as an offset. We conducted data cleaning, generation of summary statistics, and data visualization using R version 4.0.3  and the dplyr , sf, ggplot2, and tmap R packages . We used the glmmTMB package in R  for modeling.\n\n【17】### Results\n\n【18】##### Active Tick Sampling and Pathogen Detection\n\n【19】We recorded active tick sampling and _A. phagocytophilum_ genotyping results for _I. scapularis_ tick specimens . We categorized sampling sites according to the presence of _A. phagocytophilum_ genetic variants . Of 91,163 adult _I. scapularis_ ticks collected during 2008–2020, we tested 43,520 for _A. phagocytophilum_ ; of 38,782 nymphal ticks, we tested 25,748 . Among those tested for _A. phagocytophilum_ , 3,207 (7.4%) adults and 1,183 (4.6%) nymphs were determined to be positive. _A. phagocytophilum_ genotyping deterimined that _I. scapularis_ adults had a higher prevalence of Ap-ha (5.4%) than Ap-V1 (1.7%), whereas _I. scapularis_ nymphs had a higher prevalence of Ap-V1 (2.6%) than Ap-ha (1.7%). We observed co-infection of Ap-ha and Ap-V1 in _I. scapularis_ adults (0.04%).\n\n【20】##### ERI\n\n【21】The Hudson Valley and Capital District regions in the eastern portion of the state generally exhibited higher Ap-ha ERI than the Central and Western regions of New York . Ap-V1 ERI of _I. scapularis_ nymphs was generally higher in the Western and Hudson Valley regions than in the Capital District and Central NY regions, but the levels were highly variable and exhibited no obvious temporal trend. Overall, ERI for Ap-ha increased in the later years of the study period for all 4 regions.\n\n【22】##### Retrospective Bernoulli Space-Time Cluster Analysis\n\n【23】We detected increased RR of Ap-ha or Ap-V1 in 9 clusters of adult  and 6 clusters of nymphal _I. scapularis_  ticks in the period 2008–2020. Among the 9 clusters of _I. scapularis_ adults, 7 exhibited increased RR of Ap-ha and 2 exhibited increased RR of Ap-V1. Among the 6 clusters of _I. scapularis_ nymphs, 4 exhibited increased RR of Ap-ha and 2 exhibited increased RR Ap-V1. Clusters of Ap-ha tended to be located in the Hudson Valley and eastern Capital District regions of New York, whereas clusters of Ap-V1 tended to be located in the Western and northern Capital District regions of New York, near the border with Canada. Analysis of _I. scapularis_ adults revealed 3 of 13 years in our study period with no clusters of Ap-ha , whereas clusters of Ap-V1 were present in all years but 2019 and 2020 . Analysis of _I. scapularis_ nymphs exhibited similar results; clusters of Ap-ha were present in all but 2 years of the study period (2009 and 2010), and 1 year, 2020, was without a cluster of Ap-V1 .\n\n【24】##### Spearman Rank Correlations and Zero-Inflated Regression Models\n\n【25】Anaplasmosis incidence was significantly correlated with Ap-ha ERI in _I. scapularis_ adults for all 9 years analyzed, whereas 6 of the 9 years analyzed were correlated for _I. scapularis_ nymphs . Statistically significant correlation coefficients in _I. scapularis_ adults were 0.36–0.75, an increase in the minimum and maximum correlation coefficients compared with non–variant-specific PCR results . Statistically significant correlation coefficients in _I. scapularis_ nymphs were 0.19–0.68, a decrease in the minimum coefficient and an increase in the maximum coefficient compared with correlations calculated using the non–variant-specific PCR results .\n\n【26】ZINB regression models of Ap-ha and Ap-V1 in _I. scapularis_ nymphs failed to converge, likely because of an insufficient number of observations within the year, latitude, and longitude covariate combinations. We detected notable spatiotemporal interaction only in Ap-ha in _I. scapularis_ adults . The final model for Ap-ha–infected _I. scapularis_ adults indicated high interaction between both year and latitude and year and longitude in the negative binomial portion of the model; however, the LRT indicated the model with year and longitude interaction was not better fit than the model without the year and longitude interaction (p = 0.0976). Model coefficients indicate increasing log counts of Ap-ha–infected _I. scapularis_ adults north of 43°N as year categories increased: for 2012–2014, β = 2.36 (95% CI 0.32–4.40); 2015–2017, β = 2.76 (95% CI 0.76–4.47); and 2018–2020, β = 3.79 (95% CI 1.77–5.80). log counts of Ap-ha–infected _I. scapularis_ adult at latitudes from 42°N to 43°N only increased in the final year category: for 2012–2014, β = 0.27 (95% CI −0.60 to 1.14); 2015–2017, β = 0.50 (95% CI −0.34 to 1.33); and 2018–2020, β = 1.39 (95% CI 0.56– 2.21). In addition, the log counts increased in year category 2018–2020 between 74°W and 76°W (β = 0.78 \\[95% CI 0.02–1.53\\]), although the LRT indicated that using the year and longitude interaction did not improve model fit. Of note, the interaction between the 2012–2014 year category and west of 76°W longitude category exhibited a wide 95% CI (−6,096.27 to 6,062.91) because no positive Ap-ha _I. scapularis_ ticks were found in all 68 site visits at those latitudes over that period. The zero-inflated portion of the model indicated a significant difference in the log odds of being an excessive zero between all latitude categories and longitude categories. For latitudes between 42°N and 43°N, β = 2.15 (95% CI 0.31–3.99); north of 43°N, β = 3.82 (95% CI 1.92–5.72). For longitudes between 74°W and 76°W, β = 1.30 (95% CI 0.53– 2.08); west of 76°W, β = 3.95 (95% CI 2.96–4.94). Only the 2018–2020 year category significantly differed from the reference group: for that category, β = −1.68 (95% CI −3.31 to −0.06).\n\n【27】### Discussion\n\n【28】Our study describes the landscape of Ap-ha and Ap-V1 genetic variants in New York, which has direct public health implications on the incidence of anaplasmosis. Continued geographic expansion of the Ap-ha variant in New York, as shown in this study, will result in a growing area of increased anaplasmosis risk for residents of the impacted regions. The current distribution of _A. phagocytophilum_ variants and associated anaplasmosis risk in New York is characterized by elevated risk in the Hudson Valley and Capital District region predominated by Ap-ha, compared with other geographic regions with low or variable Ap-V1 prevalence . During our study, the range of Ap-ha expanded relative to Ap-V1 over time, whereas Ap-V1 was largely unchanged and remained the dominant variant in the Western and northern Capital District regions. Those regions border the Canada provinces Ontario and Quebec, where Krakowetz et al. also found Ap-V1 to be the predominant variant, indicating the range of Ap-V1 may extend from the spatial clusters detected in our analysis northward into both provinces.\n\n【29】The results from the ZINB regression model support our variant cluster detection; Ap-ha expanded northward at an increasing rate over time, and some westward expansion is evident. Of note, these directions are relative only to the spatial extent of New York; our analyses only examined New York data. Including data from Massachusetts, Connecticut, and Vermont could indicate a westward expansion of entomological risk from Ap-ha. If the true spread of Ap-ha is occurring radially from neighboring states east of New York, the phenomenon would appear as the northward and westward increase relative to the borders of New York, as we observed in our results.\n\n【30】The geographic dynamics of Ap-ha and Ap-V1 are also likely linked to the deer–tick–rodent cycle and the varied reservoir competency of key vertebrate hosts. The observed geographic range expansion of Ap-ha, whereas that of Ap-V1 remained stable, indicates that the variants may not have a directly inverse relationship. Furthermore, co-infection of Ap-ha and Ap-V1 within individual ticks was rarely observed in our study and others , suggesting some competitive interaction between pathogen variants within the vector. The varied _A. phagocytophilum_ genotype prevalence across _I. scapularis_ tick life stages points to a developmental stage-specific association; the exact ecologic mechanism is unknown. One possibility is that Ap-V1 acquired during a larval tick bloodmeal may be later outcompeted by a subsequent infection of Ap-ha acquired during a nymphal bloodmeal. This phenomenon may be plausible because infection with either variant is maintained within the tick vector from one developmental stage to the next; higher prevalence of Ap-V1 in the nymphal stage did not yield a higher rate of Ap-V1 infection in adult ticks of the same cohort . Other possibilities include that _I. scapularis_ larvae may be more likely to feed on deer than mice in certain geographic regions, that small-mammal populations in certain regions may not harbor Ap-ha, or that another small mammal may serve as an alternate reservoir for Ap-V1 in nature. The difference in Ap-ha prevalence between _I. scapularis_ adults and nymphs could cause higher anaplasmosis incidence during the early spring and autumn months, when _I. scapularis_ adults are most active, but investigating this possibility was beyond the scope of this study.\n\n【31】It is likely that competition between Ap-V1 and Ap-ha occurs primarily between particular Ap-V1 and Ap-ha clusters . The region between these clusters should be a continued area of focus for epidemiologic research. Competition of variants at the local scale will likely result in spatial changes in the incidence of anaplasmosis; the expansion of Ap-ha clusters and large but unchanging clusters of Ap-V1 observed in our study match the dynamics of human anaplasmosis incidence depicted in Russell et al. Our study showed tests for correlation using only Ap-ha in ERI calculations moderately increased the correlation between ERI and reported anaplasmosis incidence; this finding suggests that surveillance testing to detect _A. phagocytophilum_ in host-seeking ticks must be variant-specific to yield the most accurate assessment of anaplasmosis risk.\n\n【32】Our study had several limitations, including spatiotemporal variability in tick sampling and the limited spatial extent of our data. Collection sites are more numerous and in closer proximity to one another in the Capital District region than other regions . Heterogenous sampling effort likely resulted in larger clusters of Ap-V1 than Ap-ha , because the a priori parameters of the cluster analysis forced clusters to include the same maximum number of ticks, regardless of the distance between sites. Therefore, Ap-V1 clusters in the Western region required a larger radius to include the same number of ticks than Ap-ha clusters in the Capital District region, potentially limiting spatial resolution in western New York. Lower sample sizes at varying latitudes and longitudes also likely reduced statistical power of the ZINB models. In addition, the directionality of the emergence of new clusters and spatiotemporal interaction in the ZINB models are limited by the lack of data outside of New York, possibly biasing results.\n\n【33】Given the changes in spatial distribution of the Ap-ha variant of _A. phagocytophilum,_ we suggest medical providers in newly emergent areas familiarize themselves with the signs and symptoms of anaplasmosis to streamline prompt and accurate diagnosis and treatment to ensure the best patient outcomes. Tickborne disease prevention education campaigns should target populations along the leading edge of Ap-ha advancement in New York and elsewhere. Continued differentiation and monitoring of the Ap-ha and Ap-V1 variants of _A. phagocytophilum_ to document rate and directionality of spread is critical; further studies will elucidate the ecologic factors driving the expansion of Ap-ha and the resulting increase in anaplasmosis. The results of our study and others can be used to educate medical practitioners and to guide public health policy and disease prevention efforts in New York.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0654328c-a444-4fa4-8175-9a0520898163", "title": "Pandemic (H1N1) 2009 Surveillance for Severe Illness and Response, New York, New York, USA, April–July 2009", "text": "【0】Pandemic (H1N1) 2009 Surveillance for Severe Illness and Response, New York, New York, USA, April–July 2009\nOn April 23, 2009, a nurse from a high school in New York City (NYC) called the Department of Health and Mental Hygiene (DOHMH) to report an outbreak of respiratory illness . The cause of the outbreak was rapidly confirmed to be influenza A pandemic (H1N1) 2009 virus. This outbreak was detected just a few days after initial reports of mild disease caused by pandemic (H1N1) 2009 virus in California and Texas  and at the same time as an outbreak of severe respiratory disease associated with pandemic (H1N1) 2009 virus in Mexico . Information about the clinical severity and transmission characteristics of this new influenza virus was limited. Given preliminary media reports about the Mexican outbreak and concern that NYC might also experience widespread severe disease, DOHMH launched a large-scale public health response.\n\n【1】Before the spring of 2009, DOHMH routine surveillance systems for influenza included 1) syndromic surveillance for medication sales, school absenteeism, and emergency department visits for influenza-like illness (ILI) ; 2) electronic laboratory reporting of confirmed cases from commercial and hospital laboratories; 3) active surveillance of all NYC virology laboratories to determine the weekly number of specimens submitted for influenza testing and the percentage of those positive; 4) typing samples of influenza isolates obtained from patients in NYC hospitals at the DOHMH Public Health Laboratory (PHL); 5) enhanced passive surveillance for pediatric influenza deaths; 6) monitoring trends in influenza and pneumonia-related mortality through the DOHMH Vital Registry; and 7) monitoring outpatient ILI through the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA) Influenza-like Illness Surveillance Network , a sentinel network through which providers reported weekly on the proportion of ILI in their practices during influenza season. The DOHMH had also created a plan for local response to a potential influenza pandemic, including enhanced surveillance to guide public health officials in determining how to prioritize use of antiviral agents and vaccines . Surveillance data could also inform community control measures, such as school closures. Proposed surveillance strategies in this plan focused on mechanisms for monitoring trends in hospitalizations and deaths, but not necessarily for trying to count every severe case. Methods were also proposed for obtaining more detailed clinical and epidemiologic data for a sample of cases.\n\n【2】The DOHMH also has an incident command system (ICS), an agency-wide structure for addressing and responding to emergencies that is different from the usual DOHMH structure. Divided into 10 sections, the ICS is led by an incident commander who reports directly to the Commissioner of Health . All DOHMH employees are assigned to a section within the ICS and can be called on to assist their section upon activation of the system. In a public health emergency, the Surveillance and Epidemiology Section establishes and conducts surveillance to assess the illness and deaths associated with the event and conducts any needed epidemiologic studies to guide the public health response. ICS activation provides surge capacity by increasing the workforce available to conduct surveillance or epidemiologic activities beyond the staff members who are normally responsible for the specific disease or public health issues involved in the emergency.\n\n【3】We describe some of the surveillance methods used in the investigation of pandemic (H1N1) 2009 in NYC from April to July 2009. DOHMH investigated the high school outbreak , and set up an enhanced citywide surveillance system to track the scope and severity of infections. The agency also prioritized identification and diagnostic testing of patients with severe or fatal cases of ILI in hospitals or clusters of those with ILI in schools and other congregate settings; this surveillance was essential because evidence of severe pandemic (H1N1) 2009 would have prompted more aggressive public health control measures. In addition, because surveillance of cases in hospitalized patients, and particularly of fatal cases, was an important part of this investigation, we provide an overview of epidemiologic findings among hospitalized patients.\n\n【4】### Methods\n\n【5】On Saturday, April 25, 2009, when preliminary laboratory results suggested a likely pandemic (H1N1) 2009 outbreak at high school A, the DOHMH activated its ICS and initially mobilized >200 staff members for a large-scale public health response (later adding additional staff). From April 25 through May 8, the agency also expanded its hours of operation to 7 days a week from 9:00 am to 9:00 pm ; staff worked in shifts to cover the extended hours.\n\n【6】The ICS was deactivated on May 8, since minimal evidence existed of community circulation of pandemic (H1N1) 2009. By mid-May, however, DOHMH noticed an increase in ILI, especially in schoolchildren. On May 17, 2009, the first NYC death from pandemic (H1N1) 2009 virus occurred. In response to these developments and to increasing reports of hospitalized case-patients, DOHMH reactivated its ICS on May 19. This second activation continued until July 7, 2009.\n\n【7】##### Enhanced Citywide Surveillance\n\n【8】##### Active Surveillance for Critically Ill Case-Patients\n\n【9】Starting April 26, the DOHMH conducted active citywide surveillance in hospital intensive care units (ICUs) for severe, unexplained, febrile respiratory illnesses (defined as a temperature \\> 100.4°F ( \\> 38°C) and pneumonia, acute respiratory distress syndrome, or respiratory distress (as diagnosed by clinicians) with no known cause. DOHMH staff contacted all 57 NYC hospitals with medical or pediatric ICUs daily by telephone and queried the clinician in charge of the ICU that day to determine the number of patients with conditions that met the surveillance definition. Active ICU surveillance was discontinued on May 8 since few cases of severe illness were being identified **.**\n\n【10】##### Enhanced Passive Surveillance for Hospitalized Case-Patients with Noncritical Illness\n\n【11】To ascertain the number of hospitalized patients with pandemic (H1N1) 2009 outside of the ICU setting, DOHMH relied on enhanced passive surveillance. Providers were notified of reporting requirements through the NYC Health Alert Network, which sends faxes and email alerts to 29,000 clinicians and healthcare institutions in NYC, and through daily conference calls with all NYC acute care facilities. DOHMH set up a dedicated NYC telephone access line to triage provider calls. Providers were initially asked to report any hospitalized patients outside of the ICU setting with severe, unexplained, febrile respiratory illnesses (as defined above). However, because of the increasing number of calls and limited staff and laboratory testing capacity at the NYC PHL, beginning on May 12, providers were asked to only report non-ICU cases of severe, febrile respiratory illness if initial test results were positive for influenza A virus by enzyme immunoassay, PCR, direct fluorescent antibody test, or virus culture at the hospital laboratory. However, DOHMH continued to accept reports on all patients with febrile respiratory illness who were in the ICU or were receiving ventilation, regardless of influenza testing status.\n\n【12】##### Active Laboratory Surveillance\n\n【13】During the week after the recognition of pandemic (H1N1) 2009 in NYC (April 25–30), DOHMH actively collected specimens from laboratories chosen to be geographically representative of the city to determine whether evidence existed of community circulation of the pandemic virus that was not associated with the outbreak at the high school. Five sentinel laboratories were selected and asked to submit 1–3 influenza A virus–positive specimens from the previous 2 days to the NYC PHL to test for pandemic (H1N1) 2009 virus.\n\n【14】##### Case-Patient Interviews\n\n【15】During the first 3 weeks of the outbreak, DOHMH staff attempted telephone interviews of all patients (or their proxies) who had confirmed pandemic (H1N1) 2009 for demographic, epidemiologic, and clinical information. Providers of care for hospitalized case-patients were also interviewed to gather information about patient demographics, underlying conditions, and clinical course of illness. Once community circulation in NYC was established, DOHMH stopped interviewing patients about possible risk of exposure (e.g. travel to Mexico, school attendance).\n\n【16】##### Surveillance for Deaths\n\n【17】To track pandemic (H1N1) 2009–related deaths, DOHMH asked that hospitals report any fatal cases of unexplained, acute, febrile respiratory illness to DOHMH and to the Office of the Chief Medical Examiner (OCME). The OCME collected specimens and performed autopsies on any patient whose death was preceded by a sudden, unexplained, febrile respiratory illness, as well as for all pediatric patients who died with clinically compatible illness in which there was a positive influenza test result, a sudden unexplained death thought to be due to a natural cause, or death of a child from an unknown febrile respiratory illness. If no testing results for pandemic (H1N1) 2009 virus were available from the hospital, OCME collected a postmortem nasopharyngeal swab specimen for influenza diagnostic testing at PHL. In addition, the dataset of patients who tested positive for pandemic (H1N1) 2009 virus was matched weekly with the NYC Vital Records database of recent deaths in NYC to ensure that no pandemic (H1N1) 2009 deaths were missed.\n\n【18】##### Laboratory Methods\n\n【19】DOHMH physicians screened reported potential cases to determine if they met testing criteria, and if so, nasopharyngeal specimens were requested. Recognizing the need to prioritize PHL resources for hospitalized patients and those with fatal cases, DOHMH specifically requested that clinicians not test patients with mild ILI unless the patient was part of a reported cluster in a school, jail, nursing home, or other congregate setting.\n\n【20】Specimens were initially tested for influenza A or B viruses, and then, if positive for influenza A, were further tested for seasonal influenza A virus (H1N1 or H3N1) by using the QIAamp Viral RNA manual extraction method (QIAGEN, Valencia, CA, USA) and real-time reverse transcription–PCR by using the Cepheid SmartCycler (Cepheid, Sunnyvale, CA, USA). Initially, specimens that were positive for influenza A virus, but not seasonal influenza A virus subtypes H1N1 or H3N1, and suspected to be pandemic (H1N1) 2009 virus were sent to CDC for confirmation. Then, on May 11, the PHL started to perform the CDC Influenza Virus Real-time reverse transcription–PCR detection and characterization panel for pandemic (H1N1) 2009 virus on all nonseasonal influenza A specimens by using a high-throughput system including an automated extraction system and ABI7500 Fast-Dx (Life Technologies, Carlsbad, CA, USA). Beginning on May 20, PHL performed the same CDC assay on all influenza specimens by using the same high throughput system.\n\n【21】A confirmed case of pandemic (H1N1) 2009 was defined as a person who had a specimen that was PCR positive for pandemic (H1N1) 2009 virus. A probable case was defined as a patient with nonsubtypeable influenza A virus infection for whom confirmatory testing was not conducted. Confirmatory influenza testing was performed at PHL, CDC, or the New York State Wadsworth Center Laboratory.\n\n【22】##### Analytic Methods\n\n【23】We analyzed surveillance data to describe NYC residents who were hospitalized with pandemic (H1N1) 2009 in NYC from the start of the first ICS activation to the end of the second activation (April 24–July 7). We also calculated pandemic (H1N1) 2009 rates by dividing the number of confirmed and probable cases among hospitalized patients by NYC population counts from the US Census 2000. We examined rates by demographic characteristics of hospitalized patients and performed direct age-adjustment by using weights based on US Census 2000 .\n\n【24】Additionally, patient poverty level was assessed by linking ZIP code of residence with income and population data from the US Census 2000. We defined neighborhoods using the United Hospital Fund (UHF) designation, which aggregates adjoining ZIP codes to create 42 NYC neighborhoods . We then created a neighborhood poverty variable by categorizing UHF neighborhoods into tertiles (low-, medium-, and high-poverty neighborhoods) based on the percentage of residents living <200% of the federal poverty level, according to the US Census 2000, and calculated rates for confirmed and probable pandemic (H1N1) 2009 cases by UHF neighborhood poverty status . Poverty data were available for 993 of the 996 persons who were hospitalized with confirmed or probable pandemic (H1N1) 2009.\n\n【25】For all analyses, significance was determined at p<0.05. All statistical analyses were conducted by using SAS 9.2 (SAS Institute Inc. Cary, NC, USA).\n\n【26】### Results\n\n【27】##### Hospitalizations\n\n【28】During April 24–May 7, corresponding to the first ICS activation, 15 patients with confirmed or probable cases of pandemic (H1N1) 2009 were hospitalized (median stay was 1 day). At that time, most cases were linked to the high school influenza A outbreak and only 2 case-patients reported travel to Mexico. No deaths had been reported. Since few cases of severe illness had occurred, the ICS was deactivated, and staff who would normally be involved in communicable disease outbreak investigations continued to monitor pandemic (H1N1) 2009 activity.\n\n【29】By July 7, the end of the second ICS activation, 996 patients had been hospitalized. The distribution of 996 hospitalized case-patients (929 confirmed and 67 probable) over time, including the increased incidence in late May, can be seen in Figure 1 . From April 24 through July 7, the estimated age-adjusted rate of confirmed and probable pandemic (H1N1) 2009 hospitalizations was 12.3/100,000 NYC residents (95% confidence interval \\[CI\\] 11.8–13.4). The rate among patients < 4 years of age (40.9/100,000, 95% CI 35.6-46.3) was almost 7× that among those \\> 65 years of age (6.0/100,000, 95% CI 4.5–7.7) . The estimated age-adjusted rate of pandemic (H1N1) 2009 hospitalized patients in high-poverty neighborhoods (18.4/100,000, 95% CI 16.8–20.1) was significantly higher than that in low-poverty neighborhoods (8.9/100,000, 95% CI 7.6–10.4) .\n\n【30】##### Deaths\n\n【31】The first NYC death occurred on May 17. Additional information about NYC pandemic (H1N1) 2009 deaths has been published elsewhere .\n\n【32】### Discussion\n\n【33】The experience of NYC with pandemic (H1N1) 2009 demonstrates the need for flexibility in surveillance approaches and ongoing modification of surveillance methods to best respond to a changing public health emergency. Although DOHMH had not planned to do such intensive active and enhanced surveillance during an influenza pandemic , active case-based surveillance was initially implemented because little was known about the severity of this novel strain of H1N1, and public health officials were concerned on the basis of initial media reports from Mexico. To learn more about the severity of illness, DOHMH focused on surveillance of hospitalized cases and deaths. Surveillance and reporting requirements were modified when it became clear that circulation of pandemic (H1N1) 2009 was citywide, but surveillance for deaths and hospitalized cases continued to help officials assess the severity and at-risk groups for pandemic (H1N1) 2009, and the resulting information helped inform DOHMH planning and response to this new virus.\n\n【34】Approximately half of hospitalized patients lived in a high-poverty neighborhood; this association between poverty and severe illness has been reported for seasonal influenza . Our finding of the association between young age and severe illness is consistent with other studies , and the greater proportion of persons 0–17 years of age in low-income neighborhoods in NYC  may contribute to this distribution. Other possible explanations include higher attack rates among residents living in crowded housing, or that residents with known risk conditions in high-poverty neighborhoods may be less likely to receive early treatment or prophylaxis, given that the proportion of people without personal doctors is higher in high-poverty areas relative to low-poverty areas (20% vs. 11%) . In addition, the proportion of uninsured persons in low-income areas (18%) is higher than the proportion in high-income areas (9%), according to NYC’s Community Health Survey from 2008 . Future studies should assess poverty status and its relationship to severe influenza illness.\n\n【35】Our analysis had several limitations. By limiting testing to those patients who had positive influenza A test results (unless patients were in the ICU), our surveillance approach systematically undercounted hospitalized patients with pandemic (H1N1) 2009. Although this enabled us to monitor hospitalization trends, we most likely do not have a complete count of cases. Published studies have found a wide range of results for the sensitivity of rapid influenza testing for the pandemic (H1N1) 2009 strain (17%–70%) . Applying the published range of sensitivities to our results would suggest that the true number of hospitalized patients in NYC ranged from 1,400 to 6,000, which is 1.5–7.0× higher than those for cases detected and confirmed. Also, because of the limited amount of data collected on all patients, we were unable to examine variables at the individual level; such data (for example, having a primary care physician and insurance status) may have modified the findings regarding the relationship between poverty and severe illness. Lastly, demographic and economic information was from 2000, and changes may have occurred.\n\n【36】Surveillance data from the spring outbreak informed NYC planning and response to pandemic (H1N1) 2009 during the 2009–10 fall and winter influenza season. Because young children represented a large proportion of hospitalized cases and because of the role children likely play in transmission, NYC created a school-based vaccination program for elementary and middle schoolchildren and vaccinated all children who had parental consent. In the fall of 2009, >60 influenza diagnostic and treatment community based centers were established for persons with ILI who did not have a primary care physician; an advice hotline, staffed by nurses, was created to answer questions and help connect NYC residents to care. Antiviral medications were made available to those who could not afford them, and points of distribution provided the vaccine free of charge to New Yorkers, initially targeting those who had risk factors for severe pandemic (H1N1) 2009 as identified in NYC and elsewhere .\n\n【37】DOHMH has continued to use emergency room and outpatient syndromic surveillance systems to follow trends in influenza-like activity citywide. We also requested passive reporting of influenza hospitalizations by all city hospitals and collected some data on clinical status and risk factors. Finally, to more effectively monitor the clinical and epidemiologic characteristic of pandemic (H1N1) 2009 during the fall and winter seasons, we established a sentinel hospital surveillance program at 5 sites where active surveillance and influenza testing were conducted on any patient with fever and respiratory syndromes. Collection of isolates from sentinel hospitals and active laboratory surveillance also allowed circulating influenza subtypes, as well as antiviral resistance, to be monitored. Surveillance guided and informed the NYC response to pandemic (H1N1) 2009, and this experience will help NYC plan a response to future epidemics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a61198e8-80b8-4a79-abac-6929a5bf307c", "title": "Genetic Variation of SARS Coronavirus in Beijing Hospital", "text": "【0】Genetic Variation of SARS Coronavirus in Beijing Hospital\nA novel severe acute respiratory syndrome–associated coronavirus (SARS-CoV) has been implicated as the causative agent of a worldwide outbreak of SARS during the first 6 months of 2003 . From March 4 to June 18, Beijing had 2,521 cases and 191 deaths from SARS . Because of the poor fidelity of RNA-dependent RNA polymerase, genetic variation typically forms a heterogeneous virus pool in RNA virus populations, including coronaviruses such as mouse hepatitis virus (MHV) . This feature makes viruses highly adaptable and contributes to difficulties in preventing and controlling viral disease. SARS-CoV, a single-stranded RNA virus, has been reported with relatively less variability in analyses of a limited number of viral isolate collections . Furthermore, no SARS-CoV quasispecies have been documented, as they have been in many other RNA viruses, including hepatitis C virus (HCV) , HIV , and MHV .\n\n【1】During the SARS outbreak in Beijing, 132 SARS patients were hospitalized and treated on our unit at Beijing Hospital, including the first cluster of case-patients in the area . To characterize genetic variation among SARS-CoV transmitted in the Beijing area, we sequenced 29 full-length S genes of SARS-CoV from 20 hospitalized SARS patients, since S glycoprotein plays a key role in virus-host interaction and is predicted to be the main target of immune response . Samples that were analyzed represented the timespan of the epidemic. To exclude culture-derived artifacts and estimate mutational heterogeneity, viral RNA was directly extracted from raw clinical samples, and a TA-cloning assay was used with direct analysis of reverse transcriptase–polymerase chain reaction (RT-PCR) products. We compared these sequences with all previously documented S-gene sequences of SARS-CoV.\n\n【2】### Materials and Methods\n\n【3】##### Patients and Samples\n\n【4】All patients in the study were hospitalized on our unit with a confirmed diagnosis of SARS. Samples from patients included plasma, throat swab, sputum, and stool; these were stored at –70°C for extraction of viral RNA. A total of 64 RNA samples from 28 SARS-CoV–positive patients (detected by using BNI primers recommended by the World Health Organization ) were initially used in S-gene amplification, but only those that generated all six overlapping fragments covering the full-length S-gene sequence  were included in the sequence analysis. As a result, 29 RNA samples from 20 patients were included in the study . All patients had received ribavirin and steroid combination therapy.\n\n【5】##### RNA Extraction\n\n【6】RNA extraction was performed in a biosafety level 3 (P3) laboratory. RNA was extracted directly from plasma samples. Sputum samples were shaken for 30 min with an equal volume of 1.0% acetylcysteine and 0.9% sodium chloride, followed by isolating supernatant by centrifuging (10,000 _g_ x 3 min). Throat swab and stool samples were suspended with phosphate-buffered saline (PBS) containing 10 U/mL RNasin (Promega, Madison, WI) and shaken for 10 min, followed by isolating supernatant by centrifuging as mentioned above. RNA was extracted according to the manufacturer’s instructions by using the QIAamp Viral RNA Mini Kit (Qiagen, Hilden, Germany).\n\n【7】##### Nested RT-PCR\n\n【8】Screening RNA for SARS-CoV was based on the method by Drosten et al. For the S-gene amplification, 18 pairs of primers were designed by using MacVactor computer software (Accelrys Inc, San Diego, CA) based on the BJ01 strain of SARS-CoV  . Among them, six pairs (sense/antisense: S1aF/S1aB, S2aF/S2aB, S3aF/S3aB, S4aF/S4aB, S5aF/S5aB, S6aF/S6aB) were used as outer primers, six pairs (sense/antisense: S1bF/S1bB, S2bF/S2bB, S3bF/S3bB, S4bF/S4bB, S5bF/S5bB, S6bF/S6bB) were used as inner primers, and six pairs (sense/antisense: S1cF/S1cB, S2cF/S2cB, S3cF/S3cB, S4cF/S4cB, S5cF/S5cB, S6cF/S6cB) were designed for direct RT-PCR product sequencing. The sequences covering the full-length S gene were amplified separately as six overlapping fragments (F1b, F2b, F3b, F4b, F5b, and F6b) . The one-step RT-PCR Kit (Qiagen) was used for reverse transcription and the first round of PCR amplification with outer primers. Thermal cycling consisted of 50°C for 30 min; 95°C for 15 min; 10 cycles of 95°C for 30 s, 57.5°C for 30 s (decreasing by 1.5°C every other cycle), 72°C for 1 min; 40 cycles of 95°C for 30 s, 54°C for 30 s, 72°C for 1 min. Afterwards, 2 μL of the product was used as a template for the second round of PCR amplification in 100-μL volume with inner primers with Taq DNA polymerase (MBI Fermentas, Hanover, MD). Thermal cycling consisted of 30 cycles of 95°C for 25 s, 54°C for 25 s, 72°C for 50 s. In some cases, Transcript III RNase H –  Reverse Transcriptase (Invitrogen, Carlsbad, CA) was used for reverse transcription, according to the manufacturer’s instructions. The next two rounds of PCR amplification were performed by using Platinum Pfx DNA Polymerase with a higher fidelity (Invitrogen). The reaction condition was set as above, with a twofold elongation at 68°C instead of 72°C. All reactions were carefully carried out to avoid contamination.\n\n【9】##### TA-Cloning\n\n【10】RT-PCR products were purified by QIAquick PCR Purification Kit (Qiagen) or QIAquick Gel Extraction Kit (Qiagen), with a final volume of 30 μL of elution. The ligation and transformation were performed according to the manufacturer’s instructions by using pGEM-T Vector System II (Promega). Transformants were selected in LB-agar plate containing 100 μg of ampicillin, 100 μg of 5-bromo-4-chloro-3-indolyl β-L-fucopyranoside (X-gal), and 200 μg of isopropylthiogalactoside. _Escherichia coli_ from white clones was added to 5 mL of LB culture for overnight growing at 37°C with vigorous shaking. Plasmid was purified by QIAprep Spin Miniprep Kit (Qiagen). The recombinant plasmids for sampling sequence analysis were screened by electrophoresis in 1% agarose containing 0.5 μg/mL of ethidium bromide.\n\n【11】##### Sequencing and DNA Analysis\n\n【12】For each S-gene fragment, four to six clones were screened. To verify variations, 5–50 additional clones generated from independently prepared, RNA-derived RT-PCR products were sequenced in two to four independent experiments. The cloned plasmids were prepared from different RT-PCR products and were directly sequenced for confirmation. DNA sequences were obtained with the use of an automated ABI 377 sequencer (Applied Biosystems Inc. Foster City, CA). For cloned plasmids, SP6 and T7 primers were used for two-directional sequencing reactions. For PCR products, specific primers (sense: S1cF–S6cF; antisense: S1cB–S6cB) were used for two-directional sequencing reactions. Analysis and comparison of nucleotide and amino acid sequences were carried out with the DNASTAR computer software (DNASTAR Inc. Madison, WI). The S gene sequence of BJ01 strain was taken as the reference for variation analysis.\n\n【13】### Results\n\n【14】With the designed six pairs of primers, all six overlapping S-gene fragments were amplified by nested RT-PCR from 29 RNA samples. However, most RNA samples initially included in the study, though positive for SARS-CoV with BNI primers, failed to simultaneously generate all six overlapping S-gene fragments and were excluded from further sequence analysis. Disintegration of the virus and low viral load in the raw samples likely accounted for these failures.\n\n【15】One hundred and thirteen sequence variations distributed in nine variant sites were identified in analyzed sequences that were compared to the reference BJ01 strain of SARS-CoV. BJ01 is an isolate from a tissue-culture propagated sample  and is used as reference strain in other studies . With the exception of one site (position 21702), other variant sites have not, to our knowledge, been documented in humans. Seven of nine variant sites were nonsynonymous. Figure 2 shows the identified variant sites compared to the reference sequence.\n\n【16】### Discussion\n\n【17】We identified novel variant sites and the coexistence of sequences with and without S-gene substitutions in SARS-CoV. Theoretically, a replicating RNA virus expresses a range of genetic and phenotypic variants and has the potential to generate novel virions, which may be selected in response to environmental pressures. RNA viruses generally tolerate high levels of mutagenesis because of their limited genetic complexity . Mutations have the potential to be pathogenic (e.g. giving the virus immunity to neutralizing antibodies, cytotoxic T cells, or antiviral drugs ). The dynamics of error copying and sequence decomposition are time-dependent. In HIV infection, for example, one adaptive substitution in the _env_ gene occurred every 3.3 months or 25 viral generations, averaging across patients .\n\n【18】In our study, a higher variation frequency in the S gene was identified for SARS-CoV compared to previous reports . This difference may be due to a broader sample collection covering a longer timespan of infection. In addition, since virus isolates were not passaged in culture, the whole mutant repertoire is more likely to be detected, since no reverse mutation occurs in cell culture. Our observation most likely reflected the real situation in vivo. Variations were unlikely to result from Taq polymerase errors, since we repeated the experiments for all variations from preparing independent RNA and RT-PCR products and used Platinum Pfx DNA polymerase, which has a high fidelity, to confirm the results in some cases. We could not exclude the possibility that some variations were from defective genomes. However, the fact that the variations remained detectable in the sequences from two or three specimens of the same patient, obtained at different times, suggested that these variations might be active and extensible in vivo.\n\n【19】Sequences with and without substitutions (referred to BJ01) were simultaneously detected in the sequences from seven samples, which suggests the existence of SARS-CoV quasispecies. Furthermore, S-gene sequences from different samples collected at different times from the same patient showed similar, but not exactly identical, variation profiles in four participants (patients 4, 5, 6, and 19 in Table 1 ); this implies that a dynamic mutational process may exist in vivo. Table 2 summarizes the variations occurring in 29 analyzed S-gene sequences from 20 individual SARS patients.\n\n【20】One nonsynonymous change observed at position A1023G is within the heptad repeat (HR) domains, which is thought to be important for virus entry, and previous study on MHV showed that it would have some effect on virus infection . At this stage, we cannot rule out the possibility that this change affects the biological outcome of the virus, but further experiments need to be addressed in the near future.\n\n【21】We observed the coexistence of the S-gene sequences with and without substitutions and time-dependent variation profile in some patients. These observations suggest the possible existence of SARS-CoV quasispecies in an acute infection. In this study, however, the limitation of clinical sample collection and difficulty in directly amplifying full-length S gene from raw clinical samples restricted further extensive study for dynamic mutant distributions of the virus. In addition, the sequencing clone number was conditioned by the scale of the project, and this may have led to some minor variant sequences escaping analysis. Another factor possibly affecting the stability of the viral genome is the administration of the antiviral drug ribavirin. That ribavirin enhances mutagensis of RNA viruses has been addressed . Therefore, the artificial effect of ribavirin on the SARS-CoV mutant spectrum remains to be clarified.\n\n【22】The genetic variation of SARS-CoV remains limited in relation to many other RNA viruses such as HIV-1, HCV, and MHV. The probable reason is that SARS-CoV only causes an acute, self-limited infection, which may prevent persistent long-term mutant development in vivo as occurs in chronic RNA viral infections. Notably, some modules in the S protein remain conserved, e.g. the fusion-important HR domains. Although some variations may predict changes of protein functional features, no obvious correlation exists between mutation and clinical disease manifestation from the limited data reported here. Instead, the variation profile was closely correlated with epidemiography; e.g. patients 3–8 were infected in one hospital.\n\n【23】In conclusion, we report here some new variant sites in the S gene of coronavirus and possible existence of SARS-CoV quasispecies in some patients, though in limited numbers. This knowledge furthers our understanding of this emerging virus.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "85ee4918-5f64-4b3c-807f-8e6948c62fea", "title": "Recovery of Cryptococcus gattii from an Infected Ventriculo-Peritoneal Shunt, Illinois, USA", "text": "【0】Recovery of Cryptococcus gattii from an Infected Ventriculo-Peritoneal Shunt, Illinois, USA\n_Cryptococcus gattii_ is a fungus found in soil and decaying organic materials . _C gattii_ infections have been reported in tropical and subtropical regions worldwide. In the United States, _C. gattii_ human infection is rare; <300 cases have been documented, of which 169 were reported to the Centers for Disease Control and Prevention (CDC) during 2005–2013 . Most cases were reported in southern California before a rise in cases occurred after 1999 in the Pacific Northwest . We report a case of ventricular shunt infection by _C. gattii_ in an immunocompetent person in Illinois.\n\n【1】A 40-year-old man from Lake County, Illinois, with no known medical problems was admitted in October 2015 for evaluation of hydrocephalus. The patient reported 4 months of throbbing frontal headaches, nausea, and vomiting. Progressive confusion, altered memory, intermittent gait, and balance disturbances also were present. No visual changes, fevers, chills, or seizures were reported. The patient had no travel outside of Illinois and no ill contacts. Computed tomography (CT) scan of the brain demonstrated hydrocephalus, which was concerning because it indicated possible abnormalities in the flow of cerebrospinal fluid (CSF). A right frontal ventriculostomy catheter was placed. Repeat CT imaging of the head showed a possible mass within the right cerebellar hemisphere and surrounding vasogenic edema. To determine whether an infectious pathogen was the cause, we performed a workup that included HIV screening, _Echinococcus_ serologic testing, interferon gamma release assay, cysticercosis serologic testing, and 3 CSF cultures; all results were negative. Results of a complete blood count with differential and comprehensive metabolic panel were unremarkable. The patient was not receiving immunosuppressive therapy nor had any other known risk factors associated with immunosuppression. A right ventriculo-peritoneal (VP) shunt with a programmable valve was placed, and the patient was discharged to home in stable condition.\n\n【2】One month later, the patient was readmitted with recurring symptoms. CT imaging of the head showed stable ventricular size. Contrast-enhanced magnetic resonance (MR) of the brain showed abnormalities above the tentorium, possibly representing a cystic mass obstructing the foramen of Monroe bilaterally, with pronounced distention of both lateral ventricles. CSF studies showed a leukocyte count of 3/μL (reference range 0–5/μL) with lymphocytic predominance (93% \\[reference range 40%–80%\\]) and protein level of 42 mg/dL (reference range 15–45 mg/dL). We observed large round yeasts on Gram stain of CSF. The VP shunt was externalized. Cryptococcal antigen (Immy; Norman, OK, USA) was positive in the CSF (1:160 titer). Cultures from the CSF grew yeast that we identified as _Cryptococcus neoformans_ by using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (Vitek MS IVD Database version 2; bioMérieux, Durham, NC, USA). We then subcultured the organism to CGB Agar (L-canavanine, glycine, bromothymol blue; Hardy Diagnostics, Santa Monica, CA) to differentiate _C. neoformans_ from _C. gattii_ . The organism, which produced blue coloration on CGB agar, was determined to be _C. gattii_ and was confirmed as _C. gattii_ molecular biotype VGI by multilocus sequence typing performed at CDC (Atlanta, Georgia, USA). Induction with amphotericin B and flucytosine was given for 14 days and high-dose fluconazole (800 mg/d) was subsequently given as consolidation therapy for 8 weeks. The dose was then decreased (to 200 mg/d) for maintenance therapy. The patient was lost to follow-up after his first outpatient clinic visit.\n\n【3】We postulate that our patient likely had a cryptococcoma with a low organism burden on initial presentation. We found no cases of VP shunt infection attributable to _C. gattii_ in the literature. Only 10 cases of VP shunt infections attributable to _C. neoformans_ have been reported; the time from shunt placement to symptom onset ranged from 10 days to 20 years . Six of 10 cases resulted from shunt placement in persons previously infected . The patient we report had onset of symptoms 4 weeks after VP shunt placement, likely reflecting an underlying infection before VP shunt placement.\n\n【4】Only 4 isolates of _C. gattii_ have been identified from the Midwest region of the United States; these isolates were identified as VGI and VGIII types  . A recent study demonstrated that a large subset of isolates from throughout the United States were VGI, including a cluster of isolates with a single multilocus sequence type originating in the southeastern United States . The isolate in this case was identified as molecular type VGI and by multilocus sequence typing was shown to have the same sequence type as isolates from patients in Florida and Georgia and isolates from the environment in Washington.\n\n【5】Infections attributable to _C. gattii_ are not confined to tropical and subtropical regions. The case we describe serves to extend the known range of this organism to include Illinois. Infections might be missed, given that many laboratories do not routinely differentiate _C. gattii_ from _C. neoformans_ . Mortality rates can range from 13% to 33% . Thus, clinicians and laboratorians must have increased awareness of this emerging infectious disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7c324dd-b6f4-4bc1-bb4a-c494ef91654a", "title": "Enhancing Workplace Wellness Efforts to Reduce Obesity: A Qualitative Study of Low-Wage Workers in St Louis, Missouri, 2013-2014", "text": "【0】Enhancing Workplace Wellness Efforts to Reduce Obesity: A Qualitative Study of Low-Wage Workers in St Louis, Missouri, 2013-2014\nAbstract\n--------\n\n【1】**Introduction**  \nThe objective of this study was to examine workplace determinants of obesity and participation in employer-sponsored wellness programs among low-wage workers.\n\n【2】**Methods**  \nWe conducted key informant interviews and focus groups with 2 partner organizations: a health care employer and a union representing retail workers. Interviews and focus groups discussed worksite factors that support or constrain healthy eating and physical activity and barriers that reduce participation in workplace wellness programs. Focus group discussions were transcribed and coded to identify main themes related to healthy eating, physical activity, and workplace factors that affect health.\n\n【3】**Results**  \nAlthough the union informants recognized the need for workplace wellness programs, very few programs were offered because informants did not know how to reach their widespread and diverse membership. Informants from the health care organization described various programs available to employees but noted several barriers to effective implementation. Workers discussed how their job characteristics contributed to their weight; irregular schedules, shift work, short breaks, physical job demands, and food options at work were among the most commonly discussed contributors to poor eating and exercise behaviors. Workers also described several general factors such as motivation, time, money, and conflicting responsibilities.\n\n【4】**Conclusion**  \nThe workplace offers unique opportunities for obesity interventions that go beyond traditional approaches. Our results suggest that modifying the physical and social work environment by using participatory or integrated health and safety approaches may improve eating and physical activity behaviors. However, more research is needed about the methods best suited to the needs of low-wage workers.\n\n【5】Introduction\n------------\n\n【6】Obesity, a major risk factor for diabetes, affects more than one-third of adults in the United States and is associated with several demographic and socioeconomic factors, including low income . Several studies have found that obesity rates are generally higher among working class occupations than professional occupations, even after controlling for demographic factors .\n\n【7】From a sociological perspective, the environments in which people live and work are strong influences on obesity and diabetes . The work environment is especially important because many adults spend a significant amount of time at work and because obesity affects employers through reduced productivity and absenteeism as well as increased health care costs and disability . Numerous studies acknowledge the negative health consequences of workplace factors such as stress, low autonomy, poor coworker and managerial support, and unhealthy physical work environments . These workplace risk factors may be more common in low-wage and working-class jobs and may explain some occupational differences in obesity prevalence .\n\n【8】Promoting health through worksite wellness programs is a national priority. The Affordable Care Act creates new incentives to promote employer wellness programs and encourage opportunities to support healthier workplaces . The National Institutes of Health and the Centers for Disease Control and Prevention have targeted worksites as a priority location for health interventions because they offer an efficient means of delivering and evaluating programs and provide opportunities to reach socially disadvantaged populations . However, data for the effectiveness of workplace health programs are limited and may not be generalizable to all types of workers . National data show that blue-collar and service workers are less likely to work for an employer who offers health promotion activities and are less likely to participate in such programs when offered .\n\n【9】This study focused on a little-studied health disparity — workplace health promotion among low-wage workers. The objective of the study was to examine through interviews and focus groups 1) worksite culture, environment, and policies that influence healthy eating and physical activity; and 2) barriers that reduce worker participation in workplace health promotion programs. An understanding of how the workplace affects health behaviors is can inform design of effective interventions to reduce and prevent obesity.\n\n【10】Methods\n-------\n\n【11】We partnered with a large health care system and a national labor union representing retail workers to recruit study participants. Qualitative data collection included interviews with key informants (eg, employer representatives, union leaders, benefits administrators) and worker focus groups with both partner organizations. The workforce in the union was relatively homogenous with regard to income and included workers in jobs such as cashier and merchandise stocker. Within the health care system, we targeted hospital work departments and locations that employed a large proportion of low-wage workers, including housekeepers, patient care technicians, and food service workers. This study was approved by the Washington University institutional review board.\n\n【12】We interviewed 10 individuals from the union partner: 4 local union leaders, 5 store representatives, and 1 health benefits administrator. Key informants were recruited in person or through email, and interviews were conducted in person or over the telephone. We asked about current and previous wellness initiatives offered to employees, employee participation in these initiatives, and potential barriers to participation. Informants were also asked about workplace factors that influenced health behaviors (ie, physical activity and healthy eating) and employee attitudes about health and wellness.\n\n【13】We conducted a total of 9 focus groups involving 61 workers. Twenty hospital employees (4 men and 16 women) participated in 4 groups. Forty-one unionized retail workers including 12 men and 29 women participated in 5 focus groups. Focus group participants were recruited through their work department, store, or local union hall. The research team attended union meetings to recruit members in person and posted flyers in break rooms at selected stores and hospital departments. We used a semistructured script to guide focus group discussions. The scripts covered 11 broad domains with follow-up questions and prompts for each domain . All group discussions were audio recorded and transcribed. Transcriptions were entered into QSR International’s NVivo 10 software (QSR International Pty Ltd), and all were coded by 2 independent raters using a predefined code book based on the domains in the focus group script. After initial coding and consensus of all transcripts, we applied a phenomenological approach for data analysis to find the “essence” or common themes across individual experiences . The purpose of the thematic analysis was to answer 2 questions: “what impacts healthy eating and physical activity” and “what can be modified at the workplace?” Through systematic review and discussion, codes were merged and grouped under main themes. Each transcript was re-read and re-coded for consistency.\n\n【14】Results\n-------\n\n【15】### Key informant interviews\n\n【16】The informants indicated that very few wellness programs related to weight management were offered to retail workers. The union-sponsored health plan covered some costs for nutritional counseling, but that benefit was not well advertised. The employer-sponsored initiatives such as an onsite gym or weight loss programs were primarily available to employees in the corporate offices, not to workers in retail stores. Both the union and employer representatives recognized the need for workplace wellness programs but were unsure about how to proceed with developing and implementing a program to reach their diverse and widespread workforce.\n\n【17】Informants described various programs available to employees but noted several barriers to effective program implementation, including lack of management commitment at some levels, limited budgets, and communication and advertising limitations. One informant described results of a focus group conducted among employees of 1 hospital department regarding awareness of existing wellness programs and preferred methods of communication; results indicated that most workers were unaware of the wellness program and did not regularly use company email, which was the primary method of communicating information about the wellness program. Workers preferred to get information via personal email, text message, or in person. Workplace wellness efforts within the health care organization varied by worksite; some sites were more successful in promoting and delivering their wellness initiatives than others. Informants thought the size of organization and motivation of appointed representatives for each location influenced program success. An informant from a smaller hospital mentioned several successful wellness initiatives at her location, including an onsite gym, exercise classes, and 2 weight-loss challenges each year, and an informant from a larger hospital discussed struggles to find effective communication methods to reach all worker groups.\n\n【18】### Worker focus groups\n\n【19】The final list of themes from the focus group analysis included 10 work-related themes and 10 general themes . Workers commonly discussed how their job characteristics contributed to their health. For example, they mentioned that physical demands and stress of their jobs left them too exhausted or unmotivated to exercise or plan healthy meals . Many also described how the physical environment affected their health (eg, small work area, concrete floors). Past or current company programs and priorities was another common theme identified, although details varied by group. Overall, the retail workers talked about lack of wellness programs; some mentioned store weight-loss competitions and previous company campaigns but felt that their employers and union did not prioritize health and wellness. Responses of the health care worker groups differed; those working in a large hospital setting were much less aware of wellness initiatives and felt less company or management support for health promotion. Many were aware of the onsite gym and the weight-loss program, but cost, work schedule, and home responsibilities made it difficult to participate. Conversely, a group working in a smaller clinic felt tremendous upper-management support and described numerous workplace supports, including a produce garden at the worksite, access to exercise equipment, afternoon stretch breaks, and healthy potluck lunches.\n\n【20】Workers also discussed schedules and breaks as having a significant impact on their healthy eating and physical activity. For many retail workers, their schedules varied week-to-week, making it difficult to maintain any routine. Workers from both organizations stated that short and interrupted breaks made it difficult to eat healthy. They discussed how food options —healthy or unhealthy and purchased or provided for free (eg, incentive lunches, holiday parties) — affected their eating behaviors at work. Workers from both organizations felt that their workplaces had a lack of quick, convenient, and low-cost healthy food options. Moreover, in all groups we heard that free food was almost always unhealthy. Nearly all workers commented that social support and accountability to coworkers would improve their ability to initiate and maintain healthy behaviors.\n\n【21】General themes were those that may be related to the workplace but also extended into workers’ personal lives. For example, workers often discussed how intrapersonal factors (eg, motivation, willpower) and home life (eg, responsibilities, family support) affected their health behaviors both in the workplace and at home. Workers often discussed how their jobs influenced their health in terms of not having the money, time, or energy to exercise or plan healthy meals. Some workers also discussed the roles that health issues and transportation played in initiating and sustaining healthy behaviors.\n\n【22】Discussion\n----------\n\n【23】This study highlights factors related to obesity as described by 2 low-wage work groups; our findings are consistent with results from a similar study among low-wage workers in various industries . The workplace was often viewed as a barrier to healthy eating and physical activity; however, workers supported the concept of workplace health promotion and offered suggestions for overcoming many of the identified barriers. As demonstrated in this study, the workplace may be effective in engaging populations at risk for obesity and related illnesses, though it may be necessary to go beyond traditional workplace wellness approaches. Using more innovative methods may increase program reach, effectiveness, and sustainability.\n\n【24】Policy changes have increasingly been recognized as essential components of worksite health promotion  and are more sustainable than individual-level behavior interventions . Policies promoting a culture and environment conducive to reducing obesity can be a strong catalyst to behavior change. These can include top-level policies, such as offering a health care plan that has wellness options or implementing organizational policies that provide for access to low-cost healthy foods at the worksite, encourage active transportation to and from work, or allow for flexible work schedules to encourage lunch or break-time physical activity. The work environment (both indoor and outdoor) is also an important component of behavior change and can have a significant impact on behavior choice . An environment that encourages less sedentary work and more physical activity could include well-placed and maintained stairwells for stair use versus elevators or distant parking.\n\n【25】Changes solely in the workplace environment may not be enough to encourage healthy behaviors . Health behavior decisions are affected by the social context in which they are made, such that the social support and social norms surrounding a health issue have a substantial effect on how that health behavior is perceived. Changing social norms and fostering a supportive work environment for the desired behavior is a necessary complement to the other levels of intervention. Social norms have been studied as a way to promote nutrition  and physical activity .\n\n【26】Workplace participatory approaches may foster social support and help to overcome organizational and employee barriers to program success. Most worksite weight-loss programs have relied on a top-down approach, rather than a participatory approach based on employee involvement in the design of interventions . In workplaces where employees generally have little influence on their work environment, similar to those sampled in this study, participatory approaches can result in better program implementation and subsequent health improvement . The recently described Healthy Workplace Participatory Program (HWPP) includes work environment changes, as well as healthy eating and physical activity interventions . A small study based on HWPP found promising changes in behaviors and weight loss in a pre–post evaluation of a participatory worksite intervention . To our knowledge, this HWPP-based study is the only controlled study to date using a worker health participatory program to attain weight loss. Future research should implement and evaluate workplace participatory interventions for weight loss.\n\n【27】Workplace wellness programs should also use effective communication strategies to engage workers from diverse work groups and backgrounds. As demonstrated with the health care system in this study, many low-wage workers were not aware of the wellness programs that were available to them. The same programs, however, have good participation from other work groups in the health care organization, primarily because of the method of communication. Rapid changes in information technology have enabled new interventions that use mobile telephones and other mobile devices (mHealth). These techniques show great promise for weight reduction in low-income populations , and such interventions are readily scalable to larger populations .\n\n【28】Although we did not directly ask about incentives, several participants discussed monetary incentives as a possible motivator to eat healthy and exercise. The use of incentives is common in workplace wellness programs; employers could maximize the benefits of incentives by incorporating lessons from behavioral economics. For example, the increasingly popular approach of delivering incentives through health insurance premium adjustments is unlikely to be as effective as more frequent and immediate rewards for behavior. This is because people tend to discount the future, meaning that they respond more readily to immediate than delayed costs and benefits . The participants in our study commonly discussed cost as a barrier to eating healthy and exercising. As suggested by others , low-income workers may be more likely to change and sustain healthy behaviors if provided with financial support for healthy food and participation in other weight-loss activities. Employers should also be aware of the limitations of incentives for behavior change. Recent reviews have shown behavioral effects to be relatively short-lived after incentives are removed , and considerable attrition is found in workplace programs for weight loss . More research is needed to determine the optimal timing, magnitude, and structure of incentives, but results to date suggest that incentives may need to be an ongoing feature of the workplace to have maximum impact.\n\n【29】Finally, employers may consider integrating traditional occupational safety and health programs (ie, those that focus on health hazards unique to the workplace) with health promotion and wellness programs (ie, those that focus exclusively on lifestyle factors off the job). The Total Worker Health program was launched by the National Institute of Occupational Safety and Health (NIOSH) to support the development and adoption of research and best practices to integrate these approaches and address health and safety risks at multiple levels, including the work environment (physical and organizational) and individual behaviors. This integrative approach may lead to greater adoption of interventions by management and workers and hence to improvements in the health of workers , but more research is needed to evaluate both the development process and the effectiveness of integrated programs .\n\n【30】The results of this study can help inform future worksite interventions for low-wage workers; however, our study has several limitations. First, we collected data from key informants who could be contacted or agreed to be interviewed. Second, although the participants in the focus groups represented a range of positions and worker groups, they were limited to those available during the implementation of the focus group discussions. Although using a convenience sample may be a limitation, those who elected to participate in the interviews or focus groups were able to provide helpful insights on the topic. Future intervention planning would need to be preceded by additional input from a broader participant base. Third, the information we collected may not be generalizable to other health conditions or work settings. Despite these limitations, the key informants and focus group participants provided rich and potentially actionable information on addressing obesity at the worksites of these worker populations.\n\n【31】Workplaces can provide an effective venue for engaging low-income populations at risk for obesity and related illnesses. Results of this study suggest that future worksite interventions for low-wage workers can improve reach, effectiveness, and sustainability if they embrace more innovative methods than those used in current workplace wellness programs. Future interventions should address workplace policies and environment and social norms that affect health behavior decisions. Communication strategies and financial incentives should be better aligned with the needs of low-wage workers. Workplace participatory programs are a promising approach to engage workers in health improvement.\n\n【32】Tables\n------\n\n【33】#####  Table 1. Focus Group Domains and Questions, Qualitative Study of Low-Wage Workers, St. Louis, Missouri, 2013–2014\n\n| Domain | Questions | Examples, Clarification, Follow-ups, Probes |\n| --- | --- | --- |\n| Work schedule | Tell us about a typical work day. | How many hours do you usually work? What opportunities do you have for breaks? |\n| Healthy eating priority | Is eating healthy a personal priority for you? | Do you try to eat healthy? What do you do at home to eat healthy? Are you satisfied with your diet? |\n| Eating at work | When do you eat while at work? What do you eat while at work? | How do you decide what you will eat while at work? |\n| Exercise priority | Is regular exercise a personal priority for you? | Do you try to exercise? How often, where do you exercise? Are you satisfied with your level of physical activity? |\n| Physical activity at work | What kind of physical activity/exercise do you do at work? | Do you do anything in addition to your normal work routine to be more physically active? (eg, take the stairs, walk during break times) |\n| Worksite health facilitators | What aspects of work at \\[organization\\] seem to help you or your coworkers stay healthy while at work? | Current wellness or safety programs that are helpful? Helpful aspects about physical environment or company policies that promote health? What qualities of your job make you feel good? Keep you fit? Do your work relationships contribute to health? How? |\n| Worksite health barriers | Which aspects of your work or work environment get in the way of being healthy? | Are there things about your work tasks or the way work is organized that make it difficult for you to take care of your health? What aspects of work prevent you from engaging in healthy activities outside of work? |\n| Health concerns | What health issues are you most concerned about for yourself? | How concerned are you about missing work due to illness/injury? |\n| Current wellness programs | Are you aware of any health and wellness programs currently or previously offered to employees? (ie, weight-loss, smoking cessation) | Have you or any of your coworkers participated in any of these wellness programs? |\n| Communication | How does your employer communicate important information to you? | What about health information? |\n| Future workplace programs | How likely are you to participate in workplace wellness programs in the future? What about nutrition and exercise programs, specifically? | What factors might influence your decision to participate? (ie, cost, location, other). How can your employer/union do a better job of promoting wellness in employees? |\n\n【35】#####  Table 2. Main Focus Group Themes and Number of Associated Coded References, Qualitative Study of Low-Wage Workers, St. Louis, Missouri, 2013–2014\n\n| Theme (N) a | Topics Included |\n| --- | --- |\n| **Work-related theme** | **Work-related theme** |\n| Job characteristics  | Physical and mental demands, stress, physical environment, safety, workplace rules |\n| Company priorities and programs  | Company health promotion programs, perception of company priorities for employee health |\n| Food options  | Food options at work (free or available for purchase) |\n| Communication  | Communication of health information, preferred methods of communication |\n| Work schedule  | Schedule, time of day worked |\n| Social support/accountability  | Desire for social support or being held accountable, camaraderie |\n| Management support  | Perception of management support, employee–management relationships |\n| Facilities  | Aspects of current facility related to health or suggestions for changes to facilities |\n| Breaks  | Relationship between breaks and health behaviors |\n| Other  | Knowledge from job, suggestions for general workplace changes |\n| **General theme** | **General theme** |\n| Intrapersonal  | Motivation, willpower, impulse, desire to be healthy/look good |\n| Financial  | Company discounts, cost of food, gym memberships |\n| Home life  | Cooking at home, food restrictions, outside environment, other priorities/responsibilities |\n| Time  | Not enough time, availability of quick options |\n| Energy  | Lack of energy, need energy |\n| Food preferences  | How eating habits/preferences affect food choices |\n| Planning  | Lack of routine, difficulties of planning, reasons behind planning or not planning |\n| Convenience  | Convenience of food options, wellness programs; choices that require little effort |\n| Personal health  | Physical and mental health as barriers to eating well or participating in physical activity |\n| Transportation  | Influence of transportation on participation in wellness programs |\n\n【37】a  N = number of times this theme was referenced.\n\n【38】#####  Table 3. Sample Comments and Coded Themes, Qualitative Study of Low-Wage Workers, St. Louis, Missouri, 2013–2014\n\n| Comment | Theme Coded |\n| --- | --- |\n| “If any employer is really serious about wanting a healthier work environment and employees then they have to make sure they have the proper rest time. I am squishing my two 15-minute breaks together to make my half-hour lunch.” | Company priorities and programs, breaks |\n| “I think I would \\[go to the workplace gym\\] because I think somebody would go with me from here. You’d have a buddy. You have so many friends inside of \\[the store\\]. I mean I have friends at other \\[stores\\] and I could be like ‘Hey, meet me at our gym.’” | Social support-accountability, company priorities and programs |\n| “When I first started working here I thought it was the oddest thing that I would walk to the cafeteria and I would see nurses, techs, eating when they are walking, eating at the elevator . but now I know why they do that, you know, ‘cause sometimes that is all the time they get.” | Breaks, time, job characteristics |\n| “And that’s another thing, they got a lot of good different varieties during the day, but at night, there is not much to choose from.” | Work schedule, food options |\n| “But it is funny because they put \\[smoking cessation ads\\] in the break room but the smokers don’t go in the break room, they go outside. So nobody saw it.” | Communication |\n| “And I have to say, she \\[upper-level manager\\] don’t throw it down your throat . I don’t think anybody does. They put the option out there and it’s your choice to participate or not. They give us the resources to use and they say here, now it is up to you They will promote something \\[monthly\\] that most of us probably didn’t know . to help us.” | Company priorities and programs, management support |\n| “I feel like not having set schedules makes it kinda hard to exercise, because sometimes you work early in the morning, sometimes you’ll work late at night. Throws off your sleep schedule.” | Work schedule |\n| “If you’re too tired and you’re stressed out, you don’t want to do anything but eat that fattening food and curl up in a little ball and go to bed. You don’t plan for tomorrow; you just have to get through the day.” | Planning, energy |\n| “I’m a food addict, I’ll admit it; I like food. I have all intents and purposes of going to the salad bar and picking the good lettuce, the good stuff, the good fruits, the good vegetables, but man as soon as that \\[BBQ smoker\\] hits me, I’m gone!” | Intrapersonal, food preferences |\n| “I prepare my lunch every morning. I work and then I actually walk every day . up to 5, 6, 7 miles every day . except for today because all of us had double shifts. So that’s it, I have the will power, I’m not gonna lie. Most people don’t know me, but I’ve dropped a ton of weight. I was quite large and I just made a goal this year that I was gonna take care of myself.” | Intrapersonal, planning, work schedule |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "246cc1c8-67c3-43d8-991b-32f23c094a31", "title": "Bombali Virus in Mops condylurus Bat, Kenya", "text": "【0】Bombali Virus in Mops condylurus Bat, Kenya\nThe virus family _Filoviridae_ is divided into 5 genera: _Cuevavirus, Marburgvirus, Ebolavirus, Striavirus,_ and _Thamnovirus_ . Six distinct Ebola viruses have been described; 4 are known to cause human disease . These include highly lethal pathogens capable of producing large outbreaks, namely Bundibugyo, Sudan, and Zaire Ebola viruses, the last responsible for the devastating 2013–2016 outbreak in West Africa and an ongoing extended outbreak in the Democratic Republic of the Congo . Although the natural reservoirs of Ebola viruses remain unconfirmed, considerable evidence supports a role for bat species, particularly fruit bats, analogous to findings implicating _Rousettus aegypticus_ fruit bats as a reservoir for Marburg virus .\n\n【1】The most recent Ebola virus to be identified is named Bombali virus (BOMV) and was reported in August 2018 in mouth and fecal swabs collected from free-tailed insectivorous bat species (family Molossidae) _Mops condylurus_ and _Chaerephon pumilus_ in Sierra Leone . Although BOMV is not known to infect humans, its envelope glycoprotein shares the same NPC1 receptor as other filoviruses and is capable of mediating BOMV pseudotype virus entry into human cells . We describe the presence of BOMV in tissues and excreta of an Angolan free-tailed bat ( _M. condylurus_ ) captured near the Taita Hills in southeastern Kenya, the easternmost distributional range of this bat species , >5,500 km from the original BOMV identification site in Sierra Leone . We also screened human serum samples collected from febrile patients in the Taita Hills area for markers of BOMV infection.\n\n【2】We identified BOMV in an adult female bat (B241) by reverse transcription PCR and next-generation sequencing. This bat was captured along with 15 others in mist nets in savannah habitat near a small river in May 2018; only this bat was BOMV positive; (6% prevalence). Viral RNA was present in lung, spleen, liver, heart, intestine, mouth swab, and fecal samples but absent from the brain, kidney, urine, and a few fleas found on the bat; viral loads were especially high in the lung . These tissue-positive findings confirm that BOMV can infect _M. condylurus_ and is not an artifact of its insect diet, which could not be discounted from the previous analysis on the basis of mouth and fecal swabs . We also screened lung samples of sympatric _C. pumilus_ bats (n = 13) and other bat species  captured from the same area in February 2016 and May 2018; all were negative for BOMV RNA. Serologic analysis revealed antibodies against Ebola virus in the blood of the tissue-positive bat , but specific antibodies were not found in blood from the other bats .\n\n【3】Our tissue-positive findings provide a strong host association between BOMV and _M. condylurus_ bats; it is possible that BOMV–positive findings from other bat species result from local spillover or contamination. Moreover, phylogenetic analysis of the full BOMV genome from the bat lung revealed 98% nucleotide sequence similarity with the prototype reported in Sierra Leone  . Considering the high sequence similarity between the 2 locations and that _M. condylurus_ bats, like most insectivorous bats, are believed to travel only short distances , BOMV is likely to be distributed throughout much of sub-Saharan Africa . However, further monitoring of _M. condylurus_ and _C. pumilus_ bats and other sympatric species across Africa is required to support this hypothesis.\n\n【4】Because _M. condylurus_ bats commonly roost in human structures, such as house roofs , human exposure to this species is more likely than for many other bat species. Therefore, we screened for markers of human infection with BOMV by studying serum samples collected from febrile patients who sought treatment at clinics in the Taita Hills area during April–August 2016. Clinics are located in the surrounding areas, all within 15 km of the BOMV–infected bat collection site . We screened patients for filovirus RNA (n = 81) and Ebola virus–specific IgG (n = 250) by an immunofluorescence assay using Zaire Ebola virus VP40–transfected VeroE6 cells as antigen . Many samples, including all those screened for filovirus RNA, were from patients who reported contact with bats in the home or workplace. We found no evidence of filovirus infection by either screening method, providing no support that BOMV easily infects humans or is a common cause of febrile illness in the area. Ongoing surveillance is nonetheless necessary, and we cannot exclude the possibility that BOMV was a recent introduction to the Taita Hills area.\n\n【5】Our results markedly expand the distributional range of this new Ebola virus to eastern Africa and confirm the _M. condylurus_ bat as a competent host. Like Goldstein et al. we stress that the virus is not known to infect humans, a premise supported by our screening of febrile patients in the Taita Hills area. Potential efforts to eradicate bats are unwarranted and may jeopardize their crucial ecosystem roles and human health .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3b6b734d-0831-427c-adb1-8883c9c20e11", "title": "Coronavirus Disease Contact Tracing Outcomes and Cost, Salt Lake County, Utah, USA, March–May 2020", "text": "【0】Coronavirus Disease Contact Tracing Outcomes and Cost, Salt Lake County, Utah, USA, March–May 2020\nBy July 2021, >33 million cases of coronavirus disease (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), were documented in the United States, and most cases involved contact tracing by health departments . Preventing SARS-CoV-2 transmission through contact tracing requires rapid diagnosis, immediate isolation of cases, and rigorous tracking and precautionary isolation of close contacts . Because SARS-CoV-2 appears to be most transmissible before and immediately after symptom onset, clinical and transmission studies have shown that timely identification of cases and contacts is essential to preventing transmission . In addition, mathematical models have shown contact tracing, when implemented with other mitigation measures, can effectively reduce community spread of SARS-CoV-2 .\n\n【1】Evaluations of contact tracing for tuberculosis and HIV have found that contact tracing is an effective and sustainable approach to transmission reduction when disease prevalence is low but that contact tracing becomes less cost-effective as disease prevalence increases compared with other approaches, such as provider-initiated testing and intensified case finding . Programmatic data on outcomes and costs of contact tracing for COVID-19 are limited but essential for aiding public health agencies in designing or improving existing contact tracing programs . We aimed to quantify contact tracing efforts in Salt Lake County, Utah, USA, to examine how contact tracing affected case-finding, evaluate key contact tracing time intervals, and estimate the staff time and salary costs required to conduct investigations.\n\n【2】### Methods\n\n【3】We examined persons with laboratory-confirmed or probable COVID-19 cases and their close contacts retrospectively by using Salt Lake County Health Department (SLCoHD) surveillance data. We quantified the yield from each index case that generated a contact investigation and created transmission chains. We also examined 25 index cases and close contacts prospectively to estimate staff time and salary cost spent in contact tracing efforts.\n\n【4】##### SLCoHD Contact Tracing Procedures and Testing Guidelines\n\n【5】During March 12–May 3, 2020, SLCoHD staff traced all reported case-patients with laboratory-confirmed SARS-CoV-2 infection and their close contacts. Close contacts of any confirmed or probable case-patients were traced until no further symptomatic or positive contacts could be identified. Early in the study period, state guidelines called for prioritizing testing symptomatic close contacts of confirmed COVID-19 case-patients. Later in the study period, testing was available to anyone with approval from their healthcare provider.\n\n【6】##### Definitions\n\n【7】We defined a confirmed COVID-19 case as detection of SARS-CoV-2 RNA by real-time reverse transcription PCR . According to the Council of State and Territorial Epidemiologists definition, a probable case is one that meets clinical criteria and epidemiologic evidence with no confirmatory laboratory testing performed for COVID-19, meets presumptive laboratory evidence and either clinical criteria or epidemiologic evidence, or meets vital records criteria with no confirmatory laboratory testing performed for COVID-19 . We defined a probable case as a symptomatic close contact to a confirmed case-patient. We defined close contacts as anyone < 6 feet of a confirmed case-patient or a symptomatic contact to a confirmed case-patient (i.e. a probable case) for \\> 15 minutes, \\> 2 days before the case-patient’s symptom onset and until the case-patient began strict isolation or until the contact’s last exposure to the case.\n\n【8】##### Index Case Identification and Transmission Chains\n\n【9】SLCoHD staff conducted contact tracing investigations via telephone interview. Interviews included 5 components: providing isolation or quarantine guidance; monitoring contacts for 14 days after their last exposure to a case, with the option for daily phone calls or text messages; entering demographic data for contacts into the Utah National Electronic Disease Surveillance System  for linkage and tracking; community notifications, including notifying businesses, workplaces, event venues, churches, or persons who might have been exposed to confirmed cases; and providing resources, such as information on housing or financial support, SARS-CoV-2 testing locations, and where and when to seek medical care.\n\n【10】We grouped contacts into 3 main categories: confirmed cases, probable cases, and contacts under observation. We further divided the 3 categories into 8 subclassifications: confirmed cases comprised index, symptomatic positive, and asymptomatic positive cases; probable cases comprised untested but symptomatic persons; and contacts under observation comprised persons who were asymptomatic not tested, symptomatic negative, or asymptomatic negative, as well as unknown status cases . Status of probable cases and contacts under observation could change during the quarantine period; for instance, a probable case could become a symptomatic positive case if the contact had a SARS-CoV-2–positive test result during the quarantine period.\n\n【11】##### Data Source\n\n【12】We used EpiTrax surveillance data to retrospectively construct COVID-19 transmission chains for all confirmed index case-patients and contacts. We abstracted demographics, exposure history, SARS-CoV-2 test results, symptoms, and underlying conditions for confirmed or probable cases. We also abstracted investigation notes and applicable dates for last exposure to the confirmed or probable case, symptom onset, symptom resolution, initial health department contact, COVID-19 tests, monitoring period, hospital admission and discharge, and death. We also identified each contact’s relationship to their respective index case-patient, such as household or nonhousehold contact and generation of contact (first through sixth generation).\n\n【13】We chose a priori to systematically select 10% of laboratory-confirmed cases diagnosed during March 12–May 3, 2020, in Salt Lake County. However, during that period, the number of cases identified in Salt Lake County grew. Our final sample represented 8% of the total 2,757 cases.\n\n【14】##### Effort Time and Cost\n\n【15】We selected 25 index case-patients and prospectively documented the time spent interviewing them and their 144 contacts, from time of initial health department interaction with the index case-patient to the end of each contact’s 14-day monitoring period. Interviewers prospectively recorded time needed to complete all 5 investigation components for the selected index cas-patients. We grouped contacts into 1 of the 8 subclassifications and applied a β-PERT distribution to Monte Carlo simulation to estimate time and staff salary required to conduct contact tracing investigations for each of the 8 disease statuses . We used the minimum, mean, and maximum time documented investigating each of the 8 disease subclassifications as parameters for the simulation . We estimated salary cost by multiplying the median wage of all staff involved in contact tracing by the total number of hours spent on the contact tracing investigation . Costs comprised time spent conducting all interviews (i.e. cost per index case and cost per contact, including those that were ultimately unreachable or out of jurisdiction) and for community notifications. We excluded nonstandardized costs, such as overhead, overtime, and time and costs for trainings.\n\n【16】##### Data Management and Analysis\n\n【17】To quantify contact tracing efforts, we evaluated the number of contacts yielded and investigated from each index case. We did not reclassify symptomatic contacts to an index case-patient if their symptom onset date was earlier than their respective index case-patient, but we did include them in the analysis. We used R  and Stata  software for data management and descriptive analysis. We calculated 95% CIs for estimated time intervals between events, such as symptom onset, testing, and initial contact, and for estimated cost per type of case or contact investigation. This activity was reviewed by the Centers for Disease Control and Prevention and was conducted consistent with its policy and applicable federal laws .\n\n【18】### Results\n\n【19】##### Index Case Identification and Contact Tracing\n\n【20】Of the 229 cases identified from the line list, 45 were excluded; 12 were excluded because the case-patient was a contact of a previously included index case and 33 because of incomplete data . Our final analysis included 184 index cases and 1,499 linked contacts. Among linked contacts, 922 were first-generation, 387 second-generation, 99 third-generation, 39 fourth-generation, 49 fifth-generation, and 3 sixth-generation contacts. Third-, fourth-, fifth-, and sixth-generation contacts were directly or indirectly linked to first-generation contacts of patients who tested positive, who had confirmed cases, or who had symptomatic but untested probable cases . Among 184 index case-patients, 153 (83%) did not have known contact with a laboratory-confirmed COVID-19 case-patient. Across all generations, we identified a median of 5 (range 0–97) contacts and a mean of 2.03 confirmed and probable secondary cases for each index case . Of 1,499 contacts, 96 were unreachable; 89 were unreachable or did not have adequate information to trace, and 7 were out of jurisdiction and did not have final disease status. Of 1,499 contacts, 374 (25%) became confirmed or probable cases, of which 285 (19%) were confirmed and 89 (6%) were probable. The rate of secondary case detection was ≈31% among first-generation contacts; ≈16% among both second- and third-generation contacts; and ≈12% among fourth-, fifth-, and sixth-generation contacts.\n\n【21】##### Disease Status at Initiation and End of the Contact’s Monitoring Period\n\n【22】Among 1,499 contacts, 277/1,027 (27%) were tested during their monitoring period . Of the 277 tested contacts, 98 (35%) were SARS-CoV-2–positive after initial health department interaction. Among the 362 (24%) SARS-CoV-2–negative contacts, 183 (51%) had tested negative before their initial health department interview and 179 (49%) tested negative after the initial interview.\n\n【23】The proportion of household contacts who were symptomatic and positive increased from 11% at initial health department interaction to 18% after the monitoring period . When comparing the final disease status of contacts exposed within their household versus outside of their household, more contacts exposed within their households received testing (23% vs. 13%) (data not shown).\n\n【24】##### Key COVID-19–Associated Dates\n\n【25】The median time from symptom onset to initial health department interaction was 7 days (interquartile range \\[IQR\\] 4–10 days) for index cases compared with 4 days (IQR 1–7.25 days) for first-generation contacts . The median time from laboratory PCR test collection to initial interview was 2 days (IQR 2–4 days) for index case-patients compared with 0 days (IQR 2–4 days) for first-generation contacts. Index case-patients generally started isolation on the day of the initial SLCoHD interview (median 0 days, IQR 0–3 days). First-generation contacts reported having quarantined themselves for a median of 0 days (IQR 0–5 days) before initial interview. First-generation contacts reported a date of last exposure as a median of 4 days (IQR 0–7 days) before the initial interview; household contacts reported a median of 1 day (IQR 0–5 days), and nonhousehold contacts reported a median of 6 days (IQR 4–9 days). The time between last exposure to isolation decreased for each subsequent generation . Among 270 contacts who reported ongoing exposure, such as persons who could not or did not isolate, 96% were household contacts.\n\n【26】##### Effort and Staffing Cost\n\n【27】We calculated time and salary cost (in USD) required to conduct contact tracing . Total time required to investigate 184 index cases and their 1,499 contacts was 1,102 staff hours at a total cost of $29,234 . Median time and cost spent investigating an index case and all successive generations of contacts was 4.16 hours (95% CI 4.06–4.72 hours) at $107.22 (95% CI $92.60–$120.70).\n\n【28】Time and costs varied depending on the status of the contact. For each index case, the median investigation time was 79.23 (95% CI 76.56–81.40) minutes and median cost was $33.67 (95% CI $32.34–$35.22). Negative asymptomatic cases required the least amount of staff time, 21.50 (95% CI 21.05–22.08) minutes costing a median of $9.29 (95% CI $9.07–$9.50). The total time spent on community notification for exposure to a confirmed case was 84.13 hours . Each notification took a median of 34.67 (95% CI 32.45–37.78) minutes, including 121 (66%) index case-patients who requested work excuse letters and 14 (7.6%) index case-patients who requested notifications to community locations, such as medical facilities, event venues, churches, and grocery stores. The average gross hourly wage for salaried epidemiologists, nurses, and office support staff involved in contact tracing efforts was $29.52 (range $23.61–$35.42) .\n\n【29】### Discussion\n\n【30】Our analysis of contact tracing of 184 index cases and 1,499 close contacts in Salt Lake County, Utah, highlights the substantial cost and time needed for these investigations. In addition, we found that, for successive generations of contacts traced, fewer cases were identified, and the time between symptom onset and SARS-CoV-2 testing decreased. However, changing quarantine or social distancing guidance during the investigation period also might have resulted in fewer cases in later generations. These findings highlight the effectiveness of contact tracing to guide control measures and reduce onward transmission of SARS-COV-2. Other jurisdictions can use these findings to examine their contact tracing yields, effort, and key COVID-19–associated time intervals to help guide programmatic changes.\n\n【31】Contact tracing is resource intensive . Every index case investigated produced a transmission chain containing a median of 5 linked contacts. The median time to investigate these transmission chains was 4.16 (95% CI 4.06–4.72) hours at a cost of $107.22 (95% CI $92.60–$120.70). During the study period, 2,757 COVID-19 cases in Salt Lake County required investigation, which we estimate to have resulted in ≈$300,000 and ≈11,500 staff hours spent conducting these investigations. The time spent by contact tracers reflects resources needed to interview, educate, and enter data for cases and contacts and to write work excuse letters and conduct community notifications. The finding of lower yields in later generations highlights the need for further studies to examine the cost-benefit of tracing multiple generations of contacts .\n\n【32】We found that 6% of contacts were unreachable or out of jurisdiction, which is lower than the 17% unreachable contacts identified through a text messaging–based system in a previous study . However, consistent with another study , we found a high proportion (83%) of index case-patients that did not have known contact with a laboratory-confirmed COVID-19 case-patient. The prevalence of cases without an identified epidemiologic link raises concerns over unrecognized transmission , which suggests contact tracing efforts alone might not be sufficient to stop disease transmission.\n\n【33】Our contact tracing yields, laboratory confirmation of infection among 19% of contacts, were higher than those in South Korea (4%), and Shenzhen (15%) and Guangzhou (17%) in China . Consistent with findings from recent studies , we found household contacts were infected at a higher rate (32%) than nonhousehold contacts (16%). The finding of higher infection rates among household contacts reinforces the importance of evaluating prevention measures, such as using hotel services for contacts unable to separate themselves from other household members . Compared with index cases (n = 184), confirmed secondary cases (n = 285) identified through contact tracing generated about one fourth of the contacts and less than one fifth of the secondary cases. During the study period, testing capacity was limited, delaying health department notifications and initiation of contact tracing investigations, which might have increased yields because case-patients spent more time not knowing their infection status . In addition, because primarily symptomatic persons received testing, positive results might have resulted in higher rates and thus higher yields.\n\n【34】Modeling shows the probability of COVID-19 control decreases with long delays from symptom onset to case isolation, fewer cases ascertained by contact tracing, and increasing transmission before symptom onset . Thus, time intervals between symptom onset, laboratory testing, and initial health department interview provide insight into the efficiency of contact tracing investigations . One study found that contact tracing for COVID-19 reduced the time to test confirmation by 2.3 days and time to contact isolation by 1.9 days . Similarly, we observed a 3-day decrease in the time from symptom onset to initial health department interview starting with first-generation contacts and noted to be the same or further decreasing in most subsequent generations. The time interval from symptom onset to initial health department interview was longer than that from symptom onset to first positive test or from symptom onset to isolation initiation. This time interval decreased between the first-generation and sixth-generation contacts; later generation contacts might have had more opportunity to follow health department recommendations and for the health department to promptly recommend testing when indicated. Although the usefulness of contact tracing in the setting of sustained SARS-CoV-2 transmission has been questioned , consistent with other studies, our findings show that contact tracing reduced transmission; only one fourth of contacts traced and quarantined experienced COVID-19–like symptoms or tested SARS-CoV-2–positive.\n\n【35】New technologies, such as mobile telephone application–based symptom monitoring and electronic contact tracing platforms, might alleviate some of the burden needed to carry out investigations. In Utah, contacts could opt to receive daily phone calls or text message notifications. Text messaging might improve efficiency by decreasing time for contact follow-up, but it requires additional resources, a robust information technology infrastructure, and strong data protection safeguards . Smartphone technology is another powerful tool for contact tracing; a widely accepted smartphone application that does not have major privacy concerns, including the collection of personal data such as location, might prove useful . In addition, technology such as point-of-care SARS-CoV-2 testing, where results can be obtained within 48 hours, could reduce laboratory turnaround time. Rapid tests aid in quickly identifying index cases and contacts to implement isolation protocols  and could improve contact tracing metrics. Online platforms that can identify how cases and contacts are linked, such as MicrobeTrace , also could aid in the management of investigations by reducing duplicative efforts, thereby improving efficiency.\n\n【36】The ongoing COVID-19 pandemic and emergence of the SARS-CoV-2 B.1.617.2 (Delta) variant have demonstrated the need for continuing layered prevention strategies, including contact tracing . Our findings can help local and state jurisdictions determine the cost, effort, and yields associated with implementing a comprehensive contact tracing program, factors that are crucial for guiding policy decisions. Our data, coupled with further cost studies, can help inform resource allocation, including staffing needs and roles, technology requirements, and strategies to evaluate cost-effectiveness. In addition, our findings can be used to develop mathematical models to determine the need to scale up contact tracing to focus on all cases and contacts or to scale down and focus only on first-, second-, and third-generation contacts, as well as to decide who to interview, such as high-risk contacts or household contacts.\n\n【37】Our study’s first limitation is that our approach might not be generalizable because Utah’s surveillance system enables linkage between cases and contacts, which might not be available in other jurisdictions; differences could also exist in contact participation across jurisdictions. Second, during March 2020, testing was available only for persons meeting initial COVID-19 symptom criteria , which might have reduced case identification and the ability to test contacts. Third, interventions such as social distancing guidance and stay-at-home-orders introduced during March–May 2020 might have decreased transmission. Fourth, information was derived from interviews, which have a potential for recall bias, including naming all contacts . Fifth, costs of contact tracing are underestimated because we could not account for overtime benefits, such as time-and-a-half pay; overhead, such as staff health insurance and facility utility costs; staff training time; time spent providing services to the community, such as time to drop off masks; and other expenditures. Sixth, we could not track how many persons complied with recommendations to self-isolate or quarantine; the ability to determine whether cases and contacts complied with recommendations would aid in further quantifying contact tracing yield and effort. Finally, patients who do not seek care, potentially because of presymptomatic or asymptomatic infection, are a further challenge to preventing additional cases because SARS-CoV-2 shedding is highest early in illness . We found that 2% of asymptomatic contacts tested SARS-CoV-2–positive and 76% of asymptomatic contacts were not tested. Therefore, the attack rate might have been underestimated given the large proportion of asymptomatic contacts who did not get tested.\n\n【38】In conclusion, our analysis highlights the importance of contact tracing to reduce transmission of SARS-CoV-2. However, the effectiveness of contact tracing is contingent upon availability of substantial resources and rapid testing capacity. Persons should seek testing as soon as they experience COVID-19–like symptoms and begin isolation while results are pending. Because of early viral shedding, health department messaging should strongly direct contacts to obtain testing when possible, especially contacts with a higher risk for exposure, such as caregivers within households, populations in congregate settings, and contacts with underlying conditions; or for contacts who have an occupation requiring them to be in contact with other vulnerable persons, such as long-term care facility workers, daycare workers, and those who work with unvaccinated persons . Contact tracing metrics evaluated in this study can help other jurisdictions design, improve, and scale up contact tracing programs as needed for their specific epidemiologic contexts. Health departments should consider adjusting their approach to contact tracing as the situation evolves and adopting new technologies as these become available.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7176b811-309b-43cd-b430-3e2269889686", "title": "Mycobacterium neoaurum Contamination", "text": "【0】Mycobacterium neoaurum Contamination\n**To the Editor:** In reviewing \"Rapidly Progressive Dementia due to _Mycobacterium neoaurum_ Meningoencephalitis,\" by Heckman et al. I found, contrary to the authors' conclusion, that _M. neoaurum_ was more likely a contaminant than a cause. First, within the granulomatous brain lesions, the strongest evidence for the authors' conclusion, no acid-fast bacilli were isolated or identified on special stains; thus, the Kochs postulates were not satisfied. Rather, the lesions were likely rheumatoid nodules. Longstanding rheumatoid arthritis commonly causes granulomalike rheumatoid nodules. I did a PubMed search using \"rheumatoid nodule in the brain\" and 7 articles were found . A \"rheumatoid endarteritis\" search found 25 articles. Heckman et al. failed to exclude or discuss this possibility.\n\n【1】Second, _M. neoaurum_ is a rare environmental mycobacterium that grows in ≤2 days on sheep blood agar and is not difficult to culture. As the authors stated, there have been 8 reports of this organism, 7 isolated from blood and 1 from urine. The blood isolates were associated with either central venous catheter or intravenous drug use. Thus, _M. neoaurum_ is of low virulence and unlikely to cause spontaneous infection in tissue unless inoculated accidentally, perhaps. Third, polymerase chain reaction (PCR) is exquisitely sensitive and prone to contamination. The problem is worse when bacterial DNA is amplified by using highly conserved primers. The PCR reagents, from the Taq polymerase (of bacterial origin) to water, contain sufficient, despite minute quantity, bacterial DNA to be amplified . Although direct sequencing of the amplicon is often blurry because of its low quantity and mixed content, when cloned, each amplicon may be ligated to the vector and proliferates and gets sequenced later.\n\n【2】Therefore, I believe the presence of _M. neoaurum_ DNA, not the organism itself, represented contamination. Generally, drawing cause-disease conclusion based on PCR sequencing needs vigilance to satisfy the modified Koch postulates .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a2eea334-62b5-4184-bdab-9219d19774ea", "title": "Transmission Dynamics, Border Entry Screening, and School Holidays during the 2009 Influenza A (H1N1) Pandemic, China", "text": "【0】Transmission Dynamics, Border Entry Screening, and School Holidays during the 2009 Influenza A (H1N1) Pandemic, China\nPandemic influenza A (H1N1) 2009, hereafter referred to as A(H1N1)pdm09, spread rapidly, resulting in millions of cases and ≈18,000 deaths in ≈200 countries . On August 10, 2010, the World Health Organization (WHO) declared that the world had entered the postpandemic period . Much has been published about the epidemiology of the pandemic in Western countries , but far less has been published about the experience of a large and diverse country, such as the People’s Republic of China. In addition, although many countries adopted so-called early containment strategies, data on their effectiveness are rare .\n\n【1】In response to the evolving global spread of A(H1N1)pdm09 virus infection, China established national surveillance on April 30, 2009. Initially, the country implemented an aggressive containment strategy based on the national pandemic preparedness plan, including isolation of all suspected case-patients in designated hospitals, contact tracing, medical observation of persons exposed to patients with confirmed cases, and border entry screening . On May 11, the first case of A(H1N1)pdm09 in mainland China was identified in a traveler returning from the United States . We report the transmission patterns of A(H1N1)pdm09 in China from that time through November 2009 and analyze the effectiveness of border entry screening and holiday-related school closures on transmission using multiple data sources from surveillance systems and specific investigations.\n\n【2】### Methods\n\n【3】##### Sentinel Surveillance for Influenza-like Illness\n\n【4】National sentinel hospital-based surveillance for influenza-like illness (ILI) was launched in China in 2005. This type of surveillance is primarily dedicated to virologic surveillance with a goal of providing information for annual WHO influenza vaccine selection . Each week, 193 sentinel hospitals in 30 provinces report the total number of outpatient visits and the number of those patients with ILI by age group to a centralized online system maintained by the Chinese Center for Disease Control and Prevention (China CDC). In addition, respiratory specimens are collected each day from the first or second ILI case-patient who visits each hospital’s outpatient clinic. This collection results in virologic samples from 10–15 respiratory tract specimens per hospital each week. Specimens are sent to 1 of the 62 province- or prefecture-level disease control centers for testing. Laboratory results are reported weekly online to China CDC. These data are collected systematically throughout the year and are an unbiased sample of the timing of influenza activity.\n\n【5】##### Individual Case-based Surveillance\n\n【6】During the early containment phase of the 2009 pandemic (until mid-July 2009), an individual case–based surveillance system was implemented. A(H1N1)pdm09 virus infection was added to China’s list of notifiable communicable diseases on April 30, 2009. Persons with suspected A(H1N1)pdm09 infection were identified through active surveillance with border entry screening and medical monitoring of close contacts exposed to confirmed case-patients or through passive reporting by clinicians when those patients sought health care. Any person entering China was required to undergo screening at the border (any point of entry into China from another country or from a neighboring region, such as Hong Kong Special Administrative Region), regardless of border type or travel mode. All patients with suspected A(H1N1)pdm09 virus infection, regardless of its clinical severity, were admitted to designated hospitals for containment . Upper respiratory specimens were collected and sent to the national sentinel ILI surveillance network of 62 laboratories for A(H1N1)pdm09 testing by real-time reverse transcription PCR (rRT-PCR) . All suspected and laboratory-confirmed cases were reported online within 24 hours to China CDC by public health officers in county-, prefecture-, and province-level disease control centers and clinicians nationwide. Data posted on a standardized reporting card included sex, age, place, overseas travel history, and date of symptom onset.\n\n【7】##### Outbreak Surveillance\n\n【8】In accordance with recommendations from the Ministry of Health of China, local disease control centers were asked to investigate all institutional or community outbreaks (e.g. associated with particular schools or shared public transport vehicles) by using the case definition for acute respiratory illness (ARI). Data on all suspected cases, probable cases, and confirmed cases were reported online to China CDC.\n\n【9】##### Investigation of Cases Linked to International Travel\n\n【10】In addition, through July 31, a joint team from local disease control centers and China CDC investigated confirmed international travel–related cases  to collect detailed epidemiologic information. A standardized questionnaire was used to collect data about international travel histories, date of symptom onset, and reported symptoms on arrival in China. Data on contacts were also obtained. In accordance with Ministry of Health recommendations, all close contacts of confirmed case-patients were quarantined at home or in designated hotels and monitored daily for fever and respiratory symptoms for 7 days after their last exposure to a confirmed case-patient.\n\n【11】We also learned whether the case was detected at the border. Data were not available on how these case-patients entered mainland China (e.g. by air, sea, or land).\n\n【12】##### Case Definitions\n\n【13】A case-patient with ARI had fever (temperature \\> 37.3°C), and/or recent onset of \\> 1 of the following: rhinorrhea, nasal congestion, sore throat, or cough. A case-patient with ILI had a body temperature \\> 38°C with either cough or sore throat in the absence of an alternative diagnosis. A person with a suspected case of A(H1N1)pdm09 virus infection had ARI and 1 of the following: illness onset within 7 days after travel to an area with \\> 1 confirmed A(H1N1)pdm09 cases or within 7 days after close contact with a confirmed case-patient. A person with a confirmed case had ARI and laboratory evidence of A(H1N1)pdm09 virus infection diagnosed by rRT-PCR of respiratory specimens. A person with a probable case had ARI that was epidemiologically linked to a patient with a confirmed case. On the basis of information about overseas travel and any identified links to other known case-patients, all reported confirmed cases were classified as international travel–related cases, individual domestic cases, and institutional or community outbreaks.\n\n【14】##### Change in Surveillance Strategy\n\n【15】By mid-August 2009, as A(H1N1)pdm09 activity expanded, the national surveillance strategy changed from individual case-based surveillance to identification of hospitalized patients who required medical treatment for complications, identification of outbreaks, and ongoing routine sentinel ILI surveillance. Only patients who required hospital care were admitted; patients with milder infection were cared for at home.\n\n【16】##### Statistical Analysis\n\n【17】The serial interval of an infectious disease is defined as the time between onset of symptoms in an index patient and onset of symptoms in an infected contact. We analyzed data on transmission among the first 47 identified clusters we investigated, each with a single index case, to estimate the serial intervals associated with 60 infected contacts.\n\n【18】We estimated the incubation period distribution using data from the 22 persons with identified single-day exposures and the 35 persons with identified multiple-day exposure intervals , excluding 3 persons with exposures implying incubation periods of >20 days . We report the posterior median and 95% credible interval (CrI) of the mean and SD of the incubation period.\n\n【19】Doubling times in case numbers were estimated from the epidemic curve of weekly ILI incidence attributable to A(H1N1)pdm09 virus infection, obtained by multiplying raw ILI data by the weekly proportion of ILI case-patients who tested positive for A(H1N1)pdm09 virus. Those estimates, along with the evidence-based assumption that the generation time of influenza A(H1N1)pdm09 had a mean of 2.6 days and an SD of 1.3 days  (consistent with data analyzed on the serial interval), were used to estimate the effective reproduction number of A(H1N1)pdm09 virus infections in China. A simple epidemic model was fitted to the A(H1N1)pdm09 virus–attributable ILI case curve on the calendar weeks before and after the National Day Holiday (October 1–8) to estimate the effect of holidays on effective reproduction numbers and reporting rates. The model is based on the observation that numbers of cases increase at a rate that is a function of the reproduction number and the generation time of the disease . From the rate of growth of case numbers observed in the epidemic and the generation time of influenza A(H1N1)pdm09, the model can be used to derive the reproduction number of the disease for different intervals. In the past, this approach has been used to estimate the reproduction number of 1918 pandemic influenza in US cities .\n\n【20】International travel–related case-patients who had symptoms on arrival were classified as either “having fever” or “without fever but having respiratory symptoms” . Frequency tables (with χ 2  tests) were constructed to examine the univariate associations between the probability of detection at the border and patients’ characteristics. Univariable and multivariable logistic regression models were used to examine potential predictors of the probability of detection at the border (fever on arrival, time between onset and arrival, age group, and province) individually and simultaneously (i.e. using univariable and multivariable regression models, respectively) and to quantify their effects.\n\n【21】### Results\n\n【22】##### Confirmed Cases\n\n【23】During May 7–November 30, 2009, a total of 71,665 persons with confirmed A(H1N1)pdm09 virus infection were reported to China CDC. Of those, 932 (1%) were related to international travel; 27,806 (39%) cases were detected during domestic outbreak investigations; and 42,917 (60%) were domestic nonoutbreak cases . The first case-patient was a traveler who returned from the United States with illness onset on May 7; the first domestic case-patient had symptom onset on May 10 . The origin of reported cases slowly shifted: most cases were international travel related until early June; in June, roughly half were international travel related and the other half were domestic; in July, most cases were domestic . The last known international travel–related case was reported on July 31, after which intensive border entry screening was gradually reduced. Irrespective of the type of case, persons 5–24 years of age were most affected, with the proportion of cases ranging from 64% in international travelers to 94% in outbreak cases . The proportion of persons 25–49 years was <12% in all categories, except international travel–related cases, for which it was 28% (likely because persons 25–49 years were overrepresented among international travelers).\n\n【24】The infection spread rapidly throughout China; 11 provinces (containing many of the most globally connected cities) reported confirmed cases in May, and all but 5 western provinces reported cases in July . By September, all provinces reported confirmed cases. Geographic variation occurred in the incidence of confirmed cases per 1 million persons throughout the epidemic, but how much this variation was caused by surveillance system variation (e.g. differences in access, use of health care, in laboratory capacity) is difficult to determine.\n\n【25】##### Sentinel ILI Surveillance\n\n【26】The percentage of visits for ILI from sentinel surveillance increased slowly from May 2009 through the end of August 2009, although the percentage was lower than that observed during the same months in 2007 and 2008 . In September 2009, ILI activity increased substantially and was higher than in the 2 previous seasons. ILI activity decreased sharply during the National Day Holiday, then rebounded at the end of the holiday period. Similar fluctuations were observed for other influenza viruses . The number and proportion of influenza-positive cases from sentinel ILI surveillance increased stably from May 2009 onward; A(H1N1)pdm09 became the predominant strain at the end of September and subsequently declined after early December.\n\n【27】##### Serial Interval and Incubation Period\n\n【28】In the household setting, the average serial interval was 2.6 days (95% CI 2.2–3.0 days, panel A). Similar results were obtained in the analysis restricted to data from the 38 clusters in which the single index case-patient transmitted infection to a single contact. The incubation period had a mean of 2.2 days (95% CrI 1.9–2.5 days) and an SD of 1.0 days (95% CrI 0.8–1.2 days) .\n\n【29】##### Transmissibility and Effect of Holidays on Spread\n\n【30】We estimated that the effective reproduction number changed from 1.25 (95% CrI 1.22–1.28) before the National Day Holiday (August 31–September 30) to <1 during that holiday (0.79; 95% CrI 0.69–0.90) and back to 1.23 (95% CrI 1.15–1.32) after that holiday (October 7–October 25) . The National Day Holiday was therefore found to reduce the effective reproduction number by 37% (95% CrI 28%–45%). Our model also predicted that underreporting had increased by 19% (95% CrI 6%–31%) and 32% (95% CrI 11%–48%), respectively during the first and second calendar weeks of the National Day Holiday. However, the 8-week summer school holiday appeared to have had a limited effect on transmission as measured by A(H1N1)pdm09 virus–attributable ILI incidence, in contrast to what was observed in other countries, such as the United Kingdom . The doubling time during the summer school holiday (8.7 days during July 13–August 30) was similar to that observed in the month after schools reopened in September (7.1 days) . Using the rate of growth observed during July–August , we extrapolated the A(H1N1)pdm09 virus–attributable ILI case curve back in time and inferred that the first sentinel-detected ILI case caused by A(H1N1)pdm09 virus occurred in China in week 19 (May 11–17), near the date when the first imported case was detected (May 11).\n\n【31】##### Effectiveness of Border Entry Screening\n\n【32】International travel–related cases were detected either at the border or later by contact tracing and passive case finding within the country. Overall, 37% of international travel–related cases ever detected were detected at the border. The timing of onset of symptoms affected the probability of detection by symptom screening at the border. Half (468/932) of international travel–related case-patients ever detected had onset of symptoms \\> 1 days after arrival .\n\n【33】Among international travel–related case-patients who had symptoms on arrival, those with fever were significantly more likely to be detected at the border. The percentage of patients detected at the border was as follows: 76% for those with fever, 63% for those without fever but with respiratory symptoms (χ 2  4.41; df 1; p = 0.036; n = 464). Overall, 74% of persons ever detected with symptom onset on or before the day of arrival were identified at the border. Multiple logistic regression modeling showed a significant interaction (p = 0.023) between whether a case-patient had a fever on arrival and the time between symptom onset and arrival (stratified by those with onsets 0 or 1 day before arrival and those with onsets >1 day before arrival). Thus, if a case-patient had a fever on arrival, then the time since onset was irrelevant. Similarly, the odds ratios (ORs) were similar for those with fever on arrival and onset 0 or 1 day before arrival (OR 1.80) relative to those with no fever and onset 0 or 1 day before arrival) and those with fever on arrival and onset >1 day before arrival (OR 1.91 relative to those with no fever and onset 0 or 1 day before arrival) . However, among persons who did not have a fever on arrival, those who had been ill longer before arrival (>1 day) were more likely to be detected at the border (the percentage of detection = 83%; OR 2.36, Table 3 ). After adjusting for these effects, neither age group nor province affected the probability of a case being detected at the border.\n\n【34】### Discussion\n\n【35】We described transmission patterns of A(H1N1)pdm09 virus infection in China during 2009 by using multiple epidemiologic data collected from surveillance and investigations. The age distribution and transmission dynamic parameters, including incubation period and serial interval, are consistent with those observed in other countries .\n\n【36】We can put an upper boundary only on the effectiveness of the border screening adopted early in the pandemic because data are available only on cases detected (either at the border or later through case-tracing), rather than missed cases. Given that travelers with mild illness or subclinical infection might not seek health care, a substantial proportion of international travel–related cases were likely never detected and therefore did not appear in our dataset. Hence, the proportion of imported cases that were detected at the border was, at most, 37%. Assuming the doubling time of the global epidemic in May was similar to that seen in China during July–August (8.7 days), and if the border screening reduced transmission from case-patients with imported cases by 37% (i.e. isolation of detected cases was 100% effective), the epidemic in China would have been delayed by 4 days (the additional time taken for cumulative imported cases to reach the level they would have reached in the absence of border controls). Thus, border controls likely delayed the epidemic by only a few days, even assuming few imported cases were missed altogether. This conclusion is supported by the observation that the trajectory of the epidemic in China appears relatively similar to that seen in the United Kingdom, another country to which infection had spread early in May but that did not employ border screening. Clearly, symptom-based border screening cannot detect infections among persons who are asymptomatic on arrival.\n\n【37】Our analysis suggests that the October national holiday might have reduced transmission by as much as 37% and reporting by ≈20%–30%. The National Day Holiday in China is similar in scope to the Christmas holiday in Western countries, with all kindergartens, schools, and universities and many businesses being closed. Most citizens leave their routine work, but festivals, mass gatherings, and travel occur during this period. However, the Summer School Holiday appears to have reduced transmission by a minimal amount (no more than 3% reduction in the effective reproduction number), in contrast to the large drop seen in other countries such as the United Kingdom. Why this discrepancy exists are unclear but might relate to the much more frequent use of collective childcare and summer camps and schools by Chinese parents during summer holidays than is typical in many European countries. In addition, seasonal factors that can limit transmission in temperate countries in summer might have had more limitedly affected the southern subtropical provinces of China.\n\n【38】The effective reproduction number for A(H1N1)pdm09 in China ranged from 1.2 to 1.3, which is consistent with that observed in other countries, although in the lower range. In comparison, the effective reproduction number was ≈1.4 in the United Kingdom in June–July 2009 . Because the proportion of the population <15 years of age is similar in both countries, demographic differences would not appear to explain these differences. However, spatial heterogeneity in the efficiency of spread and desynchrony between the epidemics in different regions of China might lead to the underestimation of transmissibility on a national scale. This remains a topic for future analysis. We relied on A(H1N1)pdm09 virus–attributable ILI incidence to estimate the epidemic growth rate because the proportion of ILI case-patients who tested positive for influenza increased substantially during the pandemic . As a consequence, the growth rate of the ILI incidence curve underestimates the epidemic growth rate . A similar approach was used by Baguelin et al.\n\n【39】Our study has several limitations, which are inevitable, given that many of the data were collected as part of public health control rather than specifically to inform epidemiologic characterization of the pandemic. Case-based surveillance established by many countries in the early phase of the pandemic was critical to monitor early emergence and extent of geographic spread. However, in retrospect, those systems were not able to monitor the growth in case numbers over time because the ability to identify cases and conduct outbreak investigations could quickly be limited by saturated resources, for example, laboratory diagnostic capacity. Furthermore, the change from reporting individual cases regardless of clinical severity to reporting hospitalized cases likely affected the reporting rate of confirmed A(H1N1)pdm09 cases during mid-July and mid-August. In contrast, sentinel surveillance was not influenced by the change in case-based surveillance during the pandemic. However, for a country as large and diverse as China, some geographic variability is almost unavoidable in the quality of the surveillance system and capacity of health care system. This variability could make comparison of incidence levels by geographic zone somewhat difficult.\n\n【40】Improving and monitoring the homogeneity of the Chinese surveillance and health care system are challenging, yet vital, tasks to improve the monitoring of future pandemics. The effects of other interventions also need to be assessed, for example, strict case isolation, contact tracing, and medical observation, which might have helped delay the spread at early containment stage of the pandemic.\n\n【41】Thus, the overall picture of the epidemiology and transmission dynamics of A(H1N1)pdm09 that emerges from the surveillance data is comparable to that in many European countries and the United States. Border entry screening during the influenza pandemic delayed spread in China by a few days, at most, but the autumn school holidays reduced the effective reproduction number by ≈40%.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "313701da-cb02-4036-8b22-24f47b8f0bce", "title": "Apparent Triclabendazole-Resistant Human Fasciola hepatica Infection, the Netherlands", "text": "【0】Apparent Triclabendazole-Resistant Human Fasciola hepatica Infection, the Netherlands\n**To the Editor:** In December 2007, a 71-year-old sheep farmer sought care with a 4-month history of intermittent right upper quadrant pain, night sweats, anorexia, and a 5-kg weight loss. His medical history was unremarkable, and he had not traveled outside the Netherlands for ≈30 years. Physical examination revealed no abnormalities.\n\n【1】Blood tests showed an elevated erythrocyte sedimentation rate of 35 mm/h (reference 1–15 mm/h), normocytic anemia (hemoglobin 7.0 mmol/L \\[reference 8.5–11 mmol/L\\]), and eosinophilia (2.5 × 10 9  cells/L \\[reference 0.0–0.5 × 10 9  cells/L\\]). Levels of alkaline phosphatase, γ-glutamyl transferase, and alanine aminotransferase were elevated (146 U/L \\[reference 10–120 U/L\\], 143 U/L \\[reference 5–50 IU/L\\], and 54 U/L \\[reference 0–45 U/L\\], respectively). Levels of bilirubin and aspartate aminotransferase were normal. Computed tomography of the liver showed several irregularly shaped low-attenuating lesions ranging in size from 1 to 4 cm. High titers of IgG (640 \\[cutoff 40\\], determined by enzyme immunoassay) against _Fasciola hepatica_ were detected. Subsequently, _F. hepatica_ eggs were detected in fecal samples.\n\n【2】The patient, who spontaneously had become asymptomatic shortly after seeking care, was treated unsuccessfully with the benzimidazole derivative triclabendazole (TCBZ) on 3 separate occasions during the next 2 years. He was first treated with a single dose of 10 mg/kg TCBZ (Fasinex suspension; Novartis Animal Health Ltd. Surrey, UK), then with 2 doses 24 hours apart, and on the last occasion with 2 doses of TCBZ (Egaten; Sipharm Sisseln AG, Sisseln, Switzerland) 10 mg/kg 12 hours apart; the last 4 treatments were taken with food. Feces remained positive for _F. hepatica_ eggs after each treatment. IgG titers remained positive (320, by enzyme immunoassay), and flukes could be visualized by ultrasound in the gallbladder and common bile duct . Thereafter, the patient was treated with nitazoxanide (500 mg 2×/d for 7 days); however, fecal samples remained positive for _F. hepatica_ eggs. Lastly, after recent experiments of a combination therapy in a rat model , we treated the patient with TCBZ (Egaten, 10 mg/kg) combined with ketoconazol 10 mg/kg taken with food. Still, his feces remained positive for _F. hepatica_ eggs.\n\n【3】Fascioliasis is a zoonotic disease caused by the foodborne trematode _F. hepatica_ or _F. gigantica_ , which has a complex life cycle and mainly affects sheep and cattle . Eggs of the adult worms (2–4 cm) that live in the bile ducts of the final host are excreted in the feces and develop into larvae (miracidia) in water. The miracidia then penetrate, and further develop in, snails of the family Lymnaeidae. Free-swimming cercariae exit the snail and attach to aquatic vegetation, where they encyst as metacercariae. After ingestion by the host, they excyst in the intestine and migrate through the intestinal wall to the liver, where they mature into adult flatworms that reside in the bile ducts .\n\n【4】Fascioliasis affects millions of humans worldwide ; however, fascioliasis acquired in the Netherlands has been reported only sporadically , even though _F. hepatica_ infection in sheep and cattle is prevalent there . The patient in this report had not eaten watercress or other aquatic plants and had not ingested ditchwater. However, he had worked in and around ditches on farms in the area, admitted chewing grass sporadically, and might have occasionally ingested vegetables previously fertilized with livestock manure. The patient remains asymptomatic but infected.\n\n【5】TCBZ is the treatment of choice for fascioliasis. In a review by Keiser et al. the efficacy of treatment with TCBZ was shown to yield egg clearance in 78%–100% of patients after 1 dose of 10 mg/kg and in 92%–100% after 2 doses of 10 mg/kg each 12 or 24 hours apart. In livestock, TCBZ resistance is being reported increasingly . Mass treatment of sheep and cattle with TCBZ (Fasinex) or in combination with other anthelmintic drugs is common in the Netherlands , and the first cases of resistance were described in 1998 in sheep and cattle in the province of North Holland, the area of residence of the patient reported in this study . During 1998–2004, resistance to TCBZ, proven by fecal egg count reduction tests, was found on 14 farms in the same area .\n\n【6】The findings in this case are most likely explained by TCBZ resistance, although we note that repeated TCBZ courses are not 100% effective against fascioliasis . Re-infection can be excluded because fecal samples were tested for eggs 1–3 months after each treatment. This description of apparent TCBZ-resistant fascioliasis in a human highlights the human health implications of (massive) anthelmintic use in livestock.\n\n【7】Further studies on TCBZ resistance and on therapeutics for fascioliasis need to be conducted. In addition, the role of antimicrobial drugs in the treatment of livestock needs to be more rigorously evaluated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "46cdd1ac-26c2-41f5-bdc0-dcf03bc5073e", "title": "Estimated Annual Numbers of Foodborne Pathogen–Associated Illnesses, Hospitalizations, and Deaths, France, 2008–2013", "text": "【0】Estimated Annual Numbers of Foodborne Pathogen–Associated Illnesses, Hospitalizations, and Deaths, France, 2008–2013\nFoodborne pathogens are of public health concern worldwide . Estimates of the total number of foodborne illnesses and associated hospitalizations and deaths are needed to assess their effect on health and to set priorities for surveillance, prevention, and control strategies. In 2000, the number of foodborne illnesses and associated deaths in France was estimated by using data from 1990–2000. However, for most pathogens, data were lacking to derive estimates at the population level .\n\n【1】Since that study, specific surveillance systems have been implemented in France for _Campylobacter_ spp. hepatitis A virus , and hepatitis E virus  . Additional surveys have been conducted to provide information on healthcare-seeking behavior and the incidence of acute gastroenteritis in the general population  ( _5_ ) and on physician practices in requesting fecal samples for patients with acute gastroenteritis  . Furthermore, the quality and availability of other nonspecific data sources (e.g. hospital discharge registers and health insurance reimbursement data) have improved and are increasingly used for epidemiologic studies in France . Thus, recent and valid data are available to estimate the population-level health effects of several foodborne pathogens. Such estimates have recently been generated for _Campylobacter_ spp. and nontyphoidal _Salmonella_ spp. (hereafter referred to as _Salmonella_ spp.), the 2 main causes of foodborne bacterial infections in France . Taking into account this improved knowledge and data availability, we conducted a study to estimate the annual number of illnesses, hospitalizations, and deaths associated with 15 foodborne pathogens in France.\n\n【2】### Methods\n\n【3】Using data sources from 2008–2013, we estimated the number of illnesses, hospitalizations, and deaths in France resulting from 15 foodborne pathogens: 10 bacteria ( _Bacillus cereus_ , _Campylobacter_ spp. _Clostridium botulinum_ , _Clostridium perfringens_ , Shiga-toxin–producing _Escherichia coli_ \\[STEC\\], _Listeria monocytogenes_ , _Salmonella_ spp _. Shigella_ spp. _Staphylococcus aureus_ , _Yersinia_ spp.); 3 viruses (hepatitis A virus, hepatitis E virus, norovirus); and 2 parasites ( _Taenia saginata_ , _Toxoplasma gondii_ ). We used France’s 2010 census population (62,765,235 persons) for the estimates.\n\n【4】We used different statistical models, depending on the most suitable data available for each pathogen, with many inputs to estimate the number of illnesses, hospitalizations, and deaths . For most proportions we defined a lower and upper bound and a beta distribution with 2 parameters derived from a method of moments, assuming a mean _m_ \\= (lower + upper bound)/2 and an SD = (upper bound − _m_ )/2 . We used lognormal probability distributions for model inputs derived from a national survey on acute gastroenteritis in France  and for the annual numbers of reported illnesses, hospitalizations, and deaths. For final estimates, we multiplied the distributions by using Monte Carlo simulation (10,000 iterations) with R version 3.3.2 . We report median values and use ranges between the 5th and 95th percentiles of the output distribution to define a 90% credible interval (CrI 90%  ).\n\n【5】##### Illnesses\n\n【6】To estimate the numbers of illnesses, we obtained surveillance data from the mandatory notification system ( _C. botulinum_ , _L. monocytogenes_ , hepatitis A virus, and foodborne disease outbreaks) and from national reference laboratories and their laboratory surveillance networks ( _C. botulinum, Campylobacter_ spp. STEC, _L. monocytogenes_ , _Salmonella_ spp. _Shigella_ spp. _Yersinia_ spp. hepatitis A virus, hepatitis E virus, and _T. gondii_ ). Inclusion in these surveillance systems implies that the ill person sought medical care, had laboratory testing prescribed, and had a specimen submitted for laboratory testing and that the laboratory identified the pathogen and reported the positive result to the surveillance system. These steps can be summarized into 2 multiplication factors: an underreporting factor defined as the match between the total number of laboratory-confirmed illnesses and the number of laboratory-confirmed illnesses reported to the surveillance system; and an underdiagnosis factor taking into account the proportion of cases that were not laboratory-confirmed because the patient did not seek medical advice or was misdiagnosed. We took both multiplication factors into account to estimate the number of illnesses from mandatory notification data and national reference laboratory data.\n\n【7】Previously published parameters for estimating the number of _Campylobacter_ spp.– and _Salmonella_ spp.–associated illnesses  were used as a proxy to estimate the level of underdiagnosis for _Yersinia_ spp. (using _Campylobacter_ spp. data) and _Shigella_ spp. (using _Salmonella_ spp. data). For _C. botulinum_ and _L. monocytogenes,_ we assumed that 80%–100% of the cases were in persons who sought medical care and had laboratory-confirmed diagnoses. To account for underreporting, we conducted ad hoc laboratory surveys for _Campylobacter_ spp. _Salmonella_ spp. _Shigella_ spp. and _Yersinia_ spp. and we conducted a capture–recapture study for _L. monocytogenes_ .\n\n【8】In France, cases of _B. cereus_ , _S. aureus,_ and _C. perfringens_ infection are notified only through mandatory notification of point-source foodborne disease outbreaks. For these pathogens, we assumed that the multiplier between the number of confirmed outbreak cases and the number of community cases of foodborne origin would be similar to that estimated for _Salmonella_ spp. We estimated the number of illness caused by _T. gondii_ and hepatitis A and E viruses from seroprevalence data and the number of illnesses caused by _T. saginata_ from health insurance reimbursement data for niclosamide (a drug used to treat tapeworm infestation). We used data from the literature to estimate the number of illnesses caused by STEC. To estimate the number of norovirus cases, we applied a proportion (14%–22%) of norovirus-associated acute gastroenteritis cases to the annual number of acute gastroenteritis illnesses in France . This proportion was based on findings from a 2008–2009 community study in the United Kingdom  and a meta-analysis of 175 studies published during 1990–2014 . Model inputs used for each pathogen are presented in Technical Appendix Table 1.\n\n【9】##### Hospitalizations\n\n【10】We used the French Hospital Information System (FHIS) as the main data source for estimating the number of hospitalizations. The system is a national database of hospital records that contains sociodemographic information (age, sex, and residence area) and medical information (main cause for admission, concurrent medical conditions, modes of admission, and discharge) . Diseases are coded according to the International Classification of Diseases, 10th revision . We extracted all hospital records with a patient discharge date during January 2008–December 2013 and containing an ICD-10 code of interest as the main cause for admission or as a concurrent medical condition.\n\n【11】We used the number of hospital records with pathogen-specific ICD-10 codes to estimate the annual number of hospitalizations for 8 pathogens, 4 of which cause acute gastroenteritis . We did not redistribute records with only unspecified gastroenteritis codes to the 8 pathogens, but we did correct for undercapture, taking into account the proportion of fecal samples tested for each pathogen and the sensitivity of fecal culture. When data were available, we compared trends over time and patient age and sex distributions of the hospital data with surveillance data from the national reference laboratories ( _Campylobacter_ spp. _Salmonella_ spp. _Shigella_ spp. _Yersinia_ spp. and hepatitis E virus) and with mandatory notification data (hepatitis A virus).\n\n【12】We used the number of hospital records with acute gastroenteritis–associated ICD-10 codes (A00–A06.2 and A06.9–A09.9) to estimate the annual number of persons hospitalized for acute gastroenteritis. We then divided that number by the total number of persons with acute gastroenteritis to estimate the percentage of those persons who were hospitalized (0.58%–0.75%) . For norovirus, _B. cereus_ , _C. perfringens_ , and _S. aureus,_ we applied the proportion of hospitalizations for acute gastroenteritis to the annual number of illnesses for each pathogen to estimate the annual number of hospitalizations. For STEC, we used the proportion of hospitalizations estimated for _Salmonella_ spp. and _Campylobacter_ spp. as a proxy. For _C. botulinum_ and _L. monocytogenes_ , we used surveillance data from the mandatory notification system .\n\n【13】##### Deaths\n\n【14】We explored death certificate data from the French national mortality database ( **Institut National de la Santé et de la Recherche Médicale** , CépiDc \\[Epidemiology Center on Medical Causes of Death\\]) and data from FHIS to estimate the number of foodborne illness–associated deaths. For both data sources, we extracted all records for 2008–2013 with an ICD-10 code of interest as the main cause of death or hospitalization or as a concurrent medical condition. Compared with data from FHIS, death certificates contained fewer pathogen-specific ICD-10 codes; therefore, we used the hospital information system data as the main data source for estimating the number of deaths.\n\n【15】To estimate the number of deaths from _Campylobacter_ spp. _Salmonella_ spp. _Shigella_ spp. _Yersinia_ spp. hepatitis A and E viruses, _T. saginata_ , and _T. gondii_ infections, we used the number of hospital records with a pathogen-specific ICD-10 code and death shown as the mode of discharge _._ To estimate the number of norovirus-associated deaths, we applied the proportion of deaths among hospitalized case-patients with an ICD-10 code associated with viral gastroenteritis (ICD-10 codes A08.0–A08.4) to the annual number of hospitalizations for norovirus (0.18%–0.30%). This proportion was also used as a proxy to estimate the number of deaths from _B. cereus_ –, _C. perfringens_ –, and _S. aureus_ –associated hospitalizations. For _C. botulinum_ and _L. monocytogenes_ , we used mandatory notification data to estimate the number of deaths .\n\n【16】##### Foodborne Transmission\n\n【17】To estimate the number of foodborne illnesses and associated hospitalizations and deaths, we applied a pathogen-specific proportion of foodborne transmission . For 11 of the 15 pathogens, we used estimates published in the United States in 2011 . For norovirus and hepatitis A virus, data from more recent studies were used . For hepatitis E virus and _T. saginata_ , the proportions of foodborne transmission were estimated on the basis of discussions with experts from the French Public Health Agency.\n\n【18】### Results\n\n【19】Overall, the pathogens included in our study accounted for 4.9 million cases of illness (CrI 90%  4.2–6.2 million), 42,500 hospitalizations (CrI 90%  37,242–50,526), and 368 deaths (CrI 90%  335–486) each year in France. Of those 4.9 million cases, 1.5 million were caused by foodborne pathogens (CrI 90%  1.28–2.23 million), of which 880,500 (59%) were caused by bacteria; 579,500 (38%) by viruses; and 45,000 (3%) by parasites. These foodborne illnesses led to 17,281 hospitalizations (CrI 90%  15,520–20,785) and 248 deaths (CrI 90%  223–350).\n\n【20】Norovirus ranked first as the cause of foodborne illnesses (34%), third as a cause for foodborne illness–associated hospitalizations (20%), and seventh as a cause of foodborne illness–associated deaths (3%). _Salmonella_ spp. ranked third as the cause of foodborne illnesses (12%), second as a cause for hospitalization (24%), and first as a cause of death (27%). _L monocytogenes_ ranked second (26%), before _Campylobacter_ spp. (17%), as a cause of foodborne illness–associated deaths .\n\n【21】### Discussion\n\n【22】We estimated the population-level number of illnesses, hospitalizations, and deaths in France caused by 15 pathogens with the potential for foodborne transmission. _Campylobacter_ spp. _Salmonella_ spp. and norovirus were responsible for 73% of all foodborne illnesses and 76% of all associated hospitalizations. The pathogens that cause most foodborne illnesses or hospitalizations are not necessarily those that cause the most deaths: _L. monocytogenes_ caused <0.1% of all foodborne illnesses but ranked second as a cause of foodborne illness–associated deaths, just behind S _almonella_ spp.\n\n【23】We used different approaches, depending on the most suitable data that were available, to generate estimates. We could not easily compare our results with previous estimates from France  and other countries because of different data sources, assumptions, and methods. Nevertheless, recent estimates of the burden of foodborne illnesses in the European region also indicated that the 3 most frequent causes of foodborne illness were norovirus (ranked first), _Campylobacter_ spp. (second), and _Salmonella_ spp. (third) . These pathogens were also among the leading causes of foodborne illnesses and hospitalizations in North America  and Oceania . _Salmonella_ spp. and _L_ . _monocytogenes_ accounted for ≈50% of all foodborne illness–associated deaths in France, and were also responsible for most foodborne illness–associated deaths in other high-income countries .\n\n【24】We estimated the number of most pathogen-specific illnesses by using laboratory-based surveillance data corrected for underreporting and underdiagnosis, and we used well-documented estimates for _Campylobacter_ spp. and _Salmonella_ spp. We assumed that the parameters regarding healthcare-seeking behavior and laboratory practice for _Yersinia_ spp. and _Shigella_ spp. were similar to those for _Campylobacter_ spp. and _Salmonella_ spp. respectively. The validity of these assumptions is difficult to explore; further studies would be needed to produce more robust estimates of the true level of underdiagnosis for these 2 pathogens in France.\n\n【25】For _B. cereus_ , _C. perfringens_ , and _S. aureus_ , we assumed that the multiplier between the number of outbreak cases and the number of foodborne illnesses would be similar to that for _Salmonella_ spp. An alternative approach for _C. perfringens_ would have been to apply a proportion of acute gastroenteritis cases by this pathogen estimated in the United Kingdom (0.3–1.7%)  to the annual number of acute gastroenteritis illnesses in France. This approach would result in an estimate (CrI 90%  84,450–278,964) within the range of the estimate in our study. The estimates for _B. cereus_ , _C. perfringens_ , and _S. aureus_ indicate that the effect of these pathogens in terms of foodborne illnesses appears to be high in France. However, only foodborne illness outbreak data were available to estimate the number of illnesses for these pathogens, and more data are needed to confirm our estimates.\n\n【26】We included hepatitis E virus in our study because, in France, indigenous cases of hepatitis E have been shown to be associated with foodborne transmission, particularly through consumption of products containing undercooked or raw pork liver . We estimated the number of hepatitis E cases in France from a seroprevalence study conducted in 2013, and the proportion of cases caused by foodborne transmission was assumed to be between 75% and 100%. Further studies, in particular on the proportion of foodborne transmission of hepatitis E in France, are needed to confirm these estimates.\n\n【27】Our use of seroprevalence and health insurance drug reimbursement data to estimate the numbers of _T. gondii_ – and _T. saginata_ –associated foodborne illnesses was similar to methods previously used in France . Our results indicated a decrease in the number of foodborne illnesses over the past decade (from 51,600 to 12,000 cases for _T. gondii_ and from 64,500 to 33,000 cases for _T. saginata_ ). These decreases may be explained by fewer exposures to the parasites , by changes in food habits, and by improved hygiene practices in meat production. For _T. saginata_ , the number of illnesses may be underestimated because the decrease might also be explained by a shift of treatment from niclosamide to praziquantel for this infection over the past decade in France.\n\n【28】We estimated the number of illnesses caused by norovirus by applying a proportion of acute gastroenteritis cases caused by this pathogen to the annual number of acute gastroenteritis illnesses in France. The final estimate for France is lower than that for other countries that used a similar method , primarily because of a lower estimated incidence of acute gastroenteritis in France  but also because we used a lower proportion of foodborne norovirus transmission (12%–16%) on the basis of an extensive study published in 2015 . Despite these differences and their effect on the final estimate, norovirus ranked first in terms of foodborne illnesses in France and appears to be a key foodborne cause of acute gastroenteritis.\n\n【29】The FHIS was our main data source for estimating numbers of hospitalizations and deaths associated with the 15 pathogens in our study. The relevance of this data source may be questioned because of limitations in diagnosis accuracy and in consistency of disease coding. For most of the pathogens, we estimated the number of hospitalizations by using the number of hospital records with specific ICD-10 codes. We compared trends over time and age and sex distributions of the hospital data with surveillance data from the national reference laboratories and with mandatory notification data. Trends and distributions were similar between the different data sources, supporting the use of FHIS data to estimate the number of hospitalizations. For _Campylobacter_ spp. _Salmonella_ spp. _Yersinia_ spp. and _Shigella_ spp. we corrected the number of hospitalizations and deaths for underdiagnosis, taking into account a proportion of fecal samples tested for each pathogen and the sensitivity of fecal culture. However, for the other pathogens, no specific underdiagnosis multiplier could be estimated and, therefore, the estimates presented in this study are probably conservative. An overestimation is also possible if the pathogen of interest did not cause the illness that led to the hospitalization but was, nevertheless, coded as a concurrent medical condition.\n\n【30】A high number of hospitalizations due to acute gastroenteritis were reported in the FHIS without a specific ICD-10 code because not all hospitalized patients were systematically tested for all pathogens that cause acute gastroenteritis. We used the proportion of hospitalizations for acute gastroenteritis as a proxy to estimate the number of hospitalizations for norovirus, _B. cereus_ , _C. perfringens_ , and _S. aureus_ because testing for these pathogens is infrequently performed in France and because these pathogens cause illnesses with similar symptoms and severity. This proportion (0.58%–0.75%) is lower than that estimated for _Campylobacter_ spp. (0.9%–1.9%) and for _Salmonella_ spp. (1.2%–3.6%), which is plausible considering that illness caused by _B. cereus_ , _C. perfringens_ , and _S. aureus_ is less severe than that caused by _Campylobacter_ spp. and _Salmonella_ spp. Data sources described in the literature to estimate the number of hospitalizations for norovirus, _B. cereus_ , _C. perfringens_ , and _S. aureus_ infections include hospital discharge data and data from foodborne disease outbreaks . Estimating the number of hospitalizations for these pathogens is challenging, and these different methodologic approaches have a major effect on the final estimate. For norovirus, despite differences in methodology and healthcare systems, our estimate (all modes of transmission) of the number of hospitalizations was in the same range as those estimated in North America  and in the Netherlands .\n\n【31】Data to estimate the number of deaths associated with foodborne illnesses are scarce and difficult to obtain. We explored death certificate data but decided not to use that source because few records contained pathogen-specific ICD-10 codes. Hospital discharge data were the only or the most reliable data source available to estimate the number of deaths for most pathogens included in this study. However, deaths may occur after hospitalization discharge or without hospitalization at all. Therefore, our estimates are uncertain and are probably underestimated, even though we did not take into account the possibility that underlying concurrent conditions, not foodborne pathogens, may have caused or contributed to death.\n\n【32】As pointed out in the literature, difficulties in accurately determining the proportion of foodborne pathogen transmission is a key factor contributing to the uncertainty of foodborne illness estimates . Different methodologic approaches, such as epidemiologic and microbiologic approaches, intervention studies, and expert elicitation, have been used to estimate the proportion of foodborne transmission . Overall, in high-income countries, foodborne transmission has been considered a major transmission route for several bacterial pathogens ( _B. cereus_ , _Campylobacter_ spp. _C. perfringens_ , _L. monocytogenes_ , _Salmonella_ spp. _S. aureus_ ) and a minor transmission route for norovirus and hepatitis A virus. Nevertheless, comparison of the estimates by using expert elicitation shows greater variability and higher uncertainties, depending on how the experts were recruited, the expert panel size, or the elicitation method used . We decided to use the proportion of foodborne transmission published in the United States in 2011  as these proportions were based on epidemiologic and microbiologic data rather than expert elicitation. It is possible that food consumption patterns and frequency and type of microbiologic contamination differ between the United States and France and may influence pathogen exposure, resulting in a different proportion of foodborne pathogen transmission in the 2 countries. Further research is needed to obtain specific source attribution estimates for France.\n\n【33】The 15 foodborne pathogens in our study were selected on the basis of their perceived public health significance, their occurrence in France, and the availability of a minimum of data. Other known pathogens with potential foodborne transmission exist (e.g. other non-STEC pathogenic _E.coli_ , rotavirus, and _Cryptosporidium_ spp.), and the total numbers of foodborne illnesses and associated hospitalizations and deaths presented in this study are likely conservative.\n\n【34】We took into account new data sources that allowed for accurate estimates of foodborne illnesses and associated hospitalizations and deaths at the community level in France. Our estimates entail several assumptions, and a high degree of uncertainty remains for some of them. Our estimates indicate that substantial numbers of foodborne pathogen–associated illnesses, hospitalizations, and deaths occur each year in France, necessitating the prioritization of prevention and control strategies by food safety policymakers. We did not specifically consider the effect of sequelae linked to these illnesses when generating our estimates. Thus, our findings capture only part of the overall effect of foodborne infections, and they clear the way for further research on the public health burden of foodborne pathogens in France, taking into account complications and sequelae.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4070705b-9d37-4c2d-9a19-cc0e08a0e6f5", "title": "Cryptosporidiosis Associated with Ozonated Apple Cider", "text": "【0】Cryptosporidiosis Associated with Ozonated Apple Cider\n_Cryptosporidium_ spp. are protozoan parasites transmitted by the fecal-oral route that cause prolonged diarrhea. Only 2 reports describe outbreaks associated with apple cider , and none have been associated with ozonated cider. In October 2003, a northeast Ohio health department identified 12 local residents with laboratory-confirmed cryptosporidiosis; 11 had drunk a locally produced, ozonated apple cider (cider A) in the 2 weeks before illness. The cider was embargoed on October 24, and we initiated an investigation by using epidemiologic and molecular techniques to determine the cause and extent of the outbreak and the role played by the cider and ozonation.\n\n【1】### The Study\n\n【2】We defined a probable case as a northeast Ohio resident with otherwise unexplained diarrhea for >3 days from September 1 to November 30, 2003, and a laboratory-confirmed case as a person with diarrhea and a positive _Cryptosporidium_ laboratory result. Case finding encompassed interviewing persons with diarrhea who came to local health departments and emergency rooms and participants of school outings at which cider A was served. We then conducted 2 epidemiologic studies in which questionnaires showed exposures classically associated with _Cryptosporidium_ transmission such as food, drinking and recreational water, person-to-person contact, animals, and travel.\n\n【3】Study 1 compared laboratory-confirmed case-patients and 2 controls (persons without diarrhea, abdominal pain, or vomiting) per case matched on age and county of residence and identified through random-digit dialing. Additionally, we conducted a retrospective cohort study of school children (study 2) who attended field trips at which cider A was served.\n\n【4】Stool samples from case-patients were screened by wet preparation and tested for _Cryptosporidium_ by using an immunofluorescent assay (Meridian Merifluor _Cryptosporidium_ / _Giardia_ DFA kit, Meridian Bioscience, Cincinnati, OH, USA). Identification of _Cryptosporidium_ in these samples was attempted by using 2 methods: 1) genotyping isolates by polymerase chain reaction–restriction fragment length polymorphism (PCR-RFLP) analysis of the small subunit (SSU) rRNA gene  with subtyping by DNA sequence analysis of the GP60 gene , and 2) amplification of the SSU rRNA gene with genotype differentiation by a microsatellite marker (ML-1) .\n\n【5】Cider samples were concentrated by centrifugation, and water samples were concentrated by Environmental Protection Agency method 1623 . _Cryptosporidium_ genotyping was performed by using the same methods described for stool samples . Newly designed GP60 primers were used to subtype cider samples .\n\n【6】We identified 23 laboratory-confirmed and 121 probable case-patients with onset dates from September 3 to November 19, 2003 ; the first cider-related case occurred September 22. The median patient age was 20 years (range 1–80). The median incubation period was 7 days (range 1–21), and median period of diarrhea was 7 days (range 3–52). Two patients were hospitalized and none died.\n\n【7】In study 1, we enrolled 19 laboratory-confirmed case-patients and 38 age-matched, community-based controls. Twelve of 19 case-patients, but 0 of 38 controls, had drunk cider A. Although the matched odds ratio (OR) for this association was incalculably high, the lower limit of the 95% confidence interval (CI) was 5.6. Although 3 other exposures were also associated with illness by univariate analysis , only drinking cider A was associated with illness in a conditional logistic regression model that included all of these exposures (estimated OR 14.0, 95% CI 1.8–167).\n\n【8】In study 2, we enrolled 402 persons who participated in outings at which cider A was served. Thirty-three (10%) of the 329 persons who drank cider A became ill, while only 2 (3%) of 73 who did not drink cider A became ill (adjusted relative risk 4.7, 95% CI 1.2–18.1). Only drinking cider A remained significantly associated with illness in a multivariate logistic regression model that included 4 other exposures that increased risk in univariate analysis (estimated OR 5.7, 95% CI 1.2–26.6).\n\n【9】No employees from the orchard or the separate cider pressing facility reported diarrhea from September 1 to November 7, 2003 (date of interview). The water supply for both was negative for _Cryptosporidium_ ; employees used \"few\" dropped apples for cider production. During production, an ozonating apparatus (Golden Buffalo Company, Orange, CA, USA) was used to treat the cider, which was then stored in refrigerated tanks. Most cider was ozonated a second time, then sold in plastic jugs on-site and at nearby grocery stores. Remaining cider was sold through a tap in the orchard's store. In contrast to jugged cider, this cider was not reozonated.\n\n【10】We performed a _Cryptosporidium_ PCR on all 14 available samples from the laboratory-confirmed case-patients. Twelve (85.7%) of these were PCR positive; 11 of these 12 samples were identified as _Cryptosporidium parvum_ and 1 as the cervine _Cryptosporidium_ genotype (W4). Subtype identification of stool samples yielded 2 closely related _C_ . _parvum_ subtypes (IIaA15G2R1 and IIaA17G2R1).\n\n【11】The remaining contents of a jug of cider A that a laboratory-confirmed case-patient had partially drunk were also positive by PCR for _C_ . _parvum_ subtype IIaA17G2R1. This case-patient's stool sample yielded the same subtype of _C_ . _parvum_ , as did 4 other case-patients' stool samples; all of these persons drank cider A in the 2 weeks before illness onset.\n\n【12】### Conclusions\n\n【13】Our investigation strongly implicates cider A as the cause of this outbreak. The timing of cider A production closely paralleled the outbreak, and drinking cider A was the only predictor significantly associated with illness in both univariate and multivariate analyses of both epidemiologic studies. Furthermore, detection of _C_ . _parvum_ subtype IIaA17G2R1 from the sample of partially drunk cider A from a laboratory-confirmed case-patient provided further evidence of this link.\n\n【14】The 2 _C_ . _parvum_ subtypes found in this outbreak likely represent a common contamination source; both are common in cattle, and multiple subtypes are commonly found on farms . This outbreak highlights the need for continued development of molecular biologic methods because these techniques will be useful to identify and define future _Cryptosporidium_ outbreaks and supplement epidemiologic associations.\n\n【15】An issue raised by this outbreak is the role of ozonation in the treatment of apple cider. New regulations (Hazard Analysis and Critical Control Point \\[HACCP\\] standards) enacted by the US Food and Drug Administration (FDA) in 2001  require juice manufacturers to demonstrate a 5-log reduction of \"the most resistant microorganism of public health significance\" in their production process.\n\n【16】Because this is the third reported _Cryptosporidium_ outbreak related to unpasteurized apple cider , whatever sterilization procedure is used must be effective against _Cryptosporidium_ . Although pasteurization kills _Cryptosporidium_ oocysts , no data exist on the use of ozonation against _Cryptosporidium_ in food or juice products, where turbidity and low temperature render ozonation less effective . Furthermore, ozonation is difficult to standardize because effectiveness depends on contact time and concentration.\n\n【17】The consideration of ozonation is important because effective disinfection would have prevented the outbreak unless contamination occurred at the final step before distribution. Furthermore, of the 12 ill persons in the case-control study who drank cider A, 6 drank once-ozonated cider and 6 drank twice-ozonated cider, suggesting that even repeated ozonation was inadequate to kill _Cryptosporidium_ . The failure of ozone could have been due to an inherent inadequacy for killing _Cryptosporidium_ in apple cider or improper use; either possibility emphasizes the problems with ozonation in this setting and the need for further testing before its use is accepted.\n\n【18】Given the paucity of evidence supporting ozonation for apple cider disinfection or for killing _Cryptosporidium_ in this product, and its apparent failure in this outbreak, the FDA issued an addendum to its HACCP rule . This addendum advises that juice makers should not use ozone in their manufacturing process unless they can prove a 5-log pathogen reduction through ozonation. To our knowledge, no studies have established this reduction to date.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6cd7c96d-ac98-436f-b00a-992ffe92a2a6", "title": "Cervical and Breast Cancer Screening Among Mexican Migrant Women, 2013", "text": "【0】Cervical and Breast Cancer Screening Among Mexican Migrant Women, 2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Information on cervical and breast cancer screening among Latinas in the United States is limited. Even less information is available on screening practices of migrant women who engage in circular migration. We examined rates of cervical and breast cancer screening and the extent to which sociodemographics and other characteristics explain screening practices of Mexican migrant women who return to Mexico from the United States.\n\n【3】**Methods**\n\n【4】We used data from a cross-sectional probability survey of Mexico-born migrant women who returned, through Tijuana, to Mexico from the United States in 2013. The sample consisted of women who returned involuntarily (via deportation) or voluntarily; 177 reported authorized documentation status, and 36 reported unauthorized documentation status in the previous 12 months. Descriptive statistics were calculated and logistic regressions were estimated.\n\n【5】**Results**\n\n【6】Of 36 undocumented migrant women, 8 (22.2%) had a Papanicolaou test and 11 (30.6%) had a mammogram in the previous year; of 177 documented migrants, 83 (46.9%) had a Papanicolaou test and 68 (38.4%) had a mammogram. Undocumented migrants were less likely than documented migrants to receive a Papanicolaou test (odds ratio \\[OR\\] = 0.29; 95% confidence interval \\[CI\\], 0.12–0.67); the likelihood was similar after adjustment for sociodemographic, migration, and acculturation factors (adjusted OR = 0.33; 95% CI, 0.12–0.90). Having health insurance (adjusted OR = 4.17; 95% CI, 1.80–9.65) and a regular source of health care (adjusted OR = 2.83; 95% CI, 1.05–7.65) were significant predictors of receiving a mammogram but not a Papanicolaou test.\n\n【7】**Conclusion**\n\n【8】Public health programs are needed to improve access to cervical and breast cancer screenings for Latina migrant women in general and undocumented circular migrants in particular.\n\n【9】Introduction\n------------\n\n【10】Foreign-born Latinas are more likely than US-born Latinas and white women to receive a diagnosis of late-stage cervical or breast cancer , probably because cancer screenings are underused by this population. Latinas not born in the United States have lower rates of cancer screening than US-born Latinas, white women, and black women . Undocumented Latinos in the United States also underuse cancer screening services . This underuse of screening services places undocumented Latina immigrants at greater risk of late-stage cancer diagnoses compared with their documented counterparts. In addition, mobility may be a barrier to accessing health services . One-third of Mexican migrants engage in circular migration , defined as repeated migrations between point of origin and destination. Yet, to our knowledge, no research has examined health care use by Mexican women who engage in circular migration.\n\n【11】The US Preventative Services Task Force (USPSTF) recommends a mammogram every 2 years for women aged 50 to 74 and a Papanicolaou (Pap) test every 3 years for women aged 21 to 65 years. To address the need for additional research on rates of, and factors associated with, receipt of cancer screening services by Mexican migrant women, we examined differences in receipt of cervical and breast cancer screening by documented and undocumented Mexican circular migrants. First, we compared the prevalence of self-reported, previous 12-month receipt of cervical and breast cancer screening between documented and undocumented Mexico-born Latinas who returned to Mexico from the United States. Second, we examined the extent to which sociodemographic and other characteristics explain screening practices, with an emphasis on documentation status. Because research on migrants shows that documentation status, health insurance, regular source of care, and acculturation are associated with use of preventive health care services , we hypothesized that these factors would also predict the level of use of cervical and breast cancer screening.\n\n【12】Methods\n-------\n\n【13】### Sample\n\n【14】We used 2013 data from the project _Migrante_ , which comprised a series of cross-sectional probability surveys of Mexican migrants in Tijuana from 2007 to 2015 . One-quarter of migrants who travel south from the United States to Mexico travel through Tijuana . The _Migrante_ surveys used a multistage sampling design, and samples consisted of migrants surveyed at key transit points in Tijuana. Eligible respondents were aged 18 years or older, were born in Mexico or another Latin American country, were fluent in Spanish, and had never before participated in the _Migrante_ survey. Mexican migrants were approached consecutively as they were crossing through the sampling point and then screened for eligibility. Details on survey methods are described elsewhere . The Health Sciences Minimal Risk Institutional Review Board at the University of Wisconsin-Madison and the institutional review board of the Mexico Section of the US–Mexico Border Health Commission approved the project.\n\n【15】The _Migrante_ survey conducted in 2013 focused on the use of health care services. In that year, 4,215 eligible male and female migrants were screened for eligibility  and 2,441 agreed to participate (58% response rate). For this study, we analyzed data from women migrants returning from the United States. These data came from 2 groups of women in 2 migration flows: one group comprised Mexico-born Latinas returning to Mexico from the United States via deportation (deported flow), and the other group comprised Mexico-born Latinas recently arrived from the United States on their way to their communities of origin in Mexico (southbound flow). The deported-flow migrants were recruited for the study in Tijuana’s deportation station; this group consisted of 61 migrants who were intercepted during their attempt to cross the border into the United States or who crossed successfully but were later deported. Most of the 191 southbound-flow migrants recruited for this study were returning voluntarily; they were permanently or temporarily established in the United States, and they were heading to their communities of origin in Mexico. Ten women in the southbound-flow, however, were returning to Mexico because of deportation.\n\n【16】By design, our sample consisted of circular migrants, defined as migrants who had completed a migration cycle; that is, they left their communities of origin, moved to the United States, and returned to Mexico either voluntarily or involuntarily (because of deportation). Immigrants (those who travel in only one direction \\[south to Mexico\\] and never return to the United States) were not included in _Migrante_ sampling frameworks for southbound-flow or deported-flow migrants. Therefore, we use the term “circular migrants” throughout this article.\n\n【17】We restricted the analytical sample to migrants who had spent 30 days or more in the United States during the previous 12 months. We imposed this restriction because most migrants spend 30 days or more in this country; the restriction should increase the relevance of our findings to health policies in the United States. Another reason for this restriction was that data on some predictors of health care access were available for this subset. After imposing this restriction, data on 39 respondents were excluded from the analysis (35 from the deported flow; 4 from the southbound flow). The final analytical sample consisted of 36 undocumented women (10 from southbound flow; 26 from the deported flow) and 177 documented women (all from the southbound flow).\n\n【18】### Measures\n\n【19】The primary outcome of interest was self-reported receipt of mammogram and Pap test in the previous 12 months. Survey participants reported whether they had received these services in the previous 12 months, and if yes, in which country or countries. The main predictor was self-reported documentation status in the United States in the previous 12 months. Undocumented migrants were defined as all women from the deported group and women from the southbound group who answered yes to the question “During the last 12 months in the United States, were you undocumented any of the time?”\n\n【20】We collected self-reported data on the following sociodemographic characteristics: age, marital status, and education. We analyzed data on mammograms in two ways: overall and by age group (younger than 50 and 50 or older). We also analyzed data on Pap tests overall and by age group (younger than 21 and 21 or older). Marital status was categorized into unmarried, married but not living with a spouse in the United States, and married and living with a spouse in the United States. Participants reported their highest level of education completed, and a binary variable was created to stratify those who had completed high school and those who had not. We also collected data on migration characteristics, health care characteristics, and level of acculturation. Participants reported the amount of time spent in the United States in the previous 12 months (recoded into 30 d to <6 mo, 6 mo to <12 mo, or 12 mo) and in their lifetime (recoded into <1 y, 1–4 y, 5–9 y, and ≥10 y). Survey participants reported having or not having a regular source of health care and any form of health insurance in the previous 12 months in the United States. Acculturation was measured by using a scale adapted from Finch et al . A continuous variable was derived from 4 questions that assessed English proficiency (for example, “In the United States, what language did you prefer to speak?”). For each question, participants received a score of 0 (Spanish or a native language always or most of the time), 1 (Spanish or a native language as much as English), or 2 (English always or most of the time). The higher the score (range 0–8), the greater the acculturation to the United States.\n\n【21】### Statistical analysis\n\n【22】First, we compared the characteristics and rates of previous 12-month cervical and breast cancer screening between documented and undocumented women. Descriptive statistics (mean, standard deviation) were determined for continuous variables. Frequency distributions were calculated for categorical variables. To assess differences between the groups, we used _t_ tests and χ 2  tests. Second, we examined the extent to which differences in characteristics explained differences in receipt of Pap test and mammogram between the documented and undocumented women. Multivariate logistic regressions were estimated. A block-by-block approach was used to examine the extent to which sets of variables explained differences in screening receipt between documented and undocumented women. The primary analysis consisted of a series of 4 logistic regression models to test the relationship between documentation status and screening receipt, ranging from an unadjusted to a fully adjusted model. The first model included only a term for documentation status. The next three models sequentially added covariates for demographic variables (age, marital status, education), migration characteristics (time spent in the US in the previous 12 months and during lifetime), health care access (health insurance status, regular source of care), and acculturation. Documented status was used as the reference category, and other reference categories included unmarried marital status, living in the United States for less than 30 days in the previous year, and living in the United States for less than 1 year during a lifetime. Analyses were completed with the entire analytical sample. Sensitivity analyses were performed with subsamples formed according to the latest screening recommendations (ie, aged ≥21 for Pap test and aged ≥50 for a mammogram) to examine the robustness of the findings. The results did not change substantially, but statistical power was significantly reduced. For that reason, we focused our study on the entire analytical sample. Analyses were performed with STATA/SE version 14.0 (StataCorp LP). Statistical significance was determined at the .05 level.\n\n【23】Results\n-------\n\n【24】Of undocumented migrants, 22.2% (8 of 36) reported having a Pap test in the previous year, compared with 46.9% (83 of 177) of documented migrants, all of whom were aged 21 or older . Of undocumented migrants, 30.6% (11 of 36) had a mammogram, and 38.4% (68 of 177) of documented migrants had a mammogram; this difference was not significant. We found no significant differences in mammogram receipt between women younger than 50 and women 50 or older. Of women who received either of these services, most received them only in the United States. Of 91 women who received a Pap test, 65 received it only in the United States, 21 only in Mexico, and 4 in both countries. For mammograms, 79 women received the screening: 61 only in the United States, 14 only in Mexico, and 3 in both countries. One woman received both a mammogram and Pap test but received neither in the United States or Mexico. In general, undocumented migrants were significantly younger and significantly more likely to have been in the United States for the entire previous 12 months.\n\n【25】In Model 1 (unadjusted) of the logistic regression analyses for Pap test, undocumented migrants were significantly less likely than documented migrants to receive a Pap test (odds ratio \\[OR\\] = 0.29; 95% confidence interval \\[CI\\], 0.12–0.67) . In Model 2, when age, marital status, and education were included, we found no change from Model 1 (adjusted OR = 0.30; 95% CI, 0.12–0.72). In Model 3, which included variables for time spent in the United States, health insurance status, and regular source of care status, the adjusted OR of documented status was 0.35 (95% CI, 0.13–0.95). In Model 4, which included acculturation level, we found no change from Model 3 (adjusted OR = 0.33; 95% CI 0.12–0.90). Documentation status was the only predictor significantly associated with the odds of Pap test receipt in any of the models.\n\n【26】In Models 1 through 4 of the logistic regression analyses for mammogram receipt, we found no significant differences between documented migrants and undocumented migrants in the likelihood of receiving a mammogram . After adjustment for sociodemographic factors, migration characteristics, and acculturation level (Model 4), age was significantly associated with increased odds of mammogram receipt (adjusted OR = 1.06; 95% CI, 1.02–1.09). Having health insurance (adjusted OR = 4.17; 95% CI, 1.80–9.65) and a regular source of health care (adjusted OR = 2.83; 95% CI, 1.05–7.65) were significant predictors of mammogram receipt. Model 4 also demonstrated significantly increased odds of mammogram receipt with increased level of acculturation (adjusted OR = 1.25; 95% CI, 1.01–1.55).\n\n【27】Discussion\n----------\n\n【28】Our findings on the prevalence of cancer screening receipt support and contribute to research demonstrating that Latina migrants have lower screening rates compared with other populations. Of undocumented migrants, 22.2% reported Pap test receipt in the previous year, compared with 46.9% of documented migrants. By comparison, in the United States in 2010, the percentage of women aged 21 years or older who received a Pap test within the previous 3 years was 79.1% among non-Hispanic white women and 74.7% among Hispanic women  and the percentage of women aged 18 to 29 who received a Pap test within the previous year was 73.1% . In our study, 30.6% of undocumented migrants and 38.4% of documented migrants received a mammogram. By comparison, in 2010 in the United States, the percentage of women aged 40 or older who received a mammogram within the previous year was 51.5% among non-Hispanic white women and 46.5% among Hispanic women .\n\n【29】We found a significantly greater percentage of Pap test receipt among documented migrants than among undocumented migrants, even after adjustment for other factors. This finding is consistent with research suggesting that undocumented Latinos underuse cancer screening services  and expands previous research. However, we did not find a significant difference in the rate of mammogram receipt or in the likelihood of mammogram receipt between the documented and the undocumented migrant groups. Although the reason for the difference in findings between the 2 types of screenings is unclear, the findings may indicate distinct differences in the contextual factors influencing migrant women’s use of screening services, such as pregnancy status and prenatal care, different costs of the screening procedures, perception of discomfort, and level of invasiveness associated with screening procedures . Perhaps these factors play a more important role for breast cancer screening than for cervical cancer screening, rendering the role of documentation status less important in predicting the likelihood of mammography receipt.\n\n【30】Having a usual source of care and having health insurance are important predictors of breast and cervical cancer screening receipt , and undocumented status is associated with being less likely than other Latinos or whites to have medical insurance . Accordingly, in our study, undocumented migrants were less likely (but not significantly less likely) to have health insurance than were documented migrants. Mammography screening adherence is associated with having health insurance, an annual physical examination, or a recent physician visit . Insurance coverage and visits to a primary care provider within the previous year are also associated with Pap test receipt , whereas low rates of cervical cancer screening among immigrant women are associated with lack of a usual source of health care . Interestingly, in our study, having health insurance and a regular source of health care were significant predictors of mammogram receipt but not of Pap test receipt.\n\n【31】Latinos with high levels of acculturation use more health care services than Latinos with low levels of acculturation . After controlling for several factors, our study found that increased acculturation significantly predicted receipt of a mammogram but not of a Pap test. Other research suggests that higher levels of acculturation predict greater use of cervical cancer screening . This discrepancy might be attributed to our small sample of undocumented migrant women or to differences between circular migrants and established immigrant populations in the United States. Our sample had low levels of acculturation, but another study of Latinas in the Midwest who had low levels of acculturation showed that having a usual source of health care was a significant predictor of both mammogram and Pap test receipt .\n\n【32】That health care access and level of acculturation did not predict receipt of a Pap test might be due to several factors. We had a small sample of undocumented migrants, and we expect that a larger study would produce results consistent with other results published in the literature. The percentage of migrants who had a usual source of health care was high in this sample, considering the low levels of health insurance coverage. Furthermore, Pap test receipt is a less expensive type of screening that may be easier to schedule and complete than mammography is, making the Pap test potentially easier to obtain through community clinics, reproductive and sexual health clinics, or during other health clinic visits.\n\n【33】This study had several limitations. The study design may overrepresent the number of highly mobile migrants. However, circular Mexican migrants are a difficult-to-reach, understudied population, and our methods shed light on their patterns of health care use. The survey was conducted in Tijuana, and results may not apply to migrants traveling through other Mexican border regions. The response rate was moderate, and the size of the subsample of women in the survey was small, which resulted in lower than ideal sample sizes. Additional research with larger samples of migrant women is needed to estimate rates of cancer screening more accurately. Self-report of breast and cervical cancer screening may be unreliable. Future research could consider validating self-reported screening receipt with clinical records where possible. Considering the low levels of acculturation in the sample, our results on associations with acculturation level may not extrapolate to more acculturated Latina populations. Our estimates reflect and compare screening rates during the 12 months before the _Migrante_ survey in 2013. Although this uniform timeframe allowed us to compare the rates of two groups, we cannot directly compare our data with data on screening rates measured during the past several years.\n\n【34】Despite these limitations, this study provides unique and critical data on cervical and breast cancer screening use among an understudied population of Mexican migrant women. Documented and undocumented migrant women differ in the way they use cancer screening services, and public health programs in the United States should be developed to improve rates of cancer screening among Latina migrant women, especially undocumented circular migrants. To intervene appropriately, additional research is needed to better understand the relationship between documentation status and cancer screening receipt among migrant women. Future research should aim to better understand the factors that predict cancer screening, identify potential targets for intervention to increase rates of screening receipt, and examine how the implementation of the Affordable Care Act has affected receipt of women’s preventive health care services in this population.\n\n【35】Tables\n------\n\n【36】#####  Table 1. Characteristics of Mexican Female Migrant Study Participants, by Documentation Status, Tijuana, Mexico, 2013 a, b, c  \n\n| Characteristic | Undocumented d , n (%) (N = 36) | Documented e , n (%) (N = 177) | _P_ Value f |\n| --- | --- | --- | --- |\n| **Cancer screening during previous 12 months** | **Cancer screening during previous 12 months** | **Cancer screening during previous 12 months** | **Cancer screening during previous 12 months** |\n| Had a mammogram | 11 (30.6) | 68 (38.4) | .26 |\n| Aged <50 y | 8 (22.2) | 26 (14.7) | .57 |\n| Aged =50 y | 3 (8.3) | 42 (23.7) | .66 |\n| Mammogram by location | Mammogram by location | Mammogram by location | Mammogram by location |\n| No receipt of service | 25 (69.4) | 99 (55.9) | .62 |\n| Only in the United States | 9 (25.0) | 52 (29.4) | .62 |\n| Only in Mexico | 1 (2.8) | 13 (7.3) | .62 |\n| In both the United States and Mexico | 1 (2.8) | 2 (1.1) | .62 |\n| In neither United States nor Mexico | 0  | 1 (0.6) | .62 |\n| Data missing | 0 (0.0) | 10 (5.6) |  |\n| Had a Papanicolaou test | 8 (22.2) | 83 (46.9) | .001 |\n| Aged <21 y | 0  |  | NA |\n| Aged =21 y | 8 (22.2) | 83 (46.9) | .001 |\n| Papanicolaou test by location | Papanicolaou test by location | Papanicolaou test by location | Papanicolaou test by location |\n| No receipt of service | 28 (77.8) | 84 (47.5) | .046 |\n| Only in the United States | 7 (19.4) | 58 (32.8) | .046 |\n| Only in Mexico | 1 (2.8) | 20 (11.3) | .046 |\n| In both the United States and Mexico | 0 (0.0) | 4 (2.3) | .046 |\n| In neither United States nor Mexico | 0 (0.0) | 1 (0.6) | .046 |\n| Data missing | 0 (0.0) | 10 (5.6) |  |\n| **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** |\n| Age, mean (SD), y | 38.9 (8.1) | 47.9 (14.0) | <.001 |\n| Marital status | Marital status | Marital status | Marital status |\n| Unmarried | 18 (50.0) | 69 (39.0) | .17 |\n| Married, not living with spouse | 6 (16.7) | 16 (9.0) | .17 |\n| Married, living with spouse | 12 (33.3) | 82 (46.3) | .17 |\n| Data missing | 0 (0.0) | 10 (5.6) |  |\n| Completed high school | 9 (25.0) | 69 (39.0) | .08 |\n| **Migration characteristics** | **Migration characteristics** | **Migration characteristics** | **Migration characteristics** |\n| Time spent in the United States during previous 12 months | Time spent in the United States during previous 12 months | Time spent in the United States during previous 12 months | Time spent in the United States during previous 12 months |\n| 30 d to <6 mo | 3 (8.3) | 27 (15.3) | .003 |\n| 6 mo to <12 mo | 10 (27.8) | 83 (46.9) | .003 |\n| 12 mo | 23 (63.9) | 57 (32.2) | .003 |\n| Data missing | 0 (0.0) | 10 (5.6) |  |\n| Time spent in the United States during lifetime, y g | Time spent in the United States during lifetime, y g | Time spent in the United States during lifetime, y g | Time spent in the United States during lifetime, y g |\n| <1 | 0  | 10 (5.6) | .07 |\n| 1–4 | 0  | 16 (9.0) | .07 |\n| 5–9 | 5 (13.9) | 18 (10.2) | .07 |\n| \\=10 | 30 (83.3) | 107 (60.5) | .07 |\n| Data missing | 1 (2.8) | 26 (14.7) |  |\n| **Health care access g** | **Health care access g** | **Health care access g** | **Health care access g** |\n| Has had any health insurance during previous 12 m | 14 (40.0) | 95 (53.7) | .06 |\n| Has a regular source of health care | 26 (74.3) | 114 (64.4) | .68 |\n| **Acculturation** | **Acculturation** | **Acculturation** | **Acculturation** |\n| Level of acculturation based on language h , mean (SD), y | 1.9 (2.4) | 1.3 (1.8) | .11 |\n\n【38】Abbreviations: NA, not applicable; SD, standard deviation.  \na  Data source: Project _Migrante_ , which comprised a series of cross-sectional probability surveys of Mexican migrants in Tijuana from 2007 to 2015 .  \nb  Sample restricted to migrants who had spent =30 days in the United States during the previous 12 months.  \nc  Values are number (percentage) unless otherwise indicated.  \nd  Undocumented defined as all women from the deported flow and women from the southbound flow who answered yes to the question “During the last 12 months in the United States, were you undocumented any of the time?” The deported flow comprised Mexico-born Latinas returning to Mexico from the United States via deportation, and the southbound flow comprised Mexico-born Latinas recently arrived from the United States on their way voluntarily (not via deportation) to their communities of origin in Mexico.  \ne  Documented defined as women from the southbound flow who answered no to the question “During the last 12 months in the United States, were you undocumented any of the time?”  \nf  ? 2  and _t_ tests used to determine _P_ values.  \ng  One women in the deported flow did not answer this question.  \nh  On a scale of 0 to 8, with 0 = lowest level of acculturation, 8 = highest level of acculturation. Scale adapted from Finch et al .\n\n【39】#####  Table 2. Factors Associated With Receipt of Papanicolaou Test Among Mexican Migrant Women, Tijuana, Mexico, 2013 a, b  \n\n| Factor | Model 1, AOR (95% CI) | Model 2, AOR (95% CI) | Model 3, AOR (95% CI) | Model 4, AOR (95% CI) |\n| --- | --- | --- | --- | --- |\n| **Main predictor** | **Main predictor** | **Main predictor** | **Main predictor** | **Main predictor** |\n| Documented c during previous 12 mo | Reference | Reference | Reference | Reference |\n| Undocumented d during previous 12 mo | 0.29 (0.12–0.67) | 0.30 (0.12–0.72) | 0.35 (0.13–0.95) | 0.33 (0.12–0.90) |\n| **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** |\n| Age |  | 0.99 (0.97–1.02) | 1.00 (0.97–1.03) | 1.00 (0.97–1.03) |\n| Marital status | Marital status | Marital status | Marital status | Marital status |\n| Unmarried |  | Reference | Reference | Reference |\n| Married, not living with spouse |  | 1.72 (0.63–4.71) | 2.00 (0.65–6.14) | 2.16 (0.70–6.69) |\n| Married, living with spouse |  | 1.51 (0.82–2.79) | 1.46 (0.72–2.98) | 1.56 (0.76–3.22) |\n| Completed high school |  | 1.77 (0.94–3.32) | 1.62 (0.79–3.34) | 1.46 (0.69–3.07) |\n| **Migration characteristics** | **Migration characteristics** | **Migration characteristics** | **Migration characteristics** | **Migration characteristics** |\n| Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months |\n| 30 d to <6 mo |  |  | Reference | Reference |\n| 6 mo to <12 mo |  |  | 0.50 (0.17–1.52) | 0.53 (0.17–1.62) |\n| 12 mo |  |  | 0.57 (0.18–1.81) | 0.60 (0.18–1.92) |\n| Time spent in the United States during lifetime, y | Time spent in the United States during lifetime, y | Time spent in the United States during lifetime, y | Time spent in the United States during lifetime, y | Time spent in the United States during lifetime, y |\n| <1 |  |  | Reference | Reference |\n| 1–4 |  |  | 0.73 (0.13–4.23) | 0.78 (0.14–4.47) |\n| 5–9 |  |  | 0.41 (0.07–2.44) | 0.41 (0.07–2.44) |\n| \\=10 |  |  | 0.68 (0.13–3.59) | 0.59 (0.11–3.16) |\n| **Health care access** | **Health care access** | **Health care access** | **Health care access** | **Health care access** |\n| Had any health insurance during previous 12 mo |  |  | 1.92 (0.90–4.09) | 1.88 (0.88–4.03) |\n| Has a regular source of health care |  |  | 2.10 (0.88–5.03) | 2.19 (0.91–5.28) |\n| **Acculturation** | **Acculturation** | **Acculturation** | **Acculturation** | **Acculturation** |\n| Level of acculturation based on language e |  |  |  | 1.11 (0.91–1.36) |\n\n【41】Abbreviations: AOR, adjusted odds ratio; CI, confidence interval.  \na  Data source: Project _Migrante_ , which comprised a series of cross-sectional probability surveys of Mexican migrants in Tijuana from 2007 to 2015 .  \nb  Sample restricted to migrants who had spent =30 days in the United States during the previous 12 months.  \nc  Undocumented defined as all women from the deported flow and women from the southbound flow who answered yes to the question “During the last 12 months in the United States, were you undocumented any of the time?” The deported flow comprised Mexico-born Latinas returning to Mexico from the United States via deportation, and the southbound flow comprised Mexico-born Latinas recently arrived from the United States on their way voluntarily (not via deportation) to their communities of origin in Mexico.  \nd  Documented defined as women from the southbound flow who answered no to the question “During the last 12 months in the United States, were you undocumented any of the time?”  \ne  On a scale of 0 to 8, with 0 = lowest level of acculturation, 8 = highest level of acculturation. Scale adapted from Finch et al .\n\n【42】#####  Table 3. Factors Associated With Receipt of Mammogram Among Mexican Migrant Women, Tijuana, Mexico, 2013 a, b  \n\n| Factor | Model 1, AOR (95% CI) | Model 2, AOR (95% CI) | Model 3, AOR (95% CI) | Model 4, AOR (95% CI) |\n| --- | --- | --- | --- | --- |\n| **Main predictor** | **Main predictor** | **Main predictor** | **Main predictor** | **Main predictor** |\n| Documented c during previous 12 mo | Reference | Reference | Reference | Reference |\n| Undocumented d during previous 12 mo | 0.64 (0.30–1.39) | 1.01 (0.43–2.34) | 1.30 (0.47–3.58) | 1.14 (0.40–3.22) |\n| **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** | **Sociodemographic characteristics** |\n| Age |  | 1.04 (1.01–1.06) | 1.05 (1.02–1.08) | 1.06 (1.02–1.09) |\n| Marital status | Marital status | Marital status | Marital status | Marital status |\n| Unmarried |  | Reference | Reference | Reference |\n| Married, not living with spouse |  | 1.54 (0.57–4.14) | 2.41 (0.74–7.86) | 2.75 (0.84–9.02) |\n| Married, living with spouse |  | 1.27 (0.68–2.37) | 1.13 (0.53–2.41) | 1.25 (0.57–2.72) |\n| Completed high school |  | 1.95 (1.02–3.75) | 1.78 (0.82–3.85) | 1.45 (0.65–3.23) |\n| **Migration characteristics** | **Migration characteristics** | **Migration characteristics** | **Migration characteristics** | **Migration characteristics** |\n| Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months | Time spent in the United States in the previous 12 months |\n| 30 d to <6 mo |  |  | Reference | Reference |\n| 6 mo to <12 mo |  |  | 0.30 (0.09–1.03) | 0.33 (0.09–1.14) |\n| 12 mo |  |  | 0.45 (0.13–1.60) | 0.50 (0.14–1.80) |\n| Time in the United States during lifetime, y | Time in the United States during lifetime, y | Time in the United States during lifetime, y | Time in the United States during lifetime, y | Time in the United States during lifetime, y |\n| <1 |  |  | Reference | Reference |\n| 1–4 |  |  | 0.64 (0.09–4.56) | 0.68 (0.09–4.89) |\n| 5–9 |  |  | 0.46 (0.06–3.60) | 0.45 (0.06–3.59) |\n| \\=10 |  |  | 1.00 (0.16–6.20) | 0.70 (0.11–4.55) |\n| **Health care access** |  |  |  |  |\n| Had any health insurance in previous 12 mo |  |  | 4.24 (1.85–9.69) | 4.17 (1.80–9.65) |\n| Has a regular source of health care |  |  | 2.62 (1.01–6.91) | 2.83 (1.05–7.65) |\n| **Acculturation** | **Acculturation** | **Acculturation** | **Acculturation** | **Acculturation** |\n| Level of acculturation based on language e |  |  |  | 1.25 (1.01–1.55) |\n\n【44】Abbreviation: AOR, adjusted odds ratio; CI, confidence interval.  \na  Data source: Project _Migrante_ , which comprised a series of cross-sectional probability surveys of Mexican migrants in Tijuana from 2007 to 2015 .  \nb  Sample restricted to migrants who had spent =30 days in the United States during the previous 12 months.  \nc  Undocumented defined as all women from the deported flow and women from the southbound flow who answered positively to the question “During the last 12 months in the United States, were you undocumented any of the time?” The deported flow comprised Mexico-born Latinas returning to Mexico from the United States via deportation, and the southbound flow comprised Mexico-born Latinas recently arrived from the United States on their way voluntarily (not via deportation) to their communities of origin in Mexico.  \nd  Documented defined as women from the southbound flow who answered no to the question “During the last 12 months in the United States, were you undocumented any of the time?”  \ne  On a scale of 0 to 8, with 0 = lowest level of acculturation, 8 = highest level of acculturation. Scale adapted from Finch et al .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0cfb360b-cc01-4b19-918c-76ff350705dd", "title": "Salmonellosis in the Republic of Georgia: Using Molecular Typing to Identify the Outbreak-Causing Strain", "text": "【0】Salmonellosis in the Republic of Georgia: Using Molecular Typing to Identify the Outbreak-Causing Strain\n_Salmonella_ species, which cause a variety of clinical manifestations from mild gastroenteritis to septicemia, are one of the leading causes of foodborne illness worldwide . Approximately 50% of cases of human disease caused by salmonellae are produced by _S._ Enteritidis and _S._ Typhimurium  . _S._ Typhimurium is of particular concern because of the recent emergence of a highly antibiotic-resistant strain (resistant to ampicillin, chloramphenicol, streptomycin, sulfonamides, and tetracycline) designated as definitive type (DT) 104  . In approximately 75% of human _Salmonella_ cases, the bacterium is acquired from meat, poultry, or eggs .\n\n【1】### The Study\n\n【2】In May 1998, three apparently distinct outbreaks of salmonellosis were identified in the Republic of Georgia. Symptoms in all three included acute diarrhea (100% of cases), severe stomach cramps (35%), nausea and vomiting (22%), and fever (97%). The first outbreak affected children attending a birthday party in Kojori, a suburb near Tbilisi, Georgia. Ten of 14 children attending the party came down with acute diarrhea and were hospitalized within 3 days. _S._ Typhimurium was isolated from stool samples of four patients.\n\n【3】The second outbreak, 4 days later, affected guests at a wedding reception in the village of Asureti (40 kilometers east of Tbilisi). Fifty of approximately 100 guests had diarrhea, and 18 were hospitalized. Salmonellosis was confirmed by the isolation of _S._ Typhimurium from seven of the hospitalized patients.\n\n【4】The third outbreak occurred in Tbilisi approximately 14 days after the first outbreak. Thirty-one of 50 guests at a birthday party were hospitalized for acute diarrhea within 3 days. Salmonellosis was culture-confirmed in 14 cases (i.e. _S._ Typhimurium was isolated from 14 of the patients). All persons with diarrhea in all three outbreaks took various doses of antibiotics (ciprofloxacin or ceftriaxone) when they first became ill.\n\n【5】All 59 hospitalized persons were interviewed to determine a possible common source of infection. In addition, 32 persons who had diarrhea but were not hospitalized and 71 healthy guests were interviewed. Eggs were implicated in the first outbreak; the eggs were used to prepare uncooked icing for a homemade cake. In the second outbreak, eating chicken served during the wedding banquet was associated with illness. In the third outbreak, chicken from a farmers' market in Tbilisi was implicated. No food samples were available for bacteriologic analysis.\n\n【6】Stool samples from the 59 hospitalized patients were examined for _Salmonella_ , _Shigella_ , and _Yersinia_ by standard techniques. _Salmonella_ were isolated from stool samples of 25 (42%) of the patients; all 25 isolates were identified as _S._ Typhimurium strains by the API20E test system, and 18 of them were randomly chosen for subsequent analysis. No other pathogen was isolated.\n\n【7】Arbitrary primed polymerase chain reaction (AP-PCR) was performed by using an RAPD kit (Amersham Pharmacia Biotech, Piscataway, NJ). All typing was done with primer #6 of the kit (5'- CCC GTC AGC A - 3'), which gave distinctive, reproducible patterns with three or more major bands. Bacterial DNA for AP-PCR was obtained, and amplification was performed  . After the amplification cycles, the samples were incubated at 72°C for 5 minutes and analyzed by electrophoresis in 2% agarose gel in TAE buffer.\n\n【8】The rapid pulse-field gel electrophoresis (PFGE) procedure developed for typing _Escherichia coli_ O157H7 strains was used for PFGE typing of the outbreak strains  . The strains were analyzed, in separate experiments, by digesting DNA with _Xba_ I, _Avr_ II, and _Spe_ I restriction enzymes, and _Xba_ I-digested _S._ Newport strain amO1144 was used as the reference strain in all experiments .\n\n【9】A distinct pattern was observed in all 18 strains when they were analyzed by AP-PCR (data not shown), which suggests close genetic relatedness. This finding was confirmed by PFGE typing, i.e. _Xba_ I-, _Avr_ II-; _Spe_ I-macrorestriction patterns generated by PFGE showed that all strains tested had an identical PFGE pattern. The pattern was distinct from those obtained for 18 other _S._ Typhimurium strains isolated in the Republic of Georgia at about the same time, which were grouped into five PFGE types distinct from that of the outbreak-causing strain.\n\n【10】The national molecular subtyping network, PulseNet, includes a database of the PFGE patterns of _E. coli_ O157 and _Salmonella_ group B and D strains isolated in the United States. We screened local databases of two public health laboratories in Washington and Maryland that participate in PulseNet, and we used the Internet to compare the PFGE pattern of the Georgian outbreak strain with _Salmonella_ patterns obtained in these two laboratories. At least three strains among >300 _S._ Typhimurium isolates (grouped into approximately 50 PFGE types) in the two databases were almost identical to the Georgian isolate. However, the PFGE patterns of the strains differed slightly when their DNA was digested with _Avr_ II before PFGE. The patterns obtained by AP-PCR and PFGE ( _Xba_ I- and _Spe_ I-digests) were almost indistinguishable; however, when DNA of the same strains was analyzed by PFGE after digestion with _Avr_ II, differences were detected. For example, the PFGE pattern of strain 00354 (a Seattle isolate) had a large fragment not present in the Georgian outbreak strain and had lost one small fragment . In addition, the PFGE pattern of strain 01587 (also from Seattle) lacked one fragment present in the Georgian outbreak strain and had two other small fragments . These differences may result from a single genetic event in the bacterial DNA and are associated with a spontaneous point mutation resulting in either creation or loss of a restriction site. Therefore, the Seattle and Maryland isolates were classified as closely related  to the Georgian outbreak strain. In Washington, the strain was associated with severe diarrhea like that of the Georgian patients, which suggests that the strain was strongly virulent. (Clinical data for the Maryland isolate were not available.) Detailed evaluation of the pathogenic potential of the outbreak strain (and the closely related U.S. isolates) will require determination of the possible links between unusually severe cases of salmonellosis and isolation of _S._ Typhimurium strains having closely related PFGE patterns, and testing the strain for virulence in laboratory animals.\n\n【11】The outbreak-causing _S._ Typhimurium strain was resistant to ampicillin, chloramphenicol, streptomycin, and tetracycline, but was susceptible to cephalosporins and sulfonamides. Two of the above closely related strains (00354 and 9294-99) had an antibiotic-susceptibility pattern similar to that of the Georgian outbreak strain. The only difference in the pattern for the third closely related strain  was that it was susceptible to chloramphenicol.\n\n【12】### Conclusions\n\n【13】Epidemiologically, the outbreaks had no obvious connection. Patients were hospitalized in two different hospitals in two different cities and were not associated with more than one outbreak. In addition, no common source of infection could be identified. Thus, the three outbreaks were initially thought to be separate. However, the observation that they occurred within a short period (2 weeks between the first and third outbreaks), were caused by the same serotype of _Salmonella_ , were associated with severe diarrhea, and were limited geographically to a 80-kilometer radius raised the possibility that the outbreaks may have been related. Therefore, we used molecular typing techniques to characterize the _Salmonella_ strains isolated from the patients in each outbreak. Our observations suggest that what appeared to be three distinct outbreaks of salmonellosis were, in reality, parts of one large outbreak caused by a distinct clonal strain of _S._ Typhimurium. The common source of infection and the transmission route for the outbreak-causing _Salmonella_ strain are not known.\n\n【14】The clinical diagnosis of salmonellosis was confirmed by isolating _S._ Typhimurium from 25 (42%) of the hospitalized patients. All the patients had treated themselves with ciprofloxacin and ceftriaxone (readily available without prescription in Georgia) before hospitalization, which may have contributed to our inability to isolate _Salmonella_ (or other potential pathogens) from most of them. Fifty-nine (65%) of 91 persons diagnosed as having salmonellosis had diarrhea severe enough to require hospitalization. The hospitalization rate of these patients was approximately seven times higher than the usual hospitalization rate (<10%) for _Salmonella_ cases in Georgia and approximately three times higher than the average hospitalization rate reported for _Salmonella_ patients in the United States  . Although some persons with diarrhea may have requested hospitalization after hearing that other ill guests were hospitalized, the clinical reports indicate that the high hospitalization rate reflects a severe manifestation of the disease. Since the AP-PCR and PFGE typing results suggested that a single clone was the causative agent in all cases, the question of its possibly increased virulence arose. It has recently been reported  that some _Salmonella_ serotypes/strains cause more severe illness. The recent emergence of _S._ Typhimurium DT104 strains having increased virulence  , in addition to being multidrug resistant, also highlights the possibility of supervirulent strains emerging worldwide.\n\n【15】_S._ Typhimurium, in contrast to _S._ Enteritidis, which is a highly clonal organism  , is a fairly diverse serotype  . Therefore, detection of closely related _S._ Typhimurium strains in geographically distinct loci (the Republic of Georgia, and the East Coast and West Coast of the United States) may signal worldwide spread or emergence of closely related clonal groups of _Salmonella_ having increased virulence. This possibility may be confirmed by worldwide (or nationwide) standardization of molecular typing protocols and further strengthening of data-sharing capabilities between laboratories involved in the molecular typing of pathogenic microorganisms. An example demonstrating the potential value of such a database is our finding, during screening of the two participating PulseNet laboratories, that two seemingly unrelated strains (Seattle isolate 00354 and Maryland strain 9294-99) were clonal. As described above, this observation was confirmed by both antibiotic-susceptibility testing and side-by-side molecular typing of the strains in question. An epidemiologic link between the two isolates is being investigated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ef806b84-c78d-4303-bcbb-04cc33ec761f", "title": "Signs or Symptoms of Acute HIV Infection in a Cohort Undergoing Community-Based Screening", "text": "【0】Signs or Symptoms of Acute HIV Infection in a Cohort Undergoing Community-Based Screening\nThe detection of acute HIV infection (AHI) is critical to HIV prevention and treatment strategies . Clinical diagnosis of AHI is difficult, however, because the signs and symptoms that occur during seroconversion are frequently not recognized as an indicator of AHI . Although screening programs that rely on point-of-care HIV antibody testing will reliably identify persons with established infection, these tests fail to detect AHI . The Centers for Disease Control and Prevention began addressing this problem by updating recommendations for the laboratory diagnosis of HIV in healthcare settings to include initial fourth generation HIV-1 p24 antigen–based immunoassays . However, previous studies indicate that sensitivity of p24 antigen detection for AHI might not exceed 80% . In addition, most testing programs in nonhealthcare settings continue to rely on routine antibody testing alone, with specific testing for AHI conducted only for persons with signs or symptoms.\n\n【1】Although previous studies focused on retrospective evaluation of AHI symptoms in persons diagnosed with early seropositive HIV infection  or cases identified by symptom-based AHI screening, the actual proportion of persons with AHI who are symptomatic at the time of testing remains unknown. We investigated the proportion of persons with AHI who have ongoing or recent signs or symptoms at the time of their diagnostic test in a cohort undergoing community-based universal AHI screening.\n\n【2】### The Study\n\n【3】We analyzed AHI signs and symptoms in 90 patients given a diagnosis of AHI during 2007–2014. As part of this confidential HIV testing program, routine, individual donation, HIV nucleic acid amplification testing (NAT) has been provided to all rapid antibody–negative participants since June 2007 (samples for NAT are obtained at the time of rapid antibody testing) . AHI was defined as having a negative or indeterminate HIV antibody test result in the presence of detectable HIV-1 RNA, corresponding to Fiebig stages I–II, with a mean estimated date of infection within the previous 10 days (95% CI 7–14 days) . Dates of infection were estimated for all recently infected patients using previously published criteria on the basis of serologic and virologic test results .\n\n【4】At each patient’s first visit after documentation of AHI diagnosis (median 4 days, interquartile range \\[IQR\\] 3–6 days after AHI testing), we obtained blood samples for CD4 and viral load testing and collected detailed information regarding occurrence, duration, and start and stop dates for 11 signs and symptoms associated with AHI . Participants were also asked to specify any other symptoms. In addition, patients who participated during 2007–2011 were asked if they had sought medical attention for any signs or symptoms. Typical AHI (i.e. \\> 2 signs/symptoms) was defined according to criteria described by Braun et al.\n\n【5】For statistical analysis, SPSS version 21 (SPSS, Inc. Chicago, IL, USA) was used. For analysis on signs or symptoms compatible with AHI, signs or symptoms that started \\> 5 days before the estimated date of infection (i.e. before the 7–14 day 95% CI) were excluded. The University of California San Diego’s Human Research Protections Program approved the study protocol, consent process, and all study-related procedures.\n\n【6】All 90 participants were male and self-identified as men who have sex with men (MSM). Median age was 29 (range 18–67) years. Half (50%) of participants reported white race; 29% reported Hispanic ethnicity. Median number of male partners reported for the previous 12 months was 20 (IQR 14–31). A total of 72 (80%) patients had signs or symptoms associated with AHI that occurred within 2 weeks before undergoing NAT; of these 72 patients, 47 (52% of the study population) had ongoing signs or symptoms, while signs or symptoms had resolved by the time of testing for 25 (28% of the study population). Twelve (13%) reported signs or symptoms starting after testing, while 6 (7%) reported the absence of signs or symptoms . A total of 66 patients (73% of the study population) reported headache, pharyngitis, or myalgia occurring during the 14 days before AHI testing.\n\n【7】Overall, 69 patients (77%) reported signs or symptoms that met criteria of compatibility with AHI . Onset of signs or symptoms compatible with AHI occurred at a median of 5 days (IQR 0–8, range –4 to 15 days) after the estimated date of infection. Neither viral load nor CD4 count correlated with duration or actual number of signs or symptoms.\n\n【8】Data on whether a patient sought medical attention because of signs or symptoms were available for 42 (47%) of 90 patients; of these, 12 (29%) reported that they sought medical attention because of their signs or symptoms and 30 (71%) did not. Significantly higher viral loads were observed for those who sought medical attention compared with those who did not (median 6.1 \\[IQR 5.7–6.7\\] log copies/mL vs. 4.7 \\[IQR 3.4–5.5\\] log copies/mL; p<0.01).\n\n【9】Overall, 70 (78%) of the 90 patients fulfilled criteria for having typical AHI and 20 (22%) did not (of the latter, 14 had only 1 sign or symptom, and 6 were asymptomatic). Patients with typical AHI had significantly higher viral loads compared with patients without (p<0.01). A total of 61 (85%) of 72 patients with signs or symptoms before NAT testing fulfilled criteria for having typical AHI. In addition, 40 (85%) of 47 patients who had ongoing signs or symptoms at the time of AHI testing fulfilled criteria for having typical AHI at that time.\n\n【10】### Conclusions\n\n【11】We characterized signs or symptoms relative to the date of AHI diagnosis among persons seeking HIV testing in a program offering universal AHI screening. Two findings are notable: 1) 52% of participants reported ongoing signs or symptoms at the time of AHI testing, and 2) 80% reported signs or symptoms occurring within 2 weeks before undergoing testing.\n\n【12】These findings may have major clinical implications for community-based settings that restrict AHI testing to persons with ongoing signs or symptoms. This practice may be relatively insensitive in settings where MSM undergo HIV screening frequently . Our results show that expansion of AHI screening to include those with signs or symptoms during the 2 weeks before the test may increase the yield of AHI diagnoses by more than half.\n\n【13】Although our results may allow for estimation of sensitivity of signs and symptoms for AHI in persons seeking HIV testing, specificity of signs and symptoms remains unknown (in this study, signs and symptoms were not assessed in those who tested negative, and no control group was available). Estimates on frequency of signs and symptoms in HIV-negative persons (i.e. specificity) ranged widely in previous studies. Although in one study a specificity of 65% was estimated for influenza illness–like symptoms , specificities ranging from 38% to 91% for recent symptoms were estimated in another study . Limitations of those studies include the fact that exact time frames for occurrence of signs or symptoms (e.g. ongoing at the time of testing or occurring within the last 14 days) have not been evaluated, which makes comparison of results difficult. A limitation in our study is that all cases of AHI occurred among MSM. Therefore, our results may not be applicable to populations other than MSM, although previous studies have reported that clinical features of AHI may not differ by sex and age of patients .\n\n【14】In summary, HIV diagnostic testing strategies that limit AHI testing to patients with ongoing signs or symptoms may fail to identify many persons with AHI. In contrast, HIV NAT provided for MSM who report signs or symptoms during the preceding 2 weeks (representing 80% of AHI diagnoses) may increase the yield of AHI diagnoses by more than half.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e9b5e0a0-45e0-49da-8774-95206dad5e56", "title": "Human Parechovirus Infection, Denmark", "text": "【0】Human Parechovirus Infection, Denmark\nHuman parechoviruses (HPeVs) have recently been recognized to cause a variety of symptoms ranging from mild diarrhea to sepsis and meningitis, particularly among young children. HPeVs belong to a large family of nonenveloped, positive-sense, single-stranded RNA viruses, the _Picornaviridae_ , which comprises 12 genera (and 5 proposed genera). Six genera are associated with human infections: cardiovirus (saffold virus), cosavirus, enterovirus (EV), hepatovirus (hepatitis A), kobuvirus (Aichi virus) and HPeV. HPeV1 and HPeV2, originally known as echovirus 22 and 23 of the EV genus, respectively, were reclassified in 1999 as a separate genus ( _Parechovirus_ ) on the basis of genetic and biologic differences . Since the reclassification, the number of known HPeVs has increased and now totals 16 genotypes .\n\n【1】HPeV1 is known to be associated with asymptomatic infection. HPeV3 seems to be more or less well established . The remaining known HPeVs are clinically unexplored.\n\n【2】Whereas HPeV3 has been reported to be associated with sepsis-like syndrome, meningitis, encephalitis, and hepatitis in neonates and young infants , most HPeV infections are asymptomatic or associated with mild respiratory and/or gastrointestinal symptoms . HPeV incidence has been reported to show a seasonal pattern in temperate climates, with different types cocirculating simultaneously . Although HPeV infections are relatively common in most settings, experience from long-term population surveillance is somewhat sparse. We used 4 years of national laboratory surveillance to describe the molecular epidemiology of HPeV, including phylogenetic characteristics of the emerging HPeV epidemic, in Denmark.\n\n【3】### Materials and Methods\n\n【4】##### Study Design\n\n【5】During January 2009–December 2012, a total of 6,817 specimens were collected from 4,808 children from all regions of Denmark: 2,006 cerebrospinal fluid (CSF) samples from 1,952 children; 1,963 fecal samples from 1,608 children; 1,057 blood samples from 1,025 children; 682 respiratory samples from 610 children; 571 autopsy samples from 228 children; 302 swab specimens from 291 children; and 236 other samples. In general, samples were collected from patients with symptoms compatible with EV infection, particularly meningitis, sepsis-like illness, respiratory symptoms, and/or diarrhea. The CSF samples were collected from patients who typically had fever and/or clinical signs compatible with meningitis or encephalitis. The fecal samples were collected from a mixture of patients with community-acquired infection and hospitalized patients who typically had fever, diarrhea, and/or meningitis/sepsis-like illness. Autopsy material was analyzed for EV and HPeV as part of standard procedure irrespective of symptoms. Samples were sent by mail to the Department of Microbiological Diagnostics and Virology at the Statens Serum Institut (Copenhagen, Denmark) to undergo viral diagnostic and isolation and real-time PCR for EVs and HPeVs. HPeV infections detected in the same person within a 3-week period were considered to be the same infection. During the study period, the Department of Microbiological Diagnostics and Virology was the only laboratory conducting HPeV diagnostics in Denmark; therefore, the data reflect all HPeV infections detected in Denmark.\n\n【6】##### Laboratory Analyses\n\n【7】##### Sample Preparation\n\n【8】Only fecal samples and biopsy samples needed special preparation before use of the general nucleic acid extraction protocol. Fecal samples were prepared as a 10% wt/vol suspension in minimal essential medium, followed by centrifugation at 3,500 × _g_ for 30 min to remove inhibitors. Biopsy samples were suspended in Lysis/Binding Buffer from the MagNa Pure LC Total Nucleic Acid Isolation Kit (Roche Diagnostics, Mannheim, Germany), followed by homogenization.\n\n【9】##### Nucleic Acid Isolation\n\n【10】Nucleic acids were extracted from 200 μL sample material. All sample types, except CSF, were processed by using the MagNa Pure 96 DNA and Viral NA Small Volume Kit on the MagNa Pure 96 instrument (Roche Diagnostics) according to the manufacturer’s specifications. Nucleic acids from CSF were isolated by using the QIAamp DNA Blood Mini Kit on the QIAcube instrument (QIAGEN, Hilden, Germany) following the manufacturer’s specifications.\n\n【11】##### Amplification and Detection\n\n【12】For amplification, 5 μL of extracted nucleic acids were used per reverse transcription PCR (RT-PCR) reaction (total volume 25 μL) by using the OneStep RT-PCR Kit (QIAGEN). The reaction mixtures contained 1 μmol/L of each primer and 0.2 μmol/L probe. The primers and probe used have been published , and the reaction mixture also contained an assay for EV. The HPeV-specific probe was labeled with a Hex dye. The Mx3005P real-time thermocycler (Agilent Technologies A/S, Hoersholm, Denmark) was used for amplification and detection with the following settings: 50°C for 20 min, 95°C for 15 min, followed by 45 cycles of 95°C for 15 s and 55°C for 1 min.\n\n【13】##### Genotyping\n\n【14】We amplified 256–259 bp of the viral protein (VP) 3/VP1 region (from position 2159–2458 in relation to L02971) in a nested PCR. The first-round RT-PCR was conducted by using primers Harv1-F and Harv1-R  with a OneStep RT-PCR kit (QIAGEN) and the following thermocycler conditions: 50°C for 30 min, 95°C for 15 min, followed by 40 cycles of 95°C for 30 s, 42°C for 30 s, and 60°C for 45 s. The second-round PCR was conducted by using primers Harv2-F and Harv2-R  and the following thermocycler conditions: 95°C for 6 min, followed by 40 cycles of 95°C for 30 s, 60°C for 30 s, and 72°C for 45 s. This protocol was followed by a final extension at 72°C for 10 min. PCR amplification was followed by sequencing.\n\n【15】Before sequencing, PCR products were treated with exo-SAP IT (GE Healthcare, Buckinghamshire, UK). PCR products were sequenced by using the dideoxynucleotide chain termination method with the ABI Prism BigDye Terminator Cycle Sequencing Reaction kit on an ABI Prism 3100 automated sequencer (Applied Biosystems, Naerum, Denmark). Sequencing was performed with the forward and reverse primers from the second-round PCR. Sequences were assembled in BioNumerics 6.5 (Applied Maths, Kortrijk, Belgium). The sequences have been submitted to GenBank under accession nos. KF300772–KF300885.\n\n【16】##### Phylogenetic Analysis\n\n【17】Assembled sequences were aligned with reference sequences by using the Simmonic sequence editor . Phylogenetic and molecular evolutionary analyses were conducted by using MEGA 5.0 software . Genetic distances were calculated by using the Jukes-Cantor parameter at the nucleotide level, and the phylogenetic trees were constructed by using the maximum-likelihood method with 500 bootstrap replications .\n\n【18】### Results\n\n【19】##### Sample Material\n\n【20】##### Study Population\n\n【21】During January 2009–December 2012, from 6,817 samples from 4,804 children, we detected HPeV RNA in 202 (3%) specimens from 149 (3%) children from all counties of Denmark. Of the 149 individual HPeV cases, 25 (17%) were detected in CSF samples, 105 (70%) in fecal samples, and 19 (13%) in a variety of clinical specimens (8 pharynx/tonsil swabs, 2 bronchio-lavage fluids, and 9 biopsy specimens from children who died unexpectedly \\[5 from abdominal lymph nodes, 1 from the small intestine, 2 from the large intestine, and 1 from pulmonary tissue\\]). Of the 149 individual HPeV cases, 125 had sufficient material or RNA for further VP3/VP1 regional subtyping.\n\n【22】##### Frequency of HPeV-infected Patients\n\n【23】HPeV was detected in 52 (3%) of 1,744 patients in 2009, in 31 (2%) of 1,729 patients in 2010, in 33 (2%) of 1,939 patients in 2011, and in 33 (2%) of 1,405 patients in 2012. Of the 149 HPeV-positive patients, 52 (35%) tested positive in 2009, 31 (21%) in 2010, 33 (22%) in 2011, and 33 (22%) in 2012.\n\n【24】HPeV3 infections occurred in all months of the year, albeit with a clear seasonal pattern; only few cases occurred during the winter and spring months of November–May, with a marked increase during June and July, peaking during the fall months of September and October . HPeV1 infections show a seasonality pattern very similar to that of the HPeV3 infections, although no cases were observed during March–June.\n\n【25】##### Epidemiology of HPeV-infected Persons\n\n【26】Age was inversely associated with risk for HPeV infection among children <5 years of age: the overall median age of HPeV-infected children was 39 days (interquartile range \\[IQR\\] 22–71 days), and the median age of HPeV-negative children was 16.3 years (IQR 318 days–41.8 years). Children with HPeV3 infection (median age 37 days \\[IQR 19–59 days) were significantly younger than children with HPeV1 (median age 199 days \\[IQR 84–303 days\\]).\n\n【27】##### Clinical Features of HPeV Infection\n\n【28】Information about symptoms and the source of the sample was available for 89 (60%) of children with HPeV. A wide range of symptoms were reported: slightly more cases were associated with meningitis (36 cases) than diarrhea  and fever , but sepsis-like symptoms, convulsions, apathy, and general discomfort also were reported .\n\n【29】##### Virologic and Molecular Findings\n\n【30】Our sequencing of 1 of the least conservative regions of the capsid (VP3/VP1) resulted in the following: 90 HPeV3, 21 HPeV1, 8 HPeV6, 4 HPeV5, and 2 HPeV4. The genotype was assigned by BLAST analysis of the sequence against all published sequences in GenBank .\n\n【31】HPeV3 was more frequently associated with disease in neonates than was any other HPeV genotype. The mean age of HPeV3-infected children was 1.6 months, compared with 8.9 months for HPeV1-infected children (n = 20), and 8.1 months for HPeV6-infected children (n = 6) ( _t_ test for mean age difference, p = 0.01). Sequences were available from 8 of the 9 children who died unexpectedly; of these, 4 sequences were HPeV1, 2 were HPeV3, 1 was HPeV5, and 1 was HPeV6. Of the 36 patients with reported meningitis, HPeV types were available for 32; of these, 30 (94%) types were HPeV3, and 2 (6%) were HPeV1. Of the 4 patients with sepsis-like syndrome, sample material for typing was available for 3; all 3 were identified as HPeV3. HPeV types were available for 25 of the 29 patients with reported diarrhea; of these, 19 (76%) types were HPeV3, 4 (16%) were HPeV1, and 2 (8%) were HPeV6. All 25 CSF samples were HPeV3. HPeV1 and HPeV3 were detected throughout the study period; HPeV6 was detected in all years except 2010; and HPeV5 emerged only in 2012.\n\n【32】Sequence data from 124 of these 125 samples were of sufficient quality and were used to create phylogenetic trees , which also included reference sequences obtained from GenBank for each genotype identified in this study. The phylogenetic analyses revealed the existence of 5 closely related clades of HPeV3 circulating in Denmark throughout the study period, with most clades found all years of the study, implying cocirculation of these clades without genetic selection of either clade in the sequenced area. In addition, there was no particular geographic distribution of the individual clades. The other genotypes appear to follow the same pattern of circulation and of little evolutionary change within each genotype over time. However, data for the genotypes other than HPeV3 were insufficient to substantiate a division into individual clades.\n\n【33】We created a phylogenetic tree  including a representative strain from each of the HPeV3 clades detected in our study in Denmark, combined with matching HPeV sequence from Europe available in GenBank. This tree showed that clades 1–4 were most closely related to strains from Spain  and Italy  during 2006–2009, whereas clade 5 was identical (in the sequenced area) to a simultaneous strain from Germany . We found an intragenotype variation of 3.9%, within HPeV1 of 5.6%, within HPeV4 of 11.6%, within HPeV5 of 1.6%, and within HPeV6 of 3.5%.\n\n【34】### Discussion\n\n【35】Our study establishes HPeV as important differential diagnosis in young and often severely ill children in Denmark with fever and/or central nervous system symptoms. The sample material received for diagnostic testing, where a significant portion of the HPeV-positive samples are CSF, illustrates the severity of these infections. Without routine testing for HPeV of infants suspected to have EV infection (EV and HPeV3 infections are indistinguishable in infants ), these children’s illnesses would not have been diagnosed, which would have resulted in unfocused treatment, such as unnecessary antimicrobial drug therapy and unnecessary long hospital stays, as suggested by Wolthers et al. In our selected material (selected for suspected enteroviral disease), HPeV3 was by far the most prevalent HPeV type and was the only genotype detected in CSF. The high prevalence of HPeV3 contrasts with findings from other studies , which have established HPeV1 as the most common virus circulating in the population. However, these studies did not select the patients on the basis of symptoms because the children were part of healthy birth cohorts from prospective diabetes studies. In Denmark, although HPeV infections were detected throughout the year, the illness showed a marked seasonality with peaks during July and September, coinciding with the EV season. These findings underscore the need for HPeV and EV differential diagnostics in young children who have nonbacterial meningitis and severe disease in general caused by the similar clinical characteristics of HPeV and EV . The observed seasonality (i.e. occurrence of HPeV3 every year) indicates an endemic pattern of HPeV infection in Denmark. This finding contrasts with other HPeV studies from the United Kingdom and the Netherlands, where a biannual cycle of HPeV3 infections has been observed; the reason for this difference is unknown .\n\n【36】In our study, the children infected with HPeV (and in particular HPeV3) were very young (median age 39 days), which is consistent with prior studies . However, a recent study from Japan  has established that adults also can be infected with HPeV3, possibly resulting in epidemic myalgia.\n\n【37】We also showed that HPeV3 in Denmark did not undergo significant genetic diversification during 2009–2012, at least not in the sequenced part of the capsid (VP3/VP1) . This observation was in line with comparisons of the strains from Denmark with strains from Europe in which only limited variation periodically  and geographically was observed . The genetic distances in the trees are small (which further substantiates the results of limited evolutionary change). These indications of limited genetic evolution correspond well with results from a study in the Netherlands that described HPeV during 2000–2007 . The reason for the lack of intratypic diversification of the otherwise intertypically heterogenic capsid area is unknown. Perhaps most infections are subclinical and do not elicit an immunologic response, which might otherwise cause the virus to evolve to escape the immune response. Structural constraints might also exist that limit the genetic variation to ensure proper capsid formation.\n\n【38】Our data supported previous findings that HPeV3 is commonly associated with meningitis/sepsis symptoms among young infants, and we found that convulsions and apathy were reported among children with such severe cases. Because HPeV3 has been reported to cause white matter injury in 9 of 10 infected neonates, resulting in a range of complications from cerebral palsy in 1 child to epilepsy in another child to learning disabilities in a third child , focused longitudinal follow-up and cognitive evaluation of these children during childhood might be indicated to better understand the long-term consequences of this emerging viral disease.\n\n【39】In conclusion, HPeV was a clinically significant virus rivaling EVs in young children in Denmark. In particular, HPeV3 was circulating in our clinically selected material, with 5 different clades cocirculating over most of the years of the study. The patients were from all around Denmark demonstrating the need for central or disseminated HPeV diagnostics in Denmark so that HPeV is tested for in all young children suspected of infection with HPeV or EV. Testing might possibly reduce the length of hospitalization and limit the inappropriate use of antimicrobial drugs. Furthermore, doctors and especially pediatricians should be aware of the symptoms of HPeV-associated diseases and should be encouraged to collect a sample (primarily CSF and preferably combined with a fecal sample) for diagnostics. Centralized surveillance of this virus could provide deeper insight into the behavior of HPeV and might shed light on the clinical significance of the HPeVs other than type 3.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cc98c930-488f-4864-8eda-432f11ff00dc", "title": "Prevention of Chronic Hepatitis B after 3 Decades of Escalating Vaccination Policy, China", "text": "【0】Prevention of Chronic Hepatitis B after 3 Decades of Escalating Vaccination Policy, China\nHepatitis B virus (HBV) causes ≈240 million chronic infections and ≈780,000 deaths from cirrhosis and liver cancer annually . Recognizing the large global disease burden, the United Nations Sustainable Development Goals for 2030 include combating hepatitis. HBV has been highly endemic in China, where serosurveys in 1979 and 1992 indicated a 10% prevalence of HBV surface antigen (HBsAg) . High rates of chronic HBV infection among infants indicated that the infection occurred in early childhood . Historical HBV transmission built a reservoir of ≈90 million chronically infected persons in China , accounting for 30% of the global burden of chronic HBV infection .\n\n【1】The government of China adopted increasingly comprehensive strategies to prevent HBV transmission, including immunization, promotion of safe injection practices, blood donation screening, and surveillance . Implementation of China’s immunization strategy began in 1985 with licensure of plasma-derived hepatitis B vaccine (HepB). A recombinant vaccine was licensed in 1992 and managed nationally. The strategy was designed to interrupt perinatal HBV transmission and provide newborns lifelong protection from HBV with a birth dose of HepB followed by 2 additional doses during infancy. Before 2002, HepB was managed as a type 2 vaccine for which parents or adult vaccinees had to pay out-of-pocket. In 2002, China integrated HepB into its Expanded Program on Immunization (EPI), making the vaccine available at no cost to children through 14 years of age . During 2009–2011, China conducted a HepB catch-up campaign for children <15 years of age who were born during 1994–2001; this campaign vaccinated ≈68 million children with HepB. In 2011, China launched a program integrating prevention of mother-to-child transmission of HIV, syphilis, and HBV in 1,156 counties (representing 44% of pregnant women in China) and then expanded the program nationwide in 2015, covering all pregnancies .\n\n【2】The effectiveness of China’s HBV prevention measures is evaluated by using national serologic surveys; the fourth such survey was conducted in November 2014. We report results of this survey in the context of China’s HBV prevention and control measures, along with reanalysis of the 1992 and 2006 surveys .\n\n【3】### Methods\n\n【4】##### Survey Conduct\n\n【5】Target populations for all 3 surveys were local residents residing in national disease surveillance points (DSPs) for \\> 6 months. DSPs were selected by the Chinese Academy of Preventive Medicine (now the Chinese Center for Disease Control and Prevention \\[China CDC\\]) to be representative of the population of China. Population demographic and socioeconomic conditions and morbidity and mortality continue to be representative .\n\n【6】The surveys’ targeted age ranges were 1–59 years in 1992 and 2006 and 1–29 years in 2014. The 2014 serosurvey used age groups of 1–4 years, 5–14 years, and 15–29 years; we reanalyzed the 1992 and 2006 surveys by using these groupings.\n\n【7】The 1992 serosurvey used a multistage cluster sampling strategy. Three villages were identified at random from each of 145 DSPs; families were randomly selected from each village from lists of residents; and all age-appropriate family members were selected . The total sample size was 67,017.\n\n【8】The 2006 serosurvey used a 3-stage cluster sampling strategy to identify 369 townships at random for the first stage and 369 villages at random for the second stage. Finally, 81,775 persons, stratified into age groups of 1–4 years, 5–14 years, and 15–59 years, were selected at random from a list of village residents . Children 1–14 years of age were oversampled to increase the precision of estimates for young children.\n\n【9】The 2014 serosurvey used a 2-stage cluster random sample from the same 160 DSPs that were used in the 2006 serosurvey. First, we allocated the same proportion of the sample to each region (eastern, middle, and western) and location type (urban and rural); second, in each region, we allocated samples to each DSP by using a probability-proportional-to-size method; third, in each DSP, we randomly selected 3 villages or communities, and from these, samples in grouping of 1–4 years, 5–14 years, and 15–29 years from each village or community were selected based on simple random sample. In all, 324 villages/communities were selected from the 38,527 villages/communities in the DSPs, with a sampling probability proportional to their size. Then persons were selected by simple random sample from local government lists of residents of sampled villages/communities into the age-group strata. The sample size calculation was based on expected HBsAg prevalence extrapolated from the 2006 survey by age group (0.7% for 1–4 years, 1.5% for 5–14 years, and 5.0% for 15–29 years) and was powered to detect differences of \\+ 50% the expected point prevalence. The final target sample size was 31,024. Lists of eligible persons were sampled systematically until the target sample size was reached.\n\n【10】The field investigation methods were identical in the 3 serosurveys . The interviews were carried out by trained professionals through house-to-house visits in the order of the sample listings. Communities had been notified in advance of the survey. Working persons and school children were interviewed during weekends or after school hours. For persons who were not at home, the interview staff made up to 3 additional home visits within 1 week. If, after 3 unsuccessful visits, the person could not be found, he or she was considered missing. Face-to-face interviews with the respondent or the respondent’s parent were completed by trained staff by using standard questionnaires to obtain basic information, including sex, birthdate, ethnicity, birthplace, and HepB vaccination history of the children <15 years of age (validated by parent-held certificate or village vaccination record).\n\n【11】##### Laboratory Technique\n\n【12】All specimens were tested in the National Hepatitis Laboratory of the Institute for Viral Disease Control and Prevention, China CDC. For the 2006  and the 2014 serosurveys, ELISA reagents were used to detect levels of HBsAg, anti-HBV surface antigens (anti-HBs), and anti-HBV core antigens (anti-HBc). HBsAg \\> 2.1 IU was considered positive for consistency across serosurveys. Specimens yielding inconsistent or indeterminate results were retested by using microparticle enzyme immunoassay reagents (Abbott Laboratories, Chicago, IL, USA). HBsAg-positive specimens were tested for HBV e antigen and anti–HBV e antigen also by using Abbott microparticle enzyme immunoassay reagents. For the 1992 serosurvey, HBsAg, anti-HBs, and anti-HBc were tested by solid-phase radioimmunoassay (SPRIA) .\n\n【13】##### Statistical Analysis\n\n【14】In the 2006 and 2014 surveys, data were double-entered into EpiData version 3.02 (EpiData Association, Odense, Denmark) and verified for consistency and then were analyzed by using SAS version 9.4 (SAS Institute, Inc. Cary, NC, USA). Statistical methods of the 2014 serosurvey were identical to those of the 2006 serosurvey. To ensure representativeness of poststratification adjustments, sample weighting components were village selection probability and age-specific and person-selection probabilities within the village. The weight per person _i_ was _w ji  \\= w j  × w i,j  × w adj _ , where _w j _ was the reciprocal of the probability of including village _j_ , _w i,j _ was the reciprocal of the conditional inclusion probability of person _i_ from village _j_ , and _w adj _ was an adjustment factor for person _i_ so the sum of weights equaled China’s population. We used the SAS procedure surveyfreq to calculate point estimates and 95% CIs of serologic markers by using weighting adjustments; Taylor series linearization was used for variance estimations. The 1992 survey had no design weighting, so we determined unweighted point prevalence and 95% CIs.\n\n【15】The HBsAg, anti-HBs, and anti-HBc prevalence of persons 1–29 years of age covered in the 1992 and 2006 serosurveys were reanalyzed to be consistent with the format of the 2014 serosurvey by 3 age groups (1–4 years, 5–14 years, and 15–19 years), as well as sex, ethnicity, location type (urban or rural), region, and year of birth.\n\n【16】##### Vaccination Coverage and HBsAg Prevalence by Birth Year\n\n【17】HepB vaccination history for children <15 years of age was coded as vaccinated (i.e. birth dose plus 2 more doses or incomplete series), unvaccinated, or unknown, based on children’s immunization certificates. Coverage levels of children born during 1985–1991, 1992–2005, and 2006–2013 were determined from the 1992, 2006, and 2014 surveys, respectively. Weighted HBsAg prevalences for the 1962–1991, 1976–2005, and 1985–2013 birth cohorts were determined by using the 1992, 2006, and 2014 surveys for each birth cohort included in the respective surveys.\n\n【18】##### Analyses of Cases Averted\n\n【19】We estimated the number of chronic HBV infections prevented in the 1992–2013 birth cohorts by using Goldstein’s model, which was used in the 2006 survey to estimate baseline disease prevalence and cases prevented . This model provides estimates of total numbers of cases and deaths caused by acute HBV infection and numbers of cases and sequelae from chronic HBV infection, including cirrhosis and primary hepatocellular carcinoma that would develop during the lifetime of a birth cohort. The key inputs to the model are baseline HBsAg seroprevalence in the entire population and among women of childbearing age and HepB coverage. Figures on the effect of vaccination by birth cohort were summed to estimate the overall effect by using birth cohort sizes of 16.97 million persons per year. We assumed a baseline HBsAg prevalence of 8.58% among women of childbearing age, uniformly distributed, with 30% also being HBeAg positive. Among 5-year-old children, 32% were assumed to become chronically HBV-infected (anti-HBc positive) by 5 years of age and 55% to be chronically infected by 30 years of age; these percentages represented force of infection without vaccination .\n\n【20】##### Quality Control\n\n【21】China CDC convened expert groups to guide design, fieldwork, laboratory testing, and analyses for the 3 surveys. Pilots were conducted before each survey. County CDC staff administered questionnaires and collected and managed blood specimens.\n\n【22】##### Ethical Reviews\n\n【23】The 1992 survey was approved by Chinese Academy of Preventive Medicine’s Ethical Review Committee; the 2006 and 2014 surveys were approved by China CDC’s Ethical Review Committee. In 2006 and 2014, participants were informed of the study purpose and their right to keep information confidential. Consent was obtained before interview and blood drawing.\n\n【24】### Results\n\n【25】##### Response Rate\n\n【26】In the 2014 survey, investigators visited selected houses up to 3 times and invited 38,142 persons to participate. Of those invited, 31,772 gave consent, yielding an 83.3% response rate. Among those consenting, 59 (0.2%) were excluded because of insufficient serum for laboratory analysis. The final sample was 31,713 persons . Demographic characteristics of the subjects in the 3 surveys were similar except that the percentage of the sample residing in urban areas increased from 25.7% to 49.6% during 1992–2006, and 49.8% in 2014, reflecting China’s urbanization . Among respondents <15 years of age in the 1992, 2006, and 2014 surveys, 96.5% (21,638 of 22,419), 81.6% (32,732 of 40,129), and 18.0% (2,923 of 16,239) had vaccination records, respectively.\n\n【27】##### HBV Serologic Markers\n\n【28】HBsAg prevalence among persons 1–4 years, 5–14 years, and 15–29 years of age in 2014 was 0.3%, 0.9%, and 4.4%, respectively. We compared HBsAg, anti-HBsAg, and anti-HBc results from the 3 surveys . HBsAg prevalence among 1–29-year-olds declined from 10.1% to 2.6% during 1992–2014. Declines were observed in all age, sex, ethnicity, location type (urban/rural), and regional groups. Among children <15 years of age, HBsAg prevalence declined from 10.5% to 0.8%. Prevalence of anti-HBs among 1–29-year-olds increased from 25.4% to 57.8% during 1992–2014. Prevalence of anti-HBc declined from 45.8% to 13.0% during 1992–2014, declining in all subpopulations.\n\n【29】In 1992, HBsAg prevalence was 10% across all age groups , consistent with HBsAg prevalence in 1979 . In 2006, HBsAg prevalence was high among 20–29-year-olds (8.3%) and low among 1–4-year-olds (1.0%). Similar trends in HBsAg prevalence were observed seen in the 2014 survey.\n\n【30】The relative decline in HBsAg prevalence was uneven by region and age group . Among 1–4-year-olds, eastern, central, and western region prevalences decreased by >95%, but among 15–29-year-olds, declines were 62.0%, 62.1%, and 37.0%, respectively. The decline in HBsAg prevalence was >95% among 1–4 year-olds regardless of rural/urban status , but among 15–29-year-olds, the decline was greater among urban residents than rural residents (68.4% and 44.3%). HBsAg prevalence by birth cohort and HepB coverage, when mapped against the timeline of important immunization program events, were noticeably affected as incremental interventions were added .\n\n【31】##### Cases Averted\n\n【32】During 2010–2014, China prevented an additional estimated 4 million chronic HBV infections on top of the 24 million chronic infections prevented during 1992–2009 . In total, 28 million chronic HBV infections were averted, and 5 million deaths from HBV infection complications were prevented\n\n【33】### Discussion\n\n【34】Compared with the prevaccine era, chronic HBV infection in China has been reduced by 90% (from 10.5% to 0.8%) among children <15 years of age and by 97% (from 9.9% to 0.3%) among children <5 years of age. Disparities by region and urban/rural status that existed among young children in 1992 and 2006 were largely eliminated by 2014. Lower HBsAg prevalence among young children in 2014 (1.0%) compared with 2006 (0.3%) shows increasing effectiveness of the program.\n\n【35】HBsAg prevalence among 1–29-year-olds declined 46% during 1992–2006 (from 10.1% to 5.5%) and 52% during 2006–2014 (from 5.5% to 2.6%). As a result of China’s program, an estimated 120 million HBV infections and 28 million chronic infections were averted.\n\n【36】Without postexposure prophylaxis provided by the HepB birth dose, 30% of infants born to HBsAg-positive mothers will become infected, and 90% of the infections will become chronic . Because administering a birth dose is challenging when childbirth happens in the home, a key element of China’s success was promoting facility-based childbirth. Implementation of the timely birth dose was accelerated by the GAVI hepatitis B project, which promoted the birth-dose policy in rural and western areas of China . Additional strategies, such as promoting safe injection practices and screening donated blood, have also been important for chronic HBV prevention. In 2000, China passed a regulation banning the reuse of medical devices labeled for single use. In 2005, the Chinese Medical Association published clinical guidelines for injections and other skin-piercing procedures. In 2007, autodisable syringes became available for vaccine injections, and by 2010, reusable injection equipment was eliminated in China and disposable and autodisable syringes became universally used . Since 1988, donated blood has been screened for HBV serologic markers, and since 2015, HBsAg-negative donated blood has been tested for HBV DNA . Although HBV infection caused by unsafe injections and blood transfusion has been reduced, modeling shows that the newborn and infant vaccination strategy has been independently responsible for preventing ≈95% of chronic HBV infections in China .\n\n【37】The government of China regards health equity as important for social justice and fairness . In 2000, the ministries of health and finance and the State Council implemented a program to reduce maternal mortality rates and eliminate maternal/neonatal tetanus. The government established insurance plans to ensure access to healthcare and birth facilities, especially in impoverished, remote, or ethnic minority areas. The in-hospital delivery rate increased from 44% in 1985 to 99% in 2013 . By using the principle “whoever delivers the baby vaccinates the baby,” virtually all infants born in birthing facilities receive a birth dose of HepB .\n\n【38】Timely vaccination is used as an evaluation measure of public health effectiveness . High 3-dose vaccination coverage has been maintained continuously from 2009 through 2015, having increased from 70% in 2002 to >95% in 2009 and afterward. HepB birth-dose coverage increased from 22% in 1992 to 71% in 2002 and 94% in 2013 . The national program integrating prevention of mother-to-child transmission of HIV, syphilis, and HBV has been providing HBsAg screening for pregnant women and hepatitis B immunoglobulin for all infants born to HBsAg-positive women since 2012 .\n\n【39】Vaccines used in the program have been evaluated periodically. When HepB became government-supported in 2002, the dose provided was 5 μg/0.5 mL, which was known to prevent infection in 85%–90% of children born to HBsAg-positive women . To improve effectiveness, the dose was increased to 10 μg/0.5 mL in 2011. China’s model supports the United Nations Sustainable Development Goals and the World Health Organization’s new Global Hepatitis Framework by greatly reducing HBV transmission with strategies that integrate HBV prevention into the healthcare sector.\n\n【40】Strengths of this study include a sound sampling strategy with comparable methods across 3 surveys separated in time, identical laboratory procedures in the 2006 and 2014 surveys, and use of sufficiently large sample sizes to support precise estimates. Our study provides previously unpublished coverage levels of plasma-derived HepB before licensure of the recombinant vaccine.\n\n【41】Weaknesses of the study include the use of different laboratory methods for the 1992 survey than those used for the 2006 and 2014 surveys, different nonresponse rates of the 2006 and 2014 surveys, underestimation of immunity indicated by anti-HB levels (because when antibody levels wane, memory B cell mediated anamnestic response to HBV exposure can maintain protection from infection), undersampling of the migrant population (because only those residing \\> 6 months in a given survey area were included), and the fact that HepB coverage levels among teens and adults are not measured in China. The 1992 survey used SPRIA for HBV infection serologic markers, and the 2006 and 2014 surveys used ELISA. According to previous studies , SPRIA is less specific than ELISA for detecting HBsAg; this difference could have led to overestimation of the relative decrease in HBsAg prevalence because ELISA testing was used in 2006 and 2014. However, we believe that the effect on our results is modest, especially for the current estimates of HBsAg prevalence, because the ELISA tests used are considered acceptable at international standards. We used the same HBsAg cutoff values for ELISA testing in the 2006 and 2014 surveys for the sake of consistency; however, higher cutoff values are used more frequently now.\n\n【42】HBsAg prevalence among several birth cohorts was measured by \\> 1 survey . Of interest is that results are more consistent for younger, double-measured birth cohorts than for older, double- and triple-measured cohorts. These differences might be attributable to several reasons. Persons in the double-measured birth cohorts were 22 years older in the 2014 survey than the 1992 survey and were 8 years older in the 2014 survey than the 2006 survey. HBsAg prevalence has a small, natural decline with age . This natural decline will increase the HBsAg prevalence differences from the 1992 survey, as will the age-based accumulation of deaths caused by complications of chronic HBV. Finally, lower specificity of the 1992 survey can lead to an upward bias of the differences from the 1992 survey.\n\n【43】Our study has 3 main programmatic implications. First, the annual need for perinatal postexposure prophylaxis remains substantial. The prevalence of HBsAg in women of childbearing age and the size of the birth cohort in China (16.97 million), implies that 750,000 to 1 million infants are born to HBsAg-positive women annually .\n\n【44】Second, prevention measures must continue for decades. The age group with the highest HBsAg prevalence corresponds to the age groups with the highest fertility rate in China (69.5/1,000 for those 20–24 years and 94.0/1,000 for those age 24–29 years of age) . Even when children of today become adults, nearly 200,000 infants will be born to HBsAg carriers each year and will need postexposure prophylaxis to prevent HBV infection.\n\n【45】Third, many newborns still become chronically infected. Although China has reduced perinatal transmission by 97%, an HBsAg prevalence of 0.3% in a birth cohort of 16 million implies that 50,000 perinatal infections still occur annually. Additional strategies will be needed to eliminate vertical transmission.\n\n【46】We believe that the success HBV prevention should be communicated to stakeholders to help sustain confidence in the immunization effort. Confidence in vaccines can be fragile, as was made evident by a temporary loss of confidence in HepB in 2013 and 2014 , and showing the strongly positive impact of vaccination may help maintain or restore confidence .\n\n【47】Our results raise several questions. Can the current strategy eliminate perinatal transmission? HepB is not 100% effective, and additional strategies may need to be used. Antiviral prophylaxis during the third trimester for HBsAg-positive pregnant women with high HBV DNA is being shown to decrease perinatal transmission of HBV  and may need to become a standard of care in the future.\n\n【48】Should postvaccination serologic testing (PVST) become a recommended standard in China? PVST can help confirm whether an HBV-exposed infant is protected, is susceptible and needs to be revaccinated, or is infected and needs referral for follow-up care. The cost-effectiveness, feasibility, and acceptability of PVST for HBV-exposed infants in China should be evaluated.\n\n【49】Can adults at risk of HBV infection be vaccinated? Identifying cost-effective means to protect at-risk adults from HBV has potential to avert infections .\n\n【50】Finally, treating the estimated 90 million persons with chronic HBV infection is critically important . Prevention works, but not always perfectly, and many adults were born before prevention of HBV was possible.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "168c4eda-c319-40d3-b11e-47f5a08ce807", "title": "Individual-Level Fitness and Absenteeism in New York City Middle School Youths, 2006–2013", "text": "【0】Individual-Level Fitness and Absenteeism in New York City Middle School Youths, 2006–2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Youth health-related fitness positively affects academic outcomes, although limited research has focused on the relationship between fitness and school absenteeism. We examined the longitudinal association between individual children’s fitness and lagged school absenteeism over 4 years in urban middle schools.\n\n【3】**Methods**\n\n【4】Six cohorts of New York City public school students were followed from grades 5 through 8 (school years 2006–2007 through 2012–2013; n = 349,381). A 3-level longitudinal generalized linear mixed model was used to test the association of change in fitness composite percentile scores and 1-year lagged child-specific days absent.\n\n【5】**Results**\n\n【6】Adjusted 3-level negative binomial models showed that students with a more than 20% increase, 10% to 20% increase, less than 10% increase or decrease, and 10% to 20% decrease in fitness from the year prior had 11.9% (95% confidence interval \\[CI\\], 7.2–16.8), 6.1% (95% CI, 1.0–11.4), 2.6% (95% CI, −1.1 to 6.5), and 0.4% (95% CI, −4.3 to 5.4) lower absenteeism compared with students with a more than 20% fitness decrease.\n\n【7】**Conclusion**\n\n【8】Cumulative effects of fitness improvement could have a significant impact on child absenteeism over time, particularly in high-need subgroups. Future research should examine the potential for school-based fitness interventions to reduce absenteeism rates, particularly for youths who have fitness drop-offs in adolescence.\n\n【9】Introduction\n------------\n\n【10】Youth physical activity and health-related fitness (henceforth fitness) positively affects academic outcomes , potentially acting through pathways involving enhanced cognition and memory  or improvements in both physical and psychosocial wellness . Fitness and physical activity are strongly associated, and frequent vigorous physical activities are likely to improve fitness . For example, daily physical activity of at least moderate intensity is associated with reduced clustering of cardiovascular risk factors in youths, including high blood pressure, insulin level, lipids, and adiposity . However, accelerometry data show that only 42% of children aged 6 to 11 years meet international physical activity recommendations for at least 60 minutes per day of moderate to vigorous physical activity . Although these rates are similar to rates in European countries , declines in physical activity are steeper from childhood to adolescence in the United States compared with declines in other nations . This national trend is also evident in New York City (NYC), where 40% and 20% of youths aged 6 to 12 and 14 to 18, respectively, meet physical activity recommendations .\n\n【11】Another established predictor of academic performance is school absenteeism , which may mediate the observed fitness–academic achievement association. Maintaining regular attendance, defined as missing fewer than 6 excused or unexcused days per year, predicts academic success . School absenteeism, regardless of reason, predicts poor academic achievement and is associated with poor school adjustment; alcohol, tobacco, and substance use; increased rates of teen pregnancy; juvenile delinquency; and both family and home–school disengagement . Fitness improvements may both directly and indirectly reduce absenteeism, working potentially through pathways involving self-esteem, physical health, mental health, and cognitive processing .\n\n【12】Limited research has examined the fitness–absenteeism relationship , demonstrating consistent inverse associations between fitness and school absenteeism. For example, Blom et al demonstrated that students with greater fitness had lower odds of more than 8 absences per year (odds ratio \\[OR\\], 3.31; 95% confidence interval \\[CI\\], 1.51–7.28 for students with 6 compared with less than 5 healthy fitness zones achieved) . Two other articles found significant crude associations between student physical activity and absenteeism . These studies drew predominantly from cross-sectional data and did not account for a range of potential confounders, including contextual factors that contribute to absenteeism and fitness. For example, neighborhood poverty contributes to parent–school engagement and youth fitness . Similarly, school size affects programs and policy toward school attendance and physical activity . The bulk of research on fitness and absenteeism is unable to support causal hypotheses given that temporality of exposure and outcome are not known. Nuanced research in this area that draws from individual-level measures collected over multiple years and includes school-level factors is necessary to better inform policy in support of increased school-based fitness programs.\n\n【13】We analyzed the longitudinal association between change in fitness and 1-year lagged absenteeism in 6 cohorts of NYC public school students based on year of initiating middle school and followed consecutively over 4 years (fitness change from grades 5 to 6, 6 to 7, and 7 to 8 paired with days absent per year for grades 6, 7, and 8, respectively) during a 7-year study period (2006–2007 through 2012–2013). We hypothesized that improvements in fitness (cardiorespiratory, muscular endurance, and muscular strength fitness composite percentile scores) would predict lower subsequent absenteeism.\n\n【14】Methods\n-------\n\n【15】### Study population\n\n【16】Data were drawn from the NYC FITNESSGRAM (Fitnessgram) data set jointly managed by the NYC Department of Education (DOE) and Department of Health and Mental Hygiene (DOHMH) . It comprises annual fitness assessments collected by DOE for approximately 870,000 NYC public school students per year (grades K–12) starting in 2006–2007. This study was approved by the City University of New York and DOHMH institutional review boards.\n\n【17】The Fitnessgram is based on the Cooper Institute’s Fitnessgram, which has both strong reliability and validity . Fitnessgram performance tests provide a health assessment related to present and future health outcomes. NYC schools are mandated to have 85% or more of eligible students complete the test each year. Inclusion criteria for this study included enrollment in a NYC public school that collected Fitnessgram measurements for 2 or more consecutive years while in grades 6 through 8 during the study period (2006–2007 through 2012–2013) . Student cohorts were defined based on year of initiating grade 6. Students were excluded (n = 6,225) if they were enrolled for less than n − 5 days per school year (where n is the maximum number of days enrolled across all students in each given year \\[n range: 292–297 days\\]) to ensure a consistent period of observation across school years with different total instructional days per year. Next, students were excluded if they did not take the Fitnessgram test for 2 or more consecutive years (n = 56,464), attended schools with poor-quality fitness data (n = 350), or changed schools during 6th through 8th grade (to be able to account for school clustering in the analysis; n = 44,977). After the above exclusions, the final sample of 6th through 8th graders included 349,381 unique students (51% male, 83% born in the United States, 38% Hispanic, 28% non-Hispanic black, and 16% non-Hispanic white; mean \\[standard deviation (SD)\\] school population = 541 ). Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively, across 624 schools.  \n\n【18】Sample selection flowchart for the association of fitness and absenteeism in New York City (NYC) public middle school students, 2006–2007 through 2012–2013. \n\n【19】### Measures\n\n【20】The primary exposure was a categorical variable representing age- and sex-specific percentage change in fitness composite percentile scores based on the sum of percentile scores for the Progressive Aerobic Cardiovascular Endurance Run (PACER), muscle strength and endurance (curl-up and push-up) tests . Scores were converted to percentiles to account for expected improvements in performance with increasing age and by sex. The fitness variable was categorized as more than 20% decrease, 10% to 20% decrease, less than 10% change, 10% to 20% increase, and greater than 20% increase in performance from the year prior, consistent with longitudinal research on fitness and academic outcomes drawing from the Fitnessgram data set .\n\n【21】The primary outcome variable for this analysis was student-level number of days absent per year. Annual enrollment and attendance records were matched to Fitnessgram results by a unique student identifier.\n\n【22】Adjusted models included sex, age, race/ethnicity, place of birth, socioeconomic status (SES), and school size. These covariates predict both fitness and absenteeism . Age at the time of height and weight measurement was treated as a continuous variable. Race/ethnicity was based on school enrollment forms completed by parents and grouped into 5 categories: Hispanic, non-Hispanic black, non-Hispanic white, Asian/Pacific Islander, and other. Place of birth (United States vs foreign country) was included as a covariate based on literature demonstrating that immigration status is predictive of physical activity  and school attendance . SES was defined as the percentage of households in the students’ school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] poverty area) according to American Community Survey 2007–2012 data . School size classified schools, as per the literature, as small (<400 students) or nonsmall (≥400 students) .\n\n【23】Change in obesity status from the year prior (obese to not obese, consistently not obese, consistently obese, not obese to obese) was also included as a potential confounder based on the literature . Body mass index (BMI) is collected annually as a part of the Fitnessgram curriculum. Obesity was defined as having a BMI in the 95th percentile or higher for the same sex and age group using 2000 Centers for Disease Control and Prevention guidelines . Change in obesity status category was used in lieu of changes in BMI percentile to capture meaningful shifts in body composition associated with school outcomes .\n\n【24】### Statistical analysis\n\n【25】Descriptive statistics were computed to summarize sample characteristics. Next, trends in absenteeism (days absent) by fitness, grade, and demographics were examined.\n\n【26】Because observations were nested within students, nested within schools, mixed-model methods were used. Specifically, a series of crude and adjusted 3-level longitudinal generalized linear mixed models with random intercepts for student and school effects were fit to assess the fitness–absenteeism association while accounting for clustering and individual- and school-level confounders.\n\n【27】First, to determine the extent of variation in absenteeism at the school level, an unconditional model with random intercepts was fit to the data (model 1). The school-level intraclass correlation (ICC) was calculated as the ratio of the variance for the school divided by the sum of the 3 variance parameter estimates, represented as σ 2  school  **/ (** σ 2  student  \\+ σ 2  school  \\+ σ 2  ε  ). Although univariate distributions for days absent demonstrated a long right-tailed Poisson distribution, the ICC was calculated based on a linear model given that the ICC definition is not well defined for Poisson models .\n\n【28】Next, the longitudinal association of change in fitness and lagged number of days absent per year was assessed by using a 3-level crude longitudinal negative binomial mixed model with random intercepts and the exposure, child-specific change in fitness from the year prior, as well as an offset term representing total instructional days per school year included in the model (model 2). Negative binomial models were used because data were overdispersed. β Coefficients represented the effects of the exposure, change in fitness on outcome, 1-year lagged number of days absent per year. Absenteeism rates were computed by calculating the incidence rate ratio, represented as exp(β).\n\n【29】Finally, potential individual- and group-level confounders were added to the model (model 3). Confounding variables included level-1 time-varying covariates for grade, year (to control for potential cohort effects), and change in obesity status from the year prior, level-2 covariates for individual sociodemographic factors (sex, race/ethnicity, place of birth), level-3 covariates for school size and SES, and interactions (grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and SES\\*race/ethnicity).\n\n【30】In these analyses, students contributed fitness-change data for 5th to 6th, 6th to 7th, and/or 7th to 8th grades (n = 349,381 unique students; 675,318 observations). A 2-sided _P_ value of less than .05 was considered significant. Analyses were performed using SAS version 9.4 software (SAS Institute, Inc).\n\n【31】Results\n-------\n\n【32】Just under 40% of students had less than 10% change in fitness from the year prior, followed by greater than 20% increase (20%), greater than 20% decrease (19%), 10% to 20% increase (12%), and 10% to 20% decrease (12%) . The mean (SD) number of days absent per year were highest among boys (11.0 \\[11.7\\]) and Hispanic (12.6 \\[12.9\\]) and non-Hispanic black (12.3 \\[13.1\\]) racial/ethnic groups . Mean days absent were also highest among students who were born in the United States (11.3 \\[12.1\\]) compared with those who were born in a foreign country (11.1 \\[13.8\\]).\n\n【33】Overall, the mean number of days absent per year decreased with improvements in fitness scores from the year prior. The mean (SD) days absent per year for students with the lowest (>20% decrease) to highest (>20% increase) improvements in fitness were 11.9 (12.8), 11.1 (12.2), 10.7 (11.9), 10.3 (11.3), and 10.3 (11.2). Also, fitness decreased and absenteeism increased with increasing grade . Moreover, for students in the same grade, the difference in mean days absent for those with improved versus diminished fitness became larger with increasing grade level . For example, mean (SD) days absent for students with the greatest increase (>20%) in fitness were 9.6 (10.1), 9.8 (10.8), and 11.9 (12.7), for students in 6th, 7th, and 8th grades, respectively. In contrast, mean (SD) days absent for students with the greatest decrease (>20%) in fitness were 10.6 (11.3), 11.6 (12.6), and 13.9 (14.3), for students in 6th, 7th, and 8th grades, respectively.  \n\n【34】Mean days absent per year by grade across fitness-change categories in New York City public middle school students (N = 349,381), 2006–2007 through 2012–2013. Change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior. Categories are based on tabulated mean estimates. \n\n【35】The ICC (model 1) demonstrated a sizable degree of variance in student absenteeism explained by schools (9%). Results from model 2 showed all levels of change in fitness were significantly associated with absenteeism ( _P_ < .001). Compared with the reference category (>20% decrease in fitness), the absenteeism rate decreased 13.3% (95% CI, 8.3–16.6), 8.3% (95% CI, 3.3–12.7), 5.6% (95% CI, 1.9–9.0), and 1.6% (95% CI, −3.0 to 6.2) for those who had a greater than 20% increase, 10% to 20% increase, less than 10% change, and 10% to 20% decrease in fitness composite percentile scores from the year prior, respectively.\n\n【36】After adjusting for covariates (sex, race/ethnicity, change in obesity status from the year prior, place of birth, SES, and school size), and including interactions (grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and SES\\*race/ethnicity), β estimates for the association of fitness change and lagged number of days absent per year diminished but remained significant ( _P_ < .005). Relative to the reference category (>20% decrease in fitness), the absenteeism rate decreased 11.9% (95% CI, 7.2–16.8), 6.1% (95% CI, 1.0–11.4), 2.6% (95% CI, −1.1 to 6.5), and 0.4% (95% CI, −4.3 to 5.4) for those who had a greater than 20% increase, 10% to 20% increase, less than 10% change, and 10% to 20% decrease in fitness composite percentile scores from the year prior, respectively (model 3).\n\n【37】Sensitivity analyses were run to determine the effect of days of enrollment exclusions, BMI categorization specification, and total years of consecutive fitness change data on findings. Results showed slightly more conservative estimates for the magnitude of effects, although the inverse dose–response association remained consistent and significant ( _P_ < .001, _P_ \\= .004, and _P_ \\= .01 for enrollment, BMI, and fitness data sensitivity models, respectively).\n\n【38】Discussion\n----------\n\n【39】We found that all levels of 1-year change in fitness were significantly associated with absenteeism ( _P_ < .001) in both crude and adjusted models. Furthermore, consistent levels of fitness improvement each year at the greater than 20% level (vs >20% decrease) were found to have the potential to reduce a student’s number of days absent substantially. For example, a child with a mean 10 days absent in 6th grade would have 6.5 days absent per year in 8th grade and 1.5 days absent per year in 12th grade. This change in days absent represents a shift well within the range of regular attendance (≤5 days absent per year). Findings here are consistent with the existing cross-sectional literature on fitness and absenteeism , lending strong support for future research on the effects of youth fitness interventions on school absenteeism. NYC programs unrelated to fitness promotion have shown a 15% reduction in chronic absenteeism in 100 high-need schools over 2 years , through implementing “early warning” flags to identify at-risk students, family and student “success mentors,” progress monitoring systems, and community collaborations. However, despite gains and similar programs nationally, high absenteeism rates remain widespread, including 5 million to 7.5 million chronically absent US students each year .\n\n【40】Strengths of this study were being the first article to the authors’ knowledge to examine the association of change in fitness and lagged absenteeism, drawing from multiple years of multilevel data. Also, this analysis included a large and diverse study sample of approximately 349,000 students comprised of 6 cohorts.\n\n【41】Findings from this study may not be generalized to other cities or nationally, given a high minority and low-income population in NYC. Future work should examine potential differences in the fitness–attendance relationship by race/ethnicity and poverty status, given higher absenteeism observed in this study among both non-Hispanic black and Hispanic students and those attending schools in high poverty areas. Furthermore, although DOE protocols promote retesting students who are absent on the original testing dates, a large number of students were excluded because of missing Fitnessgram tests for 2 or more consecutive years, insufficient enrollment period, or moving schools. Not all students are required to take the Fitnessgram, including those with chronic health conditions such as severe asthma. These students, however, would be more likely to have higher absenteeism given psychosocial, family, and health factors associated with moving and long-term absences . These effects potentially would move the association farther from the null.\n\n【42】Although we offer evidence in support of a causal association between fitness change and absenteeism, a bidirectional relationship may exist between exposure and outcome. For example, it is possible that children who have higher absenteeism are more sedentary, particularly if they are ill or occupied in nonactive ways (eg, video-game playing, watching television). Domestic factors may also persist over time. In this sense, although this analysis lagged absenteeism to fitness, the temporality of exposure and outcome could be reversed. Future research should explore the directionality of fitness and absenteeism in more detail, in addition to the role of chronic conditions in this association.\n\n【43】In our study, systematic bias and differential measurement error are possible, given that the Fitnessgram data are not collected for research purposes. Data were not available on many student- and school-level factors, including self-esteem, drug and alcohol use, family structure, and individual household poverty (such as income or eligibility for free or reduced-price lunch). These factors may influence not only absenteeism but also motivation to perform well on fitness tests. Absence of this data makes it difficult to disentangle these relationships. Future work should research whether mental, social, or emotional health and peer or parent influence are antecedents to fitness on the hypothesized fitness–attendance causal pathway. This research may shed light on why some adolescents have fitness performance drop-offs and may garner particular attendance benefits from these interventions.\n\n【44】Although testing protocols are designed to promote consistency across administers, Fitnessgram testing sites may vary in their implementation of the protocol. However, in NYC the Fitnessgram is administered by physical education teachers who receive formal training on conducting the test, including manuals, video-based training, and site visits, as well as calibrated scales .\n\n【45】Fitness levels in US youths decline with increasing age at rates faster than in other nations. Diminished fitness is shown in longitudinal studies to be associated with lower academic performance, and cross-sectionally to be associated with higher absenteeism. We present evidence for a longitudinal inverse dose–response association between fitness and absenteeism in NYC middle school youths. Cumulative effects of consistent fitness improvements from 6th through 12th grades may shift a child from chronic absenteeism to regular attendance. Future research should examine the effectiveness of school-based fitness interventions to reduce absenteeism rates, particularly within subgroups that have fitness drop-offs in adolescence. Findings may inform policy mandating increases in school fitness time, including increased classroom-based physical activity and both stricter school physical education and recess policies.\n\n【46】Tables\n------\n\n【47】#####  Table 1. Demographic and Fitness-Change Characteristics of New York City Public Middle School Students (N = 349,381), 2006–2007 Through 2012–2013\n\n| Characteristic | n a,b (%) |\n| --- | --- |\n| **Sex** | **Sex** |\n| Male | 177,355  |\n| Female | 172,026  |\n| **Race/ethnicity** | **Race/ethnicity** |\n| Asian or Pacific Islander | 58,295  |\n| Hispanic | 134,453  |\n| Non-Hispanic black | 99,363  |\n| Non-Hispanic white | 55,857  |\n| **Language spoken at home** | **Language spoken at home** |\n| English | 197,727  |\n| Spanish | 86,052  |\n| Other language | 65,602  |\n| **Place of birth** | **Place of birth** |\n| United States | 289,160  |\n| Foreign country | 60,149  |\n| **Change in fitness c (all years)** | **Change in fitness c (all years)** |\n| \\>20% Decrease | 126,115  |\n| 10%–20% Decrease | 79,172  |\n| <10% Change | 253,161  |\n| 10%–20% Increase | 82,117  |\n| \\>20% Increase | 134,753  |\n| **Change in obesity status d (all years)** | **Change in obesity status d (all years)** |\n| Obese to not obese | 36,029  |\n| Consistently not obese | 504,762  |\n| Consistently obese | 119,235  |\n| Not obese to obese | 27,273  |\n| **School-area poverty e** | **School-area poverty e** |\n| Low poverty | 62,238  |\n| Medium poverty | 119,219  |\n| High poverty | 89,407  |\n| Very high poverty | 78,510  |\n| **School size** | **School size** |\n| Attending small schools (<400 students) | 59,856  |\n| Attending nonsmall schools (≥400 students) | 289,525  |\n\n【49】a  N for missing place of birth = 72; N for missing area poverty = 7; N for missing or having >1 race/ethnicity = 177.  \nb  Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively, across 624 schools.  \nc  Based on change in change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \nd  Obesity status was defined according to Centers for Disease Control and Prevention growth chart–derived norms for sex and age (in months), based on a historical reference population, and used to compute the body mass index (BMI) percentile for each child. Obesity was defined as having a BMI ≥95th percentile for youths in the same sex and age (in months) group.  \ne  Based on percentage of households in the school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] area poverty) drawing from the American Community Survey 2007–2012 .\n\n【50】#####  Table 2. Mean Days Absent per Year Across Student- and School-Level Demographic and Fitness-Change Characteristics in New York City Public Middle School Students (N = 349,381) a  , 2006–2007 Through 2012–2013\n\n| Characteristic | Student-Level b , Mean (SD) | School-Level c , Mean (SD) |\n| --- | --- | --- |\n| **Sex** | **Sex** | **Sex** |\n| Male | 11.0 (11.7) | 11.2 (11.5) |\n| Female | 10.1 (11.0) | 10.4 (10.8) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Asian or Pacific Islander | 5.5 (7.7) | 6.4 (8.3) |\n| Hispanic | 12.6 (12.9) | 13.3 (13.2) |\n| Non-Hispanic black | 12.3 (13.1) | 12.8 (13.3) |\n| Non-Hispanic white | 10.0 (9.7) | 10.7 (10.2) |\n| **Language spoken at home** | **Language spoken at home** | **Language spoken at home** |\n| English | 11.9 (12.1) | 12.0 (11.9) |\n| Spanish | 10.9 (11.1) | 11.0 (10.9) |\n| Other language | 6.0 (7.4) | 6.5 (7.5) |\n| **Place of birth** | **Place of birth** | **Place of birth** |\n| United States | 11.3 (12.1) | 11.7 (11.5) |\n| Foreign country | 11.1 (13.8) | 8.1 (8.8) |\n| **Change in fitness (all years) d** | **Change in fitness (all years) d** | **Change in fitness (all years) d** |\n| \\>20% Increase | 10.3 (11.2) | 11.0 (11.6) |\n| 10%–20% Increase | 10.3 (11.3) | 10.8 (11.5) |\n| <10% Change | 10.7 (11.9) | 11.8 (12.6) |\n| 10%–20% Decrease | 11.1 (12.2) | 11.6 (12.4) |\n| \\>20% Decrease | 11.9 (12.8) | 12.7 (13.2) |\n| **Grade e** | **Grade e** | **Grade e** |\n| Grade 6 | 10.2 (11.0) | 10.8 (11.1) |\n| Grade 7 | 10.9 (12.5) | 11.2 (12.2) |\n| Grade 8 | 13.1 (14.5) | 13.1 (13.6) |\n| **School-area poverty f** | **School-area poverty f** | **School-area poverty f** |\n| Low poverty | 8.5 (9.2) | 8.9 (9.3) |\n| Medium poverty | 9.5 (10.3) | 9.8 (10.2) |\n| High poverty | 11.1 (11.7) | 11.4 (11.6) |\n| Very high poverty | 13.1 (13.3) | 13.1 (12.9) |\n| **School size** | **School size** | **School size** |\n| Small schools (<400 students) | 12.0 (12.3) | 11.8 (11.9) |\n| Non-small schools (≥400 students) | 10.3 (11.1) | 11.8 (11.0) |\n\n【52】a  N for missing place of birth = 72; N for missing area poverty = 7; N for missing or having >1 race/ethnicity = 177.  \nb  Student-level columns do not account for school clustering.  \nc  School-level columns account for school clustering.  \nd  Based on change in change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \ne  Students in 6th, 7th, and 8th grades contributed 177,281, 220,769, and 186,135 student-years, respectively.  \nf  Based on percentage of households in the school zip code living below the federal poverty threshold (low \\[<10%\\], medium \\[10%–20%\\], high \\[>20%–30%\\], and very high \\[>30%\\] area poverty) drawing from the American Community Survey 2007–2012 .\n\n【53】#####  Table 3. Association of Fitness Change and Attendance in New York City Public Middle School Students a  , 2006–2007 Through 2012–2013\n\n| Fitness Change b | Unadjusted (Model 2) c , IRR d (95% CI) | Adjusted (Model 3) c,e , IRR d (95% CI) |\n| --- | --- | --- |\n| \\>20% Increase | 1.13 (1.09–1.18) | 1.12 (1.07–1.17) |\n| 10%–20% Increase | 1.08 (1.03–1.14) | 1.06 (1.01–1.11) |\n| <10% Change | 1.06 (1.02–1.09) | 1.03 (0.989–1.07) |\n| 10%–20% Decrease | 1.02 (0.97–1.06) | 1.00 (0.96–1.05) |\n| \\>20% Decrease | 1 \\[Reference\\] | 1 \\[Reference\\] |\n\n【55】Abbreviations: CI, confidence interval; IRR, incidence rate ratio.  \na  N = 349,381 students in 6th, 7th, and 8th grades; 675,318 observations across 624 schools.  \nb  Change in fitness composite percentile scores based on Progressive Aerobic Cardiovascular Endurance Run (PACER) Push-up and Curl-up Fitnessgram tests from the year prior.  \nc  Based on 3-level longitudinal negative binomial mixed models.  \nd  All estimates, _P_ < .001.  \ne  Adjusted for sex, race/ethnicity, change in obesity status from the year prior, place of birth (United States or foreign country), school size, and school-area poverty, and including interactions grade\\*race/ethnicity, grade\\*sex, grade\\*place of birth, and school-area poverty\\*race/ethnicity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "affca9e4-e644-43e8-a359-3bd9ff9a4dc2", "title": "SARS-CoV-2 Vaccine Breakthrough by Omicron and Delta Variants, New York, USA", "text": "【0】SARS-CoV-2 Vaccine Breakthrough by Omicron and Delta Variants, New York, USA\nAs of August 10, 2022, the SARS-CoV-2 pandemic had claimed >6.4 million human lives globally, >1 million in the United States, and >70,000 in New York state . Virus evolution and adaptation have been observed in persistently infected immunocompromised persons  and animal reservoirs , leading to the potential for new, highly adapted variants.\n\n【1】Novel variants of SARS-CoV-2 have shown increased rates of transmission and immune evasion . In particular, Omicron has evolved a suite of unique mutations, which have greatly increased its infectiousness , increased its ability to evade current vaccines , and decreased the effectiveness of convalescent plasma transfusions and monoclonal antibody treatments . To a lesser degree, the Delta variant showed some of these same patterns of increased infectiousness  and potential for immune evasion compared with earlier strains that preceded Delta .\n\n【2】Prior literature has also shown differences in vaccine effectiveness for SARS-CoV-2 lineages associated with variation in vaccine type, time since vaccination, and patient age. Before emergence of the Delta and Omicron variants, data showed reduced neutralizing antibody protection for the Janssen vaccine  compared with the Pfizer  and Moderna  vaccines  and slightly stronger protection for Moderna compared with Pfizer vaccines . An effect of time since vaccination has been demonstrated for the Delta variant . Younger persons were found to be more likely to be infected with Omicron .\n\n【3】To test the associations between vaccination status, vaccine type, and time since vaccination with lineage identity during the emergence of new variants of SARS-CoV-2, we conducted a matched case–control study. We performed analyses for the emergence of the Omicron and Delta variants in New York, USA. The study was waived by the New York State Department of Health (NYSDOH) Institutional Review Board for Human Subjects Research review.\n\n【4】### Methods\n\n【5】##### Data Analysis\n\n【6】##### Omicron Emergence Analysis\n\n【7】We analyzed emergence of the SARS-CoV-2 Omicron variant from November 28, 2021, through January 24, 2022 . We matched persons infected with Omicron (case-patients) to persons infected with any other virus lineage (controls). Case-patients (n = 1,439) included infection with B.1.1.529 and all BA sublineages (at the time of the analysis, none were classified as BA.2 through BA.5). Controls (n = 728) were persons infected with all other SARS-CoV-2 lineages circulating during the period of Omicron emergence (all sequenced control samples in the matched dataset were Delta variant, B.1.617.2 or AY sublineages). We defined the start of the Omicron emergence period as the first detection in the genomic surveillance dataset (although Omicron was present in the state before that date). The emergence period ended when the last non-Omicron case was detected in the surveillance dataset. One additional case of infection with Delta was identified >14 days after the last date in the surveillance dataset but was excluded because the sensitivity analysis indicated that it would not substantively change the analysis results. We matched case-patients to controls on the basis of specimen collection date (± 6 days), location (using New York state economic regions ), patient age, and patient sex. We matched age according to age groups: 0–4, 5–11, 12–17, 18–29, 30–49, 50–69, 70–89, and \\> 90 years. If an exact match could not be found, we allowed mismatches for sex. We used 1-to-1 matching, without replacement (i.e. each case-patient was matched to a unique control). We performed matching in 2 stages. In the first stage, we considered all possible matches for each case-patient. To maximize the sample size, we then sorted case-patients such that the case-patients with the fewest possible matches would be matched to controls first. To estimate odds ratios (ORs) and 95% CIs, we performed 3 sets of conditional logistic regressions.\n\n【8】In analysis 1, we included vaccinated and unvaccinated persons. Key variables tested were vaccination status (binary: yes/no), booster status (yes/no), vaccine type (none, Pfizer, Moderna, Janssen), time since last vaccination or booster (3 factor levels: unvaccinated, vaccinated <90 days, vaccinated \\> 90 days). We explored time since completion of initial vaccination and time since booster but found these factors were less predictive and overlapped strongly with the combined time since last vaccination or booster variable and therefore excluded them.\n\n【9】In analysis 2, we examined the association between patient age and virus lineage and therefore removed age as a matching criterion. We performed a conditional logistic regression using age, other main variables for context, and interactions. For this analysis, we did not perform sorting before matching. We examined age in 2 ways: with each age group treated as a factor and with each age group treated as a continuous predictor. Model exploration revealed that a mixture of categorical and continuous predictors best described the underlying data structure .\n\n【10】In analysis 3, we again matched case-patients to controls on the basis of age, but we excluded unvaccinated persons to allow time since last dose (vaccination series or booster) to be treated as continuous variables. Unvaccinated persons could not be included in this analysis because assigning them NA (not applicable) would cause these values to be excluded, and 0 would be an unrealistic value.\n\n【11】We tested leverage by removing each case–control pair sequentially, refitting the model and noting the change in the OR. We selected models by using Akaike information criterion (AIC) scores . Models with lower AIC scores have more model support, and models with ΔAIC >2 are generally considered less likely models. Because a more complicated nested model can be within ΔAIC of 2, nested models were required to be within 2 × no. model parameters to be considered tied . Of note, AIC provides a relative ranking of models but provides no information on the absolute fit of the model. We examined the fit of each model by considering its statistical significance and the OR estimates. When test results were not significant, we examined the magnitude of the OR. More research was deemed necessary if the estimated OR was large enough to be a public health concern but 95% CIs included 1.\n\n【12】We performed all analyses in R 4.1.2  by using the package survival for conditional logistic regressions code  . We created the New York state map in ArcGIS 10.6  by using a 2017 Tiger Shapefile from the US Census Bureau  and Admin 1 States, provinces 50-m cultural vector shapefile from Natural Earth Data (as of March 18, 2022) .\n\n【13】##### **Delta Emergence Analysis**\n\n【14】We analyzed emergence of the SARS-CoV-2 Delta variant during March 19, 2021–August 15, 2021 . The Delta analyses followed the same methods used for the Omicron analyses but with focal virus lineages (603 case-patients) including B.1.617.2 and all AY sublineages. Nonfocal virus lineages (1,816 controls) were all other lineages circulating during the period of Delta emergence (62% B.1.1.7 and Q.4 Alpha, 20% B.1.526 Iota, 3.5% P.1.X Gamma, 1% B.1.351.X Beta); none of the other non–variant of concern strains (13.7% combined) exceeded 5%. We excluded booster-associated variables because booster doses were not available . We omitted the vaccinated-only analysis because of low statistical power (n = 12 pairs).\n\n【15】##### Power Analysis\n\n【16】Statistical power for conditional logistic regression is nonlinear and depends on estimated probabilities. Although we used multiple conditional logistic regression for the analyses described above, to make the power analyses easier to set up and interpret, we calculated statistical power for univariate logistic regression by using the WebPower package  as a simplifying assumption. We examined statistical power to detect an OR of 2 with a sample size of 110 for a range of probability values (0.1–0.9 for the upper probability); we adjusted lower probability to give an OR of 2. We then used the upper probability value with the highest power (0.7) to assess statistical power for ORs of 2, 3, and 4 for sample sizes of 50–350 by increments of 50.\n\n【17】##### Data Sources\n\n【18】Respiratory swab specimens that were positive for SARS-CoV-2 by real-time reverse transcription PCR were sent from clinical laboratories across the state for whole-genome sequencing at the NYSDOH Wadsworth Center as part of an enhanced genomic surveillance program. Samples were selected for sequencing on the basis of cycle threshold value and region of patient residence; the goal was full geographic coverage across the state. Sample selection criteria did not change over the course of the study period. We matched samples to demographics in the Communicable Disease Electronic Surveillance System and vaccination records in the New York State Immunization Information System. For persons from whom multiple samples were collected, we included only the earliest collected sample with genome available.\n\n【19】Vaccination status for each person was based on dates of sample collection and administration of vaccines. A person was considered unvaccinated if the sample was collected before any vaccination, vaccinated if the sample was collected >14 days after completion of vaccination (first dose of Janssen, second dose of Pfizer or Moderna vaccine), and boosted if the sample was collected any time after receiving a booster of any vaccine type. We removed from the study persons who were partially vaccinated (sample collected between initial dose and 14 days after vaccination completion, n = 261 \\[90 with Moderna and 171 with Pfizer vaccine\\]) and persons who received a greater number of vaccinations than normal. This study does not apply to persons who received a third dose as part of their vaccination series (e.g. potentially immunocompromised persons); these persons were removed from the dataset because of different vaccination history and low sample sizes (58 persons who received a third dose <135 days after their second dose were removed).\n\n【20】##### Sequencing Methods\n\n【21】We performed whole-genome amplicon sequencing of SARS-CoV-2 by using a modified version of the Illumina ARTIC protocol  with ARTIC V3 primers in the Applied Genomics Technology Core at the Wadsworth Center, as previously described , and amplified later samples with ARTIC V4 primers. We sequenced samples with particularly low virus titers by using AmpliSeq chemistry on the Ion Torrent S5XL sequencer, as previously described .\n\n【22】 In that chart, the first column shows the GISAID accession number, and the subsequent columns indicate whether the identification number was used in the respective analyses. Data are coded such that –1 indicates records that were removed before analysis, 0 indicates records that met the basic overall study criteria but were not matched for a particular analysis, and 1 indicates that the record was included in the analysis.\n\n【23】### Results\n\n【24】##### Omicron Emergence\n\n【25】In analysis 1, >80% of 272 case-patient/control pairs were 18–69 years of age; most were from the Capital and Mid-Hudson regions . Among controls, 8% had received a booster, and among case-patients, 22% had received a booster. Among controls, 56.6% were unvaccinated; among case-patients, 30% were unvaccinated . Sample sizes were 177 for Pfizer, 109 for Moderna, and 22 for Janssen vaccine recipients. The variables most associated with an Omicron lineage identity were vaccination (OR 3.1, 95% CI 2.0–4.9; p<0.001) and booster status (OR 6.7, 95% CI 3.4–13.0; p <0.001) .\n\n【26】In analysis 2 (309 pairs), when patient age was removed as a matching criterion, younger age was also predictive of an Omicron infection; log-odds of infection with Omicron generally decreased as age increased (OR 0.962, 95% CI 0.950–0.974) . Significant patterns beyond this log-linear age effect were found for persons in 2 age groups. log-odds of infection with Omicron were lower for persons 0–4 years of age than predicted by a log-linear age effect alone  and higher for persons 18–29 years of age than predicted by a log-linear age term alone; risk was highest for those 18–29 years of age . OR estimates for vaccination status (OR 4.8, 95% CI 2.8–8.1) and booster status (OR 38.5, 95% CI 15.9–93.2) were higher than in the analysis that used age as a matching criterion .\n\n【27】In analysis 3 (vaccinated-only persons, 129 pairs), the probability of infection with Omicron decreased with an increased number of days after the last vaccine dose (OR 0.996, 95% CI 0.993–0.999) . Vaccine type was also included in the top statistical models  and the trend toward reduced odds of Omicron infection after vaccination with the Janssen vaccine was borderline significant (OR 0.351, 95% CI 0.132–0.935, relative to Pfizer vaccine; OR 0.388, 95% CI 0.149–1.009, relative to any mRNA vaccine) .\n\n【28】##### Delta Emergence\n\n【29】In analysis 1 (55 pairs), 75% were 18–69 years of age; 89% of case-patients/controls were from the Finger Lakes, Long Island, and the Mid-Hudson regions . A total of 74.5% of controls and 61.8% of case-patients were unvaccinated . Vaccine type, time from last vaccination, and an interaction of the 2 were not significantly associated with an increased likelihood of infection with Delta than any other virus lineage in the fully matched conditional logistic regression . Vaccination status was the top model (OR 2.4, 95% CI 0.8–6.8; p = 0.08). Vaccine type had no significant effect (p = 0.12), but estimated ORs were 2.9 (95% CI 0.9–8.9) for Pfizer, 0.38 (95% CI 0.04–4.2) for Moderna, and 2.0 (95% CI 0.17–23.6) for Janssen.\n\n【30】The power analysis showed that a sample size of 110 (55 pairs) would have a 15%–45% chance of obtaining a significant result for an OR of 2 under the simulated probability distributions. A sample size of \\> 255 would be needed to have \\> 80% power for an OR of 2. A sample size of 110 could have < 78% power to detect an OR of 3 and 93% power to detect an OR of 4. A sample size of 24 could detect an OR of 22 with 80% power but would only have 36% power to detect an OR of 4.\n\n【31】When case-patients and controls were no longer matched on the basis of age (66 pairs), vaccine type was the top model , suggesting that odds of being infected with Delta rather than any other virus lineage increased by a factor of 7.3 (2.0–26.7) for those receiving the Pfizer vaccine relative to unvaccinated persons. Effects for Moderna (2.0, 95% CI 0.25–17.1) and Janssen (0.46, 95% CI 0.04–4.76) vaccines were substantial but not individually significant.\n\n【32】### Discussion\n\n【33】Our exploration of vaccine breakthrough, vaccination status, and time since vaccination in this matched case–control study adds to the body of evidence supporting immune escape of SARS-CoV-2. Some results may seem counterintuitive because of the study design. For example, although a booster increases protection against infection with Omicron compared with absence of a booster , history of a booster was associated with Omicron (the emergent strain) and not Delta (the established strain) infection. This finding is consistent with evidence that suggests that having a booster is less effective for preventing infection with Omicron than with Delta . Similarly, vaccine effectiveness has been shown to wane with time ; therefore, we hypothesized that increased time after vaccination would decrease the odds of being infected with the emergent strain.\n\n【34】Our analysis of New York state genomic surveillance data yielded results that are consistent with previous research showing an increased probability of breakthrough for Omicron compared with other variants for both vaccinated and boosted persons . In a similar study in Connecticut, USA, comparing odds of infection with Omicron versus Delta , an OR of ≈2 (95% CI 1.5–3.7 or 1.5–2.2, depending on time after vaccination) was found for vaccinated persons and ≈3 (95% CI 1.8–4.9) for boosted persons. These estimates are lower than the estimates from our study of 3.1 (95% CI 2.0–4.9) for vaccinated persons and 6.7 (95% CI 3.4–13.0) for unvaccinated persons, but the 95% CIs overlap between the 2 studies. A strong pattern of the emergent strain shows increased ability for vaccine breakthrough compared with other strains circulating at the time. Studies of prior variants of concern have found significant vaccine breakthrough in emergent variants. For example, Kustin et al. found that vaccine breakthrough for Alpha (B.1.1.7) was more likely compared with prior strains . Similarly, Tartof et al. found evidence for increased rates of vaccine breakthrough by Delta (B.1.617.2), although waning vaccine immunity was also a factor in that study . In addition, Rosenberg et al. showed increased breakthrough during the Delta emergence period and suggested that this effect was independent of waning immunity .\n\n【35】When we restricted the analysis to vaccinated persons only, time after vaccination was a statistically significant factor; probability of Omicron infection decreased with increased time after vaccination. The time-after-vaccination variable combined persons who had recently received a booster with those who had recently completed their primary series. Adding a variable to indicate booster status did not improve the model fit . Of note, most persons in this study were >3 months past completion of their initial vaccination series. Boosters were more recent, and therefore vaccination status and booster status probably encoded much of the same information as a time-after-last-dose variable. No time-after-vaccination effect was detected if the data were coarsely divided into persons who had and had not received boosters, suggesting that more examination of this variable may be necessary. This variable was not found among the top models in the Delta emergence analysis.\n\n【36】Younger persons were more likely to be infected with Omicron than with Delta during the Omicron emergence period, although the data in this study cannot be used to distinguish a physiological basis from a behavioral basis for these age effects. Kahn et al. found Delta and Omicron infection be equally distributed by age among unvaccinated persons but to shift strongly toward younger persons among vaccinated persons ; however, Accorsi et al. found elevated rates of Omicron infection among vaccinated and unvaccinated persons . It is possible that the age group effects are the result of a greater degree of socialization and other behavioral risk factors among persons 18–29 years of age. In 2020, college campus re-openings were associated with increased transmission of SARS-CoV-2 . Because Omicron infections can break through vaccinations, college campuses may have increased the likelihood of persons in this age group being infected with SARS-CoV-2 . The age group effect for preschool children (0–4 years of age) may represent a reduced level of socialization for this group. This effect, although included in the top model identified by the information theoretic approach here, was not statistically significant, so it also may be an artifact of low sample sizes for this age group. Other research has found that vaccines were not equally effective among age groups . Vaccine effectiveness in New York was very low for persons 5–11 years of age, who received a lower dose (10 μg) of the Pfizer vaccine than for vaccinated persons \\> 12 years of age who received a 30-μg dose . However, the log-linear age effect detected here was not driven by children <12 years of age. When children <12 years of age were removed from the analysis, the estimated OR changed from 0.962 to 0.957 (95% CI 0.944–0.971), suggesting that the magnitude of the effect is greater when young children were removed from the analysis. Larger estimates for vaccination status and booster status were also greater when children <12 years of age were removed from the analysis (vaccination status OR 5.4, 95% CI 3.1–9.7; vaccination plus booster status OR 43.0, 95% CI 17.1–108.5). Vaccination rates and booster rates changed substantially during the study periods as well , but any resulting biases were probably controlled for by the case–control study design.\n\n【37】Sample sizes were generally too small to detect robust vaccine type effects. The Janssen vaccine showed borderline significantly reduced OR for infection with Omicron relative to the Pfizer vaccine in 1 statistical model . This result would be consistent with improved performance against Omicron infection or with worse performance of this vaccine against Delta infection, as has been observed . Otherwise, OR estimates showed the potential for substantial differences, but overlapping 95% CIs prevent drawing robust conclusions .\n\n【38】Statistical power was constrained by the limited emergence periods and the relatively small percentage of viruses from COVID-19 case-patients that were sequenced. For Delta, the emergence period occurred during a time of reduced sequencing, because of low overall incidence during the summer of 2021, when Delta displaced previous strains . For Omicron, a larger sequencing effort was made, but the emergence period was considerably shorter because of the rapid dominance of the Omicron variant . Sample sizes could potentially be increased by expanding the regional scope of the study or incorporating sequencing results from other research laboratories.\n\n【39】We used only 1 matched set for each analysis. However, because case-patients were randomly matched to controls, other matches were possible. This limitation could be overcome by assessing significance with Monte Carlo simulation over the range of possible matches. That said, visual examination of leverage plots based on removing a single pair suggested that the results were generally unlikely to change with the removal of any single data point. The exception is the Delta analysis, in which a change of 1–2 data points would change the overall statistical significance of the results  without much change in the estimated OR.\n\n【40】In conclusion, this analysis of the emergence of the Omicron and Delta variants in New York, USA, based on sequenced virus identity broadly supports the results of prior studies . Vaccines offered less protection against Omicron infection, thereby increasing the number of potential hosts for emerging variants.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9b827e35-0dd3-425d-a1dc-2b442d3a230f", "title": "Geographic Expansion of Sporotrichosis, Brazil", "text": "【0】Geographic Expansion of Sporotrichosis, Brazil\nHigh rates of human cases of sporotrichosis caused by _Sporothrix brasiliensis_ transmitted by cats have been reported in Brazil since 1998 . The main referral center for the treatment of this mycotic disease, Oswaldo Cruz Foundation (Fiocruz) in Rio de Janeiro, recorded ≈5,000 human cases during 1998–2015  and 5,113 feline cases during 1998–2018 . However, these numbers only represent cases diagnosed at 1 institution, and actual incidence rates likely are higher.\n\n【1】During 1998–2017, Brazil experienced a geographic expansion of sporotrichosis. The southeast region had the largest occurrence of human and animal cases , but outbreaks and case reports of feline sporotrichosis have been described from other regions  . In regions only reporting feline cases, zoonotic transmission probably is going unnoticed.\n\n【2】Zoonotic sporotrichosis also has been reported in the United States, India, Malaysia, Argentina, Mexico, and Panama . In Malaysia, isolates from cases caused by _S. schenckii_ sensu stricto  have included clonal reproduction, which could indicate ongoing emergence of a genotype that is adapting to the feline host , similar to what was reported for _S. brasiliensis_ in Brazil . Also, the occurrence of zoonotic sporotrichosis due to _S. brasiliensis_ in Argentina is alarming because it points to a potential transboundary expansion of this virulent species to other regions in Latin America. Despite rules implemented for pet travel, poor control over road transportation might contribute to the spread of sporotrichosis in Brazil and could pose a risk for spread beyond its borders .\n\n【3】Fungal infections generally are neglected , and public health policies and strategic plans for prioritizing such infections are lacking. Inadequate surveillance of fungal infections leads to unnoticed emergence, such as seen with zoonotic sporotrichosis.\n\n【4】The rise and spread of sporotrichosis cases in Brazil were overlooked for several years, making a previously rare disease frequent and uncontrolled in many regions. Continuing socioeconomic and environmental difficulties, such as economic and social inequality, poverty, unemployment, urban agglomeration, and poor basic sanitation, coupled with scarce and inadequate health services, are fueling this expansion. In Rio de Janeiro, despite the high number of cases and the strain sporotrichosis puts on public health services, an animal sporotrichosis control program that included free diagnosis and treatment was not implemented until 16 years after the epidemic began. Nevertheless, given the chaotic situation in this region, the control measures used were insufficient. Even with the spread of the disease to other states in Brazil, compulsory notification is performed by only a few specific municipalities.\n\n【5】The absence of a comprehensive feline sporotrichosis control program in Brazil, the multifactorial difficulty in managing sick cats, and the lack of knowledge of sporotrichosis control measures by most of the population have contributed to the growing number of human and animal cases. A One Health approach is key for effective surveillance and successful control. Coordinated actions among veterinarians, laboratory practitioners, surveillance authorities, and other healthcare workers will ensure broader investigations and promote prevention, detection, and assistance for human and animal cases.\n\n【6】Early diagnosis of feline sporotrichosis is essential to guarantee appropriate prevention for owners, especially those at higher risk for infection, such as persons with immunosuppression. In addition, prompt treatment in felines can rapidly reduce the fungal load and risk for transmission of _Sporothrix_ by cats . Thus, the availability of itraconazole, the first-line treatment for humans and animals, is essential in health units of affected areas.\n\n【7】The pattern of feline sporotrichosis appears to be changing in the world, with new cases of zoonotic transmission by other _Sporothrix_ species appearing . Health authorities from neighboring countries should be aware of the signs and symptoms of disease to identify cases early and rapidly implement prevention and control measures. Atypical cases and treatment failures emphasize the need for studies focusing on the detection of potential antifungal resistance and alternative therapeutic strategies. The emergence of new species or changes in the behavior of known species also should be assessed, to identify variations in the ecoepidemiology and in host–pathogen interactions.\n\n【8】If health authorities in Rio de Janeiro had taken measures to control and prevent sporotrichosis in the feline population at the first appearance of human cases, the current scenario could be different and likely would have cost less to the health system in the long term. Considering the remarkable spread of sporotrichosis in the past decade, effective public health actions, including free medication and service for animals, are urgently needed to prevent additional cases in affected areas. We encourage a One Health approach to curb further expansion of sporotrichosis in humans and animals in Brazil.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e1d34538-25ba-4893-a341-051b652a11da", "title": "Commute Times, Food Retail Gaps, and Body Mass Index in North Carolina Counties", "text": "【0】Commute Times, Food Retail Gaps, and Body Mass Index in North Carolina Counties\nAbstract\n--------\n\n【1】**Introduction**The prevalence of obesity is higher in rural than in urban areas of the United States, for reasons that are not well understood. We examined correlations between percentage of rural residents, commute times, food retail gap per capita, and body mass index (BMI) among North Carolina residents. **Methods**We used 2000 census data to determine each county’s percentage of rural residents and 1990 and 2000 census data to obtain mean county-level commute times. We obtained county-level food retail gap per capita, defined as the difference between county-level food demand and county-level food sales in 2008, from the North Carolina Department of Commerce, and BMI data from the 2007 North Carolina Behavioral Risk Factor Surveillance System. To examine county-level associations between BMI and percentage of rural residents, commute times, and food retail gap per capita, we used Pearson correlation coefficients. To examine cross-sectional associations between individual-level BMI (n = 9,375) and county-level commute times and food retail gap per capita, we used multilevel regression models. **Results**The percentage of rural residents was positively correlated with commute times, food retail gaps, and county-level BMI. Individual-level BMI was positively associated with county-level commute times and food retail gaps. **Conclusions**Longer commute times and greater retail gaps may contribute to the rural obesity disparity. Future research should examine these relationships longitudinally and should test community-level obesity prevention strategies.  \n\n【2】Introduction\n------------\n\n【3】In the United States, the prevalence of obesity is higher in rural than in urban populations . Area-level factors that contribute to this disparity are not well understood, but one underlying mechanism may be the food environment. Obesity prevalence is lower in census tracts containing a supermarket , and rural areas have few supermarkets, which generally have a healthier mix of low-cost food items compared with local convenience stores . Accessibility to healthy food is also difficult in rural areas because convenience stores are more common than supermarkets . Rural residents may regularly travel to urban areas in neighboring counties to shop for food because of convenience along the route to work, better prices, wider selection, or one-stop shopping offered at discount “supercenters” (eg, Walmart) . This pattern of food shopping among rural residents may create a retail shortfall or “gap” for food venues in rural areas, causing rural food venues to have a decreased share of the market. A large food retail gap may exacerbate rural food deserts , or areas where residents have limited access to affordable, healthy food , when smaller food venues in underserved areas close as business is lost to nearby discount supercenters . Rural residents’ prolonged travel time to larger supermarkets or supercenters not only increases the retail gap in the rural county but decreases the frequency of food shopping. In turn, diet quality may decrease as rural residents purchase less fresh produce and more processed foods . Another hypothesized mechanism underlying the rural-urban obesity disparity is that rural residents may spend more time traveling to work or to obtain goods and services than do their urban counterparts. Obesity is associated with urban sprawl , time spent in cars , and vehicle miles traveled per day . One Los Angeles-based study found that distance traveled to the nearest supermarket was positively associated with higher body mass index (BMI) . To our knowledge, no studies have examined the associations between distance to food shopping location, commute times, and BMI among rural and urban residents. To better understand associations between area-level factors and obesity, we conducted ecologic analyses of associations between the percentage of rural-dwelling residents, commute times, food retail gaps, and BMI for all 100 North Carolina counties. We hypothesized that 1) the percentage of rural residents per county is positively correlated with commute time and food retail gap per capita, 2) county-level commute time is positively correlated with food retail gap per capita, and 3) both commute time and food retail gap per capita are positively correlated with county-level mean BMI. In separate individual-level, contextual analyses, we examined individual-level BMI as the dependent variable and county-level commute time and food retail gap per capita as independent variables. We hypothesized that longer commute times and greater food retail gaps per capita would be positively correlated with individual-level BMI.  \n\n【4】Methods\n-------\n\n【5】### Percentage of rural residents\n\n【6】We calculated the percentage of rural residents for all North Carolina counties by dividing the number of county residents who lived in a rural area according to 2000 census criteria  by the county population. The percentage of rural residents ranged from 4% to 100%. \n\n【7】### Commute times\n\n【8】We generated reports for county-level commute times for 1990 and 2000 from US census data from the North Carolina Department of Commerce Economic Development Intelligence System. Census data were derived from answers to the census long-form questionnaire. Respondents who worked outside the home estimated the number of minutes it took to get from home to work each day, and commute time was derived by dividing the total number of minutes by the number of workers aged 16 years or older who did not work at home. We examined associations by using the 1990 and 2000 commute times and the difference in commute times between 1990 and 2000. The difference in 1990 and 2000 commute times describes broad shifts in county-level commuting over 10 years. \n\n【9】### Food retail gap\n\n【10】We defined the food retail gap as the difference between county-level demand for food and county-level sales of food. We obtained the food retail gap for each North Carolina county from the North Carolina Department of Commerce Economic Development Intelligence System. The Environmental Systems Research Institute (ESRI) calculated retail gaps by subtracting county-level retail sales (supply) of products for a particular industry category in 2008 from county-level demand for products in that industry category in 2008. ESRI estimated demand using data on consumer expenditures from the Bureau of Labor Statistics and InfoUSA, a commercial database marketing system. ESRI calculates the food retail gap for North American Industry Classification System (NAICS) codes 445 (representing food and beverage stores) and 722 (representing the food services and drinking places) separately. For these analyses, we used food retail gaps calculated from individual and combined NAICS codes 445 and 722. Venues included in the food and beverage stores subsector (NAICS code 445) sell food and beverages from fixed point-of-sale locations, such as supermarkets, grocery stores, convenience stores, meat markets, produce markets, and specialty food stores. Venues included in the food services and drinking places subsector (NAICS code 722) prepare meals, snacks, and beverages to customer order for consumption on and off the premises, such as full-service restaurants, limited-service eating places (fast-food restaurants), special food services, and drinking places. To control for population density, we calculated the food retail gap per capita by dividing the ESRI-estimated food retail gap by the 2007 county population estimate provided by the US census. A negative retail gap indicated that county-level sales were greater than county-level demand; a positive retail gap indicated that county demand was greater than county sales. For example, if County X has 1 chain supermarket and neighboring County Y has a large discount supercenter, residents of County X may begin grocery shopping at the supercenter, creating a positive food retail gap in County X and a negative food retail gap in County Y as residents’ food dollars are spent in the neighboring county. This could result in closing of the 1 chain supermarket in County X, making travel to the discount supercenter a necessity for obtaining groceries. \n\n【11】### Body mass index\n\n【12】We estimated county-level mean BMI using self-reported height and weight for respondents to the North Carolina Behavioral Risk Factor Surveillance System (BRFSS); responses were aggregated over 5 years . The 5-year aggregate provided an adequate number of responses for reliable estimates for counties with low population densities (single-year estimates for rural counties are unstable). We calculated mean weighted BMI using SUDAAN version 10.1 (Research Triangle Institute, Research Triangle Park, North Carolina), which accounts for BRFSS oversampling of minorities. The mean (standard deviation) county-level BMI was 27.7 (0.85) kg/m 2 . The median (interquartile range) was 27.6 (25.9-30.1) kg/m 2 . We conducted individual-level, contextual analyses using data from the 2007 North Carolina BRFSS for respondents aged 18 to 65 years with valid county identifiers. Because of confidentiality concerns, BRFSS does not provide county identifiers for residents of counties with fewer than 50 respondents. We excluded those counties. The individual-level sample consisted of 9,375 respondents from 64 counties. The mean population of the 64 counties included was 123,968, and the mean population of the 36 counties excluded was 25,322. The individual-level mean (SD) BMI was 28.1 (6.4) kg/m 2 . \n\n【13】### County-level census data\n\n【14】To control for economic interdependence of adjacent counties, we examined the Rural to Urban Continuum Codes (RUCC) as a covariate. The RUCC is a 9-level ordinal scale used by the Economic Research Service to classify counties according to adjacency to metropolitan areas . We included a diversity index as a potential covariate in analyses because of associations between racial/ethnic mix and availability of food venues (eg, supermarkets , fast-food restaurants ) and to account for North Carolina counties’ varied race/ethnicity distributions . The diversity index represents the percentage of times 2 randomly selected people in each county would differ by race/ethnicity . The index is calculated by squaring the proportions of residents in each racial/ethnic group, summing the squares, and subtracting the result from 1. We determined both the county-level diversity index and the percentage of residents who lived below the poverty level using 2000 census data. We calculated the percentage of residents who lived below the poverty level by dividing the number of residents below the poverty level in 1999 by the estimated 1999 county population. North Carolina is divided into 3 regions (Coastal Plain/Eastern, Appalachian Mountain/Western, and Piedmont Plateau) with distinct demographic and socioeconomic characteristics. Thus, we also examined the variable “region” as a potential covariate. \n\n【15】### Statistical analyses\n\n【16】For county-level ecological analyses, we used SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina) to calculate correlation coefficients for percentage of rural residents, food retail gap per capita, commute time, and BMI for all 100 North Carolina counties. We used backward selection to construct linear regression models to examine the associations among county-level independent variables of commute times and food retail gap per capita, using county-level mean BMI as the dependent variable. Percentage of rural residents, diversity index, percentage below poverty, and region were potential covariates and were eliminated from the model in successive steps if the _P_ value for the parameter estimate was .05 or higher. We examined the potential multicollinearity among covariates by computing their corresponding tolerance values. The tolerance is the proportion of variance in a given independent variable that is not explained by all of the other covariates; we found a tolerance value for all of greater than 0.1, which has been widely used as the threshold for multicollinearity in linear regressions . For individual-level, contextual analyses, we constructed multilevel linear regression models; the dependent variable was individual-level BMI from 2007 BRFSS respondents (n = 9,375). County-level independent variables were food retail gap per capita, commute time in 2000, and difference in commute times between 2000 and 1990. Sex, age, race/ethnicity, and education were individual-level covariates, and the RUCC was a county-level covariate. Region was added as a third level. Multilevel regression analyses allowed us to assess associations between individual-level BMI and area-level factors, accounting for the fact that people who reside in the same county are not independent observations . We examined the association between individual-level BMI and the 5 county-level variables of interest (commute time in 2000, difference between 1990 and 2000 commute times, retail gap per capita for NAICS code 445 \\[food and beverage stores\\], retail gap per capita for NAICS code 722 \\[restaurants and drinking places\\], and combined retail gap per capita) in separate models. The first 3 models to examine the association between BMI and county-level variables of interest were 2-level random intercept models. Model 4 included additional regional dummy variables to account for fixed effects from region. We used SAS version 9.2 for individual-level, contextual analyses, with estimates weighted to adjust for BRFSS oversampling.  \n\n【17】Results\n-------\n\n【18】Summary statistics for the individual-level data among 2007 BRFSS respondents by region are reported in Table 1 . \n\n【19】### County-level analyses\n\n【20】Percentage of rural residents was significantly correlated with both the commute times in 1990 and 2000 and the difference in commute times between the 2 years, food retail gap per capita for restaurants and drinking places, overall food retail gap per capita, and BMI . We found significant positive correlations between commute time and retail gap per capita . There were significant positive correlations between total food retail gap per capita and BMI and between the difference in commute times from 1990 to 2000 and BMI. In linear regression analyses adjusted for county-level diversity index and the percentage of residents below the poverty level, a positive association was found between commute time in 2000 and BMI (parameter estimate, 5.24; standard error, 1.86; _P_ \\= .006). We also found a significant positive association between food retail gap per capita and BMI when controlling for region and population percentage below poverty (parameter estimate, 0.024; standard error, 0.006; _P_ < .001). In linear regression models with county-level mean BMI as the dependent variable and difference in commute times from 1990 to 2000 and retail gap per capita as independent variables, the most parsimonious model included the covariates population percentage below poverty and regional fixed effects and explained 43% of variance in county-level BMI. When 2000 commute time and food retail gap per capita were included as independent variables, controlling for diversity index and percentage below the poverty level, the model explained 40% of variance in county-level BMI. \n\n【21】### Individual-level analyses\n\n【22】The point estimates for each of the county-level variables of interest (commute time and retail gap per capita) are presented for 4 model specifications . In Model 1, we did not include any additional covariates. Individual covariates were added in models 2 and 3. In model 4, regional fixed effects were added. All 5 measures of county-level commute time and food retail gap per capita were positively associated with individual-level BMI. These effects were significant in the unadjusted model (model 1), and the significance remained when individual-level and regional covariates were included in models 2, 3, and 4, with the exception of average commute time increase in model 4. When 2000 commute time and retail gap per capita were both included in the same model with individual-level and regional covariates, the parameter estimates for the county-level variables of interest were no longer significant.  \n\n【23】Discussion\n----------\n\n【24】Our results demonstrate a positive correlation between percentage of rural residents and 1) commute times and 2) food retail gap per capita, suggesting that counties with a higher percentage of rural residents have longer commute times and greater retail shortfalls, and thus residents may generally spend food dollars outside their county of residence. Previous studies have found positive associations between BMI and travel distance to grocery stores  and time spent in cars . We found significant cross-sectional correlations between individual-level and county-level BMI and 1) commute times and 2) food retail gap per capita, but significance did not remain when both were included in the individual-level model. This attenuation could be due to model over-adjustment if commute time and retail gap are both on the causal pathway explaining the relationship between rural residence and BMI. These analyses support strategies presented in _Recommended Community Strategies and Measurements to Prevent Obesity in the United States_  to improve geographic availability of supermarkets in underserved areas and provide incentives to food retailers to offer healthier food and beverage choices in underserved areas. If implemented, these strategies would decrease travel times necessary for accessing healthy, affordable foods among low-income and rural residents. When combined with health education efforts and mass media campaigns encouraging healthy food choices, more accessible and affordable healthy foods may lead to healthier food consumption patterns and to lower obesity prevalence in these groups. In a qualitative study of rural Georgia adults, participants identified several barriers to obtaining healthy foods, including poor selection, limited time, fuel prices, and the distance (15-45 miles) to larger communities with bigger stores and better selection . Another study found that longer distance traveled to the primary grocery store was associated with higher BMI . This previous work, taken together with our results, supports the notion that rural residents who travel farther to shop for food may purchase less healthful food. However, we did not measure the distance to the locations where people shopped and assumed that a positive food retail gap indicated a general trend for rural residents to shop for food outside their county of residence. Future work should assess the relationship between commute times and the locations where they purchase food. Future work should also include mediational analyses to examine the relationships between commute time, food shopping frequency and location, diet quality, and BMI. This study has several limitations. Foremost is the ecological design, which used several different data sources. The inconsistent timing of data collection for commute times , food retail gaps , and BMI  is an additional limitation. However, we used the most recent data available, and average commute time is a proxy for distance between place of employment and residence . A related limitation is the exclusion of people in the 36 counties where BRFSS did not provide county-level identifiers, pointing to the need for more work to examine rural populations. An additional caveat is that we used self-reported height and weight from BRFSS to calculate BMI, potentially biasing results toward the null if hypothesized relationships between commute times, food retail gaps, and BMI truly exist, because of potential underestimation of weight status. The use of a commercial business database (InfoUSA) to obtain sales data is also a limitation, because such databases may contain errors . Finally, in these analyses, we assumed commute time referred to time spent driving. Some people may walk or bike to work instead of drive; however, few Americans actively commute . This study is the first to examine correlations between commute times, food retail gap per capita, and mean BMI in counties in North Carolina. We present an approach to studying the association between BMI and variables related to the built and economic environments, providing support for the notion that economic and built environment factors are related to obesity.  \n\n【25】Tables\n------\n\n【26】#####  Table 1. Characteristics of 9,375 Respondents by Region, North Carolina Behavioral Risk Factor Surveillance System, 2007 a\n\n【27】CharacteristicRegion bWesternn = 1,789Easternn = 3,190Piedmontn = 4,837BMI c , kg/m 227.8 (6.1)28.7 (6.6)27.9 (6.3)Age, y47.6 (12.1)46.3 (12.4)46.0 (11.9)Men, %37.2 (48.3)35.7 (47.9)37.0 (48.3)High school diploma, %58.6 (49.3)60.5 (48.9)50.5 (50.0)Non-Hispanic black, %5.0 (21.8)24.8 (43.2)18.0 (38.4)Non-Hispanic white, %87.9 (32.6)63.4 (48.2)72.3 (44.7)Hispanic, %3.8 (19.1)5.3 (22.5)5.4 (22.5)County-level percentage residing in rural areas57.9 (22.6)44.8 (24.2)25.8 (20.9)County-level diversity index × 100 d18.6 (7.1)49.0 (12.1)43.6 (10.3)County-level percentage below the poverty level12.1 (2.0)15.5 (3.8)9.8 (1.8)County-level commute time in 1990, minutes19.4 (1.4)19.5 (2.3)19.9 (1.7)County-level commute time in 2000, minutes22.5 (2.0)24.0 (3.3)24.0 (2.2)Commute time difference , minutes3.2 (1.2)4.4 (1.6)4.1 (0.8)Retail gap per capita (NAICS code 445) e−251.2 (−353.3 to 124.2)63.9 (−120.9 to 159.4)−44.3 (−279.7 to 373.9)Retail gap per capita (NAICS code 722) f150.7 (−362.2 to 228.6)−116.5 (−211.5 to 358.1)98.8 (−152.8 to 361.9)Combined retail gap per capita (NAICS codes 445 + 722)−6.5 (−668.8 to 334.4)−147.6 (−455.7 to 457.9)240.4 (−85.9 to 517.4)Abbreviations: BMI, body mass index; NAICS, North American Industry Classification System.a Respondents resided in 64 North Carolina counties with valid values for all covariates for regression analyses, weighted to population.b All values are reported as mean (standard deviation), except those for retail gap per capita, which are reported as median (interquartile range).c BMI was unavailable for 441 respondents: 73 for the Western region, 143 for the Eastern region, and 225 for the Piedmont region.d Calculated by squaring the proportions of residents in each racial/ethnic group, summing the squares, and subtracting the result from 1 .e Retail gap per capita calculated by subtracting county-level sales of products for a NAICS category in 2008 from county-level demand for products in that category in 2008. NAICS code 445 defined as stores that sell food and beverages from fixed point-of-sale locations, including supermarkets, grocery stores, convenience stores, meat markets, produce markets, and specialty food stores.f NAICS code 722 defined as food services and drinking places that prepare meals, snacks, and beverages to customer order for consumption on and off the premises, including full-service restaurants, limited-service eating places (fast-food restaurants), special food services, and drinking places. \n\n【28】#####  Table 2. Correlation Between Percentage of Rural Residents in 100 North Carolina Counties and Mean Commute Times, Food Retail Gap Per Capita, and BMI\n\n【29】VariableCorrelation With Percentage of Rural Residents aCommute time 19900.56Commute time 20000.59Commute time difference 0.25Retail gap per capita (NAICS code 445) b0.19Retail gap per capita (NAICS code 722) c0.43Combined retail gap per capita (NAICS codes 445 + 722)0.31County-level BMI0.21Abbreviations: BMI, body mass index; NAICS, North American Industry Classification System.a _P_ values ranged from <.001 to .04 using a _t_ test except that for retail gap per capita (NAICS code 445) ( _P_ \\= .06).b Retail gap per capita calculated by subtracting county-level sales of products for a NAICS category in 2008 from county-level demand for products in that category in 2008. NAICS code 445 defined as stores that sell food and beverages from fixed point-of-sale locations, including supermarkets, grocery stores, convenience stores, meat markets, produce markets, and specialty food stores.c NAICS code 722 defined as food services and drinking places that prepare meals, snacks, and beverages to customer order for consumption on and off the premises, including full-service restaurants, limited-service eating places (fast-food restaurants), special food services, and drinking places. \n\n【30】#####  Table 3. Correlation Between BMI and Mean Commute Times and Food Retail Gap per Capita in 100 North Carolina Counties\n\n【31】VariableRetail Gap per Capita aNAICS Code 445 bNAICS Code 722 cNAICS Codes 445 + 722County-Level BMICommute time 19900.260.410.340.12Commute time 20000.350.510.440.31Commute time difference 0.290.350.340.46County-level BMI0.370.240.341.00Abbreviations: BMI, body mass index; NAICS, North American Industry Classification System.a Retail gap per capita calculated by subtracting county-level sales of products for a NAICS category in 2008 from county-level demand for products in that category in 2008. _P_ values ranged from <.001 to .01 using a _t_ test except that for commute time in 1990 and BMI ( _P_ \\= .22).b NAICS code 445 defined as stores that sell food and beverages from fixed point-of-sale locations, including supermarkets, grocery stores, convenience stores, meat markets, produce markets, and specialty food stores.c NAICS code 722 defined as food services and drinking places that prepare meals, snacks, and beverages to customer order for consumption on and off the premises, including full-service restaurants, limited-service eating places (fast-food restaurants), special food services, and drinking places. \n\n【32】#####  Table 4. Correlation Between Individual-Level BMI and County-Level Variables, North Carolina a\n\nCounty-Level VariableRegression Model b,c12342000 Commute time0.08470.07300.07330.0660Commute time difference 0.27190.18120.17910.1572Retail gap per capita (NAICS code 445) d0.00040.00020.00030.0002Retail gap per capita (NAICS code 722) e0.00040.00040.00040.0004Combined retail gap per capita (NAICS codes 445 + 722)0.00020.00020.00020.0002Abbreviations: BMI, body mass index; NAICS, North American Industry Classification System.a Individual-level BMI was the dependent variable and county-level commute times and food retail gap per capita were independent variables. Individual covariates were age, age squared, sex, education, and race/ethnicity.b Model 1: no additional covariates; model 2: individual covariates only; model 3: individual covariates + Rural to Urban Continuum Codes (RUCC) ; model 4: individual covariates + RUCC + regional dummy variables.c _P_ values ranged from <.001 to .048 using a _t_ test, except those for Model 2 for retail gap per capita (NAICS code 445 \\[ _P_ \\= .08\\] and NAICS code 722 \\[ _P_ \\= .06\\]) and for Model 4 for retail gap per capita (NAICS code 445 \\[ _P_ \\= .06\\]).d Retail gap per capita calculated by subtracting county-level sales of products for a NAICS category in 2008 from county-level demand for products in that category in 2008. NAICS code 445 defined as stores that sell food and beverages from fixed point-of-sale locations, including supermarkets, grocery stores, convenience stores, meat markets, produce markets, and specialty food stores.e NAICS code 722 defined as food services and drinking places that prepare meals, snacks, and beverages to customer order for consumption on and off the premises, including full-service restaurants, limited-service eating places (fast-food restaurants), special food services, and drinking places.  **Comment on this article at _PCD_ Dialogue**Learn more about PCD's commenting policy  |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3f928d4b-68fb-40da-9c45-2b3184180549", "title": "Revisiting the Island of Doctor Moreau", "text": "【0】Revisiting the Island of Doctor Moreau\n**Kate Gibb , _The Island of Doctor Moreau_ , 2006.** Silkscreen by hand on paper, 19.7 in × 27.6 in/50 cm × 70 cm. Digital image used with permission of the artist. London, England.\n\n【1】This month’s cover art, created by contemporary English printmaker and illustrator Kate Gibb―known for her colorful, detailed screen-printed artwork developed for musicians and fashion designers―was first featured on another cover, the 2005 reissue of a classic work of early horror and science-fiction, _The Island of Doctor Moreau_ by English writer H.G. Wells. Originally published in 1896, Wells’s novel chronicles the story of the shipwrecked Englishman Edward Prendick. Stranded on a remote, uncharted island, Prendick is nursed back to health after his ordeal at sea only to discover he has washed up on an isle of horrors. Doctor Moreau, a mad scientist, has fled to the island from England after his experiments in vivisection were exposed. Here the doctor works without restraints, accountability, or moral guidance, performing gruesome, cruel experiments designed to transform animals into humans.\n\n【2】In _The Island of Doctor Moreau,_ Wells intertwines several societal themes from the latter part of the 19th century, including the growing antivivisection movement, implications of Charles Darwin’s research on evolution, the nature of humankind as explored in Robert Louis Stevenson’s 1886 novel _The Strange Case of Dr. Jekyll and Mr. Hyde_ , and the notion of humans creating chimeras, an idea articulated earlier in Mary Shelley’s 1818 novel _Frankenstein_ . Moreau’s horrific surgical procedures triggered ethical questions and concerns about cruelty, morals, and human–animal relationships that still resonate with contemporary readers. Literature professor Roger Luckhurst notes, “In the 21st century, with genetic splicing making animal–human hybrids an actual possibility, Wells’s queasy exploration of the limits of the human in this provocative satire keeps the book incredibly relevant today.” Moreau’s efforts to control the behaviors of his menagerie of engineered creations through “The Law,” a list of prohibitions recited to the island’s “Beast Folk” to keep them from reverting to their animalistic selves, are also troubling.\n\n【3】Those who have read the novel will recall that after Moreau and his assistant Montgomery are killed, as are a number of the altered creatures, Prendick eventually escapes from the island and returns to London. He discovers, however, that he can no longer live among humans, fearing they, too, will revert to animalistic beings, and so he seeks solitude in the countryside, devoting himself to scientific studies. Gibb’s collage, which draws from the interconnected themes in the novel, features silhouettes of simians, the maw of a felid, and a lacey cross-section of a brain superimposed over a soothing blue-gray background, plus a pair of flies disturbingly close to a human eye peering at the viewer.\n\n【4】As readers of this journal know, the xenotransplantation and surgeries, close interactions of humans and animals, and coerced association of myriad animal species on Moreau’s island could potentially have led to the spread of zoonotic infections, perhaps even the emergence of novel pathogens. But no person or animal is ever infected with a zoonotic pathogen in the novel, unlike in Wells’s next novel, _The War of the Worlds_ , when invading Martians succumb to earthly pathogens to which they had no immunity, “slain, after all man’s devices had failed.”\n\n【5】Zoonotic diseases, an unavoidable consequence of human–animal interactions, are caused by microorganisms such as viruses, bacteria, parasites, and fungi. CDC’s One Health website notes that more than 6 out of every 10 known infectious diseases in humans can be spread from animals. Perhaps in a contemporary reimagining of _The Island of Doctor Moreau_ , existing or emerging zoonotic infections would factor into the narrative. In such a reworking, the isolated island setting and small number of people and animals might mitigate widespread transmission of such infections, in contrast with factors driving the emergence of new infectious diseases. Human encroachment into remote regions, factory farming, animal markets and trade, climate change, disruptions of natural areas and their ecosystems, and global migration all play a role.\n\n【6】Understanding those interconnections between humans, animals, plants, and their communal environment, a concept now known as One Health, is an important public health priority. In a 2018 article published in the _Annual Review of Animal Biosciences_ , researchers Bird and Mazet warn, “We must be prepared to recognize the signs, identify the threat, and rapidly work together to reduce the spread of infections and health consequences before they harm the health of animals and people throughout the world.” Gibb’s silkscreened image seems applicable both to the events in Wells's 125-year-old novel and to those of the present day.\n\n【7】##### About the Artist\n\n【8】Kate Gibb created 17 covers for the 2006 reissues of H.G. Wells’s works and explained, “There were strong themes and descriptive elements within the text which quickly inspired me to find and collage images quite spontaneously. I remember really enjoying the process and being pleasantly surprised by its outcome!” She has worked with fashion designers, publishers, and musicians, and her work has been featured in a number of contemporary publications. In addition to producing her print-based artworks and commercial illustrations, Gibb is an educator, most recently at England’s University of Brighton. After studying textiles at Middlesex University, London, she shifted her focus to silkscreen printing and describes herself as “a silkscreen obsessive.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "de6349ce-2c24-4d3e-90d1-dd874efec7d6", "title": "Invasive Amebiasis in Men Who Have Sex with Men, Australia", "text": "【0】Invasive Amebiasis in Men Who Have Sex with Men, Australia\n_Entamoeba histolytica_ is a pathogenic ameba that can cause invasive intestinal and extra-intestinal disease. The most frequent manifestations of invasive amebiasis are colitis and liver abscesses . Although _E. histolytica_ is one of the most common parasitic infections worldwide, invasive disease remains uncommon in industrialized counties. Recent studies from Japan, Taiwan, and Republic of Korea, areas where _E. histolytica_ endemicity is generally low, suggest that amebiasis is an emerging parasitic infection that occurs exclusively in men who have sex with men (MSM) . In Australia, the documented incidence of _Entamoeba_ spp. in the general population is 1%–4% . In MSM, the rates of _Entamoeba_ spp. carriage were previously documented to be as high as 37% . However, these studies failed to differentiate the pathogenic _E. histolytica_ from the morphologically identical nonpathogenic _E. moshkovskii_ and _E. dispar_ , therefore leaving the accuracy of these results in question. A study in Australia that used molecular methods showed _E. histolytica_ prevalence rates in MSM to be as low as 0.1% . Impaired host immunity is associated with increased pathogenicity of invasive amebiasis. Recent studies indicate an increased risk for invasive amebiasis among persons with HIV . We report 5 cases of invasive amebiasis in MSM from Sydney, New South Wales, Australia, from December 2006 through October 2007.\n\n【1】### The Cases\n\n【2】Of the 5 patients, 3 had amebic colitis and 2 had amebic liver abscesses, all were MSM, 4 were HIV-infected, mean age was 45 years (range 35–57 years), and median CD4 count was 713 cells/mm 3  . No associations among any of the patients were noted. All patients were receiving highly active antiretroviral therapy at the time of examination. Of the 5 patients, 4 had not traveled extensively in the past 4 years, but the remaining patient had traveled to Malaysia and China 6 months before onset of colitis. Other possible risk factors for acquisition were identified in 4 patients: 3 had had high-risk sex behavior at sex-on-premises venues, and the other had had a male partner who had traveled to countries where invasive amebiasis was highly endemic.\n\n【3】The patients had bloody diarrhea and abdominal pain for 2–4 weeks. Routine fecal cultures were negative for bacterial pathogens. Microscopic examination of permanently fixed, stained fecal smears was positive for _E. histolytica_ / _dispar/moshkovskii_ complex. Diagnosis of _E. histolytica_ was confirmed by PCR targeting the small subunit ribosomal DNA as described . Results of serologic examination of these patients were positive; all titers were >256 according to indirect hemagglutination antibody assay, which confirmed invasive disease. However, although antibodies against amebae indicate invasive disease, these antibodies can also be seen in persons with asymptomatic colonization with amebae .\n\n【4】The 2 patients with liver abscesses each had large, solitary abscesses in the right lobe of the liver (8 × 6 × 6 cm and 7.5 × 6.2 × 6.6 cm). In 1 patient, the abscess ruptured through the liver capsule, and collapse of the right middle and lower lung lobes and a resultant pleural effusion complicated the subphrenic collection of pus from the abscess. Levels of liver enzymes (alkaline phosphatase and gamma glutamyl transferase), C-reactive protein, and neutrophils (absolute numbers) were raised. Both patients underwent percutaneous drainage of their liver abscess; cultures of aspirated pus were negative for bacteria and fungi. Results of indirect hemagglutination antibody assay were positive; titer was \\> 256\\. Microscopy and PCR of fecal samples were negative for _E. histolytica._ For these 2 patients with amebic liver abscess, the diagnosis was delayed; they had had symptoms for >2 weeks and were then treated for bacterial liver abscess before the correct diagnosis was made.\n\n【5】All 5 patients made a successful recovery after treatment with metronidazole. In addition, all were treated for cyst carriage with paromomycin, a luminal amebicide.\n\n【6】### Conclusions\n\n【7】_E. histolytica_ carriage and invasive disease are common in the Asia-Pacific region, especially in developing countries. In countries where _E. histolytica_ prevalence is low, such as Japan, Taiwan, Republic of Korea, and Australia, rates of amebiasis are low and invasive amebiasis is uncommon. Recent reports from a number of these countries, however, suggest that invasive amebiasis is emerging as an increasingly common infection, specifically in the MSM population . MSM have a higher risk than others for intestinal parasite carriage; not only are they substantially more likely to harbor intestinal protozoa, but they are also more likely to harbor multiple parasites . These protozoa are transmitted by the fecal–oral route; high rates of oral–anal sex by MSM are considered the reason for increased rates of carriage. Because _E. histolytica_ is also transmitted by the fecal–oral route, MSM may also have an increased risk for _E. histolytica_ carriage. This higher rate of asymptomatic carriage is likely to translate into a greater risk for invasive disease. Recent seroprevalence studies in Taiwan that used indirect hemagglutination antibody assay have confirmed MSM’s statistically significant higher risk for _E. histolytica_ exposure .\n\n【8】In Japan, amebiasis has become endemic in MSM; symptomatic _E. histolytica_ infection occurs almost exclusively in middle-aged MSM in the large cities of Japan . Similar findings are reported for MSM in Taiwan . More recently, a study from the Republic of Korea documented invasive amebiasis (amebic liver abscess) in HIV-infected MSM . To date, the emergence of _E. histolytica_ infections in MSM seems to be limited to the Asia-Pacific region. In a large retrospective study of 34,000 HIV-infected patients in United States, only 2 patients had invasive amebiasis . The reasons for this geographic variation are unclear, but it is likely linked to the higher background prevalence of _E. histolytica_ infections in Asia. Regional _E. histolytica_ strains show a high degree of diversity but no major differences between regional genotypes; other factors relating to host or virulence factors may be important but are as yet undetermined . We now report local acquisition of _E. histolytica_ by MSM in Australia; as shown by the above 5 cases of invasive disease and 3 previous cases of noninvasive infections also acquired locally .\n\n【9】Of note, 4 of the 5 patients we report were HIV infected. Seroprevalence rates of _E. histolytica_ (determined by indirect hemagglutination antibody assay) are higher for HIV-infected persons then for HIV-nonninfected persons, although the reasons are unclear . Higher rates of _E. histolytica_ carriage in MSM likely reflect high-risk sex behavior and multiple exposures, resulting in increased risk for acquisition. This hypothesis is supported by the high rates of sexually transmitted infections that occur in Australian MSM who visit sex-on-premises venues . Antibody responses predominantly occur with invasive disease. Whether immunosuppression caused by HIV infection attenuates the risk for invasive amebiasis is unknown. Historically, the evidence has been contradictory, and most published studies had had severe limitations. Nevertheless, more recent data seem to indicate that HIV-infected persons are at increased risk for invasive amebiasis .\n\n【10】The emergence of _E. histolytica_ in MSM is of public health concern because it has the potential to become endemic in this population in Australia and to cause severe disease. Further study is needed to identify the reasons for the geographic variation and the role of _E. histolytica_ in invasive disease. In conclusion, invasive amebiasis has the potential to emerge as an important parasitic infection in the Asia-Pacific region, especially in HIV-infected MSM in countries where _E. histolytica_ is not endemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "74189f44-2364-473d-8e5f-952d72235f2a", "title": "Human Participant Protection in CDC Research", "text": "【0】Human Participant Protection in CDC Research\nAll research involving human participants that is conducted or supported by CDC must comply with the HHS Policy for Protection of Human Research Subjects ( 45 CFR part 46 external icon ). This includes research conducted by CDC employees or supported by CDC through funding or provision of other tangible support, whether conducted inside or outside the United States. Unless exempt, all such research must be approved by an institutional review board (IRB) prior to the start of the research. HRPO facilitates the work of the IRB and provides assistance and training for CDC staff engaged in research involving human participants. Clinical investigations that involve the use of drugs, biologics, or devices—whether unlicensed or used outside standard medical practice—are subject to IRB review and approval under 21 CFR Part 50 external icon and 21 CFR Part 56 external icon .\n\n【1】*   CDC’s Institutional Review Boards\n*   CDC’s Policy on Distinguishing Public Health Research and Public Health Nonresearch pdf icon \\[PDF – 138 kb\\] (This replaces CDC’s Guidelines for Defining Public Health Research and Public Health Nonresearch, October 1999 pdf icon \\[PDF – 39 kb\\] )\n*   CDC’s Policy on Human Research Protections pdf icon \\[PDF – 167 kb\\]\n*   Scientific Ethics Training (Obtain SEV Number)\n*   Regulatory and Ethical Codes", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aff56a6d-bb55-4071-b65d-ae5f4d61b4c0", "title": "A Project to Promote Adherence to Blood Pressure Medication Among People Who Use Community Pharmacies in Rural Montana, 2014–2016", "text": "【0】A Project to Promote Adherence to Blood Pressure Medication Among People Who Use Community Pharmacies in Rural Montana, 2014–2016\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Pharmacists can assist patients in managing their blood pressure levels. We assessed whether adherence to blood pressure medication improved among people who used community pharmacies in rural Montana after pharmacists initiated consultations and distributed educational materials developed for the Million Hearts Initiative’s “Team Up. Pressure Down.” (TUPD) program.\n\n【3】**Methods**\n\n【4】From 2014 to 2016, the Cardiovascular Health Program at the Montana Department of Public Health and Human Services conducted a statewide project to evaluate an intervention for adherence to blood pressure medication administered through community pharmacies. After the year 1 pilot, we redesigned the program for year 2 and year 3 and measured the percentage of participating patients who adhered to blood pressure medication. We also conducted a statewide survey to assess pharmacy characteristics, computer-system capabilities, and types of consulting services provided by pharmacists.\n\n【5】**Results**\n\n【6】Twenty-five community pharmacies completed Montana’s TUPD program: 8 pharmacies in the pilot year, 11 pharmacies in year 2, and 6 pharmacies in year 3. For year 2 and year 3 combined, the percentage of participating patients who achieved blood pressure medication adherence improved preintervention to postintervention from 73% to 89%, and adherence improved in 15 of the 17 pharmacies. The pilot pharmacies identified 3 major barriers to project success: patient buy-in, staff burden in implementing the project, and funding. In the statewide assessment, TUPD-funded pharmacies were significantly more likely than non-TUPD–funded pharmacies to provide prescription synchronization and medication management with feedback to the patient’s physician.\n\n【7】**Conclusion**\n\n【8】Community pharmacies in rural areas can effectively use brief consultations and standard educational materials to improve adherence to blood pressure medication.\n\n【9】Introduction\n------------\n\n【10】High blood pressure is a controllable risk factor for cardiovascular diseases (eg, heart disease, stroke) . However, patients with hypertension often find it challenging to manage their condition. Barriers to management may include problems with medication adherence, not understanding the seriousness of the condition, or difficulty making lifestyle changes. Community pharmacists can extend the reach of health care providers and assist patients in controlling their hypertension. With ready and consistent access to patients who refill prescriptions monthly, pharmacists are in a position to establish an ongoing relationship with their patients.\n\n【11】A meta-analysis of 7 randomized controlled trials showed that adherence to blood pressure medication increased more in the pharmacist-led interventions than in the control groups . In the 6 studies that provided quantitative data, adherence in the intervention groups increased from 56% (203 of 360 participants) to 68% (246 of 360 participants); in the control groups, adherence increased from 59% (190 of 320 participants) to 61% (195 of 320 participants). Another meta-analysis found that 7 of 16 pharmacist interventions significantly increased medication adherence ; the difference between adherence in the intervention groups compared with the control groups ranged from 8 to 58 percentage points.\n\n【12】Pharmacy-based interventions are also effective in improving medication adherence among people in racial/ethnic minority populations . We found no studies of pharmacy interventions to improve adherence to blood pressure medication in rural areas. Because of a shortage of primary care providers in rural areas , pharmacies in rural areas could play a larger role in improving medication adherence than pharmacies in urban areas. Pharmacists can help identify and overcome barriers (eg, financial difficulties, side effects) that health care providers may not detect during patient visits, which often are infrequent. Pharmacies also can assist patients in managing their blood pressure levels .\n\n【13】Only 2 interventions that we reviewed  provided patients with pharmacist consultations and educational materials on blood pressure medication adherence. However, these interventions did not rigorously assess the usefulness of the educational materials.\n\n【14】We evaluated whether patients’ adherence to blood pressure medication improved in rural Montana when we used pharmacy consultations in combination with educational materials that were developed for the Million Hearts Initiative’s “Team Up. Pressure Down.” (TUPD) and were designed for community pharmacists and their patients . Our secondary objective was to describe pharmacy characteristics, computer-system capabilities, and types of consulting services provided by pharmacists throughout Montana.\n\n【15】Methods\n-------\n\n【16】This study consisted of 2 components: 1) a 3-year (February 2014–June 2016) intervention to improve adherence to blood pressure medication among people using community pharmacies in rural Montana and 2) a statewide assessment (November 2015–February 2016) of pharmacy characteristics, computer-system capabilities, and types of consulting services provided.\n\n【17】Montana is the fourth largest state geographically but is ranked 48th in the United States for population density, with only 6.8 persons per square mile . Much of the state is classified as an area with a shortage of health care professionals or as a medically underserved area . According to rural–urban commuting area codes, less than 20% of Montana’s counties had census tracts with a classification of “metropolitan area core” or “metropolitan area high commuting” . For the TUPD project, most participating pharmacies were in counties outside these metropolitan areas .\n\n【18】### Community pharmacy intervention\n\n【19】In 2014, the Montana Cardiovascular Health (CVH) Program at the Department of Public Health and Human Services (DPHHS) initiated a project with 9 community pharmacies in Montana to conduct and evaluate a blood pressure medication adherence intervention. However, one funded pharmacy did not complete the project because of problems with business structure and staffing. The project used an implementation study design and 3 cohorts (Box 1). The Montana DPHHS did not require institutional review board approval because pharmacies submitted only de-identified aggregate data.\n\n【20】Box 1. Timeline for Team Up. Pressure Down. (TUPD) Blood Pressure Medication Adherence Project in Community Pharmacies, Montana, 2014–2016\n\n【21】February–June 2014: Pilot project completed with 8 pharmacies\n\n【22】June–August 2014: Formative evaluation\n\n【23】August 2014–June 2015: Year 2 cohort — 11 pharmacies\n\n【24】August 2015–June 2016: Year 3 cohort — 6 pharmacies\n\n【25】November 2015–February 2016: Community pharmacy assessment\n\n【26】The University of Montana’s Skaggs School of Pharmacy provided a list of 258 community pharmacies in Montana. A community pharmacy is designated by the Montana Department of Labor and Industry as a pharmacy that serves customers in a retail setting, such as a pharmacy chain or an independent pharmacy, rather than in an institutional setting, such as a hospital. To recruit pharmacies for the pilot project, the CVH Program mailed an application to all 258 community pharmacies listed, and the Montana Pharmacy Association emailed the announcement to its members. In addition, Montana’s Medicare Quality Innovation Network–Quality Improvement Organization helped recruit pharmacies and disseminate project materials.\n\n【27】Montana DPHHS staff members reviewed 9 applications for the pilot year. The criteria for funding included providing an estimate of the number of patients in the pharmacy who were taking blood pressure medication and the number of patients to be tracked and providing an adequate description of a project plan, including selecting, tracking, and following up with patients. Applicants also were required to describe components that could be continued by the pharmacy without external funding. In the pilot year, all 9 applicants met the application criteria and were funded. Using a similar application and notification process, we funded 2 more cohorts: 11 pharmacies in year 2 and 7 pharmacies in year 3 (one of which did not complete the project because of a staffing shortage).\n\n【28】#### Pilot project\n\n【29】Each pilot pharmacy was required to recruit at least 25 patients. Participants were required to meet the following minimum criteria: 1) being an adult aged 18 years or older, 2) having had at least one pharmaceutical claim during the previous calendar year (ie, an active pharmacy patient), and 3) having had at least one current prescription for a medication to lower blood pressure. Pharmacies were permitted to customize approaches for identifying and recruiting participants (eg, letters, direct contact).\n\n【30】As part of the project, pharmacies conducted a brief consultation with each participating patient. During the consultation, the pharmacist discussed medication management and changes in lifestyle behavior to help improve the patient’s medication adherence and blood pressure control. We asked the pharmacies to disseminate TUPD’s patient-education materials and information on the Dietary Approaches to Stop Hypertension (DASH) program  and to refer smoking patients to the Montana Tobacco Quitline . TUPD’s patient-education materials included a blood pressure journal, a medication tracker wallet card, and a medication reminder handout. Additionally, participants received a postcard with information on steps to control blood pressure and a place to list pharmacy and prescription information. TUPD’s pharmacist materials included a pocket discussion guide, a drug-adherence work-up tool (to identify and address patient barriers to taking medication), a blood pressure guide (a quick reference on taking blood pressures manually and interpreting blood pressure readings), and a pharmacy poster.\n\n【31】During the pilot program, pharmacies measured medication adherence by 1) calculating the number of days of refill for a blood pressure medication for each participating patient or 2) using another standard method, such as calculating the percentage of participating patients who achieved blood pressure medication adherence, measured as the proportion of days covered (PDC) by prescription claims as 80% or greater (based on prescription fill date and days of supply). We did not require pharmacies to adhere to a particular method of calculating adherence. Some pharmacies electronically tracked prescription fill dates, and others used an Excel (Microsoft Corp) spreadsheet.\n\n【32】Although the pilot project was designed initially to be implemented during a 10-month period, it was implemented during a 4-month period because of a delay in budget approval. After the conclusion of the pilot program, we obtained feedback from the pilot pharmacies and modified the intervention for year 2 and year 3. We also sought federal guidance on a standard definition of medication adherence  and requested additional funding so that we could recruit more pharmacies and increase the funding award to pharmacies as an incentive for them to participate.\n\n【33】#### Year 2 and year\n\n【34】In year 2 and year 3, in addition to other program improvements (Box 2), we required pharmacies to use a standardized definition for medication adherence (PDC ≥80%). We received additional funding, which allowed us to double the funding award to pharmacies. We shared lessons learned from the pilot pharmacies with the new pharmacies, emphasized project expectations, and provided additional technical assistance.\n\n【35】Box 2. Components of Team Up. Pressure Down. (TUPD) in Year 2 and Year 3, Based on Feedback From Pilot Year, Montana, 2014–2016\n\n【36】• Standardized the definition of medication adherence \n\n【37】• Expanded project time frame from 4 months to 10 months\n\n【38】• Increased the minimum number of patients required to participate from 25 to 35 (year 3 only)\n\n【39】• Doubled the funding award to pharmacies as an incentive for participation\n\n【40】• Offered 2 training options for pharmacists: a home-study blood pressure curriculum and a 1-day hands-on hypertension workshop on accurate blood pressure measurements, current guidelines, lifestyle changes, and medication management\n\n【41】• Provided additional resources to each participating pharmacy: blood pressure cuffs for on-site use and 7-day pill boxes for participating patients\n\n【42】• Provided a sample letter that pharmacists could send to health care providers informing them of their patients’ participation in the project\n\n【43】• Provided a sample press release that pharmacists could send to the local news media to inform their community of the project\n\n【44】• Created an Excel (Microsoft Corp) tracking program for such patient interventions as medication therapy management and lifestyle counseling\n\n【45】• Organized a conference call in which a previously participating pharmacist oriented newly participating pharmacists\n\n【46】• Hired a consulting pharmacist, who owns a community pharmacy, to provide technical assistance and engage pharmacists on a peer-to-peer basis\n\n【47】#### Data collection\n\n【48】The CVH Program collected data from reports filed one month after the project began and final reports. Pre-intervention and postintervention data were collected on medication adherence at the start and end of each of the 3 project periods. The CVH Program developed a final report template that community pharmacies completed at the end of each project period. The final report gave information on barriers, lessons learned, sustainable components, and suggestions for improvement. The final reports also provided data on types of counseling provided and pharmacists’ perceptions of the usefulness of TUPD materials and resources. Lastly, the final report provided aggregate data on the percentage (numerator and denominator) of participating patients who adhered to their blood pressure medication schedule. In addition, for year 2 and year 3, the CVH Program periodically requested interim feedback from the participating pharmacies on progress made and barriers encountered. A consulting pharmacist reviewed the feedback and made suggestions to address barriers as part of his technical assistance.\n\n【49】### Statewide pharmacy assessment\n\n【50】From November 2015 through February 2016, the CVH Program conducted a statewide assessment of community pharmacies to collect data required by CDC to measure grant performance. In October 2015, the CVH Program and a community pharmacist reviewed and revised a survey instrument that the program had designed and used in a statewide assessment in 2013. In November 2015, the Montana Department of Labor and Industry provided a list of licensed community pharmacies. We merged this list and the TUPD recruitment list from the Skaggs School of Pharmacy and eliminated duplicate pharmacies by matching license number, business name, and city, which yielded 259 community pharmacies. The survey, which was mailed, collected information on pharmacy characteristics (the number of pharmacists and pharmacy technicians); computer-system capabilities (acceptance of electronic prescriptions from outside health care facilities, automatic refills on certain maintenance medications, automated refill reminders for blood pressure medication); provision of prescription synchronization (the process of aligning refill dates for all of a patient’s multiple prescriptions); reimbursement of medication therapy management from Mirixa or OutcomesMTM, 2 leading vendors of medication therapy management services in Montana; and the types of consulting services provided by pharmacists. Medication therapy management is a service provided by pharmacists to optimize drug therapy and improve health outcomes.\n\n【51】### Data analysis\n\n【52】For year 2 and year 3 of the intervention, we aggregated the data from the final reports for medication adherence (percentage of participants with PDC ≥80% and total number of participants) from each pharmacy. Details on the calculation of PDC, including definition of terms, unit of analysis, and determination of numerators and denominators, are available elsewhere . To generate an overall rate, we aggregated the data on medication adherence by year. The pilot sites were excluded from the medication adherence analysis because they were not required to use a standard definition for medication adherence.\n\n【53】For the community pharmacy assessment, we analyzed data using IBM SPSS Statistics version 21 (IBM Corporation). We used χ 2  tests to assess any differences in pharmacy and consultation services offered by pharmacists, such as consultation on blood pressure medication adherence, between pharmacies funded by TUPD and pharmacies not funded by TUPD. We also used the nonparametric Mann–Whitney _U_ test to compare differences between pharmacies funded by TUPD and pharmacies not funded by TUPD in the number of pharmacists and pharmacy technicians. A _P_ value of < .05 was considered significant.\n\n【54】Results\n-------\n\n【55】Twenty-five community pharmacies completed Montana’s TUPD project: 8 in the pilot year, 11 in year 2, and 6 in year 3. All 25 pharmacies submitted a final report. For year 2 and year 3 combined (17 pharmacies), 534 patients completed the TUPD project, with 360 in year 2 and 174 in year 3; the aggregated percentage of participating patients who achieved blood pressure medication adherence increased from 73% pre-intervention to 89% postintervention . Blood pressure medication adherence improved in 15 of the 17 community pharmacies in year 2 and year 3.  \n\n【56】### Feedback from pharmacists\n\n【57】The pilot pharmacies identified 3 major barriers to project success: patient buy-in, staff burden in implementing the project, and funding. A lack of awareness of the importance of controlling blood pressure, a lack of willingness or interest in project participation, and lack of recognition of the benefits of participation were major obstacles among patients. Staff burden was the most common barrier reported by the pharmacies. Adding another program to a busy schedule was difficult. A lack of time limited the ability of the pharmacists to provide customer service and pharmacy counseling beyond the core task of dispensing medication. In the pilot project, the pharmacists’ suggestions for enhancing the project included developing a template for tracking patients, a notification letter to health care providers, and a checklist of topics to discuss with patients. These resources were added in year 2 and year 3. Other recommendations (a wallet card to log blood pressure values and a survey to obtain patient feedback) will be added in year 4.\n\n【58】Feedback from year 2 and year 3 indicated that involving the entire pharmacy team in the project helped reduce the burden of work on the pharmacists. For example, one pharmacy created a system in which TUPD materials were attached to a patient’s medication refill. When the patient picked up the refill, the pharmacy technician notified the patient that the pharmacist wanted to speak with him or her. Another pharmacy involved the pharmacy technicians in using an alert system (tracking sheet) when a study patient was in the pharmacy so that pharmacists could provide consultations.\n\n【59】TUPD-funded pharmacies reported that TUPD materials and resources were useful. Although all participating pharmacies reported distributing TUPD materials, pharmacists reported only 75% of project participants received these materials because some patients refused them. Pharmacists noted that the wallet card and journal were helpful and of interest to patients, although they noted that some of the materials could be written more concisely. Additionally, 21 pharmacies reported their pharmacists provided lifestyle counseling and medication therapy management to their patients with hypertension.\n\n【60】The pharmacists noted that most of their patients appreciated the extra attention they received during the consultations. Pharmacists adjusted the length of the consultation according to the interest level and needs of each patient. Three pharmacists suggested that the TUPD project may be most suitable for patients with a new diagnosis of hypertension because patients with long-term hypertension had already found ways to manage their condition. One pharmacy lost many project participants because of the participants’ transient employment (oil workers). In addition, 13 pharmacies noted difficulty tracking patients (eg, patient used mail-order or 90-day prescriptions, transferred pharmacies, died, was hospitalized, or moved). Three pharmacies found opportunities to collaborate with patients’ providers to improve blood pressure control.\n\n【61】Twenty pharmacies reported plans to sustain at least one project component to foster medication adherence (eg, measuring blood pressure on-site, offering counseling or medication reviews, providing blood pressure information materials, synchronizing medication, creating a system of automatic refills).\n\n【62】### Statewide pharmacy assessment\n\n【63】The response rate for the community pharmacy assessment conducted was 46% (120 of 259). The average number of pharmacists per pharmacy in Montana was fewer than 3 . We found no significant differences between TUPD-funded pharmacies and non-TUPD–funded pharmacies in the number of pharmacy staff members or pharmacy services related to whether or not automatic refills or refill reminders are provided . TUPD-funded pharmacies were significantly more likely than non-TUPD–funded pharmacies to provide prescription synchronization and medication management with feedback to the patient’s physician. TUPD-funded pharmacies also were more likely than nonfunded pharmacies to report that pharmacists were reimbursed for formal medication therapy management from Mirixa or OutcomesMTM.\n\n【64】Discussion\n----------\n\n【65】Our findings indicate that it is feasible for community pharmacies in rural areas to provide their patients with brief consultations and TUPD educational materials on how to improve blood pressure medication adherence. Our results are similar to those reported in other studies, which found that pharmacist interventions could significantly improve medication adherence . The project components in these previous studies were not identical to those in TUPD, however. Some of those interventions provided resources such as a take-home tool kit  or blood pressure cuffs for self-monitoring at home  that our project did not provide.\n\n【66】Our study differed from most other studies in that ours focused only on rural pharmacies. Although one study did examine rural Minnesota pharmacies, it was a biennial pharmacy workforce survey of outpatient pharmacies rather than an intervention . Also, we did not find any previous study that investigated use of TUPD materials.\n\n【67】Our results suggest that the pharmacies were able to customize the project to fit their needs. In addition, our findings indicate that major components of the project can be integrated into the usual practice of community pharmacies in rural areas. Pharmacies that were already being reimbursed for medication therapy management or that synchronized refills may have been more willing to participate in this project because of their experience in patient consultations.\n\n【68】This project has several limitations. First, our study did not include a control group; however, because this was a project evaluation and not a research project, a control group may not have been needed. Second, pharmacies were not required to collect data on patients’ blood pressure control. We did not institute this requirement because of limited pharmacist time, lack of adequate funding, and difficulty in bringing participants in for measurement. Since we did not require pharmacies to collect data on patients’ blood pressure, we could not conduct additional analyses. However, some participating pharmacies used a blood pressure cuff for on-site measurement, and some made the cuff available to nonparticipating patients. Third, because of the small sample of pharmacies, the results of our study may not be generalizable to all pharmacies. We expect to have a larger sample size for study when additional pharmacies are funded for 2 more years. Fourth, this project assessed only the perceptions of the pharmacists and not those of other stakeholders (pharmacy patients or health care providers). Lastly, because of the annual funding cycle of the CDC grant, we did not investigate long-term medication adherence. Despite these limitations, our project results suggest that community pharmacies in rural areas can use brief consultations and TUPD materials to improve blood pressure medication adherence.\n\n【69】The TUPD project could be expanded to other states that have community pharmacies in rural areas. In year 2, the DPHHS diabetes program broadened the TUPD project by conducting a similar project with 7 of the pilot pharmacies, targeting pharmacy patients taking blood pressure and diabetes medications. Also, in 2016 the state asthma control program recruited 2 of the pilot pharmacies to address asthma medication adherence. This expansion of the TUPD blood pressure approach indicates the willingness of community pharmacies to work on chronic disease management. Future research should evaluate whether the TUPD strategy also improves medication adherence for patients with other chronic conditions such as diabetes or asthma. In addition, research could assess blood pressure control and medication adherence in community pharmacies in rural areas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1b31274c-16e5-450f-9ace-d07867d6a1e0", "title": "Association of Dengue Virus and Leptospira Co-Infections with Malaria Severity", "text": "【0】Association of Dengue Virus and Leptospira Co-Infections with Malaria Severity\nIn tropical countries, including India, acute febrile illnesses (AFIs) constitute a group of infections with similar manifestations, such as fever, malaise, body aches, chills, hepatic and renal dysfunction, and central nervous system effects. The causative agents of AFI can be bacterial (e.g. _Orientia tsutsugamushi, Leptospira_ , and _Salmonella enterica_ serovar Typhi), parasitic (protozoans of the apicomplexa family), or viral (e.g. dengue virus \\[DENV\\], chikungunya virus \\[CHIKV\\], influenza A\\[H1N1\\] virus) . Distinguishing between the causative agents of AFIs can be difficult. In tropical climates, several AFI pathogens, such as malaria parasites, DENV, and CHIKV, occur in the same areas and during the same seasons , making it possible that >1 pathogen can infect the same person. Indeed, recent retrospective analyses based on persons hospitalized with an AFI have uncovered malaria co-infections with dengue, chikungunya, and leptospirosis in different populations across the world .\n\n【1】Despite the increasing realization that co-infections may contribute to the course and outcome of malaria, only a few studies have investigated the prevalence and nature of co-infections , which limits our ability to manage and understand AFIs, as follows. First, we do not know the spectrum of infections that a person with an AFI may harbor, leading to inadequate drug therapy. Treatment strategies based on diagnosis of a single pathogen may lead to inadvertent exposure of the undetected pathogen to antimicrobial agents, thereby contributing to generation of antimicrobial-resistant species. Second, lack of adequate data on co-infections in clinical and field settings can misdirect the field of drug and vaccine development. Pathogens such as malaria parasites, DENV, and _Orientia_ spp. have host immune-modulatory effects . Therefore, co-infections can aid or antagonize each other in terms of evading host immune responses. These interactions may have major effects on immune responses to vaccine candidates and need to be known during design of effective vaccination strategies . Third, we do not know how interactions of co-infecting pathogens lead to diverse disease outcomes affecting organ function and ultimately mortality. In India, the prevalence of malaria parasites, DENV, and CHIKV resembles the global prevalence and co-endemicity of these pathogens . Malaria infections in India are reportedly caused by _Plasmodium falciparum_ , _P. vivax, P. ovale_ , and _P. malariae_ . Several studies have also reported the occurrence of _P. vivax_ severe malaria in India as well as in Southeast Asia and South America . Our objective with this study was to define the spectrum of co-infections in patients with an AFI associated with malaria admitted to the All India Institute of Medical Sciences, New Delhi, India, a tertiary care research hospital.\n\n【2】### Materials and Methods\n\n【3】##### Study Participants and Sample Collection\n\n【4】For our prospective study, we recruited patients with an AFI (history of fever, i.e. temperature >38°C that had persisted for \\> 2 days without an identified source) from the Department of Medicine at All India Institute of Medical Sciences from July 2017 through September 2018. Every admitted consenting AFI patient was tested by PCR for all 5 _Plasmodium_ species ( _P_ . _falciparum_ , _P. vivax, P. malaria_ , _P. ovale_ , and _P. knowlesi_ ), DENV, CHIKV, _O. tsutsugamushi,_ and _Leptospira_ . The study was approved by the institute research ethics committee .\n\n【5】For each participant, we collected information about geographic location  and completed a standard questionnaire (including demographic information, history, general physical examination findings, systemic examination findings, and clinical investigation findings). To determine presumptive clinical diagnoses and treatments, we reviewed medical chart records corresponding to each participant.\n\n【6】All patient data were anonymized to protect confidentiality. Blood samples were collected and subjected to microscopy, rapid diagnostic testing, and PCR analysis for all 5 pathogens (5 species of _Plasmodium_ , DENV, CHIKV, _Leptospira,_ and _Orientia_ ). Typhoid testing was not conducted for patients with no abdominal pain or diarrhea. None of the patients recruited for this study showed indications for typhoid testing.\n\n【7】For microscopic examinations, we used peripheral blood smears (Giemsa-stained thick and thin smears) and a 3-band rapid diagnostic test kit . The rapid diagnostic test detects antigens specific to histidine-rich protein II from _P. falciparum_ and pan- _Plasmodium_ lactate dehydrogenase from _P. vivax_ , _P. malariae_ , or _P. ovale_ .\n\n【8】Patients positive for malaria by PCR were classified as having severe malaria according to World Health Organization 2015 guidelines . These guidelines define severe malaria as creatinine level \\> 3 mg/dL, bilirubin level \\> 3 mg/dL, bicarbonate level <15 mmol/L, hemoglobin level <7 g/dL for adults and <5 g/dL for children, parasite count 10%, hypoglycemia <2.2 mM, substantial bleeding, impaired consciousness, shock, prostration (defined as myalgia and arthralgia), multiple convulsions, and pulmonary edema) . The remaining patients were classified as having mild malaria.\n\n【9】##### DNA Extraction and PCR Analyses\n\n【10】From participating AFI patients, we collected 5 mL of venous blood into EDTA tubes for PCR analysis. We extracted DNA from whole blood by using a QiaAmp DNA Mini Kit  according to the manufacturer’s instructions. To detect DENV and CHIKV, we extracted RNA from TRIzol by using the isopropanol method, and we synthesized complementary DNA from RNA by using a Verso cDNA Synthesis Kit  according to the manufacturer’s recommendations. We analyzed all samples for the presence of all 5 human _Plasmodium_ species, _O. tsutsugamushi, Leptospira,_ DENV, and CHIKV. All samples were also subjected to microscopy and rapid diagnostic testing (for PfHRP2 and PvLDH genes) for malaria diagnosis. The diagnosis of DENV and its serotypes was conducted by using serotype-specific PCR primers. The presence of other infectious agents, such as _O._ _tsutsugamushi_ , _Leptospira_ , and CHIKV was detected by PCR . Randomly selected representative PCR products were subjected to Sanger sequencing to confirm species identity .\n\n【11】We categorized the types of infections or combination of infections in a person as monoinfections, mixed infections, or co-infections. Monoinfections are defined as infections with 1 species of _Plasmodium,_ mixed infections with >1 _Plasmodium_ species, and co-infections with _Plasmodium_ species and other bacterial or viral infections.\n\n【12】##### Determination of Patient Locations and Construction of Map of India\n\n【13】We were able to retrieve location data for 82 patients. We constructed a map of India based on the official maps provided by the Survey of India , as described previously . In brief, we downloaded an India map shapefile  and generated the final image by using Microsoft PowerPoint  to map each patient to their local area. In addition, the 12 patients with _P. knowlesi_ infection were asked to answer questions about time of malaria infection (as recorded in our dataset), travel outside India in 2 years preceding the malaria infection, visits from abroad by friends/relatives, and any previous malaria infections (possibility of recurrence/relapse).\n\n【14】##### Statistical Analyses\n\n【15】We recorded data on a predesigned form and managed the data in a Microsoft Excel spreadsheet and checked all entries for possible manual errors. We summarized categorical variables by frequency (%) and age as means. We used χ 2  or Fisher exact tests, or both, as appropriate, to compare frequencies between 2 groups and the Student _t_ \\-test to compare age distribution between 2 groups. We evaluated accuracy of microscopy and rapid diagnostic testing methods by using PCR as a reference for malaria diagnosis. For each of the 2 tests, we computed sensitivity, specificity, positive predictive value, negative predictive value, positive likelihood ratio, and negative likelihood ratio by using PCR as a reference. We also computed 95% CIs for each measure computed to determine the strength of association of various co-infections with malaria severity. We used bivariate and multivariate logistic regression methods to determine the odds ratio (95% CI) for each co-infection by using Stata version 15.0 statistical software . We considered p < 0.05 to be statistically significant. We created a patient baseline characteristics table by using the R version 3.4.3 package tableone . The tableone package summarizes categorical data in the form of counts and percentages and summarizes continuous data in the form of means and SDs.\n\n【16】### Results\n\n【17】##### Spectrum of Co-infections and _Plasmodium_ Mixed Species Infections in Patients with Malaria\n\n【18】We analyzed the prevalence of various co-infections and _Plasmodium_ mixed-species infections in the 66 _Plasmodium_ \\-positive samples . _P. vivax_ accounted for most (76%) (50/66) infections, whereas _P. falciparum_ accounted for 35% (23/66). _P. knowlesi_ was detected in 18% (12/66) of infections ; _P. malariae_ and _P. ovale_ were not detected in our study.\n\n【19】From the 66 _Plasmodium_ \\-positive patients, 40 (60%) samples indicated a DENV co-infection with or without other co-infecting pathogens, and 29 (44%) indicated exclusive _Plasmodium_ /DENV co-infections. _Plasmodium_ co-infections with bacteria were found for 16 (25%) patients: _Leptospira_ infections for 11 (17%) of the 66 and _O_ . _tsutsugamushi_ for 5 (8%) .\n\n【20】Mapping indicated that locations of the malaria patients in our study spanned the entire northern region of India, including the states of Rajasthan, Haryana, Punjab, Delhi, Uttar Pradesh, Bihar, and West Bengal . Patients with _P. knowlesi_ infection originated from Delhi, Haryana, Uttar Pradesh, and West Bengal. Most patients with dengue infections originated from Delhi and Uttar Pradesh. Of the 12 patients with _P. knowlesi_ infection, 5 had not traveled abroad or had direct contact with any visitors from abroad for at least 2 years before admission. No information was available for the remaining 7 patients .\n\n【21】##### Patient Baseline Characteristics\n\n【22】Detailed hematologic and biochemical parameters for all patients were retrieved from medical records . Differences between severe and mild malaria patients were found in hemoglobin levels (9.89 g/dL vs. 12.11 g/dL), hematocrit (29.93% vs. 36.45%), platelet counts (76.69 vs. 87 × 10 3  /μL), leukocyte counts (10.53 vs. 6.07) × 10 3  cells/μL), and creatinine levels (3.37 vs. 0.90). Each group contained 26 male patients; mean age for severe malaria patients was 28 years and for mild malaria patients was 32 years.\n\n【23】##### Association of Co-infecting Pathogens with Malaria Severity\n\n【24】We found that co-infection with DENV serotype 4 (DENV-4) was associated with mild malaria (adjusted odds ratio \\[aOR\\] 0.3, 95% CI 0.4–5.0), whereas infection with _Leptospira_ (aOR 1.6, 95% CI 0.4–6.8) or _O. tsutsugamushi_ (aOR 1.1, 95% CI 0.1–7.8) was associated with severe malaria. _P. vivax_ or _P. knowlesi_ monoinfection was also associated with severe malaria (aOR 2.5, 95% CI 0.9–7.2) . Other categories of _Plasmodium_ mixed-species infections did not show any strong association with malaria severity . However, the species of _Plasmodium_ may confound some of these analyses.\n\n【25】##### Relative Performance of Malaria Diagnostic Procedures\n\n【26】All 99 patients were tested for _Plasmodium_ species by microscopy (8 positive results), rapid diagnostic testing (26 positive), and PCR (66 positive) . Almost 50% of the _P. vivax_ infections escaped detection by both microscopy and rapid testing. _P._ _knowlesi_ was detectable solely by PCR. In addition, rapid diagnostic testing was able to detect only 1 of 18 _Plasmodium_ mixed-species infections . The diagnostic performance of microscopy and rapid diagnostic testing was calculated, and each was found to have poor sensitivity compared with PCR .\n\n【27】### Discussion\n\n【28】Among patients hospitalized with AFI at the All India Institute of Medical Sciences during July 2017–September 2018, the major circulating _Plasmodium_ species was _P. vivax_ and malaria/DENV co-infections predominated. A high number of severe malaria cases reported to the institute are from northern India. Among the 5 _Plasmodium_ species known to infect humans, in our study population we detected _P. falciparum_ , _P. vivax_ , and _P. knowlesi_ but found no evidence of _P. malariae_ or _P. ovale_ . Most AFI patients in this study originated from northern India across the states of Rajasthan, Haryana, Punjab, Delhi, Uttar Pradesh, Bihar, and West Bengal. The burden of co-infecting pathogens in patients with malaria was revealed by a combination of complete blood work (peripheral blood smear analysis, rapid diagnostic testing, serum renal and liver function testing) and in-depth molecular assays (PCR amplification of _Plasmodium_ species–specific genes followed by Sanger sequencing). We found a very high percentage of _Plasmodium_ /DENV co-infections in our study population. This finding can be partly attributed to the highly sensitive PCR diagnostic methods used.\n\n【29】A recent meta-analysis of the prevalence of DENV/ _Plasmodium_ /CHIKV co-infections spanning 7 geographic regions (southern Asia, Africa, Southeast Asia, South America, North America, the Caribbean, and the Middle East) showed that DENV/ _Plasmodium_ co-infections have been reported in 19 countries, including India; DENV/CHIKV co-infections have been reported in 24 countries including India; CHIKV/ _Plasmodium_ co-infections have been reported in 6 countries with only a single co-infection reported from India; and DENV/CHIKV/ _Plasmodium_ co-infections have been reported in 3 countries . According to that meta-analysis, the average reported prevalence of DENV/ _Plasmodium_ co-infection in India is ≈6.5%, which is much lower than that detected by our study. However, a more detailed analysis from the eastern India state of Odisha shows that this percentage can vary within a year, depending on season, and the highest reported prevalence of DENV/ _Plasmodium_ co-infections from this region was 31.8% during September–October, an observation similar to ours .\n\n【30】Although awareness of _Plasmodium_ /DENV co-infections is increasing, little information is available about _Plasmodium_ / _Leptospira_ or _Plasmodium_ / _O. tsutsugamushi_ co-infections . This lack of information is concerning because our study suggests that _Plasmodium_ / _Leptospira_ co-infections are associated with severe malaria. Prevalence data for co-infections with these pathogens are limited. We emphasize the need for such information because although these pathogens are carried by different vectors, they co-exist in the same geoclimactic habitats that combine a warm, moist environment with dense vegetation and poor socio-economic development . The presence of one co-infecting pathogen can influence disease outcomes, treatment outcomes, development of immunity, or drug resistance with regard to infections caused by the other co-infecting pathogen. One example is the predisposition for bacteremia to develop in persons with malaria .\n\n【31】In most malaria-endemic settings, malaria is still diagnosed by microscopic examination of Giemsa-stained peripheral blood smears and rapid diagnostic testing for parasite antigen. The rapid test is specifically designed to detect _P. falciparum_ and _P. vivax_ and is extensively used because of its speed and simplicity. For microscopy, diagnostic success depends on the skill of the technicians who observe the peripheral blood smears. We found that rapid tests and microscopy missed most of the _P. vivax_ –positive malaria cases and all _P. knowlesi_ cases and detected only 1 of 18 _Plasmodium_ mixed-species infections. This finding clearly shows the limitations of rapid testing and microscopy for comprehensive detection of malaria parasites, which have been independently observed in several studies and attributed to deletions in the HRP2 and HRP3 genes in the specific case of _P. falciparum_ infection . This problem is well recognized for healthcare workers and researchers working toward malaria elimination all over the world. Although the rapid diagnostic test for malaria has been shown to be better than microscopic examination of Giemsa-stained peripheral blood smears, PCR has been shown to be far superior to rapid testing for diagnosing low-parasitemia malaria infections . Our observations were similar; PCR was most sensitive, followed by rapid testing and then microscopy. However, rapid tests have low success rates in areas of low transmission intensities and give rise to a high proportion of false negatives . In addition, rapid tests fail to detect infections with emerging pathogens, such as the simian parasite species _P. knowlesi_ and _P. cynomolgi_ , both known to infect humans . Although recent reports highlight the improvement of rapid tests for _P. knowlesi_ detection by use of a cross-reacting pan-parasite lactate dehydrogenase feature, we were unable to detect _P. knowlesi_ by using a pan-parasite lactate dehydrogenase–containing rapid test, possibly because of low parasitemia, below the detection limit of the rapid test . _P. knowlesi_ , which was previously believed to be localized to Southeast Asia, has now been reported from various parts of the world as single case reports of travelers’ infections from areas including Oceania, Europe, and the Middle East . From India, _P. knowlesi_ infection has been reported from the Andaman and Nicobar Islands in the context of drug resistance and in a recent study by our group in the context of acute kidney injury . Historically, _P. knowlesi_ infection was discovered as a naturally occurring human infection in Malaysia in 1965 . The accurate diagnosis of _P. knowlesi_ by use of PCR took ≈40 years from its initial discovery and gave a preliminary indication of the burden of this zoonosis in Sarawak, Malaysia . Until this point, _P. knowlesi_ as a human infection was frequently misdiagnosed as _P. vivax_ , _P. malariae_ , or _P. falciparum_ infection.\n\n【32】To assess whether the infections originated locally, we surveyed the _P. knowlesi_ patients in our study group for the possibility of travel abroad or interaction with visitors from abroad within their family. The patients who responded to our questionnaire do not appear to have traveled abroad or to have had direct contact with anyone visiting them from abroad, suggesting local presence of _P. knowlesi_ . However, unknown sources of travel from Southeast Asia, a neighbor to India, cannot be ruled out. Currently, testing for _P. knowlesi_ is not included in diagnostic procedures in India, irrespective of diagnostic method (microscopy, rapid diagnostic test, or PCR), because it has not been widely reported. However, India is known to harbor both the potential vector for _P. knowlesi_ , the _Anopheles dirus_ mosquito, as well as the reservoir, pig-tailed macaques ( _Macaca nemestrina_ ), thereby making India a potential ecosystem for the proliferation of this zoonotic _Plasmodium_ species . Furthermore, the populations of _Macaca mulatta_ macaques and related species have recently expanded in northern India, particularly in the state of Uttar Pradesh, which may explain the appearance of _P. knowlesi_ in our study population representative of these areas, whereas it was not reported earlier . A recent report has also demonstrated the presence of _P. falciparum_ parasites in monkey populations from India, indicating freely occurring undetected zoonotic transfer of the malaria parasites across reservoirs and hosts. Therefore, healthcare workers and national programs should incorporate all species of malaria parasites known to infect humans, in their diagnostic portfolio.\n\n【33】In conclusion, our study clearly showed that microscopy and rapid diagnostic testing gave false-negative results for most mixed-species infections and completely missed _P. knowlesi_ infections, co-infections and mixed _Plasmodium_ infections were highly prevalent in patients with malaria, _Plasmodium_ /DENV co-infections were the most common co-occurring pathogens in our study population, _P. knowlesi_ infections were present in India, _Plasmodium_ /DENV4 co-infections were associated with mild malaria, and _Plasmodium_ / _Leptospira_ infections were associated with severe malaria. Although the ORs support the above findings, the 95% CIs for these associations were wide. CIs reflect the uncertainty of the estimated effect, and wider intervals suggest greater uncertainty. The wide 95% CIs in this study suggest that although trends were observed, additional data points are needed to determine the effect size of these associations. Wider prevalence studies investigating malaria co-infections are needed.\n\n【34】The government of India has recently declared a goal of malaria elimination by 2030, which will be a major step toward global malaria eradication because India serves as a major _Plasmodium_ reservoir, contributing to almost 4% of malaria-related deaths globally. Therefore, to make malaria elimination possible, we offer 2 recommendations based on our observations in this study, particularly for tertiary healthcare centers or centers where the burden of severe malaria cases is high. First, malaria elimination efforts will need to include strategies for malaria elimination in humans as well as animal reservoirs. Second, efforts toward the development of novel diagnostics for malaria must be renewed, and AFI diagnoses must include screening for all 5 _Plasmodium_ species, _Leptospira_ , and all 4 DENV serotypes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2d8b26b5-4121-4267-9880-27cf1c893f89", "title": "Unusual Cryptosporidium Genotypes in Human Cases of Diarrhea", "text": "【0】Unusual Cryptosporidium Genotypes in Human Cases of Diarrhea\n_Cryptosporidium_ spp. are frequently a cause of diarrheal disease in immunocompetent as well as immunocompromised humans. Over the past decade molecular methods have enabled the characterization and identification of species and genotypes within the genus. The taxonomy is under continual review, but so far 20 valid species and numerous genotypes have been described. Many are named after the original host from which the isolate was recovered and are often referred to as “host-adapted” . Most human infections are caused by _C. hominis_ or _C_ . _parvum_ but _C_ . _meleagridis_ , _C_ . _felis_ , _C_ . _canis_ , _C_ . _suis_ , _C_ . _muris_ , _C_ . _andersoni_ , _C. hominis_ monkey genotype, cervine genotype, and the chipmunk genotype I have also been detected . The immune status of the host is not necessarily linked to infection with other species/genotypes . We describe 3 unusual _Cryptosporidium_ genotypes detected in human patients with diarrhea.\n\n【1】### The Study\n\n【2】Since 2000, the UK Cryptosporidium Reference Unit has maintained a national collection of _Cryptosporidium_ oocysts . Over 16,000 _Cryptosporidium_ \\-positive human fecal samples have been submitted by primary diagnostic laboratories and characterized by the Reference Unit to identify the infecting species. In addition to the expected _C. hominis_ , _C. parvum,_ and small number of _C. meleagridis_ , _C. felis_ , _C. canis,_ and cervine genotype isolates, 3 other genotypes (skunk, horse, and rabbit) were identified in separate samples from individual patients after the onset of diarrhea in 2000 (sample W971), 2003 (sample W6863), and 2007 (sample W16103). A routinely collected minimum dataset was submitted with each sample, and further exposure data were collected for each patient from the local Consultant in Communicable Disease Control.\n\n【3】To prepare isolates for molecular characterization, oocysts were concentrated by saturated salt flotation, disrupted by boiling for 1 hour and the DNA purified by using a QIAamp DNA Mini Kit (QIAGEN, Valencia, CA, USA) as previously described . All 3 isolates were characterized by PCR–restriction fragment length polymorphism (RFLP) or bidirectional sequencing (GeneService Ltd. Cambridge, UK) at the small subunit (SSU) rRNA (≈800-bp product) , _Cryptosporidium_ oocyst wall protein (COWP) (≈550-bp product)  and heat shock protein (HSP) 70 (≈450-bp or ≈325-bp products)  genes. Sequences were compared with GenBank submissions by using the BLAST algorithm .\n\n【4】To confirm identification, phylogenetic analysis was conducted in TREECON  with other known _Cryptosporidium_ spp. and genotypes by using alignments generated in ClustalX version 2.0  and manually edited in BioEdit version 7.0.9 . All sequences generated in this study have been submitted to GenBank under accession nos. EU437411–EU437418.\n\n【5】At the SSU rRNA and HSP70 genes, sequence analysis confirmed that W971, W6863, and W16103 were skunk, horse, and rabbit genotypes, respectively . Isolate W971 was homologous with genotype W13 found in storm water, which, in turn, is the skunk genotype . Initially, the BLAST search for isolate W6863 erroneously indicated _C. parvum_ as the most probable identity at the SSU rRNA gene, but this was due to the short length (484 bp) of the only horse genotype sequence available (AY273770) for comparison. Thus, _C. parvum_ isolates that spanned our whole query sequence (787 bp) were calculated to have greater identities by BLAST. However, a detailed comparison between AY273770 and W6863 showed only 2-bp differences (including 1 insertion in our sequence) compared with 7-bp differences between W6863 and _C. parvum_ . W6863 was confirmed as a variant of the horse genotype by HSP70 gene sequence analysis and SSU rRNA gene phylogenetic analysis .\n\n【6】PCR-RFLP analysis of the SSU rRNA and COWP genes differentiated the skunk and horse genotypes from the most common human pathogens. However, identifying the rabbit genotype by PCR-RFLP at these loci was more problematic because of this genotype’s close relationship with _C. hominis_ . The sequence and restriction pattern are identical at the COWP gene and, with only 4-bp substitutions (2 occurring in _SspI_ cut-sites), the pattern is similar at the SSU rRNA gene. Increasing the resolution by running the agarose gel at an appropriate concentration and for as long as possible is important for the separation of the _C. hominis_ diagnostic band (449 bp) from the rabbit genotype (472 bp).\n\n【7】### Conclusions\n\n【8】Information on possible risk factors was collected for the 2 weeks before the onset of illness, but we cannot be sure how these 3 persons became infected with the unusual genotypes. The skunk genotype was found in a 25-year-old woman from a rural area of southwest England, who reported no foreign travel and no contact with animals. She worked and swam regularly at an adult daycare center and had spent a week during the incubation period with clients at a holiday forest park in her region. There was no information to suggest that she was immunocompromised. The horse genotype was found in a 30-year-old immunocompetent woman also from a rural area of southwest England, who reported swimming and foreign travel (destination unknown) but no contact with animals during the incubation period. The rabbit genotype was found in a 48-year-old immunocompetent woman from a rural area of northwest England, who reported foreign travel to southern Spain and contact with wild birds (feeding ducks and geese) but no contact with other animals.\n\n【9】Previously, these 3 genotypes were known to cause infections only in wild or zoo animals . Wild animals are known to be an important source of _Cryptosporidium_ oocysts in environmental samples and we have detected the rabbit genotype in surface waters and septic tank samples , but the source is unknown. Since many isolates have yet to be found in humans and although little is actually known about them, they are assumed to be insignificant to public health . The importance of unusual genotypes in humans who seek treatment for diarrheal disease warrants further investigation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9774c5b7-edb8-4dca-b53a-b17e7a0a7cae", "title": "Chikungunya in the Caribbean—Threat for Europe", "text": "【0】Chikungunya in the Caribbean—Threat for Europe\n**To the Editor:** The first evidence of chikungunya virus in the Western Hemisphere was its detection in December 2013 in the French West Indies . One month later, the virus spread to other Caribbean islands.\n\n【1】Two cases of chikungunya in siblings (an 8-year old girl and a 10-year old boy) were recently identified at Toulouse University Hospital in southwestern France. Two days after these children had returned to France from the island of Martinique (French West Indies), acute fever associated with an arthromyalgic syndrome developed in these children. The children had maculopapular, nonpruriginous rashes on their arms and legs and endobuccal petechiae. The boy had bilateral knee effusions, and the girl had a measles-like rash that became more extended. Both children also had many mosquito bites that they scratched. They were discharged on the day of their admission. These 2 cases reported in metropolitan France after the patients visited Martinique indicate rapid spread of chikungunya virus.\n\n【2】We identified the virus by sequencing a 205-nt fragment within the envelope protein E1 gene of chikungunya virus  and performing phylogenetic analyses on the basis of reference sequences. This virus was a strain from Asia , whereas virus detected in 2 children in southeastern France in September 2010 had been imported from Rajasthan, India, and was an East/Central/South Africa strain . All of these strains did not show the single amino acid substitution in the envelope protein gene (E1-A226V) that favors adaptation for dissemination by _Aedes albopictus_ mosquitoes  and would affect the potential magnitude of this outbreak.\n\n【3】_Ae_ . _aegypti_ mosquitoes are common in the Western Hemisphere, where they are the major vector of urban dengue and yellow fever, and will facilitate spread of chikungunya in this region. _Ae_ . _albopictus_ (Asian tiger mosquito) is also an efficient vector of chikungunya virus and is found in many areas, including southern Europe. This mosquito species was responsible for the extensive chikungunya outbreak on La Réunion Island in the Indian Ocean  and was involved in the first chikungunya outbreak in Italy in 2007 . In these 2 outbreaks, human and mosquito virus strains contained mutation A226V in the envelope protein gene.\n\n【4】_Ae. albopictus_ mosquitoes became established in a large area (91,150 km 2  ) of southern France in 2013, where ≈13 million persons live. This mosquito, which is highly efficient in transmitting chikungunya virus , has been present in the study area for 2 years. For these reasons, a chikungunya/dengue national control program for continental France was established in 2006. The program involves rapid virologic diagnosis of imported or suspected autochthonous cases and vector control measures. This program operates during May–November, the period when _Ae. albopictus_ mosquitoes circulate, and is based on entomologic surveillance data. The area covered by the program in 2013 was >10 times larger than that covered in 2006.\n\n【5】The presence of an effective vector, its progressive spread, and the outbreak of chikungunya in the Western Hemisphere increase concerns of a chikungunya outbreak in Europe . The greatest challenge is to find a way of interrupting the transmission chain of the virus as soon as possible. This challenge requires an effective policy of informing travelers at risk, early screening based on rapid virologic diagnosis, and effective vector control. Such control measures need an educated population to ensure emptying standing water from flowerpots, gutters, buckets, pool covers, pet water dishes, and discarded tires. They also need global antivector measures (eradication of eggs, larvae, and adults of _Aedes s_ pp. mosquitoes).\n\n【6】These measures must be extremely efficient because an outbreak of chikungunya in the Western Hemisphere could spread rapidly. All countries in southern Europe are concerned by this public health challenge, and the battle against chikungunya requires rapid establishment of a supranational organization that should be able in real time to collect and return epidemiologic, virologic, and entomologic data. Although the usual movements of tourists around southern Europe during the summer will increase the number of persons at risk in this area, an even greater threat is the international movement of >600,000 persons expected to attend the next Soccer World Cup in Brazil in 2014 .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3e27aa78-24a3-418a-85fa-50c2e28d1035", "title": "Lassa Fever in Travelers from West Africa, 1969–2016", "text": "【0】Lassa Fever in Travelers from West Africa, 1969–2016\nOriginally discovered in 1969, Lassa fever is a rodentborne viral hemorrhagic fever endemic to West Africa and caused by Lassa virus . The clinical course of Lassa fever is either not recognized or mild in 80% of patients; however, ≈20% of patients might experience severe disease, including facial swelling, hepatic and renal abnormalities, pulmonary edema, and hemorrhage. Although overall case-fatality rates for patients with Lassa fever is ≈1%, rates among hospitalized case-patients are \\> 15% . Intravenous administration of the antiviral drug ribavirin has become the standard of care for treatment of Lassa fever, but data on the efficacy of intravenous ribavirin are limited. The original study among Lassa fever patients in Sierra Leone found survival to be significantly higher (p = 0.0002) among those who obtained ribavirin within the first 6 days of illness (55%) compared with those who never received the drug (5%) . In the United States, intravenous ribavirin use is still considered investigational and may only be obtained through Emergency Investigational New Drug application to the US Food and Drug Administration .\n\n【1】Diagnosis of Lassa fever in patients arriving from West Africa might be challenging for healthcare providers unfamiliar with the spectrum of its clinical presentation, a challenge that is also common to the consideration of other viral hemorrhagic fevers in returning travelers . Additionally, although Lassa virus is not transmitted through casual contact, contact-tracing investigations of returning case-patients have often been large in scale . To quantify the frequency of case-patients having distinctive clinical features, time from patient presentation to clinical suspicion of a Lassa fever diagnosis, and the risk for secondary Lassa virus transmission, we performed a retrospective review of all 33 reported cases of Lassa fever imported from West Africa during 1969–2016.\n\n【2】### Methods\n\n【3】We searched PubMed for publications using the terms “Lassa” and “Lassa fever.” We identified additional articles by reviewing references in retrieved reports  and official correspondence by public health officials involved in these cases. We selected 74 publications discussing clinical or epidemiologic aspects of the 33 imported Lassa fever cases for review and collected information pertaining to case demographics, distinctive clinical features suggestive of Lassa fever, time from patient seeking care to clinical suspicion of Lassa fever, and number of contacts traced. We defined distinctive clinical features as fever and \\> 1 of the following: sore throat or pharyngitis, retrosternal chest pain, or proteinuria. We selected these features on the basis of the cumulative positive predictive value for fever, sore throat, retrosternal chest pain, and proteinuria for Lassa fever of 0.81 in a case–control study among 441 hospitalized patients in Sierra Leone . Although precise definitions varied between investigations, high-risk contacts were typically defined as contacts with substantial direct contact with patients or their body fluids.\n\n【4】### Findings\n\n【5】During 1969–2016, a total of 33 patients traveling from 7 West Africa countries to 9 other countries were diagnosed with Lassa fever . The median age of these patients was 45 years (range 18–72 years). Potential sources of Lassa fever exposures varied. Eleven patients were healthcare workers working in West Africa with either known or suspected exposures to Lassa fever patients; 4 patients had known exposure to rodents or history of travel to rural areas in West Africa. The only known risk factor for 18 patients was living in or traveling to West Africa. Twenty patients had illness onset during the West Africa dry season (November–April), and 10 patients had onset during the wet season (May–October); time of year for disease onset was not specified for 3 patients.\n\n【6】Twenty patients traveled to their destination on a commercial airliner; of these, 12 were symptomatic during flight. Ten patients were medically evacuated, 6 of whom had a known or suspected exposure to Lassa fever at the time of evacuation. Information on method of travel was not available for 3 patients. At the time patients sought care, medical providers were aware of travel history to West Africa for 26 (87%) of 30 patients; ascertainment of travel histories by medical providers was not described for 3 cases.\n\n【7】Of the 29 patients for whom clinical information was available , 17 (59%) had fever and \\> 1 distinctive clinical features of Lassa fever. Time from patients seeking medical care to clinical suspicion of Lassa fever by clinical providers in their destination country ranged from 1 to 22 days (median 5 days). The time from when patients sought care to patient isolation ranged from 1 to 25 days (median 7 days). We found no reports of Lassa virus PCR testing performed on any patient before 2000; however, 9 of 16 patients (56%) in 2000 and later years had a positive Lassa virus PCR test within 1–2 days of hospital admission. Of the 32 patients for whom information on isolation procedures were described, 24 patients were isolated at some point during their hospitalizations in their destination countries. Of these, 11 (34%) patients were placed in a form of isolation immediately after they sought medical care; 3 patients were transferred to biocontainment units, and the remaining 8 patients were isolated with techniques ranging from standard precautions to a combination of contact, droplet, and airborne precautions. Of the 13 patients who were isolated later in their hospital stay, 2 patients were isolated with contact and airborne precautions, and 11 were subsequently transferred to specialized hospitals with infection control capacity designed for the care of patients with highly infectious diseases. Time to isolation ranged from 3 to 15 days after hospital admission . The last 2 patients who sought care in the United States were admitted to dedicated Ebola treatment units established during the 2014–2015 West Africa Ebola epidemic. Of the 31 patients for whom outcomes were described, 12 patients died, yielding a case-fatality rate of 39%.\n\n【8】Treatment regimens were described for 23 patients. Twelve (52%) patients initially received antimalarial medications or antimicrobial drugs because of clinical suspicion of malaria or another infectious disease during their treatment course. In total, intravenous ribavirin was ordered for 7 (30%) patients. Four patients received intravenous ribavirin; 2 received a full course, and the other 2 died during treatment. Three patients had intravenous ribavirin ordered but died before receiving the medication.\n\n【9】Contact tracing investigations were either not performed or not described in the literature for 16 (48%) patients. For the remaining 17 (52%) patients, a total of 3,420 contacts were followed; the number of contacts followed per investigation ranged from 3 to 552 (median 173). Eleven contact investigations stratified contacts into high-risk and low-risk contacts, with some further separating high-risk contacts into first-line or second-line contacts . High-risk contacts were defined as having substantial exposure to patients or their body fluids, such as through direct unprotected exposure to blood or other body fluids from a case-patient. By these criteria, 139 total contacts were defined as being high-risk across 11 investigations. In 9 investigations, high-risk contacts accounted for 2%–8% of total contacts; in 2 investigations, they accounted for 40%–60% of total contacts.\n\n【10】Only 2 cases of secondary transmission of Lassa virus occurred, both in Germany. Neither of the source case-patients for these 2 patients was isolated. The first instance of transmission occurred to a physician who performed a physical examination, obtained intravenous access, and obtained blood samples from a Lassa fever patient without wearing any personal protective equipment . Because of the physician’s high-risk exposure, ribavirin prophylaxis was initiated and completed. Serologic testing was performed and yielded IgG titers of 1:320 specific to the strain of Lassa virus from the case-patient, indicating probable seroconversion in the physician. However, the physician remained asymptomatic.\n\n【11】The second instance of secondary transmission, reported in 2016, occurred in a mortician who handled the body of a healthcare worker who was evacuated from Togo to Germany and diagnosed with Lassa fever retrospectively. The mortician reported wearing 2 pairs of gloves when handling the corpse but did not wear an apron or a facial mask. The mortician reported mild upper respiratory tract symptoms before contact with the deceased patient. However, 4 days after handling the corpse, his symptoms worsened. Six days after handling the corpse, the mortician tested positive for Lassa virus by real-time reverse transcription PCR. The mortician’s clinical course was notable for fever, upper respiratory tract symptoms, and pharyngeal erythema with exudates, myalgias, and arthralgias. He received intravenous ribavirin for 10 days and oral favipiravir for 4 days, with gradual resolution of his symptoms and clinical recovery . Contacts of this secondary case-patient were followed but did not indicate any evidence of further transmission.\n\n【12】### Discussion\n\n【13】The 33 cases of imported Lassa fever that occurred during 1969–2016 posed a similar set of challenges: timely diagnosis of a rare infectious disease not endemic to the patient’s destination country, timely treatment, and prevention of Lassa virus transmission to contacts. Among patients who were not medically evacuated, the median number of days from patient presentation to clinical suspicion of Lassa fever by clinicians in the destination country was 5 days. Several factors might have contributed to this delay in diagnosis. First, patients were seen by providers in countries where Lassa fever is not endemic, requiring consideration of a travel-associated illness infrequently encountered outside of West Africa. Second, in many cases, the patients’ travel to West Africa was not known at the time they initially sought care. Third, the clinical findings of Lassa fever are variable, ranging from nonspecific symptoms, such as fever, nausea, and myalgias in the early phase, to more distinctive features later, including pharyngitis, sore throat, tonsillitis, oropharyngeal ulcers, facial and neck swelling, conjunctival injection, and proteinuria. Hemorrhage is usually seen only in a minority of cases. Although fever and \\> 1 distinctive clinical features can be suggestive of the diagnosis, they were only present in 59% of patients. In addition, of patients with a known travel history to West Africa, 12 (48%) did not demonstrate distinctive clinical features of Lassa fever. As such, providers encountering patients who have a nonspecific febrile illness after travel to West Africa should elicit a travel history and consider Lassa fever early in the differential diagnosis. Suspicion should be especially high for those patients with fever and \\> 1 of the distinctive features we have described. Although most returning travelers from West Africa with Lassa fever in 2000 or later had viremia confirmed through a positive Lassa virus test obtained within 1–2 days of admission, some patients did not have their illness diagnosed until weeks into their illness. Samples of patients with suspected Lassa fever should be obtained as early as possible and tested by Lassa virus PCR at a reference laboratory; most reference laboratories in Europe and elsewhere have demonstrated proficiency in performing Lassa virus molecular diagnostics .\n\n【14】Treatment of Lassa fever comprises effective supportive care and use of intravenous ribavirin. Although timely treatment with intravenous ribavirin depends on successful procurement of the drug, it also rests on early consideration of the diagnosis, and might even be administered before laboratory confirmation of Lassa fever diagnosis in patients with severe illness. The relative minority of case-patients who received intravenous ribavirin in our review highlights the importance of early consideration of Lassa fever in the differential diagnosis for appropriate patients.\n\n【15】Infection control was another challenge encountered by medical providers and healthcare systems caring for Lassa fever patients. The lack of appropriate use of isolation or barrier precautions in the 2 instances of secondary transmission speaks to the importance of adhering to standard precautions when caring for all patients, regardless of their diagnosis or presumed infectious status. In addition, the case of secondary transmission to the mortician in Germany illustrates the importance of maintenance of standard precautions during autopsy. Early consideration of Lassa fever as a diagnosis might also enable early institution of isolation and prevention of secondary transmission. Among those case-patients for whom a specific form of isolation was specified, most were admitted to high-security containment facilities or negative-pressure rooms with airborne precautions. Although these forms of isolation can prevent secondary transmission of Lassa virus, simple barrier or contact precautions have also been demonstrated to be safe and are less expensive and labor-intensive .\n\n【16】Contact tracing investigations frequently involved hundreds of contacts and a substantial investment of time and labor on the part of public health teams. One investigation noted that “active surveillance of contacts by public health teams was impracticable and required enormous resources, involving over 3,000 communications” . Most investigations were similarly comprehensive, involving identification and longitudinal follow-up of case-patients’ friends, family, and casual contacts, including airplane passengers, as well as numerous healthcare staff. Contacts were often separated into 2 categories: high-risk (i.e. having substantial exposure to case-patients) and low-risk (i.e. having only casual contact or proximity to case-patients). However, body temperature monitoring, home visits, and serologic testing were frequently coordinated for contacts in both high- and low-risk categories. To minimize the burden on public health systems and maximize the likelihood of successful secondary case identification, future responses should consider focusing on investigating high-risk contacts exclusively.\n\n【17】Our review had several limitations. Information on historic cases, particularly those before 1985, was incomplete and limited. In some cases, reports provided scant or no information on the physical examination or laboratory studies of patients upon admission. Reports on contact tracing provided different degrees of detail, and levels of risk assessment were variable between investigations.\n\n【18】With the ease and frequency of international travel, Lassa fever will continue to be encountered by healthcare providers in countries where Lassa fever is not endemic. Strict maintenance of standard infection control precautions in healthcare is critical for all patients and will help prevent secondary transmission of Lassa virus. Timely recognition of distinctive clinical features, earlier treatment of patients, and targeted public health responses focused on high-risk contacts will also be important components of future responses to imported cases of Lassa fever.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "afb81693-fb7a-4c50-83ab-dfc35d32ce80", "title": "New and Emerging Zoonoses", "text": "【0】New and Emerging Zoonoses\nSeventy-five percent of emerging infectious diseases are transmitted from animals to humans. The panel focused on different factors that have caused transmission from animals to humans for four diseases in recent years. The monkeypox infections in humans in the United States in 2003 were the first introduction of the disease into a human population outside Africa. This outbreak resulted from expanded global commerce and travel involving exotic rodents. Humans were infected through contact with ill pet prairie dogs, which had been housed with exotic rodents imported from Africa in April 2003. Laboratory evidence suggested that multiple species of imported rodents were infected, including rope squirrels, Gambian rats, and dormice. Through testing by classic laboratory methods and newer nucleic acid, real-time polymerase chain reaction methods, ultimately 37 human cases were identified. The disease in U.S. patients differed from that previously described in human outbreaks in Zaire/Democratic Republic of Congo. The Centers for Disease Control and Prevention and the U.S. Food and Drug Administration enacted a ban on importation of African rodents and distribution of prairie dogs in the United States to prevent additional introduction of infected animals. Zoonotic concerns remain, including whether monkeypox was transmitted to other North American mammals that may have come into contact with the imported infected rodents. Ongoing studies are focusing on the pathogenesis of infection, incubation period, length of transmissibility, and expression of disease in rodents—all which remain poorly understood.\n\n【1】Several human and animal Ebola virus outbreaks have occurred in West and Central Africa since 2001, causing 313 human cases and 264 deaths. These outbreaks have consisted of multiple simultaneous epidemics caused by different viral strains, with each epidemic resulting from humans' handling of distinct gorilla, chimpanzee, or duiker carcasses. Wildlife die-offs coincided in time and space with human Ebola outbreaks. Based on these results, Ebola virus was proposed as the cause of the rapid local collapse of these wild animal populations. Carcasses were infected by a variety of Ebola virus strains, which suggests that Ebola outbreaks in great apes resulted from multiple virus introductions from an unidentified natural reservoir host. It was proposed that outbreaks in humans could possibly be prevented or predicted by monitoring animal deaths.\n\n【2】The California sea otter, a subspecies named on federal lists of threatened species, is found only along the central coast of California. More than 40% of California sea otter deaths are attributed to infectious agents, including some more typically associated with terrestrial animal and human disease, such as _Toxoplasma gondii_ . Brain infection with _T. gondii_ has been documented to cause significant numbers of sea otter deaths in California. Growing evidence supports a land-sea connection associated with contamination of the coastal environment, and the source of infection to sea otters.\n\n【3】Domesticated cats, the terrestrial definitive hosts of _T. gondii_ , recently have been found to inhabit the coastal California landscape. From 1997 to 2001, _T. gondii_ seroprevalence was 42% (49/116), in live sea otters and 62% (66/107) in dead otters. Risk factors positively associated with _T. gondii_ seropositivity included male gender; older age; presence in Morro Bay, California; and freshwater outflow exposure. These findings illustrate pathogen pollution in the marine ecosystem and suggest that sea otters could be an indicator species for as-yet-unrecognized human health risks.\n\n【4】An outbreak of highly pathogenic avian influenza A virus subtype H7N7 occurred in the Netherlands beginning in February 2003. The Netherlands Ministry of Agriculture instituted an eradication program to control H7N7 avian influenza in poultry (30 million chickens culled; 255 farms; 20% symptomatic). An unexpectedly high number of H7N7 transmissions occurred in persons directly involved in handling infected poultry; evidence for person-to-person transmission was documented. Enhanced surveillance showed that 453 of an estimated 4,500 people thought to be exposed reported health complaints—349 reported conjunctivitis, and 67 reported other complaints. One veterinarian died. After 19 cases had been identified, all workers received mandatory influenza virus vaccination and prophylactic treatment with oseltamivir. Fifty-six percent of reported H7N7 infections arose before the vaccination and treatment program. By the time full prophylactic measures were reinforced (1 week after the first confirmation of human infection), >1,000 persons from all over the Netherlands and from abroad had been exposed. Poor compliance was observed in the use of personal protective equipment among poultry farmers and cullers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "01d1f5eb-1c3e-4e23-9f87-9e8cf38d1682", "title": "Balamuthia Amebic Encephalitis Risk, Hispanic-Americans", "text": "【0】Balamuthia Amebic Encephalitis Risk, Hispanic-Americans\n**To the Editor:** _Balamuthia mandrillaris_ , a free-living soil ameba, can cause granulomatous amebic encephalitis as well as nasopharyngeal, cutaneous, and disseminated infections in humans, nonhuman primates, and other animals. Approximately 100 published and unpublished cases of _Balamuthia_ amebic encephalitis (BAE) have been reported; most were fatal. Diagnosis of BAE is usually made at autopsy, and rarely by biopsy, in part because the amebas can be overlooked in histopathologic preparations. In recognizing BAE as a type of encephalitis that might otherwise be undiagnosed, the California Encephalitis Project  has been screening selected serum samples from patients with encephalitis for evidence of antibodies to _Balamuthia_ .\n\n【1】We describe cases of BAE in California and compare data with national data collected on _Balamuthia_ infections since the discovery of the organism in 1990. Since 1998 **,** serum and other samples (cerebrospinal fluid \\[CSF\\], throat and rectal swabs, brain tissue) from patients with encephalitis have been submitted to the California Encephalitis Project by participating physicians throughout California. The goal of the California Encephalitis Project is to provide enhanced diagnostic testing for etiologic agents of encephalitis through an intensive testing algorithm. The case definition of encephalitis is encephalopathy, plus one or more of the following: fever, seizures, focal neurologic findings, CSF pleocytosis, or electroencephalographic or neuroimaging findings consistent with encephalitis . Persons with HIV/AIDS, severel _y_ immunocompromised patients, and patients < 6 months of age are excluded from the project.\n\n【2】Serum samples were selected for screening for _Balamuthia_ antibodies if the patient had clinical or laboratory features suggestive of _Balamuthia_ encephalitis (elevated CSF protein and leukocyte counts or compatible findings on neuroimaging) and a history of outdoor occupational (agriculture or construction work) or recreational (camping or swimming) activities during which they may have been exposed to pathogenic or opportunistic free-living amebas. During the study, 215 (approximately 25%) of the >850 serum samples collected in California were tested for _Balamuthia_ infection by indirect immunofluorescence assay . Testing was conducted on acute-phase serum and a follow-up sample, when available. Serum samples were tested at dilutions from 1:2 to 1:4,096. Positive and negative control samples were run in parallel, with titers from 1:128 to 1:256 for the former and negative to 1:32 for the latter. Serum samples from patients with _Balamuthia_ encephalitis did not cross-react with _Acanthamoeba_ or _Naegleria_ , two other amebas associated with amebic encephalitis .\n\n【3】Three (1.4%) of 215 samples tested were positive for antibodies to _Balamuthia_ with titers of 1:128, 1:128, and 1:256. In the course of the study period, serum samples from four additional persons, including serum from one person who had been diagnosed by the Centers for Disease Control and Prevention (CDC), who were not part of California Encephalitis Project were positive. The diagnosis of _Balamuthia_ encephalitis was confirmed histologically or by indirect immunofluorescence staining of tissue sections in all seven cases; in one case amebas also were isolated in culture from necrotic brain tissue at autopsy . All patients were immunocompetent and of Hispanic-American ethnicity, and all died. Case-patients included two adults and three children who were native Californians, a child who had arrived from Mexico the previous year, and a child who was a native of Texas who had been diagnosed by the California Department of Health Services . The observation that all were of Hispanic-American ethnicity prompted a search through CDC’s records (n = 104) to confirm the ethnicity of BAE patients throughout the world . Patients were considered to be of Hispanic-American ethnicity if they were identified as such in case histories or if they had traditional Hispanic surnames **.** Specific confirmation of ethnicity was not available in the CDC records, and reliance on surnames to determine ethnicity might be a source of error; some Hispanic-American persons may have surnames that are not considered to be ethnically Hispanic, and vice versa. According to the records, approximately 50% of the 50 North American patients, which were confirmed by direct immunofluorescence, histopathology, or both, were Hispanic-American. Thirty-six percent of all the BAE cases occurred in Latin America. Eleven cases have occurred in California since the early 1990s, including those described above, and all but two were fatal . Eight (73%) of these 11 cases occurred in Hispanic-Americans.\n\n【4】BAE is not an insignificant disease in California, with 11 cases and 9 deaths reported in the state in the last decade. By comparison, five deaths from indigenous rabies have been reported in the state since approximately 1990 . Furthermore, BAE is likely underdiagnosed because of unfamiliarity with appearance of amebas in tissue sections and nonspecific symptoms. Unless there is a high degree of suspicion, it is unlikely that testing for _Balamuthia_ would be conducted. Most cases are diagnosed on autopsy, which is often not allowed by families. Also, BAE develops in a disproportionate number of Hispanic-Americans. Hispanic-Americans comprise 12.5% of the U. S. population (United States Census Bureau statistics for 2000) but represent approximately 50% of the cases of BAE. In California, where Hispanic-Americans make up 32% of the state’s population, they have 73% of BAE cases (p = 0.001, Fisher exact test). In the California Encephalitis Project, Hispanic-Americans accounted for approximately 25% of all cases of encephalitis, 26% of serum samples examined for _Balamuthia_ antibody, and 21% of cases of viral and bacterial encephalitis, but all BAE patients (n = 3) were in Hispanic-Americans .\n\n【5】_Balamuthia_ lives in soil  and can enter through the respiratory tract or breaks in the skin. Hispanic-Americans may be more likely to reside in agrarian settings with increased exposure to soil and opportunities for contamination of cuts and other injuries. Whether caused by environmental factors, genetic predisposition, access to medical care, or other socioeconomic factors and pressures, the reasons for the higher incidence of BAE in Hispanic-Americans warrant further study.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d68167d1-6fb2-4dfa-8c95-3d40e39f1828", "title": "Obesogenic Environments: Access to and Advertising of Sugar-Sweetened Beverages in Soweto, South Africa, 2013", "text": "【0】Obesogenic Environments: Access to and Advertising of Sugar-Sweetened Beverages in Soweto, South Africa, 2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Rates of obesity and overweight among South Africans are increasing. Food marketing has a profound impact on children and affects their lifelong eating patterns; in urban areas of South Africa, disposable incomes are growing and ultra-processed food is increasingly available at low cost. The combination of these factors will strain an already fragile health system. Our aim was to investigate the density of outdoor sugar sweetened beverage (SSB) advertising and the number of formal and informal vendors selling SSBs in a transforming, historically disadvantaged urban setting of South Africa.\n\n【3】**Methods**\n\n【4】A digital camera and global positioning system navigation system were used to record the location of SSB advertisements and food vendors in a demarcated area in Soweto. Data were collected by walking or driving through each street; a food inventory was completed for every food vendor. Spatial analyses were conducted using a geographic information system.\n\n【5】**Results**\n\n【6】A total of 145 advertisements for SSBs were found over a driven or walked distance of 111.9 km. The density of advertisements was 3.6 per km 2  in relation to schools, and 50% of schools had branded advertising of SSBs on their school property. Most (n = 104; 58%) of the 180 vendors in the study sold SSBs.\n\n【7】**Conclusion**\n\n【8】This is the first study in South Africa to document the location of billboard advertisements and vendors in relation to schools. Marketing of products that contribute to obesity is common in urban Soweto. Our findings have implications for policies that regulate SSB advertising, especially in the proximity of schools.\n\n【9】Introduction\n------------\n\n【10】Non-communicable diseases (NCDs) will be the leading cause of death on the African continent by 2030 . Between 1992 and 2005, obesity prevalence increased by 35% in sub-Saharan Africa . In 2012, South Africa had an obesity prevalence of 39.2% among females and 10.6% among males . The level of obesity among adolescent girls has increased significantly; 25% of female adolescents are overweight or obese .\n\n【11】Most South Africans have poor dietary habits, many of which start during childhood; black South Africans have the lowest dietary diversity of all South Africans and a higher-than-average sugar intake . Analysis of sugar sweetened beverage (SSB) consumption in Soweto indicated that adolescents consume between 1.1 and 1.4 servings of SSBs daily . This amount translates into 10 to 12 teaspoons per day, which exceeds the proposed World Health Organization daily recommendation of 6 teaspoons of sugar per day . This high sugar intake from a single source significantly increases the risk of developing obesity-related NCDs, especially type 2 diabetes . Furthermore, sugar has been implicated as a contributor to obesity .\n\n【12】A combination of rising incomes and discretionary spending, coupled with marketing, advertising, and availability of high-energy, processed food and beverages, the biggest source of added sugar, play a role in fostering this trend . The increasing consumption of processed products is linked to commercial advertising and their greater availability. Marketing by the food and beverage industry strongly influences long-term food and beverage preferences, and its success relies on children’s brand recognition and subsequent preference for familiar brand foods .\n\n【13】Neighborhoods of various socioeconomic statuses have different levels of exposure and intensity to advertisements of ultra-processed food and beverage products . The effect of advertising on creating and promoting an obesogenic environment has been demonstrated in the United States, where a 30% increase in food advertising resulted in increased obesity levels, and every 10% increase in the number of fast food advertisements was accompanied by a 6% increase in the consumption of SSBs . The higher density of advertisements of unhealthy foods in low-income areas was accompanied by an absence of exposure to goods and activities that promote healthier lifestyles .\n\n【14】In addition to selling SSBs, street vendors in South Africa sell other high-sugar–content items, such as candy and deep-fried doughnuts known as “vetkoek” . Other easily available street foods include burgers, deep-fried potato chips, and “kotas” (a quarter loaf of bread with a combination of deep fried chips, cheese, and meat fillings) . Advertising and access to obesity-promoting beverages and street foods contribute to obesity in South Africa .\n\n【15】Limited data are available on the density of outdoor advertising and vendors in South Africa. This study examined 2 aspects of the obesogenic environment in an urban setting in South Africa by exploring the frequency and location of outdoor advertising for SSBs and the proportion of food vendors selling SSBs. The goal was to understand how to best craft advocacy activities that limit the promotion of unhealthy products, particularly in settings in close proximity to schools.\n\n【16】Methods\n-------\n\n【17】Soweto is a historically disadvantaged area of Johannesburg that covers more than 200 square kilometers and has a population of 1.3 million. There are 1,776 households and 6,357 inhabitants per square kilometer . During the past decade, Soweto transformed economically; by 2013, four large shopping malls and several fast-food chains entered the market. This study covered 5 areas in Soweto: Klipspruit West, Mofolo South, Dube, Meadowlands, and Orlando East. During July and August 2013, data on all outdoor SSB advertising and SSB branding in a 38.3 km 2  area were collected.\n\n【18】Ethics approval was granted by the University of the Witwatersrand Ethics Committee. The study did not include human participants, and both the advertisements of SSBs and the food and beverages being sold by vendors were in the public domain.\n\n【19】Data were collected by 3 trained research assistants who either walked or drove through each street in the study area. SSB advertisements and food vendors were identified, and data were collected using 2 separate data coding sheets. In addition to collecting information on the location (global positioning systems \\[GPS\\] coordinates were determined using a Garmin Nuvi 30, Garmin Ltd.), type, and size of the advertisement, a digital photograph was taken using a Sony Cybershot DSC-W270 12.1 megapixel camera (Sony Corporation). The study team completed an inventory of all food and drink items for sale from vendors. The type of vendor was noted as informal (ie, temporary building structure), formal (ie, permanent building structure with limited resources, for example, no access to electricity or refrigeration), or a shop (ie, permanent structure with access to electricity and refrigeration), and an inventory of beverages sold at these vendors was recorded.\n\n【20】For the purpose of this study, outdoor advertising was defined as billboards, bus stop advertisements, signs placed along the sidewalk, urban art on streets or buildings, large posters, and signage for restaurants or food vendors. Items excluded were branded clothing, packaging, and taxis and buses (ie, moving targets). Advertisements of SSBs and a combination of SSBs and fast foods were included in the analysis.\n\n【21】Researchers were unable to measure the size of the advertisements, so an estimation of each advertisement size was made. Advertisements were classified as small if their dimensions were less than approximately 70 by 40 centimeters or, in terms of paper and cardboard sizes, between A4 (8.3 × 11.7 in) and A3 (11.7 × 16.5 in). Medium advertisements had dimensions of A0 (33.1 ×46.6 in) and large advertisements had dimensions that were measured in meters (eg, billboards).\n\n【22】The GPS coordinates of SSB advertisements and vendors in the study area were used to create distinct spatial point pattern objects in the R library Spatstat . A point pattern that consisted of all the advertisements and vendors in the study area was also created. Using Spatstat, the intensity (number of points per km) of each point pattern was computed. The intensity of each point pattern formed the outcome variables in the spatial point pattern analysis. To assess the association between the point patterns and the distribution of schools in the study, a covariate that measured distance from any given point in the study region to the nearest school was created using the distmap function in Spatstat. We assessed the association between the intensity of each point pattern and the covariates of interest using the Kolmogorov–Smirnov test of goodness-of-fit .\n\n【23】Homogeneous and inhomogeneous Poisson models were fitted using the R library Spatstat , and the Akaike Information Criterion was used as a basis for selecting the best fitting model . In each model, the distance to the nearest school ( _z_ ) was the primary covariate considered. Models 1, 2, and 3 were best-fitting inhomogeneous Poisson models that were fitted to the SSB, vendor, and both SSB and vendor point patterns, respectively. A fourth model (Model 4) was an inhomogeneous Poisson model with marks labeled as SSB and vendors.\n\n【24】Results\n-------\n\n【25】In total, 145 advertisements for SSBs were identified . More than half (53%) of SSB advertisements were found outside houses. Many informal vendors operated stalls from their homes. Nearly two-thirds (62%) of branded SSB advertisements were part of a display sign for a shop name, including branded signs for tuckshops (ie, small shops located in or near a school that sell snacks, candies, beverages, and food items that target children) found outside houses. Half of the primary and high schools (14 of 28) in the sample area displayed advertisements of SSBs on school premises, and 13 of these were branded school signs.\n\n【26】A total of 180 vendors were included in the study area; 27% were informal fast-food outlets, 12% were formal outlets, and 61% were shops. More than 85% of shops stocked SSBs . Formal and informal vendors both supplied fast food, although few of these vendors stocked SSBs because of lack of refrigeration.\n\n【27】The findings from the spatial analysis described the intensity of SSB advertisements and vendors in relation to schools. The intensity of the SSB point patterns in the study area of 38.3 km 2  was 3.58 points per square kilometer . Figure 1 depicts the density of SSB advertisements and their distances to schools and vendors. The figures indicate 2 school clusters in the northwestern and southeastern parts of the study area. The vendor and SSB advertisements identified were distributed around school “hotspots” .\n\n【28】Table 3 presents the intensity of the SSB and vendor point patterns against the distance to the nearest school. Intensity of SSB and vendor point patterns increases with proximity to the nearest school. The results of the Kolmogorov–Smirnov test indicated that the SSB ( _D_ \\= 0.98, _P_ < .001) and vendor ( _D_ \\= 0.43, _P_ < .001) point patterns were dependent on the distance to the nearest school. Homogenous and inhomogeneous Poisson models were fitted to the SSB advertisements and vendor point patterns. Results indicated an increase in the intensity of SSB advertisements (risk interval \\[RI\\] = −2.17, 95% confidence interval \\[CI\\] = −2.62 to −1.72) and the vendor (RI = −2.08, 95% CI = −2.48 to −1.82) point patterns with decreasing proximity to the nearest school. Approximately each square kilometer contained 1 primary or high school, 4 SSB advertisements, and 5 vendors, 3 of which sold SSBs; the most frequent advertisements were for 1 beverage company.\n\n【29】Discussion\n----------\n\n【30】Findings from this study indicate that vendors selling both SSBs and advertisements for SSBs are located in close proximity to primary and high schools in Soweto and that this placement is not random. Approximately each square kilometer contained 1 primary or high school, 4 SSB advertisements, and 5 vendors, 3 of which sold SSBs. The most frequent advertisements were for 1 beverage company.\n\n【31】This is the first study of its kind in one of the most densely populated urban areas in South Africa in the process of economic transition. Findings provide an understanding of the obesogenic environment by determining the geospatial intensity and distribution of SSB advertising, as well as the availability of ultra-processed foods and beverages. By strategically positioning advertisements in schoolyards or in close proximity to schools, children are being targeted. In another South African–based study, conducted in Western Cape schools, more than 60% of schools had a branded food or beverage advertisement board displaying the school name . Principals of these schools indicated that they had not received any monetary or program support from the sponsoring food and beverage companies, but those advertisements send an implicit message to students and the community .\n\n【32】A similar study in Sydney, Australia, found that the most frequent outdoor advertisements in close proximity to schools were for SSBs and alcoholic beverages; 24% of the total number of food advertisements located around these primary schools promoted SSBs . In New Zealand, 22% of the outdoor advertisements in close proximity to secondary schools were for SSBs . The frequency of these marketing messages influences social norms and promotes the perception that calorie-dense, nutrient-poor beverages and food products are normal .\n\n【33】To ensure the development of healthy dietary practices, especially in transforming low- and middle-income households in urban areas of South Africa, resources and efforts should be directed toward preventing obesity in these communities and understanding the causes of social determinants of obesity at a population level . Policy makers should consider developing mandatory regulations that target advertising in and around schools . This type of regulation has a precedent in the example of legislation to restrict tobacco advertising . However, an unintended consequence of restrictions placed on outdoor advertising may encourage a switch by industry to other modalities of advertising, such as television or social media .\n\n【34】Preventing obesity cannot be solved with a single solution, and managing the obesity epidemic requires efforts at both the population and individual levels . The South African Department of Health’s National Strategic Plan for Non-Communicable Diseases 2013–2017 calls for intersectoral and multidisciplinary action . Unhealthy product promotion should be limited and substituted with the sponsorship of healthy choices . A package of interventions directed toward making the environment healthier and making healthy eating a norm is needed . One of these interventions includes a ban on advertising ultra-processed products during television viewing hours for children. Others might involve regulating food and beverages in school vending machines. Finally, tuckshops could ensure that healthy, balanced school lunches are provided .\n\n【35】People feel most empowered when they make their own decisions. Children in particular are disempowered and are unable to negotiate the advertising content to which they are exposed . Ideally, prohibiting advertising of SSBs to children should be voluntary. If this does not occur in a short period, government should consider setting mandatory standards for the marketing of beverages and food to children and adolescents, as is already occurring in many world settings. The South African government released a set of draft guidelines on the labeling and advertising of food and beverages to children in 2014. Our research supplies data that will provide policy makers with evidence as they move forward . Given the growing burden of obesity in South Africa and the challenges of losing weight after adolescence, establishing these standards is now a matter of urgency .\n\n【36】A limitation of this study was that it focused on exploring the density and nature of outdoor advertising and did not account for other forms of advertising exposure, such as television, radio, telephone messaging, and print media. Further research is needed to develop a comprehensive picture of the exposure of children to advertising in other formal and informal settings and in rural and urban settings. In addition, the study determined the geospatial intensity and distribution of advertising and availability of SSBs but did not determine a causal relationship between these factors and the prevalence of obesity in Soweto. However, other studies have identified the effect of food availability and advertising on consumption patterns. Our findings have implications for policies that regulate SSB advertising, especially in the proximity of schools.\n\n【37】Tables\n------\n\n【38】#####  Table 1. Size, Location, and Format of Sugar-Sweetened Beverage (SSB) Advertisements (N = 145), Soweto, South Africa,\n\n| Characteristic of advertisement | n (%) |\n| --- | --- |\n| **Location** | **Location** |\n| House | 77 (53.1) |\n| Small shopping centre | 23 (15.9) |\n| Street/side of road | 20 (13.8) |\n| Primary or high school | 13 (9.0) |\n| Other building | 7 (4.8) |\n| Shebeen | 3 (2.1) |\n| Crèche/preschool | 1 (0.7) |\n| Transport hub/taxi stand | 1 (0.7) |\n| **Format** | **Format** |\n| Shop sign | 79 (54.5) |\n| Poster | 24 (16.6) |\n| Painted advertisement | 15 (10.3) |\n| School sign | 12 (8.3) |\n| Bus stop | 6 (4.1) |\n| Banner | 3 (2.1) |\n| Pole sign | 3 (2.1) |\n| Billboard | 2 (1.4) |\n| Umbrella/refrigerator | 1 (0.7) |\n| **Size** | **Size** |\n| Small | 11 (7.6) |\n| Medium | 94 (64.8) |\n| Large | 37 (25.5) |\n| Large billboard | 3 (2.1) |\n\n【40】#####  Table 2. Vendors (N = 180) That Sold Sugar-Sweetened Beverages (SSBs), Diet SSBs, Fruit Juice, and Milk, by Vendor Type, Soweto, South Africa,\n\n| Vendor Type | No. of Vendors | No. of Vendors That Sold Beverage Type |\n| --- | --- | --- |\n| SSBs | Diet SSBs | Fruit Juice | Milk |\n| --- | --- | --- | --- |\n| Informal (housed in a nonpermanent structure) | 49 | 1 | 0 | 0 | 0 |\n| Formal (housed in permanent structure with no access to electricity/refrigeration \\[eg, spaza shop\\]) | 22 | 3 | 0 | 0 | 0 |\n| Shops (housed in a permanent structure with access to electricity/refrigeration) | 109 | 100 | 40 | 48 | 74 |\n| Percentage of vendors that sold the item | NA | 58 | 22 | 27 | 41 |\n\n【42】#####  Table 3. Intensity of Advertisements (Points/km 2  ) Versus No. of Schools, Sugar-Sweetened Beverage (SSB) Advertisements, and Vendors in a Sample Area of 38.3 km 2  , Soweto, South Africa,\n\n| Type of Pattern | Point Type | No. of Points | Intensity (Points/km 2 ) |\n| --- | --- | --- | --- |\n| Point | SSB | 137 | 3.58 |\n| Covariate | School | 28 | 0.73 |\n| Covariate | Vendor | 184 | 4.81 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "76ee7534-1869-4367-b73f-f3610a50a8c1", "title": "Human Papillomavirus-Related Cancers Among People Living With AIDS in Puerto Rico", "text": "【0】Human Papillomavirus-Related Cancers Among People Living With AIDS in Puerto Rico\nAbstract\n--------\n\n【1】The objective of this study was to estimate the incidence of cancer and human papillomavirus (HPV)–related cancers and the risk of death (by cancer status) among people living with AIDS (PLWA) in Puerto Rico. We used data from the Puerto Rico AIDS Surveillance Program and Central Cancer Registry . Cancers with highest incidence were cervix (299.6/100,000) for women and oral cavity/oropharynx for men (150.0/100,000); the greatest excess of cancer incidence for men (standardized incidence ratio, 86.8) and women (standardized incidence ratio, 52.8) was for anal cancer. PLWA who developed a cancer had decreased survival and increased risk of death compared with those who did not have cancer. Cancer control strategies for PLWA will be essential for improving their disease survival.\n\n【2】Objective\n---------\n\n【3】Human papilloma virus (HPV) infections and HPV-related cancers are more common in people living with AIDS (PLWA) than in the general population . Although the incidence of cancer has diminished with the advent of highly active antiretroviral therapy (HAART), it has not diminished for certain HPV-related cancers . HPV-related malignancies have a distinct etiology, characterized by epithelial damage induced by persistent infection . Puerto Rico has a high burden of HPV-related cancers and HIV/AIDS . The objective of this study was to estimate the incidence of cancer and HPV-related cancers and the risk of death (by cancer status) among PLWA in Puerto Rico and compare these statistics with those in the general Puerto Rican population.\n\n【4】Methods\n-------\n\n【5】This study was approved by the institutional review board of the University of Puerto Rico Medical Sciences Campus in October 2010. We linked data from the Puerto Rico AIDS Surveillance Program and the Puerto Rico Central Cancer Registry by using Link Plus version 2.0 software (Centers for Disease Control and Prevention, Atlanta, Georgia) to describe the cancer profile of PLWA (aged ≥15 y) who were diagnosed with cancer from January 1, 1985, through December 31, 2005. We limited our study to invasive primary cancers diagnosed 3 months after an AIDS diagnosis . Overall, 29,806 cases met our inclusion criteria; we established 3 categories of cancer status: no cancer (n = 29,065), non-HPV–related cancer (n = 672), and HPV-related cancer (n = 69).\n\n【6】We included the following HPV-related cancers: cancers of the cervix, vulva/vagina, penis, anus, and oral cavity/oropharynx ; a subanalysis considered only HPV-related histology . We grouped cases according to period of AIDS diagnosis: 1985–1995 (Pre-HAART) and 1996–2005 (HAART). Using χ 2  tests, we compared the demographics of the study population by cancer status. The follow-up period of cancers among PLWA was until the date of death or December 31, 2008 (whichever occurred first). For the cancer risk analysis, we considered first and subsequent malignancies. The standardized incidence ratio (SIR) was estimated by using the indirect method and was defined as the observed cancer incidence divided by the expected cancer incidence based on Puerto Rico population rates  . SIR values were estimated by period of AIDS diagnosis, sex, and cancer status. We also measured the median survival time of PLWA to describe survival by cancer status and period of AIDS diagnosis. To assess the risk of death we estimated the hazard ratio (HR) of death with 95% confidence intervals (CIs) by using the Cox proportional hazards model, stratified by sex and period of AIDS diagnosis. Cases lost to follow-up and those alive at December 31, 2008, were censored. The proportional hazards assumption of the Cox model was tested and validated and an interaction assessment was performed. We used Stata 12.0 (Stata Corp, College Station, Texas) for the statistical analysis.\n\n【7】Results\n-------\n\n【8】The distribution of PLWA varied by sex, age, mode of HIV exposure, and period of AIDS diagnosis . The proportion of women who had an HPV-related cancer was larger than the proportion of women who had a non-HPV–related cancer or no cancer; we found similar results for PLWA whose HIV was transmitted heterosexually.\n\n【9】The highest incidences were for cervical cancer (299.6/100,000) among women and for oral cavity/oropharyngeal cancers (150.0/100,000) among men; anal cancer was the second leading cancer among both sexes. We found an excess of cancer incidence (overall, HPV-related, and non-HPV–related) among PLWA during both periods of AIDS diagnosis. Among HPV-related cancers, the greatest excess of incidence was for anal cancer among men (SIR = 86.8; 95% CI, 51.5–137.2) and women (SIR = 52.8; 95% CI, 10.9–154.3). We observed similar patterns in both time periods and for certain HPV-related histologies .\n\n【10】Overall, the median follow-up time varied by cancer status and period of AIDS diagnosis; we found longer survival times during 1996–2005 and among PLWA with no cancer (1985–1995, 2.1 y; 1996–2005, 7.5 y) than those with an HPV-related cancer (1985–1995, 0.8 y; 1996–2005, 2.6 y) or a non-HPV–related cancer (1985–1995, 0.6 y; 1996–2005, 0.7 y) (Wilcoxon _P_ <.001). Cox models (HR \\[95% CI\\]) adjusted by age at AIDS diagnosis showed that among men and women, those diagnosed with a non-HPV–related cancer had a higher risk of death than those with no cancer:\n\n| **Sex** | **1985–1995** | **1996–2005** |\n| --- | --- | --- |\n| **Men** | 1.40 (1.24–1.58) | 1.95 (1.61–2.36) |\n| **Women** | 1.85 (1.32–2.61) | 2.31 (1.59–3.35) |\n\n【12】Although no excess risk of death was observed for women with HPV-related cancers compared with those who had no cancer, men diagnosed with these cancers had a higher risk of death than those who had no cancer (HR  = 1.27; 95% CI, 0.81–2.00 and HR  = 1.32; 95% CI, 0.71–2.46); however, these risk excesses were not significant ( _P_ \\> .05).\n\n【13】Discussion\n----------\n\n【14】Our study updates information on the cancer burden among PLWA in Puerto Rico with a focus on HPV-related cancers and presents the first statistics on cancer survival and risk of death for this group. Consistent with studies worldwide and in Puerto Rico , the burden of cancer  and HPV-related cancers  was higher among PLWA than among the general population. Although comparisons should be made cautiously because of the different methods used by these studies, our study suggests higher excess incidence of cancer and HPV-related cancers among PLWA in Puerto Rico than in other populations .\n\n【15】In both periods of diagnosis, the highest excess incidence for cancer was for anal cancer. This result highlights the need for anal cancer screening among PLWA, although further research on this area is warranted . Given the lack of guidelines on anal cancer screening, clinical trials that determine the effectiveness of the Papanicolaou test for anal cancer prevention are needed . HPV vaccination  should be promoted in Puerto Rico, where vaccine uptake is low . Young PLWA should be targeted in vaccination efforts, although additional studies of vaccine efficacy among PLWA are needed .\n\n【16】We also documented decreased survival and increased risk of death (significant only for non-HPV–related cancers) among PLWA who developed a cancer compared with those who did not. Our study supports the importance of strengthening cancer screening and providing access to care among PLWA to decrease the incidence of cancer and improve survival and quality of life. Although the small number of HPV-related cancers among PLWA reduces the precision of our estimations, we conclude that PLWA in Puerto Rico have a greater burden of cancer than the general population, and this burden has a negative impact on survival. Further research and cancer prevention and control strategies are needed to reduce health disparities among PLWA in Puerto Rico. The cancer and HIV/AIDS surveillance systems should collaborate in cancer surveillance among PLWA for disease monitoring and intervention assessment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "59839ca8-05fe-4d8a-b22a-7666135ec1a8", "title": "Operating Protocols of a Community Treatment Center for Isolation of Patients with Coronavirus Disease, South Korea", "text": "【0】Operating Protocols of a Community Treatment Center for Isolation of Patients with Coronavirus Disease, South Korea\nSince the first suspected case was reported in December 2019 , the number of coronavirus disease (COVID-19) cases has risen steeply worldwide . In South Korea, COVID-19 outbreaks occurred at religious facilities and the number of cases increased drastically, especially in Daegu City and the North Gyeongsang Province, and the number of patients with asymptomatic or mild symptoms increased exponentially . In the early stages of the COVID-19 epidemic, all patients with diagnosed COVID-19 were hospitalized in negative-pressure isolation units to treat the disease and prevent the spread of infection. However, because the infection spread rapidly, the number of patients exceeded the number of available negative-pressure isolation beds. Because of limited medical resources and the COVID-19 epidemic curve, concerns grew that new facilities would be needed to isolate and care for patients in South Korea.\n\n【1】The National Health Insurance System (NHIS) of South Korea offers complete access to healthcare for the entire population . South Korea’s medical utilization rate is the highest, 16.6 outpatient visits per capita per year, among Organization for Economic Cooperation and Development (OECD) countries . Citizens of South Korea have high access to medical services. Before the COVID-19 pandemic, no one in the country anticipated a situation in which hospital admission would be denied. South Korea has 2.6 times more hospital beds than other OECD countries, 12.3/1,000 population. However, the country only has 1,027 negative-pressure isolation beds, and these are not distributed across all regions. When the COVID-19 pandemic reached South Korea, the number of available negative-pressure isolation beds decreased, and patients could not be admitted to the hospital because of the shortage of medical facilities, especially in regions where outbreaks mainly occurred.\n\n【2】When an imbalance between the demand and supply of medical resources exists, adequate triage of patients is critical for allocating limited resources to patients who can benefit the most . In a large-scale study from China, Wu et al. suggested that ≈80% of COVID-19 symptomatic patients were reported to have mild upper respiratory infection without hypoxia, and only 20% of infected patients needed medical services. Until March 25, 2020, the crude mortality rate in South Korea was 1.4%, and estimates suggested the severity of COVID-19 in the country would not be high . However, considering asymptomatic carrier transmission , the high reproductive number (R 0  \\= 2.2) , and the possibility of sudden deterioration , even patients with mild or no symptoms should be isolated and monitored.\n\n【3】On March 2, 2020, the government of South Korea started operating community treatment centers (CTCs) to provide quarantine, regular examination, and monitoring for asymptomatic and mildly symptomatic patients with laboratory-confirmed COVID-19. By March 25, a total of 17 CTCs were serving patients with mild symptoms nationwide. The CTC is designed to monitor and isolate patients with mild conditions during emerging infectious disease outbreaks. We describe the structure and operating protocols of a CTC operated by Seoul National University Hospital (SNUH) during the COVID-19 pandemic.\n\n【4】### Materials and Methods\n\n【5】##### Study Setting in the SNUH-CTC\n\n【6】SNUH is a teaching hospital with 1,700 beds. The Mungyeong Human Resource Development (HRD) Center, 153 km from SNUH, is a 7-story, 100-room facility with accommodations that is normally used for training SNUH staff. SNUH converted Mungyeong HRD to a CTC in cooperation with the government’s CTC operating policy and established a monitoring center inside SNUH. SNUH-CTC began operating on March 5, 2020 as the third CTC in the country.\n\n【7】##### Admission and Discharge Criteria\n\n【8】##### Screening Criteria for Patients for CTC\n\n【9】The Korea Centers for Disease Control and Prevention (KCDC) classified the severity of COVID-19 into very severe, severe, mild, and asymptomatic  . Mild COVID-19 is defined as alert and meeting \\> 1 of the following conditions: <50 years old, \\> 1 underlying conditions, and temperature <38°C with antipyretic drugs. Asymptomatic is defined as a patient who is alert, <50 years old, has no underlying disease, is a nonsmoker, and has a temperature of <37.5°C without antipyretic drugs. Patients classified as severe or very severe were admitted to hospitals; CTCs only accepted patients classified as having mild or asymptomatic COVID-19.\n\n【10】Patients with mild COVID-19 met \\> 1 of the following criteria for CTC admission: they did not necessarily require hospitalization; they only required monitoring; they were unable to properly self-isolate (for instance, they had no suitable place to live or lived with persons in a high-risk group); or, as determined by local government, they needed to be admitted to a CTC. Medical staff assessed patients and excluded persons at high-risk for deterioration from CTCs and recommended hospitalization.\n\n【11】##### Criteria for Discharge from the CTC\n\n【12】KCDC has 2 criteria for releasing patients from quarantine. Symptomatic patients can be discharged if symptoms disappear and they have negative results on 2 reverse-transcription PCR (RT-PCR) tests \\> 24 hours apart. KCDC recommended PCR amplification of the viral E gene as a screening test and amplification of the RdRp region of the open reading frame 1b gene as a confirmatory test. RT-PCR is considered positive only when all the genes are detected, based on the opinions of experts who detected weak and nonspecific amplification in the clinical specimens of patients who received negative results. Asymptomatic patients can be discharged if they have 2 negative RT-PCR tests \\> 24 hours apart within 7 days of diagnosis.\n\n【13】We conducted our study in accordance with the World Health Association’s Declaration of Helsinki . The study was approved by the institutional review board of SNUH .\n\n【14】### Results\n\n【15】##### Overall Structure\n\n【16】SNUH-CTC consisted of 2 centers: the patient center in Mungyeong HRD, where patients were admitted, and the monitoring center in Seoul at SNUH, where medical staff provided video consultation services . In the patient center, personnel from the Ministry of Health and Welfare, local government, hospitals, military, police, and fire agencies stayed and provided various services necessary for the operation of the CTC .\n\n【17】##### Patient Center\n\n【18】We divided the Mungyeong HRD Center into a clean area, in which medical and operating staff worked, and a contaminated area, where patients lived. Each area had a designated entrance separate from the other. We designated an area between the clean and contaminated areas as a gray zone in which personnel could remove personal protective gear or perform other required activities, such as collecting patient samples or removing waste .\n\n【19】Mungyeong HRD Center had internet service and SNUH installed an additional network to access the hospital’s electronic medical record (EMR) system. Supplies for conducting RT-PCR tests, such as swabs and a refrigerator, and a mobile radiography bus were placed next to the building. According to the national guidelines for COVID-19, physicians collected RT-PCR samples on a 2-day cycle for negative cases and on a 3- or 7-day cycle for positive cases and sent the samples to SNUH to be tested.\n\n【20】To detect pneumonia early, patients with abnormal findings on chest imaging had daily chest radiographs until normalization; patients without abnormal findings had chest radiographs every 3 days. Radiographs were read by a radiologist in SNUH through the picture archiving and communication system. RT-PCR and radiographic results could be checked in the Mungyeong HRD and the SNUH monitoring center through the EMR system. Essential medicines, such as antipyretic drugs and cough medicines, were stored in the Mungyeong HRD and provided by a physician’s prescription.\n\n【21】A physician or nurse was always on duty in the Mungyeong HRD patient center to respond to emergencies. The physicians came from many specialties, including emergency medicine, family medicine, and general surgery, to address various patient conditions and emergencies. During the day, 2 physicians and 2 nurses were on duty; at night 1 physician and 2 nurses were on duty. During each shift, 1 physician acted as the medical director, and 1 nurse acted as an infection manager. All staff, including medical staff, were checked twice a day for fever and respiratory symptoms, such as cough, sputum, stuffy nose, sore throat, chest discomfort, and dyspnea. When staff reported symptoms, the medical director checked the staff member and provided a RT-PCR test if necessary.\n\n【22】Each room of the Mungyeong HRD patient center was equipped with an automatic blood pressure monitor, digital thermometer, and pulse oximeter so that patients could check vital signs independently. Meals were provided three times a day, and laundry was done by patients in their rooms. Most patients were not permitted to have visitors, but children could be visited by parents or guardians.\n\n【23】##### Monitoring Center\n\n【24】The monitoring center at SNUH was equipped with computers and monitors, smartphone devices, webcams, headsets for video consultation, and 2 large dashboard monitors to check the patients’ vital signs and symptoms. Patients admitted to the CTC checked their blood pressure, body temperature, pulse, respiratory rate, and oxygen saturation at 9 am and 4:30 pm each day. Patients reported their symptoms, including respiratory symptoms, twice a day through a questionnaire sent through a smartphone application, and the nurse on duty monitored the responses and vital signs. The nurse provided video consultations twice a day from 9 am –12 pm and 5–8 pm. If the nurse decided that video consultation with a doctor was necessary, the doctor provided additional consultation. The doctor regularly monitored patients’ vital signs and symptoms once a day and conducted regular video consultations once every 2 days. On average, nurses and doctors provided video consultations for ≈5 minutes per patient per consult and monitored patients’ symptoms and vital signs for ≈3 minutes per patient monitoring session .\n\n【25】Radiologists at SNUH read and provided results for patients’ chest radiographs. When patients had abnormal radiography findings or the patient’s symptoms worsened, the physician at Mungyeong HRD Center consulted with an infectious disease specialist at SNUH.\n\n【26】Patients in the CTC underwent a comprehensive psychiatric assessment once a week to evaluate for depressive mood, anxiety, risk for suicide, and posttraumatic stress. The questionnaire included a standard depression module, a generalized anxiety disorder assessment, suicidality screening, a posttraumatic stress disorder checklist, and somatic symptom assessment. For high-risk groups, psychiatrists conducted a separate in-depth psychological consultation by using the video consultation system.\n\n【27】The video consultation model for patients in isolation with diagnosed COVID-19 integrated an interprofessional clinical team to provide patient-centered care. By reducing direct face-to-face consultations with infectious patients, we helped ensure the safety of medical staff. Video consultation was essential for providing patient care and helped integrate services, including monitoring vital signs and patient symptoms; providing consultation with nurses, physicians, infectious disease specialists, and radiologists; and in-depth psychological consultation by a psychiatrist, when needed.\n\n【28】##### Preparation for Emergencies\n\n【29】The CTC established an emergency referral system with nearby medical institutions to respond to emergencies or increased symptoms. In an emergency, medical staff on duty in the CTC donned protective gear to visit the patient’s room. The patient center was equipped with an emergency cart normally used in the hospital, a portable oxygen tank, and a stretcher with a negative-pressure air tent for transferring patients to the ambulance area, if needed.\n\n【30】Patients requiring hospitalization were transferred to a hospital with a negative-pressure isolation unit that KCDC designated for treating COVID-19 patients. Criteria for transport to a hospital included abnormal vital signs measured every day for \\> 3 days or evidence of pneumonia on chest radiographs. Patients were transferred by ambulance from the nearest ambulance station. If an emergency occurred, such as abrupt respiratory failure, the patient was first transferred to the nearest emergency department for treatment and stabilization before being transferred to a hospital bed .\n\n【31】##### Characteristics of Patients in the SNUH-CTC\n\n【32】In total, 113 patients were admitted to the SNUH-CTC during March 5–26, 2020. Among patients, 59 (52.2%) were female and, 54 (47.8%) were male, the average age was 30.4 years (range 9–65 years), and 7 (6.2%) had underlying conditions, 4 of whom had hypertension. The average number of days of illness before admission to the CTC was 5.1 days. Among patients admitted with symptoms, 31 (27.4%) had cough, and 1 (0.9%) had fever. Four (3.5%) patients developed fever within 3 days after admission. Twelve (10.6%) patients had abnormalities in chest radiographs performed on the day of admission, but most were nonspecific haziness or opacity; only 1 patient appeared to have pneumonia .\n\n【33】##### Outcomes of Patients in the SNUH-CTC\n\n【34】During March 5–26, the SNUH-CTC admitted 113 patients; 103 were admitted directly from home, and 10 were transferred from the hospital during the recovery period. During the 3 weeks studied, 49 patients recovered and were discharged, and 2 patients were transferred to a COVID-19–designated facility for hospitalization . The average length of stay in the CTC was 15.7 days (interquartile range \\[IQR\\] 5–21 days), and the average interval from diagnosis to discharge was 19.5 days (IQR 10–27 days). One patient was transferred to a hospital after persistent pneumonia on chest radiographs for 3 days, and another patient was transferred for close monitoring because dyspnea developed and the patient needed oxygen at 1 L/min. In both cases, the medical staff staying in the CTC evaluated the patients in their rooms and decided to transfer them after detecting the deterioration on consultation. Both patients were safely admitted to the hospital.\n\n【35】### Discussion\n\n【36】South Korea established CTCs for isolation and monitoring of patients with no or mild symptoms of COVID-19 during a pandemic in which the demand for medical resources have exceeded the supply. SNUH converted an existing accommodation facility into a medical facility and provided video consultation via a smartphone to minimize staff contact with infectious patients. The hospital operated the CTC and provided medical services and public officials from the Ministry of Health and Welfare, local government, military, police, and the fire agency supported the operation by providing food delivery and patient transfer. During the 3-week operation, 113 asymptomatic and mildly symptomatic patients were admitted to SNUH-CTC for monitoring and care.\n\n【37】As COVID-19 spreads worldwide, the shortage of medical resources has become a serious problem in many countries . The lack of medical resources, such as hospital beds, intensive care units, and ventilators, can hinder the ability to treat patients adequately . In addition, during the pandemic crisis, a shortage of quarantine facilities, in this case hospitals, could increase the transmission of infectious diseases . When the demand for medical resources is greater than the supply, proper patient triage and resource allocation are crucial. During this pandemic, hospitals might not have sufficient medical equipment, such as ventilators and extracorporeal membrane oxygenation, for all patients. To maximize resources and save the most patients, hospitals with sufficient medical equipment can provide medical services for critically ill patients. However, carefully monitoring the disease progress, even for mild conditions, can assist in documenting clinical course of emerging infectious diseases. In addition, appropriate isolation of patients who test positive for COVID-19 can prevent further spread of disease .\n\n【38】South Korea has many acute care beds and high medical accessibility with the NHIS . Despite an 80% ratio of asymptomatic and mildly symptomatic patients in the early stages of the epidemic, all COVID-19 patients in South Korea were admitted to negative-pressure isolation rooms according to the principle of first come, first served . As the pandemic rapidly progressed, hospital beds became scarce , and >2,000 patients waited at home for hospital admission, including patients in high-risk groups, such as persons \\> 65 years of age and those with underlying conditions . Before CTCs were opened in South Korea, at least 2 patients died at home waiting for hospital admission and the need for medical facilities and redistribution of medical resources increased . Some patients hospitalized in the early stages of the endemic did not need active treatment but were required to be isolated and monitored. However, infected patients who were already hospitalized could not be discharged because of the possibility of sudden deterioration and difficulties in the control and monitoring during self-isolation at home .\n\n【39】A new quarantine model is needed to ensure beds in fully equipped hospitals for severe disease cases and the capacity to monitor and isolate asymptomatic and mildly symptomatic patients. For the current COVID-19 pandemic, South Korea implemented CTCs as an intermediate model between self-isolation at home and hospital isolation. The core aim of CTCs is to isolate patients in single rooms with bathrooms and provide care with telemedicine. Because the CTC model can be adapted as surge capacity in various types of facilities, such as resorts and hotels, CTCs could quickly secure a quarantine bed in a pandemic crisis. We found that CTCs can be an alternative to fully functioning hospitals and home isolation. CTCs enabled the country to preserve hospital resources for the sickest patients and isolate patients from the community to prevent further transmission. In addition, CTCs provided an opportunity for physicians to observe COVID-19 disease progression and triage patients who deteriorate to higher care, instead of leaving patients at home.\n\n【40】We found that allowing patients to independently measure their vital signs and providing telemedicine consultations had several advantages. First, reduced contact between healthcare workers and patients minimized the risk for infection for healthcare workers. During the pandemic, the infection or quarantine of healthcare workers will exacerbate the problem of already scarce medical resources . Second, because telemedicine is possible regardless of distance, CTCs would enable regions with sufficient resources to support regions with insufficient resources. In our model, the CTC and the monitoring centers were >100 km apart. The SNUH-CTC used a video consultation model instead of conventional telephone interviews because we could observe additional visual signs or diagnostic clues through video conferences . By using self-measurement equipment and advanced telecommunication technology, including smartphones, we were able to maximize these services.\n\n【41】South Korea opened its first CTC on March 2; by March 26, a total of 3,292 patients were admitted to 17 CTCs, representing 35.6% of the 9,241 cumulative confirmed COVID-19 cases in the country. During those 24 days, no deaths or instances of respiratory failure were reported in the 17 CTCs operated. The CTC model offers safe monitoring and isolation for asymptomatic or mildly symptomatic patients with diagnosed COVID-19 during the pandemic. During shortages of medical resources, appropriate triage of patients and allocation of resources are needed so that critically ill patients receive the highest level of care and patients with less severe infection can be safely monitored and treated. The CTC model also could be useful during natural disasters in which the demand for medical care overwhelms the supply.\n\n【42】We note a few limitations of CTCs. First, because a CTC is not a hospital, appropriate response to emergencies, such as respiratory failure, might be difficult. During our CTC operations, we chose to transfer patients to surrounding COVID-19–designated hospitals for emergency treatment; future planning should include hospitals with emergency services within a short distance of the CTC. Second, because we were not able to observe patients in real time, we might not have detected a sudden emergency. To protect patient privacy, we did not install a closed-circuit television in patient rooms, but we trained the patients to contact the medical staff immediately if they had a medical emergency. However, other countermeasures, such as patient alarm bells in each room, might be needed. Third, the CTC also is a quarantine facility; patient discomfort and depression might increase during long-term admission. To try to assure patients’ mental health, we provided various psychiatric interventions; in addition to other medical services, mental health should be built into further isolation and quarantine models. Fourth, our CTC did not have a negative-pressure isolation function as an infectious disease facility.\n\n【43】In conclusion, to safely isolate and monitor the asymptomatic and mildly symptomatic patients with COVID-19, South Korea developed the CTC model as an intermediate between hospitalization and self-isolation at home. By classifying patients according to the disease severity and underlying conditions, asymptomatic and mildly symptomatic patients can be safely monitored and treated at CTCs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2f01a3bb-104f-4958-80fa-a9c77ad541f4", "title": "Correlation between Tick Density and Pathogen Endemicity, New Hampshire", "text": "【0】Correlation between Tick Density and Pathogen Endemicity, New Hampshire\nAlong the borders of the northeastern and the upper midwestern United States, black-legged ticks ( _Ixodes scapularis_ ) are invading new areas . Because this tick is the principal vector of a number of human pathogens, defining and monitoring its possible expansion are imperative. Little information is available about _I. scapularis_ invasions, including the relative rates of pathogen carriage as vectors expand their range and establish locally enzootic cycles. To assess the endemicity of 3 tick-borne pathogens ( _Borrelia burgdorferi_ , _Anaplasma phagocytophilum_ , and _Babesia microti_ ) throughout New Hampshire, we surveyed adult _I. scapularis_ vectors.\n\n【1】### The Study\n\n【2】During the fall of 2007, we established 16 sampling sites in the 10 counties of New Hampshire . Levels of reported human Lyme disease had varied among the counties in 2006. We categorized each site as high Lyme disease incidence (HLI), medium Lyme disease incidence (MLI), or low Lyme disease incidence (LLI) according to the reported number of Lyme disease cases per 100,000 persons in 2006 . As in the neighboring states of Massachusetts  and Maine , in New Hampshire, ticks are most abundant in coastal counties. The HLI sites (Strafford, Rockingham, and Hillsborough counties) were along the coast and had 37–104 reported Lyme disease cases per 100,000 persons; the MLI sites (Carroll, Belknap, Merrimack, and Cheshire counties) bordered the coastal counties and had 10–19 cases per 100,000 persons; and the LLI sites (Coos, Grafton, and Sullivan counties) were the most inland and had 5–12 Lyme disease cases per 100,000 persons.\n\n【3】We collected adult ticks rather than the conventionally sought nymphs  because we believe adults best indicate the pathogen pool in an area. Transovarial transmission of _B. burgdorferi_ is negligible, and _I. scapularis_ ticks acquire infection by feeding on infected hosts as larvae or as nymphs. Because larval ticks feed during late summer and early autumn, they are more likely to feed on migratory animals, such as birds; nymphal ticks feeding in early summer are likely to feed on resident hosts. Hence, adult ticks are more likely to have taken at least 1 resident blood meal from the site of origin, and their _Borrelia_ infection rates (and genotype frequencies) are more representative of endemicity.\n\n【4】We visited each site twice within 1 month (October or November 2007) and sampled each site at each visit for ≈25 minutes. A total of 509 adult ticks were collected by drag-sampling vegetation. As expected, ticks were most abundant at HLI sites and moderate at MLI sites; no ticks were found at the 3 LLI sites, including 2 sites (Newport and Claremont) that were close to where ticks have previously been reported . Likewise, we did not find ticks on 6 deer carcasses at hunter check stations in nearby towns (Newport, Danbury, and Bristol).\n\n【5】Ticks were transported to the laboratory, where they were frozen with liquid nitrogen and pulverized in Eppendorf tubes with plastic pestles (Kontes, Vineland, NJ, USA). DNA was extracted using Epicenter Master Complete DNA & RNA Purification Kits (Epicenter Technologies, Madison, WI, USA). Two duplex real-time–PCR reactions were developed  by using oligonucleotide primers and Taqman probes for real-time detection of total tick DNA and _B. burgdorferi_ (duplex 1) and _A. phagocytophilum–B. microti_ (duplex 2). The most common pathogen found was _B. burgdorferi_ , followed by _B. microti_ and _A. phagocytophilum_ . A total of 322 (63%) ticks carried at least 1 pathogen, and 40 (8%) ticks carried 2 pathogens. The prevalence of ticks positive for both _B. burgdorferi_ and _B. microti_ was greater than that of ticks positive for both _B. burgdorferi_ and _A. phagocytophilum_ and accounted for most (78%) coinfections. Neither of the observed coinfections ( _B. burgdorferi–A. phagocytophium_ , _B. burgdorferi–B. microti_ ) differed significantly from its expected random occurrence (contingency table analysis p = 0.487, χ 2  p = 0.926).\n\n【6】We found a significantly greater percentage of _B. burgdorferi_ –infected ticks from HLI sites than from MLI sites . _B. burgdorferi_ was twice as common in ticks from the HLI sites. Similarly, _B. microti_ was more likely to be sampled from HLI sites (5.0%) than from MLI sites (2.7%), although this difference was not significant. Linear regression showed a strong correlation ( _R 2 _ \\= 0.90) between the entomologic risk index (total number of ticks × proportion of ticks infected ) in this study and the incidence of human cases of Lyme disease by county in New Hampshire in 2007.\n\n【7】### Conclusions\n\n【8】_I. scapularis_ was initially found in New Hampshire in 1985, near the town of Lebanon . Lebanon is located in a region where we were unable to sample ticks . It is tempting to hypothesize that _I. scapularis_ was once more abundant in western counties. In 2001, A.T. Eaton noticed a distribution of ticks similar to what we found , suggesting that the current distribution has been stable for at least 7 years. Our result that high tick density areas had the highest overall prevalence of pathogens and the highest prevalence of coinfected ticks supports the finding of Hamer et al. who reported a significant difference in the rates of pathogen carriage between recently invaded and _I. scapularis_ –endemic areas . The rates of infection we found are similar to those found by Swanson et al. from 5 other northeastern states . According to their meta-analysis, ≈40% (± 13%) of 2,109 adult and nymphal ticks were infected with _B. burgdorferi_ , 21% (± 17%) with _A. phagocytophilum_ , and 9% (± 8%) with either _B. microti_ or _B. divergens_ .\n\n【9】A noteworthy exception is the prevalence of _A. phagocytophilum_ . We detected this pathogen at only 1 site (10 positive ticks from Greenland on 2 independent visits). Human anaplasmosis has been a reportable disease in New Hampshire for at least a decade , and cases have been rare (0 or 1 reported per year during 1998–2006). However, human anaplasmosis increased substantially in 2007 (3 cases) and 2008 (9 cases).\n\n【10】Human babesiosis appears to be following a similar trend (2 cases in 2005, 3 cases in 2006 and 2007, and 9 cases in 2008), although the disease has been reportable only since 2005. Lastly, we found a strong correlation between entomologic risk index and the incidence of human Lyme disease. This result contrasts starkly with a lack of correlation found by Falco et al. between the abundance of adult female ticks and reported cases of erythema migrans (a common clinical presentation of Lyme disease) in southern New York State . Improved reporting of Lyme disease by clinicians to state health officials may be responsible for this discrepancy. The data presented here suggest that ERI estimates using adult ticks are accurate proxies for the yearly incidence of human Lyme disease in regions where Lyme disease is endemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4fcf8696-81af-44db-8b91-9390dd37891b", "title": "VEB-1 in Achromobacter xylosoxidans from Cystic Fibrosis Patient, France", "text": "【0】VEB-1 in Achromobacter xylosoxidans from Cystic Fibrosis Patient, France\nAchromobacter (formerly Alcaligenes) xylosoxidans is a newly emerging microorganism isolated with increased frequency from the lungs of patients with cystic fibrosis (CF), but information about its clinical relevance is limited . A. xylosoxidans is innately resistant to many antimicrobial drugs , except piperacillin, piperacillin-tazobactam, and imipenem, and moderately susceptible to ceftazidime (45% of susceptible isolates), which is widely used to treat infection due to Pseudomonas aeruginosa . The mechanisms involved in cases of high-level resistance to ceftazidime have not been described for A. xylosoxidans. Possible mechanisms for ceftazidime resistance among gram-negative bacilli are alterations in outer membrane proteins, overproduction of cephalosporinase, or production of an extended-spectrum β-lactamase (ESBL). ESBLs are enzymes distributed worldwide  that hydrolyze oxyimino-cephalosporins and monobactams and are susceptible to β-lactamase inhibitors such as clavulanic acid and tazobactam. We report on the isolation from a CF patient of A. xylosoxidans that produced the VEB-1 ESBL. This is the first report of ESBL production in A. xylosoxidans and the first report of a VEB-1–producing isolate from a CF patient.\n\n【1】### The Study\n\n【2】During the past 10 years in our 1,600-bed university hospital, 37 CF patients had \\> 1 respiratory tract specimen that contained A. xylosoxidans. Preliminary pulsed-field gel electrophoresis of these strains has failed to identify shared isolates among the patients, but studies are ongoing. In November 2003, A. xylosoxidans 476 (AX476) was isolated from the sputum of a 17-year-old male CF patient. This patient had good pulmonary function (forced expiratory volume 1 = 99% of predicted value), had never been colonized or infected by P. aeruginosa, and therefore never received ceftazidime. The strain was identified with the Api 20NE system (bio-Mérieux, Marcy-l'Etoile, France), and antimicrobial susceptibility testing was performed and interpreted as recommended by the Clinical and Laboratory Standards Institute (formerly NCCLS) .\n\n【3】The antibiogram, which was performed by a disk diffusion method, showed AX476 to be highly resistant to ceftazidime, aminoglycosides, sulfonamides, trimethoprim, and ciprofloxacin but fully susceptible to tetracyclines, piperacillin/tazobactam, ticarcillin/clavulanic acid, and imipenem. Because of an unusual synergy between ticarcillin and ticarcillin/clavulanic acid , we compared the inhibition zones of third-generation cephalosporin disks with and without clavulanic acid (BioRad, Marnes-la-Coquette, France). The zones were 7 mm for ceftazidime and 19 mm for ceftazidime plus clavulanic acid , which strongly indicated production of an ESBL. Isoelectric focusing showed that AX476 produced a β-lactamase with an isoelectric point of 7.4. A large plasmid of ≈200 kb (pJDB1) was easily transferred by conjugation to Escherichia coli K-12 C600. The transconjugants, E. coli (pJDB1) sorbitol-fermenting, which were selected on MacConkey agar that contained 4 μg/mL of ceftazidime, were resistant to sulfonamides and trimethoprim, had reduced susceptibility to aminoglycosides, and harbored a β-lactamase with an isoelectric point of 7.4. The resistance phenotype of the isolate and the value of the isoelectric point of the enzyme suggested the production of the ESBL VEB-1 .\n\n【4】The MICs for β-lactams for AX476 and its transconjugant, determined by Mueller-Hinton broth dilution method, are shown in Table 1 . By using bla VEB-1  \\-specific primers, a positive PCR result was obtained on total DNA from AX476 and the transconjugants. All genetic analyses of bla VEB-1  published so far have identified either its chromosome  or its plasmid  location and mostly its integration within class 1 integrons of variable structure. Integrons are potentially mobile genetic elements that comprise conserved sequences that flank a variable region and may contain inserted antimicrobial drug resistance gene cassettes . The 5´-conserved segment includes the gene intI1 that encodes an integrase, the cassette integration site attI1, and a promoter responsible for the expression of the genes located downstream within the variable region. The 3´-conserved region contains either a qacEΔ1 gene that encodes resistance to quaternary ammonium compounds or a combination of 3 genes: qacEΔ1, sulI (which encodes resistance to sulfonamides), and orf5 (an open reading frame of unknown function).\n\n【5】To search for the presence of such a class 1 integron in AX476 and its transconjugant, we performed PCR on total DNA of AX476 and E. coli (pJDB1) by using the primers L1 and R1 specific for the detection of class 1 integrons . We obtained a fragment of 2.3 kb in the clinical strain and its transconjugant, which was sequenced on both strands. By using a set of primers, we deduced the structure of this integron . Three gene cassettes have been identified. The first, dhfr (dihydrofolate reductase), encoded a putative trimethoprim-resistance protein. This dhfr was identical to that reported in Salmonella enterica serovar Typhi  and to the dhfr gene cassette contained in a class 1 integron from Klebsiella pneumoniae not yet published . The second cassette, bla VEB-1  , encoded the ESBL VEB-1 first described in E. coli . The third and last gene cassette was aadB. It encoded an aminoglycoside adenyltransferase that conferred resistance to kanamycin, gentamicin, and tobramycin and was identical to other sequenced aadB gene cassettes located on integrons containing bla VEB-1  gene . VEB-1 has been detected in Enterobacteriaceae and P. aeruginosa isolates from Southeast Asia  but never in A. xylosoxidans. In France, VEB-1–producing isolates of Acinetobacter baumannii have been involved in several outbreaks of nosocomial infection in intensive care units ; however, we have not yet detected a VEB-1–producing isolate in our hospital.\n\n【6】### Conclusions\n\n【7】This finding of a VEB-1–producing A. xylosoxidans from a CF patient enhances the scant information available to laboratorians and clinicians about ESBL production by isolates from CF patients. A very recent study reports 3 ESBL-positive isolates of P. aeruginosa from CF patients in New Delhi, but the ESBL has not been characterized . Resistance to expanded-spectrum cephalosporins mediated by ESBLs has never been described in A. xylosoxidans. The detection of the ESBL production was difficult in AX476; therefore, the frequency of A. xylosoxidans isolates that produce an ESBL might be underestimated. We recommend the use of BioRad combination disks, especially for isolates that are highly resistant to ceftazidime and susceptible to piperacillin or when synergy exists between ticarcillin and ticarcillin plus clavulanic acid.\n\n【8】The origin of the strain remains unclear. Because A. xylosoxidans is widely encountered in the environment, acquisition of AX476 by our patient may have resulted from poor adherence to handwashing, contamination of respiratory therapy equipment (nebulizer), or contaminated water. We can exclude nosocomial acquisition because our patient had never been hospitalized.\n\n【9】The location of bla VEB-1  on an easily transferable plasmid might represent a clinical threat if spread among other species widely encountered among CF patients, especially P. aeruginosa. Such a transfer would create serious therapeutic problems. Therefore, to prevent person-to-person transmission, our patient visits the physician on different days than the other CF patients. If he needs to be hospitalized, our patient may not share a room with immunocompromised patients or with other CF patients anywhere in the hospital, which is the recommendation for patients with other multidrug-resistant pathogens . In conclusion, this first finding of a VEB-1–producing A. xylosoxidans from a CF patient emphasizes the need to study the mechanism(s) of resistance to ceftazidime among a wide collection of isolates originated from different centers. The sequence of the class 1 integron reported in this paper has been assigned GenBank accession no. DQ393569.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "38140acb-72e6-4ab5-9c50-25062cd8c593", "title": "Prevalence of Self-Reported Intake of Sugar-Sweetened Beverages Among US Adults in 50 States and the District of Columbia, 2010 and 2015", "text": "【0】Prevalence of Self-Reported Intake of Sugar-Sweetened Beverages Among US Adults in 50 States and the District of Columbia, 2010 and 2015\nAbstract\n--------\n\n【1】Frequent intake of sugar-sweetened beverages (SSBs) is associated with adverse health outcomes, including obesity, type 2 diabetes, and cardiovascular disease. We used combined data from the 2010 and 2015 National Health Interview Survey to examine the prevalence of SSB intake among US adults in all 50 states and the District of Columbia. Approximately two-thirds of adults reported consuming SSBs at least daily, including more than 7 in 10 adults in Hawaii, Arkansas, Wyoming, South Dakota, Connecticut, and South Carolina, with significant differences in sociodemographic characteristics. Efforts to decrease SSB consumption could consider the sociodemographic and geographic differences in SSB intake when designing equitable interventions.\n\n【2】Objective\n---------\n\n【3】Sugar-sweetened beverages (SSBs) are a leading source of added sugars in the US diet and are associated with obesity, type 2 diabetes, heart disease, kidney disease, nonalcoholic fatty liver disease, and tooth decay . SSBs, which are sweetened with various forms of added sugars, include regular soda, sweetened fruit drinks, sports/energy drinks, and sweetened coffee/tea drinks . Previous studies reported geographic differences in SSB intake . However, no study has reported SSB intake for every state. We assessed the prevalence of SSB intake among US adults by sociodemographic characteristics for all 50 states and the District of Columbia by using National Health Interview Survey (NHIS) data.\n\n【4】Methods\n-------\n\n【5】NHIS is a nationally representative, cross-sectional household survey conducted by the National Center for Health Statistics (NCHS) that uses in-person interviews. The Cancer Control Supplement (CCS), which contains dietary intake information, was administered both in 2010 and in 2015 and was approved by the NCHS Research Ethics Review Board. We used nationally weighted data from combined 2010 and 2015 NHIS CCS to examine the prevalence of consuming SSBs 1 or more times daily among 56,260 US adults aged 18 or older. Data were combined to increase the sample size and reduce the variability associated with state estimates. This study required the use of restricted NHIS files for state estimates and categorizing metropolitan status available through the NCHS Research Data Center. SSB intake was based on survey respondents’ answers to 4 questions asking about intake frequency over the past month of regular soda, sweetened fruit drinks, sports/energy drinks, and sweetened coffee/tea drinks . Sweetened fruit drinks and sweetened coffee/tea drinks included drinks that were presweetened in addition to drinks that were sweetened at home by adding sugar. Adults responded with intake frequency per day, week, or month for each beverage type. Weekly and monthly intake frequency for each type of beverage was converted to daily intake frequency by dividing by 7 or 30, respectively. To calculate frequency of total daily SSB intake, we summed responses from intake of regular soda, sweetened fruit drinks, sports/energy drinks, and sweetened coffee/tea drinks. SSB categories and frequency cutoff of once per day were used, consistent with previous studies . Differences in respondent characteristics were assessed by χ 2  tests ( _P_ < .05). Prevalence estimates were calculated for SSB categories and by state for all 50 states and the District of Columbia. Analyses were conducted with SAS-callable SUDAAN, version 9.0 (RTI) to account for a complex survey design and sampling weights.\n\n【6】Results\n-------\n\n【7】Overall, 63.0% of US adults reported consuming SSBs 1 or more times daily in combined 2010 and 2015 NHIS CCS data . US adults reported consuming the following 1 or more times daily, by beverage type: sweetened coffee/tea drinks, 39.5%; regular soda, 19.5%; fruit drinks, 5.7%; and sports/energy drinks, 5.5%. Among sociodemographic categories with significant differences overall, the prevalence of SSB intake was highest among adults aged 18 to 24 (65.0%) and 25 to 39 (65.4%), men (66.1%), Hispanic respondents (70.1%), people with less than a high school education (69.8%), people with an annual household income less than $35,000 (66.0%), people residing in nonmetropolitan areas (65.0%), and people residing in the Northeast census region (67.0%). The prevalence of SSB intake did not significantly differ by marital status.\n\n【8】By state, SSB intake of 1 or more times daily ranged from 44.5% in Alaska to 76.4% in Hawaii. These 6 states had a prevalence of daily SSB intake of 70.0% or more: Hawaii (76.4%), Arkansas (74.2%), Wyoming (73.2%), South Dakota (72.5%), Connecticut (72.2%), and South Carolina (70.2%). Only 1 state, Alaska (44.5%), had a daily intake prevalence below 50.0% . Most states had a daily intake prevalence between 50.0% and 70.0%  .\n\n【9】**  \nPrevalence of self-reported sugar-sweetened beverage (SSB) intake once daily or more among US adults by state, National Health Interview Survey Cancer Control Supplement (NHIS CCS), 2010 and 2015. SSBs include regular soda, sweetened fruit drinks, sports/energy drinks, and sweetened coffee/tea drinks. This map shows combined 2010 and 2015 data from the NHIS CCS . \n\n【10】Discussion\n----------\n\n【11】Daily SSB intake is common among US adults and is particularly high in some states and among some populations. The prevalence in our study was higher than in the 2017 Behavioral Risk Factor Surveillance System (BRFSS) survey . This discrepancy may be explained by differences in the types of SSBs assessed, modes of survey administration, methods of collecting dietary intake data, and representativeness. Previous NHIS, NHANES (National Health and Nutrition Examination Survey), and BRFSS data also showed that SSB consumption is higher among young adults, men, adults in nonmetropolitan counties, and people with low levels of education .\n\n【12】The prevalence of SSB consumption in previous studies was high in the Northeast  and in southern states , consistent with our study’s findings. The high northeastern prevalence may be due to high consumption of sweetened coffee or tea drinks . Data from the 2017 BRFSS survey  for 12 states, and data from the 2013 BRFSS survey  for 23 states also revealed state-specific differences in SSB intake. Reasons for state differences may reflect demographic differences. States and communities may also differ in SSB marketing , pricing, and access to alternatives.\n\n【13】Our study has several limitations, including self-reported information, assessment of intake frequency without volume or amount of SSBs, age of the data, and combination of data. Declines in SSB intake have occurred over time . Combining data may mask changes in prevalence in the study period. Regardless, ours is the first study to our knowledge to examine SSB intake frequency for all 50 states and the District of Columbia by using a nationally representative sample of US adults. Our findings highlight that prevalence of daily SSB intake remains high among US adults, with sociodemographic and geographic differences. Efforts to decrease SSB intake could consider the higher intake prevalence in sociodemographic and geographic subpopulations to aid design and targeting of equitable interventions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "12326a56-c975-4634-8ee1-f453f83a173d", "title": "Elizabethkingia bruuniana Infections in Humans, Taiwan, 2005–2017", "text": "【0】Elizabethkingia bruuniana Infections in Humans, Taiwan, 2005–2017\nThe _Elizabethkingia_ genus comprises gram-negative, aerobic, nonmotile, nonspore-forming, nonfermenting rod-shaped bacteria . This genus previously comprised _E. meningoseptica_ , _E. miricola_ , and _E. anophelis_ . In August 2017, Nicholson et al. proposed adding 3 new species, namely _E. bruuniana_ , _E. ursingii_ , and _E. occulta_ , to this genus . However, little information exists about these species. In this study, we report the clinical characteristics and demographics of a group of patients with _E. bruuniana_ infection in Taiwan and the molecular features of their _E. bruuniana_ isolates.\n\n【1】We conducted this study at E-Da Hospital, a 1,000-bed university-affiliated medical center in Kaohsiung, Taiwan; this study was approved by the institutional review board of the hospital . We searched the hospital database to identify microbial cultures performed during January 2005–December 2017 that yielded _Elizabethkingia_ . The isolates were initially identified by staff in the clinical microbiology laboratory using API/ID32 phenotyping kits or VITEK MS . We reidentified these species as _Elizabethkingia_ using both 16S rRNA and _rpoB_ gene sequencing. The primers and methods we used for amplification and sequencing of the 16S rRNA and _rpoB_ genes were described previously . We compared the assembled 16S rRNA gene sequences with the nucleotide sequences of _Elizabethkingia_ \\-type strains present in GenBank. We considered isolates with \\> 99.5% similarity in the 16S rRNA gene sequence members of the same species, as recommended in a previous study . We constructed a phylogenetic tree using the _rpoB_ genes of the isolates exhibiting \\> 99.5% 16S rRNA gene sequence identity with the _E. bruuniana_ type strain G0146 T  . We calculated the average nucleotide identity using OrthoANI  and computed in silico DNA–DNA hybridization (DDH) using the Genome-to-Genome Distance Calculator , using the average nucleotide identity value of \\> 95% and the DDH value of \\> 70% separately as criteria for species delineation . We sequenced the quinolone resistance–determining regions of DNA gyrase ( _gyrA_ and _gyrB_ ) and topoisomerase IV ( _parC_ and _parE_ ) to look for mutations associated with resistance .\n\n【2】For the 13-year period, we found 103 nonduplicate _Elizabethkingia_ isolates in the database of the clinical microbiology laboratory. Among these, 8 isolates shared \\> 99.5% 16S rRNA gene sequence identity with _E. bruuniana_ G0146 T  , and an _rpoB_ gene–based phylogenetic analysis revealed that 6 of the 8 isolates were more closely related to _E. bruuniana_ G0146 T  . We previously published the complete whole-genome sequence of 1 of these 6 isolates, EM798-26  . Using 16S rRNA gene sequence analysis, we initially identified this isolate as _E. miricola_ . Average nucleotide identity analysis demonstrated that EM798-26 and _E. bruuniana_ G0146 T  share 97.7% whole-genome similarity . Using in silico DDH analysis, we predicted a DDH value of 81.7% for EM798-26 and _E. bruuniana_ G0146 T  . These results support that EM798-26 and the other 5 isolates (EM20-50, EM455-89, EM828-05, EM863-68, and EM891-63) are _E. bruuniana_ .\n\n【3】These 6 isolates were collected from 6 (4 male and 2 female) patients  with a mean age of 71.7 (SD \\+ 11) years. The sources of isolation included bronchoalveolar lavage fluid (n = 2), blood (n = 2), urine (n = 1), and the tip of the central venous catheter (n = 1). All infections were healthcare associated. Two patients had septic shock, and all patients had \\> 1 concurrent medical condition, such as hypertension, diabetes mellitus, or a malignancy. Antimicrobial therapy included piperacillin/tazobactam, trimethoprim/sulfamethoxazole, levofloxacin, or tigecycline, either singly or in combination. None of the patients died of _E. bruuniana_ infection.\n\n【4】Most _E. bruuniana_ isolates were resistant to β-lactams, β-lactam and lactamase inhibitors, carbapenems, aminoglycosides, and trimethoprim/sulfamethoxazole . All isolates were susceptible to minocycline, 4 (67%) to tigecycline and levofloxacin, and 2 (33%) to ciprofloxacin. The antimicrobial susceptibility patterns we found are similar to those of other _Elizabethkingia_ spp. identified in previous studies . For example, reports from the United States, Hong Kong, and South Korea have revealed that _E. anophelis_ and _E. meningoseptica_ were frequently resistant to most β-lactams, including ceftazidime, ceftriaxone, and imipenem, but showed variable susceptibility to piperacillin/tazobactam, cefepime, ciprofloxacin, and levofloxacin .\n\n【5】To investigate the association between target gene mutations and fluoroquinolone resistance, we examined the mutations present in quinolone resistance–determining regions in these 6 isolates. We did not find nonsynonymous substitutions in the quinolone resistance–determining regions of _gyrA_ , _gyrB_ , _parC_ , and _parE_ , which suggests that mutations in these genes are not the cause of fluoroquinolone resistance.\n\n【6】In summary, our study demonstrates the clinical manifestations of _E. bruuniana_ infection and the molecular characteristics of the pathogen. Because cases in our study were limited in number, further large-scale studies are necessary to investigate the antimicrobial susceptibility patterns of _E. bruuniana_ and elucidate the clinical characteristics and treatment of _E. bruuniana_ infection.\n\n【7】Jiun-Nong Dr. Lin is an associate professor at the School of Medicine, College of Medicine, I-Shou University, Kaohsiung, Taiwan. His research interests include infectious diseases and clinical microbiology.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "699fc44c-a4a1-4a85-b4a9-595bafa479b5", "title": "Light, Reflection, Illumination", "text": "【0】Light, Reflection, Illumination\nJoaquin Sorolla y Bastida . The Wounded Foot, 1909. Medium oil on canvas 43 × 39 in. /109.2 × 99.1 cm. Digital image courtesy of the Getty’s Open Content Program, The J. Paul Getty Museum, Los Angeles, CA,USA.\n\n【1】Although expressions such as “shed some light” or “I saw the light” figure often in our everyday speech, we do not typically contemplate light. We may marvel at its seemingly mysterious expression through auroras, rainbows, lasers, or celestial objects or find comfort in its subtler forms such as sunbeams, candles, campfires, or fireflies. Artists and scientists, by contrast, do study and manipulate light, reflection, and illumination.\n\n【2】Impressionistic painters strive to capture an impression of a fleeting moment, a particular scene. Many among their numbers are considered masters for their skill at capturing light as it shimmers across surfaces. Claude Monet once described Joaquín Sorolla y Bastida Sorolla as “the painter of light above all other.”\n\n【3】Sorolla, was born in Valencia, Spain, in 1863. He and his infant sister Concha were orphaned when their parents died during a cholera epidemic 2 years later, and they were raised by their mother’s relatives. Early on, Sorolla seemed destined to become an artist: “When Joaquin was of an age to go to school, he manifested little inclination for his studies proper, though he revealed a stealthy and incorrigible craze for scrawling embryonic drawings in his copy books.”\n\n【4】In 1878, he began studies at the Fine Arts School of San Carlos, and he soon won awards at the Academy of Valencia. During 1879, the young artist traveled to Paris, where he toured exhibitions and met painters who worked in the open air, a practice that accentuated attention to light, color, and movement. That experience proved pivotal. Sorolla developed a passion for painting outdoors, preferring natural light and settings. After his military service, Sorolla subsequently attended the Fine Arts Academy in Rome on a 4-year scholarship beginning when he was 21. By the time Sorolla was 30, his paintings had been displayed across Europe and in the United States, and by the turn of the century, he was acknowledged to be among the Western world’s best living painters.\n\n【5】He created many memorable paintings portraying the sun-drenched Spanish Mediterranean beaches and seascapes; he often painted portraits outside, reinvigorating the form with his fresh perspective. Sorolla’s mastery is largely accorded to his ability to depict tones and colors of sunlight, and he dedicated his life to chasing the sun as it played over the people and places of his beloved Spain. He himself acknowledged, “I hate darkness. Claude Monet once said that painting in general did not have light enough in it. I agree with him. We painters, however, can never reproduce sunlight as it really is. I can only approach the truth of it.”\n\n【6】Sorolla cut a dashing figure, dressed in a suit, working from a table-sized palette dabbed with an array of colors squeezed from emptied tubes, and wielding yard-long brushes. The vigor with which he worked—he often finished a painting within a few days—did not diminish even as his reputation and wealth grew: he completed more than 500 paintings during an energetic 4-year spurt.\n\n【7】This months’ cover image, _The Wounded Foot,_ is among a series of paintings Sorolla made on the beach at Valencia. This painting focuses on 2 children, one of whom sits on the wet sand inspecting her foot, possibly injured from a jagged shell or broken shard of glass; her companion crouches, peering from under a wide-brimmed hat, perhaps offering comfort. That child’s arm falls beyond the edge of the canvas; blurred figures bob, swim, and frolic in the ocean. Reflected light scatters and glistens across the wet sand, swirling water, and foamy edges of waves. The casual composition belies the artist’s expertise in capturing transitory light and motion. The J. Paul Getty Museum notes that “The colored reflections of late afternoon light animate this beach scene and actively define the forms, from the injured child’s shoulder to the liquid sea and the figures playing in the water. The sun’s highlights on the hurt child’s hand, the sand around her foot, and her companion’s hat draw the viewer's attention to the injured limb.”\n\n【8】The study of light, reflection, and illumination also seized the imagination of German scientist August Köhler, one of Sorolla’s contemporaries. In 1893, Köhler developed the microscope illumination technique that both bears his name and remains in use: Köhler illumination. This technique uniformly illuminates specimens without background glare. When this means of illumination is used on Gram-stained specimens, the defining features and structures of bacteria are readily and vividly revealed.\n\n【9】Köhler illumination proved ground-breaking. Advances in detection and surveillance cannot come swiftly enough as researchers and public health professionals grapple with the increasing emergence of drug resistance in bacteria. Antimicrobial drug resistance diminishes the ability to treat bacterial infections, increases risks associated with many medical procedures, and threatens animal health and agriculture. In 2014, the United States government released _The National Action Plan for Combating Antibiotic-Resistant Bacteria_ , acknowledging the global scope of the problem and supporting the One Health approach to disease surveillance for pathogens of humans and animals as being critical for combating resistance to antibiotics.\n\n【10】Light, reflection, and illumination led an artist to capture a fleeting incident on a sunny beach in _The Wounded Foot_ and inspired a scientist to find a durable solution for a problem that had vexed researchers. The growing problem of antimicrobial resistance could make treating that child’s wound today more complicated than we imagined when the widespread use of antibiotics began in the 1940s, reminding us that we must keep shedding new light on an old problem.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "71875084-9cfb-458d-a73b-1833f1033a4d", "title": "Asymptomatic Transmission of SARS-CoV-2 on Evacuation Flight", "text": "【0】Asymptomatic Transmission of SARS-CoV-2 on Evacuation Flight\nUndocumented cases of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection have been common during the coronavirus disease (COVID-19) global pandemic . Although inflight transmission of symptomatic COVID-19 has been well established , the evidence for transmission of asymptomatic COVID-19 on an aircraft is inconclusive. We conducted a cohort study evaluating asymptomatic passengers on a flight that carried 6 asymptomatic patients with confirmed SARS-CoV-2 infections. The Institutional Review Board of Armed Force Medical Command approved the study protocol. The ethics commission waived written informed consent because of the urgent need to collect data on COVID-19.\n\n【1】### The Study\n\n【2】On March 31, 2020, we enrolled in our study 310 passengers who boarded an evacuation flight from Milan, Italy, to South Korea. This evacuation flight was conducted under strict infection control procedures by the Korea Centers for Disease Control and Prevention (KCDC), based on the guidelines of the World Health Organization (WHO) . When the passengers arrived at the Milan airport, medical staff performed physical examinations, medical interviews, and body temperature checks outside the airport before boarding, and 11 symptomatic passengers were removed from the flight. Medical staff dispatched from KCDC were trained in infection control under the guidance of the KCDC and complied with the COVID-19 infection protocol, which was based on WHO guidelines . N95 respirators were provided, and passengers were kept 2 m apart for physical distancing during preboarding. Most passengers wore the N95 respirators except at mealtimes and when using the toilet during the flight. After an 11-hour flight, 299 asymptomatic passengers arrived in South Korea and were immediately quarantined for 2 weeks at a government quarantine facility in which the passengers were completely isolated from one another. Medical staff examined them twice daily for elevated body temperature and symptoms of COVID-19. All passengers were tested for SARS-CoV-2 by reverse transcription PCR twice, on quarantine day 1 (April 2) and quarantine day 14 (April 15).\n\n【3】Asymptomatic patients were those who were asymptomatic when they tested positive and did not develop symptoms within 14 days after testing . Among the 299 passengers (median age 30.0 years; 44.1% male), 6 had a confirmed positive result for SARS-CoV-2 on quarantine day 1 and were transferred immediately to the hospital . At 14 days after the positive test, the 6 patients reported no symptoms and were categorized as asymptomatic.\n\n【4】On quarantine day 14, a 28-year-old woman who had no underlying disease had a confirmed positive test result for COVID-19. On the flight from Milan, Italy, to South Korea, she wore an N95 mask, except when she used a toilet. The toilet was shared by passengers sitting nearby, including an asymptomatic patient. She was seated 3 rows away from the asymptomatic patient . Given that she did not go outside and had self-quarantined for 3 weeks alone at her home in Italy before the flight and did not use public transportation to get to the airport, it is highly likely that her infection was transmitted in the flight via indirect contact with an asymptomatic patient. She reported coughing, rhinorrhea, and myalgia on quarantine day 8 and was transferred to a hospital on quarantine day 14. The remaining 292 passengers were released from quarantine on day 15.\n\n【5】All crew members (n = 10) and medical staff dispatched from KCDC (n = 8) were quarantined at a government quarantine facility for 2 weeks and were tested twice for SARS-CoV-2, on quarantine days 1 and 14. All 18 members of the cabin crew and medical staff were negative for SARS-CoV-2 on both occasions.\n\n【6】To reinforce our results, we performed an external validation using a different dataset. Another evacuation flight of 205 passengers from Milan, Italy, to South Korea on April 3, 2020, was also conducted by KCDC under strict infection control procedures. Among the passengers on this flight were 3 asymptomatic patients who tested positive on quarantine day 1 and 1 patient who tested negative on quarantine day 1 and positive on quarantine day 14. On the basis of an epidemiologic investigation, the authors and KCDC suspect that this infection was also transmitted by inflight contact.\n\n【7】### Conclusions\n\n【8】This study was one of the earliest to assess asymptomatic transmission of COVID-19 on an aircraft. Previous studies of inflight transmission of other respiratory infectious diseases, such as influenza and severe acute respiratory syndrome, revealed that sitting near a person with a respiratory infectious disease is a major risk factor for transmission , similar to our own findings. Considering the difficulty of airborne infection transmission inflight because of high-efficiency particulate-arresting filters used in aircraft ventilation systems, contact with contaminated surfaces or infected persons when boarding, moving, or disembarking from the aircraft may play a critical role in inflight transmission of infectious diseases .\n\n【9】Previous studies reported that viral shedding can begin before the appearance of COVID-19 symptoms , and evidence of transmission from presymptomatic and asymptomatic persons has been reported in epidemiologic studies of SARS-CoV-2 . Because KCDC performed strong infection control procedures during boarding; the medical staff and crew members were trained in infection control; all passengers, medical staff, and crew members were tested twice for SARS-CoV-2; and a precise epidemiologic investigation was conducted, the most plausible explanation for the transmission of SARS-CoV-2 to a passenger on the aircraft is that she became infected by an asymptomatic but infected passenger while using an onboard toilet. Other, less likely, explanations for the transmission are previous SARS-CoV-2 exposure, longer incubation period, and other unevaluated situations.\n\n【10】The control measures incorporated into our cohort study provide a higher level of evidence than previous studies on asymptomatic transmission . Our findings suggest the following strategies for the prevention of SARS-CoV-2 transmission on an aircraft. First, masks should be worn during the flight. Second, because contact with contaminated surfaces increases the risk for transmission of SARS-CoV-2 among passengers, hand hygiene is necessary to prevent infections. Third, physical distance should be maintained before boarding and after disembarking from the aircraft.\n\n【11】Our research provides evidence of asymptomatic transmission of COVID-19 on an airplane. Further attention is warranted to reduce the transmission of COVID-19 on aircraft. Our results suggest that stringent global regulations for the prevention of COVID-19 transmission on aircraft can prevent public health emergencies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "640c88a9-66b2-483a-b460-416c892e0215", "title": "Control of Avian Influenza in Poultry", "text": "【0】Control of Avian Influenza in Poultry\nAvian influenza (AI), which emerged from the animal reservoir, represents one of the greatest recent concerns for public health. Compared with the number reported for the past 40 years, the number of outbreaks of AI in poultry has increased sharply during the past 5 years. The number of birds involved in AI outbreaks has increased 100-fold, from 23 million from 1959 through 1998 to >200 million from 1999 through 2005 . Since the late 1990s, AI infections have assumed a completely different profile in the veterinary and medical scientific communities. Some recent outbreaks have been minor, but other epidemics, such as the Italian 1999–2000, the Dutch 2003, the Canadian 2004, and the ongoing Eurasian, have been more serious. They have led to devastating consequences for the poultry industry, negative repercussions on public opinion, and, in some instances, created major human health issues, including the risk of generating a new pandemic virus for humans through an avian-human link.\n\n【1】Influenza viruses are segmented, negative-strand RNA viruses that are placed in the family _Orthomyxoviridae_ in 3 genera: _Influenzavirus A_ , _B_ , and _C_ . Influenza A viruses are the only type reported to cause natural infections of birds and are further divided into subtypes according to antigenic characteristics of the surface glycoproteins hemagglutinin (H) and neuraminidase (N). At present, 16 hemagglutinin subtypes (H1–H16) and 9 neuraminidase subtypes (N1–N9) have been identified. Each virus has one H and one N antigen, apparently in any combination; all subtypes and most possible combinations have been isolated from avian species.\n\n【2】Influenza A viruses that infect poultry can be divided into 2 distinct groups according to the severity of disease they cause. The most virulent viruses cause highly pathogenic avian influenza (HPAI), a systemic infection in which death rates for some susceptible species may be as high as 100%. These viruses have thus far been restricted to strains that belong to the H5 and H7 subtypes and have a multibasic cleavage site in the precursor of the hemagglutinin molecule. HPAI is a lethal infection in certain domestic birds (e.g. chickens and turkeys) and has a variable clinical effect (may or may not cause clinical signs and death) in domestic waterfowl and wild birds. The potential role of wild birds and waterfowl as reservoirs of infection by HPAI strains has been described for only the Asian HPAI virus H5N1. The ecologic and epidemiologic implications of this unprecedented situation are not predictable.\n\n【3】On the contrary, viruses that belong to all subtypes (H1–H16) that lack the multibasic cleavage site are perpetuated in nature in wild bird populations. Feral birds, particularly waterfowl, are the natural hosts for these viruses and are therefore considered an ever-present source of viruses. Since their introduction into domestic bird populations, these viruses have caused low-pathogenicity avian influenza (LPAI), a localized infection that results in mild disease, primarily respiratory disease, depression, and egg-production problems. Theories suggest that HPAI viruses emerge from H5 and H7 LPAI progenitors by mutation or recombination , although >1 mechanism is likely. This theory is supported by findings from phylogenetic studies of H7 subtype viruses, which indicate that HPAI viruses do not constitute a separate phylogenetic lineage or lineages but appear to arise from nonpathogenic strains ; this indication is supported by the in vitro selection of mutants virulent for chickens from an avirulent H7 virus .\n\n【4】Such mutation probably occurs after the viruses have moved from their natural wild-bird host to poultry. However, the mutation to virulence is unpredictable and may occur very soon after the virus is introduced to poultry or after the LPAI virus has circulated in domestic birds for several months. This hypothesis is strongly supported by a recent study of Munster et al. who showed that minor genetic and antigenic diversity exists between H5 and H7 LPAI viruses found in wild birds and those that caused HPAI outbreaks in domestic poultry in Europe. The scientific evidence collected in recent years leads to the conclusion that not only must HPAI viruses be controlled in domestic populations, but LPAI viruses of the H5 and H7 subtypes should also be controlled because they represent HPAI precursors.\n\n【5】### Prevention of Avian Influenza\n\n【6】From December 1999 through April 2003, >50 million birds died or were depopulated after HPAI infection in the European Union , causing severe economic losses to the private and public sectors. These losses suggest that the strategies and control measures used to combat the disease need improvement, from disease control and animal welfare perspectives.\n\n【7】AI viruses are introduced to domestic poultry primarily through direct or indirect contact with infected birds. Transmission may occur through movement of infected poultry; movement of contaminated equipment, fomites, or vehicles; and exposure to contaminated infectious organic material. Airborne transmission over long distances between farms has not yet been demonstrated. For these reasons, if biosecurity measures are implemented at the farm level, AI infections can be prevented.\n\n【8】Outbreaks that involve large numbers of animals are characterized by the penetration of infection into the commercial circuit; that is, industrially reared poultry and all other poultry that is traded, including those from semi-intensive and backyard farms. Biosecurity (encompassing bioexclusion and biocontainment) represents the first and most important means of prevention. If biosecurity measures of a high standard are implemented and maintained, they create a firewall against infection penetration and perpetuation in the industrial circuit. However, breaches in biosecurity systems do occur. On one hand, the occurrence and extent of the breach should be evaluated and corrective measures should follow; on the other, they indicate the need to establish early warning systems and additional control tools for AI.\n\n【9】### General Aspects of Vaccination\n\n【10】Until recently, AI infections caused by viruses of the H5 and H7 subtype occurred rarely, and vaccination was not considered because stamping out was the recommended control option. Primarily for this reason, vaccinology for AI has not grown at the same rate as for other infectious diseases of animals. Data are being generated from experimental and field research in AI vaccinology, but the rather complex task of vaccinating poultry in different farming and ecologic environments still has areas of uncertainty.\n\n【11】Guidelines on disease prevention and control have been issued as joint recommendations of the World Organization for Animal Health (OIE), the Food and Agriculture Organization (FAO), and the World Health Organization . These recommendations, however, need to be put into practice in a variety of different field situations; the applicability of 1 system rather than another in a given situation must be evaluated, weighing the benefits of a successful result against the drawbacks of failure.\n\n【12】Vaccination can be a powerful tool to support eradication programs if used in conjunction with other control methods. Vaccination has been shown to increase resistance to field challenge, reduce shedding levels in vaccinated birds, and reduce transmission . All these effects of vaccination contribute to controlling AI; however, experience has shown that, to be successful in controlling and ultimately in eradicating the infection, vaccination programs must be part of a wider control strategy that includes biosecurity and monitoring the evolution of infection.\n\n【13】To eradicate AI, the vaccination system must allow the detection of field exposure in a vaccinated flock, which can be achieved by using conventional inactivated vaccines and recombinant vector vaccines. Conventional inactivated vaccines that contain the same viral subtype as the field virus enable detection of field exposure when unvaccinated sentinels left in the flock are tested regularly. This system is applicable in the field but is rather impracticable, especially for the identification of sentinel birds in premises that contain floor-raised birds. A more encouraging system, based on the detection of anti-NS1 antibodies, has been recently developed and can be used with all inactivated vaccines, provided they have the same hemagglutinin subtype as the field virus . This system is based on the fact that the NS1 protein is synthesized only during active viral replication and, therefore, is rarely present in inactivated vaccines. Birds vaccinated with such vaccines will develop antibodies to NS1 only after field exposure. Full and field testing of this system under different circumstances are still in progress , and results should be available before this system is recommended.\n\n【14】To date, the only system that enables detection of field exposure in a vaccinated population and that has resulted in eradication is based on heterologous vaccination and known as \"DIVA\" (differentiating infected from vaccinated animals). This system was developed to support the eradication programs in the presence of several introductions of LPAI viruses of the H7 subtype . Briefly, a vaccine is used that contains a virus possessing the same hemagglutinin, but a different neuraminidase, as the field virus. This vaccination strategy enables detection of antibodies to the neuraminidase antigen of the field virus. For example, a vaccine containing an H7N3 virus can be used against a field virus of the H7N1 subtype. Antibodies to H7 are cross-protective, thus ensuring clinical protection, increased resistance to challenge, and reduction of shedding, while antibodies to the neuraminidase of the field virus (in this case N1) can be used as a natural marker of infection. Experimental data on the quantification of the vaccination effect on transmission within a flock indicate that the reproduction ratio can be reduced to <1 by 1 week after vaccination . Such a reproduction ratio indicates minor rather than major spread of infection. In simple terms, such vaccination interventions will substantially reduce (although not prevent) secondary outbreaks, depending on the immune status of contact birds and flock.\n\n【15】Promising results have also been obtained with vaccines generated by reverse genetics . These vaccines are expected to perform like conventional inactivated vaccines; however, data are not yet available as to their efficacy under field conditions. Recombinant fowlpox vaccines that express the hemagglutinin protein of the field virus have also been reported to be efficacious for reducing shedding levels and providing clinical protection . They enable the detection of field exposure because vaccinated unexposed animals do not have antibodies to any of the other viral proteins. Any test developed to detect antibodies to the nucleoprotein, matrix, NS1, or neuraminidase of the field virus can be used to identify field-exposed birds in a vaccinated population. However, the performance of these vaccines in relation to the immune status of the host to the vector virus is unclear . Recent encouraging studies indicate that vaccination of day-old chicks with maternal antibodies against fowlpox has been successful. Data are lacking on the performances of such vaccines in a population that has been field exposed to fowlpox. Another aspect that must be carefully considered is the host. These vaccines are likely to induce protective immunity only in birds that are susceptible to infection with the vector virus.\n\n【16】Regardless of the vaccine and companion test used, mapping occurrence of infection within the vaccinated population is imperative, primarily to monitor the evolution of infection and to appropriately manage field-exposed flocks. Field exposure represents a means by which infectious virus may continue to circulate in the immune population; for this reason, vaccination can be considered as only part of a control strategy based on biosecurity, monitoring, approved marketing procedures, and stamping out. An inappropriately managed vaccination campaign will likely result in the virus becoming endemic.\n\n【17】Inadequate biosecurity or vaccination practices can lead to transmission between flocks and selection of variants that exhibit antigenic drift. Antigenic drift of H5N2 viruses belonging to the Mexico lineage, resulting in lower identity (less similarity) to the vaccine strain, has been described . Extensive use of vaccine in Mexico has resulted in the emergence of antigenic variants that escape the immune response induced by the vaccine. This occurrence is similar to antigenic drift that typically occurs in animals with a long lifespan (pigs and horses) that are routinely vaccinated and in human beings. Mexico has been vaccinating poultry since the HPAI outbreak in 1994 without applying the DIVA principle. Although no HPAI virus has been reported since the implementation of the vaccination campaign, LPAI viruses continue to circulate. Conversely, a similar approach in Pakistan after the HPAI H7N3 outbreaks in 1995 resulted in the isolation of HPAI H7N3 virus ≈10 years later, in 2004 .\n\n【18】The international scientific community is debating how vaccination of poultry would affect human health. On one hand, vaccinated birds shed less virus; on the other, they do not show any clinical signs of disease and could therefore act as silent carriers. Several factors contribute to the development of infection in humans: insufficient hygienic standards, the characteristics of the strain, and presence of viral dose sufficient to infect a human being. The possibility that vaccinated poultry may not shed enough virus to infect a human being is substantiated by recent field evidence. With reference to the H5N1 crisis, several countries are using vaccination to support control efforts. Vietnam implemented a nationwide vaccination campaign, which was completed in early 2006. The campaign's main achievement is that despite 61 cases of human infection between January and November 2005, no human cases of AI have been reported in Vietnam after December 2005 .\n\n【19】##### Emergency Vaccination\n\n【20】Recent outbreaks in developed countries, notwithstanding their efficient veterinary infrastructures and modern diagnostic systems, have resulted in the culling of millions of birds. Since the year 2000, AI epidemics in areas densely populated with poultry have resulted in 13 million dead birds in Italy in 1999–2000 (H7N1), 5 million dead birds in the United States in 2002 (H7N2), 30 million in the Netherlands in 2003, and 17 million in Canada in 2004. For each of these episodes, biosecurity measures implemented at the farm level were insufficient to prevent massive spread of AI.\n\n【21】Emergency vaccination for AI has become an acceptable tool, in conjunction with other measures, for combating the spread of AI. Using emergency vaccination to reduce the transmission rate could provide an alternative to preemptive culling to reduce the susceptibility of healthy flocks at risk. The effectiveness of such a program depends on variables such as the density of poultry flocks in the area, level of biosecurity and its integration into the industry, characteristics of the virus strain involved, and practical and logistical issues such as vaccine availability and adequate and speedy administration. For this reason, contingency plans that include decision-making patterns under different scenarios should be formulated.\n\n【22】Pivotal work on emergency vaccination has been done in Italy. Application of the DIVA strategy has resulted in the approval of the use of vaccination as an additional tool for the eradication of 2 epidemics of LPAI (H7N1 and H7N3) without massive preemptive killing of animals. Vaccination complemented restriction measures already in place and was integrated into an intensive monitoring program that identified viral circulation in the area  and culled infected birds. In 2000, heterologous vaccination against an H7 virus was used for the first time in the field as a natural marker vaccine. Subsequently, a DIVA strategy was used by Hong Kong to prevent the introduction of H5N1 into its territories .\n\n【23】Although use of a DIVA system enabled international trade of poultry products to continue , vaccination for AI is a new concept, which several countries are reluctant to even consider. Government authorities ultimately decide whether vaccination should be used in a given country; their reluctance is probably driven by legislative and scientific uncertainties, coupled with doubts about how this practice will be used in the field and other considerations such as exit strategy. With reference to trade implications, a new chapter of the OIE Terrestrial Animal Health Code on AI  enables the continuation of trade in presence of vaccination if the exporting country is able to produce surveillance and other data that confirm that notifiable avian influenza is not present in the compartment from which the exports come. This chapter is the result of extensive work by OIE experts and the OIE Central Bureau on the issue of reducing the effect of animal diseases through the use of vaccination and is contained in a recommendation document issued as a result of an international conference held in Buenos Aires (April 14–17, 2004) that strongly supports the use of vaccines for diseases on list A .\n\n【24】##### Prophylactic Vaccination\n\n【25】Prophylactic vaccination for viruses of the H5 and H7 subtypes is a completely innovative concept, primarily because only recently have cost-effective situations been identified. Prophylactic vaccination should generate a level of protective immunity in the target population; the immune response may be boosted if a field virus is introduced. Prophylactic vaccination should increase the resistance of birds and, in the case of virus introduction, reduce levels of viral shedding, provided the same levels of biosecurity are maintained. It should be perceived as a tool to maximize biosecurity measures when risk of exposure is high. Ideally, it should prevent the index case. Alternatively, it should reduce the number of secondary outbreaks, thus minimizing the negative effects on animal welfare and potential economic losses in areas where the density of the poultry population would otherwise result in uncontrollable spread without preemptive culling.\n\n【26】Prophylactic vaccination should be considered only when circumstantial evidence indicates that a given area is at risk. Risk for infection may be divided into 2 categories: 1) high risk for infection with either H5 or H7 subtype (e.g. from migratory birds), and 2) risk for infection with a known subtype (e.g. H7N2 in live bird markets in the United States, countries with high exposure to H5N1). For the first category, a bivalent (H5 and H7) vaccination program could be implemented. Italy has recently implemented such a program in the densely populated poultry area at risk for infection . For the second category, a monovalent (H5 or H7) program would be sufficient.\n\n【27】The choice of vaccine is crucial to the outcome of prophylactic vaccination campaigns. Ideally, vaccines that enable detection of field exposure with any AI virus should be used. Such candidates would be vaccines that enable the identification of field-exposed flocks through the detection of antibodies to an antigen that is common to all type A influenza viruses such as NP, M, or NS1. Such a strategy would detect the introduction of any subtype of AI.\n\n【28】The DIVA system, which uses heterologous neuraminidase, has some limitations in its application for prophylaxis or in situations with risk for introduction of multiple AI subtypes because the system was originally developed to fight a known subtype of AI. The main problem is that the virus against which vaccination is directed must have a different N subtype than the virus present in the vaccine, which, for prophylactic vaccination, is impossible to establish beforehand. An approach to resolving this difficulty is to use seed vaccine strains of the H5 and H7 subtypes that are exhibiting rare neuraminidase subtypes such as N5 or N8. This selection criterion of vaccine strains will greatly reduce the chance that an AI virus of a similar N subtype is introduced. In any case, for surveillance purposes, unvaccinated sentinels should be present in the flock.\n\n【29】Prophylactic vaccination should be continued as long as risk for infection exists. It can be used in a targeted manner for limited periods of time, which requires a detailed exit strategy.\n\n【30】### Conclusions\n\n【31】The scientific veterinary community must control AI infections in poultry for several reasons: to manage the pandemic potential, to preserve profitability of the poultry industry, and to guarantee food security to developing countries. Although biosecurity is recognized as an excellent means of preventing infection, in certain situations the biosecurity standards necessary to prevent infection are difficult to sustain. Vaccination is a potentially powerful tool for supporting eradication programs by increasing the resistance of birds to field challenge and by reducing the amount and duration of virus shed in the environment. Vaccination strategies that encompass monitoring of infection in the field are crucial to the success of such efforts.\n\n【32】Timely information is needed about the efficacy of vaccination in a variety of different avian species, bearing in mind the diverse farming systems used in developed and developing countries. The outcome of such efforts should be made available to the international community because decision makers lack enough information to make educated choices. An enormous effort is required from national governments and funding bodies to make resources available to research programs to develop improved control measures that can be applied under different local conditions. To maximize the global effort to combat this disease, developing and sustaining transversal research programs on AI control, which encompass veterinary and agricultural science, are imperative.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8785832a-f26b-402a-b917-7319d52e0e31", "title": "Emergence of Oseltamivir-Resistant Pandemic (H1N1) 2009 Virus within 48 Hours", "text": "【0】Emergence of Oseltamivir-Resistant Pandemic (H1N1) 2009 Virus within 48 Hours\nEarly descriptions of emergence of H275Y mutants in pandemic (H1N1) 2009 virus showed resistance after 11 and 23 days of therapy in immunosuppressed patients . Also in previous reports, transmission of mutant viruses occurred in immunosuppressed patients , although a cluster among healthy persons demonstrated that H275Y mutants could replicate and cause disease in the absence of drug pressure . Additional reports noted decreasing times to detection of resistance, from 14 to 4 days after therapy . We report development of oseltamivir-resistant pandemic (H1N1) 2009 virus in an infected woman in Singapore within 48 hours of drug treatment.\n\n【1】### The Study\n\n【2】Pandemic (H1N1) 2009 virus was first detected in Singapore in May 2009. Infected patients were placed in isolation and offered oseltamivir, and respiratory samples were collected for screening for H275Y, the principal mutation associated with oseltamivir resistance in influenza A N1 viruses. H275Y was detected in a pandemic (H1N1) 2009 virus isolated from a sample from a 28-year-old female patient on the sixth day of illness within 48 hours of her exposure to oseltamivir.  A sore throat, myalgia, redness of the right eye, and a mild fever with a productive cough had developed on the day she returned to Singapore from Hawaii. Eleven close contacts, exposed before emergence of the mutant, were given oseltamivir prophylaxis on the patient’s fourth day of treatment, and they remained well. By performing sequencing directly on 6 of her respiratory samples and on their viral isolates , we investigated the origin of this H275Y mutant (the second earliest sample of this mutation to be deposited in GenBank).\n\n【3】Only wild-type sequences were detected in samples collected on the day before, the day of, and 14 hours after initiation of oseltamivir therapy . Similarly, only wild-type sequences were detected in 192 clones generated from a sample collected a few hours before initiation of oseltamivir. Pyrosequencing directly on clinical samples collected 38 and 45 hours after initiation of therapy showed 24% and 52% mutant sequences, respectively . The relative amount of virus detected, as determined by the strength of PCR results , increased from days 3 to 5 of illness by ≈1,000-fold. Oseltamivir treatment was initiated on day 4 of illness. On the same day, her maximum body temperature (38.8°C) was recorded, although no other signs or symptoms of clinical deterioration were observed. Her fever resolved on day 5 of illness, and she was allowed out of isolation on day 7 of illness.\n\n【4】When we compared the mutant drug-resistant isolate GN285 with the wild-type drug-sensitive isolate ON129, we found only 1 aa difference, the H275Y resistance-causing mutation in the neuraminidase gene, whereas a comparison of GN285 and ON129 with the reference strain A/Texas/05/2009(H1N1) showed several mutations . Mutation PB1 I435V, shared between GN285 and ON129, did not occur in any of the other 7 drug-resistant strains included in the analysis. The whole genome maximum likelihood tree  showed that the wild-type and resistant viruses isolated from this patient were more closely related to each other than to any other virus in the analysis. Notably, GN285 and ON129 clustered together in 376 of the 500 bootstrap tests.\n\n【5】### Conclusions\n\n【6】Our data indicate that oseltamivir resistance developed within 2 days. This time is similar to the interval for development of resistance to adamantanes in subtype H3N2 viruses when 30% of treated patients shed resistant, transmissible virus within 3 days of beginning treatment . Four cases of H275Y infection have been detected, with the use of sequencing, among 1,060 pandemic (H1N1) 2009 isolates tested (0.47%) in Singapore since June 2009 (not including the case reported here). Only 1 case was found in a pretreatment sample; the other 3 were identified after treatment. The emergence of H275Y might also be affected by the timing of therapy. In the case reported here, oseltamivir treatment was begun on day 4 of illness when viral titers were almost maximal, which is probably the stage of illness best suited to select for resistant mutants because the presence of mutations is likely to be greatest when replication is greatest. Notably, rates of H275Y are high, reaching 13%  among immunocompromised groups in whom high viral titers might also be a contributory factor.\n\n【7】Pyrosequencing directly on clinical material allows the measurement of relative quantities of viral variants without introducing errors inherent in viral culture, which is known to favor the growth of H275Y mutants . Pyrosequencing cannot exclude the presence of subpopulations of <5%–10%, but if small proportions of mutant virus had been present, they would have been detected in the culture of the sample collected 14 hours after exposure to oseltamivir; however, only wild-type sequences were detected. Similarly, only wild-type sequences were detected in 192 clones derived directly from the sample collected a few hours before the first dose of oseltamivir. The phylogenetic data show, at the amino acid and nucleotide level, that the resistant and sensitive isolates cluster together, apart from other viruses. These data support the hypothesis that the H275Y mutant arose de novo from the wild-type virus from the same patient.\n\n【8】The fact that the effects of oseltamivir are likely to be greatest in severe disease , but of modest benefit in mild infections , has led to proposals for restricting the use of antiviral agents and the use of alternative antiviral agents and multidrug therapy to prevent the emergence of resistance  in severe cases. The proposed interventions may be of little consequence compared with the association and co-selection of H275Y with other genetic determinants. Although in isolation H275Y compromises seasonal influenza (H1N1) by reducing the amount of neuraminidase expressed on the cell surface, other mutations (R194G, V234M, and R222Q) may compensate and restore its expression to levels found in wild-type virus, without H275Y . This circumstance may explain the emergence and spread of H275Y in the absence of drug pressure in seasonal influenza (H1N1), which increased from being negligible in 2007 to 95% in March 2009  despite a low consumption of oseltamivir . More than 99% of all pandemic (H1N1) 2009 neuraminidases have G at position 194, which corresponds to the R194G in seasonal influenza (H1N1). However, the effects of mutations are not easily transferable among different influenza (H1N1) types and may need to be tested separately .\n\n【9】The clinical effects of the spread of H275Y in seasonal influenza (H1N1) have been minimal because seasonal influenza (H1N1) now accounts for an insignificant proportion of influenza . However, if H275Y mutants of pandemic (H1N1) 2009 emulate the expansion of resistant seasonal influenza (H1N1), the effect might be substantial because pandemic (H1N1) 2009 accounted for 90%–95% of circulating influenza A viruses in the Northern and Southern Hemispheres in late 2009 . We are speculating, however, because our laboratory data show that pandemic (H1N1) 2009 fell from 62% to 29% of 436 influenza cases from May 2010 to mid June 2010, Singapore’s main influenza season, whereas the presence of influenza (H3N2) has risen from 23% to 53% (influenza B accounts for 15%–20%). As the next influenza season in the Southern Hemisphere approaches, the relative mixture of subtype H3N2 and H1N1 viruses will be under scrutiny again, not only to predict “best bet” vaccine components but also to ascertain their associated resistance patterns. Whatever the epidemiologic data exhibit, clinicians should consider resistance when patients do not respond to treatment for pandemic (H1N1) 2009 because H275Y can emerge literally overnight, as the case reported here reminds us.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "25860ebe-2ee9-42d2-97c1-0ee336981748", "title": "Clinical and Epidemiologic Characterization of WU Polyomavirus Infection, St. Louis, Missouri", "text": "【0】Clinical and Epidemiologic Characterization of WU Polyomavirus Infection, St. Louis, Missouri\nAn initial report described the identification of WU polyomavirus in 6 (0.7%) of 890 respiratory tract samples collected in St. Louis, Missouri, USA, and in 37 (3.0%) of 1,245 respiratory tract specimens tested from Brisbane, Queensland, Australia . The goal of our study was to extend these initial findings by determining the prevalence of WU polyomavirus in a larger patient cohort in St. Louis.\n\n【1】### The Study\n\n【2】We tested 2,637 nasopharyngeal swabs or nasal washes (from patients 1 day to 88 years of age) submitted to the virology laboratory at St. Louis Children’s Hospital for routine respiratory virus detection from July 2003 through June 2004. Of these samples, 2,263 were from children < 4 years of age (including 419 newborns) and 374 were from children >4 years of age. The specimens were extracted with the automated Roche MagNA Pure LC extractor and MagNA Pure LC Total Nucleic Acid Isolation Kit (Roche Diagnostics, Indianapolis, IN, USA).\n\n【3】For real-time PCR, amplification primers WU-TAB02-F 5′- tgttgcatccatttgttacattcat-3 ′ and WU-TAB03-R 5′-GAAAGAACTGTTAGACAAATATATAGGCCTTA \\-3 ′ and the minor groove binder probe WU-TAB04-pro 5′-6FAM atgtcagcaaattcMGBNFQ-3 ′ were used with a commercially available universal TaqMan real-time PCR master mix and ABI 7500 Real-Time Thermocycler (Applied Biosystems, Foster City, CA, USA). All WU polyomavirus–positive specimens were screened for 17 additional viruses (influenza A and B; RSV A and B; PIV 1–4; human metapneumovirus; adenovirus subgroups B, C, and E; rhinovirus; and coronaviruses OC43, 229E, and NL63) by using the EraGen MultiCode-PLx respiratory virus panel as described previously .\n\n【4】Each clinical specimen was assigned a code. Collection of clinical data was approved by the Washington University Human Research Protection Office. Pertinent demographic, historical, and clinical information, when available, was collected by using a standard collection form. Statistical significance was determined by using 2-tailed Fisher exact χ 2  tests with Epi Info software version 3.4 (Centers for Disease Control and Prevention, Atlanta, GA, USA )\n\n【5】Seventy (2.7%) of the 2,637 tested specimens were positive for WU polyomavirus; 71% of the positive samples were also positive for \\> 1 other respiratory virus. Of the 70 positive samples, 5 were omitted from analysis because of chart unavailability. The remaining 65 samples were collected from 60 individual patients (5 specimens were serial samples associated with distinct clinical syndromes in 2 immunocompromised patients).\n\n【6】Of the 60 WU-positive patients, 31 (52%) were female. The ethnic breakdown was as follows: 50% African-American, 47% Caucasian, 3% other. Positive specimens were noted for patients 1 day to 15 years of age . The highest and lowest rates of infection are displayed in the Figure , panel A.\n\n【7】Patients positive for WU polyomavirus were detected throughout the year. A small peak was observed in July 2003, and a second small peak was observed in April and May 2004 . WU polyomavirus was the only virus detected in a 1-day-old full-term infant delivered by cesarean section who had been transferred to St. Louis Children’s Hospital with respiratory distress requiring intubation. He was afebrile with lung opacities on chest radiograph. Patent ductus arteriosus and pulmonary hypertension were eventually diagnosed.\n\n【8】The 3 oldest patients positive for WU polyomavirus in this cohort were immunosuppressed. They included a 12-year-old with Evans syndrome and a 15-year-old with severe combined immunodeficiency syndrome (both post–bone marrow transplant) and a 14-year-old with end-stage renal disease and asthma.\n\n【9】The most common clinical findings in the patients with WU polyomavirus are listed in the Table . The most frequent diagnoses were pneumonia (31%) (although 40% had positive bacterial cultures), bronchiolitis (25%), and upper respiratory tract infections (15%). We also compared all the measured parameters from the patients who were infected with WU alone to the patients who were co-infected with other viruses. In most cases, no statistically significant differences occurred between the co-infected and WU polyomavirus–only patients, except that more co-infected patients than WU polyomavirus–only patients had rhinorrhea (23/47 vs. 2/18; p = 0.005) and upper respiratory tract symptoms (30/47 vs. 6/18; p = 0.049). In addition, significantly more children with co-infection than with only WU polyomavirus had prior daycare exposure (18/45 vs. 1/15; p = 0.02).\n\n【10】The cohort of 2,637 samples included several sets of sequential samples taken from the same patient during the course of prolonged illness. In 2 patients, sequential samples obtained over a span of 6–8 weeks were positive for WU polyomavirus. The first patient was a 4-year-old girl with hemophagocytic lymphohistiocytosis, who had 4 distinct respiratory specimens that tested positive during a 2-month period. Her first positive specimen was obtained while she was asymptomatic during admission for a bone marrow transplant in September 2003. A second sample was obtained during a clinic visit for nasal congestion and cough in November 2003. Her third sample (also positive for coronavirus OC43) was obtained during an admission 5 days later for pneumonia. Finally, a fourth sample (negative for the previously detected coronavirus OC43) was taken 7 days later during the same admission after an episode of apnea.\n\n【11】The second patient was a 16-month-old child with biliary atresia admitted for a liver transplant in September 2003. He had 3 positive samples during a 6-week period. An initial sample taken on admission was negative for WU polyomavirus. Six weeks after transplant, fever and shortness of breath requiring intubation developed. A sample taken then was positive for both WU polyomavirus and adenovirus. Four weeks later, worsening shortness of breath and fever developed. His blood cultures were now also positive for _Klebsiella_ and _Enterobacter_ spp. and his third respiratory sample demonstrated both WU polyomavirus and rhinovirus. Two weeks later, fever, hypoxia, and increasing secretions developed, with his fourth sample positive only for WU polyomavirus. Followup samples obtained 3 months later for each patient indicated clearance of WU virus.\n\n【12】### Conclusions\n\n【13】Patients infected with WU polyomavirus in this cohort were primarily hospitalized with pneumonia, bronchiolitis, and upper respiratory tract infections. One new observation is that multiple respiratory specimens sampled from the same patient over 6–8 weeks were positive for WU polyomavirus, which suggests that WU polyomavirus may persistently infect humans. Both patients were immunocompromised, although they were able to clear infections with other viruses (coronavirus OC43, rhinovirus, adenovirus), which suggests that the continued detection of WU polyomavirus was not due to a completely incapacitated immune system. Sequence analysis of a 250-bp fragment of the VP2 gene of WU polyomavirus amplified from multiple samples from the 2 patients showed no sequence polymorphisms between the initial and later samples (data not shown). As some sequence variation has previously been reported in this locus , these data are consistent with the model of persistent infection. The detection of WU polyomavirus in the respiratory secretions of a 1-day-old infant suggests that vertical transmission of WU polyomavirus from mother to fetus may occur, although further studies are needed to verify this suggestion.\n\n【14】In conclusion, WU polyomavirus was detected in 2.7% of patients with respiratory tract infections. A high percentage of coinfection with other respiratory viruses was detected, complicating interpretation of the clinical findings. However, WU polyomavirus was the sole virus detected in 20 specimens from patients with respiratory illness, which suggests that it may be a respiratory pathogen. Finally, the observed persistence of this virus suggests analogy to BK and JC viruses in this regard .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1c22dcda-a498-4a17-a821-9a29a4dc5c9a", "title": "Clinical and Molecular Characteristics of Human Rotavirus G8P[8] Outbreak Strain, Japan, 2014", "text": "【0】Clinical and Molecular Characteristics of Human Rotavirus G8P[8] Outbreak Strain, Japan, 2014\nRotaviruses, the leading cause of acute gastroenteritis in children worldwide, are classified into G and P genotypes on the basis of 2 outer capsid proteins, viral protein (VP) 7 and VP4. A recently established extended rotavirus genotyping system based on the sequence of all 11 genome segments  grouped most human rotaviruses into 2 genotype constellations: Wa-like (G1/3/4/9-P-I1-R1-C1-M1-A1-N1-T1-E1-H1) and DS-1–like (G2-P-I2-R2-C2-M2-A2-N2-T2-E2-H2) strains.\n\n【1】In industrialized countries, rotavirus genotype G8 infection is common in bovines but rarely occurs in humans; however, the G8 strains are highly prevalent among humans in some countries in Africa . We investigated the clinical and molecular features of G8P rotavirus, which, we unexpectedly found to be the predominant genotype in southwestern Hokkaido Prefecture, Japan, in 2014.\n\n【2】### The Study\n\n【3】During March–July 2014, we obtained rotavirus-positive fecal samples from 165 children in Hokkaido with acute gastroenteritis. The children were receiving care as inpatients or outpatients at 1 of 6 medical facilities (4 hospitals and 2 clinics) in the cities of Sapporo, Tomakomai, Muroran, and Urakawa .\n\n【4】For each fecal sample, we prepared a 10% fecal suspension, from which we extracted viral RNA. We performed reverse transcription PCR on the RNA by using the SuperScript II Reverse Transcriptase (Invitrogen, Carlsbad, CA, USA); PrimeSTAR GXL DNA polymerase (Takara, Shiga, Japan); and previously described primers . We used the BigDye Terminator v.3.1 Cycle Sequencing Reaction Kit (Applied Biosystems, Foster City, CA, USA) to sequence PCR amplicons. For some of the rotavirus samples, next-generation sequencing was performed at the National Institute of Infectious Diseases in Tokyo, Japan, as described previously . Sequences of the rotaviruses used in this study were submitted to the DDBJ under accession numbers LC102884–LC103134 and LC105000–LC105532.\n\n【5】We successfully determined G and P genotypes for 148 of the 165 rotavirus samples by using the RotaC rotavirus genotyping tool . The most common genotype was G8P, which was identified in 58 samples (39.2%), followed by G1P (25.7%), G9P (20.3%), and G2P (12.8%) .\n\n【6】We obtained clinical data for all 84 patients who sought care at the hospital or clinic in Tomakomai. Demographic and clinical characteristics (e.g. age, sex, history of rotavirus vaccination, duration of fever, and duration and frequency of diarrhea and vomiting) were not substantially different between 42 patients with G8P rotavirus infection and 42 patients with non-G8P rotavirus infection. The proportion of patients admitted to hospitals was also similar in the 2 groups .\n\n【7】We selected 15 G8P strains for whole-genome analysis. All strains had the same genotype constellation, G8-P-I2-R2-C2-M2-A2-N2-T2-E2-H2, indicating a genomic backbone of the DS-1 genotype constellation. The genomes of these G8P strains shared >99.6% nt identity with each other . All 11 genome segments of strain To14-0 (the representative G8P strain in this study) exhibited the highest nucleotide identity to human G8P strains isolated in Southeast Asia in 2014 (represented by strain RVN1149 from Vietnam \\[>99.4% nt identity\\] and NP-130 from Thailand \\[>99.5% nt identity\\])  . This finding suggests that the strains share a common G8P origin.\n\n【8】The VP7 gene of rotavirus strain To14-0 shared the highest nucleotide identity with the VP7 genes of human G8P strains from Southeast Asia, including strains RVN1149 and NP-130 (99.4% and 99.7% identity, respectively), and it shared slightly lower identity to the VP7 gene of human strain 04-97s379 (97.8%) from Taiwan, which is speculated to be of bovine origin  . The VP7 genes of other G8 strains isolated in Japan were more distantly related to the To14-0 VP7 gene (e.g. human AU109 and bovine strains shared 89.5% and 81.9%–85.1%, respectively, with To14-0) . The VP7 genes of the human G8 strains prevailing in Africa were also distantly related (<90% nt identity) to the VP7 gene of To14-0.\n\n【9】Among the 11 To14-0 genome segments, 8 (VP2 _–_ VP4, VP6, nonstructural protein \\[NSP\\] 1 _–_ 3, and NSP5) were highly similar to those of the human DS-1–like P strains that have been isolated in Asia since 2012 (e.g. SKT-109, NT004, and LS-04) , including the strains isolated in this study (e.g. To14-41) . In addition, the VP6 and NSP5 genes of the strains isolated in this study were also highly similar to those of human G2P strains circulating in South Korea (strain CAU15-11) and Thailand (strain NP-M51) .\n\n【10】In contrast, the VP1 and NSP4 genes of To14-0 were only distantly related to those of the DS-1–like P strains isolated in Asia (e.g. SKT-109, NT004, and LS-04), including the strains isolated in this study (e.g. To14-41) . The To14-0 VP1 gene shared high nucleotide identity with the VP1 genes of human G10P strain PR457 from Italy (98.1%), which are probably the result of independent zoonotic transmissions . The To14-0 NSP4 gene shared high nucleotide identity with the NSP4 genes of human strains BSGH38 from India (96.7%) and the caprine G6P strain GO34 from Bangladesh (96.0%) .\n\n【11】### Conclusions\n\n【12】The clinical characteristics recorded for patients infected with G8P rotaviruses and those infected with non-G8P rotaviruses did not differ . Our findings suggest that the severity of gastroenteritis caused by newly emerging G8P rotaviruses could possibly be attenuated by 1) the existence of VP7/VP4 genotype cross-reactive (heterotypic) protective responses; 2) protective immunity associated with other segments, such as VP6 and NSP4 ; or 3) both of these factors combined.\n\n【13】The VP7 genes of the human G8P strains isolated in this study and in Southeast Asia appear to have a close relationship with bovine strains from Asia but not from Japan, and the VP7 gene of human G8 or bovine G8 strains previously isolated in Japan are distantly related to them. Therefore, the VP7 genes in the G8P strains from this study may have originated from a bovine strain from Asia. As with the VP7 genes, the VP1 and NSP4 genes are also assumed to have been derived from artiodactyl strains.\n\n【14】Eight genome segments (VP2 _–_ VP4, VP6, NSP1 _–_ NSP3, and NSP5) of the human G8P strains isolated in this study and from Southeast Asia are closely related to those of the DS-1–like P strains that have emerged and spread in Japan and other countries of Asia since 2012 . Therefore, these 8 genome segments of the G8P strains from this study may be derived from the DS-1–like P strains in Asia.\n\n【15】For the reasons we have stated, the G8P strains isolated in this study were speculated to be formed outside of Japan by multiple reassortment events between the DS-1–like P strains and bovine strains in Asia. The resulting strain was probably recently introduced into Japan.\n\n【16】The predominance of novel DS-1–like G8P strains noted in this study indicates that these strains are sufficiently adapted to humans to sustain human-to-human transmission in an industrialized country. This finding suggests that these G8P rotavirus strains could spread to other regions in the near future. Continuing surveillance is required to monitor the circulating wild-type strains, and rotavirus genotype constellations and clinical information must be analyzed to understand rotavirus virulence in humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d4d7640-1d4a-4c7a-babc-cedc57380ff4", "title": "Molecular Classification of Enteroviruses Not Identified by Neutralization Tests", "text": "【0】Molecular Classification of Enteroviruses Not Identified by Neutralization Tests\nThe human enterovirus (HEV) genus of the family Picornaviridae includes the human pathogens that cause a wide spectrum of acute disease, including hand, foot, and mouth disease  , aseptic meningitis , encephalitis , and neonatal sepsislike disease . Sixty-four serotypes of HEV have been recognized antigenically by neutralization tests with anti-HEV antibodies  . HEVs have long been classified on the basis of serotype-specific antisera in virus neutralization tests , the only method available for serotyping HEVs. However, virus neutralization is both labor- and time-intensive, and antigenic variants in many serotypes of HEV can affect test results  .\n\n【1】The HEV genome comprises a 5' nontranslated region (NTR), a long open reading frame that encodes a protein of approximately 2,100 amino acid residues, a short 3' NTR, and a polyadenylated tail. The polyprotein is co- and post-translationally cleaved to yield four structural proteins: VP4, VP2, VP3, and VP1  . Recently, attempts have been made to classify the HEV serotypes by using the partial nucleotide sequences of the HEV genomes (i.e. the 5' NTR , the VP4-VP2 junction , and VP1 ). Methods for molecular classification of HEVs should not only identify the serotypes rapidly but also detect antigenic variant strains or new serotypes. A new serotype of HEV has recently been identified by comparing the complete VP1 nucleotide sequences; its proposed name is HEV73  .\n\n【2】To investigate the HEV serotypes of six HEV-like viruses that were not neutralized by standard HEV typing sera, we determined the complete VP4 nucleotide sequences of these 6 viruses and 21 HEV antigenically defined serotypes, then performed phylogenetic analysis with another 8 HEV serotypes available from GenBank. The classifications of the untypeable viruses were confirmed by using HEV-monospecific antisera or an additional phylogenetic analysis with the VP4 sequences. The molecular classification of HEV with the complete VP4 sequences is useful for identifying the HEV serotypes.\n\n【3】### Methods\n\n【4】##### Virus Isolation and the Neutralization Test\n\n【5】The clinical specimens were injected into Vero, RD-18S, or MA104 cells to isolate viruses. All cells were grown in minimum essential medium (MEM) containing 10% fetal bovine serum (FBS) and maintained in MEM containing 1% to 2% FBS after being added to 48-well plates (Sumitomo Bakelite, Tokyo, Japan). The cells were incubated for 1 week, after which culture fluids were passaged and incubated for another week. Cultured cells showing cytopathic effects were regarded as virus isolation-positive and, together with the culture supernatant, were harvested and stored at -80°C before use. To serotype the viruses, microneutralization tests were performed with antiserum pools of Lim and Benyesh-Melnick  (Denka Seiken, Tokyo, Japan) or in-house monospecific immune sera against coxsackie virus A10 (CAV10), CAV16, and HEV71, respectively.\n\n【6】##### Viruses\n\n【7】Of the six viruses that could not be identified by the neutralization tests described above , strains OC/0071, OC/0073, and OC/00272 were isolated from patients diagnosed with aseptic meningitis by using RD-18S cells. OC/00219, OC/00260, and OC/00261 were isolated from patients diagnosed with hand, foot, and mouth disease or aseptic meningitis by using Vero cells. No sera from these patients was available for analysis. Twenty-one serotypes were isolated and identified in our laboratory during 1995-2000 ; these strains were used in the experiments. For additional investigations of HEV71, we used eight HEV71 strains isolated and identified in our laboratory .\n\n【8】##### RNA Extraction and Reverse Transcription\n\n【9】Viral RNAs were extracted from the cell-culture supernatants by using ISOGEN-LS (Nippon Gene, Tokyo, Japan). cDNAs were synthesized with an Omniscript Reverse Transcriptase Kit (QIAGEN K.K. Tokyo, Japan) according to the manufacturer’s instructions. The primers used for the synthesis were EVP-2 (5'-CCTCCGGCCCCTGAATGCGGCTAAT-3' relative to nt 444-468 in the genome of Poliovirus (PV) Sabin 1 strain)  and OL68-1 (5'-GGTAAYTTCCACCACCANCC-3' relative to nt 1178-1197 of Sabin 1), as described  .\n\n【10】##### Polymerase Chain Reaction Amplification of cDNAs\n\n【11】Polymerase chain reaction (PCR) was performed by using 2 μL of each cDNA in a 50-μL reaction mixture containing 1.5 U of Taq DNA polymerase (Takara Shuzo, Shiga, Japan), 20 pmol of EVP-2 primer, and 20 pmol of OL68-1 primer. Each reaction was incubated in a GeneAmp 9700 thermal cycler (Applied Biosystems, Foster City, CA) according to the following protocol: 5 minutes at 95°C, 40 cycles of 95°C for 30 seconds, 68°C for 30 seconds, 72°C for 1 minute, and then at 72°C for 5 minutes. After the appearance of approximately 750 bp-specific amplified fragments was confirmed by agarose gel electrophoresis, the amplicons were purified with a QIAquick PCR purification kit (QIAGEN).\n\n【12】##### DNA Sequence Analysis\n\n【13】Approximately 100 ng of purified amplicon was used in the reaction with the BigDye Terminator Cycle Sequencing FS Ready Reaction Kit (Applied Biosystems), and DNA sequencing was performed by using an ABI PRISM 310 DNA sequencer (Applied Biosystems). All DNA sequencings were performed on both strands using EVP-4 (5'-CTACTTTGGGTGTCCGTGTT-3' relative to nt 541-560 in the genome of PV Sabin 1 strain) as the forward primer and OL68-1 as the reverse primer  . Sequencer software (version 3.0; Hitachi Software, Tokyo, Japan) was used to determine the approximately 600-bp nucleotide sequence spanning 5' NTR to one third of VP2 (including all of VP4), translate nucleotide sequence to amino acid sequence, and decide the complete VP4 coding sequence of each virus.\n\n【14】##### Phylogenetic Analysis\n\n【15】A phylogenetic tree based on the complete VP4 nucleotide sequence was constructed by the neighbor-joining method  as implemented with the CLUSTAL X program . The reliability of the neighbor-joining tree was estimated by bootstrap analysis with 1,000 pseudoreplicate datasets. The complete VP4 sequences of eight HEV serotypes not isolated in our laboratory were obtained from GenBank and included in the HEV analysis. Complete VP4 nucleotide sequences of another 18 HEV71 strains were obtained from GenBank and used in the phylogenetic analysis.\n\n【16】##### Remicroneutralization Tests\n\n【17】According to results of HEV phylogenetic analysis with the complete VP4 nucleotide sequences, remicroneutralization tests using monospecific antiserum against echovirus 18 (EV18; Denka Seiken), or HEV71 (anti-HEV71/BrCr and anti-HEV71/C7 sera; both supplied by the National Institute of Infectious Diseases, Japan) were performed to confirm the serotype of the untypeable strains from the first microneutralization assay.\n\n【18】##### Complete VP1 Nucleotide Compared with Deduced Amino Acid Sequences of HEV71 Strains\n\n【19】The complete VP1 nucleotide sequences of HEV71 strains OC/00168, 0C/00219, OC/00260, and OC/00261 were determined by the same procedure described above, except for the primers. The primers used for the analysis of VP1 nucleotide sequence were 71F2399 (5' -AGAAYTTYACCATGAAACTG-3' relative to nt 2380-2399 in the genome of HEV71 MS/7423/87 strain ; the nucleotide positions of the following are also relative to this strain: 71F2793 (5'-AGACATAACTGGYTACGCCAC-3' nt 2774-2793) and 71F3042 (5'-CATGTCACCYGCGAGCGCTT-3' nt 3023-3042) as the forward, 71R2712 (5'-CTACCAARCCTGCCCTACTG-3' nt 2693-2712), 71R3066 (5'-GGTACCCGTCGTAAAACCAC-3' nt 3047-3066) and 71R3376 (5'-AAGTTGCCCACGTAGATGGC-3' nt 3357-3376) as the reverse. The VP1 nucleotide sequence of HEV71 BrCr strain  was obtained from GenBank. Sequencer software (version 3.0; Hitachi Software) was used for determination and comparison of the complete VP1 nucleotide and deduced amino acid sequences of these HEV71 strains.\n\n【20】### Results\n\n【21】##### Determination of Complete VP4 Nucleotide Sequences of HEVs\n\n【22】During May to July 2000, six viruses submitted to our laboratory (OC/0071, OC/0073, OC/00219, OC/00260, OC/00261, and OC/00272) could not be neutralized by standard pools of HEV typing sera and three antimonospecific sera . However, the cytopathic effects of these viruses on RD-18S or Vero cells were all HEV-like (data not shown). To identify the serotypes of these untypeable HEV-like viruses by a method other than the neutralization assay, we determined the complete VP4 nucleotide sequences of all 6 strains and another 21 HEV serotypes identified in our laboratory over the past 6 years. The 3' end of the VP4 gene of each virus was determined from the deduced amino acid sequences as described . The complete VP4 nucleotide sequences of all HEV strains used in this study were 207 nt long, and the deduced amino acid sequences of all VP4 proteins were 69 amino acids long (data not shown).\n\n【23】##### Phylogenetic Analysis of HEVs\n\n【24】A phylogenetic tree was constructed based on the complete VP4 nucleotide sequences of the 6 HEV-like untypeable strains, the 21 HEV serotypes identified in our laboratory as prototype strains, and another 8 HEV serotypes available from the GenBank database . The 29 different HEV serotypes defined antigenically were clustered in four distinct lineages, as described  . Three of the six untypeable strains (OC/0071, OC/0073, and OC/00272) were classified nearest to EV18. The VP4 nucleotide sequences of strains OC/0071 and OC/0073 were identical. The VP4 gene sequence of OC/00272 was the same as that of OC/99-Hanasaka, which was used as a prototype strain for EV18. The nucleotide sequences of these two clusters differed by 5 nt, but the deduced amino acid sequences were the same (data not shown). The other three untypeable strains (OC/00219, OC/00260, and OC/00261) were classified nearest to HEV71. The VP4 sequence of OC/00219 was the same as that of OC/00168, which was used as a prototype strain for HEV71. The VP4 nucleotide sequences of OC/00260 and OC/00261 were identical. The difference between these two clusters was 11 nt. The deduced amino acid sequences were the same (data not shown).\n\n【25】##### Remicroneutralization Tests\n\n【26】According to the results of the phylogenetic analysis based on the complete VP4 nucleotide sequences, remicroneutralization tests were performed. Microneutralization tests using the monospecific immune serum for EV18 were done against OC/0071, OC/0073, and OC/00272, and this serum neutralized these viruses. The same tests, using the two species of monospecific immune serum, anti-HEV71/BrCr and anti-HEV71/C7, were performed against OC/00219, OC/00260, and OC/00261, but neither serum neutralized the viruses .\n\n【27】##### Phylogenetic Analysis of HEV71 Strains\n\n【28】To establish whether OC/00219, OC/00260, and OC/00261 belong to HEV71, we used another phylogenetic analysis based on the complete VP4 nucleotide sequences of various HEV71 strains . In this analysis, we examined eight HEV71 strains isolated and identified in our laboratory from 1996 to 2000 . All these HEV71 strains except for OC/9632 were identified by microneutralization tests with anti HEV71/BrCr serum (data not shown). Of the HEV71 strains available from GenBank, two were isolated in the United States in 1970 and 1987, respectively  , four in Malaysia in 1997  , one in Singapore in 1998  , eight in Taiwan in 1998 , two in the United Kingdom in 1999, and one in China (year unknown). The HEV71 strains were clustered in three distinct genotypes, designated A, B, and C. The genotype nomenclature of HEV71 strains for phylogenetic analyses based on the VP1  and VP4  nucleotide sequences has been reported, and the results  were consistent with previous findings. Among the HEV71 strains that were identified in our laboratory, only OC/99-Ikeda was classified in genotype C. Seven of eight strains identified in our laboratory by neutralization tests were classified in genotype B; five of these had the same VP4 nucleotide sequence. OC/00219, OC/00260, and OC/00261 were also classified in this genotype. This result demonstrated that strains OC/00219, OC/00260, and OC/00261 were HEV71 serotypes.\n\n【29】##### Comparison of the Complete VP1 Nucleotide and Deduced Amino Acid Sequences of HEV71 Strains\n\n【30】Strains OC/00219, OC/00260, and OC/00261 were classified in HEV71 by the phylogenetic analysis, although these viruses were not neutralized by monospecific anti HEV71 sera. Because the VP1 protein contains a number of important neutralization sites , we determined the complete VP1 nucleotide sequences and compared the deduced amino acid sequences of OC/00219, OC/00260, and OC/00261. OC/00168 used as a prototype strain for HEV71 was also analyzed because this strain was neutralized by anti-HEV71/BrCr serum; moreover, its VP4 gene was the same as that of OC/00219 . The complete VP1 nucleotide sequences of these strains were 891 nt long, and the deduced amino acid sequences were 297 amino acids long. The differences of VP1 nucleotide sequences were 4 to 42 nt (0.4% to 4.7%), and the difference of deduced amino acid sequences was one amino acid (0.3%) among these viruses. The differences of VP1 nucleotide sequences between OC/00168 and OC/00219 were 4 nt (0.4%), and the deduced VP1 amino acid sequences of these strains were the same. The VP1 nucleotide and deduced amino acid sequences of OC/00260 and OC/00261 were identical (data not shown). The deduced VP1 amino acid sequences of these four strains were compared with that of BrCr . There were 18 amino acid (6%) differences between BrCr and other strains. The different amino acid positions of strains OC/00168, OC/00219, OC/00260, and OC/00261 against BrCr were the same. Any mutated residues distinguishable BrCr and OC/00168 from OC/00219, OC/00260, and OC/00261 were not recognized in the VP1 amino acid sequences.\n\n【31】### Discussion\n\n【32】The serotype identification of HEVs has been performed by microneutralization tests using standard HEV antiserum pools . Since >60 serotypes of HEV are known to infect humans , the HEV serotype is almost impossible to identify by using monospecific antiserum from the first microneutralization test. Furthermore, the neutralization test is labor-intensive and time-consuming, requiring several weeks. As an alternative, identification based on nucleotide sequences has been used successfully in several laboratories . To investigate the serotypes of the six untypeable HEV-like viruses that were not neutralized by the standard HEV antisera, we used phylogenetic analyses based on the complete VP4 nucleotide sequences of HEVs and were able to determine the serotype of each virus in the light of these results. OC/0071, OC/0073, and OC/00272 were thought to be EV18 strains by the phylogenetic analysis  and were neutralized by the monospecific anti-EV18 serum. These results indicate that the phylogenetic analysis based on the VP4 nucleotide sequence is consistent with the result of the microneutralization tests using the serotype-specific sera. OC/00219, OC/00260, and OC/00261 were thought to be HEV71 strains by the same analysis , but these viruses were not neutralized by the two monospecific anti-HEV71 sera. The phylogenetic analysis based on the HEV71 VP4 sequences confirmed that these viruses were HEV71 strains belonging to genotype B . We considered that OC/00219, OC/00260, and OC/00261 were all HEV71 strains not neutralized by anti- HEV71/BrCr and anti-HEV71/C7 sera, both available as standard monospecific anti HEV71 serum in Japan. These results also indicate that phylogenetic analysis with the VP4 sequences of HEVs can identify the serotypes in the same way as neutralization tests with HEV serotype-specific antisera. We are now preparing antiimmune sera against OC/00219, OC/00260, and OC/00261, respectively, to confirm antigenically that these are the prime strains of HEV71 neutralized by anti-HEV71/BrCr serum.\n\n【33】Oberste et al. have shown that HEV VP1 nucleotide sequences correlate with antigenically defined serotypes and have demonstrated the utility of VP1 sequences as a molecular surrogate for antigenic type . They have also shown that the VP1 sequences have a better correlation with HEV serotypes than the 5' NTR or the VP4-VP2 junction  . The phylogenetic analysis based on the VP4 sequences we have described also correlates well with HEV serotypings by antiimmune sera. We used 21 HEV serotypes antigenically defined in our laboratory and another 8 strains available from GenBank as prototype strains in this analysis. We do not know whether 29 serotypes are sufficient for the phylogenetic analysis of HEV, as there are >60 serotypes. The good result of HEV phylogenetic classification based on the VP4 sequences might depend on the prototype numbers (29 of 64 serotypes) that we used. Ishiko et al. who performed HEV phylogenetic analyses based on VP4 sequences  , used 45 HEV serotypes as prototype strains and obtained a phylogenetic tree similar to ours  except for a difference in the prototype strain numbers. Another phylogenetic analysis based on the VP4 sequences in this article was performed against the HEV71 strains . For this analysis, the HEV71 strains were clustered in three distinct genotypes, and the nomenclature was almost the same as for the HEV71 analyses based on the VP1 nucleotide sequences  _._ Recently, Chu et al. also reported the appropriateness of the phylogenetic analysis with the VP4 sequences for the molecular epidemiology of HEV71 outbreak in Taiwan in 1998  . These results suggest that the phylogenetic analysis based on the VP4 nucleotide sequences is also useful as a molecular surrogate for antigenic HEV serotyping. The analysis was more convenient based on the VP4 sequences than the VP1 sequences, since the complete VP4 sequence is 207 nt and the complete VP1 sequences are 834 to 951 nt  , although the 3’ third of the VP1 sequence of 365 nt was used  .\n\n【34】The VP4 nucleotide sequences of OC/99-Hanasaka and OC/00272 were identical, but the results of neutralization assays were different. OC/99-Hanasaka was easily neutralized by HEV pooled sera against EV18, but OC/00272 was not. The same results were observed for strains OC/00168 and OC/00219 HEV71, i.e. the results of their neutralization tests differed in spite of the VP4 sequence identity. These results indicate that the VP4 nucleotide sequences are highly conserved even though the neutralizable epitopes are antigenic variants. We compared the VP4 nucleotide and deduced amino acid differences of HEV71 strains, BrCr  , E1387  , OC/9632, OC/99-Ikeda, OC/0078, OC/00219, and OC/00260. HEV71 genotypes indicated 1 to 37 nt (0.5% to 17.9%) differences. However, we found no amino acid differences (100% identity) . Complete homology of the HEV71 VP4-deduced amino acid sequences has also been described , and Singh et al. demonstrated amino acid substitutions in the VP2 and VP3 regions, with the greatest variation in VP1  . These results indicate that VP4 is the most stable protein; accordingly, VP4 genes will be suitable for the molecular identification of HEV serotypes in the future.\n\n【35】VP4 is not exposed on the outer surface of the capsid, and no neutralizable epitopes appear to exist in VP4. On the other hand, VP1, VP2, and VP3 are outer capsid proteins and contain neutralizable epitopes . A number of important neutralization epitopes may exist on VP1 . To confirm the important neutralization sites on VP1, we compared the deduced VP1 amino acid sequences of HEV71 strains OC/00168, OC/00219, OC/00260, and OC/00261. OC/00168 was neutralized by anti-HEV71/BrCr serum, while OC/00219, OC/00260, and OC/00261 were not. Comparison of the deduced VP1 amino acid sequences showed that no mutated residues on the VP1 region corresponded to the result of the neutralization tests. This result indicates that either the important neutralization epiotopes for anti-HEV71/BrCr serum do not exist on the VP1 protein, or the epitopes are specifically masked in the cases of OC/00219, OC/00260, and OC/00261. Further analysis against the VP2 and VP3 regions of these strains should allow interpretation of these findings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b3ed5442-bc8c-4f67-8ca3-1ddd3f13115c", "title": "Flatboats, Travelers, Infectious Diseases, and Other River Thoughts", "text": "【0】Flatboats, Travelers, Infectious Diseases, and Other River Thoughts\nGeorge Caleb Bingham , The Jolly Flatboatmen, 1846. Oil on canvas, 38 1/8 in x 48 1/2 in/96.8 cm x 123.2 cm. Open access digital image courtesy of the National Gallery of Art, Washington, DC, USA. Patrons’ Permanent Fund. 2015.18.1.\n\n【1】“I choose to listen to the river for a while, thinking river thoughts, before joining the night and the stars.”\n\n【2】—Edward Abbey, _Down the River_\n\n【3】A constellation of synonyms exist for the word “river”: what distinguishes these naturally flowing watercourses from a beck, bourn, brook, burn, creek, rill, rivulet, runnel, or tributary may come down to the size of the stream or may simply be a regional preference. Rivers flow across every continent and on all but the smallest islands, carving, eroding, and reshaping the Earth’s topography; connecting populations; enabling access to natural resources, commerce, and trade; defining boundaries; and offering sustenance and energy. Rivers have provided routes for explorers and adventurers and have roused the imaginations of writers and artists across cultures and history.\n\n【4】An iconic depiction of life on North American rivers in the 1840s, _The Jolly Flatboatmen_ , this month’s cover image, is one of the most celebrated works by American painter George Caleb Bingham. This well-preserved painting is considered an early example of luminism, an American painting style that depicts the effects of light played out across tranquil settings, calm waters, and hazy skies. Drifting downriver, Bingham’s happy-go-lucky crew pays little mind to their surroundings: nothing in this painting hints at the potential dangers of river life—shifting sandbars and shoals, flash flooding, submerged trees, injuries and diseases, or scheming pirates.\n\n【5】The National Gallery of Art’s overview states that “The composition is at once dynamic—the dancing man and the musicians—and elegantly stable in the way Bingham arranged the figures to form an isosceles triangle.” The Gallery, which has borrowed and displayed this painting several times since the 1960s, finally purchased it from an undisclosed seller in May 2015.\n\n【6】The painting’s symmetry is striking, a point noted in a review of Bingham’s river paintings from the _New York Times_ : the rectangular flatboat floats downstream, framed by a peaceful river, misty tree-lined riverbanks, and a pale blue, cloudless sky. A pair of long oars, locked in place, jut off to either side. Front and center, a capering young man, dressed in a red shirt, dances a jig, frozen in mid-step with hands held high. One crew member plays a fiddle, and another keeps time on a pan. Other crewmen watch and listen, several locking eyes with the viewer; another, seen from behind, stretches out in repose, his head resting on his interlocked hands.\n\n【7】A raccoon pelt hangs by the ladder; a serpentine coil of rope dangles from the top deck; a turkey thrusts its head between the slats of the wooden crate doubling as a stage; a rock anchors the blue shirt drying; bed rolls are tidily stashed below the top deck. Draped over the back of the boat, a sheet of newspaper and its reflection in the river form additional, smaller triangles. According the National Gallery of Art, Bingham’s meticulous attention to details helped establish the artist’s enduring reputation.\n\n【8】Bingham, who was born in Virginia and raised in Missouri, began his career as a self-taught portrait painter. In the mid-1840s, he started painting his genre works that idealized labor and leisure on the river. Even though flatboats had become essentially obsolete by that time, Bingham nonetheless saw his reputation soar when _The Jolly Flatboatmen_ became the first piece of American art that, to borrow from today’s social media lingo, “went viral.” In 1847, the American Art-Union purchased the painting from the artist and distributed engravings to some 10,000 members across the United States, which made it one of the best known and most widely distributed works of art of its era.\n\n【9】For a time, vast numbers of flatboats, forerunners of today's barges, conveyed agricultural commodities, raw materials, whiskey, livestock, and people downriver. Stops along the way served to increase contact between local populations and the flatboat crews and travelers. Those contacts and human behaviors created opportunities for rapid and efficient transmission of many types of pathogens—including those that can cause sexually transmitted infections. Infectious diseases could be transmitted from geographically isolated populations to more densely populated communities and, conversely, from urban populations to susceptible, isolated populations\n\n【10】Today the number of people who travel for work, leisure, and adventure has exponentially increased, and global mobility contributes to the spread of sexually transmitted infections. The _CDC Health Information for International Travel_ (the Yellow Book) notes that an estimated 499 million cases of chlamydia, gonorrhea, syphilis, and trichomoniasis occur worldwide each year. Emerging and reemerging sexual infections, including antimicrobial resistant gonorrhea, chancroid, sexually transmitted hepatitis C, lymphogranuloma venereum, HIV infection, and human papillomavirus infection, further underscore the value of heightened public health surveillance and research.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c126d213-e2b9-4b66-a82b-198e2f844d56", "title": "Malaria Elimination―Not Just a Bunch of Hocus-Pocus", "text": "【0】Malaria Elimination―Not Just a Bunch of Hocus-Pocus\nQuintus Serenus Sammonicus (c. 370 CE–212 CE), Liber Medicinalis. Unknown transcriber, Abbey of St. Augustine, Canterbury, England. Parchment from 13th century. 7 in × 4.7 in/180 mm × 120 mm. British Library, London, UK. Public Domain in most countries other than the United Kingdom.\n\n【1】_Abracadabra_ ! This ubiquitous incantation remains a staple of stage magicians, children’s stories, and purveyors of pseudoscience. The exact origin of the term engenders debate, and pundits have suggested various ancient Aramaic, Hebrew, and Latin terms as the source. What is known is that its first appearance in print is found in the surviving fragments of the third century CE book _Liber Medicinalis_ (sometimes known as _De Medicina Praecepta Saluberrima_ ) written by Quintus Serenus Sammonicus. Though Serenus was the physician to the Roman Emperor Caracalla and considered “the learned man of his age,” few details of his life are known.\n\n【2】Following the practice of his time, Serenus composed his teachings as didactic poetry. The surviving fragment of _Liber Medicinalis_ includes popular treatments, remedies, and antidotes written in verse.\n\n【3】Among those remedies, Serenus proposed a magical procedure based on the wizardly word _Abracadabra_ for treating “semitertian” fever, known today as malaria. That malady devastated ancient Rome and was sometimes also called “rage of the Dog Star” as the ascendance of Sirius presaged the oppressive heat and humidity thought to cause fever and illness. Some wealthy Romans sought to escape this scourge by moving to villas they had built in the hills away from the “bad air” (malum aeris in Latin) emanating from the marshes and wetlands surrounding Rome. With a bit of alchemical panache, Serenus offered another approach documented in chapter 51 of the _Liber Medicinalis:_\n\n【4】_Inscribis chartae, quod dicitur Abracadabra:_\n\n【5】_Saepius et subter repetas, sed detrahe summae,_\n\n【6】_Et magis atque magis desint elementa figuris:_\n\n【7】_Singula quae semper rapies et coetera figes,_\n\n【8】_Donec in angustam redigatur litera conum._\n\n【9】_His lino nexis collum redimire memento_\n\n【10】Various translations of the Latin are available. This one comes from by A. C. Wootton, who, for three decades, served as editor of the trade journal _Chemist and Druggist_ .\n\n【11】“Write several times on a piece of paper the word ‘Abracadabra,’ and repeat the words in the lines below but take away letters from the complete word and let the letters fall away one at a time in each succeeding line. Take these away ever, but keep the rest until the writing is reduced to a narrow cone. Remember to tie these papers with flax and bind them round the neck.”\n\n【12】The idea underscoring this magical thinking was that by making the letters disappear, the illness would likewise vanish. This month’s cover image shows a 13th century transcription of this page from _Liber Medicinalis_ and comes from the Benedictine Abbey of St. Augustine, Canterbury, England. Like a slice of pie resting in the margin of the book, the _ABRACADABRA_ cone is visible near the lower right of the parchment. The original Latin, rendered painstakingly in ornate Gothic and Gothic cursive, explains Serenus’ process for creating the triangular charm inscribed with the enchantment and for wearing it as an amulet. Perhaps to hedge his bets, Serenus also suggested smearing lion’s fat on one’s body or wearing a domestic cat’s skin festooned with jewels to ward off these fevers.\n\n【13】Feline byproducts, bejeweled or otherwise, magic words, and amulets all failed, and malaria continues to be one of the most severe global public health problems. Fortunately, though, the scientists of today have been a bit more effective through core interventions of surveillance, diagnosis, prevention, and treatment. In many countries, using artemisinin-based combination therapy, undertaking vector control measures such as using long-lasting insecticides on bed nets and interior walls of houses, and strengthening public health infrastructure have successfully reduced cases and deaths.\n\n【14】While progress is starting to plateau in many highly malaria-endemic countries, several other countries either have eliminated, or are on the verge of eliminating, malaria. Paraguay and Uzbekistan recently celebrated their initiation into the malaria elimination club, receiving certification by the World Health Organization (WHO). El Salvador and China have recently reached zero cases. Both countries will receive their official WHO certification when they have “proven, beyond reasonable doubt, that the chain of local transmission of all human malaria parasites has been interrupted nationwide for at least the past 3 consecutive years; and that a fully functional surveillance and response system that can prevent re-establishment of indigenous transmission is in place.”\n\n【15】In just the past 10 years, the number of malaria-endemic countries has decreased from 108 (in 2008) to 90 (in 2018). In 2017, about half of all of the remaining malaria-endemic countries had reported fewer than 10,000 cases per year. WHO has outlined a strategy to continue paring down that list of malaria-endemic countries, one by one, with ambitious targets through 2030. Peering into our crystal ball, we hope, one day, to see that final country on the list like Serenus’ ultimate letter A and then _Abracadabra_ ―all gone.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "43d82a06-a2cf-4d0d-ba54-dd9a198d09f1", "title": "Carbon Disulfide", "text": "【0】*   Overview\n*   NIOSH Chemical Resources\n*   Related NIOSH Resources\n*   Selected Publications\n*   Related Resources\n*   International Resources\n\n【1】### Overview\n\n【2】CAS No. 75-15-0\n\n【3】Carbon disulfide (CS 2  ) is a colorless liquid with an ether-like odor. Exposure can cause dizziness, poor sleep, headache, anxiety, anorexia, weight loss, and vision changes. It can harm the eyes, kidneys, blood, heart, liver, nerves, and skin. Workers may be harmed by carbon disulfide. The level of exposure depends upon the dose, duration, and work being done.\n\n【4】Carbon disulfide is used in many industries. It’s used to make rubber, viscose rayon, cellophane, and carbon tetrachloride. Some examples of workers at risk of being exposed to carbon disulfide include the following:\n\n【5】*   Factory workers who work where rubber is made or processed\n*   Workers involved in cellophane production\n*   Employees who work in factories where rayon fabric is made\n*   Employees involved in the production of carbon tetrachloride\n\n【6】NIOSH recommends that employers use Hierarchy of Controls to prevent injuries. If you work in an industry that uses carbon disulfide, please read chemical labels and the accompanying Safety Data Sheets for hazard information. Visit NIOSH’s page on Managing Chemical Safety in the Workplace to learn more about controlling chemical workplace exposures.\n\n【7】The following resources provide information about occupational exposure to carbon disulfide. A useful search term for carbon disulfide includes “carbon bisulfide,” “carbon sulfide,” and “carbon disulphide.”\n\n【8】### NIOSH Chemical Resources\n\n【9】NIOSH Pocket Guide to Chemical Hazards\n\n【10】The NIOSH Pocket Guide to Chemical Hazards (NPG) helps workers, employers, and occupational health professionals recognize and control workplace chemical hazards.\n\n【11】Manual of Analytical Methods\n\n【12】The NIOSH Manual of Analytical Methods (NMAM) is a collection of methods for sampling and analysis of contaminants in workplace air, and in the blood and urine of workers who are occupationally exposed.\n\n【13】Health Hazard Evaluations\n\n【14】The Health Hazard Evaluation Program (HHE) conducts onsite investigations of possible worker exposure to chemicals. Search the HHE database for more information on chemical topics.\n\n【15】### Related NIOSH Resources\n\n【16】*   NIOSHTIC-2 search results on carbon disulfide —NIOSHTIC-2 is a searchable database of worker safety and health publications, documents, grant reports, and journal articles supported in whole or in part by NIOSH.\n*   Immediately Dangerous to Life or Health Concentrations (IDLH) of Carbon Disulfide —NIOSH reviews relevant scientific data and researches methods for developing IDLH values.\n*   Carbon Disulfide: Rubber and Plastics Chemical Manufacturing —The NIOSH Worker Notification Program notifies workers and other stakeholders about the findings of these research studies.\n*   Carbon Disulfide: Tire and Rubber Company —The NIOSH Worker Notification Program notifies workers and other stakeholders about the findings of these research studies.\n\n【17】### Selected Publications\n\n【18】*   Criteria for a Recommendation Standard: Occupational Exposure to Carbon Disulfide . DHHS (NIOSH) Publication No. 77-156 —This report increases awareness and recommends work practices to reduce exposures to carbon disulfide in the workplace.\n*   Carbon Disulfide  —Sampling and measurement data from NMAM, fourth edition.\n*   Occupational Health Guideline for Carbon Disulfide —This guideline helps stakeholders conduct effective occupational safety and health programs.\n\n【19】### Related Resources\n\n【20】*   ASTR ToxFAQs for Carbon Disulfide\n*   ASTR Toxic Substance Portal for Carbon Disulfide\n*   EPA Acute Exposure Guideline Levels (AEGLs): Carbon Disulfide\n*   EPA Chemistry Dashboard: Carbon Disulfide\n*   EPA Integrated Risk Information System (IRIS): Carbon Disulfide\n*   OSHA Federal Register Correction to Preamble of Air Contaminants Standard: Carbon Disulfide\n*   OSHA Hazard Communication\n*   OSHA Occupational Chemical Database: Carbon Disulfide\n*   NLM Haz-Map: Carbon Disulfide\n*   NLM TOXNET: Carbon Disulfide\n*   New Jersey Hazardous Substance Fact Sheets: Carbon Disulfide\n\n【21】### International Resources\n\n【22】*   European Chemicals Agency (ECHA): Carbon Disulfide\n*   Gestis Substance Database\n*   International Chemical Safety Card: Carbon Disulfide\n*   IPCS INCHEM (CICADS) 46: Carbon Disulfide\n*   IPCS INCHEM Environmental Health Criteria 10: Carbon Disulfide\n*   IPCS INCHEM Information Monograph 102: Carbon Disulfide\n*   OECD Global Portal to Information on Chemical Substances\n*   WHO Air Quality Guidelines (2nd ed.): Carbon Disulfide", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "104c9cf2-aac4-4605-b4e3-652ad5f5eb2f", "title": "Inaccurate Multilocus Sequence Typing of Acinetobacter baumannii", "text": "【0】Inaccurate Multilocus Sequence Typing of Acinetobacter baumannii\nAn adequate genotyping system is of paramount importance for infectious disease epidemiology. Two decades ago, the multilocus sequence typing (MLST) scheme was proposed as a genotyping method , and today, because of its reproducibility and portability, MLST schemes are available for many human pathogens . MLST has been instrumental in increasing understanding of the epidemiology and population structure of many bacteria. _Acinetobacter baumannii_ , a major source of nosocomial infections, is no exception, and 2 MLST schemes (Oxford and Pasteur) have been established for this species . Each scheme uses just 7 loci and, therefore, only samples a small fraction of the chromosome, which could be a serious issue for genotyping species with highly variable genomes. Some studies have shown that _A. baumannii_ has both high gene content variation  and substantial levels of recombination .\n\n【1】We revisited one of the most comprehensive genome datasets of _A. baumannii_  to construct a robust phylogeny to show that sequence type (ST) assignation in both MLST schemes does not reflect true relationships among isolates of this species. This dataset of >80 genomes covers 36 different STs according to the Oxford scheme (STox) and 19 different STs according to the Pasteur scheme (STp) . We constructed a concatenated alignment of 574 orthologous genes and conducted statistical model selection as in a previous study  and, on that alignment, constructed a maximum-likelihood phylogeny by PhyML .\n\n【2】The 2 schemes showed different levels of resolution. Although in many instances a single STp had just 1 equivalent STox, 2 STs exist in the Pasteur scheme that encompass many Oxford STs . For instance, under the Pasteur scheme, STp2 represents >15 STs in the Oxford scheme and STp1 encompasses 5 STox. Thus, the Pasteur scheme seems to have considerably less resolution than the Oxford scheme to distinguish isolates. The Pasteur scheme’s lack of resolution was not insignificant, however. STp2 comprises 43 isolates (approximately half of our dataset) showing considerable levels of genetic variation according to our phylogeny, but according to this MLST scheme, they constitute just 1 genotype.\n\n【3】Many of the STs in either scheme formed coherent (monophyletic) groups in our phylogeny. However, we recorded some clear exceptions in which isolates from some STs did not form monophyletic groups, that is, isolates with the same ST did not cluster. The most striking case is STox208 (orange tips in the phylogeny), where there are 2 well-defined groups with several isolates each and an extra isolate not close to either of those well-defined groups. We also noted that the 2 STox455 isolates did not cluster and are located far apart on the tree (green tips). Additionally, 1 of the STox369 isolates did not fall within the ST369 group (blue tips). These 3 examples show that the Oxford MLST does not accurately reflect the relationships among the isolates. Also notable is that, although for the Oxford scheme 36 STs are represented in this dataset, only 16 of them have \\> 2 isolates and therefore only in these STs could we detect problems with the clustering within any given ST. Thus, 3 of these 16 STs did not cluster the isolates properly inasmuch as these STs were polyphyletic. In summary, for the Oxford scheme we demonstrated that some STs form polyphyletic groups because 4 of the 7 loci have signals of recombination , whereas for the Pasteur scheme, we noted a serious lack of resolution for some STs because the loci used only by this scheme have the lowest levels of genetic diversity . Two previous studies noted problems with the MLST schemes for this species ; nonetheless, neither was as extensive as our study, nor did they benchmark both schemes against a genome-based phylogeny.\n\n【4】In conclusion, we showed that the correct relationships among isolates cannot be recovered using either of the MLST schemes for _A. baumannii_ . In addition, we highlighted the importance of using more powerful genotyping strategies when analyzing bacteria with highly dynamic genomes; in this regard, the ever-decreasing cost of genome sequencing will make this technology the perfect tool for genotyping bacterial species.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8ca5bd52-a679-4059-a78b-58bb80a53bd5", "title": "Correction, Vol. 9, No. 1", "text": "【0】Correction, Vol. 9, No. 1\nIn “Transfusion-Associated Babesiosis after Heart Transplant” by Joseph Z. Lux et al. errors occurred in the text. On page 118, left column, lines 20–22, the sentence should read “he became symptomatic during the typical 2- to 8-week incubation period for transfusion-transmitted _B. microti_ infection .”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "87117dfe-0566-4a0e-9c97-f62c8bab9518", "title": "Seroprevalence of SARS-CoV-2 among Blood Donors and Changes after Introduction of Public Health and Social Measures, London, UK", "text": "【0】Seroprevalence of SARS-CoV-2 among Blood Donors and Changes after Introduction of Public Health and Social Measures, London, UK\nThe first confirmed cases of coronavirus disease (COVID-19) in the United Kingdom were identified at the end of January 2020. As cases increased across all regions, surveillance data indicated that the epidemic was progressing more rapidly in London than the rest of the United Kingdom. In response to the increase in cases, hospitalizations, and deaths, the United Kingdom introduced a series of measures to limit transmission, beginning March 12, 2020 (week 11); persons with a continuous cough or fever were advised to self-isolate for 7 days, school trips abroad were cancelled, and at-risk groups were advised to avoid cruises. These measures culminated in the implementation of legally enforceable public health and social measures (i.e. lockdown) beginning March 23 (week 13) .\n\n【1】Despite the reporting of a range of surveillance data in England, including laboratory-confirmed cases, primary-care consultations, hospital and intensive care unit (ICU) admissions, and deaths , much remains unknown about the magnitude of infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus in the population, the key drivers of transmission, and the incidence of asymptomatic or mildly symptomatic infection within the UK population thus far.\n\n【2】Serologic estimates are critical to better understand epidemiologic trends and help inform policy options to control disease. These estimates also provide a denominator for estimating severity measures, such as infection fatality and infection hospitalization ratios, and to help clarify the epidemiology of COVID-19 in the population.\n\n【3】Early in the pandemic data from population-based seroepidemiologic studies were limited , and how the prevalence of SARS-CoV-2 infection varies by age was not well understood. Much remains unknown about the dynamics of the antibody response to SARS-CoV-2. The existing serologic assays target different viral proteins, and IgG responses to these proteins are likely to appear at different stages of the immune response, potentially resulting in some assays preferentially identifying those persons seroconverting earlier or later in the course of an infection ; these differences are an important factor when interpreting data from seroprevalence studies. In this article, we describe the results of testing whole blood donors in London, UK, who were anonymously tested as part of the national public health response to COVID-19. These tests were conducted using 3 different serologic assays at 3 timepoints during the epidemic that reflect transmission prelockdown, perilockdown, and immediately post lockdown.\n\n【4】### Methods\n\n【5】##### Data Collection\n\n【6】##### Sample Selection\n\n【7】A program of collecting plasma samples each week through the National Health Service Blood and Transplant Services from healthy 17–69 year old persons donating whole blood was initiated on March 23, 2020, at epidemiologic week 13. The minimum interval between serial donations was 12 weeks for men and 16 weeks for women. An average of 10,683 whole blood donations were received per month in London during March–July 2020.\n\n【8】Given the evidence of the scale of the epidemic in London, enhanced testing of London donors was implemented with donor samples from London collected during week 13 (period 1, beginning March 23), weeks 15–16 (period 2), and week 18 (period 3). Approximately 1,000 fully anonymized donations were obtained for each collection. The demographic information available from each donor included age, sex, and area of residence.\n\n【9】Blood donors are healthy persons who are excluded from donating if they experienced any acute illness for \\> 2 weeks before donating blood. In addition, specific donor exclusion criteria for coronavirus have been introduced (14 days postinfection at the time of the study, which was extended to 28 days starting June 8) . Given the standard symptomatic period, the exclusion criteria described and the fact that antibodies might take \\> 2 weeks to be detectable, prevalence estimates among blood donors probably reflect infection prevalence \\> 2–4 weeks before samples were taken.\n\n【10】To undertake the validation of test sensitivity, samples from recovering case-patients are required. To obtain convalescent serum samples from case-patients in the community, Public Health England (PHE) initiated an active request for samples from persons with PCR-confirmed cases reported early in the epidemic. These persons were asked to attend their general practitioner approximately 3–5 weeks after illness onset to provide a convalescent serum sample . Crucially, these cases were detected in the containment phase, when testing was based on epidemiologic factors such as travel. These cases should therefore be a better reflection of mild and asymptomatic infections that would not otherwise be picked up by routine testing, which was based predominantly on testing hospitalized patients at the 3 timepoints.\n\n【11】To evaluate specificity, serum samples collected before the circulation of SARS-CoV-2 also were tested. This testing was done on residual serum samples taken in 2018 and provided by the Sero-Epidemiology Unit (SEU) at PHE, Manchester , and the Royal College of General Practitioners Research and Surveillance Centre . All samples were processed and stored at the SEU.\n\n【12】##### Serologic Assays\n\n【13】We tested samples on 2 commercial assays according to the manufacturers’ instructions. Initial testing was conducted by using the SARS-CoV-2 ELISA IgG assay from Euroimmun  targeting the S1 domain, including the receptor-binding domain (RBD); testing was conducted by using the SARS-CoV-2 IgG for use on the Architect platform  targeting the nucleoprotein. Samples were tested individually and reported according to the manufacturers’ criteria. We defined Euroimmun results of 0.8 to <1.1 as equivocal and \\> 1.1 as reactive. We defined Abbott results of \\> 1.4 as reactive; we also defined an equivocal range of 0.8 to <1.4 for presentation of validation data.\n\n【14】The third assay was an in-house assay developed in the Virus Reference Department at PHE, also used retrospectively. For this ELISA, we purchased the commercial recombinant RBD subunit of the SARS-CoV-2 spike protein from SinoBiological Inc. which we expressed in HEK293 cell culture with a C-terminal mouse Fc tag (Arg319-Phe541(V367F) . We coated Nunc MaxiSorp  flat-bottomed, polystyrene, 96-well microtiter plates by diluting 20 ng recombinant protein per well in sterile phosphate-buffered saline; pH 7.2 \\+ SD 0.05  at 4°C–8°C for a minimum of 16 hours. We diluted serum at a final dilution factor of 1 in 100. We detected the binding of IgG on the plate surface by using an anti-human IgG horseradish peroxidase antibody conjugate  and 3,3′,5,5′-etramethylbenzidine . We analyzed samples in duplicate and evaluated optical density at 450 nm (OD 450  ) data by dividing average OD 450  values for individual samples by average OD 450  of a known calibrator with negative antibody levels (T/N ratio). We defined results of 4 to <5 as equivocal and \\> 5 as reactive. We defined samples as reactive for each assay independently.\n\n【15】##### Assay Validation\n\n【16】Because of the speed with which the assays have been developed, limited validation has been conducted by the manufacturers. We therefore used panels created by PHE and managed by the SEU to validate the assays \n\n【17】##### Population Data\n\n【18】We weighted overall prevalence estimates for age. We based these estimates on population data from the Office for National Statistics .\n\n【19】##### Statistical Analysis\n\n【20】We calculated observed prevalence (prev obs  ) by age group, sex, and time with 95% exact CIs. In these calculations, all results falling into the equivocal range of the assays were included as negative. Analyses were conducted in Stata 14 .\n\n【21】We corrected observed prevalence to account for the sensitivity and specificity of the assays by using an adjusted prevalence (prev adj  ) related to the observed prevalence as follows:\n\n【22】Prev obs  \\= Se × prev adj  \\+ (1 – Sp) × (1 – prev adj  ),\n\n【23】where Se denotes sensitivity, Sp denotes specificity, and prev obs  denotes the observed prevalence . We solved this relationship within a Bayesian model, along with the sampling distribution for reactive tests _n + _ ≈binomial(N, prev obs  ) and using a beta(0.5,0.5) prior for the adjusted prevalence prev adj  . We included sensitivity and specificity, which were based on positivity in convalescent and baseline serum samples, in our Bayesian model each by way of a conjugate beta-binomial model with a beta(0.5,0.5) prior, thus accounting for uncertainty of their actual value. We generated uncertainty and credible intervals by using Markov Chain Monte Carlo simulations with 500,000 iterations after a burn-in of 1,000 iterations and a thinning interval of 5, using the NIMBLE package in R software .\n\n【24】### Results\n\n【25】The overall test positivity based on the Euroimmun assay was 2.9% (95% CI 1.8%–4.4%) in week 13, 9.9% (95% CI 8.2%–11.8%) in weeks 15–16, and 13.0% (95% CI 11.0%–15.3%) in week 18. Consecutive differences between the proportion reactive at the 3 timepoints reduced over time, from 7.0% (95% CI 4.8%–9.1%) during week 13 to weeks 15–16 to 3.2% (95% CI 0.4%–5.9%) during weeks 15–16 to week 18 .\n\n【26】In comparison, results from the RBD and Abbott assays had higher positivity at week 13, RBD at 3.5% (95% CI 1.9%–5.9%) and Abbot at 3.0% (95% CI 1.4%–5.6%), compared with a positivity of 2.9% with the Euroimmun assay . The number of samples tested using each assay varied considerably for this first timepoint. At week 18 the RBD test had highest positivity at 14.1% (95% 12.0%–16.5%) and Abbott the lowest at 12.3% (95% CI 10.3%–14.6%) , although the differences in positivity estimated by the 3 assays were not significantly different at each of the 3 periods, producing overlapping CIs. We tested a smaller number of donor samples from week 13 using the Abbott assay. The geographic spread of these samples was a little more concentrated in inner London compared with the overall sample collection  but reasonably representative in terms of age .\n\n【27】After adjustment for sensitivity and specificity, Euroimmun had the highest adjusted prevalence in week 18 at 14.9%, compared with 13.3% for the Abbot assay and 13.4% for the RBD assay . In weeks 15–16, adjusted prevalence was similar among the 3 assays: 10.9% for RBD, 11.0% for Euroimmun, and 11.3% for the Abbott assay. In week 13, adjusted prevalence was lowest for RBD at 1.5% and highest for the Abbott at 3.1%.\n\n【28】Venn diagrams show the results for samples tested by all assays for the 3 timepoints . Unadjusted prevalence based on a highly specific endpoint requiring all assays to be reactive was 1.0%, 8.5% and 11.6% at the 3 timepoints, whereas if based on a highly sensitive endpoint of any assay reactive prevalence was 6.5%, 13.6%, and 14.8%. The RBD assay gave the most reactive results, but this tendency can be explained by its lower specificity . If a criterion of reactive by Abbott or Euroimmun were used (to maintain good specificity and increase sensitivity), then unadjusted prevalence would be 4.5%, 12.2%, and 13.6% for the 3 timepoints. These values compare with an unadjusted prevalence of 2.9%, 9.9%, and 13.0% for the Euroimmun assay alone. To adjust prevalence based on assay combinations, the sensitivity and specificity of these combinations is also required, but the validation data did not have all assays tested on the same negative samples to enable this calculation.\n\n【29】The analysis shows an important age effect, a decreasing prevalence with increasing age group at weeks 15–16 , based on the Euroimmun assay. Comparisons by age were not interpretable for the earlier timepoint (week 13) because of the low number of donor samples from persons in older age groups. At week 18, the difference in prevalence by age group was less pronounced, showing little difference between age groups <50 years and an increased prevalence in older age groups. When comparing age effects by assay, this effect was most pronounced in the RBD assay results and least pronounced in the Abbott assay results .\n\n【30】Although prevalence estimates from all 3 assays indicate a slightly higher prevalence among men than women in week 13 , a more pronounced gender effect appears to have occurred by weeks 15–16, when prevalence was higher in younger women than in men and older women. This difference was no longer observed by week 18, when prevalence was similar.\n\n【31】### Discussion\n\n【32】We demonstrate the value of using 3 serologic assays targeting different proteins for evaluating seroprevalence of SARS-CoV-2 and for understanding the evolution of the epidemic in London and the effects of physical distancing measures. Our results show that overall trends in prevalence estimates are similar across all 3 assays; however, we observed some notable differences. The sensitivity analysis indicates that the assay targeting the nucleoprotein identifies early infections; the assays targeting the spike protein are more reliable in picking up late infections. These results are similar to observations made by other groups . Including samples that were positive on the nucleoprotein-based assay with those reactive on a spike-based assay increased unadjusted prevalence by 1.6%, 2.3%, and 0.6% for the 3 periods.\n\n【33】Understanding the changes in sensitivity of serologic assays over time is also critical in interpreting seroprevalence data, particularly taking into account recent data that have indicated differential waning patterns for antibodies that have different targets . These findings also demonstrate the value in combining data from different serologic assays with different target proteins for determining seroprevalence.\n\n【34】We show that, in London, ≈14% donors had evidence of infection by week 18, the highest for any region of England. This pattern is consistent with data from other surveillance systems, including numbers of cases, hospitalizations, and deaths . These results also confirm transmission slowed substantially after lockdown measures were put in place, plateauing from weeks 15–16 to 18. Given the time required to develop an antibody response and the fact that donors are excluded from donating for a minimum of 14 days after an acute illness, these prevalence estimates reflect the situation \\> 2–4 weeks before the collection date. Therefore, increases observed from weeks 13 to 15–16 reflect the situation before the effects of lockdown measures fully taking effect, and results from early May reflect incidence from early to mid-April.\n\n【35】Our analysis shows a very pronounced age difference among adult age groups, particularly for samples taken in weeks 15–16, which probably reflect the epidemic dynamics under normal social-mixing patterns in a high-transmission situation , given the fact that these results were too early to have been affected by lockdown. Those findings suggest that young adults in London were infected earlier in the epidemic and older age groups affected later. The mixing patterns during lockdown have substantially changed, including less frequent contact with persons in the same age groups (i.e. less age-assortative mixing); fewer daily contacts overall; and more intergenerational mixing among persons >30 years of age, probably reflecting household compositions in these age groups . These patterns might explain, in part, some of the observed differences in trends by age group.\n\n【36】For prevalence by sex, data from London suggest that young adult women had a higher risk for infection than men of the same age, particularly before lockdown measures were implemented. However, after lockdown, those differences became less pronounced. This finding might support the hypothesis that women of childbearing age were acquiring infection before men of a similar age group. Evidence to support the idea that children are key drivers of transmission is limited , and further work is needed to address potential explanations for such a difference, including higher intensity of exposure to children, higher frequencies of occupational caring roles for women compared with men, or both.\n\n【37】The availability of large volumes of donor samples on a weekly basis provided an attractive and valuable source of samples for seroprevalence estimates. However, adult donors are not representative of the general population and are likely to be less ethnically diverse, of higher socioeconomic status, and healthier than the wider population , all of which might lead to an underestimate in population prevalence. Although donors \\> 70 years of age were excluded from donation, increased pathogenicity of SARS-CoV-2 with age might have resulted in an increased proportion of infected older donors being hospitalized and thus not available for blood donation. This tendency could result in an underestimate of seroprevalence in the oldest age groups.\n\n【38】Changes in the precise locations of sampling within regions at different periods have been observed, and this lack of consistent sampling needs to be considered when interpreting any changes over time. For example, because of limited volumes, a smaller number of donor samples from week 13 were tested using the Abbott assay. We demonstrate similar results for using 3 different assays independently and adjusting for the estimated sensitivity and specificity of the assays. We did not attempt to estimate adjusted prevalence on a combination of assays results or on the basis of changing assays cutoffs because more validation data on using multiple assays would be needed, and those data would probably indicate a pattern similar to that observed with the individual assays.\n\n【39】A range of interactions might have contributed to our results; further work is needed to understand the effect of age on antibody kinetics and the effect of age on different aspects of various assays, including sensitivity over time. These factors highlight the complexity inherent in interpreting seroprevalence surveys.\n\n【40】Despite those limitations, these results from testing blood donors have provided valuable intelligence regarding the progression of the epidemic among adults in London. Our results show that using multiple serologic assays targeting different proteins is probably informative as we try to determine the interplay between antibody kinetics and transmission dynamics in the population over time. Seroepidemiologic studies that rely on a single assay or have a single target risk incomplete ascertainment of the actual number of infections within the population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d424991a-3225-403b-9482-90fd27440a92", "title": "Sleep Disorder Symptoms Among Adults in 8 States and the District of Columbia, 2017", "text": "【0】Sleep Disorder Symptoms Among Adults in 8 States and the District of Columbia, 2017\nAbstract\n--------\n\n【1】Sleep disorder symptoms (trouble falling asleep or staying asleep, unintentionally falling asleep, snoring loudly, and episodes of having stopped breathing) among US adults (N = 59,108) from 8 states and the District of Columbia were analyzed by using data from the 2017 Behavioral Risk Factor Surveillance System. We conducted a multivariable logistic regression to assess the association between the 4 symptoms and sociodemographic characteristics, risk behaviors, and chronic conditions. The 4 symptoms were prevalent and more likely to be reported among adults with any chronic condition(s) than their counterparts without symptoms and among those who slept fewer than 7 hours compared with those who slept 7 to 9 hours.\n\n【2】Objective\n---------\n\n【3】Sleep disorders are associated with low sociodemographic characteristics, health-risk behaviors, and chronic conditions . However, limited population-level information exists on sleep disorder symptoms. Our study aimed to present estimates of sleep disorder symptoms including sleep satisfaction or quality, alertness, snoring, and observed apnea among adults .\n\n【4】Methods\n-------\n\n【5】The Behavioral Risk Factor Surveillance System (BRFSS) is an annual, state-based, random-digit–dialed telephone interview survey designed to monitor health conditions and risk behaviors in a representative sample of adults (aged ≥18 years) in all 50 states, the District of Columbia, and US territories. In 2017, 8 states (Arizona, Kansas, Minnesota, Nebraska, Nevada, North Dakota, Oregon, Tennessee) and the District of Columbia administered an optional module about sleep disorder symptoms in addition to a core module including sociodemographic and health characteristics . We considered sleep disorder symptoms as experiencing 1 or more days in the past 14 days of 1) trouble falling asleep, staying asleep, or sleeping too much, or 2) unintentionally falling asleep. Sleep disordered breathing symptoms were defined as 3) you have been told that you snored loudly, or 4) someone observed that you stopped breathing during sleep. Age-adjusted prevalence of each symptom was standardized to the year 2000 projected US adult population . Adjusted prevalence ratios and 95% CIs were estimated by using multivariable logistic regression models controlling for sex, age group, race and ethnicity, marital status, body mass index (BMI) category, any chronic disease (including heart disease \\[coronary heart disease or angina, or myocardial infarction or heart attack\\], stroke, diabetes, arthritis, history of asthma, chronic obstructive pulmonary disease, cancer \\[not including skin cancer\\], and chronic kidney disease), high blood pressure, depression, and average sleep duration (<7 h, 7–9 h, and >9 h sleep in a 24-h period). High blood pressure and depression, which have characteristics different from other chronic diseases, were included separately in the study, although all chronic conditions are strongly associated with sleep disorders . Among the 76,542 adult respondents, 59,108 with complete information were included in this study. Only significant differences are presented ( _P < .05_ ) _._\n\n【6】Results\n-------\n\n【7】The age-adjusted prevalence was 49.5% for trouble falling or staying asleep, 23.1% for unintentionally falling asleep, 41.1% for snoring loudly, and 13.6% for having stopped breathing . Three in 4 adults (74.9%) reported having any sleep disorder symptom; 38.2% reported 1, 24.6% reported 2, 9.8% reported 3, and 2.3% reported all 4 symptoms. Men were less likely to report trouble falling asleep or staying asleep but were more likely to report snoring or having stopped breathing than women. Adults aged 45 to 64 and 65 or older were less likely to report trouble falling asleep or staying asleep but more likely to report snoring or having stopped breathing than adults aged 18 to 44 years. Adults aged 65 years or older were more likely to report unintentionally falling asleep than those aged 18 to 44. Non-Hispanic Black and Hispanic adults were less likely to report trouble falling asleep or staying asleep but more likely to report unintentionally falling asleep than non-Hispanic White adults. Divorced, widowed, separated, or never married adults were more likely to report trouble falling asleep or staying asleep or unintentionally falling asleep but less likely to report snoring than married adults or members of an unmarried couple. Breathing symptom prevalence increased with increasing BMI. In addition, a higher prevalence was shown for each symptom among adults with any chronic disease, high blood pressure, or depression than their counterparts without symptoms, or who slept less than 7 hours compared with those who slept 7 to 9 hours. A U-shaped relationship between sleep duration and unintentionally falling asleep was observed. All significant differences remained after controlling for covariates.\n\n【8】Discussion\n----------\n\n【9】Sleep disorder symptoms were common among adults in the 8 states and the District of Columbia in 2017. About 4 in 10 adults reported snoring, consistent with a previous study from 12 states in the 2009 BRFSS (48.0%) and the 2005–2006 National Health and Nutrition Examination Survey (NHANES) (47.9%) ; however, prevalence for unintentionally falling asleep was lower than in an earlier BRFSS study (23.1% vs 37.9%), likely because of a shorter recall period used (2 weeks versus 30 days) . In addition, differences in prevalence by sociodemographic and health characteristics were observed for all 4 sleep disorder symptoms. For example, older adults were more likely to report 3 of the 4 symptoms. Our observation of a positive relationship between all 4 sleep disorders and any selected chronic disease, high blood pressure, depression, or short sleep duration was like that of previous studies .\n\n【10】Evidence-based interventions can improve sleep health, especially among groups at high risk for disease, such as older adults or those with short sleep duration, those who are overweight or obese, and those with any chronic condition. Educating primary care professionals or physicians to address sleep disorder symptoms for adults with obesity or chronic conditions could improve patients’ sleep health . Comprehensive public health strategies, such as enhancing surveillance and research and promoting public awareness about sleep disorders, are needed to improve sleep health at the population level. Snoring and apnea were more likely to be reported by adults who were married or a member of an unmarried couple, highlighting that sleep partners whose sleep might also be negatively affected can play a role in the diagnosis and management of sleep . Therefore, it is critical that adults recognize these symptoms and discuss them with their primary care provider who can then make referrals to a sleep specialist.\n\n【11】This study has several limitations. First, sleep disorder symptoms were based on self-report and were not verified with objective sleep studies. Furthermore, the 2 breathing symptom questions relied on observation by another individual, which could partially explain a higher prevalence of the symptoms among married adults than among never-married ones. Second, sleep disorder symptoms reflect retrospective conditions and might have recall bias, although the current study used a shorter recall period than previous studies, which might have resulted in greater accuracy . Third, our results only represent noninstitutionalized adults in 8 states and the District of Columbia. Finally, nonresponse bias, because of low response rates, may have influenced our results.\n\n【12】Sleep health should be assessed at the population level by using measures of the 4 sleep disorder symptoms to help monitor and promote sleep health in our nation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "84b33d2a-b5f1-4e52-b13d-5eb2b3f5a4d5", "title": "PCD Advances Recommendations From First-Ever External Review", "text": "【0】PCD Advances Recommendations From First-Ever External Review\n*   Article Types Focusing on Population-Based Interventions\n*   2018 PCD Collections\n*   Student Research Paper Contest\n*   Conclusion\n*   Author Information\n\n【1】Leonard Jack Jr, PhD, MSc\n\n【2】In April of this year, the Editor in Chief column provided PCD readers with information on how the journal used recommendations from its first-ever external review to enhance the journal’s worldwide usefulness to researchers, practitioners, and policy makers. The journal refined its vision and mission statement to reflect its commitment to serving as an influential journal in the dissemination of proven and promising public health findings, innovations, and practices. The review’s panel of experts strongly encouraged the journal to move away from publishing manuscripts that solely described partnerships, collaborations, and coalition building. Instead, the journal was encouraged to focus more on complementing the journal’s rich body of published work on epidemiological studies with attention to evaluating population-based interventions and policies. Since sharing recommendations from the expert panel with our readers, PCD has worked hard to advance those recommendations. This Editor in Chief column provides information on how the journal is increasing the number of publications focusing on population-based interventions, our upcoming 2018 collections, and the future direction of the Student Research Paper Contest.\n\n【3】Article Types Focusing on Population-Based Interventions\n--------------------------------------------------------\n\n【4】Over the past year, PCD has introduced 2 new article types to increase attention to evaluating population-based interventions and policies. First, PCD introduced the article type Implementation Evaluation, which recognizes that public health and clinical interventions are often collaborative, multifaceted, multicomponent, and multisite with diverse participants and stakeholders. The Implementation Evaluation article type requires that authors provide insight on how the evaluation approach could be improved. This article type also shares with readers ways to facilitate diffusion and uptake of effective interventions in comparable real-world settings. Second, PCD introduced the article type Program Evaluation Brief, which is a condensed version of the Implementation Evaluation article type. These short articles focus on understanding how various aspects of activities related to program implementation, program performance, health policy, or system change can influence health.\n\n【5】Both article types are designed to allow authors an opportunity to share lessons learned about factors or circumstances that contributed to or posed challenges to their research, which will help the field identify the most effective population health approaches. However, they are not designed to focus exclusively on providing program descriptions or theoretical framework descriptions; they also are not literature reviews, opinion pieces, commentaries, or essays.\n\n【6】Since making these 2 options available, as of July 2018, PCD has received over 30 related manuscripts. This early response indicates that these article types resonate with PCD contributors. Expanding PCD’s scope to include more research in these areas will certainly help PCD achieve its goal of publishing quality content focusing on the effectiveness of population-based interventions to improve population health.\n\n【7】2018 PCD Collections\n--------------------\n\n【8】Also in April of this year, PCD refined and published on its website 4 content areas of greatest interest to the journal:\n\n【9】*   Development, implementation, and evaluation of population-based interventions to prevent chronic diseases and control their effect on quality of life, illness, and death.\n\n【10】*   Behavioral, psychological, genetic, environmental, biological, and social factors that influence health.\n\n【11】*   Interventions that reduce the disproportionate incidence of chronic diseases among at-risk populations.\n\n【12】*   Development, implementation, and evaluation of public health law and health-policy–driven interventions.\n\n【13】To assist the journal in expanding its content in these 4 areas, PCD announced calls for papers for 2 collections:\n\n【14】1.  **Population Health, Place, and Space: Spatial Perspectives in Chronic Disease Research and Practice** . Authors are encouraged to submit manuscripts for consideration that focus on various ways in which geographic information systems and spatial analyses are used to enhance chronic disease research and public health practice. PCD is excited about publishing articles that offer insight into the role of place and space in shaping the distribution of chronic disease and that inform public health responses for chronic disease prevention and treatment. Manuscripts for this collection are due August 31, 2018.\n\n【15】2.  **Health Care Systems, Public Health, and Communities: Population Health Improvements.** Manuscripts accepted for this collection will focus on research, evaluation, and other work describing innovative and effective work to link health care and community health in ways that improve population health. Over the past decade, there have been various innovative community-driven and clinically driven prevention strategies (primary and secondary) designed to prevent and reduce the burden of chronic conditions worldwide. PCD invites manuscripts that provide timely information on jointly implemented public health and health care efforts to improve population health. Manuscripts should be submitted to PCD on or before November 16, 2018.\n\n【16】Manuscripts submitted in response to these 2 calls for papers will be reviewed and, if accepted, published on a rolling basis. Articles will be assembled into PDF collections accessible on the PCD website after all accepted articles for each collection have been published. PCD welcomes submissions that address a broad range of health conditions (eg, hypertension, diabetes, cancer, obesity, asthma, arthritis, oral health, reproductive health, substance abuse, mental health) and their risk factors, with application to a wide range of settings. Manuscripts must follow the instructions for PCD article types . Further information on submitting a manuscript is available in PCD’s Author’s Corner .\n\n【17】Student Research Paper Contest\n------------------------------\n\n【18】### 2018 Winners\n\n【19】PCD received a record number of 109 student papers for this year’s Student Research Paper Contest. Four papers were selected as winners in 3 categories: doctoral, graduate, and undergraduate. Two winners were selected in the graduate category. The winning papers and podcast interviews for each winning article will be published in PCD later this year. Please join us in congratulating this year’s student winners:\n\n【20】*   **Doctoral Winner: Brittney Keller-Hamilton, College of Public Health, Ohio State University —** Tobacco and Alcohol on Television: A Content Analysis of Male Adolescents’ Favorite Shows\n*   **Graduate Winner: Calla Holzhauser, Mathematics and Statistics Department, South Dakota State University —** Forecasting Participants in the All Women Count! Mammography Program\n*   **Graduate Winner: Fei Gao, Department of Public Health, Brody School of Medicine, East Carolina University —** Gestational Diabetes and Health Behaviors Among Women: National Health and Nutrition Examination Survey, 2007–2014\n*   **Undergraduate Winner: Leigha Vilen, Thurston Arthritis Research Center, University of North Carolina at Chapel Hill —** Education Attainment, Health Status, and Program Outcomes in Latino Adults with Arthritis Participating in a Walking Program\n\n【21】Conducting the student paper contest annually requires a tremendous amount of coordination with review panel chairs and committees, reviewers, and student authors. There are rounds of reviews and decision making to determine which manuscripts advance through each stage of the review process. Not all student papers are selected as winners. However, the contest does allow student authors of nonwinning papers an opportunity to be published once all publication requirements and expectations are met. The quality of submissions was superb this year, and PCD intends to publish 30% of the 109 papers submitted for consideration.\n\n【22】### Future directions for the contest\n\n【23】This was our most successful year so far, and the record number of submissions demanded that PCD expend its resources to the limit. PCD created multiple review committees from our editorial board, solicited additional reviewers, and processed a record number of submissions and revisions through our editorial office. Recognizing that it was likely that submissions to our contest would increase each year, we had concerns that we would not be able to continue providing the same level of attention and mentoring to students, which was the mission of the student contest since its inception. Instead of holding a contest every year, we have opted to hold contests on a less frequent basis that focus on an evolving research area or topic of interest. Moving forward, PCD will conduct the student paper contest every 3 years. The next student paper contest will take place in 2021. By focusing attention on emerging areas of research of importance to students, we can continue to ensure that the contest remains relevant to students and that we can prepare resources in advance to ensure that students get the same quality of attention, review, and mentoring that they have come to expect from PCD.\n\n【24】Conclusion\n----------\n\n【25】PCD has maintained its focus on publishing research that provides insight into the identification and use of innovative and rigorous evaluation approaches to determine the impact of interventions that address chronic disease prevention and control. We hope potential authors will consider submitting manuscripts under our new article types and under our calls for papers. Please visit the journal’s website to learn more about recent publications, upcoming collections, podcasts, and innovations adopted by the journal.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cb76e15d-7534-4b19-8505-4e4008373d27", "title": "Absence of Detectable Replication of Human Bocavirus Species 2 in Respiratory Tract", "text": "【0】Absence of Detectable Replication of Human Bocavirus Species 2 in Respiratory Tract\nSince its discovery in 2005 , human bocavirus (HBoV) has been the subject of intense investigation as a potential cause of human respiratory disease . In addition to respiratory tract and systemic infections, HBoV DNA sequences are frequently detected in fecal samples during primary infections , although a causative role in viral gastroenteritis has not been established . Other parvoviruses, including canine and bovine members of the genus _Bocavirus_ , can replicate in the gastrointestinal tract and are often linked to enteric disease .\n\n【1】Until recently, published genetic analyses reported minimal sequence variability of HBoV strains; 2 genetic lineages differed in nucleotide sequence by only 2% in the virus protein 2 (VP2) gene . However, more divergent HBoV-like variants, provisionally designated as HBoV species 2 (HBoV-2), have been identified in fecal samples from children in Pakistan and the United Kingdom. These viruses show >20% nt sequence divergence . Published primer sequences for HBoV contain several mismatches with HBoV-2 sequences that may prevent their amplification . Thus, published surveys of HBoV prevalence likely report only HBoV-1. Therefore, HBoV-2 may represent an additional, currently undetected, agent in respiratory or enteric disease.\n\n【2】### The Study\n\n【3】To investigate HBoV-2, we developed new PCR-based detection methods for HBoV by using primer sets highly conserved between HBoV-1 and HBoV-2 and species-specific primers for HBoV-2. Large-scale screening of persons in the United Kingdom and Thailand was performed to compare virus detection frequencies in respiratory and fecal samples.\n\n【4】A total of 6,138 respiratory samples from 3,754 persons (2,018 male, 1,722 female, 14 sex unknown) during January 1, 2007–June 30, 2008, were obtained from the Specialist Virology Centre (Edinburgh, UK). Samples were not identified but epidemiologic and demographic information was retained . Samples comprised 3,065 nasopharyngeal swabs/aspirates (NPAs) and throat swabs (83%). A total of 386 NPAs were obtained from 386 persons (229 male, 154 female, 3 sex unknown) in Bangkok during February 16, 2006–July 20, 2008.\n\n【5】A total of 2,500 fecal samples were obtained from patients (1,093 male, 1,398 female, and 9 sex unknown) in Edinburgh predominantly with gastroenteritis or other enteric diseases referred for bacteriologic screening during March, June, and September 2008. A total of 530 fecal samples were obtained predominantly from children (179 boys and 138 girls) <5 years of age with diarrhea during July 12, 2007–July 25, 2008, and a control group without diarrhea (116 male, 96 female, 1 sex unknown) during March 4–December 2, 2007, in Bangkok.\n\n【6】DNA was extracted from 200-μL samples of pooled or individual specimens (respiratory samples, clarified fecal supernatant) into 40 μL Tris-EDTA buffer as described . Respiratory and fecal samples from Edinburgh were screened in pools of 10; both sample types from Bangkok were screened individually. Screening was performed by using nested primers conserved between HBoV-1 and HBoV-2 in the nucleoprotein (NP)–1 gene (universal primers: outer sense \\[position 2589 in DQ000496 st2 isolate \\]: 5′-CCWATCGTCYTSYACTGCTTYGA-3′; outer antisense : 5′-TAGCYAAGTGTYTWBKGTACACATYAT-3′); inner sense : 5′-RTKSTGYGGBTTCTAYTGGCA-3′; and inner antisense : 5′-TACACATCATCCCARTAAYWACAT-3′).\n\n【7】Amplication conditions were 94°C for 2 min and 35 cycles at 94°C for 18 s, 50°C for 21 s, and 72°C for 1.5 min. Amplicons were differentiated by digestion with _Rsa_ I. Fragments were sized by agarose gel electrophoresis. All known HBoV-1 sequences contain an _Rsa_ I site between nt 2772 and nt 2773, resulting in fragments of 46 bp and 91 bp; this site is absent in HBoV-2 (undigested amplicon length of 237 bp).\n\n【8】Each pool or sample was additionally screened by using HBoV2-specific primers located in the nonstructural (NS)–1 gene (outer sense : 5′-AACA GA T G GG C AA G CA G AAC-3′; outer antisense : 5′-AGGACAAAGGTCTCCAAGAGG-3′; inner sense : 5′-AA C GA T TGCAGACAACGCCT TA T A \\-3′; and inner antisense : 5′-TCCAAGAGGAAATGAGTTTGG-3′; sites matching all known HBoV-2 variants and not matching HBoV-1 variants are underlined.) Amplification conditions were 95°C for 2 min; 5 cycles at 95°C for 45 s, 53°C for 1 min, and 72°C for min; and 35 cycles at 95°C for 30 s, 51°C for 30 s, and 72°C for 45 s. Positive pools of fecal samples from Edinburgh were divided and individual component samples were tested.\n\n【9】Respiratory and fecal samples from both centers were screened by using universal primers, and positive samples were digested with _Rsa_ I. Undigested amplicons and some predicted HBoV-1 fragments (46 bp and 91 bp) were sequenced to confirm virus identity. All samples were additionally screened with HBoV-2–specific primers; 16 undigested samples were positive with HBoV-2–specific primers, and all samples identified as HBoV-1 were negative. Thus, species-specific primers enabled effective screening of HBoV-2 among samples with high frequencies of HBoV-1.\n\n【10】HBoV-positive fecal samples were generally restricted to children <5 years of age (25 from 30 infected children whose ages were known) . Median age of children infected with HBoV-2 (7–12 months) was lower than that for those infected with HBoV-1 (1–2 years). Infections with HBoV-1 and HBoV-2 were observed at low frequencies in older persons (2 and 5 of 1,791 persons >35 years of age, respectively). For respiratory samples, HBoV-1 infections showed a similar peak incidence among children 1–2 years of age , similar to that observed for fecal samples. This age group was most frequently infected in our previous analyses of respiratory samples from Edinburgh . There were no differences in frequencies of HBoV-1 or HBoV-2 infection between male and female participants. Samples from Bangkok were divided into those from persons with diarrhea  and asymptomatic controls ; detection of HBoV-1 and HBoV-2 was restricted to persons with diarrhea (n = 12 and 2, respectively).\n\n【11】In contrast to its frequent detection in fecal samples, HBoV-2 was not detected in >6,500 respiratory samples . However, high frequencies of HBoV-1 were recorded (14% among children in Bangkok and 3.4% among children in Edinburgh); the group from Edinburgh contained a substantial number of older children (37% >5 years of age).\n\n【12】### Conclusions\n\n【13】Four conclusions can be drawn from this study. First, HBoV-2 circulates in 3 widely separated areas (United Kingdom, Thailand, and Pakistan ) and is likely distributed globally. Second, infections with HBoV-2 show a pattern of infecting young children, most <1 year of age. Third, absence of HBoV-2 in respiratory samples suggests a different tissue tropism that may influence its transmission route and ability to infect systemically and establish persistence. Determining the biologic basis for such differences will be useful in understanding the pathogenesis of HBoV-1–related respiratory disease. Fourth, at a practical level, absence of HBoV-2 in respiratory samples indicates no likely role for this virus in respiratory disease. Thus, screening methods may be adequate for detecting HBoV-associated respiratory disease. Nevertheless, the unexpectedly diverse human bocavirus group may contain additional variants with potential etiologic roles in respiratory or other diseases.\n\n【14】Since this study was completed, evidence for an interspecies HBoV-1/-2 recombinant associated with acute gastroenteritis has been obtained; the structural gene region was most closely related to HBoV-2, and NS1/NP-1 grouping with HBoV-1 . Although this recombinant would have been identified as HBoV-1 by using typing assays described in the current study, sequence analysis of HBoV-1–positive samples in this study and our previous study of respiratory samples from Edinburgh and Bangkok  identified only HBoV-1 in the study population, consistent with all other analyses of this sample type worldwide. Nevertheless, future typing assays should analyze both VP1/2 and NS/NP-1 to ensure that this and potentially other interspecies recombinants are identified. Investigation of genetic diversity of this group and development of effective screening methods for variants of HBoV is required for studies of human disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "69d7888a-d8c9-4565-a9e8-d10cf561e89c", "title": "Submicroscopic Malaria in Migrants from Sub-Saharan Africa, Spain", "text": "【0】Submicroscopic Malaria in Migrants from Sub-Saharan Africa, Spain\nSubmicroscopic malaria (SMM) is defined as low-density _Plasmodium_ infection detected only by molecular methods . SMM only occasionally causes acute disease but can infect mosquitoes and contribute to transmission .\n\n【1】In malaria-endemic countries, SMM prevalence varies widely. It is highest in areas of low transmission, where SMM represents a large proportion of the malaria reservoir . In regions to which malaria is not endemic, such as Europe, SMM prevalence is unknown but might account for up to one third of imported malaria cases . In areas such as Spain, _Anopheles atroparvus_ mosquitoes can transmit strains of _P. vivax_ , and SMM patients can be a reservoir for malaria reintroduction. We explored the frequency of imported SMM by PCR testing of a selected population of migrants to Spain from sub-Saharan Africa and describe the epidemiologic characteristics and main laboratory findings for SMM patients.\n\n【2】### The Study\n\n【3】We conducted a retrospective observational study based on data obtained after the application of an SMM screening protocol in immigrant patients of sub-Saharan Africa origin seen at the Tropical Medicine Unit of the Poniente Hospital (El Ejido, Almeria, Spain) during October 2004–December 2016. This hospital’s protocol comprised a series of complementary tests to screen for imported diseases, including chest and abdominal radiographs; blood count; liver and renal function tests, iron metabolism tests; serologic screening for syphilis, HIV, hepatitis B virus, hepatitis C virus, _Strongyloides_ , and _Schistosoma_ ; and screening for fecal parasites, urine parasites, and blood microfilariae. Finally, the hospital tested for hemoglobinopathies using high-performance liquid chromatography.\n\n【4】The study population comprised patients who had lived in Europe for <1 year (newly arrived migrants \\[NAM\\]) or who had visited their home country (i.e. visiting friends and relatives \\[VFR\\]) within the previous year who sought care for any reason other than patent malaria and were screened for SMM using the conventional nested multiplex malaria PCR. The nested multiplex malaria PCR can identify 4 human malaria species ( _P. vivax_ , _P. falciparum_ , _P. ovale_ , and _P. malariae_ ) in 2 consecutive multiplexing amplifications. The first reaction amplifies _Plasmodium_ DNA from blood samples. The second reaction enables identification of the infecting species of _Plasmodium_ . SMM was diagnosed when a patient had a positive malaria PCR result and a negative direct microscopic examination result, either a thin or a thick smear, and a negative rapid diagnostic test . We excluded from the study patients <14 years of age and patients for whom no smear and/or rapid diagnostic test for malaria was available. All SMM patients were treated according to World Health Organization guidelines .\n\n【5】We conducted 3 statistical analyses. First, we compared SMM patients with non-SMM patients. Then, within the SMM patient group, we compared those with and without filarial co-infection. We conducted a descriptive statistical analysis in which continuous variables were expressed as medians and interquartile ranges. Categorical variables were described as frequencies and percentages. We analyzed differences in continuous data between groups using nonparametric Mann-Whitney U test and used the Fisher exact test or χ 2  test, as appropriate, to compare categorical data. Finally, we conducted an explanatory multivariate logistic regression analysis to evaluate possible risk factors predicting SMM in the study population. The model used variables with p<0.2 in the bivariate analysis and those that were clinically relevant. Variables were excluded from the logistic regression model based on likelihood ratio test results . Hosmer-Lemeshow test and the area under the receiver operating characteristic curve were used to validate the model. We conducted statistical analyses using STATA software version 12 .\n\n【6】Of 2,719 sub-Saharan Africa patients seen, 370 (13.6%) were included in the study . SMM was diagnosed in 33 (8.9%) patients, of whom 11 were VFRs and 22 were NAMs. The proportion of SMM was similar in both groups: 8.7% (11/126) for VFRs and 9.0% (22/244) for NAMs (p = 0.93). For SMM patients, time spent in Spain after leaving malaria-endemic areas was shorter for VFRs (2 months) than for NAMs (6 months) (p = 0.001).\n\n【7】The _Plasmodium_ species most frequently found was _P. falciparum_ (26 \\[78.8%\\] patients), followed by _P. malariae_ (4 \\[12.1%\\]), _P. ovale_ (2 \\[6.1%\\]), and 1 mixed parasitization by _P. falciparum_ and _P. malariae_ (1 \\[3.0%\\]). Patients with and without SMM did not differ in baseline laboratory data, except for the presence of hemoglobinopathies, which occurred more frequently among SMM patients (42.4% vs. 21.3%; p = 0.01).\n\n【8】When we analyzed other associated infections , we found an important difference between SMM and non-SMM patients regarding filarial co-infection. Filariasis was present in up to 24.2% of SMM patients but in only 5.3% of non-SMM patients (p<0.01). _Mansonella perstans_ nematodes were responsible of all filarial infections; in addition, 3 patients were infected by _Loa loa_ eyeworms.\n\n【9】Among SMM patients, all filariasis was found in NAMs. These co-infected patients had higher IgE values ​​(1,080 IU/mL vs. 293.7 IU/mL \\[reference 0–100 IU/mL\\]; p<0.01) and higher total eosinophil counts (601.5 cells/μL vs. 270 cells/μL \\[reference 20–450 cells/μL\\]; p = 0.01) than those who had only SMM. The co-infected group also tended to have higher platelet levels .\n\n【10】Multivariate regression analysis applied to all 370 patients showed that having filarial infection increased the odds of having SMM by 6.49 and the existence of \\> 1 hemoglobinopathies increased the odds by 3.93. Time after leaving a malaria-endemic area correlated inversely with risk for SMM (p = 0.038) . For NAMs, filariasis increased the risk for SMM by 8.47 and hemoglobinopathies by 4.70. For VFRs, however, the only risk factor was time since last visit to their home country (the shorter the time, the higher the risk).\n\n【11】### Conclusions\n\n【12】Screening for SMM in patients from sub-Saharan Africa in a reference unit in Spain showed a prevalence of 8.9%. The presence of filarial infection or hemoglobinopathies and a shorter time since leaving malaria-endemic areas were associated with a higher risk for SMM.\n\n【13】SMM is usually asymptomatic. Infrequently, it produces acute disease, especially in children . During pregnancy, SMM has been linked to maternal anemia and to low birth weight . SMM screening in risk groups, such as pregnant women and immunosuppressed persons , could therefore be of special interest.\n\n【14】Our study highlights 2 important differences between patients with and without SMM. First, the proportion of patients infected by filariasis was higher among SMM patients. In areas to which malaria is not endemic, Ramírez-Olivencia et al. also reported a greater number of filariasis among SMM patients than among patients with patent microscopic malaria . Nematodes can alter immune system response to concomitant infections, such as _Plasmodium_ spp. Modulation of immune response produced by helminthoses, such as filariasis, might exert some protective effect against malaria, leading to lower parasitic loads, which in turn might translate into clinical protection against severe malaria .\n\n【15】The second disparity was the presence of hemoglobinopathies, a finding much more frequent among SMM patients that resulted in an SMM risk only for NAMs. Hemoglobinopathies exert a protective effect against severe malaria, favoring milder clinical manifestations and the existence of SMM .\n\n【16】The relatively high prevalence of imported SMM we found could justify implementation of systematic screening in immigrants and travelers who recently stayed in malaria-endemic areas, mainly for persons with risky conditions such as immunosuppression (especially those with HIV infection) and for pregnant women. The diagnosis and treatment of SMM also can prevent future reactivations and the existence of an occult malaria reservoir in countries to which it is not endemic. Our results suggest that the presence of filariasis, hemoglobinopathies, or both should also prompt a search for SMM because these patients are at higher risk.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "703882c9-8dc8-4e05-93e9-b1340a449d5d", "title": "Spotted Fever Group and Typhus Group Rickettsioses in Humans, South Korea", "text": "【0】Spotted Fever Group and Typhus Group Rickettsioses in Humans, South Korea\nHuman rickettsioses, known to occur in Korea, include mainly scrub typhus, murine typhus, and epidemic typhus. Scrub typhus, caused by _Orientia tsutsugamushi_ , a major rickettsial disease in Korea, is transmitted through the bites of mite larvae. An earlier study by Choi and colleagues reported that 34.3% of febrile hospital patients in autumn were seropositive for the disease . _Rickettsia typhi_ , transmitted by the fleas of various rodents, causes murine typhus, which is a milder form of typhus than human typhus . The first patient with murine typhus in Korea was reported in 1959. Two cases of murine typhus confirmed by culture were reported since 1988 , and now \\> 200 cases of murine typhus are presumed to occur annually in South Korea. Epidemic typhus is caused by _R_ . _prowazekii_ and is transmitted by the body louse . The disease is fatal in 10% to 30% of patients, depending on underlying diseases and the nutritional state of the host . The disease appeared after the end of the Korean War. Since 1951, however, no other cases have been reported in Korea .\n\n【1】Spotted fever group (SFG) rickettsioses are associated with arthropods, such as ticks, mites, and fleas . SFG comprises several divergent lineages: the _R_ . _rickettsii_ group, _R_ . _japonica_ , _R_ . _montana_ , the _R_ . _massiliae_ group, _R_ . _helvetica_ , _R_ . _felis_ , and the _R_ . _akari_ group . Recently, the nucleic acids of _R_ . _japonica_ and _R_ . _rickettsii_ were found in _Haemaphysalis longicornis_ in Korea . A previous seroepidemiologic study demonstrated that SFG rickettsioses were highly likely in Korea . No clinical human case of SFG rickettsioses, however, has been reported in Korea until now.\n\n【2】In this study, to check whether SFG rickettsioses were present in humans, serum specimens from patients with acute febrile disease were studied by using molecular sequence–based identification techniques. We report the presence of the _rompB_ gene of SFG rickettsiae, similar to _R_ . _akari_ , _R_ . _conorii_ , _R_ . _japonica_ , and _R_ . _felis_ , in serum specimens from Korean patients with acute febrile disease. The nucleic acids of both _R_ . _conorii_ and _R_ . _typhi_ were found to coexist in 7 serum specimens. This study presents the first molecular evidence of SFG rickettsioses in humans.\n\n【3】### Materials and Methods\n\n【4】##### Rickettsial Strains\n\n【5】The following strains were obtained from the American Type Culture Collection (ATCC; Manassas, VA, USA): _R_ . _typhi_ Wilmington (VR-144), _R_ . _prowazekii_ Breinl (VR-142), _R_ . _akari_ MK (VR-148), _R_ . _japonica_ YH (VR-1363), _R_ . _conorii_ Indian Tick Typhus (VR-597), and _R_ . _sibirica_ 246 (VR-151). These rickettsial agents were propagated in Vero (CRL-1586) or L929 (CCL-1) cell monolayers.\n\n【6】##### Serum Samples and Serologic Testing\n\n【7】The serum specimens analyzed in this study were obtained from South Korean patients with acute febrile illness from 1993 to 1999. The specimens were submitted to the Institute of Endemic Disease at Seoul National University’s Medical Research Center for laboratory diagnosis for scrub typhus, leptospirosis, and hemorrhagic fever with renal syndrome caused by hantavirus. Some of the serum specimens were used for the nucleic acid detection study of SFG rickettsial agents. The rationale for selecting the samples for polymerase chain reaction (PCR) analysis included the presence of immunoglobulin (Ig) M antibodies with titers from 1:40 to 1:160 against any of the tested antigens in the samples. Serologic testing was performed by indirect immunofluorescence assay (IFA) with a panel of 4 SFG rickettsial antigens, _R_ . _japonica, R_ . _akari_ , _R_ . _conorii_ , and _R_ . _sibirica_ , as previously described .\n\n【8】##### Oligonucleotide Primers\n\n【9】The oligonucleotide primers used for priming the PCRs are shown in Table 1 . The primers were developed on the basis of the _rompB_ gene sequences of _R_ . _conorii_ strain Seven , and the citrate synthase ( _gltA_ ) gene sequence of _R_ . _prowazekii_  was synthesized. The selection of the primers was based on the “primer 3” program , to obtain the optimal Tm and GC content and to avoid hairpin loop structures. The selected sequences were analyzed through the BLAST program .\n\n【10】##### Detection of _rompB_ Gene in Human Sera\n\n【11】DNA for PCR analysis was extracted from 200 μL of serum samples by using QIAamp Blood Mini Kit (Qiagen GmbH, Hilden, Germany) according to the manufacturer’s instructions. SFG and typhus group (TG) rickettsia _rompB_ gene in human sera were detected with multiplex nested PCR. The primary amplification of the specimen was performed in a final reaction volume of 50 μL. The reaction mixture contained 5 μL of prepared DNA sample, 20 pmol of _rompB_ outer forward primer (OF) and outer reverse primer (OR), 200 μM of deoxynucleoside triphosphate mixture (dNTP, Takara, Otsu, Japan), 1 x PCR buffer, 1.25 U Taq polymerase (Takara EX Taq, Takara), and distilled water. First, PCR reactions were incubated at 95°C for 5 min, subjected to 35 cycles of 95°C for 15 s, 54°C for 15 s, and 72°C for 30 s, and final extension at 72°C for 3 min in a GeneAmp PCR system 9600 (Perkin-Elmer Applied Biosystems, Foster City, CA, USA). After this, 2 μL of the amplified product was again amplified in a nested fashion with inner primer sets (rompB SFG IF, rompB SFG/TG IR, and rompB TG IF). The nested PCR reaction mixture contained 10 pmol of each primer in a PCR premixture tube (AccuPower PCR PreMix, Bioneer Corp. Daejon, Korea) that contained 1 U of Taq DNA polymerase, 250 μmol/L each of dNTP, 50 mmol/L of Tris-HCl (pH 8.3), 40 mmol/L of KCl, 1.5 mmol/L of MgCl 2  , and gel loading dye. The volume was then adjusted to 20 μL with distilled water. Nested PCR reactions were incubated at 95°C for 5 min, subjected to 35 cycles of 95°C for 15 s, 56°C for 15 s, and 72°C for 30 s, and final extension at 72°C for 3 min. PCR amplification of the _gltA_ gene of SFG and TG rickettsiae was performed by using the oligonucleotide pairs RpCS.877p and RpCS.1,258n for the primary PCR amplification and RpCS.896p and RpCS.1,233n for the secondary amplification. The primary PCR cycling condition consisted of incubation at 95°C for 5 min, then 35 cycles each of 15 s at 95°C, 15 s at 54°C, and 30 s at 72°C, followed by a final extension cycle of 3 min at 72°C. The nested PCR cycling condition consisted of incubation at 95°C for 5 min, then 35 cycles each of 15 s at 95°C, 15 s at 54°C, and 30 s at 72°C, followed by a final extension cycle of 3 min at 72°C. To avoid cross-contamination, 3 separate rooms with entirely separate equipment and solutions were used. Thus, the handling and treatment of samples and the addition of a template, the handling of DNA-free PCR reagents, and the post-PCR work were strictly separated. Aerosol-resistant tips (Axigen Scientific, Inc. Union City, CA, USA) were used for the handling of all reagents in the PCR study. The amplification products were visualized by electrophoresis on a 1.5% agarose gel stained with ethidium bromide (0.5 μg/mL) and using 1 x TAE migration buffer (pH 8.0; 40 mM Tris-acetate, 1 mmol/L EDTA).\n\n【12】##### Restriction Fragment Length Polymorphism (RFLP) Analysis\n\n【13】The PCR products were purified by using an AccuPrep PCR purification kit (Bioneer Corp.), according to the manufacturer’s instructions. Restriction endonuclease digestions were performed with 10 μL of amplified products by using _Alu_ I (New England Biolabs, Beverly, MA, USA). The digested DNA was resolved by electrophoresis through a 10% polyacrylamide gel at 100 V for 4 h in a 1 x TBE buffer (pH 8.0; 90 mmol/L Tris-borate, 2 mmol/L EDTA), and was visualized after staining with ethidium bromide.\n\n【14】##### Cloning, Sequencing, and Analysis of Nucleotide\n\n【15】All positive PCR products were cloned by using pGEM-T Easy Vector System I (Promega). Verifying whether the clones contained inserts was accomplished by digestion of plasmid DNA with EcoRI (New England Biolabs) and separation in 1.5% agarose gels. Plasmids containing DNA inserts were sequenced for both strands by using Big Dye Terminator Sequence Kit and ABI Prism 377 Automated DNA Sequencer (Perkin-Elmer Applied Biosystems), according to the manufacturer’s protocol. The obtained sequences, except for the primer regions, were aligned with the corresponding sequences of other rickettsiae deposited in the GenBank database to identify known sequences with a high degree of similarity using multisequence alignment programs, the Phydit software , and the MegAlign software package (Windows version 3.12e; DNASTAR, DYNASTAR Inc. Madison, WI, USA). Phylogenetic trees were generated by using the neighbor-joining algorithms and the Jukes and Cantor matrix. Bootstrap analysis was performed to investigate the stability of the trees obtained through the neighbor-joining method. The percentages of similarity were determined using the FASTA network service .\n\n【16】##### Nucleotide Sequence Accession Numbers Used\n\n【17】GenBank accession numbers of the _rompB_ gene sequences used for sequence comparisons are AB003681 for _R_ . _japonica_ , AF123705 for _R_ . _aeschlimannii_ , AF123706 for _R_ . _africae_ , AF123707 for _R_ . _akari_ , AF123708 for Astrakhan rickettsia strain A-167, AF123709 for _R_ . _australis_ , AF123711 for _R_ . _honei_ strain RB, AF123712 for Israeli tick typhus rickettsia, AF123714 for _R_ . _massiliae_ , AF123715 _R_ . _mongolotimonae_ , AF123716 for _R_ . _montanensis_ , AF123717 for _R_ . _parkeri_ , AF123719 for _R_ . _rhipicephali_ , AF123721 for _R_ . _conorii_ strain Seven, AF123722 for _R_ . _sibirica_ , AF123723 for _R_ . _slovaca_ , AF123725 for _R_ . _helvetica_ , AF182279 for _R_ . _felis_ , AF211820 for _R_ . _prowazekii_ strain Florida, AF211821 for _R_ . _prowazekii_ strain Virginia, AF123718 for _R_ . _prowazekii_ , AF161079 for _R_ . _prowazekii_ , AF479763 for _R_ . _amblyommii_ strain WB-8-2 rompB pseudogene, AY260451 for _R_ . _heilongjiangensis_ , AY260452 for _R_ . _hulinensis_ , L04661 for _R_ . _typhi_ crystalline surface layer protein (slpT) gene, and X16353 for _R_ . _rickettsii_ . The GenBank accession number of the _gltA_ gene sequence used for developing primers is M17149 for _R_ . _prowazekii_ .\n\n【18】### Results\n\n【19】##### Multiplex Nested PCR Amplification of _rompB_ Gene\n\n【20】Nested PCR assay, with primer pairs rompB OF and rompB OR in primary reactions and rompB SFG IF, rompB SFG/TG IR, and rompB TG IF in multiplex-nested reactions, was performed to identify the unknown rickettsial agents in the seropositive serum specimens and to differentiate between SFG and TG rickettsiae in terms of size. When the primers previously mentioned were used, the nested PCR assay generated ≈420 base pairs (bp) for SFG rickettsiae and about 230 bp for TG rickettsiae. The negative controls consistently failed to yield detectable PCR products, whereas the positive controls always gave the expected PCR products. Overall, 200 serum specimens from febrile patients from all areas of South Korea were tested. After the nested PCR was performed, the expected _rompB_ gene products were obtained from 24 seropositive serum samples. Figure 1 shows the result of electrophoresis of 24 PCR-amplified samples. Of the 24 amplified products, 16 showed the electrophoretic pattern of 1 DNA band of ≈420 bp, which corresponded to SFG. The amplified size of only 1 sample was ≈230 bp for TG. The 7 other amplified products showed an electrophoretic pattern of 2 bands of ≈420 bp for SFG and 230 bp for TG. Therefore, the 23 amplified products corresponding to SFG rickettsial agents were named H1 product, while the 8 products corresponding to TG were named H2 product. The H1 products included H1 to H24 (except H19), while the H2 products were H3-2, H7-2, H8-2, H13-2, H14-2, H15-2, H18-2, and H19 .\n\n【21】##### RFLP Analysis and Sequencing Analysis\n\n【22】RFLP analysis of the 23 H1 products corresponding to SFG rickettsial agents using _Alu_ I demonstrated that the restriction patterns of 17 H1 products were identical with that of _R_ . _conorii_ , 2 with that of _R_ . _akari_ , 1 with that of _R_ . _japonica_ , and 3 with that of _R_ . _felis_ . RFLP analysis of the 8 H2 products corresponding to TG rickettsial agents by using _Alu_ I showed that the restriction patterns of all the H2 products were identical with that of _R_ . _typhi_ .\n\n【23】##### Sequencing Analysis\n\n【24】To identify the SFG and TG rickettsiae detected in human serum specimens, nucleotide sequences of the PCR-amplified products were determined and compared with partial _rompB_ gene sequences of various rickettsial agents obtained from the GenBank database. Table 2 shows the similarity between the partial _rompB_ gene sequences of various rickettsial agents and 6 of the sequenced H1 products (clones H1, H3, H5, H10, H20, and H22). Clones H1, H3, and H20 showed 100%, 99.72%, and 98.87% degrees of similarity to _R_ . _conorii_ , respectively. Clone H10 showed 100% similarity to _R_ . _japonica_ , and clone H5 showed 100% similarity to _R_ . _akari_ . In particular, clone H22 showed 99.44% similarity to _R_ . _felis_ . All the compared H1 products showed low levels of similarity (70.90%–74.01%) to the TG species. The clones that clustered partially with the _rompB_ gene of _R_ . _conorii_ were differentiated in 3 groups by their levels of similarity: group 1 (12 H1 products with 100% similarity), group 2 (4 H1 products with 99.72% similarity), and group 3 (1 H1 product with 98.87% similarity). Clones H22, H23, and H24 clustered as the _R_ . _felis_ group. Table 3 shows the similarity between the partial _rompB_ gene sequences of various rickettsial species and H2 product sequences. All H2 products showed low levels of similarity (67.05%–69.94%) to SFG rickettsial species, such as _R_ . _sibirica_ , _R_ . _akari_ , _R_ . _conorii_ , _R_ . _felis_ , and _R_ . _japonica_ . They also showed high levels of similarity (93.64%–100%) to TG rickettsial species, such as _R_ . _prowazekii_ and _R_ . _typhi_ . The H2 products’ levels of similarity to _R_ . _typhi_ ranged from 99.42% to 100%. A neighbor-joining analysis based on partial _rompB_ gene sequences demonstrated that 17 H1 products formed a cluster with _R_ . _conorii_ , 2 with _R_ . _akari_ , 1 with _R_ . _japonica_ , and 3 with _R_ . _felis_ (data not shown). The analysis of the 8 H2 product sequences showed that the sequences of all H2 products formed a cluster with _R_ . _typhi_ and were separated from the SFG rickettsial strains (data not shown).\n\n【25】##### Nested PCR Amplification of _gltA_ Gene\n\n【26】The results of the multiplex nested PCR of the _rompB_ gene were confirmed by a second PCR assay with specific primer pairs RpCS.877p and RpCS.1,258 in primary reactions and RpCS.896p and RpCS.1,233 in nested reactions. The primer sets generated ≈338 bp for SFG and TG rickettsiae. The expected size of the _gltA_ gene fragment was generated in 22 of 24 samples that were positive for the PCR detection of the _rompB_ gene . All positive PCR products were cloned, and their sequences were determined. Since the PCR assay using primer sets for the amplification of the _gltA_ gene could not discriminate between the SFG rickettsia and TG rickettsia by size difference, the sequences of 3 clones for each PCR product were determined. The results of the sequencing analysis for _gltA_ \\-PCR amplifications were identical to those of the analysis of the rompB-PCR product (data not shown). Seven samples that were positive for both the _rompB_ genes of _R_ . _conorii_ and _R_ . _typhi_ were also positive for both of their _gltA_ genes (data not shown).\n\n【27】### Discussion\n\n【28】SFG and TG rickettsial infections occur worldwide and may cause serious diseases in humans. These pathogenic bacteria are transmitted to people by arthropod vectors, such as ticks, fleas, and lice. In this study, multiplex-nested PCR was conducted to detect and identify SFG and TG rickettsial antigens in patient sera with positive results from the serosurvey. The _rompB_ gene domain II region, which is a highly conserved region of _rompB_ , was targeted for PCR amplification for the specific detection of SFG and TG rickettsiae. Amplified DNA sequences were analyzed by using nucleotide-sequencing methods, and RFLP analysis was used to confirm the PCR results. The results indicated the presence of several SFG rickettsiae, _R_ . _conorii_ , _R_ . _akari_ , _R_ . _japonica_ , and _R_ . _felis_ , in the serum specimens. The results were also confirmed by a second PCR with specific primer pairs for the _gltA_ gene and by sequence analysis of its DNA amplicons.\n\n【29】For the first time, SFG rickettsiae in human serum specimens in South Korea have been reported. _R_ . _akari_ is a member of the spotted fever group rickettsiae and is a causative agent of rickettsial pox, a disease transmitted by the bite of _Allodermanyssus sanguineus_ , a mite ectoparasite of the domestic mouse ( _Mus muscularis_ ) . The disease was first described in New York City in 1946. _R_ . _akari_ was isolated from the Korean vole in 1957. The previous seroepidemiologic study conducted by the authors on 3,401 patients with febrile disease indicated that the seropositive rate was 16.24% for the rickettsial antigen through IFA. _R_ . _conorii_ is an etiologic agent of the Mediterranean spotted fever or boutonneuse fever . Our previous study indicated that the seropositive rate was 14.34% for the antigen. _R_ . _japonica_ , the causative agent of Oriental spotted fever, was first isolated from a patient with febrile, exanthematous illness in Japan in 1985 . The disease is now endemic in the southwestern part of Japan, where >100 cases have been described . Previous studies showed the presence of nucleic acids of _R_ . _japonica_ and _R_ . _rickettsii_ in _H. longicornis_ by PCR. Our seroepidemiologic study demonstrated that the seropositive rate was 19.9%. Although no clinical human case of SFG rickettsioses has been reported in Korea until now, this study’s findings strongly suggest the prevalence of SFG rickettsiosis in Korea.\n\n【30】_R_ . _felis_ is an emerging pathogen responsible for fleaborne spotted fever and had been considered a member of the TG rickettsiae based on its reactivity with anti– _R_ . _typhi_ antibodies. A genetic analysis of the 16S rRNA, citrate synthase, _rompA_ , and _rompB_ genes, however, placed _R_ . _felis_ as a member of SFG. _R_ . _felis_ has been reported in various countries, including the United States, Mexico, Brazil, Germany, and France . In Asia, the first case of _R_ . _felis_ infection was reported in 2003 . _R_ . _typhi_ was also among those detected in the SFG rickettsiae in the febrile disease patients’ sera. Fleas are also found to be vectors for _R_ . _typhi_ . Of major importance to the epidemiology of the rickettsioses caused by _R_ . _typhi_ and _R_ . _felis_ is the maintenance of both rickettsial agents in their hosts by transovarial transmission, and the fact that neither organism is lethal for fleas .\n\n【31】Finally, we report the presence of both _R_ . _conorii_ and _R_ . _typhi_ in serum from Korean patients. Sera from patients with SFG rickettsiosis have been reported to react with TG rickettsiae by using serologic analysis methods . The serum specimens from patients with TG rickettsiosis were also demonstrated to contain cross-reactive antibodies against SFG rickettsiae . In a previous study, approximately one third of specimens seropositive for antibodies against SFG rickettsiae had antibodies against TG rickettsiae . Therefore, the multiplex-nested PCR was designed to detect and differentiate SFG rickettsial agents from TG rickettsial agents in the patient serum specimens with positive results from the serosurvey with SFG rickettsial antigens. SFG rickettsiae and TG rickettsiae were differentiated in terms of the size of amplified products. PCR results also confirmed the RFLP and sequencing analysis. In sera taken from 7 patients, both SFG and TG rickettsial antigens were detected, which indicated dual infection. Previously, a case of dual infection with _Ehrlichia chaffeensis_ and an SPG rickettsia was reported in a human patient. Cases of dual infection with _Bartonella clarridgeiae_ and _B_ . _henselae_ in cats have also been reported, as well as infection with the 2 different genotypes of _B_ . _henselae_ . A recent report suggested that coinfection of _R_ . _felis_ with either _B_ . _clarridgeiae_ or _B_ . _quintana_ in fleas may cause dual infection in a human that comes in contact with flea feces . These reports support this study’s findings regarding the dual infection of SFG and TG rickettsiae in 7 patients. The differences between _R_ . _conorii_ and _R_ . _typhi_ vectors _,_ however, still cannot be explained, and further studies are needed.\n\n【32】In conclusion, this study confirmed, by using PCR-based amplification methods, that several SFG rickettsiae, _R_ . _conorii_ , _R_ . _akari_ , _R_ . _japonica_ , and _R_ . _felis_ , existed in the sera of Korean patients with febrile episodes. Our findings indicate that SFG rickettsiae, including _R_ . _felis_ , should be used in serologic tests on Korean patients suspected of having rickettsiosis. TG rickettsiae existed in 8 patients, and 7 of them were also infected with _R_ . _conorii_ . The evidence of double infection is expected to help describe the cross-reactivity between the patient sera of SFG rickettsioses and TG rickettsioses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "34359b1e-341b-49ac-a4d1-55ba82a83bc0", "title": "Prosthetic Valve Endocarditis due to Kytococcus schroeteri", "text": "【0】Prosthetic Valve Endocarditis due to Kytococcus schroeteri\n**To the Editor:** Bacteria belonging to the former genus _Micrococcus_ , the so-called micrococci, are usually regarded as contaminants from skin and mucous membranes. Nevertheless, micrococci have been reported as emerging pathogens in immunocompromised patients and have been described in severe infections . We describe what is, to our knowledge, the first case of prosthetic valve endocarditis caused by the newly described micrococcal species, _Kytococcus schroeteri_ . Accurate identification of this species is of particular importance as kytococci—in contrast to other micrococcal species—are frequently resistant to penicillin and oxacillin .\n\n【1】A 34-year-old woman was admitted to the hospital with acute, severe aortic regurgitation, attributable to a dissection of both the ascending and descending aorta, which extended into the supraaortic and iliac arteries. Immediate surgical intervention was performed by implantation of an aortic arch (St. Jude Medical Inc. St. Paul, MN) conduit and reimplantation of the supraaortic arteries. Ten weeks later, the patient was admitted to the hospital because of fever of 39°C. Laboratory studies showed a leukocyte count of 15.3 x 10 9  /L with 87% neutrophils and elevated C-reactive protein (180 mg/L). Transesophageal echocardiography and computed tomography suggested an abscess next to the prosthesis and showed vegetations on the prosthetic valve, which suggested endocarditis. Blood cultures yielded gram-positive cocci on four separate occasions during an 11-day period. Treatment, performed according to the antimicrobial susceptibilities of the isolates, consisted of vancomycin, gentamicin, and rifampin for 21 days. Within 1 week, the fever resolved and the leukocyte count returned to normal. Four days after antimicrobial therapy was initiated, right-sided hemiparesis and aphasia, thought to be due to an embolic cerebral stroke, developed. After those events, the aortic arch prosthesis was replaced without further complications.\n\n【2】Blood culture specimens were injected into BACTEC Plus culture vials for aerobic and anaerobic cultures and processed in BACTEC 9240 blood culture system (Becton Dickinson, Cockeysville, MD). Growth was detected in four different aerobic blood cultures after incubation of 3 to 5 days. Aerobic subcultures on Columbia agar supplemented with 5% sheep blood showed tiny, muddy-yellow colonies without hemolysis after 24 h of incubation. After 48 h, the size of colonies increased, a feature typical of _K. sedentariu_ s, which is known to grow slightly more slowly than other members of the former _Micrococcus_ genus. No or very weak reactions were found after 24 h incubation when the ID32 STAPH ATB gallery (bioMérieux Vitek, Hazelwood, MO) was used. After 48 h, the reactions with this gallery resembled those of _M. luteus_ or _M. lylae._ The probability of identification was indicated as 99.0% ( _M. luteus_ , T index of 0.77) for the profile 000003000 and 51.8% ( _M. lylae_ , T index of 0.98) and 47.2% ( _M. luteus_ , T index of 0.93), respectively, for the profile 000001000. When the ID-GPC card (VITEK 2, bioMérieux Vitek) was used, a poor selectivity was observed ( _M. luteus_ , T index 0.95; _Kocuria rosea_ , T index 0.84). All isolates were resistant to oxacillin, penicillin, fosfomycin, ampicillin, and erythromycin and susceptible to vancomycin, teicoplanin, gentamicin, netilmicin, chloramphenicol, imipenem, rifampin, tetracycline, amoxicillin/clavulanate, and ciprofloxacin, as determined by disk diffusion method performed on Mueller-Hinton agar.\n\n【3】When arbitrarily primed-polymerase chain reaction with prolonged ramp times  was used, isolates were shown to be clonal, representing one strain (DSM 13884 T  ). Since colony formations, resistance pattern, and growth rate of this strain did not correspond with the species identification, as obtained by automated systems, further phenotypic and molecular studies were conducted, confirming the micrococcal nature of this unknown strain and justifying the classification as a distinct species, _Kytococcus schroeteri_ sp. nov .\n\n【4】In addition to the _Micrococcus_ genus, bacteria belonging to the former genus _Micrococcus_ were recently divided into the genera _Kocuria_ , _Nesterenkonia_ , _Kytococcus_ , and _Dermacoccus,_ followed by rearrangement into two families ( _Micrococcaceae,_ _Dermatophilaceae_ ) of the suborder _Micrococcineae_ .\n\n【5】The traditional identification of the micrococci is based on their susceptibility to lysozyme and bacitracin and their resistance to lysostaphin and nitrofurantoin, in contrast to staphylococci, which display the opposite pattern. In automated identification systems, micrococci are included only in a limited manner. A prospective study showed an overall accuracy of results of 61.0% concerning _Micrococcus_ species when the STAPH-IDENT strip (bioMérieux) was compared with conventional identification methods .\n\n【6】Micrococcal species are ubiquitous inhabitants of the human skin and mucous membranes and are usually disregarded as contaminants in clinical specimens. Yet, various severe infections such as arthritis, central nervous system infection, pneumonia, peritonitis, hepatic abscess, endocarditis, and nosocomial blood stream infections have been documented . Since early reports of endocarditis caused by gram-positive cocci that appear in tetrads and packets often did not reliably differentiate between micrococci and phenotypically similar microorganisms, such as coagulase-negative staphylococci, the frequency of micrococcal endocarditis is difficult to ascertain and might be underestimated. However, several cases of endocarditis attributable to _M. lylae_ , _M. luteus_ , _K._ _sedentarius,_ and unspecified micrococci have been reported .\n\n【7】Regarding micrococci, data on antimicrobial susceptibilities are rare, and often the species affiliation remains unclear. In contrast to most micrococcal isolates, _K. sedentarius_ isolates, as well as those reported here, are resistant to penicillin G and oxacillin. In the patient we describe, therapy was performed with vancomycin, gentamicin, and rifampin, resulting in bacteriologic eradication and clinical cure. However, a generally accepted therapeutic regime for severe infections with kytococcal species has not yet been defined. Concerning micrococci other than kytococci, a combination of rifampin with ampicillin has been effective . Successful treatment has also been achieved with vancomycin, clindamycin, penicillin, gentamicin, or a combination of these agents. Overall, rifampin shows the highest activity against all micrococcal species .\n\n【8】This report is the first case of _K. schroeteri_ causing endocarditis on an artificial heart valve. The repeated recovery of this species from blood cultures strongly suggests a pathogenic role. We conclude that isolation of micrococci from blood specimens cannot always be disregarded as etiologically irrelevant. Results performed by automated identification systems should be interpreted with caution if micrococci are involved.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8efb534e-567a-42f3-ab7c-1834285484b4", "title": "Human Bocavirus and Gastroenteritis", "text": "【0】Human Bocavirus and Gastroenteritis\n**To the Editor:** We read with great interest the recent study by Vicente and colleagues, who suspect the human bocavirus (HBoV), a newly detected parvovirus initially described as a respiratory pathogen, to be a possible causative agent of gastroenteritis in children . These researchers investigated the presence of HBoV DNA in 527 stool samples from ambulatory patients (<36 months of age) with unrelated respiratory symptoms. Of these stool samples, 48 (9.1%) were positive for HBoV DNA. Other enteric pathogens were found in 58% of all HBoV-positive fecal samples.\n\n【1】A close taxonomic relationship exists between HBoV and bovine parvovirus, an animal virus capable of causing gastrointestinal symptoms in cattle . Taking into account the assumed high tenacity of this parvovirus against environmental factors and hospital-grade disinfectants , we believe the possibility of fecal-oral transmission, in addition to transmission via respiratory droplets, has to be considered in interpreting the observations of Vicente et al.\n\n【2】Gastroenteric symptoms have been described in up to 25% of all patients with respiratory HBoV infections . Although these observations suggest that HBoV may contribute to gastroenteritis or even be a causative agent, further studies are needed. Such studies should include control groups of asymptomatic patients and should test stool samples for HBoV. The correlation between detection of HBoV and clinical symptoms of gastroenteritis needs further confirmation in animal models, which are still not available. Nevertheless, the study by Vicente et al. did not clarify the extent of respiratory symptoms in patients with HBoV-positive stool samples. Taking into account the nature of parvovirus particles, we believe the virus likely passed through the gastrointestinal tract, as patients frequently swallow virus-containing sputum or nasal secretions. Thus, the observation that HBoV is an enteric pathogen should be considered a preliminary finding. Finally, we suggest that the role of HBoV should be investigated through histologic examination of mucosa biopsy specimens (e.g. from patients with chronic gastrointestinal diseases) to confirm pathogenicity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "929d98a3-4ae9-44a5-8004-068b1024f425", "title": "Sugar-Sweetened Beverage Intake Among Adults, by Residence in Metropolitan and Nonmetropolitan Counties in 12 States and the District of Columbia, 2017", "text": "【0】Sugar-Sweetened Beverage Intake Among Adults, by Residence in Metropolitan and Nonmetropolitan Counties in 12 States and the District of Columbia, 2017\nAbstract\n--------\n\n【1】The objective of this study was to describe the prevalence of sugar-sweetened beverage (SSB) intake among US adults (n = 68,896) residing in metropolitan and nonmetropolitan counties, by state, using data from the Behavioral Risk Factor Surveillance System. We used multinomial logistic regression to calculate adjusted prevalence ratios for daily (≥1 time per day) SSB intake. Overall, 26.0% of respondents reported daily SSB intake, with significantly higher prevalence in nonmetropolitan counties (30.9%) than in metropolitan counties (24.8%) (adjusted prevalence ratio = 1.32, 95% confidence interval, 1.26–1.39). This same pattern was significant in 5 of 11 states with metropolitan and nonmetropolitan counties. These findings could inform efforts to reduce frequent SSB intake in nonmetropolitan areas.\n\n【2】Objective\n---------\n\n【3】Sugar-sweetened beverages (SSBs) are a leading source of added sugars in diets of US adults . Frequent SSB intake is associated with increased risk of obesity, type 2 diabetes, cardiovascular disease, and other health consequences . Although a pooled analysis of 9 states found higher prevalence of daily SSB intake among adults in nonmetropolitan counties than in metropolitan counties , state-level SSB intake among adults by metropolitan and nonmetropolitan counties is unknown. This analysis aimed to determine prevalence of daily SSB intake among adults residing in metropolitan and nonmetropolitan US counties by sociodemographic characteristics, weight status, and state.\n\n【4】Methods\n-------\n\n【5】The Behavioral Risk Factor Surveillance System (BRFSS) is a state-based, random-digit–dialed, annual landline and cellular telephone survey of US adults aged 18 years or older conducted by state health departments with assistance from the Centers for Disease Control and Prevention (CDC) to monitor chronic health conditions and associated risk factors . It uses multistage stratified sampling to yield a representative sample of the civilian noninstitutionalized adult population in all 50 states, the District of Columbia, and 3 US territories. In 2017, 12 states and the District of Columbia (median BRFSS response rate, based on combined landline and cell phone data, 43.0% \\[range, 32.9%–54.0%\\] ) included an optional module with 2 questions on SSB intake: 1) “During the past 30 days, how often did you drink regular soda or pop that contains sugar? Do not include diet soda or diet pop.” and 2) “During the past 30 days, how often did you drink sugar-sweetened fruit drinks (such as Kool-Aid and lemonade), sweet tea, and sports or energy drinks (such as Gatorade and Red Bull)? Do not include 100% fruit juice, diet drinks, or artificially sweetened drinks.” Respondents reported number of times per month, week, or day; all responses were converted to daily intake, and responses for the 2 questions were summed to calculate total SSB intake. We categorized frequency of total SSB intake as none, more than 0 to fewer than 1 time per day, and 1 or more times per day. Of 80,662 adult respondents, we excluded 11,766 (14.6%; range, 10.4%–26.1%) who had missing responses to either SSB question. Our analysis included 68,896 adults who responded to both questions in the SSB module. We calculated unadjusted prevalence estimates overall, and by age, sex, race/ethnicity, education, employment status, weight status, and state. We used χ 2  tests to determine whether SSB intake differed by covariates (significant at _P_ < .05).\n\n【6】We used the National Center for Health Statistics Urban–Rural Classification Scheme for Counties to classify levels of urbanization of counties . We further dichotomized counties into metropolitan (large central metro, large fringe metro, medium metro, and small metro) and nonmetropolitan (micropolitan and noncore) . Delaware and the District of Columbia do not have nonmetropolitan counties; thus, we calculated SSB intake frequencies only for metropolitan counties in these areas. We determined adjusted prevalence ratios (APRs) and 95% confidence intervals (CIs) and compared SSB intake (<1 time per day and ≥1 time per day, with a reference of 0 times per day) between metropolitan and nonmetropolitan counties by using multinomial logistic regression, controlling for age, sex, and race/ethnicity within population subgroups (significant at _P_ < .05). All analyses were performed by using SAS version 9.4 (SAS Institute) and accounted for sampling weights and complex survey design.\n\n【7】Results\n-------\n\n【8】In 2017, 26.0% of respondents reported consuming SSBs 1 or more times per day; responses ranged from 17.0% in Vermont to 40.1% in Arkansas . The unadjusted prevalence of SSB intake differed significantly by each sociodemographic characteristic and by weight status . A higher percentage of adults in nonmetropolitan counties (30.9%) than in metropolitan counties (24.8%) reported daily SSB intake (APR = 1.32; 95% CI, 1.26–1.39; _P_ < .001) . The largest APRs for daily SSB intake between adults in nonmetropolitan counties and adults in metropolitan counties, by sociodemographic characteristic, were the following: by age, among adults aged 25 to 34 (APR = 1.45; 95% CI, 1.27–1.65); by sex, among men (APR = 1.33; 95% CI, 1.25–1.43); by race/ethnicity, among non-Hispanic others (APR = 2.01; 95% CI, 1.69–2.39); by education, among college graduates (APR = 1.51; 95% CI, 1.33–1.70); and by employment status, among employed persons (APR = 1.39; 95% CI, 1.30–1.49). At the state level, daily SSB intake in nonmetropolitan counties ranged from 17.0% in Vermont to 44.1% in Arkansas , and in metropolitan counties SSB intake ranged from 17.1% in Vermont to 37.6% in West Virginia. Adjusted daily SSB intake was significantly higher among adults in nonmetropolitan than metropolitan counties in 5 of 11 states.\n\n【9】Discussion\n----------\n\n【10】In 2017, 26.0% of adults in 12 states and the District of Columbia reported consuming SSBs 1 or more times per day. Frequency of daily SSB intake was significantly higher in nonmetropolitan counties than in metropolitan counties, with approximately 1 in 3 adults in nonmetropolitan counties and 1 in 4 adults in metropolitan counties consuming SSBs 1 or more times per day. We found this significant association in 5 of 11 surveyed states that had nonmetropolitan counties.\n\n【11】The reported prevalence of daily SSB intake was lower than the prevalence reported in BRFSS 2016 data, in which 32.1% of US adults in 9 states reported consuming SSBs 1 or more times per day . This discrepancy could be attributed to geographic differences: only 5 states were included in both analyses. Differences in SSB marketing, access, availability, or pricing may exist in different communities . Consistent with previous studies, our study showed higher prevalence of daily SSB intake among younger adults, men, non-Hispanic adults, less educated adults, and unemployed adults . However, ours is the first report describing state-level SSB intake among US adults by metropolitan status.\n\n【12】Our findings are subject to limitations. First, only 12 states and the District of Columbia included the optional SSB module in 2017, and the response rate was relatively low, although the data were adjusted for some factors related to nonresponse. Therefore, our findings may not be generalizable nationwide. Second, SSB intake was measured by 2 questions that did not specify other SSB sources, such as sweetened coffees. The 2 questions measured frequency instead of volume; thus, total amount of SSB intake could not be determined. Third, our analysis did not control for education, employment, or other characteristics potentially associated with metropolitan status. Finally, self-reported data may be subject to recall or social desirability bias.\n\n【13】Decreasing SSB intake could reduce the burden of chronic diseases among US adults. Strategies to decrease SSB intake should consider subgroups with high intake, such as people with less education, and seek to overcome challenges in nonmetropolitan counties. Our findings could inform efforts that aim to reduce frequent SSB intake in nonmetropolitan areas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2bd6f24b-07c8-4419-b24d-1a1e7d89c8be", "title": "Obstetric Outcomes and Delivery-Related Health Care Utilization and Costs Among Pregnant Women With Multiple Chronic Conditions", "text": "【0】Obstetric Outcomes and Delivery-Related Health Care Utilization and Costs Among Pregnant Women With Multiple Chronic Conditions\nAbstract\n--------\n\n【1】Our objective was to measure obstetric outcomes and delivery-related health care utilization and costs among pregnant women with multiple chronic conditions. We used 2013–2014 data from the National Inpatient Sample to measure obstetric outcomes and delivery-related health care utilization and costs among women with no chronic conditions, 1 chronic condition, and multiple chronic conditions. Women with multiple chronic conditions were at significantly higher risk than women with 1 chronic condition or no chronic conditions across all outcomes measured. High-value strategies are needed to improve birth outcomes among vulnerable mothers and their infants.\n\n【2】Objective\n---------\n\n【3】Complications from chronic conditions are a key driver of rising obstetric morbidity and mortality in the United States . The prevalence of chronic conditions, including multiple chronic conditions (MCCs), continues to rise among childbearing women . MCCs, defined as 2 or more chronic conditions, are associated with worse health outcomes, including death, and higher levels of health care utilization and costs among the non-pregnant adult population . Understanding these data influenced the development of high-value programs that improved health outcomes and lowered costs in high-risk, high-cost populations . Our objective was to measure obstetric outcomes and delivery-related health care utilization and costs among pregnant women with MCCs.\n\n【4】Methods\n-------\n\n【5】We conducted a retrospective, cross-sectional analysis of 2013–2014 data from the National Inpatient Sample , a nationally representative sample of hospital discharges in the United States. Deliveries were identified by using previous methods , and data for chronic conditions were abstracted by using relevant codes from the _International Classification of Diseases, Ninth Revision, Clinical Modification_ (ICD-9-CM)  (Box). We included 8 chronic conditions that are prevalent in the childbearing population and associated with obstetric morbidity and mortality: chronic respiratory disease, chronic hypertension, substance use disorders, pre-existing diabetes, chronic heart disease, chronic kidney disease, human immunodeficiency virus/acquired immune deficiency syndrome, and chronic liver disease .\n\n【6】Box. International Classification of Diseases, Ninth Revision, Clinical Modification Codes Used in Study\n\n| Condition | Codes |\n| --- | --- |\n| Chronic respiratory disease | 491.x–496.x |\n| Chronic hypertension | 401.x–405.x, 642.0x, 642.1x, 642.2x, 642.7x |\n| Substance use disorders | 303.01, 303.02, 303.03, 304.x, 305.0x, 305.2x–305.9x, 648.3x |\n| Pre-existing diabetes | 249.x, 250.x, 648.0x |\n| Chronic heart disease | 412.x–414.x, 394.x-397.x, 424.x, 428.22, 428.23, 428.32, 428.33, 428.42, 428.43, 745.0x–747.4x, 648.5x |\n| Chronic renal disease | 581.x-583.x, 585.x, 587.x, 588.x, 646.2x |\n| Chronic liver disease | 571.x, 572.x |\n| Human immunodeficiency virus/acquired immune deficiency syndrome | 042.x, V08.x |\n\n【8】Obstetric outcomes included preterm delivery, cesarean delivery, and severe maternal morbidity and mortality. Preterm delivery was measured by using ICD-9-CM code 644.21, and cesarean delivery was measured by using ICD-9-CM codes 740.x, 741.x, 742.x, 744.x, and 749.9. Severe maternal morbidity was measured by using standardized diagnosis and procedure codes outlined by the Centers for Disease Control and Prevention . Health care utilization measures included need for hospital transfer and length of stay. Delivery-associated hospital charges from the Healthcare Cost and Utilization Project’s cost-to-charge ratio files were used to calculate costs. Charge and cost estimates were adjusted for inflation to 2014 dollars.\n\n【9】We used multivariable logistic regression models to estimate obstetric outcomes. We generated estimates for hospital transfer, length of stay, and costs by using multivariable Poisson regression models, because these data were not normally distributed (Shapiro–Wilk test, _P_ < .001 for each). All estimates were population averages generated with post-regression predictive margins and tabulated per 100 delivery hospitalizations. Models were adjusted for age, rural versus urban residence, primary insurance payer, median household income for the patient’s zip code, and hospital region. In sensitivity analyses of our cost estimates, we controlled for obstetric outcomes to isolate the independent association between MCCs and cost. We used complete case analysis rather than imputation, because less than 3% of observations had missing data. All analyses were conducted using STATA version 14.2 (StataCorp LLC). Our analysis of de-identified data was exempt from review by the study site’s institutional review board.\n\n【10】Results\n-------\n\n【11】Our sample consisted of 1,508,413 unweighted delivery hospitalizations, representing 7,542,063 weighted delivery hospitalizations occurring nationally in 2013–2014. We identified MCCs in 12,567 unweighted delivery hospitalizations (weighted percentage, 0.83%; 95% confidence interval \\[CI\\], 0.80%–0.86%) and 1 chronic condition in 127,350 unweighted delivery hospitalizations (weighted percentage, 8.4%; 95% CI, 8.3%–8.6%). Women with MCCs were older than women with 1 chronic condition or no chronic conditions (30.2 y, 28.5 y, and 28.2 y, respectively) . A higher proportion of women with MCCs (63.8%) had Medicaid as their primary payer compared with the proportion of women with 1 chronic condition (53.2%) or no chronic conditions (42.9%).\n\n【12】Among pregnant women hospitalized for obstetric delivery, rates of preterm delivery, cesarean delivery, and severe maternal morbidity and mortality were significantly higher among women with MCCs than among women with no chronic condition or 1 chronic condition . The rate of severe maternal morbidity and mortality among women with MCCs (6.4 per 100 delivery hospitalizations) was nearly 4 times higher than among women with no chronic conditions (1.7 per 100 delivery hospitalizations). Similarly, health care utilization and costs were highest among women with MCCs compared with those with no chronic condition or 1 chronic condition, even after controlling for obstetric outcomes and length of stay . Women with 1 chronic condition were also at significantly higher risk than women with no chronic conditions across each outcome measured.\n\n【13】Discussion\n----------\n\n【14】In this nationally representative sample of delivery hospitalizations from 2013–2014, women with MCCs had worse health outcomes, higher levels of health care utilization, and greater hospital costs associated with obstetric delivery compared with women with no chronic conditions or 1 chronic condition. In particular, the rate of severe maternal morbidity and mortality was 276% higher among women with MCCs than among women with no chronic conditions. Hospital costs were highest among patients with MCCs compared with those with no chronic condition or 1 chronic condition — even after even after controlling for obstetric outcomes and length of stay. It is plausible that higher costs may have resulted from greater use of consultative or social services, data that were not captured by our methods.\n\n【15】A limitation of our study design is that clinicians were likely to document ICD-9-CM codes only for conditions that were addressed during a delivery hospitalization. As such, our point estimates of MCC prevalence are likely to be conservative. For example, some women with MCCs may have been misclassified as having 1 chronic condition or no chronic conditions. Total costs associated with MCCs are also likely to be larger than our estimates. For example, women with MCCs had higher rates of preterm delivery, and considerable neonatal costs are associated with preterm birth .\n\n【16】Our findings show that pregnant women with MCCs are a high-risk, high-cost population. The strong association between MCCs and worse delivery-related health outcomes found in our study suggests that MCCs may contribute to the high levels of maternal morbidity and mortality in the United States. MCCs also contribute significantly to the cost of delivery hospitalizations. Delineating the highest-risk and highest-cost combinations of conditions may provide crucial data for the development of high-value strategies to improve birth outcomes among these vulnerable mothers and their infants.\n\n【17】Tables\n------\n\n【18】 Table 1. Characteristics of Patients With No Chronic Conditions, 1 Chronic Condition, and Multiple Chronic Conditions (Unweighted N = 1,508,413), National Inpatient Sample, 2013–2014 a  \n\n| Characteristic | No Chronic Conditions (n = 1,368,496) | 1 Chronic Condition (n = 127,350) | Multiple (≥2) Chronic Conditions (n = 12,567) |\n| --- | --- | --- | --- |\n| **Mean age b , y** | 28.2 (28.1–28.3) | 28.5 (28.5–28.6) | 30.2 (30.0–30.3) |\n| **Payer** | **Payer** | **Payer** | **Payer** |\n| Medicaid | 42.9 (42.1–43.7) | 53.2 (52.3–54.1) | 63.8 (62.5–65.1) |\n| Private | 51.3 (50.5–52.2) | 42.4 (41.4–43.3) | 32.2 (30.9–33.5) |\n| Uninsured | 5.8 (5.5–6.0) | 4.4 (4.2–4.7) | 4.0 (3.6–4.4) |\n| **Bottom income quartile c** | 27.1 (26.3–27.9) | 33.4 (32.4–34.5) | 40.4 (38.9–41.9) |\n| **Rural residence** | 14.1 (13.6–14.5) | 14.6 (14.0–15.3) | 13.7 (12.8–14.6) |\n| **Hospital region** | **Hospital region** | **Hospital region** | **Hospital region** |\n| Northeast | 15.9 (15.2–16.7) | 17.6 (16.6–18.7) | 17.2 (15.7–18.8) |\n| Midwest | 21.2 (20.4–22.0) | 22.1 (21.0–23.1) | 22.2 (20.7–23.9) |\n| South | 38.4 (37.4–39.5) | 38.3 (37.0–39.6) | 39.9 (38.0–41.9) |\n| West | 24.5 (23.6–25.4) | 22.0 (21.0–23.1) | 20.6 (19.3–22.0) |\n\n【20】a  All data presented as weighted percentage (95% confidence interval) unless otherwise noted.  \nb  Weighted mean (95% confidence interval).  \nc  Patients living in a zip code with a median household income in the bottom national income quartile.\n\n【21】 Table 2. Weighted National Estimates of Delivery-Related Outcomes Among Patients With No Chronic Conditions, 1 Chronic Condition, and Multiple Chronic Conditions, National Inpatient Sample, 2013–2014 a  \n\n| Outcome | No Chronic Conditions | 1 Chronic Condition | Multiple (≥2) Chronic Conditions |\n| --- | --- | --- | --- |\n| **Health outcomes** | **Health outcomes** | **Health outcomes** | **Health outcomes** |\n| Preterm delivery (<37 weeks) | 5.7 (5.6–5.7) | 9.7 (9.5–10.0) | 15.1 (14.4–15.9) |\n| Cesarean delivery | 31.9 (31.7–32.2) | 41.2 (40.8–41.6) | 53.3 (52.3–54.2) |\n| Severe maternal morbidity and mortality | 1.7 (1.7–1.7) | 3.0 (2.8–3.1) | 6.4 (5.9–6.8) |\n| **Health care use** | **Health care use** | **Health care use** | **Health care use** |\n| Hospital transfer | 1.1 (0.9–1.2) | 2.0 (1.8–2.3) | 3.5 (3.1–4.0) |\n| Length of stay, in days b | 2.6 (2.6–2.6) | 3.1 (3.1–3.2) | 4.3 (4.2–4.3) |\n| **Health care expenditures,** $ c | **Health care expenditures,** $ c | **Health care expenditures,** $ c | **Health care expenditures,** $ c |\n| Mean charges per delivery hospitalization | 16,000  | 20,000  | 28,000  |\n| Mean cost per delivery hospitalization | 4,500  | 5,600  | 7,700  |\n\n【23】a  All data presented as rate per 100 delivery hospitalizations (95% confidence interval) unless otherwise noted. Adjusted for age, rural vs urban residence, payer, national income quartile for zip code of residence, and hospital region.  \nb  Weighted mean (95% confidence interval).  \nc  Inflation-adjusted to 2014 US dollars.\n\n【24】 Table 3. Weighted National Estimates of Mean Cost (95% CI) a  per Delivery Hospitalization Among Patients With No Chronic Condition, 1 Chronic Condition, and Multiple Chronic Conditions, National Inpatient Sample, 2013–2014\n\n| Model Adjustment b | No Chronic Conditions | 1 Chronic Condition | Multiple (≥2) Chronic Conditions |\n| --- | --- | --- | --- |\n| Severe maternal morbidity and mortality | 4,500  | 5,500  | 7,300  |\n| Preterm delivery | 4,500  | 5,500  | 7,400  |\n| Cesarean delivery | 4,500  | 5,300  | 6,900  |\n| Length of stay | 4,500  | 5,500  | 7,400  |\n| All outcomes | 4,500  | 5,200  | 6,200  |\n\n【26】Abbreviation: CI, confidence interval.  \na  Inflation-adjusted to 2014 US dollars.  \nb  In addition to the following demographic covariates: age, rural vs urban residence, payer, national income quartile for zip code of residence, and hospital region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2ce51278-85ca-40d1-b76b-c8d7f241fa26", "title": "Seroprevalence of Kaposi Sarcoma–associated Herpesvirus and Other Serologic Markers in the Brazilian Amazon", "text": "【0】Seroprevalence of Kaposi Sarcoma–associated Herpesvirus and Other Serologic Markers in the Brazilian Amazon\nKaposi sarcoma–associated herpesvirus (KSHV) is the cause of Kaposi sarcoma (KS) and certain lymphoproliferative diseases . KSHV seroprevalence is low (<5%) in most Western populations  and reaches 50% in some African populations , mirroring KS incidence rates . However, the highest KSHV seroprevalences worldwide (>80% in adults) have been reported in Amerindian tribes from the Amazon regions of Brazil  and Ecuador , despite the apparently low KS incidence in these populations . KSHV is thought to be transmitted through saliva between young siblings in disease-endemic areas such as French Guiana  or Africa , whereas sexual transmission in low-prevalence countries occurs within risk groups such as men who have sex with men . Modes of transmission have not been clearly determined in Amerindian populations.\n\n【1】### The Study\n\n【2】We conducted a cross-sectional study to investigate the seroprevalence and factors associated with KSHV infection in Amerindian and non-Amerindian populations living in 2 regions of the Brazilian Amazon: a remote rural region of Para State (Mapuera, on the banks of the Trombetas River) and Manaus, the capital city of Amazonas State . Serologic markers of fecal–oral (hepatitis A virus \\[HAV\\]), blood-borne (hepatitis B and C viruses \\[HBV, HCV\\]) and sexually transmitted infections ( _Treponema pallidum_ \\[syphilis\\] and herpes simplex virus type 2 \\[HSV-2\\]) were used as proxies to identify possible routes of KSHV transmission in these populations.\n\n【3】A convenience sample of unselected Amerindians and non-Amerindians living in the Mapuera area and a consecutive sample of nonpaid first-time blood donors from the Manaus blood bank (HemoAm) consented to collection of blood samples, as previously reported  Ethical approval was obtained from the institutional review board of HemoAm, the ethical board of the Brazilian Ministry of Health, and the ethics committee of the London School of Hygiene and Tropical Medicine.\n\n【4】In the absence of a definitive test to determine KSHV infection, all serum specimens were tested by using a previously validated in-house whole-virus KSHV ELISA  and 2 immunofluorescence assays (IFAs) that detected antibodies against lytic (IFA-lytic) and latent-associated nuclear antigens (IFA-LANA) . KSHV infection was defined as positivity by any of these serologic assays. Serum specimens were also tested for the agent of syphilis by using a _T. pallidum_ –specific assay (Enzygnost Syphilis; Dade Behring, Marburg, Germany); for HSV-2 antibodies by using the type-specific HerpeSelect gG2 ELISA (Focus Technologies, Cypress Hill, CA, USA), with a higher cut-off (>3.5) to increase specificity ; and for HAV antibodies by using BioELISA HAV (Biokit, Barcelona, Spain). Presence of HBV anti-core antibodies was determined by using Ortho HBc ELISA (Ortho Diagnostics, Raritan, NJ, USA) in Mapuera serum specimens and Hepanostika anti-HBc Uni-Form (Organon-Teknika, Boxtel, the Netherlands) in Manaus serum specimens. HCV antibodies were detected by using Ortho HCV 3.0 ELISA (Ortho Diagnostics) in Mapuera serum specimens and Murex Anti-HCV version 4.0 ELISA (Murex Biotech S.A. Kyalami, South Africa) in Manaus serum specimens.\n\n【5】KSHV seroprevalence was calculated separately for men and women and directly age-standardized to the Mapuera Amerindian population. The risk associated with KSHV infection was estimated with prevalence ratios (PRs) and 95% confidence intervals (CIs), adjusted for sex and age group (18–24 years, 25–34 years, and \\> 35 years for the blood donor population; 0–9 years, 10–17 years, 18–24 years, 25–34 years, and \\> 35 years for both Mapuera populations). The associations of KSHV with sociodemographic variables, indicators of socioeconomic status, and other serologic markers were estimated with odds ratios (ORs) and 95% CIs. Variables associated with a significant increased risk for KSHV (p<0.05) in univariable analysis were included in a multivariable logistic regression model adjusted for age and sex.\n\n【6】We recruited 339 Amerindians (median age 22 years, interquartile range \\[IQR\\] 13–37 years; 57.5% female) and 181 non-Amerindians (median age 17 years, IQR 9–35 years; 58.6% female) in the Mapuera communities and 1,133 blood donors (median age 25 years, IQR 21–32 years; 22.9% female) in Manaus. The blood donor population had a similar age distribution to that of the adult population in Manaus in the 2000 regional census .\n\n【7】Among Mapuera Amerindians, KSHV seroprevalence was 65.0% in those 0–9 years, increasing to 92.9% in those \\> 35 years. In contrast, among Mapuera non-Amerindians, KSHV seroprevalence was 9.8% in those 0–9 years of age, increasing to 50.0% in those \\> 35 years of age. Among blood donors, KSHV seroprevalence was 31.3% in those \\> 35 years of age and 53.8% in the 13 who were of Amerindian descent. After age standardization, KSHV seroprevalence remained lower among Mapuera non-Amerindians (30% and 27% among men and women, respectively) and blood donors (16% and 23%, respectively) than among Mapuera Amerindians. When results were compared with those of the Mapuera Amerindians, the age-and sex-adjusted PRs were 0.35 (95% CI 0.28–0.45) and 0.59 (95% CI 0.56–0.63) in Mapuera non-Amerindians and blood donors, respectively.\n\n【8】In each population, KSHV seroprevalence was slightly higher among females, and increased with age (p for trend <0.001) in Mapuera Amerindians and non-Amerindians, but not among (adult) blood donors . KSHV seroprevalence varied little with house crowding (socioeconomic indicator), and hepatitis infections, but was associated with HSV-2 infection in non-Amerindians (OR 4.2, 95% CI 2.1–8.5) and blood donors (OR 1.3, 95% CI 1.0–1.7). In Amerindians, KSHV infection was not associated with HSV-2 in univariable analysis (OR 0.7, 95% CI 0.3–1.9).\n\n【9】In multivariable analysis , KSHV infection remained associated with female sex among blood donors (age- and sex-adjusted OR \\[aOR\\] 1.3, 95% CI 1.0–1.7), and increased significantly with age in both Mapuera populations (p for trend <0.001). KSHV infection was associated with HSV-2 infection among Mapuera non-Amerindians (aOR 2.7, 95% CI 1.2–6.5) and Manaus blood donors (aOR 1.3, 95% CI 1.0–1.6), but was inversely associated with HSV-2 infection in Mapuera Amerindians (aOR 0.3, 95% CI 0.1–0.9).\n\n【10】### Conclusions\n\n【11】Our data confirm the high KSHV seroprevalence observed among Amazonian Amerindian populations . However, the inclusion of convenience samples of remote populations and first-time blood donors, who may not necessarily be representative of the adult general population and notably exclude persons who report a range of potentially high-risk behavior for sexually transmitted and blood-borne infections, may have limited the generalizibility of our findings. High KSHV seroprevalence combined with an apparent lack of KS development among Amerindian populations support the theory of genetic predisposition to KSHV acquisition, as hypothesized for other Amazonian populations, in whom segregation genetic analysis has suggested that an unidentified recessive gene may influence KSHV serostatus .\n\n【12】The high KSHV seroprevalence (65%) among Mapuera Amerindians <10 years of age contrasts with the low (9.8%) seroprevalence among non-Amerindians of the same age group living in the same area, which suggests different transmission modes in these neighboring populations. Although we did not collect data on the age of initial sexual experience in either population, the high prevalence in childhood and inverse association with HSV-2 supports nonsexual transmission of KSHV in Amerindians. Conversely, the association of KSHV infection with HSV-2 among Mapuera non-Amerindians and blood donors supports a role for sexual transmission in these groups, although saliva transmission in younger urban inhabitants cannot be ruled out. Universal HAV infection status and low rates of HBV and HCV in all populations precluded any meaningful analysis of transmission routes associated with hepatitis viruses.\n\n【13】In summary, this study contributes data on the epidemiology of KSHV infection and transmission in some Brazilian Amazonian populations. Irrespective of urban or rural setting, our data are consistent with a predominant non-sexual transmission of KSHV (most likely through saliva) in Amerindian tribes compared with a probable combination of sexual and nonsexual modes of transmission among non-Amerindian populations living in the same region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1f7e6590-cd01-4c4c-a1c6-ad4a5062a078", "title": "Antimicrobial Resistance Gene Delivery in Animal Feeds", "text": "【0】Antimicrobial Resistance Gene Delivery in Animal Feeds\nAntimicrobial resistance in bacterial pathogens is a major impediment to successful therapy, and in several instances, bacterial strains have arisen that are refractory to most available antimicrobial treatments . Resistance arises by mutation (influencing the target or efflux of the antimicrobial agent) or by the acquisition of resistance genes (encoding antimicrobial or target alteration, or alternate pathways) . The actual origins of acquired resistance genes are unknown, but environmental microbes, including the strains producing antimicrobial agents, are believed to be important sources . Substantial genetic and biochemical similarities exist between resistance determinants in antimicrobial agent–producing actinomycetes and resistance genes found in gram-positive and gram-negative pathogens .\n\n【1】Since vancomycin-resistant enterococci (VRE) were clinically isolated in Europe  and the United States , VRE infections have been reported throughout the world. These infections may be life-threatening because choices for alternative treatment are limited. Concomitant with human use of vancomycin, avoparcin, a closely related glycopeptide antimicrobial agent, has been widely used in Europe and other continents as an animal growth promoter . VRE have been isolated, commonly from pigs and chickens fed avoparcin-containing animal feed **,** and humans coming into contact with the animals (farm workers, butchers) have been shown to carry VRE ; identical clones have been found . The public health concern about the emergence and dissemination of VRE in food animals and the food supply caused the European Union to ban the use of avoparcin in animal feed in 1997. The discontinued use of avoparcin in animal feed has resulted in a reduction in the number of vancomycin-resistant organisms isolated from animals .\n\n【2】High-level glycopeptide resistance is conferred by a cluster of three genes, _vanH, vanA, vanX_ (the _van_ cluster), plus associated regulatory elements; the cluster is often carried by conjugative transposons . The _vanH_ gene encodes a D‑lactate dehydrogenase that provides the requisite D‑lactate. _vanX_ encodes a highly specific DD-peptidase that cleaves only D-Ala-D-Ala produced endogenously while leaving D-Ala-D-Lac intact. The third gene, _vanA_ , encodes an ATP-dependent D-Ala-D-Lac ligase. Replacement of D-Ala-D-Ala by D-Ala-D-Lac in the bacterial cell wall results in a thousandfold reduction in the binding of glycopeptide antimicrobial agents to their peptidoglycan target . Studies have demonstrated the presence of _vanHAX_ homologs, such as _vanH-ddlN-vanX_ , in actinomycete strains producing glycopeptides, and strong structural and functional similarity exists between the various homologs and the _van_ cluster of VRE . Some researchers have proposed that the _vanH_ , _vanA_ , and _vanX_ genes of hospital enterococci may have been acquired en bloc from the actinomycetes . Related _vanHAX_ gene clusters have been identified in _Paenibacillus_ spp. by Patel and coworkers, indicating another possible source of the _van_ cluster . Regardless of the microbial source, the feeding of crude antimicrobial preparations to animals is plausible as a delivery process for transferring the cognate antimicrobial resistance genes between producing strains and the commensal bacteria of animals ; the concomitant selection for resistance would ensure the survival of rare resistant strains. We provide evidence that a DNA-encoding homolog of the _van_ cluster is a contaminant of feed-grade avoparcin and propose that animal use both created and selected for glycopeptide-resistant strains. The emergence of vancomycin-resistant _Staphylococcus aureus_ (VRSA) is a recent sequela to this train of events involving the _van_ gene clusters .\n\n【3】### Materials and Methods\n\n【4】##### DNA Extraction from Avoparcin\n\n【5】A suspension (0.7 mL) of avoparcin (Roche, Sydney, Australia) in H 2  O (100 μg/mL) was centrifuged in a 1.5-mL Eppendorf tube for 6 min and the supernatant, after being shaken with 1 volume of phenol:chloroform:isoamyl alcohol (25:24:1), was centrifuged at 16,000 x _g_ for 3 min. The aqueous phase was subjected to two additional phenol-chloroform extractions. The nucleic acid in the pooled aqueous fractions was precipitated with ethanol; the pellet was recovered by centrifugation and further purified by using a GeneClean spin kit (BIO101) and resuspended in 100 μL of double-distilled H 2  O. The DNA concentration was measured with a fluorometer (Model TKO100, Hoefer Scientific Instruments, San Francisco, CA).\n\n【6】##### PCR Amplification of 16S rDNA Sequences\n\n【7】Primers 16S 440F and 16S 1491R  were designed to amplify partial 16S rDNA sequences. The polymerase chain reaction ((PCR) contained 2 mM MgCl 2  , 0.16 mM dNTP, 0.4 μL of each primer, _Taq_ polymerase (1 U), 3‑15 ng template, and 5% dimethyl sulfoxide (DMSO). PCR was done in a MiniCycler (MJ Research, Waltham, MA) by using the following program: 96°C, 3 min; 96°C, 30 s; 60°C, 45 s; 72°C, 1 min 30 s; 35 cycles; and 72°C, 10 min.\n\n【8】##### PCR Amplification of _vanH, ddlN,_ and _vanX_ Sequences\n\n【9】Different combinations of PCR primers  were used to amplify the entire _van_ cluster . Reaction conditions were as described previously.\n\n【10】##### Cloning of _vanH, ddlN, vanX,_ and Partial 16S rDNA Genes\n\n【11】PCR products were cloned by using vector pCR 2.1-TOPO (Invitrogen, Burling, Ontario, Canada) according to the manufacturer’s instructions, and the insertion size was confirmed by a second PCR. Plasmid DNA was extracted by using the Concert rapid plasmid miniprep system (Invitrogen).\n\n【12】##### DNA Sequence Analysis\n\n【13】Cycle sequence reactions were carried out with a BigDye terminator DNA sequencing kit (Applied Biosystems, Foster City, CA) with plasmid DNA templates. The cycle sequence program was as follows: 96°C, 1 min; 96°C, 30 s; 50–60°C (dependent on different primers and fragments), 15 s; 60°C, 4 min, for 25 cycles. Excess oligos and dyes were removed by using CentriSep spin columns (Princeton Separations, Aldelphia, NJ). Reaction products were sequenced by the Nucleic Acid and Protein Service, University of British Columbia, using an ABI PRISM 377 sequencer. Sequences were analyzed by using the standard nucleotide-nucleotide BLAST program (National Center for Biotechnology Information, Bethesda, MD). and comparisons were carried out by using CLUSTAL W (European Bioinformatics Institute, Cambridge, UK).\n\n【14】### Results and Discussion\n\n【15】Direct extraction of avoparcin powder with phenol/chloroform/isoamyl alcohol provided substantial amounts of DNA (30.5 μg/g of avoparcin) . PCR amplification of the DNA with oligonucleotide primers specific for a region of streptomycete 16S rRNA gave a single amplicon , which was sequenced and shown to be 16S closely related to that of _Amycolatopsis coloradensis_ , the producer of avoparcin. Figure 3B shows similarities between the 16S rRNA of species that produce glycopeptide antimicrobial agents.\n\n【16】To examine for the presence of genes involved in glycopeptide resistance from the antimicrobial agent–derived DNA, we used the DNA primers described by Marshall et al. The amplicons  were cloned, sequenced, and assembled, indicating a _van_ \\-like cluster closely related to that found in _A. orientalis_ and _Streptomyces toyocaensis_ . Control reactions run without added template were negative.\n\n【17】The genes encoded three putative proteins showing >50% amino acid identity to the Van H, A, and X proteins of VRE . All of the clusters have translational overlaps between the _vanA_ and _vanX_ genes and their homologs, suggesting cotranslational regulation of expression. This finding clearly implies that the _van_ cluster must be transferred and acquired in toto from any source organism.\n\n【18】We suggest that the use of crude avoparcin preparations in animal feeds from 1975 to 1996 was the origin of the _vanHAX_ cluster in the genesis of VRE (and possibly that found in VRSA) . Large amounts of avoparcin were used in animal feed; in Denmark, for example, total vancomycin use in 1994 amounted to 24 kg, whereas avoparcin use in animals was 24,000 kg . During their entire lives, broiler chickens received 15 mg/kg and pigs 20–40 mg/kg of antimicrobial agentin their feed. Each pig was fed 5–10 g of the crude drug for its life span and, consequently, received a steady dose of DNA encoding vancomycin resistance. In Europe, an estimated 100 mg of antimicrobial agents are used in animal feed for the production of 1 kg of meat for human consumption. We believe that this regimen would have favored the selection and maintenance of rare bacterial transformants carrying the resistance genes. If one bears in mind that large numbers of pigs and chickens were exposed to the antimicrobial agent, the probability of gene pick-up by bacterial commensals in the animal gastrointestinal tract would be favored, and once incorporated into a gut commensal genome, further dissemination would have followed under antimicrobial selection. The finding that organization of the _van_ cluster in contaminating DNA of the feed is identical to that in VRE, with overlapping reading frames typical of translational coupling of gene expression between the _vanA_ and _vanX_ homologs , reinforces this supposition.\n\n【19】The mechanism by which a _van_ cluster becomes functionally integrated into bacteria is not known. We propose that intestinal bacteria were the original recipients of the DNA; many of the resident strains are known to be competent for DNA uptake . However, mere uptake is not sufficient for function, and the actinomycete genes differ from VRE genes in their G+C content (approximately 65% vs. 50%) and codon usage. Given the enormous complexity of bacterial populations in the mammalian gastrointestinal tract , we assume that a variety of intestinal species may have incorporated the resistance-encoding DNA; expression (at low levels) would have been rare, depending on the compatibility of the _van_ genes with the transcription and translation system of the host. Under constant antimicrobial selection pressure, translationally competent sequences would have developed by mutation; this would not necessarily have occurred in enterococci. Nonetheless, the conversion (evolution) of the actinomycete genes into functional enterococcal genes likely would have required many generations of growth under constant selection, and any intermediate stages in this process are a matter of speculation; however, once established on conjugative transposons, the genes would be readily disseminated . A number of similar _van_ clusters have been identified in different bacterial species, and whether these evolved independently or by divergent evolution is unknown.\n\n【20】The finding of resistance genes in crude antimicrobial products intended to be fed to animals adds to the already strongly voiced opinion that use of antimicrobial agents in this way constitutes a serious public health concern and further emphasizes the need for prohibiting the use in animal feed of all antimicrobial agents that are employed in human therapy. This ban should include structurally or biologically related antimicrobial agents and the use of any compound with the potential to select for cross-resistance to another antimicrobial agent . The use of avoparcin in Denmark was prohibited in 1995 and in the European Union in 1997. Subsequently, several other antimicrobial growth promoters were banned . However, the United States and Canada permit the use of many such products, including penicillin, tetracycline, macrolides, and sulfonamides **.** Nonhuman applications of antimicrobial agents, such as in agriculture and aquaculture, should employ only chemically and biologically distinct classes of compounds developed specifically for that purpose _._ Clearly such measures should be combined with a requirement for rational and prudent measures for antimicrobial use in the human population.\n\n【21】Many antimicrobial agents (or their close structural relatives) have been used extensively as animal-feed additives. In almost all cases, crude antimicrobial preparations are used, and thus the antimicrobial agent acts as a carrier for its cognate resistance genes. These delivery systems provide the opportunity for resistant strains of bacteria to evolve and so create an enormous gene pool for antimicrobial resistance determinants in the environment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b603380a-28b1-4774-82aa-623afb23dcfb", "title": "Using Remotely Sensed Data To Identify Areas at Risk for Hantavirus Pulmonary Syndrome", "text": "【0】Using Remotely Sensed Data To Identify Areas at Risk for Hantavirus Pulmonary Syndrome\nIn 1993, a disease characterized by acute respiratory distress with a high death rate (>50%) among previously healthy persons was identified in the southwestern United States. This disease, hantavirus pulmonary syndrome (HPS), was traced to infection with an unrecognized, directly transmissible virus--Sin Nombre virus (SNV) (Bunyaviridae; Hantavirus)  . The virus was maintained and transmitted primarily within populations of a common native rodent, the deer mouse (Peromyscus maniculatus)  , and transmission to humans occurred through contact with secretions and excretions of infected mice  .\n\n【1】It has been hypothesized that the El Niño Southern Oscillation (ENSO) of 1991-92 was the major climatic factor producing environmental conditions leading to the outbreak of HPS in 1993. Unseasonable rains in 1991 and 1992 during the usually dry spring and summer and the mild winter of 1992 are thought to have created favorable conditions for an increase in local rodent populations .\n\n【2】This hypothesis is based primarily on the following observations: 1) ENSO tends to influence the timing and abundance of precipitation in the southwestern United States. 2) Some Peromyscus populations increased dramatically in areas where precipitation was above average but remained near normal levels where precipitation did not increase--this observation is based on comparison of data from only two study areas: the University of New Mexico's Sevilleta Long-Term Ecological Research station (90 km south of Albuquerque, NM), where precipitation was 2 to 3 times the previous 20-year average, and Moab, Utah, where precipitation was at or below normal in the summer of 1992. Before and during the HPS outbreak, populations of P. maniculatus did not increase in Moab, while at sites on the Long-Term Ecological Research station they were 10 to 15 times higher than normal. Moreover, both Moab and the research station were 200 km to 300 km from the epicenter of the 1993 HPS outbreak, making comparison with conditions where disease occurred uncertain. 3) 1993 case studies found that rodent abundance varied dramatically over short distances. Rodent populations were higher at HPS households than at neighboring households without disease or randomly selected households at least 25 km away  ; however, these one-time case studies provide little information on the responses of rodent populations to changes in environmental conditions. Although associating weather with HPS outbreaks is consistent with these observations, supporting data are limited. This reflects the situation for many emerging diseases, as active surveillance for unidentified diseases is rare.\n\n【3】Current attempts to understand the factors leading to HPS outbreaks focus on detailing the chain of events from weather, through changes in vegetation, to virus maintenance and transmission within rodent populations, culminating in changes in human disease risk (trophic cascade hypothesis) . An impediment to this approach, however, is the focal nature of SNV within local populations of P. maniculatus. Among local rodent populations the rates of infection vary, and some populations appear uninfected  . The reason for this is uncertain but may be related to the stochastic loss of the horizontally transmitted viruses within local populations of the reservoir  . Alternatively, the dynamics of local populations may be such that SNV cannot be maintained at very low population numbers. Under either circumstance, responses of local populations to environmental fluctuations could substantially alter human risk. Given the sporadic nature of HPS outbreaks, ongoing, longitudinal monitoring of rodent populations in the vicinity of subsequent cases is unlikely  . Consequently, inferring human disease risk from rodent-SNV population dynamics will again require extrapolating from studies in regions other than the site of the outbreak.\n\n【4】In this article, we examine the relationship of the environment to HPS risk by using locations of HPS cases as sites where people were associated with infected rodents. This approach avoids the immediate need to establish the conditions that lead some reservoir populations to be uninfected by SNV. We compare the environmental characteristics of sites where people were infected with those at sites where people were not infected. Differences in environmental conditions could indicate factors that influence either the abundance of rodents or the occurrence of virus, creating testable hypotheses of environmental conditions that influence SNV infection patterns in reservoir populations. As a partial test of our method, the analysis was repeated when cases of HPS were uncommon. Under these conditions, the identified environmental factors should indicate low levels of disease risk.\n\n【5】Two sources of information were used as measures of the local environmental conditions preceding the HPS outbreak: 1) Monthly patterns of precipitation from March to June (generated from archived weather station data in the region to estimate local precipitation patterns at both case and control sites) and 2) satellite imagery (obtained before the HPS outbreak and used as a measure of variable, local environmental conditions and HPS risk evaluated by epidemiologic analysis).\n\n【6】### Methods\n\n【7】##### Study Population and Region\n\n【8】The epidemiologic analysis was performed as a case-control study. Twenty-eight (93.3%) of 30 sites with confirmed cases of HPS identified in the region between November 1992 (identified retrospectively) and November 1994 were selected. Inclusion criteria were based on clinical disease consistent with the Centers for Disease Control and Prevention case definition that was confirmed by serologic, nucleic acid, or immunohistochemical tests  . One case was excluded because the likely site of exposure could not be established, and a second case because we could not confirm that the proper geographic location of the site was recorded during data collection.\n\n【9】Sites of exposure were established previously by investigation of each case-patient's activities and, for most fatal cases, demonstration of sequence homology of polymerase chain reaction (PCR)-amplified regions of SNV nucleic acids obtained from case-patients and rodents collected at the imputed sites of exposure . Sites of exposure for the HPS cases were at or in the immediate vicinity of households, and there usually was a history of activities that could have generated exposure to contaminated aerosols  .\n\n【10】To control for issues related to access to care and socioeconomic conditions, controls were selected randomly from all households that used the same health clinics as the HPS patients during the same period as the HPS patients. Controls were randomly selected among persons without HPS. A total of 170 persons with different residential addresses were identified from a listing of visits to all the clinics during the time of the HPS outbreak. This represented an approximately 2% random sample of addresses of the patient population. A previous study showed that subclinical infection with SNV was not observed among controls  . Geographic locations of case and control sites were established by using global positioning system (GPS) receivers to record latitude and longitude at each site.\n\n【11】The study area of 105,200 km 2  was located in the southwestern United States, incorporating the region of the original HPS outbreak . Epidemiologic surveillance was part of the HPS outbreak investigation.\n\n【12】##### Environmental Characterization of Sites\n\n【13】Monthly precipitation data recorded at 196 weather stations throughout the region from 1986 to 1993 were from the U.S. National Oceanic and Atmospheric Administration's National Climatic Data Center. Spring precipitation at the weather stations was calculated by aggregating monthly precipitation for March through June. Spring precipitation at individual case and control sites was estimated by interpolation among the nearest weather stations to each case or control site. Interpolation procedures used the eight nearest weather stations to estimate precipitation by applying trend surface algorithms from the GIS software (IDRISI, 11 ). Spring precipitation during 1992 to 1993 at each site was compared with spring precipitation during previous years by paired differences tests, with correction for multiple tests. The null hypothesis for each case or control site was that there was no increase in precipitation during the spring of 1992 to 1993 compared with the previous years. We also used satellite imagery to develop detailed characterization of local environmental conditions. We selected three archived Landsat Thematic Mapper (TM) images originally recorded in mid-June 1992 for analysis. TM records digital numbers (DN) from reflected light in six bands, three in visible and three in infrared (IR) portions of the electromagnetic (EM) spectrum. (An additional band of thermal IR energy also is recorded but was not used.)\n\n【14】The TM images were merged to form the study area of 105,200 km. 2  Nominal pixel resolution was 30 m. Images were geometrically and radiometrically corrected by the U.S. Geological Survey (USGS) Earth Resources Observation Systems (EROS) Data Center. The images were imported into a raster-based geographic information system (GIS) for geographic registration by using control points obtained from USGS quadrangle maps  . The images were resampled by a bilinear resampling procedure with a quadratic mapping function  . Corrections for atmospheric scattering of bands one to three were performed by the method of Chavez  . Latitude and longitude positions of case and control households were imported as a data layer in IDRISI GIS. Additional environmental variables incorporated in GIS included elevation, slope, and aspect of the case and control sites. Elevation was derived from the USGS digital elevation models (1:250,000 scale), while slope and aspect were generated from these data by using software in GIS.\n\n【15】Three satellite images from mid-June 1995 were selected to further validate the analysis of the 1992 imagery. There was no ENSO from 1994 to 1996, and we predicted that any relationship between the satellite imagery in 1992 and the case and control sites would indicate reduced risk in 1995. DNs for selected locations on the 1992 and 1995 images were compared to determine any significant changes in sensor calibration.\n\n【16】##### Epidemiologic Analysis of HPS by Satellite Imagery\n\n【17】The spatial distribution of HPS sites (cases) was compared with that of control sites to determine if cases were spatially aggregated within the study region  . Then, the relationship between HPS and environmental factors measured by TM imagery was examined. Because the analysis used three tiled images, the strategy for model development involved model fitting by using a portion of the study region, followed by external validation  . A sample of HPS sites and control sites was selected for analysis, by using logistic regression to examine the relationship between the odds of a site being an HPS site (outcome variable) and the DN in each TM band, elevation, slope, and aspect (predictor variables). The analysis was then repeated by using the remaining HPS and control sites to validate the model. Identifying the same model in the two analyses would indicate that the HPS risk model was robust for that period.\n\n【18】The initial model used a test area of 12,279 km 2  from the east-central region of the study area. This area included 14 case and 36 control sites and was entirely within one TM image. The validation analysis used the remaining sites (14 case and 134 control sites) and incorporated all or parts of the three TM images covering 92,921 km. 2  The average DN for each of the six TM bands used in the analysis was calculated with a 3 x 3 pixel filter centered on the location for each case or control site. This sampled a local region of approximately 8,100 m 2  around each case or control site.\n\n【19】We used logistic regression analysis to identify the best combination of TM bands and environmental variables associated with HPS status  . Elevation was dichotomized at the median elevation for case and control sites, combined. Inclusion of remotely sensed variables was based on the statistical significance of their coefficients in the model. Elevation was retained in all models because of its observed association with P. maniculatus abundance  . The logistic model was evaluated with the Deviance and the Hosmer-Lemeshow goodness of fit statistics (C) for deciles of risk. To examine the accuracy of the model predictions, we evaluated the sensitivity and specificity by creating a Receiver Operator Characteristic function  . The function compares the true-positive rate (sensitivity) against the false-positive rate (1 - specificity) of a model by using various predicted values as thresholds identifying case and control sites. In addition to examining spring precipitation, we evaluated the trophic cascade hypothesis by examining the relationship between HPS risk and vegetation growth before the HPS outbreak  , regardless of habitat type. We used data from the near infrared (band 4) and red (band 3) portions of the spectrum to generate a normalized difference vegetation index of the region. The index is a standard algorithm used as a measure of vegetative growth. We compared the index and the TM bands identified in the initial epidemiologic analysis for estimating HPS risk by comparing the Receiver Operator Characteristic's generated by each analysis.\n\n【20】### Results\n\n【21】Cases and controls occupied the same general, geographic area with the greatest difference of most extreme sites of 20 km in the north-south and 13 km in east-west directions. Despite the broad geographic overlap, cases of HPS were not randomly distributed within the area. HPS cases were spatially clustered . Despite this clustering, HPS sites were widely separated geographically. The average distance between case sites was 50.3 km (SD = 23.8 km), and the nearest neighboring sites (k = 1) were not themselves likely to be case sites.\n\n【22】Spring precipitation patterns showed substantial interannual variation at case and control sites . From 1986 through 1993, precipitation was 4.5 mm  to 110 mm . Spring precipitation decreased markedly between 1992 and 1993 . Overall, precipitation at control sites tended to be lower (65 mm) than at case sites (72 mm) during the spring each year, but there was broad overlap among sites, and yearly variation at control sites tracked that of the case sites . None of the case sites had higher precipitation during 1992 to 1993 than during the preceding 6 years (p >0.05).\n\n【23】There was a significant relationship between local environmental conditions and HPS risk as measured by the statistical association between the DN recorded by the satellite in 1992 and HPS risk the following year. The logistic regression analysis developed for the training area fit the observed data well (Deviance = 45.45; p = 0.49, df = 46; Hosmer Lemeshow C = 6.62; p = 0.58, df = 8), and none of the sites were obvious outliers. Higher level interactions between the independent variables did not change the results. Three of the six bands from the TM images ( 1 , 5 , and 7 ) were associated with the odds of HPS . Sites above the median elevation (2,094 m) were marginally associated with risk in the training area, but elevation was retained because of the relatively small sample sizes used to estimate the parameters, as well as the biologic rationale outlined elsewhere  .\n\n【24】High DN values in the blue (band 1) and mid-infrared (band 7) portions of the EM spectrum were associated with decreased risk for HPS, while high values in the mid-infrared (band 5) portion of the spectrum were a risk factor for HPS . The DN values were approximately 58 to 233 units. Each unit change in the average DN around sites altered the odds of HPS risk by 6% (band 1), 15% (band 7). Sites above 2,094 m in the test area were >4 times as likely to be HPS sites as sites below 2,094 m. Slope and aspect at the sites were not associated with risk.\n\n【25】The Receiver Operator Characteristic graph of sensitivity and specificity of the predictor function showed that at least 95% of the case sites were correctly identified until the proportion of control sites correctly identified exceeded 56% (20 of 36 control sites) . The same predictor variables from the TM imagery were identified when the analysis was repeated for the validation area. The coefficients of the validation analysis did not differ significantly from those for the training area . This model also fit the data well (Deviance = 74.49; p = 1.00, df = 144; Hosmer Lemeshow C = 6.78; p =0.56, df = 8).\n\n【26】Because the logistic models did not differ between the training and validation areas, all the sites were combined to give an overall model . The overall Receiver Operator Characteristic  had a sensitivity and specificity similar to those of the test area (95% sensitivity, 62% specificity). This threshold corresponded to a predicted value for HPS of approximately 0.10. Thus, using a predicted log odds ratio of at least 0.10 as a threshold for increased risk included 95% of the case sites and excluded 62% of the control sites.\n\n【27】The logistic function was applied to each pixel in the study area to produce a map of predicted risk   . The analysis was repeated by using satellite images from June 1995 to predict HPS risk for 1996 . There was a near elimination of predicted high-risk areas in the 1995 imagery and a broad expansion of low-risk areas compared with the 1992 images. The single case of HPS reported from the region in 1996 occurred at a site with a predicted risk of 0.16 (i.e. above the HPS threshold).\n\n【28】Areas of high vegetation growth in 1992, as measured by the normalized difference vegetation index  incorporated only a portion of the HPS high-risk areas . We evaluated vegetation growth, as measured by the index, as a predictor of HPS risk by modeling the case-control data, using the index and elevation as predictor variables. The vegetation growth model that best fit the observed data included an exponential transformation of the normalized difference vegetation index and sites above the median elevation. This index model accounted for a significant part of the variation in the HPS risk (deviance = 147.62; p = 0.99, df = 196) but did not accurately model the odds of HPS. In this analysis 11 (39.3%) of 28 case sites had standardized residuals exceeding three standard deviations, suggesting a poor fit to the data--an interpretation supported by the Hosmer Lemeshow statistic (C = 20.09; p = 0.01, df = 8), which indicated that the form of risk model fit the data poorly. The receiver operator characteristic of the vegetation index analysis  also lost sensitivity more rapidly than the analysis using TM bands 1, 5, and 7, especially over the range of values near the threshold of increasing HPS risk.\n\n【29】### Conclusions\n\n【30】Satellite imagery, combined with epidemiologic surveillance, retrospectively identified areas at high risk for HPS associated with Peromyscus populations over broad geographic regions during the 1993 outbreak. TM data identified environmental conditions near HPS sites that were measurably different from conditions in rural, populated sites where disease did not occur for nearly 1 year before the outbreak. These environmental conditions varied with the presence of ENSO . The geographic extent and general level of predicted HPS risk were higher during ENSO, supporting the view that El Niño may increase the likelihood of HPS outbreaks. The hypothesized pathway between ENSO, increased spring precipitation leading to increased vegetation growth, and subsequent HPS risk, however, was not strongly supported by the data. Possible reasons for this lack of support are discussed below.\n\n【31】In this study, we used a retrospective epidemiologic approach to risk assessment  . Therefore, odds ratios of the environmental characteristics were used to estimate the population's relative risk for HPS. This approach is valid when the cases used in the study are representative of all cases, the controls are representative of the general population, and the disease is relatively rare. Under these conditions, odds ratios approximate relative risk  .\n\n【32】HPS is rare-fewer than 1,000 cases have been identified in North America, although most occurred in the southwestern region of the United States. In this study, the cases included nearly all (28 of 30) sites in the region where HPS occurred during the outbreak. Although bias induced from this factor is unlikely, we cannot be certain that environmental factors identified with outbreaks of HPS are similar to those with the sporadic, single cases of HPS reported each year. However, the accurate identification of the site where the single case of HPS was observed in 1996 suggests that the classification may also identify risk characteristics for this group. Issues related to personal privacy make accessing these data difficult, however, because geographic locations of residences, for example, are considered personal identifiers.\n\n【33】The selection of controls focused on a population from the same socioeconomic and geographic region as the HPS cases. Although HPS cases were clustered, the maximal geographic extent of both cases and controls was similar, suggesting that the enrollment procedure adequately fulfilled this goal. Random selection of controls also was intended to control for access to care in a region where travel may be difficult. Restricting controls to those using the same health-care facilities again raises the issue of the applicability of the results to areas with different socioeconomic and cultural conditions and probably excluded much of the population within urban areas of the study site.\n\n【34】These are relatively minor potential problems. HPS cases are rare in urban areas because the primary reservoir species in North America rarely occur within urban settings. Moreover, surveys of rural housing show that infestations by Peromyscus are nearly ubiquitous in the absence of focused rodent exclusion methods .\n\n【35】Absence of a significant difference in spring precipitation at case sites during 1992-93 and the previous 6 years  may reflect either the absence of an effect (contradicting the trophic cascade hypothesis) or practical problems with estimating precipitation and incorporating conditions associated with past HPS outbreaks. Although nearly 200 weather stations were used in estimating spring precipitation at case and control sites, this still represents a relatively sparse network of sampling locations; therefore, localized precipitation could have been at too fine a spatial scale to detect. However, when we estimated precipitation at weather stations by using the surrounding stations and comparing the results with the observed precipitation, we found no difference between observed and predicted results in 1992-93. A more likely possibility is that the relatively short time series of precipitation data used makes demonstrating a statistical effect difficult. Additionally, if ENSO is a triggering event, outbreaks of HPS must have occurred in the past. Therefore, previous ENSO events may \"contaminate\" comparisons with past precipitation data because they include unrecognized HPS outbreaks.\n\n【36】The trophic cascade hypothesis predicts that ENSO leads to increased precipitation that affects vegetation growth, subsequently influencing HPS risk. The association between HPS risk and vegetation growth, as measured by the normalized difference vegetation index, was inconsistent. Areas with a high index  did correspond to areas at highest risk for HPS  in 1992. Similarly, areas at low risk were generally those with low normalized difference vegetation indexes. However, broad regions of moderate- to high-risk areas did not relate to the vegetation index, and the logistic regression model did not perform well .\n\n【37】The failure of the normalized difference vegetation index to predict HPS risk may indicate that the ecologic connections hypothesized by the trophic cascade hypothesis are complex and modulated by intervening ecologic variables. Alternatively, the normalized difference vegetation index, which is the normalized difference of DN in red and near-infrared portion of the EM spectrum, may have difficulty accurately characterizing vegetation growth in semi-arid areas that contain a complex mixture of vegetation and bare ground  . Further, detailed studies incorporating \"ground truthing\" to establish the relationship between local ecological dynamics of plant and rodent populations and satellite sensor readings will be needed to determine which of these alternatives may apply .\n\n【38】Field validation of interpretations, which is critical to testing hypotheses derived from satellite data, should also be applied to the epidemiologic analyses of HPS risk. Our approach associates three TM bands and elevation with human risk. Interpretation of what these bands detect in the environment varies (soil moisture, soil type, and vegetation structure)  . Our classification is being used to identify other sites with similar reflectance patterns in the same bands and characterize the structure and dynamics of rodent reservoir populations. Preliminary analyses show a good relationship between HPS risk predicted from satellite imagery and P. maniculatus population abundance (r = 0.92).\n\n【39】The case-control model using high-resolution spatial data from satellite imagery supports previous epidemiologic observations indicating that changes in rodent population densities and HPS risk could occur dramatically over relatively short distances  . Although extensive areas of high and low risk were evident , substantial interdigitation of these zones at higher resolution created a mosaic of high- and low-risk areas. This suggests possibly widespread \"environmental islands\" of suitable reservoir habitat imbedded within less suitable habitat and may account for the apparently focal nature of HPS outbreaks and the near random distribution of cases relative to their nearest neighbors observed in this study . The results also support epidemiologic investigations indicating that the only measurable risk factor around HPS sites during the 1993 epidemic was the abundance of P. maniculatus  .\n\n【40】Satellite imagery analysis provides an efficient survey of large geographic regions for environmental indicators of disease risk affecting human populations and has the potential to make surveillance of disease risk for rare zoonotic and vectorborne diseases practical for public health applications . For many diseases, the basis for the supposition that remotely sensed data will be useful for anticipating disease risk is that pathogen transmission is facilitated by arthropods, whose survival and reproduction are influenced by variations in temperature and humidity. The effect of climatic variability, however, on directly transmissible zoonotic agents maintained in vertebrate, especially mammalian reservoirs, is less certain and has received little attention.\n\n【41】Additionally, although the reason to assume a relationship between climate variability and infectious disease outbreaks is clear  , few studies have evaluated whether this presumed relationship actually exists. This study indicates that if these relationships do occur, they are modulated by a number of poorly understood ecologic and social conditions that will require substantial detailed studies of the pathways influencing disease risk.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1e73149d-c4df-4a69-b66e-f7effaf4594c", "title": "Recombinant Sapovirus Gastroenteritis, Japan", "text": "【0】Recombinant Sapovirus Gastroenteritis, Japan\n**To the Editor:** Sapovirus and norovirus are causative agents of gastroenteritis in children and adults. Norovirus is the most important cause of outbreaks of gastroenteritis, whereas only a few outbreaks of sapovirus have been reported . On the basis of complete capsid gene sequences, sapovirus can be divided into 5 genogroups, among which GI, GII, GIV, and GV infect humans, whereas sapovirus GIII infects porcine species.\n\n【1】We report 2 outbreaks of gastroenteritis in Hokkaido, Japan. The first outbreak (A) occurred at a college from May 29 to June 2, 2000. A total of 12 persons (11 students and 1 teacher) reported symptoms of gastroenteritis (nausea, vomiting, stomachache, diarrhea, and fever); 11 stool specimens were collected from days 1 to 7 after onset of illness . These specimens were negative for norovirus (data not shown), but 5 were positive for sapoviruslike viruses by electron microscopy .\n\n【2】The 11 specimens were then examined for sapovirus by using nested reverse transcription–PCR (RT-PCR) as described . A total of 9 (82%) of 11 specimens were positive for sapovirus. Sequence analysis showed that these 9 viruses had 100% nucleotide identity and likely represented the same sapovirus strain . To determine the number of cDNA copies per gram of stool, we performed real-time RT-PCR as described . The number of sapovirus cDNA copies ranged from 5.36 × 10 5  to 7.47 × 10 9  /g stool (median 5.49 × 10 9  copies/g stool) .\n\n【3】The second outbreak (B) occurred at a kindergarten from February 1 to 22, 2005. A total of 23 persons (15 children and 8 adults) reported symptoms of gastroenteritis (nausea, vomiting, stomachache, diarrhea, and fever); 7 stool specimens were collected . These specimens were negative for norovirus (data not shown), but all were positive for sapovirus by nested RT-PCR. The 7 sequences from this outbreak had 100% nucleotide identity and likely represented the same sapovirus strain . The number of sapovirus cDNA copies ranged from 1.14 × 10 9  to 5.41 × 10 10  /g stool (median 2.50 × 10 10  copies/g stool) .\n\n【4】One positive sapovirus specimen from each outbreak was subjected to further sequence analysis in which a single overlapping PCR fragment covering the partial polymerase gene and capsid gene was amplified. The Yak2 and Nay1 sequences shared ≈71% nucleotide identity for this fragment and likely represented different sapovirus strains. The Yak2 sequence closely matched sapovirus GIV Ehime1107 and SW278 sequences (GenBank accession nos. DQ058829 and AY237420, respectively) and had 98% and 97% nucleotide identity for the entire fragment, respectively . The Nay1 sequence closely matched the sapovirus GII C12 sequence (AY603425) and had 91% nucleotide identity for the entire fragment.\n\n【5】The Nay1 sequence closely matched the C12 sequence, which was detected in Osaka, Japan, in 2001 , whereas the Yak2 sequence closely matched the Ehime1107 sequence, which was detected in Matsuyama, Japan, in 2002  and the SW278 sequence, which was detected in Sweden in 2003 . We recently described the C12 strain as intragenogroup recombinant sapovirus strain , whereas the Ehime1107 and SW278 strains were described as intergenogroup recombinant sapovirus strains . Our results indicate that recombination sites in intragenogroup and intergenogroup recombinant sapovirus strains were at the polymerase and capsid junction . Sapovirus Sydney53 (DQ104360) and Sydney3 strains (DQ104357), which were detected in Australia from August 2001 to August 2004 , closely matched C12 and Ehime1107/SW278 sequences, respectively. These results showed that recombinant sapovirus strains are stable in the environment and may be globally distributed. Our findings also suggest a changing distribution of sapovirus-associated gastroenteritis in Hokkaido because different sapovirus GI strains were predominant sapovirus strains that caused causing outbreaks of gastroenteritis in Hokkaido .\n\n【6】In a recent study, the number of norovirus cDNA copies per gram of stool specimen was analyzed and a discrepancy was found between the different norovirus genogroups . Chan et al. found that noroviruses GI and GII showed medians of 8.4 × 10 5  and 3.0 × 10 8  copies/g of stool specimen, respectively, and speculated that increased viral loads were caused by higher transmissibility of norovirus GII strains . Our results showed that sapovirus GII Nay1 and GIV Yak2 strains showed higher viral loads than norovirus GII strains. These results suggest that a high degree of shedding of sapovirus GII Nay1 and GIV Yak2 strains may have caused the outbreak of gastroenteritis. However, to elucidate this suggestion, further studies are needed with other sapovirus strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ee1fb81f-0d5d-4c7c-b57f-09892bf8b9cd", "title": "Yellow Fever Vaccine–Associated Viscerotropic Disease among Siblings, São Paulo State, Brazil", "text": "【0】Yellow Fever Vaccine–Associated Viscerotropic Disease among Siblings, São Paulo State, Brazil\nBrazil recently experienced its largest-recorded yellow fever (YF) outbreak in decades; >2,000 human cases and ≈750 deaths occurred during July 2016–June 2018 . The virus reached the metropolitan areas, where the YF vaccine had not been previously recommended. This situation led to the need for vaccination campaigns in affected areas, which included fractionated doses of the 17DD YF vaccine. In 2017, >5 million persons were vaccinated with the standard dose in the state of São Paulo, Brazil. During January 25–July 3, 2018, the state also used a fractionated-dose vaccine as part of a dose-sparing strategy in 54 municipalities. The fractionated dose consisted of 0.1 mL (one fifth of the standard dose) administered in the subcutaneous tissue to person >2 years of age. This approach was necessary because of insufficient vaccine stock for the entire population at the height of the epidemic. In 2018, a total of 10 million persons were vaccinated; of those, 5.3 million received the standard dose and 4.7 million received the fractionated dose.\n\n【1】During the 2016–2018 YF epidemic, southeastern Brazil was the most affected area; São Paulo state had 559 confirmed cases of YF and 214 deaths attributed to YF . As a consequence, the YF vaccination coverage rate, which was 5% of the population in 2016, increased to 65% by the end of 2018 in the metropolitan area of the city of São Paulo.\n\n【2】In late 2017, the Health Surveillance Department of the State of São Paulo (Centro de Vigilância Epidemiológica “Prof. Alexandre Vranjac”) received a report of a man in the metropolitan area of São Paulo city who died after YF vaccination. The patient’s brother had died soon after YF vaccination 10 years previously, but cause of death was undetermined. Two months later, the surveillance team received a report from Natividade da Serra in São Paulo state of 3 temporary adverse events associated with YF vaccination among siblings of 1 family that ended in 2 deaths.\n\n【3】The data resulted in a suspicion of YF vaccine–associated viscerotropic disease (YEL-AVD). They were not the first suspected cases of YEL-AVD during the 2017–2018 vaccination campaign. However, the report of 2 familial clusters of severe adverse events following immunization (AEFI) related to YF vaccine in the state of São Paulo triggered a field epidemiologic investigation. In this descriptive study, we describe the epidemiologic investigation of the 2 clusters of YEL-AVD among family members.\n\n【4】### Methods\n\n【5】We used the AEFI surveillance system of São Paulo state, Brazil, to capture information from January 2017–June 2018. The cases reported are level 1 according to Brighton Collaboration Viscerotropic Disease Working Group case definition in terms of the diagnostic certainty (definite or probable YF vaccine–associated causality) . All case-patients were first-degree relatives (siblings) with YEL-AVD. We also included 1 previous case-patient who had onset of symptoms before 2017 but was related to a current case-patient.\n\n【6】We obtained information regarding the vaccine and clinical and laboratory findings from the official AEFI passive surveillance system database. A field investigation team reviewed medical records. The team also visited the patients’ families to collect clinical and epidemiologic details, in addition to the vaccination status of first-degree relatives, and searched for other suspected cases of AEFI. Last, the team traced the biologic samples for additional and specific laboratory investigations.\n\n【7】We analyzed the available serum and postmortem samples at the Instituto Adolfo Lutz, the central public health laboratory for YF and other notifiable diseases. We extracted the viral RNA from 140 µL of serum by using a QIAamp Viral RNA Minikit  according to the manufacturer’s instructions. We tested the viral genome by using the real-time reverse transcription PCR (RT-PCR) protocol as described by Domingo et al. Positive samples were submitted to a second RT-PCR reaction, according to Bae et al. This reaction can detect 17DD viral genome but cannot amplify viral genome from wild-type strains circulating in the Americas. We considered samples that tested positive by both RT-PCR protocols positive for vaccine-associated virus. We isolated the YF virus from serum samples using cultured cells of _Aedes albopictus_ , clone C6/36, and performed viral identification using indirect immunofluorescence assay with polyclonal YF antibodies and conjugated antimouse immunoglobulins. We used the US Centers for Disease Control and Prevention MAC-ELISA protocol for detecting IgM . The postmortem samples were analyzed by 2 expert pathologists in infectious diseases and YF pathology (S.D’A.I. and A.N.D.N.). We performed immunohistochemistry reactions to detect in situ YF virus antigens using a polyclonal wild YF virus strain primary antibody . This study was approved by the Research Ethics Committee of the Health Department of the Municipality of São Paulo .\n\n【8】### Results\n\n【9】##### Cluster\n\n【10】Case 1 was that of a 40-year-old white man who received the first dose of 17DD YF vaccine (lot 174VFC056Z) on December 16, 2017 . No simultaneous vaccines were administered. On day 3 after vaccination, headache, malaise, and fever developed, and he took analgesics. After a day of clinical improvement, his symptoms returned, along with a high fever, nausea, and vomiting, for the next 3 days. On day 7 after vaccination, he was admitted to the emergency department for diffuse abdominal pain and hypotension. Initial laboratory tests revealed only thrombocytopenia (platelets 51,000/mm 3  \\[reference range 150,000–450,000/mm 3  \\]). On day 8 after vaccination, he had fever, jaundice, conjunctival hyperemia, peripheral edema, hypotension, dyspnea, tachycardia, and oliguria. The patient was transferred to a tertiary-care center in critical condition experiencing respiratory distress, shock, and metabolic acidosis (arterial blood gas pH 6.8, bicarbonate 9 mmol/L, carbon dioxide partial pressure 58 mm Hg, and partial pressure of oxygen 50 mm Hg). He was started on mechanical ventilation and was administered antibiotics (piperacillin and tazobactam), fluids, and vasoactive drugs. Chest radiography revealed bilateral pulmonary congestion in the middle-upper lobes. Laboratory tests revealed increased total leukocyte count with left shift (total leukocyte count 34,700 cells/mm 3  ; promyelocytes 2%, myelocytes 4%, metamyelocytes 8%, rods 34%, and neutrophils 42%), as well as increased serum levels of creatinine, urea, creatine phosphokinase, liver enzymes, and bilirubin. His platelets dropped to 38,000/mm 3  , and the international normalized ratio (INR) increased . The serum level of C-reactive protein increased (20.66 mg/dL). The patient died on December 25, 2017, on day 9 after vaccination , day 6 after illness onset.\n\n【11】Autopsy results  revealed microvesicular steatosis with discrete and mixed portal and sinusoidal inflammatory infiltrates in the liver; acute tubular necrosis and interstitial nephritis in the kidneys; edema and interstitial mononuclear infiltrates in the myocardium; pulmonary hemorrhages; mild cerebral edema; and hypoplasia of the white pulp of the spleen. Immunohistochemistry detected YF antigens in the Kupffer cells, rare hepatocytes, and in inflammatory cells in the liver, kidneys, heart, and spleen. The same 17DD strain was identified by RT-PCR in a serum sample collected on December 25 (day 9 after vaccination), and the virus was isolated after inoculation in C6/36 cells.\n\n【12】Case 2 was that of a 35-year-old man who received the first dose of 17DD YF vaccine (lot information unavailable) on January 30, 2008 , because he wished to travel to YF-endemic areas of Brazil. On February 3, 2008 (day 4 after vaccination), he began experiencing fever (38.5°C) and myalgia. The symptoms progressively worsened; 3 days later, he was hospitalized with vomiting, diarrhea, petechiae, and jaundice. He began experiencing oliguria, mental confusion, and generalized clonic-tonic convulsions on day 7 after vaccination and was transferred to a tertiary-care center. Laboratory tests revealed high INR (1.37), increased serum direct and indirect bilirubin levels (8.49 and 7.22 mg/dL), elevated liver enzymes (aspartate aminotransferase \\[AST\\] 492 U/L and alanine aminotransferase \\[ALT\\] 189 U/L), thrombocytopenia (platelets 52,000/mm 3  ), leukocytosis (24,300 cells/mm 3  ) with an increase in neutrophil proportion (6% rods and 81% segmented cells), and acute renal failure (creatinine 4.8 mg/dL, urea 131 mg/dL). The patient was started on mechanical ventilation and administered fluids, platelet transfusions, and ceftriaxone for possible acute meningitis. However, the patient experienced progressive multiple organ dysfunction with worsening liver and renal functions and coagulopathy . The patient died on day 11 after vaccination , day 9 after illness onset.\n\n【13】After the death of the patient in case 1, the surveillance team traced and recovered a serum sample from case 2, which was collected on February 9, 2008 (day 10 after vaccination) and stored at −20°C in the Instituto Adolfo Lutz for 10 years. This sample was sent to Biomanguinhos/Fiocruz (Rio de Janeiro, Brazil) for further analysis, which revealed detectable RNA of 17DD YF with a low viral load. Patients in both cases had 2 siblings and parents who were vaccinated during the YF vaccine campaigns in the state of São Paulo. No AEFI was reported among their relatives.\n\n【14】##### Cluster\n\n【15】Case 3 was that of a 28-year-old white woman who received the first dose of 17DD YF vaccine (fractionated dose, lot 175VFC064Z) on January 25, 2018 . On day 4 after vaccination, she began experiencing abdominal pain, fever (38°C), vomiting, dysuria, and generalized myalgia. She was prescribed analgesics. On day 11 after vaccination, she returned for medical assistance because her condition had worsened and included tachycardia and tachypnea. Laboratory tests revealed leukocyte count of 19,370 cells/mm 3  with 77% neutrophils and 2% rods; hemoglobin 14 g/dL and hematocrit 40.4%; thrombocytopenia (platelets 29,000/mm 3  ); acute renal failure (creatinine 2.92 U/dL, urea 153 U/dL); and elevated serum liver enzymes (AST 464 U/L and ALT 137 U/L) and creatine phosphokinase (CPK) (99 U/L). Urinalysis revealed proteinuria, leukocyturia, and hematuria. The first differential diagnosis was sepsis caused by urinary tract infection, and the patient was prescribed ceftriaxone. She was transferred to a tertiary-care center for respiratory distress, hypoglycemia, oliguria, metabolic acidosis (pH 7.07, bicarbonate 10.3 mmol/L), and progressive shock. In the intensive care unit, she was started on mechanical ventilation and cardiovascular support, but her condition deteriorated. The leukocyte count increased to 31,500 cells/mm 3  with 88% neutrophils, 5% rods, and 5% lymphocytes, as well as a serum C-reactive protein level of 90.4 mg/dL. The levels of platelets, INR, CPK, and liver function parameters worsened . The patient experienced refractory acidosis and died the same day .\n\n【16】Case 4 was that of a 32-year-old white man who was the brother of the patient in case 3. He received the first dose of 17DD YF vaccine (fractionated dose, lot 173VFA013Z) on February 2, 2018 . On day 1 after vaccination, he began experiencing malaise, myalgia, chest pain, epigastric pain, and fever (39.3°C). He sought medical care at the local ED and was prescribed analgesics. On February 9, 2018 (day 7 after vaccination) he again sought medical care; examination revealed tachycardia (130 bpm), thrombocytopenia (platelets 80,000/mm 3  ), proteinuria, leukocyturia, and hematuria. He was prescribed fluid therapy, antipyretics, and ciprofloxacin and was discharged. On the 8th day of illness (February 10, 2018), he returned to the ED with jaundice, dyspnea, hypotension, and a capillary blood glucose level of 63 mg/dL. At that point, laboratory analysis revealed renal insufficiency (creatinine 3.1 U/dL, urea 82 U/dL); increased liver enzyme levels (AST 214 U/L, ALT 95 U/L), increased serum (6.7 mg/dL) and direct (3.12 mg/dL) bilirubin; and increased INR 1.88. CPK level was unremarkable (194 U/L). The patient was treated with fluids and intravenous glucose and was transferred to a tertiary-care center. In the intensive care unit, the patient experienced hematuria, oliguria, agitation, mental confusion, metabolic acidosis (pH 7.08, bicarbonate 15.2 mmol/L), shock, and hypoxemia. He was prescribed mechanical ventilation, ceftriaxone for possible urinary sepsis, intravenous glucose, fresh frozen plasma, cryoprecipitate, hydrocortisone, vasoactive drugs, and hemodialysis. His medical condition continued to deteriorate despite supportive therapy, and multiple organ failure developed . The patient died on day 10 after vaccination , day 9 after illness onset. A postmortem liver sample was collected, and histopathological analysis  revealed midzonal microvesicular steatosis with scattered acidophilic degeneration of hepatocytes (Councilman bodies) and discrete periportal inflammatory reaction, congestion, and cholestasis. Immunocytochemistry detected YF virus antigens in the Kupffer cells and mesenchymal cells. RT-PCR detected RNA of 17DD YF in the hepatic tissue.\n\n【17】Case 5 was that of a 29-year-old white man who was a brother of the patients in cases 3 and 4. He received the first dose of 17DD YF vaccine (fractionated dose, lot 173VFA013Z) on January 29, 2018 . On day 4 after vaccination, he began experiencing nausea, vomiting, generalized myalgia, fever, and headache. On February 7, he sought medical care; the first differential diagnosis was urinary tract infection. Initial laboratory tests revealed leukocyte count of 7,000 cells/mm 3  with 80% neutrophils and 11% lymphocytes, hemoglobin 14.8 g/dL, hematocrit 44%, and platelets 92,000/mm 3  . Urinalysis revealed hematuria and leukocyturia. The patient was discharged with antipyretics and oral ciprofloxacin. The next day, he began experiencing diffuse abdominal pain and returned to an ED. After normal abdominal ultrasonography, he was administered ﬂuid therapy and antiemetics and discharged. On February 9, 2008, he was admitted to the hospital with the same symptoms, as well as mental confusion and fever. His leukocyte count increased to 22,700/mm 3  and the platelet count dropped to 75,000/mm 3  , and other test results showed mild acute renal injury (creatinine 1.4 U/dL) and hepatitis (AST 137 U/L, ALT 121 U/L, and INR 2.04). The patient responded well to fluid therapy and antibiotics, and the laboratory parameters returned to normal levels by day 16 after vaccination .\n\n【18】The patients in cases 3–5 were 3 of 11 siblings. Their parents died years before the events reported in this study and their YF vaccination status was unknown. Of the 11 siblings, 9 were vaccinated for YF for the first time during the 2017–2018 campaign. No other AEFI were reported.\n\n【19】The patients in these cases had no recent history of travel, acute medical conditions, or other vaccinations within the previous month. The patient in case 3 was obese, but none of the other patients had relevant medical conditions. In cases 1 and 4, YF IgM was detected using MAC-ELISA. Serum samples from all patients had detectable RNA of 17DD YF strain; samples were collected on the day of death, except in case 5, in which the sample was collected day 14 after vaccination. During hospitalization, all patients had negative results on serologic tests for dengue, leptospirosis, hepatitis B core protein, hepatitis B surface antigen, hepatitis A virus, and HIV, as well as blood and urine cultures.\n\n【20】### Discussion\n\n【21】We describe 2 clusters of siblings with YEL-AVD. The clinical characteristics of all the cases match those of YEL-AVD previously described . The symptoms began within the first week after vaccination (range 1–4 days); the initial symptoms were nonspecific (fever, headache, myalgia, nausea, and vomiting) and evolved alongside laboratory abnormalities (thrombocytopenia and elevation of serum total bilirubin, hepatic transaminases, and creatinine levels). The patients who died experienced hypotension and hemorrhage, as well as acute renal and respiratory failure and coagulopathy.\n\n【22】In each cluster, at least 1 case was confirmed . In case 1, YF was detected in serum >7 days after vaccination with a typical histopathological pattern and in the liver by immunohistochemistry. Case 4 (cluster 2) included a typical histopathological pattern and positive RT-PCR result for YF in the liver. The other cases were characterized as suspected cases because of their clinical characteristics and lack of available histopathologic details.\n\n【23】Considering the Brighton Collaboration criteria for case definition , 4 patients (cases 1–4) met \\> 3 major criteria: hepatic abnormalities (total bilirubin \\> 1.5 times the upper limit of normal \\[ULN\\] or ALT or AST \\> 3 times ULN), renal abnormalities (creatinine \\> 1.5 times ULN), musculoskeletal abnormalities (CPK \\> 5 times ULN), respiratory abnormalities (oxygen saturation < 88% on room air or need for mechanical ventilation), platelet count <100,000/μL, and coagulopathy (INR \\> 1.5). Those patients were classified as level 1 diagnostic certainty according to the case definition of viscerotropic disease . Case 5 satisfied 2 major criteria (platelet count 75,000/μL and INR 2) with level 2 diagnostic certainty for viscerotropic disease . However, case 5 had YF 17DD viral RNA in blood samples 14 days after vaccination, which qualifies for diagnosis of a viscerotropic disease and definite yellow fever vaccine–associated causality, according to the Brighton Collaboration .\n\n【24】Case 2 was identified retrospectively. His clinical manifestations were typical and similar to those of his sibling (case 1). Despite the positive RT-PCR test for YFVV, the viral load was low, and viral isolation was not possible. However, this result does not invalidate the discovery. The serum sample was stored for >10 years, which could have compromised the test to identify the virus.\n\n【25】Ours is not the first report of a viscerotropic disease among siblings in Brazil. A 19-year-old woman experienced YEL-AVD and died; her 12-year-old sister also experienced a severe adverse event after 17DD YF vaccination, which suggested YEL-AVD, but she recovered . Two siblings 30 and 34 years of age with diagnoses of adrenal insufficiency (Addison’s disease) who were receiving physiologic doses of cortisone died of probable YEL-AVD .\n\n【26】Among the AEFI cases during the 2017–2018 YF vaccination campaign, cases 3 and 5 (cluster 2) received vaccines from the same lot. Case 4 (cluster 2) and case 1 (cluster 1) received vaccines from different lots. The same lots were distributed to >50 cities and were administered to hundreds of thousands of other persons without any reported serious adverse events (data not shown). No changes in the manufacturing methods of the 17DD vaccine in Brazil could account for the adverse events. Genetic mutations in the YF virus do not seem to be the cause of the adverse reactions because the vaccine virus in the previously confirmed YEL-AVD cases revealed no substantial mutation from the original lot . Furthermore, the YF virus isolates recovered from fatal cases of YEL-AVD revealed no reversion to virulence in animal models . The occurrence of adverse events might have been related to individual, genetically determined host factors, which is strengthened by the incidence of cases among siblings.\n\n【27】The cases we report strengthen the hypothesis of host genetic predisposition to YEL-AVD. Impaired immunologic profiles have suggested that a robust adaptive immune response with abnormalities in the innate immune system might be involved in the development of YEL-AEFI . The genomic signatures correlated with the immune response to 17DD YF, revealing molecular events observed in the innate immunity against viruses . Molecules involved in the innate sensing of viruses, such as cytoplasmic receptors of 2,5′-OAS (oligoadenylate synthetase) family members 1, 2, 3 and L, TLR7 , MDA-5 (melanoma differentiation-associated), and RIG-I (retinoic acid-inducible I), as well as transcription factors that regulate type I interferons (IRF7 \\[interferon regulatory factor 7\\] and STAT1 \\[signal transducer and activator of transcription 1\\]), are induced . Genetic variations could result in an impairment of the innate immune response involved in the direct control of viral replication or in mediating viral clearance. The reduced expression of CCR5 (C-C chemokine receptor type 5) because of polymorphism might impair the responsiveness of cells to ligands such as RANTES (Regulated upon Activation, Normal T Cell Expressed and Presumably Secreted) . Such a breakdown could impair the migration of monocytes to tissue sites of infection with a milder innate response that fails to limit early viral replication . An investigated fatal case of YEL-AVD revealed genetic variations in the OAS genes that encode essential proteins involved in the innate immune response to viral infections .\n\n【28】The 17DD YF virus promotes the induction of critical type I interferon (IFN) mediators, such as STAT1 and IRF7 in humans, which lead to the expression of IFN-stimulated genes with antiviral properties, thus effectively mounting an antiviral response in infected and surrounding cells . Hernandez et al. revealed IFN-αβ receptor 1 deficiency in a 14-year-old girl in whom YEL-AVD was diagnosed without any known risk factors for the disease. In this case, a single-gene inborn error of innate immunity, specifically that of type I IFN cell-intrinsic immunity, was related to the severe adverse reactions to the YF vaccine . In another study, a case-patient with YEL-AVD had a complete recessive IFN-αβ receptor 2 deficiency . The patient in case 5 in that study, the only one who survived, underwent exome sequence analysis with other ongoing studies; however, no inborn errors of type I IFN or autoantibodies against IFN I were identified .\n\n【29】In this study, the siblings in cluster 1 received standard doses of 17DD YF vaccine, whereas the siblings in cluster 2 received fractionated doses. Clinical trials have identified similar viremia and immunogenic responses between the doses. No serious adverse events were identified in these studies, although the sample size was too small for a safety evaluation .\n\n【30】Our findings strongly suggest that genetic predispositions in innate immune responses are related to the occurrence of YEL-AVD disease. Future investigations into genetic predispositions to YEL-AVD are warranted. Obtaining a thorough medical history, particularly regarding severe reactions to the vaccine in family members, is advisable before administering the 17DD YF vaccine.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eef52041-4ae2-4aea-b2f4-58033a88373a", "title": "Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa 1850 A.D.)", "text": "【0】Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa 1850 A.D.)\nDr. Ryusai Kuwata (Bunka 8/1811-Keio 4/1868) . Japanese color woodcut print advertising the effectiveness of cowpox vaccine (circa Kaei 3/1850 A.D.). Reproduced with permission from the Nihon University Medical Library, Iidamachi, Tokyo.\n\n【1】In 1849, 18 years before the Meiji Revolution, cowpox vaccination was first successful in Nagasaki, which was the only city open to foreign visitors during the Tokunaga Era. This method became widespread throughout Japan. Dr. Ryusai Kuwata (Bunka 8/1811-Keio 4/1868) from Edo (modern Tokyo) made this color woodcut print to advertise the effectiveness of the vaccination to protect against smallpox; he used this picture at the Osaka Vaccination Clinic. The white cow in the print represents vaccination.\n\n【2】It was known that among persons and families who tended cows, the clinical signs of smallpox were never serious. Following this clue, Dr. Kuwata tried vaccinating children with the contents of eruptions from cowpox. As the result, the children's illness was mild and not transmitted to others. Dr. Kuwata, who was the pioneer of vaccination in Japan, vaccinated more than 70,000 people. He died with a vaccination needle in his hand in 1868, when he was 58 years old.\n\n【3】\\* \">\n\n【4】### Front cover and text translation \\*\n\n【5】##### Use of Vaccine Has Spread from Osaka to Edo (Tokyo) for the Benefit of Mankind\n\n【6】I wonder who in the world named Smallpox a God? It is no more than a demon that deviated from the path of mankind.\n\n【7】There are approximately five ways of treating smallpox; one is called the vaccinia method. In the 1790s, a Dutchman \\[sic\\] named Edward Jenner announced this method. It is said that when this method was first used, a vaccine made from cow's blood was directly injected into patients. It was quite astonishing that this vaccine, which was made from blood drawn from cow udders and was used on children, cured smallpox.\n\n【8】The first form of this vaccine was the Dutch form, but the Chinese form was used after 1806. The Chinese form is currently being used in Japan, but many forms and methods are used throughout the world. In this manner, vaccination has become a method of treating smallpox.\n\n【9】We began using the Dutch form of the vaccine in the latter half of the 1800s. Thanks to the patronage of our loyal customers, the use of our vaccine has spread widely.\n\n【10】\\[Translator's note: At the end of this advertisement, there is a poem, the gist of which is, \"Parents should do whatever they can to ensure the well-being of their children.\"\\]\n\n【11】\\* \">\n\n【12】### Back cover and text translation \\*\n\n【13】Translation of the image that appears on back cover.\n\n【14】##### What You Should Know about Vaccinations\n\n【15】*   Babies should be vaccinated on the 70th day after birth, regardless of the season. However, babies may be vaccinated sooner if an outbreak of smallpox has already occurred nearby.\n\n【16】*   After receiving the vaccine, patients should not become infected with smallpox, but patients who come into contact with the smallpox virus before being vaccinated should provide a vaccination document that states \"smallpox\" and \"vaccine\" to the physician within 10 days after the vaccination.\n\n【17】*   If patients become infected with another disease during the vaccination period, they should, at their own responsibility, request to be examined by other physicians and receive medicine for the disease.\n\n【18】*   During the vaccination period, a patient may bathe every day in a bath of warm water up to the hips.\n\n【19】*   Patients must return to the original medical institution and undergo another examination 8 to 10 days after receiving the vaccination. If a patient fails to receive the second examination, the \"vaccination complete\" certificate will not be issued.\n\n【20】*   A person who has received the \"vaccination complete\" certificate must be vaccinated again 6 years later and must receive a third vaccination 6 years after that. In either case, the patient must have a physician confirm that the vaccination was successful; provided, however, that if an outbreak of smallpox occurs nearby during that period, another vaccination may be administered before the end of the 6-year period. \\*\n\n【21】    Cover text translation by Bonnie Mikami and Masahito Mikami", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9f4ebd8f-143f-4124-b76a-5ac3cc1d66b2", "title": "Geographic Distribution and Incidence of Melioidosis, Panama", "text": "【0】Geographic Distribution and Incidence of Melioidosis, Panama\n_Burkholderia pseudomallei_ , a gram-negative bacillus found in the environment of some tropical and subtropical regions, is the etiologic agent of melioidosis . Most melioidosis cases in the world are reported from Southeast Asia and northern Australia; only sporadic cases are reported from other regions . In the Americas, <100 acquired cases were identified from 1947 through June 2015. Only 3 cases were reported from Panama, 1 each in 1947, 1948, and 2011. However, cases were reported in Antioquia, Colombia , near the border with Panama. Melioidosis might be misdiagnosed and underreported because of the lack of diagnostic resources in the rural areas where cases are most likely to occur .\n\n【1】People become infected with _B. pseudomallei_ through inoculation in compromised derma, inhalation, or ingestion. Some evidence suggests ingestion is associated with bacteremia, even though ingestion is considered an uncommon pathway . Clinical manifestations of melioidosis are diverse and may include localized cutaneous infection, pneumonia, involvement of bones and joints, intraabdominal abscesses, sepsis, and even death . Diagnosis is usually made through blood cultures, but the bacterium often is misidentified as _B. thailandensis_ or _B. cepacia_ .\n\n【2】Current treatment for melioidosis includes an induction phase of 2–6 weeks with intravenous ceftazidime (or carbapenem for more severe cases), followed by a 2–6-month eradication phase using oral trimethoprim/sulfamethoxazole (TMP/SMX) or doxycycline. Doxycycline previously has been used for eradication, but recent studies suggest TMP/SMX is more effective .\n\n【3】During the previous 10 years, cases of melioidosis have been identified in different regions of Panama. The aim of this study is to describe the clinical signs and symptoms and geographic distribution of melioidosis in Panama to elucidate the current status of the disease in the Americas.\n\n【4】### The Study\n\n【5】We conducted a retrospective review of medical records from 2007–2017 from 2 national tertiary level hospitals in Panama City. Hospital Santo Tomás and Complejo Hospitalario Metropolitano Dr. Arnulfo Arias Madrid (CHMDrAAM) are the 2 main referral hospitals for Panama and are in the capital city.\n\n【6】We reviewed specimen registries from the microbiology laboratories at each institution; we also identified 1 case from a poster presented at a national scientific meeting. We included patients who had a culture-positive report for _B. pseudomallei_ and a clinical diagnosis of melioidosis at discharge. We excluded 2 patients with culture-positive results for _B. pseudomallei_ because their clinical diagnoses were not related to their test results.\n\n【7】The microbiology laboratories of Hospital Santo Tomás and CHDrAAM identified _B. pseudomallei_ strains from blood culture by using BacT/ALERT 3D Microbial Identification System . Both laboratories also obtained isolates of _B. pseudomallei_ from clinical specimens inoculated in Columbia agar prepared with 5% sheep blood and in MacConkey agar. Both laboratories used the VITEK 2 (bioMérieux) system to identify strains, which were then sent to the national reference laboratory at Instituto Conmemorativo Gorgas de Estudios de la Salud in Panama City, Panama, for microbiology confirmation and antimicrobial susceptibility testing.\n\n【8】We used a standardized form to collect data and then entered data into an Excel  database for descriptive analysis. The Institutional Review Board of Hospital Santo Tomás reviewed and approved this study.\n\n【9】We identified 12 cases that occurred during 2007–2017: 8 in Hospital Santo Tomás and 4 in CHMDrAAM. We obtained medical records for all but 1 case, for which we obtained data from a poster presented at the 37th American College of Physicians Annual Central America Chapter Meeting in Panama City, Panama, in 2015 .\n\n【10】The mean age of cases was 50.3 years (SD ±12 years); most (9/12) patients were male. We noted bacteremia and sepsis in most (8/12) cases, pneumonia in 6 cases, and intraabdominal abscesses in 4 cases. Other signs and symptoms included endocarditis, meningitis, osteomyelitis, and septic arthritis . Diabetes mellitus was the predominant risk factor. Most patients came from rural areas or suburbs of Panama City , and none reported travel outside of Panama.\n\n【11】All cases occurred during the rainy season, which is May–November in Panama. Five patients (41.7%) died while hospitalized; these patients had the most severe clinical manifestations of the disease, bacteremia, pneumonia, and septic shock, similar to cases reported from Central America .\n\n【12】Rapid microbiologic identification of _B. pseudomallei_ is necessary to initiate appropriate, life-saving treatments. However, laboratory results can take >48 hours, delaying appropriate antimicrobial drug therapy. Of the 7 patients in this study who survived, records showed they were treated with TMP/SMX or doxycycline, but the length of antimicrobial drug treatments were not noted in the records.\n\n【13】### Conclusions\n\n【14】The increase in reports of melioidosis in the Americas requires greater awareness of this disease among clinicians, especially those caring for patients with diabetes. Melioidosis often is misdiagnosed as pulmonary tuberculosis and scrofula ; we found 2 misidentified clinical cases in our study. More studies are needed to identify specific high-risk areas and transmission routes in the Americas. Such insights can inform earlier clinical suspicion and guide the formulation of prevention strategies.\n\n【15】Because the clinical signs and symptoms of melioidosis are nonspecific, microbiologic identification is crucial to diagnosis. Thus, improved laboratory capacity is critical to improve patient outcomes in affected areas to aid epidemiologic and antibiotic susceptibility surveillance efforts. Collaboration among countries in the region could drive efforts to describe the origins of this disease and the actual prevalence in the Americas.\n\n【16】Our study has limitations because we collected data retrospectively and only included the most severe cases in Panama. Melioidosis occurs more frequently in rural areas, and cases might not be identified because of the lack of laboratory or diagnostic tools. We provide a perspective on the processes that hinder our knowledge of this disease in Panama, such as lack of surveillance data and inadequate laboratory capacity. Our data justify the need for increased surveillance for melioidosis and reinforce the need for complete epidemiologic data and adequate strain storage for further genetic analysis. Epidemiologic studies of seroprevalence, environmental sampling, and increased access to PCR techniques and broth microdilution testing are needed to determine whether _B. pseudomallei_ is endemic to Panama and to improve treatment outcomes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8ac8e73b-7bc6-4bf8-9e0f-75982058ca5c", "title": "Dogs and Opossums Positive for Vaccinia Virus during Outbreak Affecting Cattle and Humans, São Paulo State, Brazil", "text": "【0】Dogs and Opossums Positive for Vaccinia Virus during Outbreak Affecting Cattle and Humans, São Paulo State, Brazil\nSince the first vaccinia virus (VACV) outbreak in Brazil in 1999, researchers have speculated on the origins and possible reservoirs of VACV . Wild and peridomestic rodents are known to be reservoirs of cowpox in Europe , but in Brazil, their involvement as VACV reservoirs is unclear. Although studies have reported experimental transmission of VACV between rodents and cows , this finding was not confirmed during outbreaks in Brazil.\n\n【1】The isolation and characterization of a VACV isolate in a peridomestic rodent has been described on a farm in Minas Gerais State, which raised questions about the role of rodents in VACV maintenance in Brazil . However, a recent serologic study on VACV reservoirs suggested that wild rodents might have a secondary role in VACV maintenance in São Paulo State .\n\n【2】Despite the absence of reports of clinical signs in dogs and other domestic or wild mammals during VACV outbreaks, in this report, we describe 3 dogs ( _Canis lupus familiaris_ ) and 3 opossums ( _Didelphis albiventris_ ) without clinical signs that were obtained during from a VACV outbreak. Blood samples from these animals were positive by PCR for VACV.\n\n【3】### The Study\n\n【4】This study was approved by the Ethical Committee of Animals Uses in Veterinary Medicine and Animal Production of São Paulo State University. The capture of wild animals was authorized by the Brazilian Institute of Environment and Natural Resources Renewable.\n\n【5】In October 2012, the veterinary service of Itatinga County (23°6′7″S, 48°36′57″W) in São Paulo State, Brazil, reported an outbreak similar to that of bovine smallpox that affected 2 small dairy farms. The Veterinary Medical and Animal Husbandry School of São Paulo State University were contacted, and a team of veterinarians visited the 2 dairy farms to collect samples for diagnosis and relevant information by using an epidemiologic questionnaire.\n\n【6】Both farms (1.5 km apart) used manual milking systems and had similar sanitary management of herds. The animals were vaccinated against foot and mouth disease, brucellosis, and clostridiosis. Ivermectin or albamectin were used for control of endoparasites. Animals were raised in pastures, and there were no changes in management or introduction of new animals on both farms. There were no previous outbreaks on these farms.\n\n【7】Blood and scab samples from udders of cows or nostrils of calves were collected from 31 affected animals. Serum and whole blood were collected from 2 humans who worked as milkers and had lesions on their hands and arms . Blood samples were also collected from 6 dogs, 6 pigs, 2 horses, 2 rams, 3 opossums, 1 coati ( _Nasua nasua_ ), and 2 wild rodents ( _Akodon montensis_ and _Nectomys squamipes_ ). Both domestic and wild mammals were evaluated at the time of collection for characteristic clinical signs of VACV, such as vesicles, scabs, or crusts.\n\n【8】Opossums and the coati were captured by using a trap (Tomahawk Live Trap, Hazelhurst, WI, USA) containing thigh or drumstick chicken as bait. These animals were anesthetized with tiletamine and zolazepam by using the recommended dose for each species . Wild rodents were captured by using a trap (H.B. Sherman Traps, Inc. Tallahassee, FL, USA) containing peanuts, cornmeal, oats, and canned sardines as bait. These animals were anesthetized in an autoclavable bag containing gauze soaked in ethyl ether and then euthanized by using deep anesthesia. Five trap nights were required to capture animals in the native forest areas surrounding both farms.\n\n【9】Virus DNA was extracted from scabs and total blood samples by using the Invisorb Spin Tissue Mini Kit and the Invisorb Spin Blood Mini Kit (Stratec Molecular, Berlin, Germany), respectively. A seminested PCR was conducted for amplification of the _A56R_ gene of VACV , and positive samples were submitted for gene sequencing at the Federal University of Minas Gerais State (Belo Horizonte, Brazil).\n\n【10】Responses to the epidemiologic questionnaire showed that no factors could be correlated with the 2 outbreaks. No domestic or wild mammals had clinical signs at the time of blood sample collection. However, 3 dogs and 3 opossums were positive by PCR for VACV. Gene sequences obtained (GenBank accession nos. KJ741387.1, KJ741388.1, KJ741389.1, KJ741390.1, KJ741391.1, and KJ741392.1) showed that VACV isolated in this study had the same deletion of six amino acids at position 251 that is found in group I VACVs from Brazil, such as Cantagalo virus, Araçatuba virus, Passatempo virus, and Guarani P2 virus  .\n\n【11】### Conclusions\n\n【12】The fact that the dogs and opossums positive by PCR for VACV did not have clinical signs of infection might indicate only a subclinical infection or may be related to the dichotomy of VACVs from Brazil, which is not only genetic but also biologic. Ferreira et al. showed that although there was no difference in severity of lesions in humans and cows affected by group I and II VACVs, BALB/c mice that were intranasally inoculated with group I VACVs did not have clinical signs. In contrast, mice inoculated with group II VACS, such as Belo Horizonte virus and Gurani P1 virus, had acute respiratory illness, followed by death.\n\n【13】More than 1 mammalian species, either wild or domestic, might be acting as a reservoir of group I VACVs from Brazil, possibly acquiring and transmitting the virus without showing clinical signs. This assumption corroborates the findings of Peres et al. which showed a high seroprevalence in dogs without clinical signs, questioning their possible role as a reservoir and suggesting more studies to confirm these findings.\n\n【14】Genetic and epidemiologic analysis showed that VACV circulating in this region are members of group I VACVs in Brazil. These findings might support the absence of clinical signs and raise major questions about the potential for >1 mammalian species, either wild or domestic, to act as a reservoir. More studies are needed to further elucidate this ecologic situation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0ce84fb7-a73e-4ff8-a8fe-e53fc17e1025", "title": "High Incidence of Japanese Encephalitis, Southern China", "text": "【0】High Incidence of Japanese Encephalitis, Southern China\n**To the Editor:** Japanese encephalitis virus (JEV) remains a major source of illness and death in Asia . An estimated 67,900 cases occur each year in Asia; ≈33,900 cases—half the cases in the world—probably occur in the People’s Republic of China . However, because reporting is incomplete in most countries where JE incidence is high, these estimates are based on scarce data. In China, a study conducted during 2006–2007 in sentinel hospitals in 1 prefecture each in Shandong, Hubei, Guangxi, and Hebei Provinces (all in the eastern half of China) found that 9.2% of patients with acute meningitis and encephalitis had JEV; adjusted incidence for each prefecture was 0.08–1.58 cases per 100,000 population . Incidence in these 4 prefectures is lower than that among children < 14 years of age in JE-endemic countries, where estimated incidence is 5.4 cases per 100,000 population . To assess the need for strengthening existing JE surveillance and vaccination programs, we conducted a population-based study of JE incidence in 1 area of southern China.\n\n【1】Dehong is a prefecture in western Yunnan Province, which borders Myanmar. The JE-susceptible population of Dehong Prefecture (residents < 15 years of age) is 211,337. The 2 principal cities of Dehong Prefecture—Mangshi and Ruili—are busy commercial centers surrounded by areas of extensive rice cultivation. The mosquito vector of JEV, _Culex tritaeniorhynchus_ , is predominant during summer . During 1988–2007, JEV vaccination was available only at certain clinics and only for a fee; however, since 2008, vaccination with the live, attenuated SA 14–14–2 JEV vaccine (Chengdu Institute of Biologic Products, Chengdu, China) has been included in the national Expanded Program on Immunization at no charge. The recommendation for children is vaccination at 8 months and 2 years of age .\n\n【2】To estimate JE incidence in Dehong Prefecture during January 1–December 31, 2010, we conducted an anonymous, unlinked study of all cases of encephalitis at the only 2 major children’s hospitals in the region, Dehong Prefecture Hospital in Mangshi and Ruili City Hospital in Ruili. All eligible patients admitted to these hospitals were included in the study. Inclusion criteria were age < 15 years, residency in Dehong Prefecture, clinical diagnosis of encephalitis, lumbar puncture performed (routine for encephalitis patients at these 2 hospitals), and cerebrospinal fluid (CSF) pleocytosis. After routine testing was completed, leftover CSF and serum samples were stored at −70°C until further testing, which was all conducted at the Chinese Center for Disease Control and Prevention in Beijing. All CSF specimens were tested by viral culture in C6/36 and BHK-21 cells  and tested for antibodies against JEV . Serum samples were tested for antibodies against JE virus, mumps virus, echoviruses, and coxsackieviruses . A case of JE was defined as illness in a person with IgM against JEV in CSF or serum. Clinical information was collected by using a standardized chart abstraction form. Linkages to personal identifiers were destroyed.\n\n【3】A total of 189 eligible patients were enrolled, 150 from Mangshi and 39 from Ruili. Of these, 110 (58%) were male and 78 (41%) were < 4 years of age. Enrollment peaked during summer . All patients were hospitalized within 6 days after symptom onset. A total of 22 (12%) patients were classified as having JE on the basis of IgM, in CSF for 21 and in serum for 1. Illness onset occurred during May–November ; overall incidence was 10.4 cases per 100,000 children < 15 years of age. Among these 22 children with JE, 11 were male; 20 were from rural areas; 14 were from Mangshi and 8 were from Ruili; and 5 were 0–1 years, 6 were 2–4 years, and 11 were 5–13 years of age. JEV vaccination history was infrequently recorded in the medical charts; however, JEV was more likely to be the cause of encephalitis among children who received no vaccination (22%, 6/27) than among those with unknown vaccination history (10%, 15/157). Of 5 vaccinated children, 1 had JE; however, verification of this child’s vaccination was not possible. Among 71 children who had no evidence of JE but for whom serum samples were available for testing, 5 had antibodies against mumps virus, 8 against echoviruses, and 5 against coxsackieviruses. Viral cultures of CSF from all 189 children were negative.\n\n【4】Our finding of 10.4 JE cases per 100,000 children < 15 years of age in Dehong Prefecture is higher than the estimated incidence of 5.4 cases per 100,000 population among children < 14 years of age in JE-endemic countries . Nevertheless, the true JE population incidence for Dehong Prefecture might be underestimated if some children received no medical care or were admitted to other hospitals. Adults were not studied; however, ≈90% of JE cases in China are reported among children <15 years of age ._ Unfortunately, accurate age-adjusted JE vaccination coverage data for Dehong Prefecture are not available. Although vaccination programs have markedly lowered JE incidence in China in recent years , the finding of continuing high JE incidence in Dehong Prefecture warrants further attention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6f20d46-ff57-4453-b129-b3f28d04daf4", "title": "Participation in a Diabetes Self-Management Class Among Adults With Diabetes, New Jersey 2013–2015", "text": "【0】Participation in a Diabetes Self-Management Class Among Adults With Diabetes, New Jersey 2013–2015\nAbstract\n--------\n\n【1】Identifying patient groups with low participation in diabetes self-management education can inform efforts to improve its use. Data from the 2013–2015 Behavioral Risk Factor Surveillance System were used to assess variation in participation in a diabetes self-management class in New Jersey. Nonparticipation varied significantly by race/ethnicity ( _P_ < .001), education ( _P_ < .001), health care coverage ( _P_ \\= .04), county ( _P_ < .001), years since diagnosis ( _P_ < .001), and whether a diabetes provider visit occurred in the past year ( _P_ \\= .002). Attention is warranted in identifying participation barriers among patients who live in certain counties, have less education, are without health care coverage, have been diagnosed with diabetes more recently, visit a provider less often, or belong to certain racial/ethnic minority groups.\n\n【2】Objectives\n----------\n\n【3】Diabetes self-management education (DSME) is defined as “the process of facilitating the knowledge, skill, and ability necessary for diabetes self-care” . Research has linked DSME with improved glycemic control and various indicators of preventive care . Identifying patients with low participation can inform efforts to promote DSME; however, few studies have examined the characteristics of nonparticipants . These efforts are important in New Jersey where participation falls below that of many other states . Therefore, we evaluated variation in DSME nonparticipation by various factors. We also evaluated whether a geographic association exists between program availability and program need.\n\n【4】Methods\n-------\n\n【5】The Behavioral Risk Factor Surveillance System (BRFSS) is a state-based landline and cellular telephone survey of noninstitutionalized, civilian adults in the United States. A cross-sectional study was performed using New Jersey data from the 2013–2015 BRFSS. Survey respondents were asked whether a doctor, nurse, or other health professional had ever told them they had diabetes. Respondents who reported only a history of gestational diabetes or prediabetes were excluded, resulting in a total of 4,397 respondents with diabetes. These respondents were asked, “Have you ever taken a course or class in how to manage your diabetes yourself?” Those who did not answer (n = 39) were excluded, leaving a final sample of 4,358. Research on the reliability and validity of BRFSS questions has been published .\n\n【6】We used SAS version 9.2 complex survey procedures (SAS Institute, Inc). Before combining the annual samples, we used the Rao-Scott χ 2  test to confirm that annual estimates for diabetes prevalence and the percentage of New Jersey adults with diabetes who never participated in a diabetes self-management class were similar. After combining the annual samples, annual weights were adjusted on the basis of contribution to the overall sample. Diabetes prevalence and class nonparticipation were estimated overall and by county. Nonparticipation was also estimated by various demographic, socioeconomic, and clinical factors. The Rao-Scott χ 2  test was used to assess the bivariate association between each factor and nonparticipation. We used ArcGIS 10.3.1 (Esri) to geographically display nonparticipation by county, using the Jenks natural breaks classification method . We also calculated the number of diabetes self-management programs per 100,000 adults with diabetes by county and overlaid graduated symbols reflecting these rates. The New Jersey Diabetes Prevention and Control Program maintains a statewide listing of diabetes self-management programs (recognized by the American Diabetes Association \\[ADA\\], accredited by the American Association of Diabetes Educators \\[AADE\\], or licensed by Stanford University); this listing was used to identify the number of programs in each county.\n\n【7】Results\n-------\n\n【8】The overall New Jersey annual BRFSS response rates were 41.4% , 47.5% , and 46.6%  . The estimates for diabetes prevalence ( _P_ \\= .20) and for class nonparticipation ( _P_ \\= .28) did not vary significantly by year. Based on the combined 3-year sample, the total number of New Jersey adults with diabetes was estimated to be 643,817 (9.3%; 95% confidence interval \\[CI\\], 8.9%–9.7%); 58.0% (95% CI, 55.8%–60.3%) of these adults never participated in a diabetes self-management class. Estimated class nonparticipation varied significantly by race/ethnicity ( _P_ < .001), education ( _P_ < .001), health care coverage ( _P_ \\= .04), years since diagnosis ( _P_ < .001), and whether a provider visit for diabetes occurred in the past year ( _P_ \\=.002) .\n\n【9】The estimated percentage of adults with diabetes who never participated in a diabetes self-management class and the number of diabetes self-management programs per 100,000 adults with diabetes are shown by New Jersey county in the Figure. Class nonparticipation varied by county ( _P_ < .001), ranging from 41.5% (95% CI, 30.3–52.6) of adults with diabetes in Somerset County to 69.8% (95% CI, 62.6–76.9) of adults with diabetes in Cumberland County. Program availability ranged from 3.6 programs per 100,000 residents with diabetes in Morris County to 31.9 programs per 100,000 residents with diabetes in Salem County.  \n\n【10】Estimated percentage of New Jersey adults with diabetes who have never participated in a diabetes self-management class (Behavioral Risk Factor Surveillance System \\[BRFSS\\] 2013–2015), and number of diabetes self-management programs (New Jersey Diabetes Prevention and Control Program) per 100,000 adults with diabetes (BRFSS 2013–2015), by New Jersey county. \n\n【11】Discussion\n----------\n\n【12】Our findings suggest that efforts to promote DSME should target participation barriers among patients who live in certain counties, have less education, who are without health care coverage, were diagnosed recently, visit a diabetes provider less often, or who identify as Hispanic or non-Hispanic other race (American Indian/Alaska Native, Asian, Native Hawaiian/Pacific Islander, other). These results are consistent with those of previous studies showing that nonparticipants were more likely to belong to minority racial/ethnic groups and have less education . Our findings also suggest that lower participation in certain areas may not always reflect program availability. In some counties where the need was highest, the number of programs was lowest (Passaic, Hudson); however, this association was not apparent in other counties with a high level of need (Bergen, Cumberland).\n\n【13】This study has several limitations. Respondents may have attended a course or class that was not ADA-recognized, AADE-accredited, or Stanford University–licensed; such programs were not considered in the geographic analysis. This analysis was conducted at the county level; therefore, any association between program availability and nonparticipation that exists below this level would not have been detected. Finally, we considered only the number of programs as a measure of program availability; other factors such as geographical reach, cultural or linguistic capacity, operating hours, influence of strategic partnerships, and venue characteristics may be important. This study also has several strengths. The results represent an estimated 643,817 New Jersey adults with diabetes. Also, the findings and methods may have much broader relevance beyond New Jersey, because promoting DSME continues to be a national public health priority .\n\n【14】The issue of DSME nonparticipation is complex. Community needs assessments should consider how patient-level and program-level factors contribute to nonparticipation among residents. Study findings can be used to focus these efforts on patient groups that exhibit low use of DMSE programs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cd427048-2883-4176-b510-fc4505f59475", "title": "Systematic Review of Programs Treating High-Need and High-Cost People With Multiple Chronic Diseases or Disabilities in the United States, 2008–2014", "text": "【0】Systematic Review of Programs Treating High-Need and High-Cost People With Multiple Chronic Diseases or Disabilities in the United States, 2008–2014\nPEER REVIEWED\n\n【1】PEER REVIEWED\n\n【2】*   Abstract\n*   Introduction\n*   Methods\n*   Results\n*   Discussion\n*   Acknowledgments\n*   Author Information\n*   References\n*   Tables\nTables\n------\n\n【3】#####  Table 1. Characteristics of Studies Testing the Effect of Programs Treating High-Cost, High-Needs People (N = 27) by Study Type, United States, May 31, 2008–June 10,\n\n| First Author, Year, State(s) | Practice Setting | RCT | Study Design for non-RCTs | Sample Size | Target Population | Program Type: Intervention Description |\n| --- | --- | --- | --- | --- | --- | --- |\n| Alexopoulos , 2011, California, New York | Academic medical center | Yes | — | 221 | Adults >59 years with major depression and executive dysfunction | CDSM: Problem-solving therapy in 12 weekly sessions in which participants set goals, proposed ways to reach them, created action plans, and evaluated the accomplishment of their goals. |\n| Barrett , 2010, Ohio | Hospital | No | Longitudinal; participants compared with themselves over time (no control group) | 585 | High-risk older adults (=60 years) in the community | CM: Proactive gatekeeper program and case management model used to identify at-risk older adults in the community; nonclinician volunteers underwent 1-hour to 2-hour training to recognize signs and symptoms indicating that patient needed assistance to remain safe and independent in the community. |\n| Blank , 2011, Pennsylvania | Academic medical center | Yes | — | 238 | Patients with HIV and serious mental illness | CM: Care assigned to an advanced-practice nurse who provided in-home consultations and coordinated medical and mental health services for 1 year according to a disease management model. The nurse collaborated with prescribing providers, pharmacists, and case managers to organize medication regimens and coping mechanisms for barriers to medication adherence. |\n| Boult , 2011, Maryland | Community-based primary care practices | Yes | — | 850 | Patients aged =65 years at high risk of using health services | CM: Guided care: a comprehensive assessment, evidence-based care planning, monthly monitoring of symptoms and adherence, transitional care, coordination of health care professionals, support for self-management, support for family caregivers, and enhanced access to community services. |\n| Casey , 2011, Arkansas | Tertiary care children’s hospital | No | Pre/post (no control group) | 255 | Medically complex children (<18 years) with at least 2 chronic medical conditions | CM: Improved coordination of care with PCPs, subspecialists, hospitalists, and community-based services. |\n| Comart  , 2013, Massachusetts | Long-term care | No | Case-control | 250 | Frail, medically complex seniors (=65 years) | NH: An interdisciplinary consult team formed to facilitate conversations about goals of palliative care; the team consisted of a PCP, clinical nurse specialist, chaplain, social worker, and a psychologist, who also served as the lead administrator for the program. |\n| De Jonge 2014, Washington, DC | Home-based primary care | No | Case-control | 2,883 | Frail and elderly (=65 years) Medicare beneficiaries | CM: A mobile care team that delivered medical services to homebound elders with disabling and multiple chronic conditions. The interprofessional team consisted of physicians, nurse practitioners, geriatricians, social workers, and other health care providers to provide case management and other services. |\n| Edelman , 2010, Virginia, North Carolina | Veterans Affairs medical center | Yes | — | 239 | Adults of any age with poorly controlled diabetes and hypertension | DM: Group medical clinics that comprised 7 to 8 patients and a care team consisting of a primary care general internist, a pharmacist, and a nurse or other certified diabetes educator. Each session included structured group interactions moderated by the educator; the pharmacist and physician adjusted medication to manage each patient’s hemoglobin A1c level and blood pressure. |\n| Edes , 2014, United States | Home- based primary care | No | Difference in difference | 9,425; 31 interviews conducted for qualitative analysis | Veteran Medicare beneficiaries with multiple chronic conditions | CM: Interdisciplinary teams of physicians, nurses, social workers, dietitians, pharmacists, and other health care providers working together to deliver comprehensive care services. Care services used a single-care plan with medication reconciliation and caregiver training and other practices. The program focused on those beneficiaries with multiple complex chronic conditions for whom routine clinic-based care has not been successful and effective. |\n| Friedman , 2009, New York, Ohio, West Virginia | Home visits | Yes | — | 766 | High-risk Medicare beneficiaries with disability and recent significant health care use | CM: A primary care-affiliated disease management and health promotion nurse intervention among Medicare beneficiaries with disabilities; consisted of monthly home visits by trained nursing staff who coordinated with the primary care provider, made referrals to community resources, and set goals with patients and caregivers for the following areas: telephone use, shopping, ordinary housework, money management, medication management, and meal preparation. |\n| Gellis , 2012, New York | Home health care | Yes | — | 115 | Homebound older adults with heart failure or chronic respiratory failure | DM: A telehealth monitoring system that allowed patients to report vital signs daily and enhance self-management of their medical conditions through counseling and education. |\n| Gutgsell , 2013, Ohio | Hospice | Yes | — | 200 | Adult palliative care patients | DM: Palliative care incorporating 20-minute music therapy intervals administered according to prespecified pain control protocol. |\n| Jerant , 2009, California | Academic Medical Center | Yes | — | 415 | Adults (=40 years) with 1 or more of 5 common chronic illnesses and functional impairment | CDSM: Home-based, peer-led, self-management training where individuals participated in 6 weekly sessions (via a home visit or telephone call) lasting approximately 60 minutes to 70 minutes led by a nonclinician peer using a standardized curriculum. The aim of the groups was to teach fundamental self-management tasks. |\n| Kiosses , 2011, New York | Home | No | Case study (no control group) | 2 | Depressed, cognitively impaired, disabled elderly (=65 years | CDSM: Problem adoption therapy (PATH) delivered by 12 in-home sessions conducted weekly, initial assessment, and a personalized treatment plan. |\n| Kuo , 2013, Arkansas | Academic medical center | No | Pre/post (no control group) | 120 | Medically complex children (<18 years) | CM: Improved coordination of care with PCPs, subspecialists, hospitalists, and community-based services. |\n| Li , 2013, New York, Ohio, West Virginia | Home visits | Yes | — | 499 | Medicare recipients needing or receiving help with at least 3 IADLs or 2 ADLs, who had recent significant health-care use | CM: Monthly home visits by trained nursing staff who coordinated with PCP, made referrals to community resources, and set goals with patients and caregivers for the following areas: telephone use, shopping, ordinary housework, money management, medication management, and meal preparation. |\n| Luptak , 2010, Utah | Home telehealth | No | Pre/post (no control group) | 132 | Rural veterans aged =65 years with high use of health care services | CM: A Care Coordination Home Telehealth intervention consisting of face-to-face orientation, telephone contact with a designated care coordinator, and daily monitoring sessions using an in-home telehealth device to assess participants’ medication usage, compliance, and symptoms and to provide patient education. |\n| Moggi , 2010, California | Substance use disorder programs affiliated with the Veterans Affairs | No | Pre/post (no control group) | 132 | Adults of all ages with substance abuse and personality disorders | DM: A representative sample of 15 substance use disorder programs affiliated with the US Department of Veterans Affairs selected on the basis of criteria such as large patient pool, geographic dispersion,and representative treatment orientations. |\n| North , 2008, Colorado | Veterans Affairs Medical Center | No | Pre/post (no control group) | 104 | Frail, chronically ill, homebound, elderly (=65 years) veterans | CM: Home visits, coordinated care, and referral to community resources. |\n| Ornstein , 2013, New York | Home health care | No | Longitudinal with assessments at 3 weeks and 12 weeks (no control group) | 140 | Homebound adults of all ages receiving palliative care | CM: A comprehensive initial home visit and assessment by a physician with subsequent follow-up care, interdisciplinary care management including social work, and urgent in-home care as necessary. |\n| Ouslander , 2009, Georgia | Nursing home | No | Pre/post (no control group) | 289 (beds) | Nursing homes with the highest hospitalization rates | CM: Prospective quality improvement initiative conducted by the Georgia Medical Care Foundation, the Medicare Quality Improvement Organization for Georgia. Participating NHs were provided with communication and clinical practice tools and strategies designed to assist in reducing potentially avoidable hospitalizations, and on-site and telephonic support by an advanced practice nurse. |\n| O’Toole , 2009, Rhode Island | Veterans Affairs Medical Center | No | Retrospective cohort study with assessments at 6 months at 12 months (control group) | 177 | Homeless adult veterans of all ages | CM: Chronic care model used to assign a PCP and a nurse case-manager; on-site integration of homeless-specific services, fixed day schedule for drop-in care and follow-up, patient assessment, outreach and coordination of care with community shelters, standard patient educational material, and access to self-management classes. |\n| Petry , 2010, Connecticut | HIV drop-in center | Yes | — | 170 | HIV-positive adults of all ages with cocaine or opioid use disorders | DM: A group-based contingency management intervention that rearranged the environment to frequently detect behaviors targeted for change using group sessions, weekly breath samples (screened for alcohol), and urine specimens (screened for opioids); opportunities for prizes for completing group and having substance-free specimens. |\n| Sorocco , 2013, Oklahoma | Veterans Affairs Medical Center | No | Longitudinal with assessments at 3 months and 6 months (no control group) | 6 | Elderly (=65 years) veterans with complex medical conditions and their caregivers | CM: A home telehealth monitoring system where patients provided daily vital signs and were supervised by an interdisciplinary treatment team. |\n| Takahashi , 2013, Minnesota | Academic medical center | No | Prospective cohort study (control group) | 40 | Medically complex adult (>60 years) patients with a high risk of readmission based on Elder Risk Assessment | TC: A care transition team (nurse practitioner, case manager registered nurse, PCP, and consulting geriatrician) providing care coordination and an in-home visit 1 to 3 days after discharge |\n| Wakefield , 2011, Iowa | Veterans Affairs Medical Center | Yes | — | 302 | Veterans of all ages with diabetes and hypertension | DM: Close surveillance via a home telehealth device (to monitor blood glucose and blood pressure) and nurse care management over a 6-month time period. A high-intensity group received tailored health information tips and questions; a low-intensity group responded to 2 daily questions but did not receive information tips and questions given to the high-intensity group. The primary goal of the study was clinical outcomes of hemoglobin A1c and systolic blood pressure. |\n| Wakefield , 2012, Iowa | Veterans Affairs Medical Center | Yes | — | 302 | Veterans of all ages with diabetes and hypertension | DM: Close surveillance via a home telehealth device (to monitor blood glucose and blood pressure) and nurse care management over a 6-month time period. A high-intensity group received tailored health information tips and questions; a low-intensity group responded to 2 daily questions but did not receive the information tips and questions given to high-intensity group. The study reported on secondary outcomes, such as medication adherence and self-efficacy scores. |\n\n【5】Abbreviations: ADL, activities of daily living; CM, care or case management; CDSM, chronic disease self-management; DM, disease management; IADL, instrumental activities of daily living; NH, nursing home; PCP, primary care provider; TC, transitional care; —, not applicable.\n\n【6】#####  Table 2. Summary of Evidence From 27 Successful Studies Testing the Effect of Programs Treating High-Cost, High-Needs People (N = 27) by Model Type, United States, May 31, 2008–June 10,\n\n| Model | Outcome |\n| --- | --- |\n| Study Type and Number | Patient Satisfaction a | Clinical a | Health Care Use a |\n| --- | --- | --- | --- |\n| Care and case management | 3 RCTs, 9 quasi-experimental, 1 case-control, 1 prospective cohort | 1/2 | 4/4 | 8/9 |\n| Chronic disease self-management | 2 RCTs, 1 case study | — | 1/3 | — |\n| Disease management | 6 RCTs, 1 quasi-experimental | 1/1 | 5/6 | 1/1 |\n| Nursing home | 1 Case-control | — | 1/1 | 1/1 |\n| Transitional care | 1 Quasi-experimental | — | — | 0/1 |\n\n【8】Abbreviations: RCT, randomized controlled trial; —, not applicable.  \na  The numerator is the number of studies showing a difference in outcome, and the denominator is the number of studies in which this outcome was assessed.\n\n【9】-----------\n\n【10】This appendix is available for download as a Microsoft Word document word icon \\[DOCX — 40 KB\\] .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d043e0d1-5dfc-41da-92c0-3efb7d031ca3", "title": "Novel Paramyxovirus Associated with Severe Acute Febrile Disease, South Sudan and Uganda, 2012", "text": "【0】Novel Paramyxovirus Associated with Severe Acute Febrile Disease, South Sudan and Uganda, 2012\nParamyxoviruses comprise a large family of viruses, including pathogens that cause severe disease in humans . Worldwide, >100 paramyxoviruses have been identified in bats and rodents . Among these, few have been shown to be pathogenic to humans, possibly because of limited host range and/or infrequent exposure. We describe a novel rubula-like virus that was associated with a severe acute febrile illness in a woman. The patient was a wildlife biologist who had participated in a 6-week field expedition to South Sudan and Uganda. During this expedition, she had been exposed to bats and rodents of >20 species while wearing different levels of personal protective equipment.\n\n【1】### Clinical Presentation\n\n【2】During the summer of 2012, a 25-year-old female wildlife biologist participated in a 6-week field expedition to South Sudan and Uganda, where she traveled to remote rural areas collecting bats and rodents for ecologic research. In the course of her duties, she manipulated animals in traps and mist nets, performed dissections, collected blood and tissues, and visited caves with large bat populations. She received no injuries from sharp objects and no bites or scratches from the animals with which she was working. She occasionally used respiratory protection when working with animals and specimens, and she wore a respirator while in caves. During her trip, she had no known contact with ill members of the field team, no contact with health care facilities, and no sexual contacts. She had been vaccinated against hepatitis A and B, yellow fever, measles, mumps, rubella, rabies, polio, tetanus/diphtheria, and typhoid fever, and she fully complied with a malaria prophylaxis regimen of atovaquone/proguanil. Her medical history included migraines and treatment of presumptive malaria with artemether/lumefantrine during a similar expedition the previous year.\n\n【3】Five days after returning to the United States, the woman was evaluated in the emergency department for a 2-day history of fever, malaise, headache, generalized myalgia and arthralgia, neck stiffness, a metallic taste, and sore throat. Results of rapid malaria test, performed on the day of fever onset, were negative. Other laboratory results and the patient’s vital signs at the time of admission are summarized in the Table . The patient seemed to be fatigued but alert and oriented; she was anicteric, and she had no nuchal rigidity or focal neurologic deficits. Mild erythema of the soft palate without ulcerations or exudates was noted. The spleen tip was palpable despite absence of adenopathy.\n\n【4】Examination of heart, abdomen, and lungs (including chest radiographs) revealed no abnormalities. No rash or synovitis was noted. Treatment with intravenous ceftriaxone was begun for possible typhoid fever, and artemether/lumefantrine was continued for presumptive malaria.\n\n【5】On hospital day 2, a maculopapular rash erupted over the patient’s trunk , several small aphthous ulcers appeared on her soft palate, and she had mild diarrhea. As long as the fever persisted, clear pulse/temperature dissociation was present (positive Faget sign); however, hemodynamics, oxygenation, and renal function were stable. Doxycycline was added for the expanded differential diagnosis of a rickettsial illness or plague. On hospital day 3, fever, headache, and myalgia persisted, and the patient experienced bloody emesis, mild diarrhea positive for occult blood but without frank hematochezia or melena, and minimal diffuse abdominal tenderness. Her menstrual period occurred without substantial menorrhagia. The rash became confluent; a centrifugal distribution and prominent petechia appeared at sites of trauma or pressure.\n\n【6】The possibility of hemophagocytosis was considered, and a bone marrow biopsy sample was obtained on day 4. The sample showed a mild increase in macrocytic hemophagocytosis and pancytopenia with a hypocellular marrow with myeloid hyperplasia and erythroid hypoplasia .\n\n【7】The fever slowly but progressively decreased, and the patient improved over the next few days; the last recorded fever was on hospital day 9. Abdominal pain and diarrhea resolved. Ceftriaxone was discontinued after 8 days. When the patient was discharged on hospital day 14, arthralgia and myalgia had improved, oropharynx ulcerations had healed, the rash had resolved without desquamation, and blood counts and hepatic enzyme levels were returning to reference limits. Considerable sequelae (myalgia, arthralgia, headache, malaise, and fatigue) persisted for several months.\n\n【8】### Diagnostic Workup\n\n【9】The initial suspected diagnosis was hematophagocytic syndrome (hemophagocytic lymphohistiocytosis). This clinical syndrome has been associated with a variety of viral, bacterial, fungal, and parasitic infections, as well as collagen–vascular diseases and malignancies. Initial diagnostic testing for various infectious diseases included blood screening for respiratory viruses, HIV, cytomegalovirus, and malaria parasites; all results were negative.\n\n【10】On hospital day 2, a diagnosis of a viral hemorrhagic fever was considered, and blood specimens were sent to the Centers for Disease Control and Prevention (CDC) for testing. Molecular testing results were negative for rickettsiae, filoviruses (Marburgviruses and Ebolaviruses), selected bunyaviruses (Rift Valley fever virus, Crimean Congo hemorrhagic fever virus), arenaviruses (Lassa, Lujo, and lymphocytic choriomeningitis viruses), and several arboviruses (yellow fever, dengue, O'nyong-nyong, chikungunya, and Zika viruses).\n\n【11】A pathogen-discovery deep-sequencing protocol was followed, as described . In brief, total RNA was extracted from blood and serum samples obtained 3 days after symptom onset; RNA was nonspecifically amplified with previously described primers . The cDNA library was sequenced on a 454 FLX Genome Sequencer (Roche Diagnostics, Indianapolis, IN, USA). Unassembled sequences were translated and compared with the nonredundant protein database from the National Center for Biotechnology Information by using a BLASTx algorithm . The sequence reads linked to the BLASTx results were distributed into taxa by using MEGAN . Metagenomic analysis revealed a novel paramyxovirus in the patient’s blood and serum; the virus was most closely related to Tuhoko virus 3, a rubula-like virus (family _Paramyxoviridae_ ) recently isolated from _Rousettus leschenaultii_ fruit bats in southern China . The next-generation sequence reads with homology to Tuhoko virus 3 covered ≈25% of the expected complete virus genome. Based on the sequences obtained, a series of primers were designed to amplify overlapping fragments spanning the complete genome of this novel virus. A detailed list of primers is available upon request. Amplicons of different sizes were obtained by standard reverse transcription PCR (RT-PCR) and sequenced by the standard Sanger method .\n\n【12】This novel paramyxovirus is provisionally named Sosuga virus in recognition of its probable geographic origin (South Sudan, Uganda). The complete genome of Sosuga virus was 15,480 nt long and conformed to the paramyxovirus rule of 6 . The genome organization  resembled that of most paramyxoviruses, containing 6 genes, _N_ , _V/P_ , _M_ , _F_ , _HN_ , and _L_ , encoding the 7 viral proteins: nucleocapsid (N), V protein (V), phosphoprotein (P), matrix protein (M), fusion protein (F), hemagglutinin–neuraminidase (HN), and polymerase (L). The sequence of the RNA editing site in the _V/P_ gene is identical to that of Tuhoko virus 3 . The faithful transcription of V/P generates the V mRNA, and a GG insertion at the editing site generates the P mRNA. In addition, the terminal 5′ and 3′ sequences of the virus were experimentally determined  by rapid amplification of cDNA ends as described .\n\n【13】Pairwise comparison of the full-length sequence of Sosuga virus with the closest related viruses showed 61.6%, 53.1%, and 51.4% identities, respectively, with Tuhoko virus 3, Achimota virus 1, and Achimota virus 2 (Achimota viruses were isolated from the _Eidolon helvum_ fruit bat in Ghana) . When the deduced amino acid sequences of Sosuga virus were compared with those of Tuhoko virus, 3 proteins revealed overall amino acid identities ranging from 57.4% (HN) to 84% (N). Phylogenetic analysis of Sosuga virus and other paramyxoviruses clearly showed that Sosuga virus clusters with other bat-borne rubula-like viruses, which are closely related to rubulaviruses but have not yet been classified as such .\n\n【14】### Virus Isolation\n\n【15】Virus isolation was attempted by inoculating monolayers of Vero-E6, Vero-SLAM, and H292 cells (mucoepidermoid carcinoma cells from human lungs) with patient blood and serum collected 3 days after symptom onset, but no virus isolate was obtained. As an alternative, 10 μL of the blood sample was also inoculated intracranially and intraperitoneally into 2-day-old suckling mice, which were then observed for 28 days for signs of illness. Neurologic signs developed 9–10 days after inoculation in 2 of the 20 mice; these 2 mice were euthanized 2 days later. To confirm the presence of the virus, we extracted total RNA from liver, spleen, and brains of the euthanized animals and used it as input in a specific RT-PCR designed to amplify a 2,188-bp fragment partially spanning the virus _HN_ and _L_ genes. Consistent with virus replication and observed neurologic signs, viral RNA was found in the brain but not in liver or spleen samples .\n\n【16】Brain homogenates from the euthanized mice were inoculated into fresh monolayers of Vero-E6 cells and H292 cells; 12 days after infection, a cytopathic effect, with cell rounding but no syncytia formation, became evident. Virus antigen was detected by immunofluorescence in both cell lines by using patient’s convalescent-phase serum, collected 50 days after symptom onset . Moreover, transmission electron microscopy used to examine virus morphology showed pleomorphic virions, consistent with those of paramyxoviruses .\n\n【17】### Development of New Diagnostic Assays\n\n【18】Because the patient seemed to have acquired the infection during her African research expedition, where she had had extensive contact with rodents and bats, other persons who also come in contact with bats or rodents, such as field biologists, local residents, or ecotourists, might be at risk for infection. This potential public health threat prompted us to develop diagnostic assays for the rapid detection of Sosuga virus.\n\n【19】First, we developed a TaqMan real-time RT-PCR selective for the _N_ gene and tested it on all available serum and blood samples from the patient. This test showed that the patient’s viremia peaked early in the course of the infection (cycle threshold 29.5 on day 3 after symptom onset), coinciding with the period of high fever and diverse irregularities in blood parameters . By day 9, the viremia had decreased (cycle threshold 36.3); viremia was undetectable 11 days after symptom onset.\n\n【20】Second, we developed a new ELISA specific for Sosuga virus by using the virus recombinant nucleocapsid protein produced and purified from _Escherichia coli_ . This assay was tested on all available serum samples from the patient . Although IgG and IgM were not detectable on day 3 after symptom onset (titers <50), seroconversion (IgG and IgM titers \\> 1,600) occurred 11 days after symptom onset. As expected, IgM levels later decreased (titer \\> 400), and IgG levels remained high 50 days after symptom onset.\n\n【21】In addition, the new ELISA was tested for potential cross-reactivity with some common paramyxoviruses, including mumps and measles viruses. No cross-reactivity was detected on the ELISA plates when control serum from patients with high levels of IgG against mumps and measles viruses was used, a desired feature in a new diagnostic assay because most persons have IgG to these viruses as a result of vaccination or natural infection.\n\n【22】### Conclusions\n\n【23】A severe disease affected a wildlife biologist shortly after her return from rural Africa to the United States. Because of the disease characteristics (high fever and blood abnormalities) and travel history, a viral hemorrhagic fever was suspected, and clinical samples were rushed to CDC for investigation of a possible high-risk virus. After molecular and serologic diagnostic assays ruled out several well-known human pathogens (e.g. filoviruses, arenaviruses, phleboviruses, flaviviruses, and rickettsiae) as the cause of the patient’s illness, a next-generation sequence approach was followed to detect a possible new infectious agent.\n\n【24】The combination of next-generation sequencing and metagenomic analysis identified a novel paramyxovirus; the virus genome was completely characterized by use of standard sequencing techniques. The complete virus sequence clearly indicated a relationship with other rubula-like viruses isolated from bats. Moreover, the novel virus was isolated from acute-phase serum samples by infecting suckling mice and propagating the virus in cell culture.\n\n【25】The specific molecular and serologic diagnostic assays that we developed will facilitate rapid identification of this novel infectious agent should new cases occur. We used these assays to retrospectively investigate all available clinical samples from the patient, and the results revealed periods of viremia and seroconversion.\n\n【26】Although the exact source of the patient’s infection remains unknown, the sequence similarity with bat-derived rubula-like viruses is highly suggestive. In recent years, a large number of diverse paramyxoviruses have been detected in bats , but only Nipah and Hendra viruses (genus _Henipavirus_ ) are known to cause severe disease in humans . An investigation to detect Sosuga virus in African bats is currently under way.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "19ad9398-7305-4162-bb5a-388a3a25a534", "title": "Rickettsia raoultii in Dermacentor reticulatus Ticks, Chernobyl Exclusion Zone, Ukraine, 2010", "text": "【0】Rickettsia raoultii in Dermacentor reticulatus Ticks, Chernobyl Exclusion Zone, Ukraine, 2010\n**To the Editor:** The Chernobyl Exclusion Zone (CEZ) surrounds the center of the 1986 Chernobyl nuclear power plant disaster. Preliminary study shows predominance of _Dermacentor reticulatus_ ticks in the CEZ; ticks of other species, such as _Ixodes ricinus_ , are surprisingly rare, even in habitats where they should be relatively common . A few reports document presence of _Ix. trianguliceps_ ticks . Prevalence of pathogens ( _Anaplasma phagocytophilum_ , _Borrelia burgdorferi_ s.l. _Babesia_ spp.) in these ticks is higher in the CEZ than in other regions . One pathogen transmitted by _Dermacentor_ spp. ticks is _Rickettsia raoultii_ , which has been isolated from species of _Dermacentor_ ticks found in Asia  and since 1999 has also been detected in Europe.\n\n【1】In our study, _D. reticulatus_ ticks were collected by use of the flagging method  in the CEZ in September 2010. Ticks were collected from areas where they were known to occur, around the former villages of Korohod (51°16′02′′N; 30°01′04′′E) and Cherevach (51°12′44′′N; 30°07′45′′E) and around Chernobyl city (51°17′04′′N; 30°13′25′′E). The habitats investigated included open areas and the remnants of farmlands. A total of 201 _D. reticulatus_ ticks, 87 males and 114 females, were collected and investigated .\n\n【2】DNA was extracted by use of the ammonium hydroxide method . Isolated DNA was examined for the presence of the _Rickettsia_ sp. citrate synthase gene ( _gltA_ ) by use of PCR with _Rp_ CS.409d and _Rp_ CS.1258n primers . Positive amplicons were sequenced, and sequences were edited by using AutoAssembler software (Applied Biosystems, Foster City, CA, USA) and compared with GenBank entries by using blastn version 2.2.13 . All obtained sequences were submitted to GenBank (accession nos. KX056493 and KX056494).\n\n【3】Infection with _Rickettsia_ spp. was detected in 72.64% of ticks . A higher proportion of males (80.46%) than females (66.66%) were infected. Sequence analysis showed 100% identity with _R. raoultii_ isolated from _D. marginatus_ ticks from China (GenBank accession nos. KU171018.1 and KT261764.1) and _D. reticulatus_ ticks from Poland (KT277489) and Hungary (LC060714.1). In the CEZ, the predominant tick species is _D. reticulatus_ ; no _D. marginatus_ ticks have been found in the CEZ . Thus, in this area, the _R. raoultii_ vector is _D. reticulatus_ ticks.\n\n【4】The prevalence of _R. raoultii_ infection among _D. reticulatus_ ticks (68.42%–74.07%) is significantly higher in the CEZ than in other regions. A previous study found prevalence of _A. phagocytophilum_ infection in the CEZ to be high, mainly associated with _Ixodes_ ticks  and rarely associated with _D. reticulatus_ ticks. The prevalence of _Babesia canis_ infection, also vectored by this tick, was within the usual range . The reason for prevalence of at least 2 vectored pathogens being higher in _D. reticulatus_ ticks in the CEZ than in other region is not known and needs more study. The prevalence of these pathogens among mammals that inhabit the CEZ is also not known; the influence of radiation on pathogen level has not been studied. The nucleotide sequences of _R. raoultii_ detected in ticks in the CEZ are identical to sequences originating from other regions and deposited in GenBank; the sequences of _A. phagocytophilum_ and _B. canis_ from the CEZ were also similar to those described elsewhere . If the reason for the higher _R. raoultii_ infection prevalence is radiation, then radiation also influences the ticks—some morphologic abnormalities have been noted on _D. reticulatus_ ticks collected from the CEZ .\n\n【5】This study confirms presence of _R. raoultii_ in _D. reticulatus_ ticks in the CEZ. The structure of zoonotic foci in the CEZ seems to differ from that in other regions. Confirmation of this hypothesis needs follow-up study of tickborne pathogens in wild mammals that might serve as a source of infection for ticks in the CEZ.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "efab8c9c-7ffe-4d7b-ab7e-862f2e1df51e", "title": "Malaria Outbreak in Troops Returning from French Guiana", "text": "【0】Malaria Outbreak in Troops Returning from French Guiana\nTo the Editor: In January 2005, the chief surgeon in a squadron of French policemen reported a cluster of Plasmodium vivax malaria attacks in troops returning from a 108-day operation in French Guiana. We conducted a retrospective cohort study to describe the malaria attacks and determine factors related to them.\n\n【1】A self-administered questionnaire was drawn up, with questions concerning operations in French Guiana (dates, locations) and preventive measures implemented against malaria. A malaria case was defined by the association of clinical signs and Plasmodium parasites in blood smears or quantitative buffy coat tests (per definition of military epidemiologic surveillance).\n\n【2】The 40-person mission in French Guiana (Operation Anaconda) took place from July 26, 2004, to November 6, 2004 (108 days of exposure). This mission against clandestine gold panning was conducted in a deep-forest environment where the troops were temporarily housed in villages of Brazilian gold panners. Occasionally, they washed themselves late in the evening in stagnant water near the river and patrolled outside during maximum biting periods. All troops received a chemoprophylaxis (doxycycline 100 mg daily) during the mission and for 4 weeks afterward.\n\n【3】From July 2004 through January 2005, 10 persons had \\> 1 malaria attacks (attack rate 25%) for a total of 18 malaria attacks (incidence 13/100 person-months of exposure). P. vivax was isolated for 17 attacks and P. falciparum for 1 attack . Five patients had 1 malaria attack, and 4 patients had up to 3 relapses. Six patients had a malaria attack while receiving doxycycline.\n\n【4】Regarding chemoprophylaxis compliance, 34% reported missing <1 dose per week and 32% were fully compliant. The troops did not have permethrin-impregnated battlefield uniforms as do soldiers in the French Army. They had to impregnate their own uniforms with permethrin. Only 37% said they always wore clothing that fully covered them during the mission, and 86% reported having frequently used a repellent. All reported having slept under mosquito nets. No association was found between malaria attacks and regular chemoprophylaxis intake or use of repellents. Only 1 operation in French Guiana was associated with the risk of experiencing malaria attacks: 39% of troops located in Sikini had at least 1 malaria attack versus 7% of troops in other areas (relative risk: 5.9 \\[95% confidence interval 0.8–41.7\\]).\n\n【5】The incidence rate for this study was 10 times higher than the maximum incidence rate observed for French troops deployed in Côte d'Ivoire (1.3/100 troop-months in 2004). During an earlier Operation Anaconda, 37 of 62 persons deployed near the Sikini area had \\> 1 malaria attacks (attack rate 61%). Of these, 30 had \\> 1 attacks caused by P. vivax; occasionally an attack was associated with P. falciparum .\n\n【6】Our results suggest that the Sikini area was the high-risk area for malaria transmission (although the large confidence interval reflects a lack of power in our analysis). The operation dates (15–28 September) are compatible with the duration of the first cases of malaria occurrence.\n\n【7】French Guiana is the only French territory, except for Mayotte, where malaria is endemic, with nearly 5,000 cases per year, occurring mainly along the rivers bordering Suriname and Brazil . The highest frequencies of malaria appear during the dry season (September to December) in French Guiana , but no seasonality was described near the Brazilian border .\n\n【8】The Sikini area is located near the Oyapock River (Brazilian border). The mean annual incidence in Amerindians there is 48.6%, mainly due to P. falciparum (incidence 24.8%) and P. vivax (incidence 25.9%) .\n\n【9】P. vivax malaria incidence has increased in the Oyapock region, from 30% in 1987 to 50% in 2000–2004 . French troops were deployed in an area where parasite circulation was high. Troops had contacts with clandestine gold panners, mainly Brazilian illegal residents. This population, in which malaria incidence is almost impossible to evaluate, comes from Amapa State, where the incidence of malaria is increasing . In 2003, 60.9% of patients with malaria cases at Cayenne Hospital had a Brazilian name compared with 35.4% in 2000 . Also, the gold panners diverted the river and built basins where vectors could easily multiply .\n\n【10】Initial malaria attacks were treated with chloroquine or quinine. Five patients experienced \\> 1 relapses (maximum 3 relapses). The relapses were treated with 50-mg daily doses of primaquine for 4 patients and by chloroquine for the fifth patient. Two patients had relapses after receiving primaquine. Primaquine resistance information was not available. However, resistance to primaquine has emerged in P. vivax strains .\n\n【11】We recommended that pre-impregnated battlefield uniforms be available for French policemen and chemoprophylaxis adherence be reinforced by directly observed intake by supervisory staff. Relapses of P. vivax malaria are a major therapeutic problem, particularly after primaquine therapy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "77884dee-97dd-4b93-8d87-ff65c770a62c", "title": "Atypical Chikungunya Virus Infections in Immunocompromised Patients", "text": "【0】Atypical Chikungunya Virus Infections in Immunocompromised Patients\n**To the Editor:** Chikungunya fever was first described in Tanganyika (now Tanzania) in 1952 and is now emerging in Southeast Asia. Chikungunya virus (CHIKV) infection, a self-limiting febrile illness, shares similarities with dengue fever such as headache and myalgia. Additionally, patients with CHIKV infection typically have arthralgia, arthritis, and tenosynovitis . Although usually benign, CHIKV infection may on rare occasions lead to neurologic and hepatic manifestations with high illness and mortality rates . We report 2 immunocompromised patients with CHIKV infection associated with peritonitis, encephalitis, and secondary bacterial infections.\n\n【1】Patient A, a 66-year-old Singaporean-Chinese man, had a history of chronic renal disease secondary to obstructive uropathy. His baseline creatinine level was 300–400 μmol/L. For 3 years, he had ingested traditional Chinese medicine, which we suspect was contaminated by steroids because he appeared cushingoid. An outbreak of CHIKV infection was reported at his workplace. He was admitted to National University Hospital, Singapore, in July 2008 with abdominal pain, vomiting, and fever of 1 day. He had no joint symptoms. Clinically, he had systemic inflammatory response syndrome complicated by acute-on-chronic renal failure. His creatinine level was elevated at 921 μmol/L on admission. A complete blood count showed leukocytosis (19.24 × 10 9  cells/L) with neutrophilia and thrombocytopenia (62 × 10 9  cells/L). Initial blood and urine cultures and serologic results were negative for dengue virus, but serum reverse transcription–PCR (RT-PCR) and indirect immunofluorescent assay for immunoglobulin G (IgG) (Euroimmun Medizinische Labordiagnostika, Lubeck, Germany) and IgM (CTK Biotech, Inc, San Diego, CA, USA) were positive for CHIKV . Computed tomographic scans of the abdomen showed dilated small bowel loops.\n\n【2】An urgent laparotomy did not show bowel perforation, but peritoneal cultures yielded _Klebsiella pneumoniae_ , _Escherichia coli,_ and _Candida glabrata_ , and RT-PCR from the concentrated peritoneal fluid was positive for CHIKV . He was administered appropriate antimicrobial drugs. He required repeat laparotomies because of elevated intraabdominal pressure. He subsequently received broad spectrum antimicrobial drugs to treat secondary intraabdominal infections caused by _P. aeruginosa_ and _Enterococcus faecalis_ .\n\n【3】Ventilator-associated pneumonia also developed. Despite maximal support and prolonged antimicrobial therapy, this patient died after 5 months of hospitalization.\n\n【4】Patient B, a 45-year-old Malaysian–Chinese man with diabetes mellitus, had undergone a cadaveric liver transplant in 2001 for hepatitis B liver cirrhosis. He was receiving immunosuppressants (azathioprine and prednisolone). He was admitted in August 2008 after experiencing fever, headache, and abdominal bloating for 3 days. He had no neurologic symptoms. Acute self-limiting febrile illnesses with arthritis had occurred in his hometown; CHIKV infections were suspected.\n\n【5】Results of his examination on admission were normal, except for bilateral enlarged cervical lymph nodes. Chest radiograph results were unremarkable. He had mild transaminitis (alanine aminotransferase 173 U/L, aspartate aminotransferase 170 U/L), elevated C-reactive protein (107 mg/L), and thrombocytopenia (120 × 10 9  cells/L) without leukocytosis. Results of comprehensive serum and urine microbial studies were negative for posttransplant infections. Results of serum RT-PCR were negative for CHIKV, but IgG and IgM tests were positive for CHIKV.\n\n【6】Brain magnetic resonance imaging was performed because of the patient’s persistent severe headache and transient drowsiness. It showed several nonspecific areas of enhancement, which suggested encephalitis, given the clinical scenario . However, a lumbar puncture was not performed, and hence, whether the patient’s cerebrospinal fluid contained CHIKV could not be determined. Bilateral frontoparietal white matter lesions with restricted diffusion has been suggested as an early sign of viral encephalitis . However, a retrospective series demonstrated that, in CHIKV encephalitis, abnormalities on magnetic resonance imaging were uncommon, and no pathognomonic features were found .\n\n【7】Hospital-acquired pneumonia also developed and was treated with broad-spectrum antimicrobial drugs. Bronchoscopic cultures were negative for CHIKV. The patient responded well to antimicrobial drugs, and his mental status was normal on discharge. He possibly had encephalitis associated with CHIKV infection, complicated by secondary hospital acquired pneumonia.\n\n【8】In this case, CHIKV was detected in peritoneal fluid, but because of the positive bacterial cultures, we are not confident about its causative role in patient A’s peritonitis. Although a series reported that 6 patients with CHIKV infection had perforated jejunal diverticula while receiving long-term nonsteroidal antiinflammatory drugs and steroids , the perforations were likely secondary to prolonged steroid use rather than CHIKV infection. In addition, both immunocompromised patients in our study had their CHIKV infections secondarily complicated by nosocomial infections. We note that other viral infections have been associated with bacterial translocation and secondary nosocomial infections . Whether these infections were linked to CHIKV infection or to the underlying chronic immunosuppressed state is unclear.\n\n【9】Both of our patients did not have the joint manifestations that are characteristic of CHIKV infection . More prospective studies are required to determine the full spectrum of clinical features of CHIKV infection in immunocompromised patients. Recently identified biomarkers may predict patients at risk for complications but we were unable to study them in our patients . Although most cases of CHIKV infection are self-limiting, clinicians should be alert to atypical presentations and severe complications in immunosuppressed patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "50796e87-a4ab-4eac-ab66-9872b0cbe5fd", "title": "Genetic Characterization of Avian Influenza A(H5N6) Virus Clade 2.3.4.4, Russia, 2018", "text": "【0】Genetic Characterization of Avian Influenza A(H5N6) Virus Clade 2.3.4.4, Russia, 2018\nHighly pathogenic avian influenza (HPAI) H5 virus continues to evolve and pose a threat to animals and humans. Since 2008, HPAI H5 viruses of clade 2.3.4.4 with various neuraminidase (NA) subtypes have become widespread throughout the world and have caused mass epizootics, including in Russia, where these viruses have been reported since 2014 . In 2013, H5N6 virus began circulating in China , and a case of human disease was recorded there in 2014. Since then, 23 cases of H5N6 infection in humans, including 7 fatalities, have been confirmed in China .\n\n【1】In October 2018, we collected cloacal swab samples from aquatic birds around the Volga River Basin in the Saratov region of Russia (51°26′11.7″N, 46°06′49.9″E). We isolated avian H5 influenza virus from 1 sample from a common gull ( _Larus canus_ ) by using embryonic chicken eggs. We used whole-genome sequencing to extract the virus DNA and conducted a phylogenetic analysis against strains available in the GISAID EpiFlu database . We submitted genetic data on the virus, A/common gull/Saratov/1676/2018, to the GISAID EpiFlu database .\n\n【2】Using H5 clade nomenclature designated by the World Health Organization/World Organisation for Animal Health/Food and Agriculture Organization H5 Evolution Working Group , our phylogenetic analysis showed that hemagglutinin (HA) gene of A/common gull/Saratov/1676/2018 clusters with HPAI viruses in clade 2.3.4.4 H5N6-H5/Major lineage. Our analyses also show this strain belongs to a new HA subgroup that includes human H5N6 viruses isolated in Guangxi and Guangdong Provinces, China, in 2018 . This subgroup is not represented by existing candidate vaccine viruses (CVVs) .\n\n【3】The NA gene of A/common gull/Saratov/1676/2018 appears to originate from H6N6 viruses circulating in Asia during 2010–2011  and contains the deletion from positions 59–69 in the stalk region. The polymerase basic (PB) 2 gene segment also appears to have originated from an H6 subtype . The internal gene segments PB1, polymerase (PA), nucleoprotein (NP), matrix (M), and nonstructural protein (NSP) appear to have evolved from HPAI H5 virus clade 2.3.2.1 . The 8-segment constellation leads us to classify this strain into a G1.1 genotype, as described by Bi et al.\n\n【4】We conducted a comparative genomic analysis of A/common gull/Saratov/1676/2018 against H5N6 CVVs; the most pronounced differences were several amino acid substitutions associated with potential changes in antigenic properties. We also detected unique mutations in HA D54N, L115Q, L/Q138T, P141A, N183S, and N189D, including a combination of S121Y and I151T. We noted other mutations, including HA L129S, K/M/T140V (H5 numbering), and NA N86K (N6 numbering), which could be associated with antigenic drift.\n\n【5】A/common gull/Saratov/1676/2018 had an HA polybasic proteolytic cleavage site, PLRERRRKR/G, and showed highly pathogenic properties by killing chicken embryos within 48 hours. We also identified amino acid changes associated with increased virulence to mammals , including 9 mutations in the PB2 gene, 8 in the PB1 gene, 7 in the NSP gene, 3 in the M gene, 2 in the PA gene, 1 in the HA gene, and 1 in the NA gene, along with the 59–69 deletion, an 80–84 deletion in NS1, and an NS1 ESEV terminal motif. These changes also appear in most H5N6 CVVs .\n\n【6】Comparative analysis of A/common gull/Saratov/1676/2018 against H5N6 CVVs revealed similarity in the presence of genetic elements associated with receptor binding properties. A/common gull/Saratov/1676/2018 and most CVVs had the motif QS(R)G at the receptor-binding site (nt 222–224), which is associated with an avian-like α2,3-SA receptor-binding preference . The amino acid changes in D94N, S133A, and T156A in the HA of A/common gull/Saratov/1676/2018 and most H5N6 CVVs are associated with increased binding of the virus to human-like α2,6-SA receptors . Our analysis suggests that A/common gull/Saratov/1676/2018 retains its avian status but has several mutations that potentially increase its affinity for α2,6-SA, which could indicate an affinity for both avian- and human-type receptors.\n\n【7】We evaluated the phenotypic properties of the virions by kinetics measurement with surface plasmon resonance to assess their ability to bind to receptor analogs α2,3-SA and α2,6-SA . The equilibrium dissociation constant for 3′-Sialyl-N-acetyllactosamine is 12.2 (SD ± 0.7 nmol/L) and for 6′-Sialyl-N-acetyllactosamine is 43.3 (SD ± 2.8 nmol/L) . These values show that A/common gull/Saratov/1676/2018 has prevalent affinity for the avian-like receptor with lower, but increased, affinity for the human-like receptor, compared with H5N1 strain A/rook/Chany/32/2015 clade 2.3.2.1.C.\n\n【8】Analysis of homology of A/common gull/Saratov/1676/2018 with H5N6 strains available from GISAID showed that all 8 gene segments clustered with human H5N6 strains isolated in southeast China in 2018. We noted 99% homology with human strain A/Guangxi/32797/2018 for all genes, a genetic similarity that raises the question of which pathway led to the spread of the virus. We believe A/common gull/Saratov/1676/2018 was transferred to eastern Russia through northeast Siberia, where HPAI H5N8 clade 2.3.4.4.A was detected in 2018 , the same pathway through which H5N8 virus was transferred from Southeast Asia to Europe. These viral pathogens could be spread by migratory birds over long distances along flyways from southern China to southwestern Russia during a migration season. Our study indicates that emerging H5N6 viruses are a potential threat to public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b2959688-1354-4c3f-bf77-3d38827fe1f1", "title": "Short-Finned Pilot Whale Strandings Associated with Pilot Whale Morbillivirus, Brazil", "text": "【0】Short-Finned Pilot Whale Strandings Associated with Pilot Whale Morbillivirus, Brazil\nCetacean morbillivirus (CeMV; family Paramyxoviridae, genus _Morbillivirus_ ) is an important cause of illness and death in cetaceans . The genus _Morbillivirus_ comprises 2 lineages: CeMV-1, which includes dolphin morbillivirus (DMV), porpoise morbillivirus (PMV), pilot whale morbillivirus (PWMV), and beaked whale morbillivirus (BWMV) strains; and CeMV-2, comprising the strain detected in Indo-Pacific bottlenose dolphins ( _Tursiops aduncus_ ) in western Australia, the Fraser’s dolphin morbillivirus (FDMV), and Guiana dolphin morbillivirus (GDMV) strains . GDMV has been the only strain reported in cetaceans in Brazil . Four cases of PWMV have been recorded in pilot whales of the Northern Hemisphere, on the Atlantic coast of the United States and in the Canary Islands, Spain .\n\n【1】During July–October 2020, four short-finned pilot whales ( _Globicephala macrorhynchus_ ) stranded in Brazil: 2 in Ceará state (cases 1 and 2) and 2 in Santa Catarina state (cases 3 and 4). All the animals stranded alive and died within 24 hours . We performed standard necropsies and collected tissue samples, which we fixed in 10% buffered formalin for histopathology or froze at −20°C or −80°C for molecular analysis.\n\n【2】We performed RNA extractions of all available tissues with TRIzol-LS . We performed a morbillivirus 2-step reverse transcription nested PCR to amplify the phosphoprotein gene . After DNA extraction with the QIAGEN Blood & Tissue Kit , we performed herpesvirus detection in lung (n = 2) and liver (n = 4) samples by nested pan-PCRs to amplify DNA polymerase and glycoprotein B genes ; when those were positive, we tested the remaining available tissues using the same protocols. We calculated percentage of identity among the obtained sequences and the closest ones from GenBank/EMBL/DDBJ based on p-distance. We used MEGA7  to construct the phylogram .\n\n【3】Three animals, cases 1, 3 and 4, were morbillivirus-positive, amplified in central nervous system, lung, and pulmonary lymph node samples ; sequences were submitted to GenBank . Sequences from case 1 and 3 were identical and had a single nucleotide missense mutation (99.7% nt identity, 99.2% aa identity) when compared to the sequence from case 4. The sequences from cases 1 and 3 presented the highest nucleotide (99%) and amino acid identities (96.9%) with a PWMV sequence identified in 2 pilot whales in the Canary Islands, Spain (GenBank accession nos. KT006289 \\[animal 1\\], KT006290, and KT006291 \\[animal 2\\]). The sequence from case 4 had the highest nucleotide (99.2%) and amino acid similarities (97.7%) to the same PWMV sequences. Our sequences clustered with other PWMV sequences . In addition, we detected an alphaherpesvirus by the DNA polymerase protocol in a lung sample from case 3 . The remaining tissue samples of case 3 (cerebellum, kidney, mesenteric lymph node, spleen, and liver) were herpesvirus-negative by PCR. The obtained herpesvirus has the highest similarity (99.5% nt identity, 100% aa identity) to an alphaherpesvirus obtained in a striped dolphin ( _Stenella coeruleoalba_ ) from Spain .\n\n【4】The general health of the CeMV-positive animals was poor, and all were undernourished. We compared the main pathologic findings in these animals to all other cases of PWMV strain reported in the literature .\n\n【5】Pilot whales are susceptible to DMV and PWMV; DMV caused atypical pilot whale deaths in the Mediterranean Sea . By contrast, 4 cases of PWMV infections have been recorded; 1 in New Jersey, USA, and 3 in the Canary Islands, Spain . All of them had multiorgan infections . Case 1 likely had a subacute or systemic CeMV infection characterized by meningomyelitis with gliosis and lymphocytic bronchointerstitial pneumonia. Further studies are necessary to elucidate if cases 3 and 4 manifested an infection similar to the brain-only DMV form or a systemic infection with heterogenic dissemination. The poor nutritional condition observed in all PWMV-positive animals could be the result of decreased foraging capacity caused by encephalitis . Case 3 had alphaherpesvirus and CeMV co-infection, a comorbidity previously reported in cetaceans, including pilot whales ; in this case, however, there were no associated herpesviral lesions. All PWMV-positive cetaceans we described were juveniles, which could be associated with maternal passive immunity loss.\n\n【6】The occurrence of pilot whale strandings in 2020 on the coast of Brazil could be considered atypical. Of interest, although case 1 was stranded >3,300 km away from case 3 along the coastline, it had the same PWMV sequence type, which suggests circulation of that type along the coast of Brazil. Further studies are necessary to understand the effects and epidemiology of morbillivirus in cetaceans in the South Atlantic Ocean. However, the high similarity between our sequences and the PWMV detected in the Northern Hemisphere confirms that this strain also circulates in South America pilot whales and might be enzootic in _Globicephala_ sp. whales in the Atlantic Ocean.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8753c070-709b-42c1-97f7-2ad2513cd3e2", "title": "Streptococcus pneumoniae Serotype 12F-CC4846 and Invasive Pneumococcal Disease after Introduction of 13-Valent Pneumococcal Conjugate Vaccine, Japan, 2015–2017", "text": "【0】Streptococcus pneumoniae Serotype 12F-CC4846 and Invasive Pneumococcal Disease after Introduction of 13-Valent Pneumococcal Conjugate Vaccine, Japan, 2015–2017\n_Streptococcus pneumoniae_ is a common bacterial pathogen of children . To prevent pneumococcal infectious diseases, many countries have introduced 7-, 10-, and 13-valent pneumococcal conjugate vaccines (PCVs) , which have decreased the total number of invasive pneumococcal disease (IPD) cases globally. However, serotype shifts (i.e. increased identification of serotypes not in the PCV), were observed in areas in which PCVs were introduced ; as a result, _S. pneumoniae_ remains a major cause of bacterial infections, such as meningitis, pneumonia, and otitis media. In February 2010, PCV7 was licensed in Japan and was used on a voluntary basis until April 2013. During this period, the estimated rates of PCV7 vaccination for children <5 years of age increased from <10% in 2010 to 80%–90% in 2012 . In April 2013, use of PCV7 as a routine vaccine in Japan was approved, and in October 2013, vaccine for routine use was switched to PCV13.\n\n【1】To monitor the prevalence of different serotypes, sequence types (STs), and antimicrobial susceptibilities, we conducted a nationwide surveillance study of IPD and non-IPD cases in children in Japan during 2012–2017 . This passive nationwide surveillance was conducted by 254 medical institutions in Japan. During the first 3 years, 2012–2014, we detected decreased cases of PCV7 and PCV13 serotype pneumococcal infections and increased cases of serotype 24F IPD . During the next 3 years, 2015–2017, we detected markedly increased cases of serotype 12F IPD; 7 (4.4%) of 161 IPD isolates in 2015, 23 (13.9%) of 166 IPD isolates in 2016, and 42 (24.6%) of 171 IPD isolates in 2017 were classified as serotype 12F, although only 1 isolate classified as serotype 12F was detected during 2012–2014. Consequently, serotype 12F became the most prevalent serotype isolated from patients with IPD in 2017 in Japan . The mean age of these 73 IPD patients was 40.9 (range 3–126) months. Throughout the period, only 3 non-IPD serotype 12F isolates were detected out of a total of 231 non-IPD isolates. To clarify ST prevalence, penicillin-binding protein (PBP) profiles, resistance genes, and pili detection, we conducted whole-genome sequencing analysis of the serotype 12F isolates recovered in Japan. In addition, we used Bayesian-based phylogenetic analysis to investigate the dynamics of the spread.\n\n【2】### Materials and Methods\n\n【3】##### Bacterial Isolates\n\n【4】From 23 of 47 prefectures in Japan, we obtained 1 serotype 12F IPD isolate in 2013 and 72 IPD and 3 non-IPD isolates during 2015–2017. Of the 76 isolates, 1 did not grow from the stock medium; we thus analyzed all 75 remaining isolates. We tested the antimicrobial susceptibility of the isolates to penicillin, cefotaxime, meropenem, erythromycin, and levofloxacin by using the broth microdilution method according to the Clinical and Laboratory Standards Institute guidelines . We used the MIC interpretive criteria for meningitis for this study.\n\n【5】##### Basic Whole-Genome Sequencing Protocol\n\n【6】We extracted total genomic DNA and prepared sequence libraries by using a QIAamp DNA Mini Kit  and a Nextera XT DNA Library Preparation Kit . We multiplexed and sequenced the samples by using an Illumina NextSeq system for 300 cycles (2 × 150-bp paired-end). The resulting sequences were assembled by using SPAdes version 3.13.1 . Mapping was performed by using Burrows-Wheeler Aligner version 0.7.17  with _S. pneumoniae_ strain ASP0581 (serotype 12F-ST4846, National Center for Biotechnology Information reference sequence NZ\\_AP019192.2) . Isolates with a mapping depth <20.0 were excluded from subsequent analysis. Multilocus sequence typing was performed by extracting all alleles from the assembled contigs by using BLAST+ version 2.6.0  and a reference sequence of _S. pneumoniae_ G54 . A clonal complex was defined as a group of STs sharing 5 of 7 loci in the multilocus sequence typing results.\n\n【7】##### PBP Typing, Antimicrobial Resistance Genes, Pilus Detection, and Global Pneumococcal Sequence Cluster Identification\n\n【8】We assigned PBP transpeptidase domain type numbers to the extracted pbp _1a_ , _2b_ , and _2x_ transpeptidase domain sequences of the examined isolates. The type numbers originated from previously published US Centers for Disease Control and Prevention PBP types . PBP types that had not been previously published in the US Centers for Disease Control and Prevention database were labeled with the prefix JP (e.g. _pbp1a_ \\-JP1). Some of these original PBP types from Japan had been previously published . We detected _ermB_ , _ermTR_ , _mefA_ , _mefE_ , _tetM_ , _tetO_ , _rrgA-1_ (pili1), and _pitB-1_ (pili2) genes and searched for mutations and insertions/deletions within the _folA_ and _folP_ genes in the assembled contigs by following the standards published in a previous study . In addition, we assigned Global Pneumococcal Sequence Cluster (GPSC) numbers  and detected _tet_ (S/M) by using Pathogenwatch .\n\n【9】##### _Tn_ 916-like Integrative and Conjugative Element Analysis and _cps_ Locus Comparison\n\n【10】We extracted the sequences of _Tn_ 916-like integrative and conjugative elements (ICEs) from the assembled contigs by using BLAST+  and the _Enterococcus faecalis Tn_ 916 reference sequence . The analyzed sequences were annotated by using Prokka version 1.13.7 , and the structures of the regions were analyzed manually by using the Artemis Comparison Tool . In addition, we created a phylogenetic tree for the Tn916 region by using RAxML-ng version 0.9.0 . To obtain a phylogenetic tree of the _cps_ locus, we mapped the trimmed reads to the serotype 12F _cps_ locus reference sequence  and obtained a phylogenetic tree by using RAxML .\n\n【11】##### Recombination Site Detection and Phylogenetic Tree Construction\n\n【12】We constructed a phylogenetic tree by using Genealogies Unbiased By recomBinations In Nucleotide Sequences (Gubbins) version 2.2.1 . We mapped the obtained short reads to a the complete _S. pneumoniae_ ASP0581 reference sequence   and input the aligned sequences into Gubbins, which identifies recombination events by using an algorithm that iteratively identifies loci containing increased densities of base substitutions while concurrently constructing a phylogeny based on the putative point mutations outside of these regions.\n\n【13】##### Core-Genome Analysis\n\n【14】To clarify the differences in the genomic contents of the various clades, we used Prokka version 1.13.7 , Roary version 3.12.0 , and Scoary version 1.6.16  to perform core-genome analysis. We defined genes that were exclusively found in a cluster at p<0.01, obtained with the Fisher exact test followed by the Bonferroni correction, as being specific to the cluster.\n\n【15】##### Bayesian Analysis\n\n【16】We reconstructed a tree and obtained dates of the ancestors or nodes of the ST4846 and ST6945 clades by using the Bayesian Markov chain Monte Carlo framework. For this analysis, we performed recombination predictions by using the same protocol as described for all serotype 12F isolates. Final single-nucleotide polymorphism (SNP) alignments without recombination regions were used as the input dataset for BEAST version 1.10.4 . For the phylogeographic analysis, we used BEAUti  to additionally specify a symmetric discrete trait phylogeographic model by using a Bayesian stochastic search variable selection framework  as a metric for comparing geographic signals between datasets. We calculated Bayes factors indicating the transmission support by using SpreadD3 version 0.9.6 ; consistent with convention, support was defined as a Bayes factor >3 .\n\n【17】### Results\n\n【18】##### STs and Antimicrobial Susceptibilities\n\n【19】Among the serotype 12F isolates recovered in Japan, we identified 2 STs: ST4846 (n = 59), which was the major ST, and ST6945 (n = 16), which was a double-locus variant of ST4846. Penicillin MICs for all serotype 12F isolates were < 0.25 μg/mL. Of 59 ST4846 isolates, penicillin MICs for 16 isolates were < 0.06 (susceptible), for 42 were < 0.12 (resistant), and for 1 was < 0.25 (resistant) . Penicillin MICs for all but 1 of the 16 ST6945 isolates were < 0.06 (susceptible). Of the 74 serotype 12F isolates, cefotaxime MICs for 69 isolates were < 0.06 (susceptible), and meropenem MICs for 74 isolates were < 0.06 (susceptible). For most isolates, erythromycin MICs were >128 (resistant, 71/74 isolates) and levofloxacin MICs were < 1 (susceptible, 74/74 isolates).\n\n【20】##### Whole-Genome Sequencing Statistics\n\n【21】The average (± SD) number of contigs was 55.8 (± 15.6), N50 (shortest contig length needed to cover 50% of the genome) was 69,627 (± 12,462), and mapping depth was 106.1 (± 36.1) . One isolate had a mapping depth of 17.5 and was therefore excluded from the study.\n\n【22】##### PBP Typing, Antimicrobial Resistance Genes, and Pilus Detection\n\n【23】All serotype 12F isolates contained _pbp1a_ \\-37 . All ST4846 isolates had _pbp2b_ \\-JP14, and all ST6945 isolates contained _pbp2b-_ 4\\. We found 14 aa differences between _pbp2b_ \\-JP14 and _pbp2b_ \\-4. With regard to _pbp2x_ , 55 of 58 ST4846 isolates had _pbp2x_ \\-JP27 and all ST6945 isolates contained _pbp2x_ \\-23. We found 19 aa differences between _pbp2x_ \\-JP27 and _pbp2b_ \\-23. In total, 55 of 58 ST4846 isolates had _pbp1a_ : _pbp2b_ : _pbp2x_ , which equals 37:JP14:JP27, and all 16 ST6945 isolates contained _pbp1a_ : _pbp2b_ : _pbp2x_ , which equals 37:4:23. All serotype 12F isolates had _tetM_ and _ermB_ with the exception of 3 ST6945 isolates that had only _tetM_ . Of 58 ST4846 isolates, 52 carried the _folA_ I100L mutation, but this mutation was not found in any of the ST6945 isolates. In addition, all ST4846 isolates had _folP_ insertions, and none of the ST6945 isolates had mutations in this gene. None of the serotype 12F isolates carried _tetO_ , _tet_ (S/M), _ermTR_ , _mefA_ / _E_ , Pili1, or Pili2. All serotype 12F isolates were assigned to GPSC334. Three isolates of GPSC334 were present in the GPSC database: 1 serotype 3 ST6945 isolate from Hong Kong and 2 serotype 12F isolates, ST1820 and ST1527, from Poland. Those 3 isolates were clustered into the ST6945 cluster in a subsequent recombination site-censored phylogenetic tree .\n\n【24】##### _Tn_ 916-like ICE Structure and _cps_ Locus Analysis\n\n【25】All serotype 12F isolates tested in this study had a _Tn_ 916-like ICE with _tetM_ , which encodes tetracycline resistance. Of 58 ST4846 isolates, 57 had _Tn_ 6002 , which was found in a previous study to be one of the most common _Tn_ 916-like ICEs containing erythromycin-resistance cassettes between open reading frames 19 and 20 of _Tn_ 916 . We did not obtain a completely connected contig throughout the whole length of the _Tn_ 916-like ICE region for another ST4846 isolate. With regard to ST6945 isolates, we did not obtain completely connected contigs throughout the whole length of the _Tn_ 916-like ICE region. However, 13 of 16 ST6945 isolates had _ermB_ insertions at the same position within the partial _Tn_ 916-like ICE as ST4846 isolates. The phylogenetic tree that was created by using the _cps_ locus sequences generated a ST6945-specific clade that included all ST6945 isolates .\n\n【26】##### Recombination Site Prediction, Phylogenetic Tree Construction, and Bayesian Analysis\n\n【27】When we constructed a recombination site censored phylogenetic tree of all serotype 12F isolates after recombination site prediction by using Gubbins , the tree identified 2 clusters composed exclusively of the ST4846 and ST6945 isolates. The average value of the pairwise SNP distances between isolates of the 2 clusters was 391, and the _r/m_ value (average recombination to mutation rate) of the ST4846 clade was 0.3213 and of the ST6945 clade was 0.9639. One of the recombination sites overlapped with part of the nucleotide sequence of _pbp2b_ . In addition, another recombination site overlapped with part of the _pbp2x_ nucleotide sequence . No recombination site was found in the _cps_ locus. We then performed a Bayesian analysis to estimate the time of most recent common ancestor (TMRCA) of the serotype 12F-CC4846 isolates by using BEAST. This analysis showed that serotype 12F-CC4846 in Japan arose in »1942 (95% highest posterior density \\[HPD\\] 1914–1963) . In addition, 71 genes were unique to ST4846 isolates and 71 other genes were unique to ST6945 isolates. Although 16 of the 71 genes that were unique to ST6945 isolates did not exist in _S. pneumoniae_ ASP0581, none of the 142 gene regions overlapped with the recombination regions predicted by Gubbins. We next used BEAST to estimate the TMRCA of the serotype 12F-ST4846 isolates based on only those belonging to the ST4846 clade. This analysis suggested that a common ancestor for our serotype 12F-ST4846 isolates arose in »2005 (95% HPD 2002–2009)  and generated 2 clades; the major clade (n = 44; 2015, 5/7 isolates; 2016, 17/23 isolates; 2017, 23/42 isolates), PC-JP12F, which arose in »2012 (95% HPD 2011–2013), had 5 genes that were lacking in the other ST4846 isolates . This result indicated that the isolates included in the PC-JP12F clade spread rapidly in Japan; therefore, we conducted a phylogeographic analysis of the isolates to clarify the transmission over time. This analysis revealed 5 statistically supported (Bayes factor >3) routes of transmission between 6 discrete regions in Japan . All of the supported transmission routes originated from the Kanto region, which is the central populated region of Japan, and spread to all 5 other local regions. The highest support was obtained for transmission from the Kanto region to the Tokai region (Bayes factor 197.2), which is contiguous to the Kanto region.\n\n【28】The TMRCA of serotype 12F-ST6945 isolates, estimated by using only ST6945 isolates, was 1997 (95% HPD 1925–2005). The phylogenetic tree of the _Tn_ 916-like ICE region generated ST6945-specific and PC-JP12F–specific clades . In addition, all _ermB-_ negative ST6945 isolates created a subclade within the ST6945-specific clade.\n\n【29】### Discussion\n\n【30】In our nationwide surveillance study of pneumococcal disease in children, conducted in Japan during 2012–2017, we detected a rapid increase in serotype 12F IPD. Thus, we performed a whole-genome sequencing–based molecular analysis to clarify the associated genomic characteristics and their dynamics. We found 2 lineage STs within the serotype 12F isolates recovered in Japan: ST4846, which was the major sequence type, and ST6945, which was a double-locus variant of ST4846. To investigate whether these 2 STs had the same ancestor, we compared their genetic characteristics by whole-genome sequencing. According to the STs and the finding that both ST4846 and ST6945 isolates belonged to GPSC334, these isolates appear to be closely related. Although this GPSC334 was a minor cluster in the original study and the isolates from East Asia used in the study were limited , the average pairwise SNP distance between the 2 ST isolates was 391; this value is reasonable based on the evidence that the ST4846 and ST6945 isolates belonged to the same sequence cluster in the original study. However, we found several genetic differences between the 2 lineage STs, such as differences in the PBP profile, the prevalence of _folA_ mutations and _folP_ insertions, the _Tn_ 916 structure, and the _cps_ locus sequences. In addition, we found 142 genetic differences between the 2 STs in the core-genome analysis. In general, _S. pneumoniae_ is a paradigm for recombination, and in our study, we certainly identified recombination sites, particularly in the ST6945 cluster. Therefore, we believe that these recombination events caused the discrepancy after its divergence in »1942. Of note, recombination sites were not identified in the _cps_ locus although the phylogenetic tree for the _cps_ locus generated ST-specific clades. Considering that the process of evolving from a common ancestor to 2 distinctive clades is a result of randomly accumulated mutations, recombination events, or both, there might be ST-specific genetic backgrounds that influenced the dynamics of the _cps_ region.\n\n【31】We found a major clade within the ST4846 isolates (i.e. PC-JP12F clade) that seemed to spread rapidly in Japan. Bayesian analysis suggested that this clade arose in »2012. This estimation showed a narrow 95% HPD, and we therefore believe that this estimated date was reliable. Thus, the rapid spread and high prevalence of serotype 12F-CC4846 in Japan appeared be mainly caused by this strain. In addition, the phylogeographic analysis suggested the route of transmission of this strain, which mainly involved spread from the Kanto region to other local regions. The Kanto region has 7 prefectures, including Tokyo, the capital of Japan, which contains »35% of the population of Japan and is thus the most populated of all regions in Japan. In general, in the phylogeographic analysis, Bayes factors >100 indicate decisive, 30–100 indicate very strong, 10–30 indicate strong, and 3–10 indicate substantial support for a model . Therefore, we believe that the determined transmission routes in Japan (i.e. from the urban region to countryside regions) were reasonable and reliable.\n\n【32】To date, 3 studies have demonstrated outbreaks of serotype 12F-CC4846 in Japan during 2016–2018 . Of the 3 outbreaks, 2 can be attributed to ST4846 isolates that occurred in the Chiba Prefecture in the Kanto region and in the Yamagata Prefecture in the Tohoku region. The other outbreak was attributable to ST6945 isolates in the Hyogo Prefecture in the Kinki region; in our study, ST6945 isolates were also recovered in the Kinki region . In addition, Shimbashi et al. reported that although the data were obtained from a pneumococcal surveillance study among adults, the ST6945 isolates were recovered in the Tokai, Kyusyu, and Tohoku regions during 2015–2017 . Given these findings, the serotype 12F-ST4846 isolates had already spread throughout Japan, and the serotype 12F-ST6945 isolates were still limited to several small regions but had already started spreading in Japan.\n\n【33】Serotype 12F isolates were recovered mainly from patients with IPD, including outbreak cases, in many areas globally after the introduction of PCV13 ; several studies have demonstrated that serotype 12F is associated with high morbidity and mortality rates . The STs of serotype 12F isolates in different countries exhibit differences, which indicates that prevalence of these serotype 12F isolates was not the result of global but rather of regional clonal spread. In addition, Gladstone et al. and Balsells et al. found invasiveness to be relatively higher for serotype 12F pneumococci than for other serotypes . Of note, according to the GPSC database , all serotype 12F isolates are susceptible or mildly resistant to penicillin (MIC < 0.12), similar to the results obtained for the isolates included in our study. Therefore, it is unlikely that antibiotic pressure caused the spread in Japan and other regions. Considering these findings, serotype 12F should exhibit high invasiveness and probably has the potential to be transmitted efficiently and thus cause an outbreak or regional spread. With regard to this issue, the mechanism underlying the rapid spread and high invasiveness of serotype 12F strains should be determined in future studies.\n\n【34】In this study, we identified the structures of _Tn_ 916-like ICEs in serotype 12F-CC4846. Most serotype 12F-ST4846 isolates contained _Tn_ 6002, which was also widely detected in serotype 15A-CC63 isolates in Japan . The phylogenetic tree of the _Tn_ 916-like ICE region suggested that the origins of the _Tn_ 916-ICE region in PC-JP12F isolates might differ from those of the other serotype 12F-CC4846 isolates. However, we should note that the genetic difference might be caused by mapping errors (i.e. the choice of reference sequence). In Japan, the macrolide resistance rate was >90% ; thus, further studies on _Tn_ 916-like ICEs, including its epidemiology, transmission mechanism, and functions that influence the dynamics of pneumococci, are needed.\n\n【35】We note some limitations of this study. First, we tested pneumococcal isolates that were collected in a nationwide surveillance study during 2012–2017. However, all isolates were recovered during 2015–2017, except for 1 that was recovered in 2013. Therefore, this short sampling period might affect the molecular clock analysis (i.e. TMRCA estimation). The TMRCAs of the serotype 12F-CC4846 isolates  and ST6945  had very broad 95% HPDs. Therefore, we believe that longer samplings are needed to more accurately estimate the dates. In contrast, the TMRCA of ST4846 was estimated with a narrow 95% HPD. Thus, we believe that the results of the date estimations and the subsequent analyses  were robust and reliable.\n\n【36】In conclusion, we found rapid spread of serotype 12F-CC4846 isolates among patients with IPD in Japan after the introduction of PCV13. The results identified ST4846 and ST6945 (double-locus variant of ST4846) lineages for the serotype 12F-CC4846 isolates in Japan, but many genetic differences were found between the 2 lineages. Bayesian analysis identified a major cluster within the ST4846 isolates (PC-JP12F cluster). This PC-JP12F cluster arose in »2012 and rapidly spread from the Kanto region to countryside regions. As we showed in this study, _S. pneumoniae_ serotype 12F lineages could have the potential to spread rapidly; therefore, we should monitor the trend of the lineages when they are detected.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eb21f675-9bdf-451f-ac82-4a2239bac793", "title": "Schistosomiasis Screening of Travelers to Corsica, France", "text": "【0】Schistosomiasis Screening of Travelers to Corsica, France\n**To the editor:** As members of the French Ministry of Health Working Group on autochthonous urinary schistosomiasis, we read with interest the 2 recently published articles regarding schistosomiasis screening of travelers to Corsica, France . Surprisingly, the authors of both articles lacked evidence to support the diagnosis of schistosomiasis in most of what they referred to as confirmed cases. The diagnostic standard for confirmation of urinary schistosomiasis is identification of eggs by microscopic examination of urine samples . If this criterion were applied in both reports, only 1 patient of the 7 allegedly confirmed cases would actually be confirmed.\n\n【1】The low sensitivity of microscopy is well known. Therefore, different serologic tests have been developed, including Western blot (WB). In the study based on travelers from Italy , the SCHISTO II WB IgG test (LDBIO Diagnostics, Lyon, France) was used. This test, available since 2015, is based on both _S_ c _histosoma haematobium_ and _S. mansoni_ antigens and has not been evaluated by anyone other than the manufacturer. Moreover, the authors did not report any details regarding the molecular weight and number of specific bands observed on the strip.\n\n【2】In the study by authors from the GeoSentinel Surveillance Network , both cases that could have been infected after 2013, since exposure occurred only in 2014, and 4 cases which reported bathing in rivers in Corsica other than the Cavu River had just 1 weakly positive serologic screening test. Hence, irrespective of the criteria for a confirmed case of schistosomiasis described above, it appears difficult to conclude that confirmation could rely on only 1 positive serologic test, even a WB.\n\n【3】Altogether, these 2 studies identified only 1 patient with parasitological evidence of infection that was attributable to the already known 2013 focus in Cavu River. Therefore, these articles do not provide evidence of transmission of schistosomiasis in Corsica after 2013 or outside the Cavu River.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9a6acf4a-07aa-44b6-a505-43b9d14fe926", "title": "Role of Chlamydia trachomatis in Miscarriage", "text": "【0】Role of Chlamydia trachomatis in Miscarriage\nThe incidence of _Chlamydia trachomatis_ infection has dramatically increased during the past 10 years . Mostly asymptomatic, untreated _C. trachomatis_ infections are responsible for a large proportion of salpingitis, pelvic inflammatory disease, ectopic pregnancy, and infertility in women. _C. trachomatis_ is a recognized agent of preterm labor and premature rupture of membranes . However, its role in miscarriage is unclear .\n\n【1】_C. trachomatis_ has been isolated or detected in cervical smear, urine , or products of conception . Nevertheless, none of these studies demonstrated association between isolation of _C. trachomatis_ and miscarriage. However, culturing _C. trachomatis_ is technically difficult, given its strict intracellular life cycle. Even with molecular approaches, detecting _C. trachomatis_ can be difficult because of PCR inhibitors or low number of copies often present in the lesions . Moreover, infection could be localized at deeper sites not amenable to sampling .\n\n【2】Several studies have reported a higher prevalence of _C. trachomatis_ antibodies in spontaneous  or recurrent  miscarriages. The inability to detect immunoglobulin (Ig) M or to isolate _C. trachomatis_ from any of these seropositive patients might suggest that _Chlamydia_ spp. are not directly associated with miscarriage . Other seroepidemiologic studies have failed to find any correlation between _C. trachomatis_ and spontaneous  or recurrent miscarriage .\n\n【3】The main purpose of this study was to investigate whether _C. trachomatis_ is associated with miscarriage. We used molecular, serologic, and immunohistochemical approaches to compare evidence of present and past _C. trachomatis_ infection in women with or without miscarriage.\n\n【4】### Materials and Methods\n\n【5】During November 2006–June 2009, a total of 386 women were prospectively enrolled at the obstetric department of the University Hospital of Lausanne (Lausanne, Switzerland). The miscarriage group comprised 125 women consulting at the emergency gynecology ward for an acute miscarriage. The control group comprised 261 women attending the labor ward with an uneventful pregnancy and without any history of miscarriage, stillbirth, or preterm labor. All women gave written consent, and the local ethical committee approved the study.\n\n【6】We collected demographic and obstetric data prospectively. Placenta (or products of conception in cases of miscarriage), cervicovaginal swab specimens, and serum were sampled at the time of labor and of miscarriage.\n\n【7】All serum samples were tested for IgG and IgA against _C. trachomatis_ with the Ridascreen _Chlamydia_ IgG/IgA Kit (R-biopharm, Darmstadt, Germany) according to the manufacturer’s instructions and by using Dynex DSX (Magellan Biosciences, Chantilly, VA, USA). Cervicovaginal swabs and placenta were extracted by using QIAamp DNA Mini kit (QIAGEN, Hilden, Germany). Samples were screened for _C. trachomatis_ DNA by using a TaqMan real-time PCR specific for the cryptic plasmid of _C. trachomatis_ , as described . A PCR inhibition control was used to verify that absence of amplification was not caused by PCR inhibitors. Only 1 of the 386 PCR inhibition controls was negative. This sample was thus retested at a 1:10 dilution.\n\n【8】Hematoxylin and eosin–stained histologic sections of all placentas were investigated for deciduitis, vasculitis, endometritis, or chorioamnionitis. Histologic samples were read blindly by a pedopathologist (M.-C.O.). Samples positive for _C. trachomatis_ by real-time PCR were tested by immunohistochemical analysis (IHC). Presence of _C. trachomatis_ on histologic sections was assessed by using a specific mouse monoclonal antibody, as described . To test the placental specimens, we used a commercial _Chlamydiaceae_ family–specific monoclonal antibody directed against the chlamydial lipopolysaccharide (clone AC1-P; Progen, Heidelberg, Germany) at a dilution of 1:200. Detection was performed with the Dako ChemMate detection Kit (Dako, Glostrup, Denmark) according to the manufacturer’s instructions. Antigen retrieval was performed by 10-min enzyme digestion (proteinase K; Dako). Immersion of the slides in peroxidase-blocking solution for 5 min at room temperature resulted in blocking of endogenous peroxidase activity. Specimens were incubated with primary antibody for 1 h. Sections were incubated for 10 min at room temperature with the link-antibody (Dako), followed by 10 min incubation with horseradish peroxidase (Dako) and finally developed in 3-amino, 9-ethyl-carbazole substrate solution for 10 min at room temperature and counterstained with hematoxylin. Using the antibody diluent instead of the primary antibody, we performed a negative control of each section. Moreover, 8 placentas from _C. trachomatis_ PCR-negative patients were randomly selected as negative controls. IHC was blindly read by 2 pathologists with experience in chlamydial IHC (S.B. N.B.) and confirmed by a pedopathologist (M.-C.O.).\n\n【9】We compared demographic data and risk factors of patients with and without miscarriage or _C. trachomatis_ infection by the Pearson χ 2  test (or the Fisher exact test when indicated) for categorical variables. For continuous variables, medians were compared by the Wilcoxon-Mann-Whitney test. Multivariate logistic regression was performed to identify factors independently associated with miscarriage or with _C. trachomatis_ infection. Statistical analyses were performed by using Stata version 10.0 (StataCorp LP, College Station, TX, USA).\n\n【10】### Results\n\n【11】Of 395 patients, 9 (2.3%) were excluded because of missing serum or vaginal swab samples. Sociodemographic data for the remaining 386 women are shown in Table 1 .\n\n【12】A total of 16 (4.2%) patients were positive for IgG and IgA against _C. trachomatis_ , 22 (5.7%) were positive only for IgG against _C. trachomatis_ , and 4 (1.0%) were positive only for IgA against _C. trachomatis_ . Prevalence of IgG against _C. trachomatis_ was higher in the miscarriage group (15.2%) than in the control group (7.3%; p = 0.018) . This association between miscarriage and IgG against _C. trachomatis_ remained significant, even after adjustment for age, origin, education, and number of sex partner (odds ratio \\[OR\\] 2.3, 95% confidence interval \\[CI\\] 1.1–5.1). Similarly, prevalence of IgA against _C. trachomatis_ was higher in the miscarriage group (8.0%) than in the control group (3.8%), but this trend was not significant (p = 0.091) by univariate analysis. When adjusted for age, origin, education, and number of sex partners, the association between miscarriage and IgA against _C. trachomatis_ was significant (OR 2.7, 95% CI 1.1–7.4).\n\n【13】Multivariate logistic regression including all sociodemographic variables  and _C. trachomatis_ IgG serologic results identified 5 independent factors positively or negatively associated with miscarriage: _C. trachomatis_ IgG–positive serologic results (OR 2.3, 95% CI 1.1–4.9), age \\> 35 years (OR 2.7, 95% CI 1.6–4.4), European origin (OR 0.3, 95% CI 0.2–0.5), marriage (OR 0.4, 95% CI 0.2–0.7), and 1 lifetime sex partner (OR 0.4, 95% CI 0.2–0.7).\n\n【14】_C. trachomatis_ DNA was more frequently amplified from products of conception or placenta from women with miscarriage (5 \\[4.0%\\] women) than from controls (2 \\[0.7%\\], p = 0.026). Most patients with a positive PCR result for placenta also had a positive result for vaginal swab specimens . Six of the 7 patients with _C. trachomatis_ DNA in the cervicovaginal swab specimen also had positive findings in the placenta. Thus, again, cervicovaginal _C. trachomatis_ DNA was more often detected in patients from the miscarriage group (n = 5, 4.0%) than from the control group (n = 2, 0.7%; p = 0.026). All 7 patients with _C. trachomatis_ DNA in the cervicovaginal swab also exhibited IgG against _C. trachomatis_ , whereas all patients but 1 with _C. trachomatis_ DNA in the placenta exhibited IgG against _C. trachomatis_ . Both patients with _C. trachomatis_ DNA and IgG and IgA against _C. trachomatis_ belonged to the miscarriage group.\n\n【15】All placentas were analyzed for inflammation . In the basal plate, inflammatory cells (deciduitis) were present in 15 (39.5%) of 38 patients and 91 (26.1%) of 348 patients with and without _C. trachomatis_ IgG–positive serologic results, respectively (p = 0.081). This trend was observed to a lesser extent when _C. trachomatis_ IgA serologic results were considered (7 \\[35.0%\\] of 20 vs. 99 \\[26.3%\\] of 376; p = 0.446).\n\n【16】All 8 persons with samples positive for _C. trachomatis_ by real-time PCR in the placenta (n = 7) or cervicovaginal swab specimen (n = 7) were tested by IHC . _C. trachomatis_ was confirmed in 4 of 6 placentas from women with miscarriage and in 1 of 2 placentas from women with uneventful pregnancies, whereas none of the 8 _C. trachomatis_ DNA–negative controls randomly selected exhibited the bacteria by IHC. _C. trachomatis_ predominantly localized around endometrial glands of the chorion , associated with different degree of inflammation .\n\n【17】We also compared characteristics of patients with (n = 38) and without (n = 348) _C. trachomatis_ IgG–positive serologic results. Number of pregnancies, parity, marital status, education, number of lifeltime sex partners, and smoking status were all associated with _C. trachomatis_ IgG–positive serologic results by univariate analysis. Women who declined to provide information on the number of sex partners had a _C. trachomatis_ IgG prevalence of 12.2%, whereas none of the 95 women who reported having 1 sex partner had _C. trachomatis_ IgG–positive serologic result. In multivariate analyses, independent factors positively or negatively associated with _C. trachomatis_ IgG–positive serologic results were \\> 2 lifetime sex partners (OR 3.3, 95% CI 1.4–7.7), divorced women (OR 4.9, 95% CI 1.7–14.3), European origin (OR 0.4, 95% CI 0.2–0.9), and attending a university (OR 0.2, 95% CI 0.1–0.6). Age and smoking were not independently associated with _C. trachomatis_ IgG–positive serologic results.\n\n【18】### Discussion\n\n【19】We found an association of spontaneous miscarriage with serologic (p = 0.018) and molecular (p = 0.026) evidence of _C. trachomatis_ infection. Moreover, _C. trachomatis_ in the placenta was documented by specific IHC. _C. trachomatis_ was mainly localized in the epithelial cells of endometrial glands.\n\n【20】Several studies have failed to document an association between _C. trachomatis_ and spontaneous  or recurrent miscarriage . However, these studies were conducted >10 years ago, i.e. before the recent dramatic increase in the prevalence and incidence of _C. trachomatis_ infection . Because of improved statistical power, such increased prevalence might indicate an association between _C. trachomatis_ infection and adverse pregnancy outcomes. Second, sensibility and specificity of diagnostic methods have also improved during the past decade. Thus, the high _C. trachomatis_ seroprevalence observed in the control group of several older studies, ranging from 28% to 53%  was likely to have resulted from a low specificity of the serologic test used at that time. The _Chlamydia_ IgG/IgA kit from R-biopharm we used in the present study exhibited better specificity than did 4 other commercially available tests for detecting IgG against _C. trachomatis_  and is thus more likely to identify a slight but true association. Moreover, the sensitivity of the _C. trachomatis_ TaqMan real-time PCR we used here is high, detecting even <10 DNA copies. This validated assay also detects strains that contain a recently identified 350-bp deletion in the cryptic plasmid  because the 71-bp DNA fragment amplified is 93 bp downstream from the deletion .\n\n【21】The serologic association we observed is unlikely to be due to cross-reactivity with other chlamydial species such as _C. abortus_ (previously classified as _C. psittacci_ senso lato) because we also observed a molecular association with miscarriage. Moreover, the PCR we used was specific at species level because the _C. abortus_ genome contains no cryptic plasmid. Finally, _C. abortus_ has been only infrequently associated with miscarriages in humans , mostly after zoonotic exposure.\n\n【22】Miscarriage could be induced by a persistent asymptomatic _C. trachomatis_ infection spreading to the fetal tissue or endometrium. Relatively few miscarriages occur during _C. trachomatis_ primary infection, which explains the absence of association with IgA. That several patients exhibited _C. trachomatis_ –positive serologic results without _C. trachomatis_ DNA suggests that miscarriage might also occasionally be induced by damage from a past chlamydial infection or persistent _C. trachomatis_ antibodies that might interfere with embryonic antigens .\n\n【23】A limitation of our study was the absence of investigation of other infectious etiology of miscarriage. Some viruses can produce chronic or recurrent maternal infection. In particular, cytomegalovirus during pregnancy can reach the placenta by hematogenous spread or by ascending route from the cervix. Parvoviruses also have been implicated in the development of repeated fetal loss. Among bacterial infections, _Ureaplasma urealyticum_ , _Mycoplasma hominis_ , and bacterial vaginosis have been mostly associated with miscarriages . In addition, several intracellular bacteria such as _Coxiella burnettii_ , _Brucella abortus_ , and _Waddlia chondrophila_  have been associated with miscarriage.\n\n【24】Our study shows an association between miscarriage and molecular and serologic evidence of _C. trachomatis_ infection. Several previous studies failed to document such an association probably because of the limited number of patients in some of these studies resulting from the lower prevalence of _C. trachomatis_ infection in the late 20th century and to lower sensitivity or specificity of diagnostic methods available at that time. The results of our study suggest that all women experiencing a miscarriage should be screened for _C. trachomatis_ infection and, if positive, adequately treated to prevent recurrent miscarriages. Moreover, preconceptional screening might be proposed to reduce the prevalence of this adverse pregnancy outcome.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "015fe3a7-f673-4ff5-a65e-37919bb34b97", "title": "High Diversity of RNA Viruses in Rodents, Ethiopia", "text": "【0】High Diversity of RNA Viruses in Rodents, Ethiopia\nMost emerging infectious diseases of humans or domestic animals are zoonoses, and among emerging pathogens, RNA viruses are highly represented . The synanthropic nature of some rodent species makes them important reservoirs of RNA viruses pathogenic to humans, such as hantaviruses (e.g. Seoul virus in black and Norway rats worldwide) and arenaviruses (e.g. Lassa virus in the multimammate mouse in western Africa or lymphocytic choriomeningitis virus in the house mouse worldwide). In Africa, members of the rodent genera _Mastomys_ and _Arvicanthis_ are linked to human activity; these rodents are widespread throughout sub-Saharan Africa and are crop pests and zoonotic reservoirs for human pathogens. Histories of synanthropy are likely longest for rodents in areas of early human sedentism, making RNA virus richness in early centers of domestication such as the Ethiopian Highlands of particular interest.\n\n【1】Hantaviruses (family _Bunyaviridae_ ) are RNA viruses primarily carried by rodents and soricomorphs (shrews and moles), although 2 new species have recently been described in bats . Arenaviruses (family _Arenaviridae_ ) are primarily rodent-borne RNA viruses. Members of both genera can cause life-threatening diseases in humans: arenaviruses cause hemorrhagic fevers in the Americas and Africa, and hantaviruses cause hemorrhagic fever with renal syndrome in Asia and Europe and hantavirus cardiopulmonary syndrome in the Americas. In Africa, only Lassa and Lujo arenaviruses are known to be highly pathogenic to humans. In contrast, hantaviruses have not yet been found to cause life-threatening human diseases in Africa, but hantavirus-specific antibodies have been found in human serum samples from several countries in Africa . To investigate the role of synanthropic small mammals as potential reservoirs of emerging pathogens in Ethiopia, we sampled rodent and shrew species in areas near human habitations and screened them for hantavirus and arenavirus RNA.\n\n【2】### The Study\n\n【3】Small mammals from domestic and peridomestic areas were trapped during August–December 2010 in 2 high-altitude localities, Golgolnaele (13°52′N, 39°43′E, elevation 2,700 m) and Mahbere Silassie (13°39′N, 39°08′E, elevation 2,600 m), and in 1 lower-altitude locality, Aroresha (12°25′N, 39°33′E, elevation 1,600 m), in the Tigray region of the Ethiopian Highlands. Kidney samples preserved in RNAlater reagent (QIAGEN, Hilden, Germany) and stored at –80°C were used for total RNA extraction by using the NucleoSpin RNA II Kit (Macherey-Nagel, Düren, Germany). Samples were pooled in pairs by locality and host species. RNA was reverse transcribed by using random hexamers as primers. Screening for arenaviruses was performed by using a pan–Old World arenavirus PCR targeting the large (L) gene . Screening for hantaviruses was performed by using a nested PCR assay targeting the hantavirus L gene .\n\n【4】A total of 201 small mammals from 6 species were screened for arenaviruses and hantaviruses . Among them, 1 Ethiopian white-footed mouse ( _Stenocephalemys albipes_ ) from Golgolnaele and 2 Awash multimammate mice ( _Mastomys awashensis_ ) from Aroresha were positive for arenavirus RNA; 10 white-footed mice from the 2 highland localities (6 from Golgolnaele, 4 from Mahbere Silassie) were positive for hantavirus RNA. Amplicons were purified and sequenced, and nucleotide sequences were aligned on the basis of the amino acid alignment. Phylogenetic analyses were performed on the nucleotide sequences by using a maximum-likelihood (ML) approach .\n\n【5】After sequencing of the 3 arenavirus-positive samples, 3 distinct arenavirus sequences were obtained, and an ML tree was constructed for these 3 arenavirus sequences and the partial L gene (340 bp) of representatives of Old World arenaviruses . The 3 sequences cluster with Mobala virus (80% bootstrap support), an arenavirus discovered in _Praomys_ sp. in the Central African Republic in 1983 . However, the 3 sequences from Ethiopia are not monophyletic; the 2 sequences from multimammate mice cluster together (94% bootstrap support), but the sequence from the white-footed mouse from Golgolnaele is basal to the clade (Mobala + _M. awashensis_ virus sequences), with the Menekre virus, found in _Hylomyscus_ sp. in Guinea , as outgroup. The sequences from multimammate mice on average differ from those of Mobala virus and the sequence from the white-footed mouse by the same order of magnitude in terms of amino acids: 5.0 ± 2.1% and 5.9 ± 2.2%, respectively. The average amino acid difference between the sequence from the white-footed mouse and that from Mobala virus was 8.1 ± 2.6%. Therefore, these arenaviruses seem to be 2 strains of Mobala virus carried by 2 rodent species and found in 2 localities ≈250 km apart from each other and with an altitude difference of 1,100 m.\n\n【6】After sequencing of the 10 hantavirus-positive samples, 4 distinct hantavirus sequences were obtained, 2 from Golgonaele and 2 from Mahbere Silassie. Figure 2 shows the ML tree for these 4 sequences and the partial L gene (347 bp) of representatives of hantaviruses. The tree is not well resolved, and shrew- and mole-associated hantaviruses do not cluster. Two rodent-associated clades are supported: the previously known Murinae-associated hantaviruses (69% bootstrap support) and the Cricetidae-associated hantaviruses (92% bootstrap support, with 1 exception, Rockport, Soricomorpha-associated hantavirus). Although all 4 sequences were found in _S. albipes_ mice, a Murinae species endemic to Ethiopia, they do not group with the Murinae-associated hantaviruses or with hantaviruses found in other African small mammals, such as bats  or shrews . The 4 sequences form a unique, divergent clade with the 2 sequences from Mahbere Silassie basal to the sequences from Golgonaele, which cluster together. The average amino acid difference between the sequences from Ethiopia and those from Murinae-associated hantaviruses was 27.0 ± 4.0%. Because the new amino acid sequences are at least 21.0 ± 4.0% divergent from those of other hantaviruses, we conclude that _S. albipes_ mice are carrying a novel hantavirus. We propose the name Tigray virus for this virus because it was found in the Tigray region of Ethiopia. Additional genetic characterization, in particular of the small and medium segments, will be conducted to further clarify the evolutionary relationship of this virus within the hantavirus genus.\n\n【7】### Conclusions\n\n【8】Two rodent species living in close proximity to human settlements in Ethiopia are carriers of arenaviruses and hantaviruses. Recently, several new arenaviruses and hantaviruses have been described in small mammals in Africa, but no clear association with human diseases has been found . However, arenavirus and hantavirus infections are likely severely underreported because symptoms may resemble those of many other febrile infections . Investigating the presence of antibodies for Mobala virus and the proposed Tigray virus in humans in the Ethiopian Highlands is the next step in evaluating their pathogenicity. A recent study in Guinea showed that 2/68 patients with fever of unknown origin had antibodies for Sangassou hantavirus ; a case of putative hantavirus disease (hemorrhagic fever with renal syndrome) was also reported in the Central African Republic . Hantavirus infections may thus be an unrecognized medical problem in Africa and deserve more attention.\n\n【9】In conclusion, our screening of 201 small mammals led to the identification of 2 novel strains of Mobala arenavirus and a novel hantavirus in 2 rodent species found in Ethiopia, _M. awashensis_ and _S. albipes_ . These rodents belong to the exclusively African Praomyini tribe , which hosts 5/11 arenaviruses (Lassa, Mopeia, Luna, Mobala, and Menekre viruses) and the only Murinae-associated hantavirus (Sangassou virus) described in Africa. Our results support a major role for Praomyini as hosts in the evolutionary history of arenaviruses and hantaviruses in Africa.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "80367c9e-7e5b-48fa-9d84-3d5cbc54c541", "title": "Natural Reassortment of Eurasian Avian-Like Swine H1N1 and Avian H9N2 Influenza Viruses in Pigs, China", "text": "【0】Natural Reassortment of Eurasian Avian-Like Swine H1N1 and Avian H9N2 Influenza Viruses in Pigs, China\nSwine are regarded as a mixing vessel for influenza A viruses (IAVs) . Avian, swine, and human IAVs can co-infect pigs and generate novel reassortants of zoonotic or pandemic potential. The emergence of pandemic H1N1 IAV (pH1N1), containing viral segments from avian, swine, and human viruses, highlighted the key role of pigs in contributing to IAV reassortment and evolution . Research in China also showed evidence of avian H5, H7, H9, and H10 influenza infections in pigs . Avian IAVs linked to human infection in this region contained internal genes derived from avian H9N2 viruses, indicating that the internal genes of the H9N2 virus might aid zoonotic transmission . We report detection of a swine IAV with polymerase basic (PB) 1 and matrix (M) gene segments of avian H9N2 origin.\n\n【1】In April 2021, we resumed monthly influenza surveillance program of imported pigs in a local slaughterhouse, which had been interrupted by COVID-19 outbreaks . We collected individual nasal swab samples (≈75 samples per visit), which we kept chilled in virus transport medium until they reached the laboratory. We then subjected swab samples to IAV isolation by using MDCK cells, as previously described . We identified cultures with cytopathic effect and tested them using a standard hemagglutination assay with turkey red blood cells. We tested hemagglutination-positive cultures with a universal influenza reverse transcription PCR assay specific for M segments . We studied samples that were positive for this reaction by using next-generation sequencing to deduce the full virus genomes .\n\n【2】During April 2021–February 2022, we collected a total of 829 porcine nasal swab samples . We isolated 8 IAVs: 7 from August 2021 and 1 from September 2021. Virus sequences deduced from this study are available from GISAID (isolate nos. EPI\\_ISL\\_12471293–300). We compared those sequences with reference sequences . IAVs detected in August 2021 were H3N2 viruses. The hemagglutinin (HA) and neuraminidase (NA) segments of those viruses were associated with human-like H3N2 swine influenza A virus; however, their internal gene segments all were derived from the pH1N1 lineage . These viruses were genetically not identical but highly similar. The influenza-positive pigs came from farms located in 2 provinces across southern China. Because this slaughterhouse followed a daily clearance policy requiring that all imported live pigs be slaughtered within 24 hours of admittance, our results suggest influenza transmission between pigs in the preslaughter transport chain outside Hong Kong. This H3N2 genotype was previously detected in pigs from Guangxi, China .\n\n【3】The swine H1N1 IAV that we isolated in September 2021, A/swine/HK/NS419/2021, a reassortant between multiple swine influenza lineages . The PB1 and M gene segments of this virus are of avian H9N2 virus subtype. This virus contains PB2, polymerase acidic, and NA gene segments derived from the pH1N1 lineage. Its HA and NA gene segments are of Eurasian avian-like H1N1 lineage, and its nonstructural gene segment is of a triple reassortant lineage. We further purified the isolated virus by using plaque assays to exclude the possibility of a mixed infection. We confirmed that all plaque-purified viral clones had an identical genotype.\n\n【4】The A/swine/HK/NS419/2021 isolate featured a PB1 gene segment of SH/F/98‐like lineage and an M gene segment of G1-like H9N2 lineage . Similar PB1 and M sequences have been detected in zoonotic viruses in humans , PB1 in H10N8 and M in H7N9, but we did not find mutations known for mammalian host adaptation in these 2 segments. The encoded proteins of the PB1 and M gene segments that we isolated featured amino acid sequences rarely observed in mammalian and avian IAVs, including H9 (PB1, 97K, 156N, 397V, 535V, 688I, and 704T; M1, 31I and 46V; and M2, 25S). We could not determine whether these were random or adaptive mutations. The PB1 segment of avian H9N2 is highly compatible to other polymerase genes from mammalian IAVs . Such results suggest the need for further characterization of these mutations, particularly those in the PB1 gene.\n\n【5】A recent report in China discussed multiple Eurasian avian-like H1N1 swine influenza reassortants with internal genes derived from pH1N1 and triple reassortant lineages . One group of these reassortants (genotype 4) displayed a genotype similar to A/swine/HK/NS419/2021, the only exception being that the virus’s PB1 and M gene segments were of pH1N1 lineage. That report showed that genotype 4 Eurasian avian-like swine IAVs can bind to human sialic acid receptors (i.e. α2,3), enabling efficient virus replication in human airway epithelial cells, and achieve efficient aerosol transmission in ferrets . Serologic surveillance further showed that 10% of studied swine workers were positive for the genotype 4 reassortant . Our own sequence analyses suggest that some of the genotype 4 viruses and our Eurasian avian-like H1N1 viruses might share a common ancestry (e.g. A/swine/Shandong/1207/2016). Further risk assessment on the pandemic potential of this genotype and its reassortants is needed .\n\n【6】In summary, many zoonotic IAVs in humans have genes derived from H9N2 subtypes. Our results suggest that avian H9N2 IAVs are infecting swine and reassorting with swine IAVs, which indicates the need for continued monitoring of swine IAVs in both China and outlying regions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "03cec8e8-6566-4464-999b-1b272b4111a1", "title": "Flies and Campylobacter Infection of Broiler Flocks", "text": "【0】Flies and Campylobacter Infection of Broiler Flocks\nCampylobacteriosis, caused by _Campylobacter jejuni_ , is the most common foodborne infection in industrialized countries, where it causes millions of cases of illness every year . Chicken products are the food items most often reported to be the source of human campylobacteriosis . Thus, eliminating _Campylobacter_ from broilers is important for the safety of the food supply for humans  and is a priority in animal industrial production and health programs. In spring 2003, a Danish program against foodborne _Campylobacter_ infection was launched. The strategy addresses multiple steps in the food supply chain “from stable to table” but focuses on reducing the prevalence of _Campylobacter_ in chicken production by having comprehensive hygiene barriers between broiler houses and the environment .\n\n【1】For unexplained reasons, controlling _Campylobacter_ infection cannot be controlled during summer. Even strict compliance with all biosecurity regulations has failed to control the infection. In August 2003 in Denmark, for example, 72.1% of broiler flocks were infected , a situation that left selling of _Campylobacter-_ contaminated chicken meat to consumers inevitable. Although similarity between _Campylobacter_ isolates from broiler flocks and animals in the surrounding areas has been shown , the transmission routes are not understood, as no contact between broiler flocks and animals outside the broiler house takes place in closed production systems. However, indirect contact may be established by flies that take up _Campylobacter_ as they forage on fresh animal feces. We show that _Campylobacter-_ infected flies entered a broiler house in large numbers through the ventilation systems, which suggests that flies may be an important vector in summer.\n\n【2】### The Study\n\n【3】The number of flies transported by means of ventilation air into a broiler house in Denmark was counted from July 22, 2003, to July 28, 2003, and the _Campylobacter_ carriage rate of flies captured in the environment of the broiler house was estimated. The study period was chosen because flies generally peak in activity and abundance in July to August in Denmark. In addition, chickens in the broiler house and the animals in the area around the broiler house (5 sheep, 4 horses, and 1 dog with 10 puppies) were tested for carriage of _Campylobacter_ . DNA from all _C. jejuni_ isolates was analyzed by pulsed-field gel electrophoresis (PFGE) to determine if strains from different animals were similar.\n\n【4】The broiler house (80 m x 15 m) was located at Universal Transverse Mercator Grid zone 32, East 564,137 m, North 6,294,759 m . The facility was negative-pressure ventilated through a total of 84 wall valves for air intake (16.5 cm x 52.5 cm) and 12 round chimneys (diameter 62 cm) for active air outlet through the roof. The house was emptied, and the chickens (n = 28,235) were slaughtered on July 29, 2003. Reports of local weather data from the Danish Meteorological Institute  for that week were a maximum day temperature of 25.4°C, a minimum night temperature of 11.9°C, and days with bright sunshine with no wind or rain.\n\n【5】Flies were collected in polyester nets equipped at two wall inlets (one net at each end of the house) in the dynamic air flow measured  at a pressure of 21 Pa of influx ventilation air (speed 3.6 m/s, volume 1,213 m 3  /h per inlet valve). After the nets were harvested, flies were visually sorted from other insects and counted. The flies, identified primarily to the order _Diptera_ and the families _Muscidae_ and _Calliphoridae_ , were counted; the count showed that 917 ± standard deviation (SD) (843.5–990.5) flies (a + b)/2 x 84, with a and b representing the number of flies in the two nets) had entered the broiler house per day through the ventilation system, or approximately 1 fly per 2,700 m 3  of ventilation air (1,213 m 3  /h x 84 x 24 h/917). For this specific broiler house, this amount equals approximately 30,000 flies per broiler cycle in the summer season. To estimate the possibility of roof inlets as entrance route for flies into houses with an air inlet in the roof, the fan of one chimney was switched off. This step generated inlet air in this chimney. The result showed 167 flies per day entered through this one chimney, or an average of 45 flies per 2,700 m 3  air. The flies captured in the ventilation system were used for counting only, since they were dead and dried out at the time of harvest.\n\n【6】To determine the prevalence of _Campylobacter_ in flies around the broiler house, a total of 96 flies were captured singly within a distance of 50 m from the house by rackets equipped with small disposable plastic bags. Captured flies were narcotized with CO 2  . After species or genus was determined, each fly was transferred with a pair of tweezers to live storage in a sterile plastic tube with 1 mL of saline. The tubes were kept in an insulated container, which was transported to the laboratory within 24 hours after capture.\n\n【7】For _Campylobacter_ detection, each fly was macerated in a sterile mortar, suspended in 2 mL of 0.9% saline. The mixture was centrifuged at 15,870 x _g_ for 7 min. The pellet was resuspended in 2 mL of Bolton broth (CM0983 with SR0183 and SR0048) (Oxoid, Basingstoke, UK) and vortexed before incubation for enrichment at 37°C for 24 h. After enrichment, the tube was again centrifuged, and 100 μL of the sample from each of 49 tubes was streaked onto modified cefoperazone charcoal deoxycholate agar (mCCDA) (blood-free agar base supplemented with CM739 + SR155) (Oxoid). These plates were incubated at 42°C for 48 h in a microaerobic atmosphere (6% O 2  , 6% CO 2  , 4% H 2  in N 2  ). The tubes from the remaining 47 samples were subjected to DNA analysis by a nested polymerase chain reaction (PCR) .\n\n【8】The results of conventional culture showed that, of 49 flies tested, _C. jejuni_ was isolated from 4 (8.2%) ; when a nested PCR was used, 33 (70.2%) of the 47 flies were _Campylobacter-_ positive . The species distribution according to the nested PCR when species-specific primers were used showed that, of 47 samples, 56.4% were positive with _C. jejuni_ primers, 18.0% were positive with _C. coli_ primers, and 25.6% were positive with _Campylobacter_ species primers. Co-infection with _C. jejuni_ and _C. coli_ or _Campylobacter_ spp. was found in six flies. However, PCR also detects nonculturable and dead _Campylobacter_ . The reason for dividing the flies into two equal portions, one for conventional culture and the other for PCR, was to avoid reducing the assumed low number of _Campylobacter_ on each fly.\n\n【9】Cloacal or rectal swab samples from 20 broilers, 5 sheep, and 4 horses were cultured on mCCDA agar, as described above. Rectal swabs from 11 dogs were streaked onto cefoperazone amphoricin teicoplanin (CAT) agar plates (blood-free agar base with CM739 and SR174) (Oxoid), and incubated at 42°C for 96 h. Subsequently, the swabs were tested for _Campylobacter_ by our laboratory’s routine PCR for feces . _C. jejuni_ was isolated from 20 broilers and 4 sheep and _C. upsaliensis_ from 11 dogs. One sheep and the four horses were culture-negative. However, all fecal swab samples were _Campylobacter-_ positive by the routine PCR.\n\n【10】A total of 28 _C. jejuni_ isolates from 20 broilers, 4 flies, and 4 sheep were fingerprinted with PFGE with two different restriction enzymes, _Sma_ I and _Kpn_ I. With both enzymes, 27 of the isolates had an identical PFGE pattern (S6), whereas a single broiler isolate had a slightly different, but closely related pattern (S7), which probably was derived from the more prevalent pattern. The _Sma_ I patterns are shown in Figure 2 . Twenty-seven of 28 isolates from three animal sources, broilers, sheep, and flies, and from both inside and outside the broiler house, belonged to the same clone.\n\n【11】Under experimental conditions , flies are able to transmit _Campylobacter_ among chickens. Moreover, a high prevalence of _Campylobacter-_ infected flies captured in a broiler house has been found . However, no study has yet been able to demonstrate a significant role of flies captured in the houses for transmitting infection from flock to flock . Our results suggest that the potential of flies to transmit infection depends upon a current supply to the broiler house of _Campylobacter-_ infected flies from the outside. Furthermore, the number of flies entering the broiler house must increase as the need for ventilation air increases as a consequence of the growth of chickens. Thus, the risk of introducing _Campylobacter_ to the house increases with the age of the chickens.\n\n【12】### Conclusions\n\n【13】This study has demonstrated that flies pose a threat of _Campylobacter_ infection, from which chickens currently are unprotected from April to October, when insects are in season in the Northern Hemisphere. We found that in July hundreds of flies per day passed through the ventilation system into a broiler house and that 8.2% of flies captured in the environment had the potential to transmit _C. jejuni_ from outside animals to chickens in the broiler house. These results warrant further research on how to combat the summer peak of _Campylobacter_ in broilers to improve the safety of the human food supply.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c220b05-8f5f-4c1d-ba75-d10c6132acec", "title": "Regression Model Fitting With Quadratic Term Leads to Different Conclusion in Economic Analysis of Washington State Smoking Ban [Response to Letter]", "text": "【0】Regression Model Fitting With Quadratic Term Leads to Different Conclusion in Economic Analysis of Washington State Smoking Ban [Response to Letter]\nTo the Editor:\n--------------\n\nWe were interested to read Ma and McClintocks letter  about our analysis of taxable retail sales (TRS) data . Although we agree in general that different models of data can lead to different conclusions, we disagree that our analysis misrepresents those data. Rather, we find the reanalysis and presentation in their letter to be misleading. Ma and McClintock reiterate our observation that the TRS data during this time do not follow a linear trend with an obvious upturn in 2006 through 2007 and a smaller downturn in 2002 through 2003 and question our use of a linear model to describe these data. In our analyses, we examined a segmented regression approach  to address this nonlinearity, using a theoretical break point at 2006 to delineate periods before and after passage of a smoke-free law (SFL) in Washington. Ma and McClintock put forth a linear model with a quadratic term to address the nonlinearity in these data, with no theoretical justification for the mechanism that would drive such a function. The use of a quadratic term suggests an exponential growth in TRS post-SFL, whereas we suggest a theory-based, more moderate, and flexible linear growth function due to the SFL. Although the figure presented by Ma and McClintock correctly represents their model, it misrepresents our model as a single straight-line fit when it actually has 2 linear segments .  **Figure.** Segmented and quadratic regressions fit to taxable retail sales in bars and taverns in Washington State after the implementation of a smoke-free law, from the first quarter of 2002 (1/02) through the fourth quarter of 2007 (4/07). Values are adjusted for inflation to the Consumer Price Index .  In addition, Ma and McClintocks contention that their model fits the data better, supported by an improved _R_ 2 value, is incorrect. In fact, the competing models, adjusted only for inflation one with a quadratic term to specify the post-SFL period and the other with a linear segmented term to model that period have nearly the same _R_ 2 value; our segmented model has a slightly higher value, suggesting a better fit (0.888 vs 0.875, respectively). Ma and McClintock state that their results suggest that “the smoking ban did not affect the taxable sales revenue over the time.” This statement is inaccurate in the context of the model they present. Rather, the quadratic term in their model and the post-SFL segment term in our model both provide evidence that TRS increased dramatically post-SFL. Furthermore, Ma and McClintock present a table that is flawed. That table contains projected TRS post-SFL “with and without a quadratic term.” The authors contend that the difference between projected and actual sales using a model with a quadratic term is large and opposite in sign to our estimated differences without that term. The principal error in this table is that although the “without quadratic term” column (our model projections) correctly reported the forecasted trend by using the model fit to the pre-SFL values (the first segment coefficient), the “with quadratic term” column actually uses the post-SFL predicted values as the base, as if the quadratic term applied to that interval, not to the values forecasted using a model fit to the data pre-SFL. In fact, it is because the quadratic term overpredicts “exponential growth” post-SFL that the residual values they present show the pattern they do. The comparison in that table, and the conclusions reached by Ma and McClintock, are incorrect. We also note for clarity that the raw data table presented by Ma and McClintock do not agree completely with those we provided to them, possibly because they used a different inflation adjustment than the one we provided to them. Given Ma and McClintocks misunderstandings and misrepresentations of our analysis, we stand firmly by our initial analysis and conclusions. As the authors state, we provided raw TRS data. In addition, we communicated with them about methods as an aid to their conducting a similar analysis with data from their state. Their letter to the journal is the first indication we received that they had concerns about our methods. **Myde Boles, PhD**Multnomah County Health Department and Oregon Public Health DivisionPortland, Oregon **Clyde Dent, PhD**Multnomah County Health DepartmentPortland, Oregon **Julia Dilley, PhD, MES**Multnomah County Health Department and Oregon Public Health DivisionPortland, Oregon **Julie E. Maher, PhD**Multnomah County Health Department and Oregon Public Health DivisionPortland, Oregon **Michael J. Boysun, MPH**Washington State Department of HealthPortland, Oregon **Terry Reid, MSW**Washington State Department of HealthPortland, Oregon   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6771cd87-4543-4ece-8fda-a9802c5a70b0", "title": "Surveillance and Analysis of Avian Influenza Viruses, Australia", "text": "【0】Surveillance and Analysis of Avian Influenza Viruses, Australia\nShorebirds (Charadriiforme _s_ ) and wild waterfowl (Anseriformes) represent the major natural reservoirs of avian influenza viruses (AIVs). These birds can carry all 16 hemagglutinin (HA) and 9 neuraminidase (NA) subtypes ; the viruses typically cause asymptomatic infections in these hosts. Studies in Europe and North America demonstrated the following: AIV carriage is highest in autumn but may also be high in spring; prevalence among shorebirds and ducks is increased during their northward and southward migrations, respectively; and distribution, prevalence, and subtypes involved vary from year to year . Interspecies transmission of AIV in several species of wild birds has been documented; however, the most frequent adaptation of these viruses occurs in domestic gallinaceous poultry.\n\n【1】The respiratory tract of poultry and gastrointestinal tract of waterfowl are replication sites for AIVs, and poultry are incubators for the progression of low-pathogenicity avian influenza (LPAI) virus into highly pathogenic avian influenza (HPAI) virus , usually through the acquisition of polybasic amino acids at the HA cleavage site. HPAI, particularly HPAI (H5N1), may induce up to 100% deaths in poultry and cause substantial economic losses . Strains that are highly pathogenic in gallinaceous species may cause a range of clinical signs in other avian species, from mild illness to highly contagious and fatal disease. H5 and H7 AIVs have the propensity to become HPAI and thus are a significant risk to the poultry industry. These subtypes and H9 have also caused disease and death in humans. Subtype H5N1 first caused outbreaks in wild migratory waterfowl in the People’s Republic of China in 2002 and in domestic poultry in Hong Kong Special Administrative Region, China, in 2003 . The World Health Organization has since confirmed 433 human cases of avian influenza (H5N1) with 262 deaths .\n\n【2】AIVs may be transported by infected migratory birds . Shorebirds and waterfowl usually survive infection, and transmission by migratory waterfowl over long distances within Asia and between continents has been documented . Nevertheless, the role of migratory birds in the distribution and transmission of AIVs remains controversial. Managing the potential threat of transport of AIVs by wild birds requires appropriate surveillance programs that assess the occurrence, subtypes, and pathogenicity of isolates that the birds carry.\n\n【3】Australia is isolated by sea, and shorebirds make up the majority of long-distance migratory birds that visit the continent (3 million/year ). These birds breed in Siberia (May–July) and stop off throughout Asia (April–May, July–September) in areas where HPAI (H5N1) epizootics have recently occurred (e.g. Vietnam, Thailand, Hong Kong, China, Indonesia) . Most arrive in Australia in spring (August–September) and depart in autumn (March). Shorebirds are known to carry a variety of AIVs, including subtype H5N1 . Wild waterfowl, such as ducks, geese and swans are common in Australia. However, they do not migrate out of Australia in large numbers, although they do undertake intracontinental movements and occupy the same habitats as migratory shorebirds. Collectively, these factors provide an environment that allows the assessment of the import of AIVs by migratory birds and transmission to, and distribution by, local waterfowl.\n\n【4】Until recently, only small and historical studies of AIVs have been undertaken in Australia . No outbreaks of HPAI H5 viruses have been identified, despite the close proximity of Indonesia, where AIV (H5N1) is endemic and outbreaks frequently occur. Five outbreaks of HPAI have occurred in Australia; all outbreaks were caused by H7 viruses. In all cases of disease, transmission of LPAI H7 from wild birds and subsequent mutation to HPAI after serial passage in chickens was considered the probable source . Nevertheless, the source of infection in wild birds has not been identified. Therefore, surveillance for AIVs is needed in Australia in localities where large numbers of migratory shorebirds and waterfowl occur in close proximity to poultry operations . We examined the occurrence and subtypes of AIVs carried by migratory shorebirds and waterfowl in southeast Australia over a 4-year period.\n\n【5】### Methods\n\n【6】##### Sample Collection\n\n【7】Sampling site selection  was based on abundance of migratory shorebirds, risk for transmission to waterfowl inhabiting the same area, proximity of commercial poultry, and human population density. Areas around Newcastle and Orange, New South Wales , and Melbourne, Victoria, were selected. Samples were also collected opportunistically from other locations . Sites were generally inland swamps or coastal wetlands.\n\n【8】Most samples were collected from coastal wetlands or inland swamps around Newcastle and Orange, New South Wales, and Melbourne, Victoria, with small numbers from other sites . Samples from Tasmania were included with samples from Victoria for analysis. Coastal New South Wales and Victoria samples were from sites co-inhabited by large numbers of migratory shorebirds and waterfowl. A total of 21,858 samples were collected and tested during 2005–2008 . Of these, 10,003 were from migratory shorebirds, 10,231 from waterfowl, and 1,624 from other bird species. Samples from other bird species were from birds trapped incidentally or were collected opportunistically. In most instances, the species, or pairs of sister species (e.g. grey/chestnut teal, bar-tailed/black-tailed godwit), that produced the feces collected were known. Identification of species sampled was identified by observing the bird, the bird’s footprints, and the size and shape of feces.\n\n【9】Samples were fresh feces or cloacal swabs . Fecal samples were collected from roosting or feeding flocks, and the species involved was recorded. Cloacal samples were collected from birds captured by cannon netting, funnel traps, and hand-held nets, and from ducks shot for recreation, damage mitigation, or conservation. Samples were placed in phosphate-buffered gelatin saline or brain-heart-infusion broth base, each containing penicillin (2 × 10 6  IU/L), streptomycin (0.2 mg/mL), gentamicin (0.5 mg/mL), and amphotericin B (500 U/mL), and transported chilled to our laboratories at the University of Newcastle, the Department of Primary Industries or the Orange Agricultural Institute for storage at –80°C until analysis .\n\n【10】##### PCR Detection of AIVs\n\n【11】Viral RNA was extracted according to manufacturers’ instructions by using MagMax96 viral RNA (Life Technologies, Scoresby, Victoria, Australia), or RNeasy isolation kits (QIAGEN, Doncaster, Victoria, Australia). AIVs were detected by real-time quantitative reverse transcription–PCR (qRT-PCR) by using the conserved matrix gene as the amplification target . Influenza A–positive samples were tested by using specific primers targeting H5 and H7 subtypes . Proportion tests (Pearson χ 2  statistics in R ) were used to test differences in influenza A–positive by PCR rates according to season.\n\n【12】##### AIV Subtype Determination\n\n【13】To determine AIV subtypes, HA2 and NA genes were amplified by conventional PCR and sequenced . Sequences were compared with known sequences by BLAST search  to determine subtype and relatedness to other viruses. For H5, H7, and H9 subtypes, the full HA genes were sequenced and HA cleavage sites were assessed to determine potential pathogenicity .\n\n【14】##### Phylogenetic Analysis\n\n【15】Phylogenetic trees were constructed for H5, H7, and H9 viruses by comparison of the relatedness of the subtypes isolated in this study with those from other geographic locations . HA genes of 10 H5, 3 H7, and 8 H9 viruses from this study were compared with those of representative subtypes of major AIV lineages from GenBank. Sequences were assembled and edited with SeqMan, DNASTAR Lasergene 8. Geneious (Biomatters Ltd, Auckland, New Zealand) and Se-Al  were used for alignment. MRMODELTEST 2.2  was used to determine the appropriate DNA substitution model and γ-rate heterogeneity. The best-fit model was used to generate neighbor-joining trees by using PAUP\\* 4.0 . Only strains from which a full HA gene sequence was obtained were included. Estimates of phylogenies were calculated from 1,000 neighbor-joining bootstrap replicates.\n\n【16】### Results\n\n【17】##### AIVs Detected\n\n【18】Three hundred AIVs were detected by qRT-PCR, representing a total PCR-positive detection rate of 1.4% ,of which 51 (17%) were detected in migratory shorebirds (including 16 bar-tailed godwits, 14 red-necked stints, 11 eastern curlews, and 7 red knots) and 247 in waterfowl (including 224 dabbling ducks), corresponding to rates of 0.51% and 2.4%, respectively . Two viruses were detected in other birds (Eurasian coot and whiskered tern). _Numenius_ spp. waders (predominantly eastern curlew, 11/690, 1.6%) were the most common shorebird carriers. Dabbling ducks had slightly higher detection rates (224/7,607, 2.9%) compared with all waterfowl.\n\n【19】##### PCR-positive Samples\n\n【20】PCR-positive detection rates were similar for migratory shorebirds (0.55% vs. 0.45%) and waterfowl (2.0% vs. 3.6%) between New South Wales and Victoria . Rates were highest in autumn and early winter (April–June, χ 2  \\= 18.0, degrees of freedom \\[df\\] = 3, p = 0.0004) in migratory shorebirds, and in autumn (April–May, χ 2  \\= 11.2, df = 3, p = 0.01) in waterfowl . Rates were similar for different years: migratory shorebirds 0.65% in 2005, 0.50% in 2006, 0.46% in 2007, and 0.72% in 2008; and waterfowl 2.7% in 2006, 2.6% in 2007, and 3.7% in 2008. However, rates differed substantially for different bird types, areas, and years, which could explain the high variability observed in seasonal trends . For example, the rate for migratory shorebirds in coastal New South Wales in 2008 (0.72%, mostly bar-tailed godwit and eastern curlew) was double that in Victoria in 2006 (0.38%), rates for waterfowl in Victoria (4.8%, mostly in Pacific black duck) in 2008 were almost double those in 2007 (2.8%), and rates for dabbling ducks in Victoria in 2008 (6.3%) were 3-fold greater than in coastal New South Wales in 2006 (1.9%). Rates were generally similar for different years for migratory shorebirds sampled in New South Wales and Victoria; however, rates reached 5.2% for waterfowl (mostly grey and chestnut teal) in March 2006 in Victoria, compared with 3.6% at other times.\n\n【21】##### AIV Ecology\n\n【22】It was possible to subtype 107/300 (36%) AIVs detected by qRT-PCR . It was not possible to subtype all AIVs because the conventional PCR used for subtyping was not as sensitive as the surveillance qRT-PCR. Notably, 19 H5, 8 H7, and 16 H9 AIVs were identified. No H5 or H7 AIVs contained multiple basic cleavage sites, a known molecular determinant for HPAI; therefore, all were classified as LPAI. H5, H7, and H9 subtypes represented a high proportion (43/107, 40%) of all viruses subtyped. H9 subtypes were the most common viruses identified in migratory shorebirds (5/11, 45%). H3 and H5 viruses were the most common subtypes identified in waterfowl (21/96, 22%, and 18/96, 19%, respectively). One H5 and 5 H9 AIVs were detected in migratory shorebirds, 1 H9 was from a black swan, and the remainder (18 H5, 8 H7, and 10 H9) were from dabbling ducks. In PCR-positive samples for which NA subtype was determined, we detected 1 N1, 4 N3, 1 N5, 5 N6, 4 N7, and 1 N9 . The NAs associated with H5, H7, and H9 viruses were of the following subtypes: H5N3 , H5N7 , H7N1 , H7N6 , and H7N7 .\n\n【23】The detection of PCR-positive samples was sporadic and was increased in some periods, particularly in ducks, in which larger numbers of AIVs were identified at the same time and location . During periods of increased detection, rates of up to 6.2% and 12.3% were found for migratory shorebirds and waterfowl, respectively. These events occurred throughout the year but were more common in autumn (March–May) and early spring (September). Some evidence showed different seasonal increases in rates for particular subtypes of AIVs. Twenty of 23 H3 subtypes were primarily detected from autumn to early spring (March–September), whereas 16/19 H5 and 7/8 H7 subtypes were detected from late spring to early autumn (November–March), and all 16 H9 subtypes were detected in autumn (March–May). Most H3 (13/23), H5 (12/19), and H7 (5/8) strains were detected in 2007, whereas half (8/16) of the H9 strains were from 2007 and half were from 2008. Notably, 8 H5 viruses were identified in summer (December–February) in New South Wales in 2007–2008, and only 1 strain of a different subtype (H4) was identified during this period. Also, notable increases in detection rates of H9 subtypes occurred in New South Wales in autumn in 2007 (April–May) and 2008 (March–April). In addition, rates involving numerous different subtypes increased on 3 occasions: 2 H3, 3 H5, 1 H11, 1 H12, from dabbling ducks, Victoria, March 2006; 1 H2, 1 H4, 2 H5, 2 H7, 1 H8, from ducks, Victoria, January 2007, and 3 H2, 6 H3, 1 H4, 1 H5, 1 H10, mostly from teal, New South Wales, August–September 2007. However, these increases may be the result of the large numbers of samples collected on these dates (365, 341, and 504, respectively).\n\n【24】Increased detection rates of individual AIV subtypes were generally localized because the same subtypes were only identified at the same time from different sites on 3 occasions : 1 H12, New South Wales, and 1 H12, Victoria, in March 2006; 2 H9, coastal New South Wales, and 3 H9 inland in May 2007; and 2 H11, New South Wales, and 2 H11, Victoria, in May 2008. Some evidence for cross-species infection was found with the same subtypes of virus identified in different species at the same time and location: 2 H9 in both bar-tailed godwits and eastern curlews in April 2008 in New South Wales. Full HA sequences for the 4 strains demonstrated >99.6% nt similarity; the 2 strains from the bar-tailed godwits showed 100% nt homology . Limited evidence was shown for the cross-species infection of migratory shorebirds and waterfowl, with the same subtype isolated from each group on just 2 occasions, both involving H9 viruses in New South Wales: 1 H9 from an eastern curlew and 1 H9 from a duck in May 2007 and H9s from 2 bar-tailed godwits, 2 eastern curlews, 1 black swan, and 1 chestnut teal, all in April 2008. The H9 strains from the bar-tailed godwit and eastern curlew had a >98.8% nt similarity to the H9 strain from the black swan . Sequence data were not available for the duck or chestnut teal AIVs.\n\n【25】##### Phylogenetic Analysis\n\n【26】All H5 viruses detected in this study clustered closely together and were clearly divergent from other LPAI H5 viruses from Eurasia and North America . Both the Australian and the Eurasian lineages appear to have evolved from an early lineage of H5 viruses that includes a range of strains from 1959 through 1986. The H7 strains identified in this study have a close genetic relationship with HPAI H7 viruses previously isolated in Australia during 1975–1997 and as a group are clearly distinct from Eurasian and North American H7 lineages . Notably, all Australian H7 viruses were closely related to the strains that caused pathogenic outbreaks in poultry in Australia and thus may identify a potential environmental source of these viruses. Eurasian H9 strains have evolved into 2 discrete lineages that are carried by aquatic or terrestrial birds . The Australian H9 strains detected in this study again grouped closely and, as a lineage, diverged from the Asian aquatic H9 viruses but were distinct from Eurasian and North American lineages. Although the Australian H9 viruses were a less discrete lineage than H5 and H7 viruses, the maximum bootstrapping value  confirmed that they formed their own distinct lineage. Taken together, these results indicate that the viruses within each subtype in Australia are closely related and form Australian-specific lineages that are distinct from other lineages.\n\n【27】### Discussion\n\n【28】This large surveillance effort for AIVs in Australia longitudinally and geographically characterized the extent and profile of AIVs in wild birds. We detected 300 AIVs from ≈22,000 samples tested and subtyped 107 of these. _Anas_ species ducks were the predominant carriers, and the peak of detection occurred in autumn. Detection rates varied among different locations and times. Numerous H5, H7, and H9 AIVs were detected, although no HPAI strains were identified. The Australian viruses within each subtype were closely related and formed separate clades from Eurasian or North American lineages, indicating that separate lineages of H5, H7, and H9 AIVs are circulating in Australia.\n\n【29】##### PCR-positive Rates\n\n【30】Most AIV ecology studies have been conducted in Europe and the United States . In Australia, the overall PCR-positive detection rate of 1.4% for all bird species is similar. However, rates of 0.51% for shorebirds and 2.4% for waterfowl are similar to, or less than, those detected in other geographic regions where 0.2%–20% of shorebirds and 7%–37% of waterfowl were carriers . Our results agree with those of historical Australian studies in which rates of 0.6% for all birds and 1%–5% for ducks were found . Rates for AIVs in shorebirds in Australia were previously unknown. We found rates were highest for dabbling ducks, which is consistent with findings of other studies . These higher rates may be a result of the ducks’ feeding technique of filtering soft mud, which may be an environment conducive to the persistence of AIVs.\n\n【31】##### Autumn and Winter Detection Rates\n\n【32】Detection rates for migratory shorebirds were highest during April–June and were highest in overwintering eastern curlews. This finding contrasts with results of studies from North America that show a low prevalence in winter . The Australian winter is considerably milder than winter in areas studied in North America, and differences among winter rates may result from these climatic differences.\n\n【33】Shorebirds migrate to Australia down the East Asian–Australian flyway and then subsequently disperse throughout Australia. Our results suggest that migratory shorebirds are not commonly carrying AIVs into Australia, which would be indicated by peak detections in newly arrived birds in September, but that they become infected during autumn and winter in Australia. This provides further evidence that AIV infection is not maintained during migration , although studies in Europe have shown that ducks can carry AIVs during migration .\n\n【34】Rates in waterfowl were high in early spring (September), which corresponds with the period when young birds arrive in coastal Australia . This finding agrees with those of studies from other locations, which show that immunologically naive juvenile birds carry more viruses, which may have been transmitted from adult birds or the environment .\n\n【35】##### Variability and Increases in Detection Rates\n\n【36】AIV detection rates were variable and seasonal, and periods of increased rates occurred. Large numbers of viruses were detected during some sampling periods but not others, and some subtypes were often identified at the same time and location. Detection of H5 viruses increased in the summer of 2007–08 in New South Wales; all H9 viruses were detected in New South Wales, and most H5, H7, and H9 viruses were identified in different years. These increases in detection rates were generally localized to particular times and places. Detection of the same subtypes at the same times indicated limited evidence of cross-infection, which suggests that occasionally viruses may be passed between bird species (e.g. shorebird to shorebird) and families (e.g. between shorebirds and waterfowl).\n\n【37】##### Phylogenetics\n\n【38】In this study, H1–H12 and all NA subtypes, excluding N2, N4, and N8, were detected, a similar level of diversity as that observed in other studies . The AIVs of particular concern are H5, H7, and H9 because they have been associated with outbreaks in poultry and disease in humans. Notably, in this study, we found that these were the most common subtypes, representing 40% of all AIVs identified; however, no HPAI (H5N1) strains were detected. This pattern is different from that observed in other locations, e.g. H4, H6, and H7 were most common in Sweden, and H1, H2, H4, and H6 dominated in Germany and North America . This difference may indicate variations in host–virus interactions in Australia. Phylogenetic analysis showed that AIVs of the same subtype detected in Australia are closely related and are distinct from viruses isolated from other geographic locations. We attempted to isolate viruses from on all PCR-positive samples, but only 3 viruses were recovered (2 H7, 1 H5, all from fecal samples). Antigenic hemagglutination inhibition assays  of the 2 H7 viruses showed they were antigenically similar to 5 HPAI H7 strains that caused outbreaks in poultry in Australia during 1976–1992 but were not similar to 2 Asian H7 viruses (data not shown). The genetic and limited antigenic data demonstrate that little genetic evolutionary change has occurred and suggest that no antigenic change has occurred in Australian H7 viruses over >30 years.\n\n【39】Previous studies of AIVs have shown that globally 2 separate HA AIV lineages occur: Eurasian and North American . However, our study provides clear evidence that Australian AIVs, rather than being part of the Eurasian lineage, have diverged and may be considered as belonging to a different lineage. The genetically discrete Australian lineage suggests that endemic circulation and evolutionary isolation of strains in Australia have occurred and provides little evidence for the importation of exotic strains by migratory birds .\n\n【40】This and other studies highlight the need for continued longitudinal surveillance, particularly in areas with large numbers of migratory birds and waterfowl located close to commercial poultry, and in northern Australia, which is nearest to areas where HPAI (H5N1) is endemic. Further genetic and antigenic characterization of AIVs in Australian wild bird populations should be performed. Surveillance programs can identify peaks in occurrence and may act as an early warning system. Such measures are essential for maximizing biosecurity for the poultry industry and public health agencies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "211f2026-3f8d-4080-8942-01b9152c0786", "title": "Shared Use of Physical Activity Facilities Among North Carolina Faith Communities, 2013", "text": "【0】Shared Use of Physical Activity Facilities Among North Carolina Faith Communities, 2013\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Shared use of recreational facilities is a promising strategy for increasing access to places for physical activity. Little is known about shared use in faith-based settings. This study examined shared use practices and barriers in faith communities in North Carolina.\n\n【3】**Methods**\n\n【4】Faith communities in North Carolina (n = 234) completed an online survey (October–December 2013) designed to provide information about the extent and nature of shared use of recreational facilities. We used binary logistic regression to examine differences between congregations that shared use and those that did not share use.\n\n【5】**Results**\n\n【6】Most of the faith communities (82.9%) that completed the survey indicated that they share their facilities with outside individuals and organizations. Formal agreements were more common when faith communities shared indoor spaces such as gymnasiums and classroom meeting spaces than when they shared outdoor spaces such as playgrounds or athletic fields. Faith communities in the wealthiest counties were more likely to share their spaces than were faith communities in poorer counties. Faith communities in counties with the best health rankings were more likely to share facilities than faith communities in counties that had lower health rankings. The most frequently cited reasons faith communities did not share their facilities were that they did not know how to initiate the process of sharing their facilities or that no outside groups had ever asked.\n\n【7】**Conclusion**\n\n【8】Most faith communities shared their facilities for physical activity. Research is needed on the relationship between shared use and physical activity levels, including the effect of formalizing shared-use policies.\n\n【9】Introduction\n------------\n\n【10】Increasing access to safe, affordable recreational facilities is one way to increase physical activity. The strategy is particularly important for increasing physical activity in low-income or racial/ethnic minority communities, where issues related to safe access to recreational facilities are well-documented . Shared use (working with organizations to open access to their facilities) is a promising strategy for increasing the number of safe and accessible spaces to be physically active . Although researchers have examined the prevalence of shared-use agreements in schools, little is known about implementation of and barriers to shared use of recreational facilities in faith-based settings .\n\n【11】Faith communities are effective partners for promoting healthful eating and physical activity among their members . Public health practitioners have harnessed this potential by implementing and evaluating obesity prevention programs that have included lay advisor models , targeted diabetes education , and community-based participatory research . Researchers have documented and evaluated how faith communities implement healthful eating or physical activity policies, provide spaces or time for physical activity, and promote access to healthful foods .\n\n【12】Given the importance of ecological frameworks in the design and implementation of physical activity interventions, community and environmental supports for physical activity are increasingly promoted as a strategy to increase individual physical activity levels among community members . Shared-use policies and practices, mainly in school settings, are promoted as a way to increase physical activity levels, particularly in areas that lack spaces to be active . For example, in North Carolina, a high percentage of schools (88.9%) allowed shared use of their facilities, a percentage much greater than that found in previous studies . Shared use was disproportionately lower in schools in economically distressed counties and in schools with a greater proportion of black students . Schools reported several barriers to implementing shared-use policies, including concerns about liability and maintenance  and feelings among school administrators that the community lacked interest in shared use or that administrators did not know where to start this work .\n\n【13】Research on health promotion in faith communities is increasing . Formal (ie, written) or informal (ie, verbal) agreements and policies on shared use for health promotion purposes can regulate whether and how people or groups are allowed to use the facilities of faith communities. Some faith communities adopt formal facility-use policies that outline the conditions and costs associated with individuals and groups using their space for meetings (eg, for Alcoholics Anonymous, Boy Scouts) or events. Some faith communities permit unstructured use — sometimes called open use — of their recreational spaces by individuals and groups in the community. For example, many faith communities have playgrounds that are used by neighborhood children and families . A policy allowing unstructured use may be part of an intentional decision by the leaders of the faith community to make space available to the community for open recreational use and can lead to the adoption of a formal open-use policy. On the other hand, open use may be a customary practice of the faith community or neighborhood that has not been explicitly discussed or affirmed and is occurring with no policy — either formal or informal — in place.\n\n【14】The objectives of this study were to 1) create a baseline assessment of shared use of physical activity facilities among North Carolina faith communities, 2) determine barriers to shared use, and 3) determine priorities for future programs to support shared use of physical activity facilities in faith communities.\n\n【15】Methods\n-------\n\n【16】We conducted a survey of faith-based organization in North Carolina in fall 2013. Three organizations collaborated to implement this assessment: North Carolina State University, Partners in Health and Wholeness, and the North Carolina Division of Public Health (DPH). North Carolina State University and DPH have a history of working together. In 2007, they collaborated to create the practice-tested faith-based intervention Faithful Families Eating Smart and Moving More (Faithful Families) . Partners in Health and Wholeness (PHW), sponsored by the North Carolina Council of Churches, certifies and supports congregations in their efforts to encourage healthful eating, physical activity, and tobacco cessation. As a part of its Community Transformation Grant (CTG) project, DPH worked with faith communities to promote shared use of their facilities . The institutional review board at North Carolina State University approved this research.\n\n【17】We adapted the survey used for this project from an existing survey of shared use that was administered in North Carolina public schools in 2013, which was based on an assessment developed by Spengler et al in 2011 . The survey asked faith communities whether their facilities (including meeting rooms, kitchens, gymnasiums, playgrounds, and athletic or open fields) were used by groups or individuals outside of the faith community’s membership. If facilities were available for outside individual or group use, the survey asked participants whether this use occurred through a formal policy or agreement (ie, a written contract), an informal policy or agreement (ie, verbal permission), or no policy or agreement (ie, permission to use the space had not been discussed). Faith communities that did not open their facilities to outside groups or individuals were asked a series of questions about the barriers to doing so, including liability, maintenance, not knowing where to start, and lack of space or interest. Participants were asked to what extent they agreed or disagreed (1 = strongly agree, 2 = agree; 3 = neither agree nor disagree; 4 = disagree; 5 = strongly disagree) with statements about common barriers to shared use, including not having been asked, not knowing where to start, concerns about liability, and concerns about maintenance cost.\n\n【18】We administered the survey electronically, via Qualtrics (Qualtrics LLC), and permitted faith communities with limited access to the Internet to submit paper copies of the survey. Using program records, partners distributed the survey to faith communities that had participated in Faithful Families or the PHW program (262 faith communities). CTG coordinators throughout the state also distributed the survey to faith community contacts in their counties. Any faith community could participate, regardless of tradition or religious background. Emails were addressed generically but were tailored by local staff members (Faithful Families facilitators, PHW liaisons, or CTG coordinators) to be delivered to their local contacts in the faith community. The survey was completed by clergy, deacons, health committee members, and faith community members. Using survey distribution approaches developed from the tailored-design method , we sent a presurvey email to all potential participants. This email included information about the survey, a confidentiality statement, and an invitation to either complete the survey or share it with faith community partners. The survey distribution email included a link to the survey and a contact directory for regional and state partners who could assist with any survey questions. We sent 2 reminder emails to all participants after the initial survey email. The survey was open starting October 24, 2013, for 6 weeks, with an initial deadline of November 24, 2013. The deadline was later extended to December 13, 2013. We pilot-tested the questionnaire with a representative from 6 faith communities. The pilot testing demonstrated that the survey questions were clearly stated, and no questions were changed as a result.\n\n【19】Because the survey was distributed across several networks, we do not know how many faith communities received the survey. However, according to the Association of Religion Data Archives, North Carolina has more than 15,000 faith communities . Two hundred and thirty-four faith communities completed all of the shared-use questions on the survey and were included in the analysis.\n\n【20】The questionnaire comprised 25 questions related to shared use of facilities, including questions about the county where the faith community was located, congregation size, types of facilities that were shared, type of policy or agreement governing the use (informal, formal, or none), and perceived barriers to shared use.\n\n【21】Using census data and the county information reported by survey respondents, we measured the percentage of black residents living in the county where each participating faith community was located. We classified the percentage of black residents as low (≤10%), moderate (11%–30%), and high (≥31%) . We obtained health rankings data for each county from the 2013 University of Wisconsin Population Health Institute’s County Health Rankings, which ranked each North Carolina county “according to summaries of a variety of health measures,” with 1 being the healthiest and 100 being the least healthy . Because standardized county classification systems designating rural areas are lacking , we used data from the 2010 decennial census  on the percentage of the county’s population living in rural areas (ie, outside urban areas or urbanized clusters) to characterize the rurality of counties. We obtained economic data from the North Carolina Department of Commerce’s 2013 ranking of the state’s 100 counties based on economic well-being . The 40 most distressed counties were designated as Tier 1, the next 40 as Tier 2, and the 20 least distressed as Tier 3.\n\n【22】### Data analysis\n\n【23】We used descriptive statistics to describe faith community characteristics and type of shared use. We used binary logistic regression to examine differences between congregations that shared use and those that did not share use. The regressions focused on faith community size, county economic tier, county health ranking, percentage of county population living in rural areas, and percentage of black residents in the county as key explanatory variables. Initial unadjusted models were estimated without controlling for other variables, followed by models estimated that controlled for other variables. Statistical significance was established at _P_ \\= .05.\n\n【24】Results\n-------\n\n【25】Of the 234 faith communities that responded to the survey, 78 (34.4%) were small (<120 members), 75 (33.0%) were medium sized (120–299 members), and 74 (32.6%) were large (≥300 members) . Forty-four (18.8%) faith communities in the sample were in the most economically distressed counties in North Carolina; 41.0% were in Tier 2 counties, and another 40.2% were in Tier 3 counties . Survey respondents varied by type of position and included clergy, lay health leaders, deacons, PHW liaisons, and general members of the faith community.\n\n【26】Of the 100 counties in North Carolina, 53 were represented in the survey . The distribution of the survey counties most likely reflects the interests and priorities of local PHW, CTG, and Faithful Families program staff. The largest number of faith communities that responded, by county, were from Wake County (27 respondents) and the second largest from Forsyth County (21 respondents).\n\n【27】Most respondents (82.9%) indicated that their faith communities had facilities that were shared with outside groups or individuals . Of the 186 faith communities that had classrooms or meeting space, 167 (89.8%) shared them; 66.1% (39 of 59) shared gyms, 59.1% (68 of 115) shared playgrounds, and 55.1% (38 of 69) shared athletic/open fields . Formal shared-use policies or agreements were more common for indoor facilities such as gyms (56.4%; 22 of 39) and classroom/meeting room spaces (51.5%; 86 of 167) than for outdoor spaces, whereas informal policies or agreements or no policy or agreement was more common for playgrounds (76.5%; 52 of 68), and athletic/open fields (63.2%; 24 of 38).\n\n【28】In unadjusted models, faith communities in the wealthiest (Tier 3) counties were significantly more likely (odds ratio \\[OR\\], 3.19; 95% confidence interval \\[CI\\], 1.10–9.25; _P_ \\= .03) than faith communities in the poorest counties (Tier 1) to share facilities . Similarly, faith communities in counties with the highest ranking in health outcomes were significantly more likely (OR, 3.32; 95% CI, 1.31–8.45; _P_ \\= .01) to share facilities than faith communities in counties ranked lowest in health outcomes. In addition, faith communities in counties with a moderate percentage of rural residents were less likely (OR, 0.28; 95% CI, 0.11–0.71; _P_ \\= .007) than faith communities with a high percentage of rural residents or a low percentage of rural residents to share facilities.\n\n【29】Thirty-nine faith communities reported not sharing their facilities; of these, 29 answered the question on barriers. The barrier most frequently cited was not knowing how to start the shared-use process (mean score = 2.9; 7 of 29 \\[24.1%\\] reporting). The second most common reason given was not being asked by outside groups or individuals to share (mean score = 2.7; 7 of 29 \\[24.1%\\] reporting).\n\n【30】Discussion\n----------\n\n【31】Our study yielded findings that can help shape future projects and practices related to shared use of physical activity facilities in faith communities. First, these data illustrated that faith communities are opening up their spaces for shared use. As the first study of shared use among faith communities, this study can encourage public health practitioners to expand or enhance partnerships with faith communities to encourage shared use. To aid in this work, and as a direct result of this research and a request by DPH, ChangeLab Solutions developed a guide to implementing shared-use practices in faith communities .\n\n【32】Second, our study showed that faith communities are sharing various types of spaces: classrooms, gymnasiums, playgrounds, and athletic fields. As a part of this study, we asked faith communities that shared facilities whether they were willing to share their information in a public database, which is now available online. Additional research is needed to understand how these spaces are used and whether use differs between faith communities that have formal agreements and faith communities that have informal agreements or no agreements.\n\n【33】Third, our study found that several factors were associated with differences in sharing facilities. Faith communities in counties with the greatest wealth and highest health rankings were more likely to share their facilities than faith communities in poorer counties and counties with lower health rankings. Additionally, faith communities with a moderate percentage of rural residents were less likely to share facilities than faith communities in counties with a high percentage or low percentage of rural residents. These findings correspond to findings from a study by Edwards et al , which notes that stereotypes about suburban communities as middle-class enclaves may be misguided. Instead, Edwards et al suggest that gentrification, population growth, and redevelopment have shifted poor residents to suburbs, “creating growing pockets of low-income residents concentrated in disadvantaged suburban communities” . Although our results are preliminary, they demonstrate the need for additional research on these demographic shifts and their effects on access to spaces for physical activity, particularly for low-income and racial/ethnic minority communities.\n\n【34】Fourth, our study found that faith communities did not cite liability concerns or maintenance costs as the primary reasons for not sharing their facilities. These finding are similar to those of another study in North Carolina that examined shared-use practices in schools . In our study, although faith communities did express concerns about maintenance and liability, the barriers most frequently cited were not knowing how to start the shared-use process and not being asked by outside groups or individuals to share their facilities.\n\n【35】Our study identified faith communities as potentially untapped resources for shared use and increasing physical activity. From a social-ecological perspective, place-based physical activity interventions should consider the social, physical, and organizational environment to maximize usage and promotion of shared use of facilities for physical activity . A follow-up study examining the supporting practices and characteristics of faith community facilities with varying levels of shared use could provide a better understanding of the effectiveness of shared use and, more importantly, identify strategies for increased use of these facilities.\n\n【36】This study has several limitations. First, it was not designed to serve as a comprehensive assessment of the facilities of faith communities in North Carolina. Because the study was based on a convenience sample, the results may not be representative of all faith-based organizations in the state. Because we sent the survey to a wide network of practitioners and partners, we could not determine a baseline number of faith communities to whom the survey was distributed, and therefore cannot calculate a response rate. This research focused on how faith communities shared their facilities, not whether they used the facilities of another organization. Second, the organizations that completed the assessment might represent those most interested in the topic of shared use or in promoting physical activity. As a result, a higher number of faith-based organizations that allow community access to their facilities might be represented in our data. Our data suggest the need for additional, larger studies that examine shared use policies and practices in faith communities. Third, we did not include denomination or religious affiliation in the survey, which has led us to adapt the survey instrument to include that information for future use. Fourth, the logistic regression had limitations. Because of the small sample size, some variables had small numbers of outcome events, which led to wide confidence intervals for the odds ratios. Therefore, associations between variables should be interpreted with caution; a larger sample of faith communities is needed for more precise estimates.\n\n【37】Our findings suggest that faith communities are apt partners for increasing shared use in the community setting. Faith communities have facilities that can be used for various physical activities: indoor classrooms can be used for fitness classes; gymnasiums can be used for free play, games, or fitness classes; outdoor spaces can be used for organized games or free play; and large open spaces (including parking lots) can be opened up for biking, walking, and other activities. Faith community partnerships that promote shared use could be particularly important for communities that have persistent health disparities, including low-income and racial/ethnic minority communities, where access to spaces for physical activity may be limited.\n\n【38】Tables\n------\n\n【39】#####  Table 1. Characteristics of the Sample of Faith Communities (N = 234) Participating in a Study of Share-Use Facilities for Physical Activity, North Carolina,\n\n| Variable | No. (% a ) |\n| --- | --- |\n| **Size of faith community, no. of members b (median = 200)** | **Size of faith community, no. of members b (median = 200)** |\n| Small (<120) | 78 (34.4) |\n| Medium  | 75 (33.0) |\n| Large (≥300) | 74 (32.6) |\n| **County economic tier c** | **County economic tier c** |\n| Tier 1 | 44 (18.8) |\n| Tier 2 | 96 (41.0) |\n| Tier 3 | 94 (40.2) |\n| **County health ranking d (median = 49)** | **County health ranking d (median = 49)** |\n| Low  | 77 (32.9) |\n| Middle  | 79 (33.8) |\n| High  | 78 (33.3) |\n| **Percentage of county population that is black e (median = 20.7%)** | **Percentage of county population that is black e (median = 20.7%)** |\n| Low (≤10) | 67 (28.6) |\n| Moderate  | 89 (38.0) |\n| High (≥31) | 78 (33.3) |\n| **Percentage of county that is rural f (median = 42.7%)** | **Percentage of county that is rural f (median = 42.7%)** |\n| Low (<20) | 81 (34.6) |\n| Moderate  | 75 (32.1) |\n| High (≥50) | 78 (33.3) |\n| **Share facilities** | **Share facilities** |\n| Yes | 194 (82.9) |\n| No | 39 (16.7) |\n| Did not answer question g | 1 (0.4) |\n\n【41】a  Percentages may not sum to 100 because of rounding.  \nb  Several faith communities reported their faith community size as a range and were thus not included in this analysis.  \nc  Of the 100 counties in North Carolina, the 40 most distressed counties were designated as Tier 1, the next 40 as Tier 2, and the 20 least distressed as Tier 3. Data source: North Carolina Department of Commerce .  \nd  Data source: University of Wisconsin Population Health Institute . Each county was ranked “according to summaries of a variety of health measures,” with 1 being the healthiest and 100 being the least healthy.  \ne  Data source: US Census Bureau .  \nf  Data source: US Census Bureau .  \ng  Although 1 faith community did not respond to this question, it did answer questions related to use of facilities.\n\n【42】#####  Table 2. Types of Most Frequently Shared Facilities and Types of Shared Use, Study of Share-Use Facilities for Physical Activity Among Faith Communities in North Carolina, 2013 a  \n\n| Type of Facility | Faith Communities That Have Type of Facility b (N = 234) | Shared Facility c | Type of Agreement d |\n| --- | --- | --- | --- |\n| Formal | Informal | No Agreement | Did Not Indicate Type |\n| --- | --- | --- | --- |\n| Classroom/meeting space | 186 (79.5) | 167 (89.8) | 86 (51.5) | 76 (45.5) | 3 (1.8) | 2 (1.2) |\n| Gymnasium | 59 (25.2) | 39 (66.1) | 22 (56.4) | 16 (41.0) | 1 (2.6) | 0 |\n| Playground | 115 (49.1) | 68 (59.1) | 15 (22.1) | 34 (50.0) | 18 (26.5) | 1 (1.5) |\n| Athletic/open field | 69 (29.5) | 38 (55.1) | 12 (31.6) | 18 (47.4) | 6 (15.8) | 2 (5.3) |\n| Other facility | 194 (82.9) | 188 (96.9) | NA e | NA e | NA e | NA e |\n\n【44】a  All values are number (percentage). Percentages may not add to 100 because of rounding.  \nb  Percentage calculated according to number who responded to question (n = 234).  \nc  Percentage calculated according to number of respondents that had the type of facility.  \nd  Percentage calculated according to number of respondents that shared facility.  \ne  NA, not applicable. Survey did not ask about type of policy or agreement for shared “other facilities.”\n\n【45】#####  Table 3. Unadjusted Odds Ratios for Likelihood of Shared Facilities, by Faith Community and County Characteristics, Study of Shared-Use Facilities for Physical Activity in North Carolina,\n\n| Characteristics | Odds Ratio (95% Confidence Interval) | _P_ Value | Model _R_ 2 |\n| --- | --- | --- | --- |\n| **Size of faith community, no. of members** | **Size of faith community, no. of members** | **Size of faith community, no. of members** | **Size of faith community, no. of members** |\n| Small (<120) | 1 \\[Reference\\] | 1 \\[Reference\\] | 0.03 |\n| Medium  | 1.03 (0.46–2.27) | .94 | 0.03 |\n| Large (≥300) | 2.43 (0.94–6.31) | .07 | 0.03 |\n| **County economic tier a** | **County economic tier a** | **County economic tier a** | **County economic tier a** |\n| Tier 1 | 1 \\[Reference\\] | 1 \\[Reference\\] | 0.08 |\n| Tier 2 | 0.80 (0.34–1.92) | .62 | 0.08 |\n| Tier 3 | 3.19 (1.10–9.25) | .03 | 0.08 |\n| **County health ranking** b | **County health ranking** b | **County health ranking** b | **County health ranking** b |\n| Low  | 1 \\[Reference\\] | 1 \\[Reference\\] | 0.05 |\n| Middle  | 1.64 (0.74–3.61) | .22 | 0.05 |\n| High  | 3.32 (1.31–8.45) | .01 | 0.05 |\n| **Percentage of county population that is black c** | **Percentage of county population that is black c** | **Percentage of county population that is black c** | **Percentage of county population that is black c** |\n| Low (≤10) | 1 \\[Reference\\] | 1 \\[Reference\\] | 0.04 |\n| Moderate  | 0.20 (0.25–1.33) | .20 | 0.04 |\n| High (≥31) | 0.38 (0.58–4.22) | .38 | 0.04 |\n| **Percentage of county that is rural d** | **Percentage of county that is rural d** | **Percentage of county that is rural d** | **Percentage of county that is rural d** |\n| Low (<20) | 1 \\[Reference\\] | 1 \\[Reference\\] | 0.06 |\n| Moderate  | 0.28 (0.11–0.71) | .007 | 0.06 |\n| High (≥50) | 0.47 (0.18–1.24) | .47 | 0.06 |\n\n【47】a  Of the 100 counties in North Carolina, the 40 most distressed counties were designated as Tier 1, the next 40 as Tier 2, and the 20 least distressed as Tier 3. Data source: North Carolina Department of Commerce .  \nb  Data source: University of Wisconsin Population Health Institute . Each county was ranked “according to summaries of a variety of health measures,” with 1 being the healthiest and 100 being the least healthy.  \nc  Data source: US Census Bureau .  \nd  Data source: US Census Bureau .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e1c0eacf-33b6-418c-b198-d28a6a8e047a", "title": "Risk for SARS-CoV-2 Infection in Healthcare Workers, Turin, Italy", "text": "【0】Risk for SARS-CoV-2 Infection in Healthcare Workers, Turin, Italy\nThe ongoing coronavirus disease (COVID-19) pandemic is having an unprecedented impact on the worldwide population. Seroconversion for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was described to occur 7–14 days after onset of symptoms, 100% within 19 days after clinical onset . Recent serologic data suggest that, in affected areas, SARS-CoV-2 infection had been acquired by more persons than what could be extrapolated by PCR analysis of nasopharyngeal swab specimens .\n\n【1】Large studies reported seroprevalences of 1%–6.9% . In February 2020, seroprevalence for 12 blood donors in Lodi, Italy, a heavily affected zone, was as high as 23% . Studying high-risk persons, such as healthcare workers, could be relevant for implementing preemptive and protective strategies. In Italy, 30,383 healthcare workers (of 253,619 confirmed cases; 12.0%) have been reported to be infected since the beginning of the pandemic .\n\n【2】Active healthcare workers (n = 7,457) from **Azienda Sanitaria Locale** Città di Torino public hospitals and outpatient services (Turin, Italy) were invited by email and printed leaflets to participate in our study. During April 17–May 20, 2020, they underwent blood withdrawal. SARS-CoV-2 antibodies were measured by using capillary electrophoresis and chemiluminescence immunoassay targeting IgGs against S1/S2 regions of spike protein . This assay has a sensitivity of 97.9% and a specificity of 98.5% and a 94.4% positive agreement with the plaque reduction neutralization test . SARS-CoV-2 IgG concentrations were expressed in arbitrary units/mL (AU/mL) and deemed negative if <12 AU/mL. Persons who had equivocal (12–15 AU/mL) or positive (>15 AU/mL) results provided nasopharyngeal swab specimens for SARS-CoV-2 RNA detection by using an in-house real-time reverse transcription PCR, according to Corman et al.\n\n【3】Ethics approval was obtained, and all participants signed an informed consent form. Anonymous data were collected and analyzed by using SPSS Statistics version 26  and described as number (%) or mean ± SD. Disease severity information was not collected.\n\n【4】We tested 5,444 (73.0%) of 7,457 healthcare workers; 4,068 (74.7%) were women. Participants had a mean ± SD age of 49.4 ± 10.6 years. S1/S2 SARS-CoV-2 antibodies were found in 377 (6.9%) participants; 176 (46.7%) had cured COVID-19, 146 (38.7%) had contacts with COVID-19 patients, and 55 (14.6%) had no known epidemiologic link. Seroprevalence was not significantly higher in men than in women (7.9% vs. 6.5%; p = 0.097 by χ 2  test), and no differences were observed among age groups. Mean ± SD IgG titer was 49.2 ± 39.5 AU/mL. IgG titers were higher in older participants (Pearson r = 0.227, p<0.001 and p = 0.001 by analysis of variance) and in those previously given a diagnosis of COVID-19 (57.9 AU/mL vs. 41.6 AU/mL in those without a previous diagnosis; p<0.001 by _t_ \\-test).\n\n【5】Detailed task information was available for 4,630 participants. Seroprevalence was highest in laboratory personnel (18/175, 10.3%), although numbers were small, followed by nurse assistants (44/520, 8.5%), nurses (150/1983, 7.6%), and doctors (55/755, 7.3%). A significantly higher seroprevalence was observed in healthcare workers working in close contact with patients versus those with limited/indirect contacts (7.5% vs. 5.2%; p = 0.013 by χ 2  test; odds ratio 1.464, 95% CI 1.077–1.992) .\n\n【6】Among persons who had a previously diagnosed SARS-CoV-2 infection, 176 (82.6%) had S1/S2 SARS-CoV-2 antibodies. Participants without S1/S2 SARS-CoV-2 antibodies were younger (41.4 vs. 49.1 years; p<0.001) and had a shorter time since diagnosis (36 vs. 44 days; p = 0.008). When we excluded persons who previously had COVID-19, all serology-positive participants (n = 201) provided a nasopharyngeal swab specimen for detection of SARS-CoV-2 RNA; 7 (3.5%) were positive.\n\n【7】We found that SARS-CoV-2 infection had been acquired by 6.9% of healthcare workers in Torino, Italy. Variable seroprevalence has been described among healthcare workers in Belgium , Spain , and Germany  (1.6%–9.3%): no major difference in IgG prevalence was found according to job types. In our study, the highest prevalence was observed for healthcare workers in direct contact with patients and the lowest for administrative staff members. S1/S2 IgG titers were higher in older participants and in those who had a previous diagnosis of COVID-19. In an assay validation study in Boise, Idaho, USA, a seroprevalence of 1.79% was reported; older participants had the highest rates (4%, >80 years of age).\n\n【8】Higher titers in symptomatic patients (we presume were healthcare workers given a diagnosis of COVID-19 according to the local testing policy) have been described . Although a shorter time from disease onset might explain the lack of antibodies, a lower seroprevalence in younger, previously infected healthcare workers was unexpected. A total of 3.5% of seropositive participants with no previous diagnosis of COVID-19 had positive PCR results for nasopharyngeal swab specimens; this finding might represent late-stage infections with low/no infectivity.\n\n【9】Our study has limitations, including incomplete coverage of healthcare workers (27% did not respond) and lack of complete job description and disease severity for all participants. Some persons did not show development of IgG after having COVID-19; thus, our study could have missed a subset of previously infected persons . Despite limitations, our study provides noteworthy estimates about the differential risk for acquiring SARS-CoV-2 infection by healthcare workers according to their specific job setting in a large occupational survey.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "65821d18-b3bd-4b52-bea7-c90e2c264938", "title": "Alfred Russel Wallace and the Antivaccination Movement in Victorian England", "text": "【0】Alfred Russel Wallace and the Antivaccination Movement in Victorian England\nIn 2009, the scientific community commemorated the 200th birthday of Charles Darwin and the 150th anniversary of the publication of On the Origin of Species by Means of Natural Selection. These occasions also directed the view of a wider public to the unjustly neglected figure of Alfred Russel Wallace  , explorer and codiscoverer of the principle of natural selection. In the past few years, Wallace’s work has in fact enjoyed increasing attention among the historians of science, as several new biographies and studies prove . But unlike Darwin, Wallace always was and probably will remain a serious challenge to the history of science: he stubbornly refuses to fit into the mold of the typical scientific hero. Wallace made without any doubt lasting contributions to biologic science, but the second half of his life was by and large devoted to what from today’s perspective are utterly lost causes: He became a passionate advocate of spiritualism, supported land nationalization, and fervently objected to compulsory smallpox vaccination.\n\n【1】The motives behind Wallace’s campaigns are sometimes difficult to fathom. He published copiously because this served for a long time as his major source of income, but these writings only show the public face of Wallace. Unlike Darwin, Wallace did not leave behind a large number of private letters and other personal documents; therefore, his more private thoughts, motives, and deliberations will probably remain unknown.\n\n【2】I provide a short introduction to Wallace’s life and work and then describe his contributions to the British antivaccination campaigns. Wallace’s interventions were influential; he was popular and well liked inside and outside scientific circles and, despite his controversial social reformism, commanded deep respect for his achievements and his personal qualities until the end of his long life.\n\n【3】I also briefly analyze the similarities and differences between the Victorian and contemporary vaccination debates. It has recently been argued that comparative historical analysis can play a major role in public health policy . In contemporary vaccination controversies, history is frequently used as a source of arguments , but the historical argument often is not based on up-to-date historical understanding. The polarizing controversies surrounding vaccination have never completely gone away, and the nearly unbroken tradition of debate apparently entices participants to reuse old arguments without making certain that their context is still valid. Vaccination involves national and international politics and the deeply personal sphere of child care. It is thus probably inevitable that culturally influenced ideas of bodily integrity and health from time to time are at odds with so-called vaccination technocracies .\n\n【4】### Alfred Russel Wallace\n\n【5】Alfred Russel Wallace’s humble origins contrast sharply with Charles Darwin’s privileged background. Wallace was born on January 8, 1823, in the Welsh village of Llanbadoc into an impoverished middle-class family. In 1836, when his parents could no longer support him, he was taken out of school to earn a living. He joined his brother John in London to work as a builder. In London, he regularly attended meetings at the Hall of Science in Tottenham Court Road, where followers of the utopian socialist Robert Owen lectured. Thus, as an adolescent, he became acquainted with radical sciences such as phrenology . In 1841, when Wallace was working as a land surveyor in Wales, a slump in business enabled him to devote more time to his developing interests in natural history. A few years later, while working as a teacher in Leicester, Wallace met the 19-year-old amateur entomologist Henry Walter Bates, who introduced him to beetle collecting. Wallace returned to Wales, but he stayed in touch with Bates; in their letters they discussed natural history and recent books. In 1847, inspired by reading the best-selling and scandalous Vestiges of the History of Creation, an anonymously published book that offered a naturalistic, developmental history of the cosmos and life, Wallace and Bates decided to travel to the Amazon River basin to study the origin of species, paying for their journey by working as professional specimen collectors.\n\n【6】Wallace spent the next 14 years of his life, interrupted only by a stay in England from October 1852 until early April 1854, collecting specimens in the Amazon Basin and the Malay Archipelago. As with Darwin, the geographic variation of supposedly stable species nurtured in Wallace the idea of organic change. An 1855 paper, On the Law Which Has Regulated the Introduction of New Species, is Wallace’s first formal statement of his understanding of the process of biological evolution. In this paper, he derives the law that “every species has come into existence coincident both in time and space with pre-existing closely allied species.” In February 1858, while having a severe malaria attack, Wallace connected the ideas of Thomas Malthus  on the regulation of populations with his earlier reasoning and developed a concept that was similar to Darwin’s mechanism of natural selection. Eager to share his discovery, Wallace wrote an essay on the subject as soon as he had recovered and sent it off to Darwin. This innocent act by Wallace set off the well-known and often recounted story of Darwin’s hurried writing and publication of On the Origin of Species.\n\n【7】Wallace returned to England in 1862 after the initial storm of reaction to Darwin’s theory had blown over. Together with Thomas Henry Huxley , he became one of the most vocal defenders of the theory of evolution. In the years up to 1880 he also wrote a large number of essays, letters, reviews and monographs that secured his position as one of the foremost naturalists in the United Kingdom; this status, however, did not translate into a permanent position or even some semblance of financial security. Only in 1881, after an intervention by Darwin and other eminent scientists, did he receive a Civil List Pension of 200£ per year. After 1880, having finished most of his major monographs, Wallace more and more directed his attention toward social issues and turned into a social radical—his conversion to spiritualism had already occurred in the 1860s. He remained faithful to his radical course until his death in 1913.\n\n【8】The first Vaccination Act in England was passed in 1840; it outlawed variolation (i.e. the practice of infecting a person with actual smallpox) and provided vaccination that used vaccines developed from cow pox or vaccinia virus free of charge. The 1853 Act made vaccination mandatory and included measures to punish parents or guardians who failed to comply. Changes in the law passed in 1867 permitted the authorities to enforce vaccination more efficiently. The law allowed the repeated prosecution of parents who failed to have their child vaccinated. The 1871 Act authorized the appointment of vaccination officers, whose task it was to identify cases of noncompliance. In 1889, in response to widespread public resistance, Parliament appointed a Royal Commission to draft recommendations to reform the system. The Commission published its conclusions in 1896. It suggested allowing conscientious objection, an exemption which passed into law in 1898. In the early 20th century, < 200,000 exemptions were granted annually, representing ≈25% of all births .\n\n【9】The first vaccination act mainly incited resistance from heterodox medical practitioners who were forced out of business. Large-scale popular resistance began after the 1867 Act with its threat of coercive cumulative penalties. The social and political diversity of the British antivaccination movement is vividly described by Durbach . Many of the ≈200 organizations were quite eccentric, even by the standards of the time. However, Durbach’s analysis and other analyses  show that it is not correct to portray antivaccinationists indiscriminately as antirational, antimodern, and antiscientific. Just considering the details of the vaccination practice of the mid-19th century does much to make many criticisms understandable. For instance, the widespread arm-to-arm vaccination, used until 1898, carried substantial risks, and the instruments used  could contribute to severe adverse reactions. Furthermore, many antivaccinationists appealed, like their opponents, to enlightenment values and expertly used quantitative arguments.\n\n【10】Wallace himself apparently did not hold strong opinions about vaccination until the mid-1880s. He had received a vaccination as a young man before he left for South America, and all 3 of his children were vaccinated as well. Wallace was recruited some time in 1884 to the antivaccination movement through the efforts of his fellow spiritualist William Tebb , a radical liberal who in 1880 had cofounded the London Society for the Abolition of Compulsory Vaccination. Wallace’s commitment to the antivaccination cause was without doubt motivated by his social reformism, which in turn was underpinned by spiritualism and Swedenborgianism . These metaphysical foundations led him to a holistic view of health; he was convinced that smallpox was a contagious disease, but he also was certain that differences in susceptibility caused by nutritional or sanitary deficiencies played a major role in the epidemiology of the disease.\n\n【11】Despite his strong metaphysical commitments, Wallace, however, always remained a devoted empiricist and was among the first to use a statistics-based critique of a public health problem. Some of the groundwork for Wallace’s quantitative critique was laid by the highly regarded, but controversial, physicians Charles Creighton  and Edgar Crookshank . They attacked simplistic interpretations of and conclusions from Edward Jenner’s work  and demonstrated how difficult it is to determine vaccination success and vaccination status and to know what kind of contagion was actually used in an inoculation or vaccination. In works such as Vaccination Proved Useless and Dangerous  or Vaccination a Delusion, Its Penal Enforcement a Crime , Wallace mounted his attack on several claims: 1) that death from smallpox was lower for vaccinated than for unvaccinated populations; 2 ), that the attack rate was lower for vaccinated populations: and 3 ) that vaccination alleviates the clinical symptoms of smallpox.\n\n【12】Both provaccinationists and antivaccinationists relied heavily on time series of smallpox mortality rate data, which showed a general decline over the 19th century overlaid by several smaller epidemic peaks and the large pandemic peak of 1870–1873. Their conclusions from these data differed according to the way these data were subdivided into periods . For example, if it were assumed that vaccination rates increased in 1867, when cumulative penalties were introduced and fewer dared to challenge the vaccination law, and not in 1871, when the smallpox pandemic accelerated, then the rate of decline of smallpox mortality rates was lower when vaccination was more prevalent. Wallace concluded from his analysis that smallpox mortality rates increased with vaccination coverage, whereas his opponents concluded the exact opposite. Wallace argued that the problem of determining vaccination status was serious and undermined the claims of his opponents. He asserted that the physicians’ belief in the efficacy of vaccination led to a bias in categorizing persons on the basis of interpretation of true or false vaccination scars. Additionally, epidemiologic data for vaccination status were seriously incomplete. Depending on the sample, the vaccination status of 30%–70% of the persons recorded as dying from smallpox was unknown. Furthermore, if a person contracted the disease shortly after a vaccination, it was often entirely unclear if the patient should be categorized as vaccinated or unvaccinated. Provaccinationists argued that the error introduced by this ambiguity was most likely to be random and thus would not affect the estimate of the efficiency of the vaccine. In contrast, Wallace believed that doctors would have been more willing to report a death from smallpox in an unvaccinated patient and that this led to a serious bias and an overestimation of vaccine efficiency.\n\n【13】Wallace’s holistic conception of health influenced his argument as well. He was convinced that susceptibility to the disease of smallpox was not distributed equally across social classes. Weakened, poor persons living in squalor were in his opinion less likely to get vaccinated. At the same time they would have higher smallpox mortality rates because their living conditions made them more susceptible to the disease. He supported his hypothesis that susceptibilities differ with the observation that the mortality rate of unvaccinated persons had increased to 30% after the introduction of vaccination, while the vaccinated had enjoyed a slight survival advantage. This demonstrated to Wallace that factors other than vaccination must have played a major role.\n\n【14】### Conclusions\n\n【15】The numerical arguments used by Wallace and his opponents were based on an actuarial type of statistics, i.e. the analysis of life tables and mortalities. Inferential statistics that could be more helpful in identifying potential causes did not yet exist. The statistical approach to the vaccination debate used by Wallace and his opponents could simply not resolve the issue of vaccine efficiency; thus, each side was free to choose the interpretation that suited its needs best. However, despite its indecisive outcome, the debate was a major step in defining what kind of evidence was needed . It is also unjustified to portray the debate as a controversy of science versus antiscience because the boundaries between orthodox and heterodox science we are certain of today were far less apparent in the Victorian era . What the scope and methods of science were or should be were topics still to be settled. It is thus unwarranted to portray the 19th-century antivaccination campaigners generally as blindly religious, misguided, or irrational cranks. This judgment certainly does not apply to Alfred Russel Wallace.\n\n【16】Wallace was modern, but he represented an alternative version of modernity, a version that has been sidelined in historiography until recently but has lately been acknowledged as a central cultural feature of the late 19th century . Movements such as spiritualism were not resurrections of ancient traditions but used interpretations of the most recent natural science, such as experimental psychology, evolutionary biology, and astronomy , or electromagnetism . Some, like Wallace, also contested the social role that emerging professional sciences should play. Wallace strongly favored a natural science that also addressed moral, political, social, and metaphysical concerns, and with this inclination he ran against the tide that was more concerned with developing a barrier between politics and disinterested, objective science. In the case of vaccination, Wallace argued that liberty and science need to be taken into account, but that liberty is far more important than science. Wallace only appears to have been such a heretical figure if a large portion of the social, political, and intellectual reality of Victorian and Edwardian England is blotted out of the picture.\n\n【17】To argue that, then as now, the controversies are between religiously motivated, irrational eccentrics and rational, disinterested science is historically inaccurate and distracts from substantial differences in social, political, and economic context between then and now. The Victorian vaccination legislation was part of an unfair, thoroughly class-based, coercive, and disciplinary healthcare and justice system: poor, working-class persons were subjected to the full force of the law while better-off persons were provided with safer vaccines and could easily avoid punishment if they did not comply. The National Health Service, established in 1948, was planned to bring more social justice to health care. The new health system no longer was stigmatizing and coercive. The development has not stopped there: today, there is an increasingly strong emphasis on individual choice and involvement in decision making in the healthcare system in Great Britain. Patients have become customers. The contemporary vaccination controversy has to be seen against the opportunities and challenges offered within this new environment. It has become evident that population-based risk assessments of vaccine safety often fail to convince in this new context . Parents instead evince a clinical, individual-based attitude when assessing the risks of vaccination—their own children are often judged not to be average.\n\n【18】In Great Britain, such attitudes are reinforced by the recent developments, mentioned above, in the healthcare system that encourage choice and autonomy and also by individualized perspectives concerning parenting and child development. Such a clinical perspective of parents can, however, cut both ways. The individually witnessed causal relationship between therapy and recovery in the case of tetanus and diphtheria was instrumental in the widespread public acceptance of immunization . A similar mechanism is at play in the contemporary controversies: perceived causal relationships between vaccination and the appearance of complications undermine the claims that vaccines are generally safe.\n\n【19】This analysis also illustrates that contemporary vaccination controversies take place in specific historical contexts. Colgrove  depicts in detail how vaccination became an accepted public health intervention in the United States and what factors have fueled and influenced historical and contemporary controversies. For example, compared with most countries in Europe, the risk of costly litigation for pharmaceutical companies in the United States is much higher and the role of the state is seen as far more restricted. This specific background influences forms of provaccination and antivaccination campaigning, but it also needs to be taken into account that the increasing availability of Internet resources accessible from everywhere may contribute to making the arguments and the debate more uniform across the globe.\n\n【20】Modern vaccines save lives. But worries surrounding vaccination need to be taken seriously. And the lessons taught by history are, as usual, complex. As pointed out forcefully by Leach and Fairhead , vaccine delivery systems must suit social, cultural, and political realities. Paternalistic and coercive attitudes were harmful in the 19th century and are even less appropriate in the 21st century.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ea9efbcd-16b8-4030-a3b8-57dd8327f893", "title": "Hantavirus Pulmonary Syndrome—The 25th Anniversary of the Four Corners Outbreak", "text": "【0】Hantavirus Pulmonary Syndrome—The 25th Anniversary of the Four Corners Outbreak\nOn the morning of May 14, 1993, a 19-year-old Native American man was traveling by car through the Four Corners region of New Mexico, USA—the area where New Mexico, Arizona, Colorado, and Utah meet—when he became so severely short of breath that his alarmed accompanying family members pulled into a nearby service station to call for help. By all measures, the young man, a competitive marathon runner of local renown, had been in previous good health. A few days earlier, he had visited an outpatient clinic because of fever and myalgia, was treated symptomatically, and was well enough early on the morning of May 14 to embark on a trip from his home in Crownpoint, New Mexico, to Gallup, New Mexico. However, by the time the responding ambulance crew arrived, he had collapsed because of respiratory failure. He was taken to the emergency department at the Gallup Indian Medical Center, where he found to have florid pulmonary edema, and where, despite maximal resuscitative efforts, he died in the emergency department.\n\n【1】The emergency department medical staff was understandably bewildered as to why an extremely fit adolescent athlete would so swiftly die from acute pulmonary edema. In New Mexico, any unexplained, suspicious, or otherwise irregular death is, by law, reportable to the New Mexico Office of the Medical Investigator. The officer on duty that day in Gallup was a young investigator named Richard Malone.\n\n【2】After arriving at the hospital and hearing the clinical narrative, Malone was startled by the resemblance of this death to another death that he had investigated a few weeks earlier at the same facility. At that time, he had been called after a young woman, also a Navajo tribal member, had died from acute pulmonary edema without any clinical clues pointing to a distinct etiology. Malone had referred that case for a postmortem examination to Patricia McFeeley, a University of New Mexico pathologist who worked in conjunction with the office of the medical examiner. McFeeley had reported that the young woman had died from pulmonary edema that was evident by gross and microscopic examinations. The heart of this patient was structurally normal, and results of serologic and microbiologic tests were nonrevealing.\n\n【3】The pathologist was admittedly puzzled by the case and had discussed her uneasiness with Malone. McFeeley was again at work in Albuquerque on the morning of May 14, and when Malone called and shared his thoughts on the similarity of the 2 cases, she readily agreed to perform an autopsy on the deceased person. With that, Malone made his way toward the emergency department waiting room to approach the family of the young man about obtaining permission to transport the body to the state laboratory in Albuquerque. Mr. Malone expected that he would have to gently persuade the family to agree, because the Navajo people are generally resistant to any action that could be perceived as disturbing the newly dead. When he met the gathered family, he was shocked by their shared story.\n\n【4】The patient had been en route to Gallup from his home in the small Navajo reservation village of Crownpoint that morning to attend a funeral, which was about to begin at a mortuary literally across the street from the Indian Medical Center. The planned funeral was that of his fiancée, the 21-year-old mother of his infant child. The young woman, who was also an active runner, had died only days earlier at an outlying rural reservation clinic. She had also complained only of antecedent fever and myalgia, and the decline in her health had been so precipitous at the remote clinic that there had been inadequate time to transport her to a fully staffed facility. Because Crownpoint is located on the Navajo reservation and governed by tribal rather than state law, the clinic there was not required to adhere to reporting requirements of New Mexico. Consequently, Malone’s office had no record of her death or of the surrounding circumstances. Malone recognized the relevance of this small cluster of cases, and after quickly updating McFeeley by telephone, he convinced the family of the young woman to allow her remains to be examined in Albuquerque. Malone invoked the health of their surviving infant child as a deciding factor in convincing reluctant family members to allow the state to proceed with their autopsies.\n\n【5】After ensuring that both bodies had been secured for transport, Malone sought out Bruce Tempest, the physician who served as the medical director for the Gallup Indian Medical Center. While he listened to Malone’s report, Tempest remembered that he had been involved in at least 2 recent informal consultations with other physicians who had cared for young, previously healthy tribal members who had died in a dramatic fashion from a mysterious respiratory illness. Both men agreed that immediate further action was mandated. They decided that Malone would scour the records of the state coroner for information, and that Tempest would survey his clinical colleagues in the Four Corners area for similar cases.\n\n【6】The postmortem examinations of the 2 new case-patients showed only unexplained, severe pulmonary edema. Malone and Tempest quickly uncovered several new suspicious cases from the preceding few months, and on May 17, 1993, the New Mexico Department of Health was notified of their concerns. The state officials crafted a letter that was sent to clinicians in the 4-state area of Arizona, Colorado, New Mexico, and Utah. The communication offered a brief description of the cases to date and asked that any similar cases be reported immediately to them. The mailing was effective in identifying several other potential cases.\n\n【7】Unfortunately, soon thereafter, when the lay press reported that an unexplained illness was killing young tribal members throughout the Four Corners region, a near panic of the general populace ensued. Navajo and Hopi people were shunned, disinvited from regional athletic events, and made to feel unwelcome in public places. Politicians were pressured to act. On May 28, the Friday afternoon of Memorial Day weekend, New Mexico state health officials contacted the Centers for Disease Control and Prevention (CDC), described their predicament, and asked for expert assistance.\n\n【8】Within hours of the call for help, a team of investigators assembled and mobilized. Jay Butler, an experienced epidemiologist in the Epidemic Intelligence Service at CDC, was designated as the leader. Two young Epidemic Intelligence Service officers (Ronald Moolenar and Jeffrey Duchin) assisted him. Less than 24 hours after the group had been organized, they arrived at the Albuquerque airport and were shuttled to the campus of the University of New Mexico, where they were joined by members of the University of New Mexico medical faculty, Indian Health Service physicians, and various other state and federal health officials.\n\n【9】The first order of business was case definition, and the health officials agreed to evaluate any patient from the area who, going forward from January 1, 1993, had demonstrated imaging evidence of unexplained bilateral infiltrates with associated hypoxemia. The team would also evaluate any death that had occurred with unexplained pulmonary edema. More than 30 suspected cases, with varying degrees of available clinical information, were presented to the group. The assembly then evolved into a brainstorming session, where participants were invited to offer their thoughts about potential etiologies of the outbreak. Various ideas were put forth, ranging from the exotic to the mundane. Plague, tularemia, anthrax, and multiple other potential diseases were dismissed as possibilities because of a lack of any corroborating evidence.\n\n【10】By the end of the long weekend, the consensus was that the outbreak was the result of 1 of 3 possible causes. The first consideration was that of a new, aggressive, and previously unrecognized type of viral influenza. The second was that an environmental toxin was the causative agent, which was certainly plausible in an agricultural area with a less than optimal regulatory climate and a history of military weapons testing. The third listed possibility was the most fascinating: that a previously unrecognized pathogen was the cause of the epidemic .\n\n【11】On Tuesday, June 1, fifteen members of the CDC team began an on-site, meticulous, review of medical records. They also procured tissue specimens from suspected cases, which were flown to CDC headquarters in Atlanta, Georgia, for immediate analysis. Epidemiologists interviewed patient and control families and performed detailed inspections of their homes and workplaces.\n\n【12】By Friday, June 4, scientists of the Special Pathogens Branch at CDC had tested extracted IgM from 9 patients with a panel of 25 different virus stock samples from the laboratory at CDC. Antibody from all 9 patients showed cross-reactivity with each of 3 different hantavirus species and with none of the other 22 viruses. Hantaviruses were known to be the causative agents of a family of diseases of varying severity, collectively known as hemorrhagic fever with renal syndrome (HFRS), which affect patients in the Northern Hemisphere from Scandinavia to the Korean Peninsula. The 3 hantavirus samples initially tested were Hantaan virus, the cause of Korean hemorrhagic fever; Seoul virus, the causative agent of a form of HFRS common in Asia; and Puumala virus, the cause of a relatively mild form of HFRS in northern Europe. Shortly thereafter, the same samples were found to cross-react with Prospect Hill virus, which was known to infect voles in Maryland but had never been isolated from human tissue or associated with human disease .\n\n【13】Several members of the investigating team had extensive international infectious disease experience and knowledge of the epidemiology and clinical course of HFRS. The illness was known to be caused by different types of hantavirus and to be transmitted to humans by inhalation of virus shed in rodent droppings. The syndrome is characterized by an enormous change in vascular endothelial permeability, predominantly in the kidney, with the loss of massive amounts of intravascular fluid into the renal extravascular parenchyma and retroperitoneal space. The degree of intravascular fluid depletion is so severe that hemoconcentration occurs, and patients often have pronounced increases in hemoglobin concentrations and hematocrit values.\n\n【14】The clinicians of the investigation team had noted high levels of hemoconcentration in several of the potential cases and, in light of the CDC findings, they suspected that they were now dealing with a new hantavirus disease. This conclusion was a substantial leap of thought for several reasons. At the time, in the Western Hemisphere, hantaviruses were recognized as infecting only rodents, and no case of human disease had been described. In addition, the study group patients had little evidence of renal involvement; the predominant target organ was the lung in all cases. Undeterred by these discrepancies, some group members postulated, in a profoundly prescient fashion, that the outbreak was caused by an as-yet-unrecognized hantavirus that targeted the pulmonary capillary endothelium.\n\n【15】Acting upon the new information, CDC dispatched a rodent trapping team to New Mexico. Over the ensuing week, ≈1,700 rodents were captured at patient and control sites. The most commonly secured rodent was _Peromyscus maniculatus_ , the deer mouse .\n\n【16】Concurrently, the Special Pathogens Branch in Atlanta worked feverishly to uncover the new hantavirus. On June 10, using reverse transcription PCR technology, these scientists were able to obtain a sequence from the medium segment of the RNA strand of the suspected virus. The Viral Pathology Laboratory also identified hantaviral antigens in endothelium of the pulmonary capillary bed and other tissues . Less than 1 week later, on June 16, the same team identified an identical virus basepair sequence, as well as a prevalence of hantavirus antibody, from _Peromyscus maniculatus_ mouse specimens trapped on site . The virus and its rodent reservoir had been definitively identified less than 3 weeks after CDC had assembled its task force.\n\n【17】The new virus proved difficult to culture, and it was not until November 1993 that teams from the CDC and the US Army Medical Research Institute of Infectious Diseases (Fort Detrick, MD, USA) were able to culture the virus. Their initial recommendation was to name the pathogen Muerto Canyon virus, after an involved area on the Navajo Reservation. The Navajo people reacted strongly against any further association with the disease that had led to so much initial prejudice, and tribal elders appealed to officials to reconsider. Ultimately, the new agent was officially named Sin Nombre virus (virus with no name).\n\n【18】While the bench scientists were successfully identifying the pathogen, epidemiologists and clinicians were clarifying the clinical course of the newly recognized syndrome. Eighteen patients were found to have had either serologic or PCR evidence of infection. These patients were mostly young adults, with a noticeable sparing of the extremes of life. Physical examinations were remarkable for fever, tachypnea, tachycardia, and hypotension. Severe pulmonary edema was nearly ubiquitous, and the mortality rate in the initial outbreak exceeded 75%. A distinct laboratory pattern was prominent, characterized by hypoxemia, leukocytosis with the presence of peripheral immunoblasts, hemoconcentration with a marked increase in hemoglobin and hematocrit, thrombocytopenia, and increased prothrombin and partial thromboplastin times. The predominant chest radiograph finding was bilateral parenchymal infiltrates. An unusual hemodynamic profile was also observed. Patients in whom pulmonary artery catheters had been placed showed a severe reduction in cardiac output and a marked increase in systemic vascular resistance, in association with normal or low pulmonary capillary wedge pressures, consistent with cardiogenic shock and noncardiogenic pulmonary edema. Histopathologic examination of the lungs of patients who died showed moderate lymphoid interstitial infiltration with severe alveolar edema .\n\n【19】The remarkable work of the Hantavirus Study Group, describing the newly defined hantavirus pulmonary syndrome (HPS), was published in the April 7, 1994, edition of The New England Journal of Medicine . A glowing editorial reviewing the team effort appeared in the same issue.\n\n【20】A burning question for the scientific community remained. Why did the outbreak occur in the Four Corners Region, and why did it happen in the spring of 1993? Biologists from the University of New Mexico happened to be studying the deer mouse population in that region at that time. They found that the mouse population in 1993 was 10-fold greater than it had been during the preceding spring. Working with a team of environmental scientists, those biologists demonstrated that, because of the increased moisture of an El Niño winter, there was a relative abundance of springtime vegetation in the Four Corners region that provided shelter and food for regional fauna. The resultant explosive growth of the rodent population was followed by increased human exposure to the deer mouse vector .\n\n【21】### HPS Today\n\n【22】In the ensuing 25 years, the epidemiology, virology, pathophysiology, clinical course, and treatment for HPS have been the focus of ongoing research. The Sin Nombre virus is a single-stranded, 3-segmented RNA virus of the family _Hantaviridae._ It is the cause of chronic, seemingly innocuous, and persistent infections in the host rodent. Like all hantaviruses, the Sin Nombre virus is principally associated with only 1 rodent species, in this case, the common deer mouse. Other rare hantavirus species in North America have been associated with variants of HPS, and they are also associated predominantly with 1 rodent species.\n\n【23】The disease remains extremely uncommon: <800 HPS cases have been reported in the United States during the past 25 years . There is a western states predominance of this disease, and almost all cases are caused by exposure at home or in the workplace . The temporal and spatial distribution of cases reflect fluctuations in the population of the rodent host as the virus is transmitted to humans by inhalation of aerosolized particles shed in rodent excreta. Human-to-human spread has never been reported in North America.\n\n【24】The virus has a remarkable predilection for pulmonary capillary endothelial cells and a complex and still poorly understood pathogenesis. Infection by inhalation is followed by an incubation period of 1–5 weeks. A 3–6-day prodromal period then occurs, during which patients first exhibit fever, with respiratory symptoms notably absent. The development of a cough signals the onset of the fulminant cardiopulmonary phase, which is characterized by severe capillary leak, extraordinary pulmonary edema, and myocardial depression, and lasts for up to 1 week. The current mortality rate during the cardiopulmonary phase is ≈40%. Survivors mobilize third-space fluid during the diuretic-recovery phase, which may last up to 2 weeks .\n\n【25】Most of the damage during the cardiopulmonary phase of HPS is directly related to cell-mediated immunity gone awry. During the incubation phase, there is a ubiquitous deposition of the virus within the pulmonary endothelium, with no associated changes in either the structural integrity or permeability of the microenvironment . During the relatively brief prodromal phase, circulating immunoblasts appear and humoral antibody is produced. It is during the cardiopulmonary phase that well-differentiated T cells appear on site and participate in the release of soluble mediators (among which tumor necrosis factor-α is prominent); massive changes in pulmonary capillary endothelial cell permeability result . Fluid loss into the alveolar and pleural spaces is so voluminous that the heart becomes preload deprived and cardiac output decreases. The same soluble mediators are in part responsible for depression of myocardial contractility that often leads to frank cardiovascular collapse .\n\n【26】A distinctive hematologic laboratory profile offers clues to the diagnosis of HPS. Researchers at the University of New Mexico have triaged patients with prodromal symptoms and a consistent exposure history by examining the peripheral blood smear for the presence of 5 factors: thrombocytopenia, hemoconcentration, a granulocytic left shift, the absence of toxic change, and the presence of >10% immunoblasts. If 4 of the 5 factors are present, there is ≈a 90% sensitivity and specificity for the disease . Definitive diagnosis of hantavirus infection relies on serologic testing for IgM and IgG, which is highly sensitive and specific, but because of travel times to laboratories performing the assay, it takes >72 hours in most cases to provide results.\n\n【27】Treatment of HPS is challenging. Because of the severity of endothelial leakage, fluid resuscitation can lead to worsening pulmonary edema. Because patients are maximally vasoconstricted, vasopressors are of little benefit. Inotropes, particularly dobutamine, are often used but have no demonstrated value. Steroids have been used, again with no proven benefit . Patients in the cardiopulmonary phase have a failing heart and failing lungs. Extracorporeal membrane oxygenation (ECMO) offers a supporting bridge to the diuretic-recovery phase. The University of New Mexico has pioneered the use of ECMO for HPS. Early efforts there to use ECMO were plagued by technical difficulty because health of HPS patients has a tendency to decline so rapidly that clinicians were often left attempting to obtain arterial and venous ECMO access for patients in full cardiac arrest . Physicians at the University of New Mexico have now developed a strategy of preemptively placing femoral arterial and venous access catheters in suspected case-patients at the time of hospital arrival, so that ECMO can be initiated at the earliest sign of decompensation. This strategy has resulted in an impressive 80% survival rate in patients despite overt cardiopulmonary collapse. Mortality rates in that subset of patients had previously exceeded 90% .\n\n【28】HPS was discovered and defined by the collaborative efforts of federal, state, and local investigators during the spring of 1993. The rapidity of the successful investigation was the result of the presence of competent persons at all levels and to the early actions of the New Mexico Office of Medical Investigation. HPS is a disease of healthy young persons who have a history of rodent exposure, usually at home or at work. A laboratory examination that demonstrates hemoconcentration, or the presence of immunoblasts on a peripheral blood smear, should raise immediate clinical suspicion. Severe capillary leak with massive pulmonary edema is the hallmark finding of HPS. Frederick Koster, an infectious disease physician in New Mexico who was part of the original Four Corners investigation, has described HPS associated pulmonary edema as “a disease manifestation without parallel in clinical medicine” . Suspected case-patients should always be referred to an ECMO-capable center without delay because health decline is precipitous, and ECMO may be lifesaving.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d680991e-2da1-462e-ac34-2517d73eece0", "title": "Health Care Costs and Participation in a Community-Based Health Promotion Program for Older Adults", "text": "【0】Health Care Costs and Participation in a Community-Based Health Promotion Program for Older Adults\nAbstract\n--------\n\n【1】**Introduction**EnhanceWellness (EW) is a community-based health promotion program that helps prevent disabilities and improves health and functioning in older adults. A previous randomized controlled trial demonstrated a decrease in inpatient use for EW participants but did not evaluate health care costs. We assessed the effect of EW participation on health care costs. **Methods**We performed a retrospective cohort study in King County, Washington. Enrollees in Group Health Cooperative (GHC), a mixed-model health maintenance organization, who were aged 65 years or older and who participated in EW from 1998 through 2005 were matched 1:3 by age and sex to GHC enrollees who did not participate in EW. We matched 218 EW participants by age and sex to 654 nonparticipants. Participants were evaluated for 1 year after the date they began the program. The primary outcome was total health care costs; secondary outcomes were inpatient costs, primary care costs, percentage of hospitalizations, and number of hospital days. We compared postintervention outcomes between EW participants and nonparticipants by using linear regression. Results were adjusted for prior year costs (or health care use), comorbidity, and preventive health care-seeking behaviors. **Results**Mean age of participants and nonparticipants was 79 years, and 72% of participants and nonparticipants were female. Adjusted total costs in the year following the index date were $582 lower among EW participants than nonparticipants, but this difference was not significant. **Conclusion**Although EW participation demonstrated health benefits, participation does not appear to result in significant health care cost savings among people receiving health care through a health maintenance organization.  \n\n【2】Introduction\n------------\n\n【3】Several health promotion and disease prevention programs designed for older adults have been developed and evaluated for their health benefits and resource use . These programs focus on improving older adults’ general health and encouraging self-management of chronic medical conditions. Specific aspects of health improvement, such as improving mental well-being or increasing physical activity, are often the focus of health improvement and are pursued because of a client’s interest and motivation. These programs connect clients with information and resources that help them address their personal health concerns, build confidence in health care decision making, and increase physical activity. Such health promotion programs for older adults improve health outcomes, and they have demonstrated decreased use of health care resources resulting from participation, which results in decreased health care costs . However, to our knowledge these studies used self-reported data rather than actual health care costs. EnhanceWellness (EW, formerly known as Health Enhancement Program, or HEP) targets older people at risk for functional decline. Nurses and social workers meet with community-living elders to help increase physical activity, promote social activity, improve mental health, and enhance self-management of chronic conditions to improve health and functioning. In a 1998 randomized controlled trial, EW participants increased their physical activity, decreased their use of psychoactive medications, and decreased their number of hospital days . After that study, senior centers in the Seattle, Washington, area began implementing EW, and enrollees of Group Health Cooperative (GHC), a consumer-governed, nonprofit health care system that provides both health care and medical coverage, started participating in the program. A follow-up study conducted in 2002 evaluated the program as it operated in the community, outside the controlled setting of a randomized trial . This study also demonstrated significant benefits, including a reduction in disability risk factors, improvement in health status, no decrease in functional status, and no increase in self-reported health care use. EW has been confused with the EnhanceFitness Program (EF) because of their similar names and the fact that both have been studied in a similar older adult population  However, the programs are distinct: EF is a group exercise program, whereas EW is a comprehensive, participant-centered wellness program that includes a health assessment, a tailored health plan, and motivational support to achieve a self-chosen goal. EW participants, if desired, may include regular physical activity and join EF, a covered benefit for GHC members. Less than 10% of GHC members typically participate in both programs, although not necessarily simultaneously (M. Thompson, oral communication, December 2008). Although health benefits and a reduction in hospital days have been demonstrated, EW’s effect on health care use and costs has not been previously analyzed. The availability of comprehensive cost and use data for GHC members made studying these questions with GHC members who had participated in EW attractive. We hypothesized that participation in EW would lower overall health care costs, via reductions in costly forms of health care use (especially hospitalizations).  \n\n【4】Methods\n-------\n\n【5】### Study setting\n\n【6】EW is offered at community centers, many of which are senior centers, located in the greater Puget Sound region. Senior Services, a private nonprofit organization with 250 employees established in 1967, operates EW. Nearly all nurses and social workers in EW programs in King County are employed either by Senior Services or by the hosting EW sites. The main sources of funding for Senior Services for EW programs in King County are the Aging and Disability Services of Seattle and King County and the Public Health Department of Seattle and King County. Office space and supplies are often donated by hosting sites. Participants are asked to make a donation at the time of graduation but this amount covers only a small amount of actual EW costs. Senior Services estimated that the cost to administer EW at its King County sites in 2004 was $400 per participant per year. Although EW has been disseminated beyond King County, Washington, we restricted our study to King County, where GHC is based . GHC is a consumer-governed, mixed-model health maintenance organization (HMO) with more than 500,000 members in the Pacific Northwest; according to our research, approximately 65,000 members are aged 65 years or older, and 27,900 reside in King County. Health outcomes and cost data are available and complete for all GHC members, regardless of whether they receive their care at a GHC-owned health care facility. GHC health care use and cost data have been studied and validated , and we used these data to capture our outcomes data. The institutional review boards of the University of Washington and GHC approved the study protocol. \n\n【7】### Participants\n\n【8】We chose our sample from GHC members who were aged 65 years or older, resided in King County, and voluntarily participated in EW from March 15, 1998, through April 15, 2005. From this group, we selected participants who were continuously enrolled in GHC for at least 1 year before and 1 year after the first day of their EW enrollment. The date of EW enrollment (ie, the first day an EW participant signed a consent form, formally agreeing to participate in the program) was defined as the index date. We excluded enrollees who had been in a long-term–care facility during the year before the index date because of the high costs involved that would have skewed the overall results. Each EW participant was age- and sex-matched to 3 GHC members who had not participated in EW (“nonparticipants”). Nonparticipants were assigned an index date that corresponded to the index date of the EW participant to whom they were matched, creating comparable pre-index and postindex enrollment periods. Inclusion criteria for nonparticipants were identical to criteria for EW participants. Our analysis included comparisons between 218 EW participants and 654 matched nonparticipants. \n\n【9】### Intervention\n\n【10】EW has been described in detail elsewhere . Briefly, after EW clients complete the program’s health intake questionnaire, which assesses risk factors for functional decline, they meet with a social worker or nurse for approximately 1 hour to discuss personal health concerns, review the findings of the questionnaire, and identify personal health goals. Clients develop strategies for improving health and make “health action plans.” They are encouraged, but not required, to seek out health and community services when needed. These services may include appointments with primary care providers, medical specialists, social services, or mental health services, or participation in an organized exercise program. Clients often need follow-up appointments with the nurse or social worker, either in person or by telephone. The recommended minimum time for program participation was 1 year until November 2003, at which point the recommended minimum time was reduced to 6 months. \n\n【11】### Outcome measures\n\n【12】Total health care costs during the year following the index date was the primary outcome measure. Total costs included inpatient, primary care, and nonprimary care outpatient costs. Nonprimary care outpatient costs consisted of outpatient specialty care, outpatient mental health, emergency department care, outpatient pharmacy, outpatient laboratory, outpatient radiology, long-term care, and drug and alcohol treatment costs. Secondary outcomes were inpatient and primary care costs, percentage of hospitalizations, and number of hospital days. All cost data were captured from the GHC cost accounting system previously described . \n\n【13】### Data analysis\n\n【14】Participation (yes/no) in EW was our main predictor of interest. We included age, sex, prior year health care costs or use (as appropriate), comorbidity, and tendency to use preventive services as covariates in our analyses because these factors typically influence health care use and costs. We assessed comorbidity and chronic disease burden by using the GHC diabetes and heart registries and the Charlson comorbidity index . We used the methods of the HMO Research Network, which based its index on the method outlined by Deyo et al , with the addition of peripheral vascular disorder procedure codes and outpatient encounters as recommended  to determine our Charlson comorbidity index. We assessed inclination to use preventive health services by using a preventive services score, which takes into account preventive health services and preventive visits . This score is the sum of the number of times a study participant received colon cancer screening (fecal occult blood test or flexible sigmoidoscopy), a screening mammogram, prostate cancer screening, an influenza vaccine, or a pneumococcal vaccine during the 2 years immediately preceding the index date (score range, 0-8). If the person had none of the 4 services in the past 2 years, the preventive services score was the number of primary care preventive visits the person had in the past 2 years (maximum, 2). Median household income and median education were available for analysis at the census block level for more 80% of our participants. These socioeconomic status variables were considered but not included in the final model because their inclusion did not alter our results. \n\n【15】### Statistical analysis\n\n【16】The primary analysis focused on differences in health care costs and use between EW participants and matched nonparticipants. We adjusted all costs to 2005 US dollars by accessing the Medical Care component of the Consumer Price Index for the participant’s index year . We used 2-tailed _t_ tests and χ 2 tests to compare demographic and health-related characteristics and unadjusted health care cost and use measures for participants and their matched comparisons. We used ordinary least squares linear regression to analyze cost differences, adjusting for covariates; this modeling approach yields unbiased estimates of differences in mean costs when the sample size is large . Because the distribution of health care costs is often skewed, as many people have no costs and a few have high costs, we repeated our analysis by using log-transformed costs. All analyses were performed using Stata, version 9.0 (StataCorp LP, College Station, Texas).  \n\n【17】Results\n-------\n\n【18】Most EW participants (88%) spent 6 months or longer in the program; approximately 50% spent 12 or more months, and 20% were in the program for more than 2 years. EW participants were identical to nonparticipants in terms of average age (79 y) and sex (72% female) . We noted several significant differences between the groups, including a larger comorbidity burden among EW participants, as measured by a higher Charlson comorbidity index and a larger proportion enrolled in the GHC diabetes and heart disease registries. The preventive services score was significantly higher for EW participants, suggesting a stronger tendency to receive preventive services. Total costs, inpatient costs, percentage hospitalized, and number of hospital days were not significantly different between participants and nonparticipants, either at baseline or in the year after the index date . The only significant difference was in unadjusted primary care costs, which were higher by $325 in the EW group ( _P_ < .001) at baseline and $177 higher in the year after the index date ( _P_ \\= .04). After adjusting for age, sex, prior year total costs, preventive services score, Charlson comorbidity index, and presence on the GHC diabetes or heart disease registries, total health care costs in the year after the index date were $582 lower for EW participants than for nonparticipants, but this difference was not significant. The results were unchanged when we used log-transformed costs. There were no differences in inpatient use or primary care use between the 2 groups at baseline or the year after the index date.  \n\n【19】Discussion\n----------\n\n【20】We found that, compared with nonparticipants, EW participants had nonsignificantly lower total health care costs and no difference in hospitalizations during the year following EW enrollment. This finding may have resulted from the fact that EW participants in our sample had a significantly larger comorbidity burden than did nonparticipants. Comorbidity is a major driver of hospital costs and total annual costs . Furthermore, the methods we used to adjust for comorbidity, although widely used, may not have allowed us to fully control for comorbidity differences between study groups  Many health promotion programs, some designed for the older adult population, have been associated with decreased health risks and decreased health care use . Health promotion programs evaluated by Lorig et al and Holland et al most closely resemble the EW program . These studies evaluated health outcomes and health care use, but neither assessed health care costs. Lorig et al found a significant decrease in hospitalizations and hospital days during their 6-month randomized controlled trial. The average age in this study was 10 years younger than in ours, and the 2 study groups had balanced comorbidities. Conversely, Holland et al did not find a difference in health care use between study groups during their year-long randomized controlled trial of the Health Matters Program in Sacramento, California, a program modeled after EW . Similar to our analysis, the mean age of participants in Holland’s study was 73 years. There are several differences between our analysis and the original randomized controlled trial that was used to evaluate EW . The original trial lasted 12 months, and outcomes were evaluated for the 12 months of program enrollment. After the original trial, EW evolved into a 6-month program, so our analyses included EW participants with varying duration of program participation. For most EW participants, 6 months of EW participation confers favorable effects on disability risk factors (eg, depression, physical inactivity) that are comparable to 12 months of participation . However, such reductions in disability risk factors do not appear to translate into lower overall health care costs. A strength of our analysis was that we reported actual health care costs. To our knowledge, no other analysis of a health-promotion, disability-prevention health resource has used actual cost data. Health care use is often used as a proxy for costs, or alternatively, costs are estimated from claims data . Furthermore, our health care use data were derived from automated data sources, which are more accurate in assessing health care use and costs than are self-reported data . Our study has several limitations. Our study had an observational design, which can result in residual confounding and selection bias. Residual confounding in relation to the comorbidity differences we observed between study groups is likely, although we attempted to adjust for them. Research published after our study ended demonstrated that total annual costs increase with increasing comorbidity and that 4 conditions — hypertension, depression, use of warfarin, and skin ulcers/cellulitis — should be added to the Charlson comorbidity index to accurately predict total annual costs . We used the preventive services score to address selection bias related to the potential tendency of more prevention-oriented people to participate in EW. This score has been used in prior research with GHC members but may not have fully accounted for this form of selection bias . We also considered using propensity scores to adjust for selection bias but lacked enough covariates to independently predict program participation. Another limitation was a lack of detail on health care use. In particular, we could not distinguish increased use that may have been prompted by participation in EW (eg, more visits related to health problems identified by EW). Also, apart from EF, we had no information about exercise and other health promotion programs that EW participants may have pursued as a result of their participation in EW. Finally, our sample size of just over 200 EW participants may have been too small to detect meaningful cost differences given the large variances associated with health care cost and use data. EW improves the health of older adults at risk for functional decline . However, we did not find that overall health care costs were significantly reduced by EW program participation.  \n\n【21】Tables\n------\n\n【22】#####  Table 1. Participant and Nonparticipant Demographic Characteristics, Group Health Cooperative/EnhanceWellness, March 1998-April\n\n【23】CharacteristicParticipants(N = 218)Nonparticipants(N = 654)P ValueMean age, y (SD)78.6 (5.8)78.6 (5.8)&gt;99% Female72.572.5&gt;99Mean preventive services score (SD) a3.0 (1.2)2.8 (1.4).03Mean Charlson comorbidity index (SD) b1.0 (1.3)0.7 (1.2)&lt;001% Listed on GHC diabetes registry20.614.1.02% Listed on GHC heart disease registry43.129.2&lt;001a Derived from the sum of the number of times a subject received colon cancer screening (fecal occult blood test or flexible sigmoidoscopy), a screening mammogram, prostate cancer screening, an influenza vaccine, or a pneumococcal vaccine during the 2 years immediately preceding the index date (score range 0-8; higher scores indicate receipt of more preventive services).b See Methods section for a description of this score. The mean Charlson comorbidity index and the percentage of participants listed on the Group Health Cooperative diabetes and heart disease registries were used to measure comorbidity and chronic disease burden. \n\n【24】#####  Table 2. Health Care Costs and Use of Participants and Nonparticipants at Baseline and Year Following Index Date, a Group Health Cooperative/EnhanceWellness Program, March 1998-April\n\nVariableUnadjusted ResultsAdjusted ResultsParticipants (N = 218)Nonparticipants (N = 654)DifferenceP Value bDifferenceP Value bCost, $ cTotalBaseline7,0476,207840.20−582.58Year 18,0917,977114.91InpatientBaseline7731,116−343.29−804.22Year 11,3342,162−828.21Primary careBaseline1,213888325&lt;00128.72Year 11,069892177.04Health care useNo. of hospital daysBaseline0.430.400.03.78−0.15.56Year 10.830.89−0.06.80% HospitalizedBaseline10.68.62.0.38−0.02.95Year 113.313.30&gt;99a The “index date” is date of enrollment (ie, the first day a participant signed a consent form, formally agreeing to participate in the program).b _P_ values for unadjusted results derived from _t_ tests; _P_ values for adjusted results derived from linear regression (adjusted for age, sex, prior year costs, preventive services score, Charlson comorbidity index, and presence on the Group Health Cooperative diabetes or heart registries).c Results reported in mean 2005 US dollars.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "37508d52-ae9d-4dc8-bc0e-47adc60d393b", "title": "Psychosocial Impact of SARS", "text": "【0】Psychosocial Impact of SARS\n**To the Editor:** An outbreak of severe acute respiratory syndrome (SARS) occurred from February to May 2003 in Hong Kong, China, Singapore, and Canada. According to the World Health Organization, 1,755 people were infected in Hong Kong; 386 of these were healthcare workers. A total of 300 persons died from SARS, constituting a death rate of 17% .\n\n【1】Evidence suggests that persons infected with SARS recovered physically, but SARS is associated with social and psychologic problems poorly understood by the scientific community. A survey in a convalescent hospital in Hong Kong showed that approximately 50% of recovered SARS patients showed anxiety , and approximately 20% were fearful . Approximately 20% of the rehabilitated patients showed some negative psychologic effects , which included insomnia and depression. Some patients with serious cases could not rid themselves of the memories of fighting SARS, and these memories disrupted their daily activities. These psychosocial problems may be due to the complications of SARS medications, such as ribavirin and corticosteroid. Persons who took these drugs had hair loss, major memory loss, impaired concentration, and depression. A medical practitioner in Hong Kong who recovered from SARS attempted suicide because complications from drugs made him unable to earn his living .\n\n【2】In addition to SARS patients themselves, an estimated 50% of family members of SARS patients had psychologic problems, including feelings of depression or stigmatization . They had difficulties sleeping, and some children who had lost parents cried continuously. Some children also felt embarrassed to be a member of a SARS family . The spouse of one healthcare worker who died from SARS attempted suicide at her workplace . The loss of parents who were SARS patients also impaired the growth of their children . A study conducted in China  reported that negative SARS-related information increased persons’ perception of their risk and led to irrational nervousness or fear.\n\n【3】Although data from systematic studies of SARS do not exist, evidence suggests that this disease has psychosocial consequences for SARS patients, their families, and society. While biomedical scientists must continue their efforts to clarify the genetic makeup of the SARS coronavirus, look for new medications, and develop vaccines , the social and psychologic aspects of SARS should not be overlooked. Since nearly all resources are devoted to biomedical research and medical treatment; psychosocial problems of SARS patients and their families are largely ignored. Our review of the literature using the ISI Web of Knowledge on January 17, 2004, substantiated this observation. To date, no systematic study examining psychosocial consequences of SARS has been published in scientific journals. A systematic exploration of how SARS negatively affects patients’ mental health is needed so that appropriate interventions may be implemented at individual, family, and societal levels.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7bda56b-d7b1-406e-b25f-a3c18c1f3439", "title": "Utility of Whole-Genome Sequencing to Ascertain Locally Acquired Cases of Coccidioidomycosis, Washington, USA", "text": "【0】Utility of Whole-Genome Sequencing to Ascertain Locally Acquired Cases of Coccidioidomycosis, Washington, USA\nCoccidioidomycosis, also known as Valley fever, is a disease of growing public health concern and is caused by 2 closely related fungal species, _Coccidioides immitis_ and _C. posadasii_ . _Coccidioides_ spp. are dimorphic, forming mycelia in soil and arthroconidia capable of infecting humans and certain other mammals, and spherules in mammalian tissue . This mycosis causes a wide spectrum of conditions, ranging from asymptomatic infection and mild pulmonary disease to severe pulmonary and disseminated disease, which can be life-threatening . Infection is generally caused by inhalation of pathogenic arthroconidia from disturbed soil or dust, such as through occupational or recreational activities or weather events that raise dust .\n\n【1】_Coccidioides_ spp. are endemic to warm, arid regions of the Western Hemisphere that have low rainfall . The 2 species have colonized different geographic locations: _C. posadasii_ is largely found in Arizona, Texas, Mexico, and Central and South America; and _C. immitis_ is primarily found in central and southern California. Coccidioidomycosis causes a major burden of disease in many of these areas; the annual incidence in Arizona is >75 cases/100,000 population . Case counts have recently increased in California; >5,300 cases were reported in 2016 (13.7 cases/100,000 population . Climate suitability projections predict expansion of the suitable environment for coccidioidomycosis and increasing incidence in areas that already sustain _Coccidioides_ growth .\n\n【2】During 2010–2011, three cases of coccidioidomycosis were identified and reported in southeastern Washington . Subsequently, _Coccidioides_ DNA and viable propagules were isolated from soil at a location of suspected exposure for 1 of these cases . Whole-genome sequencing (WGS) analysis of the recovered isolates demonstrated near identical genetic identity between soil and clinical isolates, further confirming endemic presence of infectious _C. immitis_ in Washington .\n\n【3】Public health surveillance for coccidioidomycosis was implemented statewide in Washington in April 2014. Before 2014, cases of coccidioidomycosis were reported sporadically, but no standard reporting procedure existed. Since 2014, the number of reported cases of coccidioidomycosis has increased each year; reported cases in Washington suspected to be locally acquired have higher rates of hospitalization and death compared with cases from other disease-endemic regions .\n\n【4】Because most persons infected with _Coccidioides_ spp. are asymptomatic or have only mild illness, and most illness self-resolves, we believe that only the most severe cases of coccidioidomycosis with exposure in Washington are being identified and reported to public health authorities. In addition, _Coccidioides_ spp. infections might be asymptomatic or only manifest as subclinical disease until a reactivation or complication develops later. Also, most residents of Washington have some travel history to historically disease-endemic areas, such as Arizona, California, or Mexico. These findings create a challenge in differentiating between autochthonous and imported cases of coccidioidomycosis, which is a useful distinction for public health surveillance in Washington.\n\n【5】To better determine the nature of exposure for reported cases of coccidioidomycosis in Washington, the Washington Department of Health, in collaboration with the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA) and the Translational Genomics Research Institute (Flagstaff, AZ, USA), initiated enhanced surveillance by incorporating WGS of clinical isolates aimed at determining the geographic origins of _Coccidioides_ spp. strains. This approach is based on previous population structure studies providing evidence that _C. immitis_ and _C. posadasii_ have well-defined geographic structures . Subsequently, Engelthaler et al. demonstrated that isolates can generally be assigned to the specific geographic populations on the basis of WGS analysis . This approach is consistent with a general trend in molecular epidemiology that uses WGS and a One Health approach to ascertain clusters or outbreaks of bacterial, fungal, viral, and parasitic diseases .\n\n【6】We report WGS analyses of isolates from 17 recent coccidioidomycosis cases in Washington, and demonstrate that 13 (76%) cases involved coccidioidomycosis most likely acquired outside Washington; 4 cases (24%) likely involved local acquisition. We discuss the utility of the WGS method to enhance epidemiologic surveillance.\n\n【7】### Materials and Methods\n\n【8】##### Cases and Isolates\n\n【9】We identified coccidioidomycosis cases through passive reporting to local health jurisdictions from healthcare providers in Washington and laboratories performing testing for residents of Washington. Confirmed cases, as classified according to the Council of State and Territorial Epidemiologists case definition , reported during 2014–2017 were included in the analysis. Reported case-patients were interviewed by local health jurisdictions to determine clinical course, travel history, and any potential soil or dust exposures. Medical records were requested when possible for complete data abstraction to case reporting forms. When the exposure was suspected to have occurred in the disease-endemic area of Washington or exposure history was unknown, available clinical isolates were sent to CDC for confirmation.\n\n【10】At CDC, all isolates were grown on brain heart infusion (BHI) agar at 25°C for 10 days. Genomic DNA was extracted by using the DNeasy Blood and Tissue kit  according to the manufacturer’s recommendations. Genomic DNA was stored at −20°C until further use. The Washington State and CDC Institutional Review Boards determined this project to be enhancing surveillance for a notifiable condition and did not require human subjects review.\n\n【11】##### Genome Sequencing, Assembly, and Analyses\n\n【12】We sequenced genomes of 18 isolates from 17 patients using Illumina  HiSeq and MiSeq sequencing platforms, as described . We prepared DNA samples for paired-end sequencing by using the Kapa Biosystems  Library Preparation Kit protocol with an 8-bp index modification and sequenced to a read length of 250 bp on the Illumina HiSeq or 250 bp on the Illumina MiSeq. All WGS data files have been deposited in the National Center for Biotechnology Information Sequence Read Archive under BioProject PRJNA472461.\n\n【13】We performed genome assembly and analyses as described . In brief, the Washington-1(B10637) strain was used as the reference for _C. immitis_ single-nucleotide polymorphism (SNP) matrices, and the B10813\\_Tx strain was used as a reference in the _C. posadasii_ SNP matrix. We aligned Illumina read data against the respective reference assemblies by using Burrow-Wheeler Aligner version 0.7.7  and identified SNP variants by using SamTools version 0.1.19 . We filtered SNP calls by using a publicly available SNP analysis pipeline  , as described, to remove positions with <10 times coverage with <90% variant allele calls, or identified by using Nucmer  as being within duplicated regions in the reference . We constructed a maximum-parsimony phylogeny on the basis of SNP matrices by using MEGA7 , including 17 described strains from various geographic areas for comparison; we determined bootstrap support for the tree by using 1,000 reiterations .\n\n【14】We extracted candidate SNPs that distinguish Washington strains from non-Washington strains  from whole-genome assemblies by using Samtools version 1.8, EMBOSS version 6.5.7, and NCBI-toolkit version 7.0.0 . We chose SNPs if their positions were consecutive or no more than 1 nt apart and had \\> 10 times coverage. In brief, we extracted an ≈1-kb region surrounding candidate SNP positions from all draft assemblies, aligned it to Washington-1(B10637) reference, and designed degenerate primers by using Primer3 version 3.0 . We compared regions with those of _C. immitis_ strain H538–4 to determine genomic loci annotation.\n\n【15】### Results\n\n【16】During 2014–2017, a total of 167 confirmed cases of coccidioidomycosis were reported in Washington. For these cases, 18 isolates  from 17 cases were available. Travel and exposure histories suggested local acquisition in Washington (9 case-patients) or were unknown at the time of specimen collection (8 case-patients).\n\n【17】We speciated 14 isolates as _C. immitis_ and 4 as _C. posadasii_ and subjected these isolates to WGS with 25–160 times sequencing coverage. A total of 68,717 SNPs were identified by comparison with the reference isolate. Phylogenetic analysis and comparison with sequenced isolates of _C. immitis_ demonstrated that 5 isolates from 4 patients in Washington were highly genetically related to each other and to isolates from the identified Washington clade that contained human and soil isolates . No more than 354 SNPs differentiated any 2 isolates within the Washington cluster, which was also supported by 100% bootstrap values . In contrast, >10,000 SNPs differentiated the Washington cluster from its nearest neighbor, an isolate from another case-patient in Washington. This case-patient (B12496) reported travel history to disease-endemic regions in California and Mexico. Tens of thousands of SNPs differentiated Washington cluster isolates from those from other states. No more than 3 SNPs were identified among multiple soil isolates collected 4 years apart from a single sampling site, and 4 SNPs were identified between isolates collected 2 years apart from 1 patient in Washington who had a chronic infection. However, 234–324 SNPs were identified in pairwise comparisons between isolates from different patients in the Washington clade .\n\n【18】Three of the 4 case-patients infected with Washington clade strains reported no travel history outside Washington. Conversely, 8 of 9 _C. immitis_ isolates that did not cluster with the Washington clade were isolated from patients who reported travel to California.\n\n【19】Washington strains can be distinguished from non-Washington strains in 3 genomic regions on the basis of annotations from _C. immitis_ strain H538-4; locus DS016982: adipocyte-derived leucine aminopeptidase; locus DS01702: hypothetical protein; and locus DS016985: glyoxal oxidase . The DNA fragment from locus DS016982 contains SNP GG at positions 94460–94461 for Washington strains and SNP CA for non-Washington strains and can be amplified by using forward primer 5′-GGTACGTCACAAGTCCCCAG-3′ and reverse primer 3′-AAGAGTACTCGCGAAGGAAGC-5′. For locus DS017021, the DNA fragment amplified with forward primer 5′-CTTGACTGTGCAGGGCCTTA-3′ and reverse primer 3′-ACCGGCCTAACTCCATGGTA-5′ encompasses SNPs GGT for Washington strains and TGC for non-Washington strains at positions 105566–105568) The fragment of locus DS016985 can be amplified by using primer pairs 5′-TTCCGCTTGATGGCTGAAGT-3′/3′-TGTGGCCCTCCTATTGCTTG-5′ and contains consecutive SNP CC for Washington strains and SNP GA for non-Washington strains (positions 232442–232443).\n\n【20】We identified 4 case-patients who had _C. posadasii_ infections by using internal transcribed spacer sequencing and confirmed by WGS. Isolates from the 4 case-patients clustered with described _C. posadasii_ populations from Arizona (data not shown). Three of those case-patients had traveled to Arizona, and 1 case-patient had an unknown travel history.\n\n【21】### Discussion\n\n【22】Determining the potential sources of exposure and geographic distribution of _Coccidioides_ in areas where coccidioidomycosis is uncommon is a surveillance challenge because patients often report travel histories to other disease-endemic areas. To address this problem, we implemented WGS to better determine the likely geographic origin of isolates obtained from patients in Washington who had suspected local acquisition or unknown exposure histories. This method is based on the observation that _Coccidioides_ populations have well defined geographic structure, and most isolates can be assigned to specific geographic areas on the basis of their WGS genotypes, such as Arizona, Texas, Mexico, central California, or Washington . We demonstrated the utility of this tool for investigating the epidemiology of coccidioidomycosis in Washington.\n\n【23】Of 18 isolates from 17 patients included in this study, 5 (28%) clustered with strains isolated from soil in Washington, which was consistent with local exposure. The remaining 13 (72%) isolates clustered with populations from other areas, such as Arizona, California, or Mexico, which was generally consistent with reported travel histories of patients. The only discordance between WGS and epidemiologic data was observed for isolate B11863, which clustered with _C. immitis_ isolates from California. The patient from whom this isolate originated reported travel out of state but also reported extensive soil exposure in a county in Washington that had previously reported autochthonous cases. Specifically, the only reported travel was to Hawaii 2 months before disease onset and to Costa Rica 1 year before disease onset, with a layover in California during which the patient did not leave the airport. In addition, the patient reported 1 trip to southern California 6 years before disease onset. This discordance was an unexpected finding and might represent more diversity among _Coccidioides_ spp. in Washington than previously understood, reactivated infection because of previous travel, or infection from a fomite or other carrier source of _C. immitis_ . Further investigation and surveillance data are needed to determine which hypothesis is most plausible.\n\n【24】Of the 4 patients infected with the local strains from Washington, 3 patients reported no travel history outside Washington. The fourth patient reported travel to Arizona 1 year before the onset of coccidioidomycosis but also reported extensive exposure to local soils in Washington and dust during hiking and gardening. This result indicates that compatible travel history alone does not exclude the possibility of local exposure and demonstrates the added value of genomic approaches to determining exposure location. Although isolates in the Washington clade were genetically related , considerably higher genetic diversity was observed for new strains compared with that detected for the original isolates from Washington identified in 2013 . Specifically, 245–299 pairwise SNPs were identified among the newly identified strains from Washington, compared with <5 SNPs detected among previously described isolates from Washington from the same exposure site . This small but apparent genetic divergence of the new isolates indicates that new infections were acquired from different local _C. immitis_ populations, likely at different sites, and suggests the presence of multiple environmental loci for _Coccidioides_ spp. in Washington. The divergent isolates were collected from soil in a location ≈70 miles from the suspected exposure location of 1 of the patients. However, in sharp contrast to the populations of _Coccidioide_ s spp. in Arizona or California, where isolates are differentiated by thousands of SNPs, the overall low genetic variability in the Washington population is consistent with a relatively small population size, strongly suggesting a recent common ancestry of Washington strains and a relatively recent expansion of _C. immitis_ to Washington,\n\n【25】Our study has 2 main limitations. First, there might be additional genetically diverse populations of _Coccidioides_ in Washington that have not yet been discovered or proven; therefore, some genotypes that have been deemed as acquired outside Washington might have been acquired locally. For example, 1 isolate from a patient who was highly suspected of having local acquisition clustered with isolates from California. Likewise, there might be genetically similar populations of _Coccidioides_ spp. in Oregon, California, or other nearby states that have not yet been documented. These genetically similar populations might add complexity to determination of location of exposure. However, to date, no clinical or environmental isolates with strong epidemiologic links to locations outside Washington that cluster with isolates from Washington have been identified. This finding is consistent with those of previous studies that demonstrated that _C. immitis_ and _C. posadasii_ have well-defined geographic structures .\n\n【26】Second, all patients in our study were given diagnoses of rather atypical cases of coccidioidomycosis that were either asymptomatic (diagnosed during biopsy), cutaneous, chronic, or unusually severe. In general, the acute primary pulmonary form of coccidioidomycosis constitutes the most common manifestation in highly endemic areas and often represents the earliest manifestation of the disease. Conversely, reactivation of latent infections can occur months to years after initial exposure, typically in immunosuppressed persons. Many complicated infections might follow a subacute or chronic disease progression. This lack of acute respiratory cases and preponderance of atypical disease in Washington indicates the likely gap between early identification and delayed diagnosis because of lack of awareness and might also explain the high prevalence of the out-of-state strains of _C. immitis_ in our study. However, even in highly endemic regions, acute pulmonary disease is often missed or delayed in diagnosis for months , supporting the possibility that an underlying background of typical acute disease might remain unidentified in Washington .\n\n【27】Our results indicate that WGS is a useful tool to assist in determining exposure location in surveillance situations in which exposure histories are unclear or unknown. For patients in this study, coccidioidomycosis was more commonly associated with travel to other disease-endemic areas compared with local exposure; however, travel to endemic regions does not preclude local acquisition of the disease. Our results also indicate that coccidioidomycosis is likely to be underdiagnosed and underreported in Washington on the basis of atypical disease manifestations for locally acquired cases. More research is needed to determine the true prevalence of locally acquired coccidioidomycosis in Washington.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6eeb867-ee2d-4499-9b66-6aab97c1f385", "title": "Bioinformatic Analyses of Whole-Genome Sequence Data in a Public Health Laboratory", "text": "【0】Bioinformatic Analyses of Whole-Genome Sequence Data in a Public Health Laboratory\nNext-generation sequencing (NGS), also known as high-throughput sequencing, has affected many fields in the study of biology but has dramatically changed the field of genomics by enabling researchers to quickly sequence whole microbial genomes, profile gene expression by sequencing RNA, examine host–pathogen interactions, and study the vast microbial diversity in humans and the environment . Despite the benefits of NGS over traditional Sanger sequencing methods, public health laboratories (PHLs) have been slow to implement this revolutionary technology. According to the Association of Public Health Laboratories, no PHLs had NGS capabilities before 2010 . The Centers for Disease Control and Prevention (CDC), through its Advanced Molecular Detection program, has supported the adoption of NGS and whole-genome sequencing (WGS) by providing funding and training to PHLs. By the end of 2015, CDC’s support had enabled 37 PHLs to acquire NGS instrumentation, with another 9 PHLs gaining NGS technology by the end of 2016 .\n\n【1】For laboratory surveillance of foodborne diseases, pulse-field gel electrophoresis (PFGE) is currently the preferred method for typing bacterial isolates and is widely used in outbreak investigations and source tracking. PFGE has been the backbone of the success of CDC’s PulseNet program since 1997 . However, the PulseNet program is aiming to replace PFGE with WGS by 2018. This trajectory resembles the path taken in the study of human genetics, in which genetic mapping based on restriction fragment length polymorphism was replaced by quasi-complete information obtained by high-throughput genomic sequencing. Although restriction fragment length polymorphism markers initially enabled the measurement of genetic distance and laid the foundation for linkage mapping, its success depended on pronounced phenotypic effects of the underlying trait and regularly dispersed markers. Once linkage to a region was identified, causality could be pinpointed through fine mapping. WGS provided not only a complete marker-map with maximum resolution at the nucleotide level but also enabled the deduction of causality and direct testing of genetic relatedness and genetic origination. The promise of this approach also extended to the study of pathogens, given that WGS ultimately enables testing of specific hypotheses regarding genotype-phenotype relationships (e.g. antimicrobial drug resistance). However, although more PHLs are adopting NGS and WGS, only a small number of these laboratories have the ability to perform the bioinformatic analyses needed to take full advantage of the data they are generating. CDC aids PHLs in conducting foodborne disease surveillance on a national scale but is unable to assist with data analysis for local foodborne disease surveillance.\n\n【2】Some of the obstacles preventing PHLs from implementing the bioinformatic-dependent analysis are the requirements for large-scale computational capabilities, complex molecular evolutionary analyses, and dedicated bioinformatics staff to perform these analyses. However, all that is really needed is a computer with a browser and a connection to the Internet. Web-based tools are available for PHLs that are looking to participate in WGS data analysis but are not ready to perform analyses in-house. Several of these tools are open-source (i.e. free of charge) and can be used to perform a range of bioinformatics analyses. Two of these tools are Illumina’s BaseSpace Sequence Hub (Illumina, Inc. San Diego, CA, USA) and the Galaxy web-based platform .\n\n【3】Because many PHLs are already using Illumina’s MiSeq sequencing platform, BaseSpace is a convenient solution that enables users to monitor the progress of sequencing runs, share data easily with others, and use 1 terabyte (TB) of data storage free of charge. Illumina provides new users with a 30-day free trial of BaseSpace, enabling users to use all of the wide-ranging bioinformatic tools available.\n\n【4】The Galaxy platform enables users to perform analyses ranging from sequence quality control and timing to whole-genome assemblies . Galaxy also enables users to track the details of each step of an analysis, making it easier to reproduce and publish the results. Galaxy enables nonexperts to perform advanced and computationally intensive analyses without having training in bioinformatics.\n\n【5】However, neither BaseSpace nor Galaxy is without drawbacks. Uploading or downloading the large files generated by NGS can be slow and might fail before finishing, requiring the entire upload or download process to be restarted. Web-based tools can also be “black boxes” where users may not know exactly what each step of the analysis is, why that step is being performed, or why results might be difficult to understand or interpret. These web-based tools might seem quick and easy to use but often do not perform as expected.\n\n【6】Bioinformatic analyses are often performed in a step-wise manner, with the output of 1 analysis being used as the input for the next. These multistep, multisoftware analyses are frequently referred to as pipelines and are often set up to run automatically from 1 step to the next without input from the user. In this perspective, we describe the bioinformatic pipeline implemented at the Utah Public Health Laboratory (UPHL) to analyze the WGS data. Sharing our experiences with this pipeline will enable PHLs to implement their own pipelines by following each step in our pipeline or by using our pipeline as a template to construct their own unique processes. All the software used in our bioinformatics pipeline are open-source and are available free of charge . We present these analyses as a function of the level of technology required, spanning everything from basic quality control performed on typical desktop or laptop computer to complex molecular evolutionary analyses that require powerful high-end Linux servers or workstations.\n\n【7】### Bioinformatic Pipeline\n\n【8】The bioinformatic pipeline developed and implemented at UPHL consists of 8 steps : 1 ) read quality control, 2 ) reference strain determination, 3 ) read mapping to the reference strain, 4 ) single-nucleotide polymorphism (SNP) and small insertion or deletion (indel) detection, 5 ) de novo genome assembly, 6 ) genome annotation, 7 ) phylogenetic tree construction, and 8 ) phylogenetic analysis. Although such processes are standard, several software solutions are available for the respective steps.\n\n【9】The first step in almost all WGS bioinformatics analyses is quality control of the raw sequencing data. It is important to remove poor-quality sequence data and technical sequences (i.e. adapter sequences). Highly accurate sequences are required for SNP detection, enabling the detection of actual SNPs and distinguishing from sequencing artifacts. Quality control in our pipeline is performed by using Trimmomatic , a multithreaded command line tool that removes adapter sequences, trims low-quality sequence from the beginning or end of a sequence, removes reads that fall below a user-defined threshold for length, and validates paired-end sequence reads.\n\n【10】The second step in the pipeline is reference sequence determination. To determine SNPs, a reference sequence is needed against which to compare sequencing reads. The choice of reference sequence might have a substantial effect on the number and type of SNPs that are detected, making this step important. We use Mash for reference sequence determination . Mash enables us to quickly compare the large set of sequencing reads generated against the reference set of 54,118 National Center for Biotechnology Information RefSeq genomes  to determine nucleotide distance and relatedness .\n\n【11】Once a reference sequence is determined, the next step in the analysis pipeline is mapping the quality-controlled sequencing reads to the reference genome. We perform read mapping by using the Burrows-Wheeler Aligner (BWA) software package with the bwa-mem option . BWA uses a Burrow-Wheeler Transform to efficiently align sequencing reads to reference genomes allowing for gaps and mismatches. The output of BWA is the standard sequence alignment map format known as SAM, which facilitates the next step in the pipeline.\n\n【12】The fourth step in the pipeline uses mapping of the sequencing reads to the reference sequence to identify SNPs and indels. We perform SNP and indel determination by using SAMtools and VarScan2, which also calculate SNP frequency in the sequence data . The output of VarScan2 can be easily viewed in the Integrative Genomics Viewer, which enables the interactive viewing of large genomic datasets . The output file of VarScan2 can also be used in more complex downstream analyses (i.e. to build SNP matrixes and phylogenetic trees).\n\n【13】The quality-controlled sequencing reads are then used for de novo genome assembly in the sixth step of the pipeline. We perform de novo genome assembly on individual isolates by using the St. Petersburg genome assembler, also known as SPAdes . The SPAdes assembler has 3 modules: sequencing read error correction; SPAdes assembly; and a mismatch corrector module. The first module error corrects the quality-controlled sequencing reads by using advanced algorithms based on Hamming graphs and Bayesian subclustering. Sequencing error correction in this manner has shown to dramatically improve genome assemblies of NGS data . The SPAdes assembly module uses the error-corrected reads and performs the actual assembly in an iterative manner making use of de Bruijn graphs. The resulting genome assembly is then used as input for the third module, which greatly reduces the number of mismatches and small indels by using BWA and results in highly accurate contigs (contiguous sequence data made up of overlapping sequencing reads) and scaffolds (ordered and oriented contigs based on paired-end read data).\n\n【14】We then annotate the resulting genome assembly to identify protein-coding genes, tRNAs, and rRNAs. We use Prokka for annotation of protein-coding genes, tRNA, and rRNA on the contigs and scaffolds generated by SPAdes . Prokka can fully annotate a bacterial genome in approximately 10 minutes on a high-end quad-core desktop computer by making use of a suite of existing software, tools, and sequence databases, such as UniProt  and NCBI RefSeq .\n\n【15】We then use shared orthologous genes to construct phylogenetic trees that provide insight into the relatedness of isolates. Once multiple genomes have been annotated, we calculate the pan genome of the combined genomes by using Roary . The pan genome consists of the union of genes shared by genomes of interest, and Roary can compute the pan genome of 1,000 bacterial genomes on a single CPU computer in 4.5 hours . In addition to determining the pan genome of the genomes of interest, Roary also generates a concatenated nucleotide alignment of the pan genome, which can be used to build a phylogenetic tree of these sequences. This pan genome alignment is used as the input to RAxML for phylogenetic tree construction . RAxML is a program that has been designed and optimized for conducting phylogenetic analyses on large datasets by using maximum-likelihood techniques to estimate evolutionary trees from nucleic acid sequence data .\n\n【16】The last step in the pipeline is phylogenetic analyses. These analyses can detect a signature of selection on individual genes and provide knowledge about the evolutionary forces acting on the genes of the sequenced isolates. The pan genome alignment can also be used to detect signatures of selection by calculating the ratio of the number of nonsynonymous substitutions per nonsynonymous site to the number of synonymous substitutions per synonymous site. The value of this ratio is used to infer the direction and magnitude of natural selection, with values >1 implying positive selection (i.e. driving change), values <1 implying purifying selection (i.e. acting against change), and values of exactly 1 indicating neutral selection (i.e. no selection). To determine the ratios for detecting signatures of selection, we use the YN00 model  implemented in the PAML software package . The PAML results are a plain text file that can be viewed in any word processor or imported into statistical analysis software, such as R, for further analysis or plotting.\n\n【17】### Laptop or Desktop Hardware\n\n【18】The bioinformatic pipeline we describe can be partitioned as a function of computer resources (i.e. the number of CPUs, the amount of RAM, and the amount of storage space). Typical laptop or desktop computers might only have enough power to perform the first steps in the pipeline, whereas a high-end workstation would have enough power to perform all the steps for hundreds of samples at once. In many cases, the limiting factor is how much RAM a computer has. Many of the more complex steps in the pipeline require large amounts of RAM, often more than what many laptops and desktops can hold. All the software described can easily be installed and run on a typical desktop or laptop computer . At UPHL, we performed steps 1–4 of the described analyses on bacterial isolates by using an Apple MacBook Pro laptop (Apple, Inc. Cupertino, CA, USA) with a single 3.2-GHz Intel Core i5 processor, 16 gigabytes (GB) of RAM, and 500 GB of storage space . Many PHLs might already have the computational resources needed to perform these bioinformatic analyses on a small number of samples in a reasonable amount of time. However, some basic command-line instructions would be needed to execute software. Numerous online resources, many of them free, will help novices learn the basics of the command-line interface. One such resource is the Biostar Handbook . This online document and e-book is an excellent resource that introduces bioinformatics and covers all of the major areas of focus in bioinformatics, including a crash course in the command-line interface.\n\n【19】### High-end Desktop Hardware\n\n【20】Computers with an increased number of processing cores, more RAM, and more storage space than the typical desktop or laptop computer will allow PHLs to perform all the analyses described here as well as more advanced and computationally intensive analyses . High-end desktops are relatively inexpensive to purchase, and it might be possible to upgrade desktops a PHL already has. All the analyses we describe here were performed at UPHL on an Apple iMac equipped with a single 3.2-GHz Intel Core i5 processor, 32 GB of RAM, and 2 TB of storage space . For 10 isolates, the analyses took ≈5 days to complete. Theoretically, the number of isolates that could be analyzed can be increased to up to hundreds of isolates on a similar high-end desktop computer; however, the amount of time to perform these analyses would also increase substantially.\n\n【21】### Beyond High-end Desktop Hardware\n\n【22】With a high-end Linux-based workstation  and a network-attached storage array, several hundred genomes can be analyzed in a reasonable timeframe . At UPHL, we invested in a high-end Hewlett-Packard workstation (HP, Inc. Palo Alto, CA, USA) with four 3.0-GHz Intel Xeon processors (Intel Corp. Santa Clara, CA, USA), each 3.0 GHz with 12 processing cores; 256 GB of RAM; and a Synology network-attached storage array (Synology, Inc. Taipei, Taiwan) with 24 TB of storage . With such a system and bioinformatics personnel in place, hundreds of genomes can be generated and analyzed in 2–3 days, providing near real-time results for disease outbreak surveillance and monitoring. In addition to high-end computer hardware, experienced personnel are needed to deploy, maintain, curate, and automate bioinformatics pipelines (i.e. bioinformaticians). To take full advantage of computational resources, programs should be automated and linked together so that as data are generated by the sequencer, they are automatically added to the bioinformatics pipelines.\n\n【23】### Discussion\n\n【24】With NGS becoming more and more important for public health laboratories, the need for bioinformatic analyses in greatly increasing. Unfortunately, the pace of WGS implementation is far outpacing the number of bioinformaticians being hired to work in PHLs and, understandably, not all PHLs will have the need, desire, or financial capacity to hire a full-time bioinformatician. The objective of this perspective is to show that bioinformatic analyses can be performed on everything from a simple laptop to a high-end Linux workstation and the user can have little to no experience in bioinformatics or can be a full-fledged bioinformatician. As the volume of sequencing data increases, the ability to connect phenotype to genotype becomes a reality. Knowing a priori that a microorganism is likely to be resistant to antimicrobial drugs or could be a highly virulent strain would greatly improve patient outcomes, improve outbreak surveillance, and help prioritize resources to combat outbreaks. By using molecular evolutionary analyses, PHLs can investigate the evolution of antimicrobial-resistance genes to track in near real-time mutations that are linked to newly acquired resistance genes or novel mutations that result in resistance.\n\n【25】NGS has the potential to revolutionize public health. NGS is not only replacing PFGE, but has the potential to replace traditional culture-based testing as well. Culture-independent diagnostic testing though metagenomic sequencing and analysis has the ability to quickly identify pathogens without applying any type of selection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6539c7fa-2b30-4509-a9bc-8d4f4f94d4fd", "title": "Enhancing Lyme Disease Surveillance by Using Administrative Claims Data, Tennessee, USA", "text": "【0】Enhancing Lyme Disease Surveillance by Using Administrative Claims Data, Tennessee, USA\nLyme disease is the most common tickborne disease in the United States, with >36,000 cases reported to the Centers for Disease Control and Prevention (CDC) during 2013 . Tennessee, a low-incidence state, reported only 25 Lyme disease cases during 2013 . In addition, _Borrelia burgdorferi_ –infected ticks have been identified in only 1 Tennessee county .\n\n【1】CDC estimates that Lyme disease may be underreported by a factor of 10 . A study using administrative claims data from a Tennessee health insurance provider similarly estimated that Lyme disease incidence is 7-fold higher than is reported to the Tennessee Department of Health (TDH) . To determine the usefulness of claims data, which can vary in accuracy , we evaluated medical records of persons given a Lyme disease diagnosis in claims data or surveillance in Tennessee.\n\n【2】### The Study\n\n【3】We examined Lyme disease cases reported to TDH and compared them with diagnoses identified from Blue Cross Blue Shield of Tennessee (BCBST) claims data during January 2011–June 2013. BCBST is a health insurance provider covering ≈50% of Tennessee’s population. TDH cases met the national surveillance case definition for Lyme disease , consisting of the following criteria: clinical (erythema migrans \\[EM\\] rash or late manifestation of disease), laboratory (positive results by immunoassay followed by positive western blot results), and exposure and endemicity (possible exposure to infected ticks < 30 days before rash onset). A person with physician-diagnosed disease who met laboratory criteria was considered to have a probable case. A person with a confirmed case had an EM rash and either met laboratory criteria, had possible exposure to ticks, or had a late manifestation of disease and positive laboratory results. We defined Lyme disease diagnosis for a BCBST-insured person as assignment of \\> 3 primary or secondary codes for Lyme disease (088.81, International Classification of Diseases, Ninth Revision \\[ICD-9\\]), recorded in the claims data.\n\n【4】We used deterministic matching to identify persons in BCBST and TDH data. Medical records of one third of BCBST-insured persons whose cases were not reported to TDH were selected for review. Records were requested from the office visit on the date of Lyme disease diagnosis and for 1 office visit before and after diagnosis. BCBST-insured persons with a Lyme disease diagnosis were then classified according to the case definition . BCBST-insured persons not meeting the case definition were assigned into the following categories: 1) subsequently ruled out through negative laboratory testing, 2) self-reported or physician-recorded history of Lyme disease (before the study period), or 3) insufficient data for case determination. This analysis was exempted from institutional review board review.\n\n【5】During the study period, ≈3 million Tennessee residents were insured by BCBST, and 391 (0.01%) met criteria for diagnosed Lyme disease. During the same period, TDH received 74 reports of Lyme disease (9 confirmed, 65 probable). Of these, 24 (32%) persons were BCBST-insured at time of diagnosis . No differences by age and sex were noted between the 391 BCBST-insured persons and 74 TDH case-patients, and most were identified in highly populated counties (Davidson, Hamilton, Knox, Shelby).\n\n【6】Five Lyme disease cases were identified in both BCBST and TDH data, 386 appeared in BCBST data only, and 19 appeared in TDH data only. All 5 matched persons were classified by TDH as having probable cases. Of the 386 persons only in BCBST, 123 were randomly sampled; 106 medical records were reviewed; only 4 (3.8%) met the case definition (2 confirmed, 2 probable). Extrapolating the proportion of true cases (3.8%) identified from this sample, we believe that ≈14 additional cases would have been identified through review of BCBST claims data during the 2.5-year study period. Adding 14 additional cases to the 24 confirmed and probable cases already reported to TDH among BCBST-insured persons, 38 cases would have been identified. Only 19 of the 38 cases would be identified through review of BCBST data (sensitivity 50%). Of 391 BCBST-insured persons with \\> 3 ICD-9 codes for Lyme disease, 19 met the national case definition (positive predictive value 5%).\n\n【7】Of 102 BCBST-insured persons selected for review whose conditions did not meet the case definition, 22 were subsequently ruled out by laboratory testing after the visit in which the diagnosis was coded. For 27, evidence was insufficient to determine case classification, and 53 had a history of Lyme disease (23/53 \\[43%\\] had been prescribed antibiotic medications to treat Lyme disease).\n\n【8】Nineteen BCBST-insured persons met the case definition and were reported to TDH as having Lyme disease but were not identified as such in BCBST claims data during the study period. In all instances, no ICD-9 code for Lyme disease was coded in billing records, despite the diagnosis in the medical record and subsequent reporting to TDH. The 4 most frequent ICD-9 codes used for these persons were fever (21%), myalgia/myositis (21%), malaise and fatigue (16%), and gynecologic examination (16%).\n\n【9】### Conclusions\n\n【10】By supplementing passive surveillance with BCBST claims data, we identified 20% more Lyme disease cases than were reported to TDH. The additional cases were diagnosed by clinicians and coded as Lyme disease in administrative claims. In this low-incidence state, most BCBST-insured persons with diagnosed Lyme disease did not meet the case definition, and the positive predictive value of BCBST data was low. The resources required to determine true cases from those diagnosed in BCBST claims data were substantial. Without an improved algorithm for identifying true cases, using these administrative data to supplement health department surveillance would be unsustainable.\n\n【11】Medical records of one fourth of the sample lacked sufficient information for case determination, and records of half showed a history of Lyme disease. Strikingly, none of the persons with a history of Lyme disease had any previous ICD-9 code for Lyme disease recorded by BCBST. Also surprisingly, 8 persons who first appeared to be incident case-patients, according to the BCBST algorithm, had been reported to TDH in the past (for 1 case-patient, >10 years earlier). These previously reported cases decreased the positive predictive value of BCBST data.\n\n【12】Among BCBST-insured persons not meeting the case definition, diagnoses were made by a limited number of clinicians. Understanding how these few clinicians came to diagnose many persons with Lyme disease may aid physician training. Of BCBST-insured persons with a history of Lyme disease, approximately half had current prescriptions for antimicrobial drugs. Although we were unable to assess whether any of these prescriptions represented long-term treatment for a chronic Lyme disease diagnosis, providers and patients should be educated regarding the lack of effectiveness and risks associated with long-term antimicrobial therapy .\n\n【13】Half of the BCBST-insured persons had a self-reported or physician-recorded history of Lyme disease that could not be verified by our cross-sectional analysis. One quarter of medical records had insufficient information to make a case determination, stemming from a lack of timely and adequate laboratory testing. Whether these data quality deficiencies biased our results is unknown. A history of Lyme disease does not exclude the potential for reinfection , but the large proportion of persons in this category would be unlikely, given the low incidence of Lyme disease. Southern tick–associated rash illness, caused by _B. lonestari,_ produces an EM-like rash and may have confounded our use of administrative claims to identify Lyme disease .\n\n【14】This study was a special collaboration between TDH and BCBST medical informatics staff and required substantial resources of personnel and time, a level of surveillance not sustainable long-term. Although claims data offer an opportunity for identifying additional Lyme disease cases for public health surveillance, a more efficient means for differentiating cases from noncases is needed before such a system will be practical.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51bdae33-b256-483d-b5a2-895a7dc67096", "title": "Multiplex PCR−Based Next-Generation Sequencing and Global Diversity of Seoul Virus in Humans and Rats", "text": "【0】Multiplex PCR−Based Next-Generation Sequencing and Global Diversity of Seoul Virus in Humans and Rats\nHantaviruses (order _Bunyavirales_ , family _Hantaviridae_ , genus _Orthohantavirus_ ) pose a worldwide public health threat and are the causative agents of hemorrhagic fever with renal syndrome (HFRS) in Eurasia and hantavirus pulmonary syndrome in the Americas . HFRS is caused mainly by Old World hantaviruses, such as Hantaan virus (HTNV), Seoul virus (SEOV), Dobrava–Belgrade virus, and Puumala virus, that are transmitted to humans by inhalation of dust contaminated with rodent excreta (saliva, urine, and feces) or bite by an infected rodent. Annually, 150,000 cases of HFRS are reported (case-fatality rate range <1%–15%) . Clinical signs and symptoms include headache, myalgia, abdominal and back pain, nausea, vomiting, diarrhea, proteinuria, thrombocytopenia, hemorrhage, and renal failure . The typical disease course consists of 5 phases: febrile, hypotensive, oliguric, diuretic, and convalescent: the phases vary in length from several hours to several days. A difficulty in diagnosis is the extensive incubation period from the time of exposure to the onset of symptoms, which might be as long as 50 days. There are no effective vaccines or antiviral agents against hantavirus infection.\n\n【1】SEOV has a negative-sense, single-stranded, tripartite RNA genome . The large (L) segment encodes an RNA-dependent RNA polymerase, the medium (M) segment encodes 2 membrane glycoproteins (Gn and Gc), and the small (S) segment encodes a nucleoprotein. Brown rats ( _Rattus norvegicus_ ) and black rats ( _R. rattus_ ) are the primary reservoir hosts of SEOV and have a worldwide distribution .\n\n【2】SEOV infections have been reported in Asia, Europe, the Americas, and Africa . HFRS caused by SEOV is responsible for 25% of clinical cases and is a mild form with a case-fatality rate of <1% in Asia . Recently, an outbreak of SEOV-induced HFRS was reported in the United Kingdom among rat owners, breeders, and distributors of the pet animal market . In the United States, outbreaks of SEOV infections occurred in 11 states in 2017; there were 17 confirmed SEOV-infected patients . SEOV was identified in New York, New York, and is considered an urban public health threat .\n\n【3】Whole-genome sequencing of SEOV is a prerequisite for tracking SEOV infections and evaluating disease risks for development and implementation of preventive and therapeutic strategies. Acquisition of viral genome sequences plays a critical role in surveillance, identification, and risk mitigation of outbreaks of virus infection . Next-generation sequencing (NGS) is a potent tool for defining virus genome sequences. However, an obstacle for obtaining virus genomic information is ultra-low virus RNA loads in the clinical specimens. To enrich the low amount of viral RNA, we developed a multiplex PCR–based NGS that showed high coverage of HTNV genome sequences from HFRS patients .\n\n【4】In this study, we collected 1,269 _R. norvegicus_ rats in an urban HFRS-endemic area in South Korea during 2000–2016. We report a robust strategy for whole-genome sequencing of SEOV and provide useful insights into epidemiologic characteristics and phylogeographic diversity of a unique worldwide hantavirus.\n\n【5】### Materials and Methods\n\n【6】##### Ethics\n\n【7】Human samples were provided after informed consent was obtained. The study was approved and conducted in accordance with ethicals guidelines for the Korea University Institutional Animal Care and Use Committee. Live trapping of rats at US military training sites and installations was approved by US Forces Korea in accordance with regulation 40–1 (Prevention, Surveillance, and Treatment of Hemorrhagic Fever with Renal Syndrome). Rats were humanely killed by cardiac puncture, and tissues were collected under isoflurane anesthesia in accordance with procedures approved by Korea University Institutional Animal Care and Use Committee protocol #2010–212.\n\n【8】##### Sample Collection\n\n【9】We tested retrospective HFRS patient serum samples obtained from the Korea Bank for Pathogenic Viruses (Seoul, South Korea). We collected _R. norvegicus_ rats during 2000–2016 by using collapsible live-capture traps (Tomahawk Live Trap Co. Hazelhurst, WI, USA, and H.B. Sherman, Tallahassee, FL, USA). Traps were set at intervals of 1–2 m and examined early the next morning over a 1–2-day period at US Army training sites. For the US Army Garrison in Seoul, we used baited live capture traps (Tomahawk Live Trap Co.) or glue boards. Captured rats were submitted to the 5th Medical Detachment/Medical Command Activity–Korea, US Army Garrison (Yongsan, Seoul), and then transported to the College of Medicine, Korea University (Seoul), where they were held in a Biosafety Level 3 laboratory until processing. Live rats were humanely killed by cardiac puncture under isoflurane anesthesia and identified to species by using morphologic criteria and PCR, when required. Serum, lung, spleen, kidney, and liver tissues were collected aseptically and frozen at −70°C until used.\n\n【10】##### Indirect Immunofluorescence Antibody Test\n\n【11】We used an indirect immunofluorescence antibody (IFA) test for serum samples from HFRS patients and live rats. We initially diluted samples 1:32 in phosphate-buffered saline and then tested them for IgG against SEOV. We applied diluted serum samples to slides containing SEOV-infected Vero E6 cells fixed with acetone and incubated wells at 37°C for 30 min. The slides were washed, fluorescein isothiocyanate–conjugated goat antibody to human and rat IgG (ICN Pharmaceuticals, Laval, Quebec, Canada) was added, and slides were incubated at 37°C for 30 min. We then washed the slides again and examined them for virus-specific fluorescence by using a fluorescent microscope (Axio Scope; Zeiss, Berlin, Germany).\n\n【12】##### Real-Time Quantitative PCR\n\n【13】We performed real-time quantitative PCR (qPCR) for total RNA by using the high-capacity RNA-to-cDNA Kit (Applied Biosystems, Carlsbad, CA, USA) in a 10-μL reaction mixture containing 1 µg of total RNA. We used an SYBR Green PCR Master Mix (Applied Biosystems) in a StepOne Real-Time PCR System (Applied Biosystems). We performed reactions at 95°C for 10 min, followed by 45 cycles at 95°C for 15 s, and then 1 cycle at 60°C for 1 min. Primer sequences specific for SEOV S segments were SEOV-S719F: 5′-TGGCACTAGCAAAAGACTGG-3′ and SEOV-S814R: 5′-CAGATAAACTCCCAGCAATAGGA-3′.\n\n【14】##### Reverse Transcription and Rapid Amplification of cDNA Ends PCR\n\n【15】We extracted total RNA from serum or lung tissues of seropositive samples by using TRI Reagent Solution (Ambion Inc. Austin, TX, USA). We synthesized cDNA by using the High Capacity RNA-to-cDNA Kit (Applied Biosystems) and random hexamer or OSM55 (5′-TAGTAGTAGACTCC-3′). For initial identification, we used oligonucleotide primers for SEOV L segment as described . To obtain the 3′ and 5′ termini genome sequences of SEOV, we performed rapid amplification of cDNA ends (RACE) PCR by using a 3′-Full RACE Core Set and a 5′-Full RACE Core Set (Takara Bio Inc. Kusatsu, Shiga, Japan), according to the manufacturer’s specifications. We purified PCR products by using the LaboPass PCR Purification Kit (Cosmo Genetech, Seoul, South Korea). We performed sequencing in both directions of each PCR product by using the BigDye Terminator v3.1 Cycle Sequencing Kit (Applied Biosystems) on an automated sequencer (Applied Biosystems).\n\n【16】##### Multiplex PCR–Based NGS\n\n【17】We designed multiplex PCR primers for SEOV L, M, and S segments and amplified cDNA by using primers  and primer mixtures and Solg 2× Uh-Taq PCR Smart Mix (Solgent, Daejeon, South Korea), according to the manufacturer’s instructions. We performed the first and second enrichments in a 25-μL reaction mixture containing 12.5 μL of 2× Uh pre-mix, 1 μL of cDNA template, 10 μL of primer mixture, and 1.5 μL of distilled water. Initial denaturation was at 95°C for 15 min, followed by 40 cycles or 25 cycles at 95°C for 20 s, 50°C for 40 s, and 72°C for 1 min, and a final elongation at 72°C for 3 min.\n\n【18】We prepared multiplex PCR products by using the TruSeq Nano DNA LT Sample Preparation Kit (Illumina, San Diego, CA, USA) according to the manufacturer’s instructions. We mechanically sheared samples by using an M220 focused ultrasonicator (Covaris, Woburn, MA, USA). The cDNA amplicon was size-selected, A-tailed, ligated with indexes and adaptors, and enriched. We sequenced libraries by using the MiSeq benchtop sequencer (Illumina) with 2 × 150 bp and a MiSeq reagent V2 (Illumina). We imported and analyzed Illumina FASTQ files by using EDGE .\n\n【19】##### Phylogenetic Analysis\n\n【20】We aligned and edited virus genome sequences by using the multiple sequence alignment with high accuracy and high throughput algorithm . We generated phylogenetic trees by using the maximum-likelihood method in MEGA version 6.0  and models for analysis according to the best fit substitution model (TN93 + gamma + invariate for L segments, general time reversible + gamma + invariant for M segments, and T92 + gamma for S segments). We assessed support for topologies by bootstrapping for 1,000 iterations. The prototype strain used, SEOV 80-39, was isolated from _R. norvegicus_ rats captured in Seoul in 1980.\n\n【21】### Results\n\n【22】##### Retrospective Analysis of HFRS Patient Specimens\n\n【23】We found that specimens collected in 2002 from 6 HFRS patients were positive for SEOV by ELISA . We confirmed that the HFRS specimens were serologically positive for SEOV by IFA . Titers of SEOV-specific antibody ranged from 1:128 to 1:4,096. Reverse transcription PCR detected the partial sequence of L segment (nt 2946–3335) from 2 HFRS patients (Hu02-180 and Hu02-258).\n\n【24】##### Epidemiologic Surveillance of _R. norvegicus_ Rats\n\n【25】We collected 1,269 _R. norvegicus_ rats in urban HFRS-endemic areas in South Korea, including the city of Seoul (1,226/1,269) and Gyeonggi (40/1,269), Gangwon (1/1,269), and Jeollanam (2/1,269) Provinces . A total of 76 (6.2%) of 1,226 rats collected in Seoul were serologically positive for SEOV. However, we found IgG against SEOV in only 1 (2.3%) of 43 rats collected from the other areas, including Gyeonggi, Gangwon, and Jeollanam Provinces. We detected SEOV RNA in 13 (16.9%) of 77 seropositive _R. norvegicus_ rats. Serologic prevalence of SEOV in male rats (7.5%, 43/576) was not significantly different from that in female rats (5.0%, 34/684) (p = 0.0763 by χ 2  test). Serologic prevalence of SEOV in rats by weight (age) was 6.1% (21/342) in those weighing < 50 g, 5.9% (22/374) in those weighing 51–100 g, 5.8% (30/521) in those weighing 101–200 g, and 13.8% (4/29) in those weighing 201–300 g. Seasonal prevalence of SEOV infection in rats was 5.8% (11 of 190) in spring (March–May), 4.4% (19/433) in summer (June–August), 6.4% (27/420) in fall (September–November), and 10.0% (19/190) in winter (December–February).\n\n【26】##### SEOV RNA Loads in Tissues from Seropositive _R. norvegicus_ Rats\n\n【27】To measure viral load of SEOV RNA in _R. norvegicus_ rats, we performed real-time qPCR for seropositive samples from lungs, livers, kidneys, and spleens . Viral load for SEOV showed ranges from tissues of 5 rats (Rn02-15, Rn10-134, Rn10-145, Rn11-44, and Rn11-53) that were positive by serologic and molecular screening (IFA+ PCR+). Rat Rn10-145 showed the highest amount of SEOV RNA in all tissues, followed by rats Rn02-15 and Rn10-134. Rat Rn11-44 showed the highest amount of SEOV RNA in all tissues except liver. Rat Rn11-53 showed the highest amount of SEOV RNA load in lung tissues, but virus RNA was not detectable in liver, kidney, and spleen tissues.\n\n【28】##### Multiplex PCR–Based NGS for Retrospective HFRS Patient and _R. norvegicus_ Rat Specimens\n\n【29】We determined viral loads for HFRS patient specimens by using real-time qPCR. Cycle threshold (C t  ) values ranged from 27.5 to 36.8 . To perform multiplex PCR–based NGS for SEOV, we designed multiplex PCR primers to amplify every 150-bp sequence for the entire SEOV tripartite genome. We recovered genomic sequences of SEOV from 6 SEOV-positive patient samples. We sequenced human sample Hu02-258, which showed the highest viral load (lowest C t  value), for 99.6% of the L segment, 99.7% of the M segment, and 91.6% of the S segment. Recovery rates for SEOV genomic sequences from samples Hu02-180 and Hu02-529 showed a correlation with viral loads. Samples Hu02-112, Hu02-294, and Hu02-668 showed high recovery rates of SEOV S and M segments despite lower viral loads (highest C t  values). However, the L segment showed relatively low coverages (85.0% for Hu02-180, 68.2% for Hu02-294, and 72.7% for Hu02-668).\n\n【30】Using total RNA extracted from rat lung tissues, we determined viral loads by using real-time qPCR. C t  values ranged from 16.1 to 27.6. We applied multiplex PCR–based NGS for whole-genome sequencing of 4 SEOV strains in the IFA+ PCR+ rats captured in South Korea during 2000–2016. Coverage of genomic sequences of SEOV was 99.1%–99.7% for L segments, 99.2%–99.7% for M segments, and 98.3%–99.4% for S segments. We observed a correlation between C t  values and multiplex PCR–based NGS coverages . Whole-genome sequences from Rn10-134, Rn10-145, Rn11-44, and Rn11-53 were obtained with termini sequences of 3′ and 5′ ends. SEOV sequences were deposited in GenBank (accession nos. MF149938–MF149957).\n\n【31】##### Global Diversity of SEOV\n\n【32】We generated phylogenetic trees by using nearly complete genome sequences of SEOV and the maximum-likelihood method. Phylogenetic analysis demonstrated distinct phylogenetic groups (groups A–F). Group A contained SEOV strains from northeastern and southeastern China and an SEOV strain from North Korea. Group B contained SEOV strains from Southeast Asia (Singapore and Vietnam) and France. Group C contained SEOV strains from South Korea and Japan and SEOV strains Tchoupitoulas from Louisiana in the United States. Group D contained an EOV strain from Jiangxi and Hubei Provinces in southeastern China. Group E contained strains from the United Kingdom and the United States (New York, NY, and Baltimore, MD). Group F contained SEOV strains from mountainous areas in southeastern China.\n\n【33】We obtained 9 genome sequences of SEOV S segments from HFRS patients and _R. norvegicus_ rats. Phylogenetic analysis of SEOV S segments showed that group A formed a monophyletic lineage with group D . Group C genetically clustered with group E. The phylogeny of group B was distinct from those of groups A, C, D, and E. Group F from mountainous areas in China formed a lineage that was independent from the other groups obtained from rats collected in urban areas.\n\n【34】We obtained 6 genome sequences of SEOV M segments from HFRS patients and _R. norvegicus_ rats . Phylogenetic analysis of SEOV M segments showed distinct phylogenetic clusters (groups A–F). These phylogenetic patterns showed that M segments of SEOV had genetic heterogeneity when compared with S segments.\n\n【35】We obtained and phylogenetically analyzed 5 SEOV L segments . The SEOV L segment from an HFRS patient and _R. norvegicus_ rats captured in South Korea clustered to form a monophyletic group with SEOV 80-39. SEOV strains from China belonged to a genetic lineage with SEOV DPRK08 from North Korea. SEOV strains from the United Kingdom and Baltimore formed a close phylogenetic group. SEOV IR33 and IR473 obtained from laboratory outbreaks in the United Kingdom were independent from other SEOV strains.\n\n【36】### Discussion\n\n【37】NGS is a robust tool for obtaining extensive genetic information and completing whole-genome sequences . However, molecular enrichment plays a critical role in amplification of pathogen genome sequences from clinical or animal specimens. Our previous study showed recovery of nearly whole-genomic sequences of HTNV from HFRS military patients by using virus-targeted molecular enrichment .\n\n【38】In this study, whole-genomic sequencing of SEOV, an etiologic agent of mild HFRS worldwide, was applied to samples from retrospective HFRS patients and seropositive _R. norvegicus_ rats by using multiplex PCR–based NGS. Nearly whole-genome sequences of SEOV tripartite RNA, on the basis of SEOV 80-39 (prototype strain), corresponded to viral loads of patient serum samples and rat lung tissues. Phylogenetic analyses of the genome sequence of SEOV tripartite RNA supported worldwide distributions of SEOV and identified 6 genetic lineage groups. Group A contained SEOV strains from northeastern and southeastern China and North Korea. Group B contained SEOV strains from Singapore and Vietnam in Southeast Asia and Lyon in France. Group C contained SEOV strains originating primarily in South Korea and Japan and an SEOV strain from Louisiana in the United States. Group D consisted of SEOV strains from southeastern China, including Jiangxi and Hubei Provinces. Group E contained SEOV strains from the United Kingdom and eastern United States (New York and Baltimore) and formed a monophyletic lineage. Group F contained SEOV strains from mountainous areas in southeastern China .\n\n【39】SEOV originated in China and spread worldwide during movement of rats coincidently with human activities (e.g. commercial trade, travel, and migration by railways and through seaports) . The close genetic relationship of SEOV in South Korea and Japan was probably caused by geographic distance and historical activities (e.g. commerce and occupation by Japanese forces). The genetic lineage containing strains from Southeast Asia and France might have originated during colonization or on trade routes that extended distribution of SEOV-infected rats . Recently, SEOV outbreaks have been reported in the United Kingdom and United States. Clinical cases showed that SEOV infections were identified among pet owners, breeders, and distributors . The genetic relationship of SEOV between counties probably reflects movement of rats associated with the animal pet market.\n\n【40】The prevalence of hantaviruses (e.g. HTNV and Imjin virus \\[MJNV\\]) in natural reservoir hosts has showed sex- and weight (age)–specific differences . However, in our study, the incidence of SEOV in _R. norvegicus_ rats was not dependent on sex and weight (age). Epidemiologic differences in hantavirus infections between _A. agrarius_ and _R. norvegicus_ rats might be, in part, caused by ecologic differences, reservoir host distributions, and behavior (e.g. association with humans) . Seasonal circulation of SEOV infection was maintained over 1 year, suggesting an enzootic infectious cycle. These observations might suggest that preventive strategies for disease risk mitigation focus on limits of rat populations all year.\n\n【41】Our previous study demonstrated differential amounts of HTNV RNA in lung, kidney, liver, and spleen tissues of rodents collected in areas in which HFRS is prevalent . In addition, the genomic RNA load of MJNV, a shrewborne hantavirus, showed various patterns in different tissues in nature . IFA+ PCR+ shrews showed high and various loads of MJNV RNA in all tissues. MJNV RNA from IFA– PCR+ shrews was detected in lung but not in kidney, liver, or spleen tissues, indicating an early phase of infection before MJNV-specific IgG was produced . In our study, rats Rn02-15, Rn10-134, and Rn10-145 showed various amounts of SEOV RNA in all tissues. Rat Rn11-44 had high levels of SEOV RNA in all tissues except the liver. Virus RNA in rat Rn11-53 might reflect the early phase of SEOV infections because of highest viral load in lung tissues but not other tissues. Patterns of SEOV RNA loads might indicate systemic infections in nature and active circulation of virus among rat populations in urban HFRS-endemic areas.\n\n【42】Diversity of virus genomes results from genomic variation or exchanges . RNA viruses show high mutation rates caused by deficiencies in proofreading by virus polymerases. Genomic variation also results from a mechanism of host immune evasion . Genetic exchanges, such as reassortment and recombination, lead to the generation of divergent virus progeny . Our previous studies identified reassortment and recombination of hantaviruses, including HTNV and MJNV, in nature . Using nearly complete sequences of SEOV S, M, and L segments, phylogenetic analyses demonstrated that S segments of group A SEOVs formed a cluster with those of group D SEOVs and that L and M segments of group A SEOVs showed a close phylogenetic relationship with those of group B SEOVs. The S segment of group C SEOVs grouped phylogenetically with group E SEOVs. However, L and M segments of group C SEOVs formed a distant genetic cluster from those of group E SEOVs. Phylogenetic analysis of SEOV S segments showed a differential pattern from that of SEOV M segments, indicating a genome organization compatible with genetic exchanges in nature. To clarify genetic events among SEOV worldwide, whole-genome sequences of the SEOV L segment need to be investigated. Application of multiplex PCR–based NGS will be useful in elucidating phylogenetic patterns of the SEOV L segment.\n\n【43】In conclusion, this epidemiologic survey of _R. norvegicus_ rats in urban HFRS-endemic areas of South Korea identified the prevalence and distribution of SEOV. We applied multiplex PCR–based NGS to whole-genome sequencing of SEOV tripartite RNA from retrospective serum samples from HFRS patients and rat tissues. Phylogenetic analyses demonstrated the global distribution and genetic diversity of SEOV on the basis of nearly complete genome sequences. This study provides useful information for SEOV-based surveillance, disease risk assessment, and mitigation against hantavirus outbreaks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4e012391-aca7-4e11-8143-975a13aaab4c", "title": "Adherence to Antimicrobial Inhalational Anthrax Prophylaxis among Postal Workers, Washington, D.C., 2001", "text": "【0】Adherence to Antimicrobial Inhalational Anthrax Prophylaxis among Postal Workers, Washington, D.C., 2001\nIn October 2001, two letters with _Bacillus anthracis_ spores were mailed to offices on Capitol Hill, Washington, D.C. Both letters were processed at the Washington, D.C. Processing and Distribution Center (DCPDC) of the U.S. Postal Service (USPS). Inhalational anthrax developed in four DCPDC postal workers; two died. More than 2,000 workers and business visitors to the private work areas of DCPDC were potentially exposed to aerosolized _B. anthracis_ spores during October 12–21 . To prevent inhalational anthrax, 60 days of antimicrobial therapy was recommended (primary: ciprofloxacin 500 mg/orally twice a day or doxycycline 100 mg/orally twice a day; alternative: amoxicillin 500 mg/orally twice a day).\n\n【1】Although inhalational anthrax most often develops in the first 7–10 days after exposure, incubation periods as long as 43 days have been reported in Sverdlovsk, Russia  ; in animal studies, inhalational anthrax occurred after 58 days despite 30 days of antimicrobial therapy  . Therefore, completion of the full 60 days of prophylactic antimicrobial therapy was essential for all postal workers potentially exposed to _B. anthracis_ spores at the DCPDC.\n\n【2】Adherence to long-term drug regimens is problematic, and multiple factors influence adherence status, such as regimen factors (e.g. number of pills needed daily), structural factors (e.g. ability to access drugs), individual factors (e.g. cognitive limitations, depression), and health-care provider factors (e.g. ability to listen to and communicate effectively with patients) . Among the DCPDC workers, typical adherence issues associated with short-course antimicrobial therapy were complicated by the high levels of stress associated with the bioterrorism event and the illnesses and deaths of coworkers, stigma from other postal workers and community members because of erroneous concerns that DCPDC workers were contagious, and the relatively longer duration and potential adverse effects associated with the therapy. The DCPDC facility was closed October 21, 2001, and employees were displaced to work in other area mail facilities, contributing to ongoing disruptions of the workers’ daily lives and further complicating adherence. Last, the dynamic nature of the bioterrorist event created a system of evolving health-risk communication that, combined with the many inconsistent sources of information about the event and anthrax, contributed to confusion and misinformation.\n\n【3】In response to the first bioterrorism-related outbreak of inhalational anthrax in the United States, strategies to promote adherence to antimicrobial prophylaxis among more than 2,000 DCPDC workers were rapidly implemented. To facilitate future adherence activities in similar events, we evaluated the interventions that were used to support adherence and examined the factors that influenced adherence to the prophylactic regimen in DCPDC workers.\n\n【4】### Methods\n\n【5】##### Qualitative Data Collection\n\n【6】Qualitative data were collected from open-ended interviews (i.e. ones in which interviewer writes down exact responses of interviewee) with convenience samples of the postal worker population throughout the 60-day period to develop and evaluate the interventions and to collect information on the determinants of adherence. The findings from the qualitative interviews were used to develop and validate the close-ended questions (i.e. those with a defined set of answers to choose from, such as yes or no) included in the quantitative survey questionnaire. Information was collected through observation, one-on-one contact, informal small group discussions, and focus group interviews with workers, as well as through interactions with USPS management, worker union representatives, and USPS Employee Assistance Program personnel.\n\n【7】Two staff members from the Centers for Disease Control and Prevention (CDC) conducted five focus group interviews with DCPDC workers during December 13–16, 2001. DCPDC shift supervisors selected six to eight workers to participate in each focus group. During the interviews, workers’ responses were noted verbatim on a large flip chart visible to participants at all times. The first author also carried out individual qualitative open-ended interviews during routine interactions with workers throughout December 2001.\n\n【8】The first author conducted all analyses. Notes were immediately reviewed for accuracy at the completion of all interviews and entered into a word-processing software program. Qualitative analysis included several rounds of coding by subject or theme, as well as content analysis and comparison of responses across groups. Analysis focused on both commonly repeated themes (reported by at least 50% of the respondents) and rare points of view.\n\n【9】##### Interventions to Promote Adherence\n\n【10】To develop appropriate adherence interventions, we obtained support from the USPS management, Employee Assistance Program, and postal service unions. We conducted open-ended interviews with postal workers from various jobs and shifts and incorporated known adherence strategies  to develop interventions.\n\n【11】Public health staff carried out repeated group question-and-answer sessions and informal contact with workers. These sessions consisted of large and small group and one-on-one interactions to counsel workers. Motivational messages were distributed through the USPS communication infrastructure. In addition, several types of written materials were distributed at the worksite and to workers’ homes, including booklets of frequently asked questions about anthrax and antimicrobial therapy, antimicrobial pocket guides with calendar memory aids, and handouts describing ways to minimize stress and recognize the known adverse effects of antimicrobial therapy, such as gastrointestinal upset and yeast infection. Posters and table tents, both with motivational messages, were placed in the workplace. We also provided a letter for workers to take to their personal health-care provider clarifying which area postal workers needed extended prophylaxis and the recommended regimens. This letter was also distributed directly to area health-care providers. Further, after free antimicrobial agents were no longer available, access to antimicrobial agents and reimbursements was facilitated. Finally, clinical team members and a local health-care provider answered specific questions about adverse effects or potential drug interactions, and the local health-care provider consulted with workers free of charge.\n\n【12】In addition, multiple Morbidity and Mortality Weekly Reports , Health Alert Network alerts, and live broadcasts were disseminated throughout the prophylaxis period to give health-care providers detailed information on which groups needed extended prophylaxis, the recommended regimens, and clinical signs of inhalational anthrax disease.\n\n【13】##### Quantitative Survey\n\n【14】At five mail facilities, trained interviewers administered a close-ended questionnaire to a convenience sample of all DCPDC employees working the day shift (7 a.m.–3 p.m.) on December 18–20, 2001, days 57–60 of the 60-day regimen. Prophylaxis was first offered October 21, 2001, and most workers picked up prophylaxis on October 22 or 23, 2001. Most (80%) of the displaced DCPDC employees worked at these five facilities. Compared with the day shift, more employees work the swing shift and night shift, when the mail collected during the day is processed.\n\n【15】The questionnaire collected information on demographic characteristics, adherence behaviors, enablers and obstacles to adherence, and information about the implementation of interventions. To assess adherence, workers were asked to respond to five questions located throughout the survey. (For example, “Are you still taking antibiotics for anthrax?” \\[Possible responses: No, Yes, Declined\\] and “If you forgot to take any of your pills yesterday, how many pills did you miss?” \\[Possible responses: None, One, Two, Three.).\n\n【16】Because we were interested in adherence to the recommendation to complete 60 days of prophylaxis, workers were divided into one of three categories. Adherence was defined as full if workers reported they continuously took their antimicrobial therapy throughout the 60-day period, never reduced their dosage, and did not forget any pills the previous day. Adherence was defined as intermediate if workers reduced the dosage, forgot a pill the previous day, or stopped their antimicrobial therapy and restarted at least once. Adherence was defined as discontinued if workers stopped their antimicrobial therapy and never restarted.\n\n【17】To analyze predictors of nonadherence, we carried out a three-step logistic regression modeling procedure. First, we modeled overall nonadherence (intermediate adherence and discontinued groups combined) compared with full adherence. For this model, we were interested in understanding the differences between those workers who were fully adherent and those who were not fully adherent, including workers who completely discontinued therapy. Second, we modeled intermediate adherence compared with full adherence. For this model, we were interested in understanding the differences between those who were nonadherent but who had not completely discontinued therapy and those who were fully adherent. Third, we modeled the discontinued group compared with the full adherence group. For this model, we were interested in assessing the differences between those who had completely discontinued therapy and those who were fully adherent.\n\n【18】Variables examined were based on previously published articles on adherence and those associated with perceived risk and potential exposure to _B. anthracis_ spores in this setting. Inhalational anthrax developed in employees who worked on a sorter machine and in the government mail section of the DCPDC  . Variables included age, sex, race, perceived risk of breathing in _B. anthracis_ spores, work location during exposure period, work description during the time of interview, trouble remembering to take pills, experiencing anxiety, physical signs of stress, severity of adverse effects, and adverse effects negatively affecting work performance. For all analysis, SAS 8.2 (SAS Institute, Inc. Cary, NC) was used. For univariate analysis, two-tailed p values were calculated by chi-square test for dichotomous variables. Potential covariates for the logistic regression models included those with p<0.20 in univariate analysis, and possible confounders. We followed a backward elimination strategy to remove nonsignificant covariates in building final parsimonious models. A p<0.05 was determined to be statistically significant.\n\n【19】For all qualitative and quantitative interviews, workers were informed that their participation was voluntary and anonymous. Anthrax infections did not develop in any of the workers who participated in the interventions or interviews.\n\n【20】### Results\n\n【21】##### Characteristics of Participants\n\n【22】Of 251 DCPDC workers invited to participate in the questionnaire, 245 (98%) agreed. Among participants, 124 (51%) were male, and 214 (88%) identified themselves as black. Only 1 (0.5%) worker was 18–24 years of age, 74 (30%) were 25–44 years, 163 (67%) were 45–64 years, and 6 (2%) were \\> 65 years of age.\n\n【23】##### Comparison of Adherence among Workers\n\n【24】Among those who completed the questionnaire, 98 (40%) reported full adherence, 45 (18%) discontinued prophylaxis and never restarted, and 102 (42%) were classified as intermediate. Overall, 186 (76%) workers were taking prophylaxis at the time of the interview, including 88 (86%) of the 102 classified as in the intermediate group. Among the intermediate group, 14 (14%) reported discontinuing prophylaxis and restarting at least once, but they were not taking antibiotics at the time of the interview. A total of 45 workers from the discontinued group and 48 workers from the intermediate group reported stopping prophylaxis.\n\n【25】Among the 102 workers classified as intermediate, 40 (39%) reported ever reducing the dosage, 65 (64%) forgot to take at least one pill the previous day, and 48 (47%) reported discontinuing prophylaxis and restarting at least once. Among those who restarted, 20 (42%) missed at least one pill the previous day, and 22 (46%) reported they had ever reduced the dosage.\n\n【26】We examined reasons for stopping prophylactic antimicrobial therapy . Most workers reported that several factors influenced their decision to discontinue prophylaxis; 60% cited five or more reasons. Trouble managing adverse effects to antimicrobial agents was the most common reason. Concern over possible long-term adverse effects associated with prolonged antimicrobial therapy was the second most common reason for stopping. Similar reasons were given by the workers who reported reducing the dosage of the prescribed antimicrobial therapy. Workers who stopped therapy also reported lacking sufficient information about anthrax and antimicrobial therapy, specifically, information from USPS or CDC.\n\n【27】##### Predictors of Nonadherence\n\n【28】We wanted to understand the differences between those who were not fully adherent, excluding those who completely discontinued therapy, compared with those who were fully adherent. We therefore modeled intermediate adherence compared with full adherence. Characteristics of these populations and univariate analysis are in Table 2 . Independent predictors of intermediate adherence included experiencing “a lot” of adverse effects to antimicrobial therapy, trouble remembering to take pills, as well as age <45 years . Experiencing “a lot” of adverse effects, trouble remembering to take pills, and age <45 years were also risk factors for nonadherence in a model combining the intermediate adherence and discontinued groups compared with full adherence (data not shown).\n\n【29】We wanted to understand the differences between those who completely discontinued therapy and those who were fully adherent. We therefore modeled the discontinued group compared with the full adherence group. Characteristics of these populations and univariate analysis can be found in Table 4 . Independent predictors of discontinuing therapy included experiencing “a lot” of adverse effects, anxiety, and age <45 years . Those workers who reported a high perceived risk of having breathed in _B. anthracis_ spores during October 12–21, 2001, were significantly less likely to have discontinued therapy. Those who experienced five or more physical signs of stress were also significantly less likely to have discontinued therapy.\n\n【30】##### Postal Workers’ Experiences and Qualitative Evaluation of Interventions\n\n【31】A total of 38 workers participated in five focus groups, and 22 participated in individual qualitative interviews. The age, sex, and race/ethnic characteristics of qualitative interview participants were similar to those of respondents to the survey questionnaire.\n\n【32】When asked in focus groups and individual qualitative interviews about what adherence interventions were helpful, workers consistently cited repeated visits by public health staff to worksites. Workers reported that the ability to ask personal questions and the distribution of various materials covering multiple health- and work-related issues helped workers complete prophylaxis and promoted adherence by providing accurate and needed information about anthrax, antimicrobial therapy, risk for disease, and the outbreak investigation. Workers reported that this information helped reduce their stress levels and motivated them to continue prophylaxis.\n\n【33】Workers recalled receiving little information at the free antimicrobial distribution sites, and some had forgotten or misunderstood the initial information given. Several opportunities to speak with public health staff were necessary to clarify questions, especially as new issues arose. However, some workers complained that public health staff could not provide adequate answers to all their questions, such as those related to the long-term status of viable _B. anthracis_ spores inhaled into the lung, the long-term effects of extended antimicrobial therapy, environmental sampling results, the need for personal protective gear, and other occupational health concerns.\n\n【34】In the questionnaire, 82% of workers reported they wanted to receive public health information in a variety of formats, including both orally and written, as well as information from the media. The questionnaire showed that only 3% of workers did not participate in oral communication interventions, 2% did not receive written materials distributed to employees at the worksite or at their homes, and 21% did not see posted signs and messages at work.\n\n【35】### Discussion\n\n【36】After the first bioterrorism-related anthrax outbreak in the United States, we rapidly developed and implemented multiple adherence interventions to prevent inhalational anthrax in >2,000 DCPDC workers. This was the first time adherence interventions have been conducted and evaluated in an applied public health bioterrorism response. Our interventions promoted the message that adherence was essential for the full 60 days of antimicrobial therapy. Further, the interventions were carried out during the entire 60-day period. Seventy-six percent of postal workers were taking antimicrobial prophylaxis at the time of the evaluation. Despite differences in assessing adherence, the adherence found in this study was relatively high compared with other studies of adherence to short-course antimicrobial therapy. For example, Ley  reported approximately 50% adherence in a review of adherence studies to short-course antibiotics, and Brookoff  reported only 31% adherence to a 10-day course of doxycycline (n=386) for outpatient treatment of pelvic inflammatory disease.\n\n【37】Many issues hindered adherence in this anthrax outbreak, including adverse effects of the antimicrobial prophylaxis, such as gastrointestinal upset and yeast infection, trouble remembering to take the pills, perceived risk, anxiety, and physical signs of stress. Although these factors occurred in the context of a bioterrorism event, similar adherence obstacles have been reported elsewhere . Additional issues complicating adherence among postal workers included the large number of workers affected, occupational health and other work-related issues, limited capacity of local departments of health to undertake a program to promote adherence for a large number of people in an emergency, and the hysteria and media coverage associated with this bioterrorism event, which likely magnified miscommunication and workers’ confusion.\n\n【38】In developing the intervention protocols, we drew upon lessons learned from adherence strategies for isoniazid treatment for latent tuberculosis infection and highly active antiretroviral therapy for HIV infection. Studies of these strategies conclude that interventions must be multifaceted, ongoing, flexible, individualized, and repetitive to achieve optimal adherence levels . Our interventions included many of these characteristics, such as repeated visits, clarifying questions, counseling workers, incorporating pill-taking into daily routines, and providing workers with as much information as possible about anthrax and antimicrobial therapy. Inhalational anthrax as a disease and bioterrorism-associated disease are complex issues and relaying this information to people was difficult. Therefore, multiple formats (verbal, written, and graphic) were necessary to effectively communicate information to workers.\n\n【39】Many workers mistook signs of stress (e.g. complaints of fatigue, lack of sexual drive, and increased crying) for adverse effects of the antimicrobial therapy. Further, the stress associated with the bioterrorist event magnified the adverse effects associated with prophylaxis. For some symptoms, distinguishing between adverse effects of stress and those of the antimicrobial therapy, such as gastrointestinal upset, was impossible. Those who worked close to areas where coworkers with inhalational anthrax had worked reported more physical signs of stress, had a higher perceived risk of having breathed in _B. anthracis_ spores, and were also more likely to have continued therapy. Those who had anxiety were more likely to have discontinued therapy. Published articles report associations between anxiety or depression and nonadherence , and some researchers posit that the inability to cope with anxiety is the better predictor of nonadherence  . These findings highlight the importance of communicating early and repeatedly the known adverse effects people should expect, and how to manage all potential effects, including those caused by prophylaxis and stress or anxiety related to bioterrorist events.\n\n【40】Only self-reports were collected to assess adherence in this evaluation. Several studies suggest that self-reporting overestimates adherence, while reports of nonadherence are usually valid . Therefore, our results may have overestimated adherence, but it is unlikely that we overestimated the number of persons who discontinued prophylaxis. Data were collected from a convenience sample and may not be representative of all DCPDC workers. A March 2002 phone survey among DCPDC workers (62% response rate) reported similar age, sex, and race/ethnicity characteristics  . Because we did not have a control group who did not receive interventions to promote adherence, we cannot measure the effectiveness of our interventions; however, our adherence findings were similar to those of other studies that were not implemented in the setting of a bioterrorist emergency response . In addition, the evaluation was conducted during the holiday season, the busiest time of the year for the USPS, and we were permitted to conduct the questionnaire only with workers on the day shift (7 a.m.–3 p.m.). The experiences of day-shift workers may be different from those who work other shifts, although, based on the qualitative findings carried out with workers from all shifts and the continual interactions with workers throughout the 60-day period, these findings likely reflect the experiences of most DCPDC workers. Last, our evaluation may have been affected by the general media coverage of the bioterrorism events.\n\n【41】Nonadherence is common and should be expected in all settings, especially in a bioterrorism-related context that involves further challenges and complications to adherence. Considering the large number of workers who took less than the recommended regimen, evaluating adherence promotion interventions during bioterrorist outbreaks is very important. In emergency settings, adherence programs may overburden local departments of health because they require ongoing personal interactions and are labor-intensive when large numbers of people are affected. Efforts to develop a plan to promote adherence in the event of a bioterrorism outbreak, which could be tailored to the situation and implemented immediately, will aid future public health emergency responses where adherence to recommended prophylaxis is necessary to save lives. During occupational exposures, supplementing occupational health resources may be necessary. To optimally promote adherence, such plans should incorporate continual interaction with the affected persons, provide consistent and clear messages, and include interventions that help persons incorporate pill-taking into daily routines and manage known adverse effects, including those caused by prophylaxis, anxiety, and stress related to bioterrorism events.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "81bc1260-ed0e-40ca-a6f6-1e222dc585d7", "title": "Taenia solium Tapeworm Infection, Oregon, 2006–2009", "text": "【0】Taenia solium Tapeworm Infection, Oregon, 2006–2009\nNeurocysticercosis (NCC) is a parasitic disease caused by central nervous system infection with _Taenia solium_ larval cysts. It is the most common helminthic infection of the central nervous system and a leading cause of acquired epilepsy in Latin America, Southeast Asia, and central Africa . The disease also is increasingly of clinical and public health concern in the United States, primarily in immigrants and travelers from cysticercosis-endemic regions .\n\n【1】Cysticercosis is acquired through fecal–oral transmission of tapeworm eggs shed in the feces of a human carrying intestinal tapeworms. Ingested eggs release oncospheres, which invade the intestinal mucosa and disseminate throughout the body to form larval cysts. NCC occurs when cysts develop in the central nervous system and is the primary source of illness and death . The tapeworm’s complete life cycle occurs in regions with poor sanitary infrastructure, where foraging pigs have access to human feces. Most NCC cases in the United States probably were acquired in cysticercosis-endemic areas by immigrants or travelers who entered the United States already infected with cysts . However, immigrants and travelers also can harbor intestinal tapeworms, and domestic transmission of NCC does occur .\n\n【2】Few states require reporting of cysticercosis; thus, population-based epidemiologic data in the United States are limited. Even in jurisdictions that require reporting, the clinical nature of NCC diagnosis complicates surveillance efforts because no single laboratory test definitively establishes the diagnosis. Surveillance therefore relies on clinician or institutional reporting. In 1989, California became the first state to require reporting; 112 cysticercosis cases were reported during the first year, for a crude annual incidence of 1.5 cases per 100,000 Hispanics . A retrospective case-series from Oregon based on hospital discharge diagnoses during 1995–2000 estimated an annual incidence of 0.2 cases per 100,000 general population and 3.1 cases per 100,000 Hispanics . In 5 cases, no exposure to a cysticercosis-endemic area was documented, which suggests the possibility of local transmission.\n\n【3】Oregon adopted administrative rules for _T. solium_ reporting in 2002 after the coroner’s examination implicated hydrocephalus secondary to obstructing ventricular cysts in 2 unexplained deaths . However, no subsequent efforts were undertaken to stimulate passive reporting or to actively find unreported cases. As a result, only 7 NCC cases, all in Hispanics, were reported to public health officials during the first 5 years of reporting. Oregon has a rapidly growing Hispanic population, which currently represents 11% of the total population. Approximately half of all Oregon Hispanics report birth outside the United States . In the context of an increasing population at risk, the small number of passively reported cases suggests inadequate surveillance.\n\n【4】Identification and treatment of tapeworm carriers in the United States could prevent additional NCC cases. However, intestinal tapeworm infection produces few symptoms, and the prevalence is typically <1%–2%, even in regions where cysticercosis is endemic . During the 1980s, Los Angeles (LA) County, California, adopted a program of screening for tapeworm carriers with some success. By screening household members of NCC case-patients using light microscopy on fecal samples, the county identified an intestinal tapeworm carrier in 7% of its overall investigations and in 22% of investigations involving domestically acquired NCC . Improved screening methods have been developed in the interim, including an ELISA for _Taenia_ sp. coproantigens in feces and an enzyme-linked immunoelectrotransfer blot (EITB) for serum antibodies against _T. solium_ tapeworm . Serologic methods are desirable because they are specific to _T. solium_ intestinal infection and highly sensitive (99%) and avoid the collection and processing of potentially infectious feces .\n\n【5】Our objective was to evaluate the utility of public health surveillance for _T. solium_ infection in Oregon. We implemented population-based active surveillance to determine the incidence of cysticercosis. We also piloted screening specifically for additional _T. solium_ infection among affected households by using a combination of symptom screening, laboratory analysis of fecal and serum specimens, and radiographic imaging.\n\n【6】### Methods\n\n【7】##### Case Definition and Surveillance Period\n\n【8】We used the cysticercosis case definition in State of Oregon communicable disease investigative guidelines . This definition classifies cases as confirmed, presumptive, or suspected according to published consensus criteria . Because clinical criteria for a definitive diagnosis (pathologic specimen, radiographic imaging demonstrating the scolex or direct visualization of the parasite on fundscopic eye examination) are seldom available, a presumptive diagnosis is common. We defined cases as infection diagnosed initially during January 1, 2006–December 31, 2009. Cases in non-Oregon residents were excluded. The Institutional Review Boards at Oregon Health & Sciences University and the State of Oregon Public Health Division reviewed and approved this study.\n\n【9】##### Case Ascertainment\n\n【10】During the 2009 calendar year, we requested quarterly reports of International Classification of Diseases, 9th Revision, billing codes for cysticercosis (123.1) from all Oregon hospital systems. The first request included identification of historical cases going back to January 1, 2006; subsequent reports included only cases for the current quarter. All hospitals reported inpatient admissions, and those with integrated electronic medical records systems reported outpatient visits as well. To identify additional cases, we queried the main regional reference laboratory for cysticercosis serologic testing in Oregon. We also searched Oregon vital statistics data for deaths related to cysticercosis under all listed causes of death, both by diagnosis code and by keyword (i.e. cysticercosis, neurocysticercosis, and taenia). We obtained medical charts for all reported cases to verify the diagnosis and to extract clinical and epidemiologic data.\n\n【11】To stimulate passive reporting, we sent a letter to clinicians likely to diagnose NCC, including migrant worker health providers, radiologists, pathologists, neurologists, neurosurgeons, and infectious disease and emergency department physicians. We also distributed a newsletter about _T. solium_ infection and reporting requirements to all licensed Oregon physicians.\n\n【12】##### Household Investigations\n\n【13】Persons with confirmed and presumptive cases diagnosed after July 1, 2008, were eligible for household investigation. After obtaining informed consent from the case-patient and all household contacts, the study physician used a standard interview tool to gather demographic, clinical, and epidemiologic data . From each participant, we collected 1 fecal sample preserved in 10% formalin and a finger-stick blood sample on quantifiable filter paper preserved in StabilZyme Select (SurModics; Eden Prairie, MN, USA) stabilizer. We offered noncontrast computed tomography (CT) scan of the head to any household contact with clinical history of seizures or severe or chronic headaches or with any positive finding from laboratory tests.\n\n【14】##### Laboratory Methods\n\n【15】Laboratory processing was conducted at the Centers for Disease Control and Prevention Parasitology Diagnostics Laboratory (Atlanta, GA, USA). Fecal samples were examined by light microscopy for _Taenia_ spp. eggs or proglottids and by ELISA for _Taenia_ spp. coproantigens . Serum samples were analyzed by EITB for antibodies against _T. solium_ cysts (EITB lentil lection–bound glycoprotein) and against _T. solium_ adult tapeworms (recombinant EITB \\[rEITB\\]) . The EITB lentil lection–bound glycoprotein uses a semipurified fraction of homogenized _T. solium_ cysts containing 7 _T. solium_ glycoprotein antigens . The rEITB for taeniasis is based on baculovirus expression–purified recombinant antigen rES33 . We defined active intestinal (adult) tapeworm infection by either a positive ELISA coproantigen or by _Taenia_ spp. eggs or proglottids in the fecal sample. We interpreted a positive serum rEITB in participants with negative fecal findings to indicate cleared _T. solium_ intestinal tapeworm infection.\n\n【16】##### Data Analysis\n\n【17】Annual incidence rates were expressed as the number of cases per 100,000 population, with denominator estimates obtained from American Community Survey yearly estimates . We analyzed data using STATA version 10 (Stata Corp. College Station, TX, USA). Continuous variables were assessed by using either Kruskal-Wallis or Mann-Whitney tests for differences among or between groups of interest. We used the Fisher exact test to compare distributions of proportions or to examine association between pairs of categorical measures. All tests are 2-sided, with significance set at 0.05.\n\n【18】### Results\n\n【19】We found 143 unique reports with diagnosis code 123.1 for Oregon residents during the surveillance period. Of the 56 (39%) reports we excluded, insufficient chart information was available to verify diagnosis or incidence year for 18 cases, and 38 cases were diagnosed before 2006. Of the remaining 87 cases, 79 (91%) were identified through active surveillance, including 75 (86%) through hospital queries, 2 (2%) through laboratory queries, and 2 (2%) during household investigations. Eight (9%) cases were spontaneously reported by clinicians, of which 6 were from 1 infectious disease clinician in a tertiary care hospital.\n\n【20】Of the 87 remaining cases, 19 (22%) were confirmed, 53 (61%) presumptive, and 15 (17%) suspected. Confirmed and presumptive cases therefore accounted for 72 (83%) of the total. All case-patients had radiographic imaging of the head; 83 (95%) had a CT scan, and 52 (60%) had magnetic resonance imaging (MRI). Confirmed and presumptive case-patients were 8.5× more likely than suspected case-patients to have received an MRI (odds ratio 8.5, 95% confidence interval 2.0–50.2). Birth country and travel history were not recorded for 12 of the suspected case-patients; an epidemiologic link to a cysticercosis-endemic area would have changed the classification to presumptive in all 12. The other 3 suspected case-patients had radiologic evidence suggestive of cysticercosis or links to a cysticercosis-endemic area, but their symptoms could have been explained by other diagnoses. Suspected case-patients were more likely to be female (p = 0.04) and older (p<0.01) and to have calcified lesions (p<0.01) than were confirmed or presumptive case-patients .\n\n【21】Of the 72 confirmed and presumptive case-patients, 41 (57%) were hospitalized at time of diagnosis. The median inpatient stay was 4 days (interquartile range \\[IQR\\] 3–9), accounting for a total of 292 hospital days during initial illness only. Of these 41 hospitalizations, intensive care was involved in 16 (39%). Suspected case-patients were less likely to receive treatment with antiparasitic drugs (p = 0.03) or corticosteroids (p = 0.03); otherwise, hospitalization and treatment did not differ between patients with confirmed or presumptive cases. No deaths occurred for which cysticercosis was listed as a contributing factor.\n\n【22】We excluded suspected cases from incidence calculations . Sixty-nine (96%) cases occurred in Hispanics. Including the 12 suspected cases for whom epidemiologic data were unavailable would increase the estimated mean annual incidence to 0.6 cases per 100,000 Oregon residents and 6.7 per 100,000 Hispanic Oregon residents. More case reports occurred during the active study year , but the number of reports was not significantly higher in 2009 than in previous years (p = 0.08).\n\n【23】Country of birth was documented in the medical charts of 55 case-patients . Three (5%) were US born; all were Hispanic. One was a 49-year-old man who denied any international travel; he had 1 obstructing fourth ventricular cyst confirmed by surgical pathology. Another US-born case-patient was a 24-year-old man with new-onset seizures, a single cystic parenchymal lesion found on MRI, and positive serologic test results for _T. solium_ cysts. His only international travel included 1 week in Mexico 17 years before diagnosis. For both of these case-patients, family members reported ongoing travel to and from Mexico. Travel history was not available for the final US-born case-patient, a 57-year-old man with multiple parenchymal calcifications, seizures, and psychosis.\n\n【24】Thirty-two confirmed or presumptive NCC cases were initially diagnosed after July 1, 2008, and were therefore eligible for household investigation. We investigated 22 (69%) cases . Of the 10 cases that were not investigated, 7 patients could not be located with the contact information available in the chart, 2 were identified in other household investigations, and 1 was unable to provide informed consent because of psychosis. No case-patient refused the offer for household investigation. We found no significant difference between the 22 cases we investigated and the 10 we did not with respect to patient demographic or clinical characteristics. We did not identify any significant difference between the 22 investigated cases and the 40 confirmed or presumptive cases diagnosed before July 1, 2008. For investigated case-patients, median time since immigration to the United States was 10 years (IQR 6–14 years) and median time from last international travel to a cysticercosis-endemic country was 5 years (IQR 2–10 years).\n\n【25】A median of 6 (IQR 4–7) persons resided in each household. Of 111 total contacts, 79 (71%) were foreign born, and 41 (37%) reported international travel within the past 2 years. All fecal samples were negative by light microscopy and ELISA for coproantigen. One household contact had serum antibodies against _T. solium_ adult tapeworms in 2 (9%) separate household contact investigations. In 1 household, the seropositive person was the brother of the index NCC case-patient. In the other household, the seropositive person was the husband of the index NCC case-patient. Nine case-patients (41%) and 1 (1%) household contact had circulating antibodies against _T. solium_ cysts.\n\n【26】We offered head CT scans to 11 household contacts, 3 on the basis of positive serologic test results and 8 on clinical history. Of 9 who accepted, 2 had parenchymal calcifications consistent with NCC. One was a 7-year-old child from Myanmar (Burma) who had resettled with his family in Oregon 1 year earlier. He had a 3-year history of recurrent, untreated, generalized seizures that had not been reported to his physician. His mother was the household index case-patient; she sought care initially for severe headache and new-onset seizure; she had positive serologic test results for _T. solium_ cysts and >20 parenchymal cystic lesions. The boy’s father had serum antibodies against _T. solium_ intestinal tapeworm infection with negative results of fecal studies. The other NCC case-patient was an adult man from Mexico City, Mexico, with an occipital parenchymal calcification and chronic headaches; he had immigrated 21 years earlier and denied international travel since immigration. We found no other evidence of _T. solium_ infection in his household, other than the original case-patient.\n\n【27】### Discussion\n\n【28】In Oregon, _T. solium_ causes illness, particularly among the Hispanic population, which maintains ongoing contact with cysticercosis-endemic Latin America through immigration and travel. The mean annual incidence among Hispanics of 5.8 cases per 100,000 population is the highest documented rate within the United States, 4× the estimates from California in the mid-1980s, and 2× the previous estimate for Oregon . Although we documented no deaths directly resulting from cysticercosis, the morbidity and associated use of the health care system are high. Hospitalization at time of diagnosis was common, and intensive care was required in more than one third of hospitalizations. We did not quantify the health resource use related to treatment and follow-up of cases, but we noted surgical complications, shunt failure, and side effects from prolonged steroid use.\n\n【29】The relatively high incidence of cysticercosis in this study probably reflects increased case ascertainment rather than any increase in the underlying risk. Prior studies have relied primarily on hospital discharge data for case finding, which do not capture emergency department visits unless they result in inpatient admission. By requesting quarterly reports based on hospital billing codes, we were able to capture emergency department diagnoses. Many of these appear to have been less clinically severe, including uncomplicated new-onset seizures and headaches from calcified or nonobstructing cysts. In others, we found subsequent inpatient stays for treatment complications or clinical deterioration. Oregon’s comparatively high incidence could alternatively be explained by underlying migration patterns, specifically if preferential migration to Oregon occurred from the highly cysticercosis-endemic central Mexican highlands. However, we have no definitive evidence to either support or refute this hypothesis.\n\n【30】Despite improved case ascertainment, for several reasons the incidence reported in this study most likely underestimates the true incidence of NCC in Oregon. First, we excluded suspected cases from incidence calculations. Although clinical and demographic characteristics of patients with suspected cases were similar to those with confirmed and presumptive cases, a documented epidemiologic link to a cysticercosis-endemic area was not found in all medical charts. For most persons with suspected cases, epidemiologic evidence suggesting exposure to _T. solium_ would have changed the classification to presumptive. Including these suspected cases increased the mean annual incidence to 6.7 cases per 100,000 persons among Hispanics. Second, although several hospital systems reported outpatient visits related to cysticercosis, most outpatient visits in the state were not captured. Because less clinically severe disease can be diagnosed and treated completely in the outpatient setting, we may have missed these cases. Finally, we suspect that underdiagnosis is common, particularly in patients seeking care for headache related to intermittent inflammation around degenerating or calcified parenchymal cysts. The threshold for obtaining neuroimaging in the primary care setting often is high for chronic or intermittent headaches. The prevalence and health-resource use of headache related to NCC have not been characterized in cysticercosis-endemic or -nonendemic areas.\n\n【31】Opportunity to prevent NCC within the United States is primarily limited to identifying and treating domestic carriers of _T. solium_ tapeworms. The numerous reports of NCC among US-born persons who have never traveled implicate domestic exposure to _T. solium_ eggs . We found only 1 such person during our surveillance period, although other population studies have described probable domestic transmission in 7%–10% of NCC cases . Although most infected foreign-born persons were likely to have acquired infection outside the United States, some foreign-born case-patients may have acquired their disease within the United States. We documented frequent travel to and from cysticercosis-endemic areas among NCC case-patients and their household members, which suggests ongoing risk for tapeworm acquisition.\n\n【32】Despite the use of highly sensitive methods for testing serum and feces, we were unable to detect current intestinal tapeworm infection by screening household members of NCC case-patients. However, our case definition for current adult tapeworm infection was conservative. We found 2 contacts with circulating serum antibodies and negative results of fecal analysis, and we chose to interpret this discordance as evidence of past but cleared infection. However, given the high sensitivity and specificity of the rEITB, the unknown duration of antibody persistence, and the fact that we tested just 1 stool sample, an alternate interpretation of this discordance might be active adult tapeworm infection with false-negative results of fecal analysis.\n\n【33】We may not have been able to replicate the prior success in LA County, California, where tapeworm carriers were identified in 7% of investigated households , for other reasons. First, our sample size in this pilot program was small, and chance alone could explain the difference. Second, cases in LA County were based on date of symptom onset rather than date of diagnosis. Because symptom onset can substantially predate diagnosis, the cases we investigated may have been systematically biased toward more remote exposure. We defined cases according to date of diagnosis because exact symptom onset can be difficult to determine, particularly for chronic or intermittent headaches. Third, substantial underlying differences in case-patients and household contacts could exist between Oregon and LA County. With LA County’s proximity to the Mexico border, case-patients and household members may have traveled outside the United States more recently or more frequently. In Oregon, the median time since immigration for case-patients was 4 years longer than the median time since immigration for case-patients in the LA County study. Similarly, the surveillance or investigation results from each study may not be generalizable to other states or other countries in which cysticercosis is not endemic. Finally, _T. solium_ control efforts have been initiated in many areas of Latin America, and the underlying prevalence of tapeworm infection among immigrants from those regions might have decreased in the 20 years since the LA County program was implemented.\n\n【34】The strategy of routine screening for tapeworm carriers among household contacts of a person with symptomatic NCC may be inherently limited because of the long latency between exposure to _T. solium_ eggs and development of symptoms. We did find evidence of past tapeworm infection and possible transmission to other household members. Specific clinical or demographic characteristics of an NCC case-patient might correlate with the presence of a tapeworm in a household member, such as young age, remote exposure in a cysticercosis-endemic area, and viable or multiple lesions. Our sample size was too small to evaluate the effectiveness of limiting investigations based on these variables.\n\n【35】Even though the role of public health in tapeworm screening to prevent domestic transmission remains unclear, the public health system has other functions related to _T. solium_ infection. Primary among these is a focus on the health of household members who are at increased risk for _T. solium_ infection. Improved selection criteria for household investigations may increase the likelihood of detecting current tapeworm infection. Early identification, referral, and surgical treatment of chronic headache caused by hydrocephalus could prevent serious complications. Education of household members also is crucial because they may travel frequently to and from cysticercosis-endemic areas. Recognition of the need to avoid eating undercooked pork and to maintain good hygiene can reduce infection among travelers. Finally, increasing clinician awareness about _T. solium_ infection is a necessary public health function, particularly for clinicians who care for Hispanic and other immigrant populations. Public health intervention should focus on the health of household members and on increasing awareness of the disease among affected families and among clinicians.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d6bc3f6d-959e-45f7-85c1-433b8cd86bb1", "title": "Salmonella-based Rodenticides and Public Health", "text": "【0】Salmonella-based Rodenticides and Public Health\n_Salmonella enterica_ serotypes Typhimurium and Enteritidis have been used as rodenticides since the late nineteenth century. This use was explored after _S._ Typhimurium was discovered during a lethal epizootic in a research mouse colony . Researchers soon realized that the strains of _S._ Typhimurium used as rodenticide were identical to strains causing “meat poisoning” and might cause disease among humans. Use of _S._ Typhimurium rodenticides was discontinued early in the twentieth century, but _S._ Enteritidis continued to be used as a rodenticide in the United Kingdom and Denmark until the early 1960s. In 1954  and again in 1967 , the World Health Organization (WHO) recommended that _Salmonella_ \\-based rodenticides not be used because they posed a hazard to human health.\n\n【1】In spite of these recommendations, _Salmonella_ \\-based rodenticides are still produced and used in Central America, South America, and Asia . Biorat (Labiofam, Cuba), one _Salmonella_ \\-based rodenticide that is currently used in several countries , is made by coating rice grains with a combination of _S._ Enteritidis and warfarin. Currently, the Biorat product label offers no warning regarding the risk for human salmonellosis. Indeed, product information indicates that this product contains a strain of _Salmonella_ that is pathogenic to animals but not to humans .\n\n【2】In 1995, the Centers for Disease Control and Prevention (CDC) received a sample of Biorat that had been distributed in Nicaragua , and in July 2001, U.S. custom authorities seized a shipment of Biorat destined for distribution in the United States. These incidents prompted us to compare Biorat with Ratin, one of the major _Salmonella_ \\-based rodenticides used before the early 1960s; in addition, we summarize the public health hazards of _Salmonella_ \\-based rodenticides.\n\n【3】### Microbiologic Findings\n\n【4】We compared three isolates of _S_ . Enteritidis recovered from _Salmonella_ \\-based rodenticides. Two isolates (of the Biorat strain) were from Biorat samples collected in 1995 and 2001. The label from the Biorat product obtained in 2001 states that Biorat contains 1.25% “monopathogenic” _Salmonella_ and 0.02% hydroxycoumarin. Pooled samples of the 1995 Biorat product yielded 1x10 8  CFU _S_ . Enteritidis per gram of Biorat granules, and the 2001 product yielded 200,000 CFU per gram. A third isolate, _S._ Enteritidis var. Danysz (of the Ratin strain) was recovered from Ratin by the Danish Veterinary Laboratory in the 1920s or early 1930s.\n\n【5】Isolates were serotyped and biochemically characterized , subtyped by pulsed-field gel electrophoresis (PFGE) by using the restriction enzymes _Xba_ I and _Bln_ I , and phage-typed. Phage-typing was performed at the National Laboratory for Enteric Pathogens, National Microbiology Laboratory, Canadian Science Centre for Human and Animal Health.\n\n【6】Both the Biorat strains and the Ratin strain were identified as _S_ . Enteritidis, phage type (PT) 6a. The two Biorat strains were indistinguishable from each other by PFGE with the restriction enzymes _Xba_ I and _Bln_ I. PFGE patterns of the Ratin strain differed from those of the Biorat strain by three bands with _Xba_ I and by five bands with _Bln_ I .\n\n【7】Neither the Biorat strains nor the Ratin strain was shown to decarboxylate lysine. By contrast, four _S_ . Enteritidis PT 6a isolates in the CDC culture collection from sources unrelated to Biorat or Ratin were positive for lysine decarboxylase. Threlfall et al. reported that the strain of _S_ . Enteritidis PT 6a from Biorat is indistinguishable from those of the Ratin and Liverpool rodenticide strains by plasmid profile typing , as both strains contained plasmids of approximately 59, 4.0, and 3.0 MDa.\n\n【8】### Public Health Hazard\n\n【9】Since the mid 1980s, _S_ . Enteritidis has caused a global pandemic of foodborne illness associated with eggs and poultry as a result of infection of the internal organs of chicken . Because of this pandemic, _S_ . Enteritidis has become the most common serotype of _Salmonella_ isolated from humans worldwide .\n\n【10】The _S_ . Enteritidis strain found in Biorat is similar to the strain found in Ratin, a discontinued European product that caused human illness. Both strains were the same phage type, were indistinguishable by plasmid profile typing, and were different from 97% of salmonellae  in that they did not decarboxylate lysine. Although differences were noted between PFGE patterns of the Biorat strain and the Ratin strain, the similarities suggest that they may have originated from a common strain. Because the strains are similar and no evidence shows that the Biorat strain has decreased virulence, the Biorat strain is likely as pathogenic to humans as the Ratin strain.\n\n【11】In a retrospective study in Denmark from 1926 through 1956, Martin Kristensen identified 122 patients infected with the Ratin strain , including 5 (4%) who died, 3 of whom were children. Twenty-two (18%) of 122 patients were reported to have eaten food items contaminated with Ratin, while 43 (35%) had handled the rodenticide. In 1956, Taylor  also reported several outbreaks of food poisoning associated with _S._ Enteritidis rodenticides and concluded that the use of bacterial rodenticides should be stopped.\n\n【12】Rodenticides containing salmonellae were evaluated during a plague outbreak in San Francisco in 1895 ; they were found to have no definable impact on the rodent population, but they caused illness and death in humans who prepared and handled them. In 1921, Willfuhr and Wendtland  reported several outbreaks of human _Salmonella_ infections from rodenticides. In one of these outbreaks, Russian prisoners of war who ate a large number of Ratin potato baits became ill, and two died. In another outbreak in 1918, two persons died and approximately 35 became ill after eating a cake that had been intentionally contaminated with Ratin . From 1920 through 1940, other outbreaks associated with _Salmonella_ \\-based rodenticides were reported, and several of these outbreaks included deaths .\n\n【13】The Biorat product insert, as well as information available on the Internet , claims that the product is not harmful to humans and does not contaminate the environment. Recent newspaper articles have generated interest in using _Salmonella_ \\-based rodenticides as an alternative to chemical rodenticides. Advocates for the use of Biorat claim, “\\[Biorat\\] has absolutely no secondary effects on other animals, on the environment, or on humans…. It contains a strain of _Salmonella_ that only affects rats” .\n\n【14】_S._ Enteritidis causes severe diarrheal illness, which can be life-threatening, especially among children, the elderly, and immunocompromised persons. We have not identified any peer-reviewed, scientific data on the safety of _Salmonella_ \\-based rodenticides, and to our knowledge, all strains of _S._ Enteritidis are capable of causing human illness. Noting the hazards of _Salmonella_ \\-based rodenticides, many countries have banned their use, and WHO has repeatedly recommended against use of salmonellae in rodenticides.\n\n【15】Current concerns about bioterrorism suggest an additional public health threat posed by a commercially available strain of _S._ Enteritidis. _Salmonella_ \\-based rodenticides have already been used intentionally to cause human illness ; however, human illness may more commonly be caused by inadvertent exposure to _Salmonella_ \\-based rodenticides. These rodenticides are generally mixed with grains to form baits . Biorat, for example, is made with whole rice and can easily be mistaken for food. Ingesting a few grams of bait, with at least 200,000 CFU per gram, could easily cause a severe case of salmonellosis.\n\n【16】To determine why _Salmonella_ \\-based rodenticides are still used despite information about the public health hazards, we conducted a literature search with the keywords “ _Salmonella_ ” and “rodenticide” . We found 10 articles, in addition to the recent public health publications discussed above ; none addressed the public health hazards of _Salmonella_ \\-based rodenticides. Many of the reference materials we used to prepare the present article were not written in English or were not retrievable from current electronic databases. The continued use of _Salmonella_ \\-based rodenticides may likely be related to the fact that the content of important but dated scientific papers is unlikely to be known to current decision-makers.\n\n【17】_Salmonella_ \\-based rodenticides may contain an approved rodenticide, such as warfarin, in concentrations high enough to kill rats, and the addition of _S._ Enteritidis has not been shown to increase the effectiveness of the poison . Extensive use of _Salmonella_ \\-based rodenticides in the past may have increased the prevalence of _Salmonella_ in rodents  and consequently increased the potential for human salmonellosis by transmission from rodents to food or food animals. Unfortunately, a misperception exists that some strains of _S_ . Enteritidis are not pathogenic to humans. We recommend informing rodent-control authorities and the public that _S._ Enteritidis is a known human pathogen and that use of _Salmonella_ \\-based rodenticides has had severe public health consequences. Effective and safe alternatives to _Salmonella_ \\-based rodenticides are available worldwide.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "de02e4eb-6ed7-4318-850e-559745d47052", "title": "Pathogenic Lineage of mcr-Negative Colistin-Resistant Escherichia coli, Japan, 2008–2015", "text": "【0】Pathogenic Lineage of mcr-Negative Colistin-Resistant Escherichia coli, Japan, 2008–2015\n**To the Editor:** Colistin is a last-line drug for treatment of multidrug-resistant, gram-negative bacterial infections, including those caused by _Escherichia coli_ . We report colistin-resistant _E. coli_ isolates from Japan, including a global-spreading pathogenic lineage, serotype O25b:H4, sequence type (ST) 131, and subclone _H_ 30-R (O25b:H4-ST131- _H_ 30R).\n\n【1】We tested 514 _E. coli_ isolates obtained from clinical specimens taken at Sapporo Clinical Laboratory Inc. (Sapporo, Japan) and Sapporo Medical University Hospital in Japan during 2008–2009  and 2015, respectively. Samples were processed according to Clinical and Laboratory Standards Institute guidelines . Identification of O25b:H4-ST131, O25b, H4, and ST131 were determined as described previously . For identification of the _H_ 30Rx subclone of O25b:H4-ST131, _H_ 30 was determined by PCR using a specific primer set , R was determined according to ciprofloxacin MIC, and x was determined by detecting 2 single-nucleotide polymorphisms, as previously described .\n\n【2】Four _E. coli_ isolates exceeded the colistin resistance breakpoint (>2 mg/mL) . None of the patients from whom the _E. coli_ isolates were derived had a history of colistin treatment. Three of the 4 colistin-resistant isolates belonged to a pandemic lineage, O25b:H4-ST131- _H_ 30R, which has been isolated from urinary tract and bloodstream infections . The frequency of colistin-resistant ST131 _E. coli_ isolates among O25b:H4-ST131 was 2.2%. This lineage is fluoroquinolone resistant and is frequently resistant to β-lactams because it possesses CTX-M–type extended-spectrum β-lactamase genes .\n\n【3】The colistin-resistant isolates reported were resistant to fluoroquinolones, and 1 (SME296) was resistant to cephalosporins (due to expression of _bla_ CTX-M-14  ). Another colistin-resistant _E. coli_ isolate (SME222) belonged to O18-ST416, which is also known as an extraintestinal pathogenic _E. coli_ , although this lineage has not previously been reported to exhibit colistin resistance.\n\n【4】The colistin-resistant _E. coli_ isolates we identified were sensitive to carbapenems and aminoglycosides, including amikacin, whereas previously it was reported that some _E. coli_ ST131 isolates exhibited resistance to carbapenems by possessing carbapenemases, such as NDM-1 and KPC-2; the NDM-1–possessing ST131 isolate also exhibited resistance to amikacin . Thus, these findings may affect future antimicrobial choices because of the clonal dominance, multidrug resistance, and pathogenicity of the isolates.\n\n【5】Recent studies reported a plasmid-mediated colistin resistance gene, _mcr-1_ , in various countries . In addition, a novel plasmid-mediated colistin resistance gene, _mcr-2_ (76.7% nucleotide identity to _mcr-1_ ), was found in _E. coli_ isolates in Belgium . These genes encode a phosphoethanolamine transferase family protein, which modifies the lipid A component of lipopolysaccharide . The colistin-resistant _E. coli_ isolates we identified did not possess _mcr-1_ or _mcr-2_ , although the MICs for colistin were the same as or higher than that of the transconjugant of a _mcr-1_ –harboring plasmid in an _E. coli_ ST131 isolate (4 mg/L) reported by Liu et al. Thus, these colistin-resistant isolates may have other colistin resistance mechanisms. For example, modification of lipid A with 4-amino-4-deoxy- l \\-arabinose or phosphoethanolamine, caused by chromosomal mutations in _mgrB_ , _phoPQ_ , and _pmrAB_ genes, might occur and could be responsible for the resistance. This polymyxin-resistance mechanism is seen in _Enterobacteriaceae_ ; however, other novel mechanisms are also conceivable.\n\n【6】In conclusion, we report colistin resistance in a major global-spreading extraintestinal pathogenic _E. coli_ strain, O25b:H4-ST131- _H_ 30R, in Japan. This strain acquired colistin resistance without carrying a plasmid bearing the _mcr_ gene. Clarifying the colistin-resistance mechanisms in these isolates is necessary if we are to forestall the emergence of multidrug (including colistin)–resistant O25b:H4-ST131- _H_ 30R. The worst-case scenario is the global spread of this isolate, which has acquired resistance to the last-line antimicrobial drug, colistin.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2c546e47-a0cc-4ef9-a6f9-8e68519d02d3", "title": "The Darkest Place Is under the Light House", "text": "【0】The Darkest Place Is under the Light House\nUtagawa Hiroshige . Plum Garden at Kameido . From the series One Hundred Views of Edo. Color woodblock print (37.7 cm × 26.5 cm). Honolulu Academy of Arts, Hawaii, USA, Gift of James A. Michener, 1991 \n\n【1】“Hiroshige’s death cannot be too much deplored,” read the note next to his name on a list of the famous who perished in the 1858 cholera epidemic . Utagawa Hiroshige himself, before his death at age 62, is said to have written in verse, “I leave my brush at Azuma and go on the journey to the Holy West to view the famous scenery there” . Metaphorical meaning and questionable authenticity aside, when interpreted in Western terms, the farewell becomes prophetic since the artist, long overlooked in his native Japan, was discovered and brought to light in the West.\n\n【2】The low cost of production made woodblock prints, Hiroshige’s main medium, widely accessible—a print sold for the price of a bowl of noodles . The subjects, eloquent snapshots of the provinces, remained unknown to the cultured class in Japan, until his popularity rose in Europe and the United States. How the prints first found their way to Western artists, who appreciated and imitated them is legend. James McNeill Whistler , who was very influential in introducing Japanese art to England, might have seen his first Hiroshige at a rundown Chinese tea house near London Bridge or on the wrapper on a pound of tea .\n\n【3】Van Gogh so admired Japanese prints that he copied some, among them Plum Garden at Kameido, on this issue’s cover. “I envy the Japanese artists,” he wrote, “for the incredible neat clarity which all their works have” . Hiroshige’s influence was greatest on the impressionists (Claude Monet, Pierre August Renoir, Mary Cassatt) and postimpressionists (Paul Cézanne, Henri de Toulouse-Lautrec, Paul Gauguin). His landscapes, “simple as breathing,” “easy as buttoning one’s waist-coat,” influenced Western painting away from representation toward light, color, and emotion .\n\n【4】Hiroshige’s biography is pieced together from anecdotes as there are few authentic records of his life. He was born Andō Tokutarō in Edo, present-day Tokyo, a precocious child with an eye for the unusual and the detailed. Orphaned in his early teens, he managed to receive art instruction, first from amateur painter Okajima Rinsai, a friend and neighbor, and at age 15, from the art establishment. In the short course of a year, he was admitted to the Utagawa School as designer of color prints. According to Japanese custom, an accomplished apprentice was given a name that generally incorporated part of the master’s name. Apprentice of Toyo **hiro** , Andō was named **Hiro** shige. The diploma, in Toyohiro’s own writing, read Utagawa Hiroshige. The artist later also used the names Ichiryūsai and Ryusai.\n\n【5】Hiroshige’s father, Gen’emon Andō, a hereditary retainer of the shōgun, was a fireman, and when he died, Hiroshige kept his modest post, eking out a living until he could relinquish the post to another member of the family and devote himself to art. Then he set off to see and draw the provinces. A wanderer and bon vivant, he lingered on the road, pausing to observe and sketch, mixing with country folk, dining at local eateries. His discriminating tastes and humorous accounts of people and places, recorded in his diaries, were largely lost in the 1896 earthquake, but his absorption with natural beauty, which he held “exquisite, beyond capability of describing with brush” , survived in his landscapes.\n\n【6】Legend has it that seeing the work of master printmaker Katsushika Hokusai  inspired Hiroshige to become a _ukiyo_ \\- _e_ artist, to create images of the “floating world.” These images, drawn from the transient world of actors and others in Edo’s theater district, expanded to encompass scenes of nature and eventually the life of the common people: “Living only for the moment, turning our full attention to the pleasures of the moon, the snow, the cherry blossoms, and the maple leaves; singing songs, drinking wine, diverting ourselves in just floating, floating…like a gourd floating along with the river current; this is the floating world” .\n\n【7】Hiroshige formed his own interpretation of the floating world, which he summed up in the inscription on The Hundred Views of Mount Fuji: “…the old man \\[Hokusai\\] had drawn grasses, trees, birds, animals, and other things in his usual talented brush….his work focused upon making things interesting….I simply reproduce sketches of what I had seen before my eyes” . But true to life as Hiroshige was, he captured the pathos, not the details, favoring white spaces, flattened forms, organic scenes in brilliant color. Like van Gogh, he tried to draw not “a _hand_ , but the gesture, not a mathematically correct head, but the general expression….In short, _life_ ” .\n\n【8】Hiroshige was very prolific. He created thousands of images of his beloved Edo and surrounding provinces—bridges, roads, temples—under all manner of weather, day and night. He named them all personally, as he also had a talent for verse, which he dispersed generously and with wit. His prints were copied and reprinted freely.\n\n【9】Plum Garden at Kameido, in a series created just a year before his death, shows the master’s unparalleled facility with Japanese topography. This close-up of a plum tree, thick trunk framing the famed gardens, young sprouts shooting out the edge, blossoms placed seductively against a sky flushed pink, is an essay in perspective. The tree trunk, a main attraction, is what we are less likely to see, its spare immediacy too close to the lens. Hiroshige wants us to venture past it into the spring extravaganza beyond.\n\n【10】Perspective, a Western influence, graces many of Hiroshige’s later works, which having excelled in capturing life as he saw it, now explored its depth. Master of illusion, he brought what he saw into focus, knowing full well that the scene was but a composition of life elements, not life itself.\n\n【11】Hiroshige’s dilemma with perspective is not unlike the scientist’s, who also draws selected objects closer for a better look. But magnification and clarity are no guarantee of true perspective in the laboratory any more than in art. Out in the open, under the proverbial lighthouse, lies always the risk of missing the obvious in close and plain view. And despite the science frame, zoonotic and vectorborne interactions and connections within the natural environment, like the strolling visitors in the garden at Kameido, can easily be overlooked.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d8c4ee3b-44ae-40b2-85fd-3c4b7fed046a", "title": "Trachoma Decline and Widespread Use of Antimicrobial Drugs", "text": "【0】Trachoma Decline and Widespread Use of Antimicrobial Drugs\nTrachoma is disappearing in many parts of the world, even in the absence of specific control programs. It is a disease of the rural poor, and as living conditions have improved during the last century, a corresponding decline in trachoma has occurred . In Western Europe and the United States, trachoma virtually disappeared by the late 20th century. Other infectious diseases such as syphilis, chancroid, tuberculosis, and leprosy also began to subside in Europe and the United States during this time. This downward trend seems to have begun before, and continued into, the antimicrobial drug age. Therefore, many attribute this decline to socioeconomic factors, such as improved sanitation and social changes, and even to legislation to control venereal disease, rather than to antimicrobial drugs. Addressing the importance of antimicrobial agents in the disappearance of these infectious diseases retrospectively is difficult. In the case of trachoma, we have a unique opportunity to observe the effect of rising antimicrobial pressure in the community on a disease that is in decline but has not yet disappeared.\n\n【1】From 1998 to 2001, a region of western Nepal was monitored for trachoma prevalence, following mass antimicrobial drug distribution for trachoma. A dramatic fall in disease prevalence was observed that could not be attributed to the effect of the trachoma control program alone . We conducted a survey of pharmacies in the same region and found a surprisingly large quantity of antimicrobial drugs were being used for indications other than trachoma control . Here, we evaluate whether this background antimicrobial use may be responsible for the downward secular trend in the prevalence of trachoma.\n\n【2】### Analysis of Decline in Trachoma Prevalence in Western Nepal\n\n【3】From May 1998 to May 2001, a total of 25 villages from three subdistricts (known as Village Development Committees) in the Kailali and Konchapur districts of far-western Nepal were monitored for clinically active trachoma. During this time, an annual mass azithromycin treatment program began. At each visit, all children 1–10 years of age were examined for signs of clinically active trachoma by using the World Health Organization  simplified trachoma grading system . In total, >20,000 examinations were performed; 180–650 children were examined during each village visit . The presence of a secular trend, a downward trend independent of the trachoma program was evaluated by monitoring one third of the villages for 6 months before any antimicrobial drug treatment was given. Seasonal variation was determined by performing village visits in both the spring and the fall. No other specific trachoma prevention activities such as hygiene, fly control, or water supply programs were instituted during the course of this study .\n\n【4】Trachoma prevalence data were analyzed by using a multivariate autoregression (AR1) model with the following covariates: effect of the trachoma program, seasonal variation, and secular trend. The analysis showed that the trachoma program’s distributions of antimicrobial drugs alone could account for some, but not all, of the observed reduction in clinically active trachoma . A substantial proportion of the decrease in trachoma prevalence 6 months posttreatment was attributable to a secular trend, independent of the trachoma program’s effect and seasonal changes (26% decrease, p < 0.001, 95% confidence interval \\[CI\\] 15%–35% decrease).\n\n【5】### Antimicrobial Pressure from Outside the Trachoma Program\n\n【6】From February to May 2000, all pharmacies and government health posts in the Geta subdistrict of Kailali were surveyed to establish the total quantity of antimicrobial drugs distributed. All of these will be called pharmacies for the purposes of this article. Information obtained included the number of years each medicine hall had been open and, for each patient, age, antimicrobial agent, amount distributed, and patient’s village. Pharmacy purchase receipts from this time period were also collected for analysis. The survey was repeated in September 2001 to gain additional patient information and to ensure that no gross seasonal variations occurred .\n\n【7】We analyzed these data to determine what percentage of the total antimicrobial agents distributed had antichlamydial activity. Susceptibility testing suggested that trimethoprim-sulfonamide combinations, tetracycline, macrolides, chloramphenicol, and amoxicillin are all effective against chlamydia. Also, other penicillins, cephalosporins, and the fluoroquinolones (ciprofloxacin and norfloxacin) are less effective antichlamydial agents . However, susceptibility testing for chlamydia has been difficult to standardize , and alternative assumptions could alter these percentages somewhat. For example, including ciprofloxacin, which has some effect against _Chlamydia trachomatis_ , would have increased the proportion effective against chlamydia by 12%, but we used the lower, more conservative figure for analysis. To facilitate direct comparison of different antimicrobial agents, the total amount of antichlamydial antimicrobial drugs was converted into the standardized unit of defined daily doses (DDD). DDD is defined as the assumed average maintenance dose per day for a drug used for its main indication in adults . For children, the number of prescriptions given per child per year was calculated with 1998 census data. Both DDDs and the prescriptions per person-year are convenient measures to compare antimicrobial pressure, although neither is ideal; DDDs do not take into account the duration of each drug’s antichlamydial activity, and prescriptions are not for a uniform amount of medication.\n\n【8】We estimated that pharmacies in Geta distributed 3.0 DDD of antimicrobial drugs per person per year in 2000 . Sixty-eight percent of these prescriptions were effective against chlamydia . Thus, pharmacies distributed 2.0 DDD per person per year of antichlamydial agents. Forty-nine percent of all antimicrobial agents were distributed to children 0–10 years of age, and 33% to preschool children 0–5 years of age. We estimated that on average 1.2 prescriptions of antichlamydial agent are given to each preschool child per year .\n\n【9】The number of pharmacies in Geta subdistrict has increased from 2 to 14 within the last 20 years, coinciding with the decrease in trachoma prevalence in the Tarai region of Nepal  . Eight of these pharmacies (57%) have been open for < 5 years, and 10 (71%) for < 10 years. In the last 20 years, the number of pharmacies has increased sevenfold, while the population of Geta has grown by approximately twofold, which suggests that more than three times as many pharmacies exist per person currently than in 1980.\n\n【10】### Antimicrobial Drug Use within the Trachoma Program\n\n【11】The trachoma control program in Kailali and Konchapur distributed single-dose oral azithromycin annually, as per World Health Organization (WHO) guidelines and covered an estimated 80% of the targeted population with its antimicrobial treatments . One gram of azithromycin is the recommended single dose in an adult to treat ocular chlamydial infection. This dose is equivalent to 3.3 DDD/person . For children, the recommended single dose of azithromycin is 20 mg/kg. The average dose for all ages (adults and children) was found to be approximately 2.3 DDD/person . With a treatment coverage of 80% of the entire population as recommended by WHO, a trachoma program would therefore administer 1.8 DDD/person at each mass distribution of antimicrobial agents.\n\n【12】### Antimicrobial Drug Use Necessary for Elimination of Trachoma in Western Nepal\n\n【13】Using a previously described mathematical model, we estimated the frequency of mass azithromycin distributions, and the amount of antimicrobial drug needed, to eliminate infection from this region of western Nepal . Before treatment, the average prevalence of active trachoma was 17% in children 1–10 years of age in western Nepal . With antimicrobial drug treatment that is 95% effective in a person and with 80% coverage of the population, the model indicates that mass treatments would be needed every 1.7 years (20.4 months) in western Nepal to progressively reduce the prevalence of active trachoma. Therefore, mass treatments given annually would be more than enough to eliminate ocular chlamydial infection.\n\n【14】### Discussion\n\n【15】The amount of antichlamydial drugs given out by pharmacies in Geta (2.0 DDD/person/year) is slightly more than the estimated amount that would bring about the elimination of ocular chlamydial infection in this region of western Nepal (1.9 DDD/person/year). Children, in particular preschool children, are by far the most likely to harbor ocular chlamydia. Pharmacies distributed nearly one half of the total antimicrobial agents to children 0–10 years of age, and one third to children 0–5 years of age. Preschool children received 1.2 prescriptions per year of antimicrobial drugs that are effective against chlamydia, which is far more than the estimated 0.6 per year that would eliminate infection. We therefore conclude that antibiotics given for reasons other than trachoma control may play a role in the disappearance of trachoma in this region.\n\n【16】The prevalence of active trachoma has decreased in many regions of the world in the absence of programs specifically targeting this disease . From 1981 to 1996, active trachoma in children declined from 30% to <10% in each of two adjacent districts of western Nepal; one district had an intense trachoma control program; the other district did not . Surveys in the Kailali and Konchapur districts of western Nepal have shown a large secular trend, suggesting that active trachoma would have disappeared rapidly even if a trachoma program had not been implemented . This situation is not unique to Nepal. A village in Gambia had hyperendemic trachoma in 1959 (66% prevalence in children), yet a followup survey in 1987 found that active disease had nearly disappeared, after only a modest 2-year control program of tetracycline administration . A study in Malawi showed a 50% reduction in active trachoma over a 16-year period in the absence of a specific trachoma program .\n\n【17】What might be the cause of this secular trend seen in so many countries? Various socioeconomic factors have been associated with the disappearance of trachoma, but studies have had difficulty establishing causality for any of them . In particular, facial hygiene and fly density are both believed to be related to trachoma activity . Several studies have associated dirty faces with active trachoma , but a trial involving intensive face-washing produced a modest (and statistically insignificant) decrease in clinically active trachoma at 1 year . The face fly ( _Musca sorbens_ ) has been implicated as a vector of trachoma . A recent study in the Gambia found that regular insecticide spraying in villages did reduce active trachoma ; however, future controlled studies are necessary to determine the sustainability of this promising measure.\n\n【18】What role have antimicrobial agents played in the disappearance of trachoma? In a person, ocular chlamydial infection can be successfully treated with a single dose of azithromycin . At the community level, controlled trials in Tanzania, Gambia, and Egypt have shown that a single course of azithromycin can markedly reduce ocular chlamydial infection, even 1 year later . Our findings in this study support the hypothesis that the rising use of antimicrobial drugs in the community for indications other than trachoma may contribute to the disappearance of this disease.\n\n【19】Several of the principal antimicrobial drugs used in Nepal for systemic infectious diseases have antichlamydial action. National treatment guidelines for childhood pneumonia recommend co-trimoxazole (a combined preparation of sulfonamide and trimethoprim) as the treatment of choice, followed by amoxicillin or oral chloramphenicol as second-line therapy . Other childhood infectious diseases are treated according to the adapted WHO Integrated Management of Childhood Illness . WHO recommends chloroquine as the first-line therapy for malaria in Nepal, and sulfadoxine-pyrimethamine for chloroquine-resistant cases . The latter drug has antichlamydial activity through its sulfonamide component, sulfadoxine.\n\n【20】Why trachoma is disappearing should be investigated before it is gone, so that this knowledge can be applied to other diseases. If infection in a region is already in decline, the effect attributed to a trachoma control program may be exaggerated, and the program’s success may not be duplicated in less fortunate areas. Conversely, beneficial factors could be introduced in areas where a downward secular trend does not already exist. Much discussion has taken place about the dangers associated with the indiscriminate use of antimicrobial drugs. These problems should be balanced against the benefits. The widespread use of antimicrobial drugs in developing countries for indications other than trachoma may play a role in eradicating one of the world’s leading causes of preventable blindness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "251b4f51-f792-4a58-9651-fad8a8d2a897", "title": "Death of Woman with Peripartum Influenza B Virus Infection and Necrotizing Pneumonia", "text": "【0】Death of Woman with Peripartum Influenza B Virus Infection and Necrotizing Pneumonia\n**To the Editor:** Pregnant women are at increased risk for severe influenza-related complications . Bacterial pneumonia with Panton-Valentine leukocidin-producing (PVL) _Staphylococcus aureus_ is infrequently described in the literature as occurring concurrently with influenza B virus infection . Additionally, only 2 occurrences of peripartum PVL-methicillin-resistant _S. aureus_ (MRSA) pneumonia have been described . We report a case of influenza B virus and PVL-MRSA co-infection during pregnancy.\n\n【1】In December 2012, a previously healthy pregnant woman, 38 years of age, at 37 weeks’ gestation and in active labor, sought treatment in a New York hospital reporting 2 days of fever, productive cough, shortness of breath, and pleuritic chest pain. Household contacts included children with influenza-like illness. The patient had declined influenza vaccination while receiving prenatal care. On arrival, examination showed that her temporal temperature was 101.6°F, blood pressure was 122/71 mm Hg, pulse was 121 beats per minute, respiratory rate was 40 breaths per minute, and oxygen saturation was 89% on room air; bilateral inspiratory crackles were heard on lung auscultation. Rapid influenza screening of a nasopharyngeal swab sample by using ELISA was negative for influenza A and B viruses. Culture of the patient’s nares was positive for MRSA colonization. Laboratory evaluation showed leukopenia of 1500/μL, and although imaging was limited by the patient’s lead apron, a chest radiograph demonstrated bibasilar opacities .\n\n【2】The differential diagnosis for this patient included influenza pneumonia, community-acquired pneumonia, and MRSA pneumonia; treatment with oseltamivir, ceftriaxone, vancomycin, and azithromycin was started. Because of impending respiratory failure, she was admitted to the Medical Intensive Care Unit where mechanical ventilation was initiated and she underwent a spontaneous vaginal delivery of a live male infant. The patient’s condition deteriorated and progressed to severe acute respiratory distress syndrome with multiple organ failure and required substantial inotropic support. Subsequent laboratory studies showed the following results: leukocyte count 400/μL, lactate 4.2 mmol/L, pH 7.16, PaCO 2  36 mm Hg, PaO 2  68 mm Hg, HCO 3  12 mmol/L, and oxygen saturation of 87% at 1.0 FiO 2  . Repeat imaging demonstrated diffuse infiltrates in all lung fields . Because the patient responded poorly to treatment, vancomycin was discontinued and linezolid was started. Despite lung recruitment maneuvers and inhalation of nitric oxide, the patient remained hypoxemic. Extracorporeal membrane oxygenation was initiated and the patient was transferred to another institution.\n\n【3】After transfer, culture of 1 peripheral blood sample obtained at admission identified MRSA, and viral culture of the patient’s nasal swab sample isolated influenza B virus. Genetic testing of the MRSA isolate identified a PVL-producing USA300 _spa_ 1 clone carrying staphylococcal cassette chromosome _mec_ type IV. The patient died 2 weeks later from overwhelming sepsis. The neonatal course was notable for a birth weight of the infant of 2,825 g and Apgar scores of 5 and 8 at 1 and 5 minutes, respectively. He was intubated and transferred to the Neonatal Intensive Care Unit with an arterial cord blood pH of 6.78 and base deficit of 16 mmol/L. Nasal swab culture isolated methicillin-sensitive _S. aureus_ . Viral culture of endotracheal aspirate was negative for influenza A and B viruses. Blood cultures were sterile. He received vancomycin for 1 week and was discharged home to the family on day 8 of life.\n\n【4】This case emphasizes the potential lethality of respiratory complications related to seasonal influenza. Colonization of the patient’s nares with MRSA, possibly PVL-producing, may have predisposed her to a bacterial co-infection, consequentially increasing her risk for death from influenza . _S. aureus_ clones USA300 and USA400 are emerging causes of community-acquired pneumonia in healthy adults and are leading to a rise in co-infections with influenza and MRSA. These 2 infections have been shown to act synergistically in animal models to induce a rapidly progressive necrotizing pneumonia associated with severe leukopenia . This is unlike classic secondary bacterial pneumonia, which typically occurs in a biphasic course with influenza .\n\n【5】Although methicillin susceptibility does not influence the mortality rate of PVL- _S. aureus_ pneumonia , antibiotic drugs should be administered early and selection should reflect local resistance patterns. When making the diagnosis, physicians should recognize that the sensitivity of rapid influenza diagnostic tests is low and should not be relied on when a high level of clinical suspicion exists . Despite trivalent vaccine correspondence with circulating influenza B virus in 5 of 10 influenza seasons during 2001–2011 , vaccination against seasonal influenza is still the most effective way to prevent this potentially fatal condition. Availability of a quadrivalent influenza vaccination, introduced for the 2013–14 influenza season, should improve future incidence of influenza B virus infection. Because PVL-MRSA colonization is becoming more prevalent , necrotizing pneumonia must be considered in critically ill patients during influenza season.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e527baa1-a3c3-4c44-9b7c-0c63891450a5", "title": "Control and Prevention of Anthrax, Texas, USA, 2019", "text": "【0】Control and Prevention of Anthrax, Texas, USA, 2019\nThe zoonotic disease anthrax, caused by the bacterium _Bacillus anthracis_ , has been known to humankind for thousands of years and is endemic to most continents . It is a naturally occurring disease of herbivores that incidentally infects humans through contact with animals that are ill or have died from anthrax or through contact with _B. anthracis_ –contaminated byproducts such as meat, hides, hair, and wool . Transmission routes include cutaneous, ingestion, inhalation, and injection; cutaneous accounts for most (95%) cases worldwide . In the United States, human risk is primarily associated with handling carcasses of hoofstock that have died of anthrax; the primary risk for herbivores is ingestion of _B. anthracis_ spores that can persist in suitable alkaline soils in a corridor from Texas through Colorado, the Dakotas, and Montana .\n\n【1】The 2 state agencies responsible for anthrax surveillance in Texas are the Texas Department of State Health Services (DSHS) and the Texas Animal Health Commission (TAHC). Samples that are culture-positive for _B. anthracis_ at veterinary reference laboratories are reported to DSHS and TAHC. Veterinarians treating animals with illnesses compatible with anthrax must also report to DSHS and TAHC. Suspected cases of human anthrax are immediately reportable to DSHS. Samples or isolates from human cases are forwarded for identification to local public health reference laboratories. In Texas, animal anthrax cases are most commonly reported from the triangular area bounded by the towns of Uvalde, Ozona, and Eagle Pass , which includes portions of Crockett, Val Verde, Sutton, Edwards, Kinney, Uvalde, Zavala, and Maverick Counties in southwestern Texas.\n\n【2】During 2000–2018, a total of 63 animal anthrax cases were confirmed by culture of _B. anthracis_ in a reference laboratory (annual mean 3.3, range 0–20 cases/year) . Because only 1 animal per affected premise usually is reported in a given year, the number of cases is a substantial underrepresention of the total number of affected animals and properties. The last naturally occurring human case of cutaneous anthrax associated with livestock exposure in Texas was reported in 2001 .\n\n【3】### Texas Outbreak\n\n【4】##### Animal Cases\n\n【5】Texas Veterinary Medical Diagnostic Laboratory confirmed the first anthrax case of 2019 in an exotic antelope carcass from Uvalde County on June 19. Overall in 2019, the laboratory reported 25 culture-positive animals, including cattle, horses, white-tailed deer, antelope, and a goat, from Crockett, Kinney, Sutton, Uvalde, and Val Verde counties. The last confirmed animal case was reported on August 21. Unconfirmed numbers reported to DSHS staff suggest that >1,000 animal losses might be attributed to the 2019 outbreak .\n\n【6】Implementing control measures (i.e. vaccination and proper carcass disposal) was challenging; thin topsoil over bedrock, vast and inaccessible terrain, and burn bans triggered by hot, dry weather conditions made it difficult for livestock owners and landowners to identify and bury or burn dead animals. Livestock owners can sometimes cover dead animals with tarps if burial or burning is not an option. However, because properties in this area of Texas can be thousands of acres and not particularly navigable, reaching dead animals to cover and protect them from scavengers (that might further distribute _B. anthracis_ –contaminated remains) is often not feasible.\n\n【7】Another obstacle to controlling the outbreak was the inability to address the contribution of wildlife to the initiation and perpetuation of disease spread (e.g. lack of a licensed vaccine and impracticality of using physical or chemical restraint to administer vaccine “off label” to wildlife species). In addition, reports of vaccine-associated adverse events among goats and horses  made some owners reluctant to vaccinate these species. Among confirmed animal anthrax cases in species for which vaccination is indicated (cattle, goats, horses, sheep, and swine) , a third are reported to have been vaccinated before illness. Of those, the median number of days from most recent vaccination to specimen collection was 8 days (range 3–82 days) . The frequency and effect of antibiotic use subsequent or simultaneous to vaccination was unknown.\n\n【8】##### Human Case Report\n\n【9】On July 23, 2019, a non-Hispanic White man in his 70s from the anthrax-affected area who had a history of cardiovascular disease and hypertension visited his physician for evaluation of 2 lesions near his right knee. Four days earlier, a small red spot had emerged and gradually enlarged and became painful. He reported no fever and used no over-the-counter medications. When asked about animal exposures because of where he lived, he reported that he and his daughter had moved 2 fly-covered deer carcasses from beneath his porch before lesion onset. He was wearing shorts and a shirt while moving the carcasses, and his affected leg was scraped by the velvet-covered antlers. He also reported being bitten by a fly. The deer carcasses were not tested for anthrax, and the patient disposed of them.\n\n【10】On examination at his physician’s office, the patient’s vital signs were as follows: blood pressure 177/87 mm Hg; heart rate 76 beats/min; and temperature 98.3°F. Below and lateral to his right knee was an indurated, raised, erythematous 5-cm lesion with small ulcerations that oozed serosanguinous fluid and was surrounded by a blanched halo. Just proximal to his right knee was a nonindurated erythematous macule . No popliteal or inguinal adenopathy was present. After 2 swab specimens were obtained from the larger lesion, the patient was given a cephalosporin intramuscularly, and a prescription for ciprofloxacin was called in to his pharmacy of choice more than an hour’s drive from his home. Because it was too late to send the specimens anywhere for testing on that day, the swabs were mailed directly to the Texas Department of State Health Services Laboratory on Wednesday after a phone consultation with the state health department.\n\n【11】The patient began his ciprofloxacin the next evening (July 24). On July 26, after having taken 4 doses of his antibiotics, he was feeling worse and sought additional care at the emergency department of hospital A, more than an hour’s drive from his residence. Concurrently, the state laboratory notified his primary-care physician that a preliminary laboratory report for the specimen was PCR-positive for _B. anthracis_ ; this result was confirmed by culture the following week (August 1) . His physician relayed the information first to the patient and then to hospital staff. Upon arrival to the hospital, the patient reported pain, difficulty walking, and nausea. He reported intermittent spontaneous drainage of a dark, jelly-like material from the larger wound. He reported no fever, chills, chest pain, shortness of breath, pain at rest, numbness, or tingling. He did not use tobacco products.\n\n【12】At hospital A, he reported that his exposure had been »3 weeks earlier. At examination, his vital signs were blood pressure 132/71 mm Hg; heart rate 91 beats/min; and respirations 24 breaths/min. He was afebrile. He had a nondraining, nonerythematous eschar 7.2 cm × 5 cm on the lateral aspect of the right calf and a painless, nondraining, nonerythematous 3.3 cm × 2 cm eschar on the lateral aspect of the right knee . His leukocyte count was 12,000 (10 3  cells/µL); hemoglobin, 15.5 g/dL; hematocrit, 46.9% g/dL; platelets, 83,000 (10 3  cells/mL); blood urea nitrogen, 35 mg/dL; and creatinine, 2.6 mg/dL. His antibiotic was switched to intravenous doxycycline (100 mg every 12 hours). He was discharged on hospital day 13.\n\n【13】### Control and Prevention Measures\n\n【14】##### Control Measures for Animal Outbreaks\n\n【15】Because naturally occurring human anthrax cases in endemic countries are almost always related to exposure to infected animals or their byproducts, control of animal anthrax essentially eliminates human risk. The primary control measure for animal anthrax is annual preventive vaccination; however, once an outbreak occurs, other control measures include ring vaccination, proper carcass disposal to avoid further environmental contamination, and quarantine (i.e. limit animal movement from the affected and nearby properties, animal contact with anthrax-contaminated sites, and contact between affected and nonaffected herds) . On the basis of anecdotal reports and 1 small study, tabanid flies (e.g. deer and horse flies) might play a role in transmission; whether fly control is achievable or would be effective remains an open question .\n\n【16】The attenuated Sterne-strain of _B. anthracis_ is used globally for vaccination among domestic livestock . Because the vaccine is live-attenuated, concurrent antibiotic administration can substantially diminish efficacy. If an animal is given antibiotics either 10 days before or after vaccination, revaccination is recommended . Whether concurrent administration of antibiotics played a role in diminished vaccine efficacy in the Texas outbreak is unclear.\n\n【17】Proper and safe carcass disposal is critical for controlling anthrax outbreaks in enzootic areas because inappropriate carcass disposal seeds the soil with spores and increases the risk for future epizootics. Global recommendations  and codified Texas regulations  for carcass disposal are similar: the carcass should be burned in place, using a pyre or other method that leaves only ash and allows the destruction of the contaminated soil as well (i.e. “burnt until it is thoroughly consumed”) . When a carcass cannot be burned, global recommendations are to bury it deeply . The historic practice of adding lime should be avoided . High soil calcium levels, either from the addition of lime or as occur naturally in southwest Texas, are conducive to _B. anthracis_ spore survival  and increase the likelihood of future outbreaks. The least desirable disposal method is leaving the carcass in place, because scavenging can further disseminate the spores and increase future exposure risks for susceptible animals. Alternative carcass disposal methods are needed in areas where the standard recommendations to burn or bury carcasses are impractical. This need is particularly pronounced where there is an abundance of susceptible wildlife species that are not vaccinated or where there is poor vaccination coverage of domestic hoofstock.\n\n【18】##### Prevention of Human Cases in Endemic Areas\n\n【19】Human and animal health authorities should remind at-risk populations of the following prevention measures when animal cases are first identified. During animal outbreaks of anthrax, persons who handle and dispose of infected animals are at highest risk for exposure. However, exposure can be minimized through use of personal protective equipment, which should include gloves that can be disinfected or disposed of, long sleeves and pants, and footwear suitable to the terrain that can be disinfected . Even in the absence of a recognized anthrax outbreak, veterinarians and ranchers in endemic areas should always keep anthrax in mind as they interact with members of susceptible species that are ill. To do otherwise can result in inadvertent exposure to anthrax.\n\n【20】Antibiotic postexposure prophylaxis (PEP) is another important component of prevention. In the former Soviet Union, before 1965, 58/339 (17%) of patients who did not receive antibiotic prophylaxis after cutaneous exposures had onset of anthrax; in contrast, only 5/287 (2%) who received prophylaxis had onset of anthrax .\n\n【21】If skin or mucus membrane contact occurs during carcass disposal, persons should seek medical attention and receive antibiotic PEP for 7 days  and have their symptoms monitored for 14 days. Although aerosol exposure is unlikely in cases of natural cutaneous exposures, if potential aerosol exposure also occurred, antibiotic PEP should be administered for up to 60 days and anthrax vaccine may be considered.\n\n【22】Persons who live and work in anthrax-endemic areas and who anticipate interacting with animals that are dying or have died of anthrax might wish to consider preexposure prophylaxis with anthrax vaccine adsorbed (AVA). For preexposure prophylaxis of persons at high risk for _B. anthracis_ exposure, AVA is administered intramuscularly as a priming series at 0, 1, and 6 months, with booster doses at 12 and 18 months and annually thereafter . Health departments in endemic areas that have existing vaccination programs can acquire AVA from the manufacturer.\n\n【23】##### Healthcare Infection Control Issues for Cutaneous Anthrax\n\n【24】A person with cutaneous or other type of anthrax (e.g. injection, ingestion, or inhalation) cannot transmit disease through aerosol or droplet. However, spores that could remain on a person’s skin, hair, or clothing after an exposure before they bathe or shower and change clothes might possibly transfer to someone else’s skin and cause cutaneous anthrax . Although incubation periods of < 1 day are reported, patients usually wait a few days to seek care, making it likely that they would already have bathed and changed clothes before seeking care. It is therefore unlikely that healthcare personnel would be secondarily exposed to spores.\n\n【25】Although cutaneous anthrax lesions can be contagious before the institution of effective antibiotic therapy, they become sterile in <1 day once therapy has begun . Lesions should be covered until the patient has had 24 hours of effective antibiotics. Contact precautions should be used for the first day; after that, standard precautions are sufficient.\n\n【26】Disposable items that have been in direct contact with the anthrax lesion, any tissue removed during debridement, and potentially infectious wound care materials  should be disposed of in a biohazard bag according to guidelines for disposal of any potentially infectious material. No additional disinfection is needed beyond what is regularly scheduled for the facility. Nondisposable surfaces in direct contact with the anthrax lesion or wound drainage can be disinfected with a 0.5% hypochlorite solution, a commercial product such as SporGon , or other sporicidal agents such as an Environmental Protection Agency–registered antimicrobial product effective against _B. anthracis_ spores ; products effective against _Clostridium difficile_ spores might also be appropriate \n\n【27】##### Diagnosis\n\n【28】Although an eschar is the cardinal sign of cutaneous anthrax, in its early stages, anthrax can manifest as a group of small vesicles that might be pruritic. The lesion might be surrounded by erythema and swelling but is usually painless. Lymphadenopathy can occur, and constitutional symptoms including fever and headache are also possible. Localized cutaneous anthrax can disseminate to become a systemic disease. Although a substantial portion (10%–40%) of patients with cutaneous anthrax would die if left untreated , most can recover with treatment . Meningitis is also a possible, and typically fatal, complication .\n\n【29】In the United States, cutaneous anthrax is decidedly rare: other causes of eschars and eschar-like lesions include poxvirus infections (e.g. cowpox, vaccinia, orf), rickettsial infections (e.g. scrub typhus and _Rickettsia parkeri_ rickettsiosis), ulceroglandular tularemia, staphylococcal or streptococcal infections, and noninfectious causes such as insect or spider bites. Obtaining a good exposure history is key to determining the likelihood of various etiologies among the differential diagnoses and determining the best specimens to collect. Patients seeking care with an eschar or eschar-like lesion should be asked about recent exposure to dead or dying herbivores or biting flies in an anthrax enzootic area; recent animal bites or scratches; and recent contact with lagomorphs, rodents, fleas, ticks, and spiders.\n\n【30】A Gram stain of a swab specimen from the lesion can often quickly identify possible cases and narrow the differential diagnosis . Specimens for tests such as Gram stain, culture, and PCR to rule anthrax in or out  must be collected before the use of antibiotic therapy because they will rapidly become negative after the implementation of therapy . Specimens can be sent to sentinel laboratories for preliminary assessment. Specimens for which _B. anthracis_ is not ruled out by a sentinel laboratory should promptly be sent to a Laboratory Response Network (LRN) laboratory for confirmation . LRN is a network of laboratories established to respond to biologic and chemical threats and other public health emergencies that consists of 3 types of laboratories. Private and commercial laboratories comprise the first tier of the LRN and are described as sentinel laboratories. Laboratories that receive reagents, protocols, and specialized training to perform confirmatory testing for multiple agents in high-risk environmental or clinical samples comprise the second tier of LRN and are referred to as reference laboratories. Specialized characterization of organisms, bioforensics, select agent activity, and handling of highly infectious biologic agents is performed at national laboratories, the third tier of LRN. However, with approval from public health authorities, specimens from lesions that are highly suspicious based on clinical or epidemiologic grounds can be sent directly from clinicians to an LRN laboratory .\n\n【31】##### Notification\n\n【32】Clinicians should promptly notify their local or state health department when they suspect anthrax, although the mandated timing varies by jurisdiction. State and territorial health departments should notify the Centers for Disease Control and Prevention (CDC) within 4–24 hours  of the initial report for patients whose illness meets the probable or confirmed case definition . Presumptive positive results from an LRN laboratory must be reported within 2 hours to the state and CDC.\n\n【33】##### Treatment\n\n【34】Cutaneous anthrax lacking systemic manifestations such as fever, tachycardia, tachypnea, hypotension, leukocytosis, or leukopenia can usually be treated with 7 days of an oral antibiotic. Patients with cutaneous anthrax should only continue oral antibiotics for PEP after antibiotic treatment is complete if the patient was also exposed to aerosolized spores; this would rarely be indicated for naturally acquired cutaneous infections because aerosol exposures are unlikely .\n\n【35】Systemically ill patients should be evaluated for meningitis; if meningitis can be ruled out, they should be treated with at least 2 intravenous antibiotics (1 that is bactericidal and 1 that inhibits protein synthesis to block toxin production). Antibiotic therapy should continue for \\> 2 weeks or until the patient is stable. If meningitis is present, \\> 3 antibiotics should be used ( \\> 1 should be bactericidal, \\> 1 should inhibit protein synthesis, and all should have good central nervous system penetration). Antibiotic options for treatment and prevention of anthrax are listed in Tables 1 and 3 .\n\n【36】Systemically ill patients (whether from cutaneous, ingestion, inhalation, or injection exposures) are candidates for 1 of the Food and Drug Administration–approved anthrax antitoxins. The antitoxins are available through the Strategic National Stockpile pending a consultation with an anthrax subject matter expert at CDC, which can be reached by calling the Emergency Operations Center .\n\n【37】Surgery might occasionally be indicated for lesions complicated by compartment syndrome. However, surgery usually is not necessary for cutaneous anthrax .\n\n【38】### Public Health Implications and Conclusion\n\n【39】Anthrax is endemic to parts of the United States. Epizootics emerge with varying frequency when climatic conditions favor the uncovering of soilborne _B. anthracis_ spores with subsequent consumption by susceptible herbivores. Humans contract cutaneous anthrax through contact with animals that are ill or have died from anthrax or contact with _B. anthracis_ –contaminated byproducts; this risk is increased during epizootics. The outbreak we describe was confirmed in June 2019, but its actual start date is unknown; reliable recognition of epizootics might be impeded when they occur in vast, rough, and sparsely populated areas such as those in the anthrax-endemic areas of Texas. These same geographic characteristics create challenges in implementing the recommended disease control interventions, including appropriate carcass disposal and broad use of animal anthrax vaccine in species for which the vaccine is licensed, as well as off-label use in other species. Wild herbivores (e.g. white-tailed deer and exotic hoofstock) contributed to the 2019 Texas outbreak, but effective mitigation (carcass disposal or vaccination) of the risk they posed could not be adequately achieved.\n\n【40】The cutaneous anthrax patient associated with this outbreak was apparently exposed through a scratch on the leg from the antler of an untested deer carcass. The physician he visited in rural Texas included anthrax in the differential diagnosis, obtained and submitted diagnostic samples before treating the patient, and provided the patient with a prescription for oral ciprofloxacin. Anthrax was identified through PCR and confirmed through culture at the state reference laboratory from swab specimens of a leg lesion. The patient was treated as an outpatient with appropriate antibiotics until his condition worsened and required a 13-day hospitalization. The necessity for hospitalization might have been related to a few-week delay in seeking treatment. Despite the delay, the patient, like most patients with cutaneous anthrax, survived with antibiotic treatment .\n\n【41】As soon as anthrax is recognized in an animal population, public health and animal health agencies must collaborate to heighten awareness among medical and animal health communities, as well as among ranchers and other inhabitants of at-risk areas. Timely delivery of information to ranchers on proper carcass disposal and appropriate use of personal protective equipment, as was done through various alerts, might reduce the number of exposures. If exposure is recognized, antibiotic PEP should be considered by medical providers. AVA may be appropriate for persons at high risk for exposure, such as veterinary staff and ranch workers in endemic areas; however, this process involves a long-term commitment to annual booster shots to ensure protection.\n\n【42】Ranchers and veterinarians should receive authoritative information on animal vaccine use to break the cycle of transmission (including emphasis on avoiding administration of antibiotics 10 days before or after vaccine administration). Even in the absence of a recognized anthrax outbreak, veterinarians and ranchers in endemic areas should keep anthrax in mind as they interact with ill members of susceptible species. Doing otherwise might result in inadvertent exposure to anthrax. A survey of ranchers in the outbreak area is planned by TAHC to assess knowledge, attitudes, and practices regarding anthrax, including information on livestock vaccination.\n\n【43】Recent federal anthrax guidance has focused on the treatment of systemic anthrax, including meningitis, rather than on the more common cutaneous form of the disease. Given that half the cases in the 2001 anthrax incident in the United States  were cutaneous anthrax and most sporadic cases in the United States and worldwide are cutaneous, this article provides an overview of prevention and control measures for animals and a single resource for the prevention, diagnosis, infection control, and treatment of naturally acquired cutaneous anthrax.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "044d140c-9f25-4e31-b86a-a948d9ec0efa", "title": "Invasive Group A Streptococcal Infection and Vaccine Implications, Auckland, New Zealand", "text": "【0】Invasive Group A Streptococcal Infection and Vaccine Implications, Auckland, New Zealand\nDuring the 2 decades since recognition of streptococcal toxic shock syndrome (STSS), there have been many publications on invasive group A streptococcal (GAS) infections, some population-based . The spectrum of infection caused by _Streptococcus pyogenes_ varies widely from invasive disease, such as bacteremia, sepsis, necrotizing fasciitis (NF), and STSS, to noninvasive infection, most commonly pharyngitis with suppurative complications, such as otitis media, and nonsuppurative complications, such as acute rheumatic fever (ARF) and acute glomerulonephritis (APSGN).\n\n【1】GAS infection causes a substantial number of illnesses and deaths, especially in the developing world, with ≈500,000 deaths worldwide annually, attributable mostly to ARF and its sequelae, rheumatic heart disease, and invasive infection . GAS disease and its sequelae, including GAS pharyngitis, have been well documented in New Zealand .\n\n【2】With renewed interest in GAS vaccines , understanding the complete spectrum of disease, including invasive GAS disease, in diverse populations is essential. The vaccine most completely studied is a 26-valent vaccine based on _emm_ types and M subtypes collected across GAS diseases from the United States . We previously published a population-based approach of laboratory surveillance for invasive bacterial diseases in Auckland’s public hospitals where all persons with acute disease would be admitted . Using this approach, we demonstrate the effects of invasive GAS on the Auckland population to complement our knowledge of other GAS-associated diseases by using prospectively collected incidence data, clinical characteristics, associated underlying conditions, and the associated _emm_ types. This study also provided an opportunity to establish the direction of further investigations and to focus interventions in New Zealand.\n\n【3】### Methods\n\n【4】##### Surveillance\n\n【5】We enrolled patients during January 1, 2005–December 31, 2006. Patients were included if they resided in metropolitan Auckland and had a GAS isolate cultured from a previously sterile body cavity. Patients with STSS were included in accordance with the consensus definition ; STSS also was the diagnosis for patients who were dead on arrival or who died within 48 hours after illness onset and for whom laboratory data were insufficient in accordance with the methodology of Davies et al. NF was defined as tissue necrosis diagnosed by histopathologic examination or by the treating surgeon during surgical debridement. Patients could have had >1 diagnosis, with the exception of bacteremia without a source. Clinical syndromes, such as skin or soft tissue infection, had to be accompanied by recovery of an isolate from a normally sterile site or specimen, such as blood, to meet the case definition. Nosocomial infection was defined as GAS infection in patients who had been hospitalized for >72 hours. Invasive GAS infection was defined as postpartum if it occurred in a woman who was pregnant or < 30 days after delivery or who had clinician-defined puerperal fever, chorioamnionitis, or a septic abortion. Women from whom GAS was isolated from amniotic fluid or placenta alone were excluded . Our study was approved by the regional ethics committee and each hospital’s research committee and Maori research committee.\n\n【6】Data were collected from the microbiology laboratories serving all 3 Auckland regional District Health Board (DHB) hospitals, i.e. Auckland City Hospital and Starship Children’s Hospital (Auckland DHB); Middlemore Hospital, which includes Kidz First Children’s Hospital (Counties Manukau DHB); and North Shore Hospital and Waitakere Hospital (Waitemata DHB). All Auckland residents with serious medical illness would attend 1 of these hospitals.\n\n【7】Auckland (2006 population: 1,387,780), New Zealand’s largest city, comprises one third of the country’s population and is the country’s most ethnically diverse city. In 2006, 19.0% of residents self-identified as Asian, 14.4% as Pacific Islander, 11.1% as indigenous Maori, and 56.5% as European. The climate is temperate, with summer occurring during December through March. We used New Zealand birth data for infants <1 year of age and customized New Zealand census charts for DHBs as denominators.\n\n【8】We obtained demographic and clinical features by reviewing medical charts and electronic documents. To ensure complete surveillance, we requested International Classification of Diseases, 10th Revision, diagnoses from DHB data managers. We contacted the regional coroner and forensic pathologist to seek out records of deaths (including deaths in the community) caused by GAS infection and scrutinized intensive care unit (ICU) data for diagnoses of shock from GAS, STSS, or NF. Disease severity was determined by length of stay, ICU admission, and use of surgical and medical procedures.\n\n【9】We assigned each invasive GAS infection in the Auckland region a deprivation score by using the New Zealand Deprivation Index 2006 . This index measures socioeconomic status (SES) in small areas according to 9 variables (income, income assistance, education, access to a car and phone, household crowding, employment, single-parent family, housing rented or owned).\n\n【10】##### Laboratory Techniques\n\n【11】β-hemolytic colonies on blood agar were typed as Lancefield group A by using commercially available latex agglutination kits (Pro-Lab Diagnostics, Austin, TX, USA). Group A isolates were sent to Environmental Science and Research Laboratory (Wellington, New Zealand) for _emm_ typing by using established procedures . Concordance between _emm_ types and M serotypes has been established . Antimicrobial drug sensitivities were determined by routine methods .\n\n【12】##### Estimates of Vaccine Benefit\n\n【13】We used _emm_ typing to estimate the proportion of cases and deaths caused by _emm_ types in the proposed 26-valent vaccine. These _emm_ GAS types are 1, 2, 3, 5, 6, 11, 12, 13, 14, 18, 19, 22, 24, 28, 29, 33, 43, 59, 75, 76, 77, 89, 92, 94, 101, and 114 . We then calculated potential vaccine efficacy in the most at-risk Auckland populations: persons <5 years of age and \\> 65 years of age.\n\n【14】### Results\n\n【15】##### Epidemiology\n\n【16】During the 24-month study period, we identified 333 patients who potentially had invasive GAS infections. Of these, we excluded 118 who did not fulfill the inclusion criteria. The most common reasons for exclusion were isolation from a nonsterile site or residence outside metropolitan Auckland at the time of diagnosis. Using the electronic discharge summaries based on International Classification of Diseases, 10th Revision, coding, we identified and included 10 (4%) additional cases that fit the case definition.\n\n【17】The 225 patients were from all ethnic groups: European (77 \\[34%\\] patients), Maori (69 \\[31%\\]), Pacific Islanders (70 \\[31%\\]), and other ethnicities (7 \\[3%\\]). For 2 patients, no information was available about ethnicity. For the 225 patients, median age was 53 years (range 0–97 years), and 119 (53%) patients were male. Ethnic disparities, although notable in the extremes of life, did not differ significantly by age . The 198 patients with invasive GAS infection for whom SES information was available were more likely to originate in areas designated by the New Zealand Deprivation Index 2006 as lower SES areas than in higher SES areas . Forty-nine percent of case-patients were from the lowest SES quintile.\n\n【18】##### Case-Fatality Rate\n\n【19】Twenty-two patients died, for an overall case-fatality rate (CFR) of 10% . Fourteen of these patients died within 72 hours after hospital admission. Three infants (one 2 months of age and two 4 months of age) who died in the community had STSS. One death previously had been attributed to sudden infant death syndrome.\n\n【20】The median age of patients who died was 62 years (range 2 months–86 years). Eighteen adults who died had multiple concurrent illnesses. The highest CFR (31%) was for infants (a total of 4 deaths in three 4-month-old infants and one 2-month-old infant); these were the only deaths among children <15 years of age.\n\n【21】All infants who died had GAS-positive blood cultures. One who died in the community also had GAS-positive cerebrospinal fluid. Three of the 4 deaths occurred in the community and are attributed to STSS. The illness of the fourth (hospitalized) infant also met the criteria for STSS. Bronchopneumonia was found at post-mortem examination in 2 infants (1 hospitalized, 1 in the community). Two of the infants who died in the community had additional pathogens isolated from postmortem blood cultures ( _Staphylococcus aureus_ in both cases and _Streptococcus pneumoniae_ and viridans streptococci in 1 each) but no gram-negative organisms.\n\n【22】##### Clinical Features\n\n【23】The most common clinical feature was skin and soft tissue infection (97/225; 43%) . Of the 30 patients with STSS, 26 (87%) had an underlying condition before the onset of acute GAS disease. Median age at STSS occurrence was 57 years (range 2 months–86 years). Six cases occurred in children <5 years of age. Empyema (4 cases; p<0.0001) and brain abscess (2 cases; p = 0.0011) occurred more frequently in children < 14 years of age than in adults. The incidence of bacteremia with no focus of infection was 1.4 cases per 100,000 persons per year overall, but 3.7 per 100,000 for children <5 years of age (n = 7).\n\n【24】Seven cases of GAS postpartum infection were recorded for women 15–49 years of age, for a rate of 0.16 cases per 1,000 live-born infants (Maori and Pacific Islander, 0.21 cases/1,000 live-born infants). No deaths occurred in this group. We also identified 3 premature neonates with invasive GAS disease unrelated to cases in adults; 1 infection was nosocomially acquired.\n\n【25】##### Risk Factors\n\n【26】Of the 223 patients for whom data were available, 58 (26%) had no underlying condition or other risk factor, 114 (51%) had 1 or 2 risk factors, and 64 (28%) had \\> 3 risk factors. In the \\> 15-years age group, 67 (35%) had heart disease, 60 (32% \\[23 Maori, 25 Pacific Islanders\\]) had diabetes, and 21% had either renal disease (39 persons) or lung disease (40 persons). Cigarette smoking was the most common nondisease-related risk factor (56 \\[30%\\] of 189 persons \\> 15 years of age).\n\n【27】##### Microbiological Analysis and Potential Vaccine-Preventable Disease\n\n【28】GAS was most frequently isolated from peripheral blood cultures (184 \\[82%\\]). Other sources were surgical specimen (37 isolates), tissue specimen , joint aspirate , pus aspirate , catheter blood culture , peritoneal aspirate , cerebrospinal fluid , and postmortem blood .\n\n【29】Of the 225 cases, 205 (91%) GAS isolates were available for _emm_ typing . Seventy (34%) of 205 cases had an _emm_ type that was contained in the 26-valent vaccine. The proposed 26-valent vaccine could prevent 30% of GAS invasive cases in children <5 years of age and 15% of cases in persons \\> 65 years of age .\n\n【30】Of the 225 isolates, 1 (0.4%) was resistant to erythromycin and 1 (0.4%) had intermediate sensitivity to erythromycin. Three (1.3%) were resistant to clindamycin.\n\n【31】##### Disease Severity\n\n【32】Hospitalization was required for 222 patients (3 deaths occurred in the community). Length of stay was >10 days for 105 (47%); mean length of stay was 15.9 days (range 1–153 days). Thirty-eight (17%) required ICU admission (mean length of stay 4.5 days; range 1–9 days); maximum length of stay was 19 days. Nosocomial infection was responsible for 12 (5%) of the 225 cases. Seventy-five (33%) patients required at least 1 surgical procedure, predominantly drainage, débridement, or washouts. One patient (2 years of age) with STSS required intravenous immunoglobulin.\n\n【33】### Discussion\n\n【34】Our New Zealand study is population based and prospective. The overall annual incidence rate for greater Auckland of 8.1 cases per 100,000 persons per year is more than double or triple the rate of earlier reports elsewhere in the industrialized world. Annualized rates reported from other industrialized countries were 3.5 per 100,000 in 2007 in the United States ; 1.5 in Ontario, Canada; and 3.1 in the Netherlands .\n\n【35】This study was conducted in metropolitan Auckland, where studies are ongoing to assess GAS disease, including endemic ARF , APSGN , and streptococcal pharyngitis  with associated _emm_ typing. Our study was conducted in close association with ongoing active surveillance for ARF and its associated _emm_ types  and APSGN surveillance. Approximately 50–70 new ARF cases (90% in persons <20 years) occur each year in this population  and a similar number of APSGN. The incidence of streptococcal pharyngitis has been carefully determined in a randomized controlled trial for ARF control at ≈60 cases per 100 child-years during a 4-year period in a population of ≈12,000 persons 5–19 years of age . This rate is considerably higher than that documented recently from Fiji (14.7/100 child-years) . Serotypes in ARF cases in our study were diverse ( _emm_ 58, 74, 75, 76, 92, 99, and 53), mirroring an earlier report ( _emm_ 53 and 58 associated with ARF) .\n\n【36】The annualized rate for Maori and Pacific Islanders <1 year of age (75/100,000) was similar to rates reported from Kenya  and greater than the rate more recently reported from Fiji (44.9/100,000)  from prospective studies. Nearly 50% of cases occurred in the lowest SES quintile of Auckland. Indigenous Maori and Pacific Islanders are overrepresented in lower SES areas of Auckland. Ethnically disparate rates for invasive GAS parallel these findings, with overrepresentation of these groups. The New Zealand Deprivation Index uses multiple parameters, including housing, income, and education. In addition, access to health care may be deficient  and perhaps health knowledge as well. The role of crowded housing in the population in our study has been recently documented for epidemic meningococcal disease  and may have a more substantial role for GAS disease, which is considered to be even more contagious . The high likelihood of an associated risk factor in the adult population, such as a chronic disease or another association, has been reported many times .\n\n【37】Skin infections have been documented as a major cause of illness in Auckland . More recently, New Zealand surveillance data  reported highly discrepant hospitalization rates for serious skin disease: Maori and Pacific Islanders <15 years of age are more likely to be hospitalized (unadjusted rate ratios 2.77 \\[95% CI 2.66–2.88\\] and 4.47 \\[95% CI 4.27–4.68\\], respectively) than are New Zealand European children 0–14 years of age. These data also reflected more hospitalizations for persons living in the most deprived quintile , which most likely is related to poor access to primary care and perhaps health knowledge. High population-based rates of invasive disease caused by methicillin-sensitive _S. aureus_ , mostly bone and joint disease, also have been documented .\n\n【38】The overall CFR from our study (10%), with a high CFR for STSS (63%), mirrors other studies in the industrialized world . This CFR suggests good access to hospital care and efficiently delivered secondary and tertiary care, including ICU admission. A recently reported CFR (28%) from Fiji suggests otherwise from the developing world .\n\n【39】We included in our study all 3 infants who died in the community and from whom GAS was cultured . GAS is a rare finding from postmortem specimens . In all 3 cases, only gram-positive organisms were isolated (1 solely group A streptococcus from blood and cerebrospinal fluid). Studies in which careful precautions have been taken to reduce contamination show that approximately two thirds of blood cultures yield negative results, 2 in 9 yield 1 isolate, and 1 in 9 show mixed growth. GAS infection as the sole cause of death was less certain in 2 cases in our study in which >1 potentially disease-causing species was cultured. We characterized the 3 infant deaths as STSS according to Davies et al. a definition that produces higher rates of STSS and a higher CFR in children than in other reports. We look forward to further investigations in this area.\n\n【40】Current health strategies for preventing illness and death from invasive GAS infections are limited. The rate of nosocomial infection in our study was low. The high rates in postpartum women and in infants require further investigation. We were unaware of any links between cases in our series. In New Zealand, index cases of invasive GAS disease are not investigated by public health authorities . Primordial strategies, such as of the provision of less-crowded housing  and hand-washing education, need further consideration .\n\n【41】The currently available vaccine most advanced in clinical trials  comprises 26 _emm_ types representing population-based, practice-based, and historical assessments from the United States . Its applicability to the population in our study might be less than ideal. Thirty-four percent of disease was caused by _emm_ types in the proposed 26-valent vaccine. Data are accruing from other sites (79% _emm_ coverage with the 26-valent vaccine in the United States, 69% in Europe, and 40% in Fiji) . Our data can contribute to a recent global estimate suggesting the current formulation of an experimental multivalent GAS vaccine may not be ideal in areas of most need . The effectiveness estimate in our study  suggests that fewer than one third of invasive GAS cases in children <5 years of age and perhaps 15% of cases in persons \\> 65 years of age could be prevented. This finding is of particular concern in a New Zealand population where other GAS-associated diseases cause a substantial amount of illness and death.\n\n【42】The rates in our study, driven largely by high rates in indigenous Maori and Pacific Islanders, are higher than those previously reported from industrialized countries and similar to reports from Fiji and Kenya. The rates suggest a need for more investigation and planned interventions in populations at highest risk. Our study also supports the role of GAS as a pathogen for invasive disease, particularly because of its effect on all age groups.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "712b8c8e-deb9-4854-aa98-cf12ce8a27ac", "title": "Hendra Virus Infection in Dog, Australia, 2013", "text": "【0】Hendra Virus Infection in Dog, Australia, 2013\nHendra virus (HeV) is a paramyxovirus (genus _Henipavirus_ )  that causes respiratory and neurologic disease in horses and humans; the case-fatality rate is >60%. Fruit bats are the reservoir hosts  and excrete virus in urine . Disease outbreaks among horses occur sporadically along the eastern coast of Queensland and New South Wales, Australia. Infection of veterinarians after close contact with infected horses presents a serious occupational hazard. After confirmation of Hendra infection in horses, an affected farm is quarantined by animal health authorities. All horses, cats, and dogs determined to be at risk for infection are monitored for clinical signs and tested for virus until they are no longer considered to be potentially infected. Cats and dogs are included in this surveillance because they have been shown to be susceptible to experimental infection with HeV . In July 2013, during investigation of HeV infection in a horse near Macksville, New South Wales, Australia, infection was also detected in a dog on the same farm.\n\n【1】### The Study\n\n【2】The infected horse was a 6-year-old Australian stock horse gelding. HeV RNA was detected by quantitative reverse transcription PCR (qRT-PCR) in EDTA-treated blood (cycle threshold \\[C t  \\] 26.82), serum (C t  30.87), and nasal swab samples (C t  34.56) collected on July 4, 2013. Later that day, the horse was killed by shooting. During follow-up investigations on July 6, negative HeV results (qRT-PCR and ELISA) were obtained from whole blood, serum, and nasal swab samples collected from 2 additional horses; whole blood and oral swab samples collected from 2 dogs; and oral swab samples collected from a third dog. These dogs were from the same farm as the HeV-positive horse.\n\n【3】Twelve days later, additional blood samples were collected (placed in EDTA or allowed to clot) from the 3 dogs, and oral swab samples were collected from 1 of these dogs (a 6-year-old cross-bred female fox terrier). HeV RNA was detected in the EDTA-treated blood (C t  31.48) and serum (C t  34.01), but not from the oral swab samples, from this dog. Results from all samples from the other dogs were negative by qRT-PCR and ELISA. Serum from the dog with positive results by qRT-PCR gave a weak positive result by ELISA and a virus neutralization titer of 8. The dog showed no signs of ill health, although it had winced several times, suggesting discomfort or pain. Because the transmission risk posed by the dog was uncertain, it was euthanized 14 days after collection of the first samples. Blood (placed in EDTA or allowed to clot); oral, nasal, rectal, and vaginal swab samples; and urine were collected immediately thereafter. The cadaver was immediately transported to the laboratory, and a postmortem examination was conducted later that day.\n\n【4】No external gross abnormalities were detected. Internal examination revealed diffuse marked reddening of all lung lobes and overlying dark patchy discoloration of dependent lobes; abundant frothy tracheal and bronchial fluid; enlargement and diffuse reddening of bronchial, tracheobronchial, and mandibular lymph nodes; prominent and diffuse reddening of both tonsils; and prominent white streaks at the corticomedullary junction of both kidneys. The spleen and liver were enlarged with rounded edges, and the liver had a mild cobblestone pattern . Histopathology findings closely aligned with gross findings; lesions in the brain were also histologically detected. The predominant lesion, found in decreasing severity in kidney, brain, lymph nodes, spleen, liver, intestine, and lung, was fibrinoid necrosis of vessels with marked segmental to diffuse vasculitis, disruption of subendothelial tunica intima, and expansion with thick bands of deeply eosinophilic hyaline to fibrinoid material admixed with karyorrhectic debris and degenerate neutrophils . Surrounding inflammatory infiltrates (plasma cells, lymphocytes, and karyorrhectic debris) often effaced and replaced surrounding normal structures. Cerebral and cerebellar meninges were moderately expanded with lymphocytes, plasma cells, and macrophages , and cerebral vasculitis was associated with surrounding malacia. Pulmonary alveoli were flooded with lightly eosinophilic fluid (edema) containing scattered erythrocytes, plasma cells, and macrophages. Hepatocytes were diffusely expanded, and floccular vacuolation was compressing adjacent sinusoids. Small amounts of viral antigen were detected in a necrotic glomerulus and within the media of a renal arteriole by immunoperoxidase staining.\n\n【5】An extensive range of fresh tissues and swab samples were collected for testing by qRT-PCR, and HeV RNA was found in many of the tissues . No virus was isolated from any of the tissues in cell culture. Serum collected at the time of euthanasia was positive by ELISA; virus neutralization titer was 128. All other animals on the farm remained seronegative when sampled 4 weeks after the infected dog had been euthanized. Laboratory methods are described in the Technical Appendix .\n\n【6】### Conclusions\n\n【7】Dogs and cats have been infected with HeV under experimental conditions. Previously, a dog located on the same property as 3 infected horses in Queensland, Australia, was found to be seropositive  without having shown clinical signs. The dog reported in this article, which also remained clinically healthy, was naturally infected and was identified during the acute stages of infection. Viral RNA was detected in this animal 12 days after euthanasia of the clinically affected horse. The dog was known to have been in close contact with the live infected horse and is suspected of having been exposed to its blood after the horse was euthanized. The epidemiologic and laboratory evidence supports transmission of HeV from horse to dog. In horses naturally infected with HeV, the development of neutralizing antibodies is associated with virus clearance from the infected animal. The detection of seroconversion and rising neutralizing antibody titers in canine serum collected ≈14 and then 16 days after putative virus exposure is consistent with the early stages of HeV infection and aligns with the low viral RNA levels in blood and a wide range of tissues (the highest levels were found in liver, bronchial lymph node, kidney, and myocardium). Failure to isolate virus in cell culture was probably the result of increasing antibody levels. It is difficult to establish from the qRT-PCR results whether virus replication occurred in tissues such as kidney, liver, myocardium, and spinal cord or whether this finding represents residual RNA from blood. However, the levels in these sites were 10–100-fold higher than that in blood, suggesting either local replication or accumulation of viral RNA. Very low levels of viral RNA were detected in the soft palate, pharynx, and tonsil, although virus was not detected in nasal, oral, rectal, or vaginal swab samples and urine. The risk for transmission of HeV from infected dogs to other susceptible species—including humans—remains unknown.\n\n【8】The histopathologic finding of widespread necrotizing vasculitis supports the current understanding of the pathogenesis of HeV infection, during which virus binds to the endothelial ephrin-B2 transmembrane protein receptor  and localizes in vessel walls, leading to endothelial cell damage. The most severe vascular lesions were found in kidney, brain, and lymph nodes; the lungs were relatively spared, and fulminant pulmonary edema and interstitial pneumonia were not significant findings in this case.\n\n【9】The route of infection for the dog reported here is unknown, but the dog was in close contact with the infected horse and is suspected to have had contact with its blood. Because viral loads in acutely infected horses are usually very high, dogs can be readily infected and should be kept away from infected horses, which seem to be efficient amplifying hosts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9d130b80-f405-42cc-8a59-dc3a60bb81c0", "title": "Sindbis Virus Antibody Seroprevalence in Central Plateau Populations, South Africa", "text": "【0】Sindbis Virus Antibody Seroprevalence in Central Plateau Populations, South Africa\nSindbis virus (SINV) is a mosquitoborne virus that belongs to the Togaviridae family; SINV is considered an arthritogenic alphavirus, which is known to cause self-limiting acute febrile illness (AFI) in Africa, Australia, Asia, and Europe and occasional debilitating arthritis that can persist for years after infection . Outbreaks are associated with heavy rainfall and temperature changes that favor mosquito breeding. Associations between SINV infection and acute or chronic arthralgia and myalgia have been described in Finland and Sweden . The extent of chronic debilitating disease caused by SINV in South Africa remains largely unknown.\n\n【1】SINV was identified as a cause of human disease in South Africa in 1963, and subsequent studies confirmed that the virus was present in mosquito populations in the central plateau region, which includes Free State Province . We investigated the seroprevalence of SINV in selected human populations of Free State Province. We used an in-house ELISA to detect SINV-specific IgG in serum and confirmed positive serum samples using neutralization assays . We screened a total of 568 stored serum samples retrospectively and anonymously. All available stored samples were tested and included 165 serum specimens submitted to the Division of Virology, National Health Laboratory Service, for routine clinical pathology tests from patients who attended the rheumatology clinic at the Universitas Hospital, Bloemfontein, South Africa, during 2013–2017 and 267 serum samples submitted to the National Health Laboratory Service during 2008–2010 from patients with AFI and no confirmed diagnosis. No clinical data were available; however, most attendees at the rheumatology clinic had chronic arthritis. We also included 136 serum samples from healthy volunteers that were collected during 2016–2017 for seroepidemiology studies of Crimean-Congo hemorrhagic fever virus and other vectorborne diseases.\n\n【2】We confirmed 11 serum samples were negative for SINV antibodies using a commercial immunofluorescence assay ; these samples were used to determine ELISA cutoff values. Positive control serum was obtained from 1 patient who had a laboratory-confirmed SINV infection. We obtained institutional ethics approval for this study from the Health Sciences Research Ethics Committee, University of the Free State , and informed consent was available for samples collected for the seroepidemiology study , negative control serum panel , and positive control .\n\n【3】We determined optimal reagent dilutions for the ELISA using checkerboard titrations. We diluted serum samples 1:100 and tested for reactions to SINV-specific and mock antigens . We detected reactions using horse radish peroxidase-conjugated antihuman IgG (1:8000) and 2,2′-azino-di-3-ethylbenzthiazoline-6-sulfonate . We measured optical density (OD) values at 405 nm and calculated net OD values by subtracting each sample OD obtained with mock antigen from the OD value obtained with SINV antigen. To normalize data, percent positivity (PP) for each sample was calculated as PP = (mean net sample OD ÷ mean net OD of the positive control) × 100.\n\n【4】We used the mean PP value \\+ 2 SD for 11 SINV-negative serum samples derived from a total of 83 replicates to determine the cutoff value between positive and negative samples . We tested SINV antibody-positive serum samples for neutralizing antibodies using a 50% tissue culture infectious dose serum neutralization assay; samples were considered positive for neutralizing antibodies if the titer was \\> log 10  1.0, equal to a serum dilution \\> 1:10 .\n\n【5】We detected SINV antibodies in 31/165 (18.8%) serum samples from patients who attended the rheumatology clinic, 13/136 (9.6%) samples from residents of SINV-endemic regions (high risk), and 25/267 (9.4%) samples from patients with AFI but no diagnosis . Of the total number of SINV-positive samples, ≈45% of the samples were from patients who attended the rheumatology clinic . We detected neutralizing antibodies with endpoint titers ranging from 1:20 to \\> 1:640 in 65 of 69 SINV antibody-positive serum samples; 4 samples showed discordant results.\n\n【6】SINV seroprevalence in South Africa for 2006–2009 was 5.4% and increased to 12% after heavy rainfalls in 2010 . A 9.4%–9.6% seroprevalence in persons at high risk and for febrile patients is within an expected range, considering that the samples were collected over a 10-year period during which substantial rainfall in Free State Province was associated with arbovirus outbreaks . Rheumatology clinic attendees had the highest percentage (18.8%) of samples with SINV-specific IgG, compared with 9.4% for residents from SINV-endemic regions and 9.6% for patients with AFI. However, limitations exist when comparing cohorts collected at different time points, and undetected outbreaks might have been responsible for higher seroprevalence among the rheumatology clinic patients. Prospective studies in national tertiary or specialist healthcare clinics should elucidate the contribution of viral infections to chronic arthritis. Our results suggest that associations between SINV infections and arthritis have been underreported in South Africa, and SINV infection should be considered a potential cause of arthritis in this country.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c8748de3-7911-484a-ad86-44cf3d3037ef", "title": "Multiple Transmission Chains within COVID-19 Cluster, Connecticut, USA, 2020", "text": "【0】Multiple Transmission Chains within COVID-19 Cluster, Connecticut, USA, 2020\nDuring widespread community transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), transmission chains are sometimes unclear. Although often unavailable, viral genome sequencing can complement epidemiologic investigations.\n\n【1】In fall 2020, the Connecticut Department of Public Health analyzed data from contact tracing interviews and initially identified 5 cases of coronavirus disease (COVID-19), the illness caused by SARS-CoV-2, in employees of a single workplace within 1 week. One employee also worked at an elementary school and fitness center; in those settings, several contacts of this employee later tested positive for SARS-CoV-2. At the time, the weekly community case rate in this county was 141 cases/100,000 persons , reflecting high community transmission according to thresholds set by the Centers for Disease Control and Prevention (CDC) . To better characterize this cluster, we investigated its scope, phylogenetic relationships, and factors associated with transmission.\n\n【2】### The Study\n\n【3】We defined a cluster-associated case as COVID-19 in a coworker, primary contact, or secondary contact of the initial 5 employees; all cases were diagnosed by a viral test (i.e. antigen or nucleic acid amplification tests) authorized for emergency use by the Food and Drug Administration . We defined the investigation period as starting 1 week before symptom onset of the earliest workplace case and ending 2 weeks after symptom onset of the last workplace case. We assessed symptoms, onset dates, adherence to prevention strategies, and potential exposures. This activity was reviewed by CDC and was conducted in accordance with applicable federal law and CDC policy (e.g. 45 C.F.R. part 46.102(l) , 21 C.F.R. part 56; 42 U.S.C. 241(d); 5 U.S.C. 552a; 44 U.S.C. 3501 et seq.).\n\n【4】We extracted SARS-CoV-2 RNA from clinical nasopharyngeal specimens and conducted genomic sequencing using an amplicon-based approach with the MinION  . We reconstructed maximum-likelihood phylogenetic trees using IQ-Tree with a general time-reversible nucleotide substitution model  .\n\n【5】Overall, we identified 16 cluster-associated cases in 6 workplace employees, 3 school staff members and students, 2 fitness center attendees, and 5 household contacts. Symptom onset was generally earlier among workplace employees than among school and fitness center contacts .\n\n【6】The workplace employed 35 persons and provided in-person customer service. After the first employee (W-1) experienced symptoms on day 1 and tested positive for SARS-CoV-2, the workplace closed and recommended SARS-CoV-2 screening for other employees. In addition to the 5 initial cases, we identified 1 other case in a workplace employee . All 6 employees worked during the week before their symptoms began .\n\n【7】In total, 4 of the 6 employees agreed to be interviewed . W-1 reported a potential exposure outside the workplace during the week before symptom onset. Two employees (W-2 and W-5) had contact with each other outside of work. No other employees reported contact with coworkers or members of coworkers’ households outside the workplace. Some employees were unable to maintain 6 feet of distance from coworkers and occasionally removed masks near coworkers. To increase air circulation, ventilation system fans were run continuously. Customers were not required to wear masks, and customer visits lasted 45–60 minutes.\n\n【8】One employee (W-3) also worked at an elementary school that offered in-person education 5 days a week. W-3 worked at the school on outbreak days 1–3; W-3’s symptoms developed on day 3. Three school contacts of W-3 subsequently tested positive for SARS-CoV-2 infection: a staff member (S-1) and 2 students (S-2 and S-3). S-1, a staff member, spent most of their time in a neighboring classroom but had brief contact with W-3 while substituting for W-3’s classroom. W-3 and S-1 reported strict adherence to prevention measures, including masking and social distancing, and did not have contact outside of school. To improve ventilation, the classroom windows were kept open. Among ≈15 students in W-3’s classroom, 2 asymptomatic students (S-2 and S-3) tested positive for SARS-CoV-2. S-2 was tested after a family member (S-2 \\[HH1\\]) had COVID-19 symptoms; another family member (S-2 \\[HH2\\]) later experienced symptoms as well. S-3 was tested after being notified that another person in the classroom tested positive for SARS-CoV-2.\n\n【9】W-3 taught an indoor fitness class on day 2, the day before their symptom onset. Approximately 6 clients attended the 1-hour class. Attendee F-1 experienced symptoms on day 5; attendee F-2 experienced symptoms on day 7. A household contact of F-2 (F-2 \\[HH\\]) later tested positive for SARS-CoV-2. W-3 and F-1 reported that attendees wore masks before and after the class but removed them during distanced (i.e. >6 feet) exercise. Information regarding facility ventilation was unavailable.\n\n【10】We acquired 13 specimens for viral genome sequencing. Specimens were unavailable for 2 workplace employees (W-2 and W-5) and 1 student household contact (S-2 \\[HH2\\]). The resulting genomes clustered into 2 separate lineages . Cluster 1 comprised 11 genomes, of which 9 were identical or differed by 1 mutation. These 9 genomes were extracted from samples from W-3, W-3’s household contact, the school staff and students, the fitness center attendees, and household contacts of persons at the school and fitness center . The other 2 genomes in cluster 1 were isolated from W-1 and W-6. W-1 was the only employee to work during the infectious period (defined as beginning 2 days before symptom onset); however, sequences for W-3 and W-6 differed from W-1’s sequence by \\> 3 mutations. Cluster 2 comprised genomes isolated from a workplace employee and the household contact of another employee ; there was no known epidemiologic link between these 2 persons.\n\n【11】### Conclusions\n\n【12】We found that the 16 members of a single COVID-19 cluster were involved in multiple transmission chains. Epidemiologic and genomic evidence supported transmission in the school and fitness center but not the workplace. These findings highlight challenges in accurate delineation of SARS-CoV-2 transmission chains and emphasize the benefits of combined epidemiologic and genomic investigation.\n\n【13】Although diagnostic specimens are often discarded by laboratories soon after testing, rapid identification of this cluster enabled the acquisition of specimens from 13 of the 16 cases. Our results suggest that infection was directly transmitted from W-3 to \\> 6 other persons within their household, school, and fitness center. Classroom transmission of SARS-CoV-2 is uncommon in the context of prevention strategies such as masking and distancing; previous studies have suggested that most school-associated cases are acquired outside of school . However, our results suggest that staff-to-staff and staff-to-student transmission occurred in this classroom. This investigation also adds to evidence that indoor exercise without masks can facilitate SARS-CoV-2 transmission . Fitness centers might consider moving high-exertion exercise outdoors, improving ventilation, and promoting mask use during indoor exercise. Mask use during indoor exercise was mandated in Connecticut later in November 2020 .\n\n【14】Genomic data did not indicate SARS-CoV-2 transmission among workplace employees. Divergence among viral sequences of workplace employees and the SARS-CoV-2 evolutionary rate of ≈1 mutation per 2 weeks  suggest that the 4 other workplace cases were each acquired independently. However, workplace transmission from unidentified employees or customers remains possible. In addition, a workplace employee and household contact had unrelated sequences, suggesting that they also were infected independently . This apparent workplace cluster, disproven by sequencing, highlights challenges in defining transmission chains during widespread SARS-CoV-2 community transmission. These findings highlight the crucial role of genomic sequencing in clarifying transmission chains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4c8c35ac-8b2a-4020-83df-0c60ef5d82d3", "title": "Melioidosis Manifesting as Chronic Femoral Osteomyelitis in Patient from Ghana", "text": "【0】Melioidosis Manifesting as Chronic Femoral Osteomyelitis in Patient from Ghana\nMelioidosis is becoming a serious emerging disease worldwide. _Burkholderia pseudomallei_ , the causative agent of melioidosis, is a gram-negative, aerobic bacillus found in wet soil and surface water. Human infection occurs by contact with contaminated soil or water through percutaneous inoculation, inhalation, or ingestion. There is often seasonal variation in incidence in association with heavy rainfall. Human cases are mainly reported in high endemicity areas of Southeast Asia and northern Australia, with sporadic reports from other tropical areas, although past research has predicted that many more areas have the prerequisite climate for _B. pseudomallei_ . The purpose of this study was to investigate a case of melioidosis manifesting as chronic femoral osteomyelitis in a patient from Ghana.\n\n【1】### The Study\n\n【2】A 33-year-old man from Ghana who had untreated type 2 diabetes mellitus reported a 2-month history of pain and swelling in the left knee. He had emigrated to the United Kingdom 14 months previously from Ghana, where he had lived in a rural northern area (Bolgatanga Province) for 16 months, working as a building project manager. During this period, he traveled to work on a motorbike over unpaved roads. He had occasional night sweats while in Ghana but had no other symptoms and had not sought treatment. He also traveled to urban cities in Nigeria (Lagos and Abuja) as part of his job. He reported visiting Scandinavia, Germany, Brazil, China, and South Africa over the preceding 7 years, although he had always stayed in urban areas and reported no exposure to soil or surface water.\n\n【3】He was febrile (temperature 38.3°C), tachycardic (110 beats/min), and normotensive. His left knee was painful and had a suprapatellar effusion. He had increased levels of inflammatory markers (leukocyte count 11.1 × 10 9  cells/L, predominantly neutrophils; C-reactive protein level of 221 mg/L; and erythrocyte sedimentation rate of 47 mm/h). Septic arthritis of the left knee was diagnosed. Knee aspirate showed neutrophil polymorphs but a negative Gram stain result and negative cultures. He was empirically given intravenous flucloxacillin, with a plan for a washout. A lytic area was observed on a radiograph of his left femur. Magnetic resonance imaging showed extensive osteomyelitis of the left femoral shaft and metaphysis, including ring enhancement and sinus formation . He underwent incision and debridement of the left femur, during which purulent material was expressed; multiple tissue samples were processed for culture and histologic analysis.\n\n【4】He was empirically given teicoplanin and meropenem. Peripheral blood cultures obtained on the day of hospitalization yielded no growth. Tests results for HIV and syphilis were negative. After 48 hours of culture on standard medium, multiple bone marrow samples yielded an organism identified as _B. pseudomallei_ by mass spectrometry. Teicoplanin was stopped, and meropenem started instead.\n\n【5】The isolate was referred to the National Reference Laboratory for Antimicrobial Resistance and Healthcare-Associated Infections (London, UK), which confirmed _B. pseudomallei_ . MICs determined by gradient diffusion  and interpreted by using European Committee on Antimicrobial Susceptibility Testing breakpoints . The isolate was susceptible to meropenem, imipenem, doxycycline, ceftazidime, and trimethoprim/sulfamethoxazole. Multilocus sequence typing showed a novel sequence type, ST1914, a single-locus variant of 3 other _B. pseudomallei_ isolates originating in Eritrea, Gabon, and Nigeria .\n\n【6】Computed tomography of the chest, abdomen, and pelvis showed no other foci of infection. Because he had labile blood glucose readings and an increased level of hemoglobin A1c, he was given antidiabetic medication. After 14 days of intravenous meropenem, he was switched to oral trimethoprim/sulfamethoxazole (960 mg 2×/d; 160 mg of trimethoprim and 800 mg of sulfamethoxazole) and doxycycline (100 mg 2×/d), completed 2 months of oral antimicrobial drugs, and showed a good clinical response. At the end of treatment, his inflammatory markers had returned to standard levels. Twelve months after he initially sought care, he had full range of movement of his left knee and well-healed surgical scars. He had standard levels of inflammatory markers; a repeat radiograph of his left femur showed changes consistent with his previous debridement and no evidence of ongoing osteomyelitis .\n\n【7】### Conclusions\n\n【8】For our patient, _B. pseudomallei_ infection was probably acquired in Ghana. Melioidosis is underreported in known disease-endemic foci, and modeling has suggested that it is probably endemic to 34 countries that have never reported cases, including 24 in Africa; West Africa was identified as the highest risk area, followed by Central Africa on the basis of environmental suitability . However, only a handful of sporadic cases have been reported from Africa, probably the result of underdiagnosis caused by resource-limited laboratories and public health systems .\n\n【9】Using modeling, Limmathurotsakul et al. estimated an annual incidence of 389 (range 111‒1,446) melioidosis cases in Ghana . Studies to elucidate the incidence of melioidosis in West Africa, including Ghana, are underway . Whole-genome sequencing and phylogenomic analysis of _B. pseudomallei_ isolates have demonstrated that Australia was an early reservoir, with onward transmission to Southeast Asia and then to southern Asia. Strains from Africa group into a single clade originating from ancestral clades in Asia and human migration from Indonesia to Madagascar >2,000 years ago might have led to dissemination of the organism into Africa with subsequent introduction of _B. pseudomallei_ into the Americas through the transatlantic slave trade .\n\n【10】The timing of this patient’s manifestations and his history of living in rural areas suggest that he was infected in Ghana. Although he had an extensive travel history, including to other known or potential melioidosis-endemic countries, such as Brazil and Nigeria, he stayed in urban settings and had no rural or soil exposure except in Ghana.\n\n【11】Our patient had untreated diabetes mellitus, a major predisposing factor for melioidosis. Increasing prevalence of diabetes mellitus in Africa means an expected corresponding increase in melioidosis cases. In several countries in Asia, hemoglobinopathies are also associated with increased illness and death caused by melioidosis. The milieu of hyposplenism, defective macrophage and neutrophil chemotaxis, and phagocytosis with iron overload are implicated in the pathogenesis of melioidosis in patients who have thalassemia . A recent case series from the Democratic Republic of the Congo described melioidosis in 3 children who had sickle cell anemia, 2 of whom died . Hemoglobinopathies are widely prevalent in West and Central Africa and might be an emerging risk factor for melioidosis in settings in Africa.\n\n【12】Approximately 7.6%–14.4% of melioidosis cases have musculoskeletal involvement . A longer duration of infection appears to increase the risk for bone and joint involvement. _B. pseudomallei_ osteomyelitis requires surgical debridement and prolonged treatment with antimicrobial drugs, usually intravenous ceftazidime or a carbapenem, followed by up to 6 months of high-dose, oral trimethoprim/sulfamethoxazole to achieve cure and prevent recurrent infection, which might occur in < 16% of cases after primary infection . In the case we describe, because of a lack of experience with melioidosis osteomyelitis, the patient received only 2 months of dual oral antimicrobial drugs after 2 weeks of intravenous antimicrobial drugs. Despite this suboptimal treatment, he has had no relapse during 12 months of follow-up. Extensive surgical debridement with good source control might have reduced the risk for recrudescent infection. However, the patient will need to be closely monitored for recurrent infection.\n\n【13】Our case report extends the range of countries in Africa implicated as sources of culture-confirmed melioidosis. We provide additional evidence that melioidosis is underdiagnosed in Africa. This disease should be part of the differential diagnosis of patients with diabetes who have a history of travel in tropical regions and infective symptoms. Strengthening laboratory capacity in Africa will better enable detection of _B. pseudomallei_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a7180c9d-4046-4fe7-bd44-6d5ba44f5e5a", "title": "A Pilot Study for Using Fecal Immunochemical Testing to Increase Colorectal Cancer Screening in Appalachia, 2008-2009", "text": "【0】A Pilot Study for Using Fecal Immunochemical Testing to Increase Colorectal Cancer Screening in Appalachia, 2008-2009\nAbstract\n--------\n\n【1】**Introduction  \n**The Appalachian region of the United States has disproportionately high colorectal cancer (CRC) death rates and low screening rates. The purpose of this pilot study was to assess acceptability of a take-home fecal immunochemical test (FIT) and the effect of follow-up telephone counseling for increasing CRC screening in rural Appalachia.\n\n【2】**Methods  \n**We used a prospective, single-group, multiple-site design, with centralized laboratory reports of screening adherence and baseline and 3-month questionnaires. Successive patients, aged 50 or older, at average CRC risk and due for screening were enrolled during a routine visit to 3 primary care practices in rural Appalachian Pennsylvania and received a free take-home FIT and educational brochure. Those who had not returned the test 2 weeks later were referred for telephone counseling.\n\n【3】**Results  \n**Of 232 patients approached, 200 (86.2%) agreed to participate. Of these, 145 (72.5%) completed the FIT as recommended (adherent) and 55 (27.5%) were referred for telephone counseling (nonadherent), of whom 23 (41.8%) became adherent after 1 to 2 counseling sessions, an 11.5 percentage-point increase in screening after telephone counseling and 84% FIT adherence overall. Lack of CRC-related knowledge and perceived CRC risk were the screening barriers most highly associated with nonadherence. Although not statistically significant, the rate of conversion to screening adherence was higher among participants who received telephone counseling compared to an answering machine reminder.\n\n【4】**Conclusion  \n**If confirmed in future randomized trials, provider-recommended take-home FIT and follow-up telephone counseling may be methods to increase CRC screening in Appalachia.\n\n【5】Introduction\n------------\n\n【6】Colorectal cancer (CRC) is the third leading cause of cancer death in the United States . The largely rural Appalachian region has high CRC death rates. The average annual age-adjusted CRC death rate in northern and central Appalachia, including Pennsylvania, exceeds that of non-Appalachian United States . Although screening can reduce the risk of dying from CRC, barriers to screening persist in Appalachia, including high levels of poverty, low levels of education, limited access to health care providers and facilities, and geographic isolation . During 2006 through 2008, only 52% of adults aged 50 or older in Appalachian Pennsylvania reported having had a colonoscopy or sigmoidoscopy in the past 5 years, less than in non-Appalachian Pennsylvania and the United States overall . The _Healthy People 2020_ CRC screening objective is 70.5% in the general population aged 50 to 75 . To reach this goal, people must be offered a test they will accept and use.\n\n【7】The fecal immunochemical test (FIT) is an evidence-based , cost-effective , and underused  screening method. FIT has been used in several countries for years; however, the test is less commonly used in the United States. Curry et al  found colonoscopy was the standard of care among physicians in Appalachian Pennsylvania and that their awareness of FIT as an evidence-based screening method was low. Even if most providers in Appalachia recommend FIT, use might remain low if other patient barriers persist. In studies with urban low-income and racial/ethnic minority residents, telephone counseling was successful in overcoming barriers and achieving significant increases in CRC screening . Thus, if FIT is expected to become a widespread screening method, patient acceptability of FIT should be examined. The objective of this study was to assess acceptability of a take-home FIT and the effect of follow-up telephone counseling for increasing CRC screening in rural Appalachia.\n\n【8】Methods\n-------\n\n【9】### Design\n\n【10】We used a prospective, single-group, multiple-site design. Our primary outcomes were 1) acceptance of a provider-recommended, take-home FIT, as measured by enrollment and initial FIT adherence rates, and 2) FIT adherence rates after telephone counseling. The study was conducted in 2008 through 2009 and guided by principles of community-based participatory research. The Community Advisory Committee of the Northern Appalachia Cancer Network, an affiliate of the Appalachia Community Cancer Network , was involved in all phases of the research. The Penn State Milton S. Hershey Medical Center and Fox Chase Cancer Center institutional review boards approved the research.\n\n【11】### Development of the telephone counseling intervention\n\n【12】Using constructs from the cognitive-social health information processing model (cognitive, affective, cultural, and economic factors; health values; and coping strategies) , we developed a message library of telephone scripts and a problem-solving approach to tailor the counseling protocol to help participants work through their barriers to screening. We then conducted a focus group of rural Appalachian Pennsylvania community members and CRC survivors to review the counseling scripts. On the basis of their review, we modified the counseling scripts to help address the prevailing silence around cancer and CRC in Appalachian communities. We then programmed the counseling scripts into the Population RESearch Application Generation Environment (PRESAGE) computer-assisted telephone interviewing system at Fox Chase Cancer Center and trained a master’s-level telephone counselor on CRC screening guidelines and tests, the counseling scripts, and the PRESAGE data management program.\n\n【13】### Study sample and recruitment\n\n【14】We recruited participants through 3 rural primary care practices in 3 geographically separated counties in Appalachian Pennsylvania with CRC death rates (26.3%, 20.3%, 20.2%) higher than both the national (16.7%) and Pennsylvania state (19.2%) averages . The population of the counties was primarily white (95.4%, 97.4%, 98.5%), similar to the other 49 Appalachian Pennsylvania counties and the larger northern and central Appalachian region . The sites were identified with assistance of our Northern Appalachia Cancer Network community advisory committee and included a hospital-based women’s primary care clinic, a stand-alone federally qualified health center, and a practice affiliated with a large primary care network. Eligibility included being aged 50 or older, at average CRC risk, and not in compliance with American Cancer Society recommendations for CRC screening. Every day, practice staff reviewed the medical records of potentially study-eligible patients scheduled for a routine appointment that day. During the patient visit and after confirming the patient as asymptomatic for bowel disease, the attending physician or nurse practitioner offered the patient the opportunity to enroll in the study. After providing signed consent, participants completed a contact information form and a 43-item baseline questionnaire that assessed demographics and CRC-related knowledge, attitudes, and beliefs as potential barriers to screening. The questionnaire was adapted from a validated instrument  used in a randomized trial that tested a mailed fecal occult blood test (FOBT) with and without telephone reminders on screening outcomes in rural Minnesota . From May 1 to September 30, 2009, we approached 232 patients, of whom 8 (3.4%) refused and 5 (2.2%) were deemed ineligible, yielding 219 (94.4%) enrolled patients. Of these, we eliminated 19 (8.7%) from the analysis who did not complete the baseline questionnaire, did not report their age, or reported having been screened for CRC during the baseline questionnaire. Thus, 200 (86.2% of 232 who consented) made up the final study sample.\n\n【15】### Intervention\n\n【16】During the patient examination, participants received a take-home InSure FIT kit (Enterix, Inc, a Quest Diagnostics Company, Edison, New Jersey) at no charge and the American Cancer Society brochure _They know how to prevent colon cancer — and you can, too_  and were asked to complete and mail the test in a prepaid envelope to a central laboratory within 7 days. Those who remained nonadherent at 2 weeks postenrollment, as confirmed through the laboratory’s online Care360 Physician Portal, were referred for telephone counseling. Their contact data and baseline questionnaire barriers scores were uploaded to the Fox Chase telephone counselor via the PRESAGE data management system.\n\n【17】The telephone protocol consisted of up to 10 call attempts for a single counseling session. After 10 failed attempts, the telephone counselor left a “cue to action” reminder message on the participant’s answering machine. We considered those without an answering machine lost to follow-up. For patients contacted by telephone, the counselor used the message scripts and problem-solving techniques to address barriers reported on the baseline questionnaire and during counseling. The counseling was client-centered; therefore, the duration of counseling sessions was determined by each client’s needs. Participants who still remained nonadherent 2 weeks after the initial counseling call received a “booster” call to reinforce the importance of screening and address remaining barriers. All study participants were mailed a 3-month follow-up questionnaire similar to the baseline questionnaire, except demographic questions were eliminated and questions were added about the counseling process (for those counseled) and intent to be screened in the next month (for those still nonadherent after counseling). The response rate for the 3-month questionnaire was low, completed by 114 (57%) of the 200 participants, of whom only 4 had received telephone counseling. We therefore excluded the 3-month data from the analysis.\n\n【18】### Analysis\n\n【19】We classified the prevalence and types of FIT screening barriers as 5 summary categories: 1) lack of knowledge and perceived risk (eg, age to begin screening, screening frequency), 2) unpleasantness (eg, “too messy”), 3) inconvenience (eg, lack of time), 4) cost (eg, high copayment, being uninsured), and 5) literacy issues (eg, test instructions too complicated). We used descriptive statistics to characterize participants and χ 2  tests to determine any demographic differences between initially adherent and nonadherent participants. We also used the Fisher exact test and odds ratios (ORs) with 95% confidence intervals (CIs) to assess the association between conversion to FIT screening and brief counseling (1-4 min) and comprehensive counseling (5-10 min), compared with answering machine reminder (<1 min). Multivariate logistic regression was used to test associations between baseline barriers and initial screening nonadherence (referral to telephone counseling), estimated as ORs. All demographic variables including sex, age, education level, marital status, household income, health insurance status, health care coverage type, and study site were entered into the multivariate model, and backward selection was used to identify significant variables. Study site ( _P_ < .001) and health insurance status ( _P_ \\= .04) remained significant and were included in the final model. Significance was set at _P_ < .05; all tests were 2-sided. We determined that a final sample size of 200 was necessary to detect a telephone counseling conversion rate (62%) estimated from another study . Data were analyzed using SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina).\n\n【20】Results\n-------\n\n【21】Most study participants were female, married, and low- to middle-income, and had health insurance, a high school education or less, and a mean age of 61 (range, 50-89) .\n\n【22】### FIT adherence\n\n【23】Overall, 145 (72.5%) initially completed the FIT and 55 (27.5%) who remained nonadherent were referred for telephone counseling, of whom 30 received brief counseling, 15 received comprehensive counseling, 8 were reached by answering machine reminder only, and 2 were lost to follow-up. Among these 55, 23 completed FIT screening after telephone counseling, representing an 11.5 percentage point increase in screening and 84% (n = 168) adherence to FIT screening overall. Fourteen used FIT after brief counseling, 6 after comprehensive counseling, and 3 after answering machine reminder only. Although not significant, the likelihood of converting to screening was higher among those who had received any telephone counseling compared to answering machine reminder (OR, 1.33; 95% CI, 0.28-6.27). Among the 45 counseled, 27 who remained nonadherent 2 weeks later received a booster call, of whom 10 subsequently completed the FIT. We found no difference between the comprehensive and brief counseling groups in FIT adherence following the booster call. The number of call attempts averaged 3 (range, 1-10 calls) for the initial counseling session and 2 (range, 1-2 calls) for the booster session. Median time to completed screening after counseling was 16 days (interquartile range, 8-33.5 d).\n\n【24】### Barriers associated with screening nonadherence\n\n【25】Among those referred for counseling, the most prevalent type of screening barrier at baseline was lack of CRC-related knowledge and perceived risk (100%), followed by unpleasantness (63.6%), inconvenience (60.0%), literacy (32.7%), and cost (32.7%), even though FIT was provided at no charge. In multivariate logistic regression, 5 barriers from the baseline questionnaire were significantly associated with initial nonadherence .\n\n【26】When asked on the baseline questionnaire, “Do you have any comments about colorectal cancer screening?” participants reported FIT as preferable to colonoscopy, citing reasons such as fear of adverse effects (eg, perforation of the colon) and dislike of the preparation for colonoscopy. Additionally, 66.7% of insured participants reported not knowing whether their health insurance covered CRC screening.\n\n【27】Discussion\n----------\n\n【28】We found that a free, provider-recommended, take-home FIT was highly acceptable among average-risk, rural Appalachian, primary care patients; 94.4% who were approached initially agreed to participate and 72.5% completed the test as recommended. Our laboratory-confirmed FIT adherence rate was higher than the 40.0% to 66.6% self-reported adherence to colonoscopy or sigmoidoscopy in the past 5 years in the Appalachian regions of 6 states, including Pennsylvania , suggesting that FIT may be a more acceptable screening test for Appalachian residents. A randomized Australian trial found population-based FIT screening was almost double that of FOBT (40% vs 24%) by 12-week follow-up after introduction of FIT, dueto its brush sampling technique and lack of dietary restrictions . A population-based Dutch study  found a significantly higher completion rate forFIT than FOBT (59.6% vs 46.9%). Thus, ease of obtaining the test specimen with FIT may partially account for our observed high initial FIT acceptance/adherence rates. It is also plausible that the high adherence rate may have been due in part to provider recommendation, one of the strongest known predictors of CRC screening . Nevertheless, our study is the first to our knowledge to demonstrate highacceptability and adherence to provider-recommended FIT screening in Appalachia and the United States.\n\n【29】Telephone counseling led to an 11.5 percentage-point increase in screening of initially nonadherent patients. Compared to a study by Dietrich et al  of low-income women in community and migrant centers in New York City, our screening adherence rate after counseling was lower (62.0% vs 41.8%). However, that investigation delivered an unrestricted number of calls for up to 18 months and assessed CRC screening of any type. A subsequent study of low- and moderate-income women in New York City by the same research team using 3 calls placed within 8 months found that women who received telephone counseling were 1.7 times as likely to be up to date with CRC screening at follow-up . Our findings more closely align with this latter study.\n\n【30】Although 3 of 8 of our participants reached only by answering machine reminder completed the FIT, it is unclear whether these recorded messages served as a cue to action to complete screening. Mosen and colleagues  found that in a large managed care population,up to 3 automated telephone calls and a reminder call (if needed) significantly improved FOBT testing by 6-month follow-up. Additional studies are needed to determine whether telephone counseling can yield a higher rate of screening than automated calls. In addition, our booster call had little effect among those counseled, suggesting that eliminating the booster call in future interventions could reduce costs otherwise incurred by the counselor’s extra time and effort.\n\n【31】Our finding that lack of CRC-related knowledge and perceived risk was the most prevalent type of barrier to screening is supported by Vanderpool and Huang . Data from the Health Information National Trends Survey , which included a newly created “Appalachia residence” variable, showed that, compared with non-Appalachians, Appalachian residents had significantly greater perceived risk of developing cancer in the future, were significantly more likely to associate cancer with death, believe people can know they have cancer before a diagnosis, believe everything causes cancer, and avoid physician visits when having symptoms. The 5 variables we found associated with screening nonadherence (benefits of screening don’t outweigh the risks; a normal test result doesn’t lessen the need to worry about CRC; early detection doesn’t make CRC easier to treat; screening might be physically uncomfortable; screening is too difficult to understand) are thus amenable to telephone counseling and warrant future additional research. Health care providers and counselors should address these barriers to screening and remain mindful that cultural norms, beliefs, and the social stigma of cancer that persists in Appalachia may underlie patients’ reluctance to discuss screening. As a focus group participant stated, “No one talks about colorectal cancer in our community, not even in the newspaper.” Future telephone counseling interventions should include scripts tailored to the target audience. Our finding that most insured participants did not know whether their health insurance covered CRC screening suggests that future telephone counseling interventions should include policy-level information, such as state mandates for insurance coverage of CRC screening.\n\n【32】This study had several limitations. Our sample was limited to mostly white, primary care patients scheduled for a routine visit, most of whom had health insurance. We did not collect data on race/ethnicity; however, the population of the 3 study counties was 94.2% white, similar to all 52 Appalachian Pennsylvania counties and slightly more than the entire 13-state Appalachian region . Future research in a more racially/ethnically diverse area of Appalachia could validate our results. We enrolled only patients due for screening; thus, the prevalence of being current with screening recommendations was zero. Our study may have biased patients toward initial screening adherence because the FIT was offered at no cost. It was not possible to determine which participants could have had the test reimbursed by their health insurance. However, our finding that most did not know whether their insurance covered any CRC screening lessens potential bias of the free FIT. The study period was too short to evaluate longer-term intervention effects, such as FIT rescreening. Finally, the response rate to our 3-month questionnaire was low. Our experience in rural communities suggests that a more personalized approach, such as a telephone interview or reminder call, might generate a higher follow-up response rate.\n\n【33】This study had several strengths. It was conducted in a part of Appalachia that has high CRC death rates by researchers who have extensive experience in this population. Community involvement ensured cultural and literacy appropriateness. Finally, the study used theory-based counseling delivered by telephone in the privacy and convenience of participants’ homes, which otherwise might not be available to this rural population .\n\n【34】Provider-recommended FIT and follow-up telephone counseling may increase CRC screening in Appalachia. A randomized trial is needed to determine the effect of the intervention in the larger Appalachian population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "55373d6b-3469-4977-a795-d960b99244f6", "title": "Past, Present, and Future of Japanese Encephalitis", "text": "【0】Past, Present, and Future of Japanese Encephalitis\n**To the Editor:** We are writing in response to the perspective on Japanese encephalitis (JE) by Erlanger et al. Growing awareness is encouraging, yet because JE is a largely neglected disease, information is often contradictory or not readily available. We would like to supplement the authors’ review with clarification on available vaccines and actions countries are taking to evaluate and control JE.\n\n【1】There is room for improvement or expansion on collecting and reporting JE surveillance data. However, as vaccine availability increases, many countries are eager to determine the impact of JE and to make informed decisions on immunization programs. For example, surveillance in Indonesia from 2005 through 2006 confirmed human cases throughout the country . In Cambodia, JE surveillance commenced in 2006, and an immunization program is being planned . Regional JE laboratory networks established by the World Health Organization are also helping countries gather this information by strengthening diagnostic capacity.\n\n【2】Cambodia plans to introduce the live, attenuated SA 14-14-2 vaccine from China’s Chengdu Institute of Biological Products. This vaccine has recently become internationally available and is increasingly replacing the inactivated, mouse brain–derived vaccine in Asia. A single dose of the SA 14-14-2 vaccine demonstrated 96% efficacy after 5 years , and the Institute’s commitment to an affordable price for developing countries has broadened accessibility . The government of India introduced the SA 14-14-2 vaccine in 2006, and nearly 50 million children 1–15 years of age have been reached through vaccination campaigns and routine immunization. The vaccine also is available through public programs or private markets in China, Nepal, South Korea, Sri Lanka, and Thailand.\n\n【3】JE vaccine candidates in late-stage development for children include a live, attenuated chimeric virus vaccine and an inactivated, Vero cell–derived vaccine, each based on the SA 14-14-2 virus strain. Additionally, 2 inactivated, Vero–cell derived vaccines based on the Beijing-1 strain are being developed in Japan .\n\n【4】New vaccine development, along with progress in surveillance and immunization, offers promise for sustainable control of clinical JE. To achieve this, global partners are working together to develop a strategic plan for JE control by 2015 .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a263a8bd-47ee-477f-8e4c-5a69b1aaf8e1", "title": "Multicenter EuroTravNet/GeoSentinel Study of Travel-related Infectious Diseases in Europe", "text": "【0】Multicenter EuroTravNet/GeoSentinel Study of Travel-related Infectious Diseases in Europe\nIn recent years, growth in international travel has been ≈6% per year, and similar trends are expected in the future . This growth has been strongly driven by travelers to newly popular destinations in Asia and the Pacific, Africa, and the Middle East . Approximately 80 million persons from industrialized nations travel to the developing world each year, and an estimated >200 million persons now reside outside their country of birth .\n\n【1】European travelers represent most of the international travelers, with Germany, United Kingdom, France, and Italy the leading countries of origin . With few exceptions, no European consensus exists on recommendations for travelers about risk assessment, malaria prophylaxis, or vaccinations. International references include the World Health Organization green book , which emphasizes risk assessment by rates of diseases in local populations; and the Centers for Disease Control and Prevention yellow book , which examines risk in the context of American travelers. Yet, whether these guidelines are appropriate in the European context is not known.\n\n【2】The intense international traffic between Europe and the rest of the world means that travelers have become a key element in the global spread of infectious diseases. These diseases may be introduced into domestic European populations and environments that are receptive to further spread. In 2003, severe acute respiratory syndrome (SARS) was introduced to France by 1 patient who returned from Vietnam . Malaria has recently reemerged in Italy and in France (Corsica), resulting from local transmission by anopheline mosquitoes that fed on travelers who had become infected with _Plasmodium vivax_ during travel . More recently, chikungunya virus (CHIKV) appeared as a paradigm of an infectious disease that rapidly became global as highly viremic travelers acted as efficient carriers of the virus . After CHIKV-infected persons in eastern Africa, Indian Ocean islands, India, and Southeast Asia, a new CHIKV variant reached Europe and affected local populations in Italy through 1 infected traveler (the index case-patient) and transmission by indigenous European mosquito vectors . In April 2009, an influenza A pandemic (H1N1) 2009 virus emerged in humans in North America and reached Europe soon after through returned travelers .\n\n【3】European physicians should be prepared to encounter and recognize infectious imported diseases. Facing the symptoms and syndromes in the ill returned traveler requires an understanding of the common etiologic agents encountered by different populations of travelers . Accurate epidemiologic data are needed about travel-associated infectious diseases in travelers returning to European countries. Some data on diseases among Europeans who traveled to developing countries recently have been published but were limited to 1 country of origin , a short period of study, specific diseases , a specific destination , or a certain type of traveler . A comprehensive multicenter comparison of the spectrum of illnesses among European travelers, including a broad sample of destinations, has been missing. Our objective in this study was to determine the epidemiology of travel-related infectious diseases in a large set of ill returned European travelers over a substantial period and to compare this with the epidemiology of disease in travelers from other industrialized countries outside Europe.\n\n【4】### Patients and Methods\n\n【5】##### Data Source\n\n【6】The GeoSentinel Surveillance Network consists of specialized travel/tropical medicine clinics on 6 continents where ill travelers are seen during or after traveling to a wide range of countries and where information about travelers is prospectively recorded  in a standardized format. To be eligible for inclusion in the GeoSentinel database, patients must have crossed an international border and have received medical attention at a GeoSentinel clinic for a presumed travel-related illness. We included western European patients who sought treatment at GeoSentinel sites after travel from March 1997 through November 2007. Persons were placed in 3 different categories: classic traveler, immigrant traveler, and expatriate traveler . Reasons for travel were classified as the following: tourism, business, research/education, missionary/volunteer work, or visiting friends and relatives (VFRs). Individual countries visited were grouped into 12 regions . Medical data included the final physician-assigned diagnosis, according to a standardized list of 556 possible individual diagnoses of infectious diseases that were also categorized under 21 broad syndromes as previously described . European patients were compared with all other ill non-European returned patients on the basis of information obtained from GeoSentinel sites in the United States, Canada, Australia, and New Zealand.\n\n【7】##### Statistical Analysis\n\n【8】Data were entered and managed in Microsoft Access (Microsoft Corp. Redmond, WA, USA). In our evaluation, proportionate morbidity refers to the number of cases of a specific diagnosis (or of a group of specific diagnosis within a syndrome group) compared with all cases of ill returned travelers seen at GeoSentinel clinics during the same period. Differences in proportions (qualitative variables) were tested by using Pearson χ 2  or Fisher exact tests. Analysis of variance or Kruskal-Wallis tests were used for quantitative variables. Because of the large numbers of statistical tests performed, a p value < 0.001 was considered significant.\n\n【9】Diagnosis, exposure regions, residence region, and travel types were analyzed by using multiple correspondence analysis (MCA) . MCA was performed by using the ANADEV freeware , developed by the Laboratory of Biomathematics, Faculty of Medicine of Marseille. Odds ratios (ORs) (European vs. non-European) by diagnosis were estimated by using logistic regression and adjusted for travel duration. All statistical tests were 2-sided. Percentages and odds ratios (with 95% confidence intervals), comparisons, and graphic analysis were carried out by using the R 2.8.1 environment .\n\n【10】### Results\n\n【11】##### Demographic and Travel Data\n\n【12】A total of 17,228 European patients were included: 13,913 (80.8%) classic travelers, 2,415 (14.0%) immigrant travelers, and 900 (5.2%) expatriate travelers . Demographic and travel data are presented in Table 2 . Most patients were seen as outpatients who sought treatment at the clinics <2 weeks post travel. Immigrant travelers sought markedly less pretravel advice and were more likely to be inpatients than other groups; differences were significant (p<0.0001). Furthermore, European patients’ main destination was Africa, followed by Asia; the proportion of patients returning from sub-Saharan Africa, Indian Ocean islands, and south-central Asia was higher in sites in Italy, France, and the United Kingdom, respectively . Non-Europeans (12,663 patients) had a lower proportion of immigrant travelers in the inpatient category, and non-European expatriates were younger, had a longer duration of travel, and sought pretravel advice more often (p<0.0001).\n\n【13】##### Final Etiologic Diagnosis\n\n【14】The proportionate morbidity of some broad syndromes or etiologic diagnoses was higher in patients travelling to specific regions. This was obvious for acute diarrhea in North Africa, south-central Asia, and the Middle East, and etiologic diagnosis such as _Campylobacter_ spp. in south-central and Southeast Asia, _Shigella_ spp. in North Africa and south-central Asia, _Giardia_ spp. in south-central Asia and South America and amebas in south-central Asia. Febrile systemic illnesses were more frequently reported from Indian Ocean islands, sub-Saharan Africa, and Oceania. _P. falciparum_ malaria was more frequently observed in travelers returning from Indian Ocean islands and sub-Saharan Africa, _P. vivax_ malaria in travelers from Oceania, Indian Ocean islands, and South America, and _P. ovale_ and _P. malaria_ e malaria in travellers from Indian Ocean islands and sub-Saharan Africa. Dengue was more frequently reported from Southeast Asia, chikungunya from Indian Ocean Islands, rickettsioses from sub-Saharan Africa, and salmonellosis from south-central Asia. Proportionate morbidity for dermatologic conditions was higher in Oceania, Southeast Asia, Central America, South America, and the Caribbean, including animal-related injuries requiring rabies postexposure prophylaxis (PEP) in North Africa, the Middle East, and Southeast Asia; larva migrans in Southeast Asia, the Caribbean, South America, and Central America; leishmaniasis in Central America and South America; and myasis in Central America. Finally, respiratory syndromes were more frequently reported in travelers returning from eastern Europe and northeastern Asia; genitourinary and sexually transmitted diseases (STDs) were more frequent in travelers from eastern Europe, Southeast Asia, and the Caribbean; schistosomiasis was more frequent in travellers from Africa and cerebromeningeal infections were more frequent in travelers from eastern Europe and North Africa) (p<0.0001) .\n\n【15】Also, the proportionate morbidity of some broad syndromes or etiologic diagnoses was higher in persons returning to specific European countries, as illustrated for falciparum malaria (Italy, France), dengue (United Kingdom), CHIKV infection (France), animal-related injuries requiring rabies PEP (France, United Kingdom) and cerebromeningeal infections (Italy) (p<0.0001). The proportionate morbidity was also higher in some categories of traveler, such as diarrhea and dermatologic diseases (in classic tourist travelers), falciparum malaria and genitourinary infections and STDs (immigrant travelers who were VFRs), and _P. vivax_ malaria (missionary/expatriate travelers) (p<0.0001). (For details)\n\n【16】MCA highlights the possibility of diagnosis in certain groups and shows an association between German patients, who are classic travelers (traveling for tourism to Southeast and south-central Asia) and a diagnosis of acute diarrhea. The MCA also showed that French, Swiss, or Italian patients who are classified as immigrant or expatriate travelers (VFRs or travelers for missionary purposes to Africa or Indian Ocean islands) are most likely to seek treatment for febrile illness .\n\n【17】Compared with the corresponding proportion of disease in non-European travelers, European classic tourist travelers had a lower proportionate morbidity (adjusted for travel duration) for certain diagnoses, such as schistosomiasis, cutaneous larva migrans, and animal-related injuries requiring rabies PEP, and a higher proportionate morbidity for genitourinary infections, STDs, and respiratory diseases when traveling to specific regions . Also, the _P. falciparum_ malaria proportionate morbidity in immigrant travelers (VRFs) after travel to Africa or the Indian Ocean islands was higher in Europeans compared with non-Europeans .\n\n【18】### Discussion\n\n【19】Despite the large number of patients investigated here in Europe for the assessment of travel-related illness, our work does not analyze all infectious illness in all returned patients. The results do not represent the broad spectrum of illness typically seen at nonspecialized primary care practice where mild or self-limited conditions would be found with higher frequency . The intake at sites reflects a mixed population of tertiary care and self-referred patients. Diagnoses that may be underrepresented include diseases of short incubation, many cases of which manifest during travel. However, GeoSentinel captures a sentinel sample of travelers; we have no reason to believe that cases we have not captured would have a different pattern of geographic acquisition than those in GeoSentinel. Also, we cannot relate our data collected on ill travelers to the total number of travelers to or from the area concerned. Because of this absence of denominator, incidence rates cannot be calculated or a numerical risk provided for travel to a particular destination. Absolute risk can be estimated only by monitoring cohorts prospectively, as was conducted in a few studies in the 1980s. Relatively small sample sizes and the limited number of destinations visited by travelers originating in 1 country are usually insufficient to elucidate destination-specific risk for individual diagnoses. Risk also could be calculated from the rate of illness in all travelers to each destination. However, capturing data on all ill travelers to just 1 destination, or even accurately ascertaining the denominator of all travelers to that destination, is not easily accomplished. No published studies have been able to describe this approach on a multicountry or worldwide basis.\n\n【20】However, given these caveats, the major strengths of our analysis are its focus on proportionate disease and the large numbers of patients in the database, which reduces the population-specific bias found in many smaller studies. Important published studies on several aspects of travel medicine have used the GeoSentinel database, now identified as a main source for the epidemiology of travel-related illness . We selected and discussed specific syndromes and their causes. The European aspect of our study is unique.\n\n【21】Most patients in our survey were outpatients. Ubiquitous or cosmopolitan infections involving the skin and the respiratory, gastrointestinal, and urinary tracts were found frequently in our study as were imported tropical diseases (although the specific tropical/cosmopolitan disease ratio cannot be calculated accurately because etiolgoc agents were not systematically identified or recorded). As previously emphasized, healthcare providers should not overlook such cosmopolitan infections when examining patients returning from the tropics . Overall, of 10 ill European returned travelers, 4 had a gastrointestinal disorder, 2 experienced a febrile systemic illness, 2 sought treatment for a dermatologic problem, and 1 had a respiratory disease. Acute diarrhea is the most common travel-associated disease , and we show here that some destinations are more frequently associated with some specific causes. Also, all categories of European travelers to North Africa, south-central Asia, and the Middle East (but particularly classic tourist travelers) should be targeted for pretravel advice regarding diarrhea risk and self-treatment . Furthermore, the importance of respiratory diseases in travelers has been exemplified with clusters of measles after importation , and more recently, the emergence and global spread of influenza A pandemic (H1N1) 2009 virus . Moreover, seasonal influenza, which affects 5%–15% of the world's population annually and has been considered the second most frequent vaccine-preventable infection in travelers, is probably underestimated in returned travelers .\n\n【22】We highlight here that malaria remains the most common specific diagnosis in ill returned patients who have a systemic febrile illness . _P. falciparum_ was the most commonly identified malaria species causing these infections, which mirrors situation in sub-Saharan Africa, a major source of malaria for European ill returned patients . The risk to travelers of acquiring malaria varies by destination. However, as shown here, the traveler profile also is an important determinant of malaria risk. _P. falciparum_ malaria is a rare diagnosis among native Germans traveling for tourism but it is a frequent diagnosis among immigrant travelers from Italy and France who visit friends and relatives in sub-Saharan Africa and the Indian Ocean islands. As shown here, immigrant travelers (VFRs) rarely seek pretravel advice, and they are known to comply poorly with malaria chemoprophylaxis . Therefore, immigrant travelers represent a major group at risk for imported malaria in Europe, and an improved approach to educate this population about risks and prophylaxis needs to be developed.\n\n【23】Dengue is now considered one of the major causes of fever in ill returned travelers, who even may serve as important sentinels of new outbreaks of dengue in dengue-endemic areas . Here, dengue virus was the second most commonly identified pathogen responsible for fever, particularly in patients who returned from Southeast Asia. The incidence of dengue has been considered to be higher than that of other so-called typical travel-related diseases, such as vaccine-preventable hepatitis A and typhoid fever . Because of rapid, intercontinental transportation, European physicians now encounter patients with arbovirus infections that have short incubation periods, such as dengue, and patients who are still viremic. These factors raise the possibility of introducing the virus to non–dengue-endemic areas where competent vectors are prevalent, as was demonstrated for CHIKV in 2007 .\n\n【24】Some aspects described here may also influence medical practice that affects returned patients. For example, enteric fever caused by _Salmonella_ infection was mainly observed in patients returning from south-central Asia, where multidrug resistance has been established and fluoroquinolone resistance is increasing .\n\n【25】Our results show the increasing importance of rickettsioses in ill returned travelers, particularly African tick-bite fever, which affects travelers to sub-Saharan Africa, especially those who go on safari and military personnel. These groups of travelers need to be singled out to receive advice on tick-bite prevention .\n\n【26】Our study also reinforces the view that dermatologic conditions are a leading cause of health problems in travelers . Pretravel advice should support the traveler’s use of impregnated bed nets and repellents, promote the practice of efficient clothes drying and ironing to prevent myasis, and discourage direct contact of skin with wet soil to prevent larva migrans transmission.\n\n【27】Notably, a larger numbers of patients seeking rabies PEP were observed in France and the United Kingdom, where GeoSentinel clinics include rabies treatment centers. This highlights the potential for rabid animal–related injury in travelers, particularly in North Africa and the Middle East .\n\n【28】German ill travelers were overrepresented in our collective database because of the historical development of GeoSentinel and the predominance of Germans among European travelers. Furthermore, each GeoSentinel site has specific characteristics, and some would be considered as sentinel sites for diseases in specific categories of travelers returning from particular countries. For example, at the site in Marseille, France, the French colonial past has a large effect on the profile of imported disease. The city has the largest community of inhabitants from the Comoros Islands, Indian Ocean, including first- to third-generation migrants. Immigrant travelers (VFRs) from the Comoros Islands are major importers of _P. falciparum_ malaria and were key to creating the initial alert about the CHIKV disease outbreak .\n\n【29】Differences in disease patterns between countries of origin may reflects national differences in the characteristics of the traveling population, the distribution of travel destinations, and referral and access to medical care. In addition, accommodation standards, eating habits, and other risk behavior at a given destination may reflect the national and cultural background of the traveler. These circumstances also apply when comparing European and non-European returned patients. However, although the non-European comparative group is heterogeneous, the diversity allows us to highlight some characteristics of European travel-related illnesses, such as the falciparum malaria within immigrant travelers (VFRs) in sub-Saharan Africa and the Indian Ocean islands. The economic situation of immigrants in Europe is unlikely to be as secure as that of second- or third-generation immigrants living in the United States, even if they have an easy access to the health system, including university hospitals in many cities. These factors, together with a higher likelihood of having severe imported diseases, such as malaria, may explain the high rate of immigrant travelers (VFRs) who were hospitalized. In Marseille, most of the immigrant travelers originating from Comoros claimed that some types of antimalarial chemoprophylaxis are too expensive for a whole family who travels every 2 years to visit friends and family.\n\n【30】European and non-Europeans ill returned travelers may also have a different code of conduct and behavior. For example, classic tourist travelers from Europe to Asia have a higher proportion of STDs than do other travelers. Again, our ill travelers probably do not reflect the whole population of travelers returning from the tropics with STDs because many probably consult their general practitioners first. However, a broad spectrum of STDs recently have been highlighted as common causes of health impairment among European travelers returning from the tropics, and Asia has destinations known for sex tourism .\n\n【31】Furthermore, depending on the destination, tourist travelers seem to be less frequently afflicted by diseases transmitted by contact of skin with fresh water or wet soil (schistosomiasis and larva migrans) and interaction with animals (animal-related injuries requiring rabies PEP); these facts suggest that they may be more compliant with travel health recommendations. We have no clear explanation, however, for the higher respiratory disease–related illnesses for European tourists traveling to Africa and America, but we note that SARS was imported to Europe in this way.\n\n【32】### Conclusions\n\n【33】Clinicians encountering returned patients have an essential role in recognizing, and communicating travel-associated public health risks . In this context, surveillance in European travelers that encompasses a wide range of sites in Europe, including some with local specificity, is crucial to determine the epidemiology of travel-associated disease, to detect alarming events, and, if required, to organize a rapid response . Our combined European data can be used as background evidence for the practice of travel medicine in Europe.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9e6e5d10-bec7-407e-9360-7624d56ed94c", "title": "Widespread Disease in Hedgehogs (Erinaceus europaeus) Caused by Toxigenic Corynebacterium ulcerans", "text": "【0】Widespread Disease in Hedgehogs (Erinaceus europaeus) Caused by Toxigenic Corynebacterium ulcerans\nHedgehogs across northern Belgium are currently being affected by an ulcerative skin disease. The purpose of this study was to identify the cause of these skin lesions.\n\n【1】### The Study\n\n【2】During May and June 2020, we tested 81 hedgehogs ( _Erinaceus europaeus_ ) that had ulcerative skin lesions and were provided by the public to 4 animal rescue centers across northern Belgium . Cases derived from 3 provinces in Flanders (East Flanders, Antwerp, and Limburg); total surface area of these provinces is 8,310 km 2  . All hedgehogs were individually housed, and we conducted sampling after euthanasia or natural death. We obtained 60 _Corynebacterium ulcerans_ isolates from ulcers or abscesses on the head or limbs from 53 of 81 investigated hedgehogs; all were adult males. For 6 animals, we obtained >1 isolate from different lesions.\n\n【3】Although _C. ulcerans_ was isolated most often, lesions yielded abundant, polybacterial growth . We showed by systematic postmortem examinations that 9 animals had a good body condition generally but had multiple cutaneous ulcers on the head and limbs. Histopathologic analysis of skin of these animals showed subacute, extensive, ulcerative dermatitis and suppurative exudation and crusting. Inflammation sometimes extended to the subcutis and even to underlying skeletal muscles.\n\n【4】In some instances, we observed nodular inflammation consisting of central necrosis admixed with degenerated neutrophils and bordered by a small rim of macrophages (abscess formation) and fistulation. We observed intralesional microcolonies of gram-positive bacilli. We subjected organs that showed macroscopic abnormalities to histopathologic analysis. Four animals had interstitial pneumonia, 1 animal had ascending hepatitis, and 1 animal had fibrinosuppurative epicarditis and intralesional gram-positive bacteria.\n\n【5】Despite presence of parasites related to skin disease (fly maggots; myiasis, n = 5; _Sarcoptes scabiei_ , n = 1; and _Caparinia_ spp. (n = 1) and pathogens related to systemic disease (herpesvirus, n = 2  and lungworms; _Crenosoma striatum_ , n = 2), we found no consistent evidence for other causes of primary disease. Although evidence is insufficient to conclusively attribute the observed lesions to _C. ulcerans_ , its widespread and high-level occurrence in diseased male hedgehogs is a serious concern, given frequent exposure of humans to hedgehogs and because _C. ulcerans_ is the predominant cause of human diphtheria in many countries in Europe .\n\n【6】_C. ulcerans_ isolates from hedgehogs belong to several clusters. We identified 56 isolates of _C. ulcerans_ to the species level by using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry  and sequencing of the _rpoB_ gene . We also typed isolates by analysis of infrared spectra . Isolates grouped with _C. ulcerans_ strains from humans and other animals (hedgehogs and red foxes \\[ _Vulpes vulpes_ \\] from Germany)  and clustered in 3 sublineages . The high diversity is similar to that reported by Berger et al. and results argue against nocosomial infections and emergence and spread of a single _C. ulcerans_ clone in the hedgehog population in Flanders. Instead, the high diversity of the isolates suggests _C. ulcerans_ endemicity in the hedgehog population.\n\n【7】We found limited acquired antimicrobial resistance in _C. ulcerans_ isolates from hedgehogs. We compiled MIC data for all _C. ulcerans_ isolates . Acquired resistance against enrofloxacin was detected in 4 isolates.\n\n【8】Most _C. ulcerans_ isolates from hedgehogs produce toxins. We evaluated presence and expression of toxins by detection of the diphtheria toxin gene ( _toxE_ ) by using a duplex PCR  and the Elek test . Results showed a positive result for this gene in 50/56 isolates by PCR and positive (26/56 isolates) or weak positive (16/56 isolates) results by Elek test. One animal was positive for the _toxE_ gene in 1 location ( _C. ulcerans_ isolate from a head lesion) and negative for the gene in another location (isolate from a foot lesion).\n\n【9】Although diphtheria vaccination coverage in humans is high in Belgium, since 2010, sporadic cases (14 cases during 2010–2017) of infection by toxigenic corynebacteria have occurred . Because presence of the _toxE_ gene and toxin production are associated with pathogenicity in humans, these results suggest a zoonotic potential of most hedgehog-derived _C. ulcerans_ isolates.\n\n【10】### Conclusions\n\n【11】Hedgehogs are mammals that are abundant in Europe and are frequently observed in nature reserves and urbanized areas. Because of their defensive behavior, sick animals are easily brought to animal rescue centers by the public, as testified by the large number of animals we examined in a short time frame during this study. The nature of their spiny defense promotes breaching and inoculating of the human epidermis with bacteria during handling. Other potential routes of transmission might include bite wounds or contact with the contaminated environment of the hedgehogs. Several potentially zoonotic or anthroponotic bacterial species, including _C. rouxii_ and _Streptococcus pyogenes_ , are associated with ulcerative lesions in diseased hedgehogs .\n\n【12】Although Gower et al. showed that the major risk factor for _C. ulcerans_ infection in humans is exposure to domestic animals (e.g. dogs, cats), widespread occurrence of toxigenic _C. ulcerans_ in most diseased hedgehogs across Flanders should prompt authorities to alert all stakeholders, including members of the public and staff at animal rescue centers, to take precautionary measures when handling hedgehogs. Although vaccination against _C. diphtheriae_ protects against _C. ulcerans_ disease, exposure to _C. ulcerans_ from susceptible persons might result in severe disease . Recommendations should include wearing protective gloves and cleaning and disinfecting hands and fomites after contact with a hedgehog, as well as vaccination of persons who are frequently exposed to hedgehogs.\n\n【13】Treatment of infections with pyogenic coryneform bacteria in animals is challenging, and the 4 rescue centers involved in this study reported poor treatment success. Results of antimicrobial susceptibility testing suggest that this finding is not caused by acquired antimicrobial drug resistance but probably by insufficiently high antimicrobial drug concentrations reaching the _C. ulcerans_ bacteria inside pus. Therefore, debrideing the lesions should be included in any treatment. Euthanasia should be considered for severe cases.\n\n【14】Emergence of _C. ulcerans_ infection in hedgehogs is consistent with an increasing number of reports of _C. ulcerans_ infections in wildlife across Europe and warrants attention across the continent . Reports dating back from the 1950s and the presence of several distantly related clusters of _C. ulcerans_ in this study argue against a recent introduction of this pathogen in wildlife populations in Europe and favors the hypothesis that the observed and previously unreported high numbers of diseased hedgehogs result from pathogen emergence from a disease-endemic state. Although most wild animals affected with _C. ulcerans_ have systemic infections , in our study, the manifestations of cutaneous disease dominated.\n\n【15】The finding that only male hedgehogs had this disease and that lesions are found mostly on body parts not covered with spines suggests the _C. ulcerans_ infections might be opportunistic infections of wounds, arising from male-specific behavior during the mating season. Bite wounds are well known to be susceptible to infection with opportunistic pathogens that are part of the oral microbiota . Strain typing suggests that hedgehogs are a major reservoir of highly diverse _C. ulcerans_ isolates. Active surveillance should elucidate the magnitude of this reservoir in healthy hedgehogs and the impact on the population level. Until the mechanisms underpinning the observed emergence of this potentially zoonotic wildlife disease from its disease-endemic state can be clarified, persons handling hedgehogs should take precautions to prevent possible transmission of _C. ulcerans_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e8d4df33-7d49-4c59-8ce6-9f390d4a4071", "title": "Bartonella Species in Blood of Immunocompetent Persons with Animal and Arthropod Contact", "text": "【0】Bartonella Species in Blood of Immunocompetent Persons with Animal and Arthropod Contact\nAttempts to isolate _Bartonella_ sp. from immunocompetent persons with serologic, pathologic, or molecular evidence of infection are often unsuccessful; several investigators have indicated that _Bartonella_ isolation methods need to be improved . By combining PCR and pre-enrichment culture, we detected _B. henselae_ and _B. vinsonii_ subspecies _berkhoffii_ infection in the blood of immunocompetent persons who had arthropod and occupational animal exposure.\n\n【1】### The Study\n\n【2】From November 2004 through June 2005, blood and serum samples from 42 persons were tested, and 14 completed a questionnaire, approved by the North Carolina State University Institutional Review Board. Age, sex, animal contact, history of bites, environment, outdoor activity, arthropod contact, travel, and medical history were surveyed. Bacterial isolation, PCR amplification, and cloning were performed by using previously described methods . Each blood sample was tested by PCR after direct DNA extraction, pre-enrichment culture for at least 7 days, and subculture onto a blood agar plate . An uninoculated, pre-enrichment culture was processed simultaneously as a control. Methods used for DNA extraction and conventional and real-time PCR targeting of the _Bartonella_ 16S-23S intergenic spacer (ITS) region and heme-binding protein (Pap31) gene have been described . Conventional PCR amplicons were cloned with the pGEM-T Easy Vector System (Promega, Madison, WI, USA); sequencing was performed by Davis Sequencing, Inc. (Davis, CA, USA). Sequences were aligned and compared with GenBank sequences with AlignX software (Vector NTI Suite 6.0 (InforMax, Inc. Bethesda, MD, USA) . _B. vinsonii_ subsp. _berkhoffii_ , _B. henselae_ , and _B. quintana_ antibodies were determined by using a modification of a previously described immunofluorescence antibody assay (IFA) procedure .\n\n【3】Study participants included 12 women and 2 men, ranging in age from 30 to 53 years; all of them reported occupational animal contact for >10 years . Most had daily contact with cats ( 13 persons) and dogs ( 12 persons). All participants reported animal bites or scratches (primarily from cats) and arthropod exposure, including fleas, ticks, biting flies, mosquitoes, lice, mites, or chiggers. All participants reported intermittent or chronic clinical symptoms, including fatigue, arthralgia, myalgia, headache, memory loss, ataxia, and paresthesia . Illness was most frequently mild to moderate in severity, with a waxing and waning course, and all but 2 persons could perform occupational activities. Of the 14 participants, 9 had been evaluated by a cardiologist, 8 each by an infectious disease physician or a neurologist, and 5 each by an internist or a rheumatologist. Eleven participants had received antimicrobial drugs.\n\n【4】When reciprocal titers of \\> 64 were used, 8 persons were seroreactive to _Bartonella_ antigens . _B. henselae_ or _B. vinsonii_ subsp. _berkhoffii_ was detected or isolated from all 14 participants. At the time of initial testing, _Bartonella_ DNA was amplified directly from 3 blood samples, from 7 pre-enrichment liquid cultures, and from 4 subculture isolates . For 5 persons, results of PCR and culture of initial samples were negative. Overall, _Bartonella_ DNA was amplified from 11 (28%) of 40 extracted blood samples, 13 (33%) of 40 pre-enrichment cultures, and 5 isolates. For 7 persons, _B. henselae_ DNA was amplified at multiple time points. _Bartonella_ DNA was never amplified from any PCR control or uninoculated culture control.\n\n【5】By using the ITS target region, 2 distinct _B. henselae_ ITS and Pap31 strains were sequenced, _B. henselae_ Houston I (HI) (GenBank NC-005956) and _B. henselae_ San Antonio 2 (SA2) (GenBank AF369529). Within the noncoding ITS region, _B. henselae_ SA2 strains have a 30-bp insertion (ATT GCT TCT AAA AAG ATT GCT TCT AAA AAG) located 518 bases downstream from the 16S gene. Only _B. vinsonii_ subsp. _berkhoffii_ types I and II were detected .\n\n【6】### Conclusions\n\n【7】Persistent human infection with _B. bacilliformis_ and _B. quintana_ has been previously documented, whereas infection with _B. henselae_ (cat-scratch disease \\[CSD\\]) is generally considered self-limiting . Recently, _B. henselae_ DNA was amplified from the blood of a child 4 months after CSD diagnosis . Our study indicates that _B. henselae_ and _B. vinsonii_ subsp. _berkhoffii_ can induce occult infection in immunocompetent persons and that detection can be enhanced by combining PCR with pre-enrichment culture. Considering only the results from initial blood samples, PCR detected _Bartonella_ DNA in 3 samples, all of which were subsequently PCR positive by subculture or enrichment culture. In samples from 5 persons, pre-enrichment was necessary, and in 5 other persons, sequential sampling was necessary to detect _Bartonella_ infection. Intermittent bacteremia, as occurs in _B. henselae_ –infected cats , antimicrobial drug administration, low bacterial copy numbers, and low inoculum volume (1 mL) may have contributed to intermittent detection or inability to isolate _Bartonella_ spp. from some participant samples. Although our approach is an improvement over historical isolation approaches, our results emphasize ongoing limitations associated with the detection of _Bartonella_ infection. Obtaining stable _Bartonella_ subcultures (n = 5 in this study) has proven problematic for other specialized laboratories that routinely culture for _Bartonella_ spp. To our knowledge, the _B. vinsonii_ subsp. _berkhoffii_ type II isolate described in our study is the only type II human isolate reported to date . Various combinations of _B. henselae_ and _B. vinsonii_ subsp. _berkhoffii_ strain types were detected in the same blood sample or sequential blood samples. The coexistence of _B. henselae_ genetic variants has been described among primary patient isolates, which suggests that multiple genotypes may emerge within the same person .\n\n【8】Overall, 57% of persons tested were seroreactive to 1 or all 3 _Bartonella_ test antigens. Previous reports from the United States identified a _B. henselae_ seroprevalence of 3% in healthy blood donors and a cumulative seroprevalence of 7.1% to both _B. henselae_ and _B. quintana_ antigens in veterinary professionals . In this and other studies, serologic test results did not correlate with PCR amplification or isolation results. Antigenic variability among _B. henselae_ test strains can cause false-negative IFA results in persons with suspected CSD. Also _B. henselae_ , _B. quintana_ , or _B. elizabethae_ antibodies were not detected in some persons with DNA evidence of active infection .\n\n【9】Animal contact, often to a wide spectrum of domestic and wild animal species, is an obvious consequence of the daily activities of the study population, which is biased by veterinary occupational exposure and by self-selection (volunteer bias). Cats are considered the primary reservoir host for _B. henselae_ , whereas coyotes and foxes are considered reservoir hosts for _B. vinsonii_ subsp. _berkhoffii_ . Detection of _B. vinsonii_ subsp. _berkhoffii_ in 4 of 5 Californian participants could be related to the high prevalence of bacteremic coyotes in this region as well as to the potential transmission by a tick vector . All 14 participants reported frequent arthropod exposure. Although _Bartonella_ spp.transmission by ticks has not been proven, several recent studies have identified _Bartonella_ DNA in questing ticks, ticks attached to animals, and ticks attached to humans .\n\n【10】Despite reporting chronic or episodic illness, most participants continued to effectively maintain daily professional and personal activities. The symptoms described in the study patients are very similar to those described in a community and hospital-based surveillance study of CSD patients, in whom CSD-associated arthropathy was an uncommon chronic syndrome affecting mostly young and middle-age women . Our study was initiated to investigate the feasibility of combining PCR with pre-enrichment culture. Prospective studies, with appropriate controls, are needed to characterize the prevalence and clinical relevance of persistent _Bartonella_ infection in immunocompetent persons.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1efcf35c-6ece-45d6-b991-67962ec3f1e0", "title": "Increasing Macrolide and Fluoroquinolone Resistance in Mycoplasma genitalium", "text": "【0】Increasing Macrolide and Fluoroquinolone Resistance in Mycoplasma genitalium\n_Mycoplasma genitalium_ infection is a major cause of urethritis in men and is associated with cervicitis, pelvic inflammatory disease, preterm birth, and spontaneous abortion in women . In the United States, Australia, and Europe, the recommended first-line treatment for _M. genitalium_ infection is the macrolide azithromycin. However, a recent meta-analysis documented a rapid decline in its efficacy, from 85% before 2009 to 67% after 2009; the highest levels of resistance were in the Asia-Pacific region . The second-line therapy recommended by the US Centers for Disease Control and Prevention  is the fluoroquinolone moxifloxacin. Quinolones target the DNA gyrase (comprising GyrA and GyrB) and topoisomerase IV (ParC and ParE). Quinolone binding involves serine at position 83 ( _Escherichia coli_ GyrA numbering) and the acidic amino acid 4 positions away (D87 or E87)  . Mutations affecting these residues or surrounding sequence (the quinolone resistance-determining region, QRDR) may confer resistance .\n\n【1】Moxifloxacin treatment failure is being increasingly reported, particularly in the Asia-Pacific region , along with increasing detection rates of resistance mutations . Although several studies have reported the prevalence of QRDR mutations in _M. genitalium_ , most mutations have not been linked with treatment outcomes. Our aims with this study were to report the prevalence of mutations in the _parC_ and _gyrA_ genes in patients with _M. genitalium_ infection, to correlate specific mutations with moxifloxacin outcomes, and to determine the prevalence of dual (macrolide/fluoroquinolone) resistance.\n\n【2】### The Study\n\n【3】From July 1, 2012, through June 30, 2013, samples were collected from consecutive _M. genitalium–_ infected participants at the Melbourne Sexual Health Centre in Australia . Detection of _M. genitalium_ , load quantitation, and sequence analyses were performed as described previously . Overall, 155 patients (112 men, 43 women) with PCR-confirmed _M. genitalium_ infection were recruited, representing 90% of patients with infections diagnosed at the Centre over the study period. We obtained adequate samples from 140 of the 155 patients to generate baseline _parC_ and _gyrA_ gene sequences; these 140 formed the study group.\n\n【4】Patients were initially given a single dose of 1 g azithromycin. The 54 for whom this treatment failed (positive by PCR test-of-cure at day 28 or persistent symptoms before day 28, with no identified reinfection risk) were given moxifloxacin (400 mg/d for 10 d). The 6 for whom moxifloxacin treatment failed were given pristinamycin (1 g 4×/d for 10 d). This study was approved by The Alfred Hospital Ethics Committee , and informed consent was obtained from patients.\n\n【5】In pretreatment specimens, various single-nucleotide polymorphisms (SNPs) were observed in the _parC_ and _gyrA_ QRDR . Of the 19 (13.6%) of 140 samples with ParC substitutions, 16 had S83 mutations (14 S83I, 2 S83R) and 3 had D87N substitutions.\n\n【6】We found a significant association between detection of ParC S83 mutations and treatment failure. _M. genitalium_ from all 6 patients for whom moxifloxacin failed but from only 3 of the 48 patients for whom moxifloxacin was effective had the ParC S83 mutation (p<0.0001 by Fisher exact test) . The 3 infections successfully treated despite a change at ParC S83 are of interest. For these patients, low bacterial load may have contributed to therapeutic success , led to spontaneous clearance, or resulted in false-negative follow-up PCR . However, in contrast, treatment failed for 1 patient with a low anal load of _M. genitalium_ and S83 change. Similar to the findings for this study, in the parent cohort of 155 patients, organism load influenced apparent azithromycin cure; 7% of infections carrying markers of azithromycin resistance were cured by azithromycin, and organism load was significantly lower than that among those with resistant infections for whom azithromycin treatment failed .\n\n【7】The prevalence of S83 changes in this study is higher than that detected in a study at Sydney Sexual Health Centre (Sydney, New South Wales, Australia) (8.4%, n = 143) . Studies in Japan reported prevalence ranging from 3.6% (n = 28) to 29.4% (n = 51) and 36.8% (n = 19) , although 1 study involved a cohort at higher risk (female sex workers). A low prevalence of S83 mutation has been observed in Europe (1.5% in France, 5% in England and Germany) . This mutation has been associated with moxifloxacin failure in 3/3 cases in the Sydney-based study .\n\n【8】SNPs that changed the ParC acidic residue (D87) were rare (2.1%) and because of low numbers could not be associated with treatment outcomes. Other studies found higher frequency of this change (3.5%–7.1%) ; authors of 1 study reported an association with levofloxacin failure .\n\n【9】_M. genitalium_ GyrA lacks the S83 residue common to GyrA of other bacteria, having instead a methionine at the equivalent position (M95). This enzyme is therefore probably partially resistant to quinolones. GyrA changes (at M95 or D99) occurred at a frequency of 5.0% (7/140) but could not be correlated with treatment outcome because they occurred concurrently with S83 changes in ParC. Previously, a GyrA M95I change was associated with _M. genitalium_ treatment failure in 1 patient .\n\n【10】Patients who received moxifloxacin were followed up with a PCR test-of-cure at 14 and 28 days. For the 6 for whom treatment failed, the mutation profiles in follow-up specimens were unchanged from the initial premoxifloxacin sequence, suggesting lack of resistance selection in vivo after moxifloxacin.\n\n【11】A total of 60 (42.9%) of the 140 pretreatment samples had macrolide-resistance mutations . Both macrolide and _parC_ fluoroquinolone mutations at S83 or D87 were present in 12 (8.6%) of the 140 samples. Prevalence of fluoroquinolone resistance markers was higher in samples with (20%, 12/60) than without (8.8%, 7/80) macrolide-resistance mutations, although this difference did not reach statistical significance (p = 0.08). This finding suggests that successive treatment failures with first-line, then second-line, antimicrobial drugs are generating strains resistant to 2 classes of drugs. Previous studies found lower levels of combined macrolide and fluoroquinolone mutations in men attending a urology clinic (3/51, 5.9%)  and higher levels in a high-risk population (female sex workers; 4/16, 25%) .\n\n【12】This study has limitations. The resistance profiles for the infecting strains of _M. genitalium_ were not tested in in vitro culture. There may be other unknown changes in the genome that confer resistance to the drugs of interest. In addition, the resistance levels reported are probably underestimates because samples were collected in 2012–2013 and levels have probably risen since then .\n\n【13】### Conclusions\n\n【14】We found high frequency of ParC S83 changes associated with fluoroquinolone resistance in a sexually transmitted infection clinic in urban Australia; these changes were associated with moxifloxacin failure. The high level of dual markers for macrolide/fluoroquinolone resistance suggests successive treatment failure after sequential monotherapy leading to the serious outcome that ≈10% of _M. genitalium_ infections are not treatable with recommended or readily available antimicrobial drugs. In the absence of alternatives, treatment with pristinamycin cured all 6 patients with dual-class resistance infections (G.L. Murray et al. unpub data).\n\n【15】This study highlights the urgent need for antimicrobial drug resistance surveillance and the value of diagnostic assays that report the presence of resistance markers to optimize treatment. Our results suggest that it is time to reconsider the indications for azithromycin and invest in trials of different available as well as novel classes of antimicrobial drugs for _M. genitalium_ treatment. They also raise serious concerns about sequential use of monotherapy and the need to evaluate combination therapies as we enter a new era of untreatable sexually transmitted infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eedb30bd-d869-40fb-a482-48efb8734915", "title": "Subcutaneous Infection with Dirofilaria spp. Nematode in Human, France", "text": "【0】Subcutaneous Infection with Dirofilaria spp. Nematode in Human, France\n**To the Editor:** The article by Foissac et al. titled Subcutaneous infection with _Dirofilaria immitis_ nematode in human, France  presents an interesting and challenging diagnostic dilemma. The paper described, but did not illustrate, the worm as having a strongly ridged external surface of the cuticle—a feature known not to exist on _Dirofilaria immitis_ , the dog heartworm. However, molecular sequencing of the specimen demonstrated much closer similarity to _D. immitis_ than to _D. repens_ , the most common cause of zoonotic subcutaneous dirofilariasis infection in Europe.\n\n【1】Well-described morphologic features of parasites, including in tissue sections, have long been the standard for diagnosis. More recently, molecular diagnostics have helped in many of these difficult cases. However, in some cases, the morphology and molecular diagnosis are discordant. On the basis of the data in the article, the worm does not seem to represent _D. repens_ . A more likely possibility is some other species for which no sequences are yet available for comparison. In such a worm, the regions sequenced must be similar to _D. immitis_ , and distinct from _D. repens_ , to achieve the observed results.\n\n【2】\\[\\[AA:F11:PREVIEWHTML\\]\\]\n\n【3】When one encounters a case such as this, where well-validated morphologic features  are contradictory to the molecular analysis, one must exercise caution in arriving at a final diagnosis. One disadvantage of morphologic and molecular diagnostics is an absence of information on poorly described and characterized pathogens or new pathogens that have yet to be identified. No good algorithm exists to resolve these conflicts other than to explore all possibilities. The diagnosis in the described case is probably best left as a _Dirofilaria_ species of the _Dirofilaria_ ( _Nochtiella_ ) type, members of which exhibit marked cuticular ridging, and not _D._ ( _Dirofilaria_ ) _immitis_ type, members of which have as a feature an absence of cuticular ridging.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fefee3ee-b0b9-4990-8198-5cb01012b2d7", "title": "Collaborative Drug Therapy Management: Case Studies of Three Community-Based Models of Care", "text": "【0】Collaborative Drug Therapy Management: Case Studies of Three Community-Based Models of Care\nAbstract\n--------\n\n【1】Collaborative drug therapy management agreements are a strategy for expanding the role of pharmacists in team-based care with other providers. However, these agreements have not been widely implemented. This study describes the features of existing provider–pharmacist collaborative drug therapy management practices and identifies the facilitators and barriers to implementing such services in community settings. We conducted in-depth, qualitative interviews in 2012 in a federally qualified health center, an independent pharmacy, and a retail pharmacy chain. Facilitators included 1) ensuring pharmacists were adequately trained; 2) obtaining stakeholder (eg, physician) buy-in; and 3) leveraging academic partners. Barriers included 1) lack of pharmacist compensation; 2) hesitation among providers to trust pharmacists; 3) lack of time and resources; and 4) existing informal collaborations that resulted in reduced interest in formal agreements. The models described in this study could be used to strengthen clinical–community linkages through team-based care, particularly for chronic disease prevention and management.\n\n【2】Introduction\n------------\n\n【3】In collaborative drug therapy management (CDTM), qualified pharmacists working in the context of a defined protocol are permitted to assume professional responsibility for performing a full scope of services: assessing patients; ordering drug therapy–related laboratory tests; administering drugs; and selecting, initiating, monitoring, continuing, and adjusting drug regimens . Authority for CDTM is generally incorporated into a state’s pharmacy practice act in the sections describing pharmacists’ scope of practice.\n\n【4】Pharmacist–provider collaborative practice agreements (CPAs), such as CDTM, are a strategy for expanding the pharmacist’s role in team-based care with other providers and improving health outcomes. CPAs can link patient care provided in traditional clinical settings with pharmacist care in community-based settings. CPAs emerged in the 1960s  and are now legally enabled in most states; however, the range of services authorized under each state’s practice act varies .\n\n【5】Pharmacist patient care services provided through CPAs have been shown to improve patient outcomes for diabetes, hypertension, anticoagulation, and other chronic diseases . The 2014 Community Preventive Services Task Force (Task Force) recently issued recommendations showing strong evidence for team-based care involving pharmacists and nurses to improve hypertension control and other chronic disease risk factors . Despite the noted benefits, pharmacists (particularly in community settings) are not routinely providing CDTM , although they may be collaborating informally with physicians to make drug therapy recommendations. One increasingly common opportunity for this informal collaboration is the use of medication therapy management (MTM), a required benefit for select Medicare Part D beneficiaries . In MTM as most commonly defined, a pharmacist reviews a patient’s medication regimen and must suggest changes to the prescribing physician for approval, rather than make any changes independently. This activity is permitted in any pharmacist’s scope of practice. CDTM takes this relationship a step further by enabling the pharmacist to make independent drug therapy changes under a protocol that may enhance the efficiency of the pharmacist and health care delivery.\n\n【6】There are more than 60,000 community-based pharmacies in retail settings (supermarkets, chain drug stores, and independent pharmacies), and approximately 39% of federally qualified health centers (FQHCs) have onsite pharmacies across the United States . The Centers for Disease Control and Prevention (CDC) funded this study to understand how CPAs, such as CDTM for hypertension management, are implemented in community pharmacies and to explore ways for more pharmacists to provide CDTM. The objectives of this study were to understand how CDTM practices were implemented in 3 community settings and to identify common and unique facilitators and barriers to implementing CDTM.\n\n【7】Methods\n-------\n\n【8】### Case study site selection and inclusion criteria\n\n【9】We selected sites from states with scope of practice laws authorizing pharmacists to perform CDTM in any practice setting for a broad array of health conditions. We consulted with experts in MTM, CDTM, and collaborative models in pharmacy settings and reviewed the literature  to understand how CPAs might be implemented in different practice settings and to identify potential study sites. Three criteria emerged as the primary considerations for final site selection: duration, scope, and reach. Therefore, we sought a variety of sites, including some that were newly implemented (vs experienced), some that offered limited services (vs the full scope of CDTM services authorized by law), and some that reached a broad and diverse population (vs a more limited homogenous population).\n\n【10】We identified community pharmacies on the basis of expert recommendations and our literature review and contacted 10 sites. Each site received an email explaining the study and an invitation to participate. We excluded 5 sites for these reasons: CDTM services were not implemented because of the time and resources required, the contractual language of CPAs was considered prohibitive, or CDTM reimbursement mechanisms were lacking; 1 site declined; and another site did not respond. Ultimately, we used a combination of the criteria to select 3 sites that had CPAs in place: El Rio Community Health Center, Osterhaus Pharmacy, and Kerr Drug.\n\n【11】The research protocol was approved by the ICF International (ICF) institutional review board. CDC and Purdue University deferred to ICF.\n\n【12】### Recruitment\n\n【13】We worked with each site’s point of contact to identify potential key informants. Participants had to provide signed informed consent, be aged 18 years or older, and be comfortable speaking in English.\n\n【14】A purposive sample of potential informants at each site was recruited by email. To meet the study objectives, 9 key informants were recruited to share experiences on CDTM implementation. Three informants were recruited from each site: a pharmacist, a physician, and 1 other (eg, a pharmacy resident or administrator). Participants were not remunerated.\n\n【15】### Data collection\n\n【16】Case studies included key informant interviews and onsite observations. Before the site visits, interviewers completed a half-day training. Site visits took place during May through July 2012. Two study team members traveled to each site to conduct the interviews during a day-and-a-half visit. Because of the inability to schedule an in-person visit, we conducted telephone interviews for the Kerr Drug site. Interviews lasted about an hour and were audio recorded with consent.\n\n【17】A semistructured interview guide (available on request from the authors) was developed to focus on 6 topics: 1) CPA/CDTM policy implementation (eg, describe your CDTM policy and related guidelines for compliance with state law), 2) stakeholders, 3) effects of CPA/CDTMs on practice, 4) evaluation, 5) reimbursement, and 6) lessons learned and recommendations for implementing CPA/CDTM.\n\n【18】### Description of case study sites\n\n【19】#### El Rio Community Health Center (El Rio)\n\n【20】El Rio, an FQHC, is the largest provider of medical and dental services to uninsured and Medicaid-covered populations in Pima County, Arizona. Of 76,190 patients seen in 2011, 76% had an income at or below the federal poverty line, 48% received Medicaid, 28% were uninsured, 13% had private insurance, and 8% were Medicare recipients. El Rio serves a large Hispanic and Native American patient population, many with diabetes. El Rio has extensive experience implementing CDTM; its on-site pharmacists began entering into CPAs with El Rio providers in 2000. In 2012, approximately 800 patients received CDTM services, mostly for diabetes. The CDTM protocols also cover hypertension, hyperlipidemia, asthma, and other conditions. CPAs authorize pharmacists to assess patients, review medication regimens, adjust medications in approved drug classes, and perform specified examinations (eg, foot examinations) as well as patient drug reviews for medications that require monitoring, such as anticoagulation therapy. El Rio bills for CDTM services through Medicare Part D and for diabetes CDTM services as an accredited site for diabetes self-management training.\n\n【21】#### Osterhaus Pharmacy\n\n【22】Osterhaus Pharmacy is an independent, community pharmacy in the largely white (95%) rural community of Maquoketa, Iowa. Osterhaus Pharmacy serves approximately 6,500 patients annually, offering various patient care services. Approximately 60% of patients have diabetes, hypertension, and/or hyperlipidemia. In 2012, about 13% of patients received Medicaid and 42% were Medicare beneficiaries. Since 2000, the pharmacy has implemented limited elements of CDTM with a family medicine and emergency medicine group practice: a medication substitution and an immunization protocol. Although Osterhaus Pharmacy has extensive CDTM experience, this model is less comprehensive than authorized by Iowa law. For example, the CPAs operate under limited conditions, such as medication substitution based on patient insurance or influenza vaccination criteria, or the CPAs limit the scope of their services (ie, preclude modification of drug dosages based on laboratory or physical findings). In addition, the pharmacy and medical practice collaborate informally by providing MTM and other services.\n\n【23】#### Kerr Drug\n\n【24】Kerr Drug, a regional pharmacy chain, operates several retail stores in North Carolina that serve a varied patient population. Since 2007, Kerr Drug has provided MTM services to Medicare Part D beneficiaries. Although state law allows clinical pharmacist practitioners (CPP) (a distinct pharmacist credential that designates pharmacists practicing under a CPA) to perform a broad array of CDTM services, Kerr Drug recently implemented limited elements of CDTM. The Chapel Hill location completed a pharmacogenetic feasibility study involving a CPP arrangement with the primary investigator for the pharmacist to order genetic tests from a laboratory. To perform any other CDTM services, the CPP would need to collaborate with each study participant’s primary care provider. Kerr Drug’s feasibility study was funded by the University of North Carolina and grants.\n\n【25】### Analysis approach\n\n【26】Data analysis involved audio recorded debriefs by interviewers within a week of each site visit and a review of the recordings by 2 site visitors. Excel software (Microsoft Corp) was used to select salient quotations. Thematic categories guided by the case study questions (ie, key features, barriers, facilitators, and lessons learned) were selected and discussed for and across each site. Results were synthesized and sent to at least 1 key informant from each site for feedback.\n\n【27】Results\n-------\n\n【28】### Key features of CDTM policy implementation\n\n【29】The elements of CDTM used at each site varied .\n\n【30】### Key barriers to CDTM policy implementation\n\n【31】Key barriers to CDTM policy implementation raised by pharmacists and physicians across the 3 sites included a lack of reimbursement mechanisms for CDTM services, difficulty establishing trusting relationships with providers, and the time and resources needed to perform CDTM patient care services . Respondents reported that a key reason for not entering into CPAs was that pharmacists were not recognized as providers under federal law and, therefore, unable to bill for services. Physicians reported that many of their physician colleagues were initially hesitant to relinquish control of their patients’ drug therapy to pharmacists, particularly if they do not practice at the same location. However, it was reported that the distrust wanes over time. In addition, each state has different requirements, such as residency training or continuing education, for pharmacists to be eligible to engage in CDTM . The pharmacists reported that the application costs for various certifications and the time needed to devote to continuing education requirements can be burdensome. Finally, Kerr Drug and Osterhaus Pharmacy pharmacists achieved a great deal of collaborative patient care by making therapeutic recommendations (ie, MTM) without entering into CPAs. Their reasons for preferring informal collaborations to CPAs included the time and logistics required to create CPAs and the limited scope of diseases or medications that may be included in a CDTM protocol. In summary, compensation for these services was identified as a barrier, so in some cases, the disadvantages of the required time and logistics for CPAs outweighed the perceived benefits.\n\n【32】### Key facilitators to CDTM policy implementation\n\n【33】The pharmacists and physicians interviewed at all 3 sites reported that physician buy-in, affiliation with an academic partner (eg, college of pharmacy), and having well-trained pharmacists on staff facilitated their ability to implement CDTM . Pharmacist and provider collaboration — even on an informal or limited basis — helped solidify working relationships and increased provider buy-in over the long-term. Informants found that after providers began to experience the benefits of CDTM and other avenues of collaboration, they were more apt to collaborate.\n\n【34】Each site reported facilitators unique to their setting. At El Rio, the pharmacists widely disseminated reports describing positive patient outcomes, which helped to increase support for the collaborative care model. In addition, El Rio’s chief clinical pharmacist worked methodically to build relationships with newly employed officers, administrators, and providers. These relationships increased the number of patients referred to El Rio’s pharmacists and strengthened support among all stakeholders (eg, physicians). El Rio informants reported that a recent amendment to the state pharmacy act made engaging in CDTM less burdensome than in previous years. Finally, El Rio’s CDTM protocols are written broadly to give pharmacists substantial freedom in choosing how they care for patients.\n\n【35】Kerr Drug informants reported that the introduction of entry-level doctors of pharmacy (PharmDs) into the pharmacy profession contributed to the willingness of physicians to work collaboratively with pharmacists because of the rigorous scientific and clinical training involved in attaining the PharmD degree and because graduating physicians gain exposure to PharmD students during medical training. Furthermore, Kerr Drug has offered a pharmacy residency program for more than 12 years, and it serves as a training site for students. These factors increased interactions among Kerr Drug pharmacists and providers and raised providers’ support for and trust of the pharmacy profession. Similarly, the Osterhaus residency program was a critical part of collaborative relationships with providers. Osterhaus Pharmacy informants also mentioned having enough physical space to provide privacy for pharmacist–patient consultations. Finally, greater use of MTM via Medicare Part D and North Carolina’s CheckMeds program, which provides free pharmacist MTM services to beneficiaries enrolled in Medicare prescription drug plans, made it easier for pharmacists to enter into CPAs because providers realized the advantages of working closely with pharmacists.\n\n【36】Discussion\n----------\n\n【37】To our knowledge, this is the first study of its kind to examine real-life examples of CDTM implementation in various community settings. Our results demonstrate that models for CDTM can be tailored to the needs of the pharmacists, the practitioners, and the patients they serve on the basis of the level of trust, training, and familiarity among practitioners. Even when state law allows practitioners and pharmacists to determine the scope of CDTM services , compensation mechanisms and legal requirements for certification and training and existing informal collaborative relationships limited pharmacists’ options or interest in expanding the array of CDTM elements offered.\n\n【38】A consortium, which was convened by the American Pharmacists Association Foundation (APhAF) and charged with developing strategies to advance pharmacist patient care services, identified 7 principles for optimizing the role of pharmacists in patient care . These principles resonate with the emergent themes from these case studies. The consortium reported that successful collaborations are established mutually by the collaborating health professionals on the basis of trust and demonstrated competence in a regulatory context that allows practitioners to establish the scope of the agreement . A 2011 survey of pharmacists also found that trustworthiness and professional interaction are predictive of established collaborative care relationships with physicians, whereas trustworthiness and role specificity are predictive of newly established collaborations . Each site reflects different levels of maturity in CDTM, and establishing trust over time through repeated professional interactions and the demonstrated value of pharmacist patient care services was a critical factor.\n\n【39】Informal collaboration between pharmacists and providers established trust and added value to patient care services, but it also resulted in some pharmacists reporting little need to enter into CPAs to perform more advanced patient care services, particularly given the logistics of these agreements and the limited compensation for CDTM services. Even though the case studies were conducted in states with permissive scope of practice laws that allow the practitioners to set the terms of the CPAs, some administrative and procedural legal requirements affected the study participants’ capacity to engage in CDTM, primarily because the costs and time commitment were considered burdensome.\n\n【40】One of the most common challenges for pharmacists and pharmacies reported by the sites and supported in the literature is the lack of sustainable compensation mechanisms . The APhAF consortium reported this challenge and stated that a scalable, sustainable, and financially viable business model is necessary for the successful implementation of pharmacist patient care services . Giberson et al describe several federal and state CDTM models that are successful because pharmacists are compensated for the patient care services they provide, but they explain that the private sector has yet to incorporate these models, in part because pharmacists lack recognition as providers under federal law . Furthermore, a survey of pharmacist clinicians practicing CDTM in 2 states suggested that CDTM is a business loss: respondents billed on average $6,500 per month for their services, far less than the average cost of hiring a pharmacist clinician . This highlights potential compensation challenges, even when states have tried to reduce financial barriers to expanding the provision of pharmacist services. These case study sites are funded by grants and private and public payer reimbursement for some services, including Medicare Part D and immunization fees, but not all services. A lack of sustainable compensation for pharmacist patient care services and the need for recognition of pharmacists as providers were reported across the sites. Therefore, although evidence indicates that expansion of pharmacists’ roles through CDTM could greatly benefit public health , new compensation models are needed for individual practices to implement CDTM.\n\n【41】The Task Force recommendations for team-based care involving pharmacists and nurses to improve cardiovascular disease risk factors underscore the results of this study. The Task Force recommendations are based on recent literature summarizing examples of collaborative models of hypertension management, including models that involved pharmacist interventions. Notably, the Task Force found larger improvements in hypertension control when pharmacists were team members, and medication adherence was greater when team members could change antihypertensive medications independent of or with approval of the primary care provider. However, the Task Force noted the need for appropriate reimbursement mechanisms for team members that may improve the perceived “benefit to barrier” ratio reported here and encourage more pharmacists to participate in hypertension control CPAs as an alternative to informal collaborations with providers .\n\n【42】This study has several limitations. The sites selected might not be representative of all community-based CDTM practices. For example, Kerr Drug pharmacists served patients participating in a research study. El Rio is an FQHC where pharmacists, physicians, and other providers practice in a relatively closed setting. Furthermore, time and resource constraints prevented full transcription of audio recordings and analysis via formal data-coding procedures. Finally, the number of sites was small. It is not known whether visiting more sites would have identified additional themes.\n\n【43】As health care delivery systems increasingly adopt models of team-based care, such as CDTM, business and practice models and policies need to adapt accordingly. Although pharmacist interventions positively affect hypertension and other chronic diseases, these case studies highlight challenges and varying approaches to implementing CDTM. Pharmacists, other providers, and decision makers can use these findings when considering collaborative practice models to expand the pharmacist’s role in team-based care, link patient care in clinical settings with community-based services, and improve health outcomes. Results of this study will be available as guidance documents on CDC’s website.\n\n【44】Tables\n------\n\n【45】#####  Table 1. Elements of Collaborative Drug Therapy Management (CDTM) Offered at 3 US Sites, May–July\n\n| CDTM Element | Case Study Site |\n| --- | --- |\n| El Rio Community Health Center a | Kerr Drug b | Osterhaus Pharmacy c |\n| --- | --- | --- |\n| Perform patient assessments | X | X | X |\n| Order drug therapy–related laboratory tests | X | X |  |\n| Administer drugs | X |  | X |\n| Monitor and continue drug regimens d | X | X | X |\n| Select, initiate, and adjust drug regimens d | X |  | X |\n\n【47】a  All elements authorized pursuant to Arizona Revised Statutes §§32–1901, 32–1970 and 32–1974 (AZ Admin Code R4–23-421 to R4–23-429 – Sections R4–23-421 – §R4–23-429 repealed by final rulemaking at 17 A.A.R. 2600, effective February 4, 2012 \\[Supp. 11–4\\]).  \nb  All elements authorized pursuant to North Carolina General Statutes §§ 90–18(c)(3a) and 90–18.4 and North Carolina General Statutes §§90 85.1 et seq, and NC Admin Code tit 21 §46 .3101.  \nc  All elements authorized pursuant to Iowa Code §155A.3 and 155A.44 and IA Admin Code tit 657 §8.2 & §8.34 and IA Admin Code tit 653 §13.4.  \nd  Pursuant to specific boundaries of the CDTM agreement.\n\n【48】#####  Table 2. Key Barriers to Collaborative Drug Therapy Management (CDTM) Policy Implementation at 3 US Sites, May–July\n\n| Barrier | Case Study Site |\n| --- | --- |\n| El Rio Community Health Center | Kerr Drug | Osterhaus Pharmacy |\n| --- | --- | --- |\n| Lack of reimbursement for CDTM services | X | X | X |\n| Provider hesitation to trust pharmacists to deliver expanded services through CDTM | X | X | X |\n| Lack of administrative resources and time | X | X | X |\n| Proliferation of informal collaboration |  | X | X |\n| Limited physical space to provide care | X |  |  |\n| Changing laboratory infrastructure |  | X |  |\n| Difficulty in building relationships with providers |  | X |  |\n| Limited access to patients’ clinical information |  |  | X |\n| Patient challenges (eg, walk-in appointments, scheduling additional care) |  |  | X |\n\n【50】#####  Table 3. Key Facilitators to Collaborative Drug Therapy Management (CDTM) Policy Implementation at 3 US Sites, May–July\n\n| Facilitator | Case Study Site |\n| --- | --- |\n| El Rio Community Health Center | Kerr Drug | Osterhaus Pharmacy |\n| --- | --- | --- |\n| Increased stakeholder buy-in | X | X | X |\n| Use of academic partnerships or pharmacy students or residents | X | X | X |\n| Adequately trained pharmacists | X | X | X |\n| Organizational culture of collaboration | X |  | X |\n| Fewer regulatory or legal restrictions for engaging in CDTM agreements | X |  |  |\n| Broadly written CDTM protocols for providing patient care | X |  |  |\n| Patient satisfaction with receiving care from a pharmacist | X |  |  |\n| Sufficient space for patient counseling |  |  | X |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d48622f0-0865-476d-be3c-4f09f15dfeb2", "title": "African Buffalo Movement and Zoonotic Disease Risk across Transfrontier Conservation Areas, Southern Africa", "text": "【0】African Buffalo Movement and Zoonotic Disease Risk across Transfrontier Conservation Areas, Southern Africa\nSince the early 2000s in southern Africa, Transfrontier Conservation Areas (TFCAs) have been created to promote biodiversity conservation and local development . Increased connectivity between protected areas is designed to promote wildlife movement, ecosystem functioning, and genetic exchange and lead to increased wildlife populations, which should benefit communities living in these areas (e.g. through tourism and sustainable use of natural resources). Small-scale crop and livestock production are the main livelihood options for poor farmers living in communal lands in TFCAs. The extensive wildlife–livestock–human interface areas in TFCAs potentially result in human–wildlife conflicts, including crop destruction by wildlife, competition for resources between wild and domestic ungulates, livestock predation by wild carnivores, and poaching of wildlife; these conflicts are likely to increase as wildlife populations expand . The potential for the emergence and spread of infectious diseases is also of concern because of increased contact between wild and domestic hosts .\n\n【1】The Great Limpopo TFCA (GLTFCA) was created in 2002 and straddles Mozambique, South Africa, and Zimbabwe. It includes the Limpopo, Kruger, and Gonarezhou National Parks (NPs) and other land-use types surrounding the parks . The African buffalo ( _Syncerus caffer caffer_ ) population in Kruger NP is known to maintain animal diseases, including zoonoses such as bovine tuberculosis (bTB) and brucellosis. Buffalo are also suspected of playing a role in the epidemiology of Rift Valley fever . In 2009, a bTB strain related to the strain occurring in buffalo in northern Kruger NP was detected in buffalo in Gonarezhou NP, suggesting a recent spread from Kruger NP in South Africa to Gonarezhou NP in Zimbabwe . Although possible explanations were proposed for this transfrontier spread, including direct transmission from buffalo to buffalo or from an unidentified wild or domestic ungulate species to buffalo , these modes of transmission were not supported by firm data. We report preliminary results from telemetry studies and visual observations of individually identified African buffalo within the GLTFCA.\n\n【2】### The Study\n\n【3】During 2008–2013, a total of 68 satellite or global positioning system radio collars were deployed on African buffalo captured in southern Gonarezhou NP; in northern Kruger NP, south of the Limpopo River; and in Zimbabwe, north of Limpopo River (on Sengwe communal land). Of the 68 buffalo, 47 were adult females, selected because their behavior is representative of core herd movements. Two adult males were also equipped with global positioning system devices because males are believed to move between herds ; however, these devices failed after a few weeks because the collars fell off. Nineteen subadult female buffalo 2.5–4.5 years of age (age determined by teeth eruption) were also selected because individuals from this group are believed to disperse from their native herds . During chemical immobilizations of buffalo, blood samples were taken and stored appropriately for disease screening, and individually numbered ear-tags were applied.\n\n【4】During the study, extraordinarily long-distance movements for 3 subadult females were plotted by satellite telemetry readings . In January 2014, a 2.5-year-old female buffalo collared in South Africa walked a maximum direct distance of 95 km. In 6 days, she crossed into Zimbabwe, then into Mozambique, and into Zimbabwe again to enter Gonarezhou NP, with localizations within the home range of the buffalo herd in which bTB was first diagnosed in a female buffalo in 2008 . This subadult buffalo later left the park and visited a commercial farm area before reentering Gonarezhou NP. In February 2014, another 4-year-old female buffalo walked a direct distance of 64 km in 8 days. Finally, in March 2013, a 4.5-year-old female captured in July 2011 was sighted in a location deep into communal land at a distance of 96 km from her capture site. In contrast to these young female buffalo, no adult females collared in this study moved such long distances outside their home range during 2008–2014. The long-distance travel of these 3 subadult females occurred over a few days during the rainy season  and included movements outside the GLTFCA boundary.\n\n【5】### Conclusions\n\n【6】Our findings strengthen the hypothesis that bTB was spread from Kruger NP to Gonarezhou NP through buffalo-to-buffalo transmission by subadult females that dispersed from their native herds. Buffalo populations in Kruger and Gonarezhou NPs are connected through long-distance movements of individuals, specifically prebreeding heifers. Although this movement is important for buffalo conservation in TFCAs, it could also facilitate the spread of animal diseases, including zoonoses, across borders. In 2010 and 2011, bTB, Rift Valley fever, and brucellosis were detected in Kruger NP buffaloes, although a previous study failed to detect brucellosis in the Gonarezhou NP population  . Buffalo ID65, which was initially captured in Kruger NP, was seen among a breeding herd in Gonarezhou NP, indicating the possibility of direct, buffalo-to-buffalo transmission of bTB by dispersing infected individuals, without the need for bridge hosts (e.g. other wild or domestic ungulate species) .\n\n【7】Additional ecological information on buffalo dispersion is needed: frequency of dispersion events; size, age, and sex composition of the dispersing groups; and information about whether dispersed individuals later return to their home ranges. Subadult females appear to be particularly prone to dispersing behavior, unlike adult females, and we speculate that they may do so in small groups of individuals that are approximately the same age . No record exists of subadult female buffalo mixing with male bachelor groups, which are also known to connect to adjacent herds . So far, the drivers of such movement patterns are unclear. One possible explanation may be an out-breeding mechanism  that occurs before the start of reproduction; subadult females may leave their native herd to begin their reproduction in a distant herd to minimize in-breeding. Furthermore, abundant resources (i.e. water and grazing areas) available during the rainy season maximize the probability of success of such behavior.\n\n【8】We found that subadult females were infected with bTB, brucellosis, and Rift Valley fever , diseases with different mechanisms of transmission. Age and social position in the herd may influence individuals’ rate of exposure to pathogenic infections and consequently may affect the dynamics of infection within and between herds. Our results indicate that subadult female buffalo could play a role in the spread of diseases among distant populations, across protected areas and international borders, and during the rainy season. This seasonal pattern contrasts with the timing of most wildlife and livestock contact between adult females, which has been observed to occur predominantly during the dry season in the study area . Buffalo have been observed far outside the boundaries of protected areas, even outside the GLTFCA, in communal land where livestock farming is the main livelihood; these observations considerably widen the wildlife–livestock interface area where disease spread can occur . Wildlife–livestock interfaces can encompass large areas, rather than being a fence or strip of land at the edge of protected areas. These data should assist in refining disease modeling by showing the importance of temporal and spatial considerations and by redefining variables (e.g. age and sex) involved in risk for pathogen spillover or emergence (i.e. identifying super-spreaders) .\n\n【9】Our results suggest that the spillover of bTB and other zoonoses at the wildlife–livestock–human interface constitutes a risk to animal and human health in the GLTFCA . The health issue in TFCAs cannot be overlooked and must be part of any management decision. Combining ecological and epidemiologic knowledge is necessary to understand disease dynamics in these complex agro-ecosystems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d140e61c-0887-4b32-869c-2081ffc993da", "title": "Leishmania infantum in Tigers and Sand Flies from a Leishmaniasis-Endemic Area, Southern Italy", "text": "【0】Leishmania infantum in Tigers and Sand Flies from a Leishmaniasis-Endemic Area, Southern Italy\nVisceral leishmaniasis, caused by infection with _Leishmania infantum_ protozoa, is listed among the most neglected tropical diseases, affecting thousands of persons, most of whom are among the world’s most vulnerable populations . The disease is associated with the presence of phlebotomine sand fly vectors; domestic dogs typically act as reservoirs. Among felids, domestic cats have recently gained prominence as putative reservoirs of _L. infantum_ , whereas cases of infection in other felids have been reported occasionally .\n\n【1】In February 2019, a tiger (index case), born and raised in a zoologic park in southern Italy, had a nonhealing laceration that tested positive for _L. infantum_ DNA on a skin punch biopsy. Because tigers are considered an endangered species, the presence of an active _L. infantum_ transmission focus in a facility visited by thousands of visitors each year deserves attention. Therefore, we conducted an epidemiologic study to investigate the prevalence of _L. infantum_ infection in the local tiger and sand fly populations, along with the sand flies’ host blood-feeding preferences.\n\n【2】### The Study\n\n【3】During March–June 2019, we tested 20 tigers born at the zoologic park (Safari Park, Apulia region, Brindisi Province, southern Italy) and living in an open enclosure for _L. infantum_ infection. We smeared lymph node aspirates on slides for the cytologic examination; we also cultured and processed these specimens, along with whole blood, skin punch biopsy, and conjunctival, nasal and oral swab specimens, for the detection of _L. infantum_ DNA by quantitative PCR (qPCR) . We tested for feline leukemia virus (FeLV) and feline immunodeficiency virus (FIV) by using proviral DNA from blood, as described previously . We detected _L. infantum_ antibodies by using an immunofluorescence antibody test (IFAT), as described previously in a study in cats . During May–November 2019, we collected sand flies in the tigers’ enclosure biweekly by using sticky traps and light traps and identified each specimen by using morphologic keys. We performed conventional PCR for blood-meal identification in sand flies by using primers cyto 1 (5′-CCATCAAACATCTCAGCATGAAA-3′) and T2893R (5′-GTTGGCGGGGATGTAGTTATC-3′), which target the mitochondrial cytochrome b. The protocol of this study was approved by the ethics committee of the Department of Veterinary Medicine at the University of Bari (Bari, Italy).\n\n【4】Tigers enrolled in the study ranged in age from 6 months to 11 years and weighed 70–220 kg ; all were apparently healthy or had unrelated conditions, except for 1 (index case), which had a large nonhealing laceration extending from the left loin region to the left thoracic region . Overall, 9 (45%) of the 20 tigers tested positive for _L. infantum_ by IFAT, 5 (25%) tested positive by qPCR, and 5 (25%) tested positive by both methods . The tigers were positive by qPCR on lymph node aspirates and skin punch biopsy. None of the conjunctival swab specimens tested positive. We did not detect _L. infantum_ cytology or culture of lymph node aspirates in any of the tigers. All tigers were negative for FeLV and FIV.\n\n【5】During May–November 2019, we collected a total of 580 sand flies. The most abundant species was _Phlebotomus perniciosus_ (n = 491), followed by _Sergentomyia minuta_ (n = 69) and _P. neglectus_ (n = 20). Of the 190 females collected, 151 (26%) were _P. perniciosus_ , 4 (<1%) were _P. neglectus_ , and 35 (6%) were _S. minuta_ . Specimens for 8 (5.3%) _P. perniciosus_ sand flies and 1 (2.9%) _S. minuta_ sand fly tested positive for _L. infantum_ DNA. Of the 190 females examined, 63 (33.1%) _P. perniciosus_ , 3 (1.6%) _P. neglectus_ , and 2 (1.1.%) _S. minuta_ sand flies tested positive for tiger DNA ; we detected no other mammalian DNA (e.g. from cats, dogs, rats, or humans) in blood-fed or -unfed specimens. Consensus sequences of the vertebrate host mitochondrial cytochrome b from all female sand flies (positive specimens) displayed 100% identity to the nucleotide sequences of _Panthera tigris_ available in the GenBank database (accession nos. MH124112 and KC879295).\n\n【6】### Conclusions\n\n【7】The high prevalence (45%) of _L. infantum_ infection recorded indicates that tigers living in the zoologic park are highly exposed to sand flies and thus have a high risk for acquiring the parasite. The finding of engorged sand flies that fed on tigers and were also positive for _L. infantum_ suggest that tigers could be an alternative host of this parasite; however, the possibility that _L. infantum_ –positive sand flies had acquired the infection from another host, before feeding on tigers, cannot be ruled out.\n\n【8】Although _Leishmania_ spp. infection has been scantly described in wild felids , the diagnosis of this parasitic infection should also be considered while screening these animals for pathogens potentially impairing their health and welfare. No information is available on the immune response against _L. infantum_ infection in tigers, and serologic tests have not been validated for this host, but one could reasonably suspect that their antibody production would follow a pattern similar to that occurring in cats. Nonetheless, the absence of _L. infantum_ DNA in tigers that were positive for _L. infantum_ antibodies (4/9 tigers \\[44.4%\\]) could be expected, given that this lack of correlation between molecular and serologic positivity has also been observed in cats , indicating that the diagnosis of the infection in these animals might be a difficult task, as it is in cats. The detection of _L. infantum_ DNA in the lymph node aspirate and skin biopsy suggests that these tissues are more suitable than blood for the diagnosis of this infection, as previously reported in dogs and cats . Otherwise, the conjunctival swab seems to be not as good a sample for this purpose in tigers. Unlike some studies with cats , no correlation between _L. infantum_ infection and FIV, FELV, or both FIV and FELV infection has been observed in the tigers in our study.\n\n【9】The predominance of _P. perniciosus_ sand flies, along with their positivity for _L. infantum_ DNA already recorded in southern Italy , is somewhat expected, given that this sand fly species is recognized as the main vector for _L. infantum_ in different foci of visceral leishmaniasis in Italy . The high proportion of _L. infantum_ –infected sand flies suggests that the risk for parasite transmission in this environment should be considered. Furthermore, the detection of _L. infantum_ DNA in _S. minuta_ sand flies has already been reported in southern Italy (4.2%) and Portugal (4%) . In addition, although consideration of the role played by _S. minuta_ (the proven vector of _L. tarentolae_ ) in the circulation of _Leishmania_ spp. of zoonotic concern has been raised , further studies are necessary to fully assess its vector role.\n\n【10】_P. perniciosus_ sand flies frequently feed on tigers, because dogs are not allowed to roam in the zoo, the role of tigers as local reservoir hosts needs to be ascertained. Because _P. perniciosus_ sand flies feed on a wide range of domestic and wild animals, and because _L. infantum_ might infect the sand flies after taking a blood meal from infected felids , the role of tigers in the transmission cycle of _L. infantum_ is probable.\n\n【11】In summary, _L. infantum_ infection should be included in the differential diagnosis of infectious diseases in tigers in areas where visceral leishmaniasis is endemic. The role of tigers as sentinels for _L. infantum_ , the occurrence of _P. perniciosus_ sand flies infected by the protozoan, and its abundance in the study area might represent an eminent risk for animals and humans living in or visiting the zoo. Therefore, prevention measures are needed for providing protection against _L. infantum_ infection in these animals and for controlling sand flies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fde68a9f-a0f9-453d-9ea5-b01cd52458d3", "title": "Risk Factors for Nipah Virus Encephalitis in Bangladesh", "text": "【0】Risk Factors for Nipah Virus Encephalitis in Bangladesh\nHenipaviruses (family _Paromyxoviridae_ , genus _Henipavirus_ ) are enveloped RNA viruses that cause respiratory illness in pigs and horses and respiratory illness and encephalitis in humans . After a 4- to 18-day incubation period, human disease can rapidly progress from mild illness (fever, headache, myalgia) to coma and death within 10 days; the case-fatality ratio is 40%–76% . The first recognized human Henipavirus infections occurred in 1994 in Australia, where a respiratory disease among horses was associated with illness in 2 humans . The etiologic agent, Hendra virus, was subsequently isolated from asymptomatic flying foxes (fruit bats of the family _Pteropodidae_ ) . Field et al. suggested that horses, identified as the intermediate hosts linked to human illness, may have become infected through indirect contact with fruit bats (e.g. infected fetal bat tissues or fluids).\n\n【1】The first reported human epidemic of encephalitis caused by another Henipavirus, Nipah virus (NiV), occurred between September 1998 and April 1999 in Malaysia and Singapore and was associated with an outbreak of severe respiratory illness in pigs . Most (86%–93%) human NiV encephalitis (NiVE) infections during this outbreak involved occupational exposure to pigs, implicating these animals as an intermediate host for NiV . Outbreaks of NiVE occurred in Bangladesh during 2001 and 2003, in areas where NiV antibody–positive fruit bats have been identified . These reports, in addition to ecologic surveys conducted in Cambodia, have strengthened evidence that pteropid bats are the reservoir for Hendra and Nipah viruses .\n\n【2】An outbreak of encephalitis in Bangladesh was recognized on January 21, 2004; it affected 2 villages of Goalando township, Rajbari District, Dhaka Division, 70 km west of the city of Dhaka . Ten deaths were reported among 12 ill persons with symptoms compatible with NiVE, resulting in a case-fatality ratio of 83% . Although previous outbreaks of NiVE outside Bangladesh involved primarily men and women >25 years of age , most (75%) patients in this outbreak were boys <15 years of age. We describe a matched case-control study that was conducted to characterize the epidemiology of NiVE and, specifically, to determine if risk for NiVE was associated with contact with animals; an environmental exposure, activity, or behavior; or contact with other NiVE patients during the 2004 NiVE outbreak in Goalando township.\n\n【3】### Materials and Methods\n\n【4】##### Study Participants\n\n【5】A matched case-control study was conducted in Goalando, Bangladesh , February 18–22, 2004. Hypotheses tested in this study, as mentioned above (e.g. increased risk for NiV infection caused by contact with animals, environmental exposure, contact with fruit in season) were based upon factors associated with previous outbreaks of NiVE in Malaysia, Singapore, and Bangladesh.\n\n【6】##### Case Definition\n\n【7】A confirmed NiVE case-patient was defined as any patient with fever and symptoms compatible with encephalitis after December 15, 2003, with NiV-specific immunoglobulin M antibodies in cerebrospinal fluid (CSF) or serum by enzyme immunoassay (EIA). A probable case of NiVE was defined as a patient with a diagnosis of encephalitis in whom fever developed and who was living in the same village as a patient with a confirmed case of NiVE after December 15, 2003. Cases remained in the probable category if the patient died and a specimen for laboratory confirmation could not be obtained.\n\n【8】We conducted a population census of the affected area in February 2004; this census was the basis for selecting controls. We identified 3 controls for each case-patient. The controls were selected randomly from the population and then matched to each case-patient on the basis of gender and age group. All households identified during the census, including houses of case-patients and controls, were mapped by Global Positioning System, and data were uploaded into ERDAS Imagine 8.5 (Leica Geosystems, Atlanta, GA, USA) and merged with a November 2000 IKONOS Geo 1-m satellite image of the outbreak area (Space Imaging, Thornton, CO, USA).\n\n【9】Participation was strictly voluntary, and written informed consent was obtained for all participants; for those <18 years of age, individual and parental consent was obtained. The Bangladesh Ministry of Health and Family Welfare that requested this investigation reviewed and approved all protocols.\n\n【10】##### Study Population\n\n【11】Probable and confirmed cases identified in 2 contiguous villages of Goalando township  were included in this study. Seven of the 12 cases were clustered within 3 households. Of these 7 clustered cases, 3 occurred in 1 household, and the remaining 4 were distributed in 2 separate homes . Therefore, we conducted 2 separate analyses to assess the effect of case clustering on results. The first analysis contained the complete dataset of 12 cases and 36 controls; the subanalysis consisted of 8 cases (we randomly selected 1 case/household) and 24 matched controls. Similar results (proportions, odds ratios \\[ORs\\], 95% confidence intervals \\[CIs\\]) were obtained from both analyses. Thus, data presented in this article, including all tables, are derived from the complete dataset.\n\n【12】##### Specimen Collection and Testing\n\n【13】Serum samples and CSF were tested as previously described . When possible, a serum specimen was collected from controls.\n\n【14】##### Data Collection and Interviews\n\n【15】After informed consent was obtained, case-patients and controls were interviewed at home by trained interviewers, in their native Bengali language, with a standardized questionnaire. Information such as demographics, types of animal exposures, environmental and occupational exposures, exposure to ill persons, and history of illness was obtained. Proxy interviews of family members and/or friends were conducted for deceased patients. To minimize interview bias, proxy interview methods were also used for all controls that were matched to deceased case-patients.\n\n【16】##### Statistical Analysis\n\n【17】Exact ORs and 95% CIs were calculated by using a matched univariate logistic regression analysis in SAS version 9.0 (SAS Institute Inc, Cary, NC, USA) . Associations were considered statistically significant at p<0.05.\n\n【18】### Results\n\n【19】##### Descriptive Characteristics\n\n【20】Four (33%) cases were confirmed by EIA; the remaining 8 (67%) case-patients, from whom a diagnostic specimen was not available, were considered probable cases. Among all 13 (36%) controls who consented to blood collection, results of serologic tests for NiV-specific antibodies were negative. Furthermore, none of the controls reported having had a perceived fever or symptoms compatible with NiVE from December 15, 2003, through the week the study was conducted (February 18–22, 2004). In addition, an antibody prevalence study conducted among persons (n = 300) living in the outbreak site showed no evidence of asymptomatic or mild infection, which suggested that controls entered into the study were likely uninfected . Proxy interviews were administered to equal proportions of case-patients (83%) and controls . The median age of case-patients included in the study was 11.5 years (range 2–28 years); 9 (75%) were male, and 11 (91%) were < 15 years of age . Residences of all case-patients and controls were located within the affected villages, an area with a radius of ≈800 m .\n\n【21】##### Animal Exposures\n\n【22】In the matched case-control analysis, a greater percentage of case-patients (60%) than controls (34%) had observed or touched dead animals, although this finding was not statistically significant . We observed no differences between case-patients and controls with respect to contact with ill animals , including pigs, ruminants, and fruit bats. Chickens and ducks were often slaughtered for religious purposes or for consumption; however, close contact with these animals and their bodily fluids (e.g. blood, saliva) during this process was not associated with NiV infection . None of the case-patients or controls had known contact with pigs (healthy or ill) or pig excreta . Four (36%) of 11 case-patients and 7 (19%) of the controls observed fruit bats around their household during the night (OR 4.1, p = 0.49). However, some proxy family members and/or friends answering on behalf of patients who had died were unable to answer specific questions (e.g. Did you observe fruit bats around your house during the night?).\n\n【23】##### Environmental and Behavioral Exposures\n\n【24】A greater proportion of case-patients (83%) than controls (51%) reported having climbed trees between December 15, 2003, and February 3, 2004 (OR 8.2, p = 0.025). No statistically significant differences were observed between case-patients and controls with respect to outdoor activities such as hunting, fishing, or playing outdoor games (e.g. hide-and-seek, cricket, soccer). Eating fruit that was locally available (on trees or collected from fruit trees locally) between December and February was not associated with illness, regardless of how the fruit was collected (from the ground, picked from tree, from the market) . Although a greater proportion of case-patients reported environmental exposures (drinking raw date palm sap, harvesting date palm sap, having someone in the household who collects date palm sap, or drinking sap directly from the collection vessel), these differences were not statistically significant .\n\n【25】##### NiVE Case Exposure\n\n【26】There were strong associations between illness and 1) visiting a hospital and/or 2) having had contact with a probable or confirmed NiVE patient . In one 2-case family cluster, a mother (26 years of age) and her infant son (2 years of age) both became ill and died. The child became symptomatic 2 days before the mother’s illness onset . Among the other affected family clusters, the patients became ill within 3 days of one another ; all persons in these 2 clusters reported a history of climbing fruit trees. There was no evidence of contact of persons between case households during their illness.\n\n【27】### Discussion and Conclusions\n\n【28】In contrast to the patients in the Malaysian and Singapore outbreaks, which occurred primarily among adults, a preponderance of the NiV patients in the January/February 2004 Bangladesh outbreak were young boys. These findings, in the absence of high infection rates among adults or evidence of antibodies to NiV in the general population , suggest an association between NiV infection and some childhood activity or specific behavior. The odds of NiV infection were significantly elevated among persons who climbed trees, an activity observed almost exclusively among boys <15 years of age. This behavior is quite common among children because they gather fruit from trees. Therefore, these children may have had contact with partially eaten fruit from fruit bats or the secretions/excretions of these animals. Or, the children may have contacted contaminated fruit bat guano or urine in the trees. The percentages of case-patients playing hide-and-seek, hunting, and fishing—all of which were typical behaviorial traits of local boys—were not significantly different than those for controls. These activities generally occur outdoors; however, they do not place a child in direct contact with bat excretions or secretions, as may be true for tree climbing. Therefore, infection was apparently related to a specific behavior, tree climbing, rather than age or outdoor activities in general. Furthermore, although other exposures that may have placed persons in closer contact with bat secretions (e.g. collecting fruit or palm sap from trees, drinking palm sap directly from collection vessel) were observed more often among case-patients than controls, these findings were not statistically significant; perhaps because of the small sample size. Nonetheless, our findings can and have been used to help guide NiV outbreak investigations, leading investigators to similar conclusions as ours .\n\n【29】Fruit bats forage at night in various trees that are producing ripe fruit and often drink from palm sap collection vessels . Fruits are also a major food source for many villagers and, as a result of environmental disturbances  in the form of crop development (e.g. jute, rice, and sugar cane), the few remaining fruit trees grow only in close proximity to human dwellings . This in turn creates a situation in which fruit bats are forced into close proximity with humans, especially while these mammals are foraging and feeding. In addition, date palm sap is routinely collected in rural areas of Bangladesh between December and May. According to villagers, including palm sap harvesters, dead fruit bats are occasionally found in the collection vessels. Local villagers reported that they often observed fruit bats feeding from palm sap collection vessels, and some collectors place cloth over the opening of the vessel to prevent this (investigational team observation). In fact, a greater proportion of case-patients in our study collected palm sap, drank from the palm sap collection vessel, or had a family member who collected palm sap; however, these differences were not statistically significant. The power of our study to detect exposure risks was limited by the outbreak size. Therefore, until additional data are available, remaining cautious of date palm sap collection vessels, especially those visibly contaminated with fruit bat excreta or carcasses, would be prudent.\n\n【30】Numerous investigators have found serologic evidence suggesting that fruit bats of the genus _Pteropus_ are the reservoir hosts for NiV , and there are reports of NiV isolation from bat urine  and partially eaten fruit . Unpublished laboratory data from the Bangladesh investigation have not supported the presence of an intermediate or primary reservoir host other than _P. giganteus_ . Available data from this study, therefore, suggest direct transmission of NiV to humans through contact with bat secretions or excretions (saliva, urine, guano, partially eaten fruit) during fruit-tree climbing.\n\n【31】Although indirect contact with bats may have been the primary means of infection for this outbreak, Hsu and others  demonstrated that contact with ill cows was associated with an increased risk for NiV infection during the 2001 Bangladesh NiV outbreak. Therefore, intermediated hosts should be considered in future NiV outbreaks in Bangladesh.\n\n【32】In contrast to the patients in the Malaysia and Singapore outbreaks , most of the Bangladesh population (and all of the case-patients included in this study; data not shown) are practicing Muslims who do not consume pork and who avoid contact with pigs. None of the case-patients and controls in our study population reported any contact with pigs or pig excreta, so it is unlikely that these animals played a role in this outbreak.\n\n【33】Clustering of cases within households was a prominent feature of this outbreak ; 1 household contained 3 case-patients, all brothers of ages 7–15 years. However, the longest estimated incubation periods (duration from symptom onset to first known exposure to a NiVE family member) within the clusters reported here were less than the currently recognized 4-day minimum . This finding suggests that the family clustering may have resulted from a common source of infection (e.g. a specific tree they climbed, fruit they consumed, or palm sap collection vessel they were in contact with) rather than person-to-person transmission. Our data also show strong associations between NiV infection and visiting a hospital. However, because the participants were asked if they had visited a hospital within a range of dates (December 15, 2003–February 3, 2004) and not a specific date, we were unable to determine if they were ill with NiV before visiting the hospital or whether they acquired their infection there. Some accounts in the literature suggest person-to-person transmission of NiV; therefore, it is plausible that someone could acquire, through contact with a patient’s secretions or excretions, an NiV infection while visiting a hospital . Nevertheless, the most probable explanation for the observed association is that NiV encephalitis patients during this outbreak were severely ill, requiring hospitalization.\n\n【34】Although person-to-person transmission may have occurred in this outbreak, the initial infection (index case) may have occurred through contact with bat secretions rather than contact with an intermediate host. A limitation of our study is that we were unable to identify a specific mechanism by which person-to-person transmission may have occurred. NiV has been isolated from the respiratory secretions and urine of patients in the Malaysia, Singapore, and current Bangladesh outbreaks , which suggests a potential for NiV to be transmitted from person to person. Data based upon chain-of-transmission events and clustering of cases during other 2003 and May 2004 Bangladesh outbreaks led investigators to conclude that human-to-human transmission may have occurred . Therefore, given the potential for household or nosocomial transmission, we recommend the use of personal protective equipment (i.e. gloves, masks, gowns, and eye protection); strict hand hygiene and surface disinfection during and after contact with an NiVE patient; isolation of patients with confirmed or suspected NiV infection; and proper disposal of potentially contaminated materials.\n\n【35】In summary, tree climbing, a behavior largely engaged in by young boys, was associated with an increased risk for NiV infection; although the exact mode of transmission is unclear. Our data do not rule out the potential for person-to-person transmission. If person-to-person transmission were extremely efficient, the conditions and population density of Bangladesh (≈1,000 persons/km 2  ; total population 141 million/144,000/km 2  ) may have resulted in a much larger outbreak. Indeed, a study among health workers in Bangladesh did not find evidence of incidental transmission to persons caring for patients hospitalized with Nipah-related illnesses . Bat-to-human was the most probable route of transmission in Goalando; however, some undetermined intermediate or incidental hosts cannot be ruled out. Periodic introductions of NiV to human populations in this region may continue to occur because of the overlapping nature of human and pteropid bat habitats. Moreover, bat–human interactions are likely to increase due to bat habitat loss because the few fruit trees that remain will likely be found in close proximity to human dwellings .\n\n【36】As a prevention measure, we recommend avoiding contact with fruit bats and their secretions/excretions. We also encourage persons to wash and/or peel fruit, in addition to washing their hands, before preparing meals or consuming fruit. Greater understanding of the relationships between pteropid fruit bats, NiV, and its transmission to humans might offer additional strategies for safe coexistence and disease prevention for Bangladesh and other countries where fruit bats reside. Finally, because the geographic range of this highly lethal pathogen may correspond to the distribution of the genus _Pteropus_ , including parts of China and Australia, most of the Indian subcontinent, and Southeast Asia , factors that promote transmission from bats to humans need to be defined and the role of person-to-person transmission needs to be better characterized.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dbe129a8-db67-4ea2-9a7a-7af95b8eac1a", "title": "Phylogenetic Characterization of Crimean-Congo Hemorrhagic Fever Virus, Spain", "text": "【0】Phylogenetic Characterization of Crimean-Congo Hemorrhagic Fever Virus, Spain\nCrimean-Congo hemorrhagic fever (CCHF) is a severe disease transmitted to humans mainly by ticks, primarily of _Hyalomma_ spp. This zoonotic disease is caused by CCHF virus (CCHFV), a nairovirus in the family _Bunyaviridae_ , which was detected first in Crimea in 1944 and isolated 12 years later in the Democratic Republic of the Congo. Since the beginning of the 21st century, CCHFV has been spreading from disease-endemic areas to new regions previously considered free of the disease, particularly in areas where _Hyalomma_ spp. ticks are present. In nature, CCHFV usually circulates between asymptomatic animals and ticks in an enzootic cycle.\n\n【1】CCHFV has a negative-sense and tripartite RNA genome (small \\[S\\], medium \\[M\\], and large \\[L\\] segments) with high genetic diversity. The sequences of the S segment, which is the most conserved at the nucleotide level, could be distributed in 6 lineages . Each genetic lineage has been linked to geographic regions in Africa, the Middle East, Asia, and Europe, where _Hyalomma_ spp. ticks are present . Genotypes I, II, and III have been described in Africa; genotype IV in Asia; and genotypes V and VI in Europe. However, strains have moved between geographic regions; migrating birds, unregulated wildlife trade, livestock import and export, and a global movement of humans could have dispersed the virus or CCHFV-infected ticks .\n\n【2】In western Europe, the presence of the virus had only been detected indirectly by means of serologic methods in the serum of 2 people from southern Portugal . However, genotype III CCHFV was detected in ticks from deer captured in western Spain in 2010 and ticks from birds migrating from Morocco in 2013 . No cases among humans in Spain had been reported until 2016, when 2 autochthonous cases were diagnosed . Here, we report the complete genomic sequence of the virus from 1 of these case-patients and show the phylogenetic relationships among the 3 segments.\n\n【3】### The Study\n\n【4】We previously detected CCHFV in serum samples from the 2 patients in Spain with autochthonous CCHV ; the index case-patient died. For this study, we obtained viral RNA (3.6 × 10 7  copies/mL) from the secondary case-patient in a sample taken 4 days after onset of symptoms by using the QIAamp viral RNA Mini kit (QIAGEN, Hilden, Germany). We then amplified the virus in a single-step reverse-transcription PCR by using the SuperScript III One-Step RT-PCR system with the Platinum Taq High Fidelity DNA Polymerase kit (Invitrogen Life Technologies, Barcelona, Spain) with overlapping primers throughout the complete genome . PCR conditions were amplification at 52.5°C for 30 min, 94°C for 2 min, and then 40 cycles at 94°C for 15 s, 48°C –55°C for 30 s, and 68°C for 1 min/kb, with a final extension cycle at 68°C for 5 min. We designed primers for each segment in most conserved regions after aligning available CCHFV sequences from all genotypes retrieved from GenBank . We directly sequenced purified amplicons by using additional internal primers (data not shown). We assembled and analyzed the consensus sequence of each segment by using SeqMan Pro from the Lasergene Suite 12 (DNASTAR Inc. Madison, WI, USA). To get complete finished genomes, we used a hybrid-capture method as described by Blackley et al. by using probes designed against the CCHFV sequences obtained from amplicon sequencing.\n\n【5】To characterize the complete CCHFV genome, we performed a phylogenetic analysis of the full S, M, and L segments . The 3 segments were aligned by using ClustalW in MEGA 5.2  and representative available CCHFV sequences from GenBank of all genotypes. We generated a phylogenetic tree by using neighbor-joining algorithms and analyzed 1,000 replicates for bootstrap testing. GenBank accession numbers for sequences used in this study are MF287636 for the S fragment, MF287637 for the M fragment, and MF287638 for the L fragment.\n\n【6】The nucleotide sequence of the different CCHFV segments from the infected patient we analyzed in this study showed 99% identity with the Sudan AB1–2009 CCHFV strain  in S, M, and L segments (GenBank accession nos. HQ378179.1, HQ378187.1, and HQ378183.1, respectively)  and were grouped within genotype III (Africa 3). CCHFV found in ticks from Spain in 2010 and from birds from Morocco in 2011 also clustered in this group . In addition, no reassortant segment has been found in the analysis of the full genome, even though reassortant strains have been described in this genotype .\n\n【7】### Conclusions\n\n【8】The results of the sequence analysis we describe corroborate our previous results , obtained by analyzing a small fragment in the S segment, showing that CCHFV from genotype III (Africa III) is circulating in southwestern Europe. CCHFV circulating in Spain caused 2 autochthonous cases that resulted in the death of the index case-patient and a serious illness in the second case-patient, providing evidence of its pathogenicity. The risk for infection in Spain is considered low, but human infection caused by the bite of an infected tick has occurred 6 years after the virus was discovered in ticks . Because the virus is circulating in Spain, additional studies will be required to establish the distribution of the virus in this country.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "57320481-bdca-4ff2-81a5-06c694058389", "title": "Asian Lineage of Peste des Petits Ruminants Virus, Africa", "text": "【0】Asian Lineage of Peste des Petits Ruminants Virus, Africa\n_Peste des petits ruminants virus_ (PPRV) belongs to the genus _Morbillivirus_ , in the family _Paramyxoviridae_ . Like other members of the same genus, such as rinderpest virus, _Canine distemper virus_ , _Measles virus_ , and marine mammal viruses, PPRV is highly pathogenic for its natural hosts . As consistently reported by the World Organisation for Animal Health, PPRV causes high death rates in livestock. It has a major economic effect, particularly in the intertropical regions of Africa, on the Arabian Peninsula and in the Middle East and Asia . The main signs of the acute form of the disease are high fever, mouth ulceration, diarrhea, and pneumonia.\n\n【1】Although rinderpest virus was eradicated after intensive vaccination campaigns in the last quarter of the 20th century , PPRV has continued to spread in Africa and Asia. In East Africa, outbreaks occurred in 2007 in Uganda and Kenya and in 2009 in Tanzania ; in North Africa, an outbreak occurred in 2008 in Morocco. Genotypic classification of PPRV has identified 4 lineages and appears to be an efficient tool to survey virus spread worldwide. Genetic variability is based on partial sequencing of the fusion (F) protein gene  and of the 3′ end of the nucleoprotein (N) gene . Both are well conserved genes with ≈10% nt mean variability between the most distantly related sequences ; this variability can exceed 30% on some parts of the sequence. However, because the N gene is the most abundantly transcribed virus gene, sensitivity is better achieved with N mRNAs . Viruses of lineage I and II are restricted to western and central Africa; lineage III is common to eastern Africa and the southern part of the Middle East. In Asia, only viruses of lineage IV have been detected.\n\n【2】In Sudan, continuous outbreaks of PPRV have occurred for >30 years, mainly in sheep and goats . Although PPRV infection is well documented in small ruminants, data are rare for other species, such as cattle, buffalo, and camels. In 1997, a PPRV was isolated from pathologic samples collected during a rinderpest-like disease outbreak among buffalo in India . In September 2004, outbreaks of PPRV in Sudan affected both sheep and camels . In camels, a respiratory syndrome was the prominent disease characteristic observed, resembling a previous case reported in Ethiopia during 1995–1996 .\n\n【3】More recently, in summer 2008, Morocco reported outbreaks of PPRV for the first time . Because PPRV needs close contact for transmission, this new epizootic was likely the result of introduction of the virus into North Africa through the movement of live infected animals. To characterize PPRV strains identified in Sudan during 2000–2009 and in Morocco during the extensive 2008 outbreaks, a phylogenetic analysis was carried out on wild-type PPRV sequences obtained from biological samples collected from sheep, goats, and camels in these 2 countries.\n\n【4】### Materials and Methods\n\n【5】##### Biological Materials\n\n【6】Biological specimens were collected in different parts of Sudan during 2000–2009 from camels, sheep, and goats that showed clinical signs of PPRV. Additional camel specimens were collected after outbreaks associated with high death rates were reported from the Kassala Region in 2004. A total of 80 field samples, including lung, liver, and spleen, were obtained from 49 camels, 26 sheep, and 5 goats with PPRV-like clinical signs. For most of the animals, virus detection and identification were performed directly on the tissue samples collected; however, virus isolation in tissue culture was attempted for some samples. A historical PPRV strain isolated in Sudan in 1971, the Gedarif PPRV strain, was included in this study.\n\n【7】In Morocco, a total of 36 samples were collected from sheep displaying PPRV signs during the 2008 outbreaks. These included oral and ocular swabs and mesenteric lymph nodes, spleen, lung, and whole blood samples.\n\n【8】##### Laboratory Investigations\n\n【9】Sample aliquots first were screened for PPRV antigen detection by immunocapture ELISA (ICE test; Biological Diagnostics Supplies, Ltd. Dreghorn, Scotland). A second aliquot was used for viral RNA extraction and virus isolation in tissue culture. Reverse transcription PCR (RT-PCR) amplified the N protein gene directly from tissue samples by using a set of pan-morbillivirus primers as described by Kwiatek et al.\n\n【10】For the sequencing of the N gene, the 3′ 351 nt were obtained by employing primers NP3 and NP4  and through a modification of the initial protocol by using a 1-step method (OneStep RT-PCR mix; QIAGEN, Courtaboeuf, France). For the F gene, 11 samples were selected from samples that previously tested positive by N amplification. For the specific and sensitive detection of PPRV from these clinical field samples, the initial method developed by Forsyth et al. in 1995  was modified by using an additional pair of primer F1/FPPRrev in combination with F1/F2 in a nested PCR. Sequences positions of these primers were 5′-ATCACAGTGTTAAAGCCTGTAGAGG-3′ (PPRV-F1: 777 ± 801), 5′-GAGACTGAGTTTGTGACCTACAAGC-3′ (PPRV-F2: 1124 ± 1148) and 5′-ATATTAATGTGACAAGCCCTAGGGA-3′ (FPPRrev: 2055 ± 2079). The final amplicon was obtained at the expected length of 372 nt after 35 cycles. We analyzed 10 μL of the amplified products by electrophoresis on 1.5% agarose gel. For all positive results, 40 μL of the final product was used directly for sequencing (GATC Biotech, Constance, Germany). All sequences were deposited in GenBank under accession nos. HQ131917– HQ131958. FPPRrev and the study of the target sites of primers F1 and F2  were designed by a comparative sequence analysis of various morbillivirus F sequences from the full genomes available in GenBank (accession nos. X74443, EU267273, EU267274, AY560591, FJ905304, and AJ849636).\n\n【11】##### Sequence Data, Alignment, and Phylogenetic Analysis\n\n【12】Sequencing reactions were performed by GATC Biotech. Each nucleic acid segment on the N and F genes of the Sudan and Morocco strains was aligned with other sequences from PPRV maintained in the database or retrieved from GenBank  . Multiple alignments of 255 and 322 nt of the N and F genes, respectively, were made by using the ClustalW program . Phylogenetic analysis was carried out by using the criterion of neighborhood based on the principle of parsimony . Dissimilarities and distances between the sequences first were determined with Darwin software . Tree construction was based on the unweighted neighbor-joining method proposed by Gascuel . Trees were generated with the TreeConMATRIXW program of Darwin . Bootstrap confidence intervals were calculated on 1,000 replicates.\n\n【13】##### Virus Isolation\n\n【14】For the samples from Sudan, isolation was successful on MDBK cells with lung samples from Cam\\_8, Cam\\_169, Cam\\_318, Ov\\_Soba, and Ov\\_Al Azaza, collected from 3 camels and 2 sheep, respectively. For the samples from Morocco, PPRV Morocco\\_08\\_02 virus was isolated successfully from a sheep lung sample after infection of Vero.DogSLAMtag cells . These isolates were sequenced and compared as described above.\n\n【15】### Results\n\n【16】##### Detection of PPRV by N gene RT-PCR\n\n【17】A total of 80 animals from Sudan that initially tested positive in the ICE test were analyzed further by using RT-PCR. Results for 64 animals were positive, including 21 sheep (80.8%), 5 goats (100%), and 38 camels (77.6%) drawn from all of the regions studied: Khartoum, Blue Nile, Northern Sudan, Kassala, Kordofan, and Darfur . Of the 36 samples tested from Morocco, 16 yielded a positive RT-PCR.\n\n【18】##### Characterization of Strains Involved in the Infection of Sheep, Goats, and Camels in Sudan, and in Sheep in Morocco\n\n【19】Of the tissue samples tested, lung and lymph nodes were the most suitable for sequencing because they had the highest viral load, thereby yielding a sufficient amount of PCR product. Sequence also was obtained from several virus isolates, including the historical Gedarif isolate from 1971. The partial N gene sequences were obtained for 26 of the 64 samples from Sudan and 6 of the 16 samples from Morocco. Sequences were aligned with an extended set of PPRV isolate sequences that either were in the database or were retrieved from GenBank where they were described by Kwiatek et al. and Banyard et al.\n\n【20】In contrast to what would have been expected for isolates from eastern Africa, most of the PPRV strains collected in Sudan during 2000–2009 were clustered in lineage IV ; only a few remained in lineage III . Molecular typing also showed for the first time the presence of PPRV lineage IV in Morocco .\n\n【21】Lineage IV isolates from Sudan could be further distinguished into 2 clusters. In the first group, 17 sequences from all camel and goat isolates and some sheep isolates (camel Sudan Cam\\_1, Cam\\_3, Cam\\_8, Cam\\_169, Cam\\_223, Cam\\_264, Cam\\_268, Cam\\_304, Cam\\_330, Cam\\_352, Cam\\_318; sheep Ov\\_1, Ov\\_140, Ov\\_25, Ov\\_39300; goat Cap\\_9, Cap\\_1) showed 100% identity and matched with Saudi Arabia\\_1999\\_7 strain. Sheep Sudan Ov\\_41 differed from the previous sequences by 2 nt at position 68 (C to T) and 142 (A to G). The divergence of this cluster with consensus IV ranged from 1.2% (Cam\\_318) to 2% (Ov\\_41). Furthermore, the Saudi Arabia cluster was related closely to the 6 strains from Morocco collected from the 2008 outbreak, the closest differing by 4 nt.\n\n【22】A separate cluster containing only sheep samples collected during 2000–2008 (sheep Sudan Ov\\_10033, Ov\\_10034, Ov\\_23, Ov\\_Soba, Ov\\_Al Azaza) matched with 2 strains from central Africa (Cameroon\\_1997 \\[CAMER\\_1997\\] and Central African Republic\\_2004 \\[CAR\\_2004\\]) . This cluster was less homogeneous than the cluster previously described; nucleotide variations from the lineage IV consensus sequence ranged from 1.6% (Ov\\_Al Azaza) to 3.9% (Ov\\_23).\n\n【23】Only 2 isolates collected at the start of the study (in mid-2000) from sheep in western and eastern Sudan fell into lineage III. Sequences of these 2 field isolates remained similar to the historical strains of Sinnar 72 that was isolated 40 years earlier from the Blue Nile region , and with the Gedarif PPRV strain that was isolated from sheep in 1971 . In addition, these isolate sequences were close to an Omani strain, Ibri\\_1983  and a United Arab Emirates strain, Dorcas\\_1986 , but they differed from these strains with a nucleotide variation of 4.7%.\n\n【24】Because the dataset within the Saudi Arabia cluster was highly homogeneous, we compared an N phylogenetic tree  with an F phylogenetic tree  on a panel of viruses. These viruses were selected from the 2 lineage IV clusters defined in Figure 1 and from lineage III with additional PPRV from Asia, for which both gene sequences were available . The F phylogenetic tree confirmed the high homology within the Saudi Arabia cluster, and the cluster’s closeness to strains from Morocco. It also allowed for assignment of strains from Sudan to the 2 described clusters represented by the Arabia\\_1999\\_7 and the Central Africa strains.\n\n【25】Unlike the N gene amplifications, amplications with F-specific primers  resulted in a number of negative samples by RT-PCR because of mismatches occurring mainly in the F2 reverse primer . To circumvent this problem, another reverse primer (FPPRrev) was designed that matched with all lineages.\n\n【26】The sequences and the corresponding lineage classification generated in this study were collated to field information. Occurrence among species of the different lineages show that most of the isolates belong to lineage IV; all camel isolates grouped in the cluster Saudi Arabia, whereas only sheep isolates were found within the other cluster, Central Africa. Only sheep isolates were found in lineage III . Geographic and times distribution according to animal species and lineage in Sudan during 2000–2009 show that lineage IV was circulating as early as mid-2000 and that progressive substitution of lineage III in a large zone encompassing the eastern, northern, Blue Nile, and Khartoum regions has taken place since this date .\n\n【27】### Discussion\n\n【28】This study presents findings from PPRV surveillance in Sudan over a 10-year period and the results of tests conducted in Sudan and in Morocco during Morocco’s 2008 outbreaks . Analysis of 80 samples confirmed the wide distribution of PPRV throughout Sudan, which has been known since 1971 . Genetic characterization of 26 samples positive for PPRV also provided strong evidence of the introduction and spread of Asia PPRV lineage IV in the country. Surprisingly, the number of samples containing the indigenous lineage III decreased dramatically during the study.\n\n【29】Viruses collected in Morocco also were classified as lineage IV. The origin of the outbreaks in Morocco remains unknown, although Ayari-Fakhfakh et al. recently reported a PPRV seroprevalence in Tunisia of up to 7.45%, indicating that the virus is present across a wider area of northern Africa.\n\n【30】Analysis of the N and F genes showed that 2 clusters were identified in strains from Sudan: 1 related to a strain from Saudi Arabia, the other related to central Africa viruses. The 6 strains from Morocco collected during the 2008 outbreaks were related closely to the Saudi Arabia cluster.\n\n【31】Within the Saudi Arabia cluster, a remarkable genetic stability over a decade was observed; notably, all camel isolates fell into this cluster. The variability and nucleotide sequence was slightly higher for isolates in the central Africa cluster, within which only sheep were found. The inconsistent low genetic variability of isolates from Sudan may be a consequence of a species bias because of the limited contact between camels and sheep. When replicated in a single host that has a limited exposure to new variants, the viral genome thus may remain highly conservative. This hypothesis was verified previously for the measles virus over a shorter period . For the measles virus, it appears that the lower the level of circulation, the higher the sequence conservation .\n\n【32】Camels were not regarded as possible hosts for PPRV until 1992, when a number of authors reported PPRV seroconversion in these animals . The first documented outbreak of PPRV in camels, reported from Ethiopia in 1996, consisted of highly contagious respiratory syndromes with high illness rates but low death rates . The causative agent was confirmed to be a lineage III PPRV . This present study confirmed the etiology of the disease in camels through the virologic and epidemiologic investigations and the isolation of PPRV. Surveillance of camels furthermore allowed the virus to be detected in consecutive outbreaks in Kassala, eastern Sudan ; Atbara, northern Sudan ; and Tambool, Blue Nile region, Sudan  . Viruses from the same cluster also were recovered from sick sheep and goats in a large zone encompassing the eastern, northern, Blue Nile, and Khartoum regions as early as mid-2000. Under extensive pastoral farming conditions in these regions, camels may have served as a bridge with areas of northern Africa and contributed to the spread of a camel-derived strain of lineage IV, as seen in Morocco. In contrast, none of the camel and goat isolates were distributed in the central Africa cluster, although viruses of this cluster were cocirculating with camel viruses during the same period and in the same areas of northern Sudan, Khartoum, and Blue Nile.\n\n【33】The reason that isolates of lineage IV became predominant in Sudan, progressively replacing lineage III viruses, during the last decade remains unclear. Lineage IV has been present in Asia and in part of the Middle East for a long time, probably as low virulent strains . However, a constant rise of disease incidence recently has been associated with this lineage and suggests increased virulence . A virulent lineage IV strain may have been introduced in Africa during the 1990s, resulting in outbreaks in both camels and small ruminants. In parallel, rinderpest virus control and eradication may have favored the decline of cross-immunity in small ruminants and their increased risk for PPRV as predicted earlier by Taylor .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c04de11-ba32-4b01-8f7f-1a498e4ddd1f", "title": "Multitarget Test for Emerging Lyme Disease and Anaplasmosis in a Serosurvey of Dogs, Maine, USA", "text": "【0】Multitarget Test for Emerging Lyme Disease and Anaplasmosis in a Serosurvey of Dogs, Maine, USA\nOver the past 2 decades, the range of _Ixodes scapularis_ , the deer tick, vector of Lyme disease, anaplasmosis, babesiosis, and deer tick virus infections, has expanded in northern New England. Because Lyme disease and anaplasmosis affect humans and dogs ( _Canis lupus familiaris_ ), serosurveys of canids have proved useful for monitoring emergence of these infections. Sample selection may be confounded when dogs that are remotely exposed, vaccinated, or treated with topical acaricides are included. In recent years, however, the advent of a multitarget, in-clinic test kit (SNAP 4Dx; IDEXX Laboratories, Westbrook, ME, USA) has increased the scope and efficiency of these serosurveys. The SNAP 4Dx tests for heartworm antigen and antibodies to _Borrelia burgdorferi_ , _Anaplasma phagocytophilum_ , and _Ehrlichia canis_ on 3 drops of blood. Its sensitivity and specificity for antibodies against _B. burgdorferi_ and _A. phagocytophilum_ exceed 98% .\n\n【1】In Maine, deer ticks were first reported at a coastal site in 1988 and have since spread inland . Lyme disease has become a major public health problem; reported human cases reached 169 per 100,000 population in 1 mid-coastal county in 2008. Human cases of anaplasmosis and babesiosis are also being reported . In 1990, we conducted a statewide serosurvey to map _B. burgdorferi_ –positive dogs and to correlate their distribution with reported human cases. Four percent of 828 samples were seropositive for _B. burgdorferi_ , 89% of which were from dogs residing within 20 miles of the coast. No positivity was found among 102 dogs in the northern half of the state . Given the widespread acceptance of SNAP 4Dx tests by Maine veterinarians, we resurveyed dogs statewide in 2007 for exposure to _B. burgdorferi_ and _A. phagocytophilum._ Data from questionnaires to veterinarians and dog owners enabled assessment of the influence of the use of Lyme vaccines and topical acaricides on canine serologic test results.\n\n【2】### The Study\n\n【3】From 87 veterinary clinics solicited in 2007, we selected 47 on the basis of their size and geographic distribution. Each was supplied with 15–30 SNAP 4Dx kits (contributed by IDEXX Laboratories). Veterinarians were instructed to obtain samples from all dogs routinely tested for heartworm. In northern areas, where heartworm is rarely tested for, they were asked to collect samples from dogs undergoing surgery. They recorded each dog’s age, town of residence, Lyme disease vaccination status (ever or never vaccinated), and the test results. Each dog owner completed a form (99.6% response rate) to describe the dog, its function, history of unexplained lameness, travel history (town, state, visited within the past year), history of tick infestation, and use of tick control products (yes or no).\n\n【4】We summarized test results to town and county levels. We used Spearman rank correlation tests to examine associations between canine seropositivity, human Lyme disease cases reported to the Maine Center for Disease Control and Prevention (Augusta, ME, USA) , and the number of deer ticks submitted to our laboratory in 2007. We used _B. burgdorferi_ and _A. phagocytophilum_ test results and questionnaire responses to cross-tabulate responses and calculate the likelihood (odds ratios) of _B. burgdorferi_ and _A. phagocytophilum_ positivity as a function of risk factors by using χ 2  tests of association. We considered differences significant at p < 0.05. Analyses were conducted by using SAS version 9.2 for Windows (SAS, Cary, NC, USA).\n\n【5】Of 1,087 dogs tested across Maine’s 16 counties, 12.7% were _B. burgdorferi_ –positive and 7.1% were _A. phagocytophilum_ –positive ; 1.9% were co-infected. The distribution of all dogs seropositive for either pathogen is shown by town in Figure 1 . At the county level, canine _B. burgdorferi_ seropositivity among unvaccinated dogs correlated positively with the number of human Lyme disease cases reported for 2007(ρ Spearman  \\= 0.84; p<0.0001) and the number of deer ticks submitted to our laboratory for identification (ρ Spearman  \\= 0.63; p = 0.009). In Figure 2 , which shows statewide distributions by county north to south, only unvaccinated dogs are included in _B. burgdorferi_ –positive data shown. Dogs had been exposed to _A. phagocytophilum_ in all but 2 northern counties. At the town level, remarkably higher levels of canine _A. phagocytophilum_ seropositivity were found in southern coastal Cape Elizabeth (Cumberland County) (76.5%, n = 17) and York (York County) (58.0%, n = 19) than in towns in their immediate vicinity.\n\n【6】Overall, 54.3% of the dogs had been vaccinated against Lyme disease. Never-vaccinated dogs were 1.5× as likely to be seropositive for _B. burgdorferi_ than were vaccinated dogs (15.3% vs. 9.9%; p = 0.008) . Vaccine use was higher in 10 southern counties where Lyme disease has become endemic  than in the 6 northern counties where it is emerging (63.9% vs. 42.7%; p<0.0001) and correlated positively with the number of deer ticks submitted to our laboratory for identification in 2007 (n = 16 counties, ρ Spearman  \\= 0.63; p = 0.009). Two thirds of respondents said that their dogs had traveled out of town; however, no associations were found between _B. burgdorferi_ or _A. phagocytophilum_ seropositivity and the dog’s travel history. Three of 59 dogs in the northernmost county of Maine were _B. burgdorferi_ –positive, 1 of which had never traveled beyond its home town. Eighty-three percent of owners reported using acaricides. Despite the effective protection reported for topical acaricides , no difference in seropositivity between treated and untreated dogs was evident on the basis of their reported use . Unexplained lameness was 1.5× more likely in dogs that were only _A. phagocytophilum_ –positive than in those only _B. burgdorferi_ –positive (40.0% vs. 26.5%; p _<_ 0.06).\n\n【7】### Conclusions\n\n【8】This study demonstrates that risk of contracting Lyme disease has reached northernmost Maine and that anaplasmosis is now being transmitted to dogs throughout the lower half of the state. The study expands on nationwide SNAP 4Dx data documenting _B. burgdorferi_ and _A. phagocytophilum_ positivity in the southern half of the state . In southern coastal Maine, overabundant white-tail deer, appropriate habitat, and maritime climate all contribute to high densities of _I. scapularis_ ticks  and consequent disease transmission; thus, the remarkably high level of _A. phagocytophilum_ seroreactivity observed in the southern coastal towns of Cape Elizabeth and York calls for further work to understand the dynamics of the intense local emergence of this pathogen. The higher level of unexplained lameness in _A. phagocytophilum_ –positive dogs than in _B. burgdorferi_ –positive dogs is consistent with findings by Beall et al. who reported a 2.6× greater incidence of _A. phagocytophilum_ seroreactivity than _B. burgdorferi_ seroreactivity among 32 lame, non–co-infected dogs in Minnesota who were suspected of having either disease. The lameness also reflects the high percentage of _B. burgdorferi_ positivity among asymptomatic dogs . That _B. burgdorferi_ and _A. phagocytophilum_ seropositivity rates were essentially identical between dogs who had a history of travel and those who did not lessens concern about travel as a confounding variable, an exposure difficult to interpret in any event, given the spotty distribution of ticks even where Lyme disease is endemic .\n\n【9】In a recent study, Hamer et al. reported that a serosurvey of canines for _B. burgdorferi_ is ineffective in a region that includes areas with little _B. burgdorferi_ transmission, and less informative than analysis of ticks removed from dogs. The authors referred to the confounding influence of tick chemoprophylaxis. Our inability to detect an effect of topical acaricides may reflect their ubiquitous use for flea control and a lack of information on the frequency of their use. Although the widespread use of protective measures now complicates interpretation of serosurveys of canines, in selected dogs the availability of a reliable, multitarget test that is used routinely nationwide  remains a valuable and cost-effective method for documenting transmission of the agents of Lyme borreliosis and anaplasmosis, particularly in areas where disease is emerging.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6994b062-2909-4203-a26a-2a8187e1cd7e", "title": "Population-Attributable Risk Estimates for Campylobacter Infection, Australia", "text": "【0】Population-Attributable Risk Estimates for Campylobacter Infection, Australia\n### To the Editor\n\n【1】Many industrialized countries have a high incidence of _Campylobacter_ infections. An estimated 250,000 cases of _Campylobacter_ infection occur annually in the United States , and several sequelae compound the impact of these infections. The incidence of _Campylobacter_ infections is also important to policy-makers—in the United Kingdom it is used to assess foodborne disease–reduction strategies —and governments worldwide rely on the findings of epidemiologic and microbiological studies on _Campylobacter_ infection to shape their food-safety policies.\n\n【2】Population-attributable fractions provide added value in case–control studies by helping researchers identify the most important risk factors for a condition on the basis of risk association and frequency of exposure. In an analysis of data from a previous case–control study of _Campylobacter_ infection , Stafford et al. used population-attributable fractions to estimate the annual number of _Campylobacter_ infection cases among Australians \\> 5 years of age that were attributable to each risk factor from that study. Using this technique, they estimated that 50,500 cases annually can be attributed directly to eating chicken.\n\n【3】Population-attributable fractions have been defined as “the proportion of disease cases over a specified time that would be prevented following elimination of … exposure \\[to the specified risk factors\\]” . Therefore, removing exposure to factors not associated with disease risk will not affect disease incidence. Stafford and colleagues implicitly acknowledge this in their methods: “We calculated PARs \\[population-attributable risks\\] … for each variable that was significantly associated with an increased risk for infection.” It is surprising, therefore, that they subsequently included consumption of cooked chicken in their extrapolation, even though this exposure was not significantly associated with illness (adjusted odds ratio 1.4, 95% confidence interval 1.0–1.9, p = 0.06). Because they attributed 35,500 of the 50,500 cases of _Campylobacter_ infection to the consumption of cooked chicken, I believe that Stafford et al. overestimated the role of chicken consumption in cases of _Campylobacter_ infection by a factor of 3.4.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "08de5f8c-392f-4318-9b8b-bd0dbdee601f", "title": "The Fears of the Rich, the Needs of the Poor: My Years at the CDC", "text": "【0】To say that William Foege’s life has been inspirational is an understatement. It has encompassed literal life-and-death adventures, often intertwined with efforts to prevent and control some of the most destructive infectious diseases of the past half century. By recounting his time as an officer in the Epidemic Intelligence Service, his public health experiences in war zones, and his service as director of the Centers for Disease Control, Dr. Foege provides a front-row seat from which to watch the field of public health progress.\n\n【1】_The Fears of the Rich, the Needs of the Poor: My Years at the CDC_  is well organized chronologically, with contextual information clearly documented. Foege recounts many momentous CDC adventures and introduces the reader to numerous CDC contemporaries. However, although the title may be captivating, it does not reflect the focus of the book well. Many of Foege’s assignments involved underserved and poverty-stricken populations, and he occasionally touches on the effects of wealth and class on the health of these populations, but the book is primarily about his experiences at CDC. That being said, in stating his “three essentials for good public health programs,” Foege does say: “The first is the conviction that the basis for public health is to achieve health equity; therefore, the bottom line is social justice in health.”\n\n【2】This book was written with a public health audience in mind but would be a fascinating read for anyone with an interest in public health. Foege’s distinct colloquial voice and dry humor personalize the book, bringing his work to life. Foege provides a first-hand account of the recent history of humanity’s struggles with infectious diseases across the world, including progress made in the field of public health over the decades of his career. By sharing real stories of infectious diseases that devastated populations and how Foege and his colleagues grew into the leaders that helped bring these epidemics under control, the book provides guidance and inspiration for current and future public health workforces. Although public health can be a thankless profession, through this memoir Foege reminds us how indispensable the field is for our world’s future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f49214d2-fad2-40ec-a71e-b2e1d02cdc77", "title": "Seroprevalence of Chikungunya Virus, Jamaica, and New Tools for Surveillance", "text": "【0】Seroprevalence of Chikungunya Virus, Jamaica, and New Tools for Surveillance\n**To the Editor:** We read with great interest the recent article by Anzinger et al. who found a seroprevalence of 83.6% for chikungunya in pregnant women in the metropolitan region of Kingston, Jamaica. These data are similar to the seroprevalence found nationwide by the Jamaica Health and Lifestyle Survey III, 2016–2017 (Ministry of Health and Welfare, Jamaica), which was 82% among women, 78.5% among men, and 80.4% overall. These values enable estimating a total of 2,187,325 chikungunya infections in Jamaica during the 2014 epidemic. The government of Jamaica reported 1,420 cases of chikungunya to PAHO in 2014 and no deaths , even correcting for the proportion of unapparent infections, the proportion of cases captured by passive surveillance was <0.1%. Although there were no officially reported deaths in Jamaica, 2 cases of newborn deaths from chikungunya were reported , and 1 study found 2,499 excess deaths  during the epidemic period. The increase in mortality was greater for the extremes of age, but it occurred in several age groups .\n\n【1】Anzinger et al.’s results reinforce the findings of Sharp et al. who showed the importance of active surveillance to assess chikungunya burden. Through active surveillance implemented in Puerto Rico, it was possible to verify that 8% of symptomatic cases of chikungunya identified were captured by passive surveillance. In addition, passive surveillance identified 7 deaths, whereas active surveillance was able to confirm 31 deaths from chikungunya. However, 1,310 excess deaths were reported during the Puerto Rico epidemic in 2014 .\n\n【2】The introduction of chikungunya in the Americas has brought greater complexity to surveillance in the region, which includes some low-resource countries. It is essential to establish active and viable surveillance tools and, perhaps, new case definitions in order to better assess the population burden of this disease and the complications of acute and chronic cases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4a5442ea-91ea-4b34-bf74-eec56bbcb244", "title": "Erratum, Vol. 13, July 14 Release", "text": "【0】Erratum, Vol. 13, July 14 Release\nIn the article “Adoption of a Tai Chi Intervention, Tai Ji Quan: Moving for Better Balance, for Fall Prevention by Rural Faith-Based Organizations, 2013–2014” the author inadvertently omitted a citation:\n\n【1】> 3\\. Li F, Harmer P, Fisher KJ, McAuley E, Chaumeton N, Eckstrom E, et al. Tai Chi and fall reductions in older adults: a randomized controlled trial. J Gerontol A Biol Sci Med Sci 2005;60(2):187–94.\n\n【2】 We regret any inconvenience this omission may have caused.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1f740fb8-49b7-47f1-9dca-9c4ab4c3f413", "title": "Consequences of Failing to Investigate", "text": "【0】Consequences of Failing to Investigate\nGiovanni Battista Foggini , Laocöon (c. 1720). Bronze, 22 1/16 in × 17 5/16 in × 8 5/8 in/56 cm × 44 cm × 21.9 cm. Digital image courtesy of the Getty’s Open Content Program, The J. Paul Getty Museum, Los Angeles, CA, USA\n\n【1】High-consequence pathogens cause diseases such as Ebola virus disease, Rift Valley fever, Nipah virus disease, hantavirus pulmonary syndrome, measles, smallpox, and anthrax. Some such pathogens have the potential to spread rapidly and to cause epidemics. One of the tactics used to control the spread of such pathogens is careful investigation of cases.\n\n【2】Knowing where the danger lies, whether assessing the threat for human infection with high-consequence pathogens or defending a besieged city, is crucial to protecting the health and well-being of the public. Perhaps careful investigation might have changed the outcome of the Trojan War, a mythical protracted, bloody Bronze Age conflict pitting the kingdoms of Troy and Mycenaean Greece against one another.\n\n【3】The Trojan War is one of the most celebrated events in Greek mythology, and many incidents from that epic war have proven to be irresistible subjects for artists, who have reimagined them in sculptures, frescoes, sketches, and paintings. One oft-depicted incident is the ghastly death of the Trojan priest Laocöon and his two sons, Antiphas and Thymbraeus, by serpents.\n\n【4】The reddish-brown bronze statue of Laocöon featured on this month’s cover was fashioned by Italian artist Giovanni Battista Foggini. It is one of a series of small bronzes he created for displaying on tabletops or desks. Foggini used his talent and connections to become the leading court artist and architect to Cosimo III de’ Medici, the Grand Duke of Tuscany. Foggini frequently drew ideas from events portrayed in ancient classical literature. His bronze statue was inspired by the marble sculpture of Laocöon’s death unearthed in Rome in 1506 and later displayed in the Vatican museums.\n\n【5】In this bronze statue, Foggini recreated an event that proved pivotal in ending the war. Following a decade of skirmishes, sieges, counterattacks, and infighting on both sides, the Greek army feigns its withdrawal and leaves a massive, hollow wooden horse near the city’s gates as an offering for the gods. Laocöon senses treachery and counsels the Trojans either to investigate what is inside the horse or to burn it. When it seems his argument has swayed the majority, the gods intervene and dispatch a pair of sea serpents to silence Laocöon. In his towering epic poem the _Aeneid_ , Virgil describes the serpents’ approach:\n\n【6】Looping in giant spirals; the foaming sea  \nHissed under their motion. And they reached the land,  \nTheir burning eyes suffused with blood and fire,  \nTheir darting tongues licking the hissing mouths.\n\n【7】The _Aeneid_ tells us that the serpents “squeezed with scaly pressure” and “fastened their fangs in those poor bodies,” crushing their victims before vanishing with their bodies. Struggling with his foes, the powerful figure of Laocöon dominates the sculpture. His rippling muscles, flowing hair, and thick beard stand in contrast to his smaller and smooth-limbed sons. The J. Paul Getty Museum, where the sculpture resides today, notes that Foggini is recognized for his “exactitude in anatomical modeling and an equally precise and expert finishing of details.”\n\n【8】After witnessing the excruciating deaths of the priest and his sons, the dazed Trojans reason that the wooden horse was intended as a sacred gift for the gods, and they move this trophy inside the walls of Troy. An elite Greek force sequestered in the horse emerges during the night and opens the gates for the Greeks army, which has stealthily returned. The Greeks overwhelm the unprepared Trojans, sack the city, and win the war. Laocöon had properly understood the ruse and his admonition had proven accurate.\n\n【9】Bad decisions and character flaws were not solely responsible for the fall of Troy. The major Greek deities had their own interests in the outcome, and their many intercessions caused death and mayhem on both sides before the final reckoning. Historian Barbara Tuchman explains, “Taking sides and playing favorites, potent but fickle, conjuring deceptive images, altering the fortunes of battles to suit their desires, whispering, tricking, falsifying, even inducing the Greeks through deceit to continue when they are ready to give up and go home, the gods keep the combatants engaged while heroes die and homelands suffer.”\n\n【10】Had the Trojan leaders conducting their own careful investigation to see whether the Greek forces had actually retreated and to see what was lurking inside the horse, then the Greeks might not have succeeded in executing perhaps the best-known subterfuge described in literature. Foggini’s bronze statue reminds us that the consequences of dismissing admonitions and failing to investigate can be catastrophic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8a56b483-4d30-4f71-9f40-6b29c06ebce2", "title": "ICD-9 Codes and Surveillance for Clostridium difficile–associated Disease", "text": "【0】ICD-9 Codes and Surveillance for Clostridium difficile–associated Disease\n_Clostridium difficile_ –associated disease (CDAD) is the most common infectious cause of healthcare-associated diarrhea . Recent studies suggest both the incidence and severity of CDAD may be increasing , but no national surveillance system exists to track CDAD rates. Some studies have used International Classification of Diseases, 9th Revision (ICD-9) codes of hospital discharges to study CDAD rates . The validity of this method has not been studied. We compared CDAD rates determined by ICD-9 codes to rates determined by _C. difficile_ –toxin assays at a tertiary-care hospital to determine the sensitivity and specificity of ICD-9 code–based CDAD surveillance.\n\n【1】### The Study\n\n【2】Data were collected electronically on a retrospective cohort of patients admitted to Barnes-Jewish Hospital in Saint Louis from January 1, 2003, through December 31, 2003. Patients who had only 1 admission of <48 hours and neonates were excluded. Electronic charts were reviewed for patients who had a positive _C. difficile_ –toxin assay or the ICD-9 code indicating _C. difficile_ –associated disease (008.45)  .\n\n【3】A case of CDAD was defined as a patient with a positive _C. difficile_ –toxin assay (Tech Laboratory _C. difficile_ tox A/B II toxin assay \\[Tech Laboratory, Blacksburg, VA, USA\\]) or pseudomembranes seen on colonoscopy. Because the hospital laboratory performs a _C. difficile_ –toxin assay only on unformed stool samples and stool toxin testing is ordered based on clinical suspicion of CDAD, all patients with a positive toxin assay were considered CDAD case-patients.\n\n【4】Data were analyzed with SPSS 12.0 for Windows (SPSS, Inc. Chicago, IL, USA). Statistical analyses included κ, χ 2  , and Mann-Whitney U tests. A 2-sided p value of 0.05 was considered significant. This study was approved by the Washington University Human Studies Committee.\n\n【5】A total of 45,486 admissions among 28,417 unique patients were included in the analysis . A _C. difficile_ –toxin assay was ordered during hospitalization for 3,630 (8%) of these admissions. Toxin assays were positive (CDTA+) in 662 (18%) admissions. The _C. difficile_ ICD-9 code was assigned to 745 admissions (ICD9+). The breakdown of admissions, according to toxin assay and ICD-9 status, was as follows: 506 had both a positive toxin assay and received the ICD-9 code (concordant; CDTA+/ICD9+), 156 had a positive toxin assay but no ICD-9 code (CDTA+/ICD9–), and 239 received the ICD-9 code but did not have a positive toxin assay (CDTA–/ICD9+) . The concordance between toxin assays and ICD-9 codes was good (κ = 0.72, p<0.01). The overall mean CDAD rate by ICD-9 codes (16.4/1,000 admissions) was significantly higher then the mean rate by toxin assays (14.6/1,000 admissions) .\n\n【6】The median number of days from admission to stool collection was greater in admissions with a positive toxin assay but no ICD-9 code (CDTA+/ICD9–) than in concordant (CDTA+/ICD9+) admissions (6.0 days vs 3.0 days, p<0.01) . The first positive stool sample was collected within 48 hours of discharge for 68 (44%) of admissions with a positive toxin assay only (CDTA+/ICD9–) admissions, compared with 72 (14%) of concordant (CDTA+/ICD9+) admissions (p<0.01).\n\n【7】Upon chart review, documentation of a previous history of CDAD was evident in 142 (59%) of the ICD-9 code only (CDTA–/ICD9+) admissions. A _C. difficile_ –toxin assay had been ordered in 137 (57%) of all ICD-9 code only (CDTA-/ICD9+) admissions. One-hundred thirty (54%) had at least 1 stool negative for _C. difficile_ toxin.\n\n【8】Overall, 92% of patients with positive toxin assay (CDTA+) and 90% of patients with ICD-9 code only (CDTA–/ICD9+) received antimicrobial drug treatment for CDAD . Metronidazole was prescribed in 187 (78%) of the ICD-9 code only (CDTA–/ICD9+) admissions, compared with 591 (89%) of patients with a positive toxin assay (CDTA+) (p<0.01). For 75 (31%) of the ICD-9 code only (CDTA–/ICD9+) admissions, oral vancomycin was prescribed, compared with 130 (20%) of patients with a positive toxin assay (CDTA+) (p<0.01).\n\n【9】Thirty-four cases of CDAD were missed by _C. difficile_ –toxin assay results and subsequently identified through chart review (3 missed positive toxins, 26 CDAD patients transferred from other facilities, 3 positive outpatient toxin assays, and 2 diagnosed by colonoscopy), which brought the total number of cases with positive CDAD diagnostics to 696. ICD-9 codes correctly identified 540 of these cases and correctly classified 44,741 admissions as non-CDAD admissions (sensitivity 78%, specificity 99.7%). When the CDAD rate by toxin assays was adjusted for the additional cases, the adjusted CDAD rate was 15.3/1,000 admissions. This rate was not significantly different from the unadjusted CDAD rate by toxin assay results (RR 0.95, 95% CI 0.86–1.06) or the rate by ICD-9 codes alone (RR 1.07, 95% CI 0.97–1.19).\n\n【10】### Conclusions\n\n【11】Overall, there was good correlation between _C. difficile_ –toxin assay results and ICD-9 codes. Initially, the CDAD rate by ICD-9 codes appeared higher than the rate by toxin assays. However, once the additional CDAD cases identified through chart review were added, this difference was not significant.\n\n【12】Admissions with only a positive _C. difficile_ –toxin assay and no _C. difficile_ ICD-9 code (CDTA+/ICD9–) were more likely than concordant (CDTA+/ICD9+) admissions to have their first positive toxin assay within 48 hours of discharge. For these admissions, toxin assay results may not yet have been back at the time of discharge or CDAD may not have been considered a primary diagnosis by the physician and therefore not captured by the medical coders.\n\n【13】Antimicrobial drug treatment patterns suggest ICD-9 code only (CDTA–/ICD9+) admissions were patients who were more likely to have a history of CDAD. Metronidazole is first-line therapy for CDAD at our institution. Oral vancomycin is reserved for recurrent or severe cases. The observation that more ICD-9 code only (CDTA–/ICD9+) patients received oral vancomycin indicates that recurrent CDAD may have been suspected in these patients. In these patients, CDAD appears to have been diagnosed on the basis of the patient's history and symptoms instead of by a positive _C. difficile_ –toxin assay. This pattern has been previously reported .\n\n【14】True CDAD cases may have been misclassified among the controls. A patient who did not have a positive _C. difficile_ –toxin assay, who was not assigned the CDAD ICD-9 code, and whose diagnosis was made by colonoscopy would have been missed. However, misclassification is unlikely for two reasons. First, after charts were reviewed, only 2 additional patients were identified whose diagnosis was made by colonoscopy alone. Second, the detection of CDAD cases transferred from other institutions indicates that CDAD cases diagnosed by methods other than the toxin assays are being captured by ICD-9 codes.\n\n【15】Use of ICD-9 codes to study CDAD rates has advantages and disadvantages. ICD-9 codes are readily available from billing databases. In the absence of a national surveillance system for CDAD, ICD-9 codes provide a standard method for determining CDAD rates that can be used at all types of hospitals. However, because ICD-9 codes are assigned at discharge and not on the date of diagnosis, determining which cases are hospital acquired and which are community acquired is difficult. Also, ICD-9 codes are assigned by medical coders, who may not be able to accurately identify a patient's principal diagnoses as well as a physician or medical professional. Despite these limitations, ICD-9 codes can likely be used to identify CDAD cases and track CDAD rates when _C. difficile_ –toxin assay results are not available.\n\n【16】##### Details on ICD-9 Codes\n\n【17】The International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9) code used in this study was 008.45, \"intestinal infection due to _Clostridium difficile_ ,\" and is the only ICD-9 code related to CDAD. To apply this code, medical coders must have documentation in a patient's medical record by the treating medical providers that a patient's gastroenteritis or colitis is due to _C. difficile_ . Positive laboratory tests alone are not sufficient to warrant application of the code. At our institution, ICD-9 coding occurs, on average, 5–7 days after a patient is discharged from the hospital.\n\n【18】The ICD-9 system of classifying hospital discharge diagnoses is used throughout the United States. The definition for the code 008.45 is consistent between hospitals, although individual coding practices may vary. Although ICD-9 codes have limitations, they are readily available from administrative databases and have been used frequently to identify diagnoses and classify comorbidities .\n\n【19】A move to the International Classification of Diseases, 10 th  Revision, Clinical Modification (ICD-10) system is anticipated for US hospitals but the exact time of this transition is not yet known. The ICD-10 system does include a code for CDAD (A04.7, Enterocolitis due to _C. difficile_ ), so the ICD-based system presented here could be modified to be used with the updated coding system.\n\n【20】*   Klabunde CN, Warren JL, Legler JM. Assessing comorbidity using claims data: an overview. Med Care. 2002;40(8 Suppl):IV-25–35.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b1908c23-5006-491c-a359-152ed054d301", "title": "Senecavirus A in Pigs, United States, 2015", "text": "【0】Senecavirus A in Pigs, United States, 2015\n**To the Editor:** Senecavirus A (SVA) has been sporadically identified in pigs with idiopathic vesicular disease in the United States and Canada . Clinical symptoms observed include ruptured vesicles and erosions on the snout and lameness associated with broken vesicles along the coronary band. A recent report characterized SVA in pigs in Brazil with similar clinical symptoms in addition to a higher proportion of deaths than would be expected in pigs 1–4 days of age . Several outbreaks of this infection in pigs were reported in the summer of 2015 in the United States; the more severe clinical features resembled those seen in outbreaks in Brazil . Subsequent testing by PCR of 2,033 oral fluid samples from material submitted during 441 routine diagnostic testing procedures (from 25 states) identified 5 SVA-positive cases (1%) . Besides affecting animal health, SVA infection is notable because its clinical symptoms resemble those caused by foot-and-mouth disease and vesicular stomatitis viruses. When vesicular disease is observed in US swine, mandatory reporting and testing of animals for foreign animal diseases are required.\n\n【1】In June 2015, we collected 25 nasal and 25 rectal swab specimens from healthy pigs at 5 pig markets in North Carolina (250 total samples), representing pigs from 5 producers per market; the pigs were commingled for <12 hours. Primary markets 1 and 2 were slaughterhouses that purchased top quality pigs. Secondary market 3 was a slaughterhouse that purchased lower quality pigs (primarily underweight or herniated pigs). Market 4 was a broker that purchased pigs for culling and resold them for slaughter. Market 5 was a culled pig slaughterhouse. At markets 1–4, animals were ≈20 weeks of age; at market 5, animals were >10 weeks of age.\n\n【2】We sampled the same sites a second time in August 2015. Again, we performed metagenomic sequencing on swab specimens pooled by producer (5 specimens per pool, 50 total pools per sampling) . Reads most similar to SVA were identified in numerous pools from samplings and at 4 different markets. Quantitative reverse transcription PCR (qRT-PCR) was performed at the Kansas State Veterinary Diagnostic Laboratory (Manhattan, KS, USA) on the original pooled samples and was positive for SVA (cycle threshold \\[C t  \\] <37) for 26 (52%) pools from June and 18 (36%) pools from August. Sites 2 (n = 1 pool positive), 3 (n = 10), 4 (n = 5), and 5 (n = 10) had positive results in June, and sites 3 (n = 10), 4 (n = 1) and 5 (n = 7) had positive results in August. Both specimen types had an approximately equal number of positive results. We carried out virus isolation on swine testicle cells (positive samples from the second sampling), and 100% cytopathic effects were observed for 5 samples that tested positive for SVA by qRT-PCR with C t  values 16–21.\n\n【3】Templated assembly of the metagenomic sequencing reads with the SVA prototype strain SVV-001 genome  yielded near complete genomes from 5 pools (GenBank nos. KT827249–KT827253). The polyprotein-encoding region of the genomes showed >99% pairwise identity to each other and were most similar to sequences determined from recent outbreaks in Brazil (97%–98% nucleotide and >99% amino acid identity). Analysis of the P1 region of the genome found >99% nucleotide identity between 2015 US SVA sequences and 97% identity to SVA from Brazil. The contemporary US SVA sequences were more distantly related to SVA from an outbreak in Canada in 2011 (95% identity) and to historical US sequences (87%–92% identity). To investigate SVA phylogeny, we performed ClustalW  alignment of P1 nucleotide sequences, followed by maximum-likelihood analysis using the best-fitting Kimura 2-parameter plus gamma distribution model of evolution. The 2015 US SVA sequences were most closely related to SVA sequences from Brazil; these sequences shared a common ancestor in Canada/11-55910-2011 .\n\n【4】Our results suggest that SVA commonly circulates in secondary and culled swine markets in North Carolina and that these strains are most similar to strains characterized in 2014–2015 in Brazil, which were associated with idiopathic vesicular disease and neonatal death. Little diagnostic testing is performed on culled animals, which may in part explain the discrepancy between 1% of oral fluids submitted for diagnostic testing being positive for SVA , compared with 72% of culled swine swab specimen pools in this study . The sole sample from primary markets that was positive for SVA by qRT-PCR had a C t  of 36.9, just below the negative cutoff of 37.\n\n【5】Further research is needed to address possible correlation between SVA and health status of animals sold at lower value to cull markets. A notable distinction between contemporary SVA in the United States and Brazil, however, is that all the US samples originated from healthy animals that showed no clinical symptoms. Given the high genetic similarity between contemporary US SVA sequences and those from Brazil, additional cofactors likely affect clinical disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d586c7c0-f985-4f67-9e71-7fac536e2865", "title": "Co-infection with Avian (H7N9) and Pandemic (H1N1) 2009 Influenza Viruses, China", "text": "【0】Co-infection with Avian (H7N9) and Pandemic (H1N1) 2009 Influenza Viruses, China\n**To the Editor:** Since the first case of avian-origin influenza (H7N9) virus infection was reported in March 2013 in Shanghai, China, this virus has caused ≈400 confirmed cases . According to the Chinese Influenza Weekly Report , during the winter of 2013–14, multiple influenza viruses that infect humans co-circulated in southern China; these viruses were influenza A(H1N1)pdm09, seasonal influenza A(H3N2), and influenza B viruses, together with avian influenza (H7N9) virus, Co-infection of humans with avian virus subtype H7N9 and human virus subtypes were reported in Jiangsu Province in June 2013  and in Zhejiang Province in January 2014 , respectively. We report a fatal case of co-infection with influenza (H7N9) and A(H1N1)pdm09 viruses, which was further confirmed by virus isolation and neutralization antibody detection.\n\n【1】In a retrospective study, we identified co-infection with A(H1N1)pdm09 and novel avian-origin H7N9 virus in a 61-year-old female patient from the urban area of Shanghai, China. She had a 3-year history of thrombocythemia. In October 2013, she received an annual vaccine that included an A/California/7/2009(H1N1)pdm09–like virus, an A/Texas/50/2012(H3N2)–like virus, and an influenza B virus, as recommended by the World Health Organization . On December 26, 2013, the patient experienced influenza-like symptoms; the next day, she was hospitalized. Despite oral administration of oseltamivir, a rapidly progressive pneumonia developed, and the patient died of multiple organ failure 9 days after admission. On December 31, 2013 (day 5), 1 serum and 2 throat swab samples were collected; both throat swab samples were positive for influenza A and A(H1N1)pdm09 virus, according to the universal and subtype-specific primers provided by the Chinese Center for Disease Control and Prevention . On January 3, 2014 (day 8), 1 bronchial secretion and 1 serum sample were also collected.\n\n【2】Recent DNA sequencing analysis of the stored bronchial secretion specimen, performed on a Roche 454 platform  (Roche, Basel, Switzerland) showed co-existence of H1, H7, N1, and N9 genomic segments, which were further confirmed by real-time reverse transcription PCR (RT-PCR). The cycle threshold values of H7 and H1 were 35.65 and 29.94, respectively.\n\n【3】We then determined antibody levels of the 2 serum samples collected on days 5 and 8 by using a microneutralization assay  with MDCK cells infected with 2 representative strains, A/Shanghai/4664T/2013(H7N9) and A/Shanghai/37T/2009(H1N1), respectively. The samples were serially diluted 2-fold (1:10 to 1:1,280) at a starting dilution of 1:10; titers were expressed as the reciprocal of the highest dilution of 50% neutralization. The microneutralization titers against novel subtype H7N9 virus were 160 and 320 for the samples collected on days 5 and 8, respectively. This finding indicated that the patient could have been infected by novel subtype H7N9 viruses ; microneutralization titers against A/Shanghai/37T/2009(H1N1) were 320 for both samples.\n\n【4】To determine whether the reassortment had occurred between the 2 subtypes of influenza A viruses during the co-infection, we isolated the virus strains from the bronchial secretion on either MDCK cells (A\\[H1N1\\]pdm09)  or embryonated eggs (H7N9) through several passages. The hemagglutination titers of the isolated H7N9 (A/Shanghai/Mix1/2014\\[H7N9\\]) and A(H1N1)pdm09 viruses (A/Shanghai/Mix1/2014\\[H1N1\\]) in allantoic fluid were 32 and 8, respectively. Cultures of the 2 isolates were subjected to RT-PCR for detection of 8 genomic segments. The RT-PCR products were sequenced on an ABI 3730XL Automatic DNA Analyzer by using an ABI Prism BigDye Terminator v3.1 Cycle Sequencing Kit (both from Applied Biosystems, Foster City, CA, USA). The full-genome sequences were submitted to the National Center for Biotechnology Information Influenza Virus Resource (GenBank accession nos. KJ946415, KJ946416 and KP058484–KP058489 for A/Shanghai/Mix1/201\\[H7N9\\] and KM607860, KM607861 and KP058490–KP058495 for A/Shanghai/Mix1/2014\\[H1N1\\]).\n\n【5】Phylogenetic trees were constructed by using the neighbor-joining method in MEGA 5.1  to estimate relationships with selected influenza A virus reference sequences obtained from GenBank and the Global Initiative on Sharing Avian Influenza Data database. DNA sequence analysis showed that no genetic reassortment had occurred between the 2 subtypes because only H7N9 or A(H1N1)pdm09 genomic fragments were found in the culture of the respective isolate . Phylogenetic analyses of the hemagglutinin and neuraminidase genes revealed that the A/Shanghai/Mix1/2014(H7N9) and A/Shanghai/Mix1/2014(H1N1) viruses were clustered into the clade of A/Shanghai/01/2014(H7N9)-like viruses   and A/Shanghai/6109/2014(H1N1)-like viruses , respectively. In addition, the phylogenetic trees of the 6 internal genes were close to those of H7N9 viruses or A(H1N1)pdm09-lineage viruses that had recently circulated in China .\n\n【6】Co-infections with H7N9 and other influenza subtypes have been reported from Jiangsu  and Zhejiang  Provinces as well as in our study, indicating the potential risk for co-infection during the influenza season in China. Although reassortment was not detected in this co-infection, a potential risk for emergence of a new pandemic strain by reassortment between these 2 viruses (with humans as mixing vessels) should not be ignored. To reduce the risk for emergence of new viral subtypes, the public health and scientific communities should enhance surveillance for co-infection with influenza (H7N9) virus and other influenza virus subtypes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9d403660-ad1e-4f20-bbf5-5dfc060c0b0b", "title": "Use of National Pneumonia Surveillance to Describe Influenza A(H7N9) Virus Epidemiology, China, 2004–2013", "text": "【0】Use of National Pneumonia Surveillance to Describe Influenza A(H7N9) Virus Epidemiology, China, 2004–2013\nSince 2004, the Chinese Center for Disease Control and Prevention (China CDC) has conducted surveillance for pneumonia of unknown etiology (PUE) to facilitate timely detection of novel respiratory pathogens, such as severe acute respiratory syndrome (SARS) and avian influenza. On March 31, 2013, health authorities in China reported the first human infection with avian influenza A(H7N9) virus to the World Health Organization . In response to the emergence of A(H7N9), China CDC and provincial and local CDCs introduced testing for A(H7N9) virus of all persons with reported PUE. As of May 3, 2013, a total of 127 laboratory-confirmed A(H7N9) cases, resulting in 24 deaths, had been reported from 10 provinces and municipalities in mainland China (hereafter referred to as affected areas). The median age of these case-patients was 62 years; most (71%) were males.\n\n【1】Most confirmed case-patients had severe disease , and an analysis of national influenza-like illness surveillance data has not found evidence of widespread A(H7N9)-associated mild illness . After preliminary epidemiologic and virologic information pointed to live-poultry markets (LPMs) as a possible source of infection , retail and wholesale LPMs were closed in several major cities in which A(H7N9) was confirmed, including Shanghai, Nanjing, and Hangzhou. The number of new cases declined in these cities after LPM closures .\n\n【2】However, these reports of A(H7N9) geographic occurrence, demographic patterns, and effectiveness of control measures depend not only on the number of confirmed A(H7N9) cases but also on surveillance and on reporting and testing patterns. Although the number of cases has been studied at length, reported cases are a function of surveillance, and A(H7N9) reporting and testing patterns have not been examined in detail. We describe the PUE surveillance system in China and analyze the proportion of tested persons who test positive in mainland China for A(H7N9) by province, age, and sex before and after LPM closures to assess the possible role of surveillance bias.\n\n【3】### Methods\n\n【4】##### Surveillance for PUE before A(H7N9) Emergence\n\n【5】From 2004 through March 2013, health care facilities of all types in China were required to report any patient who had no clear diagnosis and whose illness met 4 criteria. These criteria were 1) fever (axillary temperature \\> 38 ◦  C); 2) radiologic characteristics consistent with pneumonia; 3) reduced or normal leukocyte count or low lymphocyte count during early stages of disease; and 4) worsening of symptoms or no obvious improvement after 3–5 days of standard antimicrobial treatment.\n\n【6】Upper or lower respiratory tract specimens from each patient were tested for influenza A(H5N1) virus and for SARS-coronavirus (SARS-CoV) and, beginning in October 2012, for Middle East respiratory syndrome coronavirus. Some provinces also tested for seasonal influenza A (subtypes H1N1 and H3N2) and, after 2009, pandemic H1N1 2009 and B viruses, but this testing varied by province. If specimens were negative for A(H5N1) and SARS-CoV, no further testing was required. Data were collected on age, sex, location, occupation, and dates of illness duration and on who reported the case.\n\n【7】Cases were reported by clinicians directly to the China Information System for Disease Control and Prevention (CISDCP), the nationally notifiable disease reporting system, through an Internet-based platform. Before China CDC became involved in any response, expert consultation committees were required at the county, prefecture, and provincial levels to determine whether the case was SARS or A(H5N1) on the basis of clinical or laboratory evidence. If SARS and A(H5N1) were excluded and there was no other diagnosis, cases were designated as “disease of other unknown cause,” and no further investigation was conducted. However, for clusters of PUE cases, i.e. \\> 2 PUE cases for which an epidemiologic link was identified, the provincial CDC sent the specimens to China CDC for further testing if the provincial expert consultation committee could not provide a clear diagnosis, and China CDC would guide or become directly involved in the field investigation if needed.\n\n【8】##### Surveillance for PUE after A(H7N9) Emergence\n\n【9】In response to the emergence of A(H7N9), 3 key changes in this system were implemented. First, starting on March 31, 2013, all specimens from reported PUE cases were required to be tested not only for influenza A(H5N1) but also for seasonal influenza A, influenza B, and influenza A(H7N9) by real-time reverse transcription PCR . If a specimen was positive for influenza A but could not be subtyped, further testing would be performed. If test results for both influenza types A and B were negative, specimens would be tested for SARS-CoV and Middle East respiratory syndrome coronavirus. Second, local-level evaluation of cases was streamlined in early April 2013. After cases were reported, specimens were sent directly for testing to local and/or provincial CDCs, bypassing the expert consultation committees. Third, to avoid delay in A(H7N9) diagnosis, the fourth reporting criterion above (antimicrobial treatment failure) was replaced with a requirement that the pneumonia etiology could not be attributed to an alternative clinical or laboratory diagnoses. Clinicians were given flexibility to determine how to interpret this criterion, and specific tests were not specified.\n\n【10】Respiratory specimens collected from patients whose illnesses meet the modified PUE case definition are sent to the local and/or provincial influenza network laboratory for testing for A(H7N9). (The first A\\[H7N9\\] case in a province is confirmed by China CDC and subsequent cases by the provincial CDC.) In addition, as of April 5, clinicians could also specify whether a patient had a suspected or confirmed A(H7N9) case by using a separate specific case definition and laboratory evidence of possible A(H7N9) infection  and reported directly to CISDCP. In this analysis, we focused only on the historical and current performance of the PUE surveillance system.\n\n【11】To better understand testing patterns during the A(H7N9) outbreak, we looked at historical reporting in the PUE surveillance system from January 2004 through March 2013. We also examined all PUE cases reported to China CDC during March 30–May 3, 2013, and calculated the proportion positive for A(H7N9) by province and in different age and sex groups. To assess whether LPM closures helped control the epidemic and to account for any reduction in testing, we examined the number of confirmed A(H7N9) cases and the proportion of PUE case-patients who tested positive for A(H7N9) in the week before and the 2 weeks after LPM closures in Shanghai (population 30.5 million), Nanjing (population 8.2 million), and Hangzhou (population 8.8 million). The LPMs were first closed in these cities on April 6, April 8, and April 15, respectively. At the time the study was conducted, the estimated median incubation period of A(H7N9) infection was 6 days (interquartile range 4–7) . We separated postclosure results into those in the first and second weeks after closure in each LPM (1–7 days and 8–14 days, respectively) and compared proportions before and after LPM closure using a χ 2  test for trend. A Pearson χ 2  test was used to compare the proportion of men and women who tested positive for A(H7N9), and significance was defined by α<0.05. SPSS software version 19.0 (SPSS, Chicago, IL, USA) was used for statistical analysis.\n\n【12】### Results\n\n【13】During January 2004–March 30, 2013, a total of 1,016 cases were reported to the PUE surveillance system, of which 976 (96%) had a final diagnosis available. Thirty-nine (4%) cases were identified as A(H5N1), accounting for 91% of the 43 avian influenza A(H5N1) confirmed in humans in mainland China during 2005–2013. No SARS cases were identified. 744 (76%) PUE cases had no clear final diagnosis. In most months <10 PUE cases were reported, and a mean of 10 cases were reported each month (range 0–168). The number of reported cases increased during identified outbreaks, such as the SARS outbreak in 2004, when the system was first established, and avian influenza A(H5N1) outbreaks in humans during the winter and spring of 2005–06 and early 2009 .\n\n【14】During March 30–May 3, 2013, a total of 1,118 PUE cases were reported from 24 provinces, with earliest onset on January 26. PUE cases peaked at 61 per day on April 8, 2013, and then dropped rapidly in the following 3 weeks . A total of 1,002 (90%) PUE cases reported were from affected areas, which constitute 43% of the Chinese population, and 116 (10%) were from from unaffected areas (57% of the population). Most PUE cases were reported from Shanghai (468 \\[42%\\] of 1,118) and Zhejiang (388 \\[26%\\]). Of the 1,002 PUE cases from affected areas, 94 (9%) were confirmed as A(H7N9), which represents 74% of all 127 confirmed A(H7N9) cases in mainland China as of May 3. The remaining 33 cases were reported either through the influenza-like illness surveillance system (6 cases) or directly to CISDCP (27 cases).\n\n【15】Among the affected areas, Jiangsu reported the highest percentage of PUE positive for A(H7N9) (74%). This was followed by Hunan (33%), Henan (27%), Fujian (18%), Zhejiang (14%), Jiangxi (10%), Shanghai (4%), Beijing (3%), and Anhui and Shandong (0 cases each) .\n\n【16】Of all PUE cases from the affected areas, 288 (29%) occurred in persons <25 years of age; 399 (40%) were 25–59 years, and 315 (31%) were \\> 60 years. The number of PUE cases among female patients was lower overall (449 \\[45%\\] of 1,002) and in each age group except the 15–24-year and 25–59-year groups. Among persons \\> 60 years of age, many more men than women were reported through the PUE systems (198 men vs. 117 women) .\n\n【17】Of PUE cases confirmed to be A(H7N9), 1 (1%) was in the 5–14-year age group, 42 (45%) were in patients 25–59 years of age, and 51 (54%) were in patients \\> 60 years of age. The proportion of PUE cases positive for A(H7N9) was higher in adults (11% and 16% in persons 25–59 and \\> 60 years of age, respectively) than in children, teenagers, and young adults (0%, 1%, and 0% in persons <1–4, 5–14, and 15–24 years of age, respectively). Overall, more positive A(H7N9) cases occurred in men than in women (62 vs. 32), and men and women differed significantly in the proportion positive for A(H7N9) (11% vs. 7%, p = 0.027). In persons \\> 60 years of age, twice as many A(H7N9) cases occurred in men than in women (34 vs. 17), although the proportion of PUE cases that were positive for A(H7N9) was not significantly higher in men than in women (17% vs. 15%; p = 0.539) .\n\n【18】The total number of PUE reported cases declined after LPM closures in Hangzhou and Nanjing but increased in Shanghai in the 1–6 days after closure, then dropped in the 7–14 days after closure. The number of confirmed A(H7N9) cases in Shanghai and Hangzhou after officials closed LPMs declined from 11 and 15 cases, respectively, in the week before closure to 4 and 4 cases during the 1–7 days after closure. In the 8–14 days after closure, 1 and 0 cases were confirmed in those cities, respectively. The proportion of PUE cases positive for A(H7N9) also declined from 14% and 25% before closure to 2% and 12% 1–7 days later and 1% and 0% 8–14 days later, respectively (χ 2  test for trend, p<0.001 in Shanghai; p = 0.056 in Hangzhou). In Nanjing, 5 positive A(H7N9) cases occurred in the week before LPM closure, with 1 in the 14 days after closure (p = 0.564). When data from the 3 areas are combined, the number of positive cases declined from 31 cases in the week before closure (21% of PUE cases positive for A\\[H7N9\\]) to 8 cases (4% positive) 1–7 days after closure; it decreased further to 2 cases (2% positive) in the 8–14 days after closure (p<0.001). In Shanghai, >1.5 times the number of PUE cases were tested for A(H7N9) in the 8–14 days after LPM closure than before closure, although testing decreased in Hangzhou and Nanjing after LPM closure. These data suggest that the decline in absolute numbers was not a surveillance artifact but a real effect .\n\n【19】### Discussion\n\n【20】Our study examined the Chinese national PUE surveillance system and its utility during the influenza A(H7N9) outbreak in the spring of 2013. Historically, the PUE system had been underused, and reporting had been inconsistent. The number of reported PUE cases increased above minimum levels only during known outbreaks of A(H5N1) and SARS, the only pathogens for which there had been testing. We describe several changes made to the PUE system during the A(H7N9) outbreak that increased its sensitivity and timeliness, resulting in increased reporting; yet, we demonstrated low frequency of PUE reporting from unaffected provinces. Moreover, some provinces were clearly prescreening possible A(H7N9) PUE cases before reporting, which resulted in wide variations in percent positivity. Nevertheless, data from the PUE system demonstrated that 1) A(H7N9) cases were indeed more common in elderly persons; 2) men are at higher risk than women for PUE and A(H7N9) virus infection; and 3) the decline in reported cases after LPM closure probably reflects a true decline in the number of cases, not merely a decline in testing.\n\n【21】Historical data from the PUE surveillance system demonstrated that the system has consistently been underused. Before the A(H7N9) outbreak, it was used to report most A(H5N1) cases in China. However, the PUE system was not (and still is not) used consistently. In 1 study, which examined all cases of community-acquired pneumonia in 6 hospitals over 1 year (April 1, 2008–March 31, 2009), 442 (29%) of the 1,506 community-acquired pneumonia cases met PUE criteria and should have been reported to the PUE system . In contrast, only 1,016 PUE cases in all of China were reported during a 9-year period. We showed that the number of cases surged when an outbreak occurred, either during the SARS outbreak or during publicized A(H5N1) outbreaks. This surge may reflect enhanced administrative requirements from health authorities  or enhanced clinician awareness of respiratory viruses.\n\n【22】Before April 2013, the administrative burden of reporting a case to the PUE system gave clinicians little incentive to participate. Reporting a PUE case triggered requirements, such as cooperating with an epidemiologic investigation, collecting specimens, providing clinical information for expert committees, and moving patients to isolation wards. In return, clinicians received little information; 76% of reported PUE cases had no final specific diagnosis, and clinicians were told only whether the cases were SARS or A(H5N1). Streamlining the PUE reporting system and decreasing the requirements involving expert consultation committees probably contributed to the large increase in PUE reporting during the A(H7N9) outbreak; more PUE cases were reported during the study period than in the prior 9 years of PUE surveillance.\n\n【23】During the A(H7N9) epidemic, reporting increased substantially only in affected areas, leading to huge variation between provinces in PUE reporting. Of most concern is that during the A(H7N9) outbreak, areas with no human cases grossly underreported PUE cases. Most (92%) reported PUE cases were negative for A(H7N9) and were probably caused by other etiologies. Thus, we would expect to see a comparable number of PUE cases reported in affected and unaffected areas. However, 68% of all PUE cases were reported from Shanghai and Zhejiang province; together, these 2 provinces constitute only 6% of the total population of China. By contrast, only 10% of all PUE cases were reported in the 21 unaffected provinces; these constitute 57% of the population .\n\n【24】In addition to surveillance bias away from provinces unaffected by the A(H7N9) outbreak, variation probably occurs among provinces in the screening that precedes reporting a PUE case. Some provinces reported PUE cases before extensive testing; in other provinces, clinicians may send specimens directly to the local CDC for testing first, then report only those that had a positive result as PUE cases. This scenario was documented in a previous analysis of the PUE system during 2004–2007 . The discrepancy in the proportion of positive cases in different provinces (74% in Jiangsu vs. 4% in Shanghai) indicates that prescreening was most likely a factor in PUE reporting practices during the A(H7N9) outbreak. The sharp decline in PUE reporting noted after mid-April also might reflect increased availability of A(H7N9) testing at the local and provincial levels. The ability to test for A(H7N9) locally enables clinicians and local health officials to bypass PUE reporting and instead report a case to CIDSP as a suspected or confirmed A(H7N9) case; this raises the question of how much the PUE system will be used if future large outbreaks of A(H7N9) occur.\n\n【25】Despite the limitations of the PUE reporting system, it yielded important epidemiologic information. First, we found that the older age distribution of persons with A(H7N9) was probably true and not a result of surveillance bias because testing was extensive among young persons, and the percentage positive increased in persons \\> 60 years of age. This contrasts sharply with A(H5N1) cases in China in which the median age of infection is 26 years . Second, more PUE cases were reported among men who were also more likely to test positive; the reason may be that men are at higher risk for any pneumonia, perhaps because of underlying respiratory comorbidities, but the increased percentage positive for A(H7N9) among men also suggests a specific risk for A(H7N9), especially among working-aged men. The reason may be that these men are more exposed to poultry through occupation or behavior. Third, PUE surveillance analysis suggested that LPM closure did reduce A(H7N9) transmission to humans, whereas a previous report indicated that the number of new A(H7N9) cases declined after LPM closure , this decline could have reflected decreased testing and not an actual decline in A(H7N9) incidence. Our analysis shows that, although the number of persons reported with PUE and tested for A(H7N9) virus decreased after LPM closure, the proportion of PUE testing positive for A(H7N9) also decreased in the weeks after closure. Investigation of A(H7N9) cases in China has found that 77% of cases for which information was available have had poultry exposure, many through contact with LPMs . In the 1997 outbreak of A(H5N1) in Hong Kong, poultry were culled and LPMs closed . These measures controlled the outbreak, and A(H5N1) disease was not reported again in humans until 2003.\n\n【26】Our study has several limitations. First, the incidence of A(H7N9) in the 3 areas with LPM closure that we studied may have decreased regardless of LPM closure. It is possible that LPM closures were associated with—but not the cause of—the waning number of cases. This decreasing incidence could have been the case had there been a short wave of infected poultry passing through LPMs. Also possible is that, as with A(H5N1), A(H7N9) may be seasonal in birds and therefore in humans, with lower transmission during the spring and summer months. Second, although we demonstrate that the proportion of PUE cases positive for A(H7N9) decreased after LPM closure, the substantial decrease in reporting and testing immediately after market closure in Hangzhou may have resulted in missed cases and exaggerated the apparent effect of closure. In addition, how much increased local testing for A(H7N9) may have affected PUE reporting is unknown.\n\n【27】This study identified several major problems with the PUE surveillance system, including low and uneven levels of participation and inconsistency among provinces in how the system is used. Given its potential value in monitoring future A(H7N9) activity, the system’s overall objectives and reporting procedures should be further evaluated. The continued threat of additional viral adaptation to human hosts leading to increased transmissibility lends added urgency to the ongoing improvement of the PUE system to better understand the epidemiology of A(H7N9), detect outbreaks, and evaluate control measures.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cd15fbf5-c33f-41d9-97d7-1e1e7cf98c3e", "title": "Emerging Invasive Group A Streptococcus M1UK Lineage Detected by Allele-Specific PCR, England, 2020", "text": "【0】Emerging Invasive Group A Streptococcus M1UK Lineage Detected by Allele-Specific PCR, England, 2020\nUpsurges in invasive group A _Streptococcus_ (GAS) infections have been widely reported in England and elsewhere , emphasizing the need to examine the relationship between circulating _S. pyogenes_ that cause pharyngitis and scarlet fever and cases of invasive disease. Although many factors, such as exposure history, underlying conditions, viral co-infection, and genetic susceptibility, might increase susceptibility to _S._ _pyogenes_ infection, strain-specific virulence might also be crucial.\n\n【1】In England, where both scarlet fever and invasive _S. pyogenes_ infections are notifiable, pronounced upsurges in scarlet fever were recorded over an 8-year period , but subsided during the COVID-19 pandemic. During the 2015–16 season, a notable increase in invasive infections was observed that had not been evident previously . Both scarlet fever and invasive infections were associated with the emergence of M1 UK  , a new sublineage of _emm_ 1 _S. pyogenes_  that appeared to outcompete the highly successful, contemporary epidemic _emm_ 1 M1 global  strain, which emerged and spread globally during the 1980s . Despite an unchanged phage repertoire, M1 UK  strains produce more superantigenic scarlet fever toxin SpeA (streptococcal pyrogenic exotoxin A) than contemporary M1 global  _S. pyogenes_ strains .\n\n【2】_emm_ 1 _S. pyogenes_ strains are highly virulent  and disproportionately associated with invasive infections; any increase in the prevalence of _emm_ 1 strains in persons with pharyngitis or scarlet fever is, therefore, a public health concern. Known distribution of M1 UK  is largely limited to those countries undertaking and reporting genome sequencing . M1 UK  has been identified in other countries in Europe, from a single isolate in Denmark  to dominant status in the Netherlands . The lineage has also been reported in North America; the Public Health Agency of Canada reported that 17/178 (10%) of _emm_ 1 isolates from 2016 were M1 UK  . This finding contrasts with a reported M1 UK  frequency of just 0%–2.8% of _emm_ 1 isolates in the United States, according to the Active Bacterial Core surveillance system of the US Centers for Disease Control and Prevention; however, the low US frequency was associated with severe infections . Of note, most reports used genomic data that were >5 years old, so a reappraisal of prevalence is needed. A recent study in Australia using data through 2020 indicated expansion of M1 UK  in Queensland and Victoria . The authors identified acquisition of an additional phage encoding superantigen genes _ssa_ and _spec_ and a single-nucleotide polymorphism (SNP) implicated in SpeA upregulation in the M1 UK  lineage. Multicountry increases in GAS infections  since pandemic restrictions were lifted underscore the importance of increasing global surveillance of lineages that have potentially enhanced fitness, such as M1 UK  .\n\n【3】### The Study\n\n【4】Genetic distinction between M1 UK  and M1 global  strains is possible by using whole-genome sequencing to detect the 27 SNPs that characterize the M1 UK  lineage , but sequencing technology is not available in all countries. We designed an allele-specific PCR (AS-PCR) method to detect M1 UK  \\-specific SNPs in the _rofA_ , _gldA_ , and _pstB_ genes. We chose amplification targets to separate M1 UK  and M1 global  strains but also to identify strains from less common intermediate sublineages that had only 13 or 23 of the 27 M1 UK  \\-specific SNPs . We optimized PCR conditions for each pair of amplicons by using DNA from control strains for each lineage . Collecting bacterial samples from patients was part of routine clinical care; collecting surplus samples after anonymizing patient information was approved by the West London National Research Ethics Committee .\n\n【5】To evaluate allele-specific PCR, we tested whether the _rofA_ and _pstB_ primers correctly identified lineages of 27 newly genome-sequenced noninvasive _emm_ 1 _S. pyogenes_ strains isolated during 2017–18 and collected by the infection bioresource at Imperial College. We artificially enriched the isolates for M1 global  strains to ensure adequate numbers of each lineage: 8/27 isolates were M1 global  , and 19/27 were M1 UK  . PCR amplification of _rofA_ and _pstB_ alleles from those isolates assigned 100% of strains to the correct lineage previously identified by sequencing .\n\n【6】To evaluate the ability of AS-PCR to identify _emm_ 1 isolates from M1 global  , M1 UK  , and intermediate sublineages , we tested 16 strains from 2013–2016 that comprised 4 isolates each of M1 global  , M1 13snps  , M1 23snps  , and M1 UK  lineages . SNPs were correctly detected in the _rofA_ gene from all M1 13snps  , M1 23snps  , and M1 UK  isolates . SNPs were also correctly detected in _gldA_ from all M1 23snps  and M1 UK  isolates but not M1 global  or M1 13snps  isolates .  Finally, SNPs in _pstB_ were only identified in M1 UK  isolates .  Thus, in all cases, SNP profiles determined by AS-PCR were consistent with strain-specific genome sequences.\n\n【7】In England, submission of all isolates from invasive infection is requested by the UK Health Security Agency reference laboratory for _emm_ genotyping. _emm_ 1 isolates are routinely the dominant genotype among invasive sterile-site isolates, typically representing 20%–30% of invasive infections. During 2020, when incidence of common respiratory infections was reduced by COVID-19–related public health interventions, _emm_ 1 _S. pyogenes_ frequency varied each month from 0%–24% of all invasive infections and decreased toward the end of the year. We subjected all 305 invasive _emm_ 1 _S. pyogenes_ isolates from 2020 that were available for this study to AS-PCR . AS-PCR identified M1 UK  \\-specific SNPs in _rofA_ , _gldA_ , and _pstB_ in 278/305 (91.1%) of isolates, which were, therefore, assigned to the M1 UK  lineage. No SNPs were detected in the remaining 27 isolates, which were assigned to M1 global  ; no intermediate lineage _emm_ 1 strains were identified in isolates collected during 2020 by using AS-PCR.\n\n【8】We performed Western blot analysis of 10 M1 UK  isolates identified by AS-PCR. We confirmed that SpeA production was similar to M1 UK  strains tested previously; however, we did not quantify SpeA production.\n\n【9】### Conclusions\n\n【10】The longevity of emergent _S. pyogenes_ lineages in a population is difficult to predict. Although an _emm_ 89 emergent  acapsular lineage has disseminated globally , an emergent _emm_ 3 SpeC-producing lineage, associated with upsurges in scarlet fever and invasive infections, ceased to be detectable within a few years . Taken together with previously reported genome-sequenced _emm_ 1 isolates , AS-PCR results indicated that the M1 UK  lineage continued to expand among invasive _S. pyogenes_ isolates from 2016 to the end of 2020 in England.\n\n【11】Increased invasive GAS activity in several countries  indicates a need for ongoing surveillance of novel lineages, given the potential public health effects. AS-PCR provides a readily available method to detect M1 UK  that is straightforward and, for screening purposes only, can be simplified by using only _rofA_ primers to identify M1 UK  or associated sublineages. A limitation of our study is that the assay requires validation in reference laboratory settings. AS-PCR does not replace genome sequencing as the preferred method for surveillance of highly pathogenic bacteria, but sequencing is not widely available and is expensive.\n\n【12】_emm_ 1 strains have accounted for >50% of invasive infections in children in England during the 2022–23 season . Our results indicate that the M1 UK  lineage remained dominant in England and expanded to the end of 2020, and contact tracing in 2018 demonstrated a high frequency of secondary acquisition of M1 UK  in school outbreak settings . Given the recognized association between _emm_ 1 _S. pyogenes_ and fatal outcome of invasive infections , enhanced surveillance for the M1 UK  sublineage is warranted. We conclude that AS-PCR is a readily available method to determine whether _emm_ 1 _S_ . _pyogenes_ isolates belong to the M1 UK  clade without need for genome sequencing and will improve surveillance of invasive GAS strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a3ae5f64-e645-4eec-9968-4d1759e55778", "title": "Hospital Preparedness and SARS", "text": "【0】Hospital Preparedness and SARS\nOn March 5, 2003, the first patient with severe acute respiratory syndrome (SARS) died in Toronto, Ontario, Canada. This index patient was a 78-year-old woman who, upon returning to Toronto from Hong Kong, transmitted the new variant coronavirus to her family . On March 7, her son was admitted to hospital, and he subsequently died on March 13. His unrecognized disease led to nosocomial transmission of this disease in Toronto . As of August 28, 2003, a total of 375 cases of suspected and probable SARS had been identified in Toronto; most of these cases occurred within healthcare facilities . A minority of cases were related to household and community transmission, most acquired after hospital visits. The last community-acquired case of SARS-associated coronavirus (SARS-CoV) infection was identified on April 13, 2003 .\n\n【1】On the basis of the absence of new cases two incubation periods after the last case, barrier precautions were downgraded in Toronto hospitals on May 8, 2003. However, on May 23, the medical community realized that nosocomial transmission of SARS to patients and visitors had been occurring on a single ward in North York General Hospital (NYGH) throughout April and early May . Staff had been protected by personal protective equipment and therefore, because of the absence of staff cases and an epidemiologic link, the identification of the cases was delayed.\n\n【2】On May 23, a second phase of the outbreak (SARS II) was declared at NYGH, and the hospital was designated as a level-3 institution, which indicated that SARS had been transmitted through unprotected exposure . Consequently, a 10-day work quarantine for all staff was imposed. While this action prevented a major staffing shortage, it required all staff to wear N95 respirators at all times in the facility. When not at work, staff were at home, in home quarantine. During SARS II at NYGH, 55 patients were admitted with a diagnosis of SARS, and another 200 patients were assessed in the emergency department. We discuss the multidisciplinary and cross-departmental response used to establish SARS care at NYGH and offer recommendations that may help other hospitals prepare for an outbreak of SARS or any other infectious agent.\n\n【3】### Building Preparation and Engineering\n\n【4】##### Wards\n\n【5】At the peak of SARS II, NYGH had 46 patients with investigated, suspected, or probable SARS in respiratory isolation in private, negative-pressure rooms . This was accomplished because two nearly constructed, empty, hospital wings were available. Within 72 hours of the declared outbreak, two units were converted into SARS wards, one with 22 rooms, the other with 27. Each private, negative-pressure room had no drapes and contained minimal equipment: one chair, a bedside table, a hamper for discarded linen, a garbage bin for contaminated equipment, and a hand sanitizer. Outside each room, a table held the personal protective equipment for staff entering the rooms. Outside each SARS ward were change-rooms for staff to change in and out of scrubs at the beginning and end of each shift.\n\n【6】##### Intensive Care Unit\n\n【7】We learned that the intensive care unit (ICU)’s capacity is one of the factors that governs the number of SARS patients a hospital can manage. Since approximately 20% of patients with SARS require ICU care, the maximum number of patients with SARS that a hospital can manage can be calculated . At NYGH, the ICU’s capacity was 22 rooms, which allowed the care of approximately 80 SARS patients in the hospital at any time. The ICU had private, self-contained, glass-enclosed rooms. The adjacent ward was a clean unit containing the standard ICU equipment as well as tables with personal protective equipment. Outside the ICU, a change-room was stocked with fresh scrubs and linen-disposal bins.\n\n【8】##### Emergency Department and SARS Assessment Clinic\n\n【9】Similar principles were applied to the emergency department, which had eight private, negative-pressure rooms. This department was closed to the public because of the hospital’s level-3 status but stayed open for hospital employees and recently discharged patients. At the request of the provincial government, a SARS clinic was established to assess members of the public with symptoms of SARS. This clinic was constructed within 1 week in the 1,782–square foot ambulance bay. It contained eight negative-pressure isolation rooms built with pipe framing and plastic walls and ceilings. Areas for clerical work, registration, and changing personal protective equipment were also created. Other components included an area for case review, a lead-lined x-ray room, and an x-ray viewing room. A 40 x 20–foot tent was placed at the entrance of the clinic to provide ample space for a waiting area .\n\n【10】##### Engineering and Maintenance\n\n【11】The above-mentioned wards were considered SARS units, and the same engineering principles were applied to each. Each patient’s room met the minimum requirement of six air changes per hour. Twice daily, the engineering department tested the negative-pressure status in all SARS units and patient rooms and presented the results to the hospital administration. In addition, an external company conducted daily assessments of the air circulation within the rooms. Highly trained engineering staff and clear blueprints and plans of the facility’s ventilation system were needed to implement all the required changes.\n\n【12】We recommend that hospitals take the following building preparation and engineering steps to prepare for an emerging infectious disease: 1) complete an assessment of their current facilities and capabilities; 2) ensure that current blueprints of the facility’s ventilation system are always accessible to facilitate expedient changes; 3) work with all relevant departments, including the proposed wards, ICU, and emergency department, to develop a strategy that allows for the rapid construction or conversion of the maximum number of private, negative-pressure rooms; and 4) identify in advance a timeline and areas of responsibility for constructing the maximum number of private rooms.\n\n【13】##### Personnel\n\n【14】Our biggest challenge during the outbreak was insufficient personnel. Most personnel were required at the beginning of each phase and were then needed for approximately 3-1/2 weeks. Although more personnel were recruited, they did not start work for 1 to 2 weeks after the initial influx of patients. We required additional nurses, unit managers, infection control personnel, housekeeping, ward clerks, and supply stocking and inventory staff. Physicians recruited to manage the outbreak included primary-care doctors, infectious diseases consultants, hospital epidemiologists, public health physicians, emergency department physicians, and radiologists.\n\n【15】##### Nursing Staff\n\n【16】In the SARS wards, we aimed for a high ratio of nurses to patients. At the beginning of the outbreak, the ratio was approximately 4–5 patients per nurse, a potentially dangerous ratio that could lead to transmission. During SARS II, the ratio was 1:1 if the patient was on oxygen requiring hourly monitoring and 2:1 for more stable patients. In the ICU, the ratio was two nurses per patient, which allowed for one nurse in the room and another outside. To avoid transmission, nurses were extensively trained in SARS patient care, the use of personal protective equipment, the potential risks for transmission, and preparedness for high levels of stress.\n\n【17】##### Housekeeping\n\n【18】Dedicated and well-trained housekeepers were very important during the outbreak. Our housekeepers were trained in proper cleaning techniques and the use of personal protective equipment.\n\n【19】##### Physicians\n\n【20】At NYGH, we used a most responsible physician (MRP) model for patient care, i.e. a primary care doctor (including emergency department physicians, general internists, family physicians, surgeons, and anesthesiologists who volunteered to care for SARS patients) directly cared for the patients. The patient-to-physician ratio for the MRP was 5–10 SARS patients per physician. One infectious disease consultant was assigned to each SARS ward, and one also covered the SARS ICU for a ratio of 20 to 30 SARS patients per infectious disease consultant. The MRP conducted all direct patient care, reviewed all cases, wrote the notes and, at mid-day, reviewed all cases with the infectious disease consultant, who was also responsible for alerting the MRP of new developments pertinent to SARS, for making changes in patient management, consulting with the emergency department for SARS assessments, and communicating with the onsite public health physician and outbreak management team. Training the emergency department physicians in SARS procedures was vital; our emergency department physicians became adept at evaluating potential SARS cases, which resulted in fewer patients being referred to infectious disease consultants.\n\n【21】We recommend that hospitals take the following personnel steps in advance of an emerging infectious disease event: 1) Calculate the maximum number of beds available for conversion to negative-pressure rooms on the wards, in the ICU, and the emergency department. The resulting figure will indicate the number of staff (including nurses, allied healthcare workers, and physicians) required from day 1. 2) Develop a system to identify those staff members who would be available to start working as part of the outbreak team within 24 hours. Such staff must be prepared for training and able to commit their services for a minimum of 3 to 4 weeks. 3) Generate a plan to meet the extra cost of hiring vital personnel (the greatest economic cost during such an outbreak). 4) Prepare for intensive training of both skilled staff and all other hospital employees in the use of personal protective equipment and infection-control procedures.\n\n【22】##### Departmental Work Load\n\n【23】The SARS outbreak affected every hospital department. After NYGH was identified as a level-3 institution, only the two SARS units, the SARS ICU and the emergency department, continued to function. Most non-SARS patients were discharged, which left only 20 patients in this 400-bed hospital. However, every department’s continued contribution was needed. Occupational health played a major role in reviewing which healthcare workers could return to work. Environmental services and housekeeping were greatly affected by additional requirements throughout the hospital. Security ensured that unauthorized persons did not enter the hospital; a security staff member, with a nurse, escorted SARS patients on transports between departments, logging the date, time, and persons involved in the transfer. Diagnostic imaging staff and equipment were strained. Because of the isolation measures, SARS patients’ x-rays were taken with portable machines; two technicians were needed. The laboratory was overloaded due to the increased number of daily samples, which required a blood technician system to collect all samples at 7:00 a.m. Pharmacy staff handled increased orders and organized Health Canada’s approval requirements for ribavirin and interferon.\n\n【24】We recommend that hospitals conduct a review of each department’s existing capacity and capabilities for handling an outbreak. Strategies should then be developed to address any deficiencies.\n\n【25】##### Policies, Procedures, and Documentation\n\n【26】During the outbreak, Toronto hospitals developed standardized systems for all implicated procedures, including code blues, patient transfers, and other infection-control procedures.\n\n【27】Of vital importance was the policy for patient oxygenation and early transfer to the ICU. SARS patients in need of oxygen can deteriorate rapidly, requiring intubations within 6 to 12 hours, a high-risk procedure that can lead to further nosocomial transmission . At NYGH, patients who had an oxygen saturation < 92% and needed any amount of supplemental oxygen had their vital signs with oxygen saturation monitored every 2 hours instead of every 4 . If patients required more than 4 L/min of oxygen, the monitoring increased to every hour. Once patients required >6 L/min of oxygen, they were transferred to the ICU. Such early transfer allowed for elective, early intubation to be done in a controlled environment by minimal staff, which resulted in a reduced risk for transmission. Staff at intubations wore T4 Personal Protection Systems (Stryker Instruments, Kalamazoo, MI), although these items were not proven to have be beneficial .\n\n【28】Specific standardized forms were developed, including emergency department SARS consult sheets that included all the appropriate key questions regarding exposure, date of onset of symptoms, specific symptoms, laboratory investigations, and chest x-ray findings; admission order forms, which allowed for standard orders for the nurses and MRPs ; and progress note forms, which documented symptoms, temperature, oxygen saturation and requirement, laboratory and x-ray results, and the daily plan. These documents both streamlined the process of daily review of 10 to 20 patients and standardized the level of care.\n\n【29】We recommend that hospitals do the following: 1) consider obtaining existing documentation and policies from other hospitals, such as NYGH; 2) develop an organized process of documentation that will facilitate an organized response to patient needs; and 3) assess different systems equivalent to the T4 Personal Protection Systems (Stryker Instruments) for particle removal efficiency and air-flow rate to choose the optimal system before an outbreak.\n\n【30】##### Infection-Control Service\n\n【31】Before the SARS outbreak, NYGH had only two infection-control practitioners (ICPs). During the outbreak, additional ICPs were recruited, and hospital epidemiologists from other institutions arrived to create a system and infrastructure for infection control. We expanded an extant infection-control team to organize all the policies, systems, and structures for future infection control. Extra staff included a coordinator, four ICPs, a nurse clinician, a public health nurse, an administrator, a hospital epidemiologist (an infectious disease specialist with training in hospital epidemiology), and a clinical infectious disease physician. Members of this team made daily ward rounds to answer questions and conduct surveillance for fever and symptoms. In addition, they met several times a week to review policies, coordinate teaching, and address all other issues. To ensure consistent levels of infection-control practice, a system that reviewed the quality of practice was established: it was essential for the ICPs to maintain some degree of authority on these issues.\n\n【32】Because infection-control issues are vitally important, a hospital should do the following: 1) identify an appropriate number of ICPs for the hospital size ; 2) include a qualified hospital epidemiologist on the infection-control team; 3) include a public health physician or designate on the team; 4) maintain constant vigilance during symptom surveillance; 5) sustain excellent standards of staff training and communication and apply continuous monitoring of infection-control practices; 6) ensure that all policies and documentation go through this team; and 7) provide the team with the necessary authority to work effectively throughout the hospital.\n\n【33】##### Personal Protective Equipment and Fit-Testing of Respirators\n\n【34】The constant availability and use of personal protective equipment (much of which was disposable) was essential during the outbreak, including the following: N95 respirators, goggles, face shields, hair nets, gowns, and scrub suits. Specific policies and procedures were developed for putting on and removing personal protective equipment. For example, before entering a SARS patient’s room, a staff member wore an N95 respirator, goggles, face shield, hair net, a gown over scrubs, and two pairs of gloves. The order in which personal protective equipment was removed when a staff member exited a patient’s room was exact. For example, inside the room by the door, the first pair of gloves was removed, followed by the hair net, the face shield, and the second pair of gloves; next, hands were washed with quick-drying antiseptic solution, and the gown was carefully removed; then the hands were washed again before the staff member left the room. In the hallway, hands were washed, goggles removed and disposed of, hands washed again, respirators removed, hands washed, and finally, a new N95 respirator was donned. During SARS II, the provincial government issued a directive requiring that respirators be fit-tested at all hospitals. This recommendation proved to be difficult to implement at NYGH because we were in the midst of an outbreak.\n\n【35】We recommend that, in advance of an outbreak, hospitals should do the following: 1) prepare clear policies on the proper use of personal protective equipment during such an outbreak; 2) ensure that adequate supplies of essential items required for personal protective equipment will be available or are already stocked; 3) have staff fit-tested for respirators and document results or obtain forms of higher level respiratory protection that do not require fit-testing (e.g. loose-fitting powered air-purifying respirators).\n\n【36】##### Education and Training\n\n【37】All staff had to be trained and educated on every aspect of SARS, including the proper use of personal protective equipment, risks to themselves and their families, and infection-control policies and procedures. At NYGH, training was conducted by a group of nurse clinicians assigned to each unit. Daily full-day, mandatory training sessions for all staff working on the SARS wards were created and included topics such as the proper way to don personal protective equipment, the psychological impact of SARS, and general infection-control practices.\n\n【38】We recommend that hospitals do the following: 1) include all departments in training, preferably in advance of an outbreak; 2) develop a program for certification in “Readiness to Manage an Infectious Medical Disaster Outbreak”; 3) develop a “train-the-trainer” model together with continued quality assurance monitoring.\n\n【39】##### Public Health Outbreak Management Team\n\n【40】One unique feature of this outbreak was the formation of a mobile public health outbreak management team. It included two physicians, a manager, and five investigators (either public health nurses or inspectors), who were stationed beside the hospital coordinators and infectious disease specialists and remained onsite 24/7 for 4 weeks. This setup promoted outstanding communication and excellent relations between all parties, which allowed rapid exchanges of information that led to swift contact tracing and the quarantine of persons identified as having had unprotected exposure to a SARS patient. The public health nurses attended morning ward meetings to review management plans for patients admitted overnight, followed patient progress directly on the wards, and attended the regular infection-control team meetings.\n\n【41】We recommend that the healthcare system do the following during an outbreak: 1) facilitate effective communication between public health and hospital staff; and 2) establish a common information technology platform that allows for a streamlined, accessible flow of data between jurisdictions.\n\n【42】##### Management and Administration\n\n【43】At NYGH, a 24-hour command center administered all the details connected to managing the outbreak and answered all questions. Department heads met daily at 9:00 a.m. which allowed them to impart important information to their staff. At 11:00 a.m. the SARS management committee held a meeting at which all decisions for the hospital were subsequently implemented. The key frontline players—including unit managers from each ward, infection control, infectious disease, and the chief of medicine—met daily to exchange information and properly manage the outbreak. Forums were regularly held by the hospital president to answer questions from the staff. All media contacts went through the single public relations department, to transmit a single message during this controversial time. We recommend that hospital administrations be prepared to play a pivotal role during such an outbreak.\n\n【44】##### SARS Follow-up Clinic\n\n【45】Patients recovering from SARS were discharged to remain in quarantine at home for an additional 7–10 days. Follow-up appointments were made for days 11 and 30 postdischarge. A single physician coordinated and ran the SARS follow-up clinic out of the emergency department. Again, a list of standard, step-by-step procedures was made for the assessment, and a checklist was designed . The physician assessed symptoms and reviewed follow-up laboratory tests. Convalescent-phase serologic tests were conducted 3–4 weeks after the onset of symptoms, and if results of a polymerase chain reaction test were positive for SARS-CoV, the test was repeated. Follow-up chest x-rays and, occasionally, computed tomographic scans were performed. A psychologist and a social worker provided psychological assessment and support during the follow-up visits. For SARS follow-up clinics in other hospitals, we recommend that, using standardized and organized methods, the hospitals prepare plans for an extensive follow-up system.\n\n【46】##### Psychological and Psychosocial Management\n\n【47】Psychological and psychosocial support for both patients and the entire hospital staff was necessary during the SARS outbreak. Staff were affected by the fear of contracting and transmitting this new disease; SARS patients experienced stress because of their isolation, fear for their lives, guilt, anger, anxiety, and depression.\n\n【48】At NYGH, we put together a SARS psychological team (including social workers, psychiatric crisis nurses, psychiatrists, and infectious disease specialists) that developed a plan to manage the psychological impact on patients and staff. Patients were seen at least twice weekly. A social worker phoned each patient on days 2 and 6 postdischarge to follow up. A psychiatric crisis-line phone number was given to every patient in case he or she needed urgent attention. An outpatient system with psychiatrists was put in place to handle posttraumatic stress syndrome. These services were also established for all hospital staff. A quiet staff room was available for relaxation or discussion with a team member. After the outbreak, debriefing sessions were held with trained psychologists and counselors. We recommend that hospitals’ psychiatry departments, in conjunction with their hospital administration, develop a response plan for a crisis outbreak.\n\n【49】### Research\n\n【50】Research is imperative during such an outbreak, particularly for a new disease. The physicians and staff who were managing the outbreak had minimal time to do research, but they had many urgent questions. At NYGH, infectious disease and internal medicine physicians from Health Canada, Toronto Public Health, and other organizations came in to help with the research. The ethics board was prompt in attending to required approvals, often a lengthy process. Funding for such emergency research—another important factor—was provided.\n\n【51】To facilitate emergency research, we recommend that hospitals do the following: 1) identify potential members for collaborative research before an outbreak; 2) establish an expedient process for ethics approval; and 3) be prepared to alert funding agencies for the need for additional funding and support.\n\n【52】### Conclusion\n\n【53】A multidisciplinary approach to manage the second phase of the SARS outbreak in Toronto was undertaken at NYGH. This successful approach was only possible with the hard work and collaboration of many people as well as open and active communication maintained among all departments, employees, and patients. Many lessons, taken from this experience, can be applied by hospitals preparing themselves for such an outbreak. Finally, the policies, procedures, and documents developed at our institution and others are freely available to other centers to review and adapt as appropriate.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f2f80052-8ee1-4c6a-afbb-4f8590160d81", "title": "Vibrio vulnificus in Taiwan", "text": "【0】Vibrio vulnificus in Taiwan\nAwareness of _Vibrio vulnificus_ as a threat to human health has evolved during the past 30 years . In Taiwan, Yuan et al. first reported _V. vulnificus_ infection in a patient with septicemia and leg gangrene in Kaohsiung County in 1985 . Chuang et al. described an additional 27 cases during a 5-year period from May 1985 to July 1990 and demonstrated three major discernible syndromes: primary septicemia, wound infection, and gastrointestinal diseases. The disease had a high mortality rate (41%) . Chuang et al.’s report was also the first to demonstrate the recurrent nature of this disease. Since then, many clinicians and researchers from Taiwan have reported risk factors and the clinical spectrum of this disease on the basis of an increasing number of reported cases . Many factors have been associated with increased vulnerability of Taiwanese people to _V. vulnificus_ infection. These include the high prevalence of hepatitis B or C virus infection-related hepatic diseases (liver cirrhosis and hepatoma), the environment, and the popularity of preparing and eating raw or undercooked seafood . These factors have drawn considerable interest to finding optimal therapeutic regimens for this infection, as well as to identifying the pathogenesis, ecology, and the reservoirs of this microorganism.\n\n【1】We describe the clinical features of 84 recently identified patients with _V. vulnificus_ infection treated from 1995 to 2000 in Taiwan and report the results of molecular typing of 50 isolates of _V. vulnificus_ from these patients. We also summarize the recent advances in understanding this newly recognized disease from the Taiwan perspective.\n\n【2】### Disease Prevalence\n\n【3】Taiwan is a small island situated off the southeast coast of the Asian continent with a population of >22 million people. Figure 1 shows the annual number of reported cases and the estimated prevalence of _V. vulnificus_ infection (per 10 6  persons) from 1985 to 2000 in Taiwan . Two peaks occurred: one in 1988 to 1990 (0.354–0.450/10 6  persons) and the other in 1996 to 2000 (0.606–1.237/10 6  persons). Most reported cases (>90%) occurred in residents of southern Taiwan. In Taiwan, the temperature of surface seawater is usually >18°C, except for February, when it is 17°C–23°C . Nearly all cases occurred in the late spring to early fall (April–October), when the seawater temperature is 20°C–29°C. The peak months for infections were June–August (summer season) when the temperature of surface seawater in Taiwan was approximately 26°C–29°C .\n\n【4】The reasons for the increased rate of _V. vulnificus_ during the past 2 decades are not fully understood. The extent to which the increasing number of cases may be caused by increasing disease activity or improved recognition by clinicians or laboratory workers is also unclear. Since the first report of _V. vulnificus_ infection in 1987 and subsequent reports in both humans and environmental studies, clinicians in Taiwan have become increasingly aware of the clinical features of this disease, and laboratory workers more likely to understand how to isolate and identify this pathogen accurately.\n\n【5】### Environmental Habitants and Reservoirs\n\n【6】The occurrence of _V. vulnificus_ infections in cultured shrimp and eels has been reported in Taiwan . A monthly survey on the distribution of _Vibrionaceae_ in seawater from five major harbors in Taiwan was conducted from July 1991 to February 1994 . Among the 1,167 _Vibrionaceae_ isolates, _V. vulnificus_ accounted for 67 (5.7%) . This finding indicates that the organism exists autochthonously around the coastal waters or aquatic habitats in Taiwan. Most isolates (91%) from marine water and oysters were indole-negative (biotype I) but some belonged to biotype II (ornithine decarboxylase- and mannitol-positive) . Strains of _V. vulnificus_ serovar E (also belonging to biotype II) avirulent for eels, which were recovered from water and oysters, were reported . Ribotyping analysis of the environmental isolates indicated a great genetic divergence among these isolates . More than half of the environmental isolates exhibited virulence in mice, indicating that these isolates might be pathogenic to humans . In addition, saline and aqueous ethanol extract (lectins) from some marine algae collected from the northeastern coast of Taiwan had marked antibacterial activity against _V. vulnificus_ isolates recovered from the northeastern coast of Taiwan . Further study is needed to explore the symbiosis between marine algae and their associated marine vibrios.\n\n【7】### Clinical Features and Outcomes\n\n【8】Clinical information from 84 patients _V. vulnificus_ infection treated from 1995 to 2000 was obtained from medical records from five hospitals in Taiwan . These hospitals, with a capacities of 1,500 to 2,000 beds, included National Taiwan University Hospital, Taipei; Chi-Mei Medical Center and National Cheng-Kung University Hospital, Tainan; Chang Gung Memorial Hospital-Kaohsiung, Kaohsiung; and Kaohsiung Veterans General Hospital, Kaohsiung. Most of the patients (73%) were male. More than 80% of these patients had various underlying medical conditions with liver disease (particularly hepatitis B or C virus infection-related diseases), which accounted for more than half of the patients, followed by diabetes mellitus and steroid use. Nine patients (16.3%) had exposure to marine injuries (caused by fish or crab bones or eating raw fish) or marine environments (swimming in coastal seawater or raising fish). Although 11 (20%) patients had preexisting skin wounds, exposure of the skin wounds to salt water was not known. More than 60% of these patients had a cutaneous infection, and 50% had necrotizing fasciitis. Approximately three fourths of the patients with necrotizing fasciitis had septic shock. Characteristic cutaneous lesions in patients with necrotizing fasciitis and wounds associated with bacteremia attributable to _V. vulnificus_ are shown in Figure 2 . Twenty patients (23.8%) had primary septicemia, and 3 were complicated with septic shock.\n\n【9】Similar to the previous findings, we found no patients with gastroenteritis caused by to _V. vulnificus_ . Most patients with gastroenteritis or diarrheal illness in Taiwan do not seek care at the large teaching hospitals; they also do not usually have a stool culture, which might explain the lack of patients with gastrointestinal illness attributable to _V. vulnificus_ .\n\n【10】A third-generation cephalosporin plus minocycline was used as the definite treatment regimen in 46% of patients. Among 57 patients with cutaneous lesions, 49 (86.0%) had some form of surgical treatment (incision and drainage, débridement, fasciotomy, and amputation). The overall case-fatality rate was approximately 30% , which was similar to that reported previously among patients seen from 1995 to 1990 . Patients with spontaneous bacterial peritonitis had the highest case-fatality rate (50%), followed by necrotizing fasciitis (40.5%). Patients with cellulitis had the lowest case-fatality rate (6.7%).\n\n【11】### Antimicrobial Drug Resistance and Treatment Options\n\n【12】MICs were determined and interpreted by using the MIC interpretive criteria for _Enterobacteriaceae_ recommended by the National Committee for Clinical Laboratory Standards . All isolates of _V. vulnificus_ in Taiwan, which were collected from the previous studies, were susceptible to the following agents (MIC90): ampicillin (1 μg/mL), carbenicillin (4 μg/mL), cephalothin (4 μg/mL), cefamandole (2 μg/mL), cefotaxime ( < 0.03–0.06 μg/mL), ceftriaxone ( < 0.03 μg/mL), cefoperazone (0.12 μg/mL), aztreonam (8 μg/mL), imipenem ( < 0.03–0.12 μg/mL), gentamicin (4 μg/mL), amikacin (8 μg/mL), tetracycline (0.25 μg/mL), minocycline (0.06–0.25 μg/mL), chloramphenicol (0.5 μg/mL), and fluoroquinolones: ofloxacin ( < 0.03 μg/mL), lomefloxacin (0.12 μg/mL), ciprofloxacin ( < 0.03-0.03 μg/mL), levofloxacin (0.03 μg/mL), moxifloxacin (0.06 μg/mL), gatifloxacin (0.06 μg/mL), and sparfloxacin (0.06 μg/mL) . Few isolates were not susceptible to ceftazidime (MIC 32 μg/mL) and moxalactam (MIC 32 μg/mL) . All isolates were resistant to clindamycin (MICs \\> 256 μg/mL) . In vitro synergism between cefotaxime and minocycline against _V. vulnificus_ isolates was documented by time-kill study . Time-kill study also demonstrated that fluoroquinolones at concentrations of two times the MIC had a persistent inhibitory effect on _V. vulnificus_ for >48 hours .\n\n【13】In vivo study using a mouse model of _V. vulnificus_ infection clearly indicated that combination therapy with cefotaxime and minocycline is distinctly superior to therapy with cefotaxime or minocycline alone . A similar effect of newer fluoroquinolones as single agents compared with the cefotaxime-minocycline combination was also demonstrated in the treatment of severe experimental _V. vulnificus_ infection .\n\n【14】On the basis of the in vitro and in vivo animal studies, along with clinical outcome analysis, combination therapy with cefotaxime (2 g every 6 h intravenously) and minocycline (100 mg every 12 h intravenously) was recommended for treating adult patients with bacteremia and severe soft-tissue infection caused by _V. vulnificus_ . For severe soft-tissue infection (necrotizing fasciitis, tissue necrosis with gangrene change, and myositis), early and aggressive surgical interventions (incision and drainage, débridement, fasciotomy, and amputation) are important in saving the life of the patient.\n\n【15】### Pathogenesis\n\n【16】More than 90% of _V. vulnificus_ isolates whose biotypes were determined belonged to biotype I, which is well known to be pathogenic for humans . In 1997, Chuang et al. first demonstrated that severe damage of the connective tissue of a mouse by _V. vulnificus_ wound infection could be mediated by a recombinant extracellular metalloprotease (able to digest collagen and elastin) . Lee et al. also illustrated that extracellular products of _V. vulnificus_ were lethal to fish (moribund black porgy, _Acanthopagrus_ _schlegeli_ ) . Genes ( _vvp_ and _empV_ ) encoding the metalloprotease and gene ( _vllY_ ) encoding a novel hemolysin of _V. vulnificus_ were subsequently cloned and characterized .\n\n【17】Hor et al. showed that isogenic protease-deficient (PD) mutant of _V. vulnificus_ was as virulent as its parent strains in mice infected intraperitoneally and was 10-fold more virulent in mice infected through the oral route . A metalloprotease- and cytolysin-deficient mutant of _V. vulnificus_ also had similar virulence in mice, and its cytotoxicity for HEP-2 cells (cytotoxin) compared with those of the wild-type strains . These findings suggest that neither metalloprotease nor cytolysin is essential for the virulence or invasiveness of _V. vulnificus_ in mice. A possible multifactor interaction in bacterial virulence might be present but to an extent that is not yet clear. However, two genes, _vvn_ (encoding a periplasmic nuclease, _Vvn_ ) and _smcR_ (encoding _SmcR_ , which regulat metalloprotease gene expression), were not required for _V. vulnificus_ virulence in mice .\n\n【18】Animal studies clearly demonstrated that iron could increase the growth rate of _V. vulnificus_ , which quickly reached a lethal concentration with enhanced cytotoxicity in the iron-overloaded mice . A study of the survival of _V. vulnificus_ in whole blood from patients with different degrees of liver disease showed that high serum ferritin levels and low phagocytosis activity of neutrophils were independent and important predictors of survival of the organism in blood . These findings indicated that patients with chronic hepatitis, liver cirrhosis, and hepatoma (high serum ferritin levels and lower phagocytosis) were at high risk for _V. vulnificus_ infection . Although many putative virulence factors have been studied for this exceptionally virulent human pathogen in Taiwan, how these factors and other veiled factors (such as capsular polysaccharide and lipopolysaccharide) interact to produce dramatic infections and what host aspects (such as overproduction of proinflammatory cytokines) are essential to infection are yet to be elucidated .\n\n【19】### Molecular Epidemiologic Features\n\n【20】Results of molecular typing by using restriction fragment length polymorphism analysis of rRNA (ribotyping) among 13 clinical and environmental (from seawater and eels in southern Taiwan) isolates of _V. vulnificus_ and arbitrarily primed polymerase chain reaction analysis of 37 isolates (24 clinical isolates and 13 from seawater from coast areas around Taiwan) were previously reported . Both showed high genetic divergence among clinical and environmental isolates.\n\n【21】The concentration of _V. vulnificus_ in recent clinical and environmental isolates in southern Taiwan indicates the possibility of clonal spread in this area. In this study, 50 isolates of _V. vulnificus_ collected from 1995 to 2000 from southern (46 isolates) and northern (4 isolates) Taiwan were analyzed. These isolates included those from various clinical specimens (blood and wound pus) of 50 patients with _V. vulnificus_ infection. All isolates of _V. vulnificus_ were identified by using conventional methods and the O/129 susceptibility tests. Identification of the isolates was further confirmed by the API 32 GN system (bioMérieux Vitek, Inc. Hazelwood, MO). Pulsed-field gel electrophoresis (PFGE) analysis was performed by a method described previously by Tenover et al. DNA was digested by the restriction enzymes _Sfi_ I and _Not_ I (Promega, Madison, WI). All isolates were not identical in PFGE profiles (50 pulsotypes were found), and only two isolates from southern Taiwan were closely related (within three bands of difference). These findings support the high degree of heterogeneity among isolates of _V. vulnificus_ that cause human infections in Taiwan.\n\n【22】### Preventive Measures\n\n【23】Residents of Taiwan, particularly those with preexisting liver and other chronic, underlying medical conditions (renal disease, diabetes mellitus, chronic steroid use), should be educated in measures to prevent acquiring _V. vulnificus_ infections. This bacterium is present in warm coastal waters around Taiwan during the summer months, particularly in the southern region. Exposing open wounds or broken skin to warm salt or brackish water or to raw marine animals harvested from such waters should be avoided. Patients at high risk should wear protective clothing (e.g. gloves) when handling seafood (fish, oysters, clams, shrimp, eels, and other shellfish) and not eat raw or improperly cooked seafood. Because this disease is rapidly progressive and deadly if not recognized promptly and treated aggressively, any illness (such as fever or skin lesions), which develops in patients at risk after contact with marine animals or waters or ingestion of seafood requires immediate medical care.\n\n【24】The government in Taiwan (Departments of Health and Council of Agriculture) should encourage food companies to put warning labels on seafood containers, menus, and public health brochures. The wording of such labeling should be similar to the label required by the Florida Department of Natural Resources for all wholesale shell food and shucked products: “Consumer Information—There is a risk associated with consuming raw oysters or any raw animal protein. If you have chronic illness of the liver, stomach, or blood or have immune disorders, you are at a greater risk of serious illness from raw oysters and should eat oysters fully cooked. If unsure of your risk, consult a physician” .\n\n【25】### Conclusion\n\n【26】Residents of Taiwan have a high prevalence of chronic liver disease and are often exposed to marine microorganisms present in the sea that surrounds the island or rivers, lakes, or ponds inside the island. The presence of high genetic divergence among _V. vulnificus_ isolates from humans and the environment indicates that this virulent bacterium is ubiquitous in nature. When _V. vulnificus_ is suspected as the cause of sepsis, empiric therapy that includes a third-generation cephalosporin and minocycline should be administered. It should be standard practice for physicians to advise patients with underlying medical illness to avoid eating raw or undercooked seafood and to avoid exposing wounds to seawater.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "435d7cbb-9f21-4dc2-9131-4ac08afb7b73", "title": "Evaluating Uptake of Evidence-Based Interventions in 355 Clinics Partnering With the Colorectal Cancer Control Program, 2015–2018", "text": "【0】Evaluating Uptake of Evidence-Based Interventions in 355 Clinics Partnering With the Colorectal Cancer Control Program, 2015–2018\nAbstract\n--------\n\n【1】**Purpose and Objectives**\n\n【2】Colorectal cancer screening rates remain suboptimal in the US. The Colorectal Cancer Control Program (CRCCP) of the Centers for Disease Control and Prevention (CDC) seeks to increase screening in health system clinics through implementation of evidence-based interventions (EBIs) and supporting activities (SAs). This program provided an opportunity to assess the uptake of EBIs and SAs in 355 clinics that participated from 2015 to 2018.\n\n【3】**Intervention Approach**\n\n【4】The 30 funded awardees of CRCCP partnered with clinics to implement at least 2 of 4 EBIs that CDC prioritized (patient reminders, provider reminders, reducing structural barriers, provider assessment and feedback) and 4 optional strategies that CDC identified as SAs (small media, professional development and provider education, patient navigation, and community health workers).\n\n【5】**Evaluation Methods**\n\n【6】Clinics completed 3 annual surveys to report uptake, implementation, and integration and perceived sustainability of the priority EBIs and SAs.\n\n【7】**Results**\n\n【8】In our sample of 355 clinics, uptake of 4 EBIs and 2 SAs significantly increased over time. By year 3, 82% of clinics implemented patient reminder systems, 88% implemented provider reminder systems, 82% implemented provider assessment and feedback, 76% implemented activities to reduce structural barriers, 51% implemented provider education, and 84% used small media. Most clinics that implemented these strategies (>90%) considered them fully integrated into the health system or clinic operations and sustainable by year 3. Fewer clinics used patient navigation (30%) and community health workers (19%), with no increase over the years of the study.\n\n【9】**Implications for Public Health**\n\n【10】Clinics participating in the CRCCP reported high uptake and perceived sustainability of EBIs that can be integrated into electronic medical record systems but limited uptake of patient navigation and community health workers, which are uniquely suited to reduce cancer disparities. Future research should determine how to promote uptake and assess cost-effectiveness of CRCCP interventions.\n\n【11】Introduction\n------------\n\n【12】Screening reduces deaths related to colorectal cancer (CRC), the second-leading cause of cancer death in the US . However, despite recommendation by the US Preventive Services Task Force, CRC screening rates remain suboptimal (66% in 2018) ; rates among uninsured and low-income populations are even lower. For example, in 2018, only about 30% of people who were uninsured and fewer than 50% of individuals who received care at Federally Qualified Health Centers, government-supported safety net clinics, were up to date with CRC screening .\n\n【13】The Community Preventive Services Task Force oversees rigorous, systematic reviews of the scientific literature to identify prevention strategies with evidence of effectiveness. On the basis of these reviews, the Task Force recommends the following evidence-based interventions (EBIs) to increase CRC screening: patient reminders, provider reminders, reducing structural barriers, provider assessment and feedback, small media, one-on-one education, and community health workers, including patient navigators  . Few studies have evaluated the uptake and sustainability of EBIs in a large sample of health care clinics . Such data are needed to understand how these interventions affect population health, as well as how best to increase the scale of effective interventions. Scalability is defined as the ability of an efficacious health intervention to be expanded under real-world conditions to reach a large proportion of the eligible population .\n\n【14】In 2015, the Centers for Disease Control and Prevention (CDC) funded the Colorectal Cancer Control Program (CRCCP) with the goal of increasing CRC screening. Thirty awardees were required to partner with primary care clinics that serve high-need populations to implement EBIs to increase CRC screening. On the basis of recommendations from the Task Force, CDC named 4 EBIs as priority for implementation (patient reminders, provider reminders, reducing structural barriers, provider assessment and feedback). These 4 priority EBIs can be implemented at the health system level to change screening rates. CDC deemed the 4 other EBIs that focus on the individual level (small media, one-on-one education, community health workers, patient navigators) as optional supporting activities (SAs). Awardees could implement both EBIs and SAs.\n\n【15】Purpose and Objectives\n----------------------\n\n【16】The primary purpose of this analysis was to determine the uptake and sustainability of EBIs and SAs in clinics participating in the CRCCP program over 3 years, from 2015 to 2018. We define uptake as the initial decision to employ an EBI or SA in a clinic setting (also called adoption), while sustainability indicates integration of an EBI or SA into a clinic’s ongoing operation . With regard to SAs, we were especially interested in the uptake of patient navigation by these clinics because most clinics in the CDC program are Federally Qualified Health Centers that provide care to underserved and under-resourced populations that experience health disparities, and patient navigation is a strategy intended to reduce disparities by helping patients overcome barriers to health care . Patient navigation is well accepted in these populations  and can be integrated into existing roles in clinical settings . The Task Force recently added patient navigation, conducted by patient navigators or community health workers, to their list of recommended interventions to promote CRC screening because it increases CRC screening rates . CDC defines patient navigation for CRC screening as individualized assistance offered to patients to help address barriers and facilitate timely access to quality screening and follow-up, as well as initiation of treatment services for people diagnosed with cancer. Patient navigation includes assessment of patient barriers, patient education, resolution of barriers, and patient tracking and follow-up. Patient navigation can be provided by health care providers (eg, nurses) or lay workers (eg, community health workers) .\n\n【17】Intervention Approach\n---------------------\n\n【18】The CRCCP uses a 5-year funding cycle, and our analysis focused on the 2015 through 2020 cycle. The 30 funded awardees partnered with clinics and provided technical assistance and resources to implement Task Force–recommended EBIs. For this screening program, awardees were required to implement at least 2 of the CDC-prioritized EBIs, as well as SAs; however, awardees were not allowed to use SAs as stand-alone activities. In addition, small media, in particular, had to be paired with 1 of the 4 EBIs (eg, a mailed patient reminder could include a small media material). The screening program is based on several tenets, including integrating public health and primary care, focusing on populations with a high prevalence of disease, implementing sustainable health system changes, and using evidence-based approaches to maximize limited public health dollars . CRCCP provided an opportunity to study the uptake and sustainability of different EBIs and SAs in a large number of health system clinics that provide care to medically underserved patients and to consider their scalability. Previous studies of this program observed that the implementation of its strategies was associated with increased clinic-level screening rates .\n\n【19】Evaluation Methods\n------------------\n\n【20】CDC’s Framework for Program Evaluation was applied to design the clinic survey on which this analysis is based . Other components of the CRCCP evaluation include an annual survey of awardees , cost effectiveness studies , case studies, and studies to explore specific components of CRCCP .\n\n【21】The clinic survey was based on prior surveys  and was completed by 1 representative per clinic, similar to other studies . Data collected in the surveys included clinic characteristics such as clinic type and size, EBIs and SAs in place at baseline and annually, use of CDC resources (eg, staff time, funds, materials) toward implementing EBIs and SAs, sustainability of EBIs and SAs, and baseline and annual CRC screening rates . Uptake was defined as EBIs and SAs that are in place and operational (in use) in a clinic at the end of the reporting period. Respondents were asked about sustainability using the question, “If in place, do you consider the EBI or SA as fully integrated into health system or clinic operations and sustainable?” “High quality implementation has been achieved and a supporting infrastructure is in place along with any financial support needed to maintain the EBI/SA. The EBI/SA has become an institutionalized component of the health system and/or clinic operation” was provided as an explanation. Respondents were not asked to consider the length of time that the strategy had been implemented in their responses. Definitions for EBIs and SAs that were given to survey respondents are provided . Awardees compiled and reported data to CDC from annual clinic surveys for each participating clinic for each of the first 3 years, from 2015 to 2018.\n\n【22】### Statistical analysis\n\n【23】The study sample was limited to clinics that enrolled in the first year of CDC’s screening program  and remained in the program for 3 years (N = 355 clinics, 85% of 417 clinics enrolled). We conducted a descriptive analysis to 1) identify the proportion of clinics implementing the 4 priority EBIs and 4 SAs for each year of the study period and 2) assess whether the EBIs and SAs were perceived as integrated and sustainable by the end of the study period. For each EBI and SA, trends in use between baseline and year 3 were analyzed by using the Cochran–Armitage test for trend. Analyses were conducted using SAS software, version 9.4 (SAS Institute Inc).\n\n【24】Results\n-------\n\n【25】### Clinic characteristics\n\n【26】Most clinics were Federally Qualified Health Centers (73%), and clinic size varied. Some clinics had fewer than 500 patients aged 50 to 75 years (24%), and others had more than 1,500 patients (38%). The number of providers ranged from fewer than 5 providers per clinic (42%) to more than 20 providers (12%). Patient populations ranged from less than 5% uninsured, aged 50 to 75 (29% of clinics) to more than 20% uninsured patients (36% of clinics). Thirty-four percent of all clinics had access to free fecal testing kits. Most clinics used stool-based tests as their primary CRC screening test (56%); 29% referred patients for colonoscopy, and in 13% of clinics, the primary screening test varied by provider .\n\n【27】### Uptake of strategies to promote CRC screening\n\n【28】Uptake of strategies to promote CRC screening among clinics varied widely at baseline and throughout the study. At baseline, 50% of clinics used patient reminder systems, 72% implemented provider reminder systems, 50% used provider assessment and feedback, and 43% implemented activities to reduce structural barriers. Significant increases were observed in the uptake of all 4 EBIs in the first 3 years of the program ( _P_ < .001 for all 4 EBIs). In year 3, 82% of clinics implemented patient reminder systems, 88% implemented provider reminder systems, and 82% implemented provider assessment and feedback. At baseline, SA use was generally low; 17% of clinics used community health workers, 32% offered patient navigation, 36% used small media, and 43% delivered provider education. Among SAs, professional development and provider education increased significantly, from 43% to 51% ( _P_ \\= .001), and use of small media increased significantly, from 36% to 84% ( _P_ < .001) of clinics in year 3  .\n\n【29】**  \nPercentage of clinics that partnered with the CDC Colorectal Cancer Control Program using evidence-based interventions to promote colorectal cancer screening, analyzed using the Cochran–Armitage trend test, 2015–2018 (N = 355). \n\n【30】A substantial number of clinics implemented or resumed new strategies and discontinued or paused strategies during the study period . Overall, the proportion of clinics that changed their EBI use from the prior year ranged from 27% to 36% in the first year. These fluctuations tended to decrease in subsequent years, from 13% to 19% of clinics in year 3. SA implementation fluctuated similarly. Only 28% of clinics had patient navigation in place in the second and third year (after baseline) of the program, but almost the same proportion, 25% to 26% of clinics either newly implemented or discontinued patient navigation in the same program years. In the third year, the proportion of clinics that changed their patient navigation status (either new or discontinued) decreased to 10%.\n\n【31】In the first year of the program, clinics that implemented 2 EBIs (n = 64) also implemented on average 1.4 SAs; those that implemented 3 EBIs (n = 102) implemented an average 1.7 SAs, and those that implemented 4 EBIs (n = 110) implemented an average 2.1 SAs. Concurrent implementation of EBIs and SAs was very similar in all program years. Not all clinics, however, implemented 2 priority EBIs in the first year. The percentage of clinics that implemented fewer than 2 EBIs ranged from 22% in the first year to 11% in the second year and to 4% in the third year.\n\n【32】### Integration of strategies to promote CRC screening and sustainability\n\n【33】Among clinics that had EBIs and SAs in place by the end of each year, most considered those EBIs and SAs fully integrated into health systems or clinic operations and sustainable with or without CRCCP resources, especially in years 2 and 3 . Sustainability and integration into clinic operations during the 3-year period increased most for activities that largely focused on providers, such as provider reminder systems, an increase of 14 percentage points from 79% in year 1 to 93% in year 3. Similarly, full integration of provider assessment and feedback increased 27 percentage points, from 69% to 96% of clinics; full integration of professional development or provider education increased by 16 percentage points, from 76% to 92% of clinics, followed by full integration of small media for an increase of 11 percentage points from 81% to 92% of clinics. Sustainability and integration into clinic operations did not substantially change with patient navigation (5 percentage point increase from 87% to 92% of clinics) and for community health workers (a 3 percentage point decrease, from 99% to 96%) .\n\n【34】Implications for Public Health\n------------------------------\n\n【35】To our knowledge, this is one of only a few studies examining the uptake of evidence-based interventions to promote CRC screening in a large sample of clinics in 30 states. A 2012 study of 44 Federally Qualified Health Centers in 4 Midwestern states found that 41% of clinics had no CRC screening tracking system, although 79% reported using electronic health records . A 2016 cross-sectional survey of 56 Federally Qualified Health Centers in 7 states found that 73% of them implemented patient reminder systems, 77% implemented provider reminder systems, and 82% implemented provider assessment and feedback. The same study found that fewer clinics used patient navigators (50%) and small media (62%) . Our study builds on previous research in 3 ways: 1) by corroborating results regarding the implementation of these strategies, 2) by adding information on the uptake of 8 different EBIs and SAs, and 3) by assessing these strategies, their changes in implementation, and their sustainability and integration over a 3-year period in a large sample of clinics.\n\n【36】Overall, we observed significant uptake of 4 priority EBIs and 2 SAs, suggesting that the CRCCP contributed to increasing implementation of these strategies in the participating clinics. Our data suggest that for all strategies experimentation took place in early years of the program until clinics settled on strategies that worked for their particular contexts. In addition, many clinics required more than a year to implement at least 2 priority EBIs. Clinics that implemented fewer EBIs also tended to implement fewer SAs, and vice versa.\n\n【37】The strong and consistent uptake of priority EBIs by CRCCP clinics may exist, in part, because CDC requires that clinics implement 2 of the 4 priority EBIs, awardees provide technical assistance and implementation support to clinics, and for some clinics, financial support is provided by awardees. Another explanation is that some EBIs and SAs can be integrated into clinical practice through clinics’ electronic health records systems. For example, by using data from electronic health records, patient reminder letters can be generated and personalized with each patient’s name and address, preferred language, the name of the patient’s primary care provider, and their history of CRC screening (eg, type and time of most recent test). Although it takes resources to program electronic health records and to set up these strategies initially, clinic health information technology and automated calling and texting systems can support implementation . Whether clinics can maintain these interventions solely with their own resources after CRCCP technical and financial support has ended remains to be seen.\n\n【38】Implementation of patient navigation and use of community health workers, on the other hand, was much lower than the priority EBIs and remained low over time. CDC’s focus on the 4 priority EBIs and on sustainability could be the reason for low implementation of these activities. Patient navigation is resource intensive, requiring ongoing funding and dedicated staff. In one study, trained nurse navigators spent an average of 124 minutes per patient to deliver a 6-step protocol by telephone to navigate patients for colonoscopy . In addition, the costs of patient navigation can be substantial. An economic analysis of detailed activity-based cost information that was systematically collected in a subset of CRCCP clinics showed costs per person screened ranging from $24 to $40 in 14 clinics that implemented multicomponent interventions that included patient reminders and provider assessment and feedback. The cost per person screened was $134, however, in a clinic that included patient incentives and patient navigation in addition to patient reminders . In contrast, some studies have reported that patient navigation resulted in cost savings, especially for endoscopic facilities . A study that compared patients who were navigated to a screening colonoscopy with non-navigated patients at 1 endoscopy clinic found that navigated patients were significantly more likely to complete colonoscopy and to have adequate bowel preparation. The group of navigated patients also had significantly fewer no-shows and cancellations than the group of non-navigated patients . A business case has been made to support patient navigation in some clinical systems that led to increased revenues because of increased patient retention, physician loyalty, reduction in emergency department visits, hospitalizations, and reduced burdens on oncology providers . Some of these benefits of patient navigation, however, might not be immediate and might not be assessed. If they are assessed, benefits might not be attributed to patient navigation. As most CRCCP clinics are Federally Qualified Health Centers that might not realize many of the potential economic benefits because patients often go to endoscopy providers. CDC is planning to conduct comparative effectiveness studies to further elucidate cost-effectiveness and other barriers to implement patient navigation. For now, reimbursement through health insurers might be required to increase the scalability of this strategy in primary care settings serving populations most likely to benefit from patient navigation.\n\n【39】Many of the strategies that clinics are implementing, including provider reminders, patient reminders, provider assessment and feedback, and small media, have the potential to promote CRC screening for all patients, and they were associated with screening rate improvements in the first year of the CRCCP . With the CRCCP focus on Federally Qualified Health Centers that serve populations with high disease burden, strategies also have the potential to reduce cancer disparities. Patient navigation in particular can focus on patients who have substantial barriers to CRC screening and the least access to care . This intervention strategy, therefore, is uniquely suited to reduce cancer disparities. Cancer disparities reduction was demonstrated in a statewide CRC screening program in Delaware, population 982,895: 23% Black residents and 69% White residents. The Delaware program included financial coverage for CRC screening, treatment, and patient navigation by nurse coordinators. Statewide CRC screening rates increased from 48% among Black residents and from 58% among White residents in 2001 to 74% in both groups in 2009, and the program resulted in reduced disparities in CRC incidence and mortality . Future program evaluations could take a population health equity approach  by examining patient data of CRCCP clinics to determine if program strategies reduced disparities in CRC screening, stage at diagnosis, incidence, mortality, and which specific strategies contributed to the reduction in CRC disparities. Another set of analyses could examine trends in cancer disparities in the catchment areas of participating clinics during the implementation of CRCCP. An analysis that takes a population health equity approach would add a new perspective to the CRCCP program evaluation and provide crucial information on the value of all program strategies, including EBIs and patient navigation, in reducing CRC disparities. Further research is needed to gain a better understanding of the reasons clinics decide to implement some strategies over others and reasons other strategies are discontinued. Data from these analyses could guide future initiatives to increase CRC screening at a population level.\n\n【40】Our study included a data set with a limited number of variables and did not assess theoretical constructs, such as those of the Consolidated Framework for Implementation Research that might explain uptake and sustainability. Furthermore, many clinics could not provide data on the racial and ethnic characteristics of patients, and we were not able to examine if those characteristics were related to the uptake of intervention strategies, particularly patient navigation. Future studies should assess theoretical constructs that are relevant for implementation to illuminate the determinants of implementation and sustainability . We also did not have information about the quality of EBI and SA implementation, which likely varies considerably across clinics. Although respondents were encouraged to consult with their team, surveys were completed by one person per clinic who might not have had complete information. Responses may be influenced by respondent role in the clinic (eg, CRC champion versus a quality improvement specialist) and might also suffer from social desirability bias. Respondents were instructed to not report reducing structural barriers as a patient navigation activity, but it is possible that some respondents conflated these 2 strategies, because patient navigators often conduct work related to reducing structural barriers. Finally, CDC’s mandate that clinics implement at least 2 priority EBIs could have dictated to some extent the selection of strategies (priority EBIs versus optional SAs).\n\n【41】Our analysis focused on the uptake of 8 different strategies, all recommended by the Community Preventive Services Task Force, in a large number of clinics. Those clinics chose which EBIs and SAs to implement in the context of seeking to meet CRCCP program requirements. Primary care clinics participating in the CRCCP significantly increased implementation of 4 priority EBIs (patient reminder systems, provider reminder systems, provider assessment and feedback, and activities to reduce structural barriers) and 2 optional SAs (provider education and small media) to increase CRC screening over the first 3 program years. Uptake may be facilitated through technical and financial support provided by CRCCP awardees and integration of these strategies into clinic electronic health records systems. Implementation of patient navigation and community health workers remained flat over time, likely due, in part, to the need for ongoing funding for staff. Although use of patient navigation and community health workers may be effective strategies for reaching a clinic’s most underserved patients, additional support or encouragement may be required for clinics to add these services.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ac2807f1-6aa9-48a2-9397-7bd32bd436e8", "title": "Distribution of Streptococcal Pharyngitis and Acute Rheumatic Fever, Auckland, New Zealand, 2010–2016", "text": "【0】Distribution of Streptococcal Pharyngitis and Acute Rheumatic Fever, Auckland, New Zealand, 2010–2016\nAcute rheumatic fever (ARF) can cause rheumatic heart disease, which in turn may produce permanent heart damage . ARF is an autoimmune disease triggered in response to group A _Streptococcus_ (GAS) infection. GAS pharyngitis is generally considered the major initiator of ARF, but GAS skin infection may also play a role. Substantial knowledge gaps with regard to ARF risk factors and pathogenesis impair disease prevention and control . If GAS pharyngitis is the sole initiator of ARF, then we would expect this infection to be most common in groups in which incidence of ARF is highest. ARF rates peak among children 5–14 years of age .\n\n【1】ARF and rheumatic heart disease exert a major burden on developing countries. Disease rates are also particularly high among persons of Māori and Pacific Islander ethnicity in New Zealand . During 2000–2009, among children 5–14 years of age, ARF incidence among Māori children was 40.2 cases/100,000 children, and among Pacific Islander children, it was 81.2/100,000. By contrast, the incidence for non-Māori, non–Pacific Islander children in New Zealand was 2.1/100,000. Associations between ARF and socioeconomic deprivation have inconsistently been observed . During 2010–2013, persons living in the most deprived New Zealand neighborhoods were 33 (95% CI 19–58) times more likely to be hospitalized with ARF for the first time compared with persons living in the least deprived neighborhoods .\n\n【2】In 2011, the New Zealand government announced a major national Rheumatic Fever Prevention Programme (RFPP), aiming to reduce the national incidence of ARF by two thirds (to 1.4 cases/100,000 persons) by mid-2017 . The RFPP strongly emphasized primary prevention of ARF through sore throat management; that is, prompt detection and antimicrobial treatment of GAS pharyngitis before development of ARF . In areas with high rates of ARF, the sore throat management aspect of the RFPP had 2 components: school-based throat swabbing clinics and rapid-response primary healthcare clinics (PHCs). School-based clinics operated only when schools were in session. Children with a self-reported sore throat could have their throat swabbed free of charge, either at school or at a rapid-response PHC when certain conditions were met. If the swab sample culture produced GAS, a 10-day course of oral amoxicillin was recommended . The RFPP was reportedly the largest sore throat management program for ARF prevention ever conducted . As of 2014, school clinics included ≈50,000 children . By December 2013, a total of 83% of school clinics had been implemented and additional rapid-response clinics were being set up in PHCs. Public health messages about the value of seeking throat swabbing for those experiencing sore throat symptoms were promoted to populations considered at high risk for ARF .\n\n【3】The RFPP resulted in a large collection of high-quality diagnostic throat swab sample data, which provided a unique opportunity to describe the distribution of GAS pharyngitis across an entire population and correlate the data with ARF rates. Most RFPP throat swab samples were collected in Auckland, where ≈50% of ARF patients in New Zealand reside . Our aim was to describe the distribution of GAS pharyngitis in the Auckland population and compare it with the distribution of ARF.\n\n【4】### Methods\n\n【5】##### Setting\n\n【6】In 2013, the population of New Zealand was ≈4.5 million persons. The largest city is Auckland, where around one third of the population resides. In the 2013 census, 10% of persons in Auckland identified their ethnicity as Māori, 12% Pacific Islander, 21% Asian, and 57% European/other . Auckland comprises 3 district health boards (DHBs): Waitemata, Auckland, and Counties Manukau. Many Auckland schools (n = 75) participated in the RFPP school program. Rapid-response clinics were widely implemented .\n\n【7】The National Health Index (NHI) is a unique patient identifier widely used New Zealand health data; it can be encrypted to protect patient privacy. Demographic information encoded by the NHI includes New Zealand resident status, prioritized ethnicity, sex, birth date, DHB, and New Zealand Deprivation Index (NZDep) score. The prioritized ethnicity classification system allocates persons to a single ethnic group based on a prioritized order of Māori, Pacific Islander, Asian, and European/other. The European/other group refers to non-Māori, non–Pacific Islander, and non-Asian persons . The NZDep score is an ecologic measure of socioeconomic deprivation corresponding to a neighborhood . Deciles 1–2 (quintile 1) represent the least deprived neighborhoods, and deciles 9–10 (quintile 5) represent the most deprived.\n\n【8】Persons eligible to have their throat swabbed and receive antimicrobial treatment through the RFPP were Māori and Pacific Islander children 4–19 years of age, all children in that age group living in quintile 5 neighborhoods, and eligible children’s household contacts 3–35 years of age if they visited a school or rapid-response clinic with a self-reported sore throat. Other persons contributed throat swab samples in PHCs when clinicians decided to collect a sample separately from the RFPP .\n\n【9】##### Throat Swab Sample Data Collection\n\n【10】Since mid-2009, the sole community pathology laboratory service provider for the entire Auckland region has been Labtests . We obtained data on all throat swab samples cultured at Labtests during 2010–2016: patient encrypted NHI, age, date of swab sample collection, sample source (i.e. school clinic or PHC), and culture result (e.g. GAS). Although swab samples collected in school clinics could be distinguished from those collected in PHCs, we could not distinguish between samples collected in rapid-response clinics and those in regular PHCs.\n\n【11】##### ARF Data Collection\n\n【12】We obtained data on ARF diagnoses during 1988–2016 from the Ministry of Health (International Classification of Diseases, 10th Revision \\[ICD-10\\], codes I00-I02 and ICD International Classification of Diseases, 9th Revision \\[ICD-9\\], codes 390–392). We also obtained rheumatic heart disease diagnoses for the same period (ICD-10 codes I05-I09 and ICD-9 codes 393–398). The encrypted NHI was provided for each entry along with the demographic information it encoded. We identified the first ARF entry for each child and removed all later entries. Because ARF precedes rheumatic heart disease in the causal pathway, when identifying initial ARF hospitalizations, we excluded all persons who had been hospitalized for rheumatic heart disease before their first hospitalization for ARF. We excluded from study all admissions for non–New Zealand citizens. We also excluded hospital transfers; thus, only the first record was included for each ARF admission. In so doing, we attempted to limit the dataset to initial presentations of ARF, in accordance with the method adopted by the Ministry of Health in 2013 . We created a dataset of initial ARF hospitalizations in Auckland during 2010–2016, the initial ARF dataset, and performed basic demographic analyses.\n\n【13】##### Statistical Analyses\n\n【14】We performed descriptive epidemiologic analyses according to key outcome measures, stratified according to selected demographic characteristics and whether GAS pharyngitis was detected. We considered a throat swab sample that produced GAS on culture to indicate a case of GAS pharyngitis. Key outcome measures were incidence of throat swab samples , incidence of GAS pharyngitis, and the proportion of total throat swab samples that indicated GAS pharyngitis.\n\n【15】For all analyses, we used R version 3.3.1 . Because the RFPP was still being implemented in 2013, our later analyses focused on 2014–2016. After 2013, the annual number of swab samples collected peaked, remaining relatively stable with high population coverage. Most analyses concentrated on children 5–14 years of age . The focus is largely on samples from PHCs because the school programs intensely targeted high-risk populations on the basis of ARF incidence. When calculating seasonal rates, we multiplied numerator (swab) data by 4 to produce annualized rates.\n\n【16】##### Rate Calculations\n\n【17】If a person contributed >1 swab sample, that person would be counted >1 time in the numerator. We calculated intercensus population estimates and projections by interpolation and extrapolation, using denominator data from 2006 and 2013 population censuses . When calculating mean rates, we used the population estimate for the middle of the period. We calculated relative risks (RRs) and 95% CIs for initial ARF and GAS pharyngitis from the number of cases detected in the population.\n\n【18】### Results\n\n【19】##### Settings and Time Trends in Throat Swab Sample Collection and GAS Pharyngitis\n\n【20】During 2010–2016, a total of 1,257,058 throat swabs were collected in Auckland. The total number of throat swab samples collected each year increased dramatically; 8 times more samples were collected in 2016 than in 2010. During 2011–16, swabbing increased in school clinics but also increased 4-fold in PHCs. The throat swabbing incidence for the Auckland population plateaued in 2014–16; swabbing in school clinics peaked in 2014 (130.8 samples/1,000 person-years) and in PHCs peaked in 2015 (114.5 samples/1,000 person-years).\n\n【21】Of the swab samples, 163,534 were positive for GAS (13.0% total samples); 64,036 were positive for streptococci of group C, group G, or both (5.1% total). The annual proportion of samples positive for GAS decreased from 15.3% in 2010 to 10.7% in 2011 before increasing to 15.1% in 2016 . However, the annual incidence of GAS pharyngitis increased nearly 8-fold from 2010 to 2016. The proportion of positive GAS swab samples was higher among those collected in PHCs (15.0%) than in school clinics (12.6%) .\n\n【22】##### Sociodemographic Characteristics of Populations Contributing Throat Swab Samples\n\n【23】We determined the sociodemographic characteristics of populations who contributed throat swab samples in detail for 2014–2016. Sample collection in PHCs was highest among children 5–9 years of age (100,406 total swab samples, 356.6 swab samples/1,000 children), followed by children 10–14 years (72,980 swab samples, 266.5/1,000 children), and then children <5 years of age (68,548 swab samples, 229.1 samples/1,000 children).\n\n【24】The incidence of GAS pharyngitis was highest among children 5–9 years of age (82.9 cases/1,000 person-years), followed by children 10–14 years of age (44.3 cases/1,000 person-years). GAS was uncommon in throat swab samples from persons \\> 50 years of age, although ≈15,000 swabs were collected from persons in this age group each year. A much smaller secondary peak in incidence of GAS pharyngitis was seen among adults 30–39 years of age, and 66.2% of those affected were female .\n\n【25】In school clinics, the incidence of swab collection was highest among children 5–9 years of age (1151.3 swabs/1,000 person-years), as was the incidence of GAS pharyngitis (121.8 swabs/1,000 person-years). Of the total school clinic swab samples collected, 96.2% were from children 5–14 years of age, the target age group.\n\n【26】During the study period, 792 persons were hospitalized for initial ARF; 398 (50.3%) were children 5–14 years of age. Because incidence of GAS pharyngitis and ARF were both highest among children 5–14 years of age, we further restricted our analysis to this group.\n\n【27】##### Seasonal Distribution of Swab Sample Collection and GAS Pharyngitis\n\n【28】The incidence (and RR) of throat swab collection was highest in winter, both in PHCs (480.3/1,000 children 5–14 years of age) and overall; incidence of GAS pharyngitis was also highest in winter . In winter, the incidence of GAS detected by swab samples collected in PHCs (87.9 samples/1,000 person-years) was more than twice the rate detected by samples collected in the summer. By contrast, the proportion of GAS-positive samples was lower in winter and spring than in summer and autumn. The seasonal pattern of ARF incidence rates was roughly similar to that of GAS pharyngitis; ARF rates were highest in autumn and winter .\n\n【29】##### Throat Swab Sample Collection and Incidence of GAS Pharyngitis by Ethnicity\n\n【30】Nearly one quarter of all Pacific Islander children and one fifth of all Māori children contributed \\> 1 swab in PHCs, compared with approximately one sixth of European/other children. The proportion of samples positive for GAS was similar between these groups (20.1%–22.3%), but incidence of GAS pharyngitis was significantly higher among Pacific Islander (99.6 cases/1,000 person-years) and Māori (79.0 cases/1,000 person-years) children compared with those of European/other ethnicity (58.3 cases/1,000 person-years). Incidence of GAS pharyngitis was lowest among Asian children .\n\n【31】##### Distribution of Throat Swab Sample Collection and Incidence of GAS Pharyngitis\n\n【32】Most throat swab samples were collected from children living in the most deprived neighborhoods; \\> 1 throat swab sample was contributed by approximately one quarter of all children from quintile 5, compared with one eighth of all children from quintiles 1 and 2. The proportion of GAS-positive swab samples from children across quintiles was similar (19.5%–21.8%), but incidence of GAS pharyngitis increased with area deprivation. For children in quintile 1, incidence was 43.5 cases/1,000 person-years, but in quintile 5, incidence was 103.1 cases/1,000 person-years .\n\n【33】##### Comparison of GAS Pharyngitis and ARF Incidence\n\n【34】We compared mean annual incidence of GAS pharyngitis during 2014–2016, by selected demographic characteristics and swab sample source (i.e. PHC and total), with the mean annual incidence of ARF . The RR for initial ARF was highest among children 10–14 years of age; however, the RR for GAS pharyngitis was highest among children 5–9 years of age. Although ARF was rare among children <5 years of age, GAS pharyngitis was relatively common. The RR for initial ARF was 240.4 (95% CI 33.5–1,722.6) for children of Pacific Islander ethnicity compared with those of European/other ethnicity and was also extremely elevated for Māori children (RR 86.9, 95% CI 11.9–635.0). Of children who contributed throat swab samples in a PHC, the RR for GAS pharyngitis was highest among Pacific Islanders (RR  1.7, 95% CI 1.6–1.8), followed by Māori children, yet this discrepancy was not nearly as extreme as the RR for ARF. Higher RRs for GAS pharyngitis were also estimated for Māori and Pacific Islander children regardless of the sample collection setting. A similar pattern was noted for differences between NZDep quintiles. Seasonality was more pronounced for GAS pharyngitis incidence than for ARF incidence; at PHCs, GAS pharyngitis was least likely in summer and most likely in winter (RR 2.2, 95% CI 2.2–2.3).\n\n【35】### Discussion\n\n【36】The RFPP provided a unique opportunity to assess the distribution of GAS pharyngitis across a well-defined region over a sustained period and compare it with the distribution of ARF. To our knowledge, the RFPP produced the largest ever compilation of throat swab sample data; 1.3 million swab samples were collected in Auckland from 2010 (before RFPP) through 2016. These comprehensive throat swab sample data showed the following: the proportion of GAS-positive swab samples was fairly consistent across the population of children 5–14 years of age but varied between PHC and school clinic settings, suggesting that sample collection thresholds differed by setting; GAS pharyngitis is seasonal and shares some similarities with ARF; unlike ARF, GAS pharyngitis occurs across a wide range of age groups ; and ethnic and socioeconomic differences in GAS pharyngitis are insufficient to explain the marked inequities in ARF incidence.\n\n【37】A striking feature of GAS pharyngitis is the consistent difference in the proportion of GAS-positive samples collected in PHCs and school clinics. The proportion of GAS-positive samples collected from Māori and Pacific Islander children 5–14 years of age in PHCs (20%–21%) was nearly twice that observed for those collected by school programs (11%). At PHCs, the proportion of GAS-positive samples was similar between ethnic groups and NZDep quintiles (except somewhat lower for Asian children). One possible explanation could be that as children approach the level where their caregiver feels they are sufficiently unwell to warrant visiting a PHC, the likelihood of them having GAS pharyngitis is much the same across ethnic groups and deprivation quintiles. Consequently, the proportion of GAS-positive samples by health service may reveal information about the threshold at which persons seek treatment there. Furthermore, the literature describes GAS pharyngitis as a severely painful sore throat . It is debatable whether many affected children would therefore attend school; many may have visited a rapid response PHC instead. Consequently, GAS-positive samples from school clinics may have largely detected GAS carriage. To support this view, the proportion of GAS-positive samples from school clinics was slightly lower than the estimated prevalence of asymptomatic pharyngeal GAS carriage reported (12%) . This threshold effect has several implications. First, we should concentrate on findings from PHCs, where the threshold for attendance seems to be higher and a moderate proportion of cases are likely to represent true GAS pharyngitis (not viral pharyngitis with coincidental GAS detection on swab culture) . Second, GAS pharyngitis incidence rates are likely to provide a better indication of the distribution of this condition compared with the proportion of GAS-positive samples overall. The consistently lower proportion of GAS-positive samples from the school program, at a level equivalent to asymptomatic detection, raises questions about the effectiveness of basing the RFPP in this setting at all. There are potential ways to improve the accuracy of GAS pharyngitis diagnosis, such as through clinical decision rules, although the validity and practicality of such methods are debated .\n\n【38】The value of observing the incidence of GAS pharyngitis in PHC settings is illustrated by seasonal distribution patterns. GAS pharyngitis was more common in winter, when incidence was more than twice that in summer. Paradoxically, the proportion of GAS-positive swab samples showed the opposite pattern, being highest in summer. This effect was caused by the large increase in sample collection during winter. Most pharyngitis has a viral cause ; thus, more GAS-negative children reporting a sore throat visited a PHC in winter, reducing the proportion of swab samples that produced GAS in culture. The increase in sample collection during winter was somewhat appropriate given the increased rate of ARF.\n\n【39】Differences in the proportion of GAS swabs across ethnic and socioeconomic groups were insufficient for explaining the marked inequities in ARF incidence rates. Māori and Pacific Islander children, among whom risk of acquiring ARF is highest, were well targeted by the RFPP. Very low rates of ARF in non-Māori, non–Pacific Islander populations in New Zealand have been reported , along with ≈200 throat swabs collected/1,000 persons . This observation raises the question of whether intensive swabbing of non-Māori, non–Pacific Islander populations is appropriate. The evidence for reducing swab sample collection from young children is less certain. Although the incidence of GAS pharyngitis was elevated in groups at highest risk for ARF, the age distribution was much broader. If, as hypothesized, ARF is caused by repeated untreated GAS pharyngitis, which eventually triggers autoimmune reactions (priming) , then concentrating sore throat management strategies on young children is probably justifiable. The extreme disparities in ARF rates between ethnic and socioeconomic groups were not seen for GAS pharyngitis rates. These observations do not support the hypothesis that differences in observed GAS pharyngitis are a key pathway propelling the observed ARF inequities. Consequently, other factors that may drive ARF need to be considered, including the role of GAS skin infections . Ethnic and socioeconomic differences in exposures to environmental cofactors or host factors also need to be considered as key drivers of ARF inequities .\n\n【40】Strengths of our study include complete dataset coverage of a large, well-defined population. Well-characterized numerators and denominators permitted analysis by key demographic attributes. Microbiological analyses were performed by a single provider (Labtests) using standardized protocols. A limitation is that swab sample data reflect healthcare service use rather than representative population sampling, particularly because accurate RFPP coverage data were never collected. In addition, the RFPP deliberately targeted persons in groups at high risk for ARF, for whom GAS pharyngitis risk is potentially higher. These data cannot therefore directly measure the distribution of GAS pharyngitis. A second limitation with this study, and with the RFPP in general, is that it is impossible to know which GAS-positive children have true GAS pharyngitis and which have pharyngitis from other causes and coincidental GAS carriage. No information about clinical manifestations was collected, and serologic confirmation of infection was not sought in the RFPP (and was neither recommended nor practical) . It is likely that many persons for whom antimicrobial drugs were prescribed had viral infections and may not have benefited from treatment. Last, because the RFPP was not set up to be evaluated, it is impossible to know which swab samples were collected in rapid-response clinics and which as part of routine healthcare. Regardless, the increase in PHC swabbing correlates strongly with, and is most likely attributable to, RFPP implementation .\n\n【41】A priority for future research is to establish the pathogenic significance of GAS detection in pharyngeal swab sample cultures across different settings. Future analyses could assess the frequency of throat sample collection and GAS pharyngitis for an entire birth cohort of children in Auckland. Population incidence (cohort) studies could be useful for establishing accurate risk measures, particularly of the type conducted in Melbourne, Victoria, Australia, where GAS pharyngitis surveillance data were collected to investigate prevalence, transmission, and serology . It would be useful to establish an ongoing surveillance and monitoring program for ARF prevention that could assess specific intervention components, such as rapid-response clinics. A case–control study would be well suited to investigate factors contributing to the greatly elevated ARF risk for Māori and Pacific Islander children, beyond exposure to GAS pharyngitis alone . Specimens collected in such a study could be used for immune profiling to investigate the hypothesis that cumulative exposure to GAS is indeed a risk factor for ARF .\n\n【42】In conclusion, we found that the RFPP dramatically increased rates of throat swab sample collection among children at high risk for ARF. Throat swab sample collection is appropriate, given the goal of reducing ARF. However, because GAS pharyngitis is common in human populations, the RFPP resulted in many persons who were not at high risk for ARF undergoing throat swabbing and, for many, antimicrobial drug treatment. The population incidence of GAS pharyngitis shows some correlation with ARF risk. However, disparities in ARF incidence are vastly higher across ethnic and socioeconomic groups than are disparities in GAS pharyngitis, as measured by swab sample cultures from persons with self-reported pharyngitis. This inconsistency implies that factors other than exposure to a single episode of GAS pharyngitis alone must drive ARF development. Identifying and mitigating any modifiable risk factors may hold the key to effective ARF prevention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f84eeca9-3e0a-47c4-97cd-5fe91ef3a371", "title": "Correlation of West Nile Virus Incidence in Donated Blood with West Nile Neuroinvasive Disease Rates, United States, 2010–2012", "text": "【0】Correlation of West Nile Virus Incidence in Donated Blood with West Nile Neuroinvasive Disease Rates, United States, 2010–2012\nWest Nile virus (WNV), a mosquitoborne flavivirus, was first isolated in 1937 from a patient in Uganda . The virus was introduced into the United States in 1999, resulting in a focal epidemic that year in New York, New York. WNV then made a westward migration across the United States, becoming an endemic public health problem that is monitored through the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA) ArboNET surveillance system . Although most WNV infections are asymptomatic, they can cause a syndrome of fever and myalgia in a minority of cases and can also cause neuroinvasive disease (NID) manifesting as meningitis, encephalitis, or acute flaccid paralysis . An estimated 20%–25% of infected persons become symptomatic . During 1999–2010, an estimated 2–4 million WNV infections, 0.4–1 million cases of febrile illness, and 13,000 reported cases of NID occurred in the United States . US public health authorities received >39,000 reports of WNV infections during 1999–2013 . Early data from 1999 indicated that NID develops in <1 in 150 WNV-infected persons . However, based on 2003 data, this estimate was revised to 1 in 256 based on a correlation of NID case-report data with total WNV infection projections derived from blood donor WNV testing data .\n\n【1】The risk of transmitting WNV through blood transfusion was anticipated by CDC investigators and estimated to be as high as 2.7 transmissions/10,000 transfused blood units during the peak of the 1999 outbreak ; however, confirmed transfusion-transmitted infections were not reported until 2002 . After these reports, the US Food and Drug Administration, CDC, US blood centers, and test manufacturers quickly collaborated to develop and implement, beginning in summer 2003, routine nucleic acid testing (NAT) to screen blood donors for WNV RNA .\n\n【2】Predictions of the scale of annual outbreaks are not reliable . Environmental temperature and precipitation data may serve to estimate when WNV transmission rates in mosquitoes and birds will exceed specific thresholds, such that the probability of transmission to humans would be predicted to occur . Blood center data are a useful adjunct because testing laboratories monitor the number of WNV NAT–positive donations in near real time to determine if they should convert from NAT of minipools (MP-NAT) to more sensitive NAT of individual donations (ID-NAT) . In addition, WNV NAT–positive blood donations and NID cases are reported to CDC by state and territorial health laboratories via the ArboNET surveillance system. These reports showed that, after relatively low numbers of WNV outbreaks during 2004–2011, the WNV incidence rate in 2012 was one of the highest reported. The NID-associated death rate in 2012 was 9.9%, and the number of deaths  is the highest annual number reported to CDC; of the 286 deaths, 55.5%  were reported from 5 states: 89 (31%) from Texas, 20 (7%) from California, 16 (5.6%) in Louisiana, and 17 (6%) each from Michigan and Oklahoma .\n\n【3】Blood donors represent a readily accessible sample of the US population that is systematically screened for incident WNV infections. This screening provides an approximation of the magnitude of the WNV epidemic each year, and this estimate complements data reported to ArboNET on cases of symptomatic WNV-associated disease. Busch et al. previously estimated national and state-specific WNV infection rates in 2003 from the number and frequency of WNV NAT–positive blood donations and used those rates to provide statewide projections, which were then correlated with NID case rates. In this study, we used a larger dataset of WNV NAT–positive blood donations to model the US population incidence of WNV during 2010–2012. We also provide updated ratios of the estimated number of WNV infections to NID cases.\n\n【4】### Methods\n\n【5】##### Study Population\n\n【6】Blood donor data were extracted from operational blood center databases and provided without personal identifiers. The total number of donations and the number of NAT-confirmed WNV-positive donations were categorized by donor age, sex, and state of residence. Data were collected over the 5-month epidemic period from June 1 to October 31 during 2010– 2012. During 2003–2012, only 5 WNV NAT-positive blood donations were reported outside the months (June–October) that we included in this analysis: 4 occurred in November 2012 and 1 in April 2010 . Data for this study were obtained from the American Red Cross (Washington, DC, USA), which collects blood in 44 US states and Washington, DC; Blood Systems, Inc. (Scottsdale, AZ, USA), whose centers collect mostly in the Southwest, the Central Plains, and parts of California; the New York Blood Center (New York, NY, USA), which collects in New York and New Jersey; Carter Blood Care (Bedford, TX, USA), which collects in northern Texas; and OneBlood (Tampa, FL, USA), which collects blood throughout Florida and southern Georgia. Altogether, the current dataset is estimated to capture ≈60% of US blood donations. Blood centers provided count data stratified by age, date of donation, sex, and geographic location, and Blood Systems, Inc. provided person-specific data on all donors to enable risk factor analysis. This analysis did not constitute human subjects research because only existing data without personal identifiers were available to the investigators.\n\n【7】##### Blood Donor Screening\n\n【8】During the time of the study, blood donations from Blood Systems, Inc. the New York Blood Center, and Carter Blood Care were tested at Creative Testing Solutions laboratories (Tempe, AZ, USA) for WNV RNA by minipool NAT (pools of 16) and ID-NAT for resolution of donations within reactive pools, both using transcription-mediated amplification (TMA) (Hologic, San Diego, CA, USA; Grifols Diagnostics, Emeryville, CA, USA). The American Red Cross performed WNV NAT at its National Testing Laboratories (Stockbridge, GA, USA) using the same TMA assay. OneBlood used MP-NAT (pools of 6) based upon a PCR technique (Roche Molecular Systems, Branchburg, NJ, USA) or MP-NAT (pools of 16; Hologic/Grifols); resolution of donations within reactive pools was done using ID-NAT. Results from MP-NAT testing are monitored to determine if epidemic activity requires the triggering of more sensitive ID-NAT testing in geographic areas experiencing outbreaks . The trigger for converting from MP-NAT to ID-NAT in response to ongoing WNV activity was 1–2 reactive blood donations; for the TMA system, this was restricted to those reactive donations having a high ID-NAT signal .\n\n【9】NAT-reactive blood donations can represent a WNV-infected donor or a falsely reactive test result; thus, results must be confirmed by repeating NAT on an independent sample, by demonstrating donor seroconversion, or both . In our study, we included all ID-NAT–confirmed positive donations whether they were initially screened by MP-NAT or ID-NAT.\n\n【10】##### Statistical Analysis\n\n【11】Using only Blood Systems, Inc. data for confirmed WNV-positive and negative donations, we performed a univariate analysis of WNV NAT reactivity by donation year and donor sex and age group. We compared categorical variables by using the χ 2  test; age as a continuous variable was compared using the Student _t_ \\-test. We estimated odd ratios in a logistic regression model that included adjustment for age group, sex, region of residence, season, and month.\n\n【12】Most WNV RNA–positive persons who donate blood are asymptomatic or in the presymptomatic stage of infection; thus, we calculated the seasonal incidence of WNV on the overall dataset, assuming independence between blood donation and WNV infection . For this analysis, we used WNV RNA detection periods (i.e. number of days between first testing positive and testing negative) of ≈10.7 days by MP-NAT and ≈19.6 days by ID-NAT . These estimates were adapted from the method of Busch et al. using data from Kleinman et al. We did not have access to data on whether donations were screened by MP-NAT or ID-NAT. Given that roughly equal proportions of yield donations were derived from MP-NAT and ID-NAT screening, we used an average window of 15.1 days, assuming a 50% mixture of ID-NAT and MP-NAT screening donations during the epidemic period. We then multiplied the total donations screened for WNV RNA by 15.1 days to derive person-time for the denominator in incidence calculations; the number of corresponding NAT yield donations was used as the numerator .\n\n【13】We derived the monthly WNV incidence in each state from June through October by multiplying the number of NAT-positive donations for each month by the number of days in each month and dividing by the average period of time during which RNA is detectable . We calculated state-specific seasonal WNV incidence by adding the 5 monthly WNV incidence estimates for each year. An estimation of the number of WNV infections in each state was calculated by multiplying each state-specific seasonal WNV incidence by the corresponding population estimate obtained from the US Census Bureau . We then obtained a national seasonal estimate by summing over the estimates for participant states. An overall seasonal incidence for the 3 years was calculated weighted on the general population for each year. CIs were obtained assuming a Poisson distribution for NAT-positive donations.\n\n【14】We obtained the ratio of WNV infections to reported NID cases by state by dividing the estimated number of infections in the general population by state by the number of NID cases reported to ArboNET. This estimation was repeated for each year. We obtained CIs by applying Taylor series expansion . Analysis of correlation between WNV incidence and reported NID through ArboNet surveillance was done using a linear regression. Correlations and summarizations were expressed using R 2  . All statistical analyses were performed using Stata 12.1 (StataCorp LP, College Station, Texas, USA). We prepared graphical (maps) displays of results using ArcGIS version 9.3.1 (ESRI, Redlands, CA, USA). We did not conduct a county-level analysis.\n\n【15】### Results\n\n【16】##### Study Population and Demographic Predictors\n\n【17】A total of 10,107,853 blood donations collected during June–October in 2010–2012 were included in this study: total donations for 2010 were 3,470,405, total donations for 2011 were 3,360,443, and total donations for 2012 were 3,277,005. All donors included in the analysis were US residents; 20% lived in Western states, 27% in the Midwestern states, 32% in Southern states, and 21% in Northeastern states. Data from Blood Systems, Inc. indicated that blood donors ranged in age from 16 to 98 years (median age 45 years), and men accounted for 53% of donations.\n\n【18】Overall, 640 donations were WNV NAT positive . WNV RNA–positive blood donations clustered according to WNV epidemic activity and the catchment areas of the participating blood collection networks. Apparent clustering was observed in Southwest, Central, and Northeast states in 2010 and in Southwest and Northeast states in 2011; the pattern was much more dispersed in 2012, involving the North Central, Southwest, and Northeast states.\n\n【19】The frequency of WNV RNA was 63% higher among male than female donors and 122% higher among white than nonwhite donors. Donors from the Midwest had higher rates of WNV infection. Higher rates of NAT-positive donations were observed in 2012 versus 2010, but rates in 2011 were significantly lower than those in 2010 .\n\n【20】##### WNV Seasonal Incidence Analysis\n\n【21】Seasonal rates were 3.7 cases/100,000 donations (≈1 in 26,700) in 2010, 2.6 cases/100,000 donations in 2011 (≈1 in 38,200), and 12.9 cases/100,000 donations (≈1 in 7,800) in 2012. Over the 3 years, WNV activity was highest in August and September, as evidenced by NAT-positive rates; in 2010, rates peaked in September (7.7 cases/100,000 donations), and in 2011, rates peaked in August (7.0 cases/100,000 donations) . In 2012, NAT-positive rates peaked in August (26.9 cases/100,000 donations), but activity was high from July (16.2 cases/100,000 donations) through September (16.2 cases/100,000 donations).\n\n【22】Diverse geographic incidence patterns were observed over the 3 years . In 2010 and 2011, the states with the highest activity were Arizona, New Mexico, Nebraska, and Kansas; the incidence in 2011 was lower than that in 2010. In 2012, the epidemic grew in scale and expanded to Texas and North Central states, including South Dakota, North Dakota, Minnesota, and Wyoming. High infection incidence was also observed in Alabama and Mississippi.\n\n【23】Overall seasonal WNV incidence estimates were 33.4 cases/100,000 persons in 2010  and 24.7 cases/100,000 persons in 2011 . The estimated incidence for 2012 was 119.9 cases/100,000 persons. Among states, incidence ranged from 12.9 cases/100,000 persons in Virginia to 766.9 and 1,465.4 cases/100,000 persons in North and South Dakota, respectively .\n\n【24】National and state-specific variability in projected WNV infections generally paralleled NID rates reported to ArboNET . In 2010, 2011, and 2012, 629, 486, and 2,872 NID cases, respectively, were reported. Cumulative national estimates of WNV cases were 103,450 cases in 2010 and 76,975 cases in 2011, and the ratio of NID cases to WNV infections was 1 to 164 (95% CI 152–178) in 2010 and 1 to 158 (95% CI 145–174) in 2011 . In 2012, an estimated 376,612 WNV infections occurred, and the ratio of NID cases to WNV infections was 1 to 131 (95% CI 126–136) . Over the 3 years of the study, the weighted ratio of NID cases to WNV infections in the general population was 1 to 141 (95% CI 118–164). In addition, during 2010–2012, projected incidence correlated with NID case frequencies (R 2  value of 0.83 in 2010, 0.83 in 2011, and 0.79 in 2012) .\n\n【25】### Discussion\n\n【26】Our findings, which extended previous findings , highlight the value of using WNV NAT–positive blood donation data to model population incidence in the United States. Our period of data collection covered 3 years, including the large WNV epidemic in 2012, enabling us to demonstrate seasonal and geographic variation in incidence. Using a large geographic catchment area and multiple years of data, we were able to provide updated estimates of the ratio of NID cases to WNV infections, demonstrating stable disease penetrance over the study period and that our estimate (1:141) is closer to that reported for the year 1999 (1:140)  than that reported for the year 2003 (1:256) .\n\n【27】Our analysis of demographic factors shows seasonal and geographic variations of WNV infection rates in blood donors. This study’s incidence estimate of 12.9 infections/100,000 blood donations in 2012 is the same as that reported by Francis et al. for the 2010 outbreak in New York but lower than the 20 and 27 cases/100,000 persons reported for national data in 2003 . Kleinman et al. reported a higher rate of 35 cases/100,000 blood donations for the 2003 epidemic for a specific group of blood centers. Using American Red Cross data and a well-defined confirmatory algorithm (similar to the method in our study), Stramer et al. reported infection rates of 14.9 and 4.4 cases/100,000 blood donations in 2003 and 2004, respectively; these rates are higher than those we estimated for 2010 and 2011 (3.7 and 2.6 cases/100,000 donations, respectively), but rates in 2003 and 2012 appeared comparable. Projected WNV incidence in the general population and NID case frequency decreased slightly from 2010 to 2011 and then spiked upward in 2012 during an outbreak that spread to the Midwest with high incidence rates ; this pattern was similar to that observed during the 2003 epidemic . These results are in agreement with our data showing that the highest projected incidence generally correlated with NID case frequencies .\n\n【28】Patterns of WNV activity vary from year to year, exhibiting temporal and geographic variations of incidence, as shown by our data in the blood donor pool and corresponding projections in general population incidence. Fourteen years after their first appearance in the United States, WNV epidemics are still unpredictable and difficult to control , as confirmed by the surge of cases in 2012, resulting in 286 reported deaths, after years of relatively mild epidemic years .\n\n【29】Previous studies have noted older age and male sex as predictors of severe outcomes (e.g. NID) , but not for detection of WNV RNA, as observed in our study. The strong association that we found with white race/ethnicity is novel and is likely reflective of the fact that more white than nonwhite persons donate blood.\n\n【30】Using data for WNV NAT–positive donors, we estimated that 555,037 WNV infections occurred in the United States during 2010–2012. During that period, ArboNet reported a total of 7,407 NID and non-NID cases . Busch et al. estimated that 735,000 WNV infections occurred during 2003 in the United States, and ArboNet reported 9,862 NID and non-NID cases in 2003 . Cervantes et al. reported an estimated 85,156 WNV infections in northern Texas during the 2012 epidemic, compared with our estimate of 93,004 infections in all of Texas that year.\n\n【31】We report a weighted ratio of 1 NID case per 141 WNV infections during 2010–2012, similar to the ratio of 1 NID case per 140 WNV infections reported during the 1999 New York outbreak . The difference between our estimate and the estimate reported by Busch et al. for the year 2003 (1 NID case/256 WNV infections) may reflect yearly variations of the disease, data quality, or, most probably, the differences in the WNV NAT positivity window used in the study by Busch et al. (6.9 days) and in our study (15.1 days), which could have resulted in an overestimate of WNV incidence and NID ratio in 2003. Since the 1999 outbreak in New York, genetic evolution of WNV has been described or hypothesized in the United States and elsewhere with a hypothetical increase in virus fitness and pathogenicity . The virulent lineage 2 WNV has been implicated in increasing epidemics in Europe and Russia and with devastating cases of NID . Our findings do not support a change in virus penetrance in the United States that might have resulted in the higher number of deaths reported during the 2012 season.\n\n【32】The ratio of infection cases to NID cases is a good surveillance strategy for WNV pathogenic evolution. Although issues with case recognition and passive reporting may result in underreporting of NID cases in the general population, data on NID incidence may be more reflective of the total population that is covered by public health surveillance . On the other hand, decreased public health communication during low-incidence epidemic years could result in underreporting of WNV infections, and increased communication during more severe epidemics could result in more complete reporting.\n\n【33】Strengths of the current study include a very large study population spanning a large geographic region of the United States and a uniform blood donor sampling frame and test methods for WNV RNA. Limitations include geographic gaps in participating blood centers, leading to a potentially biased estimate of incidence in certain states. Blood donor incidence detected by NAT may underestimate infection rates in the general population by as much as 25% because of self-exclusion from donation due to WNV signs and symptoms , resulting in self-selected healthy donors. Also, blood collection centers do not draw from entire states, so some areas are proportionally not represented . Because we used operational data, we had only a limited number of demographic variables and no information on potential exposures to WNV. Last, because we did not have data on whether MP-NAT or ID-NAT screening was used for each donation, we used an average RNA detection duration period of 15.1 days and, thus, may have slightly underestimated or overestimated WNV incidence .\n\n【34】In conclusion, we used a large nationwide dataset obtained from a consortium of blood collection organizations to strengthen the idea that monitoring US blood donations for WNV RNA is a useful surveillance tool for studying the evolution of epidemics and potentially associated pathogenicity. WNV RNA blood donation data are useful for tracking epidemics prospectively (because they are collected in real time) and retrospectively as a complement to existing case-based WNV surveillance networks in the United States.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3fed602f-ca99-4381-a8bc-0015bbf94013", "title": "Pediatric Invasive Meningococcal Disease, Auckland, New Zealand (Aotearoa), 2004–2020", "text": "【0】Pediatric Invasive Meningococcal Disease, Auckland, New Zealand (Aotearoa), 2004–2020\nInvasive meningococcal disease (IMD) is a bacterial infection with typically rapid onset. In children, infection is associated with high (7%–9%) case-fatality rates (CFRs) and serious long-term sequelae . Infants and young children have the highest incidence of disease; a second peak occurs during adolescence . IMD inequitably affects Indigenous populations and persons living in areas of deprivation .\n\n【1】The bacterium _Neisseria meningitidis_ is categorized into serogroups based on its polysaccharide capsule; 6 serogroups (A, B, C, W, X, and Y) are responsible for nearly all IMD cases worldwide . The major clinical manifestations of IMD are meningitis and sepsis. Early recognition is critical because sepsis can rapidly progress to multiorgan dysfunction and death . A leading cause of admission to pediatric intensive care units (ICUs) throughout Australasia , IMD can lead to disabling, long-term sequelae for approximately one third of surviving children, including hearing loss, neurodevelopmental impairment, limb or digit loss, and scarring . Those sequelae heavily affect healthcare resources and the quality of life of affected children and their families .\n\n【2】##### Epidemiology of IMD Globally and in New Zealand\n\n【3】The global incidence of IMD has declined over the past 20 years, partly because of the availability of safe, effective vaccines for all major disease-causing serogroups and successful vaccination programs . Overall incidence of IMD in most high-income countries is well under 1.5 per 100,000 per year . In contrast, New Zealand (Aotearoa) reports the highest rate of _N. meningitidis_ serogroup B (MenB) disease in the world . During 1991–2006, New Zealand experienced a prolonged MenB epidemic caused by the B:P1.7–2,4 strain . The epidemic peaked in 2001, with an incidence of 17.4 cases/100,000 persons in the overall population and 212 cases/100,000 infants . In response, MeNZB, a strain-specific outer membrane vesicle (OMV) vaccine, was developed and delivered nationally in 3 doses to persons <20 years of age during 2004–2006 and in 4 doses to infants during 2006–2008 . Overall vaccination coverage was 80%, and coverage was higher among Pacific peoples compared with those of other ethnicities. The vaccine effectiveness of MeNZB against the epidemic strain was estimated at 68%–77% and was associated with the waning of the epidemic .\n\n【4】Since that time, regional outbreaks of _N. meningitidis_ serogroup C (MenC) and serogroup W (MenW) disease have been associated with high CFRs, prompting emergency targeted vaccination programs in 2011 and 2018 . However, since 2014, the incidence of IMD in NZ has been increasing, up to an overall rate of 2.8 cases/100,000 persons in 2019 . Almost half of cases in 2019 occurred in children <15 years of age, and the highest rates in infants <1 year of age (51.5/100,000 infants). As observed internationally, an increasing proportion of IMD caused by MenW has occurred in New Zealand, accounting for 30% of the country’s cases in 2019 . Auckland, New Zealand’s largest city, has a pediatric (<15 years of age) population of ≈320,000, which makes up 34% of the total New Zealand pediatric population . Ethnic groups in Auckland include Māori (18%), Pacific peoples (19%), and those of Asian (25%) and European (34%) heritage.\n\n【5】##### Meningococcal Vaccines\n\n【6】A 4-component MenB vaccine, 4CMenB (Bexsero; GlaxoSmithKline), was developed using 3 subcapsular antigens and the NZ MeNZB OMV vaccine . Vaccine effectiveness data from Australia, Canada, Italy, and the United Kingdom show reductions in MenB of 71%–100% in eligible cohorts 2–5 years after 4CMenB was introduced . Although there is no evidence that 4CMenB reduces _N. meningitidis_ carriage , OMV meningococcal vaccines appear to provide some protection against IMD caused by non-MenB serogroups, as well as against _N. gonorrhoeae_ . Although 4CMenB and MenACWY vaccines are funded in New Zealand for a small number of persons with high-risk medical conditions and, recently, for adolescents in certain collective residences, no meningococcal vaccines are universally funded in the National Immunization Schedule. We aimed to describe the experience of pediatric IMD in Auckland during 2004–2020—including demographic factors; clinical, microbiological, and laboratory features; treatment; and outcomes—to demonstrate the impact of IMD on children in New Zealand and to highlight the need for funding of meningococcal vaccines.\n\n【7】### Methods\n\n【8】##### Study Design and Collection of Data\n\n【9】We conducted a retrospective, observational study in Auckland. Eligible cases were those in children <15 years of age who contracted IMD while residing within the Auckland region during January 1, 2004–December 31, 2020. We included cases where _N. meningitidis_ was identified by culture or PCR from a normally sterile site (i.e. blood, cerebrospinal fluid \\[CSF\\], synovial fluid).\n\n【10】All persons who test positive for _N. meningitidis_ in New Zealand are actively notified as part of public health surveillance; isolates and DNA extracted from sterile site specimens are forwarded to the Meningococcal Reference Laboratory at the Institute of Environmental Science and Research . The institute provided all cases confirmed by _N. meningitidis_ culture or PCR. We collected data by using National Health Index numbers (a unique identifier for medical care for all persons residing in New Zealand)  from clinical and laboratory records and the National Immunization Register (an electronic record of vaccination events for New Zealand children) .\n\n【11】##### Case Definitions and Variables\n\n【12】We categorized clinical manifestations according to the presence of bacteremia, meningitis, and septic arthritis. We defined bacteremia as a positive _N. meningitidis_ culture or PCR from blood. We defined meningitis as a positive _N. meningitidis_ culture or PCR from CSF or an alternative sterile site positive for _N. meningitidis_ with a CSF leukocytosis or with clinical signs of meningitis if CSF was not obtained. We defined septic arthritis as a positive _N. meningitidis_ culture or PCR from synovial fluid or an alternative sterile site positive for _N. meningitidis_ with clinical signs of septic arthritis. We defined sepsis by Pediatric Sepsis Consensus Congress criteria . We calculated CFR as the number of children who died divided by the total number of cases. For survivors, we classified outcomes as cure, cure with sequelae, and unknown. We used a composite outcome of death and cure with sequelae in our outcome analysis.\n\n【13】We obtained population denominators from Statistics New Zealand  and recorded prioritized ethnicity using New Zealand ethnicity data protocols . We measured socioeconomic deprivation using the New Zealand Index of Deprivation (NZDep) quintiles for 2013 and 2018 . NZDep stratifies small geographic areas into equal-sized groups based on multiple measures of socioeconomic deprivation. We identified serogroup by serological means or by PCR. DNA sequence analysis of the porA gene determined the subtype. We defined the epidemic strain as MenB with the P1.7–2,4 subtype and defined vaccine subtype IMD as any serogroup with the P1.7–2,4 subtype. We determined MICs by using Etest (bioMérieux). We categorized isolates with penicillin MICs of >0.06 mg/L as having reduced penicillin susceptibility and interpreted ceftriaxone, ciprofloxacin, and rifampin MICs according to standardized breakpoints . We defined MeNZB vaccination status as fully vaccinated (received all approved doses for age), partially vaccinated (received less than approved doses for age), unvaccinated (received no doses), or ineligible (born outside of the MeNZB program period). We obtained approval for the study from the Health and Disability Ethics Committees (18/NTA/86/AM02).\n\n【14】##### Statistical Methods\n\n【15】We performed calculations using R  and OpenEpi . We included only cases with available data in the analysis of each variable. We employed a 2-tailed test to determine p values, using a significance level of 0.05, and used a Poisson model to investigate temporal trends in IMD and the epidemic strain. We used univariate logistic regression to investigate factors associated with an increased risk of death or sequelae and χ 2  test to compare rates and calculate 95% CIs. We compared MeNZB vaccination status with timing of IMD illness by using analysis of variance and independent samples t-tests.\n\n【16】### Results\n\n【17】##### Case Numbers\n\n【18】We reviewed data from 331 cases, excluding 12 cases (6 in nonresidents, 5 that were noninvasive disease, and 1 that lacked sufficient data). The remaining 319 cases of laboratory-confirmed IMD occurred in 318 children. One child had 2 unrelated episodes of IMD that occurred in 2006 and 2017. There were no documented relapses after treatment in the cohort. The average annual incidence of IMD across the study period was 5.9/100,000 population. Incidence rates declined from the tail end of the epidemic in 2004 to a nadir in 2014, then increased to a second peak in 2019 . Overall, we found a trend toward reduced incidence over the study period (Poisson coefficient −0.07 \\[95% CI −0.14 to −0.01; p<0.01\\]; rate ratio 0.92 \\[95% CI 0.90–0.95\\]). Cases were more common in winter (135/319, 42.3%), followed by spring (87/319, 27.3%), autumn (52/319, 16.3%), and summer (45/319, 14.1%) (p<0.0001).\n\n【19】##### Demographic Factors\n\n【20】Median age at time of diagnosis was 18 months (interquartile range \\[IQR\\] 7–60 months). The highest average incidence rates were among infants <1 year of age (31.5/100,000 population/year), followed by those 1–4 years of age (8.2/100,000 population/year), 5–9 years of age (2.6/100,000 population/year), and 10–14 years of age (2.0/100,000 population/year) . Average incidence rates by ethnic group were highest in Pacific peoples (13.4/100,000 population/year), followed by Māori (11.6/100,000 population/year) and those who were neither Māori or Pacific (1.9/100,000 population/year) . Based on census data and compared with non-Māori and non-Pacific groups, the unadjusted relative risk of IMD was 5.9 (95% CI 4.4–8.1) for Māori (p<0.0001) and 6.9 (95% CI 5.1–9.3) for Pacific peoples (p<0.0001). Most children (189/317, 59.6%) lived in NZDep quintile 5 (most deprived 20%) areas. The unadjusted relative risk of IMD for children living in NZDep quintile 5 areas compared with quintile 1 areas was 17.2 (95% CI 9.7–33.2; p<0.0001).\n\n【21】##### Microbiology and Laboratory Features\n\n【22】Of the 319 cases, we confirmed a microbiological diagnosis by both culture and PCR for 81 (25.4%), on culture alone for 114 (35.7%), and on PCR alone for 124 (38.9%). We compared _N. meningitidis_ culture and PCR from blood and from CSF . Blood culture was negative for 56 (78.9%) of 71 cases in children who received antibiotics before hospital admission (odds ratio 5.1 \\[95% CI 2.7–9.5\\]) compared with no prehospital antibiotics (p<0.0001). Of those 56 cases, _N. meningitidis_ blood PCR was positive in all 50 cases tested. CSF analysis was performed in 138 (67.6%) of the 204 cases classified as meningitis . CSF leukocytosis for age was present in 130 (97.7%) of 133 cases of meningitis where a CSF leukocyte count was performed. The serogroup was identified for 301 (94.4%) of the 319 cases: 245 (81.4%) were MenB, 26 (8.6%) MenW, 19 (6.3%) MenC, and 11 (3.7%) serogroup Y. Beginning in 2017, there was an increase in disease caused by MenW, which accounted for 8 (29.6%) of the 27 cases serogrouped in 2019; MenB was the remaining predominant serogroup . The epidemic B:P1.7–2,4 strain accounted for 135 cases (44.9%), waning over time, from 42 cases (14.0/100,000 population) in 2004 to 3 cases (0.90/100,000 population) in 2020 (Poisson coefficient −0.20 \\[95% CI −0.28 to −0.11\\]; p<0.01; rate ratio 0.82 \\[95% CI 0.78–0.85\\]). The proportion of isolates with reduced penicillin susceptibility increased during the study period. A MIC of >0.06 mg/L was identified in 28 (22.6%) of 124 isolates in 2004–2012 and 47 (66.2%) of 71 isolates in 2013–2020 (p<0.0001). Reduced penicillin susceptibility was identified in 20 (76.9%) of 26 MenW isolates compared with 55 (32.5%) of 169 non-MenW isolates (p = 0.012). All isolates were susceptible to ceftriaxone, ciprofloxacin, and rifampin.\n\n【23】##### Clinical Features\n\n【24】The median duration of illness before care was sought was 1 day (IQR 1–3 days). Bacteremia was present in 251 cases (78.7%), meningitis in 204 (63.9%), and septic arthritis in 10 (3.1%) . Concomitant bacteremia and meningitis occurred in 141 (44.2%) cases. No cases of chronic meningococcemia were recorded. Sepsis occurred in 172 (80.8%) of the 213 cases with complete systemic inflammatory response syndrome data.\n\n【25】##### Treatment\n\n【26】Of the 319 cases, 317 (99%) were treated in a hospital; 2 children (0.6%) died before arrival. The median duration of hospitalization was 5 nights (IQR 3–7 nights). Prehospital parenteral antibiotics were administered in 52 (16.3%) cases. In the hospital, empiric antibiotics included a third-generation cephalosporin in 294 (92.7%) cases. There were 303 children who completed a full targeted treatment course of antibiotics, which included a third-generation cephalosporin in 230 cases (75.9%), benzylpenicillin in 67 cases (22.1%), amoxicillin in 8 cases (2.6%), and another antibiotic in 4 cases (1.3%). The median duration of antibiotic treatment was 5 days (IQR 5–7 days). Dexamethasone was administered in 47 (23%) of the 204 meningitis cases. Of the 100 (31.3%) children who were admitted to an ICU (median duration of stay 1 night \\[IQR 1–3 nights\\]), 55 received \\> 1 life-saving measure: 46 received invasive ventilation, 44 received inotropic/vasopressor support, and 7 received renal replacement therapy. Plastic surgical procedures were performed in 12 (3.8%) cases, orthopedic procedures in 12 (3.8%), and neurosurgical procedures in 6 (1.9%).\n\n【27】##### Outcomes\n\n【28】Thirteen children died, resulting in a CFR of 4.1% . The average death rate over the study period was 0.24/100,000 population/year. Of the 13 children who died , 12 (92.3%) were Māori or Pacific peoples, 11 (84.6%) were living in NZDep quintile 5 areas, and 9 (69.2%) were infants <1 year of age. Ten deaths (76.9%) occurred in the community or within the first 48 hours of hospitalization. Of the 306 survivors, outcome data were complete for 258 cases (84.3%): cure without sequelae occurred in 197 (76.4%) and cure with sequelae 61 (23.6%). We classified outcome as unknown in 48 cases; all had meningitis with no available audiometry results (none had other sequelae identified on follow-up). Documented audiologic assessment occurred in 143 (72.6%) of 197 cases after meningitis. Of the 142 cases with audiometry results available, 32 (22.5%) had sensorineural impairment. Māori children with IMD had 2.5 (95% CI 1.1–6.4) times the odds for death or sequelae compared with non-Māori, non-Pacific peoples (p = 0.0366) . Pacific peoples with IMD had 2.9 (95% CI 1.3–7.2) times the odds for death or sequelae compared with non-Māori, non-Pacific peoples (p = 0.0128). Results of univariate comparisons of age, sex, NZDep quintile, season, MeNZB vaccination status, sepsis criteria, serogroup, reduced penicillin susceptibility, and prehospital parenteral antibiotics were not significant.\n\n【29】##### MeNZB Vaccination\n\n【30】Of the 163 children with complete vaccination records who were eligible for MeNZB, 114 (69.9%) had received \\> 1 dose and 64 (39.3%) were fully vaccinated at time of hospital admission. For the 97 eligible children with vaccine subtype IMD, 55 (56.7%) had received \\> 1 dose and 31 (32%) were fully vaccinated at time of hospital admission. The mean number of days between the date of last MeNZB vaccine and IMD onset increased with the number of doses received (p<0.00027) .\n\n【31】### Discussion\n\n【32】Despite a reduction in the number of cases of IMD since the MenB epidemic, the incidence of IMD in New Zealand remains double that of other high-income countries . Although MenB remains the most common serogroup in children, the epidemic B:P1.7–2,4 strain no longer dominates in the Auckland region. Rates of pediatric IMD increased in Auckland and nationally in 2014–2019, partly because of an observed global increase in MenW . Mirroring international trends in invasive bacterial disease during the COVID-19 pandemic , there was a sharp decrease in cases of pediatric IMD in NZ in 2020 after national COVID-19 control measures began, and that decrease continued through 2021 . Future patterns of pediatric IMD remain uncertain; however, there is a risk of resurgent disease exacerbated by rising poverty and socioeconomic inequity .\n\n【33】Our findings highlight the severity of IMD. One third of the cases we studied included admission to an ICU, comparable with data for international cohorts . Over half of those cases required invasive ventilation or inotropic/vasopressor support. The CFR in our cohort was 4.1%, which compares to rates for other high-income settings of 2%–12% . Sequelae occurred in 23.6% of survivors. Because outcome was classified as unknown for 48 cases that lacked audiologic data but had no other reported sequelae, we might have overestimated the proportion of survivors with sequelae. Our study revealed that 1 in 4 children did not receive an audiology assessment after meningococcal meningitis. Given this finding, we strongly recommend that children in New Zealand who are diagnosed with meningococcal meningitis receive audiology assessment before hospital discharge. Active follow-up for survivors of IMD should focus on confirming audiology assessment and screening for neurologic, developmental, and psychological effects . Our lack of access to mental health and educational data and shorter follow-up durations of ≥3 months might have underestimated the prevalence of long-term neurocognitive and psychological effects.\n\n【34】Our data demonstrate the usefulness of PCR for diagnosing culture-negative IMD . Blood culture results were negative in 79% of children who received prehospital antibiotics. However, when performed, _N. meningitidis_ blood PCR was positive in all those cases. Drew et al. similarly reported positive blood PCR in 25 of 28 IMD cases that had a negative blood culture after intramuscular penicillin . We suggest that clinicians consider using _N. meningitidis_ PCR testing, especially in the context of prior antibiotic administration. We found no statistically significant differences in clinical outcomes between children who received prehospital parenteral antibiotics and those who did not; however, our study was not powered to detect a difference in those outcomes.\n\n【35】In our cohort, bacteremia and meningitis coexisted in 44.2% of cases; we propose that CSF testing be carefully considered for those with proven meningococcal bacteremia, especially in infants. In cases with bacteremia, 85.9% had a rash at first examination; rash characteristics included purpura (50.7%), petechiae without purpura (40.4%), and blanching only (8.9%), findings similar to those reported for a pediatric cohort in Ireland . Whereas a classic purpuric or petechial rash can suggest IMD, rash at presentation might be nonspecific or absent. It is therefore important for clinicians to maintain a high index of suspicion of IMD in children with suspected sepsis without rash.\n\n【36】In our cohort, we noted an increase over time in the proportion of isolates with reduced penicillin susceptibility. Similar trends have been reported among adults in Auckland, as well as in Spain and Australia . Earlier literature reported an association between reduced penicillin susceptibility and increased complications ; however, no difference in outcomes were noted for our pediatric cohort or for the Auckland adult cohort . New Zealand guidelines recommend a third-generation cephalosporin for empiric treatment of sepsis in children . Because all isolates we studied were ceftriaxone-susceptible, reduced penicillin susceptibility is unlikely to have clinical significance for empiric therapy in New Zealand.\n\n【37】Our study illustrates the considerable inequity of IMD in the Auckland region of New Zealand. Māori and Pacific children had disproportionately higher rates of IMD and were more likely to experience complications. All but 1 death occurred in Māori or Pacific children. Children living in Auckland’s most deprived 20% of neighborhoods had rates of IMD 17 times higher than those in the least deprived 20% of neighborhoods. The relationship between ethnicity, socioeconomic deprivation, and the risk of severe childhood infections is not well understood but is likely rooted in the ongoing effects of colonization and structural racism . Recent findings from a nationally representative longitudinal study, Growing Up in New Zealand , indicate that disparities in infectious disease hospitalizations among infants of Māori or Pacific peoples can be only partly explained by socioeconomic deprivation factors. Nonetheless, household crowding has been shown to be strongly associated with epidemic IMD in New Zealand . Urgent action is needed to honor the nation’s commitment to Te Tiriti o Waitangi, the 1840 founding document that established bicultural partnership between indigenous Māori and the British Crown.\n\n【38】Addressing the upstream determinants of health is important, but vaccination remains the best strategy to control IMD and is a key method for reducing inequity . Although New Zealand’s universal vaccination programs have not yet resulted in equitable uptake, prioritizing delivery and implementation might improve coverage and outcomes for those most at risk . Despite having the highest rate of MenB in the world and some prior success with MeNZB immunization, New Zealand has not yet included 4CMenB in the National Immunization Schedule nor funded vaccine for children at highest risk of disease. The real-world evidence for 4CMenB is clear and demonstrates that control of IMD in New Zealand is within reach .\n\n【39】In conclusion, IMD remains a severe, life-threat¬ening disease in young children in New Zealand; Māori and Pacific infants and those living in areas of socioeconomic deprivation are at greatest risk. The recent increase in incidence of MenB IMD highlights the urgent case for inclusion of 4CMenB in the National Immunization Schedule. Using _N. meningitidis_ PCR to aid diagnosis of culture-negative, clinically suspected IMD, along with routine inpatient audiology assessment after cases of meningococcal meningitis, may improve clinical outcomes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "33ef43c4-2123-4463-9d13-893bfe712437", "title": "Challenges to Adolescent HPV Vaccination and Implementation of Evidence-Based Interventions to Promote Vaccine Uptake During the COVID-19 Pandemic: “HPV Is Probably Not at the Top of Our List”", "text": "【0】Challenges to Adolescent HPV Vaccination and Implementation of Evidence-Based Interventions to Promote Vaccine Uptake During the COVID-19 Pandemic: “HPV Is Probably Not at the Top of Our List”\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】The COVID-19 pandemic has prevented many adolescents from receiving their vaccines, including the human papillomavirus (HPV) vaccine, on time. However, little is known about the impact of the pandemic on implementation of clinic-level evidence-based interventions (EBIs) that help to improve HPV vaccine uptake. In this qualitative study, we explored the pandemic’s impact on EBI implementation and HPV vaccine delivery.\n\n【3】**Methods**\n\n【4】During August–November 2020, we interviewed clinic managers in a rural, midwestern state about their experiences implementing EBIs for HPV vaccination during the COVID-19 pandemic. We used a multipronged sampling approach with both stratified and purposive sampling to recruit participants from Vaccines for Children clinics. We then conducted a thematic analysis of transcripts.\n\n【5】**Results**\n\n【6】In interviews (N = 18), 2 primary themes emerged: decreased opportunities for HPV vaccination and disruption to HPV-related implementation work. Most participants reported decreases in opportunities to vaccinate caused by structural changes in how they delivered care (eg, switched to telehealth visits) and patient fear of exposure to COVID-19. Disruptions to EBI implementation were primarily due to logistical challenges (eg, decreases in staffing) and shifting priorities.\n\n【7】**Conclusion**\n\n【8】During the pandemic, clinics struggled to provide routine care, and as a result, many adolescents missed HPV vaccinations. To ensure these adolescents do not fall behind on this vaccine series, providers and researchers will need to recommit to EBI implementation and use existing strategies to promote vaccination. In the long term, improvements are needed to make EBI implementation more resilient to ensure that progress does not come to a halt in future pandemic events.\n\n【9】Introduction\n------------\n\n【10】Since March 2020, the pandemic caused by SARS-CoV-2 has affected nearly all aspects of daily life, particularly in the ways in which health care is provided. Because of fear of coming into clinics and the preference for telehealth appointments, one area that has been especially affected is pediatric and adolescent immunization . Before the pandemic, data from the 2019 National Immunization Survey–Teen showed that only 54% of US adolescents aged 13 to 17 were up to date with the human papillomavirus (HPV) vaccine series . An analysis to estimate the impact of the pandemic on HPV vaccination found that vaccination rates were 75% lower during the pandemic compared with prior periods, and statistical modeling showed that these lower rates could lead to increases in the incidence of genital warts, cervical intraepithelial neoplasia, and other HPV-related cancers if adolescents do not catch up on the HPV vaccine . Adolescence is a critical period for HPV prevention. The Advisory Committee for Immunization Practices recommends administration of 2 doses of HPV vaccine for children and adolescents aged 9 to 14 years . Initiating administration early in adolescence is linked to higher rates of on-time completion  and increased effectiveness . Therefore, understanding the extent to which the pandemic has affected adolescents’ ability to get vaccinated is imperative.\n\n【11】Challenges in vaccinating adolescents during the last decade have led researchers and quality improvement (QI) staff to develop evidence-based interventions (EBIs) and strategies to assist clinical staff in increasing HPV vaccination. For example, commonly used EBIs include reminder/recall systems, standing orders for HPV vaccination, and provider assessment and feedback . Because of the complexity of these EBIs, a substantial amount of work happens “behind the scenes” in clinics. These implementation efforts to increase HPV vaccination rates  are often conducted independently or in collaboration with academic and community partners and led by administrative, non–patient-facing staff.\n\n【12】Nearly 2 years into the pandemic, little is known about the impact of the pandemic on implementation of these EBIs to encourage vaccinations or about the experience of clinics that continued to provide routine care during the pandemic. Our attention now needs to turn to these lesser studied impacts on clinic practices that have implications for future health outcomes of adolescents. The aim of this qualitative study was to explore the experiences of clinics that continued to provide routine care during the pandemic and the impact of the pandemic on ongoing implementation efforts to promote HPV vaccination.\n\n【13】Methods\n-------\n\n【14】This study was part of a larger project using the Consolidated Framework for Implementation Research  to understand barriers and facilitators to EBI implementation focused on HPV vaccination in clinics integrated or affiliated with large health care systems in Iowa. However, only results related to the impact of COVID-19 are reported here. We conducted semistructured interviews with clinic managers or administrators working in Vaccines for Children (VFC) clinics in large health care systems in Iowa from August through November 2020. The University of Iowa Review Board determined that this study did not meet the criteria for human subjects research. All participants were provided with information about the study and its purpose, compensation, the voluntary nature of their participation, and the researcher’s contact information. We offered a $25 gift card to all participants to thank them for their time.\n\n【15】We used multipronged sampling that included stratified sampling of VFC clinics in Iowa. First, researchers examined the list of VFC clinics in the state (N = 594) and excluded clinics that were either not pediatric/family practice clinics or not integrated or affiliated with a larger health care system, resulting in a final list of 305 clinics that met inclusion criteria. We stratified clinics by congressional district and rurality; a random sample of clinics (n = 5) was drawn from each stratum (n = 8); we repeated this process, ultimately recruiting from a sample of 80 clinics and completing 9 interviews. Up to 6 attempts were made to contact the clinic manager at each clinic, either by email or telephone. Common reasons for refusal were lack of time due to COVID-19 or not currently having a staff member in an administrative or management position. When this approach did not achieve thematic saturation in interviews, directed recruitment efforts were made through professional networks. Throughout the interview process transcripts were reviewed for thematic saturation, and recruitment ended when it was determined saturation had been reached.\n\n【16】We adapted the interview guide from the CFIR . In addition to questions to address the CFIR constructs, we included questions to address the impact of COVID-19 on HPV vaccination delivery and on implementation of EBIs for HPV vaccination. Before the interview, we sent all participants a brief survey to collect demographic information about them and their clinic. All interviews were conducted by the first author (G.R.) via telephone and audio recorded. A third-party service was used for verbatim transcription.\n\n【17】We generated frequencies and appropriate descriptive statistics for survey items capturing information on participant and clinic characteristics. To analyze the interviews, we conducted thematic analysis  that explored the impact of COVID-19. In the first round of coding, we used Nvivo version 12 (QSR International) to code transcripts and created a code to identify information on the pandemic (“impact of COVID-19”). This process was completed by the first author and a trained student in a master of public health program. We then analyzed the data coded under “impact of COVID-19” using a thematic analysis approach  to identify themes and subthemes.\n\n【18】Results\n-------\n\n【19】We completed interviews with 18 individuals; interviews ranged from 19 to 50 minutes, averaging 32 minutes. Of the 18 participants, 8 were aged 27 to 39, all were women, 14 worked in a rural clinic, and 12 worked in general practice or family medicine clinics .\n\n【20】Two primary themes emerged in all interviews under the parent code of impact of COVID-19: decrease in HPV vaccination and routine care and impact of the pandemic on implementation work . In a minority of interviews, a third theme was also identified: patient safety improvements.\n\n【21】### Decreased opportunities for HPV vaccination and routine care\n\n【22】Overall reduction in in-person clinic visits due to the pandemic posed tremendous challenges to the clinics in delivering HPV vaccinations. Participants spoke about 2 main challenges in being able to vaccinate patients. First, all clinics had to implement new protocols to safely treat patients, which included reducing overall patient volume and switching to telehealth visits, both of which resulted in fewer adolescents being seen in person. To reduce patient volume, many participants reported that a respiratory clinic was established to physically separate sick patients from well patients, and this meant less time and capacity to see well patients. As one participant described, “\\[R\\]ight now, the biggest priority in our clinic is the sick clinic; we’re doing COVID testing in our sick clinic. That’s our biggest area here” (Interview-8). Other participants noted that shifts to virtual visits presented numerous challenges to HPV vaccination. In the first place, these changes required time to implement, and in the second place, specific to HPV vaccination, it meant that “when they’re pushing virtual visits, it’s not going to get them into the door to get those vaccines done” (Interview-11).\n\n【23】The second main challenge was fear of COVID-19 exposure. Most participants reflected on the fear among their patient population of coming into clinics for preventive care such as vaccinations and being exposed to the virus. Although none of the participants had calculated exact numbers for the reduction in vaccinations, nearly all identified patient fear of COVID-19 as a barrier to vaccinating adolescents. As one participant summarized, “\\[F\\]or quite a while in the spring, people were really reluctant to come to the doctor’s office, because they felt like we’re a hotbed of disease” (Interview-10). Parents and guardians were not the only ones afraid of attending routine care visits; the clinics themselves shut down for a certain period, and schools made allowances about attendance at sports physicals in recognition that parents may not want to bring their children into clinics. For example, one participant reported that “in April  . \\[the clinic\\] essentially cancelled every other visit except zero- to five-\\[year-old patients\\]” (Interview-3). Another participant spoke about the ruling that schools allowed prior years’ sports physicals to count for school entry, so they “missed some opportunity this year getting in \\[their\\] normal amounts” of those visits (Interview-9). Together, the shifts made in clinics to provide patient safety and the hesitancy among parents to take their children into clinics translated into a reduction in HPV vaccinations.\n\n【24】### Disruption to ongoing EBI implementation work\n\n【25】We identified 2 subthemes: disruptions due to logistical challenges and disruptions due to shifts in priorities during the pandemic. All participants noted that the COVID-19 pandemic had a negative impact on their ability to carry out ongoing projects and implementation of EBIs to increase HPV vaccination rates. Several participants spoke about previous efforts with QI teams for HPV vaccination projects. As one clinic manager described, “\\[I\\]t’s just kind of fizzled off with . COVID” (Interview-14). Others reflected on projects that were ongoing with external partners that had been forced to halt because of the pandemic. For example, one participant spoke about a project with a pharmaceutical company to implement new strategies to promote HPV vaccination that they had started, but with the pandemic “all those meetings and such came to an end because \\[they\\] couldn’t meet in person anymore” (Interview-6). Another clinic manager spoke about a school outreach program to promote HPV and other adolescent vaccines in which their clinic usually participates during the spring. However, she reflected that “school wasn’t in session at that time. So, we missed that opportunity” (Interview-5). Finally, several participants noted that because of the pandemic, pharmaceutical representatives who are usually allowed into the clinic to provide education were not able to come and that the regularly scheduled state immunization conference was cancelled. These participants reported that these education opportunities are the primary way staff and providers learn about updates to HPV vaccination and best practices and motivate staff to implement EBIs.\n\n【26】These interruptions were due to both logistical challenges and shifts in priorities that were necessitated by the pandemic. For example, early in the pandemic, many participants noted that because of shut-downs in routine care services, non–patient-facing staff were furloughed while multiple providers were out of the office due to mandatory quarantines resulting from COVID-19 exposure or infection. As one participant noted, “When we \\[were\\] short-staffed . that changed a lot of different workflows” (Interview-2), which meant increased time was spent to create new workflows, detracting from time available for other projects. In addition to creating staff shortages, the pandemic also disrupted regular communication between participants and others working on EBI implementation or HPV vaccine promotion. For example, one participant noted that their QI team had been redirected to focus on COVID and so “prior to COVID \\[they\\] were meeting once a month. But since COVID, \\[they’ve\\] gotten pulled into more of the COVID-related areas” (Interview-8). Another clinic manager noted that all her team communication had to switch to an online format and “there are some challenges of not having that person in front of you to talk to” (Interview-5) and that this lack of in-person communication had a particularly negative impact on implementation work. Related to fears of COVID-19 transmission, one participant noted that the clinic had to pull all educational materials from waiting areas because it didn’t “want to lay them out for the patient to pick up” (Interview-4).\n\n【27】Beyond these logistical challenges, the inability to work on EBI implementation and projects related to HPV vaccination was primarily due to the shift in priorities. As one participant summarized it, “\\[R\\]ight now with COVID, I would be amiss if I didn’t say HPV \\[is\\] probably not at the top of our list. We’re trying to make sure that people are staying healthy” (Interview-17). Many participants spoke about the challenges of maintaining safety protocols and the extra work that came along with that, and how these challenges had led to a reduced focus on HPV vaccination overall. Another participant noted that these shifts were not just occurring in clinics but also that “COVID as a whole has changed our health organization. Looking at how do we bring people in safely has been a huge thing” (Interview-1). This need to focus on patient safety, above all else, has been necessary, but all participants who spoke about these shifts said that it meant they have not had time to focus on EBI implementation for HPV vaccination.\n\n【28】### Patient safety improvements for infection control made during the COVID-19 pandemic\n\n【29】Finally, in discussing implementation of EBIs for HPV vaccination, several participants spoke more generally about how the pandemic has changed health care delivery and lessons learned for the future. These participants spoke about some positive changes that have resulted from the need to be more creative about health care delivery and patient safety. They reflected that there are likely to be some permanent changes to health care delivery for their clinics and health care systems that may have implications for how EBIs are implemented. Several reported that the health care systems their clinics are affiliated with had created special respiratory clinics ― designed to control infection ― to care for COVID-19 patients. One reported that “we plan on keeping the respiratory clinic going forever. With all our respiratory stuff, it just makes sense, really” (Interview-16). Another common change was creating separate entrances or times for well and sick patients to be seen. One participant said they “had to reinvent the wheel as far as what keeps people safe, and how \\[they\\] still operate and get things that keep people healthy, without giving them the opportunity to catch something” (Interview-1). Because of the attention they devoted to these efforts, this participant spoke with her team about continuing with these changes throughout respiratory syncytial virus (RSV) season and indicated that many of these changes make “the most sense to keep the most majority of the people healthy coming in” (Interview-1). Implications of these kinds of permanent changes are not yet known, although one participant noted that in her clinic this change had resulted in fewer staff members being available for routine preventive care in the short term.\n\n【30】Discussion\n----------\n\n【31】Results from these interviews highlight a unique perspective on the impact of the COVID-19 pandemic on adolescent HPV vaccination, that of administrative clinic staff, most of whom work in rural areas. Participants in this study, while not directly involved in health care delivery, manage much of the work that happens behind the scenes to ensure patients receive the care they need. Across interviews, the impact of the pandemic on not only adolescent HPV vaccination but on all health care delivery and related EBI implementation work was evident. Interviews focused on implementation of EBIs for HPV vaccination specifically, but many of the barriers reported in relation to this area also applied to other areas. Data from 2020 identified sharp decreases in adolescent and pediatric vaccination  as well as well-child visits  that have likely persisted into 2021. Now, with the authorization of the COVID-19 vaccine for both adolescents and children, HPV vaccination may not be at the forefront of parents’ or clinicians’ priorities. Clinics will need to recommit to or expand their HPV vaccination efforts to ensure adolescents are vaccinated on time.\n\n【32】In circumstances without the added pressure of the pandemic, clinics face challenges implementing existing EBIs to encourage HPV vaccination uptake, such as lack of staff to implement EBIs or QI initiatives, lack of knowledge about which EBIs to implement , competing priorities, the need for more staff training, and limited resources . With the added stresses of the pandemic, these challenges have been compounded, and many participants reported that because of the need to address pandemic-related issues, HPV vaccination had fallen lower on the list of priorities. This has meant that ongoing QI efforts and EBI implementation to improve vaccination rates were often halted. By necessity, the response to the pandemic has been reactive, rather than proactive, which means that processes that were already in place were not prioritized during the pandemic. To overcome this, clinic staff should refocus on implementing strategies known to work to promote HPV vaccination (eg, reminder/recall, strong provider recommendation)  and have been effective in increasing rates for other vaccines .\n\n【33】Although these interviews focused on challenges presented by the pandemic, several participants spoke about some of their unexpected findings from their efforts to continue providing health care. These participants spoke about how the pandemic was a learning opportunity in keeping patients safe during large-scale outbreaks and noted that they would continue some of their precautions in the future to deal with other infectious diseases. Although the COVID-19 pandemic has had an overwhelmingly negative impact on health care, valuable lessons have been learned about how to continue delivering primary care. However, the time spent to make these changes was at the expense of other ongoing work. For example, many participants spoke about the shift in priorities and the time that was needed to create processes for telehealth visits. Future research should focus on identifying best practices that have been developed during the pandemic to support not only future pandemic responses but potentially also dealing with typical influenza seasons.\n\n【34】Although the topical focus of these interviews was HPV vaccination, results highlight challenges that have likely been present in all implementation work during the pandemic. There have been calls from the implementation science community to use implementation science to address COVID-19 , but less attention has been paid to how to address the fact that so much of the ongoing implementation work came to a halt during the pandemic. At this juncture, the pandemic is likely far from over, and history shows us that other pandemics and epidemics will occur. Implementation science researchers need to create resilient and sustainable EBIs and implementation processes that are not as vulnerable to emergency situations. This could mean focusing on implementing practices that could be more sustainable in emergency situations, for example, ensuring systems are in place to use reminder/recall messaging. Sustainability has long been a challenge for implementation science, and many implementation studies lack an explicit definition of what sustainability means in practice , making it even harder for researchers and practitioners alike to focus on best practices in this area. The current situation and data from these interviews highlight that researchers must renew their focus on resiliency and sustainability for EBI implementation.\n\n【35】Our study has several strengths. The primary strength was the use of qualitative methods to gather detailed and descriptive information from a relatively understudied perspective during the COVID-19 pandemic, namely clinic managers working in rural settings. Interviews allowed for detailed and nuanced data from clinic managers about the challenges presented by the pandemic to HPV vaccine delivery and EBI implementation. Additionally, more than three-quarters of participants worked in rural clinics, providing another often-understudied perspective on health care delivery and implementation.\n\n【36】This study also has several limitations, primarily related to the timing of these interviews. When interview recruitment began in August 2020, COVID-19 cases were relatively low in Iowa, but by mid-November cases had risen again; therefore, participants may have had different perspectives on the impact of COVID on their work and organizations. However, despite these limitations, these results offer critical insights into this issue, and future research could focus on understanding perspectives from clinic managers in other geographic areas.\n\n【37】In summary, pre-existing low rates of HPV vaccination coupled with the impact of the pandemic threaten to leave adolescents unprotected against HPV and with increased susceptibility to HPV-related cancers. Our results have short- and long-term implications for both practitioners and researchers working in the fields of adolescent health, HPV vaccination, and implementation science. In the short term, a renewed commitment to EBI implementation for adolescent HPV vaccination is needed to ensure that those who are eligible now as well as those who may have missed doses over the past 2 years are vaccinated. Research conducted before the pandemic found that clinics do not always use EBIs and, when they do, there are significant challenges in implementing them . These challenges have been exacerbated by the pandemic, and both researchers and clinic staff will need to expend even more effort in this area. For example, for clinics that previously did not have reminder recall systems in place, these systems could be one way to identify all undervaccinated adolescents. However, implementing these new systems may require substantial effort given that the pandemic has taken priority and those involved may need to work even harder to obtain staff buy-in and leadership support. In the long term, the implementation science community needs to create more resilient and sustainable EBIs that can be easily implemented in health care systems. Doing so will help protect future adolescents against missing HPV vaccinations, as well as other preventive health care, during pandemic or emergency situations, like the one we are currently experiencing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "df374f8e-189f-4f86-b1b3-dbcac411d073", "title": "The Wages of Original Antigenic Sin", "text": "【0】The Wages of Original Antigenic Sin\n“The deliberate sin of the first man is the cause of original sin”\n\n【1】—\\[Saint\\] Augustine of Hippo, Algerian Christian theologian (354 ad –430 ad ), De nuptiis et concupiscientia \\[On Marriage and Concupiscence\\], II, xxvi,\n\n【2】What epidemiologist Thomas Francis, Jr. (1900–1969) was thinking when pondering certain inexplicable serologic data from a 1946 influenza vaccine trial may never be known. Whether in religious reverence for the beauty of science or impish delight fueled by the martini breaks of which he was so fond, Francis coined the term “original antigenic sin” to describe a curious new immunologic phenomenon. Elsewhere in this issue, Adalja and Henderson propose that original antigenic sin has altered the population age–specific incidence of infection and disease caused by influenza A pandemic (H1N1) 2009 virus and that public health responses must account for the disruption . What is original antigenic sin, what is its immunologic basis, and into what sort of trouble is it getting us?\n\n【3】Discovery of influenza viruses in the early 1930s ignited a search to understand the epidemiology of pandemic/endemic influenza. Serologic data showed that decendents of the 1918 pandemic influenza virus were still circulating and were changing antigenically (we would now say drifting and undergoing intrasubtypic reassortment); that contemporary human and swine viruses were closely related; and that over a lifetime of repeated exposures, different human birth cohorts were acquiring fundamentally different influenza infection experiences. The surprise appearance in 1946 of a new and antigenically different influenza A virus (designated influenza A′ and recently shown to be a subtype H1N1 intrasubtypic reassortant) provided Francis a unique opportunity. College students participating in a 1946 trial of the old 1946 virus vaccine were infected in March 1947 with the new A′ virus. Surprisingly, these students developed low serologic titers to the new infecting virus and higher seroconverting titers to old viruses with which they previously had been infected. Moreover, recent recipients of the old virus vaccine had the highest seroconverting titers of all to the old—but not to the new—virus .\n\n【4】Absorption studies, in which various viruses were used to selectively remove serum antibodies, suggested that repeat exposures to dominant antigens of first-infecting viruses, when seen later as lesser or secondary antigens on subsequently infecting viruses, somehow reinforced antibody responses to the original strains at the apparent expense of responses to newer strains . Francis announced “the doctrine of original antigenic sin” : “\\[t\\]he antibody-forming mechanisms appear to be oriented by the initial infections of childhood so that exposures later in life to antigenically related strains result in a progressive reinforcement of the primary antibody” . Later studies by many investigators showed original antigenic sin to be a general phenomenon associated with numerous related/sequentially infecting virus strains that contain multiple external epitopes of varying cross-specificity (i.e. ability to elicit cross-reactive antibody), including antigenically drifting viruses such as influenza A, and the more stable flaviviruses, which circulate concurrently as multiple distinct viruses, virus serotypes, and virus strains .\n\n【5】Original antigenic sin seems to be most pronounced when sequential viruses are of intermediate antigenic relatedness; when they are antigenically complex; and when sequential exposure intervals are long, consistent with ongoing selection and expansion of lymphocyte clones that have increasing antibody avidity at key cross-reactive epitopes  and possibly with epitope competition between naïve and antigen-specific B cells . A phenomenon analogous to original antigenic sin also has been described with cytotoxic T lymphocytes . Although conclusive evidence in humans is lacking, original antigenic sin recently has come under scrutiny as a possible cause of viral immune escape, enhanced disease severity, decreased efficacy of influenza vaccines , and increased incidence of influenza in 2009 after vaccination with a related virus in 2008–2009 . On a positive note, original antigenic sin has also been linked to vaccine-induction of heterosubtypic neutralizing antibodies .\n\n【6】Adalja and Henderson note that the apparently lower incidence and severity of disease in older persons during the 2009–10 influenza pandemic probably reflects immunity to previously circulating influenza (H1N1) subtypes . Reichert et al. also attribute this age structure to original antigenic sin but emphasize the importance of exposures to the changing hemagglutinin glycosylation patterns of earlier influenza (H1N1) subtypes (e.g. those circulating before and after 1948) on a background of relatively conserved T-cell epitopes . However, the possibility that the age structure of pandemic (H1N1) 2009 infection is due simply to single or repeated exposures to different or differentially exposed hemagglutinin epitopes has not been ruled out. Useful information bearing on these questions might be gained by comparing antibody levels, antibody reactivities, and the original antigenic sin phenomenon in serum samples from the various age cohorts that had early exposures to markedly different (or to no) influenza (H1N1) serotypes, e.g. persons born before 1918; during 1918–1927, 1928–1946, 1947–1956, and 1957–1976; and after 1976. Of related interest are the 2009 influenza experiences of the ≈25.6 million persons living in America vaccinated with the 1976 Hsw1N1 vaccine , including 2.5 million born during 1957–1975, when influenza (H1N1) viruses did not circulate\n\n【7】The current pandemic provides the challenge to public health responses that Adjala and Henderson describe, as well as an opportunity to extend the efforts of Francis to better understand the complicated epidemiology of influenza. Is original antigenic sin really a sin from which our immune systems need to be saved? Or is it an epidemiologic blessing in disguise? We have much more to learn. As St. Augustine wrote (Confessiones, 8, 7): “Lord make me chaste—but not yet.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "516cb850-3307-4ffa-b7ff-258078b1cc47", "title": "Plague Epizootic Dynamics in Chipmunk Fleas, Sierra Nevada Mountains, California, USA, 2013–2015", "text": "【0】Plague Epizootic Dynamics in Chipmunk Fleas, Sierra Nevada Mountains, California, USA, 2013–2015\nTo better forecast vectorborne infection dynamics, characterizing disease cycles in both hosts and vectors is critical. The rate of infection of vector species can serve as a good indicator for risk during epizootic events, especially in areas with high human–wildlife overlap, but vectors are often poorly sampled. _Yersinia pestis_ , the bacterium that causes plague, is carried by multiple flea species in western North America, where sciurids are often the primary reservoirs . Although human plague cases in this area are rare, in 2015, two cases were linked to exposures in Yosemite National Park, California, USA . In the investigation conducted to determine the source of these exposures, multiple _Y. pestis_ –positive flea and rodent species were documented, and the lodgepole chipmunk ( _Tamias speciosus_ ) was the host that was most frequently seropositive .\n\n【1】Plague surveillance in the western United States typically involves serologic testing of rodents and carnivores. Positive serologic results indicate prior plague activity. A _Y. pestis_ –positive flea, however, indicates current plague transmission and is more likely to trigger control activities . Here, we sought to characterize _Y. pestis_ infection in fleas of alpine ( _T. alpinus_ ) and lodgepole ( _T. speciosus_ ) chipmunks in Yosemite National Park and surrounding areas during 2013–2015. We focused on _T. speciosus_ chipmunks because of their documented role in the 2015 epizootic  and on _T. alpinus_ chipmunks because they co-occur with _T. speciosus_ chipmunks  and little is known about their role in plague ecology. Our goals were to describe the proportion of _T. speciosus_ and _T. alpinus_ chipmunks harboring _Y. pestis_ –positive fleas and the minimum infection prevalence of _Y. pestis_ in fleas collected from these species across multiple sites and in years with and without known epizootic activity.\n\n【2】During June–October 2013–2015, we collected fleas from tagged chipmunks. Using a metal-pronged comb, we combed each animal 5 times down the dorsum, the tail, and each hind leg and placed collected fleas into vials containing 100% ethanol. These procedures were approved by the University of California, Berkeley, Animal Care and Use Committee (Berkeley, California, USA).\n\n【3】We identified key flea specimens (N = 122)  and then cleared, dehydrated, and mounted them on microscope slides (Denver Museum of Nature and Science accession nos. ZP.2000–176). For the remaining fleas, we microscopically observed and identified the species  using keys  and mounted some fleas as references. For each host, we pooled all conspecific fleas, which resulted in 162 pools (with 291 fleas total) from 121 _T. alpinus_ chipmunks and 538 pools (with 1,096 fleas total) from 389 _T. speciosus_ chipmunks . We used molecular methods to detect _Y. pestis_ DNA in flea pools .\n\n【4】We found _Y. pestis_ –positive fleas exclusively in 2015 at 5 of the 6 sites surveyed . In 2015, 7.29% (14/192) of _T. speciosus_ hosts carried \\> 1 _Y. pestis_ –positive flea. The minimum infection prevalence of _Y. pestis_ in _T. speciosus_ chipmunk–hosted fleas was 3.28% (assuming 1 positive flea per positive pool, 18 positive pools/548 total fleas in 280 pools tested). All 3 of the flea species ( _Ceratophyllus ciliatus mononis_ , _Eumolpianus eumolpi_ , and _E. eutamiadis_ ) most commonly found on _T. speciosus_ and _T. alpinus_ chipmunks were found to be positive for _Y. pestis_  . In 2015, a total of 5.13% (2/39) of _T. alpinus_ hosts carried \\> 1 _Y. pestis_ –positive flea . The infection prevalence (not minimum infection prevalence because each positive pool contained a single flea) of _Y. pestis_ in _T. alpinus_ chipmunk–hosted fleas was 2.47% (2 positive pools/81 total fleas in 50 pools tested). Unfortunately, these fleas were too damaged to identify morphologically, and molecular species identification was not possible.\n\n【5】_Y. pestis_ –positive flea pools were detected at 5 of 6 high-elevation (2,650–3,200-m) study sites in 2015. Many of these sites are areas of high human activity, with popular hiking trails or established campgrounds. In 2015, plague risk assessments, including testing flea pools and rodent carcasses for _Y. pestis_ DNA and rodent serology, also took place at lower elevation sites (1,778 ± 553 m) in and around the park; these surveys detected _Y. pestis_ at 4 of 17 locations .\n\n【6】Altogether, our data indicate a dramatic shift in _Y. pestis_ prevalence in fleas during a plague epizootic year in California. Our results support integrating flea testing, especially those at high-elevation sites, into regular surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d1067907-c33e-42d7-b2fd-e0d9c77377fd", "title": "Melioidosis in Animals, Thailand, 2006–2010", "text": "【0】Melioidosis in Animals, Thailand, 2006–2010\nMelioidosis is a serious infection caused by the gram-negative bacillus and biothreat organism, _Burkholderia pseudomallei_ . It is the third most frequent cause of death from infectious diseases in northeastern Thailand (after HIV/AIDS and tuberculosis)  and the most common cause of community-acquired bacteremic pneumonia in northern Australia . Melioidosis also occurs in a wide range of animal species; most cases reported in the literature are in livestock in northern Australia . In Thailand, serologic studies that use the indirect hemagglutination test (IHA) have indicated that pigs, sheep, goats, and cattle are exposed to _B. pseudomallei_ , but to our knowledge, culture-confirmed melioidosis in animals has not been reported in the literature . We describe the findings of a study to estimate incidence of melioidosis in animals in Thailand and compare the geographic distribution of melioidosis in animals with that in humans.\n\n【1】### The Study\n\n【2】A retrospective study was performed to collect data on all animals recorded to have died of melioidosis and the total number of livestock in Thailand during January 1, 2006–December 31, 2010. This information was obtained from the Department of Livestock Development, Ministry of Agriculture and Cooperatives, Thailand. Information about animal melioidosis was derived from necropsies on animals that died of unknown causes which are taken to the National Institute of Animal Health in central Thailand or to 1 of 8 veterinary research and development centers throughout Thailand. Necropsy is performed principally to monitor for infectious diseases that may be associated with outbreaks in farm animals. During the study period, IHA, blood culture, and pus culture (if available) for _B. pseudomallei_ were performed if melioidosis was suspected as the cause of death.\n\n【3】Melioidosis was diagnosed as the cause of death in 61 animals. For 49 (80%) of these animals, diagnosis was based on a culture positive for _B. pseudomallei_ from \\> 1 clinical specimens; for 12 (20%) animals, cultures were negative but samples were IHA positive for melioidosis (based on a cutoff value of \\> 320). Cases diagnosed by IHA, but culture negative, were excluded from further analysis because apparently healthy animals in melioidosis-endemic areas can have high bacterial titers . The animal species affected were goats , pigs , cattle , deer , horse , and wild animals in captivity (camel, crocodile, monkey, and zebra \\[1 each\\]). Thirty-one (61%) of the 49 cases were identified during the rainy season (June–November). The estimated incidence rate of melioidosis was highest in goats (1.63/100,000/year), followed by incidence in pigs and cattle (0.02 and 0.01/100,000/year, respectively) .\n\n【4】We mapped the geographic distribution of melioidosis in goats by province and compared the meloidosis distribution with the total number of goats in the country during the same period . The average number of total goats per year for 2006–2010 was 381,405. Most (41%) were in provinces in the south, with the remainder in western (19%), central (16%), northern (16%), northeastern (5%), and eastern (3%) Thailand. Although goats were not numerous in northeastern Thailand, provinces with the first and third highest incidence of goat melioidosis (Sakon Nakohn and Khon Kaen) were situated in the northeast, a region with the highest reported incidence of human melioidosis . The incidence rate of goat melioidosis was low in the south, where the incidence of human infection has not been defined but appears to be low according to cases reported in the literature . The relative incidence of goat melioidosis was also high in western and eastern Thailand, regions where human melioidosis is not considered endemic. No reports of human melioidosis in the west have been published, and the 1 report from the east described 78 blood culture–positive cases during 3 years in Sa Keao Province, from which an annual incidence rate was calculated of 4.9 per 100,000 persons . To further evaluate this finding, we contacted 4 provincial hospitals in eastern and western Thailand, where cases in animals were observed, to request information about the number of culture-confirmed melioidosis cases in humans. Culture-confirmed human melioidosis cases were observed each year in all 4 hospitals . Our findings indicate that that the geographic area of Thailand affected by melioidosis is much greater than appreciated previously.\n\n【5】### Conclusions\n\n【6】The cases of animal melioidosis in Thailand reported here probably underestimate actual cases because necropsies are performed on a small minority of animals that die of natural causes. Goats are a major domestic animal in Thailand, particularly in western and southern Thailand. We demonstrated that the estimated incidence of melioidosis in goats relative to other animals is high and might represent a substantial cause of economic loss for goat farmers. The high susceptibility of goats to melioidosis relative to that of pigs and cattle is consistent with a previous report from Australia . Mapping of goat melioidosis demonstrated concentrations of cases in specific provinces and large areas of the country with no apparent disease. This finding might represent an artifact, based on the availability of information from specific veterinary centers, and we propose that animal melioidosis might be underestimated and widely distributed across the country. We predicted that the incidence of melioidosis in goats would parallel that in humans because both would be exposed to similar levels of environmental _B. pseudomallei_ . This finding proved to be the case in northeastern Thailand, where the incidence of human melioidosis is high and where a small population of goats also had a high relative incidence. Such was not the case in the west, however, where the relative incidence of goat melioidosis was high, but human cases have not been reported in the literature. Possible explanations include a more active approach to determine causes of death in goats or failure to diagnose or report human disease in the west and, less plausibly, the presence of an animal-adapted _B. pseudomallei_ strain that fails to infect humans. Contact with hospitals in western and eastern Thailand indicated that melioidosis was more common than the literature suggests. Further studies to determine an accurate incidence of human and animal melioidosis throughout Thailand are required. The ability to culture and detect _B. pseudomallei_ in clinical microbiology laboratories countrywide also needs to be improved .\n\n【7】Our findings support the recommendation for pasteurization of goat milk before consumption in Thailand, which is necessary to prevent human brucellosis and also will prevent ingestion of live _B. pseudomallei_ in milk. Our study was not designed to detect evidence for animal-to-human transmission in potentially at-risk populations, such as herdsmen, veterinarians, or abattoir workers. Such detection is a future objective in Thailand because presumptive zoonotic infection has been reported in Australia . Melioidosis is not currently part of the animal disease control program in Thailand, but its inclusion may now warrant review.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1156b42d-4243-4086-b1c3-e08748507258", "title": "Clostridium difficile Infection in Outpatients, Maryland and Connecticut, USA, 2002–2007", "text": "【0】Clostridium difficile Infection in Outpatients, Maryland and Connecticut, USA, 2002–2007\nIn the United States, ≈375 million episodes of acute diarrhea occur annually . Among hospitalized persons, toxin-producing _Clostridium difficile_ is a primary diarrheagenic pathogen, usually as a consequence of normal bowel flora distortion caused by antimicrobial drug therapy . _C. difficile_ infection (CDI) complicates and prolongs hospital stays, leading to increases in health care costs, illness, and death. Recent reports suggest increases in community-onset CDI among persons without recent antimicrobial drug treatment or hospitalization. We describe a prospective evaluation of CDI in persons with diarrhea who visited emergency departments (EDs) and ambulatory primary care clinics in Baltimore, Maryland, and New Haven, Connecticut, and identify microbiologic causes and epidemiologic characteristics of diarrhea. This report highlights cases of outpatient CDI, identifies factors associated with infection, and describes molecular strain characterization.\n\n【1】Patients seeking medical attention for community-onset diarrheal illnesses were enrolled from May 2002 through September 2004 in the EDs and ambulatory clinics at Yale–New Haven Hospital (New Haven, CT, USA) and from May 2002 through July 2007 at EDs and clinics affiliated with the University of Maryland (Baltimore, MD, USA) .\n\n【2】Informed consent for stool sample collection, initial and follow-up patient interviews, and medical records review was obtained from primarily urban and suburban residents, or parents/guardians for minors, who sought treatment for self-identified primary or secondary diarrhea. This research was approved by the institutional review boards at all participating institutions.\n\n【3】Participants were interviewed at outpatient clinics to assess health status, symptoms, and potential exposures to enteric pathogens, and at follow-up to determine the duration of diarrhea, whether treatment was administered, or whether hospitalization resulted from the initial visit. Stool samples collected during the visit or provided within 48 h and kept cool were homogenized and transferred into multiple vials for storage at –80°C.\n\n【4】An outpatient CDI case was defined as in outpatient with diarrhea whose stool was positive for _C. difficile_ toxins by enzyme immunoassay . Presumptive non–health care–associated (NHA) CDI was defined by the absence of an overnight stay at an inpatient healthcare facility over the previous month.\n\n【5】Traditional risk factors for CDI that were investigated included antimicrobial drug use within the past month, age \\> 65 years, serious underlying illness/weakened immune system, history of bowel or ulcer surgery, colon disease, previous CDI, and recent hospitalization. Statistical analysis was done by using SAS version 9.2 (SAS Institute, Inc. Cary, NC, USA). All p values reported are 2-sided, with no correction for multiple comparisons; p<0.05 was considered significant.\n\n【6】_C. difficile_ toxin–positive stool specimens, in 1-mL aliquots, were shipped frozen to the Centers for Disease Control and Prevention (Atlanta, GA, USA) anaerobe laboratory for culturing by direct inoculation onto cycloserine cefoxitin fructose agar (CCFA) or ethanol shock, followed by CCFA inoculation. Cultures were incubated for 48–72 h at 35°C under anaerobic conditions and examined for characteristic yellow-green fluorescence under long-wave ultraviolet light and CCFA _p_ \\-cresol odor. _C. difficile_ colonies were confirmed with indole (negative) and PRO disk (positive; Remel, Lenexa, KS, USA) tests.\n\n【7】Pulsed-field gel electrophoresis was performed on _C. difficile_ genomic DNA digested with _Sma_ I, and toxinotyping was performed . Binary toxin was assayed by PCR for _cdtB_ . Deletions in _tcdC_ were detected .\n\n【8】_C. difficile_ toxin tests were performed on 1,091/1,197 stool specimens; 43 (3.9%) of these case-patients met the case definition for outpatient CDI. The mean age of these case-patients was 43.7 years (range 4 months–88 years). Outpatient CDI case-patients were younger at Yale because a significantly greater proportion of toxin-positive children were recruited at Yale (45.5%) than at the University of Maryland (15.6%) (p = 0.04). The 43 outpatient CDI case-patients included 5 infants <1 year of age, 5 children 1–18 years of age, 23 adults 19–64 years of age, and 10 adults \\> 65 years of age; 21 were Caucasian, 18 were African American, and 4 were of other or unknown race/ethnicity (22 male and 21 female case-patients).\n\n【9】Most case-patients (36/43, 83.7%) had a recognized underlying risk factor. Twenty-seven (62.8%) had received systemic antimicrobial drugs, including ciprofloxacin, gaitifloxacin, amoxicillin, ampicillin/sulbactam, piperacillin/tazobactam, cefpodoxime, vancomycin, clindamycin, metronidazole, erythromycin, or trimethoprim/sulfamethoxazole within the preceding month; 14 (32.6%) had been hospitalized; and 15 (34.9%) had chronic illnesses or had undergone bowel surgery that potentially affect immune status or gastrointestinal function . Two persons, 1 with AIDS and 1 who underwent a previous bowel resection for diverticulitis, had been treated in the past month for CDI. Only 7 (16.3%) patients had NHA-CDI infections without identified risk factors; 3 were infants (<1 year), 1 was a child (1–18 years), 3 were adults (19–64 years), and none were elderly ( \\> 65 years) .\n\n【10】The 43 outpatient CDI case-patients were compared with the other 1,048 persons in which _C. difficile_ toxin had not been detected. Persons with CDI were, on average, significantly older than others with diarrhea, 43.7 years vs. 29.2 years, respectively (p<0.01). Outpatient CDI case-patients were more likely than _C. difficile_ –negative patients to have medical or surgical conditions (34.9% vs.16.8%, p<0.001), been recently hospitalized (32.6% vs. 8.8%, p<0.001), or to have used antimicrobial drugs (62.8% vs. 22.0%, p<0.001).\n\n【11】Co-infections with other enteric pathogens were common among CDI case-patients, including _C. perfringens_ , rotavirus , norovirus , sapovirus , and 1 each with hookworm, _Bacillus cereus_ , astrovirus, and adenovirus. The likelihood of co-infection was similar in patients with (12/36 \\[33.3%\\]) and without (3/7 \\[42.9%\\]) risk factors (p>0.1).\n\n【12】Of 43 _C. difficile_ toxin–positive stools initially tested, 39 stool samples were submitted to the Centers for Disease Control and Prevention for anaerobic culture and _C. difficile_ was isolated from 31 samples. Binary toxin was identified in 12 (38.7%). Pulsed-field gel electrophoresis identified 15 different types . No associations were found between risk factors, including age, and strain or toxinotype (data not shown).\n\n【13】NHA-CDI has been recognized for >12 years, and recent reports suggest that disease occurs without patient’s known exposure to antimicrobial drugs or other previously identified risk factors . Although we found a proportion of _C. difficile_ –positive diarrheal stools similar to that of 2 other recent prospective studies that used confirmatory culture (i.e. 1.5%–3.9%) for outpatient CDI , we also found a lower proportion of outpatient CDI cases without recognized risk factors of recent hospitalization, chronic medical conditions, recent antimicrobial drug exposure, or co-infection than did those studies.\n\n【14】One limitation of our study was using retrospective self-reporting for assessment of hospitalizations or antimicrobial drug use in the previous month, which potentially can result in recall bias. Also, antimicrobial drug therapy was assessed for only 4 weeks before diarrhea onset; exposure to antimicrobial drugs for a period longer than 1 month before patient seeks treatment may present a risk for CDI. In addition, this study was conducted at 2 urban centers in the eastern United States and may not be generalizable to other locations or clinical settings. Finally, although enzyme immunoassay detection for _C. difficile_ was the standard of care at the time of the study, it is now considered too insensitive to be used as a stand-alone diagnostic test .\n\n【15】In summary, we detected toxigenic _C. difficile_ in a similar proportion of patients to those reported in other studies of CDI. However, all but 3 patients had either known risk factors for CDI or other pathogens potentially responsible for their illness; 1 was <1 year of age. _C. difficile_ isolates responsible for outpatient CDI are genetically diverse. An evolving picture of widespread, frequent CDI among outpatients without risk factors should be tempered by these findings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a3a31d16-184c-4479-8cb2-f37b22275900", "title": "Adaptation of a Cervical Cancer Education Program for African Americans in the Faith-Based Community, Atlanta, Georgia, 2012", "text": "【0】Adaptation of a Cervical Cancer Education Program for African Americans in the Faith-Based Community, Atlanta, Georgia, 2012\nBackground\n----------\n\n【1】An estimated 12,340 cases of invasive cervical cancer are expected to be diagnosed and 4,030 deaths from cervical cancer are expected for 2013 . Cervical cancer is the second most common cancer among women worldwide, and minorities experience disparities in cervical cancer incidence and mortality rates. From 1999 through 2009, African American women in the United States had the second highest incidence rates of cervical cancer, yet they were more likely to die from cervical cancer than women of other races .\n\n【2】Many intervention strategies have been studied to increase cervical cancer screening. _The_ _Guide to Community Preventive Services_ recommends client reminders, small media (eg, printed materials), one-on-one education, provider assessment and feedback, and provider reminders as strategies to increase cervical cancer screening . However, only 3 of 8 evidence-based interventions (EBIs) that focused on African American women were found on the National Cancer Institute’s Research-Tested Intervention Programs website . Therefore, because of limitations in evidence-based options, community organizations may adapt EBIs for a new community or audience. Adaptation is defined as the degree to which an EBI is changed or modified during adoption and implementation to suit the needs of the setting or to improve the fit to local conditions . Several models exist that provide considerations and processes for program adaptation and implementation . In making program adaptations, it is important to address cultural mismatches and follow processes to ensure program changes fit the local populations or conditions .\n\n【3】In response to the high prevalence of cervical cancer and human papilloma virus (HPV) among Latino women, the Spirit Foundation Inc, in partnership with the American Cancer Society, South Atlantic Division, created the _Con Amor Aprendemos_ (CAA) program for Latino couples to increase knowledge about risk and behaviors leading to cervical cancer and HPV-related diseases. This 7-week intervention is implemented by trained _promotoras_ , or community health workers, in faith-based and community organizations. Sessions last an average of 2 hours, and each session follows a specified curriculum covering topics on anatomy, sexually transmitted infections (STIs), cervical cancer and HPV, dialogues, role playing, presentation of skits to community or church members, and education about the HPV vaccine. An extensive 2-day train-the-trainer course is a vital component of the CAA program to train a team of community members in health ministries on how to deliver the curriculum to the faith-based community. Couples are targeted to participate in the program in efforts to reduce the anxiety of the male–female partner dialogue regarding HPV and risks associated with contracting cervical cancer. The program uses innovative tools, such as a “ring of knowledge” — where program participants collect small notecards with information throughout the course of the program — games to build awareness about STIs, anatomy labeling, and a “parking lot” for participants to ask personal questions anonymously.\n\n【4】CAA has been implemented in Latino communities in Georgia, Nicaragua, El Salvador, and Bolivia. This intervention has been piloted in El Salvador with preliminary results demonstrating increased knowledge about HPV and Cervical Cancer . This article documents the process of adapting CAA for the African American community and highlights the data collection methods, adaptation of program information, and the roles of health ministry leaders to make suggestions about program modifications for a new population. The adapted program is called _With Love We Learn_ (WLWL).\n\n【5】Community Context\n-----------------\n\n【6】After successful implementation in multiple Latino communities, the Spirit Foundation Inc. considered it essential to replicate the program in the African American community in light of the high incidence of cervical cancer among African American women. Without appropriate education reinforcing the importance of screening and follow-up, the rates of cervical cancer will continue to rise among African American women. As it was in the Latino community, a faith-based setting was chosen for adaptation of CAA in the African American community because of the strong historical ties the African American community has with the church. Faith-based organizations have become one of the most common vehicles for the dissemination of prevention efforts and addressing health concerns for that population .\n\n【7】In October 2011, the Spirit Foundation partnered with 2 churches in the metro Atlanta area that would adopt WLWL in their health ministries. These churches have predominantly African American populations with congregations of 4,000 members or more . Health ministry leaders from these churches were invited to participate in the community assessment and adaptation discussions.\n\n【8】Methods\n-------\n\n【9】In 2011, the National Cancer Institute piloted the Research to Reality (R2R) mentorship program as a capacity-building initiative for public health practitioners to gain hands-on experience in evidence-based programs and decision-making practices. Mentees were paired with seasoned cancer control practitioners and worked on a year-long cancer control and prevention project to learn and apply new skills in evidence-based public health practices. The project for the Georgia R2R pair focused on the adaptation of CAA to WLWL. Orientation meetings were held with program developers and staff members to learn the details of the program, and worked for several months to gather information that would inform the key phases at which data would be gathered in the adaptation process. The adaptation summary form was the primary tool used among project staff to document the changes at each phase . The program was adapted in 4 phases: 1) review of the CAA curriculum, 2) a focus group discussion to determine changes for the WLWL curriculum, 3) train-the-trainer sessions on program delivery, and 4) a pilot intervention and follow-up focus group to evaluate the new curriculum.\n\n【10】Six health ministry leaders from the 2 partner churches reviewed the CAA program manual for general understanding of the overall curriculum. Reviews were conducted over a 2-week period, and leaders were provided with a deadline by the program developers to provide their initial feedback. Detailed notes were written throughout the manual on specific areas suggested for revision for the African American audience.\n\n【11】Following the manual review, 6 health ministry leaders met at one of the partner churches for a 2-hour focus group. During the focus group, leaders and program developers discussed the detailed components of the curriculum and explained how each session was to be conducted. Participants talked through the notes written throughout the manual on sections that needed revisions. A group of stakeholders from academia, government, and community collaborated on developing the focus group guide to gather detailed information about key changes to the curriculum. There were specific inquiries were made about changes to the core content and pedagogical and implementation components . The guide had 4 major sections that addressed the overall program and delivery, program incentives, program materials, and technical assistance in delivering the program . The focus group discussion was audio taped and transcribed verbatim. NVivo 10 software (QSR International, Burlington, Massachusetts) was used for data storage, retrieval, and analysis. A content analysis was performed to identify the range of responses and major themes related to revisions to the WLWL manual  . A summary of the manual changes by session was also recorded on the adaptation summary form \n\n【12】After incorporation of the recommended changes, the WLWL manual was revised and a 2-day training workshop was conducted to train the 6 health ministry leaders on the revised curriculum. The objective of the train-the-trainer program for CAA/WLWL is to establish community educators who are motivated to address the community with accurate knowledge of HPV and cervical cancer. The training sessions were conducted on location to provide a familiar environment for the trainers. Refresher sessions were provided closer to program implementation to ensure that trainers remained comfortable teaching information accurately and did not deviate from the set topics of the curriculum.\n\n【13】The first pilot of WLWL in the churches began in May 2013 with 15 (13 female and 2 male) participants. Participants were recruited through church announcements, fliers, and word of mouth from the health ministry leaders. Not all of the participants were couples, however all 15 participants completed the adapted program in June 2013. On completion, participants and trainers were invited to participate in a focus group discussion to gather their feedback. Participants were asked their opinions about the marketing of the program to their congregation, their comfort level with questionnaires that asked about sensitive and personal information, receptivity to the session topics and components, and their recommendations for recruiting participants in future programs. Data gathered from the pilot group of WLWL participants served as the final data collection point to include on the adaptation summary form for the last manual revisions.\n\n【14】The overall program curriculum and the focus group guide was submitted under the Emory University Social and Behavioral Institutional Review Board and received exemption from full review. Trainers were compensated for their time for the manual review, focus group discussion, and train-the-trainer session. and participants in the pilot program were compensated for their feedback during the focus group. The R2R mentorship pair, program developers, and WLWL project staff followed adaptation guidance from the literature  to summarize all of the suggestions provided for tailoring, ensuring that the recommended changes did not affect the fidelity to the original core elements or required elements of the program .\n\n【15】Outcomes\n--------\n\n【16】Overall, the program was well received by church leaders, trainers, and participants of the pilot intervention. Each phase of the adaptation process included valuable recommendations for the WLWL curriculum . Suggestions for content tailoring included changes to the cover, games related to session topics, pictures to reflect African Americans, and more effective dialogue to deliver the content of the sessions to an African American population. The modification of relevant health statistics addressing screening rates and cervical cancer mortality for African American women was also critical to effectively communicate risk data to participants. Other modifications included recommendations for stratifying the program by age groups to promote easier conversations with peers closer in age. The frequency of the sessions was condensed to twice a week for 3 weeks to retain members’ participation throughout the program . Core elements of the program, including the session topics and the use of games, pictures, posters, and dialogues for learning the material, were not changed. Although these materials were welcomed and reported to be helpful in learning, suggestions were given to make them relevant to both younger and older African American audiences in the community. The program developer, a gynecologist, and other program staff reviewed the comments and modified the pilot WLWL curriculum.\n\n【17】Interpretation\n--------------\n\n【18】Program adaptation is an important process in the translation of evidence-based programs into practice. A systematic process for adaptation is necessary to learn what potential changes can be made to a program to ensure its suitability for another population and also balance fidelity to the original program. Community engagement is equally important for this adaptation process to garner insights from key constituents and understand contextual issues in matching interventions to particular communities. Castro and colleagues summarize key processes in program adaptation including assessing your audience, selecting an evidence-based approach, preparing for adaptation with focus groups or topical expert review, adapting the program, testing adaptation materials, and refining adaptation . We followed the guidance for adaptation preparation with multiple data collection methods to learn about the target audience (needs assessment) and assist in making decisions on what program components to change or refine.\n\n【19】One of the most important considerations for adapting community programs is to understand both the intervention and the components required for program fidelity to remain uncompromised. Additionally, it is equally important to take the time to foster the right partnerships with faith-based institutions and involve the right stakeholders’ perspectives to ensure the programs’ success. Substantial organization and investment of time are needed to collect data at multiple points. Working closely with the program developers and staff on organizing the data collection throughout the adaptation phases was extremely helpful in keeping the project on track and without losing the fidelity of the original program. Having a resource like the adaptation summary form referenced throughout this case study, is highly recommended to keep records of the changes in a multiphase process.\n\n【20】The data collection and subsequent adaptation of the CAA program involved a variety of stakeholders from program developers to key members of the faith-based organizations to identify areas of original program mismatches (eg, race, community context) to the African American population. The multiple types of data collection generated many recommendations for program changes to increase the fit of the cervical cancer prevention program to African American church members. Many of our program changes fit into the area of context modification (eg, setting, population) and content modification (eg, tailoring, substituting elements) found in a recent review of adaptation of evidence-based interventions .\n\n【21】We involved the program developers in this adaptation process in order to speak to the dynamics of the program, the role of the faith-based organization that would be implementing it, and to make informed decisions on what to change in the program. This strategy has been suggested as a way to ensure that the modifications are justifiable and that changes to the content, duration, or delivery style of the program will not diminish the program’s effectiveness . Linking program developers to community organizations that are adopting evidence-based programs can increase translations of EBIs in that developers can address issues related to program implementation and adaptation . Decisions were made by the developers to change certain CAA content and implementation recommendations if they were not deemed to be “red light” adaptations (eg, theory change, dose, elimination of core elements) that would potentially jeopardize program fidelity .\n\n【22】Dissemination of evidence-based practices is increasing in the community. Many community organizations are attempting to adapt packaged programs for their own populations and settings. This project has illustrated a method for sequential data collection to inform the adaptation process. Future research should further explore how community-based participatory processes with key community members and organizations can inform program adaptations of evidence-based interventions and testing of adapted programs in communities for effectiveness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d57097e-a556-447d-ad3a-647993269f8d", "title": "Deaths Attributable to High Body Mass in Brazil", "text": "【0】Deaths Attributable to High Body Mass in Brazil\nAbstract\n--------\n\n【1】Our study estimated the proportion of deaths from major noncommunicable diseases (NCDs) that could be prevented in Brazil by reducing population-wide body mass index (BMI) to different counterfactual (optimally theoretical) scenarios. We calculated population-attributable fractions by using BMI data from a representative national survey and relative risks from a published meta-analysis. Reductions in population-wide BMI could prevent 30,715 to 168,431 deaths from NCDs per year in Brazil. Cardiovascular diseases were the most preventable causes of death (5.8%–31.5% deaths prevented). Policies are needed to reduce population-wide BMI in Brazil.\n\n【2】Objective\n---------\n\n【3】The objective of this study was to estimate preventable deaths from major noncommunicable diseases (NCDs) in Brazil by reducing the population-wide body mass index (BMI) (26.6 kg/m 2  )  to the following counterfactual scenarios: 1) theoretical minimum risk exposure level, where adults have a BMI of 22.0 kg/m 2  ; 2) reduction in 1.0 kg/m 2  at population level; and 3) reduction of BMI to levels observed in the Brazilian population in 2002 and 2003 (24.6 kg/m 2  ) to show the effect of the continuous increase in BMI over time .\n\n【4】Methods\n-------\n\n【5】We obtained BMI data from the National Health Survey, Pesquisa Nacional de Saúde (PNS), conducted in 2013 . PNS is the most recent survey using a representative sample of Brazilian adults. Participants were randomly selected in 3 stages: census tracts (primary sample units), households (secondary sample units), and household members aged 18 years or older (tertiary sample units). A total of 62,202 adults were interviewed in the final sample. Both body weight (in kg) and height (in cm) were objectively measured . We estimated BMI distribution (mean and standard deviation \\[SD\\], prevalence, and 95% confidence intervals \\[CIs\\] of overweight \\[25.0–29.9 kg/m 2  \\] and obesity \\[≥30 kg/m 2  \\]) by sex.\n\n【6】We obtained relative risk (RR) estimates and 95% CIs from the Global BMI Mortality Collaboration meta-analyses . RRs were estimated from never-smokers who had no preexisting chronic diseases and excluded the first 5 years of follow-up to reduce confounding and reverse causality . We retrieved RR estimates per 5 units of BMI for all-cause, cardiovascular disease, respiratory disease, and cancer mortality.\n\n【7】We retrieved number of deaths from cardiovascular disease ( _International Classification of Diseases, Tenth Revision_ \\[ICD-10\\] codes I00-I99 and R96) , respiratory disease (ICD-10 codes J00-J99), and cancer (ICD-10 codes C00-C97 and D00-D48) in Brazil in 2013 by sex and age group from the Brazilian Information System for Mortality .\n\n【8】We calculated population attributable fractions (PAF) by sex by using the following equation :\n\n【9】Where _P(x)_ is the population distribution of BMI (mean and SD), _P_ \\* _(x)_ is the counterfactual distribution of BMI, _RR(x)_ is the relative risk of NCD associated with BMI (per 1.0 kg/m 2  increment), and _dx_ indicates that the integration occurred with respect to the BMI level. We used a log-logit function to represent each RR value across BMI units . We performed data analysis in Stata version 15.0 (StataCorp, LLC).\n\n【10】Results\n-------\n\n【11】Overall mean BMI in Brazil increased from 24.6 kg/m 2  in 2002 and 2003 to 26.6 kg/m 2  in 2013. The mean BMI was 27.0 kg/m 2  (SD, 5.5 kg/m 2  ) in women and 26.2 kg/m 2  (SD, 4.5 kg/m 2  ) in men. Approximately 25% of women were obese, and 35% were overweight. The prevalence of obesity in men was 17%, and 40% were overweight.\n\n【12】We estimated that reducing population-wide BMI to a theoretical minimum risk exposure level (22.0 kg/m 2  ) could prevent approximately 168,431 deaths per year in Brazil. These deaths represented about 25.3% of major NCD deaths and 14.9% of all deaths that occurred in 2013. Most of these preventable deaths were from cardiovascular disease , followed by respiratory disease  and cancer  .\n\n【13】Reducing population-wide BMI in Brazil to levels observed during 2002 and 2003 (24.6 kg/m 2  ) could prevent 65,721 deaths, representing 10.0% of deaths from major NCDs and 5.8% of all deaths. A reduction in 1.0 kg/m 2  of population-wide BMI could prevent 30,715 deaths, representing 4.6% of deaths from major NCD and 2.7% of all deaths .\n\n【14】Discussion\n----------\n\n【15】Approximately 25.3% of major NCD deaths and 14.9% of all deaths could be prevented each year in Brazil by reducing population-wide BMI. Other scenarios indicated that 4.6% of major NCD deaths could be avoided by reducing 1.0 kg/m 2  of BMI and 10% of NCD deaths by reducing BMI to levels observed during 2002 and 2003.\n\n【16】The reduction of BMI would have the greatest effect on cardiovascular disease deaths, which account for one-third of all deaths in Brazil . The World Health Organization Global Plan for 2025 involves a series of targets to reduce 25% of premature mortality from major NCDs , among which is halting the rise in obesity rates. Our study considered more ambitious scenarios of BMI reduction, which can be a challenge. Obesity increase is primarily driven by obesogenic environments. To reverse this trend, some in the scientific community, especially in the fields of nutrition and physical activity, have suggested modifying obesogenic environments through fiscal and regulatory actions (eg, taxation, labeling, marketing of ultraprocessed products) .\n\n【17】Our study has limitations. Although BMI is a useful indicator of body fat, it does not differentiate between lean and adipose tissues . Furthermore, we used RR estimates from a meta-analysis that included data from 4 continents but not Brazil . These RR estimates were not stratified by potential effect modifiers (eg, sex, age). Whether these RR estimates are applicable to Brazilians is unknown. On the other hand, by using RR estimates for never-smokers who had no preexisting chronic diseases and excluding the first 5 years of follow-up, we reduced reverse causation bias and achieved more reliable estimates of deaths attributable to BMI .\n\n【18】By reducing population-wide BMI in Brazil, 30,715 to 168,431 deaths per year from NCDs could be prevented. Policies aimed to reduce obesogenic environments are needed to decrease population-wide BMI in Brazil.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bc459f48-bd3b-4629-91a0-cb13a3713c5e", "title": "Methicillin-resistant Staphylococcus aureus in Taiwan", "text": "【0】Methicillin-resistant Staphylococcus aureus in Taiwan\nAlthough most methicillin-resistant _Staphylococcus aureus_ (MRSA) illness and death are associated with healthcare facilities (H-MRSA), isolates from community-associated MRSA (C-MRSA) infections have been obtained with increasing frequency in the last few years in different countries, including Taiwan . The changing epidemiology of MRSA has become an important public health concern worldwide . MRSA arises when _S. aureus_ organisms acquire a large mobile genetic element called staphylococcal cassette chromosome _mec_ (SCC _mec_ ) . Most H-MRSA strains possess either SCC _mec_ II or III; most C-MRSA strains possess SCC _mec_ IV . Recently, a novel type V SCC _mec_ type was characterized and found in a C-MRSA isolate from Australia . With the exception of variable resistance to erythromycin, C-MRSA strains are generally susceptible to other non-β-lactam antimicrobial agents, in contrast to most H-MRSA, which are typically resistant to many of the non-β-lactam agents . Another characteristic of C-MRSA is the production of Panton-Valentine leukocidin (PVL), an extracelluar cytotoxin involved in primary skin infections and pneumonia .\n\n【1】We conducted a study to characterize the molecular epidemiology of selected MRSA isolates from the Taiwan Surveillance of Antimicrobial Resistance (TSAR), a national surveillance program of inpatient and outpatient clinical isolates in Taiwan . We describe the finding of a virulent closely related clone of MRSA and its prevalence in Taiwan.\n\n【2】### The Study\n\n【3】A total of 398 and 865 nonduplicate _S. aureus_ isolates were collected from March to May 2000 from 21 hospitals and from July to September 2002 from 26 hospitals, respectively, as part of the TSAR collection . The proportions of isolates for the 2 years were similar for outpatients (27.5% in 2000 and 28.9% in 2002); the rest of the isolates were from inpatients. The most common specimen type was wound, which accounted for 35.4% and 49.2% of all _S. aureus_ isolates in 2000 and 2002, respectively. Antimicrobial susceptibility was determined on the basis of results of MICs obtained from a broth microdilution method, following the guidelines of the National Committee for Clinical Laboratory Standards  by using custom-designed Sensititre plates (Trek Diagnostics, East Essex, United Kingdom). Overall, MRSA accounted for 238 (59.8%) of 398 _S. aureus_ isolates in 2000 and 475 (54.9%) of 865 _S. aureus_ isolates in 2002.\n\n【4】To obtain an overall understanding of MRSA throughout Taiwan, we first chose 80 MRSA isolates (68 inpatient and 12 outpatient isolates) collected in 2002 from 4 hospitals located in the north, middle, south, and east regions of Taiwan. Pulsed-field gel electrophoresis was performed according to a published protocol , and pulsotypes were assigned to clusters of isolates with >80% similarity from the dendrograms. SCC _mec_ typing and PVL gene detection were performed according to published protocols . Multilocus sequence typing (MLST) was performed on randomly selected strains from major pulsotypes, and the sequence type (ST) was assigned by using the MLST database  . Three major clusters (pulsotypes) were found, including 47 (58.8%) pulsotype A, 7 (8.8%) pulsotype B, and 18 (22.5%) pulsotype C . All 47 pulsotype A isolates had SCC _mec_ III; 4 isolates tested by MLST had ST239. In addition to being resistant to clindamycin (94%), erythromycin (100%), and tetracycline (100%), all pulsotype A isolates were resistant to ciprofloxacin (CIP), gentamicin (GEN), and trimethoprim/sulfamethoxazole (SXT). The 7 pulsotype B isolates possessed SCC _mec_ IV but were not of the 4 known IV subtypes (IV not a–d); all were CIP- and SXT-susceptible but GEN-resistant. Seventeen of the 18 isolates in pulsotype C possessed SCC _mec_ V; the other had SCC _mec_ IVa; all 18 were CIP/GEN/SXT-susceptible. Of the 10 isolates tested by MLST from pulsotype B (3 isolates) and pulsotype C (7 isolates), all had ST59. Only pulsotype C isolates were PVL-positive.\n\n【5】Because we found a large percentage (21.3%, 17/80) of SCC _mec_ V:ST59, PVL-positive clones, we selected an additional 69 CIP/GEN/SXT-susceptible MRSA isolates (25 from 2000, 44 from 2002) to determine what portion of them had the same clone. These isolates were from intensive care unit (ICU) and non-ICU inpatients, plus outpatients from 15 hospitals in 2000 and 21 hospitals in 2002 (excluding the 4 hospitals already tested). Fifteen (60%) of the 25 isolates from 2000 and 32 (72.7%) of the 44 isolates from 2002 belonged to pulsotype C, had SCC _mec_ V, and were PVL-positive. Of the 4 isolates characterized by MLST, 2 were ST59, 1 was ST388, and 1 was a new ST; the last 2 isolates differed from ST59 by 1 nt each in _gmk_ and _arcC_ genes, respectively.\n\n【6】When the 2 groups of isolates were combined, a total of 90 CIP/GEN/SXT-susceptible MRSA were studied . These isolates were mostly resistant to clindamycin (89%), erythromycin (92%), and tetracycline (82%). Of these 90 isolates, 68 (75.6%) were PVL-positive and belonged to same pulsotype C; 64 (71.1%) had SCC _mec_ V. MLST performed on 12 isolates found 10 to be ST59; the other 2 (ST338 and the new ST) are closely related to ST59. Forty-eight of these 64 isolates were from wounds (75%), but isolates were also found in respiratory (6 isolates), ear (3 isolates), blood (2 isolates), urine (2 isolates), and catheter and other body site (3 isolates) specimens. Only half (32 isolates) were from outpatients; the rest were from ICU (10 isolates) and non-ICU (22 isolates) inpatients. Two ICU isolates were from hospital-acquired infections. Because 189 (26.5%) of the 713 MRSA isolates from the 2000 and 2002 collections were CIP/GEN/SXT-susceptible, and because 64 (71.1%) of the 90 CIP/GEN/SXT-susceptible MRSA we studied were PVL-positive, SCC _mec_ V:ST59, and belonged to pulsotype C, an estimated 18.8% (26.5% × 0.711 = 18.8%) of MRSA isolates in Taiwan could be this closely related virulent clone.\n\n【7】The reason for the high prevalence of this virulent clone (pulsotype C:ST59, SCC _mec_ V, PVL-positive) of MRSA in Taiwan in both inpatients and outpatients is not known. Data on the prevalence of SCC _mec_ V are still limited, and ST59 has been described infrequently. A recent longitudinal study of MRSA isolates in the San Francisco area found that the ST59-SCC _mec_ IV has increased steadily from 1999 to become 1 of the 4 major clones associated with C-MRSA .\n\n【8】Production of the PVL cytotoxin is considered a genetic marker for C-MRSA, and although PVL-positive MRSA have usually been associated with skin and soft tissue infections, severe and fatal infections, such as necrotizing pneumonia, have been reported . Thus, PVL may confer an additional virulence advantage for this particular clone of MRSA in Taiwan. Other possible explanations are that the less resistant C-MRSA can grow faster than multidrug-resistant H-MRSA and that SCC _mec_ IV and V carried by C-MRSA may have the advantage over SCC _mec_ I–III carried by H-MRSA because they are smaller and more transferable; both of these putative advantages may contribute to their propagation .\n\n【9】The close relatedness and high prevalence of this virulent pathogen argue for a clonal expansion advantage of this particular clone. Outbreaks of C-MRSA infections caused by SCC _mec_ IV (IVa) have been reported in several countries . Since our genotyping results showed that MRSA isolates possessing SCC _mec_ V and PVL in Taiwan are clonally related, we cannot rule out the possibility of outbreaks due to this particular clone in some areas. However, our isolates came from multiple hospitals throughout the 4 geographic regions of Taiwan. Our data also showed that this particular clone was already present in 2000. In addition, this particular clone was found not only in outpatients but also in ICU and non-ICU inpatients, including in hospital-acquired infections. These findings indicate that this clone has migrated into the hospital environment; moreover, it can cause more severe infections, as shown by its presence in blood, respiratory, ear, and other specimens.\n\n【10】### Conclusions\n\n【11】Our analysis of MRSA isolates collected in 2000 and 2002 indicated that a virulent clone of MRSA (pulsotype C:ST59, SCC _mec_ V and PVL-positive), which caused wound infections primarily but also other potentially more serious infections, is highly prevalent in Taiwan inpatient and outpatient settings. Recognition of this clone can be facilitated by its antimicrobial susceptibility profile. Because the resistance pattern of these isolates differs from that of traditional H-MRSA strains, the antimicrobial susceptibility profile has important implications for treatment. Understanding the roles these strains play in MRSA epidemiology helps physicians choose the most appropriate treatment. Prompt and judicious management and infection control measures should help deter further spread of this virulent pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "45ea29da-4ace-4c5f-84af-3db8dd87e02b", "title": "Epidemiology of Escherichia coli O157:H7 Outbreaks, United States, 1982–2002", "text": "【0】Epidemiology of Escherichia coli O157:H7 Outbreaks, United States, 1982–2002\n_Escherichia coli_ O157:H7 was first recognized as a pathogen in 1982 during an outbreak investigation of hemorrhagic colitis . _E. coli_ O157 infection can lead to hemolytic uremic syndrome (HUS), characterized by hemolytic anemia, thrombocytopenia, and renal injury . Still, it was not until 1993, after a large multistate _E. coli_ O157 outbreak linked to undercooked ground beef patties sold from a fast-food restaurant chain , that _E. coli_ O157 became broadly recognized as an important and threatening pathogen. Clinical laboratories began examining more stool specimens for _E. coli_ O157 . In 1994, _E. coli_ O157 became a nationally notifiable infection, and by 2000, reporting was mandatory in 48 states. An estimated 73,480 illnesses due to _E. coli_ O157 infection occur each year in the United States, leading to an estimated 2,168 hospitalizations and 61 deaths annually , and it is an important cause of acute renal failure in children .\n\n【1】Although reported outbreaks account for only a minority of _E. coli_ O157 cases, outbreak investigations contribute greatly to understanding _E. coli_ O157 epidemiology by identifying transmission routes, vehicles, and mechanisms of contamination . Outbreak findings oblige regulatory and public health agencies and industry to evaluate prevention and control measures so similar outbreaks can be prevented. Knowledge of transmission routes and vehicles allows consumers to be educated on reducing risky behavior that can decrease their risk for infection. We report here surveillance results for _E. coli_ O157 outbreaks reported to the Centers for Disease Control and Prevention (CDC) from 1982 through 2002, to highlight the epidemiology of this emerging pathogen.\n\n【2】### Methods\n\n【3】Outbreaks of _E. coli_ O157:H7 and Shiga toxin–producing _E. coli_ O157:NM (subsequently referred to as _E. coli_ O157) investigated by state and local health departments were reported to CDC by telephone, outbreak report, or through the routine foodborne disease outbreak surveillance system . In preparation for this summary, an epidemiologist reviewed all reports including published outbreaks not otherwise reported. Information collected from each outbreak report included city, setting, and suspected transmission route and vehicle. The date of first illness, hospitalizations, number of ill persons, bloody diarrhea, culture-confirmed illness, HUS, and deaths were also obtained. We defined an outbreak as \\> 2 cases of _E. coli_ O157 infection (at least 1 culture-confirmed) with a common epidemiologic exposure. For purposes of defining an outbreak, we considered a case as a stool culture yielding _E. coli_ O157, or bloody diarrhea, or HUS. Each investigator reported the total number of outbreak-related cases, often including those with compatible clinical illness but without culture confirmation of illness. Infections acquired outside the United States were not included.\n\n【4】We defined outbreak onset as month and year first illness onset was reported, and outbreak setting as place where exposure occurred. Outbreaks due to a distributed food item and not isolated to a single venue or event were classified as communitywide. Fast-food settings were defined as establishments where payment is made before receiving food. Outbreaks were classified into 1 of 6 transmission routes on the basis of how most patients acquired the infection (foodborne, person-to-person, recreational water, drinking water, animal exposure, or laboratory). Outbreaks with a common exposure but in which a major transmission route was not identified were classified as unknown transmission route. Median outbreak sizes were compared by using the Kruskal-Wallis test. Outbreak-related HUS and death rates were compared by using a chi-square test.\n\n【5】Foodborne outbreaks were defined as the occurrence of \\> 2 cases of _E. coli_ O157 infection resulting from ingestion of a common food, or if food vehicle was undetermined, sharing a common meal or food facility. Food vehicles were grouped into the following categories: ground beef, other beef, produce, dairy, other, or unknown. Food vehicles were implicated statistically in case-control studies (p < 0.05), by isolation of _E. coli_ O157 from a suspect item, or by being the only common food item consumed by cases. A multistate outbreak was defined as exposure to a common vehicle occurring in >1 state. HUS cases were classified by individual investigators and included those cases diagnosed as thrombotic thromobocytopenic purpura following _E. coli_ O157 infection.\n\n【6】### Results\n\n【7】From 1982 to 2002, a total of 350 outbreaks were reported from 49 states, accounting for 8,598 cases of _E. coli_ O157 infection. Among cases, there were 1,493 (17.4%) hospitalizations, 354 (4.1%) cases of HUS, and 40 (0.5%) deaths. The number of reported outbreaks began rising in 1993, and peaked in 2000 with 46 . Outbreak size ranged from 2 to 781 cases, with a median of 8 cases. Median outbreak size appears to have declined from 1982 to 2002 . Most outbreaks (89%) occurred from May to November. Of the 326 outbreaks reported from a single state, Minnesota reported the most (43 outbreaks), followed by Washington (27 outbreaks), New York (22 outbreaks), California (18 outbreaks), and Oregon (18 outbreaks). Among the 350 outbreaks, transmission routes for 183 (52%) were foodborne, 74 (21%) unknown, 50 (14%) person-to-person, 21 (6%) recreational water, 11 (3%) animal contact, 10 (3%) drinking water, and 1 (0.3%) laboratory-related transmission route .\n\n【8】##### Foodborne Outbreaks\n\n【9】Food remained the predominant transmission route from 1982 to 2002 , accounting for 52% of 350 outbreaks and 61% of 8,598 outbreak-related cases. Foodborne outbreaks most frequently occurred in communities (53 \\[29%\\] of 183), restaurants/food facilities (51 \\[28%\\]), and schools (16 \\[9%\\]). Median size of foodborne outbreaks varied by setting: the smallest occurred in individual residences (3 cases), and the largest outbreaks in residential facilities (44 cases), followed by camps (36 cases). Among 51 restaurant and food facility outbreaks, 22 were from chain establishments (including 12 fast-food establishments) and 29 from single establishments. The median number of cases per restaurant/food facility outbreak was larger in chain than single establishments (21 vs. 8, p < 0.001). Among the 183 foodborne outbreaks, the food vehicle for 75 (41%) was ground beef, 42 (23%) unknown, 38 (21%) produce, 11 (6%) other beef, 10 (5%) other foods, and 7 (4%) dairy products.\n\n【10】##### Ground Beef\n\n【11】The first _E. coli_ O157 outbreak was reported in 1982 and linked to ground beef, which remains the most common vehicle among foodborne outbreaks (75 \\[41%\\] of 183) , although it accounts for only 33% of 5,269 foodborne-related cases. Outbreaks involving ground beef peaked in summer months: 71% occurred from May to August. Of the 40 outbreaks for which ground beef preparation style was reported, 27 (68%) were linked to hamburgers and 5 (13%) to meat sauce. Ground beef outbreaks occurred most frequently at the communitywide level (36 of 75 \\[48%\\]), followed by 11 (15%) at picnics or camps, 8 (11%) at individual residences, 7 (9%) at restaurants, and 4 (5%) at schools. Of the 7 ground beef–associated restaurant outbreaks, 5 occurred in fast-food restaurants in 1982 (2 outbreaks), 1992–1993 (1 outbreak), 1995 (1 outbreak), and 1999 (1 outbreak). The last hamburger-associated fast-food restaurant outbreak was reported in 1995.\n\n【12】##### Produce\n\n【13】Produce-associated outbreaks were first reported in 1991 and have remained a prominent food vehicle , accounting for 38 (21%) of 183 foodborne outbreaks and 34% of 5,269 foodborne outbreak-related cases. Produce outbreaks peaked in summer and fall; 74% occurred from July to October. Thirteen (34%) produce outbreaks were from lettuce, 7 (18%) apple cider or apple juice, 6 (16%) salad, 4 (11%) coleslaw, 4 (11%) melons, 3 (8%) sprouts, and 1 (3%) grapes. Produce outbreaks most commonly occurred in restaurants (15 \\[39%\\]), and 7 (47%) of these were reported to be due to cross-contamination during food preparation. Twenty (53%) produce outbreaks did not involve kitchen-level cross-contamination, including the 7 apple cider or apple juice outbreaks, 7 of 10 lettuce outbreaks, 3 of 4 coleslaw outbreaks, and the 3 alfalfa or clover sprout outbreaks. None were reported to be due to imported produce. The median number of cases in produce outbreaks was significantly larger than that of ground beef outbreaks, 20 vs. 8, (p < 0.001).\n\n【14】##### Other Beef\n\n【15】Types of beef other than ground beef were implicated in 11 outbreaks. Five outbreaks were due to roast beef, 2 to steak, 1 to sirloin tips, and 1 to salami. The other 2 outbreaks included on identified only as “beef” and one as “raw roast beef.”\n\n【16】##### Dairy Products\n\n【17】Seven outbreaks were due to dairy products, including 4 from consuming raw milk. The others were due to cheese curds made from raw milk, from butter made from raw milk, and from commercial ice cream bars (possibly due to cross-contamination).\n\n【18】##### Person-to-Person Outbreaks\n\n【19】Fifty outbreaks were due to spread from fecal matter of an ill person to the mouths of others. Outbreak settings included 40 (80%) child daycare centers; 5 (10%) individual residences; 3 (6%) communities, 1 (2%) school, and 1 (2%) residential facility. Outbreak size ranged from 2 to 63 cases (median 7). Person-to-person outbreaks peaked during summer; 70% occurred from June to August.\n\n【20】##### Waterborne Outbreaks\n\n【21】Thirty-one outbreaks were waterborne: 21 from recreational water and 10 from drinking water. Recreational water outbreaks were first reported in 1991; 14 (67%) occurred in lakes or ponds, and 7 (33%) in swimming pools. Size ranged from 2 to 45 cases (median 8 cases). Outbreaks occurred from June to September.\n\n【22】Drinking water outbreaks tended to be much larger than all other outbreaks, with a median size of 26 vs. 8 cases, (p = 0.08) and occurred from May to December. Drinking water outbreaks accounted for 3% of all outbreaks, but 15% of all outbreak-related cases. Four of the outbreaks were attributed to local well water systems, 3 involved municipal water supply systems, and 1 each was due to spring water, residential faucet water, and ice thought to be cross-contaminated. Two of the 3 municipal water suppliers did not use chlorination, and the other had a malfunctioning chlorinator.\n\n【23】##### Animal Contact Outbreaks\n\n【24】First reported in the United States in 1996, outbreaks due to animal contact are 1 of the newest recognized transmission routes. Direct or indirect cow or calf exposure was noted in all 11 outbreaks: 5 on farms, 2 at county fairs, 2 at petting zoos, 1 at a barn dance, and 1 at a camp. Nine of the outbreaks occurred from July to November. Outbreak size ranged from 2 to 111 cases and accounted for 4% of the 8,598 outbreak-related cases.\n\n【25】##### Laboratory-related Outbreak\n\n【26】One outbreak was reported in 2002 from a laboratory. It involved 2 culture-confirmed cases. Two technicians were infected while validating an _E. coli_ O157 sterilization technique.\n\n【27】##### Outbreaks with Unknown Transmission Route\n\n【28】Outbreaks reported as unknown transmission route accounted for 21% of outbreaks and 9% of all outbreak-related cases. Most (92%) occurred from May to November. Median size was 4 cases (range 2–140).\n\n【29】##### Multistate Outbreaks\n\n【30】Twenty-four multistate _E. coli_ O157 outbreaks were reported since 1992; they ranged from 1 to 3 per year, except in 1999, when 6 were reported. The number of states involved ranged from 2 to 8 with a median of 3. All were due to foodborne transmission. Sixteen (67%) were from ground beef and 6 (25%) from produce.\n\n【31】##### HUS Cases\n\n【32】Among 346 outbreaks that reported HUS cases, 132 (38%) reported at least 1 case of HUS (range 1–55 cases, median 2 cases), for a total of 354 HUS cases. The HUS rate (number of cases per 100 outbreak-related illnesses) was 4.1. From 1982 to 2002, the HUS rate appeared to decline overall . The HUS rate differed significantly by transmission route (p < 0.001) and was highest among swimming outbreaks (10.7), followed by person-to-person (6.8), unknown (6.7), animal contact (5.6), foodborne (3.5), and drinking water (2.1) related–outbreaks. Among foodborne outbreaks, the HUS case rate was significantly higher among ground beef–associated outbreaks compared with all other foodborne outbreaks (5.5 vs. 2.5, p < 0.001).\n\n【33】##### Deaths\n\n【34】Among 325 outbreaks that reported number of deaths, 25 (8%) reported at least 1 (range 1–4), for a total of 40 deaths. Twenty-five (63%) deaths were in persons with HUS; 15 (38%) were due to other causes. Among 12 outbreaks reporting age at death, age ranges were 1–4 years and 61–91 years. Case-fatality rate (number of deaths per 100 outbreak-related illnesses) was 0.5 and appeared to decrease from 1982 to 2002 . The case-fatality rate did not vary significantly by transmission route; however, the rate was significantly higher among outbreaks in residential facilities than in other settings (6.6 vs. 0.4, p < 0.001). Residential facilities where deaths occurred included a nursing home, a custodial institution, and an acute-care facility.\n\n【35】### Discussion\n\n【36】From 1982 to 2002, a total of 350 _E. coli_ O157 outbreaks were reported in the United States from 49 states. Despite regulatory efforts to improve the safety of the U.S. food supply, foodborne _E. coli_ O157 outbreaks remain common. Ground beef remains the most frequently identified vehicle, and produce-associated outbreaks are commonly reported. In addition, nonfoodborne transmission routes remain prominent. Person-to-person outbreaks occur most frequently in child daycare centers. Waterborne outbreaks caused by both drinking and recreational water continue to be reported, and outbreaks due to animal contact are increasingly reported.\n\n【37】In January 1993, the largest _E. coli_ O157 outbreak from ground beef was reported in 4 western states, involving >700 ill persons, mostly children; more than one quarter were hospitalized, HUS developed in 7.5%, and 4 children died . Illness was linked to eating undercooked hamburgers at a chain fast-food restaurant, prompting a recall of >250,000 hamburgers, which likely prevented many additional illnesses and deaths.\n\n【38】Outbreak investigations that implicated fast-food hamburgers have led to major improvements in meat safety in the U.S fast-food industry. In 1993, the U.S. Food and Drug Administration revised the Model Food Code for restaurants, with new temperature guidelines for ground beef . In 1994, the National Livestock and Meat Board’s Blue Ribbon Task Force developed objective measures of meat “doneness” and encouraged use of automated cooking systems . No fast-food hamburger-associated outbreaks have been reported since 1995, demonstrating that changes in the fast-food industry, such as carefully regulating cooking temperature of hamburgers, are both possible and effective.\n\n【39】In addition, outbreak investigations coupled with traceback investigations of implicated meat have identified contaminated beef lots, leading to large recalls of potentially contaminated beef . These recalls of up to 25 million pounds of beef  likely prevented many additional infections. Despite these improvements, ground beef continues to be frequently implicated in _E. coli_ O157 outbreaks. Raw beef, especially ground beef, can be contaminated with _E. coli_ O157 and should be cooked thoroughly to kill pathogens and handled carefully to avoid cross-contamination of other food items. As ground beef outbreaks are commonly reported from home-prepared ground beef, educational efforts should be focused on teaching consumers safer handling and cooking practices.\n\n【40】Outbreaks provide information about inadequacy of processing methods. For example, in 1994, an _E. coli_ O157 outbreak due to eating commercially distributed dry-cured salami product involved 23 persons; HUS developed in 13% . This outbreak prompted U. S. Department of Agriculture officials to develop regulations to ensure the safety of shelf-stable fermented sausages ; no further _E. coli_ O157 outbreaks due to U.S.–manufactured salami have been reported since.\n\n【41】_E. coli_ O157 outbreaks due to produce have become increasingly common. While half of produce-associated outbreaks were due to kitchen-level cross-contamination, which calls for further prevention efforts targeting food preparers, the other half were due to produce already contaminated with _E. coli_ O157 before purchase, including lettuce, sprouts, cabbage, apple cider, and apple juice . These produce items could have become contaminated in the field from manure or contaminated irrigation water; during processing due to contaminated equipment, wash water, or ice or poor handling practices; during transport; or through contaminated storage equipment. Washing produce with water or a chlorine-based solution reduces _E. coli_ O157 counts only modestly ; therefore, once consumers obtain contaminated produce intended for raw consumption, little can be done to prevent illness. Efforts by industry to decrease contamination of sprouts have had limited success . Until effective measures for preventing _E. coli_ O157 contamination of produce items such as lettuce, cabbage, and sprouts can be implemented, consumers should be educated about potential risk of consuming these items raw. Further regulatory and educational efforts are needed to improve the safety of produce items.\n\n【42】In 1996, a large _E. coli_ O157 outbreak occurred in 3 western states and British Columbia, involving 70 illnesses, mostly children; more than one third of patients were hospitalized, HUS developed in 20%, and 1 child died . Illness was attributable to drinking commercial unpasteurized apple juice. However, as a result of this outbreak investigation, apple cider and apple juice that are shipped interstate in the United States since 1998 are either pasteurized or, if sold raw, carry a warning label advising consumers of potential harmful bacteria in the product . Since 1998, only 2 outbreaks due to unpasteurized apple cider have been reported, 1 at a local fair and 1 from locally produced cider that carried a warning label.\n\n【43】Prevention efforts focused on hygiene are needed to reduce transmission in daycare settings. In outbreaks of other primary transmission routes, secondary cases occur, which emphasizes the importance of educating caretakers to avoid direct contact with fecal matter and to apply stringent handwashing rules.\n\n【44】Drinking and recreational water have the potential to infect many persons. The largest U.S. _E. coli_ O157 outbreak occurred in 1999 at a county fair due to contaminated drinking water and involved 781 ill persons; 9% were hospitalized, HUS developed in 2%, and 2 died . The implicated water was from a temporary unregulated well at the fairground. Properly functioning water systems with adequate chlorine levels should protect against _E. coli_ O157 contamination. Many U.S. households, however, receive municipal water that is not chlorinated. Further safeguards are therefore needed to ensure the safety of unchlorinated water systems and to ensure that chlorinated water systems are properly functioning. Educational efforts targeted at caretakers of young children should continue to help reduce contamination of recreational water areas by fecal matter .\n\n【45】Outbreaks associated with animal contact represent a newly recognized transmission route for _E. coli_ O157 in the United States. Cattle hides may become contaminated from fecal matter. Persons touching cattle or surfaces in the cattle’s environment may contaminate their hands with _E. coli_ O157. If hands are not washed thoroughly after contact with cattle or their environments, the bacteria can infect these persons through a hand-to-mouth route. Recent strategies published to help reduce transmission of enteric pathogens from farm animals to children include informing the public about risk for transmission of enteric pathogens from farm animals to humans, separating eating facilities from animal contact areas, and providing adequate handwashing facilities .\n\n【46】The overall decreased HUS and case-fatality rates in the last 2 decades likely represent increased reporting of less clinically severe outbreaks, especially after _E. coli_ O157 became a reportable disease. The high HUS rate found in swimming-associated outbreaks may be due partly to the higher proportion of young children involved and their vulnerability to development of HUS. The reason for the higher HUS rate found among ground beef–related outbreaks is unclear and may reflect reporting bias. Outbreaks occurring in residential facilities such as nursing homes had a particularly high case-fatality rate, which emphasizes the need for prevention efforts, both educational and regulatory, to lower the incidence of _E. coli_ O157 infections in such facilities.\n\n【47】Since 1992, molecular subtyping of _E. coli_ O157 by pulsed-field gel electrophoresis has improved early outbreak detection. PulseNet , the national network for comparing molecular subtypes of common foodborne bacterial pathogens, including _E. coli_ O157 since 1997, has greatly assisted in both identifying outbreaks and linking apparently unrelated outbreaks. Continued molecular subtyping of _E. coli_ O157 strains from both humans and the environment will assist in detecting outbreaks and allow for identification of multistate, geographically dispersed outbreaks due to contaminated commercial products .\n\n【48】Outbreak surveillance has several limitations. _E. coli_ O157 outbreaks captured by CDC’s surveillance system likely represent only a small proportion of outbreaks that occur. Many outbreaks go unrecognized, are classified as outbreaks of unknown etiology, and are not reported to local public health officials or CDC . Smaller outbreaks and outbreaks with unknown transmission routes and vehicles are less likely to be reported, and this summary likely under represents such outbreaks. Including patients with compatible clinical illness without culture confirmation is another limitation of outbreak surveillance. However, given the broad clinical spectrum of _E. coli_ O157 infection, and the limited number of infected persons with culture-confirmed illness , such inclusion allows us to better assess the true public health impact of _E. coli_ O157. In addition, outbreak reporting may not be uniform across time periods or states. Therefore, trends should be interpreted carefully, given the changing factors that may impact outbreak detection and reporting. The increased numbers of outbreaks reported since 1993 but with smaller sizes are likely due to increased awareness of disease, improved diagnostics, increased _E. coli_ O157 testing, and improved outbreak detection through molecular subtyping.\n\n【49】Outbreak investigations, especially for emerging pathogens such as _E. coli_ O157, are critical for better understanding these pathogens’ epidemiology, which affect policy and behavior changes. While a summary of outbreaks cannot draw firm conclusions on disease trends, illustration of transmission routes, food vehicles, outbreak size, and clinical outcomes over time empowers public health officials, regulatory agencies, and health educators to target appropriate interventions and reevaluate current prevention strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ddad8ab4-b2ed-4771-afbd-f6b0cf18340d", "title": "Pediatric Influenza Prevention and Control", "text": "【0】Pediatric Influenza Prevention and Control\nInfluenza vaccination is routinely recommended in pediatric patients of age \\> 6 months who are at high risk for influenza-related complications because they have an underlying disease or are undergoing long-term aspirin therapy and are at risk of developing Reye syndrome . Administering the vaccine to healthy children is recommended only when they live with persons at high risk , although the Advisory Committee on Immunization Practices is gradually moving toward a recommendation to vaccinate all children ages 6–23 months (because of their substantially increased risk for influenza-related hospitalizations) and children ages 2–18 years who are household contacts of children ages 0–23 months .\n\n【1】Although health authorities in industrialized countries agree with these guidelines, use of influenza vaccine in clinical practice differs. Most children at high risk for complications do not receive the vaccine, and its use in healthy infants is not routinely accepted , even though results of recent studies suggest expanding the number of children for whom vaccination should be recommended . We discuss current vaccination practices in children, reasons and possible remedies for low immunization rates, and the possibility of extending its use in pediatrics.\n\n【2】### Vaccine Practices for Children\n\n【3】High-risk children for whom influenza vaccination is routinely recommended include those with chronic disorders of the cardiovascular or pulmonary system (including asthma), chronic metabolic diseases (including diabetes mellitus), chronic renal dysfunction, and hemoglobinopathies or immunosuppression (including cases caused by medications or by HIV) . Although an association between these conditions and an increased risk for influenza complications was first suggested many years ago , the level of vaccination in such children is still much lower than recommended, although it is slightly higher when children are followed up in specialized centers rather than by primary care physicians (perhaps because children seen in specialty clinics have more severe underlying illnesses), or when data regarding immunization are collected after implementing a reminder and recall system . One study of health maintenance organizations reported influenza vaccination rates of 9% to 10% among children with asthma and a rate of 25% among those attending an allergy and immunology clinic . The use of a reminder and recall system increased vaccination coverage among children with asthma from 5% to 32% . The highest coverage was found among pediatric patients attending a cystic fibrosis treatment center, in whom a vaccination level of 79% was reached . Data collected in Italy confirm that the behavior of pediatricians is not in line with the official recommendations. Among the 274 high-risk children attending the University of Milan’s Pediatric Emergency Department during winter 2002–2003, the vaccination level was 26.3%; the highest rates were in children with HIV infection (52.3%), and the lowest rates were in those with asthma (9.5%) .\n\n【4】Few data concern the effect of encouraging vaccination in healthy children <2 years. However, comparing immunization rates among children of this age without any high-risk condition attending the University of Milan’s Pediatric Emergency Department during the two winter seasons of 2001 to 2002 and 2002 to 2003 (after the publication of the suggestion that healthy children <2 years be vaccinated) showed only a marginal increase (2.4% vs. 3.6%) .\n\n【5】### Reasons for Low Immunization Rates and Possible Solutions\n\n【6】Seven main obstacles to complying with recommendations for vaccination in children exist: 1) lack of understanding of the risk for influenza complications in children; 2) lack of knowledge of annual immunization’s efficacy in primary prevention; 3) parents’ negative reaction to parenteral vaccine administration (“Not another shot!”); 4) need for two priming doses in children <9 years old followed by annual administration; 5) fear of limited protection in younger and high-risk children; _6_ ) concerns about possible adverse events; and 7) lack of precision in current recommendations. The most important of these obstacles are lack of understanding of the risks for complications and lack of knowledge of efficacy .\n\n【7】A number of studies of adult (particularly elderly) populations have shown that knowing risk factors for influenza complications, favorable perceptions of the vaccine, and clinician recommendations are the main variables predicting the administration of influenza vaccination . However, pediatric data indicate that some providers do not recognize influenza’s clinical relevance, even when it occurs in children with severe underlying disease . A study designed to ascertain the self-reported use of influenza vaccine among pediatric oncologists found that approximately 30% did not think that influenza infection is important in children with cancer  and consequently do not recommend immunization. The central role of physicians’ opinions in determining vaccination coverage is supported by data collected in a cross-sectional study of a group of children hospitalized during the influenza season in the United States : >70% of the children were vaccinated if a physician had recommended it to their parents, but 3% were vaccinated if no such recommendation had been made. A lack of awareness that children can receive influenza vaccine was a commonly cited reason for nonvaccination .\n\n【8】The attitude of pediatricians towards influenza vaccine can be explained by the fact that its importance in high-risk children and healthy infants is mainly suggested by indirect data. Although a number of studies have shown that influenza can significantly increase hospitalization, outpatient visits, and drug consumption in high-risk children of all ages , few trials (mainly involving children with asthma) have demonstrated that vaccination is clinically useful in reducing influenza-related complications . Furthermore, data concerning the efficacy of influenza vaccine in healthy infants <2 years of age have been collected from small groups. Although a reduction in influenzalike illnesses has been shown, the data do not evaluate the importance of vaccination in reducing hospitalizations or complications . Pediatricians may be definitively convinced of the importance of preventing influenza and personally start supporting the use of vaccine when more data are available demonstrating its efficacy in children. Consequently, studies evaluating the real clinical impact of influenza vaccine, not only in children with risk factors but also in healthy infants, are needed.\n\n【9】Another probable factor preventing the use of influenza vaccines in pediatrics is that those currently licensed for use in children are parenteral (two injections for children <9 years of age being vaccinated for the first time) and require annual administration to maintain protection . Parents may be concerned about the number of injections their children receive during the course of routine early child health visits. Given the large number of vaccinations already included in the routine childhood immunization schedule, the addition of another “shot” may not sound attractive to parents and certainly not to their children. However, the availability of intranasal influenza vaccines may substantially reduce this problem . Recent advances in influenza vaccination include the development of a trivalent, cold-adapted, live-attenuated, intranasal vaccine that appears to be as effective as its intramuscular counterparts and induces a good immune response (including local immunoglobulin \\[Ig\\] A responses and secretory IgA antibodies that can protect against pathogens infecting mucosal sites) . One of the disadvantages of this vaccine is that individual susceptibility to infection with live viruses (and consequent immunogenicity) varies widely; vaccine strains’ reversion to their wild-type genotype has also been considered a potential risk, although there is no evidence that this occurs . If eventually licensed for use worldwide, intranasal vaccines can be expected to increase influenza vaccination coverage, especially in children.\n\n【10】Concerns that influenza vaccine may offer limited protection and fears of possible adverse events are further reasons for its limited use in pediatrics . However, protective antibody levels after influenza vaccination have developed in 70% to 90% of children as young as 6 months of age, although fewer younger infants seroconvert, and some high-risk children may have a lower antibody response . Childhood vaccination programs fail to be beneficial if vaccine efficacy falls to < 25%, levels that have never been reported in younger or high-risk children . Moreover, although mild local and systemic reactions to the vaccine may occur more frequently in persons who have never been exposed to the viral antigens it contains (e.g. young children), the currently licensed parenteral vaccines are generally safe and well-tolerated . Considering the possible effect that “vaccine-adverse” parents have on immunization policy in some regions, disseminating information concerning the safety, tolerability, and immunogenicity of influenza vaccination in healthy infants and high-risk children is important.\n\n【11】Influenza prevention recommendations imprecisely describe the characteristics of high-risk children, contributing to inadequate vaccination in this population. For example, the Advisory Committee on Immunization Practices recommends yearly influenza vaccination for immunosuppressed children, including those with immunosuppression due to medications  but does not specify which diseases require vaccination, the doses of the immunosuppressive drugs, or the timing of the vaccination in relation to their administration . Conversely, the American Academy of Pediatrics states that the optimal time to immunize these children is when their peripheral leukocyte count is >1,000/μL and that vaccination has to be deferred during high-dose corticosteroid administration . These discrepancies reflect a lack of data and may explain why pediatricians have different approaches in clinical practice. Specific and uniform guidelines for each group of children at high risk would be the best way to overcome this problem. Still, in many clinical scenarios decisions are based on the best information available, and recommendations cannot deal with each and every situation that the medical provider confronts.\n\n【12】Globally evaluating the main reasons for low influenza vaccination coverage in pediatrics suggests that improving knowledge of influenza among pediatricians and parents could improve vaccination practices. The medical community spends substantial amounts of time with parents trying to convince them of the need for routine vaccinations, but in many instances, vaccines are suggested on the basis of the parents’ or the healthcare providers’ perception of vaccine or diseases of greatest importance. If parents lack insurance, economic considerations also become an issue. A change of mindset is needed to enhance acceptance of influenza vaccination; providing materials to educate parents would help effect this change. As television and print advertising promotes other pharmaceutical products, similar advertising could effectively promote influenza vaccination. The first step is to define simple, unequivocal, and practical guidelines specific to different groups of children at high risk and healthy infants <2 years of age. These guidelines, for distribution to hospital physicians and primary care pediatricians, would contain detailed information regarding the consequences of influenza in such children and describe the effectiveness of influenza vaccine and the risk for adverse events. In addition, pediatricians can use recall systems to provide timely reminders for all patients.\n\n【13】### Influenza Vaccine in Children Not at Risk\n\n【14】In addition to the children for whom influenza vaccine is already recommended or strongly encouraged, other pediatric patients can receive clinical benefits from its use. One group of children who could be included on the list of vaccination candidates is those with recurrent episodes of acute otitis media (AOM). Recurrent AOM is common in infants and children, and its possible sequelae make prevention desirable . Until a few years ago, chemoprophylaxis and controlling environmental risk factors were considered the best ways to reduce the incidence of new episodes of AOM in otitis-prone children, but the emergence of drug-resistant bacteria after antimicrobial drug administration raises questions about the advisability of drug therapy . Immunoprophylaxis against respiratory viruses has received growing attention because viral infections (including influenza) are associated with many, if not most, episodes of AOM. Data showing that administering parenteral, inactivated influenza vaccine can decrease the incidence of AOM by approximately one third strongly support the use of vaccination in preventing AOM . The demonstration that live-attenuated, cold-adapted, intranasal vaccine causes a 30% reduction in the incidence of febrile AOM in healthy children without a history of ear disease leads to the same conclusion . The importance of influenza vaccination in children with recurrent AOM has been recently demonstrated by Marchisio et al. who used an intranasal, inactivated, virosomal subunit vaccine . In this study, 133 children aged 1–5 years with recurrent AOM (defined as \\> 3 episodes in the preceding 6 months or \\> 4 episodes in the preceding 12 months) were randomized to receive the vaccine (n = 67) or no vaccination (n = 66). During a 6-month period, 24 vaccine recipients (35.8%) experienced 32 episodes of AOM, and 42 control participants (63.6%) experienced 64 episodes. The overall efficacy of vaccination in preventing AOM was 43.7% (95% confidence interval 18.6 to 61.1, p = 0.002)  . Moreover, the cumulative duration of middle ear effusion was significantly less in the vaccinated children (58.0% vs. 74.5%; p < 0.0001) . As reducing the occurrence of AOM in children with recurrent episodes can have substantial clinical and socioeconomic effects, these data suggest that influenza vaccine can be considered a valid option in preventing the disease in otitis-prone children.\n\n【15】A second group of children who could be considered for influenza vaccine are those with recurrent episodes of respiratory tract infections (RRTIs). A large number of children without any immunologic problems experience multiple episodes of RRTIs during the first years of life; although these generally have a benign prognosis, they can cause substantial medical and socioeconomic problems . They are mainly caused by viruses and, during epidemic periods, influenza viruses can also be causative. Data collected in a recent study indicate that vaccinating children with RRTIs against influenza is effective in decreasing respiratory-related illness among them and their families . A total of 127 children 6 months to 9 years of age with a history of RRTIs ( \\> 6 episodes per year if \\> 3 years; \\> 8 episodes per year if <3 years) were randomized to receive the intranasal virosomal influenza vaccine (n = 64 with 176 household contacts) or a control placebo (n = 63 with 173 household contacts). During the influenza season, vaccinated children had fewer respiratory infections or febrile respiratory illnesses, received fewer prescribed antimicrobial and antipyretic drugs, and missed fewer school days than the controls ; similar benefits and a reduced loss of parental work were observed among their household contacts . These results show that the benefits of influenza vaccination extend to children with RRTIs and their families and suggest that its use in such children should be encouraged.\n\n【16】### Influenza Vaccine in Healthy Daycare and School-Aged Children\n\n【17】A number of studies have shown that otherwise healthy daycare and school-aged children are most frequently affected by influenza, and high attack rates can substantially diminish their quality of life and disrupt everyday activities . Children shed larger quantities of influenza viruses for longer periods of time than adults and thus play an important role in spreading infection in their families and communities . Negative effects of influenza in otherwise-healthy children can extend to unvaccinated household contacts, who may require substantial diagnostic and therapeutic interventions and miss a number of school or working days. Neuzil et al. found that, during the influenza season, the number of household members who became ill within 3 days of a child’s absence from school was 2.2 times higher than expected. Excess absenteeism from work also occurred among parents . In line with these observations, we have found that the household contacts of children with influenza require more medical visits, miss more working or school days, and need more help at home to care for ill children than the household contacts of children without influenza .\n\n【18】Preventing influenza by vaccination can improve these situations. A blinded, placebo-controlled study of two influenza vaccines (an inactivated split-virus vaccine and a live-attenuated, cold-adapted vaccine) in 555 school-aged children in Russia demonstrated that both were efficacious in preventing school absenteeism by reducing the number of missed school days by 47% to 56% compared to missed school days in unvaccinated children . Similarly, in a study of the effect of an inactivated, split-virus vaccine on healthy children attending daycare or school in Italy during the years 2001–2002, we found that the vaccinated children experienced fewer upper and lower respiratory tract infections, received fewer antimicrobial and antipyretic prescriptions, and missed fewer school days because of respiratory illnesses . These data suggest that the effect of influenza on otherwise-healthy daycare or school-age children may be more substantial than is usually thought, encouraging wider pediatric use of influenza vaccine to reduce the overall extent of infection.\n\n【19】Strong support for wider pediatric use comes from evaluating the household impact of influenza vaccination in healthy daycare and school-age children. In a 1995 randomized, controlled trial of influenza vaccine for preschool children, the rate of febrile respiratory illnesses was 42% less among the unvaccinated household contacts of influenza-vaccinated children than among those living with unvaccinated children . Moreover, data collected in Tecumseh, Michigan , and Japan  indicate that mass vaccination of school-age children correlates with a reduced rate of respiratory illness and all-cause community death rate, which suggests that larger scale immunization can affect community epidemics. Similarly, during the 2001–2002 influenza season in Italy, we found that, compared to the household contacts of unvaccinated children, family members of influenza-vaccinated healthy children experienced fewer respiratory tract infections, needed fewer medical visits, missed fewer working days, and required less help at home to care for ill children  . All of these findings highlight the fact that influenza in otherwise-healthy children attending daycare centers or schools has a considerable effect on their families and that the benefits of influenza vaccination extend to the family members of vaccinated persons.\n\n【20】The socioeconomic importance of influenza in childhood is confirmed by economic analyses showing that vaccinating healthy preschool and school-age children can lead to health and economic benefits during epidemic and pandemic periods . These studies used different analytic methods, outcomes, and costs but came to a common conclusion: vaccinating healthy children against influenza leads to a net cost saving, and the greatest financial benefit is observed when the vaccine is administered in a group setting . Savings are primarily due to avoided indirect costs and, in particular, reduced parental absenteeism from work.\n\n【21】### Conclusion\n\n【22】Global evaluation of the effect of influenza in pediatric patients indicates that influenza vaccination should be more widely used than is usually recommended. To protect them against the complications of influenza, increased efforts are needed to identify and recall high-risk children. Further, immunizing infants 6–23 months of age and their close contacts is recommended. Children with recurrent AOM or a history of RRTIs and healthy children attending daycare centers or schools should also be included among the pediatric groups recommended for vaccination.\n\n【23】These conclusions are based on clinical and socioeconomic considerations arising from evaluating the impact of influenza vaccination on both the children themselves and their household contacts. Improved recognition of the complications of influenza in the first years of life, with resources dedicated to provider and public education on this issue, can help reduce obstacles to using influenza vaccine. Parents might choose vaccination for their children if they were more informed about the health and economic cost of influenza, its annual attack rate in childhood (which leads to days lost from school and work), and the central role of children in disseminating the infection in households and communities. The issues that need to be addressed include educating physicians and parents about the illness caused by influenza, the cost-effectiveness and safety of licensed vaccines, adequate vaccine supplies, and the availability of intranasal products. Improved compliance associated with nasal administration should increase the use of influenza vaccination. Only a heightened and regular demand for influenza vaccine will result in sufficient vaccine supplies at all times (not just on a year-to-year basis) and place us in a better position to detect a novel pandemic influenza virus strain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4c64be31-93f2-4d1d-a9f0-de592a8cbdb8", "title": "Group B Streptococcus Infections Caused by Improper Sourcing and Handling of Fish for Raw Consumption, Singapore, 2015–2016", "text": "【0】Group B Streptococcus Infections Caused by Improper Sourcing and Handling of Fish for Raw Consumption, Singapore, 2015–2016\nA major outbreak of group B _Streptococcus_ (GBS) infection associated with consumption of a Chinese-style raw fish dish ( _yusheng_ ) occurred in Singapore during 2015 and involved 238 persons during the first half of the year . The _Yusheng_ was typically made from sliced Asian bighead carp ( _Hypophthalmichthys nobilis_ ) and snakehead ( _Channa_ spp.) and served as a side dish with porridge by food stalls within larger eating establishments. Persons with severe clinical cases had meningoencephalitis, bacteremia, and septic arthritis . GBS, or _Streptococcus agalactiae_ , was identified as the causative agent .\n\n【1】GBS is found in ≈30% of healthy adults  and is a member of the human commensal gastrointestinal and genitourinary flora . GBS is a common cause of neonatal sepsis, is acquired by newborns from the vaginal flora of the mother, and is an increasingly common pathogen among vulnerable populations . The incidence of invasive disease in adults, particularly older adults, has been increasing . GBS is also a fish and bovine pathogen . Although GBS has been shown to colonize the gastrointestinal tract of humans linked to fish consumption , foodborne transmission leading to invasive disease has not been reported. Local epidemiologic investigations conducted separately  identified a single strain of GBS serotype III sequence type (ST) 283 as the causative agent of the outbreak in Singapore during 2015. GBS ST283 had previously been isolated from tilapia in Thailand  and in adult human cases in Hong Kong . However, GBS ST283 has not been reported to colonize the human gastrointestinal tract, although to date only 1 study of fish mongers and fish handlers has specifically looked for colonization by this strain .\n\n【2】We investigated microbial safety and quality of fish sold in the Singapore market during and after the outbreak during 2015 to trace the source of GBS ST283 and provide risk assessment data to support outbreak control and prevention measures. Shortly after identification of GBS ST283 as the cause of the outbreak, these data supported implementing a ban on the sale of ready-to-eat (RTE) dishes containing raw freshwater fish, as well as imposing additional requirements for sale of RTE raw fish dishes made with saltwater fish . We report the results of our analysis, which might assist the review of guidelines for handling of fish meant for raw consumption in Singapore and other countries. This report offers unique food and environmental insights into the investigation of this outbreak and complements published epidemiologic findings .\n\n【3】### Materials and Methods\n\n【4】##### Collection of Fish and Fish Tank Water Samples\n\n【5】We collected samples of fish commonly used for raw consumption (n = 997) and fish tank water for holding live freshwater fish (n = 102) along the supply chain in Singapore during August 2015–January 2016 . We tested samples for GBS, _Aeromonas_ spp. _Listeria monocytogenes_ , _Salmonella_ spp. _Vibrio cholerae_ , and _V. parahaemolyticus_ , and determined _Escherichia coli_ counts, _Staphylococcus aureus_ counts, and standard plate counts (SPCs) . We characterized selected species to determine their virulence potential .\n\n【6】##### Statistical Analysis\n\n【7】We evaluated significant differences (p<0.05) between bacterial counts (log 10  CFU/g) and presence of specific foodborne bacteria by using Kruskal-Wallis, Mann-Whitney, χ 2  , and Fisher exact tests as appropriate. We performed analysis by using SPSS version 24.0 software (IBM, Armonk, NY, USA).\n\n【8】### Results\n\n【9】##### Raw Fish Samples from Food Stalls and Restaurants/Snack Bars\n\n【10】Although raw freshwater and saltwater fish were served as RTE food at food stalls, only raw saltwater fish were reportedly served at restaurants/snack bars. PCR positivity rates were 43.5% (20/46) for GBS and 23.9% (11/46) for GBS serotype III in sliced fish samples from food stalls. Fish sampled from restaurants/snack bars had significantly lower rates (p<0.05) of 9.2% (26/282) for GBS and 0.7% (2/282) for GBS serotype III . Among the 20 GBS PCR-positive samples from food stalls, 5 yielded isolates; these isolates were of serotype II ST652, serotype III ST283, serotype III ST335, and serotype V ST1 . The GBS ST283 isolated was from a RTE sliced fish sample sold as grass carp collected from a food stall linked to a human case, as described . We did not detect GBS ST283 in samples from restaurant/snack bars; however, we did find a range of other GBS, including serotypes Ia ST7, Ia ST103, Ia ST485, III ST651, III ST861, V ST1, V ST24, VI ST167, and VII ST1.\n\n【11】We found _Salmonella_ serogroup B ST29 (serovar Stanley) (n = 2); _V. parahaemolyticus_ (negative for _tdh_ , _trh1_ , and _trh2_ genes) (n = 1); and non–O1 _V. cholerae_ (n = 1) in freshwater fish samples from food stalls. We also isolated _V. cholerae_ from saltwater fish samples, 1 from a food stall and 1 from a restaurant. We detected _L. monocytogenes_ in 5 samples from restaurants/snack bars.\n\n【12】SPCs of most RTE raw freshwater (71.4%, 5/7) and saltwater (85.7%, 18/21) fish samples from food stalls exceeded the regulatory limit for RTE food (5 log 10  CFU/g) in Singapore . We observed no difference in SPCs for fish slices intended for raw consumption and cooking purposes . We also found that 24.8% (70/282) of saltwater fish samples from restaurants/snack bars did not comply with regulatory limits for SPCs, _E. coli_ counts (1.3 log 10  CFU/g), or both . These results showed the poor quality of RTE raw freshwater and saltwater fish sold at food stalls in comparison to those sold at restaurants and snack bars.\n\n【13】##### Comparison of Freshwater and Saltwater Fish Samples from Fresh Produce Markets\n\n【14】Fish sold at food stalls were typically procured from local fresh produce markets. For the 62 samples of whole fish and fish parts we collected from these markets, we detected GBS ST283 in 28.2% (11/39) of the freshwater fish , which included fish sold as tilapia, Asian bighead carp, grass carp, snakehead-haruan, snakehead-toman, and silver carp . However, we did not detect GBS ST283 in saltwater fish. Other GBS strains detected among these fish include serotypes Ia ST7, Ia ST23, Ia ST24, and II ST28 .\n\n【15】We detected _Aeromonas_ spp. (48.4%, 30/62), _S. aureus_ (27.4%, 17/62), non–O1 _V. cholerae_ (12.9%, 8/62) and _V. parahaemolyticus_ (negative for _tdh_ , _trh1_ , and _trh2_ genes) (6.4%, 4/62) in fish samples from fresh produce markets. There was no difference in positivity rates of these organisms between freshwater and saltwater fish. We did not detect _L. monocytogenes_ or _Salmonella_ spp. in any fish samples collected from fresh produce markets.\n\n【16】Approximately 42% (15/36) of freshwater fish muscle samples had SPCs or _E. coli_ counts, or both, exceeding regulatory limits for RTE food in Singapore . Positivity rates for GBS, GBS serotype III, and _E. coli_ , as well as SPCs for saltwater fish, were significantly lower (p<0.05) . _E. coli_ and _S. aureus_ counts for freshwater fish surfaces were significantly higher (p<0.05) than those for saltwater fish .\n\n【17】We collected 4 fish tank water samples from wet markets and supermarkets. One water sample and the live freshwater fish the tank contained were positive for GBS by PCR and non–O1 _V. cholerae_ by culture; the associated fish was positive for GBS ST283 by culture. Two other fish tank water samples and the live fish the tanks contained were positive for _E. coli_ , _S. aureus_ , or both. The level of _E. coli_ detected in each positive fish tank water sample was 1.3 log 10  CFU/500 mL, which was greater than the 1 log 10  CFU/500 mL coliform (which includes _E. coli_ ) limit set by the British Columbia Centre for Disease Control .\n\n【18】##### Whole Freshwater Fish and Fish Tank Water from Ports\n\n【19】We tested for GBS only in whole fish and fish tank water samples collected from ports. We detected GBS ST283 in 1% (6/586) of freshwater fish samples; positive samples were from Asian bighead carps imported from and farmed in Malaysia. For 98 fish tank water samples collected from ports, 55.1% (54/98) were positive for GBS, and 6.1% (6/98) were positive for GBS ST283. Three of the GBS ST283–positive fish were kept in fish tank water that was also positive for GBS ST283.\n\n【20】##### Comparison of Saltwater Fish from Fresh Produce Markets and Sashimi Suppliers\n\n【21】Our data indicate the risk for contamination of fish sold at local fresh produce markets, although saltwater fish samples from fresh produce markets had lower rates of contamination than freshwater fish samples. The SPCs and the positivity rates for _E. coli_ in saltwater fish samples from sashimi suppliers were significantly lower (p<0.05) than those for saltwater fish samples from fresh produce markets , which suggested that the microbial quality of fish could be managed by improvements in handling throughout distribution channels. None of the saltwater fish muscle samples from sashimi suppliers exceeded the Singapore SPC (5 log 10  CFU/g) and _E. coli_ (1.3 log 10  CFU/g) limits for RTE food . We did not detect GBS, _S. aureus_ , _V. cholerae_ , and _V. parahaemolyticus_ in any fish samples collected from sashimi suppliers. However, we detected _L. monocytogenes_ in 1 salmon sample.\n\n【22】##### Characterization of GBS Isolates\n\n【23】We detected 6 GBS serotypes (Ia, II, III, V, VI, and VII) and 13 STs (1, 7, 23, 24, 28, 103, 167, 283, 335, 485, 651, 652, and 861) in fish . Although most strains were within clonal complexes (1, 10, 17, 19, and 23) associated with human carriage and diseases , a total of 20 isolates from 7 sashimi samples (SGEHI2015-IV45, SGEHI2015-IV72, SGEHI2015-IV74, SGEHI2015-IV89, SGEHI2015-IV100, SGEHI2015-IV211, and SGEHI2015-IV232) did not belong to these clonal complexes. These strains had few closely related strains in the public genomic databases, and the closely related strains are mostly from animals .\n\n【24】We found GBS ST283 only among freshwater fish and water for holding freshwater fish. Genomic analyses indicated that GBS ST283 isolated from fish clustered in 2 clades . The first clade included 12 isolates from 6 fish from a food stall, a fresh produce market and a port, and 4 fish tank water samples from a port. Genome sequencing showed that these 12 isolates were nearly identical (0–2 SNPs and 0, 1, and 12 indels all in homopolymeric runs of \\> 4 nt) compared with the 2.1-Mbp genome of the reference human outbreak strain, SG-M1, isolated from a meningitis patient during the GBS outbreak in Singapore during 2015 . Isolates that clustered into the second clade were 20 isolates from 12 fish and 2 fish tank water samples and did not include any human isolates either from this outbreak or from previous reports of human GBS infecting isolates. Sequences of these isolates showed higher intraclade diversity (57–71 SNPs and 11–33 indels) when compared with the SG-M1 genome .\n\n【25】##### Characterization of _S. aureus, V. cholerae_ , and _V. parahaemolyticus_ Isolates\n\n【26】We characterized 18 _S. aureus_ isolates from 17 fish. All except 1 were obtained from fish surfaces. We detected \\> 1 enterotoxin gene in two thirds of these isolates and the _sec_ gene in 55.6% (10/18) of the isolates. Other enterotoxin genes ( _sea_ , _seg_ , _seh_ , _sei_ , and _sel_ ) were detected at much lower rates (5.6% \\[1/18\\] to 11.1% \\[2/18\\]). We detected 4 enterotoxin genes ( _sec_ , _seg_ , _sei_ , and _sel_ ) in a _S. aureus_ isolate obtained from the surface of a wolf herring sample collected from a port. We did not detect virulence genes ( _ctxA_ , _ctxB_ , and _tcpA_ ) in any of the 16 non–O1 _V. cholerae_ isolates from 9 fish and 1 fish tank water samples and did not detect virulence genes ( _tdh_ , _trh1_ , and _trh2_ ) in any of the 6 _V. parahaemolyticus_ isolates from 5 fish samples.\n\n【27】### Discussion\n\n【28】We found GBS ST283, the causative strain of a severe foodborne outbreak in Singapore, in the local freshwater fish supply chain that stretches from food stalls to local fresh produce markets and back to ports. Patients with GBS ST283 infections during this outbreak were more likely to show development of meningoencephalitis, septic arthritis, and spinal infection than were persons with non–GBS ST283 infections . Although this study suggested Malaysia as a source of the strain, the finding of the same ST in Hong Kong and Thailand  suggested that GBS ST283 is generally prevalent throughout the region.\n\n【29】Our analysis shows that there are at least 2 clades of GBS ST283 strains among fish in local markets. Fish and water strains from 1 clade were nearly identical to clinical strains from this outbreak . The small variability of 0–2 SNPs and 0–12 indels between fish and water strains and the reference human outbreak strain (SG-M1) is equivalent to variability observed in 131 clinical strains from the same outbreak reported elsewhere (0–5 SNPs from the SG-M1 reference) . Strains from a second clade of GBS ST283 had a difference of 57–71 SNPs and 11–33 indels when compared with the SG-M1 genome. Other GBS ST283 isolates, many collected in Hong Kong < 17 years before this outbreak  are also different from the SG-M1 strain ( < 129 SNPs) . We found no human-infecting isolate from Singapore or elsewhere within the second fish-associated GBS ST283 clade.\n\n【30】A major issue is whether all GBS ST283 strains are capable of causing invasive human disease by the foodborne route. If strains from the fish-associated clade are not pathogenic to humans, they could be used as effective controls for identifying the genetic basis of pathogenicity of the first clade and the cause of its emergence, which resulted in outbreak in Singapore in 2015. If these strains are pathogenic to humans, then broader tracking of the prevalence of GBS ST283 would be warranted.\n\n【31】In contrast to GBS strains that are known to cause disease outbreaks in fish , the live and whole fish from which GBS ST283 was recovered in this study did not have defects, such as corneal opacity and exophthalmia , which suggests that this ST might not be pathogenic for freshwater fish. The closest GBS fish pathogens with published genomes, GD201008–001  and HN016 , are serotype Ia ST7 strains that are distant (>4,000 SNPs) from all ST283 strains that our group and others have identified .\n\n【32】Detection of 6 GBS serotypes and 13 STs showed the diversity of GBS strains in fish. Although the sample size in this study was small and our results might not represent the distribution of GBS in all fish species, our findings provide valuable data for characterizing the public health risk from consuming raw fish. No baseline information on GBS in fish was publicly available before this outbreak because fish were not a recognized source or a recognized route of transmission of GBS. Further work on GBS STs other than ST283 is underway to investigate the role of fish as a source of GBS disease in humans.\n\n【33】Several GBS strains from sashimi had relatively few closely related strains in the public genomic databases , which suggests that the GBS population associated with saltwater fish could be different from that associated with freshwater fish and humans. Another reason for this observation is that GBS from food and environmental sources are relatively undersampled in the genomic databases than those from humans.\n\n【34】We detected GBS serotypes Ia ST23 and Ia ST7, which are associated with human carriage , in fish samples. Although GBS ST7 has been described as a fish pathogen, the presence of GBS serotype Ia ST23 has not been reported in fish . GBS serotypes Ia ST23, and Ia ST7 and _E. coli_ , which are all associated with human gut flora, suggest possible contamination of fish by effluent water.\n\n【35】The intentional introduction of animal feces into fish ponds as part of integrated farming  might further contribute to the complex flow of pathogens between animals and humans. Such findings point to areas for research to clarify the diversity and role of GBS strains in affecting animal and human health. For instance, GBS ST861, which was isolated from salmon in this study , was also isolated from a clinical case in the same year in Singapore on the basis of metadata available in the PubMLST _S. agalactiae_ database .\n\n【36】In addition to the finding of GBS ST283 in freshwater fish, detection of high SPCs and _E. coli_ and _S. aureus_ counts indicates the hazard of using such fish for preparing raw RTE dishes. Because _E. coli_ is not part of the intestinal flora of cold-blooded animals , its presence suggests contamination from polluted water, unhygienic handling, or temperature abuse after harvesting. Similarly, because _S. aureus_ is not part of usual fish flora, its presence on fish surfaces suggests possible transfer of human skin flora caused by unhygienic handling . We detected _V. parahaemolyticus_ , an organism known to grow well in seawater but lyse rapidly in freshwater , in freshwater fish samples from fresh produce markets (5.1%, 2/39). This finding was not surprising because freshwater and saltwater fish are typically sold, handled, stored, and degutted within the same confined areas in fresh produce markets. Thus, despite lower SPCs and positive rates for _E. coli_ in saltwater fish than in freshwater fish from fresh produce markets, saltwater fish procured from such environments are prone to cross-contamination.\n\n【37】Fish used by food stalls were generally obtained from such markets. Moreover, microbial counts for sliced fish samples from eating establishments indicated that most food stalls were not able to prepare RTE raw fish dishes of acceptable hygiene quality. Poor practices observed included use of common chopping boards, knives, or slicers for preparing fish slices meant for raw consumption and cooking. If fish slices are contaminated, rinsing with water cannot improve their quality .\n\n【38】In contrast to the quality of saltwater fish samples from fresh produce markets, all saltwater fish samples from sashimi suppliers complied with local SPCs (5 log 10  CFU/g) and _E. coli_ (1.3 log 10  CFU/g) limits for ready-to-eat food ; all samples were negative for GBS, _S. aureus_ , _Salmonella_ spp. and _V. parahaemolyticus_ . The compliance rate among restaurants/snack bars was higher because such premises are more likely to procure fish from sashimi suppliers that harvest fish from cleaner waters and adhere to stricter cold chain management practices. However, some saltwater fish samples from sashimi suppliers and restaurants were found to contain _Aeromonas_ spp. (47.6%, 10/21) and _L. monocytogenes_ (2.0%, 6/303), whose psychrotrophic nature has posed a challenge to the fish industry. _L. monocytogenes_ is also a concern in chilled RTE food because of its ubiquity and persistence in food-processing environments .\n\n【39】Food and environmental findings of our study were consistent with epidemiologic findings for this outbreak . Multivariate analyses of a case–control study showed that persons who had consumed _yusheng_ were more likely to acquire GBS ST283 infections than those who had not consumed _yusheng_ . However, there was no strong association between GBS ST283 infections and consumption of sashimi and sushi .\n\n【40】Findings of this study have led to implementation of new policies in Singapore. These new policies included banning the use of freshwater fish in RTE dishes and requiring procurement of saltwater fish from suppliers for raw fish approved by the Agri-Food and Veterinary Authority of Singapore. Food stalls and food establishments providing catering services were required to stop selling RTE raw fish dishes until they complied with practices required for preparing RTE raw saltwater fish dishes.\n\n【41】The number of RTE fish samples collected from food stalls was limited because eating establishments were advised to stop the sale of RTE raw fish dishes containing Asian bighead carp and snakehead during July 24–December 5, 2015, while the outbreak investigation was underway . Sampling was not random because it was part of an outbreak investigation, but it was biased toward fish species and food stalls implicated in the outbreak. Thus, contamination rates might not reflect contamination rates of all fish species sold for raw consumption in the market. Similarly, testing of samples from ports and retail outlets was performed by using different protocols, which limited comparisons that could be made.\n\n【42】In conclusion, we detected GBS ST283, which caused a severe foodborne outbreak in Singapore in 2015, in freshwater fish, not only in food stalls and fish markets, but also in ports from which fish are imported. Comparison of human and fish isolates showed as few as 0–2 SNPs between human and fish isolates of GBS ST283 on a background of a diversity of GBS and other bacteria in freshwater fish. These data indicate the risk for contamination of raw freshwater fish and underscore the need for proper sourcing and handling of all fish for raw consumption. To control the outbreak, a ban on the sale of RTE raw freshwater fish dishes was implemented, and additional requirements were imposed for the sale of RTE raw fish dishes made with saltwater fish . Our study complements the epidemiologic findings for this outbreak  and illustrates the need for public health authorities and industries to remain vigilant regarding emerging pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "17f6ecaf-7962-475c-8818-03279485d6d0", "title": "Primary Liver Abscess Caused by One Clone of Klebsiella pneumoniae with Two Colonial Morphotypes and Resistotypes", "text": "【0】Primary Liver Abscess Caused by One Clone of Klebsiella pneumoniae with Two Colonial Morphotypes and Resistotypes\nPrimary liver abscess caused by a single pathogen, _Klebsiella pneumoniae_ , has long been an important infectious complication in diabetic patients in Taiwan . All _K. pneumoniae_ strains causing primary liver abscess have a unique antimicrobial susceptibility pattern (resistant to ampicillin, ticarcillin, and carbenicillin but susceptible to other antibiotics including all cephalosporins and aminoglycosides) . Although multidrug-resistant strains of _K. pneumoniae,_ whether community acquired or nosocomial, are not uncommon in Taiwan and other countries, these isolates had never been reported as the cause of primary liver abscess .\n\n【1】### Case Summaries\n\n【2】A 42-year-old man (patient A) was admitted on July 31, 2000, for a fever of 2 weeks’ duration. He had had diabetes mellitus for 20 years and alcoholism for >10 years. Physical examination and abdominal echography showed hepatomegaly and a huge abscess (12 cm x 10 cm x 8 cm) over the right lobe of the liver. Laboratory tests showed leukocytosis (14,600/μL) with a left shift. After blood cultures, ceftriaxone (2 g every 12 hours) was given. A “pigtail” catheter was inserted for continuous drainage on the 5th hospital day. Abscess aspirate culture yielded _K. pneumoniae_ with two colonial morphotypes (isolates on trypticase soy agar plates supplemented with 5% sheep blood \\[BBL Microbiology Systems, Cockeysville, MD\\] after 24 hours of incubation in ambient air)  and two resistotypes (by the routine disk diffusion method). One (isolate A1) had mucoid, opaque colonies and was resistant to ampicillin but susceptible to cefazolin and cefoxitin, and the other (isolate A2) had nonmucoid, white colonies and was resistant to ampicillin, cefazolin, and cefoxitin. Both isolates were susceptible to amoxicillin-clavulanate and cefotaxime. Blood cultures were negative. Because of persistent fever, the antibiotic was changed to imipenem (500 mg every 6 hours) on the 11th hospital day. Fever subsided 3 days after imipenem administration was begun. Imipenem was continued for a total of 24 days, followed by ciprofloxacin (750 mg every 12 hours) for 3 weeks. A follow-up echography 4 months after antibiotic treatment ended showed that the abscess had disappeared.\n\n【3】A 66-year-old man (patient B) with diabetes mellitus was admitted on August 4, 2000, with fever and hiccups of 4 days’ duration. Physical examination was unremarkable. Laboratory tests showed elevated levels of alkaline phosphatase (585 U/L; reference range 66-220 U/L) and gamma-glutamyltranspeptidase (301 U/L; reference range <52 U/L) but normal levels of aspartate and alanine aminotransferases. Abdominal echography and computed tomography revealed an abscess (6 cm x 4 cm x 4 cm) over the right lobe of the liver and a gallbladder stone. Cefoxitin (2 g every 8 hours) was given intravenously. The abscess was aspirated on the 4th day after hospitalization. Two isolates (isolates B1 and B2) of _K. pneumoniae_ with different colonial morphotypes and resistotypes (as in isolates A1 and A2, respectively, from patient A) were found in one aspirate culture, and two isolates (isolates B3 and B4) of the same species with different phenotypes (as in isolates B1 and B2, respectively) were recovered from one set of blood cultures, performed at admission. Cefoxitin was discontinued, and cefotaxime (2 g every 6 hours) was administered. His fever resolved 2 days after cefotaxime administration was begun. The patient received cefotaxime for 15 days, followed by oral cefixime (200 mg every 12 hours) for 7 weeks. Follow-up echography showed the abscess had disappeared.\n\n【4】In vitro susceptibilities of 14 antimicrobial agents for these isolates were determined by using the standard agar dilution method  . Biotyping of thee isolates was performed with the API ID32 GN system (bioMerieux, Marcy I'Etoile, France). Random amplified polymorphic DNA (RAPD) patterns of the six isolates were determined by means of arbitrarily primed polymerase chain reaction, as described in our previous report  . A total of four primers were used: M13 (5'-TTATGTAAAACGACGGCCAGT-3'), OPH2, OPA3, and OPA9 (Operon Technologies, Inc. Alameda, CA). Pulsotypes of these isolates were determined by pulsed-field gel electrophoresis; plasmid analysis and conjugative experiment were performed as described  . For comparisons, molecular typing of an additional two _K. pneumoniae_ isolates recovered from two patients seen at our hospital in 1999 were performed simultaneously.\n\n【5】The cefoxitin- and cefuroxime-resistant isolates (isolates A2, B2, and B4) showed two- to four-fold higher MICs of cefotaxime and ceftriaxone (MICs 0.5-1 μg/mL), ceftazidime (MICs 1 μg/mL), flomoxef (MICs 0.5-2 μg/mL), ticarcillin-clavulanate (MICs 8 μg/mL), piperacillin-tazobactam (MICs 4-8 μg/mL), cefepime (MICs 0.5-1 μg/mL), cefpirome (MICs 0.12-1 μg/mL), imipenem (MICs 2 μg/mL), and meropenem (MICs 0.12 μg/mL) than those of the cefoxitin-susceptible isolates (isolates A1, B1, and B3). All isolates were susceptible to the above antibiotics and aminoglycosides (gentamicin and amikacin).\n\n【6】As shown in the Table , the identity of biotypes, RAPD patterns with four primers, and pulsotypes  was clearly demonstrated for the isolates with two different phenotypes from each patient, suggesting that they belonged to a single clone in each patient (clone 1, isolates A1 and A2; clone 2, isolates B1 to B4). Different clones isolated from two patients seen within 1 week indicate that no outbreak occurred. Isolates A1 and A2 both had two plasmids (50 kb and 5 kb). Only one plasmid (30 kb) was found in each of the four isolates (B1 to B4) of patient 2. All these plasmids cannot transfer to _Escherichia coli_ C600.\n\n【7】### Conclusions\n\n【8】Although infection caused by a single clone of one bacterial species that simultaneously possessed two obviously different colonial morphotypes has been noted previously  , infection caused by a single clone of _K pneumoniae_ exhibiting two discrete colonial morphotypes and resistotypes has never been reported. Primary liver abscess due to _K. pneumoniae_ having resistance to second-generation cephalosporins (MIC up to 128 μg/mL for isolate B2 and 256 μg/mL for isolate B4 in patient B) and higher MICs of third-generation cephalosporins (MICs up to 1 μg/mL for isolate A2 in patient A) has also not been previously reported. We believe that this decreased in vitro susceptibility contributed to the unsatisfactory response to treatment with these agents.\n\n【9】The genetic basis for the resistance to cephalosporins and for the mucoid material synthesis by some gram-negative bacteria may be chromosomally determined or dependent on the acquisition of a specific plasmid or phage . The resistant and mucoid material synthesis may also be regulated by the environment in which the bacteria grow . The association of a specific plasmid with the presence of mucoid phenotype was not found in our isolates because both mucoid and nonmucoid strains of the two clones of _K. pneumoniae_ had identical plasmid profiles. The mechanisms of the coexistence of two phenotypically distinct isolates within a single clone _K. pneumoniae_ in abscess fluid, blood, or both should be investigated. The mechanism of cefoxitin resistance in our isolates was unclear because of the failure of transferability of the plasmids existing in our isolates to the susceptible recipient. Whether this resistance was chromosome mediated or caused by other mechanisms needs further investigations.\n\n【10】These two cases suggest that primary liver abscess caused by multiresistant strains of _K. pneumoniae_ in diabetic patients may be an emerging problem in Taiwan. The belief that only strains of _K. pneumoniae_ that are susceptible to cephalosporins cause primary liver abscess has now been disproved.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "14ed3eb9-246f-4339-aa4c-637e59e383ae", "title": "Multirecombinant Enterovirus A71 Subgenogroup C1 Isolates Associated with Neurologic Disease, France, 2016–2017", "text": "【0】Multirecombinant Enterovirus A71 Subgenogroup C1 Isolates Associated with Neurologic Disease, France, 2016–2017\nEnterovirus A71 (EV-A71) comprises 7 genogroups (A–G) and various subgenogroups (e.g. B0–B5, C1–C5) . B4, B5, and C4 viruses circulate mainly in Asia, and C1 and C2 viruses have been detected in Europe . In 2016, an upsurge in neurologic manifestations of enterovirus infection was reported in France . These cases were associated with an emerging lineage of subgenogroup C1 enteroviruses first reported in 2015 in Germany and later in Spain and 4 other countries  . Our aim was to obtain the full genomes of the viruses from the specimens collected in France and track down the origin of this emerging lineage, hereafter referred to as C1v2015.\n\n【1】### The Study\n\n【2】According to consolidated data recorded from the French Enterovirus Surveillance Network, 77 laboratory-confirmed cases of C1v2015 infection occurred during March–October 2016; in comparison, 136 EV-A71 infections of all genogroups combined were recorded during 2010–2015. The C1v2015 cases were widespread throughout France and associated with various clinical manifestations, including meningitis, cerebellitis, encephalitis, and myelitis, as well as hand, foot and mouth disease (HFMD) . One fatal case resulted from HFMD and cardiorespiratory failure. We analyzed 32 clinical specimens available from 25 patients reported as having a C1v2015 infection in 2016 and 2017 . Specimens and clinical data were collected during routine clinical work-up and epidemiologic surveillance, and patient data were deidentified before this study was conducted. The study was approved by the review board Comité de Protection des Personnes Sud-Est VI  in Clermont-Ferrand, France. The study population comprised 16 hospitalized children (median age 0.1 years), 4 children seen via ambulatory care (median age 1.8 years), and 5 children with asymptomatic infection (median age 1.4 years) in a childcare facility placed under community surveillance. We obtained the complete genomes, including the full 5′ and 3′ untranslated regions (UTRs), of 18 of 20 specimens and partial genomes of 2 of 20 specimens (2,893-nt and 4,380-nt long) acquired from 18 children  . We also determined the genomes of 12 isolates recovered during routine enterovirus surveillance to investigate their genetic relationships with C1v2015 ; we selected these viruses on the basis of previous exploratory investigations of their partial sequences .\n\n【3】We performed whole-genome sequence analyses as previously described  to identify which viruses were the closest relatives of C1v2015. The C1v2015 genome appears to be a mosaic comprising 4 modules defined by distinct patterns of similarity possibly arising through recombination . The nucleotide similarity patterns for module 2 (genomic region P1 comprising 4 capsid protein genes) suggest this region was inherited en bloc from an earlier subgenogroup C1 lineage. We used genomic region P1 to determine the evolutionary relatedness between C1v2015 and earlier C1 viruses and to date when the upsurge of C1v2015 infections began in Europe . All C1v2015 viruses clustered in a lineage distinct from that comprising the C1 viruses reported during 1991–2010. The nucleotide substitution rate of C1v2015 (5.2238 \\[95% highest probability density HPD interval 4.124–6.3737\\] × 10 –3  nt substitutions/y) and earlier C1 lineages (4.6302 \\[95% HPD interval 4.1769–5.1353\\] × 10 –3  nt substitutions/y) was similar. All of the P1 sequences from these viruses, except that of the virus from patient 14, had a maximum nucleotide sequence difference from each other of 2%; the P1 sequence of the virus from patient 14 differed from that of other C1v2015 viruses by 4.8%. The close genetic relatedness between the C1v2015 sequences reported during 2015–2017 in France, Germany, Japan, and the United States was indicative of rapid widespread transmission. We estimated that interpersonal transmission of this lineage began during 2009–2011  and that its spread was sustained during 2013–2014, just 1–2 years before C1v2015 was first reported. The most recent common ancestor between C1v2015 and earlier C1 viruses was dated to 2000–2002. Seven EV-A71 subgenogroup C1 viruses from Africa and Europe were located at the base of the C1v2015 lineage , suggesting that the C1 strain involved in the emergence of C1v2015 was circulating in this region during the 2000s.\n\nThe C1v2015 genomic module 4 comprises the entire 3Dpol gene and has a 90%–95% nucleotide similarity with 4 distinct EV-A genomes: coxsackievirus A2 (CV-A2) and CV-A5 from Russia, CV-A4 from China, and CV-A6 from Turkmenistan . We performed another phylogenetic analysis to assess the temporal origin of C1v2015 using this module. With the 3Dpol phylogenetic analysis, we estimated that C1v2015 began spreading in 2010–2014 , an estimate similar to that calculated with the P1 phylogeny. The nucleotide substitution rates with this analysis were also similar (C1v2015 3.7689 \\[95% HPD interval 1.3003–6.5838\\] × 10 –3  nt substitutions/y and C1 3.6318 \\[95% HPD interval 1.6064–6.2072\\] × 10 –3  nt substitutions/y). Whole-genome sequencing analysis showed that the isolate from patient 14 (14|COC286037|FRA|2016) shared distinct 3Dpol genes with other C1v2015 viruses . Overall, data indicate that the virus from patient 14 was an early recombinant of the C1v2015 lineage .\n\n【5】Within genomic module 1 (5′ UTR, first 600 nt), we found areas of moderate nucleotide similarity (90%–95%) between the C1v2015 genome and the CV-A6 and CV-A8 genomes and lower similarity (<88%) with the EV-A71 subgenogroup C1 genomes . The C1v2015 5′ UTR was therefore inherited from an EV-A lineage virus but not from the C1 ancestors that provided the capsid region. The pattern of sequence variation in the 5′ UTR precludes the possibility of analysis with a molecular clock.\n\n【6】The genomic module 3 of C1v2015 had low similarity with all the publicly available EV-A genomes; thus, the precise origin remains unknown . The highest nucleotide similarity scores (<90% with CV-A5 genomes) indicate only a distant genetic relationship. We conclude that genes 2A (except the 5′ terminus), 2B, 2C, and 3A–3C were transferred into the C1v2015 genome from a previously unreported lineage.\n\n【7】### Conclusions\n\n【8】Thirty years after the outbreaks in central Europe , the 2016 upsurge of infections is a reminder that EV-A71 is of growing public health concern. After the B5 and C4 subgenogroup upsurges, C1v2015 is the latest example of an emerging recombinant EV-A71 associated with neurologic manifestations. Recombination, which frequently occurs in enteroviruses, is considered a factor driving this viral emergence . Compared with earlier circulating lineages of EV-A71, C1v2015 is a multirecombinant that arose through complete shuffling of all nonstructural genomic regions, although the capsid genes are phylogenetically typical of C1 viruses. Shuffling involved \\> 2 recombination events with EV-A genomes before the emergence of C1v2015 as a life-threatening pathogen . From a public health perspective, the spread of C1v2015 could have resulted from acquired genomic features, notably a unique combination of the 5′ UTR and 3Dpol gene, because recombination events clearly preceded the extensive circulation of C1v2015. The mosaic structure of the genome indicates that C1v2015 is an integral part of a large recombination network including multiple EV-A viruses transmitted in Eurasia. Given the propensity of enteroviruses to recombine their genomes and spread rapidly across distant countries  and that C1v2015 circulation continued throughout 2017 and 2018 in France, we need to determine if this virus is associated with a long-term recurrent risk for severe disease in the pediatric population through sharing data from global surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1893f40e-9e23-4678-bdcc-651f15df0e22", "title": "The Importance of Natural Experiments in Diabetes Prevention and Control and the Need for Better Health Policy Research", "text": "【0】The Importance of Natural Experiments in Diabetes Prevention and Control and the Need for Better Health Policy Research\nDiabetes has steadily increased in prevalence, becoming one of the nation’s most challenging public health threats . Prevalence among adults is now more than 10%, and diabetes is the leading cause of nontraumatic lower-extremity amputation, end-stage kidney disease, and blindness; it more than doubles the risk of heart disease, stroke, and disability . Strong clinical trial evidence indicates that much of the illness caused by diabetes is preventable, further positioning diabetes as a public health priority  and stimulating a national emphasis on the quality of diabetes care and self-management . Although many such efforts have been successful, leading to better care, risk factor control, and reduced risk of complications, new challenges have arisen. The increases in obesity and in diabetes incidence demand that health systems and communities apply primary prevention strategies at the population level while simultaneously tackling the pervasive geographic and socioeconomic disparities in diabetes prevalence, care, and complications that remain .\n\n【1】Compared to the long list of clinical best practices to prevent diabetes complications, the evidence base is thin for population- and policy-level approaches to improve health behaviors, access to and delivery of care and preventive services, and the healthful attributes of communities. This imbalance of evidence calls for a new platform of public health research for diabetes. We contend that the imbalance can be corrected by a greater emphasis on natural experiments: rigorously designed quasi-experimental studies to investigate the health effects of naturally occurring population- and policy-level approaches emanating from health systems, communities, business organizations, and governments.\n\n【2】The gaps in evidence for naturally occurring population- and policy-level approaches have not resulted from a lack of such approaches. Numerous large-scale initiatives and health-related services to reduce the risk and consequences of diabetes are taking place. Employers, health plans, health systems, and communities regularly embark on screening and wellness programs and quality-improvement programs for entire populations; state and local governments have proposed or implemented policies such as taxes on unhealthful foods, vouchers for lifestyle and community programs, or restrictions on the way social services can be used. To remain competitive in a nation where large employers and government are the dominant purchasers of health insurance, health plans frequently develop new reimbursement and benefit designs that influence patterns of services provided to large populations. Finally, national and state legislatures adopt laws that fundamentally affect the access to and delivery, quality, and costs of care and preventive services for people at risk for or diagnosed with diabetes. By 2014, features of the Affordable Care Act of 2010 are likely to change access to services and quality of care, particularly for people who were previously uninsured.\n\n【3】The gaps in evidence for naturally occurring population- and policy-level approaches have resulted from a lack of rigorous health policy research: the objective, critical examination and evaluation of the benefits and drawbacks of such approaches. Health policy studies have typically lacked control conditions, which has limited the ability to distinguish between policy effects and secular trends and gauge true effectiveness . Randomized controlled trials establish causality and quantify efficacy under ideal conditions but are often impractical for the study of health policies in a complex world. Instead of seeking more rigorous nonrandomized alternatives, health policy research has frequently settled for cross-sectional or noncontrolled alternatives that lead to ambiguous or misleading conclusions.\n\n【4】Responding to both the need and opportunity for better health policy research for diabetes, the Centers for Disease Control and Prevention (CDC) and the National Institute of Diabetes and Digestive and Kidney Diseases has initiated a multicenter research network: Natural Experiments in Translation for Diabetes, or NEXT-D. The mission of NEXT-D is to examine the effectiveness of population-level health policies on diabetes prevention, control, and inequalities through rigorous health policy research. A collaborative approach was chosen because it facilitates multisite studies and the use of common measurements and indicators. Collaboration will also enhance the design, analysis, and dissemination of translational research. The ultimate goal of the collaboration is to provide stakeholders with a clear understanding of best practices that can be implemented by employers, health plans, health systems, communities, legislatures, or governments to prevent and control diabetes.\n\n【5】NEXT-D studies are also intended to inform the priorities of the CDC-funded Diabetes Prevention and Control Programs (DPCPs) in 58 state and territorial health departments . DPCPs bring together diverse stakeholders to implement population-based interventions to improve diabetes risk factors, control, and disparities and to drive state and territorial progress toward national public health objectives. Innovative DPCP strategies that are similar across states could become candidates for NEXT-D evaluations. Conversely, several components of the NEXT-D portfolio of natural experiments may have important implications for DPCPs, including diabetes care quality improvement, access to self-management education, access to lifestyle-based diabetes prevention programs, and healthy food environments in communities.\n\n【6】Articles in this _Preventing Chronic Disease_ collection describe the NEXT-D natural experiments now under way — their rationale and importance, their design, and their intended effects. The objective of this collection is to share expertise and methods for addressing the complexity of real-world data. We hope to stimulate others to embark on and publish studies on natural experiments.\n\n【7】The NEXT-D studies have several attributes that will enhance their effect on diabetes health research and policies. First, interventions are being implemented naturally (ie, not for research purposes), and they take place among health systems, insurers, employers, the private sector, communities, and government agencies, each of which reaches a large population. As a result, study investigators do not use their own research funds for implementation, and interventions have high external generalizability. Second, the NEXT-D studies span several major public health themes, including the design of health care benefits, clinic–community partnerships, adoption of health information technology, and employer-based initiatives to screen and prevent diabetes. Third, the studies use longitudinal, controlled study designs involving diverse populations and rigorous analytic methods that aim to distinguish between policy effects and underlying trends. Fourth, through close partnerships with the organizations that implement these interventions in real-world settings, the NEXT-D studies will help to eliminate barriers to sustaining and disseminating approaches that are found to be effective at preventing and improving care for people who have diabetes. Fifth, by working in partnership with private sector and public policy decision makers, NEXT-D research teams can identify and analyze outcome indicators that are most informative (ie, provide actionable evidence) to those decision makers. Finally, the studies encompass primary and secondary prevention and complementary, nonredundant approaches. This new platform of public health research for diabetes — natural experiments — will fill the gaps in evidence for population- and policy-level approaches, correct the imbalance in the evidence base between clinical best practices and population- and policy-level approaches, and ultimately help to reduce the burden of diabetes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "539a3991-5b8f-4b71-b96c-c700258a3c06", "title": "School Closures and Student Contact Patterns", "text": "【0】School Closures and Student Contact Patterns\nDuring pandemic (H1N1) 2009, several countries closed schools  to slow virus transmission. The effects of such school closures on student contact patterns have not been directly quantified. We report these effects for students from a UK secondary school.\n\n【1】### The Study\n\n【2】We retrospectively surveyed 128 students at a coeducational, state secondary school in an urban area of West Midlands, UK, where attack rates for pandemic (H1N1) 2009 were high and (as of March 2010) levels of unemployment were among the highest in Great Britain . The head teacher selected 1 class from each of years 7–10 (equivalent to US grades 6–9, student ages 11–15 years) to participate. The school had closed for 1 week in mid-June 2009, reopened for 2 days, then closed for another week. Questionnaires were completed during class ≈2 weeks after the school reopened the second time. An electronic version of a similar questionnaire pilot tested at another school had been found comprehensible and acceptable to participants. The London School of Hygiene and Tropical Medicine ethics committee approved the study; the Health Protection Agency approved it as part of wider outbreak investigations not requiring additional approval.\n\n【3】Students reported how many times they visited specified public places before the school closure and how many times they visited these places during closure (children had been advised to not visit public places only if they were symptomatic). Students also provided information about persons who looked after them during closure.\n\n【4】For typical school days before and during closure, students reported the number of different persons spoken to (contacted) in the following groups: contacts who attended their school (contacts from the same class \\[classmates\\], the same year but a different class \\[yearmates\\], and the same school but a different year \\[schoolmates\\]) and others (age stratified to reflect the UK school system). Students were asked whether they were ill during closure and whether being ill affected their contact patterns.\n\n【5】Questionnaires were returned by 107 (84%) of 128 students. Approximately 100 students (range 99–103, depending on place visited) stated how frequently they would visit public places while school was open, and 46 stated how many times they visited these places during school closure; 45 (98%) of 46 visited \\> 1 place. Fewer students visited shops, places of worship, parks, and playing fields at least 1×/week when school was closed than when open . For other places, frequency of visits did not differ.\n\n【6】Among those who provided information about caregivers, 93 (95%) of 98 reported that \\> 1 adult looked after them during school closure; 49% reported having 2 caregivers (range 1–5). Among caregivers for whom further information was available, 125 (69%) of 182 would have seen the student on a typical school day, 54 (31%) of 173 typically worked outside the home, and 12 (34%) of 35 took time off work to care for the student during school closure.\n\n【7】Among students, 73 provided number of contacts on a typical day during school closure; 35 also provided information for a typical school day, and another 6 only provided information for a typical school day. We therefore conducted unpaired and paired analyses on data from 79 and 35 respondents, respectively. Students who provided contact data were most likely to be in years 7 or 9 but were otherwise similar to those who did not.\n\n【8】The mean totals of reported contacts were 70.3 (SD 40.8) and 24.8 (SD 22.5) during typical school days and closure, respectively . School closure was therefore associated with a reduction of 45.5 (95% confidence interval \\[CI\\] 33.8–57.2) in students’ typical daily number of contacts, a 65% relative reduction (95% bootstrap CI 52%–73%). The corresponding absolute and relative reductions in numbers of contacts with other students were 37.0 (95% CI 27.0–46.9) and 65% (95% bootstrap CI 52%–74%), respectively. The absolute and relative reductions in the numbers of contacts made with adults (including teachers) were 8.5 (95% CI 4.9–12.1) and 63% (95% bootstrap CI 45%–75%), respectively. No apparent change was found for number of contacts with adults outside school (34%, 95% bootstrap CI –6% to 63%).\n\n【9】The greatest reductions in the numbers of contacts were for students from the same school , e.g. ≈80% reduction in numbers of contacts with classmates and yearmates. Absolute reductions in numbers of contacts with persons not attending the school were small; the relative reductions had wide confidence intervals and rarely showed evidence of a genuine reduction . Paired analysis of data for 35 students with information for contacts during both periods produced similar results as unpaired analysis. Among 40 respondents who reported illness during closure and self-assessed whether they consequently contacted fewer persons, 53% stated that their contacts were reduced, 33% stated that they were not, and 15% were unsure.\n\n【10】### Conclusions\n\n【11】Closing this school was associated with a 65% reduction in face-to-face conversational contacts made by secondary school students, primarily because of reductions in contact with students from the same school. Our estimated reductions exceed estimates from analyses of surveillance data for seasonal influenza-like illness in France (24% reduction in child-to-child transmission during school holidays compared with in-school days)  and a study conducted in Belgium (19% reduction in total contacts made by children and adolescents during Easter holidays) . Our estimate of a 65% reduction in total contacts is similar to that from a survey at a primary school in Germany, in which students reported 72% fewer contacts on Sundays than on weekdays but in which all classmates were considered contacts . Consistent with findings of other studies , most students visited public places during closure, although certain places were visited less frequently while the school was closed than when open.\n\n【12】Our study has several limitations. Our definition of contact excluded nonconversational contacts (e.g. passengers on public transport), which may enable transmission, and some conversations may not involve close contact. We did not collect data about duration or intensity of contact or whether persons were contacted multiple times. Our use of a typical day does not capture variation in student behavior.\n\n【13】For logistical reasons, a 2–3 week delay occurred between school reopening and completion of questionnaires, providing potential for recall bias and underestimation of numbers of contacts during school closure (although closure was unusual). Prospective data collection was impossible and has limitations, including greater effort required from participants and therefore potentially lower response rates. The data refer to a convenience sample from 1 secondary school during what was often perceived as a mild pandemic and may not be generalizable to other situations (e.g. primary schools, different socioeconomic settings, infections with high case-fatality rates, or different seasons).\n\n【14】Most students provided data only for the closure period, and few did so for a typical school day (probably because of the order of questions). The primary analysis therefore ignored the pairing in the data. Ignoring the pairing would not affect point estimates but would reduce their precision. Paired analysis of 35 students who provided data for both periods produced similar results to the unpaired analysis.\n\n【15】Other issues must also be considered when deciding whether to close schools . Subject to the limitations described above, reactive school closures may substantially reduce the numbers of contacts made by students and may potentially reduce transmission of infection in some settings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35aec85d-e565-4359-a3fc-124d675e5052", "title": "Intrafamilial Exposure to SARS-CoV-2 Associated with Cellular Immune Response without Seroconversion, France", "text": "【0】Intrafamilial Exposure to SARS-CoV-2 Associated with Cellular Immune Response without Seroconversion, France\nCoronavirus disease (COVID-19), caused by infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is a pandemic that raises a major concern all around the world . To contain the spread of the virus, several countries have imposed population lockdowns . In France, the first cases of COVID-19 were recorded at the end of January 2020 . Due to the rapid increase of new cases and death, a lockdown was imposed during March 17–May 11, 2020. After the lifting of the lockdown, the number of new cases of SARS-CoV-2 decreased substantially. However, we cannot exclude the possibility that a second pandemic wave could occur; an increase in new cases had already been observed in the first week of August 2020 in several regions .\n\n【1】Estimating infections with immunizing effects is crucial in helping to predict the postpandemic dynamics of the virus . Serologic tests for SARS-CoV-2 have been developed to determine the extent of immunity to the virus , and immunity certifications based on the results of these tests have been considered by some countries in Europe and by the US government. Several persons belonging to households with an index COVID-19 patient reported symptoms of COVID-19 but remained seronegative even though the index patient practiced no quarantine measures. The absence of antiviral antibodies after exposure has been previously reported for other viral infections. In these cases, the presence of virus-specific T-cell responses provided proof of viral transmission . In this study, we investigated humoral and cellular responses to SARS-CoV-2 in 11 serodiscordant couples in whom 1 partner had evidence of mild COVID-19 and in 10 unexposed healthy blood donors (controls). We also explored the T-cell response against 2 human coronaviruses (HCoV) that cause common colds, given the potential cross-reactive immunity between SARS-CoV-2 and common cold HCoVs.\n\n【2】### Materials and Methods\n\n【3】##### Study Participants\n\n【4】We included in the study 11 couples in whom 1 of the 2 partners met clinical, epidemiologic, and laboratory criteria for a mildly symptomatic confirmed COVID-19 case. We collected blood samples from both partners of each couple during May 7–June 26, 2020. Ten healthy blood donors who had not been exposed to COVID-19 patients and who had tested negative for SARS-CoV-2 antibodies were enrolled as controls. All participants gave written informed consent for research according to protocols approved by the institutional review board of Strasbourg University Hospitals .\n\n【5】##### SARS-CoV-2 Reverse Transcription PCR\n\n【6】We performed in-house real-time reverse transcription PCR (rRT-PCR) tests for SARS-CoV-2 nucleic acid on samples from nasopharyngeal swab specimens collected during the symptomatic phase from 8 index patients and 3 contacts. Primer and probe sequences target 2 regions of the RdRp gene and are specific to SARS-CoV-2. Assay sensitivity is ≈10 copies/reaction .\n\n【7】##### Serologic Tests\n\n【8】We used 3 serologic assays to detect the presence of SARS-CoV-2 antibodies. The Abbott Architect SARS-CoV-2 IgG assay  is a chemiluminescent microparticle immunoassay for detecting IgG against the SARS-CoV-2 nucleoprotein and has sensitivity and specificity close to 100% . The EUROIMMUN SARS-CoV-2 assay  is an ELISA for detecting IgG and IgA against the SARS-CoV-2 S1 domain of the spike glycoprotein, including the immunologically relevant receptor-binding domain. This assay was reported to have a clinical specificity of 98% for IgG and 91% for IgA detection, with a maximal sensitivity reached after 28 days after symptom onset (IgG 98% and IgA 95%) . The Biosynex COVID-19 BSS assay  is a lateral flow assay for detecting IgM and IgG directed against the SARS-CoV-2 receptor-binding domain of the spike glycoprotein and has a sensitivity of 95.6% and a specificity of 99.4% . All 3 assays were approved by the French National Agency of Medicine and Health Products Safety for their excellent analytical performances. All tests were performed according to manufacturer instructions.\n\n【9】##### interferon-Gamma Enzyme-Linked Immunospot Assay\n\n【10】We investigated T-cell immune response against SARS-CoV-2 by performing an interferon-gamma (IFN-γ) enzyme-linked ImmunoSpot ELISPOT assay  in duplicate on fresh peripheral blood mononuclear cells (PBMC) isolated from heparin-anticoagulated blood. PBMCs were seeded at 200,000 CD3+ cells/well after dilution according to measurement of CD3+ cell frequencies by flow cytometry. They were stimulated for 20 \\+ 4 h with overlapping 15-mer peptide pools used at a final concentration of 1 µg/mL and spanning the sequences of the N-terminal portion of the SARS-CoV-2 spike glycoprotein (pool S1, amino acid residues 1–643) and the C-terminal part of the same protein (pool S2, amino acid residues 633–1273), the nucleoprotein (N), the membrane protein (M), the envelope small membrane protein (E), and the accessory proteins 3A, 7A, 8 and 9B .\n\n【11】To investigate the possibility of preexisting cross-reactive coronavirus-specific T cells, PBMCs were stimulated in parallel with peptide pools spanning the spike glycoprotein sequences of HCoV-229E (ES1 and ES2) and HCoV-OC43 (OS1 and OS2). Phytohemagglutinin (PHA) was used in duplicate as a positive control and culture medium in quadruplicate as a negative control. After colorimetric revelation of IFN-γ capture , spots were counted using an ELISPOT reader . For each condition, the mean number of spot-forming cells per million CD3+ cells was calculated from duplicates after subtraction of the background value obtained from negative controls to determine the frequency of antigen-specific T cells. The threshold defining T-cell reactivity for 1 antigen was set at >3 SD of the negative control background. The SARS-CoV-2-specific T-cell response was considered positive if analysis showed reactivity for \\> 3 SARS-CoV-2 antigens.\n\n【12】### Results\n\n【13】The median age of the 11 couples was 49 years (range 38–65 years); 11 (50%) were male . Partners who met the confirmed case definition of COVID-19 (positive for SARS-CoV-2 by RT-PCR or serology or both) were the first to report symptoms in each couple and were considered index patients (P). Because of the lockdown from March 17 to May 11, 2020, each couple stayed in the same household during this period. Therefore, the partner of each index patient was considered a close contact (C) as defined by the US Centers for Disease Control and Prevention (CDC).\n\n【14】During March 2–April 9, all index patients reported histories of \\> 1 symptoms: 8 had fever, 6 had cough, 4 had fatigue, 8 had headache, 8 had anosmia, 7 had ageusia, 3 had dyspnea, and 3 had myalgia . We tested 8 of these patients for SARS-CoV-2 using RT-PCR on nasopharyngeal samples; results for 7 were positive . The duration of symptoms varied (2–21 days, median 10 days). During this symptomatic phase, all couples rigorously washed their hands, and each avoided hugs and kisses with his or her partner except couple 2. Nine of the 11 couples slept in the same bed. Only 2 index patients, P4 and P6 (i.e. the index partners from couples 4 and 6), quarantined themselves by eating and sleeping separately or wearing a mask or both for 1 day (P4) and 3 days (P6) after symptom onset.\n\n【15】We performed serologic testing for SARS-CoV-2 antibodies in index patients at a median of 68 days (range 49–102 days) after symptom onset. All displayed IgG against the SARS-CoV-2 N protein, the spike glycoprotein, or both, as indicated by the 3 serologic assays , confirming the persistence of the SARS-CoV-2 antibodies for up to 102 days after symptom onset. Results of tests for SARS-CoV-2 IgA were positive for 7 of the 11 index patients .\n\n【16】Six of the 11 contacts (C1, C2, C4, C5, C7, and C8) experienced symptoms 1–10 days after symptom onset in their partners . We tested 3 of them for SARS-CoV-2 RNA by RT-PCR on samples from nasopharyngeal swab specimens during the symptomatic phase; results for all were negative . Three had fever, 2 had cough, 2 had fatigue, 3 had headache, 1 had ageusia, 1 had dyspnea, and 1 had myalgia. The duration of symptoms varied (1–10 days, median 7 days) . We performed serologic testing for SARS-CoV-2 at a median of 59 days (range 44–93 days) after symptom onset in symptomatic contacts and at the same time as their partners for asymptomatic contacts. All the contacts, including the symptomatic ones, were SARS-CoV-2 seronegative for IgM, IgA (except 1 equivocal result), and IgG .\n\n【17】To investigate the SARS-CoV-2–specific T-cell response in the 11 couples, we collected fresh PBMC samples on the same day as the serum collections. We then stimulated the samples with 4 structural and 4 accessory SARS-CoV-2 proteins followed by IFN-γ ELISPOT analysis. All index and contact patients had normal lymphocyte counts . All index patients showed SARS-CoV-2–specific IFN-γ responses against 4–8 SARS-CoV-2 antigens . All of their immune systems recognized the structural proteins S1, S2, N, and M, and 9 of them recognized \\> 1 accessory protein (3A, 7A, 8, or 9B), showing that SARS-CoV-2–specific T-cell responses had developed . Blood samples were collected 49–102 days after symptom onset, which suggests that antiviral T cells are maintained for up to 102 days in patients having recovered from mild COVID-19.\n\n【18】We evaluated SARS-CoV-2–specific T-cell response in contacts at a median time of 59 days (range 44–93 days) after symptom onset in symptomatic contacts and at the same time as their partner for asymptomatic contacts. Among the 6 symptomatic contacts, 4 (C1, C4, C5, and C8) displayed a positive SARS-CoV-2–specific T-cell response with a reactivity to \\> 3 SARS-CoV-2 antigens . Contact C1 exhibited T-cell reactivity against 4 SARS-CoV-2 antigens, including 1 structural protein (S1) and 3 accessory proteins; contact C5 exhibited T-cell reactivity against 2 and C8 against 3 structural proteins (N, E, and S2 for C8) and the accessory protein 9B. Contact C4 exhibited T-cell reactivity against 1 structural protein (S2) and 2 accessory proteins. Although symptomatic contact C7 exhibited T-cell SARS-CoV-2–specific response against a single antigen (structural protein S1), the frequency of IFN-γ–producing T cells was higher than that observed in his partner (mean 353 \\+ 53 vs. 126 \\+ 25 spot-forming units/1 million cells). Symptomatic contact C2 and asymptomatic contacts C6, C9, and C10 exhibited a low frequency of T-cell reactivity against a single antigen (S2 = 2, E = 1, 9B = 1) that was not considered here as a positive specific T-cell response to SARS-CoV-2 . The asymptomatic contacts C3 and C11 showed no T-cell response against any of the SARS-CoV-2 antigens .\n\n【19】We included 10 unexposed HD as controls, with a mean age of 46 years (range 29–60 years). We confirmed their SARS-CoV-2 seronegative status with the 3 serologic assays. Five of them displayed low T-cell reactivity to SARS-CoV-2 against 1 or 2 antigens (S1, S2, M, 9B) .\n\n【20】A recent study demonstrated that several CD4 T cells reacting to SARS-CoV-2 epitopes were a result of a cross-reaction with corresponding homologous sequences from commonly circulating HCoVs including OC43 and 229E, which can cause common colds . To investigate if there was a correlation between T-cell responses against SARS-CoV-2 and common cold HCoVs, we tested the 11 couples and the 10 unexposed controls for reactivity against the spike glycoprotein (S1 and S2 regions) of HCoV-229E and HCoV-OC43. All but 1 HD (HD9) showed IFN-γ–producing T cells directed against these antigens . Eight index patients (P2, P3, P4, P5, P6, P7, P10, and P11), 7 contacts (C1, C2, C4, C8, C9, C10, and C11), and 7 controls displayed a positive T-cell response against both HCoV-229E and HCoV-OC43. Three index patients (P1, P8, and P9), 4 contacts (C3, C5, C6, and C7), and 2 controls displayed positive T-cell responses only against HCoV-229E. We found no correlation between the responses to S1 and S2 peptide pools of SARS-CoV-2 and HCoVs .\n\n【21】### Discussion\n\n【22】In this study, we demonstrate that intrafamilial contacts can display a SARS-CoV-2–specific T-cell response in the absence of seroconversion, especially when they have been symptomatic. This T-cell response provides evidence that transient or anatomically contained SARS-CoV-2 infection, or both, may have occurred and that T-cell responses would be more sensitive indicators of SARS-Co-V-2 exposure than antibodies.\n\n【23】Each couple stayed in the same household during the COVID-19 episode and the partners were in close contact for a long time due to the lockdown. Although 5 contacts were asymptomatic, 6 exhibited symptoms a median of 7 days after symptom onset in their partners, suggesting that at least those 6 were infected. However, results from neither RT-PCR nor serology testing using 3 different assays and targeting 2 different SARS-CoV-2 structural proteins were positive in contacts. In contrast, analysis of SARS-CoV-2–specific T-cell response showed a positive response against \\> 3 antigens, including structural proteins in 4 symptomatic contacts, strongly suggesting that they were infected with SARS-CoV-2.\n\n【24】Five unexposed controls and 1 symptomatic and 3 asymptomatic contacts exhibited low frequencies of SARS-CoV-2 IFN-γ–producing T cells. Because these 4 contacts were exposed to COVID-19 patients and the unexposed controls donated blood in April and May 2020, it is unclear whether the detectable T-cell responses were the result of cross-reactivity with common cold HCoV antigens, as previously reported  or of SARS-CoV-2 infection. Although recent research provided direct evidence of cross-reactivity between SARS-CoV-2 epitopes and common cold HCoVs , we observed no obvious relationship between the magnitude of T-cell responses against spike glycoproteins of common cold HCoVs and SARS-CoV-2 in index patients, contacts, and unexposed HD. In parallel with our findings, another recent study  reported finding memory T-cell response against SARS-CoV-2 structural proteins in exposed family members and healthy persons lacking detectable circulating antibodies who donated blood during the pandemic.\n\n【25】There are multiple explanations for virus-specific T cells developing without any antibody response. A study in a small cohort of patients  reported that 40% of asymptomatic and 12.9% of patients with mild COVID-19 no longer had antibodies 56 days after being discharged from the hospital. In our study, the serum samples were collected between 49 to 102 days after symptom onset, so it is possible that the contacts had lost their antibodies during this period. It is also possible that very low levels of antibodies that might have developed in contacts were not detected by the serologic assays we used. The lack of specific antibodies might also be because of exposure to low doses of the virus with brief and transient viral replication, to a downstream event of protective innate immune response, or to abortive replication of defective viral genomes .\n\n【26】Eventually, the presence of SARS-CoV-2-specific T-cell response, whether because of infection with SARS-CoV-2 or a cross-reaction, might explain the mild and rapidly resolved symptoms in index patients and symptomatic contacts and the resistance of other contacts to symptomatic SARS-CoV-2 infection. However, this possible explanation needs to be investigated further in a large cohort.\n\n【27】Our study is subject to several limitations. First, our findings suffer from a limited sample size, although this is a unique cohort, and it was not possible to increase the sample size. Second, because of the unavailability of PBMCs collected before the pandemic, we recruited unexposed HD who donated their blood during the pandemic as controls, so we cannot exclude a potential infection by SARS-CoV-2 before the enrollment in the study. Third, although we detected high frequencies of T-cell response against diverse SARS-CoV-2 proteins in symptomatic contacts lacking circulating antibodies, it remains possible that a part of this response may be a result of cross-reaction with common cold HCoVs.\n\n【28】Overall, our results indicate that persons exposed to SARS-CoV-2 may develop virus-specific T-cell responses without detectable circulating antibodies. This aspect of the immune response against SARS-CoV-2 contributes substantially to the understanding of the natural history of COVID-19. Furthermore, our data indicate that epidemiologic data relying solely on the detection of SARS-CoV-2 antibodies may lead to a substantial underestimation of prior exposure to the virus. Our data may also have implications for vaccine development and tracking the future evolution of the SARS-CoV-2 pandemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f66817ca-7d36-407e-85b8-deef9955c597", "title": "Naegleria fowleri in Well Water", "text": "【0】Naegleria fowleri in Well Water\n**To the Editor:** _Naegleria fowleri,_ a protozoon found in hot springs and warm surface water, can cause primary amebic meningoencephalitis in humans. A survey of drinking water supply wells in Arizona determined that wells can be colonized and may be an unrecognized source of this organism that could present a human health risk.\n\n【1】_N. fowleri_ is a free-living ameboflagellate found in warm bodies of water such as ponds, irrigation ditches, lakes, coastal waters, and hot springs and can cause primary amebic meningoencephalitis. Humans come into contact with _N. fowleri_ by swimming or bathing, particularly in surface waters. The ameba enters the nasal passages, penetrates the nasopharyngeal mucosa, and migrates to the olfactory nerves, eventually invading the brain through the cribriform plate . From 1995 to 2004, _N. fowleri_ killed 23 persons in the United States , including 2 children in the Phoenix, Arizona, area in 2002, who had been exposed to well water but had not consumed it . There have been 6 documented deaths in 2007, all in warmer regions (Arizona, Texas, Florida) .\n\n【2】Although _N. fowleri_ ’s presence in surface waters is well documented , no previous studies on its occurrence in wells have been conducted. We studied high-volume drinking water wells operated by municipal utilities or private water companies in the greater Phoenix and Tucson, Arizona, areas. Previous data from 500 wells in the region showed temperatures ranging from 13°C to 46°C. Typical well discharges ranged from hundreds to >3,780 L per minute. Well depths varied from 100 m to >300 m.\n\n【3】Well water samples were collected by using 1-L sterile polyethylene bottles at or near the wellhead before disinfection by well owners or utilities . In phase 1, samples were collected after wells were flushed until the water was clear. During phase 2, samples were collected as water was turned on from spigots at or near wellheads (initial) and after a 3-borehole volume had flushed through the system (purged). Additional wells were sampled during this phase. Samples were tested for temperature, pH, turbidity, chlorine residual, conductance, coliforms, heterotrophic bacterial plate counts (HPC), and _Escherichia coli_ following standard methods .\n\n【4】To test for viable amebas, we spread aliquots on nonnutrient agar seeded with _E. coli_ at 37°C . We placed scrapings from the advancing front of subsequent ameba plaques in distilled water to identify enflagellation ; however, precise species identification was not possible. Live amebae were therefore harvested for PCR analysis to specifically identify _N. fowleri_ . We chose PCR over the mouse pathogenicity test because other _Naegleria_ species that are nonpathogenic in humans are lethal in mice . The genotype of isolates was not determined because all of the described genotypes found in the United States have been shown to be pathogenic in humans .\n\n【5】To concentrate trophozoites/cysts, we gently agitated samples for 2 minutes and then centrifuged and filtered them through polyethylene filters (2-μm pore; Millipore, Bedford, MA, USA). A 10-μL volume of concentrate was used as a template for nested PCR  (triplicate tests were conducted immediately and after a 2-week 37°C incubation). Positive and negative PCR products were frozen at –80°C, coded to prevent bias, and shipped to Francine Marciano-Cabral at Virginia Commonwealth University for confirmation by cloning and sequencing .\n\n【6】The general microbial quality of the wells was as follows: 73 (51%) had >500 HPC/mL; 8 (5.5%) were positive for coliforms; none were positive for _E. coli_ . Oils used to lubricate well motors may result in the growth of HPC in well water . _N. fowleri_ feeds on heterotrophic bacteria in water and could multiply in the well casing. This may explain _N. fowleri_ ’s colonization of wells.\n\n【7】The recent association in Arizona between unchlorinated drinking water and the transmission of _N. fowleri_ suggests that groundwater has been an unrecognized source of this organism. PCR detected _N. fowleri_ DNA in 11 (7.7%) of 143 wells. Of 185 total samples, 30 (16.2%) tested positive for _N. fowleri_ . The organism was most often detected after the wells had been purged (17.9% purged vs. 10.0% initial samples), suggesting that _N. fowleri_ was present in the aquifer or was released from the well casing or column during pumping. The wells testing positive for _N. fowleri_ ranged in temperature from 21.9°C to 37.4°C (average 29.0°C; median 29.5°C).\n\n【8】The live trophozoite form was confirmed in only 1 well, though 11 of 143 wells tested positive according to PCR. This discrepancy may be due to the low occurrence of trophozoites in water or to differences in assay volumes for detection of live trophozoites (0.75 mL) versus PCR (30 mL equivalent unconcentrated volume). PCR is also more sensitive, capable of detecting 100 organisms/L in an unconcentrated sample ; however, PCR did not determine if the amebas were infectious. Although PCR can determine the species by using primers for a specific gene sequence not found in other _Naegleria_ species, it cannot determine the life stage (cyst/trophozoite). Trophozoites are believed to be the infectious form of the organism ; nonetheless, cysts can be equally harmful because they may revert to trophozoites under optimal conditions . The surprisingly common occurrence of _N. fowleri_ in drinking water wells suggests that groundwaters may be an unrecognized human health threat.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c063757-029b-48e6-b078-989aedb5aca0", "title": "Human Bocavirus Infection in Children with Gastroenteritis, Brazil", "text": "【0】Human Bocavirus Infection in Children with Gastroenteritis, Brazil\nHuman bocavirus (HBoV) was first identified in pooled human respiratory tract specimens from Swedish children in 2005 and was provisionally classified within the genus _Bocavirus_ of the family _Parvoviridae_ . Previously, the only parvovirus known to be pathogenic in humans was B19 virus, which is responsible for Fifth disease in children . Because HBoV was first found in respiratory specimens, most epidemiologic studies have focused on such specimens. Shortly after its first description in Sweden, HBoV was detected in respiratory tract specimens from patients with respiratory illness in several parts of the world .\n\n【1】Other members of the family _Parvoviridae_ that infect animals cause diseases such as leukopenia/enteritis syndrome, seen most commonly in dogs 8–12 weeks of age, with clinical features of vomiting, anorexia, lethargy, and diarrhea that lead to rapid dehydration . For this reason, we hypothesized that HBoV may play a role in human gastrointestinal disease. In this study, we retrospectively tested stool specimens collected from 2003 through 2005 from Brazilian children with acute diarrhea to investigate whether this virus can infect the human gastrointestinal tract and be detected in feces, and to assess the frequency of such infections.\n\n【2】### The Study\n\n【3】A total of 705 stool specimens from Brazilian children <15 years of age (median 3.5 years) with acute diarrhea were obtained from January 2003 through December 2005 and screened by PCR for HBoV DNA. Of these specimens, 285 (40.4%) were collected from hospitalized patients and 420 (59.6%) from outpatients: 142 (20.2%) from the emergency department and 278 (39.4%) from walk-in clinics. Only 1 specimen was obtained per patient. A total of 314 (44.5%) patients were <2 years of age, 190 (27%) were 2–5 years of age, 120 (17%) were 6–10 years of age, and 61 (8.6%) were 11–15 years of age. Age was not known for 21 patients. Relevant clinical information was collected on a standard questionnaire. This information included hospitalization status, age, sex, and clinical symptoms.\n\n【4】Specimens were collected at university hospitals in 3 different cities in Brazil located in areas with distinct sanitation conditions and socioeconomic backgrounds. The specimens were previously tested for other enteric viruses and bacteria, including rotavirus, norovirus, astrovirus, adenovirus, _Escherichia coli_ , _Salmonella_ spp. _Yersinia enterocolitica_ , _Campylobacter_ spp. and _Shigella_ spp. The study protocol was reviewed and approved by the Ethics Committee of the Instituto de Puericultura e Pediatria Martagão Gesteira of the Federal University of Rio de Janeiro.\n\n【5】Stool suspensions were prepared as 10% (w/v) in phosphate-buffered saline (pH 7.2), clarified by centrifugation at 2,500× _g_ for 5 min. Two hundred microliters of each suspension was used for DNA extraction with the Wizard Genomic DNA Purification Kit (Promega, Madison, WI, USA) according to the manufacturer’s instructions. PCRs were performed as described  by using forward primer HBoV 01.2 (5′-TATGGCCAAGGCAATCGTCCAAG-3′) and reverse primer HBoV 02.2 (5′-GCCGCGTGAACATGAGAAACAGA-3′) for the nonstructural protein 1 gene. A 291-bp amplicon was generated. DNA samples were subjected to 1 cycle at 95°C for 15 min, followed by 45 cycles at 94°C for 20 s, 56°C for 20 s, and 72°C for 30 s, and a final extension at 72°C for 5 min. PCR products were detected by agarose gel electrophoresis and staining with ethidium bromide.\n\n【6】To confirm the presence of HBoV, amplified DNAs of PCR-positive samples were purified by using the Wizard SV gel and PCR Clean-Up system kit (Promega). Sequences were determined by using the BigDye Terminator Cycle Sequencing Kit and the ABI PRISM 3100 automated DNA sequencer (Applied Biosystems, Foster City, CA, USA). DNA sequences were assembled and analyzed with the SeqMan, EditSeq, and MegAlign programs in the Lasegene software package (DNASTAR, Madison, WI, USA). Nucleotide sequences obtained in this study were deposited in GenBank under accession nos. EF560205–EF560216.\n\n【7】Fourteen (2%) of 705 diarrhea stool samples were positive for HBoV by PCR. Rotavirus was detected in 84 (11.9%) samples, adenovirus in 34 (4.8%) samples, norovirus in 24 (3.4%) samples, and astrovirus in 2 (0.3%) samples. Enteropathogenic bacteria were found in 57 (8.1%) samples . The frequency of enteric pathogens identified in epidemiologic studies is variable (45%–54%) and dependent on several parameters such as country and type of method used for diagnosis . In our study, a potential pathogen was found in 215 (30.5%) samples (including HBoV-positive samples). No bacterial or virus pathogen was found in 499 (69.5%) samples. Samples were not tested for intestinal parasites, which in general account for ≈11% of the diarrhea etiology in developing countries .\n\n【8】There was no obvious temporal clustering of the HBoV-positive patients. The median age of HBoV-infected children was 1.9 years; 11 children (78.6%) were <2 years of age, 1 child was 35 months of age, 1 child was 11 years of age, and 1 child was 15 years of age. A total of 57% were boys and 43% were girls. Three patients were coinfected with other enteric viruses (1 with adenovirus, 1 with rotavirus, and 1 with norovirus). All HBoV-positive patients had diarrhea but none reported concomitant respiratory symptoms. Fever was reported in 2 patients, vomiting in 1, and bloody diarrhea in 2. One hospitalized boy (the oldest study participant) was reported to be positive for HIV and cytomegalovirus, and 1 hospitalized girl was undergoing dialysis. Ten (71.2%) HBoV-positive children were hospitalized because of diarrhea; 3 were outpatients (2 from walk-in clinics and 1 from an emergency department).\n\n【9】A semiquantitative PCR of HBoV in stool specimens was performed by using dilutions of DNA extracted from stool samples. We detected DNA up to a dilution of 10 –3  in 3 samples. In the remaining samples, DNA was detected only in undiluted samples. Sequence analysis showed high nucleotide similarity between Brazilian samples and the Chinese respiratory HBoV WLL-3 strain  from the People’s Republic of China (91.8%–99.6%) and among the Brazilian samples (96.4%–100%) . We could not compare our enteric strains with a Spanish enteric strain  because we sequenced a different portion of the virus genome. Strain MC-8 showed the lowest homology with the WLL-3 strain (91.8%) and with the other Brazilian strains (96.4%). We are conducting additional sequencing to characterize the complete genome of this strain to confirm that it represents a new variant of the virus.\n\n【10】### Conclusions\n\n【11】HBoV has been isolated from respiratory specimens from patients with acute respiratory illness, and increasing evidence suggests a causal relationship with this disease . The presence of HBoV in the human gastrointestinal tract was demonstrated by Vicente et al. as well as in our study. In the first study, virus was isolated from feces of children with gastroenteritis with or without symptoms of respiratory infection. Coinfection with other intestinal pathogens was found in 28 (58.3%) of 48 HBoV-positve samples. In our study, none of the HBoV-positive patients reported respiratory symptoms. Coinfection with other enteric viruses was found in 3 (21.4%) of 14 HBoV-positive samples. High titers of DNA in some specimens suggest that the virus replicates in the human gut. However, additional studies that include control groups are needed to demonstrate an association between HBoV infection and gastroenteritis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0a5112b5-9834-4eeb-8118-9f94176540dd", "title": "Effect of Family Income on the Relationship Between Parental Education and Sealant Prevalence, National Health and Nutrition Examination Survey, 2005–2010", "text": "【0】Effect of Family Income on the Relationship Between Parental Education and Sealant Prevalence, National Health and Nutrition Examination Survey, 2005–2010\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】We examined the association between sealant prevalence and parental education for different levels of family income, controlling for other covariates.\n\n【3】**Methods**\n\n【4】We combined data from 2005–2006, 2007–2008, and 2009–2010 cycles of the National Health and Nutrition Examination Survey. The study sample was 7,090 participants aged 6 to 19 years. Explanatory variables, chosen on the basis of Andersen and Aday’s framework of health care utilization, were predisposing variables — child’s age, sex, race/ethnicity, and parental education (<high school diploma; high school diploma; >high school diploma); enabling variables — family income (<100% of the federal poverty level \\[FPL\\]; 100%–200% of the FPL; and >200% of the FPL), health insurance status, and regular source of medical care; and a need variable — future need for care (perceived child health status is excellent/very good, good, fair/poor). We conducted bivariate and multivariate analyses and included a term for interaction between education and income in the multivariate model. We report significant findings ( _P_ ≤ .05).\n\n【5】**Results**\n\n【6】Sealant prevalence was associated with all explanatory variables in bivariate and multivariate analyses. In bivariate analyses, higher parental education and family income were independently associated with higher sealant prevalence. In the multivariate analysis, higher parental education was associated with sealant prevalence among higher income children, but not among low-income children (<100% FPL). Sealant prevalence was higher among children with parental education greater than a high school diploma versus less than a high school diploma in families with income ≥100% FPL.\n\n【7】**Conclusion**\n\n【8】Our findings suggest that income modifies the association of parental education on sealant prevalence. Recognition of this relationship may be important for health promotion efforts.\n\n【9】Introduction\n------------\n\n【10】Despite marked improvements in the oral health of children and youth in the United States over the past decades, dental caries remains one of the most common chronic childhood diseases . By age 17, almost 70% of adolescents have experienced caries  and most (90%) caries in permanent teeth occurs in the pits and fissures . Dental caries disproportionately affect low-income children — 66% of adolescents living in poverty have experienced caries compared with 54% of children living in families with incomes greater than 200% of the federal poverty level . Although US children living in poverty are at higher risk for caries, only 1 in 4 has had at least 1 dental sealant .\n\n【11】Sealants are effective in preventing and controlling dental caries in the pits and fissures of permanent teeth . A recently published Cochrane review found that sealants reduced caries by 81% at 2-year follow-up . Another review found that placing sealants on noncavitated lesions reduced the progression of caries by 70% up to 5 years after placement . Increasing sealant prevalence among children at risk for caries is a national health objective . Both the Centers for Medicare and Medicaid Services  and the National Quality Forum  have endorsed performance measures related to increased sealant prevalence among Medicaid-enrolled and privately insured children, respectively, who are at risk for caries.\n\n【12】Factors associated with a child’s not having sealants include ability to pay for dental care (ie, low family income) ; not having dental or health insurance ; sociodemographic variables, including having parents who did not graduate high school ; being of minority race/ethnicity ; and low health literacy and low oral health literacy . A recent analysis found that parents’ functional health literacy and English being spoken at home were strong predictors of sealant prevalence among California school children . The RAND Health Insurance Experiment (HIE) also found that health literacy (ie, parents’ knowledge of the medical care system) predicted use of dental health services among children . Studies further suggest that one of the strongest predictors of medical and oral health literacy is educational attainment . Knowledge of the preventive benefits of sealants is also almost 5 times higher among people with more than a high school education compared with those without a high school education (34% vs 7%) .\n\n【13】Studies also indicate that factors affecting ability to pay for services may modify the effect of education on use of dental health services, although the direction of the effect varies by study. An earlier analysis of National Health Interview Survey (NHIS) data found that higher income predicted having a sealant only among children of parents who had a high school education or more . This analysis, however, was conducted in 1989 and reported that sealant prevalence among children aged 6 to 17 years was 16% , about half the current prevalence of 31% . Another early analysis from the RAND HIE found, however, that the presence of enabling resources (ie, removal of cost sharing from dental insurance plans) among less educated people resulted in more fillings and less untreated decay .\n\n【14】The predictive power of these factors largely can be explained in the context of Andersen and Aday’s predisposing, enabling, and need (PEN) model of health services’ use . According to the PEN model, health care utilization is a function of 3 factors: 1) demographic and social characteristics that influence a person’s attitudes and valuation of health, which in turn predispose a person to use care; 2) location and availability of health care as well as personal resources that enable a person to access care; and 3) a person’s perceived need for care.\n\n【15】In this article, we examine the association between sealant prevalence and predisposing, enabling, and need variables. In light of the findings of the analysis of NHIS data, of special interest is whether the association between sealant prevalence and education, a predisposing variable, is still modified by income, an enabling resource. We hypothesize that predisposing variables will have higher predictive ability in the presence of sufficient enabling resources or alternatively that enabling resources will have higher predictive power among parents predisposed to use sealants.\n\n【16】Methods\n-------\n\n【17】### Data source\n\n【18】The National Health and Nutrition Examination Survey (NHANES) is a cross-sectional survey conducted by the Centers for Disease Control and Prevention’s (CDC’s) National Center for Health Statistics (NCHS) to assess the health and nutritional status of the civilian, noninstitutionalized US population.  The survey uses a complex, multistage probability sampling design. NHANES participants are interviewed in their homes and then complete a health examination at a mobile examination center. We used data from the NHANES interview questionnaire and the oral health examination combined for 2005–2006, 2007–2008, and 2009–2010. The response rate for the combined examined sample is 83.7%.\n\n【19】The NHANES oral health examination included a Basic Screening Examination (BSE) assessment where participants’ teeth were examined visually for presence of untreated dental caries, dental restorations, and dental sealants. In 2005–2006 and 2007–2008, the BSE was performed by health technologists among people aged 5 years and older; in 2009–2010, dental hygienists performed the BSE among participants aged 3 to 19 years only . The BSE in 2009–2010 made use of the same examination protocols that were used in 2005–2008. Comprehensive training and calibration of examiners are conducted throughout the continuous NHANES 2-year cycles. The kappa statistics for sealants between health technologists and the survey reference examiner ranged from 0.82 to 0.90 for 2005–2008 and the statistic between dental hygienists and the survey reference examiner was 0.71 in 2009–2010. These statistics could be directly compared, because the reference examiner did not change between the data collection periods .\n\n【20】We used data from the NHANES public-use files; therefore, CDC/NCHS Ethics Review Board approval was not needed. Of the 8,275 participants aged 6 to 19 years in NHANES, 7,916 had sealant data. Among these children, 749 children did not have data for family income, parental education, or both. Of the remaining 7,167 children, 74 did not have data on health insurance and 3 did not have data on general health status or usual source of care. The final study sample was 7,090 .\n\n【21】Dental sealant, the dependent variable, was recorded as present if at least 1 posterior primary or permanent tooth had a sealant, even if part of the sealant was not visible . Andersen and Aday’s PEN model was used for selecting independent variables . The main independent variable of interest was the predisposing factor, education level of head of household (referred to henceforth as “parental education”). Education was coded as “less than high school diploma,” “high school diploma,” or “more than high school diploma.” Other independent variables in this analysis included predisposing factors (child’s age, sex, and race/ethnicity), enabling factors (income, health insurance status, regular source of medical care), factors associated with future need for dental care (parent’s perceived health status of child), which studies suggest predicts oral health status of child , and survey year, to capture differences over time. Age was divided into 2 categories, 6 to 11 and 12 to 19 years, which coincide with eruption of the first and second permanent molars, around 6 and 12 years, respectively. The child’s race/ethnicity was coded as non-Hispanic white, non-Hispanic black, Mexican American, or other, which included other Hispanic, other races, or multiple races. The ratio of family income to federal poverty level (FPL) was categorized as less than 100% of the FPL, 100% to 200% of the FPL, or greater than 200% of the FPL. Four health insurance categories were created: no insurance; private or military; Medicaid or State Children’s Health Insurance Program (SCHIP); and other government health insurance (eg, state-sponsored health insurance, Indian health). Finally, parents’ perception of the child’s general health was coded as excellent or very good, good, or fair or poor.\n\n【22】SAS-Callable SUDAAN, which correctly estimates the variance for complex surveys, was used to generate estimates, standard errors (SEs), and associated confidence intervals (CIs) (SUDAAN software, release 11.0.0, RTI International). We used a χ 2  test to determine if the characteristics of children in our study sample differed from those excluded from our study as well as to examine if sealant prevalence differed among our explanatory variables. All reported differences are significant at _P_ ≤ .05 and all CIs are reported at the 95% level. We used logistic regression to identify predisposing, enabling, and need factors associated with having at least 1 sealant. To assess whether income modified the association between education and having sealants, our model included a term for interaction between these 2 variables. Point estimates of model-adjusted sealant prevalence for the 3 levels of parental education stratified by family income were also obtained from the average marginal predictions in the fitted logistic regression model . The model fit was assessed using the Hosmer-Lemeshow goodness of fit test.\n\n【23】Results\n-------\n\n【24】Excluded children had less educated parents but lived in families with income levels similar to those of children included in this study . Excluded children were also older, less likely to be non-Hispanic white, and more likely to be “other race/ethnicity.” Overall, among children aged 6 to 19 years in NHANES 2005–2010, including children excluded from our analysis (n = 7,916; data not shown), sealant prevalence was 32.9% (SE = 1.0%). Sealant prevalence did not differ between children included and excluded from our study . Furthermore, the sample used in this analysis (n = 7,090) was comparable to the total number of children in the combined NHANES database (n = 8,275; data not shown).\n\n【25】Sealant prevalence was associated with all independent variables . Sealant prevalence increased with level of parental education and family income. Sealant prevalence was also higher among older children, girls, and non-Hispanic white children compared with younger children, boys, and non-Hispanic black and Mexican American children. Children with private health insurance and those with a usual source of care also had higher sealant prevalence than did children with Medicaid/SCHIP, other government insurance, or no insurance and children without a usual source of medical care. Sealant prevalence also was higher among children in excellent to very good health compared with those in good or fair/poor health — the indicator used for future need for dental care in this analysis. Finally, compared with previous NHANES cycles, sealant prevalence was higher in 2009–2010.\n\n【26】Before controlling for potential covariates, we found that sealant prevalence differed only by parental education among the middle- and high-income groups ( _P_ values for lowest income group always exceeded .80; data not shown). Among the middle-income group (100%–200% of the FPL), the differences in sealant prevalence among children of parents who had more than a high school diploma (prevalence, 30%) or a high-school diploma (28%) compared with less than a high school diploma (22%) were 8 percentage points ( _P_ \\= .007) and 6 percentage points ( _P_ \\= .04), respectively. Among the highest income group (>200% of the FPL), the differences in sealant prevalence between children of parents who had more than a high school diploma (40%) or graduated high school (35%) compared with parents who were not high school graduates (25%) were 15 percentage points ( _P_ < .001) and 10 percentage points ( _P_ \\= .04), respectively.\n\n【27】### Logistic regression results\n\n【28】After controlling for potential covariates, the association between sealant prevalence and education was still modified by income . The odds of having a sealant among children of parents who were not high school graduates versus parents who had more than a high school education were significant among children from families with incomes at 100% of the FPL or greater. Among families with the highest income (>200% of the FPL), sealant prevalence estimated from our regression model was approximately 12 percentage points higher (38.2% vs 25.7%) for children of parents with more than a high school diploma  compared with those who did not graduate high school.\n\n【29】The findings from our logistic regression were similar to those from the bivariate (unadjusted) analysis. Younger children, boys, and non-Hispanic black children were less likely to have a sealant than were older children, girls, and non-Hispanic white children. Furthermore, uninsured children and those without a regular source of care had lower odds of having a sealant than did privately insured children and children with a regular source of care. Finally, the odds of having a sealant were still higher among children reporting better general health and those in the last cycle of NHANES .\n\n【30】Unlike the bivariate analysis, however, the multivariate analysis did not find a difference in the odds of having versus not having at least 1 sealant between Mexican American children and non-Hispanic white children (OR, 0.99; 95% CI, 0.80–1.24). Similarly, the odds of having sealants did not differ between Medicaid and privately insured children (OR, 1.02; 95% CI, 0.81–1.29).\n\n【31】Discussion\n----------\n\n【32】This study used Andersen and Aday’s PEN model of health care utilization  to examine whether the association between sealant prevalence in children and the predisposing factor, parental education, varied by the enabling factor of family income, controlling for other predisposing, enabling, and need variables. Andersen’s model has been expanded since its initial conception to include additional variables  but, as suggested by the model’s authors, we used the original model because of data availability and its suitability to our research question . We found that, although sealant prevalence did not vary by level of parental education among children from low-income families, having a parent who was educated beyond high school (vs a parent who did not graduate high school) was associated with an almost 50% increase in sealant prevalence among children from high-income families (26% among <high school diploma vs 38% among >high school diploma).\n\n【33】To the extent that education and income are good measures of predisposing and enabling variables, respectively, our findings suggest that parents predisposed to having their child receive sealants require sufficient enabling resources or that parents with sufficient enabling resources must be predisposed to having their child receive sealants. We classified education as a predisposing factor because of its strong association with health literacy , which in turn predicts higher sealant prevalence . This classification is also consistent with Andersen and Aday’s identification of education as a predisposing factor in their PEN model .\n\n【34】Although we had 3 measures of enabling resources (income, health insurance, and usual source of health care), we used income as the primary proxy for enabling resources. We did so because the other 2 items measured enabling resources for medical as opposed to dental care. Analyses conducted during the 2005 to 2010 timeframe of this study suggest that approximately one-fifth of children with health insurance do not have dental insurance — approximately 94% of US children had health insurance  while only approximately 75% had dental insurance . In addition, health insurance may be more enabling for health care than dental insurance is for dental care — dental insurance has higher copays and lower annual limits than medical insurance and, as a result, approximately 40% of dental expenditures are paid out-of-pocket .\n\n【35】The finding that increased education was not associated with increased sealant prevalence among children from low-income families is noteworthy because these children are eligible to receive Medicaid dental benefits in all states . Medicaid dental coverage, however, may be less enabling than private dental insurance. Dentists may be less likely to participate in Medicaid for reasons such as lower reimbursement rates. In 2013, for example, the average Medicaid fee-for-service reimbursement for children’s dental services was less than half (48.8%) that of commercial dental insurance charges .\n\n【36】Our findings deviated from the PEN model in that the variable we included to reflect future need for care (poor perceived child’s health status) was associated with lower sealant prevalence. These results are similar to those from another study of factors influencing receipt of preventive medical and dental care where children with self-reported poor health status were less likely to have received preventive pediatric health and dental care . Poor general health may not only indicate higher caries risk but also poorer access to care due to limitations in mobility or financial resources. Children in poor health status may use more treatment or acute care services at the expense of needed preventive services .\n\n【37】Our study has limitations. Because this analysis used cross-sectional data, our findings can be interpreted only as an association rather than a causal factor of children’s having sealants. In addition, family income, parental education, insurance, and health status were self-reported with no objective measures to confirm validity. A final limitation was the change in the NHANES examiner type from health technologists to dental hygienists between 2005–2008 and 2009–2010. However, the clinical assessment criteria did not change nor did the reference examiner change and any potential differences because of the change in examiner type should be partially captured by the survey year variable.\n\n【38】Our findings suggest that the impact of higher parental education on a child’s having sealants is greater in the presence of higher family income and that the impact of higher family income on a child’s having sealants is greater in the presence of higher parental education. Interpreting our findings in the context of oral health care for children in the United States suggests that sealant prevalence could increase. Recent health care reforms will probably enable more families to obtain preventive dental services (including sealants) for their children through increased access to dental insurance and increased supply of dental providers . Andersen and Aday argued that addressing enabling variables would be the most effective strategy to achieve equitable access to health care, as these resources could be altered with changes in government policy . Predisposing factors such as education, however, are considered less mutable in the short run. If education is indeed capturing the influence of health literacy, then it may be possible to alter a parent’s predisposition toward sealants with health literacy campaigns. The renewed focus on the importance of oral health literacy could in a short time result in more families being aware of the importance of good oral health and the preventive benefits of dental sealants. Health and oral health literacy are a focus of interest at the national level, as demonstrated in the recommendations from a recent Institute of Medicine workshop , objectives in Healthy People 2020 , and the Health and Human Services National Action Plan to Improve Health Literacy, which includes an oral health component .\n\n【39】Tables\n------\n\n【40】#####  Table 1. Characteristics of Children Aged 6 to 19 Years Included in or Excluded From Study, NHANES 2005–2010\n\n| Variable | Included in Study (n = 7,090) | Excluded From Study (n = 1,185) | _P_ Value b |\n| --- | --- | --- | --- |\n| No. a | % (SE) | No. | % (SE) |\n| --- | --- | --- | --- |\n| **Parental education** | **Parental education** | **Parental education** | **Parental education** | **Parental education** | **Parental education** |\n| <High school diploma | 2,133 | 19.2 (1.08) | 329 | 26.4 (2.38) | <.001 |\n| High school diploma | 1,687 | 23.8 (1.21) | 222 | 29.9 (2.67) | <.001 |\n| \\>High school diploma | 3,270 | 57.0 (1.40) | 295 | 43.7 (2.98) | <.001 |\n| **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** |\n| <100 | 2,180 | 21.2 (1.10) | 226 | 24.9 (3.04) | .22 |\n| 100–200 | 1,916 | 22.4 (0.95) | 167 | 19.7 (2.25) | .22 |\n| \\>200 | 2,994 | 56.4 (1.58) | 235 | 55.4 (4.11) | .22 |\n| **Child’s age group, y** | **Child’s age group, y** | **Child’s age group, y** | **Child’s age group, y** | **Child’s age group, y** | **Child’s age group, y** |\n| 6–11 | 3,146 | 43.2 (0.95) | 396 | 32.1 (1.83) | <.001 |\n| 12–19 | 3,944 | 56.8 (0.95) | 789 | 67.9 (1.83) | <.001 |\n| **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** |\n| Female | 3,480 | 48.6 (0.80) | 587 | 51.5 (1.76) | .15 |\n| Male | 3,610 | 51.4 (0.80) | 598 | 48.5 (1.76) | .15 |\n| **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** |\n| Mexican American | 2,007 | 12.9 (1.34) | 403 | 15.3 (1.74) | .009 |\n| Non-Hispanic black | 1,936 | 14.3 (1.21) | 318 | 16.0 (1.71) | .009 |\n| Other | 979 | 12.2 (1.07) | 192 | 16.7 (2.21) | .009 |\n| Non-Hispanic white | 2,168 | 60.6 (2.13) | 272 | 52.0 (3.14) | .009 |\n| **Health insurance** | **Health insurance** | **Health insurance** | **Health insurance** | **Health insurance** | **Health insurance** |\n| No insurance | 1,086 | 11.4 (0.83) | 224 | 15.0 (2.08) | .40 |\n| Medicaid/SCHIP | 1,684 | 17.4 (1.07) | 243 | 15.9 (2.03) | .40 |\n| Other government | 885 | 9.3 (0.90) | 139 | 9.1 (1.72) | .40 |\n| Private or military insurance | 3,435 | 61.9 (1.70) | 485 | 60.0 (2.94) | .40 |\n| **Regular source of care** | **Regular source of care** | **Regular source of care** | **Regular source of care** | **Regular source of care** | **Regular source of care** |\n| No | 688 | 7.5 (0.61) | 186 | 12.3 (1.59) | .002 |\n| Yes (= 1 place) | 6,402 | 92.5 (0.61) | 996 | 87.7 (1.59) | .002 |\n| **Child’s general health** | **Child’s general health** | **Child’s general health** | **Child’s general health** | **Child’s general health** | **Child’s general health** |\n| Fair to poor | 490 | 4.9 (0.31) | 110 | 7.3 (1.03) | <.001 |\n| Good | 1,945 | 23.0 (0.69) | 399 | 30.1 (1.57) | <.001 |\n| Excellent to very good | 4,655 | 72.1 (0.79) | 675 | 62.8 (2.23) | <.001 |\n| **Survey year** | **Survey year** | **Survey year** | **Survey year** | **Survey year** | **Survey year** |\n| 2005–2006 | 2,876 | 34.2 (1.95) | 443 | 27.6 (2.96) | .08 |\n| 2007–2008 | 2,047 | 32.5 (1.80) | 379 | 38.3 (3.41) | .08 |\n| 2009–2010 | 2,167 | 33.2 (1.77) | 363 | 34.1 (3.17) | .08 |\n| **Sealant** | **Sealant** | **Sealant** | **Sealant** | **Sealant** | **Sealant** |\n| No | 5,056 | 66.8 (1.04) | 611 | 69.5 (2.77) | .35 |\n| Yes | 2,034 | 33.2 (1.04) | 215 | 30.5 (2.77) | .35 |\n\n【42】Abbreviations: NHANES, National Health and Nutrition Examination Survey; SCHIP, State Children’s Health Insurance Program; SE, standard error.  \na  Sample sizes are unweighted; percentages estimated from weighted data.  \nb  ? 2  used to test for significance.\n\n【43】#####  Table 2. Unadjusted Associations Between Prevalence of Sealants and Independent Variables Among Children Aged 6 to 19 Years, NHANES 2005–2010\n\n| Variable | With Sealants (n = 2,034)% (SE) | _P_ Value a |\n| --- | --- | --- |\n| **Parental education** | **Parental education** | **Parental education** |\n| <High school diploma | 24.5 (1.58) | <.001 |\n| High school diploma | 30.7 (1.78) | <.001 |\n| \\>High school diploma | 37.1 (1.18) | <.001 |\n| **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** | **Family’s income, % of the federal poverty level** |\n| <100 | 25.7 (1.54) | <.001 |\n| 100–200 | 28.1 (1.85) | <.001 |\n| **\\>** 200 | 38.0 (1.25) | <.001 |\n| **Child’s age group, y** | **Child’s age group, y** | **Child’s age group, y** |\n| 6–11 | 29.2 (1.27) | <.001 |\n| 12–19 | 36.2 (1.34) | <.001 |\n| **Child’s sex** | **Child’s sex** | **Child’s sex** |\n| Female | 35.4 (1.34) | <.001 |\n| Male | 31.1 (1.30) | <.001 |\n| **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** |\n| Mexican American | 28.5 (1.33) | <.001 |\n| Non-Hispanic black | 22.3 (1.48) | <.001 |\n| Other | 34.1 (2.21) | <.001 |\n| Non-Hispanic white | 36.5 (1.56) | <.001 |\n| **Health insurance** | **Health insurance** | **Health insurance** |\n| No insurance | 23.1 (2.07) | <.001 |\n| Medicaid/SCHIP | 27.2 (1.84) | <.001 |\n| Other government | 33.4 (2.55) | <.001 |\n| Private or military insurance | 36.6 (1.10) | <.001 |\n| **Regular source of care** | **Regular source of care** | **Regular source of care** |\n| No | 23.1 (3.00) | .04 |\n| Yes (= 1 place) | 34.0 (1.03) | .04 |\n| **Child’s general health** | **Child’s general health** | **Child’s general health** |\n| Fair to poor | 27.3 (2.47) | <.001 |\n| Good | 27.1 (1.65) | <.001 |\n| Excellent to very good | 35.5 (1.12) | <.001 |\n| **NHANES survey year** | **NHANES survey year** | **NHANES survey year** |\n| 2005–2006 | 32.3 (2.27) | .001 |\n| 2007–2008 | 26.0 (1.04) | .001 |\n| 2009–2010 | 41.0 (1.92) | .001 |\n\n【45】Abbreviations: NHANES, National Health and Nutrition Examination Survey; SCHIP, State Children’s Health Insurance Program; SE, standard error.  \na  ? 2  used to test for significance.\n\n【46】#####  Table 3. Odds Ratios (ORs) With 95% Confidence Intervals (CIs) From Multiple Logistic Regression Model a  of Children Aged 6 to 19 Years With at Least 1 Sealant, NHANES 2005–2010\n\n| Variable | OR (95% CI) | _P_ Value b |\n| --- | --- | --- |\n| **Combined effect of income and education** | **Combined effect of income and education** | **Combined effect of income and education** |\n| **<100% FPL** | **<100% FPL** | **<100% FPL** |\n| <High school diploma vs high school diploma | 1.03 (0.75–1.42) | .85 |\n| <High school diploma vs >high school diploma | 1.07 (0.83–1.37) | .62 |\n| High school diploma vs >high school diploma | 1.03 (0.70–1.52) | .86 |\n| **100%–200% FPL** | **100%–200% FPL** | **100%–200% FPL** |\n| <High school diploma vs high school diploma | 0.72 (0.52–1.01) | .05 |\n| <High school diploma vs >high school diploma | 0.73 (0.53–1.00) | .05 |\n| High school diploma vs >high school diploma | 1.01 (0.73–1.39) | .96 |\n| **\\>200% FPL** | **\\>200% FPL** | **\\>200% FPL** |\n| <High school diploma vs high school diploma | 0.64 (0.39–1.07) | .09 |\n| <High school diploma vs >high school diploma | 0.55 (0.36–0.83) | .005 |\n| High school diploma vs >high school diploma | 0.85 (0.64–1.12) | .24 |\n| **Child’s age group** | **Child’s age group** | **Child’s age group** |\n| 6–11 | 0.67 (0.58–0.79) | <.001 |\n| 12–19 | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Child’s sex** | **Child’s sex** | **Child’s sex** |\n| Female | 1.24 (1.06–1.45) | .008 |\n| Male | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Child’s race/ethnicity** | **Child’s race/ethnicity** | **Child’s race/ethnicity** |\n| Mexican American | 0.98 (0.78–1.22) | .85 |\n| Non-Hispanic black | 0.59 (0.48–0.72) | <.001 |\n| Other | 1.01 (0.79–1.30) | .91 |\n| Non-Hispanic white | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Health insurance** | **Health insurance** | **Health insurance** |\n| No insurance | 0.72 (0.58–0.90) | .005 |\n| Medicaid/SCHIP | 1.01 (0.80–1.29) | .90 |\n| Other government | 1.26 (0.98–1.62) | .08 |\n| Private or military insurance | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Regular source of care** | **Regular source of care** | **Regular source of care** |\n| No | .70 (0.53–0.93) | .01 |\n| Yes (=one place) | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Child’s general health** | **Child’s general health** | **Child’s general health** |\n| Fair to poor | 0.87 (0.66–1.16) | <.001 |\n| Good | 0.74 (0.63–0.87) | .33 |\n| Excellent to very good | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n| **Survey year** | **Survey year** | **Survey year** |\n| 2005–2006 | 0.67 (0.50–0.88) | .006 |\n| 2007–2008 | 0.50 (0.41–0.61) | <.001 |\n| 2009–2010 | 1.0 \\[Reference\\] | 1.0 \\[Reference\\] |\n\n【48】Abbreviations: OR, odds ratio; FPL, federal poverty level.  \na  Hosmer-Lemeshow Satterthwaite, _P_ \\= .55.  \nb  Wald _F_ \\-test used to test for significance of combined effect of income and education and Wald _t_ test used for other variables.\n\n【49】Post-Test Information\n---------------------\n\n【50】To obtain credit, you should first read the journal article. After reading the article, you should be able to answer the following, related, multiple-choice questions. Credit cannot be obtained for tests completed on paper, although you may use the worksheet below to keep a record of your answers.org, please click on the “Register” link on the right hand side of the website to register. Only one answer is correct for each question. Once you successfully answer all post-test questions you will be able to view and/or print your certificate. For questions regarding the content of this activity, contact the accredited provider, CME@medscape.net . For technical assistance, contact CME@webmd.net . American Medical Association’s Physician’s Recognition Award (AMA PRA) credits are accepted in the US as evidence of participation in CME activities. The AMA has determined that physicians not licensed in the US who participate in this CME activity are eligible for _**AMA PRA Category 1 Credits™**_ . Through agreements that the AMA has made with agencies in some countries, AMA PRA credit may be acceptable as evidence of participation in CME activities. If you are not licensed in the US, please complete the questions online, print the AMA PRA CME credit certificate and present it to your national medical association for review.\n\n【51】Post-Test Questions\n-------------------\n\n【52】### Article Title: Exploring the Effect of Family Income on the Relationship Between Parental Education and Sealant Prevalence\n\n【53】**CME Questions**\n\n【54】1.  You are seeing a 10-year-old girl with a history of dental caries, and you consider applying a sealant for this patient. What should you consider regarding the epidemiology of caries and the effectiveness of dental sealants?\n    1.  Dental caries affect all children equally, regardless of family income\n    2.  Sealants may reduce the incidence of caries by more than 80% after 2 years\n    3.  Sealants are ineffective at preventing the progression of early caries\n    4.  Most children living in poverty have received dental sealants\n2.  You discuss the application of a dental sealant with the girl’s parents. According to bivariate analysis in the current study by Al Agili and colleagues, what were the effects of parental income and education on the use of sealants?\n    1.  Higher income was associated with a lower rate of sealant use; higher educational status was associated with a lower rate of sealant use\n    2.  Lower income was associated with a lower rate of sealant use; higher educational status was associated with a lower rate of sealant use\n    3.  Lower income was associated with a higher rate of sealant use; lower educational status was associated with a lower rate of sealant use\n    4.  Lower income was associated with a lower rate of sealant use; lower educational status was associated with a lower rate of sealant use\n3.  What more did the current study demonstrate regarding the relationship between parental income, educational attainment, and the application of dental sealants?\n    1.  The effect of higher educational attainment promoted higher sealant use primarily among lower-income families\n    2.  The effect of higher educational attainment promoted higher sealant use primarily among higher-income families\n    3.  Only educational attainment beyond a collegiate degree was associated with lower sealant levels among higher-income families\n    4.  Higher educational attainment promoted higher sealant use regardless of family income level\n4.  Which of the following other variables was a significant risk factor for failure to receive a dental sealant in the current study?\n    1.  Non-Hispanic black race\n    2.  Mexican American ethnicity\n    3.  Medicaid insurance\n    4.  Younger age\n\n【55】**Evaluation**\n\n| 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. |\n| --- | --- | --- | --- | --- |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7e7e85c8-8792-4966-a1bc-a7ef8bf46882", "title": "Distribution of Eosinophilic Meningitis Cases Attributable to Angiostrongylus cantonensis, Hawaii", "text": "【0】Distribution of Eosinophilic Meningitis Cases Attributable to Angiostrongylus cantonensis, Hawaii\nEosinophilic meningitis (EM) is a rare clinical entity characterized by meningeal inflammation and eosinophilic pleocytosis in the cerebrospinal fluid (CSF) . Among the infectious causes of EM, _Angiostrongylus cantonensis_ is the most common worldwide. _A. cantonensis_ , the rat lungworm, was first described in rats in 1935, in Canton, China. The parasite was first postulated to cause human infection in a fatal case in 1944 in Taiwan and was confirmed to be pathogenic for humans through investigations in the early 1960s in Hawaii .\n\n【1】Most of the described cases of symptomatic _A. cantonensis_ infection (neurologic angiostrongyliasis) have occurred in regions of Asia and the Pacific Rim (e.g. Taiwan, Thailand, and the Hawaiian and other Pacific Islands) . However, widespread geographic dispersal of _A. cantonensis_ is ongoing, facilitated primarily by infected shipborne rats and the diversity of potential intermediate hosts . Intercontinental movement of rodent definitive hosts and accidental human hosts translates into the need for worldwide awareness of the association between EM and _A. cantonensis_ infection.\n\n【2】Humans become infected by ingesting intermediate hosts, such as snails and slugs, or transport/paratenic hosts, such as freshwater crustaceans, that contain viable third-stage larvae . These larvae can migrate to the central nervous system (CNS) and cause EM . The exposure often is unrecognized and presumptive, such as through ingestion of contaminated produce. The incubation period averages ≈1–3 weeks but has ranged from 1 day to >6 weeks . Common clinical manifestations include headache, meningismus, and hyperesthesia, which usually resolve spontaneously with supportive care; severe cases can be associated with sequelae (e.g. paralysis and blindness) and death . The utility of anthelminthic and corticosteroid therapy remains controversial and may vary among _A. cantonensis_ –endemic areas .\n\n【3】Typically, symptomatic infection is presumptively diagnosed on the basis of epidemiologic and clinical criteria , as was done in this investigation. Parasitologic confirmation, by detection of larvae or young adult worms in the CSF, is unusual, albeit slightly more common in young children (particularly in Taiwan) . Investigational immunoassays for detection of antibodies to _A. cantonensis_ antigens have not been sufficiently characterized or validated to be useful for distinguishing infected and uninfected persons, particularly in epidemiologic investigations .\n\n【4】During November 2004–January 2005, 1 parasitologically confirmed and 4 presumptive cases of _A. cantonensis_ infection were reported to the Hawaii State Department of Health. The 5 cases included 3 from the Big Island of Hawaii and 2 from Oahu; 1 Oahu case was in a visitor to Hawaii whose lumbar puncture (LP) was performed elsewhere. Recognition of these 5 index cases prompted multifaceted investigations of epidemiologic, clinical, and environmental aspects of EM/ _A. cantonensis_ infection in Hawaii.\n\n【5】To assess whether the unusual temporal clustering of case reports reflected an increased incidence of EM/ _A. cantonensis_ infection, we ascertained cases through comprehensive review of statewide laboratory and medical records. Although investigations of EM/ _A. cantonensis_ infection in various Hawaiian Islands have been described since the 1960s , to our knowledge, this is the first study to systematically ascertain cases and determine regional incidence rates in this manner.\n\n【6】### Methods\n\n【7】##### Ascertainment and Classification of Cases\n\n【8】Our primary means for ascertaining potential cases of EM and _A. cantonensis_ infection was a retrospective review of CSF data provided by clinical laboratories in Hawaii for LPs performed during the study period (January 2001–February 2005). In March 2005, we obtained CSF data for 22 of 24 acute-care hospitals, which encompassed ≈93% of the state’s hospital beds; for 1 of the 22 facilities (≈7% of beds), data were unavailable for January 2001–December 2002. The total numbers of patients and LPs during the study period were unavailable (e.g. some laboratories provided CSF data only if particular criteria were met). In January 2005, 1 case of EM/ _A. cantonensis_ infection in a visitor to Hawaii whose LP was performed elsewhere was ascertained by passive physician reporting to the Hawaii State Department of Health and the Centers for Disease Control and Prevention (CDC); this case was 1 of the 5 index cases that prompted the investigation.\n\n【9】Our case definitions for EM and _A. cantonensis_ infection are provided in Table 1 . If the inclusionary criteria for EM were met, we reviewed the patient’s medical record to obtain additional information regarding the EM and to categorize cases of EM by known or likely cause (e.g. _A. cantonensis_ infection). The information collected during chart review included basic demographic data, pertinent dates (e.g. birth, hospitalization, travel, symptom onset, and LP), medical history, medications, clinical manifestations, additional laboratory and radiologic results, and discharge diagnoses. Because the primary focus of the study was _A. cantonensis_ infection, if, at the time of the LP, the patient had intracranial hardware (i.e. a well-established cause of EM) or was <2 months of age (i.e. angiostrongyliasis was epidemiologically unlikely), we collected only demographic data and discharge diagnoses.\n\n【10】We attributed cases of EM to _A. cantonensis_ infection only if this diagnosis was epidemiologically and clinically plausible and no other possible cause of EM was identified. Examples of possible alternative causes included CNS infection with other microbes, reaction to foreign material in the CNS (e.g. intracranial hardware or myelography dye), medications (e.g. intrathecal vancomycin or gentamicin), neoplasms, multiple sclerosis, and neurosarcoidosis . The study neurologist (J.J.S.) facilitated final selection and classification of cases of EM and _A. cantonensis_ infection by reviewing the available case data and ensuring that the inclusionary and exclusionary criteria were applied consistently and objectively.\n\n【11】##### Statistical Analysis and Human Subjects Protection\n\n【12】Data entry was performed with Epi Info version 2002 (CDC, Atlanta, GA, USA), and data analyses were conducted with SAS version 9.1 (SAS Institute, Cary, NC, USA). Two-tailed p values were calculated by using the Fisher exact test for binary variables and the Wilcoxon test for continuous variables. Linear and quadratic regression models were evaluated to assess whether eosinophilic pleocytosis varied with time (i.e. the interval from symptom onset to LP). We calculated incidence rates by generalizing hospital-based frequency data to the population at large for various periods and counties in Hawaii using the US Census Bureau’s annual population estimates for 2001–2004 (the estimate for 2004 also was used for January and February 2005) . We used Poisson regression analyses to compare county-specific annual rates. We defined the 46-month period of January 2001–October 2004 as the baseline period and the 4-month period of November 2004–February 2005 as the cluster period. CDC’s policies with regard to human study participants were followed in this investigation.\n\n【13】### Results\n\n【14】We identified 83 cases of EM for the 50-month study period (January 2001–February 2005); <1% of the patients whose CSF data were reviewed fulfilled the case criteria . The 83 cases included 70 (84%) during the 46-month baseline period (17–21 cases per year) and 13 (16%) during the 4-month cluster period. We attributed 24 (29%) of the 83 EM cases to _A. cantonensis_ infection and 59 (71%) to other causes . Thirty-five of these 59 cases (42% of 83) were in persons with intracranial hardware, and 9 (11% of 83) were in persons without intracranial hardware who had documented bacterial (n = 5) or viral (n = 4) meningoencephalitis.\n\n【15】The 24 cases of EM attributed to _A. cantonensis_ infection included 1 parasitologically confirmed case in an 11-month-old child and 23 clinically defined cases . EM was noted in the discharge diagnoses for 11 case-patients (46%). _A. cantonensis_ infection, as well as EM, was listed for only 2 cases: the parasitologically confirmed case and 1 other case in January 2005. The 24 case-patients had a median age of 31 years (range 11 months–45 years), and 13 (54%) were male. Of the 13 patients for whom race/ethnicity data were available, 6 (46%) were Caucasian, 3 (23%) Filipino, 3 (23%) Hawaiian/part-Hawaiian, and 1 (8%) Samoan.\n\n【16】For the 22 case-patients with known symptom onset dates, the median interval from onset to LP was 3 days (range 0–48 days); the 2 longest intervals were 14 days (2 patients) and 48 days (1 patient). When a linear regression model was applied to data for the intervals < 14 days, the longer the interval (between symptom onset and LP), the higher the CSF eosinophil percentage and absolute eosinophil count (p = 0.001 and 0.005, respectively). Compared with patients with other causes of EM, _A. cantonensis_ case-patients had significantly higher CSF leukocyte counts (median 573/mm 3  vs. 304/mm 3  , p = 0.03) and absolute eosinophil counts (median 120/mm 3  vs. 14/mm 3  , p<0.001); they also tended to have higher eosinophil percentages (median 15.0% vs. 12.0%), but the difference was not statistically significant (p = 0.08).\n\n【17】The temporal distribution of the 24 cases included 15 (63%) during the baseline period (3–5 cases per year) and 9 (38%) during the cluster period . The mean number of _A. cantonensis_ cases per month increased from 0.3 in the baseline period to 2.3 in the cluster period, whereas the mean monthly rates for cases of EM with other causes were essentially unchanged (1.2 and 1.0, respectively). Thus, the proportion of EM cases attributed to _A. cantonensis_ increased from 21% (15/70) for the baseline period to 69% (9/13) for the cluster period. The _A. cantonensis_ incidence rates for the state as a whole increased from 0.3 per 100,000 person-years in the baseline period to 2.1 in the cluster period (p<0.001) .\n\n【18】The geographic distribution of the 24 cases included 3 counties and 4 islands: Honolulu County (Oahu Island; n = 11 cases, including the case in the visitor), Hawaii County (Big Island of Hawaii; n = 9, including the parasitologically confirmed case), and Maui County (n = 4, including 3 cases associated with Maui Island and 1 with Lanai). Although the absolute number of cases was highest for Honolulu, the county-specific incidence rates (per 100,000 person-years) for the study period as a whole were higher for Hawaii (1.4) and Maui (0.7) than Honolulu (0.3) . The case-patients were significantly more likely to have been in Hawaii County than Honolulu County (risk ratio 4.6, 95% confidence interval 1.9–11.1); the comparison between Hawaii and Maui Counties was not significant (data not shown). The increases in annualized incidence rates (cases/100,000 person-years) from the baseline to the cluster periods were statistically significant for Hawaii County (1.1 vs. 7.4; p<0.001) and Maui County (0.4 vs. 4.3; p = 0.03) but not for Honolulu County (0.2 vs. 1.0; p = 0.07) .\n\n【19】### Discussion\n\n【20】This study was prompted by an unusual temporal clustering of 5 reported cases of EM/ _A. cantonensis_ infection from 2 Hawaiian Islands during November 2004–January 2005. Our primary goal was to assess whether these voluntarily reported cases reflected an increased incidence. To accomplish this, we used a laboratory- and hospital-based approach to ascertain symptomatic cases of EM and _A. cantonensis_ infection. To our knowledge, this is the first study to systematically determine incidence rates of EM and _A. cantonensis_ infection for the entire state of Hawaii or any angiostrongyliasis-endemic area. We determined that the incidence of angiostrongyliasis was higher during the cluster period (November 2004–February 2005) than the baseline period (January 2001–October 2004). The overall findings of our study support conclusions specific for Hawaii but also highlight general principles about EM and _A. cantonensis_ infection. In addition, our study may serve as a useful model in other settings. Surveillance of regional laboratory data, coupled with investigation of medical records of case-patients, may help identify temporal and geographic trends for angiostrongyliasis or other diseases.\n\n【21】Our data underscore that EM is an uncommon entity: <1% of the patients whose CSF data were reviewed fulfilled the laboratory criteria for EM. This diagnosis is commonly missed or dismissed, but the presence of eosinophilic pleocytosis is abnormal and should prompt consideration of both infectious and noninfectious causes. In our study, intracranial hardware was the most frequently identified cause of EM (42% of 83 cases). Although the presence of hardware or other foreign material in the CNS is a well-established cause of EM, the possibility of associated bacterial infection should be considered . In our study, EM also was associated with confirmed cases of bacterial and viral meningoencephalitis, as well as idiopathic cases (no microbial etiology identified) in infants evaluated because of fever or failure to thrive.\n\n【22】We found that a substantial proportion of the EM cases in Hawaii were attributable to _A. cantonensis_ infection (29%) and that the proportion was 3-fold higher during the cluster than during the baseline period. This rate increase was particularly notable in Hawaii and Maui Counties. Despite the fact that 23 of the 24 cases were clinically defined, the likelihood of misclassification was low. By definition, none of the case-patients had another possible cause of EM identified. In most angiostrongyliasis-endemic areas, parasitologic confirmation is unusual, and a presumptive diagnosis is typical. Furthermore, Hawaii is hyperenzootic for infection with _A. cantonensis_ but not _Gnathostoma spinigerum_ or _Baylisascaris procyonis_ , 2 other parasites commonly associated with EM. Our confidence that the _A. cantonensis_ cases were correctly classified as such is further increased by the findings of other components of our multifaceted investigations, which included comprehensive epidemiologic and clinical characterization of patients, with longitudinal evaluation of clinical status and sequelae .\n\n【23】One of the limitations of our laboratory/hospital-based study is the likelihood that we underestimated the numbers of cases of EM and _A. cantonensis_ infection. By definition, we did not include persons who were asymptomatic, were not medically evaluated, did not have an LP, did not have CSF data that met specified criteria for EM (e.g. if the LP was performed early or late in the course of infection, few eosinophils might have been noted), or did not meet conservative epidemiologic and clinical criteria. In addition, cases of EM/angiostrongyliasis that were associated with exposures in Hawaii but were diagnosed elsewhere were not systematically ascertained. Cases diagnosed after the end of the study period (February 2005) were not included (specifically, 2 cases reported in March and April 2005 that were associated with Hawaii County). Their existence, however, lends even more credence to the temporal clustering of cases in late 2004–early 2005.\n\n【24】A second limitation is that we cannot exclude the possibility that the temporal increases in frequency of cases were artifactual (e.g. reflected heightened awareness of _A. cantonensis_ infection or decreased thresholds for performing LPs). However, the investigation was prompted by clustering of 5 voluntary case reports during November 2004–January 2005, when EM and _A. cantonensis_ infection were not reportable conditions, and included a parasitologically confirmed case. In addition, for patients who accessed healthcare and had an LP, our methods for case ascertainment were not dependent upon clinicians considering or listing EM or _A. cantonensis_ infection in discharge diagnoses. Our methods were systematic, statewide, and unbiased.\n\n【25】We recognize the limitations and the utility of the incidence data. We calculated incidence rates by generalizing relatively small numbers of cases to the population estimates for particular periods in the state and the pertinent counties. Adjusting the frequency data for the sizes of populations and the durations of periods facilitated comparisons between counties, periods, and causes of EM. The cases of EM not attributed to _A. cantonensis_ served as a useful internal control for the conclusion that the incidence of angiostrongyliasis increased: the incidence of _A. cantonensis_ infection was significantly higher during the cluster period, whereas the incidence of the other EM cases did not increase.\n\n【26】In conclusion, we demonstrated the utility of a comprehensive, laboratory/hospital-based approach for statewide surveillance of EM and _A. cantonensis_ infection in Hawaii. We found a cluster of angiostrongyliasis cases between November 2004 through February 2005 primarily centered in Hawaii and Maui Counties. Furthermore, EM and _A. cantonensis_ infection were often not included in the discharge diagnoses for the case-patients. Our study therefore underscores the need to educate clinicians in Hawaii and elsewhere about EM and its causes, most notably _A. cantonensis_ infection, a potentially severe but preventable infection. Improved detection and reporting may facilitate recognition of clusters of cases and prompt investigations that yield valuable insights about the epidemiologic and clinical characteristics of _A. cantonensis_ infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "10082863-e1f5-4872-be02-1c80dadcb698", "title": "Noninvasive Method for Monitoring Pneumocystis carinii Pneumonia", "text": "【0】Noninvasive Method for Monitoring Pneumocystis carinii Pneumonia\n_Pneumocystis_ pneumonia remains a leading opportunistic infection associated with AIDS patients, even in the era of highly active antiretroviral therapy . In developing countries, the incidence of infection has increased dramatically, with mortality rates ranging from 20% to 80% . An important limitation in its clinical management has been the inability to evaluate therapeutic response or to temporally measure the organism numbers because of the absence of an in vitro culture system. Our laboratory recently showed that the presence of _Pneumocystis carinii_ –specific amplicons obtained from swabs of the oral cavities of nonimmunocompromised adult rats _(Rattus norvegicus)_ was predictive of the development of _P. carinii_ pneumonia after corticosteroid-induced immunosuppression . In the present study, we applied the oral swab technique in combination with quantification of organism-specific DNA using real-time polymerase chain reaction (PCR) to monitor the progression of infection in the rat model.\n\n【1】### The Study\n\n【2】Thirty-two male Long Evans rats (140–160 g) known to harbor _P. carinii_ were obtained from Room 004 at the Cincinnati Veterinary Medical Unit . All rats produced _P. carinii_ amplicons from initial oral swab samples taken before immunosuppression. After sampling, 8 of the 32 rats were euthanized and their lungs were removed and processed as described below. The remaining 24 rats were removed from the room and individually caged under barrier conditions, as described previously , to prevent transmission of infection that might occur between cage mates or from the environment. Barrier conditions consisted of the following: microisolator tops for each shoebox cage, which was then housed within a BioBubble (The Colorado Clean Room Company, Fort Collins, CO); autoclaved water, into which a sterile solution of cephadrine (Velosef; E.R. Squibb and Sons, Inc. Princeton, NJ) was injected for a final concentration of 0.200 mg/mL; autoclaved cages, bedding, and tops; and irradiated Lab Chow (Tekmar Irradiated Lab Chow, Harlan Industries, Indianapolis, IN). To provoke _P. carinii_ pneumonia, 4 mg/kg of methylprednisolone acetate (Depo Medrol; The Upjohn Co. Kalamazoo, MI) was administered to the rats weekly for 10 weeks. At 4 and 7 weeks, swab samples were obtained from groups of eight rats; the rats were then euthanized. Their lungs were removed for quantification by microscopic enumeration of organism nuclei expressed as log nuclei/mL  and real-time PCR analysis under aseptic conditions. Six rats survived the 10 weeks of immunosuppression and were processed in an identical manner.\n\n【3】DNA was extracted from the oral swabs (OS) and lung homogenate (LH), as previously described . LH DNA was evaluated by spectrophotometric analysis at 260 and 280 nm. RC primers directed to a region of the mitochondrial large subunit rRNA (mtLSU) were used for amplification of _P. carinii_ –specific DNA .\n\n【4】Real-time PCR was performed and results were analyzed on the iCycler iQ Real-Time PCR Detection System (BioRad Laboratories, Hercules, CA) under conditions of rapid melting at 95°C, annealing for 5 s at 55°C, and collection at 76° C for 10 s with 40 cycles of amplification. Five microliters of a 1/5 dilution of OS DNA or 2.5 ng of LH DNA were used in the reactions. Taq DNA (1.25 U) polymerase (Promega, Madison, WI) was used in the real-time PCR with a concentration of 2.5 mM MgCl 2  in 25-μL reactions. To monitor the accumulation of the products, 0.4 μL of 1/1,000 dilution of concentrated SYBR Green (Molecular Probes, Eugene, OR) was included in the reactions. All reactions were performed in triplicate. The mtLSU product was cloned into the TOPO-TA PCR cloning vector (InVitrogen, Carlsbad CA) (mtLSU-T-TA), quantified by spectrophotometry, and used to generate a standard curve. The cloned PCR product, ranging from 0.0005 pg to 0.5 pg per reaction, was used as a template; the threshold cycles (C T  s) of these reactions were then plotted against the log amount of plasmid per reaction in picograms.\n\n【5】_P. carinii_ DNA in the LH and OS samples was quantified by linear regression analysis of the C T  s relative to the standard curve . The concentration of _P. carinii_ DNA in the LH and OS samples, determined from the standard curve in picograms, was converted to copies per milliliter by multiplying by the dilution factor based on the original concentration of DNA. The LH copies were log transformed and expressed as log copies per milliliter. The specificity of the reactions was verified by analysis of the product-melting curves and by gel electrophoresis. All products were of the expected size (137 bp) and produced a single peak with a T m  of approximately 78°C.\n\n【6】Microscopic enumeration of nuclei of the lung homogenates was compared to real-time PCR lung homogenate results by using Tukey-Kramer Multiple Comparisons post-test to assess significance (InStat version 3; GraphPad Software, Inc. San Diego, CA). Pre- and postimmunosuppression OS samples were analyzed with the Mann-Whitney test (InStat v. 3). Spearman Rank Correlation was used to evaluate the correlation between microscopic enumeration and the real-time PCR output (Instat v.3).\n\n【7】To ensure accurate and reproducible results, the efficiency of the real-time PCR with the RC primer set was evaluated for each type of sample used in this study: mtLSU/T-TA, LH DNA, and OS DNA . The exponential amplification and efficiency of the reactions were determined by evaluating the slope of the curve generated by plotting the log of known concentrations of template DNA vs. their C T  s . The RC primer set demonstrated acceptable levels of exponential amplification and efficiency with all three templates.\n\n【8】The organism numbers in lung tissue, quantified by microscopic enumeration, increased from log 4.69 after 4 weeks of immunosuppression to log 9.35 after 10 weeks of immunosuppression . No organisms were detected in the lungs of the eight rats euthanized before the study began (level of sensitivity = ~10,000 nuclei per lung). The amount of _P. carinii_ –specific DNA quantified by real-time PCR in the LH samples increased substantially from 0 to 7 weeks, with similar levels after 7 and 10 weeks of immunosuppression . Only one of eight rats euthanized at the initiation of the experiment produced quantifiable copies of _P. carinii_ –specific DNA, with a level similar to those after 4 weeks of immunosuppression (data not shown). In every case, the postimmunosuppression OS taken from the rats at 4, 7, and 10 weeks had significantly more _P. carinii_ –specific DNA than the preimmunosuppression OS taken at the initiation of the study . The amount of _P. carinii_ –specific DNA in the OS samples also increased over time . No significant correlation was found between the amount of _P. carinii_ DNA detected in the preimmunosuppression OS samples and the amount in the postimmunosuppression OS samples, the lung homogenates, or nuclei number, suggesting that the rats had equivalent but low levels of organisms at the initiation of the study.\n\n【9】To determine the relationship between quantitation of _P. carinii_ by real-time PCR and by microscopic enumeration, results were analyzed by Spearman Rank Correlation . A significant correlation was found between both the amount of _P. carinii_ DNA detected in the postimmunosuppression OS samples and in the LH versus the number of _P. carinii_ nuclei. A significant correlation was also detected between the real-time PCR quantitation of _P. carinii_ DNA in the OS and the LH.\n\n【10】### Conclusions\n\n【11】The combination of antemortem oral swab sampling and real-time PCR amplification and quantification reported here should be useful for the study of the _Pneumocystis_ infections in other experimental models and provides a rationale for similar studies to be conducted in the clinical setting. Real-time PCR previously has been shown to be useful for quantitation of the level of infection in the lungs of infected rats and mice, but the studies were performed on postmortem samples or purified organisms  _P. jiroveci_ DNA levels from oral washes, induced sputa, and bronchoalveolar lavage fluids from humans have been quantified by using various real-time PCR techniques  as well, but the findings were used for diagnosis, detection, or quantification and did not obtain samples from individual hosts over time. In our study, the levels of _P. carinii_ DNA in the oral cavities of the rats were measured temporally and shown to correlate with the numbers of organisms in the lungs, establishing the oral swab real-time PCR technique as a surrogate means of following the progress of the infection. Successful application of this method to the human infection would enhance epidemiologic studies, permit sensitive and rapid assessment of therapeutic response, and allow basic biologic questions of carriage length and potential reservoirs to be addressed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6127e9fd-87e0-4df8-8fd9-8ba433e6f30c", "title": "Highly Pathogenic Avian Influenza A(H5N8) Virus from Waterfowl, South Korea, 2014", "text": "【0】Highly Pathogenic Avian Influenza A(H5N8) Virus from Waterfowl, South Korea, 2014\n**To the Editor:** To date, 18 hemagglutinin (HA) subtypes and 11 neuraminidase (NA) subtypes have been identified in influenza A viruses . Influenza A viruses containing HA subtypes 1–16 circulate in aquatic birds , whereas those harboring HA subtypes 17 and 18 are found in bats .\n\n【1】On January 18, 2014, the government of South Korea reported an outbreak of highly pathogenic avian influenza A(H5N8) virus in breeding ducks in the southern part of Jeollabuk-Do Province . More than 12 million poultry have since been culled, but the spread of the virus continues in duck and chicken farms. We report the genetic characterization of this virus.\n\n【2】On February 15, 2014, a total of 200 fecal samples were collected from waterfowl in the Pungse River in Chungnam Province, which is geographically close to Jeollabuk-Do Province. All samples were inoculated into hens’ eggs, and influenza A viruses were confirmed by PCR by using influenza A–specific nucleoprotein (NP) primers. We obtained 1 isolate, A/waterfowl/Korea/S005/2014 (H5N8), and sequenced the full regions of all 8 genes as described . These sequences were deposited into GenBank under accession nos. KJ511809–KJ511816.\n\n【3】We conducted a BLAST search  to identify the closest gene sequences to those of A/waterfowl/Korea/S005/2014 (H5N8) . Sequences for polymerase basic (PB) 2 (99% homology), HA (97% homology), and NP (99% homology) genes were closely related to those of A/wild duck/Shandong/628/2011 (H5N1). Sequences for PB1 (99% homology), polymerase acidic subunit (PA) (98% homology), matrix (M) (99% homology), and nonstructural (NS) (99% homology) genes were closely related to those of A/duck/Jiangsu/1-15/2011 (H4N2). Sequences for the NA (98% homology) gene were closely related to that of A/duck/Jiangsu/k1203/2010 (H5N8). Phylogenic analysis showed that all 8 genes of A/waterfowl/Korea/S005/2014 (H5N8) belonged to the Eurasian lineage, and that the HA gene clustered with clade 2.3.4 .\n\n【4】We further analyzed the amino acid sequences of the virus isolate . Positions 138 and 160 of the HA protein (H3 numbering) contained an alanine (A) residue, which was previously found to be related to enhanced binding to the human influenza receptor . The connecting peptide of HA contained an insertion of 4 basic amino acids (arginine-arginine-arginine-lysine), which is the same as in the HA of A/duck/Korea/Buan2/2014 (H5N8), an isolate from a duck farm in South Korea . Aspartic acid was found in M1 at position 30 and alanine at position 215; this combination has been connected with increased virulence in mice . The NS1 sequence contained serine at position 42, which is related to the enhanced pathogenicity in mice, but a truncation of the amino acids at positions 218–230 that has been linked with reduced pathogenicity in mice  was not identified. Asparagine was identified at position 31 of M2, which is the same in M2 of A/duck/Korea/Buan2/2014 (H5N8) and confers resistance to amantadine and rimantadine .\n\n【5】Because all 8 genes of A/waterfowl/Korea/S005/2014 (H5N8) are closely related to those of the A/duck/Korea/Buan2/2014 (H5N8) isolate that was obtained from a duck farm, it is likely that A/waterfowl/Korea/S005/2014 (H5N8) originated from infected waterfowl that had visited poultry on an infected farm . Our laboratory has studied the feces of wild birds in Chungnam Province since 2009, surveying >20,000 fecal samples from wild birds in this area each year, but we had not previously isolated avian influenza A(H5N8) virus from any samples.\n\n【6】The genetic analysis of the A/waterfowl/Korea/S005/2014 (H5N8) isolate indicates that this novel strain may have been created by the reassortment of PB2, HA, and NP segments from H5N1-like avian influenza virus; PB1, PA, M, and NS segments from H4N2-like avian influenza virus; and NA segments from H5N8-like avian influenza virus . Most genes of the virus we isolated are related to those of avian influenza viruses isolated in China, but the HA gene of A/waterfowl/Korea/S005/2014 (H5N8) showed only 97% homology to the closest HA gene in GenBank, which indicates that this gene may have been created in poultry in South Korea. To our knowledge, no outbreak of this virus in poultry farms in China has been reported, and we found no previous reports in the literature that migratory birds could carry the virus. Taken together, our data suggest that A/waterfowl/Korea/S005/2014 (H5N8) may have been reassorted in a duck farm in South Korea.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b1f58a42-0afc-45e7-a272-e98e94b5fbd0", "title": "Integrated Food Chain Surveillance System for Salmonella spp. in Mexico", "text": "【0】Integrated Food Chain Surveillance System for Salmonella spp. in Mexico\nDiarrheal diseases are leading causes of childhood illness and death in developing countries . Many of these infections are acquired through contaminated food and water, but the fraction attributable to each food category is unknown. In the face of worldwide increase in antimicrobial-resistant foodborne pathogens (FBP) and the growing globalization of travel and food trade, the World Health Organization has recommended the establishment of national networks that conduct surveillance along the entire food chain . Industrialized nations have implemented FBP surveillance systems that include a combination of passive, active, and outbreak data sources . In certain countries, outbreak data constitute a considerable proportion of human surveillance .\n\n【1】In developing countries, information on food-borne disease is scant. Passive surveillance systems are generally inadequate because 1) patients with diarrhea do not seek medical attention; 2) appropriate samples are not sent for culture; or 3) physicians may not report culture-based cases, including deaths, to a public health reference center. Similarly, outbreak information is frequently unsubstantial, either because health authorities lack the capabilities or resources for detection, or presumably, because diarrheal diseases are highly endemic and outbreaks may be less common or obvious than in industrialized countries. Furthermore, comparability between human, food, and animal data is hindered by the lack of standardized laboratory methods and the absence of joint collaboration between the medical, food, and veterinary sectors.\n\n【2】Integrated food chain surveillance systems (IFCSs) are presently established in only a few industrialized countries. Before this study, national data on FBD in Mexico were based on the syndromatic diagnosis of diarrhea. Beginning in 2002, we established an IFCS for _Salmonella_ spp. _Campylobacter_ spp. _,_ and _Escherichia coli_ in 4 states from geographically different regions. The system enabled us to identify important human health risks and to establish future research and public health priorities. This article summarizes the data on _Salmonella_ spp. obtained from March 2002 through August 2005.\n\n【3】### Materials and Methods\n\n【4】##### Study Setting and Epidemiologic Design\n\n【5】The food safety authorities in Mexico (Sistema Federal Sanitario, Secretaría de Salud) divide the country’s 32 states into 5 regions. The 4 states included in this network belong to and are representative of each of the 4 largest regions as follows: Sonora (Region I, Northwest), San Luis Potosi (Region II, Gulf-Central), Michoacan (Region III, Central-Pacific), and Yucatan (Region V, Southeast). In all states, food-animal production is a major economic activity, and most of the circulating retail meat is locally produced. Active surveillance was initiated in 2002 with samples from ill and asymptomatic persons and retail pork, chicken, and beef. Collection of food-animal intestines from chicken, swine, and cattle at slaughterhouses was initiated in mid-2003. The number of samples from food animals and retail meat was designed to reflect regional consumption of each meat product and was limited to domestic production only. In each state, 1 city was sampled per month. Cities with large populations were sampled on repeated occasions; however, different retail outlets and kindergartens were selected for each sampling session. The sampling scheme was designed to follow the food chain in a temporal fashion. Food-animal intestines were collected at each municipal slaughterhouse on day 1, followed by raw retail meat on days 2–4, and fecal samples from asymptomatic kindergarten children on days 7–14. To make the samples in each city as representative as possible, meat was purchased from at least 3 different retail outlets, and only those kindergartens with \\> 40 children were selected for the study. Samples from ill children (which included those with severe and moderate diarrhea as well as those with systemic infections) were collected at the major state government hospitals through active surveillance.\n\n【6】Network activities were incorporated into existing programs at each state health department to increase the likelihood of long-term sustainability of the IFCS. Department inspectors conducted the slaughterhouse surveillance, and diarrheal diseases were monitored at the oral rehydration units and pediatric emergency services that participated in national programs for acute enteric diseases.\n\n【7】##### Study Definitions and Ethical Considerations\n\n【8】Case definitions for diarrhea and asymptomatic children have been described . Socioeconomic indicators previously used to measure poverty , such as literacy, household sanitary infrastructure, and income, were obtained from recent national population surveys . The internal review boards and ethics committees at all participating institutions approved the protocol, and written informed consent was collected from all participants or their guardians to obtain fecal samples and use the clinical and microbiologic information for scientific studies.\n\n【9】##### Microbiologic Methods\n\n【10】Participating laboratories used the same standardized methods for isolating and identifying _Salmonella_ spp. from human and nonhuman specimens . Biochemically confirmed isolates were serotyped according to the Kaufmann-White scheme . All isolates were tested for susceptibility to ampicillin, chloramphenicol, ceftriaxone, ciprofloxacin, gentamicin, kanamycin, nalidixic acid, streptomycin, sulfisoxazole, trimethoprim-sulfamethoxazole, and tetracycline by disk diffusion according to standard guidelines . External quality control was performed twice a year by the coordinating center in Yucatan. MICs for ceftriaxone, ciprofloxacin, and nalidixic acid were determined by agar dilution . For purposes of this study, the terms “resistant” or “resistance” refer to strains with zone diameters below or MICs above the susceptible breakpoint. For ciprofloxacin, resistance was defined as an MIC \\> 2 μg/mL. Because isolates with an MIC to ciprofloxacin of 0.12–1.0 μg/mL have been associated with therapeutic failure , they were included in the analysis and are referred to in this article as “decreased susceptibility.” Isolates that were resistant to ceftriaxone (MIC \\> 16) were further tested for susceptibility to piperacillin, ticarcillin, aztreonam, cefoxitin, ceftazidime, cefotaxime, ceftiofur, cefepime, and imipenem, and also screened for extended spectrum β-lactamases by disk diffusion using ceftazidime and cefotaxime with and without clavulanic acid . Data analysis was performed with Whonet software version 5.3 .\n\n【11】##### Pulsed-field Gel Electrophoresis\n\n【12】Because of its capacity for virulence and multidrug resistance, _S._ Typhimurium isolates were selected for pulsed-field gel electrophoresis (PFGE) analysis to determine genetic relatedness among human, retail meat, and food-animal isolates from all 4 states. PFGE was performed according to the standard protocol developed by the Centers for Disease Control and Prevention , which used digestion by _XbaI_ . Results were analyzed by using the BioNumerics Software version 3.0 (Applied-Maths, Kortrijk, Belgium), and banding patterns were compared by using Dice coefficients with a 1.5% band position tolerance. A cluster was defined as a group of \\> 2 strains that shared a unique PFGE restriction pattern.\n\n【13】##### Statistical Methods\n\n【14】The χ 2  test was used for comparison of proportions, with an α value of 0.05. Association between type of infection (diarrhea-associated vs. asymptomatic) and the isolation of an extended-spectrum cephalosporin (ESC)–resistant _S_ . Typhimurium were analyzed in a 2 × 2 table, and the odds ratio (OR) and its 95% confidence interval (CI) were calculated. Associations between the median rates of _Salmonella_ spp. in retail chicken, pork, and beef and the median percentage of asymptomatic _Salmonella_ spp. infection in children for each city were calculated with Pearson’s correlation coefficient (r) using SPSS software version 10.0 (SPSS Inc. Chicago, IL, USA).\n\n【15】### Results\n\n【16】##### Epidemiologic Surveillance\n\n【17】Socioeconomic indicators and the rates of meat contamination and human _Salmonella_ infections for each state are compared in Table 1 . Socioeconomic levels for each state were graded on the basis of the 4 indicators shown in the table; Yucatan was classified as the state with the lowest level, followed by San Luis Potosi, Michoacan, and Sonora (highest level). One hundred forty-one sampling sessions were conducted in 64 cities. Samples from all 3 retail meats and kindergarten children were available for 61 cities in the 4 states for correlation analysis. In 11 cities in the state of Sonora, a strong direct correlation was found between each city’s prevalence of beef contamination and its prevalence of asymptomatic _Salmonella_ spp. infection (r = 0.91, p<0.001). A moderate correlation was found between pork meat and asymptomatic infection (r = 0.62, p = 0.04). No association was observed for retail meat contamination and human asymptomatic infection in the cities from the other 3 states.\n\n【18】_S_ . Typhimurium and _S_ . Enteritidis were the top 2 serovars isolated from ill humans (22.2% and 14.5%, respectively). Among food animals, swine were the most important reservoir of _S_ . Typhimurium (10.2% of _Salmonella_ spp. isolates), and chickens were the main reservoir of _S_ . Enteritidis (11.9% of _Salmonella_ spp. isolates) . Both humans and animals harbored a considerable diversity of serovars, ranging from 47 to 56 serovars among the different sources. A total of 392 isolates were collected from clinically ill humans. Of these, 26 were isolated from patients with bacteremia and meningitis; 20 (77%) of these isolates were _S_ . Typhimurium , _S_ . Enteritidis , and _S._ Typhi .\n\n【19】##### Antimicrobial Drug Resistance\n\n【20】The percentages of isolates from human and food-animal sources that were resistant to antimicrobial agents are given in Table 3 . Antimicrobial drug resistance was highest in ill humans and swine. Resistance to nalidixic acid was highest in _S._ Albany (57%, 65/115) and _S_ . Enteritidis (53%, 36/68) from chicken and in _S_ . Typhimurium (74%, 133/180) and _S_ . Anatum (31%, 92/294) from swine and cattle. Resistance to ciprofloxacin emerged in 2003 in _S_ . Heidelberg (10.4%, 5/48) and _S_ . Typhimurium (1.7%, 2/127) from swine. Decreased susceptibility to ciprofloxacin was detected in 16.4% of all _Salmonella_ spp. isolates (545/3,315) and was most commonly found in chickens (24.9% of all isolates from source), followed by swine (18% of all isolates from source) and ill humans (17.3% of all isolates from source). ESC resistance was first detected in serovar Typhimurium in 2002. From 2002 to 2005, ESC resistance increased from 1.6% to 4.9% and was detected in 6 other serovars. Isolates resistant to ceftriaxone were also resistant to piperacillin, ticarcillin, cefoxitin, ceftazidime, cefotaxime, ceftiofur, and aztreonam and did not show increased susceptibility in the presence of clavulanic acid, which suggests the presence of an AmpC-like β-lactamase. ESC resistance was highest in _S_ . Typhimurium (42% of all ST isolates, 132/314), followed by _S._ Bredeney (7.1%, 1/14), _S_ . Newport (6.3%, 4/64), _S_ . Reading (2.4%, 2/85), _S_ . Uganda (2.4%, 1/42), _S_ . Kentucky (2.2%, 1/46), and _S_ . Anatum (0.5%, 2/365).\n\n【21】_S_ . Typhimurium showed particularly high antimicrobial-drug resistance rates in both humans and food animals . High resistance rates to ESCs were observed in poultry (77.3%), ill humans (66.3%), and swine (40.4%); multidrug resistance to \\> 4 antimicrobial drug classes was found in 86.6% of isolates. ST isolated from an ill patient was 6 times more likely to be ESC-resistant than isolates from asymptomatic children (OR 6.3, 95% CI 2.3–17.6; χ 2  14.4, p<0.001).\n\n【22】##### PFGE\n\n【23】The network collected 314 _S_ . Typhimurium isolates, of which 311 were available for PFGE . A total of 126 PFGE patterns were identified. Fourteen clusters (boxes A–N), comprising a total of 102 strains (37 human, 37 retail meat, and 28 food-animal isolates), were common to both humans and food animals. Three patterns  were common to humans, and all 3 food animals, and 1 pattern  was found in all 4 states. For each state, we found clusters of human isolates that were indistinguishable or closely related to those found in retail meat, food animals, or both.\n\n【24】### Discussion\n\n【25】Our experience shows that the establishment of an IFCS is technically and economically feasible in a developing country such as Mexico. The system effectively identified the serovars that caused the greatest effects of disease as well as major animal reservoirs of these serovars in a setting where _Salmonella_ spp. infections are highly endemic and passive surveillance is inadequate. The epidemiologic design also corrected constraints present in other monitoring systems, such as the temporal and spatial dissociation of human, food, and animal isolates, as well as the lack of uniform laboratory methods.\n\n【26】Furthermore, the system yielded epidemiologically meaningful data that provided evidence of the magnitude of _Salmonella_ spp. as a health risk. Our average prevalence of _Salmonella_ spp. contamination in retail meats (21.3%–36.4%) and the high frequency of human _Salmonella_ spp. infection, in conjunction with PFGE clusters of geographically and temporally related human and food-animal isolates, led us to conclude that food animals are a major source of salmonellosis in Mexico. A proportion of these clusters may constitute undetected outbreaks; however, this can only be determined once reliable baseline data have been obtained.\n\n【27】Higher rates of meat contamination were observed in those states with higher poverty levels. This finding can likely be explained by the multiple inadequacies in the sanitary infrastructure that lead to increased contamination and dissemination of FBP throughout the environment, in particular, along the food chain. Interestingly, in Sonora, the state with the lowest poverty level that borders the United States, cities with high rates of beef contamination (and to a lesser degree, pork contamination) also had high rates of asymptomatic carriage of _Salmonella_ spp. among children. Sonora is one of the major beef- and pork-producing states in the country, and these retail meats are consumed more frequently than chicken by the local population, which might explain the absence of an association with the latter. Conversely, those cities with low rates of meat contamination had lower rates of asymptomatic carriage. Such associations, however, were not found in the other 3 states with higher poverty levels. The conditions in Sonora may more closely resemble the transmission of _Salmonella_ spp. infections in industrialized countries, where most infections are acquired through the food chain , whereas in the other states _Salmonella_ spp. infections are probably acquired by other modes of transmission aside from contaminated food, such as from person to person or by contact with animal feces. In settings with greater fecal-oral transmission, asymptomatic infections would not directly reflect contamination rates in the retail meat. Our findings suggest that cattle, swine, and chicken are important, but not unique, reservoirs of _Salmonella_ spp. in Mexico. Other animals not included in this study may contribute to an important proportion of these infections, and therefore, other food categories should be included in the surveillance system in the future.\n\n【28】The high rates of ESC resistance and the emerging resistance to fluoroquinolones are other important public health risks detected by the system. Our increasing resistance rates are consistent with a worldwide upsurge of multidrug-resistant nontyphoidal _Salmonella_ spp. Multidrug resistance in our _S_ . Typhimurium isolates is the most pressing concern. _bla_ CMY-2  _S._ Typhimurium is now a major public health problem in Yucatan, where its prevalence increased from 0% in 2000 and 2001 to 75% in 2004 and 2005 and where it has caused fatal infections in young infants . The higher rate of ESC resistance in ill children with _S_ . Typhimurium infection than in asymptomatic children has 2 possible explanations. The first is that these children received antimicrobial drugs shortly before their diarrheal episode, a known risk factor for acquiring multidrug-resistant _Salmonella_ spp. infection . A second possibility propounded by other investigators is that antimicrobial drug–resistant _Salmonella_ spp. is associated with increased virulence . We believe that the higher prevalence of ESC resistance in our ill children is of substantial public health importance and that the mechanisms leading to this increased resistance should be further investigated.\n\n【29】Because the network is in its early stages, it does not yet have the capabilities of a mature surveillance system. At present, it is not designed to measure the human health impact of contaminated food consumption or to perform cost-benefit analyses of prevented deaths or reduced days of hospitalization. Furthermore, it does not have a high sensitivity for outbreak detection. All of these capabilities need to be developed in the future. The main objective of the initial phase of the IFCS  was to collect baseline data with good internal validity. Enormous effort was devoted to training staff and achieving good laboratory quality control. During subsequent phases, priority should be given to measuring illness, proportion of deaths, and the economic impact of salmonellosis. This goal could be achieved by increasing the number of sentinel hospitals per state and organizing the systematic collection of clinical and cost-related data. Furthermore, future surveillance should include animal farms and data on local food consumption and antimicrobial usage. Lastly, due to its nature as an early stage study, the system yielded predominantly descriptive information. These data have served to generate several hypotheses that can be tested in future research.\n\n【30】The network described in this study included 4 representative states and was originally established in collaboration with state health authorities. It is currently being transferred to the federal food safety authorities. Based on this study, we believe that a sustainable IFCS could include a 5-state network (one from each representative region). Operating costs (laboratory supplies and staff salaries) for this system, including PFGE analysis for the top 2 serotypes found in ill children, would run US $500,000 per year, which is equivalent to ≈2% of the Federal Sanitary System’s yearly budget .\n\n【31】In conclusion, this project can serve as a model for developing countries to establish an IFCS. We believe that, in the initial stages, efforts should focus on training of staff; laboratory quality control; and achieving good integration of human, food, and animal data. Once this stage has been consolidated and good baseline data have been obtained, the system can become increasingly complex according to needs and available resources. In a period of 3 years and with a modest financial investment, our system was able to identify several important emerging public health threats that should be the focus of future evidence-based interventions. The reduction of _Salmonella_ spp. contamination throughout the food chain, as well as the prevention and control of ESC-resistant and fluoroquinolone-resistant _Salmonella_ spp. in food animals and retail meats, should receive major priority. The striking differences between the various states can be used by policy makers to design interventions and allocate resources by region to reduce the effects of FBD in those areas with the greatest risk. Finally, control measures instituted throughout the food chain should be linked to other national programs that include the use of oral rehydration therapy, the promotion of breast feeding, and improved nutrition as part of a joint effort to reduce illness and death from diarrheal disease .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "18a1b027-0795-46e7-82f9-119b1b28c1ad", "title": "Drug-Resistant Tuberculosis in Zhejiang Province, China, 1999–2008", "text": "【0】Drug-Resistant Tuberculosis in Zhejiang Province, China, 1999–2008\nIn 2009, China reported results of a nationwide drug resistance survey, which found that 5.7% of new cases of tuberculosis (TB) and 25.6% of re-treated cases were infections with multidrug-resistant TB (MDR TB), i.e. resistance to isoniazid and rifampin . These results indicated that in 2008 in China, MDR TB developed in ≈100,000 persons, which is ≈25% of the total number of TB cases  and similar to that in India .\n\n【1】In China, in addition to the 2008 national survey of TB drug resistance and 10 annual national TB surveys, surveys of TB drug resistance have been conducted in several provinces . Zhejiang is one of the few provinces that have conducted a series of cross-sectional surveys from which we can evaluate the scale of the drug-resistance problem at one time point and changes over time.\n\n【2】Data from a sequence of surveys are vital in assessing evolution of resistance to TB drugs in China and ultimately in evaluating the effect of control measures. We report findings of 3 cross-sectional surveys, 1 each of which conducted in Zhejiang in 1999, 2004, and 2008. These surveys included prevalence of MDR TB among TB cases diagnosed in clinics, trends, and risk factors for resistance to isoniazid and rifampin singly and in combination.\n\n【3】### The Study\n\n【4】We would need 784 cases (i.e. 1.96 2  0.05  0.5(1 − 0.5)/(0.07/2) 2  \\= 784) to achieve 95% precision and a margin of error of 7%, and assume no prior knowledge of prevalence of drug resistance, to measure prevalence of any form of drug resistance across the entire province (i.e. not enabling stratification) in each year. Assuming that \\> 10% of samples would be lost, we sought to obtain 900 cases. With no prior information for prevalence of drug resistance at different sites, we randomly selected 30 TB treatment centers in 30 counties (among 90 centers in Zhejiang Province) and anticipated that each center would recruit \\> 30 sputum smear–positive patients. Three surveys were conducted at the same 30 sites to obtain the same sample size in each year .\n\n【5】Sputum was collected from persons with suspected TB who came to clinics for a diagnosis. Three sputum samples were obtained (morning, midday, and evening), and patients with \\> 2 samples with positive sputum smear results were enrolled in the study. Drug sensitivity tests were performed in provincial reference laboratories by using the percentage method, and results were compared with results for standard drug-resistant strains . Quality of provincial reference laboratories was ensured by the Republic of Korea Supranational Reference Laboratory (Seoul, South Korea) during 3 surveys, and was evaluated annually by the national reference laboratory in China. Recruitment of consecutive case-patients continued until \\> 30 (often more) were enrolled at each site. Each case-patient completed a questionnaire on medical and medication history.\n\n【6】New cases, re-treatment cases, and cases of MDR TB were defined as described by the World Health Organization . Prevalence of resistance to isoniazid and rifampin or MDR TB was defined as the number of resistant cases in patients who were given a diagnosis of TB in clinics and tested for drug resistance.\n\n【7】Although surveys were not designed a priori to evaluate time trends for prevalence of drug resistance, we investigated trends by using repeated measures analysis of variance and making appropriately cautious conclusions. Logistic regression models were used to investigate factors associated with single drug resistance and MDR TB. Statistical analysis was performed by using SPSS version 17.0 (SPSS, Inc. Chicago, IL, USA).\n\n【8】Totals of 1,013, 984, and 938 MDR TB case-patients were recruited from routinely diagnosed new and re-treatment case-patients in 1999, 2004, and 2008, respectively . In the 3 surveys, 71%, 74%, and 69% of cases were in men, and 17%, 16%, and 10% were re-treatment cases.\n\n【9】In the 3 surveys, average prevalence of new cases resistant to isoniazid and rifampin and having MDR TB was 10.5% (95% CI 8.4%–12.5%), 5.1% (95% CI 3.6%–6.6%), and 3.3% (95% CI 2.1%–4.5%), respectively. Equivalent percentages among re-treatment cases were 38.5% (95% CI, 28.8%–48.2%), 33.3% (95% CI 23.9%–42.7%), and 29.2% (95% CI 20.1%–38.3%), respectively.\n\n【10】Compared with new cases, re-treatment cases were more likely to be resistant to isoniazid (odds ratio \\[OR\\] 1.8, 95% CI 1.2–2.7) and rifampin (OR 6.3, 95% CI 4.2–9.5) or to have MDR TB (OR 9.0, 95% CI 6.4–12.7) . Resistance to isoniazid was strongly associated with resistance to rifampin and vice versa (models 1 and 2; OR 19.9, 95% CI 13–31) .\n\n【11】Prevalence of resistance to isoniazid and MDR TB tended to be lower in case-patients 15–64 years of age than in those \\> 65 years of age (models 1 and 3), but this effect was not shown for rifampin (model 2) . There was no significant difference in prevalence of resistance between male and female case-patients across all surveys (models 1–3) .\n\n【12】Prevalence of isoniazid and rifampin resistance and MDR TB changed little across the 3 surveys among new and re-treatment cases . Time trends for isoniazid prevalence (increase) and MDR TB (decrease) among new cases were marginally significant ( _F_ \\= 3.33, p<0.05, and _F_ \\= 1.13, p<0.05) but in opposite directions. There were no significant trends in resistance among re-treatment cases or among men or women.\n\n【13】### Conclusions\n\n【14】Approximately 25% of the world’s MDR TB cases are in China, and it is vital to know whether this number is increasing, decreasing, or stable. There are few data with which to judge trends in drug resistance in China, although a few regions, including Shanghai municipality , Shenzhen Province , and Zhejiang Province (this study), have conducted cross-sectional surveys.\n\n【15】The principal finding of our study is that although drug-resistant TB needs careful management in Zhejiang Province , prevalence of isoniazid and rifampin resistance and MDR TB monitored at the same 30 sites changed little during 1999–2008. Although surveys were not designed to detect time trends in drug resistance, prevalence of MDR TB decreased from 8.6% in 1999 to 6.0% in 2008. This decrease in Zhejiang was consistent with changes observed during 2000–2010 in 2 national TB prevalence surveys .\n\n【16】Our results contrast with those that MDR TB prevalence increased in nearby Shanghai during 2000–2006. Shen et al. reported that introduction of directly observed treatment, short course, and other improved management practices contained spread of drug resistance in Shanghai after 2004, and introduction of similar practices in Zhejiang may also have stopped the increase in MDR TB after 2002. However, the role of improved TB control practices cannot be shown from these data. Nevertheless, possible differences among different sites underline the need for monitoring resistance trends locally and nationally in China. It is also necessary to monitor treatment outcomes, which will be linked to development of drug resistance. In this context, the percentage of patients who sought re-treatment was lower in 2008 than in previous years, which suggested that case management had improved.\n\n【17】The greatest risk factor for resistance to either isoniazid or rifampin in this study was resistance to the other drug, a finding that indicates the high risk for acquiring MDR TB after treatment failure. In this context, and consistent with previous studies , prevalence of MDR TB was higher among re-treatment cases than new cases. These results also underscore the need for following good management practices as described by the World Health Organization .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "67e521fa-3a83-4068-b8e3-fe122d9bafd6", "title": "Hantavirus Pulmonary Syndrome in a COVID-19 Patient, Argentina, 2020", "text": "【0】Hantavirus Pulmonary Syndrome in a COVID-19 Patient, Argentina, 2020\nThe current coronavirus disease (COVID‐19) pandemic, caused by severe acute respiratory syndrome coronavirus 2 (SARS‐CoV‐2), has resulted in substantial illness and death rates worldwide. Orthohantaviruses are zoonotic viruses responsible for another severe respiratory infectious disease in the Americas, hantavirus pulmonary syndrome (HPS). Although humans generally become infected with HPS through inhaling excreta generated by infected rodents, person-to-person transmission has been well documented in Argentina and Chile . Humans become infected with SARS‐CoV‐2 and orthohantaviruses in similar ways, through inhaling contaminated aerosols, and can have onset of similar respiratory syndromes. Despite these similarities, the incubation period is shorter in COVID-19 patients (2–14 days) than in HPS patients (7–45 days). Furthermore, at the time the case we describe was reported, the cumulative case-fatality rate for COVID-19 in Argentina was 2.7% ; for HPS, it was 22%–40% .\n\n【1】HPS is characterized by the onset of symptoms such as fever, myalgia, cough, dyspnea, diarrhea, and sweating. Rapid progression to shock or respiratory distress can occur within hours. Symptom-based therapy with oxygen and ventilatory or circulatory support is needed .\n\n【2】We describe a case of SARS‐CoV‐2 and Andes virus co-infection in central Argentina. The patient, a 22-year-old woman without relevant pathologic records, sought care at a local hospital in November 2020 for fever, headache, myalgia, and gastrointestinal manifestations. A nasopharyngeal swab sample tested positive for SARS-CoV-2 by reverse transcription PCR at the Instituto Biológico “Tomás Perón” . Five days after the onset of fever, the patient’s clinical status worsened, and she was admitted to the hospital. Clinical laboratory findings at admission indicated thrombocytopenia, high leukocyte count, lymphopenia, and elevated hepatic enzymes . Computed tomography revealed bilateral pleural effusion associated with interstitial infiltration, and capillary filtration with slight peripheral pulmonary ground-glass opacity .\n\n【3】Within a few hours after admission, the patient had onset of marked respiratory distress. She was then referred to the intensive care unit for orotracheal intubation and treated with ampicillin/sulbactam and azithromycin. The epidemiologic investigation established that the patient resided in a hantavirus-endemic area. Consequently, HPS was suspected, despite the COVID-19–positive diagnosis. According to the confirmation criteria used by the Hantavirus National Reference Laboratory , Andes virus infection was confirmed by the detection of specific IgM and IgG by ELISA and genomic viral RNA by quantitative reverse transcription PCR in blood .\n\n【4】Three days after the co-infection was confirmed, the patient was extubated and progressed favorably. Twenty days after onset of symptoms, she was discharged from the hospital.\n\n【5】To determine the viral genotype of Andes virus, we conducted a nucleotide sequence analysis from 2 partial fragments of viral small (496-bp) and medium (611-bp) segments, and we submitted the sequences obtained to GenBank (accession nos. OL840325 and OL840326). The highest nucleotide identities matched previous published sequences corresponding to Plata genotype of Andes virus (GenBank accession nos. EU564720 \\[96% identity\\] and AY101185 \\[97.8 identity\\]). This viral genotype is one of the prevalent pathogenic orthohantaviruses circulating in central Argentina and Uruguay .\n\n【6】Because the incubation period for HPS is longer than that for COVID-19, we might speculate that hantavirus infection occurred before coronavirus infection. The respiratory distress syndrome appeared 5 days after the onset of fever, which coincided with the characteristic prodromal period described for HPS. This condition, during the incubation period of HPS, could have induced a higher susceptibility to COVID-19. Because HPS can evolve rapidly to respiratory failure in most patients with severe disease, resulting in high case-fatality rates, alerting health-care workers from HPS-endemic areas is warranted to detect co-infections in the context of the COVID-19 pandemic. In particular, at least 2 genotypes of Andes virus can be transmitted person-to-person, and these species are prevalent in 2 of the 3 hantavirus-endemic regions of Argentina .\n\n【7】In conclusion, we detected co-infection with SARS-CoV-2 and Andes virus causing HPS in a patient from a hantavirus-endemic area. Clinicians should be aware of the possibility of co-infection for patients originating, residing, or traveling in hantavirus-endemic areas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "38523a0e-76b6-404c-a1f3-40a85c1a98fd", "title": "COVID-19 Vaccination Intent and Belief that Vaccination Will End the Pandemic", "text": "【0】COVID-19 Vaccination Intent and Belief that Vaccination Will End the Pandemic\nThe COVID-19 pandemic has profoundly affected global health and well-being. Since 2020, countries worldwide have experienced high rates of illness and death caused by COVID-19, and many societies have dealt with often stringent outbreak control measures. The successful development of effective vaccines has provided a much-wanted major step toward controlling the pandemic. However, for the vaccines to be successful during outbreak control, a high and equally distributed vaccine uptake is essential. Next to possible barriers of limited COVID-19 vaccine availability and accessibility, vaccine hesitancy can also form a considerable barrier to reaching a high vaccine uptake.\n\n【1】The public acceptance of vaccines has been a global concern for decades. Before the COVID-19 crisis, in 2019, the World Health Organization declared vaccine hesitancy as one of the top 10 global public health threats . Vaccine hesitancy has been defined as a broad range of vaccine-related attitudes and behavior, from having some doubts and delaying vaccinations up to complete refusal of vaccines . Various studies have provided insights into beliefs underlying vaccination hesitancy and vaccination intentions for childhood vaccinations ; influenza vaccinations , including pandemic influenza A(H1N1) vaccination ; and COVID-19 vaccinations . Personal beliefs that are known to have a major role in vaccination decision-making are beliefs about the need for, safety of, and effectiveness of vaccines.\n\n【2】Many studies that examine determinants of vaccination hesitancy, intentions, or behavior (e.g. studies applying the health belief model ) explore beliefs in relatively general terms. For example, surveys may simply ask respondents whether they have concerns about the safety of vaccines. It is useful to have more detailed knowledge of these beliefs for 3 reasons. First, in-depth insights into beliefs can provide more concrete input toward developing well-adapted communication . For example, concerns about safety of vaccines might be related to beliefs about the vaccine production process, long-term side-effects, and composition of vaccines. Such specific beliefs should be addressed in communications. Second, COVID-19 vaccination intentions are likely to be associated with specific beliefs that differ from those found in research during other vaccination campaigns. This reaction might be the case for beliefs about the rapid vaccine development process, the new technologies used (mRNA), and the personal freedom associated with vaccination (through vaccination entry passes). Third, there might be major differences in (the influence of certain) beliefs underlying vaccination decisions between countries or communities  (e.g. due to differences in experiences with the COVID-19 pandemic, information streams, and vaccination campaign history).\n\n【3】Consistent with earlier research on vaccination decisions , we adopted a mental models perspective in studying beliefs underlying COVID-19 vaccination intentions among persons in the Netherlands . This perspective entails a detailed study of interrelated beliefs of a subject, in this case COVID-19 and the COVID-19 vaccinations. These beliefs form a mental model underlying decisions of persons regarding COVID-19 vaccination. By gaining in-depth insights into these various beliefs, we can identify knowledge gaps and misbeliefs that need to be addressed in communications. In addition, by studying which beliefs are useful determinants of vaccination intentions, we aimed to identify beliefs that should be addressed and prioritized in communications to optimize vaccine acceptability and uptake.\n\n【4】### Methods\n\n【5】##### Study Population and Procedure\n\n【6】We conducted an online survey during March 12–22, 2021. At that time, 1.5 million of the 17.5 million residents of the Netherlands were partly or fully vaccinated against COVID-19 (data from March 14, 2021) . We sent the survey (in Dutch) to 6,810 persons in the Netherlands ( \\> 18 years of age) by using an online survey panel . Members from this survey panel were recruited by using random samples of name and address data. The sample invited for participation to this survey was selected to be representative of the general population of the Netherlands ( \\> 18 years of age) on the basis of demographic characteristics.\n\n【7】Panel members provided informed consent for participation to the survey panel. Survey completion took ≈10‒15 minutes. The Clinical Expertise Centre at the National Institute for Public Health and the Environment reviewed the study protocol and determined that the study was exempt from needing further approval from an ethics research committee .\n\n【8】##### Development of Survey Measurements\n\n【9】We measured vaccination intention as follows. All respondents who indicated to have received an invitation for a COVID-19 vaccination but who had not yet been vaccinated were asked, “Do you want to get vaccinated against the corona virus?” Respondents who indicated not yet to be invited for a COVID-19 vaccination were asked, “If you are invited for a COVID-19 vaccination, do you then want to get vaccinated?” Both questions could be answered on a 5-point Likert scale: 1. Certainly not; 2. Probably not; 3. Don’t know; 4 Probably yes; 5. Certainly yes. The answers to both questions were taken together in 1 variable that was called COVID-19 vaccination intention.\n\n【10】To identify the beliefs about COVID-19 and the COVID-19 vaccines that should be assessed in this study, we studied literature on sociopsychological determinants of vaccination intentions to identify main elements in mental models of persons underlying COVID-19 vaccination intentions . In addition, we used recent qualitative studies (survey open answer categories and in-depth interviews) on COVID-19 vaccination acceptability among members of the public in the Netherlands  to identify specific beliefs within the familiar mental model themes and to identify beliefs that might not have been identified in studies on other vaccination campaigns.\n\n【11】We studied beliefs about COVID-19 and COVID-19 vaccines by using this question: “We would like to know what you think about the corona virus/vaccination against the corona virus. For each statement, indicate to what extent it aligns with what you think. I think ….” The question was followed by 25 statements that were scored on a 5-point Likert scale (1 = certainly not to 5 = certainly yes) . The 25 beliefs can be categorized into 7 elements of mental models of persons, namely beliefs about risk for COVID-19 regarding oneself, risk for COVID-19 regarding one’s loved ones, safety of vaccination, effectiveness of vaccination, (social) benefits of vaccination, alternatives to vaccination, social norms regarding vaccination behavior, and accessibility of vaccination.\n\n【12】##### Analyses\n\n【13】We analyzed on responses of (at that time) unvaccinated respondents. Descriptive analyses were performed for vaccination intention and the 25 beliefs about COVID-19 and the vaccines. In addition, we calculated Pearson correlations (2-tailed) between vaccination intention and all 25 beliefs and between all beliefs separately.\n\n【14】We subsequently performed a regression analysis by using Random forest (RF)  in R  to assess the extent to which beliefs explain variance in vaccination intentions, and to identify which specific beliefs are good determinants of vaccination intentions . RF is a machine learning method for regression and classification based on an ensemble of decision trees. This method makes no assumptions about data distribution and is well suited to address 3 complicating features of the study responses for analyses: a dependent variable (COVID-19 vaccination intention) that is not normally distributed, many (partly intercorrelated) independent variables (beliefs), and potentially nonlinear relationships between independent variables and the dependent variable. The RF method has been successfully applied in explaining vaccination intentions  and screening intentions . We controlled the RF regression analysis for age, sex, education level, region of residence, migration background, health status, previous coronavirus infection, being invited for a COVID-19 vaccination, perceived allergy for vaccinations, employment in healthcare, and religious motivations  by adding these as independent variables to the RF model.\n\n【15】We considered 4 types of output from the RF regression analyses. The first output was the variable importance ranking (VIR), which ranks independent and control variables in terms of how much these contribute to explaining the dependent variable. The second output was the partial dependence (also known as marginal means) that indicates the direction and strength of the relationship between the independent and dependent variable. The third output was the cumulative variance explained, which is the variance explained after adding an independent variable to the model in the sequence of the VIR. The fourth output was the total variance explained.\n\n【16】### Results\n\n【17】##### Study Population\n\n【18】The survey response rate was 59% (n = 4.033). The survey population was reasonably comparable to the general population in the Netherlands ( \\> 18 years of age) for demographic characteristics .\n\n【19】##### COVID-19 Vaccination Intention\n\n【20】Most (2,266/3,628, 62.5%) unvaccinated respondents indicated that they would certainly want to get vaccinated against COVID-19, 645 (17.8%) would probably want to get vaccinated, 257 (7.1%) did not know (yet), 213 (5.9%) would probably not want to get vaccinated, and 247 (6.8%) indicated certainly not. The mean ( \\+ SD) response of vaccination intention was 4.2 ( \\+ 1.2).\n\n【21】##### Beliefs about COVID-19 and COVID-19 Vaccines\n\n【22】Descriptive statistics and Pearson correlations (2-tailed) with vaccination intention showed that all 25 beliefs were significantly (p<0.001) correlated with vaccination intention . Correlations between different beliefs about COVID-19 and COVID-19 vaccinations  showed moderate-to-strong correlations between different risk perception beliefs of COVID-19 (for self and loved ones), and, at the same time, mostly weak correlations between these COVID-19 beliefs and the several beliefs about COVID-19 vaccinations. In addition, we observed many strong correlations between the various beliefs about COVID-19 vaccinations, especially in relation to beliefs about the safety of vaccination.\n\n【23】Personal risk perceptions of respondents for COVID-19 were moderate, with values of 2.9 ( \\+ 1.0) for the belief about high likelihood of infection and 3.0 ( \\+ 1.3) for the belief about possibility of severe illness. For loved ones, these COVID-19 risk perceptions were scored relatively higher: 3.5 ( \\+ 1.1) for high likelihood of infection and 3.9 ( \\+ 1.1) for possibility of severe illness. Respondents valued the likelihood to infect others if infected themselves with a value of 3.3 ( \\+ 1.2).\n\n【24】Safety of vaccinations was generally trusted by respondents , but some notable variations in responses were observed. For example, 27.7% of respondents indicated not believing that the side effects of vaccinated were well researched (mean 3.3, SD \\+ 1.3), and 28.3% of respondents believed that the vaccines were developed too quickly (mean 2.7, SD \\+ 1.3). With regard to the effectiveness of vaccinations, respondents believed that vaccines would protect them well against COVID-19 (mean 3.8, SD \\+ 1.1). Respondents seemed somewhat unsure about whether vaccines only protect for a short while (mean 3.1, SD \\+ 1.0) and whether one can still infect others after vaccination (mean 3.0, SD \\+ 1.1). At the time of data collection, scientific knowledge about those last 2 vaccine aspects was also limited.\n\n【25】In terms of (social) benefits of vaccinations, vaccinations were commonly seen as the only way out of the COVID-19 crisis (mean 4.1, SD \\+ 1.2) and as a means to return to a life without COVID-19 restrictions sooner (mean 3.8, SD \\+ 1.3). With regard to alternatives to vaccination, respondents generally did not believe that there were (sufficient) drugs that could cure COVID-19 (mean 1.9, SD \\+ 1.0), and few respondents believed that they were immune to COVID-19 (mean 2.0, SD \\+ 1.1). Some support was found for the belief that one’s good health would protect against COVID-19 (mean 2.8, SD \\+ 1.2).\n\n【26】We found that perceived social norms were generally in favor of vaccination. Beliefs that friends and family expected that the respondent would get vaccinated (mean 4.0, SD \\+ 1.2), and the beliefs that most of the respondent’s family and friends (mean 4.2, SD \\+ 1.0), as well as most residents in the Netherlands (mean 4.0, SD \\+ 0.8), would get vaccinated were largely supported. Accessibility of vaccination did not seem a large obstacle because the belief that getting vaccinated would take a lot of time or effort was not strongly supported among the respondents (mean 2.0, SD \\+ 1.2).\n\n【27】##### Variance in COVID-19 Vaccination Intention\n\n【28】The random forest model explained 76% of the variance in COVID-19 vaccination intentions. This analysis was performed using data for 3,614 of the 3,628 unvaccinated respondents (14 respondents were excluded from the analysis because of missing values). We provide the VIR with the 10 best explaining beliefs . We also provide the cumulative variances explained and partial dependence of the 10 best explaining beliefs . Of these 10 best explaining beliefs, 5 beliefs concern safety of vaccinations, 2 beliefs are about social benefits of vaccination, 2 beliefs concern social norms regarding vaccination behavior, and 1 belief is about effectiveness of vaccination.\n\n【29】There was no clear selection of the 25 beliefs that explained variance in vaccination intention considerably more strongly than all other beliefs. Instead we observe a gradual progression in explanatory value of various beliefs (VIR with all determinants). Because there are many intercorrelations between the beliefs , and many of the beliefs are associated with vaccination intentions, the partial dependence ranges were also small . Our findings confirm that vaccination decisions are made on the basis of a complex web of interrelated beliefs (mental models), rather than on a few independent perceptions. Although a small number of these beliefs can (statistically) explain a large part of the variance in vaccination intentions, one needs to keep in mind that in reality beliefs never stand on their own. This said, the belief “the corona crisis will only end if many people get vaccinated” seems, distinctively, the strongest determinant in the model. By adding only this variable to the model, we can explain 54% of the variance in vaccination intentions.\n\n【30】We conducted sensitivity analyses in which we repeated the main RF analyses for 3 age groups (18–34 years, 35–64 years, and \\> 65 years). We repeated the main analysis with a binary dependent variable, explaining differences between those with low vaccination intentions (original values 1 and 2) and those who were unsure (value 3). Results were consistent with those of the main analyses and did not affect our conclusions.\n\n【31】### Discussion\n\n【32】Our findings provide detailed insights into COVID-19 vaccination intentions and the underlying beliefs about COVID-19 and the COVID-19 vaccines among residents in the Netherlands during 2021. No major knowledge gaps or misbeliefs were observed, but we did observe some considerable concerns with regard to the vaccine development and approval process. The beliefs assessed in our study explained a large part of the variance in COVID-19 vaccination intentions. Beliefs about the safety of vaccines, (social) benefits of vaccination, social norms regarding vaccination behavior and the effectiveness of vaccines were, relative to other beliefs, strong determinants of vaccination intention for persons. The strongest determinant in the model was the belief “the corona crisis will only end if many people get vaccinated.”\n\n【33】Our study results showed strong beliefs, and the explanatory value of these beliefs, about (social) benefits after being vaccinated or reaching a high vaccination coverage. The belief that the COVID-19 crisis will only end if many persons get vaccinated could (statistically) explain more than half of the variance in COVID-19 vaccination intentions. It is striking that this belief seemed to be, at least somewhat, a better determinant of vaccination intentions than beliefs about personal protection against the vaccine-preventable disease or beliefs about safety of vaccines, which have often been identified as the most essential psychosocial determinants of vaccination intentions . The wish for relaxation of COVID-19 control measures and for the ending of the enduring crisis seem to have been stronger among many than the wish for personal protection against disease (although these wishes are not mutually exclusive). This finding might be explained by the considerable effect of COVID-19 measures on lives of persons  and the observed moderate COVID-19 risk perceptions. Our results also suggest that persons who did not believe that high vaccination coverage is the only solution to end the COVID-19 crisis were not less likely to vaccinate.\n\n【34】We might expect that over time fewer persons will have the belief of vaccination being the only solution to end the crisis, during the winter of 2021, when lockdown measures were again necessary, despite relatively high vaccination coverage . A decrease in this belief might lead to a decrease in vaccination acceptability. In communications, we are faced with a dilemma. In the short run, providing clear future perspectives regarding personal and societal benefits after reaching a high vaccination coverage, might considerably help in motivating persons to get vaccinated. At the same time, transparency about uncertainties regarding these perspectives are necessary from an ethical point of view, but also to prevent disappointments in the future resulting from too optimistic expectations. Transparency is also crucial in remaining trust and support for control measures .\n\n【35】Consistent with previous research, we found that various beliefs about the safety of the vaccines were major determinants of COVID-19 vaccination intentions . Five of the 10 major explanatory beliefs were related to safety. Four of these 5 beliefs were about vaccine development and approval processes. Rapid development of vaccines and the approval process, and the use of new techniques (e.g. mRNA vaccines) have probably increased public concerns about vaccine safety. Concerns about rapid development of vaccines were also observed in previous research on COVID-19 vaccination perceptions  and pandemic influenza A(H1N1) virus vaccination perceptions . Authorities must provide persons with timely and transparent information about development, approval, and safety monitoring of COVID-19 vaccines to fulfil their information needs. If such information is not, or is scarcely, provided by authorities, persons are likely to search for this information elsewhere on the internet, with the considerable risk that this would lead them to vaccine skeptical sources .\n\n【36】We showed that beliefs about descriptive and subjective social norms, specifically with regard to vaccination expectations and behavior of friends and family, were also major determinants of COVID-19 vaccination intentions. The role of social norms in vaccination behavior was also suggested in previous research with regard to COVID-19 vaccinations , influenza vaccinations , and human papillomavirus vaccinations . These findings suggest the potential for interventions focused on endorsing social norms with regard to vaccination (e.g. providing narratives in communication materials for peers who vaccinated). In addition, this finding might indicate that persons are, at least partly, segregated in like-minded groups on the basis of COVID-19 vaccination intentions, which could increase the risk for local outbreaks.\n\n【37】Beliefs about the health risks posed by COVID-19 were not found among the major determinants of vaccination intentions. A similar result was seen in a study on meningococcal vaccination intentions that had a similar study approach . An explanation for this result can lie in the distribution of responses in vaccination intentions. Because most of our respondents intended to vaccinate against COVID-19, the explanatory analysis shows mainly how persons who are not (so) willing to vaccinate differ from those who do want to vaccinate, because that is where the variance in responses can be found. Perceptions of the health risk posed by COVID-19 are likely major reasons for persons to vaccinate but might not be among the most essential reasons for those who do not intend to vaccinate.\n\n【38】The first limitation of our study is that, although the study population is at large fairly comparable with the population in the Netherlands in terms of main demographic characteristics, it is not a perfect representation. Presumably, there is an overlap between those persons who are difficult to reach for vaccination with those persons who are difficult to reach for research purposes. Second, our study was cross-sectional and conducted in a period that had rapid developments in information about COVID-19 vaccinations. For example, just before the start of our data collection, Denmark announced suspending use of vaccine from AstraZeneca  after reports of possible severe adverse events; during the second half of our data collection, the Netherlands also temporarily suspended these vaccinations . Such developments might have affected the outcomes of our study (e.g. through a potential decrease in people’s trust in vaccine safety). Also, subsequent events might lead to slightly different results if the study was repeated. It would be highly valuable to repeat research like ours throughout prolonged crises and in multiple settings to monitor changes and differences. Third, our study focused on beliefs about COVID-19 and COVID-19 vaccinations to provide concrete input for communication. Our study did not address other possible major determinants of vaccination intentions, such as trust in institutions or health literacy. Fourth, we did not include beliefs about conspiracy theories in this study, which in hindsight could have added interesting insights. Such beliefs were not included because these were not pronounced in the literature nor in the qualitative data at the time we developed our survey.\n\n【39】Results of this study provide several essential key points for future research, policy, and communication. First, COVID-19 vaccinations decisions are not made purely by considering the pros and cons for one’s own health. Other (social) benefits of COVID-19 vaccination, related to the relaxation of COVID-19 control measures, are likely to play a major role in vaccination decisions of persons. Providing clear perspectives with regard to these benefits might increase vaccination uptake. At the same time, it is highly essential to address the uncertainties with regard to those social benefits and to prevent future disappointments and decreases in trust and support. Second, social norms regarding peers have been shown to be an essential factor in COVID-19 vaccination intentions, which suggests the potential for norms to induce interventions to increase vaccination uptake. Future research should focus on the characterization and identification of like-minded social networks who are hesitant to vaccinate against COVID-19 to provide well-tailored interventions. Finally, it is highly essential to provide transparent and accessible information about vaccine development and approval process and the probability of potential adverse events caused by vaccination to address concerns about safety of COVID-19 vaccines.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bd012378-8869-4f0d-857a-100a43c02ec5", "title": "Evidence of Recombinant Strains of Porcine Epidemic Diarrhea Virus, United States, 2013", "text": "【0】Evidence of Recombinant Strains of Porcine Epidemic Diarrhea Virus, United States, 2013\nPorcine epidemic diarrhea virus (PEDV) is an alphacoronavirus that causes enteric disease in swine. The disease, PED, is characterized by acute vomiting and watery diarrhea and causes high mortality rates in newborn piglets . PED was first reported in 1971 in the United Kingdom and was soon identified in many European and Asian countries . Variant PEDV strains that were fatal to young pigs, initially isolated during late 2010 in China and Southeast Asia  and in May 2013 in the United States , have posed a serious threat to the pork industry. Incidence of PEDV-associated large–scale outbreaks of diarrheal disease in China was reported at 80%–100% in suckling piglets  and outbreaks in the United States had spread to 25 states by February 2014 , causing numerous deaths in neonatal piglets .\n\n【1】How the virus entered the United States remains unknown. A phylogenetic analysis based on available full-length genomic PEDV sequences indicated that all PEDV strains were classified into 2 distinct genogroups: G1 and G2 . PEDV field strains isolated before 2010 and the derived vaccine strains were in the G1 genogroup, whereas all the new PEDV strains isolated since 2011 in China and the United States (US PEDV) are in G2. The US PEDV sequences were >99% identical to strains found in China in the subgroup G2a, suggesting their origin. In particular, the US PEDV are most closely related to strain AH2012, which was isolated in eastern China and was proposed to have come from multiple recombination events among G2 lineages of PEDV . Divergence of PEDV is driven by genetic recombination, as in other coronaviruses . Details of recombination events in the process are needed to investigate origins. To investigate the evolutionary process by which US PEDV strains hypothetically descended from precursors in China, we conducted a molecular epidemiologic analysis using PEDV-positive samples collected from eastern China since 2012.\n\n【2】### The Study\n\n【3】A total of 169 fecal and intestinal samples were collected from pigs with typical PED symptoms on 26 farms in 4 provinces of eastern China during January 2012–July 2013. The rate of PEDV-positive samples was 56.8% (96/169) as had been determined by using reverse transcription PCR (RT-PCR) specific for the spike (S) gene . From the positive samples, we selected 24 representative samples  to examine. Using RT-PCR, we determined the sequences of the full-length genomic cDNA for the strain CH/ZJCX-1/2012, identified the spike (S) gene for strains CH/ZJQZ-2w/2012 and CH/ZJDX-1/2012, and identified the region encoding structural protein genes by an order of 5′-S-ORF3-E-M-N-3′ (5′-spike protein–open reading frame 3–envelope–membrane–nucleoprotein-3′) for the remaining 21 strains. All primers were designed based on the PEDV MN strain . We purified and cloned PCR products into a vector using TA cloning. We used Vector NTI software  to assemble and analyze the sequences. We performed multiple alignments of S-ORF3-E-M-N, S, ORF3, M, N, and full-length genomes with available sequences from Asia and the United States  and performed phylogenetic analyses using the MEGA5.2 program  with the neighbor-joining method.\n\n【4】Similar to most of the sequences recently documented in PEDV strains in China and the United States, the S genes of the 24 samples have a 4,161-nt sequence that, compared with the prototype CV777 strain, shows 97.9%–100% sequence identities, and contain 2 notable insertions at amino acids (aa) 56–59 (IGEN) and 139 (N) and a deletion of 2 aa (GK) at aa positions 160 and 161 at the N terminus . A phylogenetic analysis comparing S genes showed that, based on the complete genome , all 24 strains were classified into the same group corresponding to G2. However, it is notable that the Chinese sublineage (branch) most closely related to the US PEDV strains did not include the AH2012 strain. Instead, this sublineage contained the strain CH/ZMDZY/11 and 4 other strains determined in this study. Analyses of the phylogenetic trees constructed on the basis of the S-ORF3-E-M-N genes , ORF3 or M  also indicated that the AH2012 strain was not closely related to the US branch, relative to the sublineage represented by strains CH/ZMDZY/11, CH/HuBWHYQ/2012, CH/JXZS-1223L/2012, and CH/JXZS-3L/2012 (designated the ZMDZY sublineage hereafter). The exception is the N gene–based tree, in which the AH2012 was grouped more closely to the sublineage associated with the United States than the strains in the designated ZMDZY sublineage .\n\n【5】The relationship of the AH2012 strain with the 33 PEDV strains identified in China, the United States, South Korea, and Belgium  in nonstructural protein genes was also determined by generation of 3 phylogenetic trees based on ORF1ab, ORF1a, and ORF1b genes, respectively. In accordance with the results from the N gene and the PEDV genotyping based on the full-length genomes , the AH2012 strain in these trees was most closely related to the US strains . Therefore, the strains AH2012 and CH/ZMDZY/11 displayed different phylogenetic relationships in different genome regions. Overall, the AH2012 strain was clustered closely with the US strains in the ORF1ab and the N gene region, whereas the ZMDZY sublineage was clustered closely with the US strains in the S-ORF3-E-M region.\n\n【6】To accurately determine how the US strains are related to strains AH2012 and the ZMDZY sublineage, we performed a recombination analysis using the Recombination Detection Program and available PEDV sequences . We used a multiple-comparison–corrected p-value cutoff of 0.05 in all methods. Recombination events were identified by 6 methods (Recombination Detection Program, GENECONV, BOOTSCAN, MaxChi, CHIMAERA, and SISCAN) when the US PEDV sublineage represented by the MN strain was used as a query. By bootstrap analysis, 3 putatively major recombination breakpoints were detected at nucleotides 6699, 21840, and 26882 , which generated 2 regions: 1 covered the 3′ half of ORF1a, complete ORF1b, and the N terminus of the S (first 1,207 nt); the other spanned partial S, ORF3, E, M, and partial N (first 504 nt) between the strain AH2012 (as the major parent) and the ZMDZY sublineage (as the minor parent). Although the second region (partial S-ORF3-E-M-partial N) of the US strains is associated with the ZMDZY sublineage, the source of genetic material in this region is not known, because none of the PEDV strains in this sublineage had a highly identical sequence to the consensus sequence of the US strains. It is possible that the other recombination breakpoints exist within the S gene, according to the bootstrap supports in this region, which may be determined by future study with available new sequence data. We showed that the emergent US PEDV strains are possibly descendent of 2 major lineages derived from the ZMDZY sublineage and AH2012 through recombination.\n\n【7】### Conclusion\n\n【8】Our study provides further information on the origin of the US PEDV in 2013. We identified 21 S-ORF3-E-M-N genes, 2 S genes, and 1 full-length genomic cDNA of PEDV from PEDV-positive samples collected in eastern China. Comparative genomic, phylogenetic, and recombination analyses using new and known sequence data demonstrated that the AH2012 strain is likely not the direct progenitor of emergent US PEDV strains during 2013. It is possible that replacement of a region within the partial S-ORF3-E-M-partial N region of the AH2012 strain with a corresponding fragment close to the ZMDZY sublineage (including several newly identified strains) resulted in a recombinant strain related to emergence of this virus in swine in the United States. Other unidentified recombination events and accumulation of adapted mutations within the structural protein genes were also likely involved in this process.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fc0971a6-3c86-4590-b91d-6cfbfa505ca8", "title": "Hypoglycemic Toxins and Enteroviruses as Causes of Outbreaks of Acute Encephalitis-Like Syndrome in Children, Bac Giang Province, Northern Vietnam", "text": "【0】Hypoglycemic Toxins and Enteroviruses as Causes of Outbreaks of Acute Encephalitis-Like Syndrome in Children, Bac Giang Province, Northern Vietnam\nAcute encephalitis syndrome is clinically characterized by fever, seizures, and altered mental status. This syndrome is a major public health concern in Asia; annually, >133,000 children are hospitalized with this pathology . Historically, the main etiology of acute encephalitis syndrome in Asia has been Japanese encephalitis, a vectorborne disease caused by a flavivirus (Japanese encephalitis virus) _,_ which causes >25% of cases of this syndrome in Asia. Many other viruses have been shown to cause acute encephalitis syndrome in Asia, such as enteroviruses (e.g. poliovirus, echovirus 9, enterovirus 71), and herpes simplex, measles, varicella zoster, rabies, dengue, Chandipura, and Nipah viruses.\n\n【1】However, for most cases of acute encephalitis syndrome, the specific etiology is unknown . Since introduction of Japanese encephalitis vaccine in the Expanded Program on Immunization in Asia (South Korea, China, Bangladesh, and Nepal) in 1997, a major shift has occurred; cases of Japanese encephalitis–attributable acute encephalitis syndrome have decreased, and cases of acute encephalitis syndrome not attributed to Japanese encephalitis have increased in countries with large vaccination coverage .\n\n【2】Such a shift has been observed in Vietnam, where the prevalence of Japanese encephalitis for hospitalized patients decreased from 50% in 1996 to 10% in 2009 . According to the Ministry of Health, 67% of the 1,800–2,300 cases of acute encephalitis syndrome reported each year occur in northern Vietnam, most often in Bac Giang Province . There has been a clear seasonal pattern of acute encephalitis-like syndrome in this province since 1999, with peaks in summer, particularly in young children . The symptomatology described by parents as rapid development of fever, headache, and nocturnal seizures explains the local name given to the disease (Ac Mong, meaning nightmare). Local populations had previously suggested a link with litchi cultivation because of observed synchronicity between outbreaks of acute encephalitis-like syndrome and litchi harvests.\n\n【3】A previous investigation of outbreaks of acute encephalitis-like syndrome in Bac Giang Province found that litchi cultivation appeared to be associated with acute encephalitis-like syndrome, but the link at the individual level remained unclear . Until 2007, results of all virologic investigations on patient samples remained inconclusive. We therefore performed next-generation sequencing (NGS) on cerebrospinal fluid (CSF) samples obtained since 2008 to identify unknown or unforeseen viruses. In addition, because hypoglycin A (HGA) and methylenecyclopropylglycine (MCPG) are suspected to be probable causes of similar outbreaks of acute encephalitis syndrome during litchi harvests in India and Bangladesh , we also tested serum samples for these toxins. These toxins are present in seeds and aril (flesh) of litchis  and are known to induce hypoglycemia in animal models .\n\n【4】### Materials and Methods\n\n【5】##### Study Area and Outbreak Characteristics\n\n【6】Detailed characteristics for this study have been previously reported . In brief, Bac Giang Province is a rural province that has 1.6 million inhabitants and is located in northern Vietnam. The only hospital is located in Bac Giang City, the capital of the province. Outbreaks of acute encephalitis-like syndrome in Bac Giang Province are unusual and characterized by their specific location, strict seasonality, restricted age group, rapid progression to coma, and a higher case-fatality rate than that for Japanese encephalitis .\n\n【7】##### Study Design and Data Collection\n\n【8】We used surveillance data for case-patients with acute encephalitis-like syndrome admitted to the Bac Giang Provincial Hospital during 2008–2011. The case definition used for this syndrome was fever (temperature \\> 37°5C reported by parents or at hospital admission), altered mental status or seizures, and no bacterial meningitis. We collected data from the Bac Giang Preventive Medicine Centre and the National Institute of Hygiene and Epidemiology (Hanoi, Vietnam). Patients < 15 years of age, those who had onset of acute encephalitis-like syndrome during May 1–August 31, and those who had negative results for Japanese encephalitis virus IgM in CSF or were immunized against Japanese encephalitis virus were included in the study.\n\n【9】##### CSF and Blood Samples\n\n【10】For each seasonal outbreak that occurred during 2008–2011, blood samples were obtained from patients with acute encephalitis syndrome at admission to Bac Giang Provincial Hospital for standard biochemical and hematologic analysis. CSF samples from patients were also collected by physicians. Samples were cryopreserved in liquid nitrogen for transportation and stored at −80°C. Because most patients were young children, only small volumes (20 μL–300 μL) of CSF were available for most samples. Because of age of patients and local cultural practices, no brain biopsies or necropsies were performed for children who died of acute encephalitis-like syndrome.\n\n【11】##### Virologic Analyses\n\n【12】We conducted virus isolation in RD, Vero E6, or C6/36 monolayer cells and PCR for known viruses . In addition, we also performed random NGS for 16 selected CSF samples that matched the case definition and were collected during 2008. We performed specific PCRs for contigs identified by NGS to confirm results, which also enabled comprehensive phylogenetic analysis by using other methods . Primers  were used to test CSF samples obtained from patients who had available clinical data (3 in 2008, 3 in 2009, 14 in 2010, and 21 in 2011).\n\n【13】##### Toxicologic Analysis\n\n【14】We tested 20 blood samples obtained during 2010–2011 (4 in 2010 and 16 in 2011, the only ones available from patients who had clinical data at the time when the hypoglycemic toxins hypothesis was proposed) for HGA and its metabolites and metabolites of MCPG by using a modified analytical method that has been reported . This method is based on ultra–high-performance liquid chromatography/tandem mass spectrometry.\n\n【15】Because these toxins can cause hypoglycemia by blocking the fatty acid β-oxidation pathway, we also measured concentrations of glycine and carnitine conjugates of short-to-medium chain length fatty acids in the same samples by using the same method. In addition, we quantified a spectrum of carnitine esters of 24 saturated and unsaturated fatty acids ranging from short to long chain molecules (C2–C18), including hydroxyl and dicarboxylic acids, by using tandem mass spectrometry without preceding chromatographic separation  .\n\n【16】##### Statistical Analyses\n\n【17】We compared proportions of continuous variables across groups by using the Fisher exact test and distributions of continuous variables across groups by using the Mann-Whitney U test (R version 3.2.3; R Foundation for Statistical Computing, Vienna, Austria). We performed principal component analysis for age, number of days between symptoms and disease onset, glycemia at admission, number of leukocytes in CSF, and serum levels of liver enzymes to identify grouping of characteristics that might help differentiate between infectious and toxic causes of acute encephalitis-like syndrome. We conducted principal component analysis by using Qlucore Omics Explorer software (Qlucore, Lund, Sweden).\n\n【18】##### Ethics\n\n【19】Informed consent was obtained by physicians from parents of hospitalized children before sampling was conducted. The study protocol was reviewed and approved by institutional review boards at the National Institute of Hygiene and Epidemiology and the Institut Pasteur (Paris, France).\n\n【20】### Results\n\n【21】A total of 185 children met the inclusion criteria over the study period . Median age was 5 years (interquartile range 2–8 years), and the sex ratio (male:female) was 1.4:1. The annual number of cases was higher in 2008  and 2011  than in 2009  and 2010  . Because of logistical constraints, CSF and blood samples were available for only 61 of the 185 children, of which 58 also had detailed clinical data. Therefore, these 58 patients represent the study population analyzed , including 10 patients from a previous study .\n\n【22】##### Virologic Analyses\n\n【23】NGS analysis of a pool of 16 CSF samples from the 2008 outbreak provided 116,615 nonhuman contigs from 61,291,294 nonduplicated reads with an average length of 70 nt. Among these contigs, 57 with an average length of 292 nt (range 103 nt–815 nt) matched the human enterovirus B species. Fourteen contigs were assigned to the human echovirus 30 species strain Zhejiang/17/03/CSF (GenInfo Identifier DQ246620) as best hit, with nucleotide identities ranging from 83% to 98%. The second most common reference strain matched (7 contigs assigned, with best-hit ranging from 78% to 88%) was human echovirus 33 strain Toluca-3 (GenInfo Identifier 34485451). Distinct contigs mapped at same genomic locations of these 2 reference genomes and suggested that the pool of samples presumably contained \\> 4 different virus strains. PCRs using primers designed for these contigs confirmed their sequences, identified distinct viruses in the pool, and identified patients from which the sequences had been isolated.\n\n【24】We then conducted individual NGS on 4 selected CSF samples from the pool and acquired virus genome sequences after amplification by using specifically designed PCRs. This sequencing identified 4 distinct enterovirus genomes (120486, 120492, 120488, and 120495); the first 2 genomes were closely related . Prevalence of enterovirus infection was screened by PCR for CSF samples from patients with clinical data available and showed highly variable results (from 13/19 in 2008 to 4/21 in 2011) . Virus isolations were attempted for 10 CSF samples per annual outbreak; all showed negative results.\n\n【25】##### Toxicologic Analysis\n\n【26】Although enteroviruses accounted for most (68%) cases in 2008, these viruses accounted for <20% of cases in 2009–2011. Moreover, identification of multiple and distinct enterovirus strains did not correlate with the model of an epidemic diffusion that would otherwise explain seasonal outbreaks. Therefore, we were interested in exploring alternative explanations, including 2 candidate toxins, MCPG and HGA . Twenty blood samples (4 from 2010 and 16 from 2011), which were obtained near the time of onset of symptoms, were available for this analysis. After serum analysis , we categorized children into 2 groups: 9 had high (>100 nmol/L) serum values of HGA and 11 had serum values of HGA below the lower limit of quantification (10 nmol/L), including 10 below the lower limit of detection (1 nmol/L).\n\n【27】All patients with high levels of HGA had quantifiable concentrations of methylenecyclopropylformyl (MCPF) carnitine, a metabolite of MCPG, and 6 of the 9 patients had detectable MCPF glycine, also derived from MCPG, although below the lower limit of quantification. Methylenecyclopropylacetyl conjugates, derived from HGA, were present in all of these samples, but concentrations did not reach quantifiable levels in all cases. The β-oxidation of fatty acids  was shown to be inhibited in all patients with high serum levels of HGA. As a result, concentrations of glycine and carnitine conjugates of fatty acids of short to long chain length were increased. Increased concentrations were also detected for even and odd chain length acyl compounds and unsaturated compounds in the same samples, which demonstrated complete inhibition of β-oxidation of fatty acids. Of the 9 children who had high levels of HGA/MCPG, 8 had a CSF sample tested: 1 child was positive for enteroviruses, and 7 children were negative. Of the 11 children with low levels of HGA/MCPG, 4 children were positive for enteroviruses, and 7 children were negative.\n\n【28】##### Relationship between Epidemiologic, Clinical, and Biological Findings and Etiologies\n\n【29】Of the 9 children with high levels of HGA/MCPG, 8 were hospitalized in July 2011 and came from the same eastern district (Luc Ngan), a district known to have the highest levels of litchi production in Bac Giang Province (50% of province production) and in which the harvest occurs each year during June–July . Toxin-negative samples and enterovirus-positive samples were predominantly identified in the western part of the province.\n\n【30】On the basis of results of virologic and toxicologic analyses, we compared clinical and biologic characteristics among 4 patient groups: 1) the 19 children who were positive for enteroviruses and who had either low levels of HGA/MCPG (n = 4) or no blood sample tested for toxins (n = 15); 2) the 8 children with high blood levels of HGA/MCPG and who were negative for enteroviruses (n = 7) or not tested for enteroviruses (n = 1); 3) the 7 children who were negative for enteroviruses and toxins; and 4) the 23 children who were negative for enteroviruses and who were not tested for toxins . One child was positive for enteroviruses and toxins and was therefore not included in statistical comparisons. All children included in the study had fever either before admission (reported by parents) or at admission, except for 1 child who had high levels of toxins, but no reported fever, who was included in the study because of severity of the neurologic condition of the child (repeated seizures and coma).\n\n【31】Children with high blood levels of HGA/MCPG had shorter median time between disease onset and admission (0 days vs. 2 days; p = 0.008), more seizures (88% vs. 28%; p = 0.009), more hypoglycemia (glucose level <3 mmol/L) (71% vs. 0%; p = 0.001), lower median numbers of leukocytes in CSF (3 cells/mm 3  vs. 50 cells/mm 3  ; p = 0.001), and higher median serum levels of alanine aminotransferase (48 IU/L vs. 24 IU/L; p = 0.04) and aspartate aminotransferase (68 IU/L vs. 28 IU/L; p = 0.01) than patients infected with enteroviruses. Two (25%) of 8 children who were positive for toxins died, whereas only 1 (5.3%) of the 19 children with enterovirus encephalitis died, but this difference was not significant (p>0.05).\n\n【32】Principal component analysis showed that these children with high levels of HGA/MCPG clustered differently in the projection space  than children with evidence of infection with enteroviruses . Furthermore, children not infected with enteroviruses for whom HGA/MCPG showed negative results or was not tested had profiles more similar to children infected with enteroviruses. In particular, these children had glycemia (glucose level >3 mol/L), and leukocyte counts in CSF were slightly increased .\n\n【33】### Discussion\n\n【34】This study suggests that acute encephalitis-like syndrome previously associated spatially and temporally with litchi harvests  was caused by intoxication, rather than viral encephalitis, as initially suspected. In this context of recurrent acute encephalitis-like syndrome outbreaks since 1999, no consistent viral etiology had been identified by using standard laboratory diagnostic techniques. Because of the high number of viruses known to be associated with encephalitis , we used NGS to analyze samples of patients after the annual outbreak in 2008. This hypothesis-free technique identified human enterovirus B serotypes that were further confirmed by PCR. Other pathogens were not identified.\n\n【35】Despite the link shown in this study between several acute encephalitis-like syndrome cases and enteroviruses, the frequency of enterovirus infection among clinical cases was highly variable from year to year . Therefore, we tested serum samples of a subset of enterovirus-negative and enterovirus-positive cases for HGA and MCPG metabolites. Of 20 children tested, 9 (45%) were positive for HGA and MCPG metabolites and 8 showed inhibition of the β-oxidation catabolic process. Eight of these 9 case-patients came from the same district, which is known for having the highest litchi production in the province. These case-patients were hospitalized in July 2011, at the time of the litchi harvest in the district. These 8 case-patients appeared to have distinct characteristics in comparison with those who had enterovirus acute encephalitis syndrome, including younger age, more rapid progression, higher frequency of seizures, severe hypoglycemia, lack of increased numbers of leukocytes in CSF, and moderate increases in levels of liver enzymes.\n\n【36】The clinical and biochemical presentation of these case-patients clearly matches that of case-patients reported during outbreaks linked to litchi harvests in India and Bangladesh  and of case-patients with Jamaican vomiting sickness . Investigation of an outbreak in Muzaffarpur, India, recently concluded that intoxication with HGA and MCPG was responsible for the outbreak, a finding that is consistent with our results , which also identified HGA/MCPG in young patients with hypoglycemic encephalopathy. Our study also provides a direct comparison of clinical and biologic profiles of acute encephalitis-like syndrome related to enterovirus infection versus intoxication with HGA/MCPG. Thus, we provide useful information that can be used to guide clinical decision making, particularly the need for glycemia testing for management of patients with acute encephalitis-like syndrome.\n\n【37】Our study was limited by comparison of children subjected to different tests at different times (only blood samples obtained during 2010 and 2011 were available for testing of toxins, whereas testing for enterovirus was available throughout the study) and by having used fever as an inclusion criteria. In the study by Shrivastava et al. in India, in which this inclusion criterion was not used, 61% of children with litchi intoxication were afebrile . As a result of using fever as an inclusion criteria, our study might have missed several children with HGA/MCPG intoxication. Apart from 1 patient who had enterovirus infection and high serum concentrations of hypoglycemic toxins, patients with enterovirus infections did not have higher toxin levels than patients without enterovirus infections, suggesting that subtoxic concentrations of HGA/MCPG were not associated with increased risk for enterovirus infection. However, studies with larger numbers of patients are needed to rule out this hypothesis.\n\n【38】We also tried to elucidate the link between these 2 etiologies (enteroviruses and hypoglycemic toxins) and litchi harvesting in northern Vietnam. For enteroviruses, it is likely that temperature and humidity conditions required for enterovirus circulation match those of litchi maturation and harvest. For intoxication with HGA and MCPG, the link is more obvious because toxins have been previously identified in the litchi aril and litchi seeds . In our study, cases of intoxication clustered geographically in areas of large production of litchi. Levels of hypoglycemic amino acids in the litchis are not known. Results from 2 studies suggest that MCPG concentration is highest in the seeds, followed by arils of semiripe litchis and then ripe litchis . Further investigations should compare levels of toxins across cultivars and soil, climate, and harvest conditions, as recommended by Spencer et al. To further investigate a causal link between HGA/MCPG levels and acute encephalitis-like syndrome, healthy children exposed to the same litchi intake would need to be tested. Nevertheless, the evidence of inhibited β-oxidation of fatty acids in all HGA/MCPG–positive patients in this study is a convincing demonstration that intoxication was a key driver of symptoms in these patients.\n\n【39】Intoxication with HGA/MCPG is attributed mainly to a hypoglycemic encephalopathy, secondary to inhibition of β-oxidation and an inability to produce glucose from fatty acids. This metabolic process usually takes hours, which might explain why most children have initial symptoms during the second half of the night. Shrivastava et al. reported that children who had no evening meal were at higher risk for developing hypoglycemic encephalopathy . Young children, and even more so undernourished children, have limited glycogen stores, which increases their vulnerability to the effects of intoxication with HGA/MCPG on metabolism . Concentrations of glycine and carnitine conjugates measured in serum samples might appear rather low. These conjugates would be better measured in urine samples . However, such samples were not available in this study.\n\n【40】In conclusion, this study has shown that within a context of largely viral encephalitis, particularly encephalitis caused by enteroviruses, acute hypoglycemic encephalopathy developed in some children in Vietnam during the litchi harvest, possibly after absorption of a toxin present in the aril of litchi fruits. Local populations should be sensitized to the risks associated with young children eating litchis. Also, for children coming to healthcare facilities because of acute encephalitis-like syndrome during the litchi harvest season, measurement of blood glucose concentrations and immediate infusion with dextrose for those children with hypoglycemia should be critical elements of clinical management. Use of these elements will likely increase patient survival.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "86a2847b-cc0c-41af-a514-39524ff959a3", "title": "White-Nose Syndrome Fungus (Geomyces destructans) in Bats, Europe", "text": "【0】White-Nose Syndrome Fungus (Geomyces destructans) in Bats, Europe\nWhite-nose syndrome (WNS) is a recently emerged wildlife disease in North America, which in 4 years has resulted in unprecedented deaths of hibernating bats in the northeastern United States , and is a widespread epizootic disease among bats. Although we have searched the literature describing observations of hibernating bats, we have been unable to find any similar historical accounts of white fungus growing on live hibernating bats in North America before the recent emergence of WNS.\n\n【1】In North America, WNS is known to affect 6 species of bats that use hibernation as their winter survival strategy: the big brown bat ( _Eptesicus fuscus_ ), the eastern small-footed bat ( _Myotis leibii_ ), the little brown bat ( _M. lucifugus_ ), the northern long-eared bat ( _M. septentrionalis_ ), the tricolored bat ( _Perimyotis subflavus_ ), and the Indiana bat ( _M. sodalis_ ) . Since its detection in February 2006 in a popular tourist cave near Albany, New York, USA, WNS has spread >1,300 km into Connecticut, Massachusetts, New Hampshire, New Jersey, Pennsylvania, Tennessee, Vermont, Virginia, and West Virginia in the United States and the provinces of Ontario and Quebec in Canada  in a pattern suggesting the spread of an infectious agent.\n\n【2】A recently discovered psychrophilic (cold-loving) fungus, _Geomyces destructans_ , has consistently been isolated from bats that meet the pathologic criteria for WNS, including colonization of skin by fungal hyphae causing characteristic epidermal erosions and ulcers that can progress to invasion of underlying connective tissue . _G_ . _destructans_ is identified by its distinctive asymmetrically curved conidia and has a unique taxonomic position among other _Geomyces_ spp. described to date . Its closest genetic relative is _G. pannorum_ , a ubiquitous psychrophilic, keratinolytic fungus that has been isolated from a variety of sources and geographic regions, including soil and the fur of wild mammals in France , floors of trains and ferryboats in Italy , boreal forests in Canada , and environmental samples from Arctic regions . _G. pannorum_ var. _pannorum_ has been reported as an unusual dermatophyte infecting fingernails and superficial skin of humans who have a history of close contact with soil and dust . However, _G. destructans_ differs from other common soil fungi of North America in its ability to invade the living tissues of hibernating bats.\n\n【3】After WNS was described in North America , reports dating back to the early 1980s  described repeated observations of white fungal growth on muzzles of hibernating bats in Germany. However, these bats lacked the characteristics of WNS such as associated deaths. Moreover, fungus was not identified. In response to WNS in North America, researchers in Europe initiated a surveillance effort during the winter of 2008–09 for WNS-like fungal infections among hibernating populations of bats in Europe. _G. destructans_ in Europe was previously reported in 1 hibernating bat that was sampled in France during March 2009 .\n\n【4】In this report, we describe results of a more extensive effort by scientists from 4 countries in Europe (Germany, United Kingdom, Hungary, and Switzerland) to obtain and analyze samples from hibernating bats with white patches on their faces or wing membranes. Our objectives were to identify the fungus colonizing such affected hibernating bats in Europe and to clarify its geographic distribution over a broad area of Europe.\n\n【5】### Materials and Methods\n\n【6】During ongoing annual population surveys of caves and mines conducted by national nongovernmental organizations, hibernating bats with obvious fungal growth on their bodies  were opportunistically sampled in Germany, Switzerland, and Hungary; samples were also obtained from 2 dead bats from the same hibernaculum in the United Kingdom. Approximately 366 hibernacula were visited during mid-February–mid-April 2009: 336 in Germany, 20 in Hungary, and 10 in Switzerland. Two to 214 hibernating animals were observed at each site, with the exception of 2 sites in Germany, which harbored 2,000–7,000 animals at each site.\n\n【7】Samples were collected from live bats by using 2 methods. Touch imprints were obtained by holding adhesive tape against affected areas of skin or fur, or fur clippings were obtained from affected areas of bat muzzles. All species of bats in Europe are strictly protected under the Flora, Fauna, Habitat Guidelines of the European Union (92/43/EEC)  and The Agreement on the Conservation of Populations of European Bats . We did not have permission to invasively sample or kill individual animals for histologic analysis to confirm skin infection by _G. destructans_ . Samples were shipped to the Leibniz Institute of Zoo and Wildlife Research (IZW), Berlin, Germany, for further investigations.\n\n【8】Twenty adhesive tape samples were first screened by using light microscopy, and 21 hair samples were examined by using scanning electron microscopy for conidia characteristic of _G. destructans_ . Two of the submitted samples (2 greater horseshoe bats from the United Kingdom) consisted of entire bat carcasses. Although the carcasses were examined externally for fungal growth on muzzle skin and hair, specimens were too decomposed to conduct internal pathologic examinations. Tape or hair samples from all bats were further investigated by using direct PCR amplification of fungal rRNA gene internal transcribed spacer (ITS) region DNA (ITS1, 5.8S, and ITS2). Total nucleic acids were extracted from culture, tape, or hair samples by using PrepMan Ultra Reagent (Applied Biosystems, Darmstadt, Germany) following the manufacturer’s instructions.\n\n【9】The rRNA gene ITS region DNA was amplified by using PCR with primers ITS4 and ITS5  and GoTaq DNA polymerase (Promega, Madison, WI, USA). Cycling parameters were an initial 2-min denaturation at 98°C; followed by 30 cycles of denaturation at 98°C for 10 s, annealing at 50°C for 30 s, and extension at 72°C for 1 min; and a final extension at 72°C for 7 min. For fungal isolates, rRNA gene small subunit (SSU) DNA was amplified by using PCR with primers nu-SSU-0021–5′  and nu-SSU-1750–3′  as above, except the extension time was increased to 2 min. Sequencing primers were PCR primers; nu-SSU-0402–5′ , nu-SSU-1150–5′ , nu-SSU-0497–3′ , and nu-SSU-1184–3′  were added for SSU. PCR products were sent to the Robert Koch Institute, Berlin, Germany, for direct sequencing.\n\n【10】Culture analyses of samples were performed at Munich University Hospital and IZW. After examining tape impressions by using light microscopy, we identified small areas with fungal conidia characteristic of _G_ . _destructans_ and excised them with a sterile scalpel blade. Half of the excised material was used for PCR; the remaining sample and samples of individual hairs with microscopic indication of _G. destructans_ were immediately placed onto Sabouraud dextrose agar plates containing gentamicin and chloramphenicol and incubated at 4°C and 8°C. _G_ . _destructans_ isolates obtained during this study are maintained at IZW.\n\n【11】### Results\n\n【12】We obtained and analyzed samples from live bats with obvious fungal growth on their bodies found between mid-February and the end of March 2009 at 11 sites (8 in Germany, 1 in Hungary, and 2 in Switzerland). Samples were also obtained from an additional bat in Germany in February 2008 and from 2 dead bats from a site in the United Kingdom in March 2009  All 12 hibernacula sampled contained 1–5 animals that exhibited obvious fungal growth. Forty-three samples were obtained from these 12 hibernacula and represented 23 adult bats of 6 species: 1 Brandt bat ( _M. brandtii_ ), 3 pond bats ( _M. dasycneme_ ), 1 Daubenton bat ( _M. daubentonii_ ), 1 lesser mouse-eared bat ( _M. oxygnathus_ ), 15 greater mouse-eared bats ( _M. myotis_ ), and 2 greater horseshoe bats ( _Rhinolophus ferrumequinum_ ).\n\n【13】After direct PCR amplification and DNA sequence analysis of fungal rRNA gene ITS regions, genetic signatures 100% identical with those from _G_ . _destructans_ type isolate NWHC 20631–21  were identified from 21 of 23 bats examined: 15/15 from Germany, 2/2 from Hungary, and 4/4 from Switzerland. Both bats from the United Kingdom were colonized by _Penicillium_ sp. Fungi with conidia morphologically identical to those of _G_ . _destructans_  as described by Gargas et al. were isolated in axenic cultures from 8 of 23 bats examined: 3/15 from Germany, 1/2 from Hungary, and 4/4 from Switzerland) .\n\n【14】Consistent with published descriptions for _G_ . _destructans_ , fungal colonies grew slowly and within 14 days attained diameters of 1.0 mm at 4°C and 4.0–5.0 mm at 8°C; no growth occurred at 25°C. The sensitivity of our method for isolating _G_ . _destructans_ from bat hair was comparable to published diagnostic sensitivity for culturing _G_ . _destructans_ from bat skin . Subsequent PCR/DNA sequencing analyses of the 8 isolates indicated that they all had rRNA gene ITS and SSU region DNA sequences identical to those of _G_ . _destructans_ type isolate NWHC 20631–21 (GenBank accession nos. EU884921 for ITS and FJ231098 for SSU).\n\n【15】Unlike other bats sampled in this study, the 2 greater horseshoe bats from the United Kingdom were found dead, and their nostrils were colonized by _Penicillium_ sp. These bats did not fulfill the pathologic criteria for WNS  because fungal hyphae did not invade the epidermis but remained within the superficial layer of the epidermal stratum corneum. A more complete description of the postmortem analysis of the greater horseshoe bats has been reported . _G_ . _destructans_ was not isolated in culture, and its genetic signature was not identified by PCR and DNA sequencing of samples collected from greater horseshoe bats.\n\n【16】### Discussion\n\n【17】Laboratory analyses demonstrated that 5 species of the genus _Myotis_ in Europe harbored _G_ . _destructans_ ; male and female bats were equally affected. Despite laboratory confirmation that bats obtained in Germany, Switzerland, and Hungary were colonized by _G_ . _destructans_ , deaths were not observed at collection sites. Puechmaille et al. reported a similar observation with a greater mouse-eared bat in France. Additionally, a lesser mouse-eared bat from Hungary with visible fungal infection during hibernation, from which _G_ . _destructans_ was isolated, was recaptured 5 months later (August 2009) and showed no external signs of fungal infection. On February 19, 2010, the same bat was again observed in the same hibernaculum without any visible sign of fungal growth. However, 7 other bats within that group of 55 animals displayed obvious fungal growth but were not sampled for this study.\n\n【18】In contrast, decreases in bat hibernating colonies infected by _G_ . _destructans_ in North America are often >90% , and mortality rates similar in magnitude would be difficult to miss among closely monitored winter populations of bats in Europe. Biologists in Germany and Switzerland have conducted annual censuses of bat hibernacula since the 1930s and 1950s, respectively. In Hungary, the largest hibernacula have been annually monitored since 1990. Similar death rates to those caused by WNS in hibernating bats in North America have never been documented in countries in Europe in which _G_ . _destructans_ has now been identified.\n\n【19】Although distribution of _G_ . _destructans_ in bats across Europe has not been exhaustively characterized, opportunistic sampling conducted as part of this study during the winter of 2008–09 demonstrated that the fungus was present on bats in 3 countries . The 2 most distant points from which bats colonized with _G_ . _destructans_ have been identified were separated by >1,300 km. Despite the observed distribution of _G_ . _destructans_ in Europe , the 5 bat species from which _G. destructans_ was detected migrate average distances <100 km between their summer and winter roosting sites , indicating that the fungus is most likely spread as local bat populations emerge from hibernacula, disperse, and interact with populations within their dispersal range. Identification of bats colonized by _G_ . _destructans_ from such distant sites, in addition to the relatively homogenous distribution of the fungus among sites in Germany, suggests that _G_ . _destructans_ may be widespread in Europe.\n\n【20】Regardless of widespread occurrence of _G_ . _destructans_ among bat species in Europe , deaths of bats in Europe caused by WNS, similar to those caused by WNS in North America, have not been observed. Although no bat species migrates between Europe and North America or is present on both continents , many species of the genus _Myotis_ are infected by _G_ . _destructans_ on each continent. Although the mechanism(s) by which hibernating bats died because of infection with _G. destructans_ in North America is not yet understood, bat species in Europe may exhibit greater resistance or respond differently to infection by this fungus than their counterparts in North America.\n\n【21】Before the emergence of WNS in North America, large aggregations of hibernating bats ranging from 1,000 to 50,000 animals were common in caves and mines of affected regions, and many hibernation sites in regions of North America still unaffected by WNS contained tens of thousands of bats during winter (some contain hundreds of thousands) . In contrast, aggregations of bats hibernating in caves and mines in Europe rarely exceed 1,000 animals. However, larger hibernating groups have been observed at a few natural sites, such as a cave in northern Germany with 13,000–18,000 bats  and human-made structures (e.g. Daubenton bats in bunkers and catacombs) . If host density plays a role in _G_ . _destructans_ transmission or deaths of bats, such as through increased disturbance of clustered bats, the bats in Europe may experience lower mortality rates because they form smaller hibernation groups composed of small clusters or individual bats. Apparent continental differences in susceptibility of hibernating bats to deaths associated with skin infection by _G_ . _destructans_ may indicate either circumstantial or evolved resistance in bats in Europe.\n\n【22】_G_ . _destructans_ has been detected in North America only in states and provinces where WNS has also been observed and in contiguous states. Recent emergence and spread of _G_ . _destructans_ with associated deaths of bats throughout hibernacula in the northeastern United States  may suggest ecologic release of an exotic pathogen into an uninfected ecosystem. Although this suggestion remains a hypothesis and how _G_ . _destructans_ may have been introduced to the United States is not known, initial documentation of WNS in a popular tourist cave near Albany, New York , suggests that a human vector could have been involved.\n\n【23】There are many examples of unintended introductions of fungal pathogens, particularly of those affecting plants and ectothermic animals with tissue temperatures permissive to fungal infection . One case with striking similarities is the panzootic chytrid fungus ( _Batrachochytrium dendrobatidis_ ), which has caused global decreases among amphibian species . As with skin infection by _B_ . _dendrobatidis_ in amphibians, which can alter body electrolyte levels and lead to cardiac arrest , skin infection by _G_ . _destructans_ in hibernating bats may also kill by causing irreversible homeostatic imbalance because wing membranes play major roles in water balance, circulation, and thermoregulation of hibernating bats during winter .\n\n【24】Bat species in Europe may be immunologically or behaviorally resistant to _G_ . _destructans_ because of having coevolved with the fungus. Additionally, microbial flora of bat skin or other abiotic surfaces in bat hibernacula in Europe may have also coevolved to incorporate _G_ . _destructans_ as a nonpathogenic component of the microbial community. Conversely, possible recent introduction of _G_ . _destructans_ into the United States, with subsequent infection of bat species in North America and ecosystems not infected with the fungus, provides a potential explanation for the devastating effects of WNS in North America. Although bats are reservoirs of various pathogens , research into the immune function of bats, particularly during hibernation, is just beginning.\n\n【25】In conclusion, nondetrimental colonization of bat species in Europe by _G_ . _destructans_ may be relatively common , and historical reports  suggest that such colonization of hibernating bats in Europe has occurred for several decades. In contrast to recent mass deaths associated with _G_ . _destructans_ skin infection, which is the hallmark of WNS in North America, bats in Europe appear to coexist with _G_ . _destructans_ . Studies to investigate mechanisms of pathogenesis, microbial ecology, and phylogeography of _G_ . _destructans_ will be essential for developing a comprehensive understanding of WNS. In particular, testing the hypotheses that bats in Europe are more resistant to fungal skin infection by _G_ . _destructans_ , that _G_ . _destructans_ was introduced from Europe to North America, and that environmental circumstances limit the pathogenicity of _G_ . _destructans_ in Europe seem warranted. Divergent manifestations of infection by _G_ . _destructans_ in bats in Europe and North America provide a unique opportunity to address these research objectives with the ultimate goals of better understanding WNS and developing sound strategies to manage the devastating effects of this emerging wildlife disease in North America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6986287d-a856-401c-b692-f2f5351d18e2", "title": "Modeling Immune Evasion and Vaccine Limitations by Targeted Nasopharyngeal Bordetella pertussis Inoculation in Mice", "text": "【0】Modeling Immune Evasion and Vaccine Limitations by Targeted Nasopharyngeal Bordetella pertussis Inoculation in Mice\nLess than a century ago, _Bordetella pertussis_ was rampant worldwide, causing pertussis (whooping cough) that killed millions of persons every year, mostly infants and children . Whole-cell pertussis vaccines (wP), introduced in the mid-1950s, successfully controlled the disease, but concerns over side effects led many countries to replace wP vaccines with acellular pertussis (aP) vaccines in the mid-1990s . aP vaccines reduced side effects, but outbreaks of pertussis were still noted among highly aP-vaccinated populations , and the incidence of disease has been increasing among adults vaccinated with aP vaccines as children . In addition, experiments conducted with primates and rodents show that aP vaccines prevent the symptoms of disease but do not prevent the spread of the bacterium . There is now consensus among researchers that aP vaccines confer good but short-lived protective immunity against disease but much less protection against colonization, shedding, and transmission .\n\n【1】Most of our knowledge of _B. pertussis_ has been learned from animal models of pneumonic infection that were developed during an era guided by Koch’s postulates . These animal experimental systems were designed to cause severe pathology and near-lethal virulence to simulate the most severe human disease. In pertussis models that emerged from this approach, large numbers of pathogen are introduced deep in the respiratory tract of animals, resembling extreme human infections in their severity and virulence but with more lung involvement than is generally clinically observed. In these models, high doses of _B. pertussis,_ often 10 5  –10 6  CFU, are delivered to the lungs of rodents . Larger primates, such as baboons, are inoculated by endotracheal intubation with even larger numbers, 10 8  –10 10  CFU .\n\n【2】High-dose pneumonic inoculations have provided several experimental benefits, including consistent colonization and growth of bacteria in the lungs, which induces severe pathology. Such inoculations served as assays to measure the contributions of individual virulence factors to severe disease and to develop effective vaccines. Delivery of large numbers of bacteria deep in the lungs predictably induces a vigorous and quantifiable immune response that begins to control infection within 2–3 weeks, reducing bacteria numbers below detectable levels within about 1 month  and providing an experimental system in which to develop and test vaccines to protect against such severe disease.\n\n【3】As valuable as conventional high-dose models have been, the bolus introduction of many bacteria deep into the lungs bypasses many key steps in the highly infectious catarrhal stage of pertussis, the prolonged period of early infection involving milder nonspecific upper respiratory tract symptoms. Of note, these aspects of early infection are most relevant to the current challenge of the ongoing circulation of _B. pertussis_ . Indeed, recent work has revealed that a large proportion of human infections are asymptomatic and undiagnosed  _._ Assays that specifically measure how colonization, early growth, and immunomodulation contribute to shedding and transmission during the catarrhal stage of infection, before and perhaps independent of lower respiratory tract infection, are critical for development of vaccines that can prevent transmission.\n\n【4】We describe a novel nasopharyngeal infection model in mice that efficiently establishes _B. pertussis_ infections that mimic human infections, beginning with low numbers of pathogens colonizing the upper respiratory tract. Nasopharyngeal infections in our model revealed crucial aspects of _B. pertussis_ –host interactions not observed in conventional pneumonic infection models and successfully demonstrating the failure of aP vaccines to prevent nasopharyngeal colonization. This nasopharyngeal infection system allows mechanistic study of several aspects of the early infectious process that usually are obscured by conventional pneumonic challenge. In addition, the model provided assays that are likely to be useful for development of new and improved vaccines to prevent _B. pertussis_ colonization and transmission.\n\n【5】### Materials and Methods\n\n【6】##### Bacterial Cultures and Inocula Preparation\n\n【7】We grew _B. pertussis_ strain 536, a derivative of strain Tohama I, as previously described . We then pelleted bacteria by centrifugation and resuspended it in phosphate-buffered saline (PBS) to an optical density of 600 nm (OD 600  ) of 0.1 (≈10 8  CFU/mL). We serially diluted bacteria in PBS to obtain 500 CFU in 5 μL PBS for low-dose–low-volume (LDLV) nasopharyngeal inoculation or 5 × 10 5  CFU in 50 μL PBS for high-dose–high-volume (HDHV) pneumonic inoculation.\n\n【8】##### Mouse Experiments\n\n【9】We housed C57BL/6 female mice from Jackson Laboratories  in the specific pathogen-free facility at the University of Georgia (Athens, Georgia, USA). We diluted veterinary grade antimicrobial drugs, including enrofloxacin  and gentamicin , for intranasal delivery in 10 µL PBS to mice anesthetized by using 10% isoflurane. We optimized the amount of antimicrobial drug delivered to a single dose of 45 μg enrofloxacin per mouse. Twelve hours after antimicrobial drug treatment, we delivered 500 CFU _B. pertussis_ in 5 µL PBS for LDLV nasopharyngeal infections or 5 × 10 5  CFU in 50 µL for HDHV pneumonic infections. Delivery of incula for both groups was by nasal inhalation under mild anesthesia. For vaccination experiments, we used intraperitoneal delivery, which is convenient and known to confer robust protection. In brief, we vaccinated 5-week-old mice on day 0 and gave a booster vaccine on day 28 by intraperitoneal injection of 200 µL PBS containing either wP vaccine (2 × 10 9  CFU of _B. pertussis_ Tohama I heat-killed at 65°C for 30 min) , or one tenth of a human dose of commercial aP . We inoculated mice 2 weeks after the booster vaccination (day 42 post vaccination). At indicated time points, we euthanized mice by CO 2  inhalation and excised nasal cavities, trachea, and lungs, which we homogenized in 1 mL sterile PBS by using Bead Mill 24 . We plated serial dilutions on Bordet-Gengou agar for bacterial enumeration.\n\n【10】##### Flow Cytometry\n\n【11】We prepared single-cell suspensions from collagenase-treated lungs, which we strained through 70 mm mesh and centrifuged through 44% Percoll  in Gibco RPMI 1640 medium , and then layered onto 67% Percoll in 1× PBS. We used TruStain FcX  anti-mouse CD16/32 antibody to block Fc receptor cells and performed flow cytometry by using the LSR II system . We then stained surface markers with the antibodies used to sort neutrophils, T cells, B cells, and natural killer (NK) cells. We used the following Biolegend products from Thermo Fischer Scientific: for neutrophils, CD11b (CD11b Antibody, PE-Cyanine 7), CD115 (CD115 Antibody, APC), lymphocyte antigen complex 6 locus G (Lys6G Antibody, AF488); for T cells, CD45 (CD45 Antibody, Alexa Fluor-700), CD3 (CD3 Antibody-APC); for B cells, B220 (B220 Antibody-PE-Cy7), and NK cells, NK1.1 (NK1.1 Antibody-PE) . We analyzed data by using FACS Diva version 8.0.1 (BD Biosciences) and determined percentage viability by using Zombie Aqua (Biolegend) live-dead dye.\n\n【12】##### Evaluation of Splenic Lymphocytes Responses\n\n【13】To analyze CD4 T cells and cytokines, including interleukin (IL) 17, IL-10, and IL-4, we collected spleens in ice chilled PBS (≈1°C–2°C) and then passed the mixture through a 40-µm cell strainer. We seeded 2 × 10 7  cells in a 96-well plate and stained cells according to standard protocols . We acquired data in the LSR II (BD Bioscience) and analyzed data with FlowJo 10.0 by using a standard gating strategy . In brief, we used Ghost Dye Red 710  for determining live cells, then gated CD45+ for total leukocytes and Thy1.2+ for T cells. We used CD4+ cells to evaluate levels of intracellular IL-17, IL-4, and IL-10.\n\n【14】##### _B. pertussis_ –Specific Antibodies\n\n【15】We quantified serum antibodies by ELISA using Corning Costar 96-well EIA microtiter plates (Thermo Fischer Scientific) coated with heat-killed _B. pertussis_ grown to an OD 600  of 0.600 in Stanier Scholte medium. We coated plates by using sodium-carbonate buffer (0.1 mmol/L at pH 9.5) overnight at 4°C . We considered the IgG titer to be the reciprocal of the lowest dilution in which we obtained an OD \\> 0.1.\n\n【16】We used 2-way analysis of variance and a paired 2-tailed Student _t_ \\-test in Prism version 8.0.2  for statistical analyses between the pneumonic and nasopharyngeal groups. We performed animal experiments in accordance with recommendations in the Guide for the Care and Use of Laboratory Animals, National Research Council . The study protocols were approved by the Institutional Animal Care and Use Committee at the University of Georgia (approval nos. A2016 02-010-Y3-A9 and A2016 04-019-Y3-A10).\n\n【17】### Results\n\n【18】##### Nasopharyngeal Colonization\n\n【19】_B. pertussis_ generally is considered to be specialized to its human host and to have lost the ability to efficiently colonize other animals . However, in a previous investigation, we noted that resident nasal microbiota in mice create a barrier to colonization and that perturbing the microbiota with antimicrobial drugs permitted low numbers of _B. pertussis_ to efficiently colonize the nasal cavities . To repeat this experiment and demonstrate improved ability to colonize mice, we intranasally treated groups of C57BL/6 mice (n = 4) 3 times, at 8-hour intervals, with either 45 μg enrofloxacin in 10 μL PBS or PBS only for the control group. Twelve hours after the last treatment, we intranasally delivered 500 CFU of _B. pertussis_ in 5 µL of PBS to localize the inoculum within the nasal cavity. After 3 day, no _B. pertussis_ were recovered from the nasal cavities of PBS-treated control mice, but we found all mice treated with antimicrobial drugs were colonized with thousands of CFUs of _B. pertussis_ , indicating that enrofloxacin treatment facilitated _B. pertussis_ colonization . We performed a similar experiment using gentamicin, which showed a similar increase in _B. pertussis_ colonization, indicating that the effect is not limited to enrofloxacin . We also found no notable difference in respiratory tract colonization at days 3 and 7 between C57Bl6/J and BALBC/J mice that were treated with antimicrobial drugs and inoculated , indicating that nasopharyngeal colonization largely was independent of the genetic background between the 2 mouse strains.\n\n【20】Further optimization experiments  showed that pretreatment with antimicrobial drugs reduced the infective dose from 10,000 CFU in untreated mice to <100 CFU in treated mice . The threshold for successful nasal colonization was 4.5–45 µg of enrofloxacin. Even a single enrofloxacin pretreatment allowed _B. pertussis_ to efficiently colonize mice . We settled on this relatively simple single enrofloxacin pretreatment and LDLV inoculation regimen as the experimental nasopharyngeal inoculation model.\n\n【21】##### LDLV Nasopharyngeal Inoculation\n\n【22】We first assessed the course of infection in our model by comparing it with the conventional HDHV pneumonic model of _B. pertussis_ infection. Groups of mice (4 per group) were either treated with enrofloxacin then nasopharyngeally inoculated with 500 CFU of _B. pertussis_ in 5 µL PBS; or given the conventional HDHV pneumonic inoculation of 500,000 CFU of _B. pertussis_ in 50 µL PBS. Both groups were sampled for >28 days . As usually observed in the HDHV pneumonic model, at day 3, _B. pertussis_ had grown to large numbers in the lower respiratory tract of mice, but numbers were <10,000 CFU in the nasal cavities, and were undetectable in most HDHV mice by day 21, demonstrating more rapid clearance than is observed in human infections. In contrast, _B. pertussis_ rarely reached the lungs of mice in the LDLV group , but _B. pertussis_ numbers in the nasal cavity increased nearly 100-fold to ≈10,000 CFU and persisted at this level throughout the 28-day experiment . These data indicate that in the absence of lung infection, _B. pertussis_ can efficiently colonize, grow, and persist in the nasopharynx.\n\n【23】##### Host Immune Response\n\n【24】The colonization profile of the nasopharyngeal (LDLV) model revealed profound differences in the dynamics of the infection compared with the pneumonic (HDHV) model, suggesting very different interactions with host immunity. We and others previously have shown that the large bolus of _B. pertussis_ delivered into the lungs in the pneumonic infection model rapidly activates both innate and adaptive immune components to generate a robust immune response that clears _B. pertussis_ infection in ≈4 weeks . However, this infection model is unlike natural human infection because of the extraordinarily severe pneumonic disease, the robustness of the immune response, and the speed of bacterial clearance. In contrast, delivery of low doses of _B. pertussis_ limited to the nasopharynx, more like natural exposure, resulted in localized growth in the upper respiratory tract, where the pathogen persisted at higher numbers for much longer. This finding led us to hypothesize that this more natural mode of infection might enable _B. pertussis_ to grow more gradually, the way it would naturally, and thereby provide a model system to study how it might avoid unnecessary stimulation of host immunity to persist. To examine this hypothesis, we compared the relative proportions of major groups of immune cells in the lungs and nasopharyngeal washes on day 14 after HDHV pneumonic and LDLV nasopharyngeal inoculations.\n\n【25】Consistent with prior studies, the pneumonic infection model resulted in 5-fold to 50-fold increases in numbers of neutrophils (CD11b+/CD115–/Ly6G high  ), T cells (CD45+/CD3+), B cells (CD45+/B220+), and natural killer cells (CD45+/CD3–/NK1.1+) in the lungs  and in the nasal cavities  relative to control mice. By comparison, we detected only modest increases (<2-fold) among some immune cell populations in LDLV-inoculated mice, despite having even higher numbers of _B. pertussis_ in the nose at the time. These observations indicate that _B. pertussis_ can grow from small inocula to large numbers in the nasopharynx with minimal immune response. HDHV pneumonic inoculations also resulted in a robust systemic immune response indicated by the numbers of splenocytes with significant induction of IL-17, IL-4, and IL-10 compared with uninfected naive mice . But low-dose nasopharyngeal inoculation did not result in measurable increases in cytokines. Together these data reveal substantial differences in the immune response to pneumonic versus nasopharyngeal infection models.\n\n【26】##### Persistent Nasopharyngeal Infection\n\n【27】A characteristic of pertussis in humans is the persistence of infection and disease lasting for many weeks or months; pertussis is also known as the 100-day cough. To compare persistence in the 2 contrasting infection models, we inoculated groups of C57Bl/6J mice to establish either nasopharyngeal (LDLV) or pneumonic (HDHV) infections. We then noted the presence or absence of _B. pertussis_ in the nasal cavities (detection limit 10 CFU) on days 3, 7, 14, 28, 60, 90, and 120 postinoculation. For pneumonic infection models, the percentage of mice with bacteria recovered from the nasal cavities dropped from 100% on day 7 to 25% on day 28, after which bacteria were no longer detected . In contrast, LDLV nasopharyngeal inoculation resulted in more persistent infections; 100% of mice were still colonized at day 28 and 50% at day 60. Bacteria were still detected in 1/4 (25%) mice at day 90 and were only cleared from all mice 120 days after inoculation, highlighting the extraordinary persistence of this organism when delivered in more natural low dose and volume, and providing an experimental system in which to study its persistence.\n\n【28】As previously described for the HDHV pneumonic infection model, _B. pertussis_ delivered to the lungs in large numbers induced a rapid increase in _B. pertussis_ serum IgG titers to ≈10,000 by day 28 and to ≈20,000 by day 60 . As antibody titers rose, colonization levels dropped throughout the respiratory tract , consistent with the known roles of antibodies in clearing infection . Antibody titers continued to increase after the pathogen was cleared, contributing to the strong convalescent immunity associated with the conventional pneumonic model. In contrast, after LDLV nasopharyngeal inoculation, serum _B. pertussis_ IgG levels were barely detectable even after months of persistent infection, reflecting the minimal induction, suppression, or both of host adaptive immunity by the pathogen. These lower antibody titers correlate with much slower control and clearance of infection in the nasopharyngeal infection model and in natural infections.\n\n【29】##### Convalescent Immunity\n\n【30】Conventional HDHV pneumonic infections have been shown to induce robust protective immunity. However, LDLV nasopharyngeal inoculation resulted in more persistent infection and induced lower antibody titers, either because lower numbers of _B. pertussis_ in the lungs are less immune stimulatory or because _B. pertussis_ more effectively modulates the immune response when it follows this more natural course of infection. However, in both cases, infection eventually is cleared, indicating that adaptive immunity is generated and effective. To compare the relative efficacy of convalescent immunity induced by the 2 infection models, we examined the protection each conferred against subsequent challenge.\n\n【31】Mice convalescing from prior pneumonic infection rapidly cleared a high-dose pneumonic challenge from the lungs and reduced numbers in the nasal cavity by >90% within 7 days , as previously documented . These mice showed no signs of disease, and bacterial numbers were far lower than those for unvaccinated mice challenged with the same dose , demonstrating that prior pneumonic infection confers protection against disease. In striking contrast, mice convalescing from prior low-dose nasopharyngeal inoculation had much higher numbers of _B. pertussis_ in all respiratory organs. This finding shows that mice convalescing from nasopharyngeal infection fail to prevent subsequent colonization and bacterial growth when challenged with artificially large and deep lung pneumonic inoculation. These results agree with the corresponding serum antibody titers measured  and reveal profoundly different protective immunity induced by nasopharyngeal infection than described in previous studies that used the conventional HDHV pneumonic infection model .\n\n【32】##### Vaccination Effects on Colonization\n\n【33】Although pneumonic models were central in developing aP vaccines that prevent severe disease, these assays of extreme pneumonic virulence failed to reveal the limited protection that aP vaccines provide against less severe upper respiratory tract colonization . Thus, these models did not predict the current problem of _B. pertussis_ reemergence. Therefore, we set out to test whether the LDLV nasopharyngeal model might enable us to measure the failure of the aP vaccines and provide an assay system in which improved vaccines could be developed. For our vaccination experiments, we used the intraperitoneal delivery route, which is convenient and known to confer robust protection. Groups of mice that were vaccinated with either wP or aP vaccine, and unvaccinated control mice, were challenged via LDLV nasopharyngeal inoculation. wP-vaccinated mice were substantially protected against nasal colonization and had few or no bacteria remaining by day 7 after challenge . In contrast, _B. pertussis_ colonized and grew in the nasal cavities of aP-vaccinated animals nearly as efficiently as in naive animals. These results demonstrate that aP vaccination fails to prevent nasopharyngeal colonization in this experimental system. This approach can measure the differences between wP and aP vaccines in this regard, providing an assay in which to evaluate various proposed new vaccines that might prevent colonization better than current aP vaccines .\n\n【34】### Discussion\n\n【35】Inoculating animals with high doses of _B. pertussis_ delivered deep into the lungs (HDHV) induces severe pathology in the lower respiratory tract of rodents and baboons . Postmortem descriptions of lung pathology in 8 human infants who died from infantile pertussis revealed marked leukocytosis and pulmonary hypertension , features replicated in mouse and baboon pneumonic models , suggesting that these conventional pneumonic infection models reasonably replicate the most extreme form of human disease. However, these cases are extreme; pertussis generally is described as a disease of the upper respiratory tract that induces relatively little inflammation and histopathology  and often could occur with minimal symptoms and go undiagnosed . _B. pertussis_ is highly infectious to humans, indicating that small numbers of bacteria landing in the upper respiratory tract can efficiently colonize, grow, and spread. However, conventional pneumonic infection models bypass the need to efficiently attach and establish the first microcolony, then grow and spread from there to other sites, potentially suppressing both the initial inflammatory response and the subsequent adaptive immune response. These aspects of the infectious process have not been well simulated in the HDHV pneumonic model, making it difficult to study and understand them.\n\n【36】We observed that localized application of antimicrobial drugs consistently enabled small numbers of _B. pertussis_ to efficiently colonize, grow, and establish persistent infections in the nasopharynx of mice, mimicking the early stages of natural infection. Despite the efficient colonization and growth to higher and more sustained numbers in the nasal cavity, we detected only a modest (<2-fold) responses among immune cell populations. Furthermore, infections remained localized to the upper respiratory tract and rarely progressed to the lungs, agreeing with the notion that pertussis is primarily an upper respiratory tract infection. Of note, multiple contact tracing studies identify asymptomatic carriage as the likely source of human infections . In addition, the strong inflammatory responses and high antibody titers observed in pneumonic infection models are not routinely observed in most surveys of human infections .\n\n【37】Both wP and aP vaccines prevent severe pneumonic disease in HDHV pneumonic infection experimental models in rodents and primates, and both prevent severe disease in humans. However, consensus is growing that aP vaccines fail to prevent colonization and transmission, aspects of the infection process that are poorly simulated in the pneumonic infection model. Our findings for the novel LDLV nasopharyngeal infection system show that aP vaccines provide much less protection against colonization by small numbers of _B. pertussis_ compared with wP vaccines. Thus, the LDLV nasopharyngeal infection model provides a complementary experimental system that enables the study of aspects of infection that are poorly mimicked in the HDHV pneumonic infection model. Further study of contemporary circulating _B. pertussis_ strains in the context of low-dose nasopharyngeal infections could help define the factors that contribute to the diverse mechanisms by which _B. pertussis_ evades immune responses. Such studies could elucidate how _B. pertussis_ is able to colonize, grow, shed, and be efficiently spread from host to host within aP-vaccinated populations. Furthermore, our model can guide development of new vaccines that can overcome the limitations of current aP vaccines and better control the circulation of this reemerging pathogen.\n\n【38】Dr. Soumana was a postdoctoral scholar in the laboratory of Dr. Harvill in the Department of Infectious Diseases, College of Veterinary Medicine, University of Georgia, Athens, Georgia, USA, when this work was conducted. He currently works in the Department of Medicine in the University of British Columbia, Vancouver, BC, Canada. His research interest lies in examining the contributions of respiratory microbiota on pathogenesis and disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a796a11e-abdb-4321-bafe-78004e9d8653", "title": "Addendum: Vol. 10, October 24 release", "text": "【0】Addendum: Vol. 10, October 24 release\nIn the article “Trends in Financial Barriers to Medical Care for Women Veterans, 2003–2004 and 2009–2010,” a sentence in the Acknowledgments was omitted. The additional sentence should read, “The authors thank Dr Diane C. Cowper Ripley for her review and Nichole Snyder for manuscript assistance.”\n\n【1】* * *\n\n【2】The opinions expressed by authors contributing to this journal do not necessarily reflect the opinions of the U.S. Department of Health and Human Services, the Public Health Service, the Centers for Disease Control and Prevention, or the authors' affiliated institutions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "84aed7c7-ed2a-4990-b3cd-808eb60c64e6", "title": "Highly Pathogenic Clone of Shiga Toxin–Producing Escherichia coli O157:H7, England and Wales", "text": "【0】Highly Pathogenic Clone of Shiga Toxin–Producing Escherichia coli O157:H7, England and Wales\nShiga toxin–producing _Escherichia coli_ (STEC) O157:H7 cause a wide range of gastrointestinal symptoms, including mild gastroenteritis, abdominal pain, vomiting, and bloody diarrhea . A subset of patients, most commonly the very old and the very young, go on to develop hemolytic uremic syndrome (HUS) . STEC O157:H7 are zoonotic, and transmission to humans is most commonly associated with ruminants, especially cattle and sheep. Transmission occurs by consumption of contaminated food or water or by direct contact with animals or their environment. The infectious dose is low (10–100 organisms), and person-to-person spread can occur in households, nurseries, and other institutional settings . The STEC pathotype is defined by the presence of the genes encoding Shiga toxin (Stx) type 1, type 2, or both, located on a bacteriophage incorporated into the bacterial genome . Stx1 and Stx2 can be further divided into subtypes Stx1a–1d and Stx2a–2g; Stx2a is strongly associated with causing severe disease . The STEC O157:H7 population has previously been delineated into 3 main lineages (I, I/II, and II) , and 7 sublineages (Ia, Ib, Ic, IIa, IIb, IIc, and I/ll) .\n\n【1】In England, the most common STEC serotype is O157:H7, which causes an average of 800 cases/year . All STEC O157:H7 isolated at local hospital laboratories from fecal samples from hospitalized patients and all cases in the community are submitted to the Gastrointestinal Bacteria Reference Unit (GBRU) at Public Health England for confirmation of identification and typing. From 2000 through 2016, phage type (PT) 8 with the _stx_ profile _stx1a/stx2c_ and PT21/28 with the _stx_ profile _stx2a_ or _stx2a/stx2c_ were detected most frequently in England, with PT21/28 the most frequently associated with severe disease .\n\n【2】Since 2015, all isolates submitted to GBRU have been genome sequenced. Whole-genome sequencing (WGS) demonstrates unparalleled sensitivity and accuracy in identifying linked cases . Using WGS data during outbreak investigations has improved the robustness of case ascertainment and provided forensic evidence for linking human cases to the source of their infection . Phylogenetic inference can also reveal how strains are related over time and space more accurately than other molecular typing methods and may provide insight into the evolutionary and epidemiologic context of emerging pathogenic clones .\n\n【3】In 2015, a total of 47 persons were affected by an outbreak in England of foodborne gastrointestinal illness caused by STEC O157:H7 PT8 _stx2a_ . The outbreak was associated with the consumption of contaminated prepacked salad leaves . The outbreak strain continued to cause sporadic infection and outbreaks of foodborne disease throughout 2016 and 2017 . The aim of our analysis was to investigate the evolutionary history of this newly emergent strain of STEC O157:H7 PT8 _stx2a_ and assess the risk to public health.\n\n【4】### Materials and Methods\n\n【5】##### Bacterial Strains\n\n【6】All isolates submitted to GBRU for confirmation and typing from local hospital laboratories in England and Wales during July 2015–December 2017 were sequenced for routine surveillance National Center for Biotechnology Information Short Read Archive bioproject PRJNA248042). We included an additional 74 clinical isolates of STEC O157:H7 belonging to sublineage IIb, the lineage containing STEC O157:H7 PT8 _stx2a_ , that were submitted to GBRU between January 2010–June 2015 from previous studies  . We selected these STEC O157:H7 isolates on the basis of _stx_ subtype and phage type diversity to provide context as a sample of the background population. We defined STEC O157:H7 isolates from patients who were hospitalized as a result of their gastrointestinal symptoms or who reported bloody diarrhea as highly pathogenic or as having increased pathogenic potential compared with isolates from patients who were asymptomatic or reporting nonbloody diarrhea.\n\n【7】##### Genome Sequence Analysis\n\n【8】For WGS, we extracted DNA from cultures of STEC O157:H7 for sequencing on the HiSeq 2500 instrument (Illumina, San Diego, California, USA). We mapped quality trimmed Illumina reads  to the STEC O157:H7 reference genome Sakai  using Burrows Wheeler Aligner-Maximum Exact Matching (BWA-MEM) . We identified single-nucleotide polymorphisms (SNPs) using Genome Analysis Toolkit version 2  in unified genotyper mode and extracted core genome positions that had a high-quality SNP (>90% consensus, minimum depth ×10, GQ \\> 30) in \\> 1 isolate for further analysis. We performed hierarchical single linkage clustering on the pairwise SNP difference between all strains at various distance thresholds . The result of the clustering is a SNP profile, or SNP address, that can be used to describe population structure based on clonal groups .\n\n【9】We performed recombination analysis using Gubbins  and reconstructed timed phylogenies using BEAST-MCMC version 2.4.7 . We computed alternative clock models and population priors and assessed their suitability on the basis of Bayes factor tests. The highest supported model was a relaxed log-normal clock rate with a Bayesian skyline population model. We ran all models with a chain length of 1 billion. We reconstructed a maximum clade credibility tree using TreeAnnotator version 1.75 .\n\n【10】We performed Stx subtyping as described by Ashton et al. The integration of Stx-encoding prophage into the host genome has been characterized into 6 target genes: _wrbA_ , encoding a NAD quinone oxidoreductase; _yehV_ , a transcriptional regulator; _sbcB_ , an exonuclease; _yecE_ , a gene of unknown function; the tRNA gene _argW_ ; and Z2577, which encodes an oxidoreductase . We mapped short reads from the STEC O157:H7 genomes to intact reference sequences of these genes, and aligned them with BWA MEM . We defined occupied Stx bacteriophage insertion (SBI) sites as those strains that had disrupted alignments . We used Tablet to visualize read pileups .\n\n【11】##### Data Analyses\n\n【12】The National Enhanced Surveillance System for STEC (NESSS) in England was implemented on January 1, 2009, and has been described in detail elsewhere . For this study, we extracted data from NESSS for the cases identified as being infected with strains that had been sequenced and belonging to the sublineage IIb cluster of interest (containing the STEC O157:H7 PT8 _stx2a_ , outbreak strain). We excluded asymptomatic carriers detected through screening high-risk contacts of symptomatic patients as well as patients who did not return the enhanced surveillance questionnaire (ESQ) to NESSS. Data analyzed included age, gender, and whether the patient reported symptoms of nonbloody diarrhea, bloody diarrhea, and vomiting along with whether cases were hospitalized, developed typical HUS, or died. Cases were categorized into children ( < 16 years of age) or adults, based on a priori knowledge that children are most at risk for both STEC infection and progression to HUS . Where clinical symptoms were blank on the ESQ, we coded them as negative responses for these symptoms. We divided cases into 3 groups based on _stx_ subtype: _stx2a_ , _stx2c,_ and _stx-_ negative.\n\n【13】We first described patients’ symptoms by _stx_ subtype as well as by age group and sex and also examined the distribution of _stx_ subtype by age and gender. We used Fisher exact tests to compare proportions among different groups. We assessed reporting of bloody diarrhea or hospitalization as a marker of disease severity by _stx_ subtype. We used logistic regression to calculate odds ratios (ORs) to assess bloody diarrhea by _stx_ subtype while adjusting for age (child/adult) and sex. We performed all analyses in Stata 13.0 (StataCorp LLC, College Station, TX, US).\n\n【14】### Results\n\n【15】##### Sublineage IIb\n\n【16】The STEC O157:H7 _stx2a_ clone analyzed in this study was located within sublineage IIb, and belonged to a 250 SNP single linkage cluster, designated 18%. This cluster comprised 251 clinical isolates: 138 of STEC O157:H7 _stx2a_ , 77 of _stx-_ negative _E. coli_ O157:H7, and 36 of STEC O157:H7 _stx2c_ . Since July 2015, when Public Health England implemented the use of WGS for STEC, the number of cases identified within sublineage IIb has remained stable (≈60/y). However, the number of cases of the _stx-_ negative _E. coli_ O157:H7 clone has declined, whereas the _stx2a_ and _stx2c_ clones are increasing .\n\n【17】##### Evolutionary Timescale and Stx Prophage Insertion in STEC O157:H7\n\n【18】We reconstructed a timed phylogeny of sublineage IIb . We calculated the mutation rate of STEC O157:H7 within sublineage IIb to be ≈2 mutations per genome per year (95% highest posterior density \\[HPD\\] 1.7–2.4). This rate is less than the 2.6 mutations per genome per year previously calculated across the complete STEC O157 population . Our analysis revealed that the emerging _stx2a_ clone evolved from a _stx-_ negative recent ancestor with the acquisition of _stx2a_ ≈10 years ago (95% HPD 9.0 years–12.7 years). Previously, this _stx_ \\-negative clone had evolved from a _stx2c_ progenitor ≈20 years ago (95% HPD 17.6 years–24.6 years) after the loss of _stx2c_ .\n\n【19】Historically, the majority of strains in sublineage IIb harbored a Stx2c-encoding prophage at _sbcB_ , with the _yehV_ SBI site occupied by a truncated non–Stx encoding prophage . Analysis of the short read data indicated that in the _stx-_ negative sublineage IIb clone, _yehV_ was disrupted but _sbcB_ was intact, indicating the loss of the Stx2c-encoding prophage from the SBI site. The more recently emerged sublineage IIb _stx2a_ clone had disrupted SBI sites at _sbcB_ and _yehV_ only, indicating that a Stx2a-encoding phage had been inserted into _sbcB_ , the site left vacant in the _stx-_ negative clone after the loss of _stx2c_ .\n\n【20】##### Disease Severity of Clinical Cases within the Sublineage IIb Cluster by _stx_ Subtype\n\n【21】Overall, 91.6% patients (230/251, 95% CI 88.1–95.1) had symptoms of diarrhea, and similar percentages were reported regardless of the _stx_ subtype profile of the STEC O157:H7 causing the infection . Rates of other symptoms varied; 28.3% of patients (71/251, 95% CI 22.7–33.9) reported vomiting, 35.1% (88/251, 95% CI: 29.1–41.0) experienced bloody diarrhea, and 18.7% (47/251, 95% CI: 13.9–23.5) were hospitalized. Hospitalization occurred more often for patients reporting bloody diarrhea (35.2% \\[31/88, 95% CI 25.0–45.4\\]) than those without bloody diarrhea (9.8% \\[16/163\\], 95% CI 5.2–14.1; p<0.001). Half (50.0%) of patients infected with _stx2a_ isolates reported bloody diarrhea (69/138, 95% CI 41.5–58.4), compared with 15.6% of patients infected with _stx_ \\-negative isolates (12/77, 95% CI 7.3–23.9) and 19.4% of those infected with _stx2c_ isolates (7/36, 95% CI 5.9–33.0; p<0.001). No patients were known to experience HUS, and none died.\n\n【22】Among the 251 clinical cases, 141 (56.2%, 95% CI 50.0%–62.3%) were adults and 136 (54.2%, 95% CI 48.0%–60.4%) were female. Adult patients were infected with _stx2a_ strains (61.0% \\[86/141\\], 95% CI 52.8%–69.1%) more often than children (47.3% \\[52/110\\], 95% CI 37.8%–56.7%; p = 0.030). Conversely, children were more often infected with _stx-_ negative strains than adults: 41.8% (46/110) of children (95% CI 32.4%–51.2%) versus 22.0% (31/141) of adults (95% CI 15.1%–28.9%; p = 0.001). There was also variation in _stx_ subtype by sex; proportionately more female patients were infected with _stx2a_ strains (61.0% \\[83/136\\], 95% CI 52.7%–69.3%) than were male patients (47.8% \\[55/115\\], 95% CI 38.6%–57.1%; p = 0.036). Adult patients reported bloody diarrhea (46.8% \\[66/141\\], 95% CI 38.5%–55.1%) more often than children (20.0% \\[22/110\\], 95% CI 12.2%–27.6%; p<0.001), as did female patients (40.4% \\[55/136\\], 95% CI 32.1%–48.8%) compared with male patients (28.7% \\[33/115\\], 95% CI 20.3%–37.1%), although the difference was not statistically significant (p = 0.05). The proportion of patients hospitalized did not differ significantly by sex or age group (data not shown).\n\n【23】After adjusting for age (adult or child) and sex, the odds ratio of experiencing bloody diarrhea was significantly higher in those infected with the _stx2a_ clone compared with patients infected with the _stx_ \\-negative clone . The odds of bloody diarrhea were no different for cases infected with the _stx2c_ clone than for the _stx_ \\-negative clone. Among the cases analyzed, being a child was protective for symptoms of bloody diarrhea.\n\n【24】### Discussion\n\n【25】The data described here support previous studies that showed the acquisition and loss of the Stx-encoding phage is highly dynamic in STEC O157:H7 . Most commonly described is the acquisition of _stx1a_ or _stx2a_ by a STEC O157:H7 _stx2c_ progenitor, followed by the subsequent loss of _stx2c_ in strains that acquired _stx2a_ . The involvement of a _stx_ \\-negative intermediate in this process, as captured here, has not been previously described. The loss of the Stx2c-encoding phage appears to have facilitated the acquisition of the Stx2a-encoding phage because the latter was inserted into the same SBI site, _sbcB_ , left vacant by the Stx2c-encoding phage.\n\n【26】Using phylogenetic analysis of variation at the whole-genome level, we reconstructed the recent evolutionary history of this emerging pathogenic clone within STEC O157:H7 sublineage IIb. We observed the loss of _stx2c_ from the _stx2c_ progenitor that caused a _stx_ \\-negative clone ≈20 years ago, followed by the acquisition of _stx2a_ ≈10 years ago, and later expansion as shown in Figure 1 . Previously, we showed that the historic acquisition of a Stx2a-encoding bacteriophage by a population of STEC O157:H7 PT2 _stx2c_ , belonging to lineage I/II indigenous in the UK cattle population, was associated with the first outbreaks of childhood HUS in England in the early 1980s . Subsequently, the increase in the incidence of STEC O157:H7 PT21/28 during the 1990s was linked to the acquisition of _stx2a_ by an indigenous population of STEC O157:H7 _stx2c_ belonging to sublineage Ic, resulting in the highly pathogenic contemporary clone STEC PT21/28 _stx2a_ / _stx2c_ . This clone has been associated with several outbreaks in the United Kingdom associated with a high incidence of HUS . Here, we described an _E.coli_ O157:H7 clone from yet another UK domestic lineage (sublineage IIb) that has recently acquired the Stx2a-encoding phage and is showing evidence of increasing pathogenic potential.\n\n【27】The analysis of disease severity of clinical cases by _stx_ subtype of isolates of STEC O157:H7 within the same sublineage IIb cluster showed a significant association between the presence of _stx2a_ and markers of disease severity; specifically, bloody diarrhea linked to higher rates of hospitalization. Previous studies have reported evidence of increased pathogenicity of STEC harboring _stx2a_ . However, these studies report on STEC from a wide range of different serotypes, exhibiting a wide variety of _stx_ subtypes and are based on relatively small datasets. In this study, we present the analysis of a large dataset focusing on a specific clade within a single serotype characterized by a limited number of _stx_ subtype combinations, specifically _stx2c_ , _stx_ negative, and _stx2a_ only. This analysis enabled us to make direct comparisons between specific _stx_ profiles while limiting the influence of other factors in the genome.\n\n【28】Strains of Stx-negative _E. coli_ O157:H7 are regarded as atypical enteropathogenic _E. coli_ (EPEC), defined by the presence of the intimin gene ( _eae_ ) and the absence of _stx_ and the _E. coli_ adherence factor (EAF) plasmid . EPEC are a common cause of infantile diarrhea and travelers’ diarrhea and are known to cause mild diarrhea in adults . In this study, the fact that clinical cases infected with the _E. coli_ O157:H7 _stx_ \\-negative clone reported a similar frequency of symptoms, including bloody diarrhea and hospitalization, as those infected with STEC O157:H7 _stx2c_ despite the loss of _stx_ was an unexpected finding that requires further investigation.\n\n【29】A timed phylogenetic reconstruction of the evolutionary history of a cluster of sublineage IIb charted the recent emergence of a highly pathogenic clone of STEC O157:H7 _stx2a_ . The symptom of bloody diarrhea, a marker of severity and predictor of HUS development , was strongly associated with cases infected with isolates of STEC O157:H7 harboring _stx2a_ compared with those isolates without _stx_ or those with _stx2c_ . Our analysis also illustrated the highly dynamic nature of the Stx-encoding phages. In contrast to the observed excision events of _stx2c_ \\-encoding phages in O157:H7, there is evidence to suggest that once a Stx2a-encoding phage is integrated into a population it tends to be maintained . As such, the emergence of yet another sublineage of STEC O157:H7 acquiring _stx2a_ is of public health concern. Through this study, we demonstrate that STEC O157:H7 WGS surveillance data have a role in monitoring and anticipating emerging threats to public health and in contributing to our understanding of the underlying pathogenic mechanisms associated with severe gastrointestinal illness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "871a1602-21e6-47a3-9255-439366509b28", "title": "Correction: Vol. 21, No. 8", "text": "【0】Correction: Vol. 21, No. 8\nAn incorrect word in a sentence in _Escherichia coli_ O157 Outbreaks in the United States, 2003–2012 (K.E. Heiman et al.) inadvertently changed the meaning. The sentence should have read, “The median annual number of outbreaks reported during 2008–2012 was higher than during 2003–2007 (45 vs. 33, p = 0.12) .” The article has been corrected online .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "14b1b06d-11db-44fd-a2c9-3bb3872c5c6b", "title": "Granulomatous Lymphadenitis as a Manifestation of Q Fever", "text": "【0】Granulomatous Lymphadenitis as a Manifestation of Q Fever\n**To the Editor:** Q fever is a worldwide zoonosis caused by the obligate intracellular pathogen _Coxiella burnetii_ . Human infection is usually the result of exposure to infected cattle, sheep or goats. Acute Q fever may be asymptomatic or manifest as a self-limiting febrile illness, pneumonia, hepatitis, or meningoencephalitis. Most cases of acute Q fever will resolve without sequelae, but endocarditis, granulomatous hepatitis, osteomyelitis, and endovascular infections are well-documented manifestations of chronic _C. burnetii_ infection . Recently, various atypical manifestations of acute , and chronic  Q fever have been reported as well as changing clinical presentation of Q fever endocarditis  and changing epidemiology of Q fever . Researchers have suggested that heightened awareness of Q fever among doctors, coupled with improved diagnostic methods, could increase the medical knowledge about this difficult-to-diagnose and difficult-to-treat infection . We report two cases of granulomatous lymphadenitis associated with _C. burnetii_ infection.\n\n【1】A 70-year-old man was admitted to the hospital because of weight loss, night sweats, and a continuous high-grade fever of 2 months’ duration. His past medical history was unremarkable, except for pulmonary tuberculosis treated 55 years earlier and chronic glaucoma. He lived in a rural area and had rare contact with cattle. On admission, his body temperature was 39.5°C; his right laterocervical lymph nodes were enlarged (3 cm x 4 cm) and inflamed. Blood values were unremarkable except for an elevated C-reactive protein level of 150 mg/L (normal<6). A computed tomography scan of the chest showed hilar calcifications and enlarged mediastinal lymph nodes. A biopsy of cervical lymph nodes indicated granulomatous lymphadenitis with foci of necrosis. _C. burnetii_ DNA was detected on the lymph nodes with a _C. burnetii_ –specific pair of primers that amplified an _htpAB_ \\-associated repetitive element . Results of serologic testing by indirect immunofluorescence (IF) were positive for _C. burnetii_ with immunoglobulin (Ig) G antibody titer to phase 1 and phase 2 antigen of 800 and 1,600, respectively, and IgM antibody titer to phase 2 antigen of 50.\n\n【2】A 44-year-old man was admitted to the hospital because of a continuous low-grade fever of 3 months’ duration. He had worked as a farmer for 15 years and assisted in the birth of sheep and cattle. On admission, his body temperature was 38°C, and right inguinal lymph nodes were inflamed, measuring 4 x 4 cm. A lymph node biopsy showed granulomatous lymphadenitis with stellate abscesses surrounded by palisading epithelioid cells. Serologic testing by indirect IF was positive for _C. burnetii_ with an IgG antibody titer to phase 1 antigen of 320.\n\n【3】For both patients, results of Ziehl staining and Lowenstein (Bio-Rad, Marne-La-Coquette, France) cultures of gastric aspirates (x 3) and lymph node specimens were negative for mycobaceria, as were the results of tuberculin skin tests. Other diseases were ruled out, including brucellosis, yersiniosis, bartonellosis, and chlamydial infections (by serologic testing) and fungal infections (parasitologic studies on lymph node tissue). Antinuclear antibodies were absent, and angiotensin-converting-enzyme values were normal. Both patients received doxycycline, 200 mg once a day, and rifampin, 600 mg twice a day, for 1 year, and the symptoms resolved (follow-up at 18 months for patient 1 and 9 months for patient 2, respectively). For patient 1, serologic testing after 1 year of treatment showed an IgG antibody titer to phase 1 antigen of 320.\n\n【4】Granulomatous lymphadenitis has been described during mycobacterial infections, tularemia, cat scratch disease, yersiniosis, lymphogranuloma venereum, histoplasmosis, coccidioidomycosis, and chronic granulomatous diseases . One well-documented case of acute Q fever with necrotic cervical lymphadenitis has been recently reported ; to our knowledge, granulomatous lymphadenitis has never been reported during Q fever. In both cases reported here, _C. burnetii_ was the likely etiologic agent, given the results of polymerase chain reaction and serologic studies (patient 1) or the patient’s occupation and results of the serologic testing (patient 2). Moreover, for both, no other potential cause could be identified, and the response to doxycycline-rifampin regimen was favorable. We suggest that granulomatous lymphadenitis be added to the list of atypical presentations of Q fever.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b371cbe2-fd84-4a1a-9c5f-d37007a3b78f", "title": "Leishmania major Cutaneous Leishmaniasis in 3 Travelers Returning from Israel to the Netherlands", "text": "【0】Leishmania major Cutaneous Leishmaniasis in 3 Travelers Returning from Israel to the Netherlands\n**To the Editor:** Cutaneous leishmaniasis (CL) is a protozoan disease transmitted by sand flies that usually runs a relatively mild course. Classic CL starts as a red papule at the place of the insect bite; it gradually enlarges into a painless nodule or plaque-like lesion, which eventually becomes encrusted. When the crust falls off, a typical ulcer with raised and indurated border becomes apparent. CL can cause considerable illness and may leave disfiguring and disabling scars after healing. The interplay between _Leishmania_ species and host immune response is complex, and, as a result, disease manifestations may vary substantially among species as well as among infected persons . An estimated 0.7–1.2 million new CL cases occur annually in tropical and subtropical regions of the world. CL is currently endemic in >98 countries worldwide; Afghanistan, Algeria, Colombia, Brazil, Iran, Syria, Ethiopia, North Sudan, Costa Rica, and Peru together account for up to 75% of global estimated CL incidence .\n\n【1】We report 3 travel companions from the Netherlands who all acquired CL after they participated in a short-term study course in Israel during September–October 2015. The travelers visited several places in the Negev Desert in southern Israel. All cases were confirmed by PCR with additional sequence analysis of the mini-exon locus and the 3′ untranslated region of the HSP70 locus, demonstrating _L. major_ as the causative species .\n\n【2】The first case-patient was a 55-year-old man who observed red papules on his head and shoulder 1 month after he returned to the Netherlands. Gradually, these papules increased in size, and number and showed a tendency to ulcerate. On examination at the Institute for Tropical Diseases in Rotterdam, 12 painless, hyperkeratotic, plaquelike, sharply demarcated lesions were identified, some partially ulcerated, located on the head, shoulders, arms, legs, and across the chest. Histopathologic examination on skin biopsy specimens acquired from 1 lesion revealed _Leishmania_ amastigotes, consistent with a diagnosis of CL. The patient was treated with miltefosine (50 mg orally 3×/d for 28). Clinical recovery followed gradually.\n\n【3】The second case-patient, a 52-year-old woman, noticed some red papules on both legs that gradually increased in size and ulcerated in the 2 months after return to the Netherlands. She was initially treated by a general practitioner for a presumed bacterial skin infection but did not show a clinical response to antibiotic treatment. On examination, also at the Institute for Tropical Diseases, 6 painless ulcers were seen on her legs. CL was suspected after taking into account the clinical manifestations and the recent diagnosis of CL in her travel companion. She was successfully treated with miltefosine after the diagnosis was confirmed.\n\n【4】A third case-patient, a 52-year-old woman, was diagnosed with CL after she sought treatment for a single small, sharply demarcated, painless pretibial plaquelike skin lesion on her arm that had been present for 2 months after her return to the Netherlands. Repeated PCRs of skin biopsy specimens confirmed the diagnosis of _L. major_ CL. She preferred a “wait and see” policy over treatment.\n\n【5】The 3 patients with CL, a cluster of travel companions, were conceivably infected in the Negev Desert. Only 1 previous report has documented a traveler returning from Israel who was diagnosed with CL at the Institute for Tropical Diseases during 2007–2016 . Most cases originated from the New World, in particular from South America, followed by Central America . Few of these cases originated from the Old World, where most of the global effects of CL occur. CL is more frequently diagnosed among long-term travelers such as military personnel, adventure travelers, photographers, and researchers, rather than among short-term travelers .\n\n【6】Recent reports have described a 7-fold increase in _L. major_ CL cases among inhabitants of the Negev Desert , with urban expansion into CL-endemic foci and changes in land use currently regarded as the most probable causes for this increase in incidence . A concurrent increase among travelers has not occurred so far, although the aforementioned cases might indicate that the increasing risk of contracting CL in the Negev Desert is not only restricted to its inhabitants but may also pose a risk to short-term travelers.\n\n【7】The spectrum of disease of CL is highly variable, even among persons infected with the same _Leishmania_ species , as illustrated by this cluster of _L. major_ CL cases among persons who traveled together to Israel. Because _L. major_ CL might mimic other infectious and inflammatory diseases, physicians assessing travelers with painless and persistent skin ulcers after their return from CL-endemic countries should therefore consider CL in their differential diagnosis. In conclusion, awareness should be raised among physicians and healthcare workers that CL is not exclusively limited to tropical countries but may also be acquired by short-term travelers to more temperate regions, such as southern Europe and the Levant (Syria, Lebanon, Israel, and Jordan) .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "522f73ce-7771-425d-9098-288396a1ef92", "title": "Correction: Vol. 6, No. 5", "text": "【0】Correction: Vol. 6, No. 5\nIn the article, “Prevalence of Non-O157:H7 Shiga Toxin-Producing Escherichia coli in Diarrheal Stool Samples from Nebraska,” by Paul D. Fey et al. an error occurred in reporting a primer used for amplifying the shiga-toxin gene.\n\n【1】The first complete sentence at the top of column 1, page 531, should read, “The following set of primers, which detects both stx1 and stx2, was used: 5' TTTACGATAGACTTCTCGAC 3' and 5' CACATATAAATTATTTCGCTC 3'.” We regret any confusion this error may have caused.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cd9a6c8d-4eef-47f8-9e92-1b2e790229cc", "title": "Human Monocytic Ehrlichiosis, Mexico City, Mexico", "text": "【0】Human Monocytic Ehrlichiosis, Mexico City, Mexico\nThe genus _Ehrlichia_ contains 6 species of obligately intracytoplasmic bacteria that have major roles in human and veterinary medicine. These bacteria can cause ehrlichiosis, an emerging zoonoses transmitted mainly by bites of several hard tick species of the genera _Amblyomma_ , _Ixodes_ , and _Rhipicephalus_ . In the Americas, the most relevant species that involves public health is _Ehrlichia chaffeensis_ , the etiologic agent of human monocytic ehrlichiosis, an acute disease characterized by fever, thrombocytopenia, leukopenia, alterations of coagulation, and hepatic and neurologic involvement; systemic complications can lead to death in 3% of case-patients .\n\n【1】Little is known about _Ehrlichia_ infections in Mexico. A human case was reported in the Yucatan Peninsula during 1999 in a male patient who had fever, anorexia, lymphadenopathy, cutaneous bleeding, and sore throat. Peripheral blood morulae, PCR detection of _E. chaffeensis_ , and anemia were also reported . Subsequently, a case of _E. canis_ infection was detected in the coastal state of Oaxaca in a veterinary stylist who had close contact with dogs . A study published in 2016 reported a female resident of the state of Mexico who had fever, thrombocytopenia, and alteration of liver enzyme levels and died after a long hospitalization . We report a fatal case of human monocytic ehrlichiosis in Mexico City, Mexico.\n\n【2】### The Study\n\n【3】On March 3, 2017, a 35-year-old man who was homeless (resident in Mexico City for 4 years) was admitted to the Emergency Department in the General Hospital of Xoco of the Ministry of Health of Mexico City because of trauma after a fall of 6 m from a bridge as a result of a suicide attempt. At admission, no lesions were detected in internal organs; a transtrochanteric fracture of the left femoral head was surgically repaired without complications. However, the patient had profuse bleeding during the surgical procedure, for which it was necessary to provide multiple blood transfusions. The blood units came from resident donors of Mexico City and the neighboring state of Mexico. In the postoperative period, the patient remained hospitalized for 65 days, during which behavioral alterations with several psychotic periods developed. He also had febrile episodes that evolved to a torpid state.\n\n【4】Routine laboratory analyses (blood count, blood chemistry, and serologic studies for infectious diseases) were performed. Because the patient was homeless and had persistent febrile episodes and dysfunction of the coagulation system, a possible infection by a rickettsial agent or _Bartonella_ spp. was suspected because _Bartonella quintana_ has been detected previously in human lice from persons who were homeless in Mexico City . For this reason 5 mL of whole blood was obtained, stored in EDTA, and processed for DNA extraction by using the QIAamp DNA Mini Kit  according to the manufacturer’s instructions. Blood was then examined for _Bartonella_ , _Ehrlichia_ / _Anaplasma_ , and _Rickettsia_ spp. by amplifying fragments of the _gltA, rrs_ , and _sca5_ , genes and using primers and PCR conditions specified . Positive controls  DNA\\]) were also included.\n\n【5】On April 27, 2017, antibodies against _Proteus_ OX-19 at a titer of 1:320, leukocytosis, thrombocytosis, lymphopenia, anemia, hypoalbuminemia, and coagulation and liver enzyme alterations were reported . The patient was given a diagnosis of septic shock and urosepsis and died on day 63 of hospitalization.\n\n【6】During hospitalization, leukocytosis developed, which might have been associated with the multiple traumas. The patient initially had a platelet count within the reference range, but thrombocytosis developed, and the platelet count increased to 850,000/μL. However, with persistent leukocytosis, the platelet count dropped, and thrombocytopenia (108,000/μL) developed shortly before death.\n\n【7】Total serum protein and albumin concentrations at admission were within reference ranges, but during hospitalization they decreased, as would be expected for a diagnosis of ehrlichiosis. Alterations in liver enzyme levels and coagulation times also developed. Levels of alanine aminotransferase (ALT) and aspartate aminotransferase also increased after admission; ALT showed the largest increase. The level of γ-glutamyl transpeptidase increased by >10 times over its reference value . The increase in the level of ALT could be related to hepatic alterations linked to ehrlichiosis.\n\n【8】Necropsy showed hepatosplenomegaly and pleural effusions. Molecular assays did not detect _Bartonella_ or _Rickettsia_ spp. but they did detect _Ehrlichia/Anaplasma_ spp. when primers Ehr1/Ehr2 were used. We isolated a 400-bp fragment of the 16S rRNA gene, which showed 99% identity with that of the _E. chaffensis_ strain Arkansas. In addition, a 500-bp fragment of the _dsb_ gene (present only in the members of the genus _Ehrlichia_ ) was amplified  and showed 100% identity with the sequence of _E. chaffensis_ strain Arkansas.\n\n【9】Phylogenetic analysis of the isolated sequences grouped them with sequences of 2 strains of _E. chaffensis_ (strain Arkansas and West Paces) detected in the United States; these sequences had a bootstrap value of 99. The sequences we obtained were deposited in GenBank under accession numbers MK351589 and MK370999 .\n\n【10】### Conclusions\n\n【11】Ehrlichioses represent systemic infections that can cause damage to different organs and systems, affecting the liver, meninges, brain, heart, and lungs. Because the pathophysiology of this disease is not well established, the findings obtained by necropsy are useful. During his hospitalization, the patient acquired some bacterial infections, which could explain the paradox that he initially had thrombocytosis and leukocytosis instead of thrombocytopenia and leukopenia, as would be expected for ehrlichiosis. Leukopenia and thrombocytopenia are present in >50% of patients given a diagnosis of _E. chaffeensis_ infection .\n\n【12】The patient might have been infected by blood transfusion because the long period between hospital admission and detection of thrombocytopenia is compatible with the incubation period for infection with _E. chaffeensis_ . The patient received multiple blood transfusions from donors in Mexico City and the state of Mexico; in this state, a fatal case of ehrlichiosis caused by blood transfusion has been reported . A study in Costa Rica reported a large number of _E. canis_ –infected healthy blood donors , and a study in the United States reported a patient who was infected by blood transfusion containing _E. ewingii_ . More recently, a potential case of transfusion-transmitted human monocytic ehrlichiosis was reported in a 59-year-old woman in the United States after she received a blood stem cell transplant .\n\n【13】Although this possibility is less likely, it cannot be ruled out that ehrlichiosis could be acquired by tick bite. _E. chaffeensis_ was previously detected in 3 hard tick species ( _Amblyomma americanum_ , _A. mixtum_ , and _Rhipicephalus sanguineus_ sensu 1ato)  in Mexico. Only the lone star tick ( _A. americanum_ ) has been implicated as a primary vector of this pathogen in the United States; however, its presence in Mexico has only been recorded restricted to the Nearctic region . Conversely, no studies demonstrate the role of the other 2 tick species as potential vectors of this pathogen or whether DNA of this pathogen came from an infected host from which these ticks fed.\n\n【14】For this reason, it is essential to perform systematic surveillance of this and other tick-borne pathogens in blood donors in Mexico. Studies should also be conducted with questing ticks to identify the risk to which the population is exposed and establish the actual distribution of the pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b1d99898-5464-4414-8854-c05e77986a65", "title": "Identification of a Novel α-herpesvirus Associated with Ulcerative Stomatitis in Donkeys", "text": "【0】Identification of a Novel α-herpesvirus Associated with Ulcerative Stomatitis in Donkeys\nVesicular stomatitis is a consequential disease of equids. Vesicular stomatitis virus (family _Rhabdoviridae_ , genus _Vesiculovirus_ ) is a major infectious agent with zoonotic potential that is common in the Americas. A few other infectious viral agents (equine arteritis virus, bunyavirus, caliciviruses, equine adenoviruses, and herpesviruses) have been associated with this condition in horses, but on several occasions the etiology of vesicular and ulcerative stomatitis remains undiagnosed . Noninfectious etiology may include plant- and drug-related toxicoses and photosensitization .\n\n【1】In October 2011, an outbreak of ulcerative stomatitis started in a donkey ( _Equus asinus_ ) dairy herd, comprising 106 animals, located in the prefecture of Bari, Apulia region, Italy. The outbreak appeared related to the introduction of a pregnant female 8 weeks before the onset of the disease. The mare was clinically healthy and in good physical condition at arrival and gave birth to a foal after 7 weeks. Clinical signs developed in neither the dam nor the foal.\n\n【2】Initially, the outbreak affected 34 animals (17 lactating mares with their foals). This group was separated with wood fences from the other animals, but not strictly. Fever and small nodular lesions, evolving into painful ulcers, were observed on the oral mucosa, tongue, and skin around the lips of young animals (2 weeks–4 months old) . Similar lesions were also observed sporadically on the dams’ udders and genital areas. The lesions typically resolved in 1–3 weeks. The herd owner reported weight loss in foals and interruption of lactation in dams. Two weeks after onset in the original group, the disease was observed in a separate group of animals, comprising 63 adult or yearling females and 5 yearling males. In this group, however, oral lesions were observed only in 5 yearlings and 1 mare. A third group of 4 adult males was kept apart from the other animals and was not affected by the disease.\n\n【3】Oral swab and serum samples collected from 8 animals with acute disease were sent to the laboratories at the University of Bari (Valenzano, Italy) for virologic investigation. Using an electron microscope, we observed herpesvirus-like particles in the oral swabs and detected herpesvirus DNA using consensus herpesvirus primers for the DNA polymerase and inverse terminase . We used BLASTn  to search the GenBank genetic sequence database and found the virus to be highly related to equid herpesvirus (EHV) 3 in the DNA polymerase (93.35% nt identity) and terminase (90.71% nt identity) regions.\n\n【4】We isolated the virus onto equine dermal cells from oral swab specimens. The virus was titrated and used for screening serum samples collected from the donkeys in virus neutralization assays. We detected specific neutralizing antibodies in the serum samples collected from approximately three quarters (80/106) of the animals 2 months after the beginning of the outbreak but not in the serum samples of 8 animals with acute infection, suggesting seroconversion.\n\n【5】To sequence the DNA of the isolate, we performed next generation sequencing using the Illumina MiSeq platform  and used Nextera XT (Illumina) for library preparation. We obtained the full genome sequence (147,607 bp) of asinine herpesvirus (AsHV) strain AsHV/Bari/2011/740 and annotated it using the software ORF Finder .\n\n【6】On full genome sequence analysis, strain AsHV/Bari/2011/740 appeared genetically related (87.02% nt identity) to EHV-3 strain AR/2007/C3A  (subfamily _α-Herpesviridae_ , genus _Varicellovirus_ ). Three different full-length gene targets (glycoproteins B, C, and D) of strain AsHV/Bari/2011/740 were aligned with cognate sequences representative of the genus _Varicellovirus_ listed by the International Committee on Taxonomy of Viruses  by using Geneious software version 9.1.8  and the MAFFT algorithm . We performed phylogenetic analyses with MEGAX software   using the maximum-likelihood method with the general time-reversible model, a proportion of invariant sites, and a discrete gamma distribution (5 categories) to model evolutionary rate differences among sites, and bootstrap analyses with 1,000 pseudoreplicate datasets. In the consensus phylogenetic trees , strain AsHV/Bari/2011/740 was closely related to EHV-3 sequences and distantly related to other members of the genus _Varicellovirus_ . We deposited the full-genome sequence of strain AsHV/Bari/2011/740 in the GenBank database .\n\n【7】To date, several herpesviruses have been discovered in donkeys . AsHV type 1, also called EHV-6, is an α-herpesvirus associated with ulcerative lesions . AsHV-2 (EHV-7) is a γ-herpesvirus identified in leukocytes of healthy animals . AsHV-3 (EHV-8) was isolated from the nose of immunodepressed animals  and was classified as an α-herpesvirus on the basis of the glycoprotein G sequence and poor antigenic cross-reactivity with EHV-1 and EHV-4 . AsHV-4, -5, and -6 are γ-herpesviruses identified from donkeys with interstitial pneumonitis .\n\n【8】We report the detection and isolation of a novel AsHV from an outbreak of vesicular and ulcerative stomatitis and mammillitis in a donkey dairy herd. By comparing it with other herpesvirus sequences from the databases, we identified 3 targets (glycoproteins B, C, and D) for which the sequences were available across all the varicelloviruses listed in the ICTV database and that were used for phylogenetic analysis. In these analyses, the AsHV strain appeared similar to EHV-3. By reviewing the literature, we found another donkey herpesvirus, AsHV-1, genetically related to EHV-3 on the basis of restriction enzyme and hybridization analyses . AsHV-1 was originally isolated from the vesicular and erosive lesions of the muzzle of a foal and the external genitalia and udder of its dam . Unfortunately, the prototype AsHV-1 is no longer available and it is not possible to determine its genome sequence for precise comparison . It is possible that AsHV/Bari/2011 is actually an AsHV-1 strain, although this possibility cannot be confirmed.\n\n【9】Overall, based on the chronology of the health events observed in the herd, the tendency of herpesviruses to reactivate from latent infection under stress conditions, and the seroconversion observed in the monitored animals, we hypothesized that the newly introduced mare was the vehicle for herpesvirus infection in the herd, although this possibility could not be conclusively confirmed. Also, we screened only 8 animals during the acute phase of the disease, and we do not have an exact picture of the immunological status of the animals before the onset of the disease.\n\n【10】In conclusion, we identified a novel AsHV, genetically related to EHV-3, from an outbreak of infectious ulcerative stomatitis in donkey foals. These findings extend the spectrum of pathologies potentially attributable to herpesviruses in donkeys. Considering the nature and shape of the lesions, the virus should be included in the differential diagnosis of vesicular and ulcerative stomatitis among equids. Also, it needs to be determined whether the novel AsHV can be transmitted to horses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d4034a61-f774-4322-8ce0-726654a21bb2", "title": "Rabbit Hepatitis E Virus Infections in Humans, France", "text": "【0】Rabbit Hepatitis E Virus Infections in Humans, France\nReports of hepatitis E virus (HEV) infections in humans and animals are becoming more frequent. HEV is a member of the family _Hepeviridae._ HEV strains that infect humans (HEV1, HEV2, HEV3, HEV4, and HEV7) belong to the genus _Orthohepevirus_ . In industrialized countries, HEV transmission is mainly zoonotic, and the most prevalent genotype is HEV3. This genotype is transmitted mainly by direct contact with infected pigs, eating contaminated food products, or the environment . Genotype 3 includes 3 clades, 2 (3-efg and 3-abchij) of which are found in humans and pigs and 1 (3-ra) of which is found in rabbits . HEV3-ra has a 93-nt insertion in the X domain of the genome . This virus has been identified in both farmed and wild rabbits worldwide  and a pet rabbit .\n\n【1】HEV3 infections are generally asymptomatic and self-limiting, but symptomatic acute hepatitis develops in some patients, mostly older men. Fulminant hepatitis can occur in patients with underlying liver disease, and HEV3 infections can become chronic in immunocompromised patients, such as recipients of solid-organ transplants, persons with hematologic diseases, and patients infected with HIV . Although only 1 case of infection with human HEV3-ra has been identified , the contribution of HEV3-ra to human infection remains uncertain.\n\n【2】### The Study\n\n【3】The French National Reference Center for HEV (Paris, France) analyzed 919 HEV strains obtained from patients in France infected during 2015–2016. Strains were obtained in hospitals (90%) or private medical laboratories (10%). A total of 20% of the strains were obtained from immunocompromised patients.\n\n【4】We detected HEV RNA by using a reverse transcription PCR (RT-PCR) 15189 accredited by the International Organization of Standardization (Geneva, Switzerland) . We used a nested RT-PCR to amplify a 345-nt sequence within HEV open reading frame 2 as described . To amplify a 365-nt fragment within the X domain, we performed a nested RT-PCR with outer primers 2600-DOM-X-S (5′-TAYCGRGARACYTGYTCCCG-3′) and 3050-DOM-X-AS (5′-ACATCRACATCCCCCTGYTGTATRGA-3′) and inner primers 2685-DOM-X-S (5′-AGYTTTGAYGCCTGGG-3′) and 3050-DOM-X-AS. Phylogenetic analyses were performed by using neighbor-joining methods, a bootstrap of 1,000 replicates, and MEGA version 7.0 software .\n\n【5】Among the 919 patients, 904 were infected with a genotype 3 strain. The subtype infecting 75 (8.1%) of these HEV3-infected patients could not be determined; 302 (32.9%) patients were infected with clade HEV3-abchij, 522 (56.8%) were infected with clade HEV3-efg, and 5 (0.5%) were infected with clade HEV3-ra (GenBank accession nos. KY611812–KY611816) . All 5 HEV3-ra strains had an insertion in the X domain of the genome (GenBank accession nos. KY825957–KY825961).\n\n【6】All 5 infected patients were men (median age 52 years, range 38–64 years). None of these men had epidemiologic links to HEV or had traveled abroad; 2 lived in northern France, and 3 lived in southern France. One infected patient was immunocompetent and 4 were immunocompromised. The immunocompetent patient had alcoholic cirrhosis and decompensation of his cirrhosis because of the HEV infection. He cleared the HEV infection spontaneously.\n\n【7】All 4 immunocompromised patients were asymptomatic: 2 were solid-organ transplant recipients, and 2 had hematologic malignancies that were being treated with chemotherapy. Levels of alanine aminotransferase for these 4 patients were persistently high, and plasma HEV RNA was still detected 3 months after the initial evaluation, which indicated a chronic infection . Three patients were given ribavirin therapy for 3 months. The patients with hematologic malignancies eliminated the virus, but the kidney transplant recipient had a relapse when treatment was stopped.\n\n【8】The source of infection was unclear because all 5 patients reported they had no direct contact with rabbits. Three patients lived in rural areas, but none recalled any contact with wild or farmed animals. None of them was a hunter. Two patients reported they had eaten rabbit products, but the products were always well-cooked. All patients regularly ate various pork products, and 3 frequently ate raw shellfish (oysters, mussels, or scallops). Two drank tap water, and 3 drank only bottled water. One patient had a vegetable garden .\n\n【9】### Conclusions\n\n【10】HEV3-ra can infect humans and its pathogenesis is similar to that of other HEV3 subtypes. There have been frequent reports of autochthonous HEV3 infections in several industrialized countries, particularly France . The HEV3 subtypes responsible are usually similar to those found in swine. These subtypes belong to clades 3-efg and 3-abchij . Their distribution among 919 symptomatic cases (3c, 26.7%; 3e, 2%; and 3f, 47%) in our study was similar to that reported for asymptomatic blood donors in France . However, HEV3-ra strains were not detected in blood donors, probably because of a small sample size.\n\n【11】The rarity of HEV3-ra infections in humans could be the result of fewer persons eating rabbit than pork products . In addition, we found that 80% of the HEV3-ra strains were obtained from immunocompromised patients. However, this population represents only 20% of the HEV-infected patients characterized by our laboratory. This HEV3 subtype could be less infectious than other subtypes for humans, but additional studies are needed to verify this hypothesis.\n\n【12】We could not identify the route by which our patients became infected with HEV3-ra. None had any direct contact with rabbits; 2 had eaten well-cooked rabbit. Data from a recent nationwide study in France suggested that waterborne transmission might play a role in HEV epidemiology . This type of transmission might be the way the patients in our study became infected because we have detected HEV3-ra in environmental samples .\n\n【13】To our knowledge, clinical manifestations associated with HEV3-ra have not been reported. We found that HEV3-ra infections caused severe decompensation in a patient with alcoholic cirrhosis. Chronic infections developed in the 4 immunocompromised patients. Patients in industrialized countries infected with other HEV3 subtypes also showed similar clinical findings . We also found that ribavirin had an antiviral effect on HEV3-ra. Ribavirin eliminated the virus in 2 of the 3 patients given this drug. This finding is consistent with that of a multicenter study, which reported that 78% of persons with chronic HEV3 infections were successfully treated with this drug .\n\n【14】Our findings emphasize the zoonotic risk for HEV3-ra and expand the spectrum of potential sources of human infection. The route by which HEV3-ra is transmitted to humans needs to be investigated. Longitudinal study of HEV diversity is also needed to assess trends over time.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4551daee-3392-43c4-8230-69506b69257c", "title": "Transmission Potential of Influenza A(H7N9) Virus, China, 2013–2014", "text": "【0】Transmission Potential of Influenza A(H7N9) Virus, China, 2013–2014\nFrom February 19, 2013, through April 22, 2014, a total of 429 cases of influenza A(H7N9) virus infection in humans in China were reported and occurred in 2 outbreak waves. During the first wave in spring 2013, live bird markets were closed in several parts of China ; these market closures substantially reduced the risk for infection in affected regions . During a second wave in autumn 2013 , markets were again closed in some provinces . Analysis of the largest clusters of subtype H7N9 virus infection in 2013 suggested that the basic reproduction number (R 0  , the average number of secondary cases generated by a typical infectious host in a fully susceptible population) was higher in some clusters than in others , although the absence of sustained transmission implied that R 0  was less than the critical value of 1. To determine the transmission potential of influenza A(H7N9) virus in the first and second waves in 2013, we compared symptom onset data. We also measured the extent to which market closures in 2014 reduced spillover hazard (i.e. risk for animal-to-human infection).\n\n【1】### The Study\n\n【2】We focused on the locations of the 6 largest outbreaks: Shanghai, Zhejiang, and Jiangsu (first wave) and Guangdong, Zhejiang, and Jiangsu (second wave). To infer market hazard and human-to-human transmission potential, we used a statistical model of infection spillover . We assumed that cases could be generated in 1 of 2 ways: on each day, the expected number of reported cases was equal to the sum of animal exposure and secondary cases generated by earlier infectious hosts . Use of such a framework enables estimation of the degree of human-to-human transmission from symptom onset data and of exposure hazard from markets; the accuracy of these estimates is greatly improved when the timing of a sudden change in hazard, such as a market closure, is known . We therefore constrained the timing of the drop in exposure hazard to reported market closure dates . We also estimated R 0  for each of the 6 outbreaks. For patients with known exposure, cluster reports suggest that the serial interval (time delay between symptom onset in primary and secondary case-patients) could be 7–8 days . We therefore assumed a serial interval of 7 days for our main analysis and tested a range of values from 3 to 9 days during sensitivity analysis. We adjusted for potential delays between symptom onset and case report on the basis of the distribution of delays to date .\n\n【3】During the first wave, cases were initially concentrated around Shanghai; reports centered on the city and neighboring Zhejiang and Jiangsu . A wave-like relationship between location and onset timing was apparent; distance between the location of the first case-patient in Shanghai and subsequent case-patients increased over time . The pattern of cases at the start of the second wave suggests that infection did not spread outward from a single source; in October 2013, initial cases occurred in Guangdong and Zhejiang.\n\n【4】We used our statistical model to estimate the relative contributions of animal-to-human and human-to-human transmission. In Zhejiang, Shanghai, and Guangdong, market hazard clearly increased and decreased at the start and end of the outbreak, respectively . We also estimated R 0  for different regions over the 2 outbreak waves . Although our estimates for Jiangsu did not change significantly between the 2 waves, for Zhejiang, R 0  was significantly higher for the second wave than for the first wave in spring 2013 (p = 0.045). We estimated R 0  to be 0.06 (95% credible interval \\[CrI\\] 0.00–0.25) in the first wave and 0.35 (95% CrI 0.15–0.65) in the second.\n\n【5】Using our estimates for R 0  and market hazard, we estimated the number of cases in each outbreak that resulted from human-to-human rather than animal-to-human transmission. We found evidence of a small but significant amount of transmission between humans in the first and second waves . Our findings agree with reports of possible human clusters in the first wave  and corroborate media reports of possible human clusters in Zhejiang and Guangdong during 2013–2014. We identified 5 clusters during the first wave (February–April 2013) and 8 clusters during the second wave (November 2013–May 2014); the clusters in both waves had median size of 2 cases per cluster . These conclusions were robust under different assumptions about the duration of serial interval .\n\n【6】During the second wave, market closures in Zhejiang began on January 22, 2014, and ended on January 26, 2014 . The reduction in spillover hazard after these closures was significant. We estimated that closures for a serial interval of 7 days reduced hazard by 97% (95% CrI 92%–99%). During 2013, estimated effectiveness was similar in Zhejiang (99%; 95% CrI 97%–100%) and Shanghai (99%; 95% CrI 95%–100%). These estimates are in agreement with those from other analyses for the first wave . The 95% CrI was broader for Jiangsu, however, where estimated effectiveness was 97% (95% CrI 80%–100%). In Guangdong, Guangzhou markets closed on February 16, 2014, and reopened on February 28; markets in other cities in Guangdong closed around the same time for 2 weeks. Our results suggest that these closures reduced hazard by 73% (95% CrI 53%–89%). This reduction was significantly smaller than that for Shanghai and Zhejiang (p<0.01). Our result was robust at different serial intervals of infection .\n\n【7】Despite the effectiveness of closures during the first wave, interventions in most regions were delayed until after the Chinese New Year (January 31, 2014). Some regions are investigating alternative market practices: Guangzhou has implemented a trial of a permanent ban on live poultry sales in certain markets, potentially to extend over the entire city by 2024 . Our results support recommendations made after the first wave of outbreaks in 2013 , which suggest that prompt closure of markets could lead to substantially fewer infections. However, our finding that the relative effectiveness of the shorter closure in Guangdong was lower suggests that such interventions are needed for a sufficiently long time to prevent recurrence.\n\n【8】Our study has limitations. First, case data were insufficient for us to jointly infer serial interval and transmissibility. We therefore tested our results against a wide range of plausible assumptions about the serial interval of infection . We also assumed that the market hazard increased and decreased in a simple stepwise manner . Local market density could also influence the size of spillover hazard and, hence, effectiveness of interventions . If the market hazard could be better characterized (e.g. by longitudinal serologic surveillance ), the accuracy of our estimates would probably be improved . When estimating R 0  , we did not incorporate individual-level variability in transmission and potential superspreading events. However, the framework that we used can still produce reliable estimates of R 0  when a population contains superspreaders .\n\n【9】### Conclusions\n\n【10】We found no evidence of reduced human-to-human transmission between the 2 waves. For a serial interval of 7 days, we estimated that R 0  increased in Zhejiang. Furthermore, the effectiveness of live bird market closures varied between regions; short-term closures were substantially less effective than interventions in other regions. These results emphasize the value of prompt and sustainable control measures during outbreaks of influenza A(H7N9) virus infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2110fe39-b52d-4eb3-b54f-e76ae9219636", "title": "Clean Indoor Air Regulation and Incidence of Hospital Admissions for Acute Coronary Syndrome in Kanawha County, West Virginia", "text": "【0】Clean Indoor Air Regulation and Incidence of Hospital Admissions for Acute Coronary Syndrome in Kanawha County, West Virginia\nAbstract\n--------\n\n【1】**Introduction**Secondhand smoke is a risk factor for coronary heart disease. Laws and regulations prohibiting smoking in public areas and workplaces can reduce rates of acute myocardial infarction. Our objective was to describe hospital admission rates for acute coronary events, based on smoking status, diabetes status, and sex, in the presence of a long-standing  county clean indoor air regulation (CIAR). We also examined the effect of making restaurants completely smoke-free. **Methods**We obtained hospital admission data for acute coronary syndrome (ACS) and acute myocardial infarction from all acute care hospitals serving Kanawha County, West Virginia, for 2000 through 2008. A CIAR was enacted in 1995 and revised in 2000 and 2003. We performed descriptive analyses on hospital admission rates of ACS over time and present these data by sex, age group, smoking status, and medical history of diabetes. **Results**The incidence of hospital admissions for ACS consistently declined during the period studied. This change was most pronounced among nonsmokers, people without diabetes, and women, compared with their respective counterparts. Similar benefits occurred for male smokers when the CIAR was revised to make restaurants completely smoke-free in 2004. **Conclusions**In the presence of a CIAR, a consistent decline in incidence of hospital admissions for ACS can be demonstrated. However, the benefits derived may be disproportionately affected by smoking status, diabetes status, and sex.  \n\n【2】Introduction\n------------\n\n【3】Secondhand smoke was established as a cause of lung disease in nonsmokers in 1986 . Subsequently, other diseases and adverse effects of secondhand smoke were established, including increased risk for coronary heart disease (CHD) . Specifically, secondhand smoke exposure increases CHD risk by 25% to 30% . These risks are attributed to various mechanisms including but not limited to endothelial dysfunction and arterial stiffness, increased oxidative stress, reduced heart rate variability, and increased insulin resistance . All levels of US government have been slow to provide the public with comprehensive clean indoor air policies . The greatest obstacle to making fundamental societal changes is not funding but the lack of political will . In 2009, West Virginia (along with Kentucky) had the highest rate of adult smokers in the nation, 26% . However, because of the autonomous nature of local boards of health in West Virginia, only 18 of its 55 counties currently have comprehensive indoor air regulations. The failure to provide this protection places people at risk. In the first published study of the effect of a smoking ban on heart disease rates, following legislation in Helena, Montana, that required smoke-free workplaces and public places, a significant drop in acute myocardial infarction (AMI) was observed; this reduction ended after 6 months when the ban was repealed . Similarly, AMI reductions were found when smoking prohibitions were implemented in 4 other US jurisdictions  and in Canada , Italy , and Scotland . A meta-analysis of 11 reports from 10 study locations demonstrated a mean AMI decrease of 14% after smoking bans were implemented. The effect was most pronounced for younger adults . Researchers projected that 156,400 AMIs would be prevented each year if comprehensive smoking regulations were launched in the United States. In October 2009, the Institute of Medicine concluded that sufficient evidence exists to infer a causal relationship between secondhand smoke exposure and increased risks of CHD illness and death among both men and women and that a decrease in secondhand smoke exposure decreases the risk of AMI . However, insufficient evidence exists to determine whether the beneficial effects vary by smoking status, diabetes status, and sex. Whether the decline in hospital admissions for such acute cardiac events is maintained over longer periods is unknown. Therefore, the objective of our study was to describe hospital admission rates over time  for acute coronary events, by smoking status, diabetes status, and sex, in the presence of an existing county-wide clean indoor air regulation (CIAR or regulation). We also examined the effect of making restaurants completely smoke-free.  \n\n【4】Methods\n-------\n\n【5】We retrospectively studied electronic hospital records to investigate the effect of a county regulation on public health. Since all data were de-identified, West Virginia Universitys institutional review board exempted the study from review. \n\n【6】### Setting\n\n【7】Kanawha County is the most populated county in West Virginia and is home to the state capital, Charleston. The 2009 US Census estimated its population at 191,663 (approximately 89% white and 8% African American) . Effective May 22, 1995, a modest smoking regulation was enacted by the Kanawha-Charleston Board of Health, prohibiting smoking in all enclosed public places in Kanawha County. At the time, restaurants were allowed to designate up to 50% of their seating capacity as smoking areas. On July 20, 2000, the CIAR was modified to increase penalties for violations. On April 3, 2003, a revised regulation prohibited smoking in all restaurants and at most worksites. However, to come into compliance, the regulation allowed several businesses an exemption until January 1, 2004. \n\n【8】### Data collection\n\n【9】We obtained data from the 3 major acute care hospitals that serve Kanawha County, including the largest in the state. We examined the following diagnostic codes from the _International Classification of Diseases, Ninth Revision, Clinical Modification_ (ICD-9-CM) : 410 (410.0-410.9), 411.1, 411.81, 411.89, 413.0, 413.1, and 413.9. Data inclusion criteria were all patients (de-identified) admitted from January 1, 2000, through September 30, 2008, with a primary ICD-9-CM diagnostic code of myocardial infarction, non-ST segment elevation myocardial infarction, or unstable angina (together termed ACS); Kanawha County residents; and patients who were aged 18 years or older on the date of hospital admission. For each Kanawha County resident, basic information for each patient included the date of hospital admission/discharge, diagnostic code, age, sex, race, associated medical history such as diabetes, and smoking status (whether they smoked in the past year). \n\n【10】### Statistical analysis\n\n【11】We performed descriptive analyses on the number of hospital admissions for ACS over time by sex, age group, smoking status, and patients’ medical history of diabetes. We divided the number of hospital admissions by Kanawha County’s age- and sex-specific population to calculate ACS hospital admission rates. We estimated age-adjusted rates by using year 2000 US census data as standard population. We defined the comprehensive CIAR as starting on January 1, 2004. We fitted a Poisson regression model on the data to assess the effect of the CIAR (coded 0 before 2004 and 1 afterward) on the hospital admission of ACS, adjusting for age (18-49 y, 50-59 y, 60-69 y, 70-79 y, and ≥80 y), and we treated the ordered code 1-5 as a continuous variable; sex (female, male), year as a continuous variable, season (spring: March-May; summer: June-August; fall: September-November; and winter: December-February, with spring as reference); tobacco (no, yes), and history of diabetes (no, yes). Because hospital admission for ACS may have a seasonal effect, we adjusted for seasons in our model. We also assessed the impact of the CIAR stratified by sex and smoking status after adjusting for age, year, season, and history of diabetes. We added interactions between calendar year and important variables (smoking status, diabetes status, dates of the regulations) into the model to examine whether the slope of the time trend would vary by different populations, including the enactment of the regulation. Finally, we also examined data for AMI (coded as 410.0-410.9) similar to the analysis performed for ACS.  \n\n【12】Results\n-------\n\n【13】Overall, we included 14,245 hospital patients who were admitted for ACS between January 1, 2000, and September 30, 2008 (mean age, 66 y). Of them, 8,075 (57%) patients were men; 3,633 (26%) were smokers, and 5,048 (35%) had diabetes. The number of hospital admissions of ACS per year declined over the entire period, from 1,949 patients in 2000 to 1,208 in 2008. This decline was most pronounced among nonsmokers, women, and adults without diabetes. The cumulative decrease in the age-adjusted ACS hospitalization rates between 2000 and 2008 was 37% overall, 36% for men and 39% for women. The patterns of decline over time were similar for men and women . However, when we stratified the age-adjusted rates of hospital admissions for ACS by smoking status, the observed decline of age-adjusted hospital admission rates for ACS was significant only among nonsmokers (both men and women) . Declines in the age-adjusted rates for hospital admission of ACS were also noted by diabetes status .  **Figure 1.** Age-adjusted rates of hospital admissions for acute coronary syndrome, by sex, 2000-2008. Data were obtained from the 3 general hospitals serving Kanawha County, West Virginia, and age-adjusted rates were calculated by using 2000 US Census data as a standard population.   **Figure 2.** Age-adjusted rates of hospital admissions for acute coronary syndrome, by sex and smoking status, 2000-2008. Data were obtained from the 3 general hospitals serving Kanawha County, West Virginia, and age-adjusted rates were calculated by using 2000 US Census data as a standard population.   **Figure 3.** Age-adjusted rates of hospital admissions for acute coronary syndrome, by sex and diabetes status. Data were obtained from the 3 general hospitals serving Kanawha County, West Virginia, and age-adjusted rates were calculated by using 2000 US Census data as a standard population.  A Poisson regression model for the main effect of variables of interest revealed that the incidence of hospital admissions for ACS increased significantly with age and decreased with calendar year (6% decrease per year; 95% confidence interval \\[CI\\], 4%-8%). Autumn and winter have significantly lower hospital admission rates for ACS compared with spring. The likelihood of hospital admissions for ACS was significantly lower among nonsmokers, people without diabetes, and women. We did not find additional significant change between, before, and after the removal of smoking areas in restaurants (the key change in the CIAR revision that took effect January 1, 2004) after accounting for the sustainable decline of ACS hospitalizations since the 2000 regulation revision . When we stratified the data by sex and smoking status, the results for the main model among nonsmokers were similar. Among female smokers, other than age and diabetes, we found no significant effect for any other factors. Among male smokers, there was a significant decline in time trend (7% decrease per year; 95% CI, 0.4%-12%) in hospital admission rates for ACS after the revised version of the 2003 CIAR (effective January 1, 2004), but no change over time before the revision . In addition, we performed similar analysis for AMI and found that results were similar (data not shown).  \n\n【14】Discussion\n----------\n\n【15】To our knowledge, this is the first study to describe a pattern of consistent decline in the rates of hospital admissions for ACS during an 8-year period  in a community with an existing CIAR. During this period, a significant drop in ACS hospital admission rates was observed for nonsmokers, people without diabetes, and women. However, this benefit did not reach significance for smokers. Additionally, effective January 1, 2004, when the Kanawha County regulation was further strengthened by making restaurants completely smoke-free (by removal of smoking areas in restaurants), male smokers were able to attain similar gains. Our findings indicate that by eliminating smoking areas at restaurants, bars, airports, and other public places and making them completely smoke-free, we may be able to decrease additionally the number of acute coronary events among male smokers. As expected, our study reveals that people who do not have diabetes benefit more from reduced ACS hospital admission rates regardless of sex and smoking status. We found no existing data describing the pattern of hospital admission rates for ACS in the presence of an existing CIAR over long periods. To date, only 2 of the studies have presented data on the smoking status of people affected by smoking prohibitions and have conducted analysis in nonsmokers . Although both studies found a significant decline in the number of admissions among nonsmoking patients after implementation of smoke-free legislation, only 1 also found fewer admissions for smokers . However, each study had limitations. Neither of the studies evaluated whether such prohibitions differentially affect men and women. The first study, by Seo and Torabi, had a very small sample size, lasted less than 2 years, included only nonsmoking patients in analysis, excluded many high-risk patients, and provided no information on age . The second study, by Pell et al, was also limited by less than 2 years of data analyzed, including only 10 months after implementation of the legislation in Scotland . We recognize that the risk of ACS falls rapidly after smoking cessation . In previous research, community-level studies have found that a reduction in secondhand smoke exposure after enactment of ordinances reduces hospital admissions for ACS from a range of 11% to 40% . A random-effects meta-analysis and a meta-regression of several studies to measure AMI reductions in affected communities found a reduction of approximately 15% in hospital ACS admission rates during the first year, and continuing exponential declines reaching approximately 36% in the 3 years following the implementation of strong smoke-free legislation . However, there is a paucity of data evaluating the long-term benefits of such legislation on ACS hospital admission rates. Although some recent data of a longer study from Canada demonstrated a consistent decrease in crude rates for hospital admissions due to various cardiovascular and respiratory conditions , the study did not delineate individual smoking status. Therefore, although the initial CIAR in Kanawha County was enacted in 1995, we conducted our analysis from 2000 through 2008 to evaluate whether we could identify any cardiovascular benefits in the community in terms of declines in hospital admission rates for ACS. Our results demonstrate a sustained decline in the hospital admission rates from ACS among nonsmokers in Kanawha County, West Virginia. Although this decline may not be conclusively linked directly to the CIAR, a significant benefit in the rate of ACS admissions for male smokers occurred after the regulations were strengthened in 2004 to remove all smoking areas from restaurants. Our study should be interpreted in light of several limitations. As with many of the past studies mentioned herein, our analysis is a retrospective study of hospital records; therefore, we did not have a closed study population. Our study population was theoretically free to travel from Kanawha County to other neighboring counties where a comprehensive CIAR was not in place. Despite some hospitals in our county being tertiary care centers and therefore serving residents of nearby counties, we restricted our data collection to residents of Kanawha County. Another major limitation of the study is the absence of a control population. Although several studies have used the same population but measured the effect before and after implementation of the ordinance, this was not possible in our case because our starting point of study was 5 years after the implementation of the CIAR. An external control population would have been ideal. However, no comparable population was available. Smoking status of participants was captured at the time of hospital admission on a voluntary, self-report basis. Although unvalidated statements of smoking status are problematic, the likelihood of being untruthful in disclosing smoking status when being admitted for ACS is very low . Other interventions may have affected the outcomes in our population and may serve as potential limitations of the study. First, we evaluated whether there were significant changes at local or state level in prevalence of diseases that may be considered risk factors for coronary artery disease (CAD) during this period. Specifically, we studied the Centers for Disease Control and Prevention’s Behavioral Risk Factor Surveillance System and Selected Metropolitan/Micropolitan Area Risk Trends (SMART) data to evaluate the prevalence trends for obesity, diabetes, hypercholesterolemia, hypertension, and physical activity . For each of the factors, the rates either increased or remained stable during the period of study. Additionally, a net increase in the prevalence of CAD rates was noted in the state of West Virginia. Second, smoking prevalence remained unchanged in the state from 2000 to 2008 (26% vs 27%). For Kanawha County, although smoking rates decreased from 2002 to 2008 (32% vs 24%), taking CIs into account, the change was not significant . Hence, any reduction in ACS admissions cannot be accounted for by a reduction in smoking. Third, our data did not differentiate between nonsmokers and former smokers who remained abstinent more than 1 year. We were also not able to account for those former smokers who may have switched to using smokeless tobacco. However, this is likely to be a small number of people. Fourth, in 2003, West Virginia increased its state tax on tobacco from 17 cents to 55 cents per pack. However, state tobacco tax revenue data indicate that the increase had no effect on sales , and considering the lack of significant difference in prevalence, we can dismiss the notion that changes in ACS can be attributed to a decline in smoking. West Virginia has no state law pertaining to clean indoor air, although local boards of health have the authority to enact smoking regulations. All 55 counties in the state have some form of smoking regulation. Only 18 counties (including Kanawha County) have a comprehensive regulation that includes all worksites. In our review, we found Kanawha County’s regulation to be most comprehensive and evenly enforced with high rates of compliance. We recommend that future studies measure baseline data on secondhand smoke exposure in the study population to evaluate the cause-effect relationship of lowering exposure to secondhand smoke in both the short term and the long term. Such studies should include both smokers and nonsmokers. We also recommend that such studies be followed by an analysis of potential cost savings resulting from long-term declines in rate of ACS-related hospital admissions. In conclusion, our results demonstrate that from 2000 through 2008, the rate of hospital admissions for ACS has consistently declined in Kanawha County in the presence of an existing CIAR. However, these beneficial effects have not been evenly distributed across all populations. Men and women who do not smoke and people who do not have diabetes derive the greatest benefits. Additional benefits for male smokers can be derived from further enhancing the regulations by removal of all smoking areas in restaurants.  \n\n【16】Tables\n------\n\n【17】#####  Table 1. Change in Rate of Acute Coronary Syndrome Hospital Admissions by Selected Variables, Kanawha County, West Virginia\n\n【18】VariableCoefficient (95% CI)Exponential of Coefficients (95% CI)P ValuePatient characteristicAge0.35 (0.33 to 0.37)1.41 (1.39 to 1.45)&lt;001Year of admission−0.06 (−0.08 to −0.04)0.94 (0.92 to 0.96)&lt;001Female−0.45 (−0.50 to 0.41)0.64 (0.60 to 0.66)&lt;001SeasonAutumn−0.10 (−0.16 to −0.03)0.90 (0.85 to 0.97).003Summer−0.04 (−0.11 to 0.02)0.96 (0.90 to 1.02).18Winter−0.07 (−0.13 to −0.002)0.93 (0.88 to 1.00).04Health statusNo tobacco use−0.25 (−0.31 to −0.20)0.78 (0.73 to 0.82)&lt;001No diabetes−1.66 (−1.70 to −1.61)0.19 (0.18 to 0.20)&lt;001Regulation change0.02 (−0.08 to 0.11)1.02 (0.92 to 1.12).12Abbreviation: CI, confidence interval. \n\n【19】#####  Table 2. Change in Rate of Acute Coronary Syndrome Hospital Admissions by Patient Characteristics, Season, Health Status, and Regulation Status, Based on Smoking Status and Sex, Kanawha County, West Virginia a\n\nVariableNonsmoking MenNonsmoking WomenCoefficients (95% CI)Exponential of coefficients (95% CI)P ValueCoefficients (95% CI)Exponential of Coefficients (95% CI)P ValuePatient characteristicAge0.42 (0.39 to 0.45)1.52 (1.48 to 1.57)&lt;0010.42 (0.39 to 0.45)1.52 (1.48 to 1.57)&lt;001Year−0.09 (−0.13 to 0.04)0.91 (0.88 to 0.96)&lt;001−0.07 (−0.12 to −0.02)0.93 (0.88 to 0.98).007SeasonAutumn−0.13 (−0.23 to 0.03)0.87 (0.79 to 0.97).01−0.08 (−0.20 to 0.04)0.92 (0.82 to 1.04).17Summer−0.07 (−0.17 to 0.03)0.93 (0.84 to 1.03).15−0.02 (−0.14 to 0.10)0.98 (0.87 to 1.11).76Winter−0.11 (−0.21 to 0.005)0.90 (0.81 to 1.00).04−0.05 (−0.16 to 0.07)0.95 (0.85 to 1.07).45No diabetes−1.65 (−1.72 to −1.57)0.19 (0.18 to 0.21)&lt;001−1.80 (−1.89 to -1.71)0.17 (0.15 to 0.18)&lt;001Regulation change b0.04 (−0.20 to 0.27)1.04 (0.82 to 1.31).750.05 (−0.23 to 0.32)1.05 (0.79 to 1.38).74Regulation-year0.01 (−0.05 to 0.07)1.01 (0.95 to 1.07).68−0.005 (−0.07 to 0.06)1.00 (0.93 to 1.06).89VariableSmoking MenSmoking WomenCoefficients (95% CI)Exponential of Coefficients (95% CI)P ValueCoefficients (95% CI)Exponential of Coefficients (95% CI)P ValuePatient characteristicAge−0.02 (−0.06 to 0.02)0.98 (0.94 to 1.02).310.12 (0.08 to 0.16)1.13 (1.08 to 1.17)&lt;001Year0.03 (−0.02 to 0.08)1.03 (0.98 to 1.08).220.03 (−0.03 to 0.10)1.03 (0.97 to 1.11).28SeasonAutumn−0.09 (−0.20 to 0.02)0.91 (0.82 to 1.02).12−0.03 (−.16 to 0.11)0.97 (0.85 to 1.12).71Summer−0.02 (−0.13 to 0.09)0.98 (0.88 to 1.09).68−0.05 (−0.18 to 0.08)0.95 (0.84 to 1.08).47Winter−0.04 (−0.16 to 0.06)0.96 (0.85 to 1.06).390.01 (−0.13 to 0.14)1.01 (0.88 to 1.15).89No diabetes−1.27 (−1.36 to −1.18)0.28 (0.26 to 0.31)&lt;001−1.68 (−1.79 to −1.58)0.19 (0.17 to 0.21)&lt;001Regulation change0.12 (−0.13 to 0.37)1.13 (0.88 to 1.45).330.02 (−0.29 to 0.33)1.02 (0.75 to 1.39).91Regulation-year b−0.07 (−0.13 to −0.004)0.93 (0.88 to 1.00).04−0.05 (−0.13 to 0.03)0.95 (0.88 to 1.03).18a In each Poisson model, we defined the variables as age (1: 18-49, 2: 50-59, 3: 60-69, 4: 70-79, and 5: ≥80 y), and the ordered code was treated as a continuous variable); year (1-9 for 2000-2008, treated as a continuous variable); season (spring, March-May; summer, June-August; fall, September-November; and winter, December-February, with spring as reference); no diabetes (compared with diabetes); and regulation (0: 2000-2003, 1: 2004-2008).b The relationship between regulation and year.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed August 24, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dbe9d357-b526-43bd-8ff1-53a0ae0be848", "title": "Parechovirus Genotype 3 Outbreak among Infants, New South Wales, Australia, 2013–2014", "text": "【0】Parechovirus Genotype 3 Outbreak among Infants, New South Wales, Australia, 2013–2014\nThe clinical manifestations of infection with human parechoviruses (HPeVs), members of the family _Picornaviridiae_ , are often indistinguishable from those caused by human enterovirus infections. Over the past decade, outbreaks of human parechovirus genotype 3 (HPeV3) have been reported from the Northern Hemisphere and are particularly well documented in Japan (where the virus was discovered), Canada, the United Kingdom, Denmark, and the Netherlands . Of the 16 HPeV genotypes, HPeV3 is the most aggressive and causes a sepsis-like syndrome in neonates . HPeV infection seems to follow a seasonal pattern; incidence is higher in summer and autumn . It can be spread by the fecal–oral and respiratory routes .\n\n【1】On November 22, 2013, Health Protection New South Wales (NSW), Australia, was notified of a possible cluster of HPeV cases at The Children’s Hospital at Westmead in Sydney. At that time, 7 neonates had experienced rapid onset of acute sepsis-like illness with fever >38°C and a combination of irritability/pain, diarrhea, confluent erythematous rash, tachycardia, tachypnea, encephalitis, myoclonic jerks, and hepatitis. Inquiries revealed that neonates described as “red, hot, angry” had also been admitted to other tertiary children’s hospitals in NSW . An expert advisory group comprising staff from the NSW Ministry of Health, Health Protection NSW, public health units, and the Sydney Children’s Hospital Network was convened to coordinate the investigation.\n\n【2】On November 25, 2013, PCR detection of HPeV RNA confirmed HPeV infection in 2 of the children. The NSW public health network and clinicians agreed that a surveillance program should be initiated to gather information on the epidemiologic and clinical characteristics and outcomes of children with HPeV infection.\n\n【3】In addition to the public health response, Health Protection NSW issued a media release to alert members of the public to the outbreak. On November 29, 2013, HPeV3 information including a case definition, instructions for accessing diagnostic testing, and recommended clinical management was distributed to all emergency departments, pediatricians, and early childhood health services in NSW. During the outbreak, the expert advisory group met regularly via teleconference to discuss and address any emerging issues. HPeV3 active surveillance activities were concluded on January 31, 2014, while other forms of surveillance continued into February 2014. We describe the epidemiology of the outbreak as observed through several surveillance mechanisms.\n\n【4】### Methods\n\n【5】HPeV infection is not a notifiable disease under the Public Health Act 2010 (NSW). This HPeV3 outbreak was detected and reported by clinicians alert to unusual clusters and patterns of disease. Other forms of surveillance were developed as a result of this alert. Surveillance consisted of 3 components: 1) active surveillance (case finding at the sentinel sites); 2) passive surveillance (laboratory reporting of all positive HPeV specimens from sentinel sites and elsewhere in NSW to Health Protection NSW); and 3) syndromic surveillance (reporting of infants seen in emergency departments by the NSW syndromic surveillance system that uses near real-time emergency department and ambulance data ) . The sentinel sites were 3 tertiary children’s hospitals in NSW: The Children’s Hospital at Westmead, The Sydney Children’s Hospital Randwick, and John Hunter Children’s Hospital Newcastle. Passive and syndromic surveillance continued into February 2014. In addition to surveillance, public health communication in the form of an HPeV information sheet for clinicians was distributed on November 29, 2013, alerting emergency department staff, pediatricians, and early childhood health service staff of current HPeV activity in NSW, providing a description of HPeV infection, and recommending management options (i.e. early laboratory testing and provision of supportive care after receipt of confirmation of HPeV infection).\n\n【6】##### Active Surveillance\n\n【7】Active surveillance activities commenced at the 3 hospitals (sentinel sites) on November 25, 2013, and continued through January 19, 2014; however, some retrospective case finding was included for cases with onset dating back to October 1, 2013, when the outbreak was thought to have started. A patient with a suspected case of HPeV was defined as a neonate or young infant <3 months of age with sepsis-like illness and fever >38.0°C and \\> 2 of the following: irritability/pain, rash, diarrhea, tachycardia, tachypnea, encephalitis, myoclonic jerks, or hepatitis. A patient with a confirmed case of HPeV was a suspected case-patient for whom PCR was positive for HPeV. Clinicians at the sentinel sites collected case information by using an HPeV case investigation form and PCR testing of patient stool, cerebrospinal fluid (CSF), nasopharyngeal aspirate, throat swab, rectal swab, or whole blood samples for HPeV; stool and CSF samples were preferred. These data were entered into the NSW Notifiable Conditions Information Management System. Weekly reports informed the NSW public health network of outbreak progression.\n\n【8】##### Passive Surveillance\n\n【9】All positive HPeV test results from NSW residents referred to the Victorian Infectious Diseases Reference Laboratory (VIDRL) in Melbourne, Victoria (the only laboratory in the region testing for HPeV), from October 1, 2013, through February 2, 2014, were reported to NSW Health. Specimen date; estimated illness onset date; sample type; and patient date of birth, sex, and postcode were recorded in the NSW Notifiable Conditions Information Management System.\n\n【10】##### Syndromic Surveillance\n\n【11】The NSW emergency department syndromic surveillance system monitored the number of infants <1 year of age for whom a provisional diagnosis of fever/unspecified infection was made in the emergency department and the number of patients who required hospital admission, including admission to critical care wards. A diagnosis of fever/unspecified infection can include fever symptoms, unspecified viral infection, unspecified viremia, unspecified bacteremia, unspecified bacterial infection, or unspecified infection. Weekly reports compared recent data with historical data from the previous 5 years.\n\n【12】##### Laboratory Methods\n\n【13】From all clinical samples, nucleic acid was extracted by using QIAGEN DX reagents (QIAGEN, Hilden, Germany) on a QIAxtractor NA extraction robot (QIAGEN). cDNA was synthesized by using a method previously described  and was tested in an HPeV real-time PCR selective for the 5′ untranslated region, which was developed at VIDRL . (The primer and probe sequence details for this assay can be supplied upon request to G.C.)\n\n【14】Molecular analysis to obtain the HPeV genotype was performed on selected samples that had been positive by real-time PCR. Specimens from 41 patients were selected for genotyping on the basis of ensuring representation of infants’ geographic locations, ages, sex, illness onset dates, specimen types, and sex. Identification of specific HPeV genotypes was obtained through amplification of the viral protein 1 gene by use of a gel-based seminested PCR . The generated PCR products were sequenced and compared with reference sequences by using the primers and methods described elsewhere .\n\n【15】##### Statistical Analyses\n\n【16】Descriptive analysis of epidemiologic variables and patient demographic characteristics were performed. Characteristics of infants <3 months of age, such as length of stay (LOS), were compared by using _t_ \\-tests to determine the effects of public health messaging. Analyses was performed by using SAS version 9.2 (SAS Institute, Inc. Cary, NC, USA) and Microsoft Excel (Microsoft, Redmond, WA, USA).\n\n【17】### Results\n\n【18】##### Laboratory Findings\n\n【19】From November 1, 2013, through February 28, 2014, a total of 420 specimens were submitted for HPeV PCR testing; for some patients, >1 specimen was submitted. PCR results were HPeV positive for 289 (69%) specimens from 198 patients . In addition to confirming HPeV RNA in samples from 198 patients in NSW, HPeV type 3 was identified from all 41 (21%) positive samples for which molecular analysis was subsequently performed. The phylogenetic tree demonstrating all HPeV3 isolates genotyped at VIDRL during the outbreak is reported elsewhere .\n\n【20】Because of the algorithms used in the testing, enterovirus results were also available for all samples submitted . A total of 194 patients had HPeV infection only, 4 had dual infections (HPeV and enterovirus), and 14 had enterovirus infection only . Results for the rest of the patients were negative. Focusing on CSF and fecal samples, 123 (73%) of 168 CSF samples were HPeV positive by PCR (mean cycle threshold \\[C t  \\] detection value 31.6), and 114 (73%) of 156 fecal samples were positive (mean C t  27.2) . PCR was run for 45 cycles; therefore, C t  values >45 were considered negative.\n\n【21】##### Active and Passive Surveillance Findings\n\n【22】Active surveillance identified 94 infants whose illness met the definition of a confirmed case (patient <3 months of age and HPeV-positive laboratory results, originating from sentinel sites). Passive surveillance spanning specimen collection dates from October 1, 2013, through February 2, 2014, identified another 89 laboratory-confirmed cases in NSW in patients 0–17 months of age. The outbreak peaked during the first 2 weeks of December 2013 .\n\n【23】More cases (105 \\[57%\\]) were in male than female patients; median patient age was 1.51 months (or median 46 days, range 0–537 days) . Intrafamily HPeV3 transmission was identified in twins, 2 parent–child pairs, and a set of cousins. A descriptive case series of the infants infected with HPeV3 during this outbreak, containing further clinical details on select cases, is reported elsewhere .\n\n【24】Analysis of case investigation forms from the sentinel sites reporting HPeV signs and symptoms showed that the most commonly reported signs for infants <3 months of age were fever (86%), irritability/pain (80%), tachycardia (68%), and rash (62%) . Similar signs were displayed by those \\> 3 months of age; however, 20% fewer in this age group had fever and tachycardia . As described previously, all infants at the sentinel sites were well at the time of hospital discharge, and further longitudinal follow-up studies are examining the long-term outcomes of these infections having occurred in early life .\n\n【25】Of the 183 confirmed cases, 108 (59%) were captured by the 3 sentinel surveillance sites, and another 75 (41%) were diagnosed at other hospitals. Most (57%) patients resided in the Sydney metropolitan area, and the remaining 43% were from regional or rural areas of NSW. This finding compares with 64% and 36% of the NSW population residing in metropolitan Sydney and regional/rural areas, respectively .\n\n【26】Analysis of case investigation forms for the 108 patients at the sentinel sites also showed that 103 (95%) patients were admitted to hospital and had an average LOS of 4.4  days . Mean LOS was greater for infants <3 months of age (4.5, range 1–13 days) than for those \\> 3 months of age (3.7, range 1–11 days). The rate of admission to an intensive care unit was lower for older infants (14%) than for those <3 months of age (30%) .\n\n【27】The trend statistic on the distribution of LOS in infants <3 months of age showed that LOS was significantly reduced among infants <3 months of age who became ill after the HPeV alert was sent on November 29, 2013, to emergency departments and pediatricians. Mean LOS at the sentinel sites was 5.7 days before and 4.0 days after sending of the alert (Satterthwaite _t_ \\-test, p<0.05) .\n\n【28】##### Syndromic Surveillance Findings\n\n【29】The number of infants <1 year of age with a provisional emergency department diagnosis of fever/unspecified requiring hospital admission began to rise sharply in mid-November 2013 and peaked during the first week of December . At the peak, the number of admissions was 83, compared with an average of 52 for the same week in previous years. Admissions remained elevated until mid-January 2014, when they returned to background levels. Admissions to critical care wards spiked during the second week of December, when 9 patients were admitted, well above the average of 1 admission per week for the same week in previous years . During the surveillance period, most admissions were to the 2 children’s hospitals in metropolitan Sydney.\n\n【30】### Discussion\n\n【31】This outbreak is probably one of the first large parechovirus outbreaks to be reported in Australia. We observed a large number of cases over a relatively short period (≈4 months), peaking around late spring/early summer, which is earlier than documented seasonality for parechovirus in the Northern Hemisphere . Although sequencing in this series was incomplete (21% of patients), this parechovirus outbreak was determined to have been caused by HPeV3 for the following reasons: all sequenced HPeV-positive samples were genotype 3; the epidemiology and spectrum of illness seen by clinicians at the sentinel sites was relatively homogenous and in keeping with other reports of HPeV3 infections in infants ; and PCR was enterovirus positive for only 18 patients but HPeV positive for 198 patients.\n\n【32】Infected infants in this outbreak were older than those reported elsewhere. The median age of 46 days was higher than that reported in Denmark (39 days) and the Netherlands (40 days) . The occurrence of a substantial proportion (17%) of HPeV3 infections in infants \\> 3 months of age was consistent with results from a US study reporting 18% of cases in infants >60 days of age but contrary to other data indicating that HPeV3 infection occurs almost exclusively in infants <3 months of age .\n\n【33】The age cutoff in the case definition for active surveillance at the sentinel sites may have introduced a bias toward HPeV infection being more frequently suspected in infants <3 months of age; thus, these infants might have undergone more HPeV testing than older infants, thereby underestimating the true mean age of infants affected in this outbreak. Had the age cutoff in the case definition been set at 12 months (all infants), bias toward younger infants would have been avoided, and the median age might have been older than that reported here.\n\n【34】After the November 29, 2013, release of the HPeV alert to emergency department staff and pediatricians, mean LOS among infants <3 months of age decreased by 30%, from ≈5.7 days at the start of the outbreak to 4.0 days (p = 0.0250). This statistically significant finding reflects a degree of effectiveness of public health messaging in raising clinician awareness of HPeV. During the latter part of the outbreak, after Health Protection NSW issued the alert, informed clinicians may have felt more comfortable discontinuing treatment and discharging infants sooner if their illness met the criteria of the HPeV case definition. The alert seemed to result in improved clinical management of cases, avoidance of unnecessary prolonged exposure to empirically prescribed antimicrobial drugs, provision of appropriate supportive treatment, and earlier discharge from hospital. The mean LOS during the NSW outbreak (4.4 days) was lower than that reported in the Netherlands (7 days), according to an analysis of retrospective diagnoses when no public health intervention would have taken place . Another factor that could contribute to reduced LOS in the latter part of an outbreak is the increasing age of young infants becoming ill with HPeV infection in the second half of the outbreak, although this factor was not statistically significant in this dataset .\n\n【35】The male:female split in this outbreak was less pronounced than that reported for other studies, although more cases consistently occurred in boys. Others have reported a higher preponderance of infection in boys, ranging from 70% to 90%, compared with our finding of 57% .\n\n【36】Clinical signs reported in the literature for HPeV3-infected infants were consistent with our findings of fever, irritability, and encephalitis . However, rash occurred with much higher frequency (62% vs. 17%) in the NSW outbreak than in other outbreaks .\n\n【37】Syndromic surveillance that used emergency department data proved to be a useful and timely way to monitor emergency department hospital admissions temporally associated with the HPeV3 outbreak. Increased presentation of infants <1 year of age with a provisional emergency department diagnosis of fever/unspecified infection requiring hospital admission, in particular admission to critical care, were associated with increased detection of HPeV3 at the clinical level. Emergency department syndromic surveillance reports also helped confirm an overall decline in admissions from emergency departments in early 2014, supporting the eventual withdrawal of active surveillance. Emergency department syndromic surveillance in NSW does not routinely monitor age-specific or admission-specific (hospital ward or critical care unit) aberrations; that is, all children <5 years of age are monitored as a group. In the future, data generated through the emergency department syndromic surveillance system may continue to be useful for monitoring the evolution of an HPeV or similar outbreak. The cost of maintaining emergency department syndromic surveillance like that used during this outbreak (fever/unspecified infection among children <1 year of age and admission to hospital) would need to be considered. Costs of doing so include personnel time for checking reports and investigating signals, infrastructure costs, and opportunity costs; choosing to monitor HPeV3-related signals indirectly means that signals for diseases that are not prioritized are not monitored. In addition, the sensitivity of this grouping will need to be tested in future outbreaks before it can be considered a reliable proxy indicator of a seasonal outbreak\n\n【38】We initiated sentinel surveillance on the assumption that nearly all neonates with severe HPeV3 disease would be referred to 1 of the 3 tertiary children’s hospitals in NSW. Through passive surveillance we identified an additional 41% of HPeV patients who had been seen at other health facilities. Enhanced data were not collected for these presumably milder cases, which has limited our capacity to conduct more extensive significance testing across the observed differences in clinical features. Recording more information on potential exposures (e.g. infants’ daycare attendance, existence of older siblings, and occurrence of family illness in weeks preceding infants’ admission) would have further aided our understanding of HPeV3 transmission in the community.\n\n【39】### Conclusion\n\n【40】The objectives of HPeV surveillance were achieved: document the outbreak, describe the clinical features of cases, help inform clinicians and the public, monitor the evolution of the outbreak, and add to the knowledge base. The HPeV3 infection outbreak in NSW, Australia, differed slightly from that documented in the Northern Hemisphere; the NSW outbreak apparently affected slightly older infants (as well as neonates and young infants), cases were more evenly split between boys and girls, and rash occurred at a considerably higher frequency. The value of awareness-raising communication strategies was demonstrated by the statistically significant 30% reduction in LOS during the outbreak immediately after release of the alert to emergency department staff and pediatricians. This alert helped to minimize unneeded use of antimicrobial drugs and reduce unnecessary hospitalization. Although active surveillance is resource intensive, it has helped to define HPeV3 infection in NSW and link it with a syndromic surveillance indicator in the emergency department syndromic surveillance system. Syndromic surveillance is a potentially useful proxy indicator that should be considered for future detection and surveillance of seasonal outbreaks of viral infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7864c4b4-0c0f-4dfc-b856-d22874895cae", "title": "Plagues, Public Health, and Politics", "text": "【0】Plagues, Public Health, and Politics\nOn July 1, 1665, the lord mayor and aldermen of the city of London put into place a set of orders “concerning the infection of the plague,” which was then sweeping through the population. He intended that these actions would be “very expedient for preventing and avoiding of infection of sickness” .\n\n【1】At that time, London faced a public health crisis, with an inadequate scientific base in that the role of rats and their fleas in disease transmission was unknown. Nonetheless, this crisis was faced with good intentions by the top medical and political figures of the community.\n\n【2】Daniel Defoe made an observation that could apply to many public health interventions then and today, “This shutting up of houses was at first counted a very cruel and unchristian method… but it was a public good that justified a private mischief” . Then, just as today, a complex relationship existed between the science of public health and the practice of public health and politics. We address the relationship between science, public health, and politics, with a particular emphasis on infectious diseases.\n\n【3】Science, public health, and politics are not only compatible, but in conjunction, all three are necessary to improve the public’s health. The progress of each area of public health is related to the strength of the other areas. The effect of politics in public health becomes dangerous when policy is dictated by ideology. Policy is also threatened when it is solely determined by science alone, devoid of considerations of social condition, culture, economics, and public will.\n\n【4】When using the word “politics,” we refer not simply to partisan politics but to the broader set of policies and systems. Although ideology is used in many different ways, in this case, it refers to individual systems of belief that may color a person’s attitudes and actions and that are not necessarily based on scientific evidence .\n\n【5】### Public Health Achievements\n\n【6】Science influences public health decisions and conclusions, and politics delivers its programs and messages. This pattern is obvious in many of public health’s greatest triumphs of the 20th century, 10 of which were chronicled in 1999 by the Centers for Disease Control and Prevention (CDC) as great public health achievements, and several of which are presented below as examples of ideology affecting successes . These achievements remind us of what can be accomplished when innovation, persistence, and luck converge, along with political will and public policy.\n\n【7】### Vaccination\n\n【8】Childhood vaccinations have largely eliminated once-common, terrible diseases, such as polio, diphtheria, measles, mumps, and pertussis . Polio is being eradicated worldwide. The current collaboration between the World Health Organization, the United Nations Children’s Fund, CDC, and Rotary International is a political as well as biological “tour de force,” and eradication of polio in Nigeria continues to be threatened by local political struggles and decisions. In the United States, politics has contributed to successful public health policies by requiring vaccination at school entry, which has been vital to achieving high vaccine coverage in young children.\n\n【9】Debate about vaccines offers an example of the effect of ideology on public health progress in the form of persons who oppose vaccination. These persons put communities at risk by refusing vaccination for themselves and their children and enlist political support to undermine our greatest medical advance.\n\n【10】### Family Planning\n\n【11】Safe contraception and family planning have not only improved the health of women by preventing unintended pregnancies, but they have also contributed to one of the century’s most dramatic social revolutions by helping redefine roles and opportunities for women . However, ideologic views on contraceptive practices and sexually transmitted disease (STD) prevention continue to contradict scientific observations, which leads to compromised public health policies.\n\n【12】### Control of Infectious Diseases\n\n【13】Clean water, treated to protect us from outbreaks of infections like cryptosporidiosis, is an obvious example of the interaction between public policy and infectious disease control. Public policy has sought to control infectious disease throughout history, including attempts to ban spitting in the streets around the turn of the century (an issue that resurfaced 100 years later in the context of severe acute respiratory syndrome \\[SARS\\]) and imposing restaurant inspections to ensure sanitary conditions in food preparation. Many important infectious disease issues have political and economic overtones: legionnaires disease and hotel closures, Nipah virus outbreaks and the swine industry, hantavirus and the cultural and political interplay with Native American communities, and drug resistance and inappropriate and widespread antimicrobial drug use in the food industry and medicine are just a few examples .\n\n【14】### Recognizing Tobacco Use as a Health Hazard\n\n【15】Knowing that tobacco is addictive and dangerous alone did not ensure that tobacco companies were held responsible for their role in impairing many people’s health. Rather, that accomplishment required a combination of political will and social insistence . Nonetheless, regulations on secondhand smoke continue to be debated, as science and individual ideology clash. These clashes become especially acrimonious as they reflect culture around a native-grown substance and often the product on which a state’s economy has depended.\n\n【16】### New Challenges\n\n【17】In just the past 2 years, new health challenges have occurred that illustrate the tension between economic health of a community or a business and the personal health of citizens or employees, for example, or the role of the individual versus the government in taking responsibility for health and health-related actions. In emerging infectious diseases, these new health challenges include avian flu and bovine spongiform encephalopathy, as well as SARS.\n\n【18】What makes infectious diseases particularly compelling to the public, to public health and political involvement, is that microbial agents are frightening. They come from exotic places, jump from person to person, often have no treatments or preventive measures available, and can paralyze industries and communities. Infectious agents represent our lack of control over our health, regardless of whether they are used deliberately by terrorists or are delivered by nature. Many infectious diseases have become a security issue, bringing a new set of “partners” to the microbiologic and public health table. While this arrangement is appropriate and necessary in many instances, it also has potential for abuse, by promoting anxiety and insecurity for political means, distorting public health priorities, and possibly militarizing public health institutions.\n\n【19】### Smallpox\n\n【20】The decision to implement widespread vaccination against smallpox generated substantial interest in the general public. After believing that smallpox was not a threat for many years, we were informed by the government that cause for serious alarm existed.\n\n【21】Production of large quantities of vaccine was accelerated, which was a prudent and decisive action. This action was followed by a policy that called for vaccinations for hundreds of thousands of healthcare workers and millions of first responders. The science on which this decision was based seemed shaky at best, and many chose to forego vaccination, including two distinguished academic infectious disease units. The Washington Post criticized these units, saying, “There are reasons, moral and medical, to deplore the decision of those doctors who refuse in this manner…. Their job is not to assess intelligence risks or to second-guess state public health officials but to be prepared to care for sick people, and to vaccinate healthy people” .\n\n【22】The Post’s statement may be correct, but academic infectious disease specialists have every right and responsibility to question decision-making that affects their patients and colleagues, especially when the scientific-political interface regarding that decision is unclear. Careful review of the literature and expert experience predicted substantial risks from adverse vaccination reactions.\n\n【23】The Washington Post editors seem to have missed the concept of “do no harm.” Analytic and compassionate physicians realized that, in the face of little or no threat of an attack, widespread use of a potentially toxic vaccine was not in the best interest of their patients. The decision by various academic medical centers not to widely vaccinate hospital and medical personnel seems prudent, given the revised estimates of risk and the reporting of substantial adverse reactions.\n\n【24】Bioterrorism is not the only infectious disease challenge with political implications. Existing pathogens and newly emerging diseases remind us that infectious agents can destabilize our social structure and commerce, and they may require political or policy intervention. Therefore, the danger is that ideologic stances may intrude on the process and push us away from science and even away from good public health practice.\n\n【25】### SARS\n\n【26】The SARS outbreak in Asia in 2003 provided examples of how ideology and politics can interfere with public health practices and bring criticism by ideologues. Moreover, SARS demonstrated the challenge of protecting the public’s health across national and ideologic lines. The SARS outbreak was not reported by the Chinese government for the first several months of its transmission . An ideologic perspective that required not sharing weaknesses or inadequacies with the rest of the world probably played a role in this delay. The political pressure of the rest of the world was required to convince China to acknowledge the problem and accept help.\n\n【27】Hong Kong, on the other hand, was more open. Early cases of atypical pneumonia were identified and reported, further cases were ascertained, and contact tracing was put in place. The system responded with infection control efforts, including isolation and quarantine. Nonetheless, Hong Kong faced a daunting task, with a high population density and a poorly understood disease. In the end, Hong Kong’s department of health faced substantial criticism from political opposition and the press, and a committee was formed to evaluate their response. The committee identified a number of recommendations but recognized overall the impressive response of the hardworking public health and healthcare communities . Nonetheless, persons initially critical of the response itself took the opportunity to criticize the report by an international panel. Certainly, being critical and trying to improve performance are valuable, but are they best done in the middle of the challenge and with blatant political intent?\n\n【28】### 2003–2004 Flu Season\n\n【29】For influenza, the scientific and political processes need to be improved. For many years in public health, we have recognized the threat of pandemic flu and called for the need to act . In this case, politics is more than helpful, it is essential. Preventing a flu pandemic necessitates using the resources of science, politics, and the private sector. This year, vaccine development became a matter of public concern when several children died from influenza early in the season, and the press reported that the vaccine may have lacked protection against the circulating Fujian strain.\n\n【30】Public discussions highlighted the imperfections of science, particularly related to vaccine production and distribution. Then the finding of cases of H5N1 influenza in Asian chicken flocks and other birds and several human infections and deaths rekindled apprehension about a flu pandemic with a new, lethal strain, should mutations permit person-to-person transmission. With avian flu, some government officials were slow to disclose infected flocks to protect economic interests, and these decisions could have had tremendous potential health effects around the world. Thus politics continues to influence infectious disease control on micro and macro levels.\n\n【31】### Ideology and Science\n\n【32】Early in the HIV/AIDS epidemic, the ideologies of scientists, clinicians, and politicians worked against one another as they affected decisions about paying attention to a new and emerging disease. These decisions and the ideology inherent in them were intertwined with beliefs about sexuality and sexual health. The challenge continues today as ideologic and political entities criticize the National Institutes of Health for research funding decisions, not on the basis of scientific merit, but because these groups and persons find research about commercial sex workers, truck drivers, and sexually transmitted diseases to be inappropriate as public health research topics . In this example, ideology pushes political action to question science and compromise public health.\n\n【33】In each of the cases so far described, both politics and ideology have come into play, and when ideology clouds scientific and public health judgment, decisions go awry and politics become dangerous. Having an ideology or even shouting it from rooftops is perfectly appropriate. One of the fundamental freedoms in our country is the right to believe what we want and express it. But when an person’s beliefs bring about public policies that hurt people, they should be held accountable. Condoms and abstinence are well-established, effective means of birth control and STD prevention . Both have flaws in practical application. Both can be tools in our pursuit of improved health. The denigration of either practice suggests a preference for ideology over science.\n\n【34】Scientists and public health professionals often offer opinions on policy and political issues, and politicians offer theirs on public health policies, sometimes with the support of evidence. This interaction is appropriate and healthy, and valuable insights can be acquired by these cross-discussions. Nevertheless the interaction provides an opportunity for inappropriate and self-serving commentary, for public grandstanding, and for promoting public anxiety for partisan political purposes. Public health professionals should work with politicians to resist ideologic influence, to demand good science, and to make wise decisions and policies.\n\n【35】### Conclusion\n\n【36】For scientists focused exclusively on winning at “NIH bingo,” accumulating R01s, KO1s, K15s, RO3s, R13s, and R21s, the interplay between science and politics may be irrelevant. However, most public health scientists and practitioners want to see their efforts improve the public’s health. At the same time, scientists require an environment that permits them to work as efficiently and objectively as possible.\n\n【37】The issue can be succinctly addressed with a simple diagram . On the left is science, essential to inform the practice of public health. In the middle is public health, where science is interpreted and appropriate responses are developed. And on the right is political will and policies necessary to carry out the public health impetus. The tendency is to struggle against the intrusion of politics when it is counter to our own opinion, ignores or misinterprets the science, or is driven by ideology beyond politics as usual. We are right to raise our voices against the intrusion of politics into public health in the second and third circumstances, but should take care in the first one.\n\n【38】The diagram has a clear direction of flow. Science informs public health, which leads to political change. This approach is appropriate and effective to improve health, but the process should only flow in one direction. Reversing directions in public health decision-making is just as hazardous as it is in sewage lines. Even more insidious can be the intrusion of ideology into the process, attempting to reverse the current of the science, public health, politics stream. We have seen cases where ideology or political considerations determine a desirable policy and then seek scientific justification for it, often employing faulty science. When this happens, ideology can diminish the field, discredit the discipline and its practitioners, and undermine what scientists do.\n\n【39】How should infectious disease scientists handle political and ideologic pressures in their own work? One way to handle these pressures is to be connected to the rest of the public health community. Every area of public health faces the same issues: a similar commentary would apply to chronic disease or environmental health. Science and politics are intertwined in myriad ways, and ideologic influences are encountered everywhere. Tremendous concern exists in the United States about infectious diseases. Infectious diseases research no doubt gained the spotlight, and accompanying resources, after the events of September 11, 2001, and the anthrax attacks later that year. But political winds change quickly, and this focus could easily shift.\n\n【40】The infectious disease community needs to see their role within the larger public health context and work actively to forge alliances and collaborations between their work and the work of others. The diagram can continue to flow in the right direction, science to public health to policy, but maintaining this direction requires work, which can be accomplished by recognizing interconnectedness and using the political system to improve public health through good science. Several concrete ways to accomplish the goal exist: 1) Be an advocate for infectious disease control, not just emerging infectious diseases or bioterrorism. 2) Be an advocate for public health, not just infectious diseases. 3) Be an advocate for wise public policy based on science in the context of broader societal considerations. 4) Respect the value of the interplay of science, public health, and politics, but recognize any reversal of flow and resist it when it occurs. We all need to be strong advocates for good science, good public health, and good policies and the positive value that politics can provide for all three of these.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "12fd782b-37b4-4578-9a1c-17d71af2c744", "title": "Respiratory Infections during SARS Outbreak, Hong Kong, 2003", "text": "【0】Respiratory Infections during SARS Outbreak, Hong Kong, 2003\nSevere acute respiratory syndrome (SARS) is an infection caused by a novel coronavirus that is transmitted primarily through direct mucous membrane contact with infectious respiratory droplets and through exposure to fomites. In 2003, Hong Kong reported SARS cases from March 11 to June 2. During the height of the outbreak, schools were suspended, social activities were curtailed with the closure of various public places, and the community was engaged in a sustained and intense hygiene campaign . Population education on personal hygienic measures was spearheaded by the government with concerted efforts from various organizations and the community. Surveys conducted in April and May 2003 showed that most of the population wore a face mask (76%), washed their hands after contact with potentially contaminated objects (65%), used soap when washing hands (75%), covered their mouths when sneezing or coughing (78%), and used diluted bleach for household cleaning (>50%) . Another survey on health-seeking behavioral traits conducted in June 2003 showed that >70% of respondents practiced some of these hygienic measures more frequently during the SARS outbreak than during the pre-SARS period .\n\n【1】### The Study\n\n【2】We postulated that these populationwide anti-SARS measures would have effects on other infections spread by the respiratory route. In this study, we examined whether such measures also affected the incidence of some common acute viral respiratory infections.\n\n【3】The study period was January 1, 1998, to December 31, 2003. Data were obtained from the Government Virus Unit (GVU), a public health and diagnostic virology laboratory serving public and private hospitals and outpatient clinics in Hong Kong. At GVU, all respiratory specimens are routinely cultured using 4 continuous cell lines, Rhesus monkey kidney, Madin-Darby canine kidney, rhabdomyosarcoma, and human laryngeal epithelium, which could support the growth of various viruses including influenza, parainfluenza, respiratory syncytial virus (RSV), and adenovirus. On detection of specific cytopathic effects, the viruses are identified with standard protocols .\n\n【4】For each month of the study period, we obtained the number of respiratory virus isolates as a proportion of the total number of respiratory specimens processed by GVU. We computed the percentage change in the proportion of positive specimens (PPS) for each virus between each month of 2003 and the mean PPS in the same month of the preceding 5 years , which served as the reference period. For comparison purposes, we obtained the monthly number of positive tests for immunoglobulin M (IgM) antibody against hepatitis B core antigen (anti-HBc) and the corresponding total number of tests performed in the study period. The percentage change in PPS was calculated as above. Although a positive IgM anti-HBc test result indicates acute hepatitis B infection or an exacerbation of chronic hepatitis B infection, as we were testing the same catchment of population throughout 1998 to 2003, the proportion of exacerbations of chronic hepatitis B infection is assumed to have remained unchanged during the study period.\n\n【5】The Table shows the change in PPS in 2003 for the various viruses in comparison with the reference period. In 2003, the monthly number of respiratory specimens ranged from 665 to 5,432 (mean 1,399), in comparison with a range throughout the years 1998–2002 of 757 to 3,162 (mean 1,334.5). A surge in the number of specimens was noted during March and April 2003 (5,432 and 3,758, respectively). During March to July 2003, marked reductions in PPS occurred compared with the reference period for influenza virus, parainfluenza virus, RSV, and adenovirus, particularly in the months of April, May, and June. This reduction corresponded to the period when anti-SARS measures in the community were most rigorous. In contrast, similar changes in PPS were not observed for hepatitis B, which is caused by a bloodborne virus with a different mode of transmission than that of the 4 respiratory viruses . Since August 2003, instead of reductions in PPS, a rebound in isolation rates was observed for the 4 viruses.\n\n【6】The 2003 SARS outbreak overlapped with the traditional seasonal peak from March to September for RSV in Hong Kong . In 2003, the RSV peak season shifted to August–October. The accumulation of susceptible infants offset the infection control measures instituted against respiratory infections as well as the normal seasonality; as a result, RSV activity increased in the late months of 2003. Figure 2 illustrates the usual seasonal variation of the 4 respiratory viruses and their pattern from 1998 to 2003.\n\n【7】Discerning whether the observed effects in our study were real or apparent is important. The surge in specimens during March and April 2003 suggested that physicians were more inclined to order tests for patients with respiratory symptoms at the height of the SARS outbreak. This fact could conceivably dilute PPS for respiratory viruses. However, since May 2003, the number of tests has returned to normal levels; however, PPS remained significantly decreased during May to July 2003. Thus, PPS reductions cannot be explained by a dilution effect caused by an increased number of specimens processed. Furthermore, after we controlled for the patients' age group differences, PPS for influenza virus remained depressed when compared to PPS in the reference period (data not shown). The same pattern was true for adenovirus.\n\n【8】Population coverage for influenza vaccination in Hong Kong has been <15% throughout the study period , so vaccination was unlikely to have resulted in reduced influenza circulation in the community. The concomitant significant reduction in PPS for all 4 respiratory viruses in the same period argues against 2003's being a milder year for influenza. Temporally, the moderation of PPS reductions since August 2003 (the last SARS case was reported on June 2) supported the hypothesis that the effects of populationwide anti-SARS measures on the incidence of respiratory viruses were real.\n\n【9】With the recent outbreaks of highly pathogenic avian influenza among poultry in Asian countries, and the associated human infections, pandemic planning for influenza has been undertaken with renewed efforts on a worldwide basis . In pandemic preparedness planning, control measures have traditionally focused on the use of antiviral chemotherapy and the expedient development of an effective vaccine. However, such strategies may not be feasible, especially in countries with limited resources. An effective vaccine would probably become available only during the latter phase of the pandemic. Information concerning the effects of increased social distance and communitywide hygiene measures on the incidence of common viral respiratory infections at a population level has been lacking. The SARS outbreak offers a unique opportunity to study the association. Although our study was observational and thus could not establish a causal relationship, it suggests a possible association between population-based hygienic measures and the reduced incidence of influenza and other acute viral respiratory infections. However, the relative contribution of each of these measures could not be estimated in our study. The effective implementation of such measures requires determined and sustained educational efforts from health authorities with collaboration of the public. We thus propose that stockpiling personal protective equipment and having public education campaigns on infection control practices should form an integral component in pandemic planning for the emergence of novel influenza virus strains in humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "016ee5e8-4e31-4eb8-9b89-8147955dcb8e", "title": "Development of a Community-Based Participatory Colorectal Cancer Screening Intervention to Address Disparities, Arkansas, 2008-2009", "text": "【0】Development of a Community-Based Participatory Colorectal Cancer Screening Intervention to Address Disparities, Arkansas, 2008-2009\nBackground\n----------\n\n【1】The death rate from colorectal cancer in the United States is high (16.7/100,000)  and affects medically underserved populations disproportionately . Health disparities are particularly severe in the Lower Mississippi River Delta region. The region is predominately rural and has limited numbers of health care providers and facilities, low rates of health insurance coverage, low levels of educational attainment, and high rates of poverty . Because of this limited access to basic health care resources, disease management is given priority over preventive health care . Thus, many in the region are not screened for cancer, even though screening is one of the most effective strategies for preventing colorectal cancer . By focusing on collaboration with communities disproportionately affected by disease to improve health, community-based participatory research (CBPR) is a promising approach to prevent colorectal cancer in underserved populations . Several CBPR studies have successfully increased screening for breast and cervical cancer ; however, few have targeted colorectal cancer in underserved populations . The few colorectal cancer screening interventions primarily have focused on client reminders , which exclude people who are unable to access the health care system. Empowering Communities for Life (ECL) uses a CBPR approach to increase colorectal cancer screening rates among rural, underserved populations in 2 Lower Mississippi River Delta counties by increasing the use of fecal occult blood testing (FOBT), a low-cost way to screen for colorectal cancer. The goal of the CBPR process used in ECL was to build infrastructure to conduct translational research, design materials and methods salient to the community, recruit and train lay health advisors and role models, and develop an assessment instrument. In this article, we describe the development of the CBPR partnership, development of ECL, and lessons learned by using a CBPR approach.  \n\n【2】Community Context\n-----------------\n\n【3】The community context for ECL is 2 Arkansas counties in the Lower Mississippi River Delta region, Mississippi and St. Francis. Both counties are designated as medically underserved and health professional shortage areas, and access to health care resources is further complicated by the counties and states decentralized and limited rural transportation system . Mississippi County is a predominately agricultural community ; approximately 35% of the countys total population is considered rural . The population of Mississippi County is 46,741; of this number 36% are minorities (some race other than white), and a high percentage is low-income (27% below poverty level vs 14% nationally) who either have no health insurance or are underinsured . Approximately 26% of residents have less than a high school education (vs 16% nationally) . Representing the county is Mississippi County Arkansas Economic Opportunity Commission, Inc (MCAEOC), a nonprofit organization committed to enabling low-income residents of Mississippi County to become self-sufficient. Approximately half of the population in St. Francis County is rural . Also a predominately agricultural community , St. Francis County has a population of 26,783; of this number 54% are minorities, and a high proportion are low-income (32% below poverty level vs 14% nationally) who either have no health insurance or are underinsured . Approximately 26% of residents have less than a high school education (vs 16% nationally) . Representing the county is East Arkansas Enterprise Community (EAEC), a nonprofit rural development program that assists communities in St. Francis County through financial and technical support. Both Mississippi and St. Francis counties have striking racial disparities in colorectal cancer deaths; African Americans in Mississippi (43.7 per 100,000 population per year from 1997 to 2007) and St. Francis counties (37.3 per 100,000) have higher age-adjusted colorectal cancer death rates than do whites in Mississippi (22.1 per 100,000) and St. Francis counties (26.1 per 100,000) . ECL arose from a 9-year partnership starting in 2001 between the University of Arkansas for Medical Sciences (UAMS) and 9 community-based coalitions organized as regional cancer councils representing 10 of Arkansass 75 counties. Cancer councils, originally funded by the Centers for Disease Control, identify cancer-related problems in their local communities, establish local cancer control priorities, identify and fill gaps in local service and delivery, improve communication with local health care providers, and develop intervention strategies that fit their communitys unique needs. UAMS collaborates with the cancer councils through a participatory approach in assessing the assets and needs of the coalition and in developing a research agenda responsive to community interest and priorities.  \n\n【4】Methods\n-------\n\n【5】### Building the ECL partnership\n\n【6】In 2006, the partnership received funding from the National Cancer Institute for pilot research projects to strengthen and broaden its networks across the state. The St. Francis and Mississippi cancer councils implemented pilots focused on colorectal cancer, which was the issue of interest identified by both councils. The lead organization in the St. Francis County cancer council was EAEC. The lead organization in the Mississippi County cancer council was MCAEOC. Academic partners met regularly with community partners of awarded cancer councils to implement the pilots. Data from the pilots resulted in the development and funding of ECL. ECL is a CBPR intervention designed to increase colorectal cancer screening rates via FOBT among adults aged 50 years or older who do not adhere to screening guidelines. The intervention is based on social cognitive and diffusion theories. The objectives of the partnership were to use a CBPR approach to build infrastructure to conduct research, design materials and methods salient to the community, recruit and train lay health advisors and role models, and develop an assessment instrument. The goal is to test the efficacy of ECL in a 5-year randomized controlled trial with 750 participants who do not meet colorectal cancer screening guidelines. The study is approved by the UAMS institutional review board. Each partner had negotiated subcontracts, which gave the community visible power and equity and set the stage for shared decision making . To create a strong sense of ownership, partners named the study Empowering Communities for Life. Community partners say they hope to empower members of the community through education about the benefits of screening to prevent colorectal cancer. The partnership represents the target community in several ways. Representatives from MCAEOC are Mississippi County natives and consist of 2 African American women and 1 white man. Representatives of EAEC are St. Francis County natives and consist of 2 African American women. University partners include 5 African American women, one of whom is a Mississippi County native, 2 African American men, 3 white men, and 1 Asian woman, for a total of 11 academic partners. Beginning in August 2008, the diverse partnership worked together for 9 months to develop ECL .  **Figure.** Timetable of major milestones, Empowering Communities for Life program, Mississippi and St. Francis counties, Arkansas. Abbreviation: IRB, institutional review board.  \n\n【7】### Building research infrastructure\n\n【8】In initial ECL meetings, the community partners were less vocal than academic partners in discussions of study design and intervention development. When asked, the community partners said that they were unfamiliar with many of the research terms used. To facilitate equitable collaboration, an academic partner used previously developed materials in her work with Lower Mississippi River Delta communities to develop a 4-hour training in basic research for community partners. Academic partners also developed an 8-hour training session in certification to perform research with human subjects to supplement a computerized UAMS training program, which used technical terms unfamiliar to the community partners. All community partners participated in the training. \n\n【9】### Developing ECL materials and methods\n\n【10】Intervention materials were developed for 2 intervention arms and a control arm. The lay health advisor arm consists of a PowerPoint presentation about colorectal cancer delivered by a lay health advisor, a corresponding brochure developed with community partners that reinforces the main points of the presentation, and a community members (role models) 3- to 5-minute testimony about his or her experience with colorectal cancer screening. The health professional arm consists of a PowerPoint presentation about colorectal cancer delivered by a health professional and a corresponding brochure from the American Cancer Society. The control arm consists of a presentation about cardiovascular disease delivered by a health professional and a corresponding brochure from the American Heart Association. Recipients of the intervention will be adults in Mississippi and St. Francis counties aged 50 years or older who are not adherent to colorectal cancer screening guidelines. To facilitate collaborative development of intervention components, academic partners presented initial drafts of PowerPoint presentations and brochures. Community partners reviewed the presentations and talked about the power of storytelling in the community. Academic partners described the Witness Project , a successful cancer screening program that uses storytelling, as a potential model for the storytelling component. Thus, the partnership decided to have a community member tell his or her colorectal cancer screening story in the lay health advisor arm to provide a model of screening behavior and to give participants a personal perspective on the screening experience. Community partners spoke of the importance of engaging the audience so that the presentation would be interesting to them; thus, the partnership decided to include checklists in the brochures for the lay health advisor arm for readers to indicate their own risk for colorectal cancer and symptoms of colorectal cancer they may have. The partnership also decided that the latter half of the presentation in the lay health advisor arm should consist of an interactive demonstration on how to use the FOBT. To refine the intervention, community partners practiced delivering the presentation of the lay health advisor arm to all partners, whereas academic partners delivered the PowerPoint presentations of the health professional and control arms to all partners. The academic partners ensured that community partners delivered the information accurately, whereas community partners ensured that the presentations were delivered in a way that would be interesting to the audience. All partners subsequently made revisions to the intervention and control arms. Revisions included the addition of more discussion questions, graphics, and sound effects to the lay health advisor arm presentation. Aspects of each presentation were also changed to enhance clarity. For example, the partnership decided to use peanut butter in the FOBT interactive demonstration to familiarize participants with stool handling. The PowerPoint slides and brochure were fine-tuned iteratively; several rounds of revisions and presentations increased the clarity and accuracy of the information. \n\n【11】### Selecting and training lay health advisors and role models\n\n【12】The partnership chose employees from EAEC and MCAEOC to serve as lay health advisors because of their 1) recognition in the community as providers of trusted advice and support, 2) experience as lay health advisors on previous cancer council projects, 3) ability to be discreet with participants information, 4) involvement in the project since its inception, 5) interest in project goals and activities, and 6) available time to devote to the project. Community partners developed a strategy to recruit role models who would present their personal experience with colorectal cancer screening. Role models had to reside in either Mississippi County or St. Francis County, have received some type of colorectal cancer screening in the past year, and provide informed consent. Community partners targeted people whom others naturally turn to for advice, emotional support, and tangible aid, and who were known in the community as being discreet. Academic partners developed a 20-hour lay health advisor training . The training was led by an academic partner, and initial topics included an overview of the project, the role of the lay health advisors in the project, and the importance of confidentiality. The interventions presentation components were then reviewed in detail. Each component had corresponding PowerPoint slides, presentation notes, and flash cards with questions and answers. The final part of the training included mock presentations by each lay health advisor at community sites. Community and academic partners critiqued the presenter to improve the presentation. Certification to be a lay health advisor required completion of training and passing the final exam with a score of 80% or higher. To maintain the level of competence achieved through the training, lay health advisors met with one another and with academic partners to practice the presentation. Role models underwent 5 hours of training, which was developed by academic partners and refined by community partners. The training began with an overview of the project, the intervention presentation, the job of the role model, and the importance of confidentiality. A lay health advisor delivered the intervention presentation, which gave a basic overview of colorectal cancer and the importance of screening. Role models were divided into groups, which were co-facilitated by academic and community partners. Each role model was asked to tell his or her story based on a given outline. Feedback was given to each role model. To maintain the level of competence achieved through the training, role models met with community and academic partners to practice the presentation. \n\n【13】### Preparing the assessment instrument\n\n【14】The assessment instrument was created to assess participants self-reported medical history and preventive health services, knowledge of screening recommendations, and attitudes regarding preventive behaviors. Academic partners presented a list of demographic, behavioral, and psychosocial factors associated with FOBT use for the partnership to decide which factors to include in the assessment. For each factor chosen, the partnership decided which questions to include by using previous questionnaires . Community partners said the survey would need to be engaging for participants to give honest answers. The partnership decided to use an audience response system (OptionPower 3.2, Option Technologies Interactive, Orlando, Florida), which presents assessment questions in PowerPoint that participants can answer using a keypad. Academic partners drafted the assessment and trained lay health advisors to use the audience response system; during a series of meetings at which lay health advisors practiced administering the assessment, community and academic partners made revisions to maximize readability and clarity.  \n\n【15】Outcome\n-------\n\n【16】Development of ECL helped strengthen the collaborative relationship between the partners . An outcome of ECL was the recruitment of 11 academic and 5 community partners in a collaborative relationship to develop a CBPR colorectal screening intervention. To develop a stronger research infrastructure within the partnership, trainings were conducted to produce human subjects certification and greater engagement among all 16 partners. Training in research methods resulted in the development of a randomized controlled trial design to test the strategies to promote colorectal screening through ECL, for which the return rate for the FOBT will be the primary outcome measure. Another outcome is the production of theory-based interactive PowerPoint presentations for all intervention arms of ECL that cover the importance of colorectal cancer and cardiovascular disease screenings, production of a brochure for the lay health advisor intervention, and the incorporation of role models to describe their personal experience with colorectal cancer screening. To implement ECL, 6 lay health advisors and 23 role models were recruited. All lay health advisors and 22 role models were certified. The partnership also produced an assessment instrument using an audience response system that evaluates patient experiences in the health care system, colorectal cancer screening behavior and knowledge, cardiovascular disease screening behavior and knowledge, risk factors for cancer and cardiovascular disease, and opinions about cancer and cardiovascular disease prevention.  \n\n【17】Interpretation\n--------------\n\n【18】Given the distance between community and academic partners (191 miles between UAMS and MCAEOC; 95 miles between UAMS and EAEC), the partnership initially decided to alternate regular meetings with conference calls. However, during the conference calls, community partners were less vocal than academic partners. Given that this was the first large-scale research project both communities had been a part of, there was hesitancy in sharing ideas. Some community partners said that working with the university felt like the small town meeting the big city, which made them uncomfortable contributing to discussions. Thus, community and academic partners decided to meet face to face until university partners developed skills to communicate in a community-friendly way and a level of comfort and familiarity between the partners was achieved, which occurred approximately 6 months into the project. In face-to-face meetings, academic partners discovered that they were able to read body language to see whether their questions were being understood, which allowed for adjustments in how questions were worded. Face-to-face meetings also included visual aids to help community partners understand the research. Dialogue was further facilitated by open discussions of community culture and role-playing activities. Community partners revealed that the university partners were seen as authority figures who know what is best and should not be questioned. Because of this perception, community partners spoke up only when they felt strongly about project decisions; voicing their opinions at all was the equivalent of shouting them. With this understanding, academic partners learned to listen carefully to community partners and to give great weight to every comment. Academic partners also emphasized the importance of community partners expertise, whereas community partners learned to view academic partners more realistically.  \n\n【19】Conclusion\n----------\n\n【20】To our knowledge, only a few studies have developed a colorectal cancer prevention intervention for an at-risk population using a CBPR approach . ECL is a theory-grounded intervention that builds on community resources to address cancer disparities by increasing colorectal cancer screening in an underserved population. Community-based participatory strategies incorporating sound research methods and health behavior theory have guided the development and implementation of this study. A product of a 9-year partnership, ECL may be a useful model for community-based interventions to increase colorectal cancer screening among rural, underserved groups, and a step toward eliminating disparities in health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f3c37dd9-4984-4c8d-9217-41e6c95ea1a9", "title": "Outbreak of Haff Disease along the Yangtze River, Anhui Province, China, 2016", "text": "【0】Outbreak of Haff Disease along the Yangtze River, Anhui Province, China, 2016\nHaff disease is an unexplained rhabdomyolysis that occurs within 24 hours after consumption of certain types of freshwater or saltwater fish . It was first reported in 1924 in the vicinity of Königsberg along the Baltic coast near Frisches Haff . Over the next 9 years, an estimated 1,000 persons were affected by similar outbreaks, occurring seasonally in the summer and autumn in this area . Although subsequent outbreaks were identified in several other countries, such as Sweden , the former Soviet Union , Brazil , Japan , and China , the etiology has not yet been determined. An unidentified heat-stable toxin similar to cyanotoxins or palytoxin, but primarily myotoxic and not neurotoxic, is thought to be the cause of Haff disease ; however, evidence supporting this hypothesis has been scant.\n\n【1】In July 2016, the number of rhabdomyolysis cases reported to the National Foodborne Disease Surveillance System (NFDSS) in China dramatically increased in Anhui Province compared with previous years. Most of the cases were reported in Wuhu and Ma’anshan, cities in Anhui Province in eastern China. Epidemiologic features were compatible with Haff disease . Preliminary investigation implicated crayfish as the vector. On August 5, the number of cases surpassed 200, prompting an emergency investigation by the Chinese Field Epidemiology Training Program, together with the Anhui Province Center for Disease Control and Prevention (CDC). The objectives of the investigation were to describe the epidemiologic and clinical characteristics, trace back the implicated vectors, identify possible risk factors, and recommend control measures.\n\n【2】### Methods\n\n【3】##### Case Definition and Finding\n\n【4】We defined a case of rhabdomyolysis as any person with elevation in creatine kinase (CK) value plus clinical manifestations of myalgia or limb weakness . We defined a Haff disease case as illness in any person with acute onset of rhabdomyolysis after ingestion of freshwater fish or seafood within 24 hours in Anhui Province during June–August 2016. We searched for physician-diagnosed rhabdomyolysis cases from the NFDSS, an internet-based, passive surveillance system for foodborne diseases searchable by food source in China. We also reviewed the outpatient and inpatient medical records in hospitals in Wuhu and Ma’anshan during the outbreak period to search for potential rhabdomyolysis cases.\n\n【5】Local Anhui Province CDC staff or Chinese Field Epidemiology Training Program trainees interviewed all rhabdomyolysis case-patients, either in-person or by telephone, using a structured questionnaire. Information collected included age, sex, date of onset, disease duration, clinical symptoms, potential risk factors (e.g. food, drugs, alcohol consumption, intense exercise, allergy history, underlying chronic illness), and the quantity of crayfish consumed. The researchers also obtained laboratory test findings from hospital medical records. They applied the Haff disease case definition to rhabdomyolysis cases to identify Haff disease cases and collected blood and urine specimens from Haff disease case-patients for further analysis.\n\n【6】In total, 673 rhabdomyolysis cases were identified in Anhui Province during June–August 2016. Of these, 99.9% (672/673) were compatible with the definition of Haff disease. All but 1 patient consumed cooked crayfish before symptom onset. The patient who did not eat cooked crayfish was a steelworker who had been working in the factory before onset, suggesting his illness might have been caused by heatstroke.\n\n【7】##### Case–Control Study\n\n【8】Although nearly all Haff disease case-patients ate cooked crayfish, we did not know what percent of persons who did not become ill also ate meals containing crayfish, given that crayfish were widely available during June–August 2016. Therefore, we conducted a matched case–control study to assess the association between eating crayfish and Haff disease. Cases in this study were persons who met the definition for Haff disease, had shared a meal with someone else before symptom onset, and consented to participate in the study. We identified \\> 1 control per case; controls were selected among persons who shared the suspected meal of exposure with the case-patient before symptom onset. Controls had no clinical symptoms compatible with rhabdomyolysis and consented to participate in the study. Persons with other illnesses (e.g. fever, cold, injury, etc.) were disqualified as controls. In total, 67 cases and 108 controls were enrolled in the case–control study. Trained investigators conducted telephone-based interviews August 7–15, 2016, using a standardized questionnaire.\n\n【9】##### Traceback of Food and Environmental Investigation\n\n【10】For all Haff disease cases, we conducted a traceback investigation for the source of the implicated food by interviewing case-patients, restaurant owners, fishermen, and crayfish sellers. We conducted an environmental investigation of potential contamination along the distribution chain or unusual events during the outbreak period. We investigated the restaurants where case-patients had a meal before onset to find out where the crayfish came from and how they were cooked. We also visited crayfish farms, settings where crayfish were caught, and factories along the Yangtze River to identify whether the implicated crayfish or the environment in which the crayfish were raised had been contaminated.\n\n【11】##### Data Analysis\n\n【12】We performed statistical analysis using SPSS Statistics 20 . We compared cases and controls by χ 2  test. Significant risk factors (p<0.05) in the χ 2  tests were included in a multivariate Cox proportional hazard model to determine the odds ratio (OR) and 95% CI for the potential risk factors associated with Haff disease. All of the p values were 2-sided, and p<0.05 was considered significant.\n\n【13】### Results\n\n【14】##### Confirmation of the Outbreak\n\n【15】In total, we verified 672 Haff disease cases in Anhui Province during June–August 2016. All cases occurred in 7 cities along the Yangtze River in Anhui Province; 83.3% (560/672) of the cases occurred in Wuhu (334 cases) and Ma’anshan (226 cases). We focused our investigation on the cases that occurred in Wuhu and Ma’anshan. Of the 560 case-patients in Wuhu and Ma’anshan, 495 (88.4%) completed the questionnaires; all 495 had consumed crayfish within 24 hours before symptom onset. The epidemic curve suggested a continuing common-source outbreak .\n\n【16】##### Descriptive Epidemiology\n\n【17】The outbreak started at the end of June, peaked in mid-July to early August, and lasted through August 17. Of the 495 case-patients, 197 (39.8%) were hospitalized; mean length of hospital stay was 7.3 ± 3.2 days. No deaths were reported. The mean age of the case-patients was 38.7 ± 13.5 years; 323/495 (65.3%) patients were female. Although cases were widely distributed in the 2 cities, 87.7% (434/495) were in residents from urban areas close to the Yangtze River. Each case-patient consumed a mean of 11.6 ± 6.1 crayfish pieces. The mean incubation period was 6.2 ± 3.8 hours.\n\n【18】##### Clinical Characteristics\n\n【19】All 495 case-patients experienced myalgia that was local or diffuse, involving the back, waist, whole body, neck, limbs, and chest . A total of 271/495 (54.7%) experienced muscle weakness. Additional symptoms included brown urine, dyspnea, vomiting, abdominal pain, dizziness, and headache. Symptoms of nerve paralysis and fever were rare. Acute renal failure was not observed.\n\n【20】##### Laboratory Characteristics\n\n【21】We reviewed the laboratory test findings of blood and urine for some cases. The mean value of myoglobin was 330.0 ± 121.2 ng/mL, and mean CK level was 5,439.2 ± 4,765.1 U/L. In >80% of the cases, the levels of muscle-type CK and aspartate aminotransferase were abnormally elevated. In addition, 50.0% of case-patients were positive for urinary occult blood and proteinuria .\n\n【22】##### Case–Control Study\n\n【23】In the case–control study, 100% of the 67 cases and 93.3% (101/108) of controls ate crayfish during their shared meal (OR = ꝏ, 95% CI 0.92–ꝏ). We observed a significant dose-response relationship between the number of pieces of crayfish eaten and Haff disease (χ 2  \\= 29.225; p<0.001) . Further analysis showed that eating crayfish liver was associated with increased disease risk (OR = 4.0, 95% CI 1.2–12.7).\n\n【24】##### Traceback and Environmental Investigation\n\n【25】Wuhu and Ma’anshan are located in the middle to lower reaches of the Yangtze River. Crayfish is a popular dish for residents of these 2 cities. Before the Haff disease outbreak, Anhui Province experienced heavy rainfall, which caused the largest flood disaster in decades. Consequently, rain or floodwater was retained in irrigation ditches and detention ponds for an extended time, and the amount of crayfish caught on the shores of the Yangtze River or its connected ditches was 5–10 times more during the outbreak period. However, no industrial or chemical contamination along the Yangtze River was reported.\n\n【26】The only common risk factor for all cases was eating crayfish, which were cooked thoroughly. We conducted a traceback investigation of the source for the implicated crayfish in Ma’anshan and Wuhu by interviewing persons in markets, restaurants, fisheries, and settings where crayfish were caught, as well as fishermen; we were able to trace 50.1% (248/495) of the implicated crayfish to their sources. Of these, 96.8% (240/248) were wild crayfish caught on the shores of the Yangtze River or its connected ditches. When we consulted with crayfish biologists, we found that the species of crayfish implicated during this outbreak was _Procambarus clarkii_ .\n\n【27】##### Public Health Measures\n\n【28】Local governments issued a warning about the dangers of eating crayfish. In addition, public health departments instituted continuous surveillance and investigation of the outbreak.\n\n【29】### Discussion\n\n【30】The epidemiologic and traceback investigations of a large outbreak of Haff disease in Anhui Province, China, indicated that all case-patients consumed crayfish within 24 hours before symptom onset; the implicated crayfish were caught on the shores of the Yangtze River or its connected ditches. The case–control study revealed that eating the liver of crayfish was associated with an increased risk for disease; the risk increased as the quantity of crayfish eaten increased.\n\n【31】In China, the earliest reported outbreak of Haff disease was in Beijing in 2000 and involved 6 cases . An epidemiologic study revealed that all patients ate crayfish before onset, suggesting a link between crayfish and Haff disease . Although the literature shows that eating several species of fish, such as buffalo fish , salmon , freshwater pompano , marine boxfish , and pomfrets , could trigger Haff disease, almost all Haff disease cases in China were associated with eating crayfish . In recent years, Haff disease outbreaks have been reported in other cities in China . These outbreaks prompted the China CDC to conduct a thorough investigation of Haff disease. Crayfish have become a popular seafood for residents in central and eastern China, especially in June–September. Previous studies have reported that Haff disease shows a seasonal pattern, and outbreaks usually occur in the summer and fall months . Although a large Haff disease outbreak caused by eating freshwater pomfret occurred in October 2009 in southern China , most crayfish-related outbreaks , clusters , and sporadic cases  occurred predominantly in the summer. Seasonal crayfish harvest and consumption in June–September likely increases the opportunities for exposure, which may partially explain the seasonal pattern of Haff disease in China .\n\n【32】The most commonly reported clinical features in this outbreak were myalgia and muscle weakness, as well as abnormal levels of myoglobin and CK. Increased serum myoglobin concentration is the basis for early diagnosis of rhabdomyolysis ; however, myoglobin concentrations tend to normalize within 6–8 hours following exposure. Thus, the window of opportunity for diagnosis is short . Of note, elevated myoglobin concentrations were observed in all case-patients who were tested in this study; this may be due to prompt medical care and timely laboratory testing in the hospital.\n\n【33】Rhabdomyolysis is a common life-threatening syndrome characterized by the injury of skeletal muscle resulting in the leakage of intracellular contents into the circulatory system . Patients with rhabdomyolysis usually experience myalgia, muscle weakness, raised serum CK, and brown urine . The etiologic spectrum of rhabdomyolysis is extensive, including crush injuries, ischemia, strenuous exercise, extreme body temperatures, drugs, toxins, infections, hereditary causes, and inflammatory or autoimmune muscle disease . A substantial number of patients may have no cause identified. We found that nerve paralysis and fever were rare symptoms, all crayfish were cooked thoroughly, and no industrial or chemical contamination was identified; therefore, this outbreak was unlikely to have been caused by infectious or chemical etiologies. Diaz et al. reported that an unidentified, heat-stable, algal toxin with primarily myotoxic rather than neurotoxic properties in seafood has been proposed as a cause of Haff disease ; whether this toxin also exists in crayfish remains unknown.\n\n【34】Although many Haff disease cases have occurred in cities located in the middle to lower reaches of the Yangtze River, the association between Haff disease and crayfish caught from Yangtze River has not been elucidated in the published literature . In recent years, 3 other large Haff disease outbreaks have been reported in Nanjing and Tongling, 2 other cities located in the middle to lower reaches of the Yangtze River . The fact that these outbreaks all occurred in the middle to lower reaches of the Yangtze River suggests that crayfish could be their common etiology.\n\n【35】Studies using a mouse model have found that the hazardous substance from crayfish could cause rhabdomyolysis . This hazardous substance is specific to certain batches of crayfish. A dose-response relationship has also been observed. These findings in laboratory animals were consistent with the results of human epidemiologic investigation  and with our case–control study findings.\n\n【36】Our study had several limitations. First, because we lacked data on how many persons ate crayfish in the 2 study cities, we could not calculate the attack rates. Second, not all crayfish were traced back to their sources. Third, we were unable to conduct animal experiments to prove causation.\n\n【37】In conclusion, during this outbreak, the risk for Haff disease was associated with eating crayfish along the Yangtze River. The etiology of Haff disease remains elusive due to lack of knowledge of the underlying disease mechanism of rhabdomyolysis. Our findings might help researchers isolate the toxin that causes this disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "49faa39b-b3f8-47b4-9bab-8501efab1021", "title": "Knowledge and Behavioral Effects in Cardiovascular Health: Community Health Worker Health Disparities Initiative, 2007–2010", "text": "【0】Knowledge and Behavioral Effects in Cardiovascular Health: Community Health Worker Health Disparities Initiative, 2007–2010\nAbstract\n--------\n\n【1】**Introduction**  \nCardiovascular disease is the leading cause of death in the United States, and disparities in cardiovascular health exist among African Americans, American Indians, Hispanics, and Filipinos. The Community Health Worker Health Disparities Initiative of the National Heart, Lung, and Blood Institute (NHLBI) includes culturally tailored curricula taught by community health workers (CHWs) to improve knowledge and heart-healthy behaviors in these racial/ethnic groups.\n\n【2】**Methods**  \nWe used data from 1,004 community participants in a 10-session curriculum taught by CHWs at 15 sites to evaluate the NHLBI’s health disparities initiative by using a 1-group pretest–posttest design. The curriculum addressed identification and management of cardiovascular disease risk factors. We used linear mixed effects and generalized linear mixed effects models to examine results.\n\n【3】**Results**  \nAverage participant age was 48; 75% were female, 50% were Hispanic, 35% were African American, 8% were Filipino, and 7% were American Indian. Twenty-three percent reported a history of diabetes, and 37% reported a family history of heart disease. Correct pretest to posttest knowledge scores increased from 48% to 74% for heart healthy knowledge. The percentage of participants at the action or maintenance stage of behavior change increased from 41% to 85%.\n\n【4】**Conclusion**  \nUsing the CHW model to implement community education with culturally tailored curricula may improve heart health knowledge and behaviors among minorities. Further studies should examine the influence of such programs on clinical risk factors for cardiovascular disease.\n\n【5】Introduction\n------------\n\n【6】Cardiovascular disease (CVD) is the leading cause of death in the United States, accounting for 32% of deaths . Disparities in the prevalence of CVD and associated risk factors (eg, hypertension, high blood cholesterol, smoking, overweight, and obesity) have been documented in racial and ethnic minority populations . African Americans have the highest rate of hypertension in the United States (44% vs 29% among non-Hispanic whites) , and Hispanics and African Americans have the highest rates of obesity . American Indians have the highest prevalence of coronary heart disease, which affects 11.6% of their population , and Filipinos are significantly more likely than non-Hispanic whites to have hypertension .\n\n【7】To address these health disparities, the National Heart, Lung, and Blood Institute (NHLBI) created the Community Health Worker (CHW) Health Disparities Initiative, drawing on science-based materials, resources, and educational programs to improve health in minority and underserved communities. A key strategy uses a CHW-driven, community-based, participatory approach to deliver community education sessions to promote heart health and reduce health disparities. CHWs are known by many names, such as _promotores de salud_ , community health representatives, or community educators; however, we refer to them here as CHWs.\n\n【8】Before beginning the community education sessions, CHWs are trained on NHLBI’s heart health curricula. The curricula have a common core, and each curriculum component is designed to be culturally and linguistically appropriate for African Americans, American Indians, Filipinos, or Hispanics. The curricula are designed to increase knowledge and improve lifestyle behaviors associated with cardiovascular health through culturally competent  community education activities. Supporting materials include picture cards, recipe books, and risk factor booklets that are used to reinforce key messages . Figure 1 shows the logic model for the community education strategy.\n\n【9】Smaller CHW programs in Hispanic communities in Arizona, Texas, and California increased knowledge  and improved heart health behaviors . Programs using NHLBI’s curriculum in conjunction with clinical management have also demonstrated reductions in heart disease risk factors, including improvements in low-density lipoprotein cholesterol level, triglyceride level, blood pressure, weight, and glycated hemoglobin level . However, other studies and systematic reviews of CHW programs have shown mixed evidence of CHWs effectiveness in the prevention and control of CVD .\n\n【10】Our study assesses effects on heart health knowledge, CVD risk factor behaviors, confidence in preparing heart healthy food, and readiness to change behavior among 1,004 community participants in group education sessions. Our study was conducted in various settings, including community-based organizations, clinics, training centers, tribal organizations, and public housing.\n\n【11】Methods\n-------\n\n【12】### Setting and intervention\n\n【13】From 2007 through 2010 trained CHWs delivered NHLBI’s heart health curricula to small groups at 15 sites across the United States. The sites were selected through a competitive process and included community-based organizations, clinics, and tribal organizations in African American, American Indian, Filipino, and Hispanic communities. A curriculum was developed tailored to each of these races/ethnicities. Participating sites recruited adults aged 18 years or older from the general population in their communities. Participants attended 10 sessions that covered identifying risk factors for heart disease, recognizing the signs of heart attack, preparing healthy meals, eating healthy on a budget, controlling blood pressure, controlling weight, managing diabetes, lowering high blood cholesterol, increasing physical activity, and decreasing smoking. The sessions were taught weekly or biweekly and lasted approximately 2 hours. CHWs collected information from participants before and after completing all the sessions by using self-report instruments to assess changes in heart health knowledge, attitudes, and behaviors.\n\n【14】### Evaluation measures\n\n【15】We used an observational 1-group, pretest–posttest design to examine changes in outcomes between baseline and completion of the program. The data collection instrument, called “My Health Habits,” was a survey included as part of each curriculum. Data were routinely collected as part of the community education program to allow sites to monitor their own outcomes and were later used for our evaluation. The National Institutes of Health Office of Management and Budget gave the survey instrument a clinical exemption for collection of participant data. NHLBI contracted with the American Institutes for Research to analyze the data for this summary outcome evaluation. The “My Health Habits” survey was used to assess 5 outcome domains:\n\n【16】**1\\. Heart health knowledge.** The 13 knowledge items were summed into an average percentage score for each participant where the score indicated the percentage of items answered correctly.\n\n【17】**2\\. CVD food-related risk factor behaviors.** The 25 food-related behaviors frequency items were scored on a 4-point scale (1 = never, 2 = sometimes, 3 = most of the time, and 4 = always). An overall summed average frequency of the 25 items was calculated, as well as 3 subscales addressing salt and sodium consumption (9 items), cholesterol and fat consumption (8 items); and weight management behaviors (8 items).\n\n【18】**3\\. Physical activity.** Nine items assessing frequency and intensity of physical activity were used to categorize participants as physically active or not physically active. “Physically active” was defined as engaging in moderate to vigorous aerobic physical activity during leisure time at least 2 days a week for more than 30 minutes each day. Otherwise, participants were classified as not physically active.\n\n【19】**4\\. Confidence in preparing heart-healthy food.** This was a 1-item measure scored on a 4-point scale (1 = not confident, 2 = somewhat confident, 3 = confident, and 4 = very confident). The item was dichotomized by combining the 2 lower categories (not confident and somewhat confident) and 2 higher categories (confident and very confident) on the basis of its bimodal distribution.\n\n【20】**5\\. Stages of change.** The stage of change measure is based on the transtheoretical model, which assesses an individual’s readiness for change along the following 5-stage continuum: precontemplation, contemplation, preparation, action, and maintenance . This survey measure is presented to respondents as a scenario that describes family members at each stage along the continuum and asks them to indicate who they identify with most. The grandmother, for example, represented the action stage and was described as someone who “takes classes to learn how to improve her health and puts into practice what she learns. She uses recipes to cook healthfully and takes walks every day.” We dichotomized this 5-point measure by combining the first 3 stages of behavior change (precontemplation, contemplation, and preparation) and the last 2 stages (action and maintenance). This measure was not included in the survey administered at Filipino sites.\n\n【21】### Data analysis\n\n【22】For each site, data from the pretests and posttests were linked by a participant identification number when one was available. We used linear mixed effects models  for continuous outcomes and generalized linear mixed models  for categorical outcomes to estimate changes in outcome measures at the start and end of program implementation. These models account for potential correlations among participants within the same study site or from repeated measures for the same participant. For respondents with missing scores at the pretest but not the posttest, or vice versa, the observed portion of their data was kept in the analysis. By using the Maximum Likelihood estimation method in the analytic models, we obtained regression coefficients that were adjusted for the missing values by assuming that the data were missing at random. The parameter estimates from the Maximum Likelihood estimation are very similar to those obtained from the multiple imputations. The former, however, is more efficient because adjusted regression coefficients and standard errors can be obtained without generating complete data sets with imputed pretest or posttest scores . Demographic and CVD risk characteristics (eg, age, sex, told by a health professional they had diabetes, family history of heart disease) and the setting of program implementation (ie, clinic, community based organization \\[CBO\\], or other type of site) were included in the models as control or predictor variables. When possible, results for each curricula tailored to a racial/ethnic group were reported. Associations between participant characteristics related to CVD (eg, having diabetes or a family history of heart disease) and outcomes were examined by using multiple regression models. Statistical analyses were conducted using SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina).\n\n【23】Results\n-------\n\n【24】### Participant and site characteristics\n\n【25】The program was implemented most often through CBOs, which served 44% of participants, followed by clinics, which served 19% . The remaining participants attended sessions in other community settings such as schools or apartment buildings. Seventy-five percent of participants were women, and their average age was 48 years. Approximately 50% of participants were Hispanic, 35% African American, 7% American Indian, and 8% Filipino. In terms of CVD risk factors, 23% had been told by a health care professional that they had diabetes and 37% reported a family history of heart disease.\n\n【26】### Response rates\n\n【27】Overall, 1,004 total participants attended the education sessions. Of these, 849 participants (85%) completed both the pretest and posttest surveys later used for the evaluation; 123 (12%) completed only the pretest, and 32 (3%) completed only the posttest. The percentage of participants who had matched data from the pretest and posttest varied across curricula: 99% for the African American curriculum, 92% for the Hispanic curriculum, 40% for the American Indian curriculum, and 13% for the Filipino curriculum. The percentage of Filipino participants that had both a pretest and posttest that could be linked was low because 2 of the 3 Filipino sites did not use the unique identification numbers needed for linkage. In our analysis we kept the observed portion of data for those respondents whose data could not be linked between pretest and posttest and those who had missing scores at the pretest but not the posttest, or vice versa.\n\n【28】### Overall outcomes\n\n【29】We adjusted the outcomes at pretest and posttest for demographic and CVD risk characteristics (ie, age, sex, personal history of diabetes, and family history of heart disease), and whether the program was implemented through a clinic, CBO, or other type of site . We calculated overall results representing summary estimates of changes in the outcomes from pretest to posttest and curriculum-specific results.\n\n【30】**Heart health knowledge.** The percentage of correct answers on the heart health knowledge questions significantly increased, on average, from 48% to 74% pretest to posttest ( _P_ < .001) .\n\n【31】**CVD food-related risk factor behaviors.** The frequency of self-reported food-related behaviors associated with cardiovascular health increased significantly on the overall score, from 2.5 to 2.9 on a 1 to 4 scale, and in each of the 3 subdomains: salt and sodium consumption, cholesterol and fat consumption, and weight management ( _P_ < .001) .\n\n【32】**Physical activity.** The percentage of participants classified as physically active increased from 33% to 65% pretest to posttest ( _P_ < .001) .\n\n【33】**Confidence.** A significantly greater proportion of participants reported being confident or very confident in being able to prepare heart healthy foods for themselves and their families posttest (88%) than on pretest (40%) ( _P_ < .001) .\n\n【34】**Stages of change.** Eighty-five percent of participants, on the basis of the scenarios presented, self-identified as being in the action or maintenance stage of change at posttest compared with 41% at pretest ( _P_ < .001) .\n\n【35】**Significant predictors of changes in knowledge and behavior.** Age and personal history of diabetes were found to be significant predictors for the magnitude of change in the overall score of heart health behaviors. Younger participants and those without a known history of diabetes made the greatest gains in the adoption of heart-healthy behaviors. Personal history of diabetes and family history of heart disease were significant predictors of heart health knowledge. Participants who did not have diabetes or did not know their status and those who did not say they had a family history of heart disease had larger improvements in heart health knowledge than those who responded affirmatively to either of these questions.\n\n【36】### Participant’s program satisfaction\n\n【37】More than 97% of participants were satisfied or very satisfied with the curriculum. Most indicated they shared the information learned with their friends (81%), family (95%), coworkers (24%), or with other associates (21%).\n\n【38】### Curriculum-specific results\n\n【39】We analyzedthe results to determine whether they varied across groups using the different curricula. The American Indian and Filipino study sites had considerably fewer participants (67 and 82, respectively) than African American or Hispanic sites (354 and 501, respectively), so these results should be interpreted with caution.\n\n【40】**Knowledge.** Heart health knowledge increased significantly with all 4 curricula, ranging from 22 percentage points for the African American curriculum to 33 percentage points for the Hispanic curriculum.\n\n【41】**Behavior.** Participants in all 4 curricula had improvements in frequency of self-report of adopting healthy food-related behaviors that ranged from 0.2 points for the American Indian curriculum to 0.4 points for the Hispanic curriculum. This increase was significant for all curricula except Filipino sites, where the sample size was small and the individual pretest and posttest measures could not be linked. Although the unlinked cases were included in the analyses, the comparisons in the unmatched analyses are less precise and have less statistical power to detect differences than comparisons with matched cases. Self-reported physical activity also increased significantly in all groups except those using the Filipino curriculum.\n\n【42】**Confidence.** Participants in the African American and Hispanic curricula reported significantly higher levels of confidence in their ability to prepare heart healthy foods after participating in the program. Participants in the Filipino and American Indian curricula also reported higher confidence, but levels were not significant.\n\n【43】**Stage of change.** African American and Hispanic participants had a significantly larger proportion of participants at the action or maintenance stage of change at posttest than at pretest compared with American Indians, Alaska Natives and Filipinos. Results were not significant for American Indian participants. The Filipino sites did not report on the stage-of-change measure.\n\n【44】Discussion\n----------\n\n【45】This study is the largest evaluation of a CHW-based program to date. It assesses the effect across 4 different minority groups, unlike other interventions that target specific ethnic groups or are not culturally grounded. Findings from this study provide evidence that CHW-led interventions have a positive effect on heart health knowledge and self-reported health behaviors in minority and underserved communities. The CHW model has been used historically to bridge the gap between the health care system and its clients. CHWs have been effective at providing community education and at helping patients navigate the system, thus promoting improved health outcomes . The results observed in this evaluation are in line with other studies that used a similar research design . With few exceptions, positive results were observed across all curricula. Community participants generally increased their heart health knowledge and improved in self-reported health behaviors (eg, increasing physical activity) that can reduce CVD risk. Additionally, participants progressed to a stage of change more closely aligned with practicing heart healthy behaviors on a regular basis. Given the successes of the experiences documented here, consideration should be given to expanding this program in similar minority and underserved communities. More rigorous evaluations are needed to further examine CHW’s abilities to effectively improve behavior change as well as improve clinical outcomes (eg, blood pressure). In addition, the curricula may need to be updated to reflect the current science (eg, revised physical activity and nutrition guidelines) and implemented with different methods (eg, online education) to keep up with rapid changes in technology.\n\n【46】Because it was based on a convenience sample, our evaluation did not include a control group; therefore, the observed changes cannot be attributed solely to the intervention and are not generalizable beyond the study population. Anecdotally, program implementation strategies varied across sites (eg, duration and number of sessions, teaching style of CHWs) to accommodate resource limitations and community needs. These and other issues could have contributed to the variability in results across the 4 curricula. Lack of unique identification numbers from the Filipino sites limited the ability to match pretest and posttest scores for individuals. Although the analysis included these individuals by applying appropriate methods to deal with missing data, results for these sites might have been more robust had the data been matched. Finally, the data collected were self-reported and subject to recall and social desirability biases.\n\n【47】Despite limitations, the changes we observed were large, significant, and consistent across settings and racial/ethnic-specific curricula applied to over 1,000 participants. The results add to the evidence that the CHW model is effective in improving heart health knowledge and behaviors and can move participants along a continuum of being able to implement and sustain positive heart health behaviors. Robust predefined evaluation studies using a control group and measuring clinical outcomes would contribute more evidence on the effectiveness of the CHW model to reduce risk factors for CVD. It also would be valuable to examine the content and methods of the CHW training program and to link training quality and program fidelity to outcomes for community participants.\n\n【48】Cardiovascular health disparities persist among racial and ethnic minority groups. Programs are needed that are culturally and linguistically tailored to these groups and are easy to implement with limited resources. The curricula used in this intervention program met these criteria and were well received by the CHWs, community participants, and organizations. The curricula can be more widely applied to help reduce racial and ethnic disparities in cardiovascular health. NHLBI continues to expand implementation of the curricula through building partnerships with various organizations (eg, public housing authorities, academia, CBOs, clinics) in an effort to help reduce CVD disparities. For example, another study of 97 Filipino participants showed promising results for an expanded version of the program where individuals screened for being at risk for heart disease were followed and managed in a clinic setting in addition to participating in group education activities led by CHWs . Findings from clinical trials of similar interventions provide additional evidence of the effectiveness of this type of program in preventing CVD and improving health outcomes . Further development and testing of similar efforts using CHWs to decrease risk factors for CVD and improve outcomes are needed to support widespread implementation of this type of approach . This study adds to the body of literature by showing that CHWs can be part of the solution to address heart health disparities by using NHLBI's curricula in minority communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7c0a9277-8cac-4358-a416-5dff8752fff0", "title": "Successful Scientific Writing and Publishing: A Step-by-Step Approach", "text": "【0】Successful Scientific Writing and Publishing: A Step-by-Step Approach\nAbstract\n--------\n\n【1】Scientific writing and publication are essential to advancing knowledge and practice in public health, but prospective authors face substantial challenges. Authors can overcome barriers, such as lack of understanding about scientific writing and the publishing process, with training and resources. The objective of this article is to provide guidance and practical recommendations to help both inexperienced and experienced authors working in public health settings to more efficiently publish the results of their work in the peer-reviewed literature. We include an overview of basic scientific writing principles, a detailed description of the sections of an original research article, and practical recommendations for selecting a journal and responding to peer review comments. The overall approach and strategies presented are intended to contribute to individual career development while also increasing the external validity of published literature and promoting quality public health science.\n\n【2】Introduction\n------------\n\n【3】Publishing in the peer-reviewed literature is essential to advancing science and its translation to practice in public health . The public health workforce is diverse and practices in a variety of settings . For some public health professionals, writing and publishing the results of their work is a requirement. Others, such as program managers, policy makers, or health educators, may see publishing as being outside the scope of their responsibilities .\n\n【4】Disseminating new knowledge via writing and publishing is vital both to authors and to the field of public health . On an individual level, publishing is associated with professional development and career advancement . Publications share new research, results, and methods in a trusted format and advance scientific knowledge and practice . As more public health professionals are empowered to publish, the science and practice of public health will advance .\n\n【5】Unfortunately, prospective authors face barriers to publishing their work, including navigating the process of scientific writing and publishing, which can be time-consuming and cumbersome. Often, public health professionals lack both training opportunities and understanding of the process . To address these barriers and encourage public health professionals to publish their findings, the senior author (P.Z.S.) and others developed Successful Scientific Writing (SSW), a course about scientific writing and publishing. Over the past 30 years, this course has been taught to thousands of public health professionals, as well as hundreds of students at multiple graduate schools of public health. An unpublished longitudinal survey of course participants indicated that two-thirds agreed that SSW had helped them to publish a scientific manuscript or have a conference abstract accepted. The course content has been translated into this manuscript. The objective of this article is to provide prospective authors with the tools needed to write original research articles of high quality that have a good chance of being published.\n\n【6】Basic Recommendations for Scientific Writing\n--------------------------------------------\n\n【7】Prospective authors need to know and tailor their writing to the audience. When writing for scientific journals, 4 fundamental recommendations are: clearly stating the usefulness of the study, formulating a key message, limiting unnecessary words, and using strategic sentence structure.\n\n【8】To demonstrate usefulness, focus on how the study addresses a meaningful gap in current knowledge or understanding. What critical piece of information does the study provide that will help solve an important public health problem? For example, if a particular group of people is at higher risk for a specific condition, but the magnitude of that risk is unknown, a study to quantify the risk could be important for measuring the population’s burden of disease.\n\n【9】Scientific articles should have a clear and concise take-home message. Typically, this is expressed in 1 to 2 sentences that summarize the main point of the paper. This message can be used to focus the presentation of background information, results, and discussion of findings. As an early step in the drafting of an article, we recommend writing out the take-home message and sharing it with co-authors for their review and comment. Authors who know their key point are better able to keep their writing within the scope of the article and present information more succinctly. Once an initial draft of the manuscript is complete, the take-home message can be used to review the content and remove needless words, sentences, or paragraphs.\n\n【10】Concise writing improves the clarity of an article. Including additional words or clauses can divert from the main message and confuse the reader. Additionally, journal articles are typically limited by word count. The most important words and phrases to eliminate are those that do not add meaning, or are duplicative. Often, cutting adjectives or parenthetical statements results in a more concise paper that is also easier to read.\n\n【11】Sentence structure strongly influences the readability and comprehension of journal articles. Twenty to 25 words is a reasonable range for maximum sentence length. Limit the number of clauses per sentence, and place the most important or relevant clause at the end of the sentence . Consider the sentences:\n\n【12】*   By using these tips and tricks, an author may write and publish an additional 2 articles a year.\n\n【13】*   An author may write and publish an additional 2 articles a year by using these tips and tricks.\n\n【14】The focus of the first sentence is on the impact of using the tips and tricks, that is, 2 more articles published per year. In contrast, the second sentence focuses on the tips and tricks themselves.\n\n【15】Authors should use the active voice whenever possible. Consider the following example:\n\n【16】*   Active voice: Authors who use the active voice write more clearly.\n\n【17】*   Passive voice: Clarity of writing is promoted by the use of the active voice.\n\n【18】The active voice specifies who is doing the action described in the sentence. Using the active voice improves clarity and understanding, and generally uses fewer words. Scientific writing includes both active and passive voice, but authors should be intentional with their use of either one.\n\n【19】Sections of an Original Research Article\n----------------------------------------\n\n【20】Original research articles make up most of the peer-reviewed literature , follow a standardized format, and are the focus of this article. The 4 main sections are the introduction, methods, results, and discussion, sometimes referred to by the initialism, IMRAD. These 4 sections are referred to as the _body_ of an article. Two additional components of all peer-reviewed articles are the title and the abstract. Each section’s purpose and key components, along with specific recommendations for writing each section, are listed below.\n\n【21】**Title.** The purpose of a title is twofold: to provide an accurate and informative summary and to attract the target audience. Both prospective readers and database search engines use the title to screen articles for relevance . All titles should clearly state the topic being studied. The topic includes the who, what, when, and where of the study. Along with the topic, select 1 or 2 of the following items to include within the title: methods, results, conclusions, or named data set or study. The items chosen should emphasize what is new and useful about the study. Some sources recommend limiting the title to less than 150 characters . Articles with shorter titles are more frequently cited than articles with longer titles . Several title options are possible for the same study .  \n\n【22】Two examples of title options for a single study. \n\n【23】**Abstract** . The abstract serves 2 key functions. Journals may screen articles for potential publication by using the abstract alone , and readers may use the abstract to decide whether to read further. Therefore, it is critical to produce an accurate and clear abstract that highlights the major purpose of the study, basic procedures, main findings, and principal conclusions . Most abstracts have a word limit and can be either structured following IMRAD, or unstructured. The abstract needs to stand alone from the article and tell the most important parts of the scientific story up front.\n\n【24】**Introduction** . The purpose of the introduction is to explain how the study sought to create knowledge that is new and useful. The introduction section may often require only 3 paragraphs. First, describe the scope, nature, or magnitude of the problem being addressed. Next, clearly articulate why better understanding this problem is useful, including what is currently known and the limitations of relevant previous studies. Finally, explain what the present study adds to the knowledge base. Explicitly state whether data were collected in a unique way or obtained from a previously unstudied data set or population. Presenting both the usefulness and novelty of the approach taken will prepare the reader for the remaining sections of the article.\n\n【25】**Methods** . The methods section provides the information necessary to allow others, given the same data, to recreate the analysis. It describes exactly how data relevant to the study purpose were collected, organized, and analyzed. The methods section describes the process of conducting the study — from how the sample was selected to which statistical methods were used to analyze the data. Authors should clearly name, define, and describe each study variable. Some journals allow detailed methods to be included in an appendix or supplementary document. If the analysis involves a commonly used public health data set, such as the Behavioral Risk Factor Surveillance System , general aspects of the data set can be provided to readers by using references. Because what was done is typically more important than who did it, use of the passive voice is often appropriate when describing methods. For example, “The study was a group randomized, controlled trial. A coin was tossed to select an intervention group and a control group.”\n\n【26】**Results** . The results section describes the main outcomes of the study or analysis but does not interpret the findings or place them in the context of previous research. It is important that the results be logically organized. Suggested organization strategies include presenting results pertaining to the entire population first, and then subgroup analyses, or presenting results according to increasing complexity of analysis, starting with demographic results before proceeding to univariate and multivariate analyses. Authors wishing to draw special attention to novel or unexpected results can present them first.\n\n【27】One strategy for writing the results section is to start by first drafting the figures and tables. Figures, which typically show trends or relationships, and tables, which show specific data points, should each support a main outcome of the study. Identify the figures and tables that best describe the findings and relate to the study’s purpose, and then develop 1 to 2 sentences summarizing each one. Data not relevant to the study purpose may be excluded, summarized briefly in the text, or included in supplemental data sets. When finalizing figures, ensure that axes are labeled and that readers can understand figures without having to refer to accompanying text.\n\n【28】**Discussion** . In the discussion section, authors interpret the results of their study within the context of both the related literature and the specific scientific gap the study was intended to fill. The discussion does not introduce results that were not presented in the results section. One way authors can focus their discussion is to limit this section to 4 paragraphs: start by reinforcing the study’s take-home message(s), contextualize key results within the relevant literature, state the study limitations, and lastly, make recommendations for further research or policy and practice changes. Authors can support assertions made in the discussion with either their own findings or by referencing related research. By interpreting their own study results and comparing them to others in the literature, authors can emphasize findings that are unique, useful, and relevant. Present study limitations clearly and without apology. Finally, state the implications of the study and provide recommendations or next steps, for example, further research into remaining gaps or changes to practice or policy. Statements or recommendations regarding policy may use the passive voice, especially in instances where the action to be taken is more important than who will implement the action.\n\n【29】Beginning the Writing Process\n-----------------------------\n\n【30】The process of writing a scientific article occurs before, during, and after conducting the study or analyses. Conducting a literature review is crucial to confirm the existence of the evidence gap that the planned analysis seeks to fill. Because literature searches are often part of applying for research funding or developing a study protocol, the citations used in the grant application or study proposal can also be used in subsequent manuscripts. Full-text databases such as PubMed Central , NIH RePORT , and CDC Stacks  can be useful when performing literature reviews. Authors should familiarize themselves with databases that are accessible through their institution and any assistance that may be available from reference librarians or interlibrary loan systems. Using citation management software is one way to establish and maintain a working reference list. Authors should clearly understand the distinction between primary and secondary references, and ensure that they are knowledgeable about the content of any primary or secondary reference that they cite.\n\n【31】Review of the literature may continue while organizing the material and writing begins. One way to organize material is to create an outline for the paper. Another way is to begin drafting small sections of the article such as the introduction. Starting a preliminary draft forces authors to establish the scope of their analysis and clearly articulate what is new and novel about the study. Furthermore, using information from the study protocol or proposal allows authors to draft the methods and part of the results sections while the study is in progress. Planning potential data comparisons or drafting “table shells” will help to ensure that the study team has collected all the necessary data. Drafting these preliminary sections early during the writing process and seeking feedback from co-authors and colleagues may help authors avoid potential pitfalls, including misunderstandings about study objectives.\n\n【32】The next step is to conduct the study or analyses and use the resulting data to fill in the draft table shells. The initial results will most likely require secondary analyses, that is, exploring the data in ways in addition to those originally planned. Authors should ensure that they regularly update their methods section to describe all changes to data analysis.\n\n【33】After completing table shells, authors should summarize the key finding of each table or figure in a sentence or two. Presenting preliminary results at meetings, conferences, and internal seminars is an established way to solicit feedback. Authors should pay close attention to questions asked by the audience, treating them as an informal opportunity for peer review. On the basis of the questions and feedback received, authors can incorporate revisions and improvements into subsequent drafts of the manuscript.\n\n【34】The relevant literature should be revisited periodically while writing to ensure knowledge of the most recent publications about the manuscript topic. Authors should focus on content and key message during the process of writing the first draft and should not spend too much time on issues of grammar or style. Drafts, or portions of drafts, should be shared frequently with trusted colleagues. Their recommendations should be reviewed and incorporated when they will improve the manuscript’s overall clarity.\n\n【35】For most authors, revising drafts of the manuscript will be the most time-consuming task involved in writing a paper. By regularly checking in with coauthors and colleagues, authors can adopt a systematic approach to rewriting. When the author has completed a draft of the manuscript, he or she should revisit the key take-home message to ensure that it still matches the final data and analysis. At this point, final comments and approval of the manuscript by coauthors can be sought.\n\n【36】Authors should then seek to identify journals most likely to be interested in considering the study for publication. Initial questions to consider when selecting a journal include:\n\n【37】*   Which audience is most interested in the paper’s message?\n\n【38】*   Would clinicians, public health practitioners, policy makers, scientists, or a broader audience find this useful in their field or practice?\n\n【39】*   Do colleagues have prior experience submitting a manuscript to this journal?\n\n【40】*   Is the journal indexed and peer-reviewed?\n\n【41】*   Is the journal subscription or open-access and are there any processing fees?\n\n【42】*   How competitive is the journal?\n\n【43】Authors should seek to balance the desire to be published in a top-tier journal (eg, Journal of the American Medical Association, BMJ, or Lancet) against the statistical likelihood of rejection. Submitting the paper initially to a journal more focused on the paper’s target audience may result in a greater chance of acceptance, as well as more timely dissemination of findings that can be translated into practice. Most of the 50 to 75 manuscripts published each week by authors from the Centers for Disease Control and Prevention (CDC) are published in specialty and subspecialty journals, rather than in top-tier journals .\n\n【44】The target journal’s website will include author guidelines, which will contain specific information about format requirements (eg, font, line spacing, section order, reference style and limit), authorship criteria, article types, and word limits for articles and abstracts.\n\n【45】We recommend returning to the previously drafted abstract and ensuring that it complies with the journal’s format and word limit. Authors should also verify that any changes made to the methods or results sections during the article’s drafting are reflected in the final version of the abstract. The abstract should not be written hurriedly just before submitting the manuscript; it is often apparent to editors and reviewers when this has happened. A cover letter to accompany the submission should be drafted; new and useful findings and the key message should be included.\n\n【46】Before submitting the manuscript and cover letter, authors should perform a final check to ensure that their paper complies with all journal requirements. Journals may elect to reject certain submissions on the basis of review of the abstract, or may send them to peer reviewers (typically 2 or 3) for consultation. Occasionally, on the basis of peer reviews, the journal will request only minor changes before accepting the paper for publication. Much more frequently, authors will receive a request to revise and resubmit their manuscript, taking into account peer review comments. Authors should recognize that while revise-and-resubmit requests may state that the manuscript is not acceptable in its current form, this does not constitute a rejection of the article. Authors have several options in responding to peer review comments:\n\n【47】1.  Performing additional analyses and updating the article appropriately\n\n【48】2.  Declining to perform additional analyses, but providing an explanation (eg, because the requested analysis goes beyond the scope of the article)\n\n【49】3.  Providing updated references\n\n【50】4.  Acknowledging reviewer comments that are simply comments without making changes\n\n【51】In addition to submitting a revised manuscript, authors should include a cover letter in which they list peer reviewer comments, along with the revisions they have made to the manuscript and their reply to the comment. The tone of such letters should be thankful and polite, but authors should make clear areas of disagreement with peer reviewers, and explain why they disagree. During the peer review process, authors should continue to consult with colleagues, especially ones who have more experience with the specific journal or with the peer review process.\n\n【52】There is no secret to successful scientific writing and publishing. By adopting a systematic approach and by regularly seeking feedback from trusted colleagues throughout the study, writing, and article submission process, authors can increase their likelihood of not only publishing original research articles of high quality but also becoming more scientifically productive overall.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3053612d-7ed2-463c-8a4d-1ad6786305b8", "title": "Owl on a Tree Trunk and Two Robins", "text": "【0】— Utamaro Kitagawa\n\n【1】Utamaro Kitagawa . Owl on a Tree Trunk and Two Robins. Reprinted with permission of the Claude Monet Museum, Giverny, France.\n\n【2】This issue was originally published without an accompanying cover story.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e60f7f92-abda-4857-9d6c-0aa3b80a7f5d", "title": "Pandemic (H1N1) 2009 Virus in 3 Wildlife Species, San Diego, California, USA", "text": "【0】Pandemic (H1N1) 2009 Virus in 3 Wildlife Species, San Diego, California, USA\n**To the Editor:** The influenza A pandemic (H1N1) 2009 virus rapidly created a global pandemic among humans and also appears to have strong infectivity for a broad range of animal species . The virus has been found repeatedly in swine and has been detected in a dog, cats, turkeys, and domestic ferrets and in nondomestic animals, including skunks, cheetahs, and giant anteaters . In some cases, animal-to-animal transmission may have occurred, raising concern about the development of new wildlife reservoirs .\n\n【1】In 2009, the first recognized occurrence of pandemic (H1N1) 2009 in southern California in April was followed by a surge of cases during October through November . During this time, respiratory illness developed in a 12-year-old male American badger ( _Taxidea taxus taxus_ ), a 19-year-old female Bornean binturong ( _Arctictis binturong penicillatus_ ), and a 7-year-old black-footed ferret ( _Mustela nigripes_ ) housed in a San Diego zoological garden.\n\n【2】The 3 affected animals had clinical signs that included lethargy, inappetance, dyspnea, nasal discharge, and coughing. The severity of disease in the badger and binturong necessitated euthanasia; the ferret recovered with antibiotic and fluid therapy. Postmortem examination revealed bronchopneumonia with diffuse alveolar damage in the badger and interstitial pneumonia with diffuse alveolar damage in the binturong. Bacterial cultures and Gram stains of affected lung samples were negative.\n\n【3】Molecular analyses for several groups of viruses, including _Herpesviridae_ , _Paramyxoviridae_ , _Adenoviridae_ , and all influenza A viruses, were performed on frozen lung samples from the badger and binturong and on frozen conjunctival and pharyngeal swabs from the ferret. Results of PCRs specific for segments of influenza A nucleoprotein, matrix protein, hemagglutinin, and neuraminidase genes were positive in samples from all 3 animals, and DNA sequencing of amplicons identified the viruses as pandemic (H1N1) 2009. Influenza A virus was not detected in samples from the ferret after it recovered. Results of PCRs for all other viruses were negative. Immunohistochemical evaluation of lung samples from the badger for antigens of influenza A virus  showed rare staining in bronchiolar epithelial cells .\n\n【4】Respiratory disease in all 3 affected animals seemed to be caused by pandemic (H1N1) 2009 virus. The badger and binturong were generally healthy, no other pathogens were detected, and pulmonary lesions were consistent with influenza pneumonia. In these animals, pandemic (H1N1) 2009 infection was especially aggressive, resulting in irreversible disease. Reports of pandemic (H1N1) 2009 virus in skunks and anteaters also describe severe disease in those species .\n\n【5】In contrast, the infected black-footed ferret in our study had relatively mild clinical illness, consisting only of lethargy. This finding was surprising given recent experimental studies that reported the current pandemic (H1N1) 2009 virus was more pathogenic in domestic ferrets ( _Mustela putorius furo_ ) than typical seasonal influenza viruses . However, several factors could have resulted in the low level of disease in this animal, such as prior immunity to influenza viruses or a low exposure dose. It is also possible black-footed ferrets are innately more resistant to influenza infection than domestic ferrets.\n\n【6】The origin of infection in these cases was not determined but was most likely an infected human. All animals had some level of contact with caretakers or veterinarians and were housed separately from other wildlife species. None of the potential human sources of virus had clinical signs before the animals became ill; however, influenza infections in humans can often be mild . Wild animals, such as opossums and skunks, that occasionally enter the zoological garden, represent another possible source. Good hygiene and husbandry practices used within the enclosures of the badger, binturong, and ferret failed to prevent infection, which suggests pandemic (H1N1) 2009 is efficiently transmitted to these species. Descriptions of infection in giant anteaters and cheetahs kept under similar conditions also support high transmissibility of influenza A viruses to animals, as do ongoing findings for swine .\n\n【7】Although ferrets are known to be susceptible to influenza A virus, to our knowledge, influenza in badgers and binturongs has not been reported. Badgers and binturong have been housed in zoological gardens for decades without incidence of influenza. Increased surveillance for influenza by the scientific community during the pandemic may have resulted in the novel recognition of infection in these species. Alternatively, the current pandemic (H1N1) 2009 virus may have a broader host range and stronger virulence than viruses in the past.\n\n【8】Pandemic (H1N1) 2009 was first detected in humans in March 2009 and reached pandemic levels by June of that year, rapidly establishing a rich pool for the development of genetic variants. Naturally acquired disease has now been described in 10 animal species, and experimental infection has been reported in an additional 2 animals (mice and cynomolgus macaques) . The ubiquity of pandemic (H1N1) 2009 and its ability to infect a diverse range of hosts is worrisome for the health of wildlife and for the possibility of creating additional reservoirs that could alter the evolution of subtype H1N1 viruses by applying varied selection pressures and establishing new ways of generating unique reassortant strains.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6d95b55f-b5a0-43dd-babf-44ab4b555cd3", "title": "Bacteremia Caused by Mycobacterium wolinskyi", "text": "【0】Bacteremia Caused by Mycobacterium wolinskyi\n**To the Editor:** _Mycobacterium wolinskyi_ is a rapidly growing mycobacterium that belongs to the _M. smegmatis_ group, which includes _M. smegmatis_ sensu stricto and 2 species described in 1999 ( _M. goodii_ and _M. wolinskyi_ ) . Only 9 cases of infection caused by _M._ _wolinskyi_ have been reported , and these included 3 cases of bone infection and 1 case of infection of a hip prosthesis. All patients had a history of surgery after traumatic injury and all specimens were isolated from the surgical wound. In our study, we used molecular diagnostic tools and report a case of bacteremia caused by _M. wolinskyi_ .\n\n【1】In November 2006, we diagnosed non-Hodgkin lymphoma in a 22-year-old woman. A venous port was implanted, and 4 courses of rituximab (anti-CD20 monoclonal antibody) plus additional chemotherapy (cyclophosphamide, epirubicin, vicristine and prednisolone) were administered from December 2006 through May 2007. No unfavorable sequelae occurred after chemotherapy, and the tumor showed a complete response. In August 2007, we admitted the patient to our hospital because of a spiking high fever (up to 40°C), chills, and pain in the left knee. On physical examination, the patient had a tender, warm, erythematous, and swollen left knee. These symptoms progressed to other joints, including the left hip and ankle.\n\n【2】Laboratory data showed a normal leukocyte count (3.4 × 10 9  cells/L). The patient’s C-reactive protein level increased from 1.13 mg/dL (on the day of admission) to 24.95 mg/dL (7 days after admission). We drew 2 sets of blood samples from a peripheral vein for culture and incubated these cultures (BACTEC 9240 Continuous Monitoring Blood Culture System; Becton Dickinson, Sparks, MD, USA) using BACTEC Aerobic Plus and Anaerobic Plus medium (Becton Dickinson). Within 3 days, the cultures tested positive for acid-fast bacilli.\n\n【3】The isolate was identified by 16S rRNA gene amplification of an 880-bp region (corresponding to positions 27–907), as previously described . For amplification, we used broad-range primers 16S-27f (5′-AGA GTT TGA TCM TGG CTC AG-3′) and 16S-907r (5′-CCG TCA ATT CMT TTR AGT TT-3′). For sequencing 16S rDNA, we used either the primer 16S-27f or 16S-519r (5′-GWA TTA CCG CGG CKG CTG-3′). We performed both forward and reverse (5′ and 3′) sequencing. For accurate analysis of the data, a 492-bp variable region (corresponding to positions 27– 519) was carefully analyzed after it was compared with sequences of _Mycobacterium_ spp. in the BLAST database , as described . The results showed 99% similarity between our isolate and _M. wolinskyi_ .\n\n【4】A few days later, we obtained synovial fluid by needle biopsy and cultured samples in BACTEC Aerobic Plus and Anaerobic Plus medium (Becton Dickinson) and on trypticase soy agar. Within 3 days, these cultures were also positive for _M. wolinskyi_ . Arthroscopically assisted arthrocentesis and debridement showed a turbid joint and the debrided tissue showed inflammatory processes within the synovial tissue and the presence of acid-fast bacilli . We grew cultures of acid-fast bacilli on trypticase soy agar after 2 to 4 days. The colonies were nonchromogenic, smooth to mucoid, and off-white to cream on Middlebrook 7H10 and trypticase soy agar.\n\n【5】We tested the in vitro antimicrobial susceptibility using the broth dilution method . The isolate susceptible to amikacin, cefoxitin, imipenem, doxycycline, and ciprofloxacin and resistant to sulfamethoxazole, clarithromycin, and tobramycin. We initiated treatment of the patient with moxifloxacin, minocycline, and amikacin 1 day after the athroscopy and the patient’s fever subsided within 72 hours. We continued amikacin therapy for 1 month and administered moxifloxacin and minocycline for 6 months.\n\n【6】This patient is unique because she had a case of bacteremia caused by _M. wolinskyi,_ and she had no history of major traumatic injury. The bacterium might have been introduced during implantation of the venous port or during minor trauma that went unnoticed. The chemotherapeutic regimen administered to our patient may have played a role in the infection. Immunosuppression by treatment with rituximab (an anti-CD20 monoclonal antibody) and a steroid during chemotherapy may have worsened the patient’s B-cell function and thereby weakened her immunity _._ Surgical debridement followed by antimicrobial therapy for at least 6 months is the suggested treatment for _M. wolinskyi_ infection, and we followed this regimen. Because of the frequency of relapse and resistance, we used combination therapy with multiple antimicrobial agents.\n\n【7】This case suggests that immunocompromised patients may be vulnerable to infection by rapidly growing mycobacterium such as _M. wolinskyi_ . In such cases, we suggest antimicrobial drug treatment, based on in vitro susceptibility. More data on antimicrobial drug susceptibility should be collected for treatment of this type of infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7d3b343e-24c8-42cf-9e8e-07c97ed6b570", "title": "Addressing Racial and Ethnic Disparities in COVID-19 Among School-Aged Children: Are We Doing Enough?", "text": "【0】Addressing Racial and Ethnic Disparities in COVID-19 Among School-Aged Children: Are We Doing Enough?\nAbstract\n--------\n\n【1】The disproportionate impact of COVID-19 and associated disparities among Hispanic, non-Hispanic Black, and non-Hispanic American Indian/Alaska Native children and teenagers has been documented. Reducing these disparities along with overcoming unintended negative consequences of the pandemic, such as the disruption of in-person schooling, calls for broad community-based collaborations and nuanced approaches. Based on national survey data, children from some racial and ethnic minority groups have a higher prevalence of obesity, asthma, type 2 diabetes, and hypertension; were diagnosed more frequently with COVID-19; and had more severe outcomes compared with their non-Hispanic White (NHW) counterparts. Furthermore, a higher proportion of children from some racial and ethnic minority groups lived in families with incomes less than 200% of the federal poverty level or in households lacking secure employment compared with NHW children. Children from some racial and ethnic minority groups were also more likely to attend school via online learning compared with NHW counterparts. Because the root causes of these disparities are complex and multifactorial, an organized community-based approach is needed to achieve greater proactive and sustained collaborations between local health departments, local school systems, and other public and private organizations to pursue health equity. This article provides a summary of potential community-based health promotion strategies to address racial and ethnic disparities in COVID-19 outcomes and educational inequities among children and teens, specifically in the implementation of strategic partnerships, including initial collective work, outcomes-based activities, and communication. These collaborations can facilitate policy, systems, and environmental changes in school systems that support emergency preparedness, recovery, and resilience when faced with public health crises.\n\n【2】Introduction\n------------\n\n【3】The population health impact of COVID-19 has exposed decades, if not centuries, of inequities that have systematically undermined the physical, social, material, and emotional health of racial and ethnic minority groups . The disproportionate impact of COVID-19 and associated disparities in outcomes among some racial and ethnic minority populations is documented across age groups, including among children . Reducing these disparities along with the inequitable economic and social impact of the pandemic on families from racial and ethnic minority groups requires broad community-based and underused collaborations, as well as innovative approaches.\n\n【4】In this article, we highlight health disparities and inequities among children and teenagers from racial and ethnic minority groups. We discuss education as a major social determinant of health and the impact of restricted access to in-person school, and we describe disparities in underlying chronic medical conditions and social inequities associated with poverty and systemic disadvantage. In combination, these factors exacerbate poor health outcomes in populations disproportionately affected by social conditions beyond their control, including infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes COVID-19. In light of documented disparities and the potential collateral damage inflicted by the COVID-19 pandemic, such as delays in health care, increases in social isolation, and decreases in vaccination rates , we invoke a renewed sense of urgency in attending to the population health status of children and teens from racial and ethnic minority groups.\n\n【5】We argue for community-based approaches that are organized to achieve greater proactive and sustained collaborations between local health departments, local school systems, and other public and private organizations. Although these approaches are not new, the impact of the COVID-19 pandemic on school-aged children experiencing systemic disadvantage reintroduces and amplifies the need for community-based collaborations and strategic partnerships. Since the causes of health disparities are complex and multifactorial, eliminating these health disparities cannot be accomplished by a single sector or entity . As trusted members of their communities, partners from the public and private sectors can play a key role in improving population health . Revitalizing old partnerships and establishing new collaborations may reduce COVID-19 disparities; improve and protect the physical and mental health of children and teens from racial and ethnic minority groups experiencing systemic disadvantage; and advance health equity, which is the opportunity for everyone to be as healthy as possible . We posit that these partnerships and collaborations can facilitate policy, systems, and environmental changes within school systems that support emergency preparedness and recovery and resilience when faced with public health crises.\n\n【6】Education is a Major Social Determinant of Health\n-------------------------------------------------\n\n【7】Education is a major social determinant of health and is essential to achieving health equity . Educational attainment and disparities in health are closely linked . Moreover, education is highly correlated with income and occupation, and “less education predicts earlier death” . Furthermore, through occupational attainment, education most often determines access to health care and health-related benefits, including paid time off and paid sick leave . Adults with less education report worse general health, more chronic conditions, and more functional limitations than those with higher levels of education .\n\n【8】School closures (during the spring of 2020) in response to the COVID-19 pandemic, including in-person kindergarten through grade 12 (K-12) schools, and the safe reopening of schools and institutions of higher education have been at the center of public health decision making to ensure guidelines protect students, teachers, faculty, and staff. According to Fay et al, along with the economy and the health care system, schools are “a third pillar of a pandemic-resilient society” . Besides education, schools provide facility-based services on which many students and families rely, including academic intervention supports, food and nutrition programs, childcare, after-school support, and social, physical, and mental health services. Schools can also serve as an extension of the home environment and offer a protective social environment for some students . As enduring community institutions, services provided by schools to “communities made vulnerable by systemic racism, inadequate insurance, family instability, environmental toxicity, and poorly paid jobs”  are essential to the overall well-being and psychosocial health of students experiencing poverty and systemic disadvantage . In this article, students experiencing systemic disadvantage include those who are disproportionately from racial and ethnic minority groups, with disabilities, experiencing homelessness, in foster care, and for whom English is a second language .\n\n【9】Children who experience poverty and systemic disadvantage, who are more likely to be from racial or ethnic minority groups, may be at higher risk of infection, severe illness, and death from COVID-19 . Because of school closures, children already experiencing disadvantage may have limited access to the facility-based services, academic supports (eg, private instructors, learning pods), virtual learning options, and digital technologies needed to successfully complete their grade-level academic requirements . Parents’ lives are also impacted by school closures. Some parents may not be able to return to work or may not have paid leave, flexible schedules, or options to work remotely and may have to balance how they provide supervision to several children at home with different academic schedules . Parents’ comfort with their own educational attainment and confidence to help their children academically is also a factor in how well students perform . These challenges may widen the gap in academic performance for students experiencing disadvantage compared with their more privileged counterparts. Conversely, in-person learning during the pandemic may pose challenges for these children and their families. Schools that serve communities with a disproportionate number of people experiencing poverty are often under-resourced, overcrowded, and understaffed, which increases risk for COVID-19 transmission in schools and adds to the challenges associated with safe reopening . Furthermore, some students may live with others who, for various reasons, are at increased risk for COVID-19 infection or live in intergenerational or crowded housing, which may influence their parent’s or guardian’s decision to send them to in-person school. Under-resourced schools may also have reduced capacity to offer high-quality virtual learning or be able to provide the supports needed for students with disabilities or other special needs. To mitigate the impacts of COVID-19 on the scope and quality of educational resources available to these students, decision makers, programs, and interventions must consider health disparities and social inequities and act on the unique conditions that could increase students’ risk of infection and severe illness from COVID-19.\n\n【10】Disparities in Underlying Medical Conditions and Social Determinants of Health\n------------------------------------------------------------------------------\n\n【11】As of May 2021, the US continues to experience substantial levels of SARS-CoV-2 transmission. Although less common than in adults, children and teens are still at risk of developing severe illness and complications from COVID-19; approximately 1 in 3 children hospitalized with COVID-19 were admitted to the intensive care unit, similar to the rate among adults . Although evidence on which medical conditions in children are associated with increased risk is limited, children with the following conditions might be at increased risk for severe COVID-19: obesity, diabetes, asthma, other chronic lung disease, congenital heart disease, medical complexity, severe genetic disorders, sickle cell disease, chronic kidney disease, severe neurologic disorders, inherited metabolic disorders, and immunosuppression due to malignancy or immune-weakening medications .\n\n【12】Serious racial and ethnic health and health care inequities persist for children with chronic health conditions . National estimates indicate significant disparities in the prevalence of chronic disease conditions that may place some children and teens at increased risk for severe illness from COVID-19 . Nearly 1 in 5 children aged 2–19 years (19.3) in the United States have obesity . The prevalence of obesity among Mexican American (26.9), Hispanic (25.6), and non-Hispanic Black (NHB; 24.2) children was higher than among non-Hispanic White (NHW; 16.1) and non-Hispanic Asian (8.7) children during 2017-2018. Children who have obesity are more likely to have risk factors for adult cardiovascular disease , including high blood pressure and high cholesterol, increased risk of impaired glucose tolerance, insulin resistance, and type 2 diabetes, as well as asthma  and sleep apnea . Racial and ethnic disparities also are evident in asthma, diabetes, and cardiovascular disease. In 2018, the prevalence of current asthma among NHB (14.2), Hispanic overall (8.0), and Mexican American (7.0) children was higher than among NHW (5.6) children younger than 18 years .\n\n【13】The SEARCH for Diabetes in Youth Study has reported disparities in the incidence of type 2 diabetes per 100,000 among children aged 10–19 years. During 2014–2015, NHB (37.8), American Indian (32.8), Hispanic (20.9), and Asian/Pacific Islander (11.9) children had higher incidence rates of type 2 diabetes than NHW (4.5) children  . Additionally, using 2013–2016 data from the National Health and Nutrition Examination Survey (NHANES), Jackson et al reported that among children aged 12–19 years, the estimated prevalence of hypertension (≥130/80 mm Hg) was 4.2. However, the prevalence for NHB (6.3) and Mexican American (4.9) children was higher than among NHW children (3.0) . According to Lopez et al, mortality rates resulting from congenital heart disease significantly declined during 1999–2017 among all races/ethnicities, although disparities in mortality rates persisted among NHB children in comparison with NHW children; the highest mortality rate was in infants (<1 year) of all races/ethnicities . Improvements in cardiovascular health have not been equally shared by US children aged 12–19 years of varying socioeconomic status. A study using NHANES data reported increases in the prevalence of obesity among only adolescents from low-income (18.1–21.7) and middle-income (17.1–26.0) households from 1999 to 2014. During 2011–2014, significant disparities in prevalence of obesity were observed between adolescents from low-income and high-income households (21.7 vs 14.6). Although no significant disparities were observed in children aged 12–19 years in the prevalence of prediabetes, diabetes, hypertension, or hypercholesterolemia, the prevalence of prediabetes and diabetes increased (21.4–28.0) among adolescents from low-income households during 1999–2014 .\n\n【14】Approximately 42 of children hospitalized with COVID-19 during March 1 through July 25, 2020, had 1 or more underlying medical conditions . The most prevalent conditions among these children were obesity (37.8) and chronic lung disease, including asthma (18.0). For hospitalized children aged 5–17 years, Hispanic (42.3) and NHB (32.4) children had a higher prevalence of underlying conditions compared with NHW children (14.1); Hispanic (47.2) and NHB (31.8) children also had higher hospitalization rates than NHW children (12.6) .\n\n【15】The families of children experiencing systemic disadvantage likely share similar COVID-associated health risks and, therefore, may be more likely to be hospitalized or die if they contract COVID-19 . As of November 30, 2020, compared with NHW individuals, hospitalization rate ratios were 4 times higher among non-Hispanic American Indian or Alaska Native people and Hispanic or Latino people, and 3.7 times higher among NHB or African American people . Likewise, deaths were 2.8 times higher for NHB or African American people and Hispanic or Latino people, and 2.6 times higher for non-Hispanic American Indian or Alaska Native people compared with NHW people . Family and household members may be at increased risk of exposure to COVID-19 through their occupation . Parents then play an important role in ensuring strict adherence to established mitigation measures by everyone in the household .\n\n【16】Inequities in Social Determinants of Health\n-------------------------------------------\n\n【17】Racial or ethnic minority populations are more likely to experience lower socioeconomic status, live in crowded housing, and possibly be employed in occupations that require in-person work . Furthermore, access to health care may be limited, including obtaining testing and care for COVID-19 . Compared with NHW (26%) and Asian/Pacific Islander children (25%), a larger proportion of NHB (58%), American Indian (56%), and Hispanic (53%) children younger than 18 years lived in families with incomes less than 200% of the federal poverty level in 2019 . Compared with NHW (21%) and Asian/Pacific Islander children (21%), a larger proportion of NHB (41%), American Indian (44%), and Hispanic (31%) children’s parents lack secure employment . In addition, Hispanic and NHB children, regardless of their families’ income, are more likely than NHW or Asian children to attend schools with a high proportion of students from families with incomes below the federal poverty level .\n\n【18】School districts that serve a high proportion of students who are from racial and ethnic minority populations and students who are from families with lower incomes receive less state and local funding than schools that serve a lower proportion of these groups . School funding determines the availability of student supports, classroom sizes, and a myriad of other factors that can affect student learning . Under-resourced schools may be unable to sufficiently address students’ academic, social, emotional, and mental health needs that were exacerbated by the COVID-19 pandemic without support from community institutions and resources, including public health. However, in light of new federal funding through the American Rescue Plan , these school districts have a new opportunity to invest in meaningful and productive partnerships.\n\n【19】In addition to the potential for overcoming educational inequities, promotion of resilience may prevent or ameliorate the impacts of social adversity on children. Evidence suggests that specific individual (eg, cognitive skills, emotion regulation, self-esteem), relational (eg, relationships with caregivers), and school factors (eg, academic engagement) are associated with resilience . Factors that promote resilience can be considered at multiple levels (eg, individual, family, environmental) and are complimentary to public health efforts .\n\n【20】Community-Based Approaches to Reducing COVID-19 Disparities\n-----------------------------------------------------------\n\n【21】Understanding the social context of populations with high rates of COVID-19 infection and severe illness is critical to the development, implementation, and evaluation of public health prevention strategies. Although structural long-term solutions to eliminating racial and ethnic health disparities are optimal and preferred , evidence suggests that immediate relief and support during the COVID-19 pandemic can be achieved when local public health departments, school leaders, and community partners join forces. For example, the Coordinated Approach to Child Health: Curriculum & Training (CATCH) program consists of comprehensive and coordinated programs, policies, and services that involve partnerships between families, schools, and the community . This school health program focuses on coordinating the efforts of teachers, school staff, and the community to promote healthy behaviors to prevent childhood obesity. Through this approach, programs had greater impact in reducing overweight and obesity when schools worked with community-based partners . Using a coordinated approach can impact the way communities conceptualize and address problems and can enhance implementation of strategies . This approach may help address the unique challenges some children face throughout the pandemic and support transitions into early pandemic recovery and beyond.\n\n【22】Another way to inform focused prevention strategies is for school districts to develop plans that can be tailored at the individual school level to address gaps in learning and well-being for the students. According to a study by researchers at Johns Hopkins University, most state and territorial boards of education (89%, 48 of 54) have individual plans with provisions to narrow gaps in learning and well-being that may have been exacerbated by school closures for children experiencing poverty and systemic disadvantage . Some of these provisions include providing access to digital technologies and corresponding training and support for students and parents; special virtual instructional support (eg, tutoring); prioritization of children experiencing disadvantage for in-class instruction; and accommodation of schedule-related or childcare needs of parents with lower income, people of color, or essential workers. Because states and school districts may have implemented their reopening plans differently, partnerships and collaboration with public health departments and community-based organizations could help with monitoring the execution and reach of those plans as well as assessing critical needs to ensure that equity considerations are implemented. Examining these provisions can inform models and standards to use during the COVID-19 pandemic and for emergency preparedness planning.\n\n【23】Plans should be comprehensive and consider disparities in conditions that could affect educational achievement, including mental health and emotional well-being, within the context of the COVID-19 pandemic. For example, compared with 2019, the proportion of mental health–related visits for children aged 5–11 and 12–17 years increased approximately 24% and 31%, respectively; these increases began in April 2020, corresponding to the time in which many schools were required to close . Younger adults (aged 18–24 y), Hispanic people, NHB people, essential workers, and unpaid caregivers for adults reported having experienced disproportionately more adverse mental health outcomes .\n\n【24】Zimmerman et al found what they describe as “nuanced contextual covariables in our society that provide a fuller back story” to the complex association between educational attainment and health outcomes . Namely, they identified social skills, emotional dysregulation, trauma, abuse, and neglect, among other variables that should be addressed when the goal is to increase educational attainment. Moreover, Hahn and Truman argue that another essential element in the pathway from educational attainment to health outcomes is the “psychosocial environment,” which includes sense of control (eg, work-related factors, health-related behaviors, stress), social standing (social and economic resources, stress), and social support (social and economic resources, health behaviors, family stability, stress) . If these variables require attention absent a global pandemic, then they cannot be ignored during this public health crisis. Partnerships can facilitate obtaining resources to promote coping and resilience, reduce health and mental health disparities, and expand access to services to support children’s and teens’ mental health. For example, schools could help link children and their families to community health centers for affordable mental health support services.\n\n【25】Implementing Strategies to Advance Health Equity Through Partnerships\n---------------------------------------------------------------------\n\n【26】Community-based public and private sector partnerships are a cornerstone of community health promotion, chronic disease prevention, and a range of health equity initiatives. In addressing COVID-19 disparities and consequent social and health inequities, we borrow from the evidence base and experience of other public health interventions. Dicent Taillepierre and colleagues identified several elements in program design that enhance health equity, including consideration of sociodemographic characteristics, understanding the evidence base for reducing health disparities, leveraging multisectoral collaboration, using clustered interventions, engaging communities, and conducting rigorous planning and evaluation . Considering these elements and other experiences that support the benefit of community-based partnerships, we propose immediate actions that can be taken to respond to the pandemic, as well as to establish and track outcomes .\n\n【27】We propose 4 evidence-based approaches to form community-based partnerships, including initial collective work, outcome-based activities, and communication efforts, that collaborators can use to improve health equity among students from racial and ethnic minority groups . First, education departments should identify organizations with the mission and expertise to support tailored efforts to ameliorate education inequities among children and teens who are experiencing systemic disadvantage and falling behind academically. Multiple sectors and community actors such as clergy and faith-based organizations, YMCA, YWCA, Boys & Girls Clubs of America, Head Start programs, federally qualified community health centers, and parent–teacher associations can be effective community-based partners to protect students and support access to equitable education .\n\n【28】Relevant community partners can supplement available resources and sponsor critical activities to meet students’ unique needs . Participating community-based organizations should be aware of the characteristics of a community, including language, race, ethnicity, countries of origin, and other factors that could affect health status, access to health care, and the provision of culturally and linguistically responsive prevention messages .\n\n【29】Second, to facilitate successful collaborations, initial collective work by partners is needed to define the problem and create a shared vision to achieve specific outcomes. Assessments to inform policy, systems, and environmental change are needed. These assessments can include public health data describing the impact of COVID-19 in the community of interest, particularly among children and teens enrolled in school; school system equity plans to mitigate exposure and transmission of COVID-19; and reviews of the school system’s digital learning capacities. Place-based approaches can align community members, businesses, institutions, and others in a collaborative and participatory process to address health and contextual factors influencing the social well-being of children within a defined community . For example, these efforts planned with community members offer an opportunity to strategically assess and monitor trends in population health status and the needs and assets of a community.\n\n【30】Third, it is important for partners to take the lead or facilitate activities that focus on outcomes they have the capacity to achieve. For example, the Boys & Girls Club of America can provide tutoring services and other extra-curricular activities to minimize academic delays and poor performance on standardized tests. Later, rigorous program evaluations can document the effectiveness of these strategies post-pandemic .\n\n【31】Finally, communication is one of the core components for promoting and improving public health . Ongoing communication between schools, parents, and community-based organizations is essential. Particularly, a commitment to transparency is needed so that parents and the larger community are kept apprised of partnership efforts and informed when outcomes are on track. Partners can leverage various media outlets, including social media, to disseminate tailored prevention messages as well as connect students and parents to health care services. For example, existing digital platforms can be used for tutoring small groups or individual students. Telemedicine, including telehealth technologies, can be used to provide counseling to families about coping with stress. Although these evidence-based approaches are not new to public health, there are new opportunities to scale these approaches for greater reach and impact in communities disproportionately impacted by COVID-19.\n\n【32】Because of their critical role for all children and the disproportionate impact that school closures can have on those students experiencing systemic disadvantage, it is crucial that K-12 schools open safely and remain open for in-person learning . Community engagement and partnerships are foundational to public health and its core value of social justice . Partnerships can help facilitate delivery of quality virtual learning, policies, and systems changes that keep classrooms safe for in-person learning, and they can facilitate communication strategies that ensure the dissemination of scientifically sound public health prevention strategies that build community confidence in the safe reopening of schools. In addition to facilitating and sustaining in-person learning, partnerships can help prevent further exacerbation of educational inequities, support parents’ full return to work and more everyday activities in different settings, and fuel economic recovery. Because the needs, risk factors, assets, and resources vary across communities, local public health departments and school boards of education should work with local organizations that can help provide tailored support. Moreover, local organizations are more likely to be perceived as trustworthy and credible by communities . Recent federal funding opportunities can help facilitate and sustain these partnerships. The American Rescue Plan Elementary and Secondary School Emergency Relief Fund, with funds totaling $122 billion, supports efforts by states, Puerto Rico, and the District of Columbia to reopen K-12 schools safely and to equitably expand opportunity for students experiencing disadvantage . These funds can be used to implement strategies, including evidence-based interventions, to meet the social, emotional, mental health, and academic needs of students. Furthermore, the Centers for Disease Control and Prevention (CDC) is providing $10 billion to states to support COVID-19 screening and testing for K-12 teachers, staff, and students . Partnerships can leverage these funding opportunities and aid the implementation of rapid response efforts needed to facilitate learning.\n\n【33】The COVID-19 pandemic has not only exposed longstanding health and social inequities in the US but also revitalized efforts to achieve authentic community engagement in promoting mitigation efforts to end the pandemic. Partnerships between local health departments, local school systems, and other public and private organizations can offer immediate support to these children and teens during the COVID-19 pandemic and over the long term as we move into the recovery phase.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a9c0a206-2329-4d92-a15b-1d2210919690", "title": "Blastomycosis Mortality Rates, United States, 1990–2010", "text": "【0】Blastomycosis Mortality Rates, United States, 1990–2010\nBlastomycosis is a systemic infection caused by the thermally dimorphic fungus _Blastomyces dermatitidis_ that can result in severe disease and death among humans and animals. _B. dermatitidis_ is endemic to the states bordering the Mississippi and Ohio Rivers, the Great Lakes, and southern Canada; it is found in moist, acidic, enriched soil near wooded areas and in decaying vegetation or other organic material . Conidia, the spores, become airborne after disruption of areas contaminated with _B. dermatitidis_ . Infection occurs primarily through inhalation of the _B. dermatitidis_ spores into the lungs, where they undergo transition to the invasive yeast phase. The infection can progress in the lung, where the infection may be limited, or it can disseminate and result in extrapulmonary disease, affecting other organ systems .\n\n【1】The incubation period for blastomycosis is 3–15 weeks. About 30%–50% of infections are asymptomatic. Pulmonary symptoms are the most common clinical manifestations; however, extrapulmonary disease can frequently manifest as cutaneous and skeletal disease and, less frequently, as genitourinary or central nervous system disease. Liver, spleen, pericardium, thyroid, gastrointestinal tract, or adrenal glands may also be involved . Misdiagnoses and delayed diagnoses are common because the signs and symptoms resemble those of other diseases, such as bacterial pneumonia, influenza, tuberculosis, other fungal infections, and some malignancies . Accurate diagnosis relies on a high index of suspicion with confirmation by using histologic examination, culture, antigen detection assays, or PCR tests .\n\n【2】Antifungal agents, such as itraconazole for mild or moderate disease and amphotericin B for severe disease, can provide effective therapy, especially when administered early . With appropriate treatment, blastomycosis can be successfully treated without relapse; however, case-fatality rates of 4%–22% have been observed . Although spontaneous recovery can occur , case-patients often require monitoring of clinical progress and administration of drugs on an inpatient basis. Previous studies estimated average hospitalization costs for adults to be $20,000; that is likely less than the current true cost . Some reviews of outbreaks indicate a higher distribution of infection among persons of older age, male sex , black, Asian, and Native American racial/ethnic groups , and those who have outdoor occupations . Both immunocompetent and immunocompromised hosts may experience disease and death , although _B. dermatitidis_ disproportionately affects immunocompromised patients, who tend to have more rapid and extensive pulmonary involvement, extrapulmonary infection, complications, and higher mortality rates (25%–54%) .\n\n【3】Past studies have expanded the knowledge about blastomycosis through focusing on cases documented in specific immunocompromised persons and statewide occurrences or in areas in which the disease is endemic ; however, such studies may be limited for making definitive conclusions by their scope and small sample size. Much remains unknown about the public health burden of blastomycosis-related deaths in the United States. Reports suggest an increase in the number of blastomycosis cases in recent years . Clearer identification of risk factors from national data may raise awareness of blastomycosis in the United States and support adding it to the list of reportable diseases in regions where the pathogen is endemic to improve surveillance and control. In this study, we assessed the public health burden of blastomycosis-related deaths by examining US mortality-associated data and evaluating demographic, temporal, and geographic associations as potential risk factors.\n\n【4】### Methods\n\n【5】##### Data Source\n\n【6】We used publicly available multiple-cause-of-death (MCOD) data from the National Center for Health Statistics to examine blastomycosis-related deaths in the United States during 1990–2010. These data are derived from US death certificates and include information on the causes of death coded by the International Classification of Diseases, 9th and 10th Revisions (ICD-9, ICD-10), demographic variables of age, sex, and race/ethnicity, date of death, and geographic region of residence.\n\n【7】##### Case Definition\n\n【8】We defined a case-patient as deceased US resident listed in the MCOD dataset during 1990–2010 whose death certificate listed blastomycosis as the underlying or contributing cause of death. The ICD-9 code 116.0 (years 1990–1998) and ICD-10 codes B40.0–B40.9 (years 1999–2010) were used to identify blastomycosis-related deaths.\n\n【9】##### Analysis\n\n【10】To ensure more stable estimates, we aggregated data for the study period. We calculated mortality rates and rate ratios (RRs) with 95% confidence limits (CLs) by age, sex, race/ethnicity, geographic region, and year of death using a maximum likelihood analysis presuming the response variable had a Poisson distribution , and with bridged-race population estimates data and designated geographic boundaries from the US census. We computed age-adjusted mortality rates using adjustment weights from the year 2000 US standard population data. We assessed temporal trends in age-adjusted mortality rates using a Poisson regression model of deaths per person-years in the population, designating year and age group dummy variables as independent variables, and the population as the offset. We calculated the percentage change by year based on the estimated slope parameter and examined the Poisson regression models for overdispersion. We performed all analyses using SAS for Windows version 9.4 (SAS Institute Inc. Cary, NC, USA).\n\n【11】### Results\n\n【12】We identified 1,216 blastomycosis-related deaths among 49,574,649 deaths in the United States during 1990–2010. Among those 1,216 deaths, blastomycosis was reported as the underlying cause of death for 741 (60.9%), and as a contributing cause of death for 475 (39.1%). The overall age-adjusted mortality rate for the period was 0.21 (95% CL 0.20, 0.22) per 1 million person-years. Using Poisson regression, we identified a 2.21% (95% CL −3.11, −1.29) decline in blastomycosis-related mortality rates during the period .\n\n【13】##### Age\n\n【14】The mean age at death from blastomycosis was 60.8 years. Using 75 as the average age at death , we calculated that 19,097 years of potential life were lost. The mortality rates associated with blastomycosis increased with increasing age, peaking in the 75- to 84-year age group . The mean age at death from blastomycosis was significantly lower among Hispanics (p<0.01), Native Americans (p<0.01), blacks (p<0.01), and Asians (p<0.01) than among whites based on the _t_ test for difference in means.\n\n【15】##### Sex\n\n【16】Death related to blastomycosis was significantly more likely in men than in women (p<0.05). The average age at death was significantly lower for men than for women (p = 0.02) . The annual mortality rate over the period obtained from using Poisson regression declined for both men and women .\n\n【17】##### Race/Ethnicity\n\n【18】Native Americans and blacks were significantly more likely to die from blastomycosis-related complications than whites; overall, Asians and Hispanics were significantly less likely to die of blastomycosis than other groups . The annual mortality rate over the period declined among blacks and whites .\n\n【19】##### Geographic Region\n\n【20】Most (96.7%) of the blastomycosis-related deaths occurred in the southern and midwestern regions, and a small proportion of deaths occurred in the northeastern and western regions. The midwestern region had the highest mortality rate, followed by the southern, northeastern, and western regions . Percentage changes in mortality rates per year over the period, calculated by using Poisson regression, showed an increase in mortality rates in the midwestern region, and a decline in the southern region .\n\n【21】### Discussion\n\n【22】Our findings indicate that blastomycosis is a noteworthy cause of preventable death in the United States. These findings confirm the demographic risk factors of blastomycosis indicated in previous case reports and extend these to mortality rates. Blastomycosis death occurred more often among older persons than among younger persons , and more often among men than women . The age association found likely represents waning age-related immune function and higher prevalence of immunocompromising conditions. The observed sex differences in blastomycosis mortality may be attributable to differences in occupational or recreational exposures that increase risk for infection . For example, those who work outdoors involving construction, excavation, or forestry, or participate in outdoor recreational activities such as hunting , may more likely be exposed than those who principally work indoors.\n\n【23】The disproportionate burden of blastomycosis deaths sustained by persons of Native American or black race is also consistent with previous reports . Increased exposure and prevalence of infection, reduced access to health care, and genetic differences may play a role in the observed race-specific disparities in blastomycosis mortality rates . A finding of the current study is that even though persons of Asian descent are at lower risk for dying from blastomycosis than whites, those who died from blastomycosis did so at a much younger age (22.6 years younger). This disparity is even greater in the midwestern region, where Asians died at an age 27.2 years younger than did whites.\n\n【24】Consistent with the recognized geographic distribution of _B. dermatitidis_ , we found that death related to blastomycosis occurred more often among persons who resided in the midwestern or southern regions than among those in the western and northeastern regions. During the study period, the southern region showed decreases in mortality rates, and the midwestern region, which had the highest mortality rate, showed an increase in rate.\n\n【25】The use of population-based data and large numbers can provide insight, though some limitations associated with using MCOD data should be considered. First, potential underdiagnosis and underreporting of death related to blastomycosis may lead to underestimates of mortality rates and the true public health burden of blastomycosis in the United States. Low physician awareness of blastomycosis may be a contributor. Second, it was not possible to verify accuracy of recorded data or access supplemental data. For example, there may be reporting errors regarding correct race/ethnicity identification on death certificates and in population census reports. Third, we could not adjust for other possible confounders (i.e. smoking, socioeconomic factors, activity, lifestyle, occupation) because these data are not recorded on death certificates. These limitations must be considered along with our findings.\n\n【26】This study sheds light on the scope of the incidence of blastomycosis in the United States, though the true incidence may be greater than that reported here. Although _B. dermatitidis_ infection may be difficult to prevent because of its widespread distribution in areas where blastomycosis is endemic, deaths resulting from blastomycosis can be prevented with early recognition and treatment of patients with symptomatic infection. The continued incidence of blastomycosis in the United States, as indicated by the observed modest decrease in the mortality rates over the 21-year study period, calls for improvement in provider and community awareness, which may lead to including blastomycosis as a diagnostic consideration in patients with pulmonary disease refractory to treatment. Our findings, recent reports of disproportionately high infection rates among Asians , and the lack of decline in the mortality rates in the midwestern region support further investigation. We also encourage improvements in blastomycosis surveillance that involve examining trends in incident cases, hospitalization (including length of stay), timely diagnosis, and treatment to further elucidate the burden of blastomycosis in the United States.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "87de562e-428e-480e-aab0-aa3391c06e63", "title": "Resolution of Novel Human Papillomavirus–induced Warts after HPV Vaccination", "text": "【0】Resolution of Novel Human Papillomavirus–induced Warts after HPV Vaccination\nHuman papillomaviruses (HPVs), small, double-stranded DNA viruses with a circular genome of ≈8,000 bp, are assigned to different genera and species on the basis of their major capsid protein gene (L1) nucleotide sequence, which reflects their tropism (cutaneous or mucosal) and potential to induce tumors. Most HPVs belong to genera alpha (e.g. genital and wart-associated types), beta, or gamma (cutaneous types) . HPV infections are common, and the prevalence of cutaneous viral warts is 3%–5% in children . Warts, benign HPV-induced lesions, usually regress spontaneously within several months. Immunodeficiency predisposes to persistent HPV infections and the development of generalized verrucosis .\n\n【1】We report the remission of cutaneous warts of prolonged duration in an immunosuppressed patient after HPV vaccination. The study was performed according to the declaration of Helsinki; written informed consent was obtained from the patient.\n\n【2】### The Patient\n\n【3】In 1979, a 41-year-old, White woman received a diagnosis of B cell chronic lymphocytic leukemia and was treated with chlorambucil and prednisolone, followed by radiation therapy and splenectomy, resulting in a durable, complete remission of the leukemia. In October 2002, breast cancer was detected in the patient; the breast was surgically removed, and lymph node dissection was performed. Six cycles of chemotherapy were administered during November 2002–March 2003. In February 2010, after a 12-year history of slowly progressing cutaneous warts, the patient sought medical care for numerous, flat, erythematous warts that were coalescing into large plaques on her forearms, backs of hands, and fingers . Immunophenotyping revealed a markedly decreased CD4/CD8 ratio . During October 2005–December 2009, the patient received topical and ablative treatments for the warts (salicylic acid, podophyllotoxin, 5-fluoruracil cream, imiquimod 5% cream, cryosurgery, surgical curettage, electrocautery, and CO 2  laser therapy), but clinical improvement was not sustained.\n\n【4】Complete regression of cutaneous warts has been reported in persons after HPV vaccination ; thus, we vaccinated the patient with the quadrivalent HPV (qHPV) vaccine (Gardasil, Sanofi Pasteur MSD SNC, Lyon, France), which contains L1 proteins of HPV types 6, 11, 16, and 18 as virus-like particles. Three doses were given during July 2010–January 2011. The patient’s pre- and postvaccination CD4/CD8 counts did not differ substantially . In April 2011, three months after the third injection, all skin lesions had resolved , and in July 2011 and March 2012, the patient was still in complete remission.\n\n【5】### The Study\n\n【6】For virologic analyses, 20 biopsy specimens from the patient’s fingers, backs of hands, and forearms and 1 specimen each from the cheek and back were available (all were collected before the patient received the first dose of qHPV vaccine). DNA extraction and HPV typing were performed as described . Histopathologic analysis revealed features typical of benign cutaneous warts, including acanthosis, parakeratosis, and numerous koilocytes , similar to warts caused by HPV-3 . A2/A4 PCR  was used to amplify HPV DNA from all biopsy specimens obtained before vaccination. Sequences of the PCR products were analyzed by using BLASTn  and were 100% homologous to a 261-bp fragment named HPVXS2 . Three overlapping PCR fragments covering the entire genome of HPVXS2  were amplified by using Phusion HotStart II HF DNA Polymerase (Fermentas, St. Leon-Rot, Germany) and ligated into pJET1.2/blunt (Fermentas): fragment 1, XS2-M19fw 5′-GAATTGAGTCTTGCACCAGAGG-3′ and XhoIrev 5′-ATCTCGAGTCGCTGTCGCTTT-3′; fragment 2, XS2-M15fw 5′-GTATCTAGCACACGAGAAGTAC-3′ and XS2–6413rev 5′-ATGGTGTCCCCGACAACCC-3′; fragment 3, XS2–6258fw 5′-CACCATGTAAACAGACTGCGTC-3′ and XS2-M8rev 5′-ACCCAAATTGTTCTTTAAACTTACC-3′.\n\n【7】MacVector software version 12.7.3 (MacVector, Inc. Cary, NC, USA) was used to determine the organization of the predicted open reading frames (ORFs) and perform phylogenetic analyses. The results showed grouping of HPVXS2 within the alpha-2 species , and in each case, the L1 ORF was <90% homologous to the closest relative. Thus, HPVXS2 can be considered a novel HPV type .\n\n【8】An HPVXS2-specific quantitative real-time PCR was established. In brief, a 20-μL reaction contained 10 μL of LightCycler 480 Probes Master (Roche, Mannheim, Germany), 0.1 μmol/L probe no. 46 (5′-ATGGCTGC-3′) of the Universal Probe Library (Roche), 0.2 μmol/L each primers XS2-L1fw 5′-CATTTGTCAGTCTGTTTGTAAATATCC-3′ and XS2-L1rev 5′-TCTGCGCAGGTAAAAGAACA-3′, and 2 μL extracted DNA (QIAamp DNA Mini Kit; QIAGEN, Hilden, Germany). Cycling conditions were 95°C for 10 min and 45 cycles at 95°C for 10 sec, 60°C for 30 sec, and 72°C for 5 sec. Virus load was expressed as HPV DNA copies per β-globin gene copy (HPV/β-globin) . In 17 of the patient’s warts, HPVXS2 loads ranged from 903 to 99,571 (median 14,534) HPV/β-globin. Virus loads were much lower in a seborrheic keratosis from her back and a benign nevus from her cheek (1.424 and 0.012 HPV/β-globin, respectively). Two skin specimens obtained 14 months after the third qHPV vaccine dose were HPVXS2-negative.\n\n【9】To estimate the proportion of HPVXS2-positive specimens among archived, extracted DNA, we screened 62 skin warts from 17 immunocompetent and 24 immunosuppressed patients. HPVXS2-DNA was present in warts from 3 HIV-positive women. Two warts were co-infected with HPV-57, and HPVXS2 loads were low (0.0002 and 0.034 HPV/β-globin, respectively). One wart contained HPVXS2 only (virus load 6.853 HPV/β-globin). We also screened 449 swab samples collected for a previous study; the samples were of normal forehead skin from HIV-positive men and HIV-negative male controls . HPVXS2 DNA was present in 16.2% (34/210) and 0.8% (2/239) of specimens from HIV-positive and HIV-negative men, respectively (p<0.001; χ 2  test, 2-sided). Among the HIV-positive men, those with CD4 counts of <350 cells/μL were more likely than those with CD4 counts of >350 cells/μL to be HPVXS2-positive, but the difference was not statistically significant (25.0% vs. 13.5%, p = 0.101). Virus loads were low (0.014 and 0.023 HPV/β-globin) in the 2 HPVXS2-positive HIV-negative controls. Virus loads of >1.0 HPV/β-globin were found on normal skin of 10 of 34 HPVXS2-positive HIV-positive men (range 0.002–35.0 HPV/β-globin; median 0.107 HPV/β-globin; interquartile range 1.266 HPV/β-globin).\n\n【10】### Conclusions\n\n【11】HPVXS2 fragments were previously identified in hyperkeratotic benign papillomas, squamous cell carcinomas, and dermatitis on 3 renal transplant recipients at consecutive visits and in different parts of the body . In our study, HPVXS2 was identified in disseminated warts from a patient who had had a splenectomy and in benign skin warts from 3 HIV-positive patients. HPVXS2 DNA loads were well above 1.0 HPV/β-globin in the patient who had received a splenectomy and in an HIV-positive woman with HPVXS2 monoinfection. These virus loads are in line with those in warts induced by other HPV types . HPVXS2 was found more frequently on normal skin of HIV-positive than HIV-negative men. Thus, immunocompromised persons seem to have difficulties in clearing HPVXS2. However, virus loads remain low to moderate, and the infection in immunocompromised persons is clinically inapparent in most cases.\n\n【12】The patient in our study was free of warts 3 months after the last dose of qHPV vaccine, even though HPVXS2 is not closely related to the vaccine virus types . This observation correlates with published case reports , but it is still surprising, given that clinical improvement was not seen in 5 patients with HPV-6–positive condylomas who received the same vaccine . The qHPV vaccine was shown to induce type-specific humoral and cellular immune responses in an immunodeficient patient , but the mechanisms leading to regression of skin warts associated with heterologous HPV types have not been analyzed. One explanation could be that vaccination led to a general stimulation of the immune system, and innate immunity destroyed virus-infected cells.\n\n【13】We report a single observation; however, correlation does not necessarily imply causation, and a placebo effect is possible. Considering our findings, immunologic studies elucidating the mechanisms that lead to wart clearance and controlled clinical trials should be initiated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9b011f77-12fe-4b94-92f1-97644d289e6c", "title": "High Rates of Antimicrobial Drug Resistance Gene Acquisition after International Travel, the Netherlands", "text": "【0】High Rates of Antimicrobial Drug Resistance Gene Acquisition after International Travel, the Netherlands\nAntimicrobial drug resistance is a public health threat worldwide that limits clinical treatment options for bacterial infections. Most research on antimicrobial drug resistance has been focused on resistance in clinically relevant pathogenic bacteria. However, a vast and largely unexplored reservoir of resistance genes is present in nonpathogenic bacteria living in the environment or as commensal agents . Because of horizontal gene transfer (HGT) among microbes of diverse species and genera, antimicrobial drug resistance mechanisms in an organism, regardless of whether it is a pathogen, have the potential to emerge in clinically relevant pathogens . Several of such HGT interactions between clinically relevant pathogens and environmental species have been described; for example, the plasmid-mediated quinolone resistance encoding _qnrA_ gene originated from the chromosomes of the aquatic bacterium _Shewanella algae_ . Another well-known example is the extended-spectrum β-lactamase (ESBL) encoding _bla_ CTX-M  gene, which originates from chromosomal genes of environmental _Kluyvera_ species  and has emerged as the most prevalent cause of plasmid-mediated ESBL.\n\n【1】Resistance reservoirs have unpredictable and immense potential for rendering antimicrobial drugs ineffective. The human gut microbiota warrants special attention because of its high density of microorganisms and high accessibility . The gastrointestinal tract is constantly exposed to numerous bacteria from the environment, e.g. food, water, soil, other humans, or animals. These incoming bacteria often harbor antimicrobial drug resistance genes , which can be transferred to the indigenous microbial communities through HGT, where they may enrich the pool of available antimicrobial resistance elements in the gut microbiota.\n\n【2】Potential for intercontinental transfer of antimicrobial drug–resistant bacteria in the microbiota necessitates studies that focus on the antimicrobial resistance of the gut microbiome as a whole, the so-called “gut resistome,” by using culture-independent metagenomic approaches . Metagenomic approaches avoid the bias that is introduced when selective culturing is applied because ≈80% of the gut microbiota is not cultivatable .\n\n【3】Travel to geographic areas in which rates of bacteria that are resistant to antimicrobial drugs are high has been indicated as a risk factor for the acquisition of such bacteria . Studies in Australia , Sweden , and the Netherlands  have shown that international travel is a major risk factor for colonization with ESBL-producing _Enterobacteriaceae_ . Likely, these resistant strains are acquired from the environment during travel, e.g. through food consumption . Because the human intestinal microbiome will come in contact with many different bacterial species from travel-related environments, the effect of international travel on antimicrobial drug resistance is most likely limited to neither opportunistic pathogens, such as _Escherichia coli_ , nor to ESBL-encoding resistance genes.\n\n【4】In this study, we aimed to investigate the effect of international travel on the human gut resistome. By using a targeted (PCR-based) metagenomic approach, we compared the presence and relative abundance of specific resistance determinants in the entire human gut microbiome before and after international travel.\n\n【5】### Materials and Methods\n\n【6】##### Population and Design\n\n【7】Healthy long-distance travelers were recruited during November 2010–August 2012 through travel clinics  located in the southern part of the Netherlands. Travelers consenting to participate were asked to collect a fecal sample before and immediately after travel and to provide records of the duration and destination of their travel, illnesses or complaints during travel, drug use, and antimicrobial drug use within the 3 months preceding travel. The fecal samples were sent to clinics by regular mail on the same day of collection and were processed on the day of receipt. The study comprised 122 travelers.\n\n【8】The countries visited were categorized into geographic regions. These regions were Southeast Asia (Asia excluding the Indian subcontinent and the Middle East), the Indian subcontinent (Bangladesh, Bhutan, India, Nepal, Pakistan, and Sri Lanka), northern Africa (countries north of the equator), southern Africa (countries south of the equator), southern Europe, Central America, and South America.\n\n【9】##### Fecal Specimen Processing and DNA Extraction\n\n【10】Fecal samples were diluted 10-fold in peptone/water solution (Oxoid, Basingstoke, UK) containing 20% (vol/vol) glycerol (Merck, Darmstadt, Germany) and homogenized by vortexing. They were stored at −20°C until molecular analysis was performed.\n\n【11】For the extraction of metagenomic DNA, 200 μL of diluted feces was added to a 2-mL vial containing 0.5 g of 0.1 mm zirconia/silica beads (BioSpec, Bartlesville, OK, USA), 4 glass beads, 3.0–3.5 mm (BioSpec), and 1.2 mL of lysis buffer from the PSP Spin Stool Kit (Stratec Molecular, Berlin, Germany). Samples were disrupted in a Magna Lyser device (Roche, Basel, Switzerland) in 3 cycles of 1 min. at 5,500 rpm. Subsequently, metagenomic DNA was isolated from the samples by using the PSP Spin Stool Kit according to the manufacturer’s instructions. DNA was eluted in 200 μL elution buffer and stored at −20°C until further analysis.\n\n【12】##### Real-time PCR\n\n【13】Real-time PCR was performed to detect and quantify the β-lactamase–encoding genes _cfxA_ , _bla_ CTX-M  , and _bla_ NDM  ; tetracycline resistance­–encoding genes _tetM_ and _tetQ_ ; macrolide resistance–encoding gene _ermB_ ; aminoglycoside resistance­–encoding gene _aac(6_ ′ _)-aph(2′′)_ ; and quinolone resistance encoding genes _qnrA_ , _qnrB_ , and _qnrS_ . The 16S rDNA was amplified as a reference gene to normalize for the amount of bacterial DNA in the samples.\n\n【14】The 16S rDNA, _cfxA_ , _tetM_ , _tetQ_ , _ermB_ , and _aac(6′)-aph(2′′)_ targets were amplified by using a MyiQ Single-Color Real-Time PCR Detection System (BioRad, Hercules, CA, USA) in 25-μL reactions containing 12.5 μL iQ SYBR Green Supermix (BioRad) and 5-μL template DNA. Melting curves were checked for each sample to confirm amplification of the correct product. For every target, amplified PCR products of 10 random positive samples were separated by agarose gel electrophoresis to control for purity and size of the amplicons. Finally, for all genes except the 16S rDNA (because of expected heterozygous amplicons), these products were sequenced by using the PCR primers and an ABI BigDye Terminator v1.1 Cycle Sequencing Kit (Applied Biosystems, Foster City, CA, USA). Sequencing data were obtained by using an ABI 3730 DNA Analyzer (Applied Biosystems) and were analyzed by using BLAST .\n\n【15】The _bla_ CTX-M  , _bla_ NDM  , _qnrA_ , _qnrB_ , and _qnrS_ genes were amplified on a 7900HT Fast Real-Time PCR System (Applied Biosystems) in 25-μL reactions containing 12.5 μL ABsolute QPCR ROX Mix (Thermo Scientific, Waltham, MA, USA) and 10-μL template DNA. The _bla_ CTX-M  assay enables identification of the various phylogenetic groups by use of 4 probes. The probes to detect _bla_ CTX-M  groups 1 and 2 were combined in the first reaction, and the probe to detect _bla_ CTX-M  group 9 was combined with a probe to detect all groups except for the CTX-M-1 group in a second reaction. All primer and probe sequences and PCR conditions for each target are displayed in Table 1 .\n\n【16】To determine the efficiency of the PCR, cycle thresholds obtained from a series of 5 template DNA dilutions of at least 3 different samples were graphed on the y-axis versus the log of the dilution on the x-axis. For _bla_ NDM  , a clinical isolate was used because no positive fecal samples were available. The PCR efficiencies were 16S rDNA, 94.0%; _cfxA_ , 99.0%; _tetM_ , 97.6%; _tetQ_ , 95.9%; _ermB_ , 95.5%; _aac(6’)-aph(2”)_ , 97.0%; _bla_ CTX-M-1+2  , 98.2%; _bla_ CTX-M-9+2–8-9–25  , 96.7%; _bla_ NDM  , 98.4%; _qnrA_ , 97.4%; _qnrB_ , 101.0%; and _qnrS_ , 102.5%.\n\n【17】We determined PCR detection limits for _bla_ CTX-M  , _qnrB_ , and _qnrS_ . Clinical isolates harboring these genes were suspended in a 0.5 mol/U McFarland solution, then diluted 10-fold in sterile saline solution. Quantification of CFU in the suspensions was achieved by inoculating blood agar plates (Oxoid) and counting the number of colonies after overnight incubation at 37°C. Next, 20 μL of the quantified suspensions was mixed with 180 μL of feces and submitted to DNA extraction as described above. Subsequently, quantitative PCR was performed on extracted DNA to generate standard curves for quantification. For _bla_ CTX-M  , the detection limit was 12–40 CFU/PCR. For _qnrB_ and _qnrS_ , the detection limit was 1–5 CFU/PCR.\n\n【18】##### Statistical Analyses\n\n【19】We calculated differences in relative resistance gene abundances between samples from before and after travel for each traveler by using the ΔΔCt method with a Pfaffl modification to correct for PCR efficiency (ratio: Etarget^ΔCTtarget/Ereference^ΔCTreference) , which is the standard method to measure the relative change in mRNA expression levels by using real-time PCR. However, in this study, rather than measuring mRNA expression levels, the relative amount of target DNA present was measured by using this method. The 16S rDNA was used as the reference gene.\n\n【20】To better visualize increases and decreases in gene abundances in graphs, we converted abundance ratios to a fold change. To determine the overall abundance change of a resistance gene, ratios were log-transformed. A 2-tailed, 1-sample _t_ test was used to test whether the mean log ratio significantly differed from 0.\n\n【21】The number of fecal samples positive for a resistance gene after travel was compared with the positive samples obtained before travel by using the McNemar test for paired samples. Multivariable logistic regression analyses were used to test for the association between age, sex, travel destination and duration, traveler’s diarrhea, and antimicrobial drug use preceding travel (independent variables) and the acquisition of antimicrobial resistance genes (dependent variable). The association between acquisitions of multiple resistance genes was determined by a χ 2  test. All analyses were performed by using IBM SPSS Statistics version 20 . Results were interpreted as statistically significant when p was <0.05.\n\n【22】### Results\n\n【23】##### Study Population\n\n【24】The study comprised 122 travelers (71 women, 51 men) whose median age was 43 years (range 18–72 years). The median length of stay abroad was 21 days (range 5–240 days). Fourteen participants traveled for >60 days; 5 participants traveled for \\> 120 days. Most participants visited 1 country; 22 visited >1 country. Six participants visited >1 of the defined geographic regions ; 7 participants did not provide information about their destination.\n\n【25】##### Prevalence of Resistance Genes in Fecal Samples\n\n【26】After travel, samples from 5 participants contained _bla_ CTX-M  genes of 2 different phylogenetic groups. Before travel, single CTX-M variant was detected for 2 of these persons, and _bla_ CTX-M  genes were not detected for the other 3 persons. After travel, the gene was not detected in the samples of 6 persons who were positive for the _bla_ CTX-M  gene before travel. The carbapenemase-encoding gene _bla_ NDM  was not detected in any sample.\n\n【27】The prevalence of both _tetM_ and _tetQ_ was very high in the fecal samples. The _tetM_ gene was present in all samples before travel and in 121 (99.2%) samples after travel, and _tetQ_ was detected in all samples before and after travel. The prevalence of the _ermB_ gene was also high in samples both before and after travel (99.2% for both). The prevalence of the _aac(6′)-aph(2′′);_ gene was not altered by traveling; this gene was present in 79 (64.5%) of samples before travel and in 86 (70.5%) samples after travel.\n\n【28】Before travel, prevalence of the quinolone resistance genes _qnrA_ , _qnrB_ , and _qnrS_ was relatively low: 0.8%, 6.6%, and 8.2%, respectively. After travel, each of the 3 genes increased: _qnrA_ , _qnrB_ , and _qnrS_ were detected in 3.3%, 36.9%, and 55.7% of samples, respectively. _qnrB_ and _qnrS_ were significantly higher after than before travel (p<0.001).\n\n【29】##### Relative Gene Abundance Before and After Travel\n\n【30】Because the prevalence of the _cfxA_ , _tetM_ , _tetQ_ , and _ermB_ genes was very high before and after travel, we compared the relative abundance of the genes in both samples from each traveler to determine whether traveling influenced the gene abundance. For all 4 genes, the observed changes in gene abundance per traveler were distributed between increases and decreases . Determining the overall increase or decrease of the abundance of each gene showed that none of the investigated genes changed significantly (p>0.05 for all) in abundance after travel.\n\n【31】##### Effect of Travel Destination and Other Risk Factors on Gene Acquisition\n\n【32】The rate of acquisition of a _bla_ CTX-M  gene was highest for travelers visiting the Indian subcontinent (58.1%; p<0.05, OR 26.22, 95% CI 2.86–240.38) . Travel to other regions was associated with a _bla_ CTX-M  acquisition rate of 17.9% for Southeast Asia and 31.3% and 29.4% for northern and southern Africa, respectively. In the combined category comprising southern Europe, Central America, and South America, 1 _bla_ CTX-M  acquisition (6.3%) was detected in a traveler who had been to southern Europe (Turkey).\n\n【33】The acquisition of the _qnrB_ gene was not associated with travel to a specific region, whereas the acquisition of _qnrS_ was highest for Southeast Asia (75.0%; p = 0.001, OR 15.7, 95% CI 3.1–79.2), and second highest for the Indian subcontinent (61.3%; p<0.05, OR 9.2, 95% CI 1.9–43.9). The acquisition rate was also elevated for northern Africa (43.8%) and southern Africa (35.3%) but not significantly so.\n\n【34】We also investigated associations between age, sex, travel destination and duration, traveler’s diarrhea, and antimicrobial drug use preceding the travel and the acquisition of resistance genes. No associations were found .\n\n【35】##### Phylogenetic Groups of _bla_ CTX-M  Genes and Association with _qnr_ Genes\n\n【36】Of the 41 _bla_ CTX-M  genes acquired during travel, 24 belonged to the CTX-M-1 group, 2 belonged to the CTX-M-2 group, 6 were of the CTX-M-9 group, and 9 were positive for the CTX-M-2–8-9–25 probe but not for the CTX-M-2 or 9 probe, indicating that these genes were in groups 8 or 25. The CTX-M groups acquired per region are shown in Table 4 . In contrast, 9/11 CTX-M types detected in the pre-travel samples belonged to the CTX-M-9 group and 2/11 to the CTX-M-1 group.\n\n【37】Associations between the acquisitions of the _bla_ CTX-M  and _qnr_ genes were also investigated . The acquisition of a _bla_ CTX-M  gene was not associated with that of a _qnrB_ (p = 0.305) or _qnrS_ gene (p = 0.080); neither was the gain of a _bla_ CTX-M  gene of the CTX-M-1 group, which was the dominant acquired type (58.5%) associated with the acquisition of either _qnrB_ (p = 0.631) or _qnrS_ (p = 0.256).\n\n【38】### Discussion\n\n【39】We used a metagenomic approach to study effects of international travel on part of the resistome of the human gut microbiota. Our results provide insights into the prevalence of the investigated resistance genes in the human gut microbiota and demonstrate high rates of acquisition of the ESBL encoding gene _bla_ CTX-M  and quinolone resistance encoding genes _qnrB_ and _qnrS_ related to international travel. The prevalence of these genes increased from 9.0%, 6.6%, and 8.2% before travel to 33.6%, 36.9%, and 55.7% after travel, respectively.\n\n【40】Prospective cohort studies among travelers from Australia , the Netherlands , and Sweden  showed that international travel was a risk factor for colonization with ESBL-producing _Enterobacteriaceae_ spp. and that travel to India or the Indian subcontinent was the highest risk factor. These findings agree with the rates of _bla_ CTX-M  acquisition found in our study, which were highest for travelers to the Indian subcontinent.\n\n【41】The phylogenetic types of the _bla_ CTX-M  gene that were acquired in our study group were clearly dominated by CTX-M group 1, especially in the Indian subcontinent. This geographical association corresponds to the aforementioned cohort studies , which showed that ESBL-producing _Enterobacteriaceae_ identified in travelers to India or the Indian subcontinent mainly comprise CTX-M group 1. Although the statistical power of our study was insufficient to analyze the specific CTX-M groups, it was striking that genes of the CTX-M-2 group were detected twice and those of either group 8 or 25 were detected 9 times. In previous studies, these CTX-M groups were not detected at all  or were detected only sporadically . The difference in results could be caused by our use of a metagenomic approach, which might detect _bla_ CTX-M  in a much wider array of species than did studies investigating specific cultured _Enterobacteriaceae_ spp. This difference in approach might furthermore explain that of the _bla_ CTX-M  genes detected before travel in the population in our study, most (9/11, 82%) were of the CTX-M-9 group, which contrasts studies that report that _bla_ CTX-M-15  (which belongs to the CTX-M-1 group) is predominant in ESBL-producing _Enterobacteriaceae_ in the Netherlands . Aside from the different method used, the population sizes in these studies were larger than the cohort in our current study.\n\n【42】Plasmid-mediated quinolone resistance genes, such as the _qnr_ variants, provide low-level quinolone resistance. However, these genes are relevant because they facilitate the emergence of higher-level resistance and thus can speed the development and spread of resistance to these antimicrobial agents . Although foreign travel has been associated with the acquisition of plasmid-mediated quinolone resistant–positive isolates , these genes have thus far not been focused on in prospective cohort studies investigating the effects of travel on antimicrobial resistance.\n\n【43】A study by Vien et al. that investigated the prevalence of the _qnr_ genes in fecal swab samples from children in Vietnam who had acute respiratory tract infections  showed very high _qnrS_ prevalence (74.5%). Travel to areas with such a high prevalence could be a major risk factor for acquisition of these genes. Five (83%) of 6 participants in our study who had traveled to Vietnam acquired a _qnrS_ gene. In total, 11 volunteers had traveled to Cambodia, Thailand, Vietnam, or a combination of these geographically neighboring countries, and 9 (82%) acquired a _qnrS_ gene. These data suggest that organisms carrying the _qnrS_ gene are highly prevalent in these areas and that travelers visiting these areas have a high risk for exposure to those organisms.\n\n【44】Coexistence of _qnr_ genes with various other resistance genes, such as _bla_ CTX-M  , on the same plasmid is well known  and could be related to our finding that both types of genes were more prevalent in the study participants’ samples after travel. However, we found no association between these genes in these samples. The _qnrS_ gene was most often acquired by travelers who visited Southeast Asia and, to a lesser extent, the Indian subcontinent, whereas the acquisition rate for _bla_ CTX-M  was clearly highest for travelers to the Indian subcontinent but was not higher for travelers to Southeast Asia than for travelers to other regions. These findings indicate that although travel to the Indian subcontinent is a high-risk factor for acquiring both of these genes, these risk factors are not necessarily related.\n\n【45】Compared with culturing methods, a metagenomic approach has the advantage of being able to detect resistance in a much wider array of species; however, a limitation is that it is not yet known in which organisms the acquired resistance genes detected in our study are present, nor if they are being expressed. Another limitation of our study is that the study population was not large enough for us to conduct a more extensive risk analysis. Future studies that conduct more extensive analyses for risk factors, such as antimicrobial drug use, travel destination, and duration of travel, would benefit from larger populations. Furthermore, in future studies inclusion of a follow-up sampling of travelers would be highly relevant for investigating the period in which these acquired resistance genes remain in the resistome and if the perseverance or even HGT of these genes in the resistome is promoted by factors such as selective pressure introduced by antimicrobial drug use. Little is known about the duration of travel-acquired resistant organisms in the human microbiota, although their continued viability plays a key role in the ability to further spread these organisms or resistance elements.\n\n【46】During our investigation of several targeted resistance genes, it became evident that resistance genes from foreign environments are being introduced into the gut resistome at high rates related to international travel. Although the consequences of these changes in the resistome are difficult to predict, the introduction of these genes into the genetic pool of resistance elements may create opportunities for the horizontal transfer to other organisms in the gut microbiota.\n\n【47】Our study data demonstrated an increasing prevalence of _bla_ CTX-M  , _qnrB_ , and _qnrS_ genes in the feces of healthy volunteers from the Netherlands immediately after they returned from international travel. These findings contribute to the increasing evidence that travelers contribute to the spread of antimicrobial drug resistance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "36e21ea1-2f98-4a33-834e-f0b970af0aa3", "title": "Diagnostic Assays for Crimean-Congo Hemorrhagic Fever", "text": "【0】Diagnostic Assays for Crimean-Congo Hemorrhagic Fever\nCrimean-Congo hemorrhagic fever (CCHF) is a tick-borne zoonotic disease caused by a virus (CCHFV) belonging to the _Nairovirus_ genus . The disease is asymptomatic in infected animals but can develop into severe illness in humans, with case-fatality rates as high as 50% in some outbreaks . The incubation period is typically 3–7 days, with sudden onset of myalgia, headache, and fever that can develop into a severe hemorrhagic syndrome . CCHFV is transmitted by tick bite (from mainly _Hyalomma_ spp. ticks) or by contact with blood or tissues from infected livestock or patients with CCHF .\n\n【1】Sporadic cases of CCHF and community and nosocomial outbreaks have been increasingly reported, and the disease’s geographic distribution is the most extensive among tick-borne diseases. Currently, CCHFV is enzootic in southeastern Europe (Bulgaria, Albania, Kosovo, and Greece), southern Russia, and several countries in the Middle East, Africa, and Asia . Given the abundance of vectors, potential hosts, favorable climate and ecology, and intensified human travel, emergence and rapid establishment of new CCHF foci in other countries are substantial risks . Emergence or reemergence of CCHF poses a serious public health threat because it is highly contagious and highly lethal, has the potential to cause nosocomial infection, and is difficult to treat, prevent, and control. In addition to enhanced surveillance and development of therapeutics, access to early, sensitive, and specific laboratory diagnosis is a key factor in increasing preparedness in Europe and other countries at risk .\n\n【2】Although viral isolation is the standard for CCHF diagnosis, because it has to be done in high-containment biosafety level 4 facilities, the number of laboratories that can perform this technique is limited. Moreover, because cell cultures lack sensitivity and usually only detect the relatively high viremia level encountered during the first 5 days of illness, viral isolation is not without error or uncertainty. As a consequence, reference laboratories have been using the best available practicable methods to determine the presence or absence of infection . These methods include conventional and real-time quantitative reverse transcription PCR (RT-PCR and qRT-PCR) for detection of the viral genome  and indirect immunofluorescence assays (IFAs) or ELISAs for detection of specific IgM and IgG antibodies . No consensus on the most efficient molecular and serologic testing method has been reached.\n\n【3】In this context, a working group of experts from reference laboratories was constituted under the initiative of the European Network for Diagnostics of Imported Viral Diseases to take part in a multicenter study of CCHF diagnostic tests. The aim of this study was to evaluate and compare the performance of, and review the operational characteristics of, available CCHF diagnostic tests by using panels of well-characterized, archived serum samples from patients from geographically diverse settings.\n\n【4】### Materials and Methods\n\n【5】##### Study Participants and Diagnostic Tools\n\n【6】Experts from 5 institutions participated in this study: Bundeswehr Institute of Microbiology, Munich, Germany; Department of Microbiology, Aristotle University of Thessaloniki, Thessaloniki, Greece; Center for Vectors and Infectious Diseases Research, National Institute of Health, Águas de Moura, Portugal; Institute of Microbiology and Immunology, Medical Faculty, Ljubljana, Slovenia; and Center for Microbiological Preparedness, Swedish Institute for Infectious Disease Control, Solna, Sweden. Diagnostic methods that could be performed in standard laboratory facilities were selected on the basis of a systematic review of the literature and the experiences of the members of the working group. During April 2010, an extensive search of available CCHF diagnostic tools was performed by using both generic (Google) and scientific (PubMed) Internet-based search engines. To meet the selection criteria, assays had to be commercially available or in the prerelease phase at the time of our assessment (or have quality assessed reagents and well-defined protocols for noncommercial assays); yield early and rapid results (within 5 hours); not require the purchase of specific equipment; and have demonstrated sufficient scope to detect diverse CCHFV variants or antibodies. The reporting of results was conducted according to Standards for Reporting of Diagnostic Accuracy criteria .\n\n【7】##### Patient Status Definition and Samples\n\n【8】Because no reference test for CCHV is universally accepted, patients with clinically suspected CCHF were confirmed on the basis of results of serologic and molecular diagnostic tests that were in use in the CCHF reference laboratories at the time of the study . These cases were defined by the either positive rule: detection of CCHFV genome or CCHFV-specific IgM, IgG, or both, during either the acute or convalescent phase of the disease. Each participant in the working group contributed a panel of archived serum samples that had tested positive for CCHFV by IgM, IgG, or both, and a panel of archived serum RNA extracts from which CCHFV genome had been detected; samples were collected from patients with laboratory-confirmed CCHF infection. Negative controls were samples from healthy persons who originated from disease-endemic or at-risk areas (e.g. blood donors) and samples from febrile patients with a diagnosis of 1 of the conditions that can produce symptoms similar to those of CCHFV infection (e.g. hemorrhagic fever with renal syndrome, leptospirosis, West Nile fever, chikungunya). All samples were included in the study with the consent of the patients.\n\n【9】##### Assay Methodology and Data Collection\n\n【10】Tests were performed according to the manufacturers’ instructions or according to validated protocols provided by the developer. Working group members tested the selected assays in duplicate on their respective sample panels within their facilities. Results of qualitative assays (IFAs and low-cost, low-density \\[LCD\\] arrays) were read by 2 independent readers. Results were collected at the end of each testing session by using a standard report datasheet and combined into a final database.\n\n【11】##### Data Analysis\n\n【12】Results obtained for each selected diagnostic tool were compared in a 2×2 table with results from the reference in-house diagnostic to estimate indices of sensitivity, specificity, and corresponding 95% CIs. In addition, test results were compared with the results of a composite reference standard (positive if detection of specific CCHFV genome or specific CCHF IgM or IgG antibodies by in-house reference methods; negative otherwise) to confirm the specificity estimates and corresponding 95% CIs. Statistical analysis was performed in STATA/SE version 12.0 software (StataCorp, College Station, TX, USA). CIs were calculated by using binomial exact methods. A univariate analysis was conducted by using the Fisher exact test to identify factors influencing test sensitivity (i.e. patient country of origin, severity of disease, number of days after illness onset that sample was collected, and sample storage time before testing); p<0.05 was considered significant. A multinomial exact logistic regression analysis was performed to identify independent factors influencing sensitivity, including all variables associated with sensitivity in the univariate analysis (p<0.1). Data on operational characteristics of each test (i.e. ease-of-use, level of staff training required, time, ease of interpretation) were gathered through a questionnaire. Tests were scored on operational characteristics.\n\n【13】### Results\n\n【14】##### Selected Diagnostic Methods\n\n【15】Six diagnostic assays met the criteria for inclusion in the study. For specific CCHF serodiagnosis, a commercial IgM and IgG ELISA (Vector-Best, Novosibirsk, Russia) and a commercial IgM and IgG IFA (Euroimmun, Luebeck, Germany) were selected. For detection of the CCHFV genome, a commercial real-time RT-PCR (Altona-Diagnostics, Hamburg, Germany) and a low-cost, low-density macroarray  were used. Characteristics of the selected tests are shown in Table 2 . After selection, assays were purchased directly from the manufacturers and shipped according to their instructions to the working group members by express delivery.\n\n【16】##### Characteristics of Study Population and Sample Panels\n\n【17】The serum panel constituted for the evaluation of the serologic tests consisted of 66 stored serum samples from acute-phase CCHF patients (those who recovered or died) and patients with confirmed CCHF diagnosis who had an early recovery; 32 samples from febrile patients who had symptoms compatible with CCHFV infection; and 41 samples from healthy persons. Molecular tests were evaluated by using a panel of RNA extracts from acute-phase patient serum samples: 54 samples from patients with confirmed CCHF diagnosis, 16 samples from febrile patients who had symptoms compatible with CCHFV infection, and 5 samples from healthy persons. Characteristics of the patient population and the sample panels are shown in Table 3 . Confirmed CCHF case-patients originated from Iran, Kosovo, Albania, Turkey, and sub-Saharan Africa; most had moderate CCHF. Patients with symptoms compatible with CCHF infection included patients who had a diagnosis of leptospirosis, chikungunya fever, hemorrhagic fever with renal syndrome (HFRS), Q fever, tularemia, brucellosis, and West Nile encephalitis. Serum samples were collected 5–49 days after onset of symptoms; RNA extracts were obtained from serum samples collected 2–14 days after onset of symptoms. Storage time until testing ranged from 1 to 23 years for serum samples and 1 to 4 years for RNA extracts.\n\n【18】##### Performances of Selected CCHF IgM Serology Assays\n\n【19】A total of 138 and 90 samples from the collected patient serum panels were tested for CCHFV-specific IgM by the Vector-Best ELISA and the Euroimmun IFA, respectively. Because of limited sample amounts, IFA could not be performed on all collected samples. When compared with the reference IgM serology tests, the sensitivity of the IgM ELISA ranged from 75.0% to 100.0% for different laboratories, with an overall sensitivity of 87.8% (95% CI 78.6%–96.9%). For the IgM IFA, sensitivity ranged from 75.0% to 100.0%, with an overall sensitivity of 93.9% (95% CI 85.8%–100.0%). Overall specificity was estimated to be 98.9% (95% CI 96.7%–100.0%) for the IgM ELISA and 100% for the IgM IFA . When a composite reference standard (described in the Methods section) was used as reference, the observed specificity was 100% for both tests.\n\n【20】##### Performances of Selected CCHF IgG Serology Assays\n\n【21】A total of 137 and 92 samples from the collected patient serum panel were tested for CCHFV-specific IgG by the Vector-Best ELISA and the Euroimmun IFA, respectively. When compared with the reference IgG serology tests, the estimated sensitivity for the IgG ELISA ranged from 75.0% to 100.0%, with an overall sensitivity of 80.4% (95% CI 69.5%–91.3%). For the IgG IFA, sensitivity ranged from 40.0% to 100.0%, with an overall sensitivity of 86.1% (95% CI 74.8%–97.4%). Specificity was estimated to be 100% for both assays .\n\n【22】##### Performances of Selected CCHF Molecular Assays\n\n【23】A total of 71 and 70 samples, respectively, from the collected panel of serum RNA extracts were tested for the presence of the CCHF genome by the Altona-Diagnostics CCHFV qRT-PCR and the CCHF LCD array. When compared with the results of the reference genome detection methods, sensitivity ranged from 42.9% to 100%, with an overall sensitivity of 79.6% (95% CI 68.3%–90.9%) for the qRT-PCR and from 25.0% to 100% with an overall sensitivity of 83.3% (95% CI 72.8%–95.5%) for the LCD array. Both assays demonstrated a specificity of 95.5% (95% CI 90.6%–100%) , which increased to 100% when the results of a composite reference standard were used as reference.\n\n【24】##### Factors Influencing Diagnostic Sensitivity\n\n【25】The influence of several patient and sample characteristics on the sensitivity of the selected assays was analyzed by univariate analysis . The country of origin of the patient was found to be significantly associated with the sensitivity of the IgG IFA (p = 0.02), the qRT-PCR (p<0.001), and the LCD array (p = 0.02). However, after multivariate analysis, this association only remained significant for the qRT-PCR assay (p<0.001). In particular, the qRT-PCR was found to be less sensitive for samples from patients originating from Turkey (adjusted odds ratio \\[OR\\] 0.04, 95% CI 0.00–0.87) and from Albania (adjusted OR 0.02, 95% CI 0.00–0.16).\n\n【26】##### Operational Characteristics of the CCHF Diagnostics\n\n【27】Scores for operational characteristics are summarized in Table 5 . The ELISA test obtained a higher overall score (8.5/10) compared with the IFA (6.7/10). The IFA scored lowest in the ease of interpretation of results (1.3/2) and in the requirement for specific technical training (0.3/1). The observed scores for molecular tests were within the same range (6.0–6.3/10). Both molecular assays demonstrated low scores for technical complexity (1.3–1.5/2) and training requirements for equipment and technique (0.3–0.5/1).\n\n【28】### Discussion\n\n【29】A number of published studies have described major epidemics, community and nosocomial outbreaks, and the ecology of CCHF . These reports have shown that a distinct epidemiologic situation can arise in regions where the virus is endemic but also that new foci can emerge . The World Health Organization has listed CCHF among the emerging diseases for which control and prevention measures should be renewed and intensified . In addition, an assessment of the importance and magnitude of vector-borne diseases initiated by the European Centre for Disease Prevention and Control identified CCHF as a priority disease for the European Union .\n\n【30】A strong laboratory capacity, in particular standardized approaches for diagnostic methods and assay validation, has been identified as a short-term priority in CCHF-endemic areas and regions where CCHFV could be expected to circulate . The aim of this study was to identify and evaluate easily available, simple-to-perform CCHF diagnostic methods considered most suitable for widespread use in CCHF-endemic areas and countries at risk.\n\n【31】We assessed the performances of 2 commercially available IgM and IgG serologic tests, the Vector-Best CCHF ELISA and the Euroimmun CCHF IFA, and 2 assays for viral genome detection, the Altona-diagnostics CCHF qRT-PCR and a CCHF LCD array. The IgM and IgG ELISAs showed a sensitivity of 88% and 80%, respectively, lower than the numbers given by the manufacturer. These assays were validated by the manufacturer by using serum panels from CCHF cases originating from southwestern Russia . Therefore, lower sensitivity for certain serum samples tested in this study may reflect antigenic variation among CCHFVs circulating in other countries.\n\n【32】Observed sensitivities of the IgM and IgG IFA were 93.9% and 86.1%, respectively. Although these estimates are higher than those observed for the IgM and IgG ELISAs, these results may have had a sampling bias because not all serum samples tested by ELISA could be tested by IFA. This bias was, however, minimized, because the tested serum panel included 15/16 false-negative samples observed for the ELISAs.\n\n【33】The sensitivity of the selected molecular assays was found to be more modest (79.6% for qRT-PCR and 83.3% for LCD array) than for serologic methods and to be associated with the patient country of origin. This result is consistent with the finding that the application of molecular assays in different settings is hampered by the high diversity of the CCHFV genomes, whereas serologic methods can have a broader use due to cross-reactivities. In particular, the qRT-PCR seems to be less sensitive for patients originating from the Balkans region and Turkey than for patients from other countries compared with in-house reference molecular methods. The in-house methods developed by reference laboratories are optimized for detection of strains circulating in that area, which may result in a lower detection limit when compared with methods that cover a broader spectrum. However, other factors, such as RNA degradation due to freezing and thawing cycles or inhibition of PCR reactions because of inhibitory compounds in the samples, may have contributed to decreased sensitivity of molecular methods compared with serologic methods.\n\n【34】The observed specificity was excellent for all assays, ranging from 95.5% to 100% when compared with the reference method and equal to 100% when compared with a composite reference standard. However, predictive positive and negative values for the different assays could not be calculated because precise prevalence data are not available for most CCHF-endemic areas, and these data cannot be predicted for areas where the virus could emerge.\n\n【35】The CCHF IFA demonstrated a higher complexity in equipment, technique, and interpretation. However, the interpretation of fluorescence patterns may enable trained users to differentiate positive from cross-reactive serum samples, whereas such false positives may not be avoided with the ELISA. The operational characteristics of the molecular assays were comparable. Both methods required regular equipment maintenance and specific training for appropriate use of the equipment; however, this was more apparent for the real-time qRT-PCR method. The LCD array technique was considered to be complex but acceptable. Training for technique and result interpretation was recommended for each method.\n\n【36】All participating laboratories are reference centers for CCHF laboratory diagnosis and surveillance in their respective countries, and some are World Health Organization collaborating centers. The protocols from reference methods in use at each site have been extensively validated previously . In addition, the laboratories participated in a recent international external quality assessment . Therefore, local conditions at the participating sites and validity of reference methods were considered comparable.\n\n【37】The multicenter design of this study enabled the testing of a large sample size, representative of \\> 1 population, so findings could be more generally applicable. In addition, sample panels were constituted without any selection for disease severity, and negative controls included not only healthy patients but also patients who had a wide range of other conditions, thereby avoiding inflated estimates of diagnostic accuracy.\n\n【38】Our study has some limitations. Because archived samples were used for the study, specimen quality could have been affected; however, statistical analysis demonstrated that sample storage time and temperature did not influence sensitivity. Also, the use of different sample panels for serologic and molecular testing did not enable calculation of the added value of combining the serologic and molecular CCHF diagnostic methods evaluated.\n\n【39】The results of this study give additional guidance on the type of CCHF diagnostic tools that could be used in different contexts. During a large outbreak, easily interpretable tests for simultaneous analysis of numerous samples, such as the ELISA and real-time qRT-PCR, might be considered useful tools to identify CCHF cases. Methods available in smaller format size and demonstrating a long shelf life, such as the IFA and LCD array, could be used to identify sporadic cases or to confirm single cases as part of a larger outbreak.\n\n【40】This study demonstrates that efficient, well-characterized serologic and molecular assays and protocols are available for CCHF diagnosis. The on-site use of such assays by outbreak assistance laboratories would greatly diminish the risks posed by the handling, packaging, and shipping of highly infectious samples. Moreover, acquiring diagnostic reagents would be more time- and cost-effective for laboratories than would the organization of compliant packaging and shipment of hazardous biologic material to reference laboratories abroad. Nevertheless, laboratory personnel should receive the appropriate training to perform the different assays (e.g. during international workshops or network meetings). Collaborative evaluations of diagnostic methods remain essential to guide decision-making, especially with emerging diseases, where a standard is frequently missing, and laboratory expertise is rare.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9f46a2e1-7330-4a95-8db9-180aa5dbb3a5", "title": "Exposure to Bat-Associated Bartonella spp. among Humans and Other Animals, Ghana", "text": "【0】Exposure to Bat-Associated Bartonella spp. among Humans and Other Animals, Ghana\n**To the Editor:** Human contact with wildlife is a leading cause of disease spillover. Bats, in particular, host numerous zoonotic pathogens, from henipaviruses to lyssaviruses . In Ghana, the straw-colored fruit bat ( _Eidolon helvum_ ) frequently and closely interacts with humans through roosting in urban areas and human harvesting of bushmeat. Large colonies live in Accra, the capital city, and >128,000 bats, on average, are hunted for food yearly in southern Ghana alone . Serologic evidence of human infection with novel paramyxoviruses from _E. helvum_ bats  supports concerns regarding this contact. In addition, Kosoy et al. isolated several new strains of _Bartonella_ that were found in >30% of _E. helvum_ bats, whereas Billeter et al. found _Bartonella_ in 66% of their ectoparasites , with _Bartonella_ transmissibility to other species unknown. This prevalence causes concern because many _Bartonella_ species are zoonotic and cause substantial human disease . Previous studies of febrile patients in Thailand have shown prevalence rates of < 25% for antibodies against zoonotic _Bartonella_ species . Serologic studies have been conducted in Europe and in the United States, but few studies have examined such prevalence in Africa among patients and in the general population .\n\n【1】To address these concerns, we conducted a prevalence study in Ghana, West Africa, for evidence of bat-associated _Bartonella_ infection in humans and other common animal species. We sampled humans who had close contact with fruit bats and also sampled domestic animals that lived around the bat colonies.\n\n【2】We obtained serum samples from 335 volunteers from Accra and the Volta region who had close contact with _E. helvum_ bats and also sampled 70 domestic animals that lived underneath bat colonies (5 cats, 23 chickens, 7 cows, 6 dogs, 21 goats, 8 sheep) in Accra. We used 3 testing approaches: culture, PCR, and indirect immunofluorescent assays for serologic testing. We tested serum specimens for antibodies against _B. henselae_ , _B. quintana_ , _B. clarridgeiae_ , _B. vinsonii vinsonii_ , _B. elizabethae_ , and _Bartonella_ strain E1–105, which had been isolated from _E. helvum_ bats .\n\n【3】All culture results for human and domestic animal samples were negative for _Bartonella_ species. One human serum sample was positive for _B. clarridgeiae_ by PCR, which was confirmed by repeat testing. No other human samples were consistently positive by PCR. Of 70 animal blood clots and 62 serum samples tested by using PCR, 1 serum sample from a cat tested positive for _B. henselae_ . One human serum sample was positive by immunofluorescent assay for antibodies against _B. henselae_ at titers of 1:128, another had reactivity to _B. henselae_ at 1:64, and 1 sample was reactive at 1:32. Five human serum samples were reactive to _B. quintana_ at titers of 1:32.\n\n【4】The absence of evidence of any human exposure to bat-associated _Bartonella_ suggests that the species isolated from _E. helvum_ bats never or rarely infects humans in Ghana. If _Nycteribiidae_ bat flies serve as the vector for _Bartonella_ transmission between bats as hypothesized, then the high host specificity of these vectors  could explain why little infection is spilling over to other species. However, no experimental studies have confirmed that bat flies are competent vectors of bat-associated _Bartonella_ species or that these ectoparasites only bite bats. These facts must be confirmed because bat flies are occasionally found on other animals and whether the parasites can successfully use these animals as hosts is unknown . Although further studies are needed to clarify the dynamics of _Bartonella_ species infection in _E. helvum_ bats, as well as the species’ zoonotic potential, the current risk of spillover of this bat-associated _Bartonella_ species appears low in West Africa. This fact may be useful in directing limited public health resources.\n\n【5】The seroprevalence to _B. henselae_ in healthy human participants in this study was <1%. The low levels of seropositivity to _B. henselae_ and _B. quintana_ are consistent with those found in the only other study on _Bartonella_ in humans in sub-Saharan Africa: a survey of 155 subjects in the Democratic Republic of Congo showed 1% seroprevalence of _B. henselae_ and <1% seroprevalence of _B. quintana_ .\n\n【6】The results of study in the Democratic Republic of Congo and this study contrast with some studies in Asia and Europe, which show higher rates of human exposure to _Bartonella_ species. For example, a study of febrile patients in Thailand found serologic evidence of exposure to _Bartonella_ infection in 25% of patients .\n\n【7】Laudisoit et al. were, to our knowledge, the first to report evidence of _Bartonella_ infection in humans in Africa. Our study contributes to this nascent effort of understanding _Bartonella_ on the continent. Because a substantial proportion of _Bartonella_ prevalence studies have been done on hospital patients, our study provides a survey of the general population to help determine background infection rates and illuminate the complex risks posed by this zoonosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "03e3ba4c-c54e-45a2-82a5-26348a3692e0", "title": "Haemophilus influenzae Type B and Streptococcus pneumoniae as Causes of Pneumonia among Children in Beijing, China", "text": "【0】Haemophilus influenzae Type B and Streptococcus pneumoniae as Causes of Pneumonia among Children in Beijing, China\nNew vaccines for _Haemophilus influenzae_ type b (Hib) and _Streptococcus pneumoniae_ effectively prevent disease caused by these pathogens . However, the relatively high cost of these vaccines inhibits their widespread use in developing countries. Therefore, for most developing countries, surveillance for disease caused by Hib and _S. pneumoniae_ is needed to demonstrate that the investment in these highly effective vaccines is warranted  .\n\n【1】Data on the incidence of Hib and pneumococcal disease in China are limited to a few meningitis surveillance studies, which suggest a surprisingly low incidence of confirmed Hib meningitis. In most cases the rate is 5 to 25 times lower than those observed in areas of North America, Europe, Africa, South America, and Oceania where careful surveillance studies have been carried out . However, the apparent low incidence in these studies is difficult to interpret because of concerns that physicians may not routinely perform lumbar punctures on all patients with suspected meningitis, that the laboratory methods may have been inadequate, and that widespread use of oral antibiotics for outpatients may have artificially reduced the yield of cultures  . In part because of the uncertainty surrounding the local rate of Hib disease, China has not yet made Hib vaccination a routine infant vaccination. Many authors and organizations have called for further studies of the epidemiology in China, but few data are currently available .\n\n【2】Pneumonia is a leading cause of illness and death among children worldwide  . Demonstrating the role of bacterial agents such as _H. influenzae_ and _S. pneumoniae_ in pneumonia, however, is difficult. The most widely accepted method for demonstrating that a bacterial agent causes pneumonia is isolation of the bacterium from cultures of blood or, in some cases, lung aspirates. However, blood cultures are rarely positive in pediatric pneumonia even when microbiologic techniques are optimal, and fastidious organisms such as Hib will not grow unless the appropriate culture medium is used. In China, parents and physicians often resist the collection of blood for cultures, and given the high rate of prior antibiotic use, the yield from these specimens is likely to be low.\n\n【3】Because alternatives are needed to traditional methods of documenting Hib and _S. pneumoniae_ as causes of pneumonia in China, we designed a study to investigate the use of a readily available source of specimens that is likely to be positive more often than a blood culture. We compared the rates of isolation of Hib and _S. pneumoniae_ from nasopharyngeal swabs and blood cultures of patients with radiographically confirmed pneumonia and a group of control patients without pneumonia.\n\n【4】### The Study\n\n【5】The People's Republic of China is the most populous nation in the world, with an estimated population of >1.2 billion and > 20 million births each year . Beijing, the capital, has a population of approximately 11 million people. Beijing Children's Hospital is one of the largest children's hospitals in Asia, with >700 beds in 24 wards. The outpatient department typically has 2,500 to 4,000 patient visits per day.\n\n【6】Before patients were enrolled, a study nurse or physician explained the study to the parent or guardian, answered questions, and obtained written informed consent. The protocol for this study was approved by the human subjects review committees at Beijing Children's Hospital and the Centers for Disease Control and Prevention.\n\n【7】Pneumonia cases were defined as illness in patients 2 to 60 months of age with radiographic evidence of pulmonary infiltrates and at least three of the following: fever (temperature 38.0°C), tachypnea (>50 breaths per min for infants <12 months old, and >40 breaths per min for children 12 to 60 months old), cough, auscultation findings indicative of lower respiratory disease (including rhonchi, crackles, or bronchial breath sounds), or chest indrawing.\n\n【8】Specimens for blood culture were collected from all 96 pneumonia patients. Urine specimens were obtained from 89 (93%) pneumonia patients and 199 (93%) controls.\n\n【9】Chest radiographs from all pneumonia patients were interpreted initially by a radiologist at Beijing Children's Hospital, who characterized the pattern of infiltrates as evidence of alveolar consolidation only, interstitial infiltrates only, or a mixed pattern with evidence of interstitial infiltrates and consolidation. The radiographs were then recorded by a digital camera and read later by one of the investigators who is a pediatrician (SFD). The pediatrician characterized the radiographs as having evidence of obvious or not obvious pneumonia. Both the radiologists and the pediatricians were blinded to the colonization status of the patients.\n\n【10】Control patients were children with a diagnosis of diarrhea or dermatitis who had no indication of respiratory tract disease. All subjects were recruited from patients attending the Outpatient Department. None of the participating children had received Hib vaccine. Of 121 eligible pneumonia patients identified in the Radiology department, 96 (79%) were enrolled; 214 controls were also enrolled, 169 with diarrhea and 45 with dermatitis. Cases and controls were frequency-matched by age in a 1-to-2 ratio in the following age categories: 2 to 5 months, 6 to 11 months, 12 to 35 months, and 36 to 60 months.\n\n【11】### Laboratory Methods\n\n【12】After measuring the distance from the nares to the earlobe, a researcher inserted a fine flexible Dacron swab past the point of mild resistance to the posterior pharynx. The swab was rotated twice, withdrawn, and immediately used to make a uniform suspension in a skim milk/glucose/glycerol solution. The suspension was then plated onto a blood agar plate with gentamicin (for isolation of pneumococci) and a chocolate agar plate supplemented with X and V factors and bacitracin (for isolation of _H. influenzae_ ). Plates were refrigerated before inoculation and warmed to room temperature before use.\n\n【13】Within 3 hours of inoculation, blood agar plates were placed in an incubator at 35°C to 37°C and 5% to 10% CO2, and individual colonies were isolated in the Microbiology and Immunology Laboratory, Beijing Children's Hospital. After 18 to 36 hours of incubation, the plates were assessed for the appearance of α-hemolytic colonies resembling streptococci. A second plate was then streaked for confluent growth and an optochin disk placed on it. Strains inhibited by optochin were confirmed as pneumococci by bile solubility testing. For isolation of _H. influenzae_ , X and V factor-supplemented chocolate agar plates were placed in an incubator at 35°C to 37°C and 5% to 10% CO2, and single colonies were isolated. After overnight incubation, the plates were assessed each morning for 2 days for the appearance of large, flat, colorless-to-gray opaque colonies, without hemolysis or discoloration of the medium. _H. influenzae_ were determined by dependency for X and V factors and absence of hemolysis on blood agar plates. All isolates were serotyped by using type-specific antiserum.\n\n【14】For blood cultures, which were done in the Clinical Microbiology Laboratory at Beijing Children's Hospital, 2 to 5 ml of blood was inoculated into blood culture bottles that contained agents to inhibit antibiotic activity. The bottles were incubated by the BacTec 9120 system (Organon Teknika, Durham, NC). All positive blood cultures were subcultured onto blood agar and chocolate agar plates. These plates were prepared by the same techniques and quality control used in the research laboratory at Beijing Children's Hospital. For colonies with appropriate morphologic features on subcultures, the same protocol for identification of pneumococcus and _H. influenzae_ was used as described earlier. Reagents and media were checked weekly for their ability to support the growth of control strains of _H. influenzae_ and _S. pneumoniae_ .\n\n【15】Urine specimens were tested for antigens to _Legionella pneumophila_ serogroup 1 by an optical immunoassay and for evidence of prior antimicrobial activity by a _Micrococcus luteus_ inhibition assay. To determine if _Chlamydia pneumoniae_ were present in the nasopharynx, nasopharyngeal swabs were cultured on HEp-2 cells by using a modification of standard techniques involving repeat centrifugation steps  . _C. pneumoniae_ cultures were performed only on a subset of 30 patients >2 years old.\n\n【16】### Statistical Analysis\n\n【17】Recent day-care attendance was defined as attendance in the last month before enrollment. Exposure to other children was defined as residing in the same household with at least one other child <18 years old. All analyses were conducted by SAS (SAS for Windows v.6.12; SAS Institute, Cary, NC). Logistic regression was used to assess the independent association between colonization and pneumonia and to adjust for potential confounders. Odds ratios (OR) were calculated by comparing pneumonia patients with control patients without pneumonia. Separate ORs were calculated for both the clinical and radiologic definitions of pneumonia. ORs with 95% confidence intervals that do not include 1.00 and p values <0.05 were considered statistically significant.\n\n【18】### Results\n\n【19】Four patients who were initially enrolled as pneumonia patients were later determined to have myocarditis and were thus excluded. One pneumonia patient had a blood culture positive for _S. pneumoniae_ ; no patients had a blood culture positive for _H. influenzae_ . All urine specimens were negative for _L. pneumophila_ serogroup 1. _C. pneumoniae_ was not cultured from any of the 30 specimens tested. Eleven pneumonia patients were hospitalized. Among the pneumonia patients, 45 had a pattern of alveolar consolidation, 23 had an interstitial pattern, and 28 had a mixed pattern (both consolidation and interstitial).\n\n【20】Comparison of the patients with radiographic pneumonia and control patients showed that the distribution of ages was not consistently one pneumonia to two nonpneumonia patients in all age groups . Although the differences in the distributions are not statistically significant, we adjusted all subsequent analyses by age to account for any residual confounding by this factor. Pneumonia patients were significantly more likely to have received antibiotics before attending the outpatient clinic, to attend day care, and to have other children in the household .\n\n【21】Of the 96 patients who had radiographic evidence of pneumonia, 32 (33%) were considered to have obvious pneumonia by the pediatrician's reading. The prevalence of Hib colonization was significantly greater among pneumonia patients than among patients without pneumonia . The association was stronger, however, when the patients meeting the clinical definition of pneumonia were compared with the controls (OR = 5.8) than when the radiologist's definition of pneumonia was used (OR = 4.2). Multivariate analysis by logistic regression showed that the association remained after the data were adjusted for potential confounders (recent antibiotic use, recent day-care attendance, and other children in the household), and in the case of the clinical definition of pneumonia, increased the strength of the association.\n\n【22】In univariate analysis, colonization with _S. pneumoniae_ was not significantly higher among pneumonia patients than among patients without pneumonia . This association, however, was confounded by the higher proportion of pneumonia patients with prior antibiotic use, an exposure that decreases colonization substantially. After the data were adjusted for prior antibiotic use, recent day-care attendance, and exposure to other children in the household, colonization with _S. pneumoniae_ was significantly associated with pneumonia by either the radiographic or clinical definition of pneumonia.\n\n【23】### Conclusions\n\n【24】We found that Hib and _S. pneumoniae_ can be isolated significantly more often from the nasopharynx of children with radiographically confirmed pneumonia in China than from comparable children without pneumonia, demonstrating the potential role of these bacteria as a cause of pneumonia in Chinese children. These results suggest that further studies should be undertaken to quantify the role of these agents in China, the prevalent serotypes, and the potential impact of vaccines for their prevention.\n\n【25】The prevalence rates of Hib and _S. pneumoniae_ colonization observed among controls are similar to those we reported in a study of outpatients at Beijing Children's Hospital, as well as by other investigators . The prevalence of Hib colonization among nonpneumonia patients (~2%) is consistent with that observed in other studies of young children in China, but lower than commonly seen in other developing countries (5%-15%) . Even in industrialized countries such as the United States, Finland, and the United Kingdom, pre-vaccination colonization rates of 3% to 6% were generally observed  . Host and environmental factors may play a role in this apparently low prevalence of Hib colonization in nonpneumonia patients. For example, widespread overuse of antibiotics in children and the relatively small family size as the result of the \"one family, one child\" policy may contribute to this observation.\n\n【26】The rate of positive blood cultures among pneumonia patients was lower (1%) than that generally observed in studies from other countries, but consistent with reports of a low rate of bacterial meningitis and invasive disease from mainland China and Taiwan . In rural Papua New Guinea, _S. pneumoniae_ and _H. influenzae_ were isolated from the blood of 11% and 12%, respectively, of children >6 years old with moderate or severe acute lower respiratory tract illness  . However, a recent study from Dallas found no positive blood cultures among 106 ambulatory outpatients <5 years old with radiographically confirmed pneumonia  . Thus, the rate of positive blood cultures was lower than that seen in hospital-based studies in developing countries but consistent with recent data from studies in industrialized countries of children with pneumonia who attend outpatient clinics.\n\n【27】Several steps were taken to ensure the quality of the blood specimens collected and their processing. For example, all blood specimens were collected before intravenous antibiotics were administered. The protocol for the study was to inoculate bottles with 2 and 5 ml of blood, a volume considered optimal for the 15-ml blood culture bottles used in the BacTec system, and the blood culture bottles contained resin-coated beads designed to neutralize the antibacterial activity of antecedent antibiotic use.\n\n【28】The observation that colonization with Hib and _S. pneumoniae_ is more likely among pneumonia patients, especially those with radiographically obvious pneumonia, strengthens the contention that pneumonia is associated with these agents. This finding is consistent with the observation from studies with Hib conjugate vaccine showing that the protection afforded by these vaccines is most apparent in children with obvious radiographic pneumonia or alveolar consolidation and not apparent in children with milder forms of pneumonia . However, an inherent limitation of case-control studies is that they cannot establish that the exposure, in this case colonization with Hib or _S. pneumoniae_ , preceded the onset of illness.\n\n【29】The potential role of other agents of pneumonia besides Hib and _S. pneumoniae_ remains unclear. In this study, _H. influenzae_ colonization (regardless of serotype) was also associated with pneumonia (data not shown). The observation that no patients had antigens to _L. pneumophila_ serogroup 1 in urine is consistent with findings from the United States and elsewhere indicating that this agent rarely causes pneumonia among young children  . The finding that none of 30 patients >2 years of age were positive for _C. pneumoniae_ by culture suggests that if this organism causes pneumonia in this age group, alternative methods for detection are needed to define its role. The potential role of other agents such as _Mycoplasma pneumoniae_ and viruses such as respiratory syncytial virus warrants further investigation. Characterizing the epidemiology of these agents may have important implications for the choice of empiric therapy in pneumonia patients and for the development of vaccines for pneumonia prevention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "67bb5ff5-3b3b-49ce-843a-000470fcc9ba", "title": "Dengue Fever in Burkina Faso, 2016", "text": "【0】Dengue Fever in Burkina Faso, 2016\nDengue is an emerging viral disease mainly found in the tropical and subtropical zones, and a major public health concern worldwide . Dengue fever is a mosquitoborne viral infection caused by 4 distinct dengue viruses (DENVs): DENV-1‒4. In some countries of sub-Saharan Africa, the circulation of all 4 viruses has been reported . However, availability of rapid tests and molecular diagnosis by reverse transcription PCR (RT-PCR) in resource-limited settings remains a challenge.\n\n【1】During October 29, 2016‒November 21, 2016, we screened 1,947 suspected dengue cases using a rapid diagnostic test (SD BIOLINE Dengue Duo, Standard Diagnostics, Seoul, South Korea), which detects DENV nonstructural protein 1 (NS1) and dengue-specific antibodies (IgM and IgG), in response to an outbreak of acute febrile illness in Burkina Faso. All patients with acute febrile illness during this period were suspected to have dengue; notably, some patients had biphasic fever with severe headache, myalgia, arthralgia, and rash. Patients who tested positive for NS1 or DENV antibodies were considered to have a probable DENV infection. All participants provided informed consent as specified by the Declaration of Helsinki, and approval of this study was obtained from the national ethics committee.\n\n【2】Of the 1,947 blood samples tested, 1,327 were positive for NS1, DENV antibodies, or both. Of the 13 country regions investigated, the central region, which includes the city of Ouagadougou, was the most affected, having 1,679 of the 1,947 suspected cases (case fatality ratio 1.2% \\[20/1,679\\]) and 1,307 of the 1,327 probable cases. Of the 20 deceased patients, 18 were positive for NS1 and 2 were positive for NS1 and DENV IgM. The outbreak peaked November 11‒14. Blood samples from 35 randomly selected patients were sent to the National Reference Laboratory for Influenza (Bobo-Dioulasso, Burkina Faso) for confirmation using the Centers for Disease Control and Prevention trioplex real-time RT-PCR protocol  followed by singleplex to identify the infecting DENV serotype. Of the 35 patient samples that were selected, 22 were positive for NS1, 3 were positive for both NS1 and IgG, 3 were positive for IgG, 2 were positive for both NS1 and IgM, 1 was positive for both IgM and IgG, and 4 were negative. Nineteen (54.3%) cases were positive for DENV, and no cases were positive for Zika or chikungunya viruses . Eleven patients were infected with DENV-2, 6 were infected with DENV-3, and 1 patient was co-infected with DENV-2 and DENV-3. We submitted our samples to the World Health Organization Collaborating Centre for Arbovirus Reference and Research, Institut Pasteur de Dakar (Dakar, Senegal), which confirmed our results.\n\n【3】In Burkina Faso, dengue represents an added burden to an infectious disease landscape dominated by malaria; therefore, implementation of molecular diagnostic testing is urgently needed to identify the correct etiologic agent associated with the disease. The trioplex real-time RT-PCR detected 19 cases of DENV. A total of 3 serum samples positive for NS1 were negative by this assay. These negative results can be explained in part by declining viremia levels that became undetectable around the time of molecular testing, although testing with a larger representative sample size could have provided more information.\n\n【4】We found DENV-2 to be the dominant serotype in this outbreak, followed by DENV-3. No cases of DENV-1 or DENV-4 were found, although testing a larger number of specimens might have revealed the co-circulation of these DENV serotypes. Human cases of DENV-2 in Burkina Faso is supported by previous reports of DENV-2 circulating in mosquitoes . The presence of DENV-3 in Burkina Faso is not surprising, considering this serotype has been previously reported in the region; in 2009, DENV-3 was the main etiologic virus of the outbreak in Cape Verde, which affected >17,000 persons, and was reported in 6 persons in Senegal who traveled to Italy and died . DENV-3 was also detected in the DENV outbreak in Côte d’Ivoire in 2008 .\n\n【5】We speculate that increased international travel between neighboring countries and mosquito circulation has led to DENV-2 and DENV-3 successfully crossing the border into Burkina Faso. This pilot study shows DENV-2 and DENV-3 are both circulating in Burkina Faso and causing human disease. Molecular diagnostics, vector control strategies, and risk communication should be implemented in Burkina Faso in preparation for future outbreaks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4e266b29-ac77-4c9a-b04c-8269bce0411b", "title": "Racial Disparities in Blood Pressure Control and Treatment Differences in a Medicaid Population, North Carolina, 2005-2006", "text": "【0】Racial Disparities in Blood Pressure Control and Treatment Differences in a Medicaid Population, North Carolina, 2005-2006\nAbstract\n--------\n\n【1】**Introduction**Racial disparities in prevalence and control of high blood pressure are well-documented. We studied blood pressure control and interventions received during the course of a year in a sample of black and white Medicaid recipients with high blood pressure and examined patient, provider, and treatment characteristics as potential explanatory factors for racial disparities in blood pressure control. **Methods**We retrospectively reviewed the charts of 2,078 black and 1,436 white North Carolina Medicaid recipients who had high blood pressure managed in primary care practices from July 2005 through June 2006. Documented provider responses to high blood pressure during office visits during the prior year were reviewed. **Results**Blacks were less likely than whites to have blood pressure at goal (43.6% compared with 50.9%, _P_ \\= .001). Blacks above goal were more likely than whites above goal to have been prescribed 4 or more antihypertensive drug classes (24.7% compared with 13.4%, _P_ < .001); to have had medication adjusted during the prior year (46.7% compared with 40.4%, _P_ \\= .02); and to have a documented provider response to high blood pressure during office visits (35.7% compared with 30.0% of visits, _P_ \\= .02). Many blacks (28.0%) and whites (34.3%) with blood pressure above goal had fewer than 2 antihypertensive drug classes prescribed. **Conclusion**In this population with Medicaid coverage and access to primary care, blacks were less likely than whites to have their blood pressure controlled. Blacks received more frequent intervention and had greater use of combination antihypertensive therapy. Care patterns observed in the usual management of high blood pressure were not sufficient to achieve treatment goals or eliminate disparities.  \n\n【2】Introduction\n------------\n\n【3】Racial and ethnic health disparities have become a prominent issue in the national debate about health care in the United States and have been particularly well-documented in cardiovascular disease (CVD), including stroke, coronary heart disease, heart failure, and high blood pressure . Death rates from CVD are higher among blacks and have decreased at a slower rate than among whites, effectively widening the disparity . High blood pressure is the single most important modifiable risk factor for cardiovascular disease, yet blood pressure control is achieved in only one-third of all patients with high blood pressure . Among patients with regular medical care, only 48.9% of blacks have their blood pressure adequately controlled, compared with 59.7% of whites . A number of factors are important in achieving adequate control of high blood pressure, including biological, cultural, social, and health care provider and system factors . Although access to health care has dominated the national debate about the inadequacies of the US health care system, racial and ethnic disparities among patients with similar access to care and similar socioeconomic status are known to exist . Previous studies have found higher awareness and treatment of high blood pressure among blacks than among whites, but poorer control; demographics, socioeconomic status, comorbidities, and behavioral risk factors appear to play little role in explaining these racial differences . Among patients receiving care for high blood pressure, provider nonadherence to treatment guidelines or failure to pursue treatment goals aggressively are known to contribute to low attainment of treatment goals for blood pressure. To our knowledge, however, no prior studies have explored the role of clinical practice patterns in racial disparities in blood pressure control. Medicaid is the largest provider of health insurance for low-income and minority populations in the United States, and Medicaid patients have a disproportionate share of cardiovascular risk factor prevalence, uncontrolled blood pressure, and associated illness and death . We reviewed the charts of a representative sample of adult Medicaid recipients in North Carolina with diagnosed high blood pressure managed in the primary care setting. The objectives of this analysis were to 1) identify differences in blood pressure control between black and white Medicaid recipients with high blood pressure managed in the primary care setting; 2) examine whether these differences could be explained by differences in demographic factors, comorbidities, or provider characteristics; and 3) determine whether black patients with blood pressure above goal had received differential management for high blood pressure compared with that of white patients during the prior year.  \n\n【4】Methods\n-------\n\n【5】### Study population\n\n【6】We used Medicaid administrative data to select a representative sample of North Carolina Medicaid recipients aged 21 years or older with high blood pressure managed in the primary care setting. Recipients were enrolled with Medicaid for at least 11 months from July 1, 2005, through June 30, 2006, and had an office visit with a diagnosis of high blood pressure (ICD9 401xx), excluding pregnancy-induced high blood pressure. We excluded patients who had any office visits with a cardiologist or endocrinologist during this time and those receiving dialysis services for end-stage renal disease. This study was performed as a quality improvement activity of the North Carolina Division of Medical Assistance and was exempted from review by the University of North Carolina Office of Human Research Ethics. North Carolina had a traditional fee-for-service (FFS) program for Medicaid recipients and 2 managed-care programs during the study period: Carolina ACCESS (CA-I), in which recipients are assigned to a primary care provider (PCP), and ACCESS II (CA-II), which additionally incorporates community-based care management and quality improvement initiatives. PCPs were identified according to administrative assignment for eligible patients in the CA-I and CA-II systems. For FFS patients, the PCP was identified by examining professional services claims submitted during the eligibility year with the following specialty type: general or family medicine, internal medicine, obstetrics and gynecology, pediatrics, federally qualified health center, rural health center, nurse practitioner, or health department. The provider who had submitted the most claims (or the most recent claim in case of a tie) was identified as that patient’s PCP. In Medicaid administrative data, “provider” refers to a single physician or a larger practice organization. To ensure a representative statewide sample and adequate sampling from 8 counties planning a high blood pressure initiative for CA-II enrollees, we used a stratified cluster sampling design and randomly selected PCPs within 4 sampling strata (CA-II patients in pilot counties, CA-II patients in nonpilot counties, CA-I/FFS patients in pilot counties, and CA-I/FFS patients in nonpilot counties). We excluded providers with fewer than 5 eligible patients. A total of 4,046 charts were reviewed from March through July 2007. Of these, we excluded 224 patients from analysis because there was no high blood pressure diagnosis in the chart; 60 patients because they had no office visit after June 30, 2005; and 20 patients because no blood pressure measurement was documented. We limited our analyses to patients identified as black or white in the medical record, or if not available in the record, according to self-reported race in Medicaid enrollment data. We could not determine patient race for 3.5% of charts reviewed. The final sample included data for 2,078 black and 1,436 white patients from a total of 160 providers. We abstracted medical record data from the offices of selected PCPs by using an electronic clinical abstraction tool developed by Michigan Peer Review Organization and the North Carolina Division of Medical Assistance. Q Mark Inc (Q Mark Inc, Englewood, Colorado) provided trained nurses for the chart abstractions who passed interrater reliability and consistency tests. Reviewers followed systematic guidelines and read all summary documents in the chart as well as clinic notes and correspondence for a 12-month look-back period from the most recent visit. Each chart was reviewed by a single reviewer. PCP specialty was determined by self-identification of the billing practice as recorded in Medicaid administrative data. Length of time with PCP was calculated on the basis of the earliest service date and the most recent service date documented in the chart. Providers located in a county with a population density of more than 200 people per square mile, according to US Census 2000 data, were classified as urban; all others were classified as rural. \n\n【7】### Measures\n\n【8】All study analyses were based on medical record documentation. The goal for blood pressure treatment was defined as less than 130/80 mm Hg for patients with diabetes and less than 140/90 mm Hg for all others, in accordance with the Seventh Report of the Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure (JNC 7) . A comprehensive, uniform dictionary of all clinical conditions and terms meeting study definitions of high blood pressure, diabetes, hyperlipidemia, cardiovascular disease (including coronary disease, stroke, and peripheral arterial disease), tobacco use, chronic obstructive pulmonary disease, and asthma was used to identify the presence of these conditions as documented in the medical record. Chronic kidney disease was defined as having an estimated glomerular filtration rate (eGFR) <60 mL/min/1.73 m 2 and was calculated by using the isotope dilution mass spectrometry (IDMS)-traceable Modification of Diet in Renal Disease (MDRD) Study equation from the most recent serum creatinine level documented in the medical record. Body mass index (BMI) was calculated from most recent weight and height documented in the medical record, when available. If no height was recorded in the medical record, the sex-specific median height of the study population was used to calculate BMI. Tobacco use status, creatinine, and weight were not available for 31%, 9%, and 2% of patients, respectively. Antihypertensive agents listed on the patient’s medication regimen at the time of abstraction were recorded. Combination therapy was defined as the use of 2 or more of the following antihypertensive drug class categories: angiotensin converting enzyme (ACE) inhibitors, angiotensin receptor blockers, beta blockers, calcium channel blockers, thiazide diuretics, other diuretics, vasodilators, and antiadrenergic agents. \n\n【9】### Statistical methods\n\n【10】We used the most recent blood pressure measurement available from the patient’s medical chart to assess the prevalence of above-goal blood pressure in blacks compared with whites. Next, we examined the bivariate relationships between race and patient and provider characteristics that may influence blood pressure control. To assess potential explanations for racial disparities in blood pressure control, we used logistic regression to calculate odds ratios (ORs) for the association between blood pressure control and race (black vs white) and expected covariates. First, in the step 1 full model, we tested for contributions of patient characteristic variables in predicting blood pressure control, including sex, age, comorbidities, and number of medications. Covariates associated with blood pressure control with a _P_ value less than .10 were included in the final model. In step 2, we added provider characteristics, including PCP specialty, rural versus urban location, number of years of care with current PCP, and number of visits to PCP during the prior year. Covariates associated with blood pressure control with a _P_ value less than .10 were included in the final step 2 model. To examine the hypothesis that differential treatment patterns may contribute to observed differences in blood pressure control, we analyzed treatment characteristics for the subset of black and white patients with blood pressure above goal. Treatment characteristics included discussion of medication adherence, diet, weight reduction, exercise, sodium restriction, and moderation of alcohol; change in antihypertensive medication regimen in the prior year; and number of antihypertensive drug classes prescribed in combination. We additionally examined provider response to high blood pressure during office visits within the year before the most recent office visit, up to 5 visits per patient (n = 4,812 visits for blacks, n = 2,931 for whites). For visits with blood pressure above goal, we examined the likelihood that patients had the following care components: 1) documentation of a lifestyle recommendation (any recommendation for medication adherence, diet, weight reduction, exercise, sodium restriction, or moderation of alcohol), 2) change in antihypertensive medication regimen, and 3) a documented plan for follow-up care. To analyze data, we used SAS versions 9.1 and 9.2 (SAS Institute, Inc, Cary, North Carolina). Weights were applied to correct for the unequal chance of being selected for patient clusters within providers in the 4 sampling strata, and for unit nonresponse. Analyses accounted for the clustering of patients within providers and for stratification. For significance testing, the _F_ \\-adjusted Rao-Scott χ 2 square and Wald χ 2 square tests were used.  \n\n【11】Results\n-------\n\n【12】The proportion of patients who had met their blood pressure goal was significantly lower among black patients than white patients (43.6% vs 50.9%, _P =_ .001) . A greater proportion of blacks were women (74% vs 65%, _P <_ .01), and age distribution was similar. Although the presence of most comorbidities was similar, blacks were less likely to have hyperlipidemia, chronic kidney disease, chronic obstructive pulmonary disease, asthma, or reactive airway disease, and were less likely to smoke. Whites were more likely than blacks to have 8 or more total active medications (56.6% vs 46.6%). Looking specifically at antihypertensive medications, however, nearly half (46.7%) of blacks were on 3 or more antihypertensive drug classes compared with a third (31.3%) of whites. Geographic location, length of time with current PCP, and number of office visits in the prior year did not differ by race. In step 1 of the logistic regression modeling , when controlling for patient characteristics, blacks were significantly less likely than whites to have their blood pressure controlled (OR = 0.75; 95% confidence interval \\[CI\\], 0.61-0.93; _P =_ .009). Inclusion of provider characteristics to the model in step 2 had little additional effect on the association between race and blood pressure control (OR = 0.78; 95% CI, 0.64-0.96; _P =_ .02), and the relationship remained significant. In addition to race, diabetes, weight status, and PCP specialty other than family practice or internal medicine were associated with poor blood pressure control in the final model. Among patients who had not achieved their blood pressure goal (n = 1,157 blacks and n = 688 whites) , blacks were more likely than whites to have received counseling regarding sodium restriction (12% vs 8.5%, _P =_ .006), whereas other types of lifestyle recommendations (medication adherence, diet, weight reduction, exercise, and moderation of alcohol) did not differ significantly by race. Only 47.4% of black and 47.2% of white patients with blood pressure above goal had any documentation of lifestyle recommendations during the prior year. Use of combination antihypertensive therapy was more common among blacks ( _P_ < .001). Blacks were more likely than whites to have had a change of antihypertensive medication regimen during the prior year (46.7% vs 40.4%, _P_ \\= .02). A total of 14,583 office visits were reviewed. Blood pressure was elevated during 4,812 (57.2%) office visits during the prior year for blacks, and 2,931 (49.4%) office visits for whites . During office visits with above-goal blood pressure, blacks were significantly more likely than whites to have a documented lifestyle recommendation (medication adherence, diet, weight reduction, exercise, sodium restriction, or moderation of alcohol) (17.6% vs 13.9%, _P =_ .002) and more likely to have any documented intervention (medication change or lifestyle recommendation) (35.7% vs 30.0%, _P_ \\= .021). There was no significant difference between races in the likelihood of antihypertensive medication change. A follow-up care plan was noted during 64.3% of above-goal visits for blacks and 69.1% of above-goal visits for whites ( _P =_ .08). Planned follow-up within 4 weeks was noted for only 27% of these visits for both races.  \n\n【13】Discussion\n----------\n\n【14】In this statewide sample of Medicaid patients with high blood pressure managed in the primary care setting, blacks were less likely than whites to have their blood pressure controlled. We found that adjusting for observed patient and provider characteristics slightly attenuated the relationship between race and blood pressure control but did not completely explain racial differences. One strength of this study is that the sample is representative of a statewide Medicaid population with high blood pressure, spanning multiple systems of care and treatment localities. Medicaid recipients are characterized by many factors known to be associated with poor blood pressure control or poor health outcomes, including low socioeconomic status and higher prevalence of multiple comorbidities . Our findings are consistent with prior observations that racial differences in blood pressure control among treated patients are not explained by socioeconomic factors, nonpharmacological management, health insurance, or comorbidities . Despite health care coverage, access to care, and frequent office visits, an unexplained racial disparity in blood pressure control still exists. Provider characteristics, and quality and intensity of care have been shown to be significant causes of health disparities . Differences in blood pressure control may conceivably be due to less aggressive care patterns in black patients, culturally insensitive care, or other differences in counseling and follow-up . However, in our study, disparities in blood pressure control do not appear to be explained by differential treatment. Among those with blood pressure above goal, blacks were more likely than whites to have received counseling about sodium intake, to have been prescribed 3 or more blood pressure agents in combination, and to have a change of therapy within the prior year. Within each visit with high blood pressure, the likelihood of medication change and planned follow-up did not differ by race, although blacks were more likely than whites to receive a therapeutic lifestyle recommendation. Lack of appropriately aggressive care, or clinical inertia, has been cited as a cause for suboptimal control of chronic disease risk factors across much of the US health care system . We confirmed considerable evidence of clinical inertia for both black and white patients. Fewer than half of patients with blood pressure above goal had documentation of any lifestyle counseling in the past year. During visits with high blood pressure, medical therapy was changed on only 1 in 5 opportunities. In addition, 28% of black patients and 34% of white patients with uncontrolled blood pressure were treated with fewer than 2 antihypertensive agents, which may not be sufficient to achieve blood pressure goals . We were unable to explore many characteristics of patients, health systems, and environments that may contribute to racial disparities in blood pressure control, including health literacy, medication adherence, and barriers to following therapeutic lifestyle recommendations . Racial differences in the metabolic and hormonal pathogenesis of high blood pressure may contribute to the prevalence and severity of high blood pressure among blacks, although differences in socioeconomic conditions, access to care, and health-related knowledge or attitudes are thought to play a larger role . Researchers have examined the extent to which perceptions of racial/ethnic discrimination can adversely affect health . Negative attitudes attributed to discrimination have been linked to adverse physiologic reactions involving blood pressure, and researchers have hypothesized that the chronic triggering of these cardiovascular reactions due to discrimination could lead to the development of high blood pressure . These reactions may be caused by various factors, including worry about blood pressure, care-seeking behavior of patients, lack of trust, majority provider behavior toward minority patients, or miscommunication between patients and providers . This study had several limitations. We may have overestimated blood pressure control in this population because all patients sampled were receiving primary care services, and patients with more complicated disease (those seeing cardiologists and endocrinologists and those on dialysis) were excluded. Our study population had a lower proportion of patients older than 65 years than the source Medicaid population, probably because of these exclusions. Generalizability to other populations is also limited. Medicaid eligibility requires meeting state-specific thresholds of low income and assets, in addition to categorical requirements of being elderly, disabled, or pregnant, or having dependent children. Our analyses were limited to information obtainable in the medical record and relied on the accuracy of clinic blood pressure measurements and completeness of chart documentation, which may be particularly unreliable in assessing the extent of therapeutic lifestyle counseling. We counted as evidence of counseling any mention of lifestyle factors or medication adherence in the visit note. In summary, the gap between current care and ideal care for both black and white Medicaid recipients with high blood pressure is substantial, even among patients with frequent access to primary care. Racial disparities in blood pressure control are not readily explained by socioeconomic, demographic, or comorbidity differences or by provider characteristics or treatment patterns. Current care patterns are not sufficient to eliminate racial disparities in blood pressure control or to achieve desired treatment goals. The consequences of ineffective health care for high blood pressure, in terms of avoidable cardiovascular illness, death, and health care costs, disproportionately affect blacks. Emerging models of high blood pressure care, incorporating patient-centered care teams and planned, longitudinal stepped care approaches, show promise for improving outcomes across all patient populations . It cannot be assumed, however, that equal access and equal treatment will lead to equal outcomes. Closing the gap of racial disparities may require a more concerted clinical effort for racial minorities and better coordination between health care providers and community resources that can address cultural and health literacy needs and support patient self-management efforts in the home and community setting. Further research is needed to guide these efforts.  \n\n【15】Tables\n------\n\n【16】#####  Table 1. Blood Pressure Control, a Patient, Provider, and Treatment Characteristics of Medicaid Patients With Hypertension, by Race, North Carolina, 2005-2006 b\n\n【17】CharacteristicBlack (n = 2,078)White (n = 1,436)Total (N = 3,514)P Value cnWeighted % (95% CI)nWeighted % (95% CI)nWeighted % (95% CI)High blood pressure1,15554.8 (51.5-58.2)92364.4 (60.6-68.1)2,07859.2 (56.2-62.1)&lt;001Blood pressure at goal92143.6 (40.5-46.8)74850.9 (47.1-54.6)1,66946.9 (44.2-49.6).001Age group, y21-3937117.8 (15.2-20.5)24216.6 (13.4-19.7)61317.3 (15.0-19.5).6840-641,30262.3 (59.6-65.1)91764.3 (61.3-67.3)2,21963.2 (61.0-65.5)≥6540519.8 (16.4-23.3)27719.1 (15.9-22.4)68219.5 (16.9-22.2)SexMen52225.8 (23.2-28.3)49034.6 (31.9-37.3)1,01229.8 (27.7-31.8)&lt;001Women1,55674.2 (71.7-76.8)94665.4 (62.7-68.1)2,50270.2 (68.2-72.3)Weight distribution dNormal (BMI &lt;25 kg/m 2 )29814.5 (12.5-16.4)22715.6 (13.6-17.6)52515.0 (13.4-16.5).40Overweight (BMI 25-29.9 kg/m 2 )44121.9 (19.3-24.4)34023.4 (21.0-25.8)78122.6 (20.8-24.3)Obese (BMI &gt;30&nbsp; kg/m 2 )1,29263.7 (60.2-67.1)84561.0 (58.6-63.5)2,13762.5 (60.3-64.6)Comorbidities and risk factorsDiabetes80239.6 (37.2-42.1)53037.9 (34.5-41.2)1,33238.8 (36.8-40.9).40Hyperlipidemia82641.6 (37.7-45.6)67646.7 (43.7-49.7)1,50243.9 (41.1-49.3).02Cardiovascular disease37619.2 (16.5-21.9)27017.4 (14.2-20.7)64618.4 (16.2-20.7).38Current tobacco use e56439.4 (34.7-44.2)54951.5 (47.0-55.9)1,11345.2 (41.1-49.3)&lt;001Chronic kidney disease (eGFR&lt;60) f41224.7 (22.0-27.5)38428.7 (26.5-30.8)79626.5 (24.5-28.5).01COPD or asthma/reactive airway disease35717.5 (15.5-19.5)32722.8 (19.2-26.4)68419.9 (17.9-21.9).006No. of total active medications0-331914.5 (12.1-16.9)1348.8 (6.3-11.3)45311.9 (10.3-13.6)&lt;0014-781138.9 (35.5-42.2)50334.5 (31.7-37.3)1,31436.9 (34.6-39.2)≥894846.6 (42.4-50.8)79956.6 (53.5-59.8)1,74751.2 (5.9-24.3)Provider and treatment characteristicsPCP specialtyGeneral/family practice97149.6 (34.9.64.3)85263.0 (45.3-80.5)1,82355.7 (41.0-70.3).03Internal medicine82130.5 (17.4-43.7)48027.6 (12.1-43.0)1,30129.2 (16.5-41.9)Other/unknown28619.8 (8.9-30.8)1049.5 (1.4-17.5)39015.1 (5.9-24.3)Geographic location, by provider county gRural95757.3 (43.6-71.0)66649.0 (28.9-69.0)1,62353.5 (38.2-68.8).26Urban1,12142.7 (29.0-56.4)77051.0 (31.0-71.1)1,89146.5 (31.2-61.8)Length of time with current PCP, y h&gt;12018.7 (6.5-10.9)1167.6 (5.5-9.7)3178.2 (6.6-9.9).701-271633.1 (27.1-39.1)47631.8 (27.0-36.7)1,19232.5 (27.5-37.5)3-436018.8 (15.1-22.5)28720.7 (17.4-24.0)64719.7 (16.6-22.7)≥571639.4 (32.1-46.7)53539.9 (34.7-45.0)1,25139.6 (34.1-45.1)No. of PCP visits in past year1-226213.4 (11.1-15.7)1379.8 (6.7-13.0)39911.8 (9.7-13.8).093-456926.8 (23.3-30.4)35225.9 (22.3-29.4)92126.4 (23.4-29.4)≥51,24759.7 (54.9-64.6)94764.3 (58.4-70.1)2,19461.8 (57.4-66.2)No. of antihypertensive drug classes prescribed0-172232.3 (28.8-35.8)60740.6 (36.3-44.9)1,32936.1 (33.1-39.1)&lt;001241320.9 (28.9-35.8)37028.0 (21.4-34.6)78324.1 (19.7-28.6)349625.5 (23.2-27.9)27619.3 (17.6-21.1)77222.7 (21.0-24.4)≥444721.2 (18.4-24.1)18312.0 (9.0-15.0)63017.1 (14.3-19.9)Abbreviations: CI, confidence interval; BMI, body mass index; GFR, glomerular filtration rate; COPD, chronic obstructive pulmonary disease; PCP, primary care provider; JNC, Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure.a Blood pressure at goal according to JNC-7 standards; <130/80 mm Hg for patients with diabetes; otherwise <140/90 mm Hg .b Variables with missing data overall and by race are as follows: tobacco use (overall = 1,077, black = 717, and white = 360), chronic kidney disease (eGFR<60) (overall = 332, black = 211, and white = 121), and length of time with current PCP (overall = 107, black = 85, and white = 22). Total n for blacks, 2,078; for whites, 1,436; and overall, 3,514 (no missing data for sex, age group, both blood pressure measures, diabetes, hyperlipidemia, cardiovascular disease, COPD/asthma, provider location, PCP visits in past year).c _P_ value based on _F_ \\-adjusted Rao-Scott χ 2 test comparing black with white patients.d Weight distribution for those patients for whom both height and weight were documented in the medical chart. For patients without height, median height of the population was used (total n = 3,443; black n = 2,031; white n = 1,412. No weight abstracted for 71 patients).e Tobacco use among those who have been screened for tobacco use and whose status was known (total n = 2,437; black n = 1,361; white n = 1,076).f Chronic kidney disease for those for whom eGFR was available (total n = 3,182; black n = 1,867; white n = 1,315).g Providers located in a county with a population density of more than 200 people per square mile, according to US Census 2000 data, were classified as urban; all others were classified as rural.h For 107 patients, no first visit date was abstracted. Therefore, length of care with their provider could not be established. \n\n【18】#####  Table 2. Odds of Blood Pressure at Goal Among Black Versus White Medicaid Patients With Hypertension, North Carolina, 2005-2006\n\n【19】Characteristic aStep 1: Patient CharacteristicsStep 2: Treatment CharacteristicsFull ModelFinal ModelFull ModelFinal ModelOdds Ratio (95% CI)P Value aOdds Ratio (95% CI)P Value aOdds Ratio (95% CI)P Value aOdds Ratio (95% CI)P Value aPatientRace (black vs white)0.78 (0.64-0.96).020.75 (0.61-0.93).0090.79 (0.65-0.97).0240.78 (0.64-0.96).02Age1.00 (0.99-1.00).39NCNCNCNCNCNCSex (men vs women)1.06 (0.82-1.37).64NCNCNCNCNCNCWeight (vs BMI &lt;25) bOverweight (BMI 25-29.9 kg/m 2 )0.96 (0.75-1.24).760.88 (0.70-1.11).2860.86 (0.68-1.08).2020.88 (0.70-1.11).29Obese (BMI ≥30 kg/m 2 )0.75 (0.59-0.97).030.74 (0.60-0.91).0050.74 (0.60-0.91).0050.76 (0.62-0.93).008Comorbidities and risk factorsDiabetes0.24 (0.18-0.32)&lt;0010.26 (0.21-0.31)&lt;0010.25 (0.20-0.31)&lt;0010.26 (0.21-0.31)&lt;001Hyperlipidemia1.00 (0.82-1.21).97NCNCNCNCNCNCCardiovascular disease0.86 (0.70-1.05).13NCNCNCNCNCNCCurrent tobacco use c1.12 (0.91-1.38).29NCNCNCNCNCNCChronic kidney disease (eGFR &lt;60) d0.86 (0.72-1.04).12NCNCNCNCNCNCCOPD or asthma/reactive airway disease0.96 (0.72-1.28).77NCNCNCNCNCNCNo. of medications1.03 (1.00-1.06).03NCNCNCNCNCNCProvider and treatment characteristicsPCP specialty e (vs family practice)Internal medicine SpecialtyNCNCNCNC0.96 (0.73-1.27).780.96 (0.73-1.26).76Other/unknown specialtyNCNCNCNC0.68 (0.51-0.90).0070.68 (0.52-0.90).007Rural vs urban eNCNCNCNC1.06 (0.85-1.32).63NCNCTime with PCP, y f&lt;1 (vs &gt;5)NCNCNCNC0.89 (0.68-1.17).40NCNC1 to &lt;3 (vs &gt;5)NCNCNCNC1.00 (0.83-1.21).99NCNC3 to &lt;5 of care (vs &gt;5)NCNCNCNC1.20 (0.94-1.53).14NCNCNo. of PCP visits1-2 (vs &gt;5)NCNCNCNC0.74 (0.52-1.06).10NCNC3-4 (vs &gt;5)NCNCNCNC0.91 (0.76-1.10).32NCNCAbbreviations: CI, confidence interval; NC, not calculated; BMI, body mass index; COPD, chronic obstructive pulmonary disease; PCP, primary care provider; GFR, glomerular filtration rate.a Calculated with Wald χ 2 test.b BMI is calculated as weight in kilograms divided by height in meters squared.c Tobacco use among those who have been screened for tobacco use and whose status was known (total, n = 2,437; black, n = 1,361; white, n = 1,076).d Chronic kidney disease for those for whom eGFR was available (total, n = 3,182; black, n = 1,867; white, n = 1,315).e Providers located in a county with a population density of more than 200 persons per square mile, according to US Census 2000 data, were classified as urban; all others were classified as rural.f For 107 patients, no first visit date was abstracted. Therefore, length of care period with their provider could not be established. \n\n【20】#####  Table 3. Treatment Characteristics for Medicaid Patients at Above Goal Blood Pressure, a by Race, North Carolina, 2005-2006\n\n【21】Treatment CharacteristicBlack (n = 1,157)White (n = 688)P Value bnWeighted % (95% CI)nWeighted % (95% CI)PCP discussed the following topics during the yearMedication adherence15915.0 (11.8-18.2)7612.1 (7.7-16.6).22Diet31628.6 (22.9-34.3)21030.6 (23.3-38.0).53Weight reduction16013.8 (9.90-17.7)11616.7 (13.1-20.2).18Exercise24322.6 (17.3-27.9)14021.8 (17.2-26.3).70Sodium restriction13212.0 (8.3-15.7)678.5 (5.9-11.1).006Moderation of alcohol352.6 (0.3-5.0)202.0 (0.0-4.0).42Any lifestyle recommendation was provided cNo64052.6 (46.4-58.8)37552.8 (46.6-58.7).97Yes51747.4 (41.2-53.6)31347.2 (41.3-53.2)Number of antihypertensive drug classes prescribed0-135228.0 (24.4-31.5)24534.3 (27.1-41.6)&lt;001223320.8 (18.1-23.4)19330.9 (22.5-39.3)328726.6 (23.8-29.4)15021.3 (18.3-24.4)≥428524.7 (21.6-27.7)10013.4 (9.6-17.2)Change in antihypertensive medication regimen in the prior year55246.7 (41.9-51.5)28340.4 (35.6-45.2).02Screened for the following risk factorsDiabetes1,10696.3 (94.8-97.9)64895.0 (92.8-97.2).28Cholesterol94382.5 (79.0-85.9)58685.2 (81.1-89.3).30Family history58355.1 (46.2-64.0)42165.2 (55.7-74.7).04Smoking76468.7 (61.0-76.4)51174.0 (65.4-82.7).28Obesity27522.3 (15.4-29.2)18227.8 (21.6-34.0).07Abbreviation: CI, confidence interval; PCP, primary care provider.a High blood pressure was defined as ≥140/90 mm Hg and ≥130/80 mm Hg for patients with diabetes .b Calculated with _F_ \\-adjusted Rao-Scott χ 2 test.c Includes any documentation that medication adherence, diet, weight reduction, exercise, sodium restriction, or moderation of alcohol was addressed. \n\n【22】#####  Table 4. Provider Response to High Blood Pressure a During Office Visits, by Race, North Carolina, 2005-2006\n\nProvider ResponseOffice Visits With Black Patients With High Blood Pressure,n = 4,812 (57.2%)Office Visits With White Patients With High Blood Pressure,n = 2,931 (49.4%)P Value bnWeighted % (95% CI)nWeighted % (95% CI)Change in antihypertensive medication regimen1,06422.5 (18.9-26.1)55319.6 (16.3-22.9).18Lifestyle recommendation (total) c84317.6 (14.5-20.6)41313.9 (11.5-16.2).002Visits during which any intervention was noted (medication or lifestyle recommendation)1,68335.7 (30.9-40.4)86430.0 (26.5-33.6).02Any plan for follow-upYes3,05164.3 (56.9-71.6)1,97969.1 (64.5-73.7).08No1,76135.7 (28.4-43.1)95230.9 (26.3-35.5)Follow-up plan within 4 weeksYes1,29827.0 (23.7-30.2)80227.1 (23.4-30.8).95No3,51473.0 (69.8-76.3)2,12972.9 (69.2-76.6)Abbreviation: CI, confidence interval.a High blood pressure was defined as ≥140/90 mm Hg and ≥130/80 mm Hg for patients with diabetes .b Calculated with _F_ \\-adjusted Rao-Scott χ 2 test.c Includes any documentation that medication adherence, diet, weight reduction, exercise, sodium restriction, or moderation of alcohol was addressed.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fefbee1b-a56c-42a2-9720-2dc90a24be74", "title": "Why Are We by All Creatures Waited on?", "text": "【0】Why Are We by All Creatures Waited on?\nLucas Cranach the Elder  Cardinal Albrecht of Brandenburg as St. Jerome  Oil on wood panel (114.9 cm × 78.9 cm) John and Mable Ringling Museum of Art, The State Art Museum of Florida, a division of Florida State University, USA\n\n【1】Painter, engraver, designer of woodcuts, Lucas Cranach the Elder lived and worked during the last part of the Renaissance and embodied the integrative qualities valued by his age. In addition to accomplished artist, he was a successful entrepreneur, civic leader, and brilliant inventor. Among other innovations, he is credited with influencing the Danube school, a circle of painters along the Danube Valley, known for their advanced painterly style, printmaking, and etching; for early printing of woodcuts in color; for the full length portrait as an independent art form; and for various techniques intended to speed up painting and standardize technical processes.\n\n【2】Cranach’s early years and travels are sketchy. He was born in Germany’s Upper Franconia, the small town of Kronach, from which he took his name. He probably received the first art training from his father, the painter Hans Maler. A contemporary of two major artists from Germany, Matthias Grünewald and Albrecht Dürer , Cranach competed with them on local projects and patronage and often turned especially to Dürer’s work for inspiration. At age 30, he moved to Vienna, where he made influential associations with local humanists and painted two of his finest portraits. Some religious works showing appreciation of the beauty of nature, a characteristic of the Danube School, also date from this period. Soon he moved to Wittenberg to serve as court painter to Frederick the Wise, elector of Saxony, a position he retained for life under various electors.\n\n【3】Cranach became “one of the wealthiest burghers in Wittenberg.” He was elected city councilman several times and mayor twice, while fully engaged in painting, engraving, and directing all aesthetic needs of the court. He established and ran a prosperous studio that produced copies of his best works and all manner of decorative arts, from coin designs for the electorate to furniture. His sons, Hans and Lucas Cranach the Younger, both artists, worked in this studio and used his style so successfully that it is still difficult to fully authenticate what was done by his hand alone. The enterprise continued for decades after his death. His business acumen was such that he also ran a publishing press and had licenses to sell wine and own an apothecary.\n\n【4】Cranach’s greatest artistic contribution was his landscapes, which included elaborately detailed animals. He added to period art and Dürer’s naturalism an element of fantasy through the ornate treatment of forms. He also painted female nudes, whimsical mythologic scenes, and religious images containing contemporary everyday features. He is well remembered for his portraits, now a repository of the major figures of his age. He left behind perhaps the best portraits of Martin Luther. Other commemorated notables included Luther’s family and, despite Cranach’s own commitment to Protestantism, many Catholic clergy despised by Luther.\n\n【5】_Cardinal Albrecht of Brandenburg as St. Jerome_ , on this month’s cover, is a culmination of many elements in Cranach’s work. This type of portrait, borrowing the image of a respected person to elevate that of another, was not unusual at this time. Dürer and others practiced it successfully, and many a churchman honored it. Invoking the virtues and protection of the saint in this portrait was Cardinal Albrecht, elector and archbishop of Mainz , a patron of artists and intellectuals as well as defender of the faith during the Counter-Reformation. Accused of extravagance and worldliness, he commissioned this portrait, in the guise of a religious icon, to proclaim his own beliefs and values. An admirer of St. Jerome, he wished to be likened to him. This revered saint, the most learned man of his age, was known not so much for his asceticism, which was without blemish, but for his knowledge―translating the Bible from Greek and Hebrew into Latin, a crowning literary achievement even by today’s standards.\n\n【6】At the time of St. Jerome , there were no cardinals in the Catholic Church. The iconography that would come to define the saint “… sitting in a chair, beside him that hat which cardinals wear nowadays and at his feet the tame lion,” was put forth by Giovanni d’Andrea, a canonical lawyer at the University of Bologna, in a biography of the saint and was perpetuated in later accounts. The lion legend behind the imagery goes all the way back to Aesop, but with the saint as the figure removing a thorn from the lion’s paw and forever gaining the beast’s loyalty and affection.\n\n【7】Jerome was a popular subject in art. Cranach alone painted at least eight images of the saint. The one on this month’s cover was patterned after Dürer’s effort on the same theme, a mirror image. Same small space, a cabinet or private room serving as a study or retreat—its mathematical perspective focusing on the main figure and the positioning of objects making the space appear larger; same ample window, allowing light and shadow effects; same antler-laden chandelier. Cranach and Dürer were colleagues, though Jerome is Cranach’s patron, the all-powerful cardinal. Somber, dour even, he sits ambiguously at the lectern, on a contemplative break from reading. His hat is shown in the foreground.\n\n【8】The painting is a treasure trove of symbolism, from the religious artifacts on the wall and in the cupboard behind the saint to the books, cups, fruit, hourglass, slippers, and other objects on and around the desk; and finally the animals, an unlikely menagerie, intended to symbolize characteristics laudable to the patron: monogamy, industriousness, frugality, loyalty, rejection of earthly desires. Thrown in with the other objects, the animals seem posed and indifferent to each other and their surroundings.\n\n【9】Despite the naturalism so valued in the art of Cranach’s time and his own interest in painting them, the animals in this portrait of St. Jerome remain just icons of human values. Stylized and scattered at the feet of the saint, they show none of their individuality or wildness. John Donne , a cleric as well as poet of the ages, aware of this two-dimensional treatment of animals, and not only in art, addressed what he saw as the inherent unfairness in perceiving them as strictly serving the interests of humans. In one of his holy sonnets, he asked outright, “Why are we by all creatures waited on?” and why do these creatures that are “more pure than I,” since they “have not sinned,” “provide food to me?”\n\n【10】Images of animals in art have changed, becoming more accurate and refined as humans were able to travel the world, see them in their natural habitat and study their anatomy and physiology. And the way animals are perceived by humans has changed science. Donne’s questions may not be entirely resolved but are eased by current understanding of animal–human phylogenetic closeness and knowledge of the zoonoses. Less an adversarial “them or us” issue, this current relationship is one of connectivity and sharing, on the physiologic as well as the emotional level. Many animals have gone from simply being domesticated to becoming members of human families and valued companions as pets. Others have come closer to humans as urbanization closed in on their habitat, and yet others have traveled far from their original nests. They all, too, on the zoonotic level, in the home and in the wild, share their infections freely and interact with humans outside traditional areas of exposure.\n\n【11】In this issue of the journal, human Hendra virus infection was acquired by close contact with horses infected by spillover from fruit bats, the natural reservoir for these viruses. MRSA organisms harboring a novel variant were detected in cats and dogs, which suggests that the variant is not restricted to human hosts. In Bangladesh, where HPAI H5N1 is endemic in poultry, live bird markets are a factor in human exposure. In the United States, agricultural fairs have been associated with bidirectional, influenza virus transmission between swine and humans. Fairgoers without routine occupational exposure to swine not only may be more susceptible to swine influenza viruses than those routinely exposed, they may also expose swine to a broader range of influenza A viruses for additional mixing. Identifying risk factors for transmission in both these venues would represent a step toward effective control.\n\n【12】Like Cranach’s animals, these human–animal interactions are symbolic of other, larger values, not moral and religious but biologic. Because, despite their surface unrelatedness, the interactions described in reports from around the world tie into one important common denominator, their zoonotic potential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5c25ad37-7930-4215-94b4-b36863bb454b", "title": "A Rapid Evaluation of the US Federal Tobacco 21 (T21) Law and Lessons From Statewide T21 Policies: Findings From Population-Level Surveys", "text": "【0】A Rapid Evaluation of the US Federal Tobacco 21 (T21) Law and Lessons From Statewide T21 Policies: Findings From Population-Level Surveys\nAbstract\n--------\n\n【1】**Background**\n\n【2】On December 20, 2019, the minimum age for purchasing tobacco in the US was raised nationally to 21 years. We evaluated this law (Tobacco 21 \\[T21\\]) 1 year after implementation. We also compared states with versus without T21 policies during 2019 to explore potential equity impacts of T21 policies.\n\n【3】**Methods**\n\n【4】We examined shifts in tobacco access among 6th through 12th graders using the National Youth Tobacco Survey. To explore equity of state T21 policies among youths and young adults, the associations with tobacco use were explored separately for race and ethnicity by using data from the 2019 Behavioral Risk Factor Surveillance System (for persons aged 18 to 20 years) and the 2019 Youth Risk Behavior Survey (for high school students).\n\n【5】**Results**\n\n【6】The overall percentage of 6th to 12th graders perceiving that it was easy to buy tobacco products from a store decreased from 2019 (67.2%) to 2020 (58.9%). However, only 17.0% of students who attempted buying cigarettes in 2020 were unsuccessful because of their age. In the 2019 BRFSS, those aged 18 to 20 years living in a state with T21 policies had a lower likelihood of being a current cigarette smoker (adjusted prevalence ratio \\[APR\\], 0.58) or smoking cigarettes daily (APR, 0.41). Similar significant associations were seen when analyses were restricted to only non-Hispanic White participants but not for participants who were non-Hispanic Black, non-Hispanic Asian, Hispanic, or of other races or ethnicities. Consistent findings were seen among high school students.\n\n【7】**Conclusion**\n\n【8】Greater compliance with the federal T21 law is needed as most youth who attempted buying cigarettes in 2020 were successful. Comparative analysis of states with versus states without statewide T21 policies in 2019 suggest the policies were differentially more protective of non-Hispanic White participants than other participants. Equitable and intensified enforcement of T21 policies can benefit public health.\n\n【9】Introduction\n------------\n\n【10】Effective December 20, 2019, the US passed a federal law raising the minimum age of purchasing tobacco products from 18 years to 21 years (Tobacco 21 \\[T21\\]). This federal law was motivated by a large body of evidence showing that states and jurisdictions that had passed such policies subsequently witnessed reductions in tobacco consumption . The first 2 states to implement statewide T21 policies were Hawaii (January 1, 2016) and California (June 9, 2016) . New Jersey, Maine, and Oregon followed next in 2017 . Jurisdictions in various states also passed widely varying levels of T21 coverage of the population aged 18 to 20 years .\n\n【11】Evaluations of state and local T21 policies have been conducted by using sales and population-level data, both showing reduced overall tobacco consumption as an immediate effect . After California and Hawaii passed statewide T21 policies, monthly cigarette volume sales declined between January 2014 and December 2018 for both California (13.1%) and Hawaii (18.2%), compared with western states that had not implemented the T21 laws . Results from another study in Hawaii showed a significant drop in average monthly cigarette unit sales, together with a significant drop in menthol cigarette market share . During 2015 through 2019, cigarette sales declined for brands disproportionately used by smokers under the age of 21 . Friedman and Wu  assessed the effectiveness of local T21 policies across metropolitan and micropolitan statistical areas and metropolitan divisions (MMSAs); they found that the likelihood of smoking among youths living in an MMSA with full T21 coverage was reduced by 3.1% and among those living in an MMSA with partial T21 coverage, it was reduced by 1.2%.\n\n【12】To date, however, no study has evaluated the impact of the federal T21 law on tobacco use behaviors and perceptions. Furthermore, from an implementation perspective, it is important to glean insights learned from statewide implementation of T21 policies so they can be applied to the nationwide law. Our study was therefore conducted from both evaluation and implementation research perspectives. Our rapid evaluation sought to answer the question “One year after its passage, what has been the impact of the federal T21 law on tobacco use behaviors and perceptions among youths?” From an implementation research perspective, we sought to answer the following secondary questions: 1) Did the impact of state T21 policies on tobacco use behaviors vary among the different race and ethnic groups during 2019? (ie, was there effect modification by race or ethnicity when comparing tobacco-related outcomes between states with and states without T21 policies within strata of race or ethnicity?); and 2) Was there a differential effect of state T21 policies on cigarettes versus noncigarette tobacco products? Noncigarette tobacco products assessed were smokeless tobacco, e-cigarettes, and cigars. Because social contacts are important for tobacco access among youths who are generally price-sensitive , and because high-school–aged youths (especially those aged 16 and 17) may be socially aligned with 18-year-olds, we also sought to compare how the impact of state T21 policies compared between the primary target of T21 policies (persons aged 18–20 years) and high school youths.\n\n【13】Methods\n-------\n\n【14】### Data sources\n\n【15】Within the secondary data analysis, multiple publicly available de-identified data sets were explored to answer the research questions. All analyzed data sets drew probabilistic samples from the noninstitutionalized US population. These data sets were the 2011 through 2020 National Youth Tobacco Survey (NYTS) — an annual school-based survey of US middle and high school students; the 2019 Youth Risk Behavior Survey (YRBS) — a biennial school-based survey of US high school students; and the 2019 and 2020 waves of the Behavioral Risk Factor Surveillance System (BRFSS) — an annual telephone-based survey of US adults aged 18 years or older (from which we restricted analyses to the population of young adults aged 18 to 20 years). YRBS yields both national and state-specific estimates; BRFSS yields only state-specific estimates, and NYTS yields only national estimates. With T21 now being a federal law, NYTS was used to explore shifts in perceived ease of access of tobacco products and tobacco purchase among youths. YRBS and BRFSS were used to explore lessons from statewide T21 policies, comparing implementation outcomes between those living in states with and states without T21 policies during 2019. Data were collected from these data sources on various indicators, including tobacco use, access patterns for different tobacco products, and sociodemographic characteristics.\n\n【16】With the passing of time and the collection of more years of postimplementation data, longer-term evaluation of the federal T21 policy would be important to guide public health programs, policy, and practice. Our rapid evaluation, while conducted 1 year after policy implementation, is still valid because the data are fit for use and fit for purpose. It is inevitable that a large segment of the population surveyed in 2020 was potentially exposed to the federal T21 policy even if there was a time lag between enactment and enforcement. For example, over half (61.3%) of the 2020 BRFSS participants were surveyed between June 2020 and January 2021, suggesting that most of the 2020 survey population were potentially exposed to the federal T21 law even if we assumed that the law had no measurable influence until mid-2020 or thereafter.\n\n【17】### Measures\n\n【18】#### Evaluation outcomes for the federal T21 Law\n\n【19】With the first full year of the federal T21 law (ie, 2020) coinciding with the first full year of COVID-19, evaluating the effect of the federal T21 law on tobacco consumption is challenging, considering evidence showing COVID-19–attributable increase in tobacco consumption . Consequently, our evaluation framework considered not only consumption but also attitudes and perceptions specific to tobacco access among youths.\n\n【20】In NYTS, perceived ease of minors buying tobacco products from a store and online was defined as a response of “easy” or “somewhat easy” (vs “not easy at all”) to the question “How easy do you think it is for people your age to buy tobacco products \\[in a store/online\\]?” Students were further asked, “During the past 30 days, did anyone ever refuse to sell you cigarettes because of your age?” Categorical response options were: “I did not try to buy cigarettes in a store during the past 30 days,” “No, no one refused because of my age,” and “Yes, someone refused because of my age.” Either of the latter 2 responses was classified as having made an attempt in the past 30 days to buy cigarettes. A response of “No, no one refused because of my age” was classified as having made a successful cigarette purchase.\n\n【21】#### Implementation outcomes from statewide T21 policies\n\n【22】With half a decade having passed since the first statewide T21 policies in California and Hawaii, we were interested in implementation-related end points addressing the equity impact of T21 policies. Secondary research questions were 1) What equity impact did state T21 policies have along the lines of race or ethnicity? and 2) Was there a differential effect of state T21 policies on cigarettes vs noncigarette tobacco products? Our interest in examining whether the effect of statewide T21 policies was different among those who were White, versus those of racial or ethnic minority groups, was motivated by previous research showing more lax enforcement of access laws in communities of color coupled with targeted marketing of tobacco products in those same communities by the tobacco industry . Addressing any potential uneven impact of T21 policies across tobacco product types has equity relevance, considering the popularity of some noncigarette products among certain subgroups (eg, cigars among Black respondents ). Noncigarette tobacco products assessed in our study included smokeless tobacco, e-cigarettes, and cigars (current use defined as past 30-day use in YRBS or as use every day or on some days in BRFSS where applicable). For cigarettes, we explored outcomes that discriminated between frequent and nonfrequent smoking, based on findings from studies showing reduced consumption following implementation of statewide T21 policies . In BRFSS, we examined the following 3 outcomes representing increasingly higher frequency of cigarette smoking: smoked at least 100 cigarettes regardless of whether they now smoke (ie, cumulative threshold of cigarettes smoked); current cigarette smoker (ie, smoked at least 100 cigarettes and smoke every day or some days); and daily cigarette smoker (ie, smoked at least 100 cigarettes and smoke every day). In YRBS, we examined the following 2 outcomes: heavy cigarette smoking (ie, smoke ≥11 cigarettes per day \\[CPD\\]) and any past 30-day smoking (smoked ≥1 of the past 30 days).\n\n【23】In the 2019 cycles of YRBS and BRFSS, we classified states as having statewide T21 policies if said policies had been enacted and had gone into effect about a year before the passage of the federal T21 law on December 20, 2019 (California, Hawaii, Maine, Massachusetts, New Jersey, and Oregon ). Conversely, states that had never enacted statewide T21 policies, or those whose enacted T21 policies were not set to go into effect until 2019 or thereafter, were classified as not having T21 statewide policies as of the study period (all other states).\n\n【24】### Analysis\n\n【25】The percentage of US middle and high school students participating in NYTS who attempted buying cigarettes from a store in the past 30 days, and the percentage who were denied a cigarette sale because of their age among those who tried, was assessed overall across survey years and further stratified by school level, sex, and racial and ethnic group (Hispanic, non-Hispanic White, non-Hispanic Black, non-Hispanic Asian, and non-Hispanic other race \\[American Indian/Alaska Natives, Native Hawaiian/Other Pacific Islander, and multiracial\\]). Exploratory logistic regression analysis was used to explore correlates of perceived ease of buying tobacco products from a store and of making a successful cigarette purchase from a store among all students during 2020. Among those aged 18 to 20 years participating in the BRFSS in each state, we computed the percentage who reported current and daily cigarette smoking and compared between 2019 and 2020 using χ 2  tests (significant at _P_ < .05).\n\n【26】To measure the equity impact of statewide T21 policies as our implementation-related outcomes, we contrasted tobacco use outcomes (current cigarette, cigar, smokeless tobacco, and e-cigarette use) between those living in states with and states without statewide T21 policies in 2019, overall and within strata of race and ethnicity. Adjusted prevalence ratios (APRs) were calculated, adjusting for sociodemographic factors and alcohol use. The year 2019 was used because it preceded the federal T21 policy, allowing a distinction to be made between states exposed to T21 versus those unexposed. The year 2019 also preceded the COVID-19 pandemic in the US, thus eliminating potential confounding by the pandemic. All analyses were weighted and performed with Stata version 14 (StataCorp LLC).\n\n【27】Results\n-------\n\n【28】### Changes in tobacco use and perceptions\n\n【29】Within BRFSS, only 4 states had a significant decline in prevalence of current cigarette smoking among those aged 18 to 20 years during 2019 and 2020: Delaware, Florida, North Carolina, and West Virginia. In contrast, 8 states saw a decline in prevalence of current daily cigarette smoking among those aged 18 to 20 years during 2019 and 2020: Delaware, Maine, Nebraska, Nevada, New Mexico, Oklahoma, Pennsylvania, and South Dakota. During 2020, a total of 311,361 young adults aged 18 to 20 years smoked cigarettes daily, based on weighted population counts from all 50 states and the District of Columbia. The states with the highest prevalence of current daily cigarette smoking among those aged 18 to 20 years were Kentucky (7.2%), New Mexico (7.2%), Oregon (7.2%), Oklahoma (7.3%), Alaska (7.6%), Arkansas (8.9%), North Dakota (10.8%), Montana (11.7%), Mississippi (12.4%), Wyoming (14.4%), and Tennessee (18.1%).\n\n【30】Analysis of NYTS data revealed that the overall percentage of US middle and high school students who perceived it was easy to buy tobacco products from a store decreased overall between 2019 (67.2%) and 2020 (58.9%) ( _P_ < .001). However, this shift in perception was significant only for perceived in-store access, not perceived online access. The overall percentage who perceived it was easy getting tobacco products online was high and did not change significantly between 2019 (86.6%) and 2020 (85.8%). Furthermore, in 2020, 76.0% of US middle and high school students who perceived it would be difficult getting tobacco products from a store felt it was easy getting them online. Although perceived ease of accessing tobacco products from a store increased with increasing grade level during 2020 ( _P_ trend < .001), no significant trend was observed for perceived ease accessing tobacco products online ( _P_ trend = .26) . Among racial or ethnic population subgroups, Black students were the only ones to report no significant change in perceived access of tobacco products from a store during 2011 through 2020  .\n\n【31】**  \nChanges by grade level between 2019 and 2020 in the percentage of students who perceived it would be easy to get tobacco products in a physical store as well as online, National Youth Tobacco Survey. Students were asked “How easy do you think it is for people your age to buy tobacco products in a store?” and “How easy do you think it is for people your age to buy tobacco products online?” Categorical response options were “easy,” “somewhat easy,” or “not easy at all.” Any response other than “not easy at all” was classified as perceiving buying tobacco products as easy. \n\n【32】Overall, 10.1% of all US middle and high school students reported trying to buy a cigarette in the past 30 days, down from 14.1% in 2018 ( _P_ < .001). Attempted cigarette purchase in 2020 increased by grade level and was 2.7%, 4.3%, 5.4%, 9.6%, 15.3%, 16.0%, and 19.0% among students in the 6th, 7th, 8th, 9th, 10th, 11th, and 12th grades, respectively ( _P_ trend < .001). Most middle and high school students who tried purchasing cigarettes in the past 30 days successfully bought them, with only 17.0% overall reporting in 2020 that someone refused to sell cigarettes to them because they were underaged, an increase from 14.2% in 2018.\n\n【33】Multivariable logistic regression for all US middle and high school students during 2020 revealed that, whereas differences existed in the odds of perceiving that buying tobacco products from a store was easy along the lines of sex, race, and grade level, these differences were not significant when it came to reporting past 30-day success in purchasing cigarettes from a store among all students . Perceived ease of buying tobacco products was lower among male than female students (adjusted odds ratio \\[AOR\\], 0.80; 95% CI, 0.74–0.88), higher among Hispanic students than non-Hispanic White students (AOR, 1.11; 95% CI, 1.01–1.23), and increased with increasing grade levels, being 1.54, 2.35, 2.99, 4.19, 3.95, and 5.46 among the 7th, 8th, 9th, 10th, 11th, and 12th grades, respectively, compared with the 6th grade (all _P_ < .05). None of these factors, however, were associated with making a successful cigarette purchase in 2020.\n\n【34】### Differential effects of statewide T21 policies\n\n【35】Comparative analysis of states with versus without statewide T21 policies in 2019 revealed that such policies were differentially more protective of White adolescents than other adolescents and young adults. These findings were seen consistently in analyses of both the BRFSS data among those aged 18 to 20 years as well as within the YRBS among high school students. Results from BRFSS analysis of all those aged 18 to 20 years showed that those living in a state with T21 policies during 2019 had lower likelihood of reporting having smoked up to 100 cigarettes (APR, 0.71; 95% CI, 0.53–0.94), being a current cigarette smoker (APR, 0.58; 95% CI, 0.39–0.86), or smoking cigarettes daily (APR, 0.41; 95% CI, 0.23–0.74) . Similar associations were seen when analyses were restricted to only White young adults aged 18 to 20 years; living in an area with statewide T21 policies was associated with lower likelihood of White young adults having smoked 100 cigarettes (APR, 0.69; 95% CI, 0.49–0.97), being current cigarette smokers (APR, 0.60; 95% CI, 0.39–0.92), or smoking cigarettes daily (APR, 0.34; 95% CI, 0.17–0.71). None of these associations were significant for young adults who were Black, Asian, Hispanic, or of other races or ethnicities. Conversely, among Black young adults, statewide T21 policies were associated with current smokeless tobacco use (APR, 11.98; 95% CI, 4.55–31.55).\n\n【36】Analysis of YRBS data on high school students showed consistent findings. Living in an area with statewide T21 laws during 2019 was associated with significantly lower likelihood of being a current smoker of cigars among all students combined (APR, 0.81; 95% CI, 0.70–0.94) as well as among the White subgroup (APR, 0.78; 95% CI, 0.67–0.92), but was not significant among students who were Black, Asian, Hispanic, or of other races or ethnicities . Furthermore, state T21 policies were associated with reduced likelihood of heavy cigarette smoking among high school students (ie, smoking ≥11 cigarettes per day) for all racial or ethnic categories except Hispanic. State T21 policies were associated with reduced likelihood of any past 30-day cigarette smoking (ie, smoking ≥1 of the past 30 days) only among White and Hispanic high schoolers. Among White high schoolers who saw a significant association with state T21 policies for both heavy smoking (ie, ≥11 CPD) and any past 30-day smoking (smoking ≥1 of the past 30 days), the measure of association was stronger for the former. Specifically, living in a state with T21 policies reduced the probability of smoking 11 or more CPD among White high school students by 87% (APR, 0.13; 95% CI, 0.06–0.29), but reduced the probability of smoking on 1 or more of the past 30 days by only 33% (APR, 0.67; 95% CI, 0.47–0.97).\n\n【37】Statewide T21 policies were not associated with e-cigarette use in the overall population, nor in any racial or ethnic stratum based on analysis of YRBS data. Statewide T21 policies were also not associated with smokeless tobacco use in the overall population; within race and ethnic subgroups, however, T21 policies were associated with increased likelihood of current smokeless tobacco use among Hispanic high schoolers (APR, 3.80; 95% CI, 1.87–7.69) but with lower likelihood of current use among Black and Asian high schoolers .\n\n【38】Discussion\n----------\n\n【39】Our findings suggest that the federal T21 law was not effectively enforced during 2020 because only 17.0% of US middle and high school students who attempted a cigarette purchase in that year reported that the salesclerk refused to sell it to them because they were underaged. Perceived ease of getting tobacco products from a store was higher among Hispanic students than among White participants. Furthermore, whereas every other racial or ethnic group had a significant decrease in the percentage who felt it would be easy to get tobacco products from a store during 2011 through 2020, this percentage did not change among Black participants (57.4% in 2020). Among those aged 18 to 20 years, we found that state T21 policies were protective against cigarette smoking among White young adults but not among those of racial or ethnic minority groups. These findings are consistent with previous research showing weaker age verification enforcements among minority populations . Several factors may account for these differences in tobacco access and use, including differences in accessibility to small retail stores where age verification is less likely to be enforced, such as liquor stores, gas stations, corner stores, small kiosks, and convenience stores . The tobacco industry has been well known to target tobacco products to neighborhoods with large Black populations . Reduction of illegal tobacco sales to minors through stronger enforcement of T21 can benefit public health.\n\n【40】Access laws in general may be best optimized when implemented in concert with other evidence-based tobacco use control and prevention measures because youths mostly access tobacco products from social contacts and could adopt adaptive behaviors such proxy purchasing (via an older adult), switching to less-regulated products, or buying from less-regulated or less-enforced environments . For example, while most social media sites have banned sales of tobacco products on their sites, illegal tobacco sellers have been known to evade such restrictions by misspelling brand names, using slang, or using expressions in foreign languages to communicate with potential customers . The basis for concern regarding these channels of tobacco access is that even young children may become very savvy with such online communication and tobacco access patterns. In our study, whereas students in high school were more confident than students in middle school about ease of tobacco access from a physical store, no differences existed by grade level in perceived ease of tobacco access online.\n\n【41】The strongest associations in our study for state T21 policies were observed for smoking intensity, suggesting that these policies may have a stronger impact in cutting down on smoking than quitting completely. For example, among White high school students, we found that state T21 policies reduced the probability of smoking 11 or more cigarettes per day by 87% but reduced the probability of smoking on 1 or more of the past 30 days by only 33%. Many previous studies have linked T21 policies with reduced volume sales of cigarettes . A pause or a reduction in tobacco use must be accompanied by an intention to stop smoking for it to qualify as a quit attempt . Therefore, interventions aimed at intrinsically motivating desire to quit among youths and young adults are needed.\n\n【42】### Limitations\n\n【43】This study has some limitations. First, all assessed measures were self-reported and may be subject to misreporting. Second, the cross-sectional data sets analyzed can only support associational inferences. Third, in our state-specific analysis, there may be misclassification of individuals covered by T21 policies because we based the classification on whether a statewide policy was in existence (substate geographic identifiers were unavailable in the datasets). Consequently, individuals from jurisdictions with T21 policies may have been classified as not being exposed to T21 policies if their entire state was not covered by such a policy. Finally, there may have been some confounding from the effect of COVID-19 that was not measured. However, the direction of the bias would be expected to be toward the null, considering varying levels of pandemic-related closure of schools and other social settings that would typically facilitate smoking behavior (such closures would conceivably restrict the social channels where youths might access tobacco products). Furthermore, with government-mandated shutdowns during parts of the COVID-19 pandemic , there may have been fewer opportunities for youths to attempt to buy tobacco products from stores compared with previous years. However, results from analysis that limited the analyzed data to only pre–COVID-19 or intra–COVID-19 years are less susceptible to potential bias in this regard because of restriction.\n\n【44】### Conclusion\n\n【45】Evaluation of the federal T21 law at the 1-year mark shows it has potential to reduce ease of tobacco access among adolescents and young adults, but intensified efforts are needed to increase compliance. Our results showed that over 4 in 5 US middle and high school students who attempted to buy cigarettes in the past 30 days during 2020 were successful because only 17.0% of those who attempted to do so reported that the salesclerk refused to sell to them because they were underaged. Efforts to increase tobacco retailers’ compliance to mandatory age checks are warranted in all communities, but especially among racial and ethnic minority communities for whom our findings suggest suboptimal enforcement of state access laws. The equitable and intensified enforcement of the federal T21 law among all racial and ethnic subgroups and across all tobacco products may achieve a positive equity impact in reducing all forms of tobacco use among US youths and young adults.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bc8ef253-c628-4877-9432-3f98d5bc74af", "title": "Expansion of Shiga Toxin–Producing Escherichia coli by Use of Bovine Antibiotic Growth Promoters", "text": "【0】Expansion of Shiga Toxin–Producing Escherichia coli by Use of Bovine Antibiotic Growth Promoters\nAntimicrobial agents are commonly used subtherapeutically as feed supplements to promote growth and to prevent infection in livestock. Despite growing public health concerns about resistance associated with agricultural use of antibiotics, their use in livestock production is anticipated to increase . Each year in the United States, 1,675 tons of nontherapeutic antibiotics are used in beef cattle, particularly in feedlots, which are intensive cattle-raising systems . According to a report from the US Department of Agriculture, ionophores, tylosin, chlortetracycline, and oxytetracycline are frequently given to feedlot cattle . Ionophores, such as monensin, are included in feed mainly to increase weight gain and to prevent bovine coccidiosis . Tylosin is used to prevent diseases (e.g. hepatic abscessation) and to promote growth in cattle , whereas chlortetracycline and oxytetracycline are used as feed supplements mainly to prevent bovine pneumonia and bacterial enteritis . Although antibiotics are usually added to feed, water, or both at subtherapeutic levels, at some feedlots, chlortetracycline and oxytetracycline are used at therapeutic levels to prevent infection, particularly when calves are first introduced into feedlots .\n\n【1】Shiga toxin–producing _Escherichia coli_ (STEC), such as _E. coli_ O157:H7, is the leading cause of hemorrhagic colitis and hemolytic uremic syndrome . Shiga toxin (Stx) is reportedly produced by ≈250 different O serotypes of _E. coli_ ; non-O157 STEC infection is becoming increasingly prevalent, accounting for up to 20%–50% of STEC infections in the United States . In particular, 6 serogroups (O26, O45, O103, O111, O121, and O145) are responsible for 83% of all non-O157 infections in the United States . The _stx_ genes are encoded by lambdoid bacteriophages . Because secretion systems for Stx are lacking, the release of Stx is mediated through bacterial cell lysis by Stx phages in response to the induction of the SOS response (a cellular response to DNA damage) . Antimicrobial agents, particularly those that interfere with DNA synthesis (e.g. quinolones and trimethoprim), enhance the propagation of Stx phages and consequently increase Stx production . For this reason, antimicrobial drug treatment is not recommended for patients with enterohemorrhagic _E. coli_ infection . In contrast, antibiotics are widely used as feed supplements in cattle, which are the primary natural reservoir for O157 and non-O157 STEC strains . _E. coli_ is highly prevalent in cattle feces at levels ranging from 10 7  to 10 9  CFU/g , and _E. coli_ O157 primarily colonizes the terminal rectum in cattle and is found in cattle feces at 10 3  –10 5  CFU/g . Unlike humans, cattle are not susceptible to STEC infection because they lack Stx receptors ; thus, antibiotics do not generate clinical problems in cattle. However, bovine antibiotic growth promoters (bAGPs) may induce the propagation of Stx phages and consequently facilitate the horizontal transfer of _stx_ genes in _E. coli_ . In this study, we investigated whether bAGPs can affect the propagation of Stx phages and contribute to the diversification of Stx-producing _E. coli_ .\n\n【2】### Materials and Methods\n\n【3】##### _E. coli_ Strains, Plasmids, and Culture Conditions\n\n【4】We routinely maintained _E. coli_ O157:H7 EDL933 and all _E. coli_ isolates from cattle in Luria Bertani (LB) medium. The plasmid u66recA, a transcriptional fusion of _recA_ : _egfp_ , is described elsewhere . Detoxified EDL933 strains (Δ _stx_ 2: _Km_ and _P stx _ 2  : _gfp_ in which _stx2_ is replaced with a kanamycin resistance cassette and _gfp_ , respectively) were constructed according to a method described by Datsenko and Wanner . The _stx_ 2 promoter region was PCR amplified from _E. coli_ EDL933 with Pro\\_Stx2-F and Pro\\_Stx2-R primers . The resulting 377-bp PCR product was purified, digested with _Xba_ I, and ligated to an _Xba_ I site located immediately upstream of the promoterless _gfp_ gene in pFPV25.1 . _P stx2 _ : _gfp_ was prepared by PCR with GFP\\_BGL\\_F and GFP\\_BGL\\_R . The PCR product was cloned to a _Bgl_ II site upstream of the flippase recognition target (FRT) in pKD13 . The FRT-flanked _P stx _ 2  : _gfp_ was amplified with PCR from pKD13 by use of pKD13-F and pKD13-R primers, and the amplicon was introduced to EDL933 harboring pKD46 by electroporation. The transcriptional _P stx _ 2  : _gfp_ fusion was constructed by replacing the _stx_ 2 gene with _gfp_ in EDL933. The pKD46 plasmid was cured from the mutant by culturing at 37°C. A Δ _stx_ 2: _Km_ mutant of _E. coli_ O157:H7 EDL933 was constructed by replacing _stx_ 2 with a FRT-flanked kanamycin resistance cassette that had been PCR amplified from pKD13. The FRT- _Km_ \\-FRT amplicon was introduced into EDL933 harboring pKD46 by electroporation . The allelic exchange was confirmed by PCR with the primer sets of Stx2-F and Stx2-R, Kt and K1. We added tetracycline (50 μg/mL), ampicillin (100 μg/mL), and kanamycin (50 μg/mL) to culture media when necessary.\n\n【5】##### Antibiotics\n\n【6】Monensin, tylosin, chlortetracycline, oxytetracycline, neomycin, and sulfamethazine were purchased from Sigma-Aldrich (St. Louis, MO, USA). Ciprofloxacin was purchased from Enzo Life Sciences Inc. (Farmingdale, NY, USA).\n\n【7】##### Measurement of _P stx _ 2  _:gfp_ Expression\n\n【8】Cultures carrying the _P stx _ 2  : _gfp_ promoter fusion were collected in the exponential phase by centrifugation (5 min at 6000 × _g_ ), and bacterial cells were resuspended in fresh LB medium to ≈4.0 × 10 7  CFU/mL. After growing in LB broth supplemented with various concentrations of bAGPs and ciprofloxacin for 3 h, 200 µL samples were transferred to each well in a 96-well microplate and green fluorescent protein wavelength was measured with a fluorometer (FLUOstar Omega, BMG Labtech, Ortenberg, Germany). Fluorescence was monitored at excitation and emission wavelengths of 520 nm and 480 nm, respectively. Fluorescence intensities are reported in the instrument’s relative fluorescence units. The expression level of _rec_ A was measured in the same way with _u66rec_ A. The experiments were performed with triplicate samples and repeated at least 3 times.\n\n【9】##### Stx Phage Induction\n\n【10】Stx phages were induced by monensin, tylosin, chlortetracycline, oxytetracycline, and ciprofloxacin. Because ciprofloxacin is a well-known Stx phage inducer , we used ciprofloxacin as a control. Lysogenic strains were grown in LB broth overnight at 37°C with shaking and diluted to an optical density at 600 nm of 0.1 in NZCYM broth (Amresco, Solon, OH, USA). The cultures were incubated at 37°C with shaking (200 rpm) for 18 h in the presence (0.01, 0.1, and 1 μg/mL) and absence (control) of the 5 antibiotics. After centrifugation at 5,000 × _g_ , the supernatant was sterilized with a 0.22-μm filter and used immediately. We then added 10-fold serial dilutions of phage lysates to 1 mL of _E. coli_ C600 at the stationary phase. Then, 3 mL of top agar supplemented with 5 mmol/L calcium chloride was added to this culture and the mixture was poured on an LB agar plate. The plates were incubated at 37°C overnight, and PFU were counted the next day.\n\n【11】##### Stx Phage Transfer Assay\n\n【12】We investigated the transfer of Stx phages in the presence of bAGPs at different concentrations. Briefly, 5 mL of _E. coli_ EDL933 Δ _stx_ 2: _Km_ (donor) and 6 _stx_ 2-negative bovine _E. coli_ strains (recipients) with ampicillin or tetracycline resistance were grown in LB broth at 37°C overnight. Overnight cultures were diluted 100-fold in fresh NZCYM broth and cultured until the early exponential phase for 3 h. The donor strain (≈10 4  CFU/mL) was mixed with recipient strains (≈10 7  CFU/mL) in the presence of different concentrations of bAGPs. The mixed cultures were incubated at 37°C overnight without shaking. After incubation, 100 µL of culture was spread onto sorbitol-MacConkey agar plates supplemented with kanamycin and ampicillin or tetracycline and incubated at 37°C overnight. We calculated the transduction frequencies by dividing the number of transductants by the number of recipients.\n\n【13】##### Characterization of Transductants\n\n【14】Pink colonies growing on sorbitol-MacConkey agar supplemented with either kanamycin and tetracycline or kanamycin and ampicillin were regarded as presumptive transductants (i.e. recipients of the _stx_ 2-encoding phage 933W). The presumptive transductants of the _stx_ 2-encoding phage 933W were verified by performing multiplex PCR. PCRs were performed with specific primer pairs: Stx2-F and Stx2-R for the _stx_ 2 gene in 933W, eaeA-F and eaeA-R for _eae_ A encoding intimin , Ec1-uspA and Ec2-uspA for the _usp_ A gene encoding the universal stress protein in _E. coli_ , and O157F and O157R for a region in _rfb_ E (O-antigen-encoding) for the O157 serotype . Serologic tests were performed with O157 and O26 antiserum (Korea National Institute of Health, Osong, South Korea).\n\n【15】### Results\n\n【16】##### Enhanced Propagation of Stx Phages in _E. coli_ O157:H7 by bAGPs\n\n【17】To investigate the effect of subtherapeutic concentrations of bAGPs on the propagation of Stx phages, we first measured the level of Stx phage propagation after exposure of _E. coli_ O157:H7 EDL933 to sublethal concentrations (1, 0.1, and 0.01 μg/mL) of common bAGPs, including monensin, tylosin, chlortetracycline, and oxytetracycline. Because the bAGP concentrations used in the study were markedly less than the MICs , bAGP treatment did not affect the growth of _E. coli_ O157 (data not shown). The propagation of Stx phages was induced significantly by chlortetracycline and oxytetracycline at concentrations as low as 0.01 μg/mL . Because _E. coli_ O157:H7 EDL933 harbors 2 Stx prophages, BP-933W ( _stx_ 2) and CP-933V ( _stx_ 1) , the level of _stx_ 2 expression was specifically measured with an _stx_ 2: _gfp_ fusion construct to determine the propagation level of Stx2 phage. Consistently, bAGP treatment substantially increased the level of _stx_ 2 expression . Because the primary mechanism for antibiotic-mediated induction of phage propagation is the SOS response, we also determined the level of _rec_ A expression after exposure to bAGPs. Consistent with the changes in the level of Stx phage propagation , chlortetracycline and oxytetracycline significantly induced _recA_ expression . Of note, chlortetracycline and oxytetracycline induced Stx phage propagation and _stx_ 2 expression at levels similar to those of ciprofloxacin, a DNA-damaging antibiotic frequently used as a phage inducer .\n\n【18】##### Increased Propagation of Stx Phages in Bovine STEC Strains by bAGPs\n\n【19】When we further examined Stx phage induction by bAGPs with 3 _stx_ 2+/ _stx_ 1– _E. coli_ strains from cattle, we found that exposure to a sublethal concentration (0.1 μg/mL) of chlortetracycline and oxytetracycline significantly induced the propagation of Stx2 phage in the bovine STEC isolates, whereas 0.1 µg/mL of monensin did not induce phage propagation, and 0.1 μg/mL of tylosin exhibited strain-dependent variations in the phage induction .\n\n【20】##### Transfer of Stx Phages in Bovine _E. coli_ Isolates by Sublethal Concentrations of bAGPs\n\n【21】We determined the frequency of Stx2 phage transfer with 6 _stx_ 2-negative _E. coli_ isolates from cattle, including 3 _stx_ 1+/ _stx_ 2– _E. coli_ strains and 3 _stx_ 1–/ _stx_ 2– _E. coli_ strains, by using a detoxified EDL933 derivative in which _stx_ 2 was replaced with a kanamycin resistance cassette. The donor _E. coli_ (EDL933 Δ _stx_ 2: _Km_ , a detoxified strain) and the recipient _stx2_ \\-negative _E. coli_ strains were co-cultivated in the presence of sublethal concentrations (0.01 μg/mL and 0.1 μg/mL) of bAGPs. The recipient bovine _stx_ 2-negative _E. coli_ isolates are all sensitive to kanamycin and resistant to β-lactams or tetracycline, and the detoxified EDL933 derivative is resistant to kanamycin and sensitive to β-lactams and tetracycline. Therefore, the Stx phage transfer made the recipient strains resistant to both kanamycin and β-lactams or tetracycline. Subtherapeutic treatment of bAGPs, particularly chlortetracycline and oxytetracycline, substantially enhanced the transfer of Stx2 phage in _E. coli_ . The transduction rate was slightly increased by 0.1 µg/mL tylosin but not notably affected by 0.1 µg/mL monensin . When the concentration of bAGPs was reduced to 0.01 μg/mL, tylosin did not mediate the Stx phage transfer. However, chlortetracycline and oxytetracycline significantly mediated the transfer of Stx phage in _E. coli_ even at 0.01 μg/mL .\n\n【22】##### Induction of Stx Phage Propagation by Therapeutic Concentrations of Chlortetracycline and Oxytetracycline\n\n【23】Whereas tylosin and monensin are used at low concentrations in cattle feed, chlortetracycline and oxytetracycline are sometimes used at therapeutic levels to prevent infection . Thus, we investigated the effects of high concentrations of chlortetracycline and oxytetracycline on the propagation of Stx phages. High concentrations of chlortetracycline significantly increased the propagation of Stx phages in a concentration-dependent manner, whereas the level of Stx phage induction by oxytetracycline is already significantly high at 1 μg/mL in comparison with higher concentrations of oxytetracycline (2–8 μg/mL) and even the highest concentration of chlortetracycline used in the study (8 μg/mL) . These findings demonstrate that therapeutic application of chlortetracycline may enhance the dissemination of Stx phages more significantly than subtherapeutic doses and that oxytetracycline is a highly potent inducer of Stx phage propagation even at low concentrations.\n\n【24】##### Confirmation of Stx Phage Transfer by bAGPs\n\n【25】To confirm Stx phage transfer by bAGPs, we randomly chose transductant colonies from the co-culture experiment described earlier  for further verification with PCR to detect genes specific for the Stx2 phage ( _stx_ 2), _E. coli_ ( _usp_ A), _E. coli_ virulence ( _eae_ A), and O157 serotype ( _rfb_ E O157  ) . Figure 5 shows representative data to exhibit the dissemination of the _stx2_ gene to _stx_ 2-negative _E. coli_ O26  by exposure to bAGPs. We selected _E. coli_ O26 because this serotype is the most frequently isolated non-O157 STEC . Treatment with bAGP changed bovine _E. coli_ O26 from _stx_ 2-negative to _stx_ 2-positive. We also performed a latex agglutination test to confirm the serotype after transduction . The capability of sorbitol fermentation in non-O157 strains was confirmed by growing on sorbitol MacConkey agar plates . The results clearly showed that non-O157 _E. coli_ horizontally acquired _stx_ 2 by phage transduction after exposure to bAGPs.\n\n【26】### Discussion\n\n【27】In livestock production, antibiotics are routinely added to feed for growth promotion and disease prevention. Although these AGPs are used at subtherapeutic concentrations, a substantial number of studies have shown that AGPs may negatively affect public health by providing selective pressure to increase antibiotic-resistant pathogens . Our study showed that, in addition to growing public health concerns about antibiotic resistance, some AGPs may facilitate the transmission of virulence factors in _E. coli_ even at extremely low concentrations. Whereas the effect of monensin and tylosin on the propagation of Stx phages seemed to be marginal, chlortetracycline and oxytetracycline significantly induced the propagation of Stx phages  and mediated the transfer of Stx phages in _E. coli_ .\n\n【28】Tetracyclines are widely used in agriculture, accounting for 44% and 37% of marketed agricultural antibiotics in the United States  and the European Union , respectively. Compared with monensin and tylosin, oxytetracycline and chlortetracycline most significantly affected the transmission of Stx phages, even at concentrations as low as 0.01 μg/mL . This concentration is substantially lower than concentrations in the large intestines of cattle, which are 0.3 μg/mL after chlortetracycline feeding for growth promotion (70 mg/head/day throughout feedlot period) and 1.7 μg/mL after feeding for disease prevention (350 mg/head/day for 28 days) . A previous observational study reported that the percentage of detecting _stx_ \\-positive commensal _E. coli_ was increased in cattle from 48% to 80% by oxytetracycline injection and chlortetracycline addition to feed . Although they did not conclusively say that the increase in _stx_ \\-positive animals is from oxytetracycline and chlortetracycline treatment in cattle, they suggested that antibiotic treatment may be the reason for the increased prevalence . Sometimes, chlortetracycline and oxytetracycline are mixed with other antibiotics, such as neomycin and sulfamethazine, to maintain weight gains and feed efficiency for cattle under stress conditions . We observed that tetracycline combinations with these antibiotics induced Stx phage propagation just as comparably as a single treatment of chlortetracycline or oxytetracycline alone , suggesting that oxytetracycline and chlortetracycline are the major bAGPs that induce propagation of Stx phages. Another concern about bAGPs would be associated with poor absorption of orally administered antibiotics in animal guts . Approximately 75% of dietary chlortetracycline is excreted in cattle manure without being digested , and chlortetracycline is the antimicrobial compound that is most frequently detected in cattle manure at levels as high as 20 mg/kg . Given the high residue concentrations in mature, unmetabolized tetracycline residues may also affect the dissemination of Stx phages in cattle manure.\n\n【29】Although use of chlortetracycline and oxytetracycline in cattle is not consistent , the levels of SOS response induction and Stx phage propagation by these 2 antibiotics were comparable . In addition to subtherapeutic use in feed, therapeutic concentrations of oxytetracycline and chlortetracycline are sometimes added to feed as metaphylaxis in feedlot cattle . We observed that chlortetracycline induced propagation of Stx phages more significantly at high concentrations than at low subtherapeutic concentrations . Surprisingly, propagation of Stx phages by 1 μg/mL oxytetracycline was comparable to that of 8 μg/mL chlortetracycline , suggesting that oxytetracycline is highly effective in phage induction. Possibly, therapeutic administration of chlortetracycline, oxytetracycline, and other antibiotics, particularly those inducing the SOS response and Stx phage propagation (e.g. fluoroquinolones) , would significantly affect the spread of Stx phages; however, its effect would be limited because therapeutic antibiotics are usually used to treat disease in individual animals.\n\n【30】Previous studies have reported that antibiotic treatment significantly increases the propagation of Stx phages . However, little attention has been paid to the effects of nonprescription bAGPs on the transmission of Stx phages in _E. coli_ , although phages are a well-known vehicle for horizontal gene transfer and cattle are the primary reservoirs for _E. coli_ O157:H7. Presumably, the underestimation of bAGPs might result from low concentrations of antibiotics in cattle feed. Nevertheless, in this study, we demonstrated that some bAGPs, particularly chlortetracycline and oxytetracycline, are implicated in the diversification of _stx_ \\-positive O serotypes in _E. coli_ by facilitating the horizontal transfer of Stx phages even at substantially low concentrations. Thus, use of these agents could lead to emergence of pathogenic _E. coli._", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c15857bd-c0f2-45d7-8d20-3fa0c22572c1", "title": "High Anti–Phenolic Glycolipid-I IgM Titers and Hidden Leprosy Cases, Amazon Region", "text": "【0】High Anti–Phenolic Glycolipid-I IgM Titers and Hidden Leprosy Cases, Amazon Region\n**To the Editor:** Leprosy remains a serious public health issue. Although the World Health Organization elimination target was achieved in 2000, with a prevalence of <1 case/10,000 persons, despite progress since introduction of multidrug therapy , large pockets of poverty remain in which the disease is hyperendemic and underdiagnosed. In fact, in highly disease-endemic areas, the prevalence of previously undiagnosed leprosy cases in the general population has been reported to be 6× higher than the registered prevalence .\n\n【1】Most leprosy patients are in India and Brazil. In Brazil, new cases are concentrated in the Northeast, Midwest, and Amazon regions (from state capitals to the inner counties). Access to the health system is poor in these regions because of severe inequalities in the public health system of Brazil ,\n\n【2】A total of 34,894 new cases were registered in Brazil during 2010 , corresponding to an incidence rate of 18.22 cases per 100,000 population. Pará State accounted for 10.2% of cases (3,562 cases), an incidence rate of 46.93 per 100,000 population. When only children <15 years of age were considered, Pará registered 389 new cases of leprosy in 2010, representing 10.9% of all cases, an incidence rate of 16.52 per 100,000 population. In Oriximiná, a county with 62,794 inhabitants in northwestern Pará, ≈800 km from Belém, Pará’s capital, a mean of 13.8 cases per year were registered for the past 5 years.\n\n【3】In 2010, in Oriximiná, we collected plasma samples from 138 students 8–18 years of age, from 35 leprosy patients who received a diagnosis during 2004–2009, and from 126 contacts of these patients . We tested all of these samples for anti–phenolic glycolipid-I (PGL-I) IgM; 42% of students, 54.3% of case-patients, and 45% of case-patient contacts were seropositive. In addition to collecting samples, we clinically examined the leprosy patients and their contacts, among whom we identified 3 new leprosy cases. We did not examine students at that time. Contacts were persons from the same household or neighborhood whom the index case-patient described as a person with whom he or she had a close relationship. Leprosy cases were diagnosed in the field on the basis of clinical signs, loss of sensation on the skin lesions, and presence of enlarged nerves. For operational reasons, skin smears were not performed. All cases were diagnosed by 2 leprologists. We used the Ridley-Jopling classification, associated with the indeterminate clinical type, as defined by the Madrid classification. The ELISA cutoff for positive results was arbitrarily established as an optical density of 0.295 based on the average plus 3× the SD of the test results from 14 healthy persons from the Amazon region .\n\n【4】Because studies of the seroprevalence among contacts have reported a proportion of seropositive persons ranging from ≈1.9% to 18.4% , we returned to Oriximiná 16 months after the first visit. We examined 2 groups of students and their contacts; 1 group was positive for anti–PGL-I, and the other group was negative for anti–PGL-I. We visited 44 households in 1 week. From the 35 leprosy patients encountered during the first visit, we selected 25 households to survey (14 with an anti–PGL-I–positive contact in the household and 11 without), and among students with results of anti–PGL-I serology, we selected 19 households (11 positive with an anti–PGL-I–positive contact in the household and 8 without). During our visits to all of these households, we examined 222 persons .\n\n【5】When we arrived in Oriximiná, only 2 cases had been registered in the national notifiable diseases information system. By using our approach, 23 new cases were found after we investigated households that had a person positive for anti–PGL-I (15 multibacillary, 8 paucibacillary); we found only 7 new cases in households where residents were negative for anti–PGL-I (4 multibacillary, 3 paucibacillary) . For comparison, during the last traditional leprosy campaign in Oriximiná in 2008, eight new cases were detected. Furthermore, by using our strategy, the local public health service detected 9 additional new cases during the 4 months after our departure from Oriximiná.\n\n【6】These data emphasize that contact examination is crucial for identifying new cases  and that such investigation must be conducted periodically. Our data also indicate that subclinical infections are highly prevalent among public school students in the Amazon region and that identifying students with positive anti–PGL-I test results can lead to discovery of new leprosy cases among students’ household contacts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "730d431f-3451-46dd-8b92-23dbad1e0296", "title": "Novel Reassortant Influenza A(H1N2) Virus Derived from A(H1N1)pdm09 Virus Isolated from Swine, Japan, 2012", "text": "【0】Novel Reassortant Influenza A(H1N2) Virus Derived from A(H1N1)pdm09 Virus Isolated from Swine, Japan, 2012\nInfluenza A viruses can be transmitted between humans, swine, and birds; virus subtypes have the potential to reassort and generate new viruses by cross-breeding in the various hosts . For example, influenza A subtype H1N1 viruses reassorted in swine, and the resulting swine influenza viruses (SIVs) were transmitted to humans. The reassorted combinations have resulted in pandemic viruses as well as low-pathogenicity viruses with low transmissibility among humans. Similarly, seasonal human subtypes of influenza are transmissible to swine . In 2009, a novel strain of the H1N1 SIV subtype emerged and was associated with a pandemic . The virus, later termed influenza A(H1N1)pdm09, hereafter referred to as pH1N1, was confirmed as a reassortant virus resulting from cross-breeding of a European avian subtype H1N1 virus and a North American triple reassortant virus . Subsequently, other strains reassorted from the pH1N1 virus . We report on an isolated new reassortant H1N2 SIV derived from the pH1N1 virus and SIVs originating in Japan.\n\n【1】### The Study\n\n【2】We collected 109 nasal swab samples from pigs for swine influenza surveillance during November 2011–February 2012. Nasal swab samples were collected from healthy pigs, 6 months of age, at an abattoir in Gunma Prefecture, Japan. All samples were inoculated onto MDCK cells . All cell culture supernatants were tested by using a hemagglutination assay of a 0.7% solution of guinea pig erythrocytes . To determine the subtype of the isolate, a hemagglutination inhibition assay was performed by using ferret antiserum for A/California/07/2009 \\[A(H1N1)pdm09\\], A/Victoria/210/2009 \\[A(H3N2)\\], B/Bangladesh/3333/2007 \\[B/Yamagata-lineage\\], and B/Brisbane/60/2008 \\[B/Victoria-lineage\\] . One strain of influenza A virus, designated A/swine/Gunma/1/2012, was isolated from the samples.\n\n【3】For full genome sequencing of the influenza A/swine/Gunma/1/2012 strain, we conducted reverse transcription PCR . Segment-specific primers used for amplification and sequencing are shown in Technical Appendix Figure, panel A. Phylogenetic analysis of the nucleotide sequences was conducted by using MEGA version 5 software  and Tree Explorer version 2.12  . Evolutionary distances were estimated according to the Kimura 2-parameter method . The phylogenetic trees of hemagglutinin (HA) and neuraminidase (NA) genes were constructed by using the neighbor-joining method . In addition, phylogenetic trees based on the matrix protein, nucleoprotein genes, nonstructural protein, polymerase acid, polymerase basic 1, and polymerase basic 2 were constructed by using the neighbor-joining method. The reliability of the trees was estimated with 1,000 bootstrap replications. GenBank accession numbers assigned to the gene sequences of the analyzed strain are the following: polymerase basic 2 (AB731582), polymerase basic 1 (AB731583), polymerase acid (AB731584), HA (AB731585), nucleoprotein (AB731586), NA (AB731587), matrix protein (AB731588), and nonstructural protein (AB731589).\n\n【4】Phylogenetic trees based on HA and NA gene sequences are shown in the Figure , panels A and B. The identities of the nucleotide sequences of each gene are shown in the Table . The A/swine/Gunma/1/2012 strain was confirmed as a strain of pH1N1 virus Figure , panel A). NA gene sequences showed that the virus was located within clusters of swine-type viruses documented in Japan as the representative strains, such as A/swine/Ehime/1/1980 . The sequence identity of the NA gene between the A/swine/Gunma/1/2012 strain and other Japanese H1N2 SIV strains ranged from 85.0 to 97.5%. The identities of other genes between the A/swine/Gunma/1/2012 strain and pH1N1 virus vaccine strain (A/California/07/2009) were highly homologous (>90%). These results suggest that the A/swine/Gunma/1/2012 strain was a new reassortant of the H1N2 SIV subtype derived from the pH1N1 virus.\n\n【5】We isolated 1 strain in this study. The samples (109 nasal swabs) were collected from different pig farms ≈60 km apart. The epidemiologic association may be low among the samples, because the quarantine inspection system is well established in Japan. All samples were collected from pigs 6 months of age; therefore, the potential for infection with the virus could have been low. Additional and larger studies investigating the emergence of the parent virus of the strain may be needed.\n\n【6】### Conclusions\n\n【7】Vijaykrisna et al. found a new reassortant virus among avian-type, swine-type, and pH1N1 viruses . In addition, Monero et al. reported a new reassortant virus between SIV, identified in Italy, and pH1N1 viruses . Thus, pH1N1 virus and other types of influenza viruses can be reassorted. However, to our knowledge, reassortant H1N2 SIV strains derived from pH1N1 virus in Japan have not been identified before this report. Although the transmission of SIVs to humans has been reported sporadically, the infectious nature of this reassortant H1N2 strain among humans is unknown. The emergence of a novel H1N2 SIV strain raises further concerns about whether the virus will generate further genetic reassortments and gain virulence. Systematic influenza virus surveillance in pigs and humans should be considered.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "48910a68-88f1-4fe8-ba6a-bf4f8cf2ff4a", "title": "Geographic Distribution of MERS Coronavirus among Dromedary Camels, Africa", "text": "【0】Geographic Distribution of MERS Coronavirus among Dromedary Camels, Africa\nA novel betacoronavirus, Middle East respiratory syndrome coronavirus (MERS-CoV), was identified as the cause of severe respiratory disease in humans during 2012 . In August 2013, dromedary camels ( _Camelus dromedarius_ ) were implicated for the first time as a possible source for human infection on the basis of the presence of MERS-CoV neutralizing antibodies in dromedaries from Oman and the Canary Islands of Spain . Since then, the presence of MERS-CoV antibodies in dromedaries has been reported in Jordan , Egypt , the United Arab Emirates , and Saudi Arabia . In October 2013, analysis of an outbreak associated with 1 barn in Qatar  found dromedaries and humans to be infected with nearly identical strains of MERS-CoV. Further proof of widespread circulation of MERS-CoV among dromedaries was provided by studies from Egypt and Saudi Arabia . These findings have raised questions about the geographic distribution of MERS-CoV among camel populations elsewhere. Here, we report our assessment of the geographic distribution of MERS-CoV circulation among dromedaries in Africa by serologic investigation of convenience samples from these animals in Nigeria, Tunisia, and Ethiopia.\n\n【1】### The Study\n\n【2】In Nigeria, serum samples from 358 dromedaries that were raised for meat production were collected at abattoirs in 4 provinces (Kano, n = 245; Sokoto, n = 51; Borno, n = 51; and Adamawa, n = 11, panel A) during 2010–2011 for testing for peste des petits ruminants virus. The ages of the animals ranged from 4 to 15 years. The abattoirs also served the neighboring countries of Chad, Niger, and the Central African Republic. In Tunisia, serum samples from 204 dromedaries that were 1 to 16 years of age were collected in 3 provinces in 2009 and 2013 . Samples were collected from 155 dromedaries in Sidi Bouzid Province from 27 herds that were kept for meat production and from 39 dromedaries in Kebili Province from 16 herds that were kept for tourist rides; samples from both provinces had originally been collected for a study investigating the presence of _Anaplasma phagocytophilum._ Samples were collected from 10 dromedaries from Sousse Province that were kept for meat production because they were suspected of being infected with _Trypanosoma evansi_ . In Ethiopia, samples from 188 dromedaries, 1 to 13 years of age, were collected as part of a study evaluating the presence of toxoplasmosis and respiratory tract diseases in 3 provinces (Afar, n = 118; Somalia, n = 11; and Oromia, n = 59, panel C) during 2011–2013. All samples were taken by jugular vein puncture according to local laws, and serum samples were stored at −20°C until testing. All serum samples were shipped to the Erasmus MC laboratory in the Netherlands in agreement with Dutch import regulations.\n\n【3】The serum samples were tested for the presence of IgG antibodies reactive with S1 antigens against MERS-CoV (residues 1–747), severe acute respiratory syndrome CoV (residues 1–676), and human CoV OC43 (residues 1–760) by using extensively validated protein-microarray technology, as described . Results were expressed as relative mean fluorescent intensity (RFU) for each set of quadruplicate spots of antigen, with a cutoff of 4,000 RFU as used by Meyer et al. Human CoV OC43 S1 was used as a proxy for bovine coronavirus (ΒCoV), the latter of which is known to circulate commonly in dromedaries . High percentages of animals seropositive for MERS-CoV were observed in Nigeria and Ethiopia; the overall seropositivity was 94% in adult dromedaries in Nigeria and 93% and 97% for juvenile and adult animals, respectively, in Ethiopia . All provinces in which dromedaries were sampled in both countries showed high rates of seropositivity . The overall seropositivity in dromedaries in Tunisia was 30% for animals ≤2 years of age and 54% for adult animals. Seropositivity of 36% and 40% was observed in Sidi Bouzid and Sousse Provinces, respectively, and 100% of the dromedaries in the southern province of Kebili were seropositive. Array results were confirmed on a selection of positive and negative serum samples (n = 14 per country) in MERS-CoV neutralization tests performed as described  . Serum samples from 72%, 82%, and 67% of the dromedaries from Nigeria, Ethiopia, and Tunisia, respectively, reacted with the OC43 antigen, confirming common circulation of ΒCoV in camelids . All samples tested negative for severe acute respiratory syndrome CoV (data not shown).\n\n【4】### Conclusions\n\n【5】Since the discovery of MERS-CoV in 2012, accumulating serologic and molecular evidence demonstrates that the virus in dromedaries is genetically very similar to MERS-CoV in humans and points to the conclusion that dromedary camels are reservoirs for human infection. MERS-CoV genomic fragments have been detected in dromedaries in Qatar  and Saudi Arabia ; near full-genome sequences have been generated from dromedaries in Egypt  and full-genome sequences have been generated from dromedaries in Saudi Arabia . Here, we show serologic evidence for circulation of MERS-CoV or MERS-like CoV in dromedaries in countries in East, West, and North Africa, with possible herd-specific differences in prevalence in Tunisia. The lower seropositivity observed in herds raised for meat production in Tunisia might reflect a high turnover of camels with a continuous introduction of animals unexposed to the MERS-CoV into these herds. No camels imported from neighboring countries were found at the meat-producing farms in Sidi Bouzid and Sousse, only camels purchased from other farms in the same area or other areas in Tunisia. However, animals are frequently moved between Libya and Kebili for trade.\n\n【6】Samples in this study were collected during 2009–2011, confirming observations by us and others  that the virus circulated well before March 2012, which is the estimated time of identification of the most common ancestor for the MERS-CoV strains found in humans to date . The earliest serologic indication for circulation of MERS-CoV or MERS-like CoV in dromedaries was observed in 1992; however, this result was based on results of a whole-virus ELISA with undescribed specificity . On the basis of well-validated array and neutralization tests, the study of dromedaries in the United Arab Emirates showed the presence of MERS-CoV or MERS-Cov-like antibodies as early as 2003 . The accumulated data on MERS-CoV serology in dromedaries  show circulation of MERS-CoV or MERS-like CoV in dromedaries in Africa and the Arabian Peninsula well before 2012, when the first cases in humans were identified, and show overall high levels of seropositivity, including in animals from countries without reported human cases.\n\n【7】A question raised by these findings is whether human cases occur outside the Arabian Peninsula and if such cases are currently underdiagnosed in Africa. In addition, for the whole region, the possibility exists that MERS-CoV illness occurred before its discovery in 2012 and that such infection has been overlooked in the areas with evidence for virus circulation among animals during the past 10 years. Retrospective studies of cohorts of humans with respiratory illnesses of unknown etiology should address this notion.\n\n【8】Alternative explanations for the lack of cases in Africa could be the following: a different risk profile, for instance, related to demographics and local practices; or subtle genetic differences in the circulating virus strain. Full-genome sequencing, virus isolation, and phenotypic characterization of viruses circulating outside the Arabian Peninsula will resolve this issue. Meanwhile, awareness of MERS-CoV infections should be raised among clinicians in Africa.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "804b0a35-7a80-4fec-a15d-581d2b27e9b8", "title": "Community-Acquired Klebsiella pneumoniae Bacteremia: Global Differences in Clinical Patterns", "text": "【0】Community-Acquired Klebsiella pneumoniae Bacteremia: Global Differences in Clinical Patterns\n_Klebsiella pneumoniae_ is among the most common gram-negative bacteria encountered by physicians worldwide. It is a common hospital-acquired pathogen, causing urinary tract infections, nosocomial pneumonia, and intraabdominal infections. _K. pneumoniae_ is also a potential community-acquired pathogen. In this international collaborative study, we evaluated geographic differences and trends in three prominent presentations of community-acquired _Klebsiella_ infection.\n\n【1】First, _K. pneumoniae_ has been a recognized pulmonary pathogen since its discovery >100 years ago. The classic clinical presentation is dramatic: toxic presentation with sudden onset, high fever, and hemoptysis (currant jelly sputum). Chest radiographic abnormalities such as bulging interlobar fissure and cavitary abscesses are prominent. However, the incidence of community-acquired _Klebsiella_ pneumonia has apparently declined in the United States . In studies from the 1920s to the 1960s, _K. pneumoniae_ was considered an important cause of community-acquired pneumonia  ; however, in the last decade _K. pneumoniae_ accounted for <1% of cases of pneumonia requiring hospitalization in North America .\n\n【2】Second, a striking clinical finding concerning a new manifestation of community-acquired _K. pneumoniae_ infections has been documented. An unusual invasive presentation of _K. pneumoniae_ infection, primary bacteremic liver abscess, has been described by numerous investigators in Asia; >900 patients with _Klebsiella_ liver abscess have been reported from Taiwan in the last 10 years . In addition, case reports and small series from Korea, Singapore, Japan, India, and Thailand have been published . The Taiwanese patients with _K. pneumoniae_ liver abscess have no history of hepatobiliary disease. Seventy percent of such patients have diabetes mellitus ; 11% to 12% of the reported patients with _Klebsiella_ liver abscess have other septic metastatic lesions, including pulmonary emboli or abscess, brain abscess, pyogenic meningitis, endophthalmitis, prostatic abscess, osteomyelitis, septic arthritis, or psoas abscess .\n\n【3】The third striking clinical observation is the preponderance of _K. pneumoniae_ as a cause of community-acquired bacterial meningitis in adults in Taiwan, even in the absence of liver abscess or other sites of infection. The proportion of cases of culture-proven bacterial meningitis due to _K. pneumoniae_ in one Taiwanese hospital increased from 8% during 1981 and 1986 to 18% during 1987 to 1995  . In contrast, in a recent large review only 3 (1.2%) of 253 cases of community-acquired bacterial meningitis from the Massachusetts General Hospital were due to _K. pneumoniae_  .\n\n【4】Given these empiric observations, we established an international collaboration of researchers from each of the world’s populated continents. These investigators worked in large tertiary-care hospitals or hospitals serving veterans. One of our aims was to delineate in a single time period, with a consistent set of definitions, global differences in the clinical manifestations of serious _K. pneumoniae_ infections. We also examined the influence of prior antibiotic use on these differences in _K. pneumoniae_ infections.\n\n【5】### Methods\n\n【6】A prospective study of consecutive patients with community-acquired _K. pneumoniae_ bacteremia was performed in 12 hospitals. 1 The study period was January 1, 1996, to December 31, 1997. Records of patients >16 years of age with positive blood cultures for _K. pneumoniae_ were reviewed, and a 188-item study form was completed. All items on the form were objective criteria, allowing standardization among medical centers. The study was observational in that administration of antimicrobial agents and other therapeutic management were controlled by the patient’s physician, not the investigators.\n\n【7】Community-acquired bacteremia was defined as a positive blood culture taken on or within 48 hours of admission. Severity of acute illness at the time of positive blood cultures was assessed by a previously validated scoring system, based on mental status, vital signs, need for mechanical ventilation, and recent cardiac arrest (Pitt bacteremia score)  . Type of infection was defined as pneumonia, urinary tract infection, meningitis, incisional wound infection, other soft tissue infection, intraabdominal infection, and primary bloodstream infection, according to Centers for Disease Control and Prevention definitions  . In addition, distinctive sites of _K. pneumoniae_ bacteremia were further defined as liver abscess, meningitis, or endophthalmitis. Liver abscesses were defined by the coexistence of blood cultures positive for _K. pneumoniae_ and evidence of an intrahepatic abscess cavity by ultrasonography or computed tomography. Meningitis was defined as culture of _K. pneumoniae_ from cerebrospinal fluid, and endophthalmitis was defined as decreased visual acuity, pain, hypyon, or severe anterior uveitis in a patient concurrently bacteremic with _K. pneumoniae_ . Death was defined as including deaths from all causes within 14 days of the date the first positive blood culture for _K. pneumoniae_ was obtained.\n\n【8】Blood cultures of _K. pneumoniae_ were sent by participating hospitals on nutrient agar slants to a central study laboratory in Pittsburgh, where the identity of each isolate as _K. pneumoniae_ was confirmed by the Vitek GNI system (Biomerieux Vitek, Hazelwood, MO). Extended-spectrum beta-lactamase (ESBL) production was defined phenotypically by broth dilution as a ≥3 twofold concentration decrease in MIC for either cefotaxime or ceftazidime tested in combination with clavulanic acid compared with the MIC when tested alone. The protocol was reviewed and approved by Institutional Review Boards according to local requirements.\n\n【9】All data were entered into a central database (PROPHET version 5.1; BBN Systems and Technologies Corporation, Cambridge, MA). Contingency data were analyzed by two-tailed chi-square or Fisher’s exact tests, and continuous data were analyzed by Student _t_ test or Mann-Whitney U test.\n\n【10】### Results\n\n【11】Two hundred two (44.4%) of 455 episodes of _K. pneumoniae_ bacteremia during the study period were community-acquired cases. The percentage of cases of _K. pneumoniae_ bacteremia that were community acquired in each study country differed strikingly: 96 (68%) of 142 in Taiwan, 25 (43%) of 68 in the United States, 28 (39%) of 71 in Australia, 40 (34%) of 116 in South Africa, 6 (22%) of 27 in Europe, and 7 (17%) of 41 in Argentina. _K. pneumoniae_ bacteremia in Taiwan was significantly more likely to be community acquired than was bacteremia in the other countries combined (68% vs. 36%, p=0.0001).\n\n【12】The characteristics of patients with community-acquired _K. pneumoniae_ bacteremia from Taiwan, South Africa, and the rest of the world were compared . The source of bacteremia in community-acquired cases was geographically distinctive . Pneumonia was the most common infection worldwide, accounting for 57 (28%) of 202 cases. However, 53 (93%) of 57 of all cases of community-acquired _K. pneumoniae_ pneumonia occurred in Taiwan and South Africa.\n\n【13】Antibiotics had been used for >24 hours before admission in 21 (10%) of 202 patients. Prior antibiotic use was significantly lower in Taiwan (4 \\[4%\\] of 96 patients) and South Africa (2 \\[5%\\] of 40) than in the other countries (15 \\[23%\\] of 66) (p=0.0003). In four patients, the prior antibiotic was an oxyimino-containing cephalosporin (two in South Africa, one in Argentina, and one in Taiwan).\n\n【14】ESBL production was detected in 7 (3.5%) of 202 community-acquired strains compared with 78 (30.8%) of 253 hospital-acquired strains (p<0.00001). None of the patients with community-acquired ESBL-producing strains had recently received an oxyimino-containing cephalosporin. However, of the seven patients with community-acquired ESBL-producing _K. pneumoniae_ bacteremia, only one (a patient with pneumonia from Africa) had no recent hospital exposure. The countries from which community-acquired ESBL-producing _K. pneumoniae_ isolates were collected included Africa (three isolates), Turkey (two), the United States (one), and Australia (one). Twenty-five bloodstream isolates were ciprofloxacin resistant, of which 7 (28%) of 25 were community acquired. All these seven patients had serious underlying disease and frequent hospitalizations or nursing home admissions, and none had received a quinolone in the 14 days before hospital admission. Five of seven patients with community-acquired, ciprofloxacin-resistant _K. pneumoniae_ bacteremia were from Taiwan.\n\n【15】##### Community-Acquired Pneumonia\n\n【16】Community-acquired pneumonia due to _K. pneumoniae_ was significantly associated with alcoholism (p=0.007); 18% of patients with pneumonia were alcoholics as defined by their physicians, compared with 4% with other sources of _K. pneumoniae_ bacteremia . However, no patient with community-acquired _K. pneumoniae_ bacteremic pneumonia outside South Africa or Taiwan was an alcoholic; of these patients, one was neutropenic, two were nursing home residents with neurologic impairment (ages 81 and 90), and one was a Vietnamese immigrant to Australia with no underlying illness.\n\n【17】Community-acquired pneumonia due to _K. pneumoniae_ was significantly associated with HIV infection on univariate evaluation (p=0.002). Of the seven patients with HIV infection and _K. pneumoniae_ bacteremia (all from Africa), six had community-acquired pneumonia. Community-acquired pneumonia due to _K. pneumoniae_ was not associated with underlying liver disease, chronic renal failure, receipt of chemotherapy for malignant disease, or receipt of corticosteroids.\n\n【18】Multivariate analysis showed that residing in Africa (p=0.0001) or Taiwan (p=0.0046) and being an alcoholic (p=0.04) were significantly associated with community-acquired _K. pneumoniae_ pneumonia. HIV infection was not independently associated with pneumonia (p=0.23). The death rates from community-acquired pneumonia due to _K. pneumoniae_ were 54% in Taiwan and 56% in South Africa.\n\n【19】##### A Distinctive _K. pneumoniae_ Bacteremia Syndrome\n\n【20】Twenty-five patients had a distinctive syndrome of _K. pneumoniae_ bacteremia, which was defined by the presence of _K. pneumoniae_ bacteremia in conjunction with liver abscess, endophthalmitis, or meningitis. Of these patients, 88% (16 with liver abscess, 4 with meningitis, 1 with liver abscess and meningitis, and 1 with endophthalmitis) were from Taiwan, compared with 12% from the other countries combined (2 with meningitis from South Africa and 1 with liver abscess from Belgium) (p=0.0001).\n\n【21】Twelve (67%) of 18 patients with liver abscess had diabetes mellitus. On univariate analysis, residing in Taiwan (p=0.0001) and having diabetes mellitus (p=0.001) were significantly associated with community-acquired _K. pneumoniae_ liver abscess. Patients with liver abscess were more likely to have renal failure, but this association was not statistically significant (p=0.09). There was no association between liver abscess and gender, age, previous antibiotic use, or presence of underlying liver disease. Multivariate analysis showed that residence in Taiwan (p=0.0034), diabetes mellitus (p=0.0058), and renal failure (p=0.0178) were significantly associated with the presence of liver abscess.\n\n【22】Patients with any of the distinctive manifestations of _K. pneumoniae_ (liver abscess, meningitis, or endophthalmitis) were compared with patients with other community-acquired infections . These complications were significantly associated with diabetes mellitus (60% vs. 28%, p=0.0015) and living in Taiwan (88% vs. 42%, p=0.0001).\n\n【23】### Discussion\n\n【24】When pneumonia due to _Klebsiella_ was first described by Friedlander in 1882, he believed it to be the most common cause of bacterial pneumonia  . Although this concept was soon refuted in favor of pneumococcus, from the 1930s through the 1960s, 10 to 50 cases of _Klebsiella_ pneumonia were reported each year by large hospitals in the United States  . U.S. textbooks of medicine continue to list _K. pneumoniae_ as an important cause of community-acquired pneumonia .\n\n【25】In our prospective study, we found only four cases of community-acquired _K. pneumoniae_ pneumonia in 2 years in nine large hospitals from the United States, Australia, Europe, and Argentina. The hospitals surveyed included an inner-city veterans hospital in the United States and two large inner-city public hospitals in Australia. These three centers care for large numbers of indigent and alcoholic patients.\n\n【26】Recently published reports from the United States, Israel, and Europe support our observations. Neither Vergis et al. from the United States nor Lieberman et al. from Israel found a single case of _K. pneumoniae_ pneumonia in large multicenter studies of community-acquired pneumonia in the 1990s. Nine European studies published since 1990 show that only 14 (2.3%) of 621 patients admitted with severe community-acquired pneumonia requiring intensive-care unit admission had _K. pneumoniae_ as the presumptive etiologic agent . In contrast, _K. pneumoniae_ continues to be associated with community-acquired pneumonia in Africa and Asia. In our study, we observed 28 cases in Taiwan and 25 in South Africa, accounting for 29% and 62% of all cases of community-acquired _K. pneumoniae_ bacteremia in South Africa and Taiwan, respectively . Recent studies from Taiwan, Singapore, and South Africa corroborate these findings. In a Taiwanese study, _K. pneumoniae_ accounted for 34% of 41 cases of community-acquired bacteremic pneumonia  . _K. pneumoniae_ was the cause of 15% of community-acquired pneumonia requiring intensive-care unit admission in Singapore  . _K. pneumoniae_ was found to be the cause of pneumonia in 32% of African patients with severe community-acquired pneumonia requiring intensive-care unit admission in Johannesburg  and 11% of patients requiring intensive-care unit admission in Cape Town  .\n\n【27】In our study, 18% of patients with community-acquired _K. pneumoniae_ pneumonia were alcoholics (p=0.007) . Alcoholics in Africa and Asia may have limited access to health care (perhaps including reduced access to antibiotics) compared with those in the Americas, Europe, and Australia, and may have respiratory symptoms later. A weakness of our study is that we were not able to ascertain the duration of symptoms before each patient was hospitalized. However, a recent study of aborigines from rural northern Australia (35% of whom were alcoholics and most of whom had suboptimal access to health-care facilities) showed that none of 90 admitted to hospital with community-acquired pneumonia had _K. pneumoniae_ infection  . The hypothesis that _Klebsiella_ pneumonia is related to poor primary health care for alcoholics may therefore be less plausible.\n\n【28】Bacteremic _K. pneumoniae_ liver abscess occurred almost exclusively in patients from Taiwan , consistent with a growing number of reports from Asia describing this distinctive type of infection. _K. pneumoniae_ was the most common cause of liver abscesses in Taiwan, Singapore, and Korea in reports from 1990 to 1999 ; similarly, numerous reports of liver abscess have recently been published from Hong Kong, Thailand, and Japan . In total, >900 patients with _K. pneumoniae_ liver abscess have been reported from Asian countries in the last 10 years; in contrast, reports of only 23 patients with this condition have been published from regions outside Asia in this same period .\n\n【29】_K. pneumoniae_ meningitis in adults has also been infrequently reported from North America, Europe, and Australia, in contrast to Taiwan. In our study, five cases of bacteremic _K. pneumoniae_ meningitis were in Taiwanese patients and two in African patients . Four (57%) of 7 patients with meningitis had prior diabetes mellitus. Meningitis caused by _K. pneumoniae_ in the United States, Australia, and Europe is most often hospital acquired and associated with prior neurosurgical procedures or instrumentation. However, of 115 cases of _K. pneumoniae_ meningitis reported from Taiwan , 84% were community acquired, and 64% of cases had concurrent _Klebsiella_ bacteremia. Unlike pyogenic liver abscess, the clinical course was fulminant, with a death rate of 57% . The death rate from bacteremic _K. pneumoniae_ meningitis in our series was 71%.\n\n【30】We found only one patient (an alcoholic from Taiwan) with _K. pneumoniae_ bacteremia and endophthalmitis. _K. pneumoniae_ endophthalmitis is also likely to be far more common in Asia than elsewhere; >50 cases have been reported in the last 10 years from Asia  compared with only 10 from the United States, Europe, and Australia . More than 50% of previously reported Asian patients with _K. pneumoniae_ endophthalmitis have had concurrent liver abscess .\n\n【31】The reason for the geographic preponderance of these severe manifestations of _K. pneumoniae_ infections in Asia is unknown. The geographic diversity of _Klebsiella_ infections possibly results from interaction between bacterial variables, host variables (for example, defects in host defense caused by diabetes mellitus or alcoholism), socioeconomic factors, and possibly genetic susceptibility in different racial groups. We are studying the phenotypic and genotypic differences in _K. pneumoniae_ causing different disease manifestations in different countries. Because no more than three hospitals from each country were included in our study, our results may not necessarily be generalizable to hospitals in other regions. In addition, other countries in the same continent (e.g. other countries in Asia or eastern Europe) were not studied but may have different clinical patterns compared with the study country.\n\n【32】In summary, our results challenge the classic view of serious _Klebsiella_ infections. In the United States, Europe, Argentina, and Australia, we have observed that hospital-acquired _K. pneumoniae_ infections predominate, with community-acquired bacteremia being caused by urinary tract infection, vascular catheter infection, and cholangitis. Classic community-acquired pneumonia is no longer an important entity in these regions. In South Africa, pneumonia (especially in alcoholics) continues to be an important community-acquired infection. In Taiwan, community-acquired pneumonia persists, and distinctive infections such as liver abscess, endophthalmitis, and meningitis have emerged as substantial public health problems.\n\n【33】Wen-Chien Ko is an infectious disease physician at National Cheng Kung University Medical Collete, Tainan, Taiwan. His research interests are in the pathogenesis of _Klebsiella_ infections and the epidemiology and treatment of _Aeromonas_ infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aa7aa2e8-970d-4b5a-b021-15c82278ef32", "title": "Alone Together Then and Now", "text": "【0】Alone Together Then and Now\nEdgar Degas  Absinthe (c. 1876) Oil on canvas (92 cm × 68 cm) Photo: Hervé Lewandowski. Réunion des Musées Nationaux/Art Resource, New York, NY, USA Musée d’Orsay, Paris, France\n\n【1】“Sickly, neurotic, and so myopic that he is afraid of losing his sight; but for this very reason an eminently receptive creature and sensitive to the character of things,” wrote French writer and art critic Edmond de Goncourt about Edgar Degas. The artist knew his own difficult nature. “\\[I have\\] one terrible, irreconcilable enemy,” he once admitted to Pierre-Auguste Renoir, “myself, of course.” From those who associated with him, Degas exacted an emotional toll. “There will be a dish cooked without butter for me. No flowers on the table, very little light …. You’ll shut up the cat, I know, and no one will bring a dog. And if there are women there, ask them not to put smells on themselves …. Scent, when there are things that smell so good! Such as toast, for example. And we shall sit down to table at exactly half-past seven.”\n\n【2】“All his friends had to leave him,” Renoir reported, “I was one of the last to go, but even I couldn’t stay till the end.” Considered a misogynist by some, Degas counted among his friends Mary Cassatt, Berthe Morisot, and leading opera divas and ballerinas of his day. Accused of being a recluse, he denied it. “I am not a misanthrope, far from it, but it is sad to live surrounded by scoundrels.” Despite his uncompromising persona, he was respected by his peers, who were afraid of him, and was popular with art critics and buyers. “I was, or appeared to be, hard with everyone, owing to a sort of tendency towards roughness that originated in my doubts and my bad temper.”\n\n【3】Born in Paris during the same decade as Édouard Manet, Paul Cézanne, and Claude Monet, Degas had many opportunities. His early years were privileged, though tinged by the melancholy that followed him all his life. “I was sulky with the whole world and with myself.” Under pressure he agreed to study law but soon abandoned the effort and pursued art with a fervor that convinced his father to support him, first at the Atelier Lamothe and École des Beaux-Arts and then independently in Italy, “the most extraordinary period of my life.”\n\n【4】Degas’ early works were historical paintings in the classical tradition. Early in his training, he absorbed the methods of Auguste-Dominique Ingres, Eugène Delacroix, and Gustave Courbet and aspired to paint like Michelangelo and Raphael. But by the 1860s, he abandoned history for scenes of everyday life. While copying a Velásquez at the Louvre, he met Manet, who became his friend and brought him into the circle of impressionist painters. Though Degas exhibited with them often, he never identified himself with the movement. “What I do is the result of reflection and study of the great masters. Of inspiration, spontaneity, temperament I know nothing.” He was not interested in the transient effects of light on landscape. He preferred painting people and abhorred painting _en plein air_ . “The gendarmes should shoot down all those easels cluttering up the countryside.”\n\n【5】“Draw lines, young man, a great many lines,” Ingress once advised Degas, who took the comment to heart. “I always tried to urge my colleagues along the path of draftsmanship, which I consider a more fruitful field than that of color.” Always seeking perfection, he reworked every picture, even after it was sold, studying and repeating details until he had mastered and memorized them. Owners were known to chain his works to the wall. He experimented with many media, among them pastels, which he softened over steam into a paste and used over gouache and monotype prints. He disliked the shine of oil paints, so he removed the oil and applied with turpentine, often on paper rather than canvas.\n\n【6】Absinthe, on this month’s cover, appears to be a genre scene. But it is a portrait of Degas’ friend Marcellin Desboutin, writer, artist, printmaker, and a regular at the Café de la Nouvelle-Athènes, a meeting place for the impressionists and others in the avant garde. “I did not go to either Oxford or Cambridge,” Irish art critic George Moore said about his education, “but I went to the Nouvelle-Athènes.” Against all convention, the focal point of the portrait is a woman seated at Desboutin’s side. She is Ellen Andrée, a model who posed often for Degas and Renoir and aspired to be a serious actress, “like Sarah Bernhardt … in Phèdre.”\n\n【7】This painting of an unloving couple was called “the perfection of ugliness” by one critic and caused a stir when exhibited in London. “It is not a painting at all,” other critics said, “It is a novelette―a treatise against drink.” Desboutin, his elbow a wall between him and his companion, is detached, lost in thought. He is not even entirely in the picture―pipe, arm, and one leg cropped, eyes glaring off somewhere. She is precariously in center stage, her social status exposed. Pushed off one table, not quite at the next one, she sits in-between, as awkwardly positioned as her carafe.\n\n【8】The painting’s architecture drives the story, framing it in fresh and innovative ways. The marble tables, zigzagging across the picture, create perspective by drawing the eyes to the figures barricaded behind them, whose reflections in the mirror suggest the presence of other patrons without actually showing any. Newspapers form a bridge between the tables. The artist’s signature seems a seal of approval.\n\n【9】This nearly monochromatic snapshot lays bare human isolation in the midst of gaiety. Desputin’s drooping companion, propped up behind a glass of absinthe, represents women, many of them in Degas’ very neighborhood, caught in ill-fitting bohemianism, absinthe not withstanding, for as Oscar Wilde put it, “After the first glass, you see things as you wish they were. After the second, you see things as they are not. Finally, you see things as they really are, and that is the most horrible thing in the world.”\n\n【10】Degas’ women were often at a disadvantage, whether they sought solace in cafés, danced _en pointe_ , or washed and ironed clothes in substandard conditions for meager pay. They got sick and died young. But through their drab lives, fluffy costumes, or trappings of gaiety in busy venues, the artist’s penetrating eye captured for all to see not emotional isolation alone, which had marred his own life, but its many causes: poverty, social stigma, and underlying illnesses, not the least of them tuberculosis, rampant in his day.\n\n【11】The costumes have changed and absinthe is no longer the drug of choice, but emotional isolation lives on as do its many causes. Therapies have curtailed tuberculosis in some parts of the world, despite the emergence of multidrug resistance, but pulmonary infections caused by nontuberculous mycobacteria are on the rise, prompting investigations and gathering of data to explore and identify what Ellen Andrée was not able to find in the bottom of the absinthe glass.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b6e7c2d7-8cec-4a71-8840-e2d970deae09", "title": "Multidrug-Resistant Corynebacterium striatum Associated with Increased Use of Parenteral Antimicrobial Drugs", "text": "【0】Multidrug-Resistant Corynebacterium striatum Associated with Increased Use of Parenteral Antimicrobial Drugs\nCorynebacteria are a normal component of the microbiota of human skin and mucous membranes. At the University of Washington Medical Center (Seattle, WA, USA), _Corynebacterium striatum_ has historically been the second most commonly isolated _Corynebacterium_ species, after _C. jeikium_ . Although _C. striatum_ is a frequent colonizer , it might also be implicated as a true pathogen when isolated in multiple samples from sterile sites or from indwelling hardware and devices . Determination of whether an isolate represents infection, colonization, or contamination is based upon clinical judgment.\n\n【1】Although early reports indicated that _C. striatum_ isolates were frequently susceptible to many antimicrobial drugs, including β-lactams, tetracycline, and fluoroquinolones , more recent studies have demonstrated increasing multidrug resistance . To clarify the spectrum of disease associated with _C. striatum,_ we retrospectively extracted clinical information for immunocompetent patients with _C. striatum_ isolates to determine clinical relevance, antimicrobial drug susceptibilities, and length of parenteral therapy. For patients with device-associated infections, we compared the length of parenteral therapy for _C. striatum_ with that for coagulase-negative staphylococci, other low-virulence organisms that commonly colonize the skin.\n\n【2】### Methods\n\n【3】##### Patients\n\n【4】We used an algorithm-based query of the De-Identified Clinical Data Repository maintained by the Institute for Translational Health Sciences of the University of Washington (Seattle, WA, USA) to identify 213 patients from the university medical center infected with _C. striatum_ isolated from a clinical sample during 2005–2014. Adult patients (>18 years of age) were included if _C. striatum_ was isolated from a specimen submitted for bacterial culture and identified to the species level. Because we were interested in whether _C. striatum_ would be considered pathogenic in an immunocompetent population, patients with active immunosuppression were excluded from our analysis. We excluded 34 patients with immunosuppressant use at the time of culture with microbiological growth on the basis of pharmacy records (defined as documentation of use of corticosteroids, methotrexate, infliximab, adalimumab, tacrolimus, cyclosporine, mycophenolate mofetil, or rituximab), or with a diagnosis of active malignancy or HIV/AIDS according to codes from the International Classification of Diseases, 9th Revision. Our algorithm-based method was confirmed by using a manual chart review.\n\n【5】##### Bacterial Identification and In Vitro Drug Susceptibility Testing\n\n【6】Corynebacteria were identified to the species level if isolated in pure culture or if deemed to be clinically meaningful if present in a polymicrobial culture. Identification of corynebacteria was initially performed during 2005–2012 by using the RapID CB Plus Kit (Thermo Fisher Scientific, Waltham, MA, USA). This kit correctly identifies 95% of _Corynebacterium_ isolates to the species level . During 2012–2014, corynebacteria were identified by using matrix- assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry (MALDI Biotyper and Biotyper software versions 3.0 and 3.1; Bruker Daltonics, Billerica, MA, USA) and a cutoff score of 2.0. Use of MALDI-TOF mass spectrometry with the Bruker system database correctly identifies corynebacteria to the genus level for >99% of isolates and correctly identified _C. striatum_ in 100% (51/51) of clinical isolates tested .\n\n【7】Phenotypic susceptibility testing was performed by using the E-test (bioMérieux, Marcy l’Étoile, France) and Remel blood Mueller Hinton agar (Thermo Fisher Scientific) and incubation for 24–48 h at 35°C in ambient air, according to breakpoints of the Clinical and Laboratory Standards Institute . Because breakpoints have recently changed , we tested whether shifting the breakpoint would alter our results. Susceptibilities were available for penicillin, ciprofloxacin, clindamycin, erythromycin, and tetracycline. For patients from whom >1 isolate of _C. striatum_ were obtained, we considered only the first isolate in our analysis.\n\n【8】##### Clinical Data Extraction\n\n【9】We obtained the following variables from chart review by manual extraction: patient location when the culture was obtained (i.e. inpatient versus outpatient), whether the culture grew _C. striatum_ in pure culture or was polymicrobial, whether the treating physician considered the isolate to be clinically relevant or a contaminant, length of parenteral therapy administered, and whether adverse events were documented. If the treating physician did not comment on the isolate, the isolate was categorized as clinically irrelevant. Adverse events were defined as clinical event necessitating a change in antimicrobial agent and were graded according to the Common Terminology for Adverse Events . Serious adverse events were considered grade 3 or 4. The outpatient parenteral antimicrobial therapy practices at the University of Washington Medical Center monitor for adverse events by weekly measurement of a complete blood count and monitoring of renal function. We used these weekly measurements as a proxy to verify ongoing administration of outpatient parenteral therapy in combination with review of documentation in clinical notes.\n\n【10】##### Matched Case−Control Analysis\n\n【11】In a subset of patients with hardware-associated osteomyelitis or infections of implanted cardiac devices, we performed a matched case−control analysis to examine length of parenteral therapy in _C. striatum_ cases compared with that for persons infected with coagulase-negative staphylococci (controls). Persons infected with coagulase-negative staphylococci were chosen as controls because isolates are frequently found in clinical samples (enabling appropriate matching), colonize the skin, and generally have low virulence, similar to _C. striatum_ . Cases were matched to controls on the basis of age (± 5 years), site of infection, and presence or absence of hardware associated with the infection site.\n\n【12】##### Statistical Analysis\n\n【13】Statistical analysis was performed by using Prism 6 (GraphPad Software Inc. La Jolla, CA, USA). Data are reported as mean ± SE. Parametric data were compared by using the _t_ \\-test, and nonparametric data were compared by using χ 2  or Mann-Whitney U tests. A p value <0.05 was considered significant.\n\n【14】### Results\n\n【15】We identified 179 immunocompetent adult patients infected with _C. striatum_ isolated from clinical specimens. A substantial proportion (48%, 86/179) of these isolates were believed to be from clinically relevant infections. For comparison, in our laboratory system during a similar period, ≈42,000 isolates of _Staphylococcus aureus_ and 4,800 isolates of coagulase-negative staphylococci were recovered with in vitro susceptibilities determined.\n\n【16】Consistent with previous reports that _C. striatum_ is a colonizer of the skin and mucous membranes, isolates were frequently reported in sputum and skin samples (65/179) . Interpretation of clinical relevance varied markedly by sample site. Isolates from deep surgical specimens, such as bone and surgical hardware, were generally considered to be clinically relevant (88%–95%), whereas isolates from urine, sputum, or skin were rarely considered to be clinically relevant (10%–15%) . Isolates from bronchoalveolar lavage samples were indicated by clinicians to be clinically relevant in 45% (9/20) of cases, and all of these patients were empirically treated with vancomycin for healthcare-associated pneumonia for a mean ± SE duration of 11 ± 1.9 days. No clinical failures for vancomycin therapy were documented.\n\n【17】Eighty percent (143/179) of isolates were found in an inpatient setting. Susceptibility testing was performed for 121/179 isolates, and 72% (87/121) were resistant to all oral antimicrobial drugs tested . The percentage of drug-resistant isolates obtained from the inpatient setting did not differ from that found in the outpatient setting (p = 0.27).\n\n【18】We determined in vitro susceptibilities for several antimicrobial drugs . In general, MIC distributions were bimodal, whereby most drugs had low MICs or high MICs, except for vancomycin, which was universally active in vitro. The breakpoint of the Clinical and Laboratory Standards Institute for penicillin changed in 2015  from 1 mg/L to 0.125 mg/L. This change increased the rate of isolates considered penicillin resistant from 85% to 98%, but did not affect the number of isolates without an oral drug option because all isolates with a penicillin MIC <1.0 mg/L were susceptible to tetracycline. Daptomycin was rarely formally tested in our laboratory during this period (n = 6), including the 2 patients we previously described . Initial MICs for daptomycin were 0.032–0.125 mg/L, but it was rarely used by clinicians in our cohort. Similarly, linezolid was rarely tested (n = 10), was uniformly active in vitro, but was not used clinically.\n\n【19】All patients with infections deemed to be clinically relevant were initially given vancomycin, except 1 patient with a documented allergy to vancomycin who received daptomycin. As expected, the length of time parenteral antimicrobial drugs were administered varied widely by anatomic site of infection and presence of a foreign body. Prosthetic joint infections were treated with parenteral therapy for 54 ± 7 days, and other hardware- or device-associated infections were treated for 65 ± 10 days. One patient with a ventricular assist device−associated infection who received vancomycin for 650 days was excluded from this analysis (outlier).\n\n【20】In a subset restricted to the 38 patients with hardware (including prosthetic joint) or device-associated infections deemed clinically meaningful, we successfully matched 27 patients with control patients infected with coagulase-negative staphylococci. Eleven patients could not be matched for site of infection or age. When we compared control patients infected with coagulase-negative staphylococci with patients infected with _C. striatum_ , we found that those infected with _C. striatum_ had a longer course of parenteral antimicrobial drugs (69 ± 5 days vs. 25 ± 4 days; p<0.001) . _C. striatum_ isolates were more likely to be monomicrobial (14/27, 52%) than coagulase-negative staphylococci (5/27, 19%) (p = 0.02).\n\n【21】Five serious adverse events were associated with parenteral antimicrobial drugs in the _C. striatum_ group and only 1 serious adverse event in the coagulase-negative staphylococci group (p = 0.19). The serious adverse events (all associated with vancomycin) were 1 drug reaction with eosinophilia and systemic symptoms syndrome, 2 acute kidney injuries with creatinine levels >3 times baseline values, and 3 absolute neutrophil counts <1,000 cells/mm 3  .\n\n【22】### Discussion\n\n【23】Because _C. striatum_ can be a component of the skin microbiota, determination of whether microbiologic growth in a clinical sample represents an infection depends on clinical judgment. Previous investigations have described _C. striatum_ primarily as a nosocomial pathogen, frequently in the setting of underlying malignancy or organ transplantation . Our report documents that clinicians encountering _C. striatum_ in clinical samples of immunocompetent patients frequently consider the isolate to be a true pathogen.\n\n【24】Prior studies of small groups of patients have indicated that isolation of _C. striatum_ from bone or a medical device is typically considered by the treating physician to be relevant , and our study confirms these findings. Furthermore, our study provides support for the pathogenic role of _C. striatum_ in hardware-associated infections because we found that these infections with _C. striatum_ were more likely to be monomicrobial than infections with coagulase-negative staphylococci. In contrast to some reports in which isolation from a respiratory sample was frequently determined to be clinically relevant , less than half of the respiratory isolates in our study were considered to reflect lower respiratory tract infections.\n\n【25】We documented that a multidrug-resistant phenotype of _C. striatum_ directly affects clinical care. Osteomyelitis and hardware-associated infections are difficult to treat, often requiring a prolonged course of antimicrobial drugs. In some situations, guidelines recommend a limited duration of parenteral therapy followed by a longer period of oral therapy . A lack of well-tolerated oral treatment options active against _C. striatum_ would be expected to lead to a longer duration of use of parenteral antimicrobial drugs for patients with these infections, and our matched case−control study confirmed this expectation.\n\n【26】The longer that parenteral antimicrobial therapy is necessary, the greater the likelihood of adverse events associated with intravenous access. These events include a rate of line events (mostly thrombosis) ranging from 5 to 17 episodes/100 devices and infection rates of 0.5 to 5 infections/100 lines . Although we documented more adverse events associated with treatment in the _C. striatum_ group than in the coagulase-negative staphylococci group, this difference did not achieve a priori statistical significance, and we did not capture line thrombosis events. Furthermore, parenteral therapy is associated with substantially increased costs, even when comparing an inexpensive parenteral antimicrobial drug (vancomycin) with an expensive oral antimicrobial drug (linezolid) . In addition, parenteral options for _C. striatum_ will be increasingly limited because our group and others have reported clinical failures caused by rapid development of high-level daptomycin resistance . Because daptomycin resistance can emerge rapidly, it is reasonable to assume that increased daptomycin use could also cause resistance to this drug.\n\n【27】One oral treatment option for multidrug-resistant _C. striatum_ infections is linezolid. Although testing for _C. striatum_ linezolid susceptibility is rarely performed in our clinical microbiology laboratory, to our knowledge, linezolid resistance has never been reported for corynebacteria. Nevertheless, linezolid is poorly tolerated during the long courses of treatment required for hardware- or device-associated infections and has shown a rate of adverse events leading to treatment discontinuation ranging from 34% to 80% . None of our patients in our study were treated with linezolid for a prolonged time, which most likely reflects reluctance of physicians to use an agent with such a high rate of toxicity.\n\n【28】We demonstrated that multidrug resistance was common even in isolates that were not considered to be clinically meaningful in an outpatient setting. We hypothesize that these resistant strains of _C. striatum_ are probably circulating in the community rather than emerging under nosocomial pressure, but further studies would be needed to establish the ecologic niche of drug-resistant _C. striatum_ strains.\n\n【29】Strengths of our study include a systems-wide approach to _C. striatum_ infections in immunocompetent hosts and the large number of _C. striatum_ case reports we reviewed. Detailed clinical information, including the treating physician’s interpretation of the sample, was linked to microbiological isolates. _C. striatum_ will probably be recognized in more clinical settings because use of MALDI-TOF mass spectrometry enables _C. striatum_ to be rapidly identified to the species level with a high degree of confidence without molecular techniques .\n\n【30】Limitations of our study include its retrospective nature and use of data from 1 health system, which restricted potential generalizability of the results. Given that our study was a retrospective study conducted over a 10-year period, we also cannot state whether the isolates are clonally related or reflect a diverse group with divergent mechanisms of drug resistance. In addition, we relied on the treating physician’s interpretation to determine the clinical relevance of an isolate. We have no other way of determining whether the isolate was causing disease, but we believe that this limitation is indicative of general clinical practice. Determining causality would require a different series of mechanistic investigations.\n\n【31】Our study demonstrates that _C. striatum_ is an emerging multidrug-resistant pathogen. Our results highlight the need to identify corynebacteria to the species level, which is now readily performed by using MALDI-TOF mass spectrometry, and perform susceptibility testing for any isolate that is believed to be clinically meaningful. The frequent resistance of _C. striatum_ to all easily tolerated oral antimicrobial drugs supports the need for development of new agents with good oral bioavailability and acceptable long-term safety profiles that are active against gram-positive organisms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4c6639fd-3aae-4002-8767-c682e91546f8", "title": "Meat and Fish as Sources of Extended-Spectrum β-Lactamase–Producing Escherichia coli, Cambodia", "text": "【0】Meat and Fish as Sources of Extended-Spectrum β-Lactamase–Producing Escherichia coli, Cambodia\nIn Europe, evidence for the spread of extended-spectrum β-lactamase (ESBL)–producing _Escherichia coli_ from animals to humans via food is unclear . Few studies have been conducted in low- and middle-income countries, where colonization rates can exceed 60% . High ESBL colonization rates in low- and middle-income countries such as Cambodia are usually attributed to unrestricted consumer access to and hospital overuse of third-generation cephalosporins . However, antimicrobial drugs in classes critical for human health (e.g. β-lactams, macrolides, aminoglycosides, polymyxins) are increasingly being used in food animals . In Cambodia, weak public health protections and consumption of undercooked animal products could exacerbate the spread of ESBL-producing _E. coli_ or ESBL genes from animals to humans.\n\n【1】We had 2 goals with this study. First, we assessed the prevalence of ESBL-producing or carbapenemase-producing _E. coli_ from fish, pork, and chicken from markets in Phnom Penh, Cambodia. Second, we examined the contribution of food-origin isolates to locally disseminated ESBL-producing _E. coli_ by comparing isolates from food with isolates from healthy, colonized persons and infected patients.\n\n【2】### The Study\n\n【3】During September–November 2016, we purchased 60 fish, 60 pork, and 30 chicken samples from 150 vendors at 2 markets in Steung Meanchey district, Phnom Penh  and tested them at the Institut Pasteur du Cambodge for third-generation cephalosporin- and carbapenem-resistant _E. coli_ . We detected ESBL-producing _E. coli_ (all CTX-M-type) among 93 (62%) of 150 food samples, including 32 (53%) of 60 fish, 45 (75%) of 60 pork, and 16 (53%) of 30 chicken samples. We identified carbapenem-resistant _E. coli_ (OXA-type) from 1 pork and 1 fish sample.\n\n【4】We also selected ESBL-producing _E. coli_ from 88 recently pregnant healthy women living in Steung Meanchey and participating in the Bacterial Infections and antibiotic Resistant Diseases among Young children in low-income countries (BIRDY) program, a surveillance program of bacterial infections among young children in low- and middle-income countries . During September 2015–December 2016, ESBL-producing _E. coli_ isolates were cultured from rectal swabs or fecal samples collected at or just after delivery .\n\n【5】We further included ESBL-producing _E. coli_ from 15 Phnom Penh–based patients who sought care at the Sihanouk Hospital Center of Hope during November 2015–December 2016. ESBL-producing _E. coli_ were cultured from blood (12 patients), urine (2 patients), and peritoneal fluid (1 patient) .\n\n【6】We performed whole-genome sequencing for 1 ESBL-producing _E. coli_ isolate from each food sample and all human-origin ESBL-producing _E. coli_ isolates  and compiled genetic and phenotypic characteristics of these196 isolates . We also determined distribution of multilocus sequence types (MLSTs) encoding predominant ESBL- or carbapenemase-gene types .\n\n【7】Phylogenetic analysis of ESBL-producing _E. coli_ genomes revealed 3 distinct clans . Clan I/B2&D (n = 53) comprised mostly human-origin isolates, including isolates from colonized persons and most infected patients. Clans II/A (n = 69) and III/B1 (n = 47) included isolates from colonized persons and from food but not from infected patients. Each clan comprised an exclusive subset of sequence types (STs); clan I/B2&D included ST131 and clonal complex (CC) 38, clan II/A included CC10, and clan III/B1 included CC58 and CC156. Approximately half (21/39) of isolates in clans II/A and III/B1 from colonized patients belonged to STs detected in both humans and meat .\n\n【8】We determined the distributions of ESBL-encoding genes and resistance patterns among isolates from colonized persons by clan . The _bla_ CTX-M-55  gene was more common among colonization isolates belonging to clan II/A than to clan I/B2&D (p<0.05). Amphenicol resistance was more common among colonization isolates belonging to clan II/A than clan I/B2&D (p<0.05) and was most often encoded by _flo_ R .\n\n【9】Women colonized with amphenicol-resistant (vs. amphenicol-susceptible) ESBL-producing _E. coli_ were more likely to report having ever eaten dried poultry (adjusted odds ratio 9.0, 95% CI 1.8–45.2) . Women colonized with CTX-M-55–producing _E. coli_ (vs. other ESBL types) were more likely to have handled live poultry (adjusted odds ratio 4.6, 95% CI 1.1–19.3), but this exposure was uncommon (11/88).\n\n【10】Our genomic and epidemiologic findings suggest that ESBL-producing _E. coli_ that contaminates meat and fish in Phnom Penh may be disseminating to the community. ESBL-producing _E. coli_ were highly prevalent among the meat and fish we sampled. More than 80% of food-origin isolates were amphenicol resistant, and two thirds produced CTX-M-55. When food-origin isolates were compared with human-origin isolates, ≈40% of ESBL-producing _E.coli_ from healthy persons grouped into the same phylogenetic clans that comprised most food-origin isolates. Approximately half of these colonization isolates had MLSTs detected among food, and a substantial portion were more likely to produce CTX-M-55 and be amphenicol resistant than colonization isolates that grouped separately. The fact that chloramphenicol has not been used in human medicine for almost 20 years in Cambodia, yet chloramphenicol analogs (e.g. florfenicol, thiamphenicol) are administered to food animals , suggests a food origin for these colonizing isolates.\n\n【11】Healthy women colonized with amphenicol-resistant ESBL-producing _E. coli_ were more likely to eat poultry meat prepared by sun drying, a process that may not eliminate bacteria . Although we did not test dried meat samples for ESBL-producing _E. coli_ contamination, our finding is consistent with those of other studies . Women reported having prepared dried poultry at home. Especially in low-resource households, sun-dried meat may become cross-contaminated by raw meat, dust, animals, and flies .\n\n【12】Our findings are concerning because of growing interest in using chloramphenicol as a drug of last resort for panresistant strains of bacteria . In the early 2000s, the Cambodia government stopped purchasing chloramphenicol because of concerns about side effects. Since restriction of this drug, infections in the hospital setting have reverted to a chloramphenicol-susceptible phenotype . Nevertheless, our findings suggest that amphenicol resistance genes are circulating in the community, potentially because amphenicol use in food animals has selected for resistant bacteria that can spread to humans . This possibility is concerning because physicians in Cambodia are often unable to assess the resistance of infectious agents before prescribing antimicrobial drugs .\n\n【13】Our study had several limitations. First, for logistical reasons, we sampled meat and fish during only 1 season. Contamination rates may have differed had we sampled across seasons . Second, although we included colonization samples from healthy women, all women had recently given birth in healthcare settings. However, more than half were colonized with ESBL-producing _E. coli_ phylo-types A and B1, supporting community-associated, rather than healthcare-associated, acquisition. Third, we were unable to include clinical isolates from the same population that contributed colonization isolates. Thus, differences in colonization and clinical isolates could have resulted from population differences. Fourth, we did not sample food animals, which could have helped confirm that CTX-M-55–type and amphenicol-resistant ESBL-producing _E. coli_ circulate among them. Last, we did not investigate additional potential pathways for ESBL-producing _E. coli_ transmission to colonized women, such as contact with persons employed at farms or slaughterhouses or proximity to such operations.\n\n【14】### Conclusions\n\n【15】This study, which integrated epidemiologic and genomic methods to characterize community, clinical, and environmental data, supports concerns that the dissemination of antimicrobial drug–resistant bacteria from food animals to humans may be more likely in low- and middle-income countries . This finding is concerning because meat consumption is projected to drastically increase in these countries, and animal production that relies on routine antimicrobial drug use is being promoted to meet this demand . Particularly for low- and middle-income countries such as Cambodia, implementation of multisectoral strategies to combat antimicrobial resistance from a One Health perspective must be supported, and food safety should be prioritized.\n\n【16】Collaborators of the BIRDY program: Bodonirina Tanjona Raheliarivao, Frédérique Randrianirina, Perlinot Herindrainy, Zafitsara Zo Andrianirina, Feno Manitra Jacob Rakotoarimanana, Benoit Garin, Jean-Marc Collard, Thida Chon, Sok Touch, Arnaud Tarantola, Sophie Goyet, Siyin Lach, Veronique Ngo, Muriel Vray, Marguerite Diatta, Joseph Faye, Abibatou Ndiaye, Vincent Richard, Abdoulaye Seck, Raymond Bercion, Amy Gassama Sow, Jean Baptiste Diouf, Pape Samba Dieye, Balla Sy, Bouya Ndao, Maud Seguy, Laurence Watier, Abdou Armya Youssouf, and Michael Padget.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cc992a62-e56d-466b-a906-9b9f93e292d7", "title": "Household Transmission of Human Adenovirus Type 55 in Case of Fatal Acute Respiratory Disease", "text": "【0】Household Transmission of Human Adenovirus Type 55 in Case of Fatal Acute Respiratory Disease\nHuman adenoviruses are associated with mild and acute respiratory infections, depending on the virus type and host immunity. Human adenovirus type 55 (HAdV-55) , formerly known as HAdV-11a , is a reemergent respiratory pathogen that has caused severe pneumonia outbreaks in military and civilian populations in Europe and Asia . However, household transmission of HAdV-55 is rarely reported. We report a case of household transmission of HAdV-55 involving 3 confirmed adult cases with 1 death. Epidemiologic, clinical, and laboratory investigations, along with whole-genome sequencing, elucidate the disease progression and the pathogen origin.\n\n【1】During April 1–May 5, 2012, 7 household members (5 males and 2 females; 3 children and 4 adults) in Anhui Province, China, sequentially experienced influenza-like symptoms, including fever, productive cough, fatigue, pharyngalgia, dyspnea, and other symptoms. The youngest patient was 4 months of age, the oldest, whom we refer to as AQ-1, was a 55-year-old man. The family lived together near a farm in a house with poor sanitary and ventilation conditions.\n\n【2】The first onset of acute respiratory disease (ARD) occurred on April 1, when the index case, a 4-year-old granddaughter of AQ-1, had a febrile respiratory infection with cough. Three days later, AQ-1’s grandson, 1 year of age, displayed similar symptoms. On April 9 and 11, AQ-1’s daughter, 28 years of age, and another grandson, 4 months of age, both had influenza-like symptoms. On April 14, AQ-1 had a fever, chills, and lumbago. He was admitted to the hospital on April 14 where clinicians diagnosed pneumonia. AQ-1 had close contact with his sick grandsons and granddaughter and had not been out of the house during the month he cared for them.\n\n【3】While hospitalized, AQ-1 had bilateral pneumonia seen on chest computed tomography (CT), a temperature of 41.0°C, and low total leukocyte (3.63 × 10 9  /L) and platelet (42 × 10 9  /L) counts. AQ-1 sustained high fever and yellow phlegm despite antiinflammatory and antiviral treatment, including levofloxacin, piperacillin sodium, tazobactam sodium, and ribavirin.\n\n【4】On April 24, AQ-1 had indications of severe pneumonia, including respiratory failure, hypoxemia, double lung rales, and a mass of shadows visible on chest CT. In addition, he had indications of liver damage and multi-organ failure. Transverse chest CT images demonstrated increased areas of patchy shadows and consolidation in both lungs compared to CT images from April 22, indicative of disease progression .\n\n【5】AQ-1 died on April 27, 3 days after onset of respiratory failure, and 13 days after his illness began. On the same day, his 20-year-old son, AQ-2, and 31-year-old nephew, AQ-3, who had taken care of AQ-1 for 5 days, also exhibited symptoms of influenza-like illness. Both were hospitalized and had normal chest CT scans, but AQ-2’s leukocyte count was 5.4 × 10 9  /L and AQ-3’s was 6.7 × 10 9  /L. After antiinflammatory and antiviral treatment, including vitamin C, sulbactam, amoxicillin, amikacin, cefoperazone, ribavirin, and oseltamivir, they recovered and were discharged on May 5 .\n\n【6】We tested endotracheal aspirates from AQ-1 and throat swabs from AQ-2 and AQ-3 for influenza A and B viruses, severe acute respiratory syndrome coronavirus, human metapneumovirus, rhinoviruses, parainfluenza viruses 1–4, and HAdVs by real-time PCR. Only adenovirus was strongly positive for all 3 patients. Testing for antibodies against _Mycoplasma pneumoniae_ , _Mycobacterium tuberculosis_ _Treponema pallidum_ , hepatitis B and C viruses, and HIV, were all negative. After treatment, samples from AQ-2 and AQ-3, were negative for adenovirus by PCR.\n\n【7】We isolated AQ-1’s adenovirus in culture and sequenced the genome . Sequences for the hexon, penton base, and fiber genes were identical to those previously reported for HAdV-55. Phylogenetic analysis showed that the 3 isolates clustered closely with other strains from China . The genome of AQ-1’s strain had the highest nucleotide identity (99.951%) with QZ01\\_2011, an isolate from a military trainee in Shanxi Province, China. The second highest identity (99.948%) was with QS-DLL\\_2006, which caused a fatal ARD outbreak in a senior high school in Shaanxi Province, China  . We hypothesize the strain infecting AQ-1 and his family originated from Shanxi Province.\n\n【8】In this household transmission of ARD, the index case was a probable case because no specimens were collected to confirm virologic identification. From the timeline of illness onset in this household cluster of ARD cases , we suspect that the pathogen spread rapidly among the children and further circulated in adults who had close contact with infected children and one another.\n\n【9】HAdV-55 contains a 97.4% genome of HAdV-14 and a hexon from HAdV-11 . Since 2006, HAdV-14 has caused severe ARD in America, Europe, and Asia , with high hospitalization (38%) and case-fatality (5%) rates . Because the risk for infection among the close contacts may rise, more attention should be paid to these highly contagious pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b47f7c45-656c-4dcd-b613-a345bb5d7182", "title": "Detection of Anaplasma phagocytophilum DNA in Ixodes Ticks (Acari: Ixodidae) from Madeira Island and Setúbal District, Mainland Portugal", "text": "【0】Detection of Anaplasma phagocytophilum DNA in Ixodes Ticks (Acari: Ixodidae) from Madeira Island and Setúbal District, Mainland Portugal\n_Anaplasma_ _phagocytophilum_ (formerly _Ehrlichia phagocytophila_ , _E. equi,_ and the human granulocytic ehrlichiosis agent \\[HGE agent\\] ) is well established as a worldwide tickborne agent of veterinary importance and is considered an emerging human pathogen. The initial reports of human disease caused by _A. phagocytophilum_ , now called human granulocytic anaplasmosis, came from Minnesota and Wisconsin in 1994 . Human granulocytic anaplasmosis is an acute, nonspecific febrile illness characterized by headache, myalgias, malaise, and hematologic abnormalities, such as thrombocytopenia and leukopenia as well as elevated levels of hepatic transaminases . Since that first report, an increasing number of cases have been described, mostly in the upper Midwest and in the Northeast regions of the United States . Three years later, in 1997, acute cases of this disease were also described in Europe . Several serologic and polymerase chain reaction (PCR)-based studies described the wide distribution of _A. phagocytophilum_ across Europe and in some parts of the Middle East and Asia . Nevertheless, confirmed cases of human granulocytic anaplasmosis are rare; most European cases are described in Slovenia , with only a few reports from other European countries  and China .\n\n【1】The ecology of _A. phagocytophilum_ is still being defined, but the agent is thought to be maintained in nature in a tick-rodent cycle, similar to that of _Borrelia burdgdorferi_ (the agent of Lyme disease), with humans being involved only as incidental “dead-end” hosts . Exposure to tick bites is considered to be the most common route of human infection, although human granulocytic anaplasmosis has been reported after perinatal transmission or contact with infected animal blood . _A. phagocytophilum_ is associated with _Ixodes_ ticks that are known vectors, including _I. scapularis,_ _I._ _pacificus,_ and _I. spinipalpis_ in the United States , _I. ricinus_ mostly in southern, central and northern European regions , _I. trianguliceps_ in the United Kingdom , and _Ixodes persulcatus_ in eastern parts of Europe  and Asia .\n\n【2】In Portugal little information is available concerning the epidemiology of _A. phagocytophilum_ ; the agent was documented only once in _I. ricinus_ ticks from Madeira Island (Núncio MS, et al, unpub data). However, the true prevalence and public health impact of _A. phagocytophlum_ is likely underestimated since little research has been conducted on this bacterium in Portugal. In fact, seasonal outbreaks of enzootic abortions and unspecific febrile illness (commonly named pasture fever) in domestic ruminants, which could be attributable to _A. phagocytophilum_ , have been known to breeders and veterinarians across the country for years. Thus, to expand knowledge of _A. phagocytophilum_ in Portugal, a detailed investigation was initiated. The preliminary results concerning agent distribution are presented here. The purpose of this study was to investigate both the persistence of _A. phagocytophilum_ on Madeira Island, where it was initially described, and the presence of the agent in _Ixodes_ ticks from mainland Portugal.\n\n【3】### Materials and Methods\n\n【4】##### Tick Sampling\n\n【5】During 2003 and the beginning of 2004, adults and nymphs were collected from one site on Madeira Island (site 1, Paúl da Serra–Porto Moniz) and from five different sites in the Setúbal District, mainland Portugal (site 2, Barris–Palmela; site 3, Baixa de Palmela; site 4, Picheleiros–Azeitão, site 5, Azeitão, site 6, Maçã–Sesimbra) . Most ticks were unfed, actively questing arthropods; they were obtained by flagging vegetation on pastures and wooded areas bordering farms and country houses. In site 3, additional specimens were also collected from domestic cats ( _Felis catus domesticus_ ). The ticks were identified by morphologic characteristics according to standard taxonomic keys .\n\n【6】##### Preparation of DNA Extracts from Ticks\n\n【7】Ticks were processed individually as described . Briefly, each tick was taken from the 70% ethanol solution used for storage, air dried, and boiled for 20 min in 100 μL of 0.7 mol/L ammonium hydroxide to free DNA. After cooling, the vial with the lysate was left open for 20 min at 90°C to evaporate the ammonia. The tick lysate was used directly for PCR. To monitor for occurrence of false-positive samples, negative controls were included during extraction of the tick DNA (one control sample for each six tick samples, with a minimum of two controls).\n\n【8】##### PCR Amplification\n\n【9】DNA amplifications were performed in a Biometra T-3 thermoblock thermal cycler (Biometra GmbH, Göttingen, Germany) with two sets of primers: msp465f and msp980r, derived from the highly conserved regions of major surface protein-2 ( _msp2_ ) paralogous genes of _A. phagocytophilum_ , and ge9f and ge10r, which amplify a fragment of the 16S rRNA gene of _A. phagocytophilum_ . PCR was performed in a total volume of 50 μL that contained 1 μM of each primer, 2.5 U of Taq DNA polymerase (Roche, Mannheim, Germany), 200 μM of each deoxynucleotide triphosphate (GeneAmp PCR Reagent Kit, Perkin-Elmer, Foster City, CA), 10 mM Tris HCL, 1.5 mM MgCl 2  , and 50 mM KCl pH 8.3 (Roche), as described . Adult ticks were tested individually by using 5 μL of DNA extract. Nymphs were pooled according to geographic site, up to a maximum of 10 different tick extracts per reaction, and 10 μL of the pooled DNA was used for initial screening. All positive pools were confirmed in a second PCR round that used 5 μL of original DNA extract from each nymph. PCR products were separated on 1.5% agarose by electrophorectic migration, stained with ethidium bromide, and visualized under UV light. Quality controls included both positive and negative controls that were PCR amplified in parallel with all specimens. To minimize contamination, DNA preparation with setup, PCR, and sample analysis were performed in three separate rooms.\n\n【10】##### DNA Sequencing and Data Analysis\n\n【11】Each positive PCR product was sequenced after DNA purification by a MiniElute PCR Purification Kit (Qiagen, Valencia, CA). For DNA sequencing, the BigDye terminator cycle sequencing Ready Reaction Kit (Applied Biosystems, Foster City, CA), was used as recommended by the manufacturer. Sample amplifications were performed with the forward and reverse primers used for PCR identification , with the following modifications: 25 cycles of 96°C for 10 s, 4°C below the melting temperature of each primer for 5 s, and 60°C for 4 min. Dye Ex 96 Kit (Qiagen) was used to remove the dye terminators. Sequences were determined with a 3100 Genetic Analyzer sequencer (Applied Biosystems). After review and editing, sequence homology searches were made by BLASTN analysis of GenBank. Sequences were aligned by using ClustalX  with the neighbor-joining protocol and 1,000 bootstrap replications, and comparing with the 2 _msp2_ paralogs of _A. phagocytophilum_ Webster strain (AY253530 and AF443404), one _msp2_ paralog of USG3 strain (AF029323), and with _A. marginale msp2_ (AY138955) and _msp3_ (AY127893) as outgroups. Dendrograms illustrating the similarity of _msp2_ s were visualized with TreeView .\n\n【12】### Results\n\n【13】A total of 278 _Ixodes_ ticks were tested for _A. phagocytophilum_ DNA, including 142 _I. ricinus_ from Madeira Island and 43 _I. ricinus_ and 93 _I. ventalloi_ from Setúbal District. The site of collection, origin, and tick stage are shown in Table 1 and Figure 1 . PCR performed with the _msp2_ primers detected _A. phagocytophilum_ DNA in seven pools of nymphs (six pools of 10 _I. ricinus_ from site 1, Madeira Island, and one pool of 4 _I. ventalloi_ from site 3, Setúbal District) and also in 1 male _I. ventalloi_ from site 3, Setúbal District, as demonstrated by the characteristic 550-bp band. PCRs conducted on individual ticks that comprised positive pools confirmed the results and showed that only one nymph per positive pool contained _A. phagocytophilum_ DNA . PCR test results were negative for all _I. ricinus_ collected in the sites in Setúbal District. Overall, the infection rate was 6 (4%) of 142 for _I. ricinus_ and 2 (2%) of 93 for _I. ventalloi_ . Analysis based on direct amplicon sequencing showed the expected conserved 5′ end followed by ambiguous sequences that corresponded to the hypervariable central region of _msp2_ , as anticipated based on the presence of >52 _msp2_ copies in the _A. phagocytophilum_ HZ strain genome . Thus, for appropriate comparison and alignment, the _msp2_ 5′ sequences were edited from the positions where unambiguous reads could be determined and terminated 70 nt into the sequence at the approximate beginning of the hypervariable region. A similar alignment protocol for the 3′ end of the _msp2_ amplicons showed more ambiguous positions, which prohibited effective alignment and sequence determination. Thus, _msp2_ sequence alignments depended upon approximately 70 nt 5′ to the hypervariable region and were performed less for phylogenetic stratification of _A. phagocytophilum_ in the ticks than to confirm that the amplified _msp2_ sequences were not derived from other related _Anaplasma_ or _Ehrlichia_ spp. The nucleotide sequences determined for this 70-bp region amplified from all eight ticks showed 98.5%–85.7% similarity, 94.2%–86.9% similarity when compared to representative _msp2_ sequences of _A. phagocytophilum_ Webster and USG3 strains, and 63.7%–35.0% similarity when compared to _A. marginale_ _msp2_ and _msp3_ sequences . Sequences obtained from the two _I. ventalloi_ from mainland Portugal clustered together and separately from other _msp2_ sequences obtained from _I. ricinus_ on Madeira Island .\n\n【14】When amplified by using _rrs_ primers ge9f and ge10r, compared to _A. phagocytophilum_ U02521, sequences were 99% identical to two _I. ventalloi_ (636/640 positions and 846/848 positions, respectively) on mainland Portugal and to three _I. ricinus_ (836/841, 817/820, and 838/839 positions, respectively) on Madeira Island.\n\n【15】### Discussion\n\n【16】This study constitutes part of a larger effort to investigate the distribution of _A. phagocytophilum_ in various regions of Portugal. Our data provide supporting evidence that _A. phagocytophilum_ is present in actively questing _I. ricinus_ from Madeira Island and in _I. ventalloi_ from Setúbal District, mainland Portugal.\n\n【17】We used two approaches for identifying _A. phagocytophilum_ in ticks: 1) standard amplification of _rrs_ that can have limited sensitivity because of a single copy in each bacterial genome, and 2) amplification of _msp2_ , a gene for which as many as 52 paralogs are present in the _A. phagocytophilum_ genome and for which detection sensitivity is enhanced . The pitfall of _msp2_ amplification derives from targeting conserved sequences that flank a hypervariable central region, which results in amplicons with partial sequence ambiguity when cloning is not attempted before sequencing . These findings are highly unlikely to represent amplicon contamination since marked sequence diversity was observed, and since only a single tick from each pool was positive in each reaction. Although only limited data can gleaned by this analysis, which interrogates only nucleic acids of small size, Casey et al. have shown that _msp2_ “similarity” groups, reflecting clusters determined by a similar sequencing approach, can be useful in predicting phylogenetic relationships, particularly with reference to adaptation to specific host niches .\n\n【18】Madeira, the main island of the Madeira Archipelago, is located in the North Atlantic Ocean, about 800 km west of the African continent and 1,000 km from the European coast. On this island, _I. ricinus_ is the most abundant tick species and the only _Ixodes_ tick that was found in this study. _A. phagocytophilum_ was detected in 4% of _I. ricinus_ collected in Paúl da Serra. Our results corroborate previous findings, although prevalence here is slightly lower than the 7.5% infection rate in ticks previously collected in similar areas. These differences may be attributable to seasonal variations in _A. phagocytophilum_ prevalence within reservoir hosts or ticks or to technical aspects of detection. Regardless, studies that use a greater number of samples and that are performed in different seasons, locations, and habitats will be needed to confirm the levels of infection. Nevertheless, these findings are generally similar to those described elsewhere in Europe, although prevalence rates can vary greatly with the origin of _I. ricinus_ examined, ranging from a minimum of < 1% in the United Kingdom, France, and Sweden  to a maximum of 24% to 29% in northern Italy, Germany, and Spain . The public health importance of these findings still remains to be determined. _I. ricinus_ is an exophilic, three-host tick known to bite several domestic animals and humans in Portugal . Therefore, we can assume that the presence of _A. phagocytophilum_ on Madeira Island _I. ricinus_ suggests a potential health threat to animals and humans and should be investigated.\n\n【19】Although _I. ricinus_ is not the main tick species in mainland Portugal, it can be found across the country in habitats with favorable conditions. Focused in Setúbal District, to the south of the Tejo River, our study detected _I. ricinus_ in all five sites chosen for field work: Barris; Baixa de Palmela; Pischeleiros; Azeitão, and Maçã. In those sites, the distribution of _I. ricinus_ was accompanied by another _Ixodes_ species, _I. ventalloi_ . Another ecologically interesting finding that should be further confirmed was that, although all of the _I. ricinus_ from mainland Portugal tested negative, evidence of _A. phagocytophilum_ was found in 2% of all _I. ventalloi,_ including 5% collected in Baixa de Palmela. The _msp2_ sequences identified in these two ticks were more closely related to each other than to any _msp2_ sequence identified in ticks from Madeira Island. In contrast, _A. phagocytophilum_ _msp2_ diversity in _I. ricinus_ from Madeira Island was broad and showed overlap with gene sequences identified in North American strains, as observed for some _A. phagocytophilum_ strains in the United Kingdom .\n\n【20】To our knowledge, this identification of _A. phagocytophilum_ in ticks is the first from mainland Portugal and the first documentation of _Anaplasma_ infection in _I. ventalloi_ . This species is an endophilic, three-host tick well adapted to a broad range of habitats that vary from open, dry forest in semidesert Mediterranean areas to the mild humid conditions in the southern part of the British Isles. In Portugal, _I. ventalloi_ infest a variety of small rodents, carnivores, and lizards but have not been found to feed on humans . _A. phagocytophilum_ has already been reported in other ticks, besides the known vector species . The presence in alternate ticks is attributable to the existence of secondary maintenance cycles, in which _A. phagocytophilum_ circulates between relatively host-specific, usually nonhuman-biting ticks and their hosts . Those additional cycles would buffer the agent from local extinction and help reestablish the primary cycles . Although this hypothesis might explain our results, the competency of _I. ventalloi_ to act as vector for _A. phagocytophilum_ has yet to be demonstrated. Moreover, the different average prevalences observed in each location suggest that _A. phagocytophilum_ is not widely spread in ticks and that some reservoir animals or hosts are needed for its maintenance. Trapping and animal surveillance are needed to provide more information that could help to explain the biological importance of those findings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4b78a062-05e8-4251-b6a6-e1b30fd54678", "title": "Infection and Autoimmunity", "text": "【0】Infection and Autoimmunity\nAs the editors imply in their introduction, the relationship of infection and autoimmunity is complex, compelling, and best viewed as a physiologic process and potential consequence of normal immune recognition and immunoregulation. The editors boldly state that reading the chapters in this book brings one to the conclusion that all autoimmune diseases are infectious, until proven otherwise (my paraphrase). Add environmental triggers to the mix, and most investigators would agree.\n\n【1】The book is divided into 3 broad sections: mechanisms of autoimmunity; specific infectious agents and their associated autoimmune diseases; and, conversely, specific autoimmune diseases and their associated infectious agents. The chapters in the mechanisms section focus on particular mechanisms, and with 1 exception, are scholarly and well done. However, this section lacks a review or balanced discussion of the various mechanisms of autoimmunity and proof of causation. Fortunately, the first article in the pathogen section by Denman and Rager-Zisman provides an excellent overview. As with any compendium (56 chapters by more than 100 authors), the quality varies, but all are written by investigators who have made substantial contributions to the field. The book is recommended for clinical investigators with some background in infectious disease or immunology as a starting point and ready resource for the current state of knowledge in the field.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "02bd7f3b-9811-4ffd-a13d-6be3d6a15ed6", "title": "Short-Term Malaria Reduction by Single-Dose Azithromycin during Mass Drug Administration for Trachoma, Tanzania", "text": "【0】Short-Term Malaria Reduction by Single-Dose Azithromycin during Mass Drug Administration for Trachoma, Tanzania\nMalaria can be treated or prevented with the broad-spectrum antimicrobial drugs tetracycline or azithromycin (AZT) . In vitro, AZT interferes with malarial parasite replication by targeting the unique apicoplast organelle of the parasite . AZT inhibits malarial parasite growth 10–fold every 48 hours, and the pharmacokinetics of AZT predict that it remains at concentrations high enough to limit parasite growth for >1 week . AZT might interfere with transmission by exoerythrocytic inhibition of parasite liver stages in humans and mice  and by interference with ookinete and sporozoite production in mosquitoes . Monotherapy with AZT is not typically used to treat malaria. However AZT is highly effective against _Chlamydia trachomatis_ , the causative agent of trachoma, which causes blindness . Persons with malaria who live in trachoma-endemic regions may undergo repeated AZT therapy as part of the World Health Organization–sponsored global trachoma eradication program .\n\n【1】Data regarding the effects of AZ mass drug administration (MDA) on malaria are limited. In a cluster randomized trial in The Gambia, AZ MDA given in 3 doses (20 mg/kg) 7 days apart reduced malaria rates by half when measured at 1 time point in children 5–14 years of age . More recently, a 49% reduction in the odds of death (95% CI 29%–90%) was reported for children 1–9 years of age in an AZ MDA treatment group compared with controls in Ethiopia . Porco et al. suggested that reductions in malaria prevalence associated with AZT MDA might have contributed to observed decreases in overall deaths.\n\n【2】For malaria treatment, a randomized clinical trial that compared AZT/artesunate with artemether/lumefantrine in Muheza, Tanzania, reported that the odds of treatment failure were 5 times greater (95% CI 3.3–11.4) in the group that received AZT/artesunate . The authors postulated that the AZT/artesunate showed treatment failure because MDA for trachoma in Tanzania could have led to localized _Plasmodium_ spp. resistance to AZT . _Plasmodium_ spp. drug resistance to AZT has not been documented in the field. However, in vitro selection for AZT resistance identified a G76V mutation among conserved active site amino acids at position 71–79 in _P. falciparum_ apicoplast-encoded ribosomal protein L4 ( _Pf_ RpL4) (PFC10\\_API0043) ( _1_ .) A Cochrane meta-analysis report stated that “azithromycin’s future for the treatment of malaria does not look promising”  and cited studies in which AZT, although well tolerated, was inferior to tetracycline for malaria prophylaxis .\n\n【3】In the current study, we evaluated malaria prevalence in a cohort of 2,053 children and adults in central Tanzania to examine the effect of single dose AZ MDA on prevalent malaria infections. We also searched for _Pf_ RpL4 mutations that might confer _P. falciparum_ AZT resistance.\n\n【4】### Methods\n\n【5】##### Study Site and Sampling\n\n【6】Study participants were from 8 rural agricultural villages in Dodoma Province, central Tanzania . The study period was January 12, 2009–July 21, 2009 and was coincidental with a period in which rainfall was <60% of the average amount . AZT was offered to all the residents of the treatment villages, including residents not part of the follow-up investigation. Four treatment villages were selected on the basis of trachoma prevalence >10% in children 1–9 years of age (trachoma prevalence 12%, 18%, 18%, and 14%), rather than by randomization , which was consistent with World Health Organization AZT MDA guidelines for trachoma control . Four control villages with lower trachoma rates (8%–10%) were chosen on the basis of geographic proximity.\n\n【7】A complete census was conducted in the villages> In each village, 130 families with children <5 years of age were randomly selected for follow-up prevalence blood sampling; 1 child and 1 adult were randomly selected from each family. Sample size estimations were based on prestudy estimates of a malaria prevalence of 10% in children <5 years of age in the Dodoma region . Follow-up fingerprick blood sampling was performed at baseline and at 1, 3, 4, and 6 months later. Children were divided into 3 subgroups that were sampled once at weeks 2 (group 1), 6 (group 2), and 8 (group 3). Contemporaneous to the monthly blood sampling, staff members from villages conducted an active surveillance program in which children were visited weekly and screened for axillary temperature and questionnaires were answered regarding fever, diarrhea, and respiratory disease .\n\n【8】Blood samples were collected on ProteinSaver903 (GE Healthcare, Pittsburgh, PA, USA) for analysis by real-time PCR for all participants at each scheduled monthly visit . When a fever (temperature ≥37.5°C) in adults and children was observed or when a caregiver reported a history of fevers in children during the weekly surveillance, a rapid diagnostic test (RDT) (Paracheck-Pf; Orchid Biomedical Systems, Goa, India) and thick and thin blood films were prepared. RDTs were performed by trained study staff and interpreted in the field at the time of sampling. Two experienced microscopists at the Amani Laboratory in Muheza, Tanzania, read the slides blinded to the PCR or RDT results. Discordant results were read by a third microscopist. Persons with positive results by RDT and all febrile children <5 years of age were treated with artemether/lumefantrine according to national guidelines and removed from later analysis.\n\n【9】Information on bednet ownership was obtained through standardized participant interviews. Latitude, longitude, and altitude of home locations were measured by using the GPSMAP 76 unit (Garmin, Olathe, KS, USA).\n\n【10】##### Quantitative PCR\n\n【11】DNA extraction and real-time PCR are described elsewhere . Five 3-mm diameter punches, equivalent to 25 μL of whole blood, were removed from the filter papers. DNA was extracted by using a commercial 96-well kit (Promega, Fitchburg, WI, USA). The DNA was concentrated by glycogen acetate and acetate/ethanol precipitation and low-speed (3,000 × _g_ ) centrifugation for 30 min.\n\n【12】Multiplex real-time PCR was used to amplify the 18S _P. falciparum_ ribosomal gene with a Cy5-labeled probe . Samples were processed in duplicate on manually loaded 384-well plates. A 40-cycle standard PCR protocol was used in a CFX 384 real-time PCR Detection System Thermocycler (BioRad, Hercules, CA, USA). Baseline relative fluorescence units (RFUs) were readjusted by using Bio-Rad CFX manager software. The real-time PCR system also detected _Borrelia_ spp. reported by Reller et al _._ . The sensitivity and specificity of the real-time PCR versus that of RDT or microscopy were reported by Schachterele et al. The real-time PCR detected 1–100 parasites/μL and showed the highest sensitivity in latent class analysis on febrile study patients. For the monthly surveys, microscopic analysis was not performed for the 8,711 samples for which real-time PCR was conducted. The RFU cutoff used was greater than that for >50 negative blood samples from control patients at Johns Hopkins University. The cutoff of 650 RFUs for the last cycle of the highest real-time PCR replicate had a specificity of 100% for samples from the control group at Johns Hopkins University and a specificity of 94% for samples from febrile patients in Tanzania; microscopy was used as a reference method .\n\n【13】##### Single-Nucleotide Polymorphism Analysis of _Pf_ RpL4 Gene for AZT Resistance\n\n【14】Blood samples from treatment and control villages that had higher parasite densities by real-time PCR and microscopy were subjected to PCR amplification and whole gene fragment cloning into the pCR2.1 plasmid. Amplification and cloning were followed by full _Pf_ RpL4 gene DNA sequencing of multiple bacterial clones for each patient and examination of mutations anywhere in this gene .\n\n【15】##### Statistical Analysis\n\n【16】Data were analyzed from the perspective of an intention to treat in which participants from AZ MDA–treated villages who refused treatment were classified with those who accepted AZT as members of treatment villages. Univariate ratios were used to compare prevalence proportions in treated versus untreated villages. CIs for ratios that compared proportions of prevalent infection in treated versus untreated villages were estimated by using 2-sided exact binomial tests.\n\n【17】Maps were used to display geospatial patterns in infection prevalence and malaria clustering over time (Google Maps, Mountain View, CA, USA, and Quantum Geographic Information System Open Source Geospatial Foundation Project . Geographic coordinates of _P. falciparum_ –positive and –negative blood samples were projected into kilometers, and locations were smoothed by using quadratic kernel intensity estimation. Kernel intensity estimators predict spatial intensity at unsampled points from nearby sampled points by using a quadratic function to heavily weight the nearest measured points and de-emphasize points sampled at greater distances. Weighted integrals were used to estimate intensity smoothing parameters for quadratic functions, assuming a 2-dimensional (i.e. spatial), stationary, isotropic point pattern process. Spatial odds ratios (ratio of kernel intensities from positive blood samples to kernel intensities from negative blood samples) , were determined for _P. falciparum_ .\n\n【18】To examine the effect of AZT MDA while adjusting for malaria clusters observed on maps, we used multivariate logistic models with random effects to compare odds of prevalent malaria infection for AZT MDA treatment villages with odds for control villages. Random intercepts were fit to account for the multilevel or hierarchical design that nested persons within villages. This model enabled accurate statistical inference by adjusting CIs and p values for _P. falciparum_ clusters in villages, and this inference was apparent in prevalence maps . Residual spatial autocorrelation was assessed by using variograms, and residual temporal autocorrelation was eliminated by restricting models to 1 sampling interval . Multiple models that examined the effect of AZT MDA on prevalent malaria were fit to control for confounding by other drivers of malaria prevalence and to address the robustness of standard errors to potential spatial residual autocorrelation. Altitude and self-reported bednet ownership were held constant to control for confounding in the multivariate model. A priori, we believed any AZT MDA effect would be most perceptible between month 1 and baseline.\n\n【19】To distinguish between the effect of AZT on prevalence and incident infections, we performed a sensitivity analysis for only _P. falciparum_ infections that had been preceded by a negative _P. falciparum_ test result in the previous sampling interval. These data also underwent a second sensitivity analysis that included participants treated with artemether/lumefantrine who had been removed from the previous analyses.\n\n【20】Data management and analysis were conducted by using R statistical software . R with the SPLANCS package was used for kernel intensity estimation , and the LME4 package was used for the random effects model . All R code is available upon request.\n\n【21】### Results\n\n【22】##### Characteristics of Study Population\n\n【23】The study site in the Kongwa District of Tanzania included 8 villages and 12,898 persons. Four villages were chosen on the basis of increased prevalence of trachoma (>10%) in children 1–9 years of age. Excluded were 34 of 66 villages that had received AZT MDA in the previous year. In the 4 villages that received AZT MDA, 6,252 persons received AZT and 642 did not receive AZT. A census in untreated control villages identified 5,991 persons. These villages were chosen because of geographic proximity to treatment villages. In each village, pairs of a parent and a child <5 years of age were randomly chosen for inclusion (1,045 persons in the AZT MDA treatment group and 1,008 persons in the control group for follow-up) . Blood samples for real-time PCR testing were collected from participants at baseline and at months 1, 3, 4, and 6 (4,437 patient-time samples from AZT MDA treatment villages and 4,274 patient-time samples from control villages). At any follow-up period after baseline, 1,010 (97%) study participants in AZT MDA villages who received AZT and 976 (97%) study controls were sampled for real-time PCR at least once. At baseline, treatment and control villages reported antimalarial drug use, latrine access, education (based on the highest level of education attained by the father), and home elevations. However, self-reported bednet ownership and fever histories were higher in treatment villages than in control villages (p<0.05) .\n\n【24】##### Univariate Analysis by Time\n\n【25】Overall, the proportion of _P. falciparum_ –infected participants was highest (6%) at baseline. Infections decreased sharply in treated villages and gradually in control villages throughout follow-up period . At the baseline evaluation, 6% (53/854) of participants from AZT MDA treatment villages were positive for _P. falciparum_ compared with 6% (54/894) of participants from AZT MDA control villages. No differences in odds of infection were observed between control and treatment villages (odds ratio 1.03, 95% CI 0.68–1.55) .\n\n【26】By month 1, _P. falciparum_ prevalence in AZT MDA treatment villages decreased to 2% (14/851) and _P. falciparum_ prevalence in AZT MDA control villages decreased to 5% (37/779). The odds ratio for AZ MDA treated villages compared with control villages was 0.34 (95% CI 0.17–0.64), which is consistent with a 66% reduction in the odds of _P. falciparum_ infection in AZT MDA treatment participants compared with AZT MDA control participants. Beyond month 1, the association between of AZT MDA and reduced prevalence of malaria infection decreased. However, overall rates of _P. falciparum_ infection decreased below levels for which reliable inferences could be made.\n\n【27】A village level comparison of changes in malaria prevalence for AZT MDA treatment villages versus prevalence in control villages indicated that 1 village in each group had no change in malaria prevalence, and 3 AZ MDA treatment villages had a significant (p<0.05, by Fisher exact test) decrease in malaria prevalence (p = 0.489, p = 0.0002, and p = 0.0005) and 2 in the control group had significant (p = 0.0008 and p = 0.001) increases. Village 5 in the control group had a significant (p = 0.0001) decrease .\n\n【28】In a subgroup of 200 children examined for malaria infection at weeks 2, 6, and 8, the malaria prevalence of 2%–5% was lower than the 10% expected  Thus, reliable prevalence comparisons were not relevant. During the study, febrile patients were tested by RDT and microscopy. During the critical period of 5–42 days after AZT MDA, 46 and 48 children in the treatment and control villages, respectively, were evaluated for fever. Among febrile children, only 5 children from treatment villages and 3 children from control villages were positive by real-time PCR.\n\n【29】##### Multivariate Analysis\n\n【30】At month 1, reductions in _P. falciparum_ malaria prevalence were observed by logistic regression models that controlled for differences in self-reported bednet ownership and home altitude  and included village level random effects. The odds of _P. falciparum_ infection were 63% (95% CI 28%–81%) but less in treated villages than in control villages after adjusting for bednet ownership and home altitude in a model with village-level random intercepts. Because self-reported history of fever, malaria medication, latrine access, and education of the head of the household were not associated with malaria prevalence and did not appreciably alter the interpretations of the multivariate models, they were not included in the reported estimates. As was evident in univariate analysis, no differences in malaria rates existed at baseline, and the association between AZT MDA and reduced malaria rates decreased after the first month of follow up.\n\n【31】Spatial information on home locations enabled a thorough examination of residual spatial autocorrelation. A total of 51% (19/37) of _P. falciparum_ infections at month 1 occurred in control village 8 , which raised concerns that residual spatial autocorrelation artificially decreased CIs from the reported models despite use of the random effects model. However, a variogram of standardized residuals from the model suggested that residual spatial autocorrelation was appropriately controlled by the village level random effects. The data were reanalyzed with incident infections that had been preceded by a negative real-time PCR result for _P. falciparum_ ; results did not change appreciably. We also observed an age-independent decrease in malaria prevalence at month 1 for persons 1–10 years of age (univariate analysis) and for persons >10 years of age (univariate analysis and multivariate analysis) .\n\n【32】##### Analysis of _Pf_ RpL4 Mutations Associated with Azithromycin Resistance\n\n【33】Sequencing of full-length _P. falciparum_ ribosomal L4 protein was performed for samples from 12 patients. We did not find evidence of single-nucleotide polymorphisms (SNPs) conferring AZT resistance in amino acid region 71–79 . A synonymous SNP at position K36 was found in all bacterial plasmid clones from samples of 1 patient and in a _P. falciparum_ –positive control patient who was not from Tanzania. A single plasmid clone from a sample of participant from an AZT MDA control village contained a nonsynonymous SNP in the active site of the _Pf_ RpL4 gene at A78S. However, 17 other plasmid clones from the same blood sample did not contain the mutation, which suggested the aberrant sequence resulted from a Taq polymerase error during plasmid cloning of the _Pf_ RpL4 gene. Moreover, restriction enzyme _Hin_ dIII, which was specific for the nonsynonymous SNP at A78S, did not digest multiple PCR amplicons from the original patient blood sample, which further suggested that the SNP on the _Pf_ RpL4 gene was present at a low concentration or may have been an artifact of the Taq polymerase amplification. A78S has not been identified in other mutant bacteria associated with AZT resistance . Eighteen other nonsynonymous mutations were found in samples from participants from AZT MDA treatment villages, but these mutations were not in conserved amino acid regions for _Pf_ RpL4 associated with AZT resistance in bacteria or the selected _P. falciparum_ (except for an I39S were single occurrences among multiple clones from a single patient). Four synonymous SNPs were also detected .\n\n【34】### Discussion\n\n【35】AZT MDA was associated with a minimum reduction of 66% in odds of _P. falciparum_ infection compared with odds of reduction for controls in the first month after drug administration, as shown by univariate and multivariate models on an individual level. Beyond 1 month, prevalence in treatment and control groups decreased, and there was no difference in prevalence between treatment and control groups. In addition, no mutations were detected in samples from treated persons who had parasitemias after drug treatment in the only gene associated with in vitro selection of AZT resistance. However, these results show more modest effects than those of other studies of AZT MDA, which reported large reductions in malaria  and that AZT MDA protection against malaria contributed to broader decreases in illness and death .\n\n【36】Because our data were analyzed from the perspective of an intention to treat, participants who did not receive AZT but lived in villages that received AZT MDA were classified with those who received AZT. The intention-to-treat analysis could have biased reported results toward the null because persons in AZT MDA treatment villages who benefited from AZT might have been more likely to become infected . Because intention-to-treat analysis would probably bias results toward the null, we believe that it was a conservative analytic assumption.\n\n【37】An additional potential confounder is that all febrile children <5 years of age and adults who were positive for _P. falciparum_ by RDT were treated according to national and Integrated Management of Childhood Illness guidelines with artemether/lumefantrine, which has a lingering 4-week prophylactic effect on parasitemia. However, inclusion of artemether/lumefantrine–treated participants did not affect the relevant proportions of malaria infection at month 1 .\n\n【38】AZT treatment occurred at the village level, which might have exacerbated similarities in malaria risk among participants in the same village and created village-level clustering . Clustering within villages can bias results if it is not addressed in the statistical models . Village-level random effects were used to adjust variance estimates for within-village clustering and provide valid multilevel statistical inferences, given the clustering inherent in the study design . We argue that a relevant comparison is between the proportion of AZT MDA–treated and control villages, rather than a grouping of 4 villages in each AZT MDA intervention and control villages. Grouping findings into only 8 units on a village level does not produce relevant results and was not the design of the study.\n\n【39】AZT MDA was conducted occurred according to the WHO SAFE (surgery, antibiotics, facial cleanliness, and environmental changes) trachoma control strategy, and assigning treatment to villages by randomization would have been unethical because of the established effectiveness of this strategy in the study region. AZT MDA was examined from an observational perspective with the longitudinal cohort. We sought to minimize lack of comparability by selecting villages from a similar geographic region and by controlling for bednet ownership and altitude in multivariate analysis. We believe that residual confounding and bias are minimal because similar _P. falciparum_ infection prevalence rates were observed at baseline and at months 3–6 after the effect of the AZT MDA had decreased.\n\n【40】In Ethiopia, Porco et al _._ reported a 49% (95% CI 10%–71%) decrease in odds of death in a randomized trial of AZ MDA villages compared with control villages that received a single dose of AZ (20 mg/kg to ≤1 g) . AZT protection from _P. falciparum_ infection might have contributed to the overall decrease in deaths. Although the effect of AZT MDA on prevalent infections was strong, most infections observed were asymptomatic. The study in Ethiopia also had 3 dosing strategies (annual, biannual, and quarterly), which resulted in two thirds of study participants receiving more annual doses than used in the study in Tanzania. Our data provide only weak evidence that a reduction in malaria infection contributed to the reduction in deaths reported in Ethiopia.\n\n【41】At the study site, we did not find evidence of _P. falciparum_ AZT resistance markers on a gene previously implicated in AZT resistance in vitro. We also observed a short-lived but major reduction in malaria. AZT MDA does not produce _Plasmodium_ spp. drug resistance probably because other drugs with different mechanisms of action are used to treat malaria. This study might provide an optimistic note for AZ MDA planners and those living in areas in developing nations to which malaria and trachoma are endemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d859e0f4-6d29-4eab-883d-60e88caf1edb", "title": "Molecular Typing of Multidrug-Resistant Salmonella Blockley Outbreak Isolates from Greece", "text": "【0】Molecular Typing of Multidrug-Resistant Salmonella Blockley Outbreak Isolates from Greece\n_Salmonella_ Blockley is rarely isolated in the Western Hemisphere. According to Enter-net, the international network for surveillance of _Salmonella_ and verocytotoxin-producing _Escherichia coli_ infections, _S._ Blockley represented 0.6% of all _Salmonella_ serotypes isolated in Europe during the first quarter of 1998, a full 100-fold lower than the dominant serotype, _S._ Enteritidis (67.1%)  . However, _S._ Blockley is among the five most frequently isolated serotypes from both avian and human sources in Japan , Malaysia , and Thailand  . A single foodborne outbreak in the United States  and sporadic human infections in Europe associated with travel to the Far East  , animal infection  or carriage , and environmental isolates have also been reported .\n\n【1】Regardless of the frequency of _S._ Blockley isolation, its rates of resistance to antibiotics have been high. Among Spanish salmonellae isolated from natural water reservoirs, _S._ Blockley and _S._ Typhimurium had the highest rates of multidrug resistance  . Comparing 1980-1989 with 1990-1994, researchers from Tokyo noted an increase in the number of _S._ Blockley isolates resistant to one or more antibiotics, from 92.0% to 98.2% for imported cases and from 57.4% to 88.7% for domestic cases . In Thailand, isolates from human or other sources also had high rates of resistance to streptomycin, tetracycline, kanamycin, and chloramphenicol and lower rates to ampicillin and trimethoprim/sulfamethoxazole  .\n\n【2】Nevertheless, few attempts at typing _S._ Blockley isolates with molecular methods have been described, and these have been limited to the characterization of plasmid content .\n\n【3】During the second and third quarters of 1998, Enter-net reported higher numbers of _S._ Blockley isolates than during the same period of the previous year in several European countries  . The epidemiologic investigations conducted in Germany, England and Wales, and Greece did not confirm a source for this increase .\n\n【4】In this study, we characterized the Greek outbreak isolates further, both with respect to their antibiotic resistance phenotypes and DNA fingerprints obtained by pulsed-field gel electrophoresis of genomic DNA.\n\n【5】### The Study\n\n【6】The study sample consisted of 28 of 35 _S._ Blockley strains isolated from May to December 1998  , one strain from February 1999, and four epidemiologically unrelated control strains: one from 1996 and three from 1997. All isolates were from human cases of enteritis. Identification was performed by the API 20E system (BioMerieux S.A. Marcy l'Etoile, France) and serotyping with commercially obtained antisera (BioMerieux)  .\n\n【7】Susceptibility to kanamycin, streptomycin, ampicillin, amoxicillin/clavulanic acid, cefepime, tetracycline, chloramphenicol, trimethoprim/sulfamethoxazole, gentamicin, nalidixic acid, and ciprofloxacin was tested by a disk diffusion assay according to National Committee for Clinical Laboratory Standards guidelines  . Genomic DNA was prepared and digested with _Xba_ I (New England Biolabs)  . Chi-square tests or Fisher exact tests were used to calculate two-tailed probabilities.\n\n【8】_S._ Blockley accounted for seven of the 13,199 salmonella isolates identified in Greece from 1976 to 1997. However, 35 gastroenteritis cases due to this serotype were reported from May to December 1998  . Twenty-nine _S._ Blockley strains isolated from fecal specimens of patients with gastroenteritis during May 1998 to February 1999, along with four epidemiologically unrelated clinical isolates from 1996 and 1997, were therefore studied for susceptibility to antibiotics. The 1998 outbreak isolates were scattered throughout Greece; _S._ Blockley was isolated later, starting in August 1998, in northern Greece.\n\n【9】All isolates were susceptible to trimethoprim/sulfamethoxazole, ampicillin, amoxicillin/clavulanic acid, gentamicin, and ciprofloxacin . High resistance rates were observed to tetracycline (100%), streptomycin and kanamycin (90%), chloramphenicol (83%), and nalidixic acid (52%). Six resistance phenotypes could be distinguished  with the two major phenotypes of outbreak isolates being resistant to kanamycin, streptomycin, tetracycline, and chloramphenicol (ATC) or kanamycin, streptomycin, tetracycline, chloramphenicol, and nalidixic acid (ATCN). Most (76%) strains isolated after August 24, 1998, were nalidixic acid-resistant (resistance phenotypes ATCN, TCN, ATN), unlike strains isolated up to August 17, 1998 (17%) (1.29 <RR = 3.03 <7.11, p = 0.005).\n\n【10】When pulsed-field gel electrophoresis was used to obtain DNA fingerprints for these isolates , all belonged to the same type, A, although eight subtypes, A1-A8, could be distinguished on the basis of one to three DNA fragment differences . Two of the four isolates from previous years belonged to unique subtypes A6 and A7; the other two belonged to subtypes A2 and A8, shared by outbreak isolates . In contrast, 93% of the 1998 outbreak strains yielded PFGE patterns common to two or more isolates. Indeed, most outbreak isolates were grouped in subtypes A2 and A4, consisting of 11 and 12 isolates, respectively .\n\n【11】Pulsed-field gel electrophoresis subtypes were associated with resistance phenotypes. Most resistance phenotype ATC isolates belonged to subtype A2 (3.03. <RR = 11.86 <46.5, p = 0.0000098), while most resistance phenotype ATCN isolates belonged to A4 (2.76 <RR = 7.11 <18.30, p = 0.0000416). In addition, most isolates in the two major subtypes appearing before August 17, 1998, belonged to the A2 group, while most isolates appearing after August 24, 1998, belonged to the A4 group (1.47 <RR = 9.82 <65.45, p = 0.0006). Finally, unlike A4, the earlier A2 subtype was not isolated in northern Greece.\n\n【12】### Conclusions\n\n【13】Our results indicate that PFGE is useful in distinguishing epidemiologically related _S._ Blockley isolates since two of the four nonoutbreak isolates displayed unique PFGE patterns, A7 and A8, while PFGE patterns A2 and A4 grouped most of the 29 outbreak isolates ( 11 and 12 , respectively).\n\n【14】These two chromosomal fingerprints, differing by two DNA fragments, were associated with two distinct resistance phenotypes. The resistance phenotype of A4 isolates, ATCN, was identical to the earlier resistance phenotype of A2 isolates, ATC, except for the resistance to nalidixic acid. Nevertheless, these two PFGE/antibiotic resistance types, A2/ATC and A4/ATCN, displayed a clear distribution both in time and space.\n\n【15】The data may, therefore, indicate two main sources for the outbreak. Alternatively, and perhaps more likely, these two closely related types may together constitute the outbreak clone, evolved with time to acquire resistance to nalidixic acid. Resistance may well have originated in the food source, since several antibiotic classes are used as feed supplements in animal rearing and aquaculture in Greece: sulfonamides (trimethoprim/sulfathiazine), tetracyclines (oxytetracycline), and quinolones (oxolinic acid). However, as in other European countries , the epidemiologic investigation did not locate a common source to account for the wide geographic spread of cases  . Although travel was not mentioned in the Greek patients' questionnaire responses, the possibility that the source was an imported food cannot be ruled out. The association with smoked eel of Italian origin in the German outbreak has not been microbiologically confirmed  . The only other previous European report of a human outbreak attributed to _S._ Blockley, probably from vegetables contaminated by this organism, which was prevalent in irrigation water in the Spanish region of Granada, is anecdotal  . A documented _S._ Blockley enteritis epidemic in a U.S. hospital in 1966 was attributed to contaminated ice cream; however, this was also not microbiologically confirmed  .\n\n【16】While this serotype may remain important in Europe, its high rates of resistance to kanamycin, streptomycin, tetracycline, and chloramphenicol, which were in agreement with studies from the Far East  and Spain  , are cause for concern. Unlike the Far Eastern strains, no resistance to ß-lactam antibiotics or cotrimoxazole was observed in our study. The two dominant resistant phenotypes of _S._ Blockley from natural polluted waters in Spain were sulfonamides, streptomycin, and tetracycline; and neomycin, streptomycin, kanamycin, tetracycline, and chloramphenicol  , as in the Greek strains, except for the absence of resistance to nalidixic acid.\n\n【17】In agreement with differences in animal reservoirs and transmission routes and therefore the mechanism of resistance acquisition among different _Salmonella_ serotypes, the main patterns of resistance observed in _S._ Blockley were distinct from those predominating in the two major serotypes from isolates of both human and animal food origin in Greece. In _S._ Enteritidis, the most frequent resistance phenotype was resistance to ampicillin  , while in _S._ Typhimurium, the most frequent resistant phenotype was resistance to sulfonamides and streptomycin (A. Markogiannakis, P.T. Tassios, N.J. Legakis, unpub. obs.). Furthermore, the considerably high rate of resistance to nalidixic acid is equally unprecedented in both the Far Eastern and Spanish _S._ Blockley isolates and in other salmonella serotypes from Greece. Since resistance to nalidixic acid can be a precursor of resistance to fluoroquinolones, one of the two drug classes of choice for invasive salmonella disease, this feature of these _S._ Blockley strains is particularly disturbing. _S._ Blockley, previously a prevalent serotype in the Far East but rare elsewhere, nevertheless posed a public health problem in several European countries. The source of the European outbreaks, however, remains unclear. Given the increased international commerce in food, a collaborative study would be useful in identifying potential similarities between the recent European strains and established strains from the Far East.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5b0c65dd-4708-4f90-8aa2-74fe1658bbc6", "title": "Characterization and Source Investigation of Multidrug-Resistant Salmonella Anatum from a Sustained Outbreak, Taiwan", "text": "【0】Characterization and Source Investigation of Multidrug-Resistant Salmonella Anatum from a Sustained Outbreak, Taiwan\nNontyphoidal _Salmonella_ (NTS) is a major cause for foodborne diseases worldwide. In Taiwan, the ambient climate and flourishing pig-raising industry makes NTS infections rampant. As in other countries, salmonellosis was primarily caused by _Salmonella enterica_ serovars Enteritidis and Typhimurium in Taiwan , but rare serovars such as _Salmonella_ Goldcoast have appeared in recent years . Recommended antimicrobial treatment options for salmonellosis include fluoroquinolones and extended-spectrum cephalosporins . However, resistance to these antibiotics has been emerging in many countries, leading to increased disease prevalence, disease severity, and death and the requirement of last-line antimicrobial drugs (e.g. carbapenems) .\n\n【1】Since 2015, northern Taiwan has seen an increase in _Salmonella_ infections, caused by previously rare _Salmonella_ Anatum. The infections were also reported in central Taiwan, indicating that this outbreak had already prevailed throughout the entire island . Co-resistance to ceftriaxone and ciprofloxacin are the main feature of the outbreak clone. Evidence from epidemiologic, laboratory, and supply-chain investigations identified raw pork and poultry as the vehicle for spread of this strain. More important, genomic comparisons against the global public database indicated that this clone has appeared in Europe, Asia, and America. Given the increasing globalization of foodstuffs, these findings prompt an urgent global sharing of whole-genome sequencing (WGS) data to facilitate disease surveillance and early recognition of international foodborne outbreaks .\n\n【2】### The Study\n\n【3】Chang Gung Memorial Hospital is a main referral hospital for cities in northern Taiwan, including Taipei, New Taipei, and Taoyuan. The population in this region is »7 million. In 2012, the hospital’s clinical microbiology laboratory launched a program to monitor the NTS serovars causing human infections. All _Salmonella_ isolates from patients were collected and serotyped. Before 2015, very few _Salmonella_ Anatum isolates were recovered, and most were susceptible to antimicrobial agents. Since then, an increase has been observed, peaking in 2017 . As of June 2019, a total of 319 nonrepetitive isolates have been identified; of these, 197 (61.8%) isolates were ceftriaxone-resistant (MIC \\> 2 μg/mL), 301 (94.4%) were ciprofloxacin-resistant (MIC \\> 0.12 μg/mL), and 197 (61.8%) were resistant to both. In addition, 292 (91.5%) isolates were resistant to chloramphenic, and 295 (92.5%) were resistant to trimethoprim/sulfamethoxazole. A positive correlation was found between higher temperatures and the infections ( _r_ \\= 0.4; p<0.05) ; however, no notable effects on _Salmonella_ Anatum infections have been associated with precipitation or humidity ( _r_ <0.3; p>0.05).\n\n【4】Detailed methods are described in the Appendix . We first reviewed the clinical and laboratory characteristics of 278 patients from 2015–2018. Most patients had acute gastroenteritis, whereas a few (14/278, 5%) had invasive diseases, such as bacteremia and sepsis. In terms of age distribution, the highest number of cases were in young children . Pediatric patients (n = 169) had significantly higher rates than adult patients (n = 109) for hospitalization (79.2% vs. 55.0%; p<0.05), diarrhea (89.9% vs. 68.8%; p<0.05), and fever (89.2% vs. 58.1%; p<0.05).\n\n【5】Multilocus sequence typing indicated that the entire collection of clinical _Salmonella_ Anatum isolates belonged to sequence type 64. We randomly selected 54 clinical isolates for WGS . Both core genome multilocus sequence typing and whole-genome single-nucleotide polymorphism analyses, performed by using the BacWGSTdb database , further divided these isolates into 3 clades . Clades I and II were more closely related to each other; their most recent common ancestor occurred >21 years ago. Clade III was more distantly connected to these 2 clades. Typing based on PCR assay was performed on the unsequenced isolates. Clade I accounted for 95.6% (305/319) of all isolates, suggesting it was the cause of the outbreak. The isolates resistant to ceftriaxone, ciprofloxacin, or both clustered within clade I, whereas the isolates of clades II and III were more susceptible. Most of the clade I isolates harbored a 90-kb IncA/C plasmid carrying _bla_ DHA-1 (encoding a class C β-lactamase) and _qnrB_ (conferring resistance to quinolones). A conjugation assay demonstrated that this plasmid conferred ceftriaxone and ciprofloxacin resistance. In addition, 31 (9.7%) clinical isolates carried _bla_ CMY-2  , which was located within a >100-kb IncI1 plasmid and also encoded a class C β-lactamase. These 31 isolates carried _bla_ DHA-1  simultaneously. In 11 of them, the _bla_ DHA-1  –carrying and _bla_ CMY-2  –carrying plasmids were fused into 1 large plasmid .\n\n【6】By comparing these findings against sequences in GenBank, we found nearly identical genomic sequences for isolates in the United Kingdom, the United States, and the Philippines. The collection time for these isolates also occurred during 2015–2019, which nearly coincided with the outbreak in Taiwan. These international _Salmonella_ Anatum isolates also carried the 90-kb IncA/C plasmid ; therefore, they were likely ceftriaxone- and ciprofloxacin-resistant concomitantly. The only distinction of these international isolates was their lack of the _bla_ CMY-2  –carrying plasmid. Accordingly, we speculated that the _Salmonella_ Anatum clone had arrived in Taiwan through food trade and later acquired the _bla_ CMY-2  –carrying plasmid.\n\n【7】To trace the source of _Salmonella_ Anatum, we investigated food samples from supermarkets and traditional markets of 8 districts with high density of _Salmonella_ patients in New Taipei City and Taoyuan City, Taiwan . A total of 11 _Salmonella_ Anatum isolates were collected from pork, 4 from poultry, and 1 from beef in these regions . WGS showed that they all belonged to clades I and II, providing strong evidence that raw meats were the outbreak vehicle. All 16 isolates harbored the _bla_ DHA-1  –carrying IncA/C plasmid. Other _Salmonella_ serovars also were detected in this investigation. The overall _Salmonella_ isolation rate from retail meats was significantly higher in traditional markets than in the supermarkets (p<0.001) . In Taiwan, pork in the supermarkets is usually provided through the cold transportation chain, whereas for traditional markets pork is usually provided through the traditional chain, with notable differences. Temperatures were much lower in the cutting factory and butcher shop in the cold chain than in the traditional chain . Furthermore, pork was wrapped by plastic tissue and bags in the cold chain, but the traditional chain did not do any wrapping or packaging during transportation.\n\n【8】To clarify the contradictory findings that most infections occurred in young children even though pork is not a major food for infants, we conducted a questionnaire survey among parents of 20 infants (<1 year of age) with NTS infections and 80 parents of infants without (controls) . Parents of the infected infants more often touched, rinsed, and cooked meat before feeding other foods to their infants . Moreover, these parents were more willing to purchase meat from traditional markets rather than supermarkets. A possibility is that they bought meat from the traditional markets, then their frequent rinsing flushed the _Salmonella_ on the surface of the meats, cutting boards and knives, and sinks, and finally onto fresh vegetables, fruit, and other ready-to-eat foods that were cross-contaminated and reached the infants through parents or other caregivers. This transmission mode is of particular importance in infants and has already been reported for other bacterial pathogens such as _Yersinia enterocolitica_ .\n\n【9】### Conclusions\n\n【10】Our study sought to describe an outbreak in Taiwan caused by a multidrug-resistant _Salmonella_ Anatum clone. The questionnaire and supply-chain investigations we conducted found that the infection cases were closely associated with improper packaging during transportation and unhygienic food handling in the customers’ kitchen. The high similarity of genomic sequence between the Taiwan isolates and international isolates indicates the global dissemination of this clone and highlights the public health value of multicountry sharing of epidemiologic, trace-back, microbiologic, genomic, and food trade data.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2c4be408-95e2-479a-8221-7bbc43701440", "title": "Sand Fly–Associated Phlebovirus with Evidence of Neutralizing Antibodies in Humans, Kenya", "text": "【0】Sand Fly–Associated Phlebovirus with Evidence of Neutralizing Antibodies in Humans, Kenya\nDisease outbreaks caused by Zika, dengue, yellow fever, chikungunya, and Rift Valley fever viruses illustrate the threat posed by arthropod-borne viruses (arboviruses), which affect millions of patients worldwide each year . Major epidemic arboviruses are thought to have originated in tropical Africa, where they are known or thought to have caused local outbreaks before epidemic or pandemic spread. Early recognition of local outbreaks, including precise identification of the disease-causing agent, is key to effective preparedness against epidemics. However, active surveillance is poorly implemented in most seeding countries.\n\n【1】Monitoring programs for vectors of arboviral diseases often prioritize mosquitoes and ticks , neglecting other blood-feeding vectors, such as sand flies. Sand flies transmit protozoan parasites that cause leishmaniasis as well as phleboviruses (order _Bunyavirales_ , family _Phenuiviridae_ , genus _Phlebovirus_ ) . Sand fly–borne phlebovirus infections are known to occur in the Mediterranean basin . Some of these, such as sandfly fever Naples, Sicilian, and Toscana viruses, are of public health importance, especially Toscana virus, which causes meningitis and encephalitis in humans .\n\n【2】The International Committee for the Taxonomy of Viruses (ICTV) currently recognizes 10 viral species within the genus _Phlebovirus_ : Rift Valley fever virus (RVFV), severe fever with thrombocytopenia syndrome (SFTS) virus, Uukuniemi phlebovirus, Bujaru phlebovirus, Candiru phlebovirus, Chilibre phlebovirus, Frijoles phlebovirus, Punta Toro phlebovirus, Salehabad phlebovirus, and sandfly fever Naples phlebovirus . The last 7 species are vectored by sand flies. Novel unclassified phleboviruses (e.g. Fermo, Granada, Punique, and Massilia viruses), as well as recently discovered flaviviruses in sand flies, underline the importance of these neglected vectors of arboviral diseases .\n\n【3】Sand fly–borne phleboviruses have been studied mainly in the Mediterranean region. Prevalence data for sub-Saharan Africa remain scarce. Studies have found serologic evidence of human infections with Karimabad, sandfly fever Naples, and sandfly fever Sicilian viruses in patients from Sudan with febrile illness . Human infections with sandfly fever Naples and Sicilian viruses have been reported in Uganda, Somalia, Djibouti, and Ethiopia . However, serologic surveys may be compounded by antigenic cross-reactivity between phleboviruses (except in neutralization assays), thereby precluding the unequivocal identification of the circulating sand fly–borne virus . Few studies have incorporated viral genetic characterization.\n\n【4】We describe a previously unknown phlebovirus discovered during vector surveillance in Kenya, which we designate Ntepes virus (NPV), after the place of sampling of virus-infected sand flies. The complete genome of NPV was sequenced, viral species tropism in cell culture assessed, and pathogenicity in vertebrates proven by infection of mice. Human serum samples from Ntepes and other communities yielded evidence of human infection based on specific virus neutralization.\n\n【5】### Methods\n\n【6】##### Sandfly Trapping and Virus Isolation\n\n【7】We trapped sandflies using CDC light traps  in villages of the Marigat district, Baringo County, Kenya, in February 2014 . We homogenized pools of 5–50 female specimens in minimum essential medium (MEM), inoculated an aliquot of the clarified supernatant (50 μL) into Vero cells, and incubated it for up to 14 days with daily monitoring for cytopathic effect (CPE). We passaged CPE-positive supernatant onto fresh Vero E6/7 cells; virus stock solution was generated from the first passage and used for all further experiments. We determined the infectious titer by 50% tissue culture infectious dose (TCID 50  ) assay using 10-fold serial dilutions from –  10 –1  to 10 –12  of the virus stock inoculated in 5 wells each of subconfluent Vero cells seeded in a 96-well plate. We calculated the virus titer according to Reed and Muench . We estimated the minimum infection rate (MIR) in sandflies using the formula \\[number of positive pools/total specimens tested\\] × 1,000.\n\n【8】##### Broad Reverse Transcription PCR Screening\n\n【9】We extracted RNA from cytopathic cell culture supernatant (200 μL aliquot) using the Viral RNA Mini Kit  and eluted it in 50 μL of buffer. We performed cDNA synthesis using SuperScript III reverse transcription  and random hexamer primers , followed by reverse transcription PCR (RT-PCR) for orthobunyaviruses, alphaviruses, and flaviviruses . We screened for sand fly–borne phleboviruses by targeting the RNA-dependent RNA polymerase (RdRp) gene using degenerate primers . We compared the nucleotide sequences to GenBank using blastn  and subjected them to initial phylogenetic analysis in MEGA6 . We further extracted RNA from pooled sand flies and tested it by real-time PCR using NPV-specific primers (forward, 5′-GCAAGAAAGCACTGTGGTGG; reverse, 5′-CGTATGATGATCGGCCACCA; probe, 5′-6-FAM-ACAGCCACCTCTGATGATGC-IBFQ).\n\n【10】##### Genotyping of Sand Flies and Blood Meal Analysis\n\n【11】We amplified the barcode region of the cytochrome _c_ oxidase subunit I ( _COI_ ) gene using published primers . We extracted genomic DNA from individual bloodfed sand fly specimens using the QIAGEN DNeasy Blood and Tissue Kit (QIAGEN). We amplified a 500-bp fragment of the 12S mitochondrial rRNA gene as described , sequenced the PCR products, and compared them to GenBank database data. We inferred species-level identification on the basis of ≥98% identity spanning \\> 300 bp, as described by Valinsky et al.\n\n【12】##### Next-Generation Sequencing, Genome Annotation, and Phylogenetics\n\n【13】We purified and concentrated virions from the supernatant of infected Vero cells by ultracentrifugation through a 36% sucrose cushion. We extracted viral RNA using the QIAGEN RNeasy Kit according to the manufacturer’s instructions. We generated cDNA using the Maxima H Minus Double-Stranded cDNA Synthesis Kit and random hexamer primers . We prepared DNA libraries using the Nextera XT DNA Sample Preparation Kit and analyzed them on an Illumina MiSeq instrument with the MiSeq Reagent Kit v3 . We identified viral reads by reference mapping to phleboviruses as well as by BLAST comparisons against a local amino acid sequence library containing translations of open reading frames (ORFs) of phleboviruses. We closed sequence gaps by conventional RT-PCR followed by Sanger sequencing. We performed genome assembly using Geneious  and confirmed genome terminal sequences by rapid amplification of cDNA ends . We identified ORFs using Geneious, compared nucleotide and amino acid sequences with other sequences by blastn and blastx searches against the GenBank database, and identified protein motifs by web-based comparison to the Pfam database . We identified putative transmembrane regions by prediction of the hydropathy profile using TMHMM  and predicted N-linked glycosylation sites using the NetNGlyc 1.0 server .\n\n【14】We aligned nucleotide and amino acid sequences of the ORFs of the respective genome segments with related viral sequences in Geneious using MAFFT . Phylogenetic trees were inferred by the maximum-likelihood (ML) method using the best suitable substitution matrix (LG) identified by Modeltest, as implemented in MEGA. We performed confidence testing based on 1,000 bootstrap iterations .\n\n【15】##### In Vitro Viral Growth Kinetics\n\n【16】We infected cell lines from insects (LL-5, sand fly; C6/36, mosquito), humans (HEK293-T), small mammals (BHK-21, hamster; VeroE6/7, primate; MEF, mouse; EidNi, bat) and livestock (PK-15, swine; ZN-R, goat; DF-1, chicken; KN-R, cattle) in doublets, at a multiplicity of infection of 0.1. We harvested aliquots of infectious cell culture supernatants every 24 h for periods of 7 d and quantified viral genome copies by real-time RT-PCR with plasmid-based quantification standards.\n\n【17】##### In Vivo Pathogenesis in Suckling Mice\n\n【18】We intracerebrally inoculated 100 μL of the viral stock of the first passage, as well as 3 consecutive 2-fold dilutions, into 3–4-day-old Swiss Albino suckling mice. The doses used in the experimental infection were quantified by plaque assays in Vero cells as described previously  and corresponded to viral titers of 4 × 10 6  , 2 × 10 6  , 1 × 10 6  , 5 × 10 5  , and 2.5 × 10 5  PFU/mL. We included noninfectious MEM as a negative control. We observed all mice 2 times/day for up to 14 days for signs of disease. We homogenized brains from recently dead mice in 1 mL of cell culture media and plaque-titrated them on Vero cells.\n\n【19】##### Human Serum Samples and Neutralization Tests\n\n【20】Archived serum samples from the Marigat district hospital, taken during 2010–2011, and from Sangailu Health Centre in the Hulugho subcounty in northeastern Kenya, collected during 2010–2012, were available . We performed a virus neutralization test using 2-fold serial dilutions of serum samples (1:20 to 1:640). We mixed 50 μL of the serial serum dilutions with 70 TCID 50  of NPV. Mixtures were incubated at 37°C in the presence of 5% CO 2  for 1 h, then used for infection of a confluent Vero E6/7 cell monolayer seeded in 96-well culture plates with 2 wells/dilution. After 7 days of incubation, we recorded the highest serum dilution at which no CPE was observed in at least 50% of the wells as the neutralization titer.\n\n【21】We tested NPV-reactive human serum samples with RVFV, Gabek Forest virus (GFV), and Karimabad virus (KARV) for serologic cross-reactivity, as described earlier in this article. In addition, we tested GFV- and KARV-positive serum samples with NPV in 2-fold serial dilutions from 1:5 to 1:20 .\n\n【22】##### Ethics Considerations\n\n【23】Approval for the study was granted by the Scientific and Ethical Review Unit and Animal Care and Use Committee of the Kenya Medical Research Institute (SSC Protocol nos. 1560 and KEMRI/SERU/CVR/003/3312). All animal experiments were carried out in accordance with the regulations and guidelines of the Kenya Medical Research Institute.\n\n【24】##### GenBank Accession Numbers\n\n【25】The NPV genome was deposited in GenBank under accession nos. MF695810–MF695812. The _COI_ sequence obtained from the virus-positive sand fly pool was deposited in GenBank under accession no. MG913288.\n\n【26】### Results\n\n【27】##### NPV Isolation and Characterization\n\n【28】In total, 6,434 sand flies were trapped . A subset of 5,481 sandflies was pooled and the resulting 111 pools individually inoculated in VeroE6/7 cells. One pool consisting of 8 females induced CPE 4–5 days postinfection. Sequence analysis of the _COI_ gene of the sand flies of this CPE-positive pool suggested that sand flies were of the genus _Sergentomyia_ . We identified blood-meal hosts for 62 blood-fed specimens sampled at the same place and time as the pooled specimens. Results revealed that 56 (90.3%) had fed on humans, 2 (3.2%) on snakes, and 1 (1.6%) each on a frog, lizard, cow, and ostrich. The infectious cell culture supernatant tested negative for RVFV, orthobunyaviruses, alphaviruses, and members of genus _Flavivirus_ . We amplified a 0.5-kb fragment of the RdRp gene of sand fly–borne phleboviruses using degenerated primers . The sequence showed the highest pairwise identity of 79% to GFV and 75% to KARV. We sampled a subset of 953 individual sand fly samples 2 years after the initial study and tested it in pools of 10 by specific RT-PCR for the cultured virus; results were negative.\n\n【29】Analysis of the complete genome by next-generation sequencing confirmed isolation of a novel phlebovirus. The virus was tentatively termed Ntepes virus, after the location where the sand flies were collected. The virus exhibits the characteristic tripartite-segmented genome organization of phleboviruses, comprising the large (L) segment, which encodes the RdRp protein; the medium (M) segment, encoding a glycoprotein precursor protein (GPC) that is posttranslationally cleaved into 2 viral surface glycoproteins (Gn and Gc) and a nonstructural protein (NSm); and the small (S) segment, encoding the nucleocapsid (N) protein and a nonstructural protein (NS) in an ambisense manner . Highest sequence similarities to GFV were 93% to RdRp, 88% to GPC, 79% to Nsm, 85% to N, and 90% to NS. NPV has the typical conserved genome termini shared among phleboviruses (5′-ACACAAAG and CUUUGUGU-3′) .\n\n【30】Phylogenetic analyses of NPV RdRp, Gn, Gc, and N proteins and all available sand fly–borne phlebovirus sequences indicate that NPV forms a strongly supported clade with GFV and KARV. NPV branches as a sister taxon to GFV in all genes, suggesting NPV to be a member of the Karimabad species complex . However, the designation of the Karimabad species complex is not yet officially approved by the ICTV. For a provisional genetic classification, we analyzed the intragenetic distances among established phlebovirus species and unclassified isolates based on the RdRp gene. Pairwise nucleotide and amino acid distances between established species ranged from 38% to 62% for nucleotide distances and 39% to 68% for amino acid distances . For example, amino acid distance between Punta Toro virus and Candiru virus was 39% and between SFTS virus and sandfly fever Naples virus was 68%. Pairwise nucleotide distances ranged from 20% to 59% and amino acid distances from 6% to 69% when unclassified tentative species and variants pertaining to established species were included. For example, amino acid distance between Ponticelli virus and Adana virus was 6% and between Naples virus and SFTS virus was 69% . NPV showed 7% amino acid distance to GFV and 19% amino acid distance to KARV.\n\n【31】Classical criteria for species demarcation in phleboviruses are based on serology, with established species showing at least 4-fold differences in 2-way neutralization tests . We confirmed that NPV did not react with antiserum against its next closest relatives, GFV and KARV, in neutralization tests . NPV Gn protein was 13% different and Gc 4% different from GFV. The Gn protein of phleboviruses is the key component for neutralization and is recognized by specific neutralizing antibodies .\n\n【32】Although sequence-based species demarcation criteria have not been determined for phlebovirus species, such criteria exist for the related orthobunyaviruses. Species demarcation criteria are now based on the RdRp gene, which shows \\> 6% difference to the closest related virus. Previously unique orthobunyavirus species were defined on \\> 10% difference in N protein sequences . The N proteins of NPV and GFV differ by 15% (GFV itself is not formally classified as a species, and any of the formally classified phlebovirus species are markedly more distant from NPV in this and other genes). We conclude, upon cumulative evidence, that NPV constitutes a putative novel species within the phlebovirus genus.\n\n【33】##### Permissiveness in Vertebrates\n\n【34】To obtain initial data on permissiveness, we performed in vitro growth analyses in a broad range of cell lines derived from different insect species (sand fly and mosquito), peridomestic wildlife (rodent, nonhuman primate, and bat), and livestock (swine, goat, chicken, and cattle) species, as well as from humans. Results revealed a broad susceptibility to NPV, with peak genome copy numbers in cells derived from swine and rodents . Cells derived from sand flies but not from mosquitoes were permissive, despite using C6/36 mosquito cells that are normally broadly susceptible to arboviruses because of a defect in their antiviral RNA interference response . These findings suggest a host range for NPV similar to those of KARV and GFV, which are transmitted by sand flies and infect rodents . It is not known whether rodents are amplificatory or dead-end hosts.\n\n【35】Because GFV is known to induce fatal disease in laboratory mice , we explored similarities in pathogenicity with NPV. We intracranially inoculated 3–4-day-old Swiss Albino suckling mice, causing tremors, hind-limb paralysis, prostration, and death 5–8 days postinfection . Time to death was clearly correlated with virus dose. All animals had high infectious virus concentrations in the brain (mean 2.9 × 10 6  PFU/mL). Taken together, the in vitro and in vivo pathogenicity studies of NPV, including the pathogenicity in suckling mice, may suggest that rodents and sand flies may be involved in the maintenance cycle of NPV.\n\n【36】##### Evidence for Human Infection with NPV\n\n【37】To test whether NPV infects humans, we analyzed 187 archived serum samples: 59 samples from the Marigat district hospital in the area where NPV-infected sand flies were trapped, and 128 samples collected in northeastern Kenya at Sangailu Health Centre . All patients from Marigat, as well as 98 patients from Sangailu, had symptoms compatible with acute infectious diseases. The remaining 30 samples from Sangailu came from healthy controls.\n\n【38】Twenty-six (13.9%) serum samples neutralized NPV, with titers ranging from 1:20 to 1:320 . Women and men were infected at equal rates. Positive samples originated from Marigat (10.2%) and Sangailu (15.6%), without statistical differences in rates (Fisher exact test odds ratio \\[OR\\] 0.6, 95% CI 0.19–1.70; p = 0.37). Detection rates in Sangailu did not differ between healthy and febrile patients (Fisher exact test OR 1.4, 95% CI 0.40–4.3; p = 0.58) . No NPV nucleic acids were detected in serum samples by NPV-specific RT-PCR, suggesting no causative link to the present symptoms with NPV. The detection of NPV neutralizing antibodies in geographically unlinked regions of Kenya suggests widespread previous human exposure and infection.\n\n【39】Because NPV is genetically most closely related to GFV and KARV, we tested all NPV-positive serum samples for ability to cross-neutralize GFV or KARV. All tests yielded negative results, providing further support for the classification of NPV as a separate serotype (and species). Because RVFV frequently causes outbreaks in East Africa, we also tested against RVFV, which, according to its phylogenetic relationship with NPV, is not expected to cross-react with NPV. Seven of 26 NPV-neutralizing serum samples were also reactive with RVFV, showing titers that did not correlate in height with titers against NPV . Absence of correlation of titers suggests previous RVFV infection rather than cross-reactivity between RVFV and NPV.\n\n【40】### Discussion\n\n【41】We identified a high percentage of neutralizing antibodies to NPV in humans living in the NPV-endemic area by neutralization assay, confirming that NPV represents a distinct phleboviral species that causes infection in humans. The fact that the virus was isolated through an exploratory sampling effort is an indicator of the existence of undetected and uncharacterized viruses in this part of Kenya. Although mosquitoes have been the focus of studies on emerging arboviruses, the discovery of a novel sand fly–borne phlebovirus with evidence for human exposure across Kenya indicates the need to broaden vector surveillance activities.\n\n【42】Toscana, sandfly fever Sicilian, and sandfly fever Naples viruses are distributed in the Mediterranean region and northern Africa. GFV has been reported from Sudan, Senegal, Central African Republic, Nigeria, and Benin . KARV occurs in eastern and central Asia , as well as Sudan, Egypt, and Nigeria . According to this geographic distribution, GFV seems to be the most likely sand fly–borne phlebovirus to co-occur in Kenya. Our results show that NPV-immune serum samples do not react with GFV or KARV, suggesting that the reactivity of the positive human samples was the result of previous infection with NPV.\n\n【43】NPV in Kenya may occupy a niche that is taken by GFV and KARV in northern Africa or eastern and central Asia. Several characteristics of NPV suggest parallels between the host ranges of NPV and GFV. GFV has been detected in rodents  but has been detected in arthropods in only a single study in sand flies . Further, the virus was shown to be able to infect _Phlebotomus_ species under laboratory conditions (Tesh R. Studies of the biology of phleboviruses in sand flies. Paper presented at Yale University School of Medicine, New Haven, CT, USA, 1983), suggesting that GFV is maintained in a transmission cycle that involves rodents and sand flies and that it occasionally infects humans . NPV was isolated from sand flies and replicates in vitro in sand fly–derived cell lines but not in mosquito cells, similar to sandfly Sicilian and Naples viruses . Infection studies with cell lines derived from livestock and peridomestic wildlife species showed that NPV replicates ≈10–100 times better in rodent and swine cell lines than in cells derived from other animals, suggesting the involvement of rodents or swine as potential amplificatory hosts for NPV.\n\n【44】_COI_ gene analyses from the virus-positive sand fly pool suggests that species of the genus _Sergentomyia_ have been infected with NPV. Blood-meal analyses revealed that 90% of the analyzed blood-fed sand flies had fed on humans, confirming a likely role as vectors of NPV to humans. Our findings provide new evidence that _Sergentomyia_ flies do not strictly feed on reptiles but also feed frequently on humans .\n\n【45】The NPV antibody prevalence rate in humans (13.9%) is comparable to that of GFV, which is 17%–60% in Sudan, 3%–10% in Egypt, and 3% in Nigeria . KARV antibody prevalence is 1%–11% in Sudan, 2% in Egypt, and 1%–62% in regions of Iran and Russia . Human serum samples from northern Kenya have been tested and yielded no antibodies against GFV or KARV, which matches our results .\n\n【46】NPV appears to have a wide distribution in Kenya; we found equal exposure rates in 2 geographic sets of humans sampled >600 km apart. The serum samples from this study were collected during 2010–2012, suggesting that NPV has been present in humans since at least 2010. Sand fly pools collected in 2014 had low infection rates (MIR 0.18, 1/111 pools, 5–50 sand flies/pool), possibly resulting from collection during a period with low transmission rates. The estimated MIR is lower compared with previous sand fly infections with phleboviruses such as Punique (MIR 6.7) , Massilia (MIR 3.7) , and Toscana (MIR 2.2) viruses , although comparable to Toros (MIR 0.26) and Zerdali (MIR 0.35) viruses . The significance of just 1 isolate of the novel phlebovirus from 111 sand fly pools may seem limited, but it is noteworthy that circulation of RVFV, a phlebovirus with huge epidemic potential, is generally detected at low rates in vectors during interepidemic periods. For instance, multiple surveillance efforts sampling and analyzing thousands of primary and secondary RVFV vectors from outbreak hotspot areas failed to yield any RVFV isolates , yet RVFV infection rates in mosquitoes during the 2006–2007 outbreak in Kenya were high, ranging between 0.8 and 10.65 per 1,000 for primary vectors .\n\n【47】The outcome of infection experiments in mice suggests that NPV could cause diseases such as GFV and RVFV infection . The neglect of sand fly–borne phleboviruses in Africa is exemplified by outbreaks of acute febrile illness associated with sandfly fever Sicilian virus in Ethiopia, which, for a long time, had remained misdiagnosed as malaria , as well as an outbreak of febrile illness probably associated with sandfly fever Naples virus in Sudan .\n\n【48】The symptoms reported among most of the tested patients in this study cannot be conclusively linked to NPV infection, as indicated by antibodies in symptomatic patients and healthy controls, demanding further studies of possible disease association. Clinical studies using specific real-time PCR are necessary to detect viral RNA in humans and to measure the clinical impact of NPV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6c84c936-767b-4689-b5e5-9ba56f00db0c", "title": "Multidrug-Resistant Pseudomonas Aeruginosa Bloodstream Infections: Analysis of Trends in Prevalence and Epidemiology", "text": "【0】Multidrug-Resistant Pseudomonas Aeruginosa Bloodstream Infections: Analysis of Trends in Prevalence and Epidemiology\n**To the Editor:** Multidrug-resistant (MDR) _Pseudomonas aeruginosa_ bloodstream infection has been described only in patients with cystic fibrosis  and in isolated outbreaks in intensive-care unit (ICU) or neoplastic patients . We investigated the percentage and clinical findings of patients with _P. aeruginosa_ bacteremia having MDR strains in a 1,700-bed university hospital in Rome, Italy, over a 10-year period .\n\n【1】All consecutive patients with the first episode of community- or hospital-acquired _P. aeruginosa_ bacteremia, according to the definition of the Centers for Disease Control and Prevention  , were included in the analysis. The term MDR _P. aeruginosa_ covered resistance to ciprofloxacin, ceftazidime, imipenem, gentamicin, and piperacillin. In patients with _P. aeruginosa_ bacteremia, we evaluated age, gender, type of infection (hospital or community acquired), duration of hospitalization, risk factors, clinical findings, and outcome. Prognosis immediately before bacteremia developed was determined with the revised Acute Physiology and Chronic Health Evaluation (APACHE) III system  .\n\n【2】Bacteria were identified by using API 20NE (Biomerieux, Marcy-l’Etoile, France). MICs were determined by broth microdilution in accordance with the methods of the National Committee for Clinical Laboratory Standards. Contingency data were analyzed by the two-tailed chi-square test or Fisher's exact test, and continuous data were analyzed by Student _t_ test. Logistic regression analysis was used to determine which risk factors were independently significant. All statistical analysis was performed with the software program Statistics (Windows Systat Inc. Evanston, IL).\n\n【3】In the study period, _P. aeruginosa_ was isolated from 358 of 379,190 hospitalized patients. Among 358 patients with _P. aeruginosa_ bacteremia, 133 (37%) were hospitalized in medical wards, 103 (29%) in ICUs, 97 (27%) in surgical wards, and 25 (7%) in neonatology; 45 (12%) had HIV infection and 28 (8%) had hematologic malignancies.\n\n【4】For the study period, the overall hospital incidence of both nosocomial and community-acquired _P. aeruginosa_ bacteremia was 0.94 per 1,000 hospital admissions. In particular, the incidence increased from 9.7 to 24.7 per 1,000 hospital admissions (p <0.01; chi square for trend) in ICUs. In HIV-infected patients, the incidence increased from 1.5 to 12.4 per 1,000 hospital admissions until 1996 when, after highly active antiretroviral therapy was introduced, it decreased to 0.7 (p = 0.01, chi square for trend).\n\n【5】The first case of MDR _P. aeruginosa_ strain was isolated in the hematologic unit in 1992. After that, the hospital prevalence of MDR strains increased significantly (p=0.03) from 8% (3/37) in 1993 to 17% (9/54) in 1999. Overall, we observed 51 (14% of 358) cases of MDR _P. aeruginosa_ bloodstream infections; 49 (96%) were nosocomial. The prevalence of MDR among the total _P. aeruginosa_ bacteremia cases per ward was as follows: medical wards 1 (2%) of 60 (95% confidence intervals \\[CI\\] = 0.05-10); surgical wards 7 (7%) of 97 (95% CI = 3-15); hematologic ward 3 (11%) of 28 (95% CI = 2-28); ICUs 22 (21%) of 103 (95% CI = 13-31); and infectious diseases ward (in HIV-infected patients only) 18 (40%) of 45 (95% CI = 26-54).\n\n【6】The mean age ± standard deviation of patients with MDR _P. aeruginosa_ infections was 52±12 years (range 29 to 77); 35 patients (69%) were men, and 9 (18%) were active intravenous drug abusers. The mean Apache III score at diagnosis of bacteremia was 41±17 (95% CI = 39-56). The mean concentration of circulating polymorphonuclear cells was 2,974±2,790/mm 3  (95% CI = 2,181-3,796). In HIV-infected subjects, the mean number of peripheral CD4+ cells was 71±104 /mm 3  (95% CI = 35-106). Advanced age (odds ratio \\[OR\\] = 1.07; 95% CI = 1.04-1.10, p<0.01), HIV infection (OR = 3.94; 95% CI = 1.10-14.11, p=0.03), intravenous drug abuse (OR=13.15; 95% CI=1.65-104.5; p=0.01), and previous therapy with quinolones (OR=3.21; 95% CI=2.14-23.33; p = 0.001) were independent risk factors on logistic regression analysis.\n\n【7】The overall mortality rate of patients with _P. aeruginosa_ bacteremia was 31%; death rates were higher among patients with higher APACHE III score (mean 39 versus 27; p=0.01) and MDR _P. aeruginosa_ infections (67% versus 23%; OR=15.13; 95% CI=1.90-323.13; p=0.001).\n\n【8】This prospective surveillance of _P. aeruginosa_ bloodstream infections clearly indicates, for the first time, that multidrug resistance is statistically associated with HIV infection, as already observed for cystic fibrosis  . We also identified a significant correlation between MDR _P. aeruginosa_ bacteremia and intravenous drug abuse, advanced age, and previous quinolone use.\n\n【9】The association between isolation of MDR strains, HIV infection, and intravenous drug abuse (the most important HIV risk factor in Italy) is not an unexpected result. We have already demonstrated that hospitalized HIV-infected patients are at increased risk of acquiring nosocomial bloodstream infections compared with other immunocompromised hosts  . Age is a well-known predisposing factor for bacterial infections. In particular, older HIV-infected patients progress more rapidly to AIDS  .\n\n【10】Resistance following treatment with a single antimicrobial agent may be due in some circumstances to synergy between enhanced production of beta-lactamases and diminished outer membrane permeability  . More emphasis is now given, however, to the energy-dependent efflux of antibiotics by _P. aeruginosa_ . A single opening of a pump facilitates resistance to quinolones, beta-lactams, tetracycline, and chloramphenicol among the drug efflux  . The recent characterization of a carbapenem-hydrolyzing metallo-beta-lactamase from _P. aeruginosa_ opens new possibilities for reducing the spread of resistant strains  .\n\n【11】One limitation of our study is the absence of genotypic analysis of MDR strains. However, we are confident that a general outbreak of MDR _P. aeruginosa_ did not occur in our hospital. Nevertheless, limited outbreaks involving few patients in different wards remain a possibility. In summary, the observation that 14% of _P. aeruginosa_ bloodstream infections are multidrug resistant is worrisome and reflects the growing worldwide problem of antimicrobial resistance. In particular, the fact that HIV-infected subjects are at increased risk, as are persons with cystic fibrosis, suggests the need for ongoing worldwide surveillance of _P. aeruginosa_ in immunocompromised patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "23b04eda-2045-4193-9b8a-11a3ae4efc13", "title": "Staphylococcus lugdunensis Pacemaker-related Infection", "text": "【0】Staphylococcus lugdunensis Pacemaker-related Infection\nDuring the past decade, _Staphylococcus lugdunensis_ has emerged as an important pathogen implicated in both community-acquired and nosocomial infections . Clinical manifestations of infections with these organisms include abscesses , meningitis , ventriculoperitoneal shunt infection , spondylodiscitis , prosthetic joint infection , catheter-related bacteremia , and endocarditis . Infections with _S. lugdunensis_ tend to have a more fulminant course, with an outcome resembling that of _S. aureus_ infections rather than that caused by coagulase-negative staphylococci . In addition, these organisms are frequently misidentified as _S. aureus_ because of their morphologic appearance with yellow pigmentation and complete hemolysis when cultured on blood agar.\n\n【1】Small-colony variants (SCVs) are mainly reported in _S. aureus_ , and interest in infections with SCVs has recently increased after an association between recovery of _S. aureus_ SCVs and persistent and relapsing infection has become evident . SCVs are a slow-growing subpopulation of the species with characteristics that can associated by a common factor, i.e. alterations in electron transport . The generation time for SCVs is up to 9-fold longer than for metabolically normal strains, which results in tiny colonies that are frequently not visible until after 48 to 72 hours of incubation. Consequently, correct identification and susceptibility testing for clinical laboratories are complicated, which may result in diagnostic underestimation and therapeutic failures. While most studies have dealt with SCVs of _S. aureus_ , little is known about infections with SCVs of coagulase-negative staphylococci. Recently, 2 cases of bloodstream infections caused by SCVs of _S. epidermidis_ and _S. capitis_ , respectively, were reported . Both infections were related to foreign bodies and observed after pacemaker implantation.\n\n【2】We report the first known case of a device-related bloodstream infection due to _S. lugdunensis_ SCVs and other colony variants of this species. Of particular interest, this infection was also observed after pacemaker implantation.\n\n【3】### The Case\n\n【4】In July 2003, a 61-year-old man was transferred from a local hospital to our cardiothoracic surgery department with a diagnosis of pacemaker lead infection. Past medical history included nephrectomy in 1996 for cancer of the left kidney and implantation of a universal demand pacemaker (dual chamber pacemaker) for treatment of sick sinus syndrome in 1990. In August 2002, after being in place for 12 years, the pacemaker battery was replaced. Three months later, the patient was admitted to a local hospital with a temperature of 40°C and chills. Laboratory findings included a leukocyte count of 17,500/μL and a C-reactive protein (CRP) level of 90 mg/L. A transesophageal echocardiogram showed thickening of the left coronary aortic valve, and thrombotic material was seen on the ventricular pacemaker lead. A blood culture drawn on admission showed _S. lugdunensis_ susceptible by agar diffusion to penicillin, oxacillin, erythromycin, clindamycin, rifampin, and aminoglycosides. Antimicrobial drug therapy was instituted with intravenous ampicillin/sulbactam and gentamicin for 14 days with prompt resolution of clinical symptoms, and follow-up blood cultures remained negative. Three days later, however, a spiking fever and chills developed in the patient. Antimicrobial drug treatment was changed to intravenous vancomycin and rifampin. The patient's condition improved rapidly, and he was discharged after 3 weeks of antimicrobial drug therapy when the CRP value had returned to normal.\n\n【5】Two months later in February 2003, the patient was readmitted to the cardiology department with the presumptive diagnosis of endocarditis. During a transient febrile episode, a blood culture was obtained that again yielded _S. lugdunensis_ . Antimicrobial drug therapy was resumed with intravenous flucloxacillin and gentamicin. All 4 follow-up blood cultures obtained 3 and 4 days later, when the patient was afebrile, were again positive for _S. lugdunensis_ . An echocardiogram did not show vegetations or other evidence of endocarditis. Pacemaker removal was strongly suggested, but the patient refused. After 14 days of intravenous treatment, the antimicrobial drug regimen was changed to oral administration of flucloxacillin for 14 days. After a full recovery, the patient was discharged, but removal of the pacemaker system was recommended if clinical symptoms reappeared.\n\n【6】Four months later in July 2003, the patient came to the local hospital with recurrent high fever and chills, a leukocyte count of 12,200/μL, and a CRP value of 37 mg/L, but he did not show any peripheral sign of endocarditis. Four sets of blood cultures drawn on admission showed _S. lugdunensis_ . A transesophageal echocardiogram showed large vegetations in the right atrium inserting at the ventricular lead but no involvement of cardiac valves. The patient responded promptly to the initiation of antimicrobial drug therapy with intravenous flucloxacillin and gentamicin and became afebrile. He was then transferred to our cardiothoracic surgery department for pacemaker ablation.\n\n【7】Four days later, the complete pacemaker system, including the intracardiac leads, was removed by open heart surgery. The cardiac valves did not show signs of infective endocarditis, but large vegetations adhered to both the atrial and the ventricular lead. Follow-up blood cultures remained negative but thrombotic material scraped from the pacemaker leads was analyzed by culture. After 2 days of incubation, this material yielded nonhemolytic and nonpigmented, as well as yellow-pigmented, hemolytic colonies of variable size, which were gram-positive catalase-positive cocci, consistent with staphylococci. The results of subcultures on solid media suggested a mixed population of staphylococci, with at least 4 different colony morphologies . Four single-colony subcultures of different colony morphotypes also produced colony variations that persisted in serial subcultures of single colonies.\n\n【8】Clumping factor was not present and tube coagulase test results were negative. Identification was initially attempted with the gram-positive identification card provided with the VITEK 2 system (bioMérieux, Marcy l'Etoile, France). The large hemolytic morphotype  showed a profile consistent with _S. lugdunensis_ , with positive results for ornithine decarboxylase, trehalose, and l-pyrrolidonyl-β-naphthylamide. Other morphotypes were repeatedly identified as _S. haemolyticus_  and _S. auricularis_ , respectively. The _S. lugdunensis_ isolate that grew as tiny (pinpoint), nonpigmented, and nonhemolytic colonies was shown to be a hemin-auxotrophic SCV . The _S. lugdunensis_ isolate (large colony morphotype) was susceptible to all antimicrobial agents in the VITEK GPS-P526 card test (bioMérieux) and did not produce β-lactamase. The other morphotypes did not grow sufficiently to allow antimicrobial susceptibility testing with the VITEK system. However, susceptibility to penicillin and oxacillin was confirmed by an Etest (AB Biodisk, Solna, Sweden) for all colony variants.\n\n【9】The API ID 32 Staph system (bioMérieux) identified all morphotypes as _S. lugdunensis_ , which was later confirmed by 16S ribosomal RNA gene sequencing using the RIDOM entries . All isolates, including an additional _S. lugdunensis_ blood isolate obtained in February 2003 that produced flat, white, and nonhemolytic colonies , were compared by pulsed-field gel electrophoresis and found to be identical, although the colony morphology was different .\n\n【10】Postoperative recovery was uneventful. Treatment with intravenous flucloxacillin and gentamicin was continued for 14 days. A 72-hour electrocardiogram did not show any need for pacemaker reinsertion. Fourteen days after surgery, the patient was discharged from the hospital, after a total clinical course of 10 months with recurrent infections.\n\n【11】### Conclusions\n\n【12】Previous reports have rarely emphasized colony variation as an important feature of _S. lugdunensis_ . In the initial description of the species in 1988 , colony variation was observed in 3 of 11 strains. More recently, Leung et al. reported colony variation of _S. lugdunensis_ in a fatal case of endocarditis . Unlike other staphylococcal species such as _S. capitis_ and _S. hominis_ , which show colony variation that disappeared after extended incubation, mixed morphotypes of _S. lugdunensis_ were persistently detectable through incubation and subculture . The authors speculated that preceding antimicrobial drug therapy may play a role in producing colony variation in _S. lugdunensis_ and that previous studies may have underreported the characteristic of colony variation seen in this species.\n\n【13】Some of the aberrant morphotypes described in earlier studies may have in fact been SCVs. Both prior exposure to antimicrobial drugs and the presence of chronic or recurring infections, often with indwelling foreign devices that have been associated with SCVs of _S. aureus_ , _S. epidermidis_ , and _S. capitis_ , are features commonly observed in infections with _S. lugdunensis_ . In our case, repeated courses of gentamicin therapy may have selected for SCVs. Although the infection showed a rather benign clinical course and did not confirm other reports of _S. lugdunensis_ endocarditis in which the infection was more aggressive, it illustrates the chronic, recurrent, and persistent nature of infections with SCVs and the problems associated with delayed identification of _S. lugdunensis_ colony variants and interpretation of its clinical significance.\n\n【14】The refusal of the patient to have the pacemaker removed added to the chronic course of the infection. Although these variants were not identified until removal of the device, the clinical importance of SCVs for this persistent infection can be anticipated. Clinical isolates are often a mixed population of parent strains and SCVs. Because of their different generation times, even a small percentage of normally growing organisms may rapidly replace SCVs in liquid medium such as a blood culture during overnight incubation. Thus, SCVs may have gone undetected in previously obtained blood cultures. Increased awareness of colony variation and the possible occurrence of SCVs as a characteristic feature of _S. lugdunensis_ should be helpful in earlier recognition of the pathogen and appropriate management of the infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "da613f99-f030-4ad0-ac70-b5d4b4ddb794", "title": "Prevalence of SARS-CoV-2, Verona, Italy, April–May 2020", "text": "【0】Prevalence of SARS-CoV-2, Verona, Italy, April–May 2020\nOn May 25, 2020, Italy had the third highest number of cases and the second highest number of deaths in Europe caused by the novel betacoronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)  as part of the ongoing pandemic of coronavirus disease (COVID-19). The continuing spread of infection and the resulting strain on healthcare systems has made the identification of asymptomatic persons crucial to limiting transmission . We conducted a cross-sectional study on a representative sample of the general population to estimate the prevalence and death rate of SARS-CoV-2 infection in Verona, Italy.\n\n【1】### The Study\n\n【2】We estimated the prevalence of active or past SARS-CoV-2 infection among the population of Verona among randomly selected participants \\> 10 years of age. This investigation was an observational, cross-sectional study approved by the Ethics Committee of Verona and Rovigo provinces on April 15, 2020 , in compliance with the Strengthening the Reporting of Observational Studies in Epidemiology guidelines .\n\n【3】According to Verona’s municipal register, 235,034 persons \\> 10 years of age lived in Verona on January 1, 2020 . We used systematic random sampling to compile a list of potential participants. Because the prevalence of asymptomatic SARS-CoV-2 infection in Italy had previously been estimated at 10.0% , we decided to randomly sample 1,527 participants, resulting in a standard error of < 1.5%. We predicted a dropout rate of 35% and accordingly mailed invitations to 2,061 potential participants. We selected the first sample using a random starting point; for subsequent samples, we used a sampling interval calculated by dividing the population size by the desired sample size (235,034/2,061 = 114).\n\n【4】We collected data from April 24 through May 8, 2020. We required a parent’s or guardian’s consent for participants <18 years of age. All participants gave their informed consent. Participants first completed a phone interview about COVID-19 symptoms within the previous 15 days. Specialized staff at Istituto di Ricovero e Cura a Carattere Scientifico then collected blood and nasopharyngeal swab samples from each participant. These staff extracted total RNA from nasopharyngeal swab samples using a MagnaPure LC.2 instrument and MagNA Pure LC RNA Isolation Kit , according to the manufacturer’s instructions. We analyzed the eluted RNA by reverse transcription PCR (RT-PCR) to detect the presence of active infections . We analyzed serum samples for IgG against SARS-CoV-2 by serologic assay  to detect previous infections. Experienced laboratory personnel conducted each test independently and blindly.\n\n【5】Because neither assay has perfect sensitivity, we used latent class analysis (LCA) to estimate the prevalence of SARS-CoV-2 infection. LCA models were based on SARS-CoV-2 test results and selected clinical variables . We interpreted the outcomes as the probability that a given person was (or had been) infected . We reported all parameters and estimations with 95% CIs. We adjusted statistical models and estimations for covariates.\n\n【6】A total of 1,515 persons participated in the study . We found no significant difference in sex proportions between the general population (53% female) and the study sample (54% female). The mean age of all participants was 52.1 (SD = 20.0) years in the general population and 49.1 (SD = 22.2) years in the study sample. We summarized demographic and clinical data using descriptive statistics and measures of variability and precision . Of 1,515 participants, 9 (0.6%) tested positive for SARS-CoV-2 RNA but negative for IgG against SARS-CoV-2, 40 (2.6%) tested negative for viral RNA but positive for IgG, and 1,465 (96.7%) tested negative for both indicators. Only 1 participant tested positive for RNA and IgG. Participants who tested negative for viral RNA but positive for IgG reported symptoms such as anosmia (39.5%), temperature \\> 37.5°C (30.8%), fatigue (35.9%), and persistent cough (28.2%). Less than 2% of participants who tested negative by both tests reported symptoms.\n\n【7】We used a backward stepwise multinomial multivariate logistic regression model to compare selected COVID-19 symptoms (i.e. anosmia, dyspnea, diarrhea, and fever) in the RNA-positive and RNA-negative/IgG-positive groups with the RNA-negative/IgG-negative group. Fever and anosmia were each significantly associated with belonging to the RNA-negative/IgG-positive group (p<0.01) but not the RNA-positive group.\n\n【8】We used LCA to estimate the prevalence of infection considering the results of RT-PCR, the serologic assay, and the symptoms selected by stepwise regression. The estimated probability of belonging to class 1 (uninfected) was 0.97 and class 2 (infected) was 0.03 .\n\n【9】As of May 25, 2020, Verona had 1,528 cumulative patients in whom SARS-CoV-2 infection was diagnosed, including 144 who had died, indicating a 9.4% death rate . Verona was the province in Veneto with the most cases and deaths caused by SARS-CoV-2 . Our LCA estimated a prevalence of 3.0%, suggesting 7,051 cumulative cases (4.6 times higher than the official count). These estimates suggest that 144 reported deaths would indicate a 2.0% death rate. According to the crude rates, the 50 SARS-CoV-2–positive participants in our study would account for 3.3% of the total study population. Applying this percentage to the whole population of Verona would indicate 7,756 cases and a 1.9% death rate.\n\n【10】Of the 10 RNA-positive participants, only 1 tested positive by serologic assay. This finding raises concerns about the current screening policy of 2-step testing, which comprises a serologic assay and, if the assay results are positive, PCR. Given the economic costs associated with testing, officials should carefully advise the public on all testing options.\n\n【11】Our study has a few limitations. Because participation was voluntary, our study might have been influenced by selection bias . Also, LCA might have underestimated the accuracy of both diagnostic tests. For example, considering past and active infections together might have reduced test sensitivity. Furthermore, the PCR assay did not have 100% specificity, as is usually assumed . The model might have also underestimated the specificity of the serologic assay. However, the crude rates estimate a prevalence only slightly higher, and the death rate only slightly lower, than predicted by our model.\n\n【12】### Conclusions\n\n【13】Our study estimated the prevalence of SARS-CoV-2 infection in Verona using a random sample of its population. Similar studies are currently underway on a larger scale. The results will estimate the true circulation of SARS-CoV-2, better approximate the death rate, and inform infection containment and management. Our study provides a clear picture of the circulation of SARS-CoV-2 infection in the general population of a city and an estimation of the true death rate caused by the infection. The results also suggest that 2-step testing might not detect all active infections. We are currently organizing phase 2 of our study, during which we will conduct follow-up serologic testing on all PCR-positive and PCR-negative/IgG-positive participants, enabling the evaluation of any antibody seroconversion, negativization, or change in titer.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7adfce3-e08e-4032-a46c-2d50f8cf2365", "title": "Bertiella studeri Infection, China", "text": "【0】Bertiella studeri Infection, China\n**To the Editor:** _Bertiella_ is a genus of tapeworm in the family _Anoplocephalidae_ , many species of which exist as parasites of nonhuman primates. Two species of the genus, _Bertiella studeri_ and _B. mucronata_ , can infect humans . More than 50 cases of human infection have been recorded, and the geographic distribution of cases shows that the tapeworm exists in countries in Asia, Africa, and the Americas. We report a _B. studeri_ infection in a person; to our knowledge, this case of bertiellosis is the first in China.\n\n【1】The patient was a 3.5-year-old Chinese boy from Suzhou City, Anhui Province. The boy had a 6-month history of frequent abdominal pain. His parents had noticed living \"parasites\" in his feces for 3 months; a segment of the worm was expelled every 2 or 3 days. According to the symptoms, doctors at the local hospital diagnosed his condition as _Taenia solium_ infection and prescribed praziquantel, but no drug was available in the hospital or local drugstores. Consequently, the parents brought the child to Bengbu Medical College for further diagnosis and treatment.\n\n【2】The patient appeared healthy; routine medical examination showed normal heart, lung, liver, and spleen, and he had no fever. Though the patient had intermittent epigastric pain, the abdomen was soft and tender. A total of 133 proglottids were collected from the feces. Their average length was 0.1 cm, and the total length of all proglottids was 13 cm; each segment was 0.68–1.10 cm in width. Eggs (N = 53) were examined microscopically; they were roundish or oval, an average of 45.31 μm diameter (range 37.93–50.00 μm), and clearly showed typical pyriform apparatus, with visible hooklets . Other laboratory examinations showed hemoglobin level of 110 g/L, erythrocytes 3.9 × 10 12  cells/L, and leukocytes 8.0 × 10 9  cells/L. Although 2 species can parasitize humans, the geographic distribution and egg size of these species differ . _B. mucronata_ has smaller eggs and is found only in the New World. On the basis of the size of the proglottids , larger eggs with pyriform apparatus and hooklets, and geographic distribution, the infecting cestode was identified as _B. studeri_ .\n\n【3】The origin of infection was not confirmed; the only clue was that the boy's parents had once raised tame monkeys in a zoo. When the boy was 2 years old, he often played in the wildland, which is part of the zoo near the forest, and frequently fed and played with the captured monkeys. Further questioning showed that the boy had also been in frequent contact with wild monkeys. We could not confirm whether he had been infected by eating monkey food contaminated with mites.\n\n【4】The lifecycle of the cestode requires 2 hosts; nonhuman primates are generally the final host, while oribatid mites are the intermediate host, in which the infective cysticercoid of the cestode develops. Orbatid mites may exist in soil to maintain natural infection, and the definitive host is infected by eating or otherwise coming into contact with contaminated soil or food. Animal infection has been recorded in some provinces in China, and human bertiellosis has been recorded in Sri Lanka , Saudi Arabia , Vietnam , Japan , India , Thailand, Malaysia, and other Asian countries. However, according to the most recent Chinese authoritative text, Human Parasitology , no human bertiellosis has been recorded in China. Humans are infected by unconsciously swallowing infected mites, and in Mauritius, children were infected by eating guavas that had fallen on the soil . Other human infections may have occurred, but infected persons may have had mild symptoms and not noticed expelling the segments, so local doctors may have considered the cases to have been caused by a common cestode. To prevent human bertiellosis, the relationship between human cases and the natural host must be investigated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "419abbd8-84e2-4a4b-8bd9-13f7b826787f", "title": "Global Disease Detection—Achievements in Applied Public Health Research, Capacity Building, and Public Health Diplomacy, 2001–2016", "text": "【0】Global Disease Detection—Achievements in Applied Public Health Research, Capacity Building, and Public Health Diplomacy, 2001–2016\nInfectious disease outbreaks present a serious health threat that requires early detection and effective preventive action to avoid regional or even global spread. Such actions enhance global health security by protecting the health of persons in the affected regions and in the United States. Recent epidemics, including severe acute respiratory syndrome (SARS) during 2002–2003, pandemic influenza A(H1N1) in 2009, Ebola virus disease in 2014, and Zika virus infection during 2015–2016, underscore this risk and highlight the critical need for building core global public health capacity for detection and response.\n\n【1】In 2001, the Centers for Disease Control and Prevention (CDC) established the International Emerging Infections Program (IEIP) to conduct applied public health surveillance and research aimed at preventing infectious disease outbreaks with pandemic potential. IEIP placed CDC staff in key overseas locations to work with national public health institutes and their partners to establish sentinel surveillance and conduct applied research on emerging infectious diseases. The program was modeled after the US-based Emerging Infections Program, a network of state health departments and their partners that conduct surveillance of certain infections and thereby provide a foundation for various epidemiologic studies to explore risk factors, spectrum of disease, and prevention strategies . IEIP had a similar objective but on a global platform; namely, to conduct applied public health research in strategic global locations to prevent, detect, and control emerging and reemerging pathogens.\n\n【2】CDC established the Global Disease Detection (GDD) Program in 2004 by using existing research programs within IEIP as the scientific backbone of its GDD regional centers; this effort was made in response to data gaps identified during the SARS epidemic. The GDD Program mission was to ensure that infectious diseases were detected and stopped at the source before crossing international borders . The GDD Program, like IEIP, set up a network of CDC technical experts stationed in GDD regional centers located in multiple countries across the World Health Organization (WHO) regions. GDD regional centers were initially set up in countries with IEIP presence (Thailand, Kenya, Guatemala, Egypt, China, and Kazakhstan). Subsequently, new GDD regional centers were established in Bangladesh, India, South Africa, and Georgia . These centers serve as regional resources for neighboring countries and are a framework for improving public health and global health security through close collaboration with local partners. To date, the 10 GDD regional centers have supported ≈90 countries around the world, including the United States . The GDD regional centers have assisted US domestic public health institutions in response to infectious diseases that affected international visitors while in the United States and US citizens while abroad.\n\n【3】The GDD Program promotes intersectoral public health responses and applied epidemiologic research that include ministries of health and agriculture, academic institutions, other US government programs, and international and nongovernmental organizations. These established and trusted relationships with national governments enable more effective prevention and detection of emerging infectious diseases. The GDD regional centers also provide an in-country infrastructure that enables CDC to respond rapidly to public health threats. A critical strength of the GDD Program is the long-term assignment (i.e. 2–6 years) of epidemiologists, laboratorians, statisticians, and other diverse technical staff at GDD regional centers in host countries. The GDD technical staff work alongside locally hired technical staff to foster close collaboration and bilateral knowledge transfer with host country partners. These strategically placed GDD technical staff can have localized information for early detection of unusual infectious disease events. During public health emergencies, where time lost often equals lives lost, the ability to leverage trusted international public health scientific partnerships is essential for life-saving action. GDD field staff are often a first line of response during an epidemic. During the 2014–2016 West Africa Ebola outbreak, ≈30 GDD field-assigned staff, including US and local personnel, deployed from GDD regional centers to assist with establishing diagnostic, contact tracing, and data analysis capacity. GDD field staff’s experience in international settings was critical to the response and facilitated quick integration into ongoing response and prevention efforts. GDD’s sustained capacity-building efforts enabled these forward-deployed assets to respond quickly not only in their own regions but also across the globe.\n\n【4】### Activities and Accomplishments\n\n【5】The core activities of the GDD Program focus on applied public health research, surveillance, laboratory, public health informatics, and technical capacity building. Applied public health research refers to activities that generate data to answer a research question; test a hypothesis; evaluate a program or programmatic element (e.g. a public health practice, a surveillance system, data quality); or provide information for evidence-based decision making. Surveillance refers to activities that collect health-related data in a systematic manner over time to inform public health action. Laboratory refers to activities that collect specimens for laboratory analyses. Informatics refers to any activity that collects and aggregates data (paper-based or electronic) that could be used for further analysis. Capacity building refers to activities that increase the skills, infrastructure, or resources of individuals or partnering organizations. Current GDD Program projects incorporate multiple core activities, with technical capacity building, laboratory, and public health research being most common . These activities are essential for the identification of new health threats, monitoring and tracking of health threats over time, and for conducting applied research and pathogen discovery. At times, regional centers might conduct studies of noninfectious causes of illnesses because it is not always clear whether the etiologic agent is a pathogen, a toxin, or some other cause at the beginning of an outbreak . The GDD Program also provides a robust framework for public health diplomacy and the development and implementation of coordinated multisite activities and studies .\n\n【6】### GDD by the Numbers\n\n【7】In 2015, the GDD Program performed a portfolio review of activities in the 10 GDD regional centers for fiscal years 2015 and 2016 (October 1, 2014–September 30, 2016). The unit of analysis was a GDD Program–funded project. Multiyear projects were counted once. Projects were not weighted by the size or scope of a project; thus, a small research study was equivalent to a large, multiyear, population-based surveillance project. We excluded projects that listed HIV (n = 2) or noncommunicable disease (n = 1) as their primary focus. We classified projects into core activity areas: technical capacity building, surveillance, applied public health research, laboratory, and informatics. Activity areas were not mutually exclusive, so a project could be classified in multiple areas.\n\n【8】Overall, the 10 GDD regional centers engaged in 205 discrete projects during October 2014–September 2016 . The number of projects per GDD regional center ranged from 11 to 36. The variability in number of projects per center was attributable to a combination of factors, including the age of the center, the geographic region covered by the center, and funding and staffing resources available for the center. Capacity-building projects (n = 125) were most common . We also classified technical projects into topical areas based on the key focus of the project. Topical areas were collated and categorized by major groupings . The variability in the range of topical areas was attributable to a combination of factors, including the epidemiology of the disease (nationally and globally); available funding; the technical capacity at the local level; and the changing priorities of the United States and local partners (e.g. ministries of health, national public health institutes, and research institutes). Of 205 projects with a defined topical area, 24% (n = 50) were focused on acute respiratory illness , which is expected given that respiratory disease surveillance has been a core function since the inception of the program. Health system strengthening (n = 36), One Health (n = 30), and emerging infectious disease (n = 22) were the next most common topical areas. The increasing prevalence of these new topical areas indicates an expansion of the breadth of projects being conducted by GDD regional centers.\n\n【9】### GDD Core Activities\n\n【10】##### Applied Public Health Research\n\n【11】The GDD Program has a broad portfolio of applied public health research and special epidemiologic studies, ranging from ensuring infection control practices for Nipah virus in Bangladesh to evaluating antimicrobial drug–resistant invasive salmonellosis in Thailand . Conducting applied public health research and epidemiologic studies in international settings can address important knowledge gaps in infectious disease issues. Many of these issues would be difficult to examine in the United States, primarily because of low prevalence of many infectious diseases. International public health research studies contribute to the scientific knowledge base and help answer questions that can influence US public health policy. Examples range from gathering data for the issuance of travel notices to conducting vaccine studies needed to guide domestic vaccination guidelines .\n\n【12】GDD regional centers work closely with the international partners, often a ministry of health or national public health institute, to identify common areas of research interests and national priorities. The data generated from these collaborations have been used by host governments to quantify the public health issue and, ultimately, to guide and inform public health policy. Implementing high-quality research studies also serves as a hands-on training mechanism for international partners. Projects are conducted in collaboration with the in-country hosts, from developing the concept, writing the research protocol, implementing the study, analyzing and interpreting the data, and publishing the results. A tangible way that highlights the results of these collaborations is dissemination of findings in the scientific literature. Since the inception of the GDD Program, GDD staff have authored or coauthored ≈875 peer-reviewed scientific articles .\n\n【13】##### Surveillance\n\n【14】GDD regional centers partner with host countries to develop and strengthen surveillance for key illnesses and to limit spread of disease to the point of origin. Projects integrate laboratory, clinical, and epidemiologic information that can guide public health interventions and other control measures. GDD centers achieve this objective through several types of surveillance strategies, such as syndromic, laboratory-based, population-based, and sentinel systems . Population-based surveillance provide a framework for applied public health research that can help to characterize the burden, risk factors, and transmission characteristics of new or emerging infectious diseases and to assess the effectiveness of prevention strategies . Sentinel surveillance in a few key sites/facilities for specific or syndromic infectious diseases can help to identify emerging or reemerging pathogens .\n\n【15】Outbreaks of SARS and avian influenza A(H5N1) highlighted the need to have systems in place for detecting emerging pathogens . Thus, establishing population-based infectious disease surveillance for pneumonia and acute respiratory infections was a primary goal of the GDD Program . The resulting surveillance activities also provide a platform for other GDD core activities. Moreover, the GDD respiratory surveillance research projects have helped quantify burden of illness for pneumonia and influenza-associated acute respiratory illness, especially among children, and a high incidence of several respiratory pathogens, including respiratory syncytial virus, parainfluenza, and adenoviruses .\n\n【16】As GDD regional centers have matured, existing surveillance platforms have increasingly been adapted to include emerging pathogens, special noncommunicable disease studies, and projects focused on the animal–human interface (i.e. zoonotic diseases) . In 2014, the GDD regional centers began efforts to link common acute febrile illness (AFI) syndromic surveillance strategies across 5 regional sites (Egypt, Guatemala, India, Kenya, and Thailand) to gain a global perspective on AFI. Conducting AFI surveillance at GDD regional centers is of public health importance because AFI represents a common clinical syndrome for multiple diseases of outbreak potential or emerging zoonotic infections and provides an opportunity to evaluate novel diagnostics. Unlike respiratory illness syndromes such as severe acute respiratory illness and influenza-like illness, no international consensus case definition exists for AFI surveillance, although recommendations for improving methods have been proposed . In addition, very few published AFI etiology studies have been conducted in multiple countries. A literature review currently under way has found that, of 169 AFI studies aiming to identify etiology and published during 2005–2016, only 6 (4%) had enrolled cases in multiple countries .\n\n【17】A multisite research effort has the potential to catalyze historically disparate AFI syndromic surveillance systems toward globally comparable data of high utility at all levels for public health response. Network activities across different GDD regional centers that represent diverse disease risks enhanced the ability to study a range of infectious diseases for which a single country might not have the capacity or incidence of disease to study for evidence-based public health decision making. The GDD effort, to date, has included consistent case definition use with a focus on undifferentiated AFI, multipathogen detection of local and globally significant infectious diseases, use of standard and investigational diagnostics where feasible, and prospective sentinel health facility–based surveillance methods of \\> 1 year in duration to evaluate seasonal epidemic trends. Barriers to launch and harmonization to a common research protocol have included variation in local priority pathogens, resource availability, and time required for integration into existing public health surveillance and healthcare networks. Established enhanced AFI surveillance has thus far provided a useful platform for investigating emerging infections with a febrile illness component, such as Zika virus and scrub typhus.\n\n【18】##### Laboratory\n\n【19】Effective public health requires close collaboration between epidemiologists and laboratory scientists. GDD works with partner countries to strengthen diagnostic technical capacity for priority diseases; evaluate new laboratory diagnostics; establish frameworks for national laboratories that include quality assurance and specimen referral systems; improve biosafety/biosecurity; and train laboratory personnel on benchtop skills, laboratory management, and public health laboratory functions. These efforts have improved the capacity of GDD host countries and their regions to detect and respond to emerging infectious disease threats and to sustain these efforts through a strong cadre of laboratory scientists dedicated to improving the global public health laboratory infrastructure .\n\n【20】Research at the GDD regional centers has assisted in the detection and identification of 12 novel strains and pathogens that were new to the world and 62 novel strains or pathogens that were new to the region where they were discovered . GDD laboratorians have helped implement capacity to conduct >380 new diagnostic tests in 59 countries, improving disease detection capability and contributing to faster response times within the region.\n\n【21】##### Public Health Informatics\n\n【22】Informatics is the application of public health information systems to capture, manage, analyze, and use information to improve public health practice . Examples of the key activities include the use of electronic databases, either as the source of data or as a method to collate data, for expediting the time between data collection and use. At GDD regional centers, public health informatics is a cross-cutting activity for disease surveillance, laboratory studies, and applied epidemiologic research to ensure that data are collected and managed in a systematic and reliable manner. Most GDD data-collecting projects currently under way have an informatics component .\n\n【23】##### Capacity Building\n\n【24】Strengthening the local public health capacity and workforce are key for improving the detection and response to infectious diseases globally. The transfer of epidemiology, laboratory, and emergency preparedness skills to local public health professionals is necessary for sustainability, both nationally and across regions. Capacity building is another cross-cutting activity at the GDD regional centers and ranges from establishing or strengthening existing surveillance, laboratory, emergency preparedness, and health systems to conducting high-quality epidemiologic research studies to address knowledge gaps. This capacity is achieved through on-the-job training of local partners, providing technical expertise, conducting high-quality research studies, and collaborating on analysis of information to inform evidence-based decision making.\n\n【25】### Public Health Diplomacy\n\n【26】Scientific exchange can play a strong role in building bonds across countries. Because health is an area of concern for all nations, international projects that address a common threat, such as infectious diseases that easily cross borders, can open avenues of communication and ease tensions between the United States and other nations . GDD China serves as an example of how 2 strong national public health institutes (1 in China and 1 in the United States) can collaborate and benefit. During the West Africa Ebola outbreak in 2014, China CDC had the resources and willingness to respond but not necessarily the US CDC experience or technical expertise with Ebola outbreaks and response. Since 2006, Chinese laboratorians have worked alongside US colleagues to build greater diagnostic testing capacity throughout China. Because of this preexisting relationship, the 2 countries were able to forge a new type of collaboration in Sierra Leone; scientists from both countries worked together to offer critical training and resources to Sierra Leone to help stop the spread of the largest Ebola outbreak in history . By building strong partnerships and scientific systems, GDD protects the United States and countries around the world from threats to health, safety, and security.\n\n【27】### Lessons Learned and the Future\n\n【28】The GDD Program promotes the prompt detection and mitigation of disease threats globally. GDD works with multiple countries  to conduct applied public health research and develop and enhance public health capacity to rapidly detect, accurately identify, and promptly contain emerging and reemerging infectious diseases. The activities of the GDD Program are critical to help countries improve their disease surveillance networks and enhance laboratory capabilities for detection of emerging pathogens. The program also has greatly expanded epidemiology workforce networks to meet their commitment to global health security and the International Health Regulations 2005.\n\n【29】The activities of the GDD Program have developed needed technical capacity, advanced science, and provided critical information for policy change. Activities of the GDD regional centers have allowed a greater understanding of what infections or conditions are of concern in the countries and regions in which they work. They have increased awareness of the emergence of antimicrobial resistance and the growing threat of infections that can be acquired in healthcare settings . Strengthening disease surveillance, applied public health research, and laboratory capacity have allowed for a better understanding of pathogens associated with illnesses that present with acute fever. The activities established serve as a base or launch pad for the rapid and timely implementation of surveillance for emerging infections like Zika virus and applied epidemiologic research studies to better understand which populations are being affected and to enumerate potential factors associated with infection and spread of illness.\n\n【30】As new laboratory techniques for the detection of pathogens are developed, the GDD regional centers have served as a platform to examine the performance of these new tests in multiple settings and promote the adoption of the new techniques in multiple countries. Because of ongoing surveillance and routine collection of epidemiologic information, GDD regional centers and the countries they work with have the tools needed to best characterize pathogens that are circulating and explore potential reservoirs and sources associated with these infections. Increased informatics capacity is concurrently enabling the active linkage of information and interfacing of data housed in multiple data systems within the countries and regions.\n\n【31】GDD regional centers make critical contributions to global disease detection by improving infectious disease detection capacity through integration of applied public health research and laboratory capacity building, which in turn will generate quality data that can inform high-level policy. The GDD Program has matured and transformed over the past 10 years and continues to evolve. Further advancing the technical capacity that has already been developed is allowing the GDD Program to focus on needed research and generation of data to develop and evaluate interventions and inform policies needed to reduce burden of multiple conditions worldwide. Examples of research activities needed include studies to understand the actual burden of conditions at play, assessments of the impact of multiple conditions on local and global populations, quantification of the societal and economic costs of illnesses, and evaluation of control measures.\n\n【32】Threats posed by emerging pandemics and other infectious diseases will remain a challenge to global health security, endangering economies and decreasing political stability. GDD will continue to work with countries to strengthen core capacities and conduct applied public health research so that emerging and reemerging diseases and conditions can be detected and stopped faster and closer to the source, thereby enhancing global health security.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8f7f007d-b7bb-48e8-88d9-43e8a9128be7", "title": "Disseminated Coccidioidomycosis", "text": "【0】Disseminated Coccidioidomycosis\n**To The Editor** : Coccidioidomycosis, an infection caused by the dimorphic fungus _Coccidioides immitis_ , is endemic in the southwestern United States, parts of Mexico, and Central and South America . Patients with _C. immitis_ infection may have chronic pneumonia, fungemia, and extrapulmonary dissemination to skin, bones, meninges, and other body sites. The clinical features of coccidioidomycosis may mimic those of melioidosis, penicilliosis marneffei, and tuberculosis, which are commonly seen in some southeastern Asian countries, including Taiwan.\n\n【1】A previously healthy, 71-year-old retired gynecologist from Taiwan, visited Los Angeles in August 2003 and traveled to the San Joaquin Valley in November 2003. He had smoked 1 package of cigarettes daily for 50 years. He noted fever 5 days before returning to Taiwan on December 1, 2003. He came to a local hospital on December 4 with a temperature of 39°C and a history of 1 month of night sweats, productive cough, and weight loss of 10 kg. Chest radiograph showed diffuse nodular lung lesions bilaterally . His leukocyte count was 16.65 x 10 9  /L (neutrophils 85.6%, lymphocytes 6.2%), and C-reactive protein was 21.5 mg/dL (reference value, <0.8 mg/dL). Empiric antimicrobial drugs (amoxicillin/clavulanic acid and ciprofloxacin) and antituberculosis therapy (isoniazid, rifampin, ethambutol, and pyrazinamide) were administered. Blood and sputum specimens were negative for bacteria; HIV antibody test results were negative, but the fever persisted. A follow-up chest film showed a left pleural effusion. The pleural effusion aspirate was exudative with 3.6 x 10 9  /L leukocytes (73% neutrophils). Computed tomographic scan of the patient’s chest showed collapse of the left lower lung with central necrosis, bilateral pleural effusions, and mediastinal lymphadenopathy. Pleural biopsy by video-assisted thoracoscopic surgery showed no evidence of malignancy, but heavy lymphoplasmacytic infiltration and chronic necrotizing granulomatous inflammation were found . On December 17, 2003, 30 mg/day prednisolone orally was prescribed for intermittent fever. Biopsy material and cultures of blood samples taken at admission grew an unidentified mold, which was also isolated from the biopsy wound. The patient was discharged afebrile from the hospital on January 20, 2004. The fever recurred, with a disturbance in consciousness on January 25, 2004. Computed tomographic scan of the brain revealed no obvious organic lesions. He was referred to our hospital on January 26, 2004.\n\n【2】After the patient was admitted, fever persisted and respiratory distress worsened rapidly. He developed severe headache, seizures, and loss of consciousness. He was transferred to the intensive care unit for aggressive management of acute respiratory distress syndrome and deterioration of renal function. Chest radiograph showed coalescence of nodular shadows and almost complete white-out of bilateral lung fields . Meropenem, antituberculosis agents, and intravenous voriconazole, 200 mg every 12 hours, were administered.\n\n【3】Both the unidentified mold, which was sent to our hospital for further identification, and a mold cultured from the previous biopsy wound at our hospital were identified as _C. immitis_ by their characteristic gross and microscopic morphotypes in standard slide cultures incubated at 28°C for 10 days. Hematoxylin and eosin staining of the biopsied tissue showed many spherules.\n\n【4】Lumbar puncture was performed on January 30, 2004, and showed an elevated opening pressure of 380 cm H 2  O and a few destructed large spherules in the cerebrospinal fluid (CSF). However, cultures of CSF were negative for bacteria and fungi. After the diagnosis of disseminated coccidioidomycosis (pneumonia, fungemia, and meningitis), voriconazole was replaced by intravenous fluconazole, 400 mg/day. The patient’s intensive care course was complicated by _Pseudomonas_ pneumonia and repeated episodes of upper gastrointestinal bleeding. A second lumbar puncture was conducted on February 13, 2004, and also showed an elevated opening pressure (290 cm H 2  O). Uncontrolled coccidioidomycosis meningitis was suspected, and intrathecal amphotericin B treatment was planned. Refractory shock with bradycardia developed when an intrathecal catheter was implanted. The patient did not respond to therapy and died on February 16, 2004. The MIC of fluconzole for the _C. immitis_ isolate was 48 μg/mL, and the MIC of amphotericin B was found to be 1 μg/mL for by using the Etest (ABiodisk, Solna, Sweden) according to manufacturer’s information.\n\n【5】This case is the first to be reported of disseminated coccidioidomycosis with fulminant pneumonia, fungemia, and meningitis reported from Taiwan . Review of the patient’s travel history and clinical course indicated that the _C. immitis_ was acquired in California and that the initial manifestations had begun before the patient returned to Taiwan. Coccidioidomycosis is commonly diagnosed in disease-endemic areas but frequently overlooked in disease-nonendemic areas because of a low index of suspicion among physicians. The interval from onset of symptoms to disease diagnosis was relatively long . Our patient had chills, productive cough, weight loss, and night sweats followed by fever as the initial manifestations of this infection. These symptoms had been most frequently reported in previous coccidioidomycosis cases . Radiographic scans of the patient initially showed diffuse reticular lesions, followed by pleural effusion and consolidation. This clinical course was also fully compatible with those of previously reported cases . However, the clinical manifestations of chronic pneumonia with pleural effusion, the initial partial response to steroid treatment, and the delay in recognizing the mold contributed to delayed diagnosis of this disease.\n\n【6】The isolate was not susceptible to fluconazole (MIC 48 μg/mL). Although the National Committee for Clinical Laboratory Standards does not have a standard susceptibility method and MIC breakpoint of fluconazole for defining susceptibility against _C. immitis_ . Fluconazole has been recommended as a drug of choice for treating meningal coccidioidomycosis, particularly in patients with underlying renal disease or with disease-associated renal function deterioration . Immunocompromise secondary to steroid use, as well as resistance of the isolate to fluconazole, may have contributed to treatment failure in this patient.\n\n【7】With increasing international travel, physicians should consider those diseases that are endemic in regions where their patients have traveled. In addition to tuberculosis, melioidosis, and penicilliosis marneffii, coccidioidomycosis should be included in the differential diagnosis of chronic pneumonia in Taiwan, considering the number of residents who travel. Only then can prompt microbial investigations be conducted to accurately diagnosis and determine the appropriate antifungal treatment.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ce1cbf21-4daf-4328-b421-763e55716e52", "title": "The Impact of New York City’s Health Bucks Program on Electronic Benefit Transfer Spending at Farmers Markets, 2006–2009", "text": "【0】The Impact of New York City’s Health Bucks Program on Electronic Benefit Transfer Spending at Farmers Markets, 2006–2009\nAbstract\n--------\n\n【1】**Introduction**  \nIncreasing the accessibility and affordability of fresh produce is an important strategy for municipalities combatting obesity and related health conditions. Farmers markets offer a promising venue for intervention in urban settings, and in recent years, an increasing number of programs have provided financial incentives to Supplemental Nutrition Assistance Program (SNAP) recipients. However, few studies have explored the impact of these programs on use of SNAP benefits at farmers markets.\n\n【2】**Methods**  \nNew York City’s Health Bucks Program provides SNAP recipients with a $2 coupon for every $5 spent using SNAP benefits at participating farmers markets. We analyzed approximately 4 years of electronic benefit transfer (EBT) sales data, from July 2006 through November 2009, to develop a preliminary assessment of the effect of the Health Bucks Program on EBT spending at participating markets.\n\n【3】**Results**  \nFarmers markets that offered Health Bucks coupons to SNAP recipients averaged higher daily EBT sales than markets without the incentive ($383.07, 95% confidence interval \\[CI\\], 333.1–433.1, vs $273.97, 95% CI, 243.4–304.5, _P_ < 0.001) following the introduction of a direct point-of-purchase incentive. Multivariate analysis indicated this difference remained after adjusting for the year the market was held and the neighborhood poverty level.\n\n【4】**Conclusion**  \nWhen a $2 financial incentive was distributed with EBT, use of SNAP benefits increased at participating New York City farmers markets. We encourage other urban jurisdictions to consider adapting the Health Bucks Program to encourage low-income shoppers to purchase fresh produce as one potential strategy in a comprehensive approach to increasing healthful food access and affordability in low-income neighborhoods.\n\n【5】Introduction\n------------\n\n【6】Increasing access to fresh fruits and vegetables in low-income neighborhoods and promoting consumption of these foods are important strategies for reducing the risk of heart disease, stroke, type 2 diabetes, and cancer . However, in low-income communities, limited availability and high prices present obstacles to the purchase and consumption of fresh produce . In New York City, 2009 data showed that 17.2% of residents in low-income neighborhoods reported eating no fruits and vegetables on the preceding day, compared with 8.0% of residents in high-income neighborhoods ( _P_ < .001) . Farmers markets, which are mobile and can be located throughout urban neighborhoods, offer a promising venue for intervention to decrease this disparity .\n\n【7】In 2005, the New York City Department of Health and Mental Hygiene (DOHMH) introduced Health Bucks, a coupon-distribution program providing financial incentives for low-income New Yorkers to shop at farmers markets in the city’s highest poverty areas. Two-dollar Health Bucks coupons were given to community-based organizations for distribution to residents for use at 11 participating markets during the annual growing season (July 1–November 15). In 2006, the DOHMH expanded Health Bucks to encourage recipients of Supplemental Nutrition Assistance Program (SNAP) benefits to use electronic benefit transfer (EBT) wireless terminals at farmers markets to purchase fruits and vegetables. EBT is the mechanism through which New York State delivers cash and SNAP benefits (formerly known as food stamps) to the state’s recipients. Funds are deposited into the accounts of individual recipients and made accessible to them via state-issued SNAP benefits cards . In 2006 when DOHMH expanded Health Bucks, it gave SNAP recipients at some markets a $2 coupon for every $5 in EBT credits spent. The objective of this study was to examine the program’s effect on mean EBT sales and to determine via a preliminary assessment whether Health Bucks increased EBT spending in a sample of NYC farmers markets.\n\n【8】Methods\n-------\n\n【9】Greenmarket , the largest outdoor urban farmers market network in the United States, manages the subset of the markets in New York City that participate in the Health Bucks program. Greenmarket has been on the forefront of national efforts to encourage SNAP spending at farmers markets and has installed EBT wireless terminals at many of the markets that are members of its network. As standard practice, Greenmarket records EBT sales at markets that accept SNAP benefits.\n\n【10】Greenmarket provided DOHMH with records of daily EBT sales for each individual market on each day of operation from July 2006 through November 2009. Data fields included the name and address of the market, the market date, and the total value of EBT sales in the market on that date. Altogether, Greenmarket provided EBT sales data for 1,289 market days (ie, each day of operation for each individual market) over the study period. Thirteen market days were excluded because their EBT sales were not attributed to a specific market, and an additional 15 days were excluded because records reflected EBT sales that took place at special events rather than ongoing farmers markets. Because Health Bucks are offered only during the growing season, defined as July 1 through November 15, days outside these dates were excluded (193 days). After these exclusions, our sample included EBT sales from 24 markets, operating for 45 days on average, for a total of 1,068 market days. Markets included in the final sample spanned all 5 boroughs of New York City, and 18 of the markets were located in high-poverty neighborhoods (≥20% of residents earned less than 200% of the federal poverty level \\[FPL\\]). Eleven of the 24 markets in the sample did not accept Health Bucks during the study period. Four markets had EBT sales data before and after they began accepting Health Bucks, and the remaining 9 markets had sales data available after the market began accepting Health Bucks coupons. The number of markets represented each year varied; our data included 8 markets in years 2006 and 2007, 15 in 2008, and 23 in 2009.\n\n【11】Independent _t_ tests were used to assess differences in mean daily EBT sales among farmers markets. First, for each year of the study period, we compared differences in mean daily EBT revenue among markets that participated in the Health Bucks program and those that did not. Next, we examined a subset of markets for which EBT data were available both before and after Health Bucks coupons were offered. These markets each had a different number of market days before and after the introduction of Health Bucks because of variation in market days from year to year. Also, for one market, there was only one year of data before Health Bucks coupons were introduced compared with 3 years of data after introduction. Because sales before and after Health Bucks coupons introduction at individual markets would be correlated with each other, we used a linear mixed model to account for this dependency while assessing whether adding Health Bucks coupons as an incentive increased average daily EBT sales at these markets. Finally, we used a linear regression model to examine the impact of Health Bucks coupons on mean EBT market revenue. We adjusted for market year, because annual increases in funding of Health Bucks would be expected to cause shifts in related EBT sales each year. We also adjusted for neighborhood poverty level, because EBT use would be expected to vary with neighborhood poverty levels. FPL thresholds were used to define the neighborhood poverty level; a high-poverty neighborhood was defined as one in which 20% of residents or more earned less than 200% of FPL, a determination consistent with DOHMH standard practice. For all significance testing, the α level was set at _P_ < .05. We used SPSS for Windows version 18.0 (SPSS Inc, Chicago, Illinois) to conduct all analyses.\n\n【12】Results\n-------\n\n【13】The number of farmers markets accepting SNAP benefits via EBT terminals increased from 8 in 2006 to 23 in 2009. Average daily per-market EBT sales among all markets accepting SNAP benefits rose from $114.55 in 2006 to $465.87 in 2009. Among these markets, participation in the Health Bucks Program also expanded over time. In 2006, 5 Greenmarket markets participated in Health Bucks; this total grew to 12 participating Greenmarket markets in 2009 .\n\n【14】During the first 2 years of the study period, markets that participated in the Health Bucks Program had lower average daily EBT sales than nonparticipating markets . In 2006, average daily EBT sales in participating markets were $94.64 (95% confidence interval \\[CI\\], 71.0–118.3), compared with $161.00 (95% CI, 119.2–202.8) among nonparticipating markets ( _P_ \\= .004); in 2007, participating markets averaged $109.15 (95% CI, 81.8–136.5) in daily EBT sales, compared with $381.28 (95% CI 316.4–446.2) among nonparticipating markets ( _P_ < .001). However, this balance shifted in the later years of analysis. In 2008, when the SNAP incentive expanded and the majority of coupon distribution shifted to EBT, participating markets documented $397.17 (95% CI, 310.5–483.8) in average daily EBT revenues, compared with $211.85 (95% CI, 161.0–262.7) at nonparticipating markets ( _P_ < .001); in 2009, participating markets showed nearly double the average daily EBT revenues of nonparticipating markets ($595.73, 95% CI, 495.4–696.1, vs $301.19, 95% CI, 253.9–348.4, _P_ < 0.001).\n\n【15】To examine potential confounders, our linear regression model adjusted for the income level of the neighborhood in which the markets were located and the year in which data were collected. We found that after controlling for these factors, markets participating in Health Bucks averaged $170.79 more in daily EBT sales than nonparticipating markets (95% CI, 102.4–239.1, _P_ < .001).\n\n【16】To account for variation in market size, types of vendors, and operational hours that may have affected our comparison of EBT sales data across markets, we examined a subset of farmers markets for which EBT data were available both before and after Health Bucks were offered. We compared sales data within each individual market before and after the Health Bucks EBT incentive was introduced and found significant increases in EBT sales after introduction of the Health Bucks incentive in Market A ($340.67, 95% CI, 270.1–411.3 vs $1,607.88, 95% CI, 1341.30–1874.50, _P_ < .001) and Market B ($18.00, 95% CI, 1.1–34.9 vs $66.53, 95% CI, 50.2–82.9, _P_ \\= .041). Figure 2 displays average daily EBT sales for each market before and after introduction of Health Bucks.\n\n【17】Discussion\n----------\n\n【18】In this preliminary study of the impact of Health Bucks on EBT spending at NYC farmer’s markets, we found that offering a direct financial incentive to SNAP recipients using EBT at urban farmers markets in 3 low-income New York City neighborhoods was associated with significant increases in EBT sales at those markets. Among markets accepting EBT in 2008 and 2009, the average daily EBT sales at participating markets were nearly double the sales of markets that did not offer the incentive. After controlling for neighborhood income level and the year in which data were collected, increases in sales figures remained significant. Additionally, we compared sales data from 4 markets for which EBT data were available before and after the Health Bucks EBT incentive was introduced and found that the introduction of Health Bucks resulted in significant increases in EBT revenue in 2 of the 4 markets.\n\n【19】These preliminary results suggest Health Bucks could be a useful program model for the delivery of financial incentives that encourage low-income shoppers to visit farmers markets in neighborhoods where the availability of affordable, high quality produce in the retail environment may be limited . Our findings merit further exploration, especially considering research that suggests the consumption of fruits and vegetables may be higher among low-income people and SNAP recipients who shop in farmers markets . Studies have also documented high levels of coupon use in farmers markets among low-income older adults  and among participants in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) Farmers Market Nutrition Program . However, if more jurisdictions begin to provide these types of financial incentives, then developing successful strategies for distributing them will be essential. Health Bucks coupons offer a distribution model that should be further investigated for potential application in other settings seeking to encourage SNAP spending at farmers markets.\n\n【20】In 2008, several years after the launch of Health Bucks and midway through our study period, the New York City Human Resources Administration provided substantial financial support for the EBT incentive. This additional funding increased the number of coupons distributed from almost 20,000 in 2007 to more than 200,000 in 2008. Health Bucks was further expanded as part of the Centers for Disease Control and Prevention (CDC’s) Communities Putting Prevention to Work program , which funded grants to markets to hire market managers to operate the required EBT terminals. Health Bucks is now one of the largest farmers market financial programs in the United States, and our study’s findings provide evidence suggesting the effectiveness of encouraging EBT spending at farmers markets via coupons for fresh produce.\n\n【21】This preliminary study has limitations. First, although our findings suggest that SNAP beneficiaries spent more at markets after the Health Bucks incentive was implemented, we cannot be certain that those individuals were buying more fruits and vegetables because farmers markets sell foods and goods other than fresh produce. Second, we acknowledge that the provision of the Health Bucks incentives may have shifted SNAP spending on fruits and vegetables from other food outlets to farmers markets, rather than increasing SNAP spending on produce overall. Alternatively, SNAP recipients may have shifted their spending from farmers markets that did not offer the Health Bucks incentive to markets that did. However, a recent study examining the effects of a subsidy for fresh fruits and vegetables for postpartum women who are beneficiaries of WIC programs indicated that incentive programs did increase purchasing and consumption of fresh produce, particularly for WIC recipients who received coupons for produce at farmers markets . This finding leads us to suspect that Health Bucks may have had the same effect, indicating the importance of further research to explore this possibility. Third, Greenmarket’s data do not allow us to control for variations in size, vendor type, and operational hours in our analyses, and markets included in our analyses varied across years. These differences may have affected EBT sales; however, our internal comparison analysis of mean EBT sales in the same markets before and after the incentive aimed to address this limitation by comparing sales within the same market, where these factors would be relatively constant. Fourth, although the total number of days of sales data for this study was substantial , only 24 markets are represented in this study, and the sample sizes were limited, particularly for the analysis within the same markets. Further studies are needed to determine whether these are reliable effects. Finally, our data are limited to a nonrandom sample of participating Health Bucks markets that are members of the Greenmarket network and for which daily mean EBT sales data are available.\n\n【22】Future studies should address these limitations by exploring in greater detail how Health Bucks affects EBT spending in farmers markets citywide. One avenue for exploration would be to determine how Greenmarket’s data capture can be improved to allow for finer analyses of market characteristics and locations that may affect EBT sales. Second, studies should be designed to assess whether increased EBT spending at farmers markets is directly linked to the purchase of fresh produce at farmers markets in low-income neighborhoods. One way to accomplish this goal would be to track how SNAP recipients spend the financial incentives they receive through Health Bucks. Third, if purchases of fresh fruits and vegetables do increase, then we must also investigate whether increased spending on fresh produce leads to greater consumption of fruits and vegetables among this low-income population; we note that a link between the purchase of fresh produce and its consumption has not yet been reliably established. Finally, by providing SNAP recipients with $2 coupons for every $5 spent via EBT, Health Bucks increased SNAP recipients’ purchasing power by 40% (calculated as 40% of $5 = $2). However, programs across the country provide varying financial incentives, including many with a dollar-per-dollar match . Given the increasingly limited funding available for these types of programs, further research should examine the incentive level required to change SNAP purchasing patterns.\n\n【23】Because US health care costs for obesity and its associated health consequences reach $147 billion per year , municipalities are seeking to identify strategies that encourage healthier behaviors, particularly among populations at greatest risk. Improving the local food environment in low-income neighborhoods is a key avenue for exploration . We urge other urban jurisdictions to consider adapting the Health Bucks program where feasible to encourage low-income shoppers to purchase fresh produce as one potential strategy in a comprehensive approach to increasing access to and affordability of healthful food in low-income neighborhoods.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a140d01-8945-4dae-acb3-3b9e02d340e9", "title": "Changes Among Mexican Adults in Physical Activity and Screen Time During the COVID-19 Lockdown Period and Association With Symptoms of Depression, Anxiety, and Stress, May 29–July 31, 2020", "text": "【0】Changes Among Mexican Adults in Physical Activity and Screen Time During the COVID-19 Lockdown Period and Association With Symptoms of Depression, Anxiety, and Stress, May 29–July 31, 2020\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】We examined the association between changes in physical activity and leisure screen time and mental health outcomes during the early stages of the recommended COVID-19 stay-at-home period in a national sample of Mexican adults aged 18 years or older.\n\n【3】**Methods**\n\n【4】A cross-sectional online survey conducted from May 29 through July 31, 2020, among 1,148 participants, reported time spent in physical activity and leisure screen time during a typical week before (retrospectively) and a week during the COVID-19 stay-at-home period. Mental health outcomes during this period were measured with the Depression, Anxiety and Stress Scale (DASS-21). Linear regression models were used to estimate the associations between changes in physical activity and leisure screen time and mental health outcomes by socioeconomic status (SES), adjusting for potential confounders.\n\n【5】**Results**\n\n【6】Compared with maintaining high levels of physical activity or increasing them, decreasing physical activity was associated with higher stress scores overall, and among people of high SES, with higher scores for DASS-21, depression, and anxiety. Among participants of low and medium SES only, increasing screen time was associated with higher DASS-21, depression, anxiety, and stress scores compared with maintaining low or decreasing leisure screen time.\n\n【7】**Conclusion**\n\n【8】Results highlight the potential protective effect of physical activity and limited leisure screen time on mental health in the context of COVID-19 stay-at-home restrictions.\n\n【9】Introduction\n------------\n\n【10】Engaging in regular physical activity and limiting sedentary time can improve mental health . However, the COVID-19 pandemic has not only changed the way in which people are physically active, but it has decreased the length of time spent being active. In high-income countries, these changes have led to an increase in symptoms of depression, anxiety, and stress . Increased screen time has also negatively affected mental health . Although information on these changes is available for high-income countries outside Latin America, little is known about how lifestyle changes resulting from the pandemic have affected populations in Latin America. Studies are needed that examine the impact of stay-at-home orders on physical activity, sedentary time, and other health outcomes, as well as on socioeconomic inequalities . In Mexico, pandemic mitigation measures began in March 2020 with initial stay-at-home recommendations and workplace closures for all nonessential activities. By April 2021, Mexico ranked among the leading countries in the world for COVID-19 deaths per capita .\n\n【11】The pandemic has had profound effects on the physical inactivity epidemic . Stay-at-home restrictions have reduced opportunities for physical activity, and social isolation has led people to spend large periods of time indoors and in small quarters, with an increase in leisure screen time (eg, using mobile devices, watching television). Both physical inactivity and high levels of screen time are known to precede negative effects on mental health .\n\n【12】Physical activity in Latin America is mainly driven by necessity (utilitarian physical activity) and by the need for social interaction (leisure-based activity), more than by health or fitness benefits . In Latin America, including Mexico, most urban residents cannot afford a motor vehicle, making active transportation the only option for most people . Active travel, especially walking and bicycling, may have positive emotional effects, in part because of social interactions . Studies suggest that among Mexican adults, travel-related physical activity is an important contributor to overall physical activity, mainly among low-income people. Meanwhile, health-motivated physical activity (ie, physical activity performed at a gymnasium) is more prevalent among high-income people . Furthermore, Latin Americans tend to choose leisure-time physical activity that is social , and use informal settings not oriented to exercise or sports (eg, shopping malls) for physical activity , probably motivated by the need to interact with others. Hence, decreases in physical activity caused by stay-at-home recommendations could be affecting the mental health of Mexican adults, both because of physiologic changes (ie, exercise releases endorphins, which in turn trigger positive feelings) and in response to sociocultural factors. Furthermore, residents of affluent neighborhoods are likely to have enough space at home to exercise, whereas those living in low-income neighborhoods may rely more on public spaces to be active. Hence, the limitations COVID-19 has imposed on the use of public places may affect low-income residents to a greater extent.\n\n【13】Our study’s objective was to evaluate the association between changes in levels of physical activity and leisure screen time and symptoms of depression, anxiety, and stress among adults in Mexico during the early stages of the COVID-19 stay-at-home recommendation. We also explored how these associations differed by socioeconomic status (SES).\n\n【14】Methods\n-------\n\n【15】### Study design\n\n【16】We conducted an online survey to assess self-reported physical activity, leisure screen time, and mental health before and during the COVID-19 stay-at-home period in Mexico among a non-probabilistic sample of adult (≥18 y) internet users. In Mexico, 72% of the Mexican population has access to the internet, most users are aged 18 to 40 years, and most are in the middle and high socioeconomic levels . Data were collected from May 29 (2 months after the beginning of the COVID-19 stay-at-home recommendations in Mexico) through July 31, 2020, when the first wave of COVID-19 was increasing but had not yet plateaued. The study was reviewed and approved by the Research and Ethics Commissions at the Mexican National Institute of Public Health .\n\n【17】### Participants and selection process\n\n【18】Participants were recruited through the web pages and social media sites of 2 organizations, the National Institute of Public Health and the Latin American Congress for Physical Activity and Health Research. Two Facebook advertisements were placed to increase the number of people reached across all regions in Mexico. Participants provided informed consent online before completing the survey.\n\n【19】### Changes in physical activity\n\n【20】We used a questionnaire to assess self-reported physical activity among people we hypothesized to be affected by stay-at-home restrictions before and during the pandemic. The questionnaire was based on selected items from the self-administered past-week Modifiable Activity Questionnaire (SMAQ)  and was culturally and linguistically adapted for Mexico. Participants self-reported the frequency and duration of the following leisure-time activities: walking, bicycling, running, online physical activity classes, aerobic exercises (treadmill, stationary bicycle, stair climber) at home or at a gym, muscle-strengthening activities (eg, weightlifting) at home or at a gym, sports (eg, tennis, basketball, soccer), active videogames (eg, Wii Dance games), swimming or other water activities, and transport-related physical activities (ie, walking and bicycling), before and during the time that a typical week of the stay-at-home recommendation was in effect. We derived activity-specific and overall leisure-time minutes per week. Participants were classified as meeting the physical activity recommendations of the World Health Organization  if they reported engaging in at least 150 minutes per week of moderate to vigorous physical activity, both before and during the stay-at-home recommendation period. Next, participants were classified into groups according to changes: decreasing physical activity (meeting recommendations before but not during the stay-at-home period), maintaining inactive status (not meeting recommendations before nor during the stay-at-home period), and maintaining or increasing physical activity (meeting recommendations during the stay-at-home period). This last category considered participants who maintained or increased their physical activity, based on the low percentage in the increasing physical activity group.\n\n【21】### Changes in leisure screen time\n\n【22】A similar survey approach was used to measure leisure screen time. Participants self-reported the frequency and duration of leisure screen time during a typical day, before and during the recommended stay-at-home period. On the basis of previous studies , participants were classified as engaging in less than 2 hours or more than 2 hours per day of leisure screen time, both at baseline and during the stay-at-home period. Participants were then classified into groups according to changes in screen time: increased screen time (<2 h/d before and ≥2 h/d during the stay-at-home period), maintained high screen time (≥2 h/d before and during the stay-at-home period), or maintained low or decreasing screen time (<2 h/d during the stay-at-home period).\n\n【23】### Depression, anxiety, and stress symptoms\n\n【24】We used the Spanish version of the 21-item Depression, Anxiety and Stress Scale (DASS-21) , a set of 3 self-report subscales designed to measure symptoms of depression, anxiety, and stress over the past week. Each 7-item subscale is rated on a 4-point Likert scale ranging from 0 (Did not apply to me at all) to 3 (Applied to me very much or most of the time). Subscale scores are calculated by adding the numbers associated with response options for 7 subscale items (range 0–21). The total score was calculated by adding the totals of the 3 subscale scores, with higher scores representing greater symptoms of depression, anxiety, or stress. We also identified the presence of severe and very severe symptoms of depression, anxiety, or stress by using the cutoff points recommended for DASS-21 . DASS-21 has shown adequate psychometric properties of validity, internal consistency, test–retest reliability, and construct validity in various countries and cultures  and has been used to assess mental health around the globe during the COVID-19 pandemic .\n\n【25】### Sociodemographic characteristics\n\n【26】We collected information on age, sex, education completed, and marital status. Household SES was measured with a validated questionnaire, and participants were categorized as either high SES or middle or low SES . Other household characteristics were the presence of children aged 6 years or younger and the square meters of living space per inhabitant (<22 m 2  , 22–38 m 2  , ≥38 m 2  ). We also explored the amount of time per day in home confinement (≤19 hr/d or >19 hr/d) and changes in the participant’s work during the pandemic.\n\n【27】### Data analysis\n\n【28】The study sample size was powered to examine differences in the proportion of adults meeting physical activity recommendations or spending less than 2 hours per day in leisure screen time. We estimated that with a sample size of 528 participants, our study had an estimated 80% power to detect a 5% difference in this proportion.\n\n【29】Means and SDs and proportions were used to describe time spent in physical activity and leisure screen time before and during the stay-at-home period. The difference between these 2 times was calculated to estimate changes in physical activity and leisure screen time from before to during the stay-at-home period.\n\n【30】We performed linear regression models to analyze the associations between physical activity changes and mental health outcomes during the pandemic. Separate models were used for the DASS-21 score and its 3 subscales. Robust SE estimates were used because score distributions of DASS-21 and its subscales were right-skewed. All models were adjusted for sociodemographic characteristics and changes in leisure screen time or physical activity (ie, models exploring the association between physical activity changes were adjusted for leisure screen time, and vice versa). Additionally, because exploratory analyses indicated differences by SES in the association between outcomes and exposures, overall sample as well as stratified models were analyzed by SES. All data preparation and analyses were conducted with Stata 15.0 (StataCorp LLC). Differences were considered significant at _P_ < .05.\n\n【31】Results\n-------\n\n【32】In total, 2,096 adults accessed the questionnaire link and consented to participate, and 1,619 (77.0%) completed the survey. After eliminating participants with missing information for any of the analytical variables (n = 471), a total of 1,148 participants were included in the final analysis. Most were women (77.6%) with no children aged 6 years or younger living at home (87.3%). Most were confined to home (78.9%). About half were classified as low or middle SES (55.8%), single (51.3%), with an undergraduate degree (42.2%), and with no changes in their job during the COVID-19 pandemic (52.0%). A total of 35.3% were aged 18 to 29 . During the stay-at-home period, from 10% to 12% had severe or very severe symptoms of depression, anxiety, or stress . Differences across SES were observed for sex, age group, education level, having children aged 6 years or younger, marital status, living area per person in the household, and the average of the depression score ( _P_ < .05).\n\n【33】### Changes in physical activity and leisure screen time\n\n【34】During the early stages of the stay-at-home period, total physical activity decreased about 23%, whereas total leisure screen time increased about 70% ( _P_ < .01 for both) . Before the start of the stay-at-home period, 8 in 10 participants met physical activity recommendations, and 3 in 10 had <2 hours per day of leisure screen time. During the stay-at-home period, 7 in 10 participants met physical activity guidelines and 1 in 10 spent less than 2 hours per day in leisure screen time. Decreases in most activity types were reported, with the largest ones for walking for leisure or for transport and aerobic or strength activities at a gymnasium. In contrast, increases in online physical activities, playing active videogames, and aerobic and strength activities at home were observed.\n\n【35】### Changes in physical activity and mental health during the stay-at-home period\n\n【36】We found evidence of effect modification by SES in relation to physical activity and mental health . Among participants of low and medium SES, those in the decreasing physical activity group had higher stress scores (β = 2.36; 95% CI, 0.68–4.04) compared with those in the group that maintained active or increased physical activity. Compared with those in the group that maintained active or increasing physical activity, respondents in the group that maintained inactive physical activity status had higher scores for most mental health outcomes except for anxiety: (DASS-21: β = 4.41, 95% CI, 0.94–7.88; depression: β = 3.48, 95% CI, 0.60–6.35; stress: β = 3.36, 95% CI, 0.66–6.06).\n\n【37】Among participants of high SES, those in the group with decreasing physical activity had higher scores (ie, worse mental health) for all mental health outcomes: (DASS-21: β = 6.74, 95% CI, 3.57–9.90; depression: β = 4.20, 95% CI, 1.93–6.46; anxiety: β = 4.66, 95% CI, 2.54–6.77; stress: β = 4.62, 95% CI, 2.19–7.05) compared with those from the group that maintained active or increasing physical activity.\n\n【38】### Changes in leisure screen time and mental health during the COVID-19 pandemic\n\n【39】We found evidence of effect modification by SES in relation to screen time and mental health . Among low or medium SES participants, increasing or maintaining high levels of screen time during the pandemic was significantly associated with higher DASS-21 scores. For the group increasing screen time, values were β = 6.82; 95% CI, 4.13–9.52; values for the group maintaining high levels of screen time were β = 4.69; 95% CI, 2.74–6.63. For depressive symptoms, values for the group increasing screen time were β = 4.92; 95% CI, 2.95–6.89; for the group maintaining high levels of screen time, values were β = 2.98; 95% CI, 1.63–4.33. For anxiety, values for the group increasing screen time were β = 3.94; 95% CI: 2.00–5.88; for those maintaining high levels of screen time, values were β = 2.82; 95% CI: 1.39–4.26. For stress, values for the group increasing screen time were β = 4.80; 95% CI, 2.61–6.98; for the group maintaining high levels of screen time, values were β = 3.57; 95% CI, 1.88–5.26. All associations, with the exception of the group that maintained high levels of screen time (β = 1.94; 95% CI, 0.17–3.72), were nonsignificant for respondents of high SES.\n\n【40】Discussion\n----------\n\n【41】Our findings support the inverse association between physical activity and symptoms of anxiety, stress, and depression during the early stages of the COVID-19 pandemic. Conversely, results indicate an association between increased leisure screen time and worse mental health outcomes. These associations were modified by SES level.\n\n【42】Physical activity is a well-known behavior for supporting mental health . We found that participants who maintained or increased their physical activity during the stay-at-home period had lower levels of anxiety, stress, and depression than those who did not, echoing studies suggesting that physical activity is a relevant behavior when addressing mental health outcomes in the context of the pandemic . Results of our study also suggest that, among Mexican adults, the mechanisms by which changes in physical activity during confinement to home may be associated with mental health outcomes vary by SES. For instance, decreases in physical activity levels during the pandemic appear to be associated with mental health among Mexicans of both high and low or medium SES. Among those of low or medium SES, the association between decreasing levels of physical activity and mental health was mainly due to high levels of stress, with no observable associations with depression or anxiety symptoms. Physical activity has been linked to a reduced psychological responsiveness to physical and psychosocial stressors, resulting in psychological and physiological benefits that enable people to cope with stress more effectively . Secular changes in social interaction may also help explain this association. Studies in Mexico suggest that having low SES is associated with high levels of travel-related physical activity and the use of public spaces (ie, streets and parks) and places that are not exercise- or sports-based for physical activity (eg, shopping malls) . These activities and spaces are known to provide opportunities for social interaction . Decreases in physical activity among low-income groups may also substantially reduce social interaction. Limited access to public places for physical activity may also account for these associations. A recent qualitative study among Mexico City residents reported that although the use of urban green spaces decreased among all participants during the COVID-19 pandemic, those in low-income groups were more affected because they avoided using public transportation, their main transportation mode, to get to these places . As proposed by others, the social context of physical activity may be important in reducing the risk of poor mental health .\n\n【43】In contrast, and opposite to our initial hypothesis, decreases in physical activity levels were consistently and strongly related to higher anxiety, stress, and depression scores among high-SES participants compared with low- and middle-SES participants. Given the more health-driven nature of participation in physical activity among people of high SES , those people possibly may be more susceptible to declines in mental health outcomes in response to sudden decreases in their physical activity levels. Also, increased anxiety, depression, and stress possibly contributed to reduced physical activity. Overall, findings indicate that the COVID-19 pandemic may have limited the use of physical activity as a mental health coping mechanism for many. This underscores the need to ensure accessibility to different physical activity opportunities for all, even in times of crisis. Government responses to the COVID-19 pandemic play a major role in providing these opportunities. Optimizing public spaces for physical activity (eg, new pedestrian and cycling infrastructure) may be useful for increasing physical activity and reducing mental health inequalities during the pandemic. Internet-based cognitive behavior therapy and physical activity may also be effective treatment alternatives for those with mildly to moderately poor mental health . It must be noted that the lack of control that people have over lockdown orders, which restricted their access to public open spaces, and how these lockdown orders had differential effects across people of different SES levels, can in itself be an important determinant of mental and physical health. Future studies should explore the role that lack of control over major, top-down policy and environmental changes may have had on physical and mental health during the pandemic.\n\n【44】Conversely, results of our study showed that increased leisure screen time was associated with increased symptoms of stress, anxiety, and depression during the stay-at-home period, as suggested by a review study identifying excessive sedentary time as a risk factor for depression in adults . Findings from our study also indicate that this association differs across socioeconomic levels, with consistent associations between increased screen time during the stay-at-home period and higher depression, anxiety, and stress scores among low- and medium-SES participants, but not among their peers with high SES. In the context of the COVID-19 pandemic, increases in leisure screen time may be due to various reasons, including the need for more information about lockdown procedures and disease treatment or outcomes, the need to connect with others, or internet addiction . Also, participants of low SES possibly engaged in more mentally passive sedentary behaviors (ie, with no mental activity requirements), like watching television, than high-SES participants did, as suggested by a previous study . Mentally passive sedentary behaviors have been associated with a higher risk of depression, whereas mentally active sedentary behaviors, such as reading, may protect against the onset of depression . Although we asked participants to report their total leisure screen time, we were unable to capture the nature of their sedentary activities. Further studies investigating the effects of different types of sedentary behaviors on mental health are needed. In a broader sense, results of our study align with those suggesting that the pandemic has exacerbated social and health inequalities ; in light of this, policy makers and decision makers at local, provincial, and national levels need to recognize the importance of physical activity and screen time for mental well-being and to introduce regulatory responses, even when mobility restrictions are involved. In our study leisure screen time increased across categories of physical activity changes, suggesting that increases in screen time are widespread and may be independent of active behaviors. Thus, strategies aiming to mitigate pandemic–related mental health effects should include strategies addressing both physical activity and screen time. In line with recent guidelines , interventions aiming to replace sedentary behaviors with physical activity of any intensity have shown promising results for promoting good mental health .\n\n【45】Our study provides original data from adults across Mexico and to our knowledge is the first study focused on the association between changes in physical activity and leisure screen time on mental health during the COVID-19 pandemic stay-at-home period across a large sample of Latin American adults. Changes in these behaviors threaten the physical and mental health of Mexican adults, especially because of the uncertainty of the length of social isolation required to prevent the spread of COVID-19. Knowledge gained from our study may inform interventions aimed at improving the mental well-being of adults in Mexico or other countries that share similar socio-contextual conditions.\n\n【46】Our study had several limitations. First, although participants were asked to retrospectively report their physical activity and screen time before and during the pandemic, the cross-sectional design of the survey precludes inferring causality. The reported associations are possibly partly explained by reverse causality. Furthermore, data were collected at the initial stages of the COVID-19 pandemic in Mexico. Future studies should address variations in the measured behaviors for longer periods of time or at different time points of the stay-at-home period, which might differ from findings reported here. The stay-at-home period itself limited the possibility of using device-based measures of physical activity, which provide more accurate estimates of both physical activity and sedentary behavior . We used a self-reported questionnaire designed to measure the underlying continuity of severity of psychiatric symptoms, which is not intended to establish a psychiatric diagnosis as would many clinical measures that may be considered diagnostic “gold standards” (eg, structured clinical interview and functional neuroimaging) . Similarly, our conclusions are based on self-reports, which are prone to overestimation, especially among people with severe mental illness . Additionally, measures included a non-exhaustive list of physical activity types, and although they are based on a previously validated instrument (SMAQ) , the new version has not been validated for use in Mexico. Characteristics of the study sample resemble those of internet users in Mexico . Compared with national estimates, participants had a similar prevalence of physical inactivity and depressive symptoms . However, because of the nature of the online survey and the fact that ours was a nonprobabilistic sample, participants had a higher socioeconomic status and education level than the general Mexican population . Thus, the representativeness of our study is limited to the analytic sample. Despite this, our study uncovered possible differential effects of movement behaviors on mental health among adults from different socioeconomic backgrounds that merit further investigation. Finally, our recruitment strategy may have precluded our including people with severe and very severe depression, anxiety, or stress.\n\n【47】Results of our study highlight the favorable effects of physical activity and limited leisure screen time on mental health well-being in the context of COVID-19 stay-at-home restrictions. Customized interventions to reduce sedentary behaviors and promote physical activity could improve the mental health of adults during similar stay-at-home restrictions. Concerted efforts should be prioritized among people living in disadvantaged socioeconomic conditions with limited access to leisure physical activity opportunities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f1ebe76f-6591-40fc-a444-1687d5e67ac6", "title": "Reanalyzing the 1900–1920 Sleeping Sickness Epidemic in Uganda", "text": "【0】Reanalyzing the 1900–1920 Sleeping Sickness Epidemic in Uganda\nUganda is affected by gambian sleeping sickness, which is caused by infection with _T. b. gambiense,_ and rhodesian sleeping sickness, which is caused by _Trypanosoma brucei rhodesiense_ . _T.b. rhodesiense_ occurs in the east, whereas _T.b. gambiense_ occurs in the northwest of the country. From 1900 to 1920, the Busoga region of Uganda experienced a large-scale epidemic of the disease, during which an estimated 250,000 people , a third of the population of the region, died . It is believed that the species of parasite responsible for this first documented epidemic in Uganda was _T.b. gambiense_ and that _T.b. rhodesiense_ was introduced there in the 1940s when another, smaller epidemic was identified in the same region. However, this idea has been the subject of some debate .\n\n【1】The first published description of sleeping sickness cases in the 1900–1920 epidemic was made by A.R. and J.H. Cook in their Church Missionary Society (CMS) Hospital at Mengo on February 11, 1901 . The extent of the epidemic became clear as the number of case-patients seen at that hospital increased and as the disease was identified around the northern shore of Lake Victoria . The study of sleeping sickness at the time focused on discovering the causative agent; being newly recognized in Uganda, the disease had not been previously described in eastern Africa. The discovery of trypanosomes as a disease agent and their mode of transmission was relatively recent. Bruce  described transmission of cattle trypanosomes by tsetse flies in Zululand in 1895; the causative agent of “Trypanosoma fever” in the Gambia had been indicated in 1902 by Forde  and described by Dutton  as _Trypanosoma gambiense_ (now classified as _T. b. gambiense_ ). At the time of the Ugandan outbreak, Manson  believed that the disease was linked to _Filaria perstans_ (now known as _Mansonella perstans_ ), a blood-dwelling nematode of no clinical importance. Early efforts in Ugandan patients infected with sleeping sickness focused on detection of _F. perstans_ . _T.b. rhodesiense_ was described for the first time (in present day Zambia) in 1910 , and _T.b. rhodesiense_ was confirmed in Uganda during the next major epidemic 30 years later .\n\n【2】### A New Disease?\n\n【3】It is not known how long sleeping sickness may have existed in Uganda before 1900. Most of the pioneering scientists assumed it was a new problem to the region because they found no evidence that it had occurred there before, and they assumed that the disease always occurred as an epidemic. However, Christy  noted that sleeping sickness had probably been present long before it was first documented and that it probably originated in Busoga, the core of the present day _T.b. rhodesiense_ focus. Similarly, on reviewing the available evidence, Duke  states that “some form of human trypanosomiasis” had occurred around the Ugandan shores of Lake Victoria prior to the epidemic.\n\n【4】Confusion over the existence of sleeping sickness in this area was intertwined with the available knowledge of the causative organism. We have already seen that _T.b. gambiense_ was the only human-infective trypanosome to have been described at the time. Furthermore, sleeping sickness was recognized in many parts of central Africa . It had not been documented previously in eastern Africa, and the assumption was that the Ugandan disease was an extension of the epidemic raging westward in the (present day) Democratic Republic of Congo.\n\n【5】When the epidemic began in Uganda, Castellani  noticed two groups of distinct clinical symptoms among the patients. The infection in the first group he called Trypanosoma fever, as is was similar to the disease seen in the Gambia and ascribed to _T. gambiense_ . The second infection he called sleeping sickness, and tentatively called the trypanosome that he found in those cases _T. ugandense_ . The distinction was essentially clinical; what he called sleeping sickness was a much more virulent infection than the Trypanosoma fever caused by _T. gambiense_ . Bruce et al. described two cases of a disease “not unlike Trypanosoma fever” in two patients who had recently come to Uganda (whom he termed Nubians). The patients, a policeman and a prisoner, had arrived from the present day-Sudan, where _T.b. gambiense_ occurs today. Bruce et al. later insisted that the less acute “Trypanosoma fever” symptoms were simply the first stage of full-blown sleeping sickness caused by _T. gambiense_ .\n\n【6】If the sleeping sickness had been due to _T.b. gambiense_ , it would suggest that the parasite was imported from the west as part of large-scale human population movements that occurred at the time . Only Köerner et al. have questioned the identity of the parasite responsible for the first Ugandan epidemic. They argue that as _T.b. rhodesiense_ has occurred in stable endemic foci that can expand and cause epidemics; _T.b. rhodesiense_ was probably present in Busoga long before 1901. This is an attractive argument, as the wholesale replacement of one parasite species by another ( _T.b. gambiense_ by _T.b. rhodesiense_ ) in a region seems unlikely, and such a replacement has not been recorded in any other sleeping sickness focus. In addition, _T.b. rhodesiense_ is primarily a zoonotic parasite in which human-to-human transmission is thought to occur rarely, and human movements  in isolation from movements of the zoonotic reservoir  may not be sufficient to account for its introduction.\n\n【7】### Hypothesis\n\n【8】Here we test the hypothesis proposed by Köerner et al. that the parasite responsible for sleeping sickness in the Busoga and surrounding regions of Uganda from 1900 was _T.b. rhodesiense_ . This organism causes an acute disease, resulting in death after a period of 3–12 months , while _T.b. gambiense_ causes a chronic infection, with which a person may go about daily activities for many months or years , despite occasional and often mild symptoms. Our analysis, therefore, is based on comparison of the survival time of sleeping sickness patients estimated from clinical notes recorded during the Ugandan epidemic, with survival times of known _T. b. rhodesiense_ patients from the current disease focus in southeast Uganda, and _T. b. gambiense_ patients in western and central Africa.\n\n【9】### Methods\n\n【10】##### Archives\n\n【11】The Mengo Hospital archives (CMS Mission Hospital at Mengo), which include original patient case notes made by the Cook brothers, are held in the archives section of the Mulago Hospital at the Makerere Medical School in Kampala, Uganda. The first sleeping sickness patient recognized in Mengo was admitted on February 11, 1901 . The geographic distribution of the cases seen at this hospital extended across a wide area, although most of the patients came from the close vicinity. Many patients reporting in the latter years of the epidemic were referred to the specialist hospital run by the Royal Society Sleeping Sickness Commission in Entebbe (set up as part of the Commission’s study on the disease), or later to the sleeping sickness isolation camps on islands in Lake Victoria. Some details of the treatments prescribed and numbers of cases seen in these camps are available . At this time in the development of therapy for sleeping sickness, no effective drugs were available for the disease, so death of the patients was due to sleeping sickness and, rarely, treatment side-effects; the data included here show the true clinical course of the disease in untreated patients.\n\n【12】##### Data Collection\n\n【13】For each patient treated at the Mengo hospital and diagnosed with sleeping sickness, full details as they appeared in the archives were entered into a database, which was used for later analyses. This electronic database has been made available to the archivists in Mengo. Full details were entered for patients seen through 1910; beyond this time, many patients were turned away. In addition, the authorities were managing to bring the epidemic under control, and the number of cases was diminishing. Additional data were also extracted from the Reports of the Royal Society Sleeping Sickness Commission .\n\n【14】##### _T.b. rhodesiense_ Comparison\n\n【15】Odiit et al. published data on duration of symptoms in 30 patients who died of sleeping sickness within a week of presenting with _T.b. rhodesiense_ at the Livestock Research Institute sleeping sickness hospital in Tororo, Uganda, between 1988 and 1990.\n\n【16】##### _T.b. gambiense_ Comparison\n\n【17】Adams et al. conducted postmortems on 16 cases of fatal _T.b. gambiense_ and state that there are few fatal cases documented and that duration of illness before death is “rarely established;” this accounts for the scarcity of time-to-death data in contemporary literature. One source of _T.b. gambiense_ clinical data  presents important details of the clinical course of the disease to death but could not be used in this analysis as no estimates were made of the length of illness before treatment or death. However, Yorke  does present a summary of untreated cases between 1908 and1919 in various countries in West Africa. These countries include the French Congo (present-day Republic of Congo), the Gambia, and the Belgian Congo (present-day Democratic Republic of the Congo). Full details for each patient are not provided, and the survival times for the different groups of patients were taken as the average of the range given (e.g. five patients who were followed-up after 2–3 years were each given a survival time of 2.5 years). These data were supplemented by several more recently published case histories for _T.b. gambiense_ patients who were treated in the United States and Europe after various periods of travel in Africa ; the date of the last visit to Africa before diagnosis was taken as the infection date for these cases, and survival time was taken to the point of first treatment or death. Cases of congenital sleeping sickness from this literature were excluded.\n\n【18】##### Statistical Analyses\n\n【19】A survival analysis  was conducted to compare the time from onset of symptoms to death of the Ugandan patients in the Mengo archives and Sleeping Sickness Commission Reports  from 1901 to 1910, and known _T. b. rhodesiense_  and _T. b. gambiense_ patients . The criteria for including a patient from the Mengo archive dataset were that the case had been recorded as a sleeping sickness death following inpatient stay in the hospital, that the length of the hospital stay was recorded, and that the length of time of illness before admission had been recorded in the clinical notes. That is, an estimate of the total time of the clinical course of the illness, from onset of symptoms to death, was available. The same criteria were applied to the clinical notes appearing in the Sleeping Sickness Commission Reports . The Kaplan-Meier survival analyses were conducted in _S+_ 2000 (MathSoft, Inc. Cambridge, MA) and the survival curves were compared using the log-rank (Mantel-Cox) test.\n\n【20】### Results\n\n【21】##### Descriptive Statistics\n\n【22】From 1901 to1910, a total of 11,767 case-patients were recorded in the Mengo inpatient records. This figure excludes all patients admitted in 1902 because all the records from that year were destroyed in a fire. Just over 1% of these (204 cases) were sleeping sickness cases. The outcome of admission was biased in favor of discharge (160 cases). Five patients were referred to the Royal Society sleeping sickness hospital, and 24 deaths were recorded. Of these 24 deaths, 11  untreated patients had complete records of date of admission and death and of the duration of symptoms before admission. Sixteen cases from the Sleeping Sickness Commission Reports were included with these . All 30 cases presented by Odiit et al. were included, and 88 untreated, diagnosed case-patients were extracted from Yorke . Of these, 54 had died and 34 were still alive on follow-up (accounted for by censoring in the survival analysis). Eight contemporary _T.b. gambiense_ patients were included , all of whom survived to treatment.\n\n【23】##### Survival Curves\n\n【24】The Kaplan-Meier Survivorship curves resulting from the analysis of these data are shown in Figure 3 . The median survival times were 2 months, 4 months, and 36 months for the Tororo 1988–1990, Mengo+Sleeping Sickness Commission 1901–1910, and western African datasets, respectively.\n\n【25】##### Log-rank Test\n\n【26】The log-rank test showed no significant difference between the Ugandan 1901–1910 and 1988–1990 survival rates (χ 2  \\= 1.7; _d.f._ \\=1, _p_ \\= 0.12). Therefore, the clinical course from onset of symptoms to death in this sample of patients from the 1901–1910 epidemic in southern Uganda was not significantly different to that of patients in Tororo with _T.b. rhodesiense_ infections from 1988 to 1990. The Ugandan 1901–1910 and West African survival rates were significantly different (χ 2  \\= 184; _d.f._ \\=1, _p_ <0.001) and the Ugandan 1988–1990 and West African survival rates were also significantly different (χ 2  \\= 175; _d.f._ \\=1, _p_ <0.001). The clinical course of the disease during the Ugandan 1901–1910 and 1988–1990 epidemics is significantly shorter than the clinical course of sleeping sickness experienced by the West African _T.b. gambiense_ patients.\n\n【27】### Discussion\n\n【28】The clinical course of sleeping sickness during the period from 1901 to 1910 Ugandan epidemic does not differ from contemporary _T.b. rhodesiense_ cases in Uganda . The duration of illness from first onset of symptoms of documented _T.b. gambiense_ patients did differ significantly from both the Ugandan 1901–1910 and Ugandan 1988–1990 patients. From these finding, we conclude that the patients from the 1901–1910 epidemic in Uganda were probably infected with trypanosomes belonging to the _T.b. rhodesiense_ subspecies and that this parasite was circulating in the region at the time. The evidence is consistent with the hypothesis proposed by Köerner et al _._ . Unfortunately, no archived parasite material exists for this period, which would allow the molecular confirmation of this result by screening for the _SRA_ gene , which is specific to _T.b. rhodesiense,_ or for screening with _T.b. gambiense_ \\-specific molecular markers .\n\n【29】Given these findings, how might the observations made by Castellani , that two distinct clinical pictures were sometimes observed in Uganda at the time, be explained? The designation as “Nubians” of some of the patients suggests that these patients were from Sudan. They were certainly migrants and may well have been carrying _T.b. gambiense_ parasites before migrating that were discovered on examination in Uganda. Although local transmission of these parasites cannot be excluded, _T.b. gambiense_ was probably not responsible for the widespread deaths in the Busoga region generally. Most of the cases described by Christy  as he roamed around Uganda were of an acute disease. Hodges, who was the Medical Officer for the Uganda Protectorate, states that the time to death of Ugandan patients from realization they were sick was 3–4 months , based on a great many observations, and that the duration of illness rarely exceeded 10 months. This observation is further echoed by Low and Castellani , who state that “very chronic cases, running a course of more than a year’s duration, are very rare.” _T.b. gambiense_ , if it did exist concurrently, would probably have been limited to those areas where migrant workers for the British authorities were allowed to settle. These settlement areas were purposefully established away from tsetse-infested bush in the efforts to control the epidemic .\n\n【30】If both species of human infective trypanosome ( _T.b. gambiense_ and _T.b. rhodesiense_ ) were in Uganda during the 1901–1910 epidemic, and both were treated at the Mengo and Sleeping Sickness Commission hospitals, the data acquired from the archives for deaths would be biased towards clinical descriptions of _T.b. rhodesiense_ . The _T.b. gambiense_ data presented here demonstrate that many of the patients are treated before death. For similar reasons, _T.b. gambiense_ would be poorly represented among deaths in Mengo and the Sleeping Sickness Commission; patients would have been discharged, as the symptoms would not have been considered serious enough for them to remain in the scarce hospital beds. Also, mixed infections with both _T.b. rhodesiense_ and _T.b. gambiense_ could have occurred if _T.b. gambiense_ was being transmitted locally; however, due to the acute nature of _T.b. rhodesiense_ infections, patients infected with both parasites would most likely have been seen with _T.b. rhodesiense_ \\-like symptoms.\n\n【31】Therefore, the results of this analysis cannot exclude _T.b. gambiense_ as a cause of illness among some patients but can positively include _T.b. rhodesiense_ as a cause of sleeping sickness at the time. The epidemic is likely to have been due to _T.b. rhodesiense_ epidemic with occasional cases of _T.b. gambiense_ in patients who had migrated from _T.b. gambiense_ foci on the present northwestern border of Uganda (e.g. “Nubians”). Case number 136 from Mengo  may well be one of the occasional _T.b. gambiense_ case-patients (or the result of recall errors), as the patient reported having been ill for 24 months. This one case does not affect the outcome of the analysis, however .\n\n【32】##### Rinderpest and Cattle Restocking\n\n【33】If, as these data suggest, the 1900–1920 epidemic was due, at least in part, to _T.b. rhodesiense_ , the question arises as to its causes, and the cause of the spread of sleeping sickness through the whole of the region to previously unaffected areas. There is no doubt that the onset of the colonial administration in Uganda resulted in social changes and population mobility , which had important environmental consequences . In referring to the causation of _T.b. gambiense_ epidemics in central Africa, Lyons  blames the disruptive effects of colonization. Human movements had, however, occurred regularly throughout Africa’s history , and the situation in Uganda was doubtless more complex.\n\n【34】Some other trigger, in conjunction with these factors, likely was involved in spreading sleeping sickness from the endemic foci outwards. It has been suggested that this might have been rinderpest, an infectious disease of livestock . The start of the sleeping sickness epidemic coincides with the end of the rinderpest pandemic in cattle . In the early 1890s, and for the decade that followed, rinderpest, or cattle plague, ravaged most of Africa. Millions of cattle died from this virulent viral infection , causing sociologic and ecologic upheavals throughout the continent. Although rinderpest is often linked to the sleeping sickness outbreaks in eastern Africa, it has been possibly linked for the wrong reasons. The disease-induced cattle depopulation is generally thought to have resulted in a change in the dominant vegetation in the whole region; pasture lands reverted to bush and the distribution of tsetse-fly vector of sleeping sickness expanded . Although these ecologic changes occurred, they may not have been directly responsible for the spread of sleeping sickness.\n\n【35】The movement of cattle during livestock restocking  may be linked to the introduction of _T.b. rhodesiense_ sleeping sickness, a zoonosis with a principally domestic cattle reservoir, to previously unaffected areas that resulted in serious outbreaks of disease. With the large-scale local and regional movements of animals that occurred after the rinderpest pandemic, as animals were traded in the cattle-depopulated areas, trypanosomes may have moved with them. In conjunction with the expansion of tsetse distributions as the ecology changed, the setting was ripe for a major sleeping sickness problem. Ford  notes that in setting up the Uganda Protectorate, a great deal of cattle movements occurred, either as groups moved away from areas under British control, or in search of post-rinderpest pasture. He also points out that the culture of large-scale cattle trading was well established. Local cattle movements as part of the restocking would have been extensive. _T.b. rhodesiense_ could have spread from the core of the Busoga and other endemic foci to other tsetse-infested areas all around the northern Lake Victoria shore.\n\n【36】Recent molecular studies also challenge the conventional wisdom that _T.b. rhodesiense_ spread through East Africa and to Uganda from Zambia , where it was first described. Tilley et al. and Hide et al _._  suggest that _T.b. rhodesiense_ retains a stable genetic constitution through time and show that strains from Zambia are phylogenetically quite distinct from _T.b. rhodesiense_ in Uganda. It is therefore unlikely that the parasite spread from there to Uganda. Rather, as Köerner et al. suggested, _T.b. rhodesiense_ has probably been present in southeast Uganda, either at endemic or epidemic levels (or both at different times), for hundreds of years. The dynamics of the spread of the disease, involving cattle movements and restocking , have probably been similar since the first association of cattle and humans in tsetse-infested areas of this part of the continent.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b62eec24-f7e9-4d89-874c-fbccdc6644b7", "title": "Tetanus Vaccination", "text": "【0】Tetanus Vaccination\nHow to Pronounce Tetanus \\[MP3\\]\n\n【1】Vaccines are available that help prevent tetanus , an infection caused by _Clostridium tetani_ bacteria. Four kinds of vaccines used in the United States today help protect against tetanus, all of which also provide protection against other diseases:\n\n【2】*   Diphtheria and tetanus (DT) vaccines\n*   Diphtheria, tetanus, and pertussis (DTaP) vaccines\n*   Tetanus and diphtheria (Td) vaccines\n*   Tetanus, diphtheria, and pertussis (Tdap) vaccines\n\n【3】Babies and children younger than 7 years old receive DTaP or DT, while older children and adults receive Tdap and Td.\n\n【4】CDC recommends tetanus vaccination for all babies and children, preteens and teens, and adults. Talk with your or your child’s doctor if you have questions about tetanus vaccines.\n\n【5】What Everyone Should Know\n\n【6】Information for Healthcare Professionals\n\n【7】CDC recommends **tetanus vaccination** for:\n\n【8】*   Young children\n*   Preteens\n*   Adults\n\n【9】Related Pages\n\n【10】*   CDC’s Tetanus Website\n*   Diphtheria, Tetanus, and Whooping Cough Vaccine Information Statements\n    *   DTaP ( English / Other Languages )\n    *   Td ( English / Other Languages )\n    *   Tdap ( English / Other Languages )\n*   Photos of Tetanus Bacteria and People Affected by Tetanus  \n    Warning: Some of these photos are graphic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "649361a3-1a10-4ef2-8f90-766f29f2f187", "title": "Low Frequency of Poultry-to-Human H5N1 Transmission, Southern Cambodia, 2005", "text": "【0】Low Frequency of Poultry-to-Human H5N1 Transmission, Southern Cambodia, 2005\nFrom its identification in poultry in the People's Republic of China in 1996 and outbreak among commercial farms and live poultry markets in Hong Kong in 1997 , highly pathogenic avian influenza A (H5N1) virus has become an unprecedented epizootic and spread to domestic poultry and wild bird populations in Asia , the Middle East, Europe, and Africa . This epizootic has affected farmers and the agricultural industry, claimed human lives, and raised the specter of a global influenza pandemic, perhaps even beyond the scale of the devastating 1918 \"Spanish\" influenza pandemic .\n\n【1】In Cambodia, highly pathogenic H5N1 was first reported in poultry in January 2004 . Of 92 poultry outbreaks that year, 15 were confirmed by isolation of H5N1 viruses . During the first 4 months of 2005, 4 fatal human H5N1 cases were detected in Kampot Province, southeast Cambodia . These human cases occurred contemporaneously with unreported outbreaks of high deaths among chicken flocks throughout Kampot Province. However, H5N1 virus was confirmed in both a person and poultry in only 1 area of Kampot Province, a village in Banteay Meas District, ≈20 km from the Vietnam border and 15 km from the household of the first confirmed human H5N1 case-patient in Cambodia.\n\n【2】The patient from Banteay Meas District was a 28-year-old male farmer in whom a low-grade fever and dizziness developed on March 17, 2005. Approximately 1 week before he became sick, chickens at his home suddenly began dying. His family reported that he plucked at least 1 chicken and ate poultry that had died of illness suggestive of H5N1 disease. He may also have collected dead birds. On the third day of his illness, nonproductive cough, shortness of breath, and watery diarrhea developed. Two days later, he was transported to a Phnom Penh hospital. His condition rapidly deteriorated, and he died the next day despite mechanical ventilation and inotropic support. H5N1 virus infection was confirmed by reverse transcriptase (RT)–PCR from blood; tracheal aspirates; and nasopharynx, throat, and rectal swab specimens collected during his hospitalization .\n\n【3】The farmer's rural village provided a setting in which we could study the epidemiologic features of H5N1 virus in poultry and humans. We report results of a retrospective study of poultry deaths and an H5N1 antibody seroepidemiologic investigation among residents of this village in Banteay Meas District, Kampot Province, Cambodia.\n\n【4】### Methods\n\n【5】##### Retrospective Poultry Death Survey\n\n【6】We conducted an immediate investigation in response to notification of the confirmed human H5N1 case in Banteay Meas District. From March 25 through 27, 2005, all households located within a radius of 1 km from the H5N1 case-patient's household were mapped and positioned with a hand-held global positioning system (Garmin, Olathe, KS, USA). We collected information on illness suggestive of H5N1 among animals in each household by interviewing the head of the family with a standardized questionnaire. Households where the head of the family was not at home or could not be found were omitted.\n\n【7】A household chicken flock was considered likely to have been infected by H5N1 virus during the previous 6 months if all of the following characteristics were reported: flock death >60%, 100% case-fatality ratio, and sudden death of young and mature birds within 1 or 2 days of becoming sick. We collected sick poultry and carcasses for H5N1 virus testing. Cloacal swabs of 10 to 14 randomly selected, live, healthy poultry were also collected from each household where birds remained.\n\n【8】##### Seroepidemiologic Investigation\n\n【9】We conducted a seroepidemiologic investigation June 3–7, 2005, ≈2 months after the village reported high poultry deaths. It consisted of interviews of household members with a standardized 39-question questionnaire on demographic information and data on specific exposures to animals and the environment during the last 12 months; a 5-mL venous blood specimen was also collected from participants. Four investigation teams of 3 members each visited all households in 4 different directions, starting from the household of the confirmed human case-patient, until 300 participants were enrolled in the study. Each household was visited once, and no further attempts were made to interview absent adult household members. The sample size was estimated to have a 95% chance of detecting >1 seropositive person, if one assumes a 2% prevalence of H5N1 antibodies in the village. Written informed consent was obtained from adults or from a parent or guardian for children <18 years of age. The study was approved by the Cambodian Ethics Committee.\n\n【10】##### Laboratory Methods\n\n【11】All animal samples were placed into viral transport medium in sterile tubes in the field, kept cold, and transported daily to the National Animal Health Laboratory of the Ministry of Agriculture, Forestry and Fisheries in Phnom Penh. Cloacal specimens and organ samples from sick and dying poultry were tested for influenza A with an indirect fluorescent antibody assay. Positive results were forwarded to the virology unit of the Institut Pasteur in Cambodia for confirmation with real-time RT-PCR to detect H5 viral RNA. Human blood specimens were centrifuged, and sera were aliquoted and frozen at –80°C. Sera were shipped on dry ice to the Hong Kong Government Virus Unit Laboratory for detection of H5N1 neutralizing antibodies by microneutralization assay and confirmatory Western blot assay. Serologic evidence of H5N1 virus infection was defined as an H5N1 neutralizing antibody titer >80 with a confirmatory Western blot assay .\n\n【12】##### Statistical Analyses\n\n【13】The position codes of all surveyed households were entered into ArcGIS version 9.0 (ESRI Systems, Redlands, CA, USA). We used the space-time scan statistic to determine the cluster of households most likely to have been affected by H5N1 virus in the previous 6 months. Analysis was performed with SaTScan version 5.1.3 ; cases were assumed to follow a Poisson distribution. The space-time statistic is defined by a cylindrical window with a circular geographic base and height pertaining to time. The window is moved in space and time for each geographic location and size. We obtained several overlapping cylinders of different sizes and shapes covering the study area; each cylindrical window reflected a possible cluster. The most likely cluster is the one least likely to have occurred by chance, according to the maximum likelihood ratio test statistic. Individual and household data were entered into EpiData version 3.02 (EpiData Association, Odense, Denmark) and validated with a duplicate data file. STATA version 8.0 (StataCorp LP, College Station, TX, USA) was used for all statistical analyses. Odds ratios were estimated with bivariate logistic regression. Independent associations between demographic and behavioral data and households that were likely to be affected by H5N1 in poultry were also analyzed by logistic regression models. We accounted for the cluster effect of households with STATA's \"cluster\" option for logistic regression, which specifies that observations are independent across households but not within households. For multivariate analysis, variables with a p value <0.1 were retained in the models. Selected variables whose correlation coefficient was >0.4, which indicates colinearity between these variables, were not included in the logistic regression model.\n\n【14】These investigations were conducted as a collaborative effort between the Cambodian Ministry of Health and Ministry of Agriculture, Forestry and Fisheries, Institut Pasteur in Cambodia, the World Health Organization, Hong Kong Public Health Department, the Centers for Disease Control and Prevention, and the Food and Agricultural Organization of the United Nations.\n\n【15】### Results\n\n【16】##### Poultry Deaths in the Village\n\n【17】Of 194 households located within 1 km of the H5N1 patient's house, 163 (84%) had occupants who were home at the time of the survey. No household refused to participate.\n\n【18】Among interviewed households, 155 (95%) raised chickens (median 20, range 1–80 per household) and 52 (32%) raised ducks (median 4, range 1–50). Fifty households (31%) reared both ducks and chickens. Sixty-three households owned pigs (range 1–4 animals). From January 1 through March 26, 2005, 102 (66%) of 155 households reported deaths of chickens. Of these households, 73 (72%) recorded deaths during the last 4 weeks of this 3-month period . The median poultry flock death ratio was 90% (range 4%–100%). According to our definition, 42 households were likely to have had an outbreak of H5N1, for an overall attack rate of 27% among households with chickens. Flock death ratio was >80% in 31 of these households. The initial mean flock size in households likely to have had H5N1 in chickens was significantly larger than in households without chicken deaths (31 vs. 20, p = 0.003).\n\n【19】Eleven households with a high likelihood of H5N1 (35%) in chickens also owned ducks, although only 2 of these described simultaneous deaths of ducks: 1 reported a duck flock death ratio of 80%, the other a ratio of 100%. Seven other households reported high levels of death among duck flocks (>60%), but the number of deaths among chickens did not suggest H5N1. Overall, raising ducks with chickens was not associated with deaths in chickens (p = 0.57).\n\n【20】Cloacal swabs were collected from 28 chickens and 14 ducks. Specimens from 2 sick chickens were positive for H5N1 by RT-PCR. These chickens belonged to a household located ≈50 m from the household of the confirmed human H5N1 case-patient. The owner of the sick chickens reported that the farmer with H5N1 virus infection spent daylight hours in his compound.\n\n【21】The space-time scan statistic detected a significant cluster of 25 (60%) households with an overall relative risk of 7.9 (log likelihood ratio 34.1, p = 0.001) . The cluster was confined to the period from February 25 through March 26, with a radius of 444 m, which contained both the household of the confirmed H5N1 case-patient and the house with the 2 H5N1-infected chickens.\n\n【22】##### Seroepidemiologic Survey Findings\n\n【23】Among 93 households that were surveyed, 351 persons participated and 3 refused. An average of 4 people resided in each household, the median age was 23 years (range 1 month–81 years), and 150 (42.7%) of the sample were male; 207 (59%) were farmers of both crops and livestock. The rest of the participants were students (29.3%), had no stated occupation (18.8%), or were construction or factory workers (0.9%). Reflecting the rural setting and the common means of livelihood, ownership of animals, including poultry, was high . The number of households with chickens decreased by 17.5% (p<0.001) after the outbreak, while the number of households that possessed ducks and pigs remained similar (–15.7% and 5.7%, p = 0.43 and 0.74, respectively).\n\n【24】Because many households owned poultry or pigs, a substantial proportion of the surveyed population had regular, high-intensity contact with these animals in the 12 months before the survey; this contact included collecting, processing, and eating sick birds or birds that had recently died when H5N1 viruses were thought to be circulating among flocks in the village. Despite this finding, none of the villagers interviewed reported having a febrile or respiratory illness during the same period, and none of the 351 participants had neutralizing antibodies suggestive of H5N1 virus infection on microneutralization assay.\n\n【25】We compared exposures of residents from households with a high probability of having had an outbreak of H5N1 in their chicken flock (n = 96) with occupants from households where no chickens died (n = 166) . Bivariate analysis showed that households that purchased live poultry in the preceding year were almost 4× more likely to have had H5N1 in their flock than households that did not buy live chickens. In contrast, certain behavior by household members appeared to reduce the risk for H5N1 virus infection in their household flock by half (and the difference remained significant after controlling for poultry purchasing): cleaning cages or stalls, cleaning up poultry feathers, and handling live poultry . Slaughtering chickens was not a significant risk factor after controlling for exposures that were significant on multivariate analysis .\n\n【26】### Discussion\n\n【27】The primary finding of our investigations is that transmission of H5N1 viruses from infected poultry to humans appears to have been low in a rural Cambodian population with confirmed and suspected H5N1 poultry outbreaks, and where a human H5N1 case occurred during 2005. This finding is consistent with other studies that have described low frequency of H5N1 neutralizing antibody among healthcare workers and household contacts since 2004 . Moreover, our findings suggest that asymptomatic and mild H5N1 virus infections had not occurred in the population we investigated. Although H5N1 virus was only isolated from birds in 1 household, evidence suggested an H5N1 outbreak among numerous chicken flocks in the village beginning ≈6 weeks before the human H5N1 case was confirmed. Given that direct contact with poultry and poultry products was common among people in this village, a high proportion of villagers were presumably exposed to H5N1 virus. Genetic analysis of H5N1 virus isolates from the infected farmer and 2 chickens confirmed that no reassortment with elements of human influenza A viral genome had occurred . We cannot say why illness developed in 1 person when family, neighbors, and many other villagers who reported similar poultry exposures did not have any evidence of H5N1 virus infection.\n\n【28】The seroprevalence of H5N1 antibody in the Cambodian population surveyed was substantially lower than was found in poultry workers in Hong Kong in 1997 with the same microneutralization assay . Although this assay is a highly specific and strain-dependent test and may not detect neutralizing antibody to antigenically distinct H5N1 virus strains, the 2004 Vietnam clade 1 H5N1 virus strain used in our investigation was antigenically identical and genetically similar to H5N1 viruses circulating among poultry in Cambodia in 2005 . Nonetheless, a small chance exists that previous H5N1 virus infection might have been missed if levels of H5N1 neutralizing antibodies had declined; for example, some human influenza virus infections do not invariably result in a detectable serum antibody response . However, the kinetics of the H5N1 antibody response in humans is similar to that of human influenza A virus . In addition, recent evidence shows that the H5N1 virus results in a systemic infection likely to produce a neutralizing antibody response . When these results are considered with our findings from a sample of >350 people, H5N1 virus infection was not likely to have occurred without any circulating immunoglobulin G, even 2 months after symptom onset. H5N1 virus transmission to humans may be rare because it only occurs in exposed persons with unique host susceptibilities and a predisposition to an abnormal inflammatory response that results in severe and fatal outcomes, rather than causing a broad spectrum of illness with mild disease and subclinical infections. Nevertheless, further research is needed to better understand the immune response to H5N1 virus infection in humans.\n\n【29】Our investigations also found that some animal-handling practices, such as handling poultry, cleaning poultry stalls and cages, and collecting poultry feathers appeared to reduce the chance that a flock would be infected by H5N1 virus. This finding is in contrast to findings that handling dead or sick poultry is a risk factor for (individual) human H5N1 illness . We speculate that some practices that encourage backyard birds to stay close to the house, such as handling poultry, may be protective by reducing the distance healthy fowl need to roam to forage for food, thereby reducing interactions with wild and other domestic birds and contact with contaminated environments. Cleaning poultry stalls and cages and collecting poultry feathers may indicate a better level of general hygiene practices and may also decrease the risk by removing potentially infectious materials. These findings may highlight the value of educating farmers about hygienic animal-handling practices.\n\n【30】Other behavior appeared to modify the risk for H5N1 in domestic fowl. Purchasing live poultry increased risk. The introduction of new birds that may be harboring disease is an obvious threat to a flock. Anecdotal evidence suggests that farmers in Kampot Province had responded to the culling without compensation control measures by attempting to sell birds at the first sign of sickness during outbreaks in 2005 . Additionally, poultry trade with Vietnamese farmers was common and persisted despite the introduction of laws prohibiting such cross-border trafficking. Southern Vietnam has, like Cambodia, experienced mass H5N1 outbreaks among domestic fowl in the last few years , and the village examined in this survey was 20 km from the border.\n\n【31】Our results need to be interpreted in the context of several limitations. The interview process involved a recall period of 12 months and did not document more temporally relevant exposures immediately before or during the outbreak. The long recall period may increase the probability of exposure to potential risk factors, making households with and without suspected H5N1 virus infection in flocks more similar. In addition, the temporal association between behavioral risk factors and H5N1 virus infection in poultry was difficult to establish. On the basis of our counterintuitive findings, further risk factor studies should address this potential bias. The classification of households as likely having H5N1 in their flocks was based on a case definition suggested by Food and Agriculture Organization veterinarians. Without confirmation of H5N1 virus infection, we do not know the sensitivity and specificity of this definition and cannot quantify the degree of misclassification, if any. The study did not collect information about the origins of each flock, how long birds had belonged to each household, or internal movements of individual birds and flocks within the village. This limitation hindered mapping the likely circulation of H5N1 virus among poultry in the village, and other factors related to poultry transmission may have been missed. Although we did not record and map households we did not visit, this bias is likely to have been nondifferential because the proportions of nonvisited households were similar for all 4 investigation teams that surveyed in 4 different directions. Finally, specimen collection had limitations. Higher concentrations of virus exist in the trachea of infected birds rather than in the cloacae , but tracheal sampling was not performed in this survey because it was not acceptable to local farmers.\n\n【32】This study provides evidence of the low transmissibility of the H5N1 virus from infected poultry to humans, even in circumstances in which human-poultry interactions are regular and intense. In this instance, human H5N1 virus infection manifested as a single case of severe illness without any evidence that the virus could cause either mild disease or asymptomatic infection. However, our findings are limited to the investigation period of 2005. As H5N1 viruses continue to circulate and evolve among poultry, poultry-to-human transmission of H5N1 viruses could increase. Extensive investigations should be routinely conducted for all H5N1 outbreaks among humans and animals to monitor the nature and extent of bird-to-human or human-to-human transmission of H5N1 viruses. Additional seroepidemiologic investigations should be conducted to assess the ongoing risk for bird-to-human transmission of H5N1 among rural and other human populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "210ba613-eb96-47c9-b175-cfa75873c87d", "title": "Technical Assistance From State Health Departments for Communities Engaged in Policy, Systems, and Environmental Change: The ACHIEVE Program", "text": "【0】Technical Assistance From State Health Departments for Communities Engaged in Policy, Systems, and Environmental Change: The ACHIEVE Program\nAbstract\n--------\n\n【1】**Introduction**  \nThis study assessed the value of technical assistance provided by state health department expert advisors and by the staff of the National Association of Chronic Disease Directors (NACDD) to community groups that participated in the Action Communities for Health, Innovation, and Environmental Change (ACHIEVE) Program, a CDC-funded health promotion program.\n\n【2】**Methods**  \nWe analyzed quantitative and qualitative data reported by community project coordinators to assess the nature and value of technical assistance provided by expert advisors and NACDD staff and the usefulness of ACHIEVE resources in the development and implementation of community action plans. A grounded theory approach was used to analyze and categorize phrases in text data provided by community coordinators. Open coding placed conceptual labels on text phrases. Frequency distributions of the quantitative data are described and discussed.\n\n【3】**Results**  \nThe most valuable technical assistance and program support resources were those determined to be in the interpersonal domain (ie, interactions with state expert advisors, NACDD staff, and peer-to-peer support). The most valuable technical assistance events were action institutes, coaches’ meetings, webinars, and technical assistance conference calls.\n\n【4】**Conclusion**  \nThis analysis suggests that ACHIEVE communities valued the management and training assistance provided by expert advisors and NACDD staff. State health department expert advisors provided technical guidance and support, including such skills or knowledge-based services as best-practice strategies, review and discussion of community assessment data, sustainability planning, and identification of possible funding opportunities. NACDD staff led development and implementation of technical assistance events.\n\n【5】Introduction\n------------\n\n【6】Action Communities for Health, Innovation, and Environmental Change (ACHIEVE), a federally funded community health promotion initiative, used a collaborative development model to help communities prevent chronic diseases and decrease related risk factors. ACHIEVE was a partnership between local community organizations and national and state organizations whose goal was to create healthy, sustained communities where access to physical activity and healthful foods is high and exposure to tobacco is minimized. ACHIEVE educated and provided technical assistance to community leaders from various health-related or interested organizations on local policy and system and environmental improvements. We used reports submitted by ACHIEVE coaches (the community project coordinators) to analyze the types and value of the technical assistance and support provided by state health department expert and other resources available to ACHIEVE communities, as reported by 33 community project coordinators from 2008 through 2010.\n\n【7】The National Association of Chronic Disease Directors (NACDD) was 1 of 5 national lead organizations that participated in offering technical assistance to additional ACHIEVE communities . The other organizations were the National Association of County and City Health Officials (NACCHO), the National Recreation and Park Association (NRPA), the YMCA of the USA (Y-USA), and the Society for Public Health Education (SOPHE). NACDD and these ACHIEVE lead organizations also contracted with outside organizations and individuals to provide program development and specialized technical assistance and training opportunities.\n\n【8】NACDD was the only lead organization that required applicant communities to maintain a formal working relationship with their state health department’s chronic disease program. Consequently, in addition to NACDD staff support, state-based technical assistance was provided by expert advisors. By building relationships between local programs and state health departments, we intended to provide access to state-supported resources, ensure integration with state health department plans, and contribute to the sustainability of local ACHIEVE programs.\n\n【9】The ACHIEVE Program mission was based on the finding that citizen participation in policy development and decision making is an essential element of sustainability and, more specifically, that requiring collaborative partnerships will increase the likelihood that ACHIEVE programs and the local coalitions that manage them will be sustained after ACHIEVE funding has ended . The principal goal of ACHIEVE was to develop and implement population-based, risk-focused strategies for physical activity, nutrition, and tobacco use that help prevent or manage health risk factors for heart disease, stroke, diabetes, cancer, obesity, and arthritis. Community health action response teams (CHARTs) included 24 local and county health departments, 7 hospitals or medical centers, 2 nonprofit organizations, and 2 local health-related community coalitions. Participating community members organized to implement policy, systems, and environmental improvements; addressed barriers to physical activity, nutrition, and healthful eating; and reduced tobacco use. These groups recruited and organized local support and resources.\n\n【10】Each CHART provided 2 people called coaches to represent the team and participate in a facilitated leadership model; both coaches had equal responsibility for implementing this project in their community. Coaches typically represented the lead fiscal agency and a local partner agency. CHART members were local community members who represented broad and diverse sectors of the community: schools, universities, health care organizations (including providers and insurers), worksites and businesses, government entities, local organizations and foundations, media, and community planners. These representatives engaged their communities and set priorities for strategic action by developing community action plans (CAPs) to guide and direct their efforts. These plans gave each CHART a road map for building long-term goals (at least 3 years) and short-term objectives (1 year), and for planning activities on a timeline and with a lead partner. CAPs were dynamic documents that enabled adaptations to changes in local priorities, political will, and availability of resources and funding.\n\n【11】State health departments participated in the funding application process with the lead community organization, indicating their commitment to providing sustained technical assistance with data analysis, coalition management, community assessment strategies, and program planning, implementation, and evaluation. Each state health department appointed an expert advisor who offered support and technical assistance to the CHART and coaches. Expert advisors frequently provided expanded support linking communities to other state health department resources and other program technical assistance for media training, funding opportunities, epidemiology, and evaluation support. Expert advisors attended and participated in CHART meetings, webinars, and other required community program components and acted as ex-officio members of the CHART. NACDD staff informed the state’s chronic disease director of emerging program support and resource needs that the expert advisors should fulfill.\n\n【12】NACDD staff consultants provided ongoing support remotely throughout the project funding periods, facilitating technical assistance, providing direct consultation, and referring coaches to resources and content experts. They supported each CHART, helped sustain community activity for CHARTs that were no longer receiving ACHIEVE funding, and helped with the planning and delivery of technical assistance events such as webinars, coaches’ meetings, and action institutes. Throughout 3-year funding cycles, funded coalitions received technical assistance via emails, one-on-one phone calls, conference calls, Web-based support, and site visits. Each community had a primary point of contact with an NACDD consultant.\n\n【13】Staff of the Centers for Disease Control and Prevention (CDC) provided funding, technical support, and guidance from the Healthy Communities Program of the Division of Community Health. They used tools such as the _Sustainability Planning Guide for Healthy Communities_  to assist teams with development of sustainability plans and an online success story application to help teams develop stories that were published on the CDC website.\n\n【14】During the first year of ACHIEVE funding, representatives of community coalitions attended 2 in-person coaches’ meetings and action institutes. The meetings included presentations and interactive exercises conducted by nationally recognized experts. These experts provided training and skill-building sessions and tools to assist community coalition members in implementing policy, systems, and environmental change strategies; building effective coalitions; and conducting community assessments.\n\n【15】All ACHIEVE community coaches were provided opportunities to participate in groups called peer learning networks devoted to the process or focus area (physical activity, nutrition, or tobacco) they were working through. These groups had frequent conference calls during which they learned from each other. These interactive community networks were established at national meetings to share program successes and barriers to program development.\n\n【16】In 2010, an online, 6-part social media training component was implemented. Multiple communication platforms including Facebook, Twitter, LinkedIn, YouTube, and Flickr were included. All ACHIEVE communities attended technical assistance and training webinars every 2 months throughout each 3-year contract. Topics included community assessment, evaluation, and community education strategies. Each CHART had access to a private portal that was a secured, password-protected website that allowed them to share program documents, post and access events on a common calendar, manage task lists, and, on a Web forum feature, discuss issues with national partners and community members.\n\n【17】Methods\n-------\n\n【18】We reviewed quantitative and qualitative content reported by community project coordinators on the nature and value of technical assistance provided by expert advisors and NACDD staff and the usefulness of ACHIEVE resources in the development and implementation of community action plans. The quantitative data presented is a compilation of data reported by ACHIEVE local project coordinators in semiannual reports. Because these data included information from 3 separate cohorts (communities funded in 2008, 2009, and 2010), it is aggregated across the 3 years of reporting. The 2008 cohort of communities included 3 years of reports (startup year through the third and final year of funding); the 2009 cohort included 2 years of reports (startup and second year of funding); and the 2010 cohort included 1 year of reports (the startup year). The quantitative variables were selected because they were consistently reported across all 3 NACDD-managed cohorts . ACHIEVE community project coordinators were asked to select responses from a multiple-choice question that asked, “How has your State Expert Advisor been involved in your ACHIEVE activities during the current progress reporting period?” Because semiannual reporting took place over a 3-year period and multiple responses to questions were offered, the frequency with which responses were selected was low. Not all response options were included in the questions during the 3 years of reporting. If the number of respondents is below 61 , the question was not asked in each of the 3 years or the answer options were not asked in every community coordinator report. For these reasons, the quantitative data are described and discussed without statistical analyses. The quantitative data are summed across 3 years of semiannual CHART reports. Not all response options were included in the questions during the 3 years of reporting.\n\n【19】One of the quantitative variables reported also included qualitative text information: How were expert advisors involved in ACHIEVE activities? The text data (“other” responses) were analyzed using a grounded theory approach  to enrich understanding of the quantitative data. To facilitate analysis, we used open coding to assign codes to each of the 25 narrative responses to this question. Using subjective criteria, directional codes were assigned to each narrative response by the primary author (J.H.), by determining whether the comment reflected a positive, negative, or neutral action on the part of the expert advisors to advance ACHIEVE community initiatives. Codes describing the content of the comment were also assigned. Multiple directional and content codes were assigned to the written responses when the content was multidirectional and reflected more than 1 content area. Quotes from respondents that captured the essence or enriched understanding of the quantitative responses are reported. Assigned codes were reviewed, revised for agreement, and validated by 1 of the co-authors.\n\n【20】Results\n-------\n\n【21】We asked for responses to questions that asked how expert advisors were involved in ACHIEVE activities . The most frequent response was to the question, “identifying and sharing best practices on developing policy, systems, and environment strategies” (n = 40; 63%), followed by “providing advice on development of policy, systems, and environmental improvements change strategies” (n = 34; 53%), and “sharing strategies to leverage local government support for ACHIEVE” (n = 27; 42%). The expert advisors were also instrumental in encouraging ACHIEVE communities to disseminate the work they were doing to agencies not funded by ACHIEVE (n = 33; 52%). Half of the reported responses throughout the 3-year reporting period indicated that the “expert advisors participated in local ACHIEVE meetings” (n = 32; 50%). Nearly one-fourth of the responses indicated that the expert advisors “utilized local strategies to inform state policies” (n = 15; 23%). Only 5 responses indicated that the state expert advisors were “not involved” during a reporting period (8%).\n\n【22】Seventeen of the 25 “other” comments (68%) were positive, and the remaining 8 (32%) were neutral. Some of the positive comments were:\n\n【23】> . we just got a new expert advisor this month. So far, she has been amazing!\n> \n> In coordination with our City Planner, the expert advisors provided a Complete Streets presentation to the City Planning Commission.\n> \n> \\[The expert advisors\\] looked for funding opportunities within the state.\n> \n> \\[Name\\] was a great sounding board for ideas and thinking processes through.\n\n【24】The neutral comments were observations on the ACHIEVE process, staff turnover, or observations not based on the work of the expert advisors. Some examples of those comments were the following:\n\n【25】> Our original expert advisor got a promotion and is no longer working with ACHIEVE.\n> \n> Our previous state expert advisor left in December.\n> \n> Our expert advisor provided GoTo Meeting software and technical support to be able to hold CHART meetings and webinars for those unable to travel.\n\n【26】Three of the 25 “other” comments about state expert advisors involvement (12%) were negative (eg, “there has been little assistance from our state expert”).\n\n【27】Five out of 6 (n = 38; 84%) coaches helped collect or analyze community needs assessment data, and 82% (n = 37) reviewed the CAP . Other contributions coaches made were presenting assessment data to the CHART or community (n = 32; 71%), discussing assessment data, and drafting the CAP (n = 16 for each activity; 36%). Our results suggest that the most frequent contributions of the CHART executive team and other CHART members were collection or analysis of assessment data and review of the CAP.\n\n【28】In addition, resources outside of ACHIEVE communities (state health departments and other state partners, staff of NACDD, CDC and other national partners) contributed to the development of the CAP . CHART members contributed to the collection and analysis of assessment data (n = 33; 73%). Sixty percent (n = 27) of the CHART executive team contributed to the collection and analysis of assessment data and nearly half (n = 22; 49%) reviewed the CAP. Community partners helped with discussion of assessment data (n = 10; 22%) and review of the CAP (n = 10; 22%). The most frequently mentioned state health department staff contributions were review of the CAP (n = 17; 37.8%) and collection and analysis of assessment data (n = 15; 33.3%). These contributions were followed by discussion of assessment data (n = 11; 24.4%), presentation of assessment data to CHART or community, and drafting of the CAP (n = 6; 13.3% each).\n\n【29】NACDD staff made contributions similar to those made by state health department staff with review of the CAP (n = 21; 46.7%) and collection and analysis of assessment data (n = 9; 20.2%) being the most frequently mentioned contributions. Drafting of the CAP (n = 7; 15.6%), discussion of assessment data (n = 6; 13.3%), and presentation of assessment data to CHART or community (n = 5; 11.1%) were the next most frequent. The most commonly selected contribution from CDC staff was collection and analysis of assessment data with 11.1% of responses (n = 5) indicating this contribution. This low percentage was not unexpected because CDC staff did not have a primary role in providing technical assistance and community project support.\n\n【30】Community coaches reported the helpfulness of technical assistance resources with each report. Coaches meetings were rated as a leading resource in the “very helpful” category (15/19; 78.9%). Three-fourths (24 of 32; 75.0%) of the community coaches rated action institutes as a very helpful resource, followed by the technical assistance provided by NACDD (46 of 63; 73.0%) and the state health department expert advisors (30 of 63; 47.6%). The next most helpful resources were webinars and technical assistance conference calls (28 of 63; 44.4%), site visits by NACDD staff (5 of 13; 38.5%), peer-to-peer support (21 of 63; 33.3%), and State Healthy Communities Coordinators (20 of 61; 32.8%).\n\n【31】Discussion\n----------\n\n【32】State expert advisors were most active in providing technical assistance by identifying best practices and advising ACHIEVE CHARTs on policy, system, and environmental change strategies. This technical assistance supplemented the technical assistance and training that all CHART members received while attending training offerings. Expert advisors assisted communities in understanding and prioritizing assessment data, action planning and implementation, finding ongoing funding to sustain activity, and linking with other healthy community programs.\n\n【33】Because state expert advisors may also have had a broad responsibility to serve other communities in their state, it was perhaps consistent with their job responsibilities that they identified and shared best practices on developing policy, system, and environmental strategies to agencies not funded by ACHIEVE and used local strategies to inform state policies. The low percentage of “no state expert advisor involvement” reported by community coaches suggests that these public health professionals were widely engaged in providing technical assistance. The content of the qualitative text or other data reported supports the high level of active engagement of the expert advisors in ACHIEVE community activity. The analysis of the write-in responses suggests that, at minimum, expert advisors were welcomed by local ACHIEVE coaches, helped present best-practice strategies to community leadership, helped identify funding opportunities for ACHIEVE coalitions, and were responsive to ideas and proposed processes suggested by community coalition members.\n\n【34】Expert advisors were also engaged in the more technical aspects of community coalition activity (eg, helping collect and analyze needs assessment data, reviewing CAPs). Expert advisors may have preferred providing this type of support and coaches frequently acknowledged this support, or both. Expert advisors support that was more technical in nature may have been motivated, at least in part, by the job descriptions at the state health agency, professional interests, or formal training of the expert advisors. Because many community coalition members had no formal public health training, expert advisors may also have recognized a need for a higher level of technical assistance.\n\n【35】The data suggest that coaches made the most significant contributions to the development of CAPs across all communities. Because response frequency for contributions to the development of CAPs for the CHART executive team was comparable to that of other CHART members across all response categories, a broad, cross-section of CHART membership may have contributed at least as much as the executive team. This further suggests that ACHIEVE communities were characterized by their broad, grass-roots community participation. Other national ACHIEVE lead organizations and CDC staff were not expected to provide technical assistance and support to NACDD’s ACHIEVE communities. Consequently, community coordinators reported that these organizations contributed minimal support for the development of NACDD’s ACHIEVE teams community action plans.\n\n【36】The most helpful technical assistance and program support resources to ACHIEVE communities were those provided through interpersonal communications. Face-to-face technical assistance provided by the state expert advisors, NACDD staff, and peer-to-peer interactions was most valued. Other helpful technical assistance events, which also had interpersonal, interactive elements, were the webinars and technical assistance conference calls, the action institutes, and coaches’ meetings.\n\n【37】State health department expert advisors tended to provide technical guidance, including such skills- or knowledge-based services as assistance with best-practice strategies, review and discussion of community assessment data, sustainability planning, and identification of possible funding opportunities. Although regular or day-to-day support provided by expert advisors and NACDD staff may have been valuable for community coordinators (coaches) and their communities, events such as the coaches’ meetings, action institutes, and webinars were complementary learning opportunities that provided intensive, interactive training and education for community coalition members both off- and on-site. These learning events enabled trainers and trainees to ask each other challenging questions. This analysis of community ACHIEVE coach-reported data suggests that community coalitions that engaged in policy, system, and environmental change strategies benefitted greatly from a comprehensive, diverse portfolio of technical assistance and support strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3800c8a8-4580-4322-9202-a2f823d1f118", "title": "Intervention Mapping as a Guide for the Development of a Diabetes Peer Support Intervention in Rural Alabama", "text": "【0】Intervention Mapping as a Guide for the Development of a Diabetes Peer Support Intervention in Rural Alabama\nAbstract\n--------\n\n【1】**Introduction**  \nPeer support is a promising strategy for the reduction of diabetes-related health disparities; however, few studies describe the development of such strategies in enough detail to allow for replication. The objective of this article is to describe the development of a 1-year peer support intervention to improve diabetes self-management among African American adults with diabetes in Alabama’s Black Belt.\n\n【2】**Methods**  \nWe used principles of intervention mapping, including literature review, interviews with key informants, and a discussion group with community health workers, to guide intervention development. Qualitative data were combined with behavioral constructs and principles of diabetes self-management to create a peer support intervention to be delivered by trained peer advisors. Feedback from a 1-month pilot was used to modify the training and intervention.\n\n【3】**Results**  \nThe resulting intervention includes a 2-day training for peer advisors, who were each paired with 3 to 6 clients. A one-on-one in-person needs assessment begins an intensive intervention phase conducted via telephone for 8 to 12 weeks, followed by a maintenance phase of at least once monthly contacts for the remainder of the intervention period. A peer support network and process measures collected monthly throughout the study supplement formal data collection points at baseline, 6 months, and 12 months.\n\n【4】**Discussion**  \nIntervention mapping provided a useful framework for the development of culturally relevant diabetes peer support intervention for African Americans living in Alabama’s Black Belt. The process described could be implemented by others in public health to develop or adapt programs suitable for their particular community or context.\n\n【5】Introduction\n------------\n\n【6】The prevalence of type 2 diabetes is increasing . In the United States, the disease disproportionately affects minority populations, particularly African Americans in the rural South . Proper diabetes self-management can lead to improved glycemic control, blood pressure, and lipid levels, and can mitigate the negative health effects of comorbid conditions . However, the demands of self-management behaviors (eg, diet modification, physical activity, medication adherence, self-monitoring of blood glucose levels) can be difficult to balance and are influenced by socioeconomic, cultural, and psychosocial factors such as lack of social support, self-efficacy, coping skills, and increased barriers to self-care . Residents of rural areas face additional barriers to diabetes self-management, including limited access to health care services, providers, and education programs; high rates of poverty; low levels of health literacy; and increased distances from social networks .\n\n【7】These conditions are widespread in Alabama’s Black Belt region. Named for its dark soil and agricultural history, the Black Belt encompasses approximately 18 counties in southern Alabama. African Americans comprise more than 30% of the population; rates are as high as 80% in some areas. In this region, more than 30% of African Americans over the age of 50 have diagnosed diabetes.\n\n【8】As diabetes self-management has received more attention, strategies (eg, increasing social support) have been proposed to meet the needs of those living with diabetes . However, such programs may be difficult to implement in rural settings, and limited effects of classic support (ie, family/spouse) have strengthened the case for peer-involved programs .\n\n【9】Peer advisors are ideal for community-based support programs because of their ability to serve in a reciprocal, nonhierarchical capacity. In general, peer advisors are nonprofessionals with an intimate knowledge of the difficulties of disease management who can provide support on the basis of shared life experiences . Peer-based programs improve health behaviors as well as health status . For example, a recent peer advisor program demonstrated improved communication with health care providers, use of community resources, and diabetes management . Studies in the United States indicate that peer advisors may be successful in minority populations and with people who may distrust traditional health care systems .\n\n【10】In rural settings, the combination of peer advisors with telephone-based support may be particularly effective; it eliminates barriers such as transportation, costs of group attendance, and time constraints . Studies of telephone interventions show that they help improve glycemic control .\n\n【11】The objective of this study is to describe our experience with the use of intervention mapping as a guide for the development of a peer support diabetes intervention for a rural, medically underserved population . Intervention mapping is a systematic process that combines theory, empirical evidence from the literature, and data from the community to develop health education programs. The process involves multiple steps that begin with a community assessment and continues by fostering collaborations with community stakeholders during intervention development and program planning. Intervention mapping has been used successfully to develop health behavior programs, mostly related to cancer .\n\n【12】Methods\n-------\n\n【13】Intervention mapping is an iterative process encompassing 6 key stages. For several years before the grant funding for the peer support intervention, we conducted the literature review and semi-structured interviews. This preliminary work provided investigators with a foundation from which to respond to the Peers for Progress call for proposals; we worked with existing community partners to develop the study design. After funding was received in 2009, 8 months were spent developing the intervention, which included conducting unstructured interviews with community members and a discussion group with community members of an existing community health worker (CHW) network.\n\n【14】### Step 1. Needs assessment\n\n【15】The needs assessment was based on 1) a literature review of existing diabetes programs involving peers or lay health workers, 2) nationwide field experience, and 3) local needs assessment.\n\n【16】1\\. Literature review. A review of diabetes programs involving peer advisors around the United States was conducted, the results of which have been published elsewhere . Briefly, we found that peer advisor roles, responsibilities, and training varied across programs and were context-specific. After our review, the World Health Organization (WHO) identified 3 main peer advisor roles related to diabetes self-management: to link, to assist, and to support . We used information from our own review and from the WHO report to conceptualize the peer advisor role .\n\n【17】2\\. Field experience. In parallel with our literature review, semi-structured interviews were conducted with program managers from diabetes peer advisor programs across the country, allowing for an in-depth exploration of barriers and facilitators to implementation. This work indicated that peer advisors are prone to burnout and the positions are prone to turnover; remuneration, both monetary and nonmonetary incentives, is another consideration. A full report of the results has been published elsewhere . These lessons influenced the intervention structure and content. Specifically, plans were made for a manageable caseload  with limited paperwork and simplified forms. Additionally, support networks were designed to facilitate interaction between peers and extend to investigators and community coordinators.\n\n【18】3\\. Local needs assessment. The priority population for this study was African American adults with diabetes living in Alabama’s Black Belt. Assessments conducted by the Alabama Department of Public Health (ADPH) have documented racial disparities related to diabetes and associated complications, particularly in many Black Belt counties. ADPH determined that, when stratified by race and sex, diabetes prevalence among African American women was 7% higher than among white women . Furthermore, diabetes prevalence was significantly higher in Alabama residents whose annual household incomes were less than $35,000 . Many of these areas also lack primary health care providers, endocrinologists, and other resources, including diabetes education. ADPH reported in 2010 that endocrinologists and diabetes educators in Alabama are located in mostly urban counties with higher populations . For example, 57 of the 67 counties have no endocrinologists, and 45 of 67 counties have no certified diabetes educators . Socioeconomic constraints further exacerbate the problem; one-third of Black Belt residents have incomes below the federal poverty level . Informal needs assessments conducted among members of existing cancer community coalitions revealed that diabetes was an area of concern among community members and that a program focused on diabetes was a natural next step. Furthermore, discussions with community members and staff at partnering practices revealed that a formal diabetes education program would benefit the community. Thus, in the design phases of the study, it was decided that both trial arms would include diabetes education.\n\n【19】### Step 2: Identifying outcomes and change objectives\n\n【20】Results from Step 1 were used to identify outcomes and change objectives. Although the overarching objective of this intervention is to improve glycemic control, blood pressure, and lipid levels through a peer advisor intervention designed to improve self-care, proximal measures, including self-care behaviors, were selected as secondary outcomes . Additionally, we included patient-centered outcomes such as quality of life, depressive symptoms, and stress. Similarly, we were interested in potential antecedents to or mediators of those behaviors, such as self-efficacy and patient empowerment. These complemented our theoretical basis and our plans for peer support.\n\n【21】### Step 3: Selecting theory-based methods and practical strategies\n\n【22】The chronic care model served as a conceptual framework for the overall intervention . In previous work, we have proposed integrating peer advisors into the chronic care model at the intersection of community and health care organizations . For this study, we conceptualized the peer advisors as conducting activities within the community, helping participants access resources within the community and the health care system. The theoretical foundation of our intervention combined several complementary theories, including the health belief model, social cognitive theory, adult learning theory and empowerment model, and social networks and social support, each of which has been applied to diabetes management in previous studies  . Grounding the intervention in behavior change theory combined with measures tied to theory-based constructs allowed us to understand the pathways to any intervention effects.\n\n【23】Practical strategies for intervention delivery were based on evaluation of literature and existing peer-based programs (described earlier), including an assessment of specific needs of volunteers . Strategies were further informed by a discussion group with 7 peer advisors from previous cancer awareness projects in the Black Belt. The purpose of the discussion group was to solicit feedback on the structure of the peer advisor program, recruitment and retention strategies, methods for intervention delivery, and perceived needs. Additional input was solicited for developing training manuals, educational materials, and participant incentives.\n\n【24】### Step 4: Developing the program\n\n【25】Because many communities in rural Alabama lack access to diabetes education, we chose to develop a diabetes education program for both intervention and control participants. For the peer support intervention, we hypothesized that people with diabetes coached by trained peer advisors will experience more observational learning opportunities, reinforcement, and social support, leading to improved health behaviors, metabolic measurements, and, ultimately, quality of life. On the basis of a literature review, the team felt that people who either lived with diabetes themselves or helped a close friend or family member with their daily diabetes self-care would be the best peer advisors .\n\n【26】We then conducted a 4-week pilot of the training and the intervention. We trained 2 peer advisors and recruited 7 participants from a community health center, from an existing community diabetes support group, and by word of mouth. Peer advisors met their clients on enrollment day to conduct the needs assessment and then called their clients weekly. Peer advisors also had weekly calls with investigators to report on progress and troubleshoot problems. The calls provided an opportunity to observe intervention fidelity, reinforce skills taught during training, and offer support.\n\n【27】Results\n-------\n\n【28】The education program consists of three 30-minute interactive learning modules led by nurses or trained health educators. Content is based on information from the American Diabetes Association’s 2009 Standards of Care Guidelines  and from a segment we developed that focuses on “raising the BAR (Be prepared, Ask and learn, Reflect and reach out) on your doctor visit.” The learning modules include 1) diabetes basics and the office visit, 2) healthy eating, and 3) exercise and stress management .\n\n【29】We planned for peer advisors to provide a flexible, informal approach to assisting patients, based on role modeling and patient empowerment/activation, rather than formal delivery of a diabetes education curriculum. We identified and developed training activities consistent with 3 main roles: assist, support, and link . Training content included discussion of peer advisor roles and expectations, principles of motivational interviewing and communication, goal-setting and problem-solving, human subjects in research protections, health care and community resources, diabetes basics, and training on the protocol and forms required for process data collection.\n\n【30】The intervention begins with the peer advisor conducting an in-person needs assessment to identify an area of diabetes management that the client wants to improve. The needs assessment reviews medication adherence, diet, exercise, stress, and talking to their doctor. Using motivational interviewing techniques, the peer advisor guides the participant to set 1 SMART (specific, measurable, achievable, realistic, time-oriented) goal.\n\n【31】After the initial meeting, the peer advisor contacts the client weekly by telephone for 8 to 12 weeks (intensive intervention phase) and at least monthly thereafter (maintenance phase). At each telephone contact, the peer advisor asks the client about new problems, reviews the client’s SMART goal, and if the goal has been met, collaboratively sets a new goal. If the client does not want to discuss the goal, the peer advisor provides support and documents the discussion. If the client is having trouble attaining the goal, the peer assists the client in identifying barriers and problem-solving.\n\n【32】In a separate contact made a day or 2 before an office visit, the peer and client plan for the visit and develop a strategy for asking questions and understanding recommendations. Another contact is made a day or 2 after the visit to review what occurred, make plans as needed, and, if necessary, recontact the office to address unmet needs.\n\n【33】### Pilot testing the intervention\n\n【34】We conducted a 4-week pilot of the training and the intervention. After completion of the pilot, study investigators conducted two 1-hour group interviews, one with peer advisors and one with the clients. Overall, the intervention was well received and the study forms were deemed reasonable. Both peers and clients felt that transitioning to monthly calls after 8 weeks may not provide enough support and recommended a flexible, client-tailored approach to lengthening the call interval. The intervention was modified accordingly.\n\n【35】Feedback from the pilot peer advisors also led us to create a peer support network. Peers assigned to a particular region were asked to function as a team to enhance retention and morale, minimize burnout, and permit case reassignment should a peer drop out. Monthly support meetings were planned during the intervention period, and each peer advisor was paired with another peer advisor from their area as a one-on-one supporter.\n\n【36】### Steps 5 and 6: Planning for implementation and evaluation\n\n【37】To aid implementation and adoption, information from the pilot was used to modify the peer training manual so that it could serve both as a guide for the training sessions and as an ongoing resource during the intervention period and beyond. Currently, the program is being tested in a group-randomized trial. More than 60 peer advisors have completed training, 424 participants have been enrolled, and 200 have been matched to peer advisors.\n\n【38】Evaluation measures include biometric measures (hemoglobin A1c, blood pressure, low-density lipoprotein cholesterol, body mass index, and waist circumference) and patient-centered measures along with theory-based behavioral outcomes . Measures have been collected at baseline and will be collected again 6 and 12 months later. To facilitate this process, the study team trained several community members to assist with data collection, including biometric assessments and face-to-face interviews.\n\n【39】Process measures are being collected from peer advisors. Using the pilot-tested contact forms, peers document each contact, both scheduled and spontaneous, and, through separate forms, contacts before and after the office visit. Forms are submitted to community coordinators at monthly meetings and evaluated by coordinators and the investigative team for prompt action should deficits be identified. In addition, telephone contacts between investigators, groups of 3 to 4 peer advisors, and the community coordinator take place weekly for the first 4 weeks and at 6-week intervals during the maintenance phase. These calls allow investigators and peers to review progress, troubleshoot problems, and reinforce training.\n\n【40】Discussion\n----------\n\n【41】The systematic and collaborative approach to program development described in this article is guided by principles of intervention mapping in the context of community-based participatory research. The 6-step intervention mapping framework resulted in a peer-delivered diabetes self-management program that was informed by health behavior theory and local context to be responsive to community needs. The resultant intervention is patient-centered, peer-delivered via telephone, and focuses on building goal-setting and problem-solving skills. Community involvement in the early planning stages led to adjustments in program design and implementation that, although specific to this study, may have implications for programs targeting rural areas more broadly.\n\n【42】For most communities, willingness to partner with research institutions is balanced with the desire to advocate for needed resources. Randomized controlled trials are often viewed with skepticism as individuals or entire communities risk being relegated to the control arm, often without tangible benefits . The need to respond to community-identified needs and create sustainable partnerships becomes more apparent as research efforts increasingly focus on translating interventions into real-world settings. This is evident in the increasing number of investigators employing community-based participatory research methods.\n\n【43】Although implementing alternative designs to randomized controlled trials can overcome the resistance by many communities to the control arm, our development process, particularly feedback obtained from partnering practices and community leaders, provided us with a potential solution. We elected to develop education services for both trial arms. In the Black Belt, community health centers and organizations attempt to provide quality health care with very limited resources, and formal diabetes education is unavailable. In this study, early discussions with providers and key community leaders even before applying for funding confirmed the importance of providing education to all participants, and our community partners cited the offer of such a program as a key component of successful recruitment efforts and community engagement.\n\n【44】Early formative work with community partners also proved essential for developing support strategies for peer counselors. Previous studies have found that peer advisors are prone to burnout, leading to high turnover rates . Contributing factors include perceived isolation and lack of ongoing support, coupled with a high caseload and heavy paperwork . These lessons were confirmed by discussions with existing peer advisors in the Black Belt. Therefore, plans were made during the pilot phase for weekly calls between investigators and peers to debrief and provide support. A “buddy system” and a regional support group added another dimension of peer support. The challenge of this extensive approach is the need for significant investigator and study staff time and resources. However, investigators profited considerably from these labor-intensive contacts, obtaining reassurance on intervention fidelity, having opportunities to reinforce training, and deepening bonds with the peer advisors. Future studies are needed to better understand peer advisor support needs and to develop sustainable strategies to deliver such support beyond the life of  study.\n\n【45】Intervention mapping was a comprehensive approach to community-based intervention development that was both highly regarded by our community partners and scientifically rigorous. Key features included engaging community members from the beginning, even before applying for grant funding, and staying engaged throughout program development and implementation. Although details and specific procedures were designed for and with African Americans living in Alabama’s Black Belt and may not be generalizable to other settings, our experience and strategies may be helpful for others developing interventions in underserved communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a58f84ed-2115-421c-a38f-ec8202cbfeb1", "title": "Epidemic of Invasive Pneumococcal Disease, Western Canada, 2005–2009", "text": "【0】Epidemic of Invasive Pneumococcal Disease, Western Canada, 2005–2009\nBefore the advent of antimicrobial drugs, outbreaks of invasive pneumococcal disease were numerous. Since then, however, outbreaks have been less frequently reported and have involved fewer persons, usually those confined to closed settings such as hospitals or military barracks . Even more rare have been large outbreaks or epidemics of invasive pneumococcal disease; if and when they do occur, they tend to be caused by a limited number of pneumococcal serotypes .\n\n【1】The serotype of a _Streptococcus pneumoniae_ bacterium is designated according to the organism’s polysaccharide capsule, its major virulence factor. Worldwide, 91 polysaccharide capsular serotypes have been identified . A small subset of serotypes is responsible for most large outbreaks; these serotypes typically include, but are not restricted to, serotypes 1, 4, 5, 9V, 12F, and 23F .\n\n【2】Before 2005, large outbreaks of pneumococcal disease, including invasive pneumococcal disease caused by serotype 5, were rare in Canada. In 2002, an outbreak caused by _S. pneumoniae_ in northern Quebec, Canada, was reported, and blood culture identified 10 cases as being caused by a serotype 1 strain . We report a large epidemic of invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 in Canada that occurred during 2005–2009. The study received approval from the institutional research review committees of the health regions and the University of Alberta ethics review board.\n\n【3】### Materials and Methods\n\n【4】In Canada, invasive pneumococcal disease is nationally notifiable. For this study, cases of invasive pneumococcal disease were defined according to the national case definition: isolation of _S. pneumoniae_ from a normally sterile site, such as blood, cerebrospinal fluid, pleural fluid, biopsy tissue, joint aspirate, pericardial fluid, or peritoneal fluid . In the provinces affected by the 2005–2009 epidemic, clinical diagnostic microbiology laboratories were required by provincial health authorities to submit isolates from patients with invasive pneumococcal infections to their respective provincial laboratories, which would then send them to the National Centre for Streptococcus, in Edmonton, Alberta, for capsular serotyping and antimicrobial drug resistance epidemiologic profiling. For this study, 1 isolate per case was counted. Multiple isolates of the same serotype collected from the same patient within a 30-day period were considered to account for 1 case. Regardless of serotype, isolates collected from the same patient >30 days after the first isolate were counted as separate cases.\n\n【5】##### Clinical Data Collection\n\n【6】To elucidate features of disease caused by _S. pneumoniae_ serotype 5, we reviewed all cases of invasive pneumococcal disease in the northern Alberta area reported from 2005 through 2009. During the study period, Alberta was subdivided into 9 health regions. For cases originating in health regions 4 through 9 (located in northern Alberta), an extensive medical chart review was conducted. The total population for these health regions in 2008 was 1,888,881 . The clinical data collected were patient age, sex, aboriginal status (i.e. First Nations heritage), homelessness, substance abuse, type of invasive pneumococcal disease, outcome, and concurrent conditions .\n\n【7】##### Identification and Serotyping\n\n【8】As part of its serotyping program, the National Centre for Streptococcus pneumococcal surveillance confirmed isolates as _S. pneumoniae_ according to morphologic appearance and optochin susceptibility . All pneumococcal isolates that exhibited a positive quellung reaction when commercial type-specific antiserum (Statens Serum Institute, Copenhagen, Denmark) was used were assigned a serotype . Strains that were susceptible to optochin but for which no serotype was assigned were further tested by using the AccuProbe _Streptococcus pneumoniae_ Culture Identification Test (Gen-Probe, San Diego, CA, USA) to confirm species identification.\n\n【9】##### Antimicrobial Drug Susceptibility Testing\n\n【10】Drug susceptibility was determined by using the reference broth microdilution method described by the Clinical and Laboratory Standards Institute . The following antimicrobial drugs were tested: penicillin, cefotaxime, ceftriaxone, chloramphenicol, erythromycin, clindamycin, tetracycline, trimethoprim/sulfamethoxazole, levofloxacin, and vancomycin. All antimicrobial agents were purchased from Sigma-Aldrich Canada Ltd, Oakville, Ontario, Canada. Interpretation of MICs was based on Clinical and Laboratory Standards Institute performance standards that were current at the time of testing (M100-S15 through M100-S17) .\n\n【11】##### Pulsed-field Gel Electrophoresis and Multilocus Sequence Typing\n\n【12】_S. pneumoniae_ chromosomal DNA was prepared as described . Chromosomal DNA was restricted with 20 U of _Sma_ I (New England Biolabs, Beverly, MA, USA), and pulsed-field gel electrophoresis (PFGE) was performed by using a CHEF DR-III apparatus (Bio-Rad Laboratories \\[Canada\\] Limited, Mississauga, ON, Canada) for 23 h. The parameters used were as follows: initial pulse 3.5 s, final pulse 23.5 s, voltage 6 V/cm, and temperature 13°C. _Salmonella_ Braenderup U9812 was used as a molecular size marker. The macrorestriction pattern was analyzed by using Bionumerics version 5 (Saint-Martens-Latem, Belgium).\n\n【13】Multilocus sequence typing was performed as described . The multilocus sequence type (MLST) was searched against the online pneumococcal database .\n\n【14】##### Statistical Analyses\n\n【15】Data were analyzed by using SAS software version 9.1 (SAS Institute, Inc. Cary, NC, USA). Possible factors associated with serotype 5 among patients with pneumococcal disease were assessed. We examined the association of each demographic, substance abuse, and concurrent condition variable with outcome variable serotype 5 (yes or other serotype). For continuous variables, we used the _t_ test or Mann Whitney U test as appropriate. For categorical variables, we used the χ 2  or Fisher exact test. For variables that were significant (p<0.20) on univariable analyses, we used the following multivariable logistic regression model:\n\n【16】LOGIT P (Serotype 5) = _B_ 0  \\+ _B_ 1  _X_ 1  \\+ _B_ 2  _X_ 2  \\+ … + _B_ p  _X_ p  …… \n\n【17】In model 1, the significance of variables was assessed by using the Wald statistic. The variables that were significant at p<0.05 were retained in the model. All others were removed from the model unless they were possible confounders. In the final model, we tested βs, the effect of each variable on log odds of serotype 5 after adjustment for other associated variables. To calculate rates, we used the populations that were current for each province in January 1, 2009 .\n\n【18】### Results\n\n【19】##### The Epidemic\n\n【20】From January 1, 2000, through December 31, 2004, the National Centre for Streptococcus serotyped 5,509 _S. pneumoniae_ isolates from patients with invasive pneumococcal disease (1 isolate counted per case) from across Canada and identified 7 as serotype 5: one each in 2001, 2002, and 2003 and 4 in 2004. Since then, the Centre identified 52 isolates from patients with invasive pneumococcal disease as serotype 5 in 2005, 393 in 2006, 457 in 2007, 104 in 2008, and 42 in 2009, for a total of 1,048 cases . The number of cases caused by serotype 5 peaked in December 2006 and then slowly declined in 2007, 2008, and 2009 . Patients with invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 were from British Columbia (343 \\[32.7%\\], 7.8 cases/100,000 population) Alberta (523 \\[49.9%\\], 14.4/100,000), Saskatchewan (85 \\[8.1%\\], 8.3/100,000), and Manitoba (92 \\[8.8%\\], 7.6/100,000) . During this 5-year period, only 5 isolates of serotype 5 were detected from elsewhere in Canada: 1 from Ontario in March 2007; 1 from Quebec in June 2009; and 3 from Northwest Territories in April, July, and December 2007.\n\n【21】In western Canada during 2000–2009, the numbers of serotype 5 and other serotype isolates identified increased . The increased number of isolates submitted for typing after the onset of the epidemic indicates greater interest on the part of public health officials in western Canada in identifying circulating serotypes from patients with invasive pneumococcal disease in their provinces.\n\n【22】The epidemic primarily affected young adults (median age 41 years) . Only a small subset of cases occurred among patients <5 years of age and even fewer in those >65 years of age. Most patients were male (637 male, 395 female, and 16 unknown).\n\n【23】##### Specimen Source\n\n【24】The sources of specimens for the serotype 5 isolates from across Canada were as follows: 988 isolates from blood, 33 from lung/pleural fluid, 9 from cerebrospinal fluid, 7 from synovial fluid, 7 from chest/hip/leg fluid, 3 from pericardial fluid, and 1 from peritoneal fluid. For the univariable and multivariable analyses, isolates from patients with serotype 5 and nonserotype 5 invasive _S. pneumoniae_ were collected from northern Alberta only (1,112 cases).\n\n【25】##### Patient Characteristics\n\n【26】According to univariable analysis, serotype 5 was more prevalent than other serotypes among patients who were male (66.2% vs. 57.0%), of First Nations heritage (21.8% vs. 10.0%), or homeless (16.1% vs. 4.7%) . Among the substance-abuse categories, associations with tobacco use, alcoholism, and illicit drug use were considered significant (p<0.001 for each). With respect to concurrent conditions, cases of invasive pneumococcal disease caused by serotype 5 were significantly associated with cancer within 5 years before onset of invasive pneumococcal disease, cardiovascular disease, hematologic abnormalities, diabetes mellitus, cirrhosis, chronic renal failure, musculoskeletal impairment, and hepatitis C . For patients with bacteremia and pneumonia, invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 occurred significantly more often with pneumonia than did that caused by other serotypes (94.7% vs. 74.6%). In addition, meningitis was more common for patients in the non–serotype 5 group than in the serotype 5 group (8.0% vs. 0.7%, respectively; p<0.001). Death was less associated with infection caused by serotype 5 than by other serotypes (3.2% vs. 14.1%, respectively).\n\n【27】The multivariable logistic regression model used to examine the associations of different factors with _S. pneumoniae_ serotype 5 that were identified by univariable analyses found that First Nations heritage and homelessness were significantly associated with serotype 5 (adjusted odds ratio \\[aOR\\] 2.34; 95% CI 1.53–3.57 and aOR 1.83, 95% CI, 1.07–3.12, respectively) . Tobacco use (aOR 1.90, 95% CI 1.29–2.81) and illicit drug use (aOR 1.89, 95% CI, 1.31–2.73) were also significantly associated, whereas alcoholism was not . Among concurrent conditions, the following were significantly associated: cancer within 5 years before invasive pneumococcal disease (aOR 0.32, 95% CI 0.14–0.71), cardiovascular disease (aOR 0.51, 95% CI 0.32–0.82), hematologic abnormalities (aOR 0.19, 95% CI 0.06–0.55), and cirrhosis (aOR 0.18; 95% CI 0.06–0.50) . Associations with musculoskeletal disease and hepatitis C infection, although significant according to univariable analysis, were not significant according to multivariable analysis.\n\n【28】##### _S. pneumoniae_ Serotype 5 Characteristics\n\n【29】Antimicrobial drug susceptibility testing of 1,009 isolates indicated that all _S. pneumoniae_ serotype 5 isolates tested were susceptible to cefotaxime, ceftriaxone, tetracycline, levofloxacin, and vancomycin. A small percentage (5 \\[0.5%\\]) of the 1,009 were intermediately resistant to penicillin (MIC ≥0.125 mg/L), 2 (0.2%) were resistant to chloramphenicol, 4 (0.4%) were resistant to clindamycin, 2 (0.2%) were intermediately resistant to erythromycin, and 4 (0.4%) were fully resistant to erythromycin. For trimethoprim/sulfamethoxazole, 976 (96.7%) isolates showed intermediate resistance (MICs 1.0–2.0 mg/L) and 18 (1.8%) showed resistance (4.0 to >16.0 mg/L). The remaining 39 isolates were not available for testing.\n\n【30】During the epidemic, a subset of _S. pneumoniae_ serotype 5 isolates (91 isolates), encompassing each year of the epidemic from the 4 affected western provinces were randomly selected and subjected to PFGE for restriction fragment-length polymorphism (RFLP) analysis (12 isolates in 2005, 26 in 2006, 13 in 2007, 20 in 2008, and 20 in 2009). All isolates typed by PFGE had either an identical RFLP pattern or differed by 1 band . Extending the RFLP analysis back to the 7 serotype 5 isolates from Canada from 2000 through 2004 showed that the first similar fingerprint detected was from a person who lived in a small town in rural southeastern Alberta in March 2004.\n\n【31】To determine whether this clone had been found in the United States, we compared it with 6 serotype 5 isolates from the US Centers for Disease Control and Prevention. Three isolates were from a small cluster of cases in San Francisco, California, in 2002 and 3 were from sporadic cases in the United States in 2006 . Of these 6 isolates, the RFLP pattern for 5 isolates was identical to that of the epidemic clone and 1 isolate had a single band difference, suggesting that the serotype 5 clone in western Canada had been circulating in the United States in 2002 and 2006. This clone might have been imported into Canada from the United States; however, it might also have been imported from elsewhere in the world because sequence type (ST) 289 is the major circulating serotype 5 clone.\n\n【32】MLST analysis showed the allelic profile of the _S. pneumoniae_ serotype 5 clone to be ST289 ( _aroE16_ , _gdh12_ , _gki9_ , _recP1_ , _spi41_ , _xpt33_ , _ddl33_ ). ST289 has been listed in the MLST database . The ST289 clone was originally reported from Colombia and is contained in the Pneumococcal Molecular Epidemiology Network list of worldwide antimicrobial drug–resistant clones (designation Colombia 5  \\-19)  .\n\n【33】### Discussion\n\n【34】Large epidemics of pneumococcal disease might go unrecognized unless surveillance programs are in place to document fluctuations in serotype prevalence, as reported here. The year-to-year variability of invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 seen in some countries might actually reflect serotype 5 outbreaks similar to what we have described . For example, in 2000 in Mali, Africa, 50% of the isolates recovered from children with invasive pneumococcal disease were serotype 5, yet 2 years later; this percentage had dropped to a small portion of the total cases . This serotype 5 variability has also been reported in Chile and Israel . In Israel during 1989–1998, serotype 5 was the second most common serotype (serotype 1 was the most common) that caused invasive pneumococcal disease (12%–13% of cases among children <15 years of age) .\n\n【35】Although in other countries the number of _S. pneumoniae_ serotype 5 cases might vary from year to year, in Canada no variability for serotype 5 was evident until the 2005–2009 epidemic. Few serotype 5 isolates had been documented since 1991, when the National Centre for Streptococcus first began performing pneumococcal serotyping to support national surveillance in Canada, until 2005. This serotype 5 strain has been demonstrated elsewhere in the world, not just Canada. Data from the MLST database and published reports indicate that the Colombia 5  ST289 clone has been reported in countries in Europe, Latin America, and Africa and in the United States  . In addition, the rate of resistance to trimethoprim/sulfamethoxazole by the Colombia 5  ST289 clone has been reported as 80.8%  and 58.2%  of the Colombia 5  ST289 strains from Latin American countries.\n\n【36】The _S. pneumoniae_ serotype 5 epidemic mostly affected middle-aged men (median 41 years of age). Other risk factors were homelessness and First Nations heritage, although these factors accounted for a small percentage of the population. Because invasive pneumococcal disease reportedly affects homeless populations, the finding that homelessness was a major demographic factor associated with this epidemic is not surprising . A recent study from Toronto, Ontario, Canada, found that incidence of invasive pneumococcal disease was greater in the homeless population than in the general population  and that the variables associated with the serotype 5 epidemic (tobacco use, alcohol abuse, illicit drug use) were associated with invasive pneumococcal disease. Serotype 5 pneumococci were not identified in this study.\n\n【37】In December 2006, investigators found _S. pneumoniae_ serotype 5 affecting persons of First Nations heritage living near the city of Calgary, Alberta, and persons living in inner-city Calgary; Edmonton; and Vancouver, British Columbia . These reports indicated that the variables associated with invasive pneumococcal disease caused by this serotype were homelessness, use of illicit drugs, First Nations heritage, alcoholism, and hepatitis B or C, thereby corroborating our findings for those cases in northern Alberta . Recovery of this serotype in locations other than inner cities in western Canada (including northern Saskatchewan) suggests its spread beyond the larger metropolitan areas of western Canada .\n\n【38】A public health response to the epidemic occurred throughout western Canada. Regional health authorities conducted vaccination programs focused primarily on homeless populations in large metropolitan areas. They used the 23-valent pneumococcal polysaccharide vaccine, which contains serotype 5. As a result of these large-scale pneumococcal vaccination campaigns, the National Advisory Committee on Immunization issued an advisory statement recommending use of the 23-valent pneumococcal vaccine for homeless persons and injection drug users . Examples of public health measures used to address the outbreak in 2 health regions are contained in reports from British Columbia focusing on _S. pneumoniae_ serotype 5 outbreaks in the Vancouver downtown eastside and in the city of Kelowna . In Vancouver, investigators found that the serotype 5 strain accounted for 78% of cases of invasive pneumococcal disease. The major risk factors reported were use of crack cocaine and residence in Vancouver’s downtown eastside, an impoverished part of that city where most of the illicit-drug users and homeless persons live . As a result, Vancouver Coastal Health authorities targeted rooming houses, shelters, food banks, and other community locations . In Kelowna, public health nurses and health care providers focused a pneumococcal vaccination program on persons who were homeless and/or addicted to illicit drugs or alcohol; at the time of their report, they had vaccinated ≈1,000 at-risk persons .\n\n【39】A strength of our study is the ability of the centralized laboratory to capture and document shifts in the epidemiology of pneumococci in Canada. Regionalization of serotyping of pneumococci has the potential to miss changes in serotypes that can occur rapidly.\n\n【40】A weakness of our study is the lack of clinical data for all cases of invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 that occurred during this epidemic. Logistically, gathering all of these data was not possible; however, the clinical data from northern Alberta do indicate some of the clinical variables involved and the concurrent conditions associated with serotype 5 cases. Another limitation might be that the variables for persons with invasive pneumococcal disease caused by _S. pneumoniae_ serotype 5 (patient demographics, substance-abuse associations, concurrent conditions, type of pneumococcal disease, and outcomes) were compared with those for persons with other pneumococcal disease rather than with a healthy (nondiseased) control group. However, we thought it useful to try and determine among those with invasive pneumococcal disease whether differences existed among disease caused by serotype 5 and other serotypes.\n\n【41】We do not know why the epidemic was focused in western Canada and why large numbers of cases did not spread to eastern Canada or the United States. Clearly, we do not understand all the dynamics associated with large invasive pneumococcal disease epidemics.\n\n【42】In conclusion, we document a rare large-scale outbreak of invasive pneumococcal disease in western Canada caused by a single clone of _S. pneumoniae_ . The clone possessed a serotype 5 polysaccharide capsule and ST289, indicating that the clone is derived from the international Pneumococcal Molecular Epidemiology Network clone Colombia 5  \\-19 originally described in Colombia . RFLP comparing a collection of _S. pneumoniae_ serotype 5 isolates from the United States with the epidemic clone from western Canada showed that all isolates were identical, suggesting that this strain has been circulating within the United States. However, without direct evidence, we do not know from what part of the world this clone was originally imported into Canada.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "55f6d717-b4a5-4395-8fc8-0e54fc7dc927", "title": "Co-infection with Legionella and SARS-CoV-2, France, March 2020", "text": "【0】Co-infection with Legionella and SARS-CoV-2, France, March 2020\nThe coronavirus disease (COVID-19) pandemic spread to France in mid-February 2020 . Co-infections have been described in patients with COVID-19 , but only 3 co-infections with _Legionella_ have been reported . We report 7 cases of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and Legionnaires’ disease (LD) co-infections in France during March 2020.\n\n【1】### The Study\n\n【2】In France, LD surveillance is based on mandatory notifications to Santé Publique France, the national public health agency. To evaluate LD and COVID-19 co-occurrence, we retrospectively studied all LD case notifications with symptom onset during March 2020 and included cases in which patients had clinical or radiologic signs of pneumonia combined with _Legionella_ culture, positive _Legionella_ PCR from broncho-pulmonary secretions, or positive _Legionella pneumophila_ serogroup 1 urinary antigen test (UAT) results. There were 65 LD case notifications in March 2020 compared with 79 in March 2019. To evaluate the number of UATs, which are performed in 96% of LD cases , we contacted the 59 reporting laboratories (in 47 cities), 33 of which sent the relevant data. The number of UATs increased 2.5-fold (interquartile range: 1.6–2.8) from 3,203 in March 2019 to 8,004 in March 2020. Data obtained from 6 major UAT suppliers indicated a similar 2.1-fold (interquartile range 1.52–14.8-fold) increase in tests sold to laboratories in France, from 33,378 in March 2019 to 65,072 in March 2020. Despite these increases, the number of LD case notifications was 18% lower in March 2020 than in March 2019.\n\n【3】Among the 65 patients from the case notifications, 49 were tested for both LD and COVID-19 and 12 for LD only; no information was available for 4. The frequency of proven LD/COVID-19 co-infection was 14.3% (7/49). This finding may be an overestimate because COVID-19 incidence was <5 cases/100,000 persons in the region of residence of the 16 patients not tested at the time of symptom onset; actual co-infection frequency could be from 10.8% (7/65) to 14.3% (7/49).\n\n【4】Most patients (4/7) with co-infection lived in the Grand Est region, the area in France with the most COVID-19 cases during the study period and a region that usually reports a high number of LD cases. Median patient age was 72 years (range 37–83 years); male-to-female ratio was 6:1 , higher than for the overall COVID-19–infected population . Of interest, the male-to-female ratio for LD has elsewhere been reported as ≈3:1 , similar to the ratio observed in the LD-positive/COVID-19–negative cases.\n\n【5】At hospital admission, co-infected patients had more underlying conditions; 6 (86%) of 7 patients had ≥1, compared with 25 (60%) of 42 LD-positive/COVID-19–negative patients . For cardiovascular diseases, the proportions were 6 of 7 among co-infected and 1 of 42 among LD-positive/COVID-19–negative patients. Sources of LD exposure were reported in case notifications for 3 (43%) of 7 co-infected versus 14 (33%) of 42 non–co-infected patients; the proportions of exposure sources reported was similar between the 2 groups. Despite the implementation on March 15 of the COVID-19 national lockdown in France, halting travel, 12 (24%) of 49 exposures from the LD notifications were travel associated, a ratio similar to that in a 2017 report . Therefore, the decrease in LD cases observed in March 2020 cannot be explained by decreased travel.\n\n【6】Community-acquired LD and COVID-19 were diagnosed at hospital admission in 5 of 7 patients with both infections. For patient 2 , whose symptoms started 48 hours before admission, UAT was not performed until 7 days after admission. Hospital-acquired COVID-19 was suspected in patient 3 because he initially tested negative but was positive after a 4-week hospitalization . All 7 co-infected patients required admission to an intensive care unit (ICU; median stay 13 days, range 2–34 days). In contrast, only 10 (32%) of 31 of LD-positive/COVID-19–negative patients required ICU, similar to LD-only patients in previous reports .\n\n【7】At admission, all 7 co-infected patients had hyperthermia, 6 had cough or dyspnea, and 2 had neurologic symptoms. Five patients needed orotracheal intubation for a median of 13 days (range 3–30 days); acute respiratory distress syndrome developed in 4 patients, and 1 required extracorporeal membrane oxygenation . The median follow-up was 24 days (range 2–34 days); 2 (29%) of 7 patients died, similar to death rates for known ICU LD patients  and severe COVID-19 patients . Three (7%) of 42 LD-positive/COVID-19–negative patients died, consistent with overall LD death rates . Patient 6 died within 3 days after co-infection diagnosis. Patient 3 had progressive pulmonary deterioration and died 6 days after COVID-19 diagnosis. First-line LD treatment was appropriate for all patients; 2 patients received COVID-19 treatment .\n\n【8】The longitudinal follow-up of patient 1, a 71-year-old man receiving chemotherapy for multiple myeloma, may help decipher the kinetics of each pathogen load. Hospitalized for fever (39°C) and productive cough, he required ICU admission on day 9 because of acute respiratory distress syndrome. A thoracic computed tomography scan found left lobar atelectasis, multiple ground-glass opacities compatible with COVID-19, and pleural effusion suggesting possible bacterial infection. Results of UAT and nasopharyngeal SARS-CoV-2 reverse transcription PCR were both positive. On day 10, a serum sample was PCR positive for both SARS-CoV-2 and _Legionella_ ; each pathogen has individually been associated with COVID-19  and LD  severity. Beginning on day 10, longitudinal samples of the lower respiratory tract collected every 3–6 days showed a high SARS-CoV-2 viral load (7.5 log 10  RNA copies/100 cells), followed by a decrease to 1.3 log 10  RNA copies/100 cells within 21 days . In contrast, lung _Legionella_ DNA load increased and remained high (cycle threshold 21.9) until day 31. To identify potential bacterial co-infections, we performed a lung microbiota analysis on a D19 bronchoalveolar lavage using 16S MinION long-read sequencing technology . Similar to another study of LD microbiomes , we found a predominance of _Legionella_ (61%) and the presence of commensal lung bacteria  but no additional bacterial co-infection. On day 26, while high lung _Legionella_ DNA load persisted, a third chest scan found pseudocavitation. Persistence of culture or PCR positivity in respiratory samples, or both, has been described in patients with _Legionella_ lung abscesses, especially if immunocompromised .\n\n【9】### Conclusions\n\n【10】Our study found a substantial proportion of patients in LD notifications in France during March 2020, mostly elderly men with underlying conditions, also had COVID-19. They required ICU admission more frequently and had a higher case-fatality rate than patients without SARS-CoV-2 co-infection, but these rates were similar to that for all ICU-admitted LD patients . Overall, health effects from co-infections were more severe than from single infections, perhaps because of cumulative effects or because patients with co-infections may be more likely to have risk factors associated with poor outcomes. Another possibility is that SARS-CoV-2 infection may be more severe in this population.\n\n【11】Longitudinal monitoring of a single co-infected patient found a first phase of predominant SARS-CoV-2 replication followed by a resurgence of _Legionella_ and worsening of respiratory symptoms while SARS-CoV-2 decreased. Initial viral infection could establish pulmonary damage suitable for the bacteria to develop, similar to how bacterial superinfections develop in influenza-infected patients . Such co-infections may lead to poor prognoses as demonstrated here and elsewhere , underlining the importance of extensive screening for respiratory pathogens in patients with suspected or confirmed COVID-19. Because _Legionella_ and other pulmonary microorganisms share clinical and radiological features with SARS-CoV-2 infection, they should be included in COVID-19 differential diagnoses .\n\n【12】Members of the COVID-19 diagnosis HCL consortium: Antonin Bal, Geneviève Billaud, Grégory Destras, Vanessa Escuret, Sibyle Etievant, Emilie Frobert, Florence Morfin, and Clément Munier", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "db6fa62c-0b99-4f11-86ec-0cfe6bbf6fa6", "title": "Bartonella quintana Transmission from Mite to Family with High Socioeconomic Status", "text": "【0】Bartonella quintana Transmission from Mite to Family with High Socioeconomic Status\n**To the Editor:** Urban trench fever caused by _Bartonella quintana_ has been reported in persons who abuse alcohol and in homeless persons in large cities worldwide. Symptoms vary from asymptomatic intermittent bacteremia to serious complications . _Pediculus humanus_ lice, the known vector of the infection, are not always identified, which raises the possibility that other vectors might also be involved . We report on an outbreak of _B. quintana_ infection among a young family of high socioeconomic status and their visiting relatives.\n\n【1】The family resides in a regional city (population 104,000) in northern Czech Republic in an old, renovated apartment located on the top floor, just under the roof. In the summer of 2007, hundreds of ectoparasitic mites migrated from a hole in the roof and settled on the inner side of a permanently open window before infesting family members. Two weeks later (day 1 of symptom onset), a papular rash and pruritic vesicular lesions were noted by the parents on the body and legs of their 2 children, a 1-year-old girl and a 3-year-old boy. On day 3, the girl’s body temperature rose to 38.0°C, and the boy’s temperature rose to 39.5°C. The rash resolved in ≈10 days in both children. Vesicular lesions on the girl’s buccal mucosal membrane resolved in 5 days. Excoriated areas resulting from spontaneous rupture of lesions or scratching were still visible on day 14.\n\n【2】On day 4, a fever (temperature, 38.5°C) and intense tibialgia, which persisted for 5 days, developed in the 33-year-old father of the infected children. On day 5, a vesicular rash, which resolved in 10 days, developed in the 33-year-old mother. The children’s grandfather and both grandmothers also showed symptoms of infection within ≈14 days after having spent \\> 1 days or nights in the infected family’s household . In addition, the regional epidemiologist who was involved in the investigation showed development of a severe infection 16 days after exposure to implicated mites that escaped from a collection tube . Recurrent fevers of decreasing intensity, followed by remissions at 1-week intervals, were observed in all patients for up to 3 months.\n\n【3】Seven mites, which were collected by the father on day 6 after symptom onset, were identified as engorged and nonengorged members of the genus _Dermanyssus_ . After treatment with ethanol, the mites were investigated by culture and DNA analysis. DNA fragments specific for _Bartonella_ spp. (i.e. a 185-bp  and a 397-bp  fragment of the 16S rRNA gene) were amplified; the sequence of the 397-bp fragment was 100% similar to the _htrA_ sequence of the _B. quintana_ strain Toulouse . Results were negative for PCRs with primers for 16S rDNA of _Anaplasma phagocytophilum_  and primers for _ospA_ of _Borrelia burgdorferi_ . Only _Staphylococcus cohnii_ subsp. _urealyticus_ , as part of human or animal commensal flora, was detected on blood agar plates that were cultured for 30 days in a microaerophilic atmosphere.\n\n【4】Patient samples were analyzed by using the specific 16S rRNA primers; the _Bartonella_ \\-specific amplicon was found only in a sample that was collected on day 4 from the father. Amplification of the _htrA_ gene fragment of identical size and with identical sequences also confirmed the presence of DNA specific for _B. quintana_ in the father’s sample. Hemocultures were not performed at symptom onset, but results for patient serum samples cultured under the same conditions as the homogenized parasites remained negative. Significant titers of IgG against _B. quintana_ and _B. henselae_ or IgG seroconversion in paired serum samples were observed for all patients except the grandfather .\n\n【5】Oral clarithromycin and doxycycline were administered to the children and adults, respectively, for 10 days. The apartment was repeatedly treated with insecticide, and the hole in the roof was repaired, leading to eradication of the mites. The few dead and dry mites that were available for additional parasitologic analysis were mounted in Swan mounting medium (information about the medium is available from the authors), but no characteristics allowing differentiation between species of the genus _Dermanyssus_ were recognized during examination by light microscopy. Failed attempts were made to trap pigeons that had lived on the roof of the apartment or in the same city; however, samples from trapped synanthropic pigeons from the north (n = 20) and central (n = 33) part of the country were negative for _Bartonella_ spp. by the culture and amplification methods described above. Recurrent fever reported by adult patients resolved in 3 months, and all patients made a full clinical recovery. Laboratory findings for the patients were followed for 6 months after symptom onset .\n\n【6】The fact that the suspected vector was a hematophagous mite ( _Dermanyssus_ sp.), a parasite of synanthropic pigeons and a suspected vector of other bacterial pathogens , and that the 16S rRNA _Bartonella_ spp. gene was detected in mites ( _Steatonyssus_ sp. from the superfamily _Dermanyssoidea_ )  remains a challenge for additional study. Pigeons probably played the role of accidental host in this outbreak, but the source of the infection remains unclear.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9a6e939d-703f-49bb-b794-a8adedba0a00", "title": "Association of Burden and Prevalence of Arthritis With Disparities in Social Risk Factors, Findings From 17 US States", "text": "【0】Association of Burden and Prevalence of Arthritis With Disparities in Social Risk Factors, Findings From 17 US States\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Social risks previously have been associated with arthritis prevalence and costs. Although social risks often cluster among individuals, no studies have examined associations between multiple social risks within the same individual. Our objective was to determine the association between individual and multiple social risks and the prevalence and burden of arthritis by using a representative sample of adults in 17 US states.\n\n【3】**Methods**\n\n【4】Data are from the 2017 Behavioral Risk Factor Surveillance System. Respondents were 136,432 adults. Social risk factors were food insecurity, housing insecurity, financial insecurity, unsafe neighborhoods, and health care access hardship. Weighted χ 2  and logistic regression analyses, controlling for demographic characteristics, measures of socioeconomic position, and other health conditions examined differences in arthritis prevalence and burden by social risk factor and by a social risk index created by summing the social risk factors.\n\n【5】**Results**\n\n【6】We observed a gradient in the prevalence and burden of arthritis. Compared with those reporting 0 social risk factors, respondents reporting 4 or more social risk factors were more likely to have arthritis (adjusted odds ratio \\[AOR\\], 1.92; 95% CI, 1.57–2.36) and report limited usual activities (AOR, 2.97; 95% CI, 2.20–4.02), limited work (AOR, 2.72; 95% CI, 2.06–3.60), limited social activities (AOR, 3.10; 95% CI, 2.26–4.26), and severe joint pain (AOR, 1.86; 95% CI, 1.44–2.41).\n\n【7】**Conclusion**\n\n【8】Incremental increases in the number of social risk factors were independently associated with higher odds of arthritis and its burden. Intervention efforts should address the social context of US adults to improve health outcomes.\n\n【9】Introduction\n------------\n\n【10】Approximately 1 in 4 US adults have medically diagnosed arthritis, which is the most common cause of disability in the US, with over $300 billion in costs in 2013 . Although no cures exist for arthritic conditions, a number of biomedical and behavioral factors can modify the prevalence and burden of arthritis; these include obesity and physical activity . However, these traditionally measured factors fail to adequately explain patients’ risk of developing arthritis and the burden of arthritis (ie, mortality, morbidity, or financial cost).\n\n【11】In recent years, public policy groups who engage in community health care and cost management have increasingly recognized the influence of health risks related to social context . Whereas clinical care pathways for arthritis commonly consider biological and psychological factors , social determinants of health (SDOH) are not routinely taken into account. SDOH are broadly defined as the conditions in which people are born, work, live, and play and include areas such as economic stability, education, social and community context, and the built environment . Social risk factors are individual-level adverse SDOH that can be identified through screening tools (eg, positive for housing insecurity), although social needs are social risks prioritized by patients (eg, request for housing assistance) . Because arthritis is so prevalent and costly, it is essential that the influence of social risk factors on arthritis is better understood.\n\n【12】Studies have quantified the relative contribution of traditional biomedical factors and social influences such as education and income to the development and burden of arthritis . However, social risk factor variables do not routinely exist alone and may cluster cross-sectionally. Shifting from a single risk factor analysis to a more comprehensive perspective that seeks to understand the complexity of coexisting social risk factors may offer greater insight into the effect of the accumulation of influences on health . No studies to date, however, have examined the effect of multiple or coexisting social risk factors on arthritis prevalence and burden. Our aim was to explore the association between social risk factors and the prevalence and burden of arthritis individually and with a social risk index based on the total number of social risk factors reported. A social risk index is a composite statistic that measures changes in a representative group of systemic social issues, or a compounding measure that aggregates multiple indicators .\n\n【13】Methods\n-------\n\n【14】This study adheres to REporting of studies Conducted using Observational Routinely collected Data (RECORD) guidelines . No patients were involved in the design or conduct of this study. The study was approved by the Institutional Review Board at Duke University.\n\n【15】### Study design and participants\n\n【16】Our study was a cross-sectional analysis of the Centers for Disease Control and Prevention (CDC) 2017 Behavioral Risk Factor Surveillance System (BRFSS) survey from 17 US states that administered the SDOH module to a total of 136,432 respondents. We analyzed the data in October 2020. The BRFSS is an annual, nationally representative computer-assisted telephone survey of health-related risk behaviors, health conditions, and use of preventive services among noninstitutionalized adults, aged 18 years or older. The median response rate in 2017 in this sample for telephone and cellular telephone respondents combined was 46.5% (range, 32.8%–64.1%) . Of the states that administered the SDOH module in 2017, 13 (Florida, Georgia, Iowa, Kentucky, Massachusetts, Minnesota, Mississippi, New Hampshire, Pennsylvania, Utah, West Virginia, Wisconsin, and Wyoming) administered the module to their entire sample, whereas 4 (Colorado, Maryland, Ohio, and Oklahoma) administered the module to 1 or more splits in their samples. Split samples maximize information gathering while reducing survey fatigue by dividing the sample into equivalent probability-based samples. We merged the sample split data sets with the national BRFSS data sets by using publicly available 2017 BRFSS data files from CDC. More information about split sample methods and use is available from CDC .\n\n【17】### Social risk factor predictor variables\n\n【18】Based on the Commission on Social Determinants of Health Final Report published by the World Health Organization , we selected all 6 questions from the BRFSS SDOH module and 1 question on health care access from the core BRFSS survey. We combined these variables to create 5 social risk factors: food insecurity, housing insecurity, financial insecurity, unsafe neighborhood, and health care access hardship. These variables were selected because addressing the social risks they represent creates substantial opportunities to address the social determinants of arthritis outcomes . A positive response for food insecurity was identified when a participant responded “often true” or “sometimes true” to the statements, “The food that I bought just didn’t last, and I didn’t have money to get more” and “I couldn’t afford to eat balanced meals.” A positive response for housing insecurity was identified when a participant responded yes to the question, “During the last 12 months, was there a time when you were not able to pay your mortgage, rent, or utility bills?” A positive response for financial insecurity was identified when a participant responded, “\\[We did\\] not have enough money to make ends meet” or “\\[We\\] have just enough money to make ends meet” to the question, “In general, how do your finances usually work out at the end of the month?” A positive response to unsafe neighborhood was identified when a participant reported “unsafe” or “extremely unsafe” to the question, “How safe from crime do you consider your neighborhood to be?” A positive response for health care access hardship was identified when a participant responded yes to the question, “Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?” Each response was dichotomized to indicate exposure. Positive responses to each social risk factor were summed to create a social risk index. Social risk index scores ranged from 0 (no social risk factors reported) to 4 or more (a positive response to each social risk factor reported). Higher scores indicated increasing numbers of social risk factors.\n\n【19】### Arthritis-related outcome variables\n\n【20】To identify the prevalence of arthritis, we used an umbrella designation reflective of diagnostic conditions. For the BRFSS, arthritis is defined a yes response to the question, “Have you ever been told by a doctor or other health care professional that you have arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia?” This question has been in use since 2002 and was designed to incorporate elements of the 1994 public health definition of arthritis developed by the National Arthritis Data Workgroup . All 4 questions from the arthritis burden module were used to identify burden. Limited activities were defined as a yes response to the question, “Are you now limited in any way in any of your usual activities because of arthritis or joint symptoms?” Limited work was defined as a response of yes to the question, “Do arthritis or joint symptoms now affect whether you work, the type of work you do, or the amount of work you do?” Limited social activities was defined as the response of yes to the question, “During the past 30 days, to what extent has your arthritis or joint symptoms interfered with your normal social activities, such as going shopping, to the movies, or to religious or social gatherings?” Respondents with severe joint pain were described by those whose responses ranged from 7 to 10 to the question, “Please think about the past 30 days, keeping in mind all of your joint pain or aching and whether or not you have taken medication. On a scale of 0 to 10 where 0 is no pain or aching and 10 is pain or aching as bad as it can be, during the past 30 days, how bad was your joint pain on average?” This description for severe joint pain has been used in previous public health surveillance, and the BRFSS has been extensively validated .\n\n【21】### Covariates\n\n【22】We selected covariates on the basis of their potential to influence the predictor or outcome . Demographic characteristics were age, sex, race, ethnicity, marital status, and health insurance. Measures of socioeconomic position were educational attainment, income, current employment status, and home ownership and were included because they are conceptually linked with social risk factors. Health conditions were heart attack, coronary heart disease, stroke, asthma, skin cancer, other cancer, chronic obstructive pulmonary disease, depression, chronic kidney disease, obesity, and multimorbidity (defined as the presence of at least 2 of the aforementioned health conditions). We included these health conditions because arthritis often co-occurs with other conditions .\n\n【23】### Statistical analysis\n\n【24】Descriptive statistics for the overall sample and the social risk index were analyzed for demographic characteristics, measures of socioeconomic position, and health conditions. Colinearity among predictor social risk variables was assessed using the φ (phi) coefficient. We defined a correlation of 0.50 or more as indicative of colinearity. Prevalence and 95% CIs of each of the 5 selected social risk factors and the social risk index were calculated. Weighted Wald χ 2  analyses were used to calculate bivariate associations between arthritis prevalence and burden and between each social risk factor and social risk index score. Multivariable weighted logistic regression models adjusted for all covariates were used to examine the association of arthritis prevalence and burden with each social risk factor and the social risk index score. Multicolinearity was assessed for each model by using variance inflation factors. We defined a variance inflation factor of 3.0 or more as indicative of multicolinearity. Significance was set at _P_ < .05. Analyses were performed using R version 4.0.2 (R Foundation for Statistical Computing) and weighted to account for the complex sampling design and nonresponses. We report weighted percentages and weighted adjusted odds ratios with 95% CIs.\n\n【25】Results\n-------\n\n【26】### Sample\n\n【27】Weighted estimates of demographics, measures of socioeconomic position, and health conditions for the full sample (N = 136,432) and each of the social risk factors are presented. All ages were well represented, and respondents were predominantly White (70.9%), had an annual income of $50,000 or more (49.0%), and had at least a high school diploma (88.1%). The percentage of respondents residing in the West was 8.4%, whereas 47.4% resided in the South.\n\n【28】Overall, 54.3% (95% CI, 53.8%–54.8%) of the respondents reported experiencing none of the 5 social risk factors, 25.0% (95% CI, 24.5%–25.4%) reported 1 social risk factor, 11.2% (95% CI, 10.9%–11.6%) reported 2, 6.3% (95% CI, 6.0%–6.6%) reported 3, 2.8% (95% CI, 2.6%–2.9%) reported 4, and 0.4% (95% CI, 0.3%-0.5%) reported all 5. Mean social risk index scores were higher among women and young to middle-aged adults. Respondents identifying as multiracial, Black, and Hispanic had the highest mean social risk index scores, compared with “other” and White groups.\n\n【29】### Bivariate associations of social risk factors and arthritis prevalence and burden\n\n【30】No evidence of colinearity was found among social risk factors. The prevalence of reporting arthritis, being limited in activities, limited work, limited social activities, or severe joint pain was significantly higher among those reporting each social risk factor individually . In models adjusted for demographic characteristics, socioeconomic position, and health conditions, the presence of each social risk factor was associated with each outcome. Adjusted odds ratios ranged from 1.20 (95% CI, 1.11–1.29) to 2.33 (95% CI, 1.98–2.74) for the 5 social risk factors . No evidence of multicolinearity was found.\n\n【31】### Social risk index\n\n【32】The prevalence of each outcome increased with the number of social risk factors reported. Presented are the absolute prevalence of arthritis  and burden by number of social risk factors  .\n\n【33】**  \nWeighted absolute prevalence of arthritis by number of social risk factors in the 2017 BRFSS sample. The prevalence of arthritis increases linearly as the number of social risk factors increase. \n\n【34】**  \nthe weighted absolute prevalence of each health outcome among BRFSS participants with arthritis, 2017 BRFSS sample. The prevalence of each outcome increases linearly as the number of social risk factors increase. \n\n【35】In models adjusted for demographic characteristics, socioeconomic position, and health conditions, the gradient between each outcome and the number of associated social risk factors increased. Adjusted odds ratios ranged from 1.86 (95% CI, 1.44−2.41) to 3.10 (95% CI, 2.26−4.26) when respondents reported 4 or more social risk factors . No evidence of multicolinearity was reported.\n\n【36】Discussion\n----------\n\n【37】We observed a report of prevalence of arthritis and burden among individuals with social risk factors. Specifically, we noted a gradient in the social risk index where, for each additional social risk factor reported, the odds of a respondent reporting having arthritis, being limited in activities, work, and social activities, and reporting severe joint pain also increased. Associations persisted after controlling for measures of social context including demographic characteristics, measures of socioeconomic position, and the presence of other health conditions.\n\n【38】Our study contributes to the literature on arthritis and social risks in several ways. This study is, to our knowledge, the largest to examine the prevalence of arthritis and its associated burden by social risk factors and the first to examine the impact of co-occurring social risks on arthritis. We defined our social risk factors as a combination of individual and neighborhood-level factors and evaluated their combined influence on arthritis prevalence and burden. In contrast, many previous studies limited their perspective on social risks to only socioeconomic factors .\n\n【39】Considering the increasing prevalence and the substantial burden of arthritis among US adults, understanding the relationships between social risks and arthritis is an important public health effort . Our findings suggest that the social risk factors measured have a relationship with arthritis independent of socioeconomic position and known risk factors such as age, sex, employment, body mass index, and race or ethnicity . These factors represent modifiable social contexts that can be influenced directly through public policy, improved access to health care, and clinical programs . Furthermore, mechanisms underlying these relationships might allow mediation through other health behaviors such as physical activity, diet, and sleep . Additionally, social support and psychological state are likely proximal to health outcomes and present opportunities to develop interventions to reduce the burden of arthritis.\n\n【40】Identifying the association between social risk factors and arthritis aids understanding the social contribution to the prevalence and burden of arthritis. These findings may provide additional momentum for health systems and organizations not yet addressing social risks. Because as much as 80% of the factors that influence health are outside the traditional health care system, unmet social needs may limit an organization’s ability to achieve quality benchmarks associated with value-based payment. Though some health systems are making investments to address social risk factors , the aggregate benefit of these investments may not exceed the cost to primary participants in community health, thus limiting the funding needed to address social risks . Spreading the cost and the financial benefits of interventions to address social risk shows potential ways to increase uptake among health care systems and managed care organizations .\n\n【41】Because data on key markers of social risk are sparse in many organizations, proxies such as the percentage of a clinician’s patient population that is dually enrolled in Medicare and Medicaid are commonly used. However, because Medicaid eligibility varies from state to state and dual-enrollees’ health and risk profiles vary from state to state , we see potential value in operationalizing social risks as a count for risk-adjustment purposes. In clinical settings, a count of social risks can be calculated and interpreted easily by using social risk screening tools.\n\n【42】Implementing policies to minimize the exposure of disadvantaged populations to social risk factors, and thus reducing potential vulnerability, is urgently needed . Including screening for social risk factors in care pathways may be one way to reduce the unequal prevalence and burden of arthritis . Previous studies have identified increased referral to and use of wraparound services including clinicians, such as social workers, dieticians, and behavioral health clinicians when such pathways are implemented . Including social risk factors in stepped care models may also be an important action toward improving the equity of arthritis-related health outcomes .\n\n【43】Our study is limited by its use of cross-sectional data from the BRFSS; therefore, causality cannot be established. It is possible that the presence of functional limitations could exacerbate social vulnerabilities. For example, pain related to arthritis may be related to the ability to work, and financial insecurity might be a result of a decreased ability to work and increasing care needs. Our inability to include this variable in analyses may have altered our results. Because as much as 80% of the factors that influence health are outside the traditional health care system, unmet social needs may limit an organization’s ability to achieve quality benchmarks associated with value-based payment. Other social risks known to be associated with arthritis, such as perceived discrimination, were not available in the BRFSS. BRFSS relies on self-reported information and retrospective reporting of social risks that can introduce memory and response biases. Our findings may not be representative of all adults in the US, as only 17 states chose to include the SDOH module in 2017, and 10 of those states had a proportion of persons living in poverty below the national average that year .\n\n【44】Social risk factors individually and concurrently according to a social risk index are associated with disparities in arthritis prevalence and burden, as those with 4 or more social risk factors had nearly twice the odds of having arthritis as those with no risk factors. Future research focusing on determining mechanisms that underlie these relationships is warranted. Results from our study inform the need for those engaged in clinical care and policy making to consider the social environment of individuals with arthritis. Reducing the prevalence and burden of arthritis by addressing social determinants is warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "06d42772-4f09-4700-ae8b-e224e1d3531d", "title": "Reptile-associated Salmonellosis in Man, Italy", "text": "【0】Reptile-associated Salmonellosis in Man, Italy\n**To the Editor:** Reptiles are reservoirs of a wide variety of _Salmonella_ serotypes, including all S _almonella enterica_ subspecies and _S_ . _bongori_ . In reptiles born in captivity or kept as pets, _S. enterica_ subsp. _enterica_ is frequently isolated . _Salmonella_ strains are well adapted to reptiles, and they usually cause asymptomatic infections in such animals, while retaining pathogenicity for warm-blooded animals. For several years, reptiles have been recognized as a source of human salmonellosis. In North America, reptile-associated salmonellosis (RAS) has been reported, particularly in children, the elderly, or immunocompromised persons; severe and fatal infections are described occasionally . In contrast, only a limited amount of information on RAS is available in Europe. We report a case of RAS that occurred in an adult man in Italy.\n\n【1】A 32-year-old man had symptoms of enteritis. For 2 weeks, he had experienced intermittent watery diarrhea, mild fever, and abdominal pain. He was then treated with ciprofloxacin, and after 15 days of treatment, he recovered from enteritis. A stool sample, collected before treatment, underwent bacteriologic analysis, and _Salmonella_ spp. were identified biochemically (api 20E, bioMérieux, Marcy l'Etoile, France) and by a polymerase chain reaction assay specific for the _invA_ gene of _Salmonella_ spp . Since the man was a reptile owner, RAS, rather than a foodborne infection, was initially suspected. He owned several cold-blooded animals; all had been tested for _Salmonella_ spp. (at least 3 times at 2- to 3-week intervals), and results were negative. Three weeks before the onset of enteric symptoms, he acquired a boa ( _Boa imperator_ ) that was subjected to routine analysis for _Salmonella_ spp. in our laboratories . _Salmonella_ spp. were isolated from a cloacal swab of the snake. Subsequently, both the human and reptile _Salmonella_ isolates were characterized as _S_ . _enterica_ serovar Paratyphi B. In addition, both strains were found to be d-tartrate–fermenting (dT+) biovars , susceptible to ampicillin, amoxicillin-clavulanic acid, cephalothin, ceftazidime, gentamicin, streptomycin, chloramphenicol, tetracycline, neomycin, nalidixic acid, norfloxacin, and ciprofloxacin and resistant to sulfamethoxazole and co-trimoxazole.\n\n【2】By pulsed-field gel electrophoresis analysis of DNA, the strains displayed the same pattern, which suggests a clonal origin . The isolates were also assayed for virulence-associated genes. The _Sop_ E1 gene was detected in both isolates, and the _avr_ A gene was not detected, which is consistent with an invasive pathovar of _S_ . Paratyphi B . Conversely, the _spv_ C, _pef_ , and _sef_ genes were not detected .\n\n【3】In recent years, a general increase in RAS detection has been observed, which may be the result of the increasing diffusion of reptiles as pets and a better awareness of RAS risk. In the United States, annual reports of RAS cases are published by the Centers for Diseases Control and Prevention . In Europe, studies on free-living and captive reptiles have shown a high prevalence of _Salmonella_ spp . Nevertheless, national surveillance systems for RAS do not exist, and epidemiologic data are incomplete.\n\n【4】Notably, since Sweden became a member of the European Union in 1995, and the import restriction rules for reptiles were removed, a marked increase in RAS was observed in that country . As the deregulation of the trade in reptiles is applied, in agreement with the European Union laws, a similar scenario may be projected in other European countries. As is the case for nontyphoid salmonellosis, RAS may be underestimated, especially if patients are not hospitalized. Although a few cases of RAS have been previously reported in children in Italy , this report provides the first description of RAS in adults. _S_ . Paratyphi B dT+, also known as _S. enterica_ serovar Java, has been isolated in reptiles and tropical fish and has been associated with epidemics of human salmonellosis acquired from food, such as goat milk or chicken . The evidence shows that salmonellosis by _S_ . Paratyphi B dT+ apparently occurs more frequently in adults , while so-called exotic reptile strains seem to be more prone to causing salmonellosis in children , which has led to the proposition that _S_ . Paratyphi B dT+ strains may be highly pathogenic. By screening virulence-associated genes, both our isolates were found to be _Sop_ E1+ and _avr_ A–, a pattern usually observed in the systemic pathovars of _S_ . Paratyphi B  and associated with invasiveness, which suggests a high pathogenic potential. Accordingly, strict preventive sanitation measures should be adopted when handling reptiles , and reptiles should be always regarded as a potential source of pathogenic _Salmonella_ strains for humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3878ecb8-01c7-4356-a600-c134b694ac3a", "title": "Susceptibility of Human Prion Protein to Conversion by Chronic Wasting Disease Prions", "text": "【0】Susceptibility of Human Prion Protein to Conversion by Chronic Wasting Disease Prions\nChronic wasting disease (CWD) is a fatal contagious prion disease of cervids that is found in the United States, Canada, South Korea, and most recently in Europe . The species affected differ in these geographic areas; mule deer ( _Odocoileus hemionus_ ), white-tailed deer ( _O. virginianus_ ), North American elk ( _Cervus canadensis_ ), and moose ( _Alces alces_ ) are most commonly affected in the United States and Canada, and red deer ( _C. elaphus_ ) and sika deer ( _C. nippon_ ) in South Korea also have been affected . The first identification of CWD in Europe occurred in 2016 in wild moose ( _A. alces_ , also known as Eurasian elk) and in a free-ranging reindeer ( _Rangifer tarandus,_ closely related to the free-ranging caribou of North America), a species not previously known to be affected by CWD in wild or farmed animals . CWD is a pressing animal health issue, but whether it might become a human public health issue should also be considered.\n\n【1】Scrapie and bovine spongiform encephalopathy (BSE) are well-characterized animal prion diseases affecting animals of consumption; scrapie affects sheep and BSE cattle. Human prion diseases occur as sporadic, genetic, and acquired forms; the variant form of Creutzfeldt-Jakob disease is zoonotic BSE, acquired through the oral route and having a long incubation period . In contrast, sheep scrapie is generally considered to be of no or only very low risk to human health, although this possibility has been questioned recently . The molecular basis of prion replication is a change in conformation of the normal cellular prion protein (PrP C  ) into the abnormal and misfolded conformer (PrP Sc  ) that is partially protease-resistant (PrP res  ). However, the molecular criteria for predicting zoonotic potential for prions are unclear. Consequently, approaches to understanding the zoonotic risk for prion transmission to humans have been empirical, involving in vivo and in vitro models . Experimental transmission of mule deer CWD to nonhuman primates showed that squirrel monkeys are susceptible, whereas cynomolgus macaques appear to be resistant . Similarly, attempted transmission of deer or elk CWD to transgenic mouse lines expressing human PrP C  have largely failed, indicative of a species barrier , although this might be overcome in some instances .\n\n【2】In contrast to these in vivo approaches, we have used protein misfolding cyclic amplification (PMCA) to investigate the molecular compatibility of bovine, ovine, and cervid prions with full-length, glycosylated and glycosylphosphatidylinositol-anchored human prion protein (PrP) . We found that scrapie samples failed to convert human PrP C  to PrP res  , whereas cattle BSE converted the human protein efficiently . These observations suggest that PMCA can reproduce aspects of cross-species transmission potential and inform assessment of zoonotic risk. The single CWD-affected specimen (from a North American elk) previously available to us was found to be capable of converting human PrP C  to PrP res  . Here, we expand on the previous report; analyzing more elk specimens of 2 different genotypes (132 MM, homozygous for methionine at _Prnp_ position 132, and 132 ML, methionine–leucine heterozygous at the same position), analyzing white-tailed deer CWD specimens for the first time, and analyzing reindeer that have been experimentally infected with white-tailed deer CWD. The elk ML polymorphism at position 132 of _Prnp_ is of particular interest because it corresponds to the methionine–valine (MV) _PRNP_ codon 129 polymorphism in humans, which is itself a major genetic susceptibility factor associated with human prion disease . Although these examples of CWD all derive from North America, the inclusion of white-tailed deer (a relative of European roe deer, _Capreolus capreolus_ ), North American elk (a near relative of red deer), and, most important, reindeer might help in formulating risk assessments in Europe.\n\n【3】### Materials and Methods\n\n【4】##### Animal Tissue\n\n【5】We obtained from the National and OIE Reference Laboratory for Scrapie and CWD (Ottawa, ON, Canada) elk, white-tailed deer, and reindeer frozen brain tissue from CWD-affected animals that had been confirmed positive by using a statutory diagnostic testing regime . Small pieces of tissue were obtained, and we analyzed all for the presence of PrP res  by using Western blot, as previously described . We used transgenic mouse brains produced by gene replacement and expressing physiologic levels of the human PrP of the 3 _PRNP_ codon 129 polymorphic genotypes as the PMCA substrate .\n\n【6】##### Protein Misfolding Cyclic Amplification\n\n【7】##### Substrate and Seed Preparation\n\n【8】We homogenized transgenic mouse brains in conversion buffer by using a glass on glass manual grinder and a conversion buffer made of 1× phosphate-buffered saline (PBS), 150 mmol/L of NaCl, 1% Triton X-100, and a complete protease inhibitor cocktail (cOmplete; Roche, Mannheim, Germany) to obtain a final 10% weight-to-volume solution. We cleared the homogenized tissue by using centrifugation as described previously , aliquoted the supernatant (i.e. PMCA substrate) into 1.5 mL tubes, and stored at −80°C until used. We homogenized the CWD brain material (i.e. PMCA seed) by using 1.5-mL Eppendorf tubes and disposable polypropylene pestles that used the same buffer.\n\n【9】##### PMCA Procedure\n\n【10】We performed amplification in a programmable Q-700 sonicator attached to a microplate aluminum horn . We mixed brain homogenate CWD PMCA seeds with aliquots of PMCA substrate in a final volume of 120 µL in PCR tubes at a 1:3 ratio. We included low molecular weight heparin at 100 µg/mL  in all PMCA reactions and added EDTA to a final concentration of 6 mmol/L. To perform a comparison between samples before and after the amplification procedure, we took 19 µL of each reaction mixture before the serial cycles of sonication and incubation. Each cycle consisted of 20 s sonication (at an amplitude of 38, wattage 278–300) followed by 29 min and 40 s incubation; we repeated this procedure 96 times (48 h).\n\n【11】##### Proteolytic Treatment and Western Blotting\n\n【12】We evaluated the presence of PrP res  by Western blot after proteinase K treatment. We incubated 19 µL of each sample with proteinase K in a final concentration of 50 µg/mL for 1 h at 37°C in a standard thermoblock. Before loading, we mixed samples with an appropriate volume of 4× NuPAGE buffer (Invitrogen, Carlsbad, CA, USA) and boiled them at 100°C for 10 min. We loaded the samples on NuPAGE Novex (Fisher Scientific; Loughborough, United Kingdom) 10% Bis-Tris gels (1.0 mm, 10 wells) and subjected them to electrophoresis at 200 volts for 55 min. We transferred proteins to a polyvinylidene difluoride membrane by using 800 mA for 60 min  and blocked membranes with 2% milk for 1 h. We determined accumulated human PrP res  on the basis of the specific immunoreactivity of the 3F4 monoclonal antibody (mAb) diluted 1/10,000 (Millipore; Watford, United Kingdom). We detected CWD PrP by stripping the Western blots (Thermo Fisher, Bleiswijk, Netherlands) and reprobing them with a 6H4 antibody diluted 1/40,000 (Prionics; Schlieren, Switzerland). We used ECL antimouse IgG, peroxide-linked species-specific F(ab′)2 fragment from sheep (GE Healthcare Life Sciences; Little Chalfont, United Kingdom) as a secondary antibody diluted 1/25,000. We developed membranes by chemiluminescent detection using ECL Prime (GE Healthcare Life Sciences) and acquired digital images by using an XRS Bio-Rad system (Bio-Rad Laboratories, Hercules, CA, USA) with a CCD camera.\n\n【13】##### Criteria for Positivity\n\n【14】The criterion for conversion was 3F4 antibody detection of a pattern of 3 clear protease-resistant bands of the expected electrophoretic mobility and ratio for PrP res  in the amplified sample after a defined period of Western blot image capture. The triplet pattern had to be absent from the unamplified sample tested under the same conditions analyzed in parallel.\n\n【15】##### Precipitation of Insoluble PrP\n\n【16】We incubated brain homogenate from white-tailed deer and reindeer CWD specimens by using 20% sarkosyl (diluted in PBS) for 10 min at room temperature. We subjected samples to centrifugation for 1 h at 100,000 × _g_ at 4°C as described previously . After centrifugation, we discarded the supernatant and washed the pellet with PBS followed by a second centrifugation (100,000 × _g_ for 1 h at 4°C). We resuspended the washed pellets directly in PMCA substrate before using them in a single round of amplification.\n\n【17】### Results\n\n【18】##### Determination of Total PrP in Cervid CWD Specimens\n\n【19】We first characterized the CWD brain tissues for the presence of total PrP and PrP res  by using mAb 6H4. We detected similar levels of total PrP by using Western blotting among the elk specimens analyzed . However, we did not find readily detectable levels of PrP res  in all the samples; we detected PrP res  in 3 of the 5 elk specimens of the 132 MM genotype and both of the 132 ML samples . White-tailed deer samples showed similar expression levels of total PrP in the 2 specimens analyzed, but PrP res  levels were low . We also confirmed total PrP in the reindeer specimens, with a robust detection of PrP res  in 1 of the available samples (reindeer 1) but low levels in the other specimen (reindeer 2) .\n\n【20】##### In vitro Conversion of Human PrP by Cervid Prions\n\n【21】##### Elk ( _C. Canadensis_ )\n\n【22】We then performed a single round of PMCA, incubating the _PRNP_ codon 132 MM elk CWD seeds in humanized transgenic 129 MM mouse brain substrate. Only those elk CWD samples that had readily detectable PrP res  (as detected by 6H4 mAb) were able to produce human PrP res  (detectable by the 3F4 mAb) after Western blot and proteinase K treatment, consistent with CWD PrP res  playing a direct role in the misfolding process .\n\n【23】We then addressed the role of the human _PRNP_ codon 129 and the cervid _PRNP_ codon 132 polymorphisms in the conversion of human PrP C  . To seed PMCA reactions, we used CWD brain homogenates from 132 MM and 132 ML elk with comparable quantities of CWD PrP res   and then used humanized transgenic mouse substrates of the 3 possible _PRNP_ codon 129 genotypes (129 MM, 129 MV, and 129VV). Each of the CWD methionine homozygous (132 MM) samples resulted in human PrP res  formation when used to seed the matched humanized substrate (129 MM). The heterozygous elk seeds did not result in any detectable conversion of the humanized 129 MM PrP substrate. When we incubated these same CWD brain homogenates with the heterozygous (129 MV) and homozygous (129VV) humanized substrates, we observed low levels of human PrP res  formation but no obvious difference in efficiency between the CWD 132MM and CWD 132ML samples in the 129VV substrate .\n\n【24】##### White-tailed Deer ( _O. virginianus_ )\n\n【25】We performed similar analyses to determine the competence of white-tailed deer CWD to convert the human PrP . To maintain consistency, we homogenized the 2 available CWD white-tailed deer specimens at 10% (weight/volume) and then normalized by volume  to seed the PMCA reactions. Detection of CWD PrP res  by 6H4 antibody revealed that the levels of PrP res  in the unamplified samples were not equivalent to the elk CWD specimens used for PMCA . However, 1 of the analyzed specimens showed some conversion of the humanized 129 MM PrP substrate, although PrP res  formation was undetectable in the heterozygous and valine homozygous substrate with the 3F4 antibody . These results suggest a higher degree of molecular compatibility of the _PRNP_ codon 129 MM human genotype and CWD PrP Sc  , consistent with what we observed in most of the elk CWD 132 MM specimens.\n\n【26】##### Reindeer ( _R. tarandus_ )\n\n【27】Natural cases of CWD in reindeer have been detected in Norway, but North American reindeer were previously shown to be experimentally susceptible to white-tailed deer CWD by the oral route . We tested samples from these 2 experimentally infected reindeer  . Both reindeer specimens were capable of converting the humanized 129 MM PrP substrate, although different amounts of CWD PrP res  were detected by the 6H4 antibody in the frozen samples. Densitometry analysis suggested that 1 specimen (from reindeer 2) had roughly one tenth of the PrP res  of the other sample, showing that the reaction with small amounts of reindeer PrP res  are able to convert the humanized substrate. The _PRNP_ codon 129 MV and VV genotype substrates also were readily susceptible to conversion by the reindeer seeds . Seeding efficiency of reindeer CWD was maintained when the seeding material was normalized by using semipurified PrP res  , arguing against the possibility that the apparent enhanced seeding potential of reindeer CWD simply reflects the increased abundance of PrP res  in reindeer samples or was a result of conversion of endogenous reindeer seed–associated PrP C  .\n\n【28】### Discussion\n\n【29】Characterization of the transmission properties of CWD and evaluation of their zoonotic potential are important for public health purposes. Given that CWD affects several members of the family _Cervidae_ , it seems reasonable to consider whether the zoonotic potential of CWD prions could be affected by factors such as CWD strain, cervid species, geographic location, and _Prnp_ – _PRNP_ polymorphic variation. We have previously used an in vitro conversion assay (PMCA) to investigate the susceptibility of the human PrP to conversion to its disease-associated form by several animal prion diseases, including CWD . The sensitivity of our molecular model for the detection of zoonotic conversion depends on the combination of 1) the action of proteinase K to degrade the abundant human PrP C  that constitutes the substrate while only N terminally truncating any human PrP res  produced and 2) the presence of the 3F4 epitope on human but not cervid PrP. In effect, this degree of sensitivity means that any human PrP res  formed during the PMCA reaction can be detected down to the limit of Western blot sensitivity. In contrast, if other antibodies that detect both cervid and human PrP are used, such as 6H4, then newly formed human PrP res  must be detected as a measurable increase in PrP res  over the amount remaining in the reaction product from the cervid seed. Although best known for the efficient amplification of prions in research and diagnostic contexts, the variation of the PMCA method employed in our study is optimized for the definitive detection of zoonotic reaction products of inherently inefficient conversion reactions conducted across species barriers. By using this system, we previously made and reported the novel observation that elk CWD prions could convert human PrP C  from human brain and could also convert recombinant human PrP C  expressed in transgenic mice and eukaryotic cell cultures .\n\n【30】A previous publication suggested that mule deer PrP Sc  was unable to convert humanized transgenic substrate in PMCA assays  and required a further step of in vitro conditioning in deer substrate PMCA before it was able to cross the deer–human molecular barrier . However, prions from other species, such as elk  and reindeer affected by CWD, appear to be compatible with the human protein in a single round of amplification (as shown in our study). These observations suggest that different deer species affected by CWD could present differing degrees of the olecular compatibility with the normal form of human PrP.\n\n【31】The contribution of the polymorphism at codon 129 of the human PrP gene has been extensively studied and is recognized as a risk factor for Creutzfeldt-Jakob disease . In cervids, the equivalent codon corresponds to the position 132 encoding methionine or leucine. This polymorphism in the elk gene has been shown to play an important role in CWD susceptibility . We have investigated the effect of this cervid _Prnp_ polymorphism on the conversion of the humanized transgenic substrate according to the variation in the equivalent _PRNP_ codon 129 polymorphism. Interestingly, only the homologs methionine homozygous seed–substrate reactions could readily convert the human PrP, whereas the heterozygous elk PrP Sc  was unable to do so, even though comparable amounts of PrP res  were used to seed the reaction. In addition, we observed only low levels of human PrP res  formation in the reactions seeded with the homozygous methionine (132 MM) and the heterozygous (132 ML) seeds incubated with the other 2 human polymorphic substrates (129 MV and 129 VV). The presence of the amino acid leucine at position 132 of the elk _Prnp_ gene has been attributed to a lower degree of prion conversion compared with methionine on the basis of experiments in mice made transgenic for these polymorphic variants . Considering the differences observed for the amplification of the homozygous human methionine substrate by the 2 polymorphic elk seeds (MM and ML), reappraisal of the susceptibility of human PrP C  by the full range of cervid polymorphic variants affected by CWD would be warranted.\n\n【32】In light of the recent identification of the first cases of CWD in Europe in a free-ranging reindeer ( _R. tarandus_ ) in Norway , we also decided to evaluate the in vitro conversion potential of CWD in 2 experimentally infected reindeer . Formation of human PrP res  was readily detectable after a single round of PMCA, and in all 3 humanized polymorphic substrates (MM, MV, and VV). This finding suggests that CWD prions from reindeer could be more compatible with human PrP C  generally and might therefore present a greater risk for zoonosis than, for example, CWD prions from white-tailed deer. A more comprehensive comparison of CWD in the affected species, coupled with the polymorphic variations in the human and deer _PRNP–Prnp_ genes, in vivo and in vitro, will be required before firm conclusions can be drawn. Analysis of the _Prnp_ sequence of the CWD reindeer in Norway was reported to be identical to the specimens used in our study . This finding raises the possibility of a direct comparison of zoonotic potential between CWD acquired in the wild and that produced in a controlled laboratory setting.\n\n【33】The prion hypothesis proposes that direct molecular interaction between PrP Sc  and PrP C  is necessary for conversion and prion replication. Accordingly, polymorphic variants of the PrP of host and agent might play a role in determining compatibility and potential zoonotic risk. In this study, we have examined the capacity of the human PrP C  to support in vitro conversion by elk, white-tailed deer, and reindeer CWD PrP Sc  . Our data confirm that elk CWD prions can convert the human PrP C  , at least in vitro, and show that the homologous _PRNP_ polymorphisms at codon 129 and 132 in humans and cervids affect conversion efficiency. Other species affected by CWD, particularly caribou or reindeer, also seem able to convert the human PrP. It will be important to determine whether other polymorphic variants found in other CWD-affected _Cervidae_ or perhaps other factors  exert similar effects on the ability to convert human PrP and thus affect their zoonotic potential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f40d7c55-861e-4fbb-805f-7ccd3a69cd25", "title": "Outbreak of Hantavirus Pulmonary Syndrome, Los Santos, Panama, 1999–2000", "text": "【0】Outbreak of Hantavirus Pulmonary Syndrome, Los Santos, Panama, 1999–2000\nHantavirus pulmonary syndrome (HPS) is an infectious disease typically characterized by fever, myalgia, and headache and followed by dyspnea, noncardiogenic pulmonary edema, hypotension, and shock . Common laboratory findings include elevated hematocrit, leukocytosis with the presence of immunoblasts, and thrombocytopenia . The case-fatality rate can be as high as 52% . The etiologic agent of HPS is any one of several hantaviruses carried by rodent hosts belonging to the family _Muridae_ , subfamily Sigmodontinae . Hantaviruses are most often transmitted to humans through the inhalation of infectious rodent feces, urine, or saliva. However, strain-specific virus transmission may occur from person to person .\n\n【1】HPS was first recognized in 1993 during an outbreak of severe respiratory disease in the Four Corners Region of the United States . Since then, 363 cases of HPS have been confirmed in the United States . Sin Nombre virus (SNV) was the first HPS-causing pathogen identified; its primary rodent reservoir host is the deer mouse, _Peromyscus maniculatus_ . However, four other hantaviruses, Bayou virus, Black Creek Canal virus, New York virus, and Monongahela virus, each with a different rodent reservoir, have been characterized in the United States and associated with HPS .\n\n【2】Since 1993, HPS has also been reported and confirmed in six countries in South America—Argentina, Bolivia, Brazil, Chile, Paraguay, and Uruguay . Several distinct hantaviruses have been associated with HPS, including Andes virus (Argentina, Bolivia, Chile, and Uruguay), Bermejo virus (Bolivia), Juquitiba virus (Brazil), Laguna Negra virus (Bolivia, Paraguay), Lechiguanas virus (Argentina), and Oran virus (Argentina) .\n\n【3】Before 2000, no human hantavirus infections had been reported in Central America. However, in February 2000, health officials in Panama reported a cluster of acute respiratory illnesses in residents of the district of Las Tablas in Los Santos Province from late December 1999 to February 2000 . Human illness was characterized by a febrile prodrome with rapid progression to moderate-to-severe respiratory distress. Serum specimens from three of four patients had immunoglobulin (Ig) M and IgG antibodies to SNV. In February and March 2000, an outbreak investigation by the Panamanian Ministry of Health, Gorgas Memorial Institute for Health Studies (Panama City), the U.S. Centers for Disease Control and Prevention (Atlanta, GA), and the Pan American Health Organization was conducted in collaboration with local medical and public health officials. Sequence analysis of virus genome from human serum samples and rodent tissue led to the identification of a novel hantavirus, Choclo virus, as the cause of HPS in this outbreak . This report summarizes the clinical, epidemiologic, and environmental findings of the investigation.\n\n【4】### Materials and Methods\n\n【5】##### Case Definition\n\n【6】A suspected case of HPS was defined as fever (temperature \\> 38.3°C) and unexplained acute respiratory distress requiring supplemental oxygen, with radiographic evidence of acute respiratory distress syndrome or bilateral interstitial pulmonary infiltrates . A suspected case was also defined as an unexplained respiratory illness resulting in death, with a postmortem examination indicating noncardiogenic pulmonary edema without identifiable cause . A confirmed case of HPS was defined as a clinically compatible illness plus the presence of one of the following; 1) hantavirus-specific IgM antibodies in acute-phase serum, 2) hantavirus-specific nucleic acid sequences by reverse transcriptase polymerase chain reaction (RT-PCR), or 3) hantavirus-specific antigen by immunohistochemistry .\n\n【7】##### Case Finding and Characterization\n\n【8】At visits to two hospitals in Panama City and one in the Las Tablas District that admitted patients with suspected HPS, hospitalized patients were examined and interviewed when possible, and medical charts were reviewed. To identify past HPS patients, retrospective chart reviews were conducted on admissions dating back to August 1999 at these hospitals. In addition, medical records of any previous or current suspected HPS patients admitted to other district hospitals in the Las Tablas Province were obtained and reviewed. Clinical case information was collected on a standard abstraction form.\n\n【9】To monitor the spread of disease in Las Tablas and other areas of Los Santos Province, an outbreak communications center was established at the Ministry of Health in Panama City and staffed by physicians, public health officials, and health educators. Operations of this center included 1) passive surveillance for suspected cases of HPS, 2) a public hotline that addressed symptoms and signs of HPS and methods of prevention, and 3) nationwide distribution of HPS educational materials. Staff physicians also called hospitals throughout Panama to promote awareness of HPS among medical providers.\n\n【10】##### Community Surveys\n\n【11】Surveys for hantavirus antibodies were conducted on March 5, 2000, in six neighborhoods in the Las Tablas District and one in the Guarare District in which confirmed HPS patients resided. The primary objectives of the surveys were to determine the prevalence of hantavirus infection within households and neighborhoods of case-patients and the frequency of mild and asymptomatic infection.\n\n【12】Approximately 10–15 households, including the case-patient household, were sampled in each of the seven neighborhoods. Teams composed of public health officials, nurses, and phlebotomists visited each neighborhood. The day before the survey, pamphlets explaining the survey in Spanish were given to each household that had agreed to participate. All participants were administered a standard questionnaire that asked about demographic characteristics, illness history, and rodent exposure. Team members were familiarized with the questionnaire before the survey started. A 10-mL blood specimen (3–5 mL for children) for hantavirus serologic testing was collected from each interviewed participant.\n\n【13】##### Hospital Survey\n\n【14】A hospital survey was conducted March 9–10, 2000, among healthcare workers who cared for confirmed HPS patients at a major hospital in Panama City. This hospital received most of its cases from Las Tablas. The objectives of the survey were to 1) serologically determine exposure to hantavirus among doctors, nurses, respiratory therapists, physiotherapists, and nurses’ aides who provided direct care to confirmed HPS patients and 2) assess whether person-to-person transmission occurred. Medical staff who provided direct medical care (i.e. < 1 m from the patient) to infected patients in the emergency room and medical intensive care unit were compared with those in the coronary or neurologic intensive care units who had no exposure to case-patients. A standard self-administered questionnaire was used to inquire about timing and amount of exposures, specific types of exposures, and precautionary measures taken. A 10-mL blood specimen for hantavirus serologic testing was collected from each surveyed participant.\n\n【15】##### Rodent Investigation\n\n【16】Small mammals were sampled to determine potential hantavirus reservoir hosts. Primary sampling methods were trapping and collecting small mammals around households of confirmed and suspected HPS patients and from two uninhabited locations in Los Santos. The habitat of each trapping area was described. Small mammals were initially identified in the field on the basis of external characters together with standard keys for the region. Definitive identification that used cranial characteristics was performed at the Museum of Southwest Biology, University of New Mexico, where voucher specimens from these small mammals are currently archived. Liver, kidney, spleen, lung, heart, and a whole blood sample were collected from each trapped rodent for diagnostic testing, and carcasses were preserved for rodent speciation. Trapping and sampling were performed according to established safety protocols . Panamanian team members were trained to trap and observe safety precautions when handling potentially infectious rodents.\n\n【17】##### Diagnostic Testing\n\n【18】All available serum specimens from suspected HPS patients were tested for IgM antibodies by using an IgM-capture format and inactivated SNV antigen and for IgG antibodies by an indirect enzyme-linked immunosorbent assay (ELISA) as previously described . Serum samples from survey participants, rodents, and other small mammals were tested for IgG antibodies by using SNV antigens. IgG-positive survey participants were also tested for IgM antibodies by use of SNV antigens. Positive findings with SNV antigens in the IgG- or IgM-capture ELISAs indicate infections with New World hantaviruses rather than with SNV specifically.\n\n【19】### Results\n\n【20】##### Characteristics of HPS Patients\n\n【21】Eleven case-patients with suspected HPS were identified and reported to the Panamanian Ministry of Health from December 25, 1999, to February 29, 2000 . Three patients (25%) died; neither serum nor tissue samples were available for diagnostic testing from these three patients. The remaining eight cases were confirmed by the presence of IgM against hantavirus in at least two separate serum specimens from patients; all eight also had IgG antibodies, with five demonstrating IgG seroconversion after the first specimen. One additional patient, whose onset of illness was August 24, 1999, was identified retrospectively. Although acute-phase clinical specimens were not available for testing, review of medical records indicated that her illness was clinically compatible with HPS, and her IgG titer in February 2000 was elevated . She was subsequently confirmed as the ninth HPS patient.\n\n【22】The median age of the 12 patients was 42 years (range 26–58 years; seven (58%) were women. The primary occupation of patients was as follows: secretary , housewife , truck driver/transporter , electrician , field pump worker , seamstress , teacher , security guard , and unidentified .\n\n【23】All cases occurred in the Las Tablas, Guarare, and Tonosi Districts of Los Santos Province. In Las Tablas and Guarare, eight towns had at least one patient with suspected HPS, and seven had one or more confirmed patients . Two cases, one suspected and one confirmed, were identified in Tonosi. No clustering of cases occurred at the household level.\n\n【24】Two patients denied handling or cleaning up rodent excreta before their illnesses, while a third admitted frequently killing and handling rodents. In interviews, family members of all case-patients from Las Tablas District noted marked increases in the number of rodents in and around the home from November 1999 to February 2000.\n\n【25】##### Clinical Description of Confirmed HPS Patients\n\n【26】The spectrum of preadmission symptoms of the nine confirmed patients is documented in Table 2 . The mean temperature on admission was 38.0°C (n = 6, range 37.3°C–39.6°C). The median systolic and diastolic blood pressure values were 100 mm Hg (n = 9, range 80–130) and 63 mm Hg (n = 9, range 50–80), respectively. The median pulse rate was 102/min (n = 8, range 60–172). The median respiratory rate was 23/min (n = 8, range 14–36). No patient was unconscious or cyanotic on admission. A temperature of \\> 38.5°C was documented in all patients at some time during hospitalization. Seven of nine patients were hypotensive during their illness (median lowest systolic blood pressure = 90, range 50–100), and three required inotropic therapy.\n\n【27】Laboratory values at admission and during hospitalization are listed in Table 3 . Eight of nine patients were thrombocytopenic either on admission or at some time during hospitalization. Five of nine had hematuria (urine erythroctyes >10 per how power field, while four of nine had moderate proteinuria ( < 2+ on urine dipstick). Evidence of renal function abnormalities was present in three of nine patients whose serum creatinine levels were >2.0 mg/dL.\n\n【28】Three of nine patients had aspartate aminotransferase >500 U/L, alanine aminotransferase >300 U/L, total bilirubin >1.5 mg/dL, and direct bilirubin >1.0 mg/dL. Two patients had clinical evidence of liver disease—one with hepatomegaly and the other with scleral icterus. Hematemesis, melena, and coagulopathy (prothrombin time = 22 s, partial thromboplastin time = 167 s, fibrinogen = 161 mg/dL) also developed in one patient. Two of the three patients also had elevated serum amylase values (294 and 437 U/dL).\n\n【29】Seven of nine patients had chest x-ray examinations on admission. All had evidence of bilateral infiltrates, and one of seven had a radiographic pattern suggestive of pulmonary edema. Two patients who did not have chest x-ray examinations on admission subsequently were found to have bilateral interstitial infiltrates radiographically. Pulmonary edema of varying severity developed in four patients during the course of their illnesses; pleural effusions also occurred in three of these patients, and they were intubated within 24 to 72 hours after admission.\n\n【30】##### Community Survey\n\n【31】Interviews and serum specimens were obtained from 311 (83%) of 376 residents of seven different neighborhoods of Las Tablas and Guarare Districts in which confirmed HPS patients were identified. These 311 survey participants represented 119 households. A minimum of one blood specimen was obtained from each household. Forty (13%) of 311 survey participants had IgG against hantavirus. By sex, 25 (14%) of 178 females and 15 (11%) of 133 males had IgG antibodies. The median age of the 40 antibody-positive participants was 31.5 years (range 1–79). Age group prevalence estimates ranged from 9% (5/53, >61years of age) to 21% (6/28, 51–60 years of age). Of 47 children, 5 (11%) < 10 years of age had IgG antibodies. Antibody prevalences among patients’ neighborhoods ranged from 6% to 31% . Two of 10 household members of confirmed patients had IgG antibodies. Six (15%) of the 40 infected participants had visited a confirmed HPS patient.\n\n【32】Of 119 households surveyed, 31 (26%) had at least one member who had IgG against hantavirus . Eight (8%) of 96 households had two or more antibody-positive members, while 1 (2%) of 54 households had three or more antibody-positive members. Prevalence of antibody-positive households (i.e. one or more antibody-positive members) per neighborhood ranged from 8% to 73%.\n\n【33】Among the 40 antibody-positive participants, occupations of those with the highest antibody prevalences were students (14%), secretaries (13%), agricultural workers (13%), livestock or vegetable farmers (12%), and housewives (11%). Among the 40 infected participants, 33 (82.5%) touched rodents, 31 (77.5%) cleaned up rodent droppings (e.g. sweeping, mopping), and 29 (72.5%) killed rodents after December 1, 1999. Fifteen (37.5%) noted increased numbers of peridomestic rodents compared with previous years.\n\n【34】Only five (12.5%) of 40 antibody-positive participants recalled fever or myalgia since December 1, 1999. In contrast, the most common symptoms reported were upper respiratory in nature, such as rhinorrhea (45%), sore throat (35%), and cough (22%). Moreover, of two participants who had both IgM and IgG antibodies, one experienced only a cough without fever, while the other was asymptomatic.\n\n【35】##### Hospital Serosurvey\n\n【36】Questionnaires and serum samples were obtained from 38 directly exposed and 39 unexposed healthcare workers. No IgM antibodies were present in the 77 workers. Only one of 38 exposed workers had IgG antibodies. This person, a medical resident, directly cared for five confirmed HPS patients (each healthcare worker cared for an average of 4 patients, range 1–5) for a total of approximately 15 patient-care days (group mean 43, range 1–70). His first exposure occurred in late December 1999. While caring for patients, he wore gloves, gown, and mask most of the time and was not directly exposed to respiratory secretions. He denied any travel to Los Santos Province after December 1999 and had no history of exposure to rodents. However, he was uncertain about whether he had visited Los Santos Province before December 1999. He denied having any febrile illness after late December 1999. Of the remaining 37 healthcare workers who cared for HPS patients, four (11%) were directly exposed to respiratory secretions in the eye, nose, or mouth; 26 (70%) wore gloves, gowns, and masks most of the time.\n\n【37】One unexposed worker, an operating room assistant, also had IgG against hantavirus. She denied travel to Los Santos Province, exposure to rodents, or febrile illness after December 1999. However, history of travel to Los Santos Province before December 1999 was again uncertain.\n\n【38】##### Rodent Investigation\n\n【39】Rodent traps were set at 13 sites, 10 of which were homes and immediate surroundings of patients with confirmed and suspected HPS. One of the three remaining sites was the home of a patient previously suspected to have HPS who did not have antibodies to hantavirus. The other two locations were a rural agricultural area in the Pocri District of Los Santos Province and a late secondary forest area (two subsites) near the town of Portabelo, Montijo District, Veraguas Province. In all, 120 rodents representing nine species and seven opossums (two species) were captured . Of the 120 rodents, 52 (43%) were caught from the 10 patient household areas. A trap success of approximately 5% was achieved at these homes. Only one of the six antibody-positive rodents ( _Oligoryzomy fulvescens_ ) was captured at the household of a confirmed HPS case-patient (Las Tablas town); five were captured in the Pocri District.\n\n【40】### Discussion\n\n【41】We have documented the first human cases of hantavirus infection in Central America. A novel Panamanian hantavirus, Choclo virus, was subsequently characterized and is thought to be responsible for HPS during the outbreak . Virus genetic sequences of Choclo virus from case-patients were identical to those from _O. fulvescens_ .\n\n【42】The clinical spectrum of pulmonary disease among HPS patients in Panama varied widely from severe disease requiring intubation and cardiovascular support to mild pulmonary involvement with a benign clinical course. Extrapulmonary manifestations, such as hepatobiliary disease, hemorrhage, and central nervous system sequelae, were also present. The case-fatality rate (0% among 9 confirmed cases; 25% among 12 total suspected and confirmed cases) was noticeably lower than that described in Paraguay (12%), Chile (54%), and the United States (52%) in confirmed cases .\n\n【43】We found an antibody prevalence of 13% among household and neighborhood members of all ages from the outbreak foci. This figure is comparable to that found in Paraguay, but higher than in Chile and the United States . The percentages of hantavirus antibody-positive persons and households were particularly high in San Jose, the only town with two confirmed case-patients. Clustering of cases of HPS was not observed, and household clustering of antibody-positive persons was infrequent. Only 13% of hantavirus antibody-positive persons had a febrile illness after early December 1999, and none had an illness compatible with HPS, which suggests that mild infections occurred, as documented with other hantaviruses in the United States and Chile .\n\n【44】Person-to-person transmission of hantavirus infection was not demonstrated during the outbreak. Infrequent clustering of antibody-positive serosurvey participants by household probably reflected common exposures to infected rodent excreta peridomestically. Furthermore, only 1 of 38 medical care workers who cared for HPS patients had IgG antibodies, while none had IgM antibodies. Similarly, 1 of 39 workers who did not care for HPS patients had IgG antibodies. Thus far, person-to-person transmission has only been suggested during outbreaks caused by Andes virus in Argentina and Chile .\n\n【45】Climate data from Los Santos Province clearly demonstrated a two- to threefold increase in rainfall in September and October 1999 when compared to similar periods in the previous 4 years . Such atypical rainfall patterns may have led to increases in rodent populations, which led to more frequent contact between infected rodents and humans and subsequent human infection. Many residents of patient-neighborhoods of Las Tablas reported an unusually large number of rodents from December 1999 through January 2000. Similar patterns of environmental change followed by outbreaks of human disease have been observed in the United States and South America .\n\n【46】Ecologic observations in Los Santos provided an impression of a dry, open, and deforested region. Long-term commercial logging, agriculture (primarily corn and sugarcane), and animal husbandry practices (mainly cattle and horses) have contributed to the deforestation process. The rodent species most closely associated with corn and sugarcane fields, _Zygodontomys brevicauda cherriei_ , is a likely reservoir host for a new hantavirus, Calabazo virus . _O. fulvescens_ , the likely reservoir for Choclo virus, was found in grass of varying heights near human habitation and in cattle and horse pastures. Given the observed habitat associations of these two species, agriculture and animal husbandry practices in the Los Santos regions may have had a positive effect on populations of rodents associated with hantaviruses and may continue to augment the risk for HPS as the human population increases.\n\n【47】The social and economic impact of this outbreak was substantial. The cancellation of Carnival in Las Tablas, one of Panama’s most celebrated festivals, had profound emotional and financial effects on residents of Las Tablas. Nevertheless, the public health impact of holding Carnival could also have been substantial, given the potential for rodent exposure among thousands of visitors and participants.\n\n【48】In conclusion, this outbreak resulted in the first documented cases of human hantavirus infections in Central America. Although cases were not reported from other districts of Los Santos or other provinces of Panama during the investigation, surveillance for HPS nationwide should continue, as serologic testing capabilities have since been implemented in Panama. More extensive sampling of rodent populations would help identify other areas in Panama with large numbers of _O. fulvescens_ that could place residents at risk for Choclo virus infection. Longitudinal studies of rodents, particularly those species implicated as reservoir hosts, will be necessary to monitor the fluctuation and distribution of rodent population numbers over time and their correlation with human infection . Educational campaigns promoting risk reduction, such as proper clean-up of rodent excrement, sealing homes against the entry of rodents, and other rodent-proofing techniques, should continue as additional cases of HPS are reported in Panama.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "459161ee-1449-4b05-ba2e-0701fc2222b1", "title": "Extended-spectrum β-Lactamase-producing Flora in Healthy Persons", "text": "【0】Extended-spectrum β-Lactamase-producing Flora in Healthy Persons\n**To the Editor:** Extended-spectrum β-lactamase (ESBL)–producing gram-negative bacilli are endemic in hospitals. In intensive care units, 2% prevalence of ESBL-producing organisms has been reported . Exceedingly high rates of ESBL-producing bacteria in Indian hospitals prompted us to look at the fecal carriage of ESBL in the community .\n\n【1】One hundred healthy executives received a comprehensive health check at our tertiary care center in central Mumbai from August to September 2004. The predominant isolates from stool samples obtained for routine examination were cultured, and initial screening for ESBL production was conducted by using the disk diffusion method according to NCCLS guidelines . For these isolates, the ESBL phenotypic confirmation was performed with ceftazidime-clavulanate for an increase in zone diameter by 5 mm (disk potentiation). In addition, the ATB BLSE strip (bioMérieux, Lyon, France) was used to confirm the presence of inhibitor (sulbactam)-susceptible enzymes and to differentiate the strains from those that were either inhibitor resistant or harboring other β-lactamases, such as those of AmpC derivation. The ATB BLSE strip consists of a varying concentration of ceftazidime, 0.5–32 mg/L, and aztreonam, 0.5–8 mg/L, with varying combinations of these agents with a β-lactamase inhibitor, i.e. + sulbactam, 0.06–1 mg/L. Cefotetan (4 and 32 mg/L) and imipenem (4 and 8 mg/L) were also included in the strip. The test was considered positive when a variation of ≥4 dilutions was observed between the antimicrobial agent tested alone and the agent combined with the inhibitor. Eleven of the 100 samples screened were positive for ESBL-producing _Escherichia coli_ and _Klebsiella pneumoniae_ . Seven of the 11 were confirmed by using the ATB BLSE strip. The MIC of ceftazidime and aztreonam in all 7 isolates was 8 μg/mL. We might be underreporting ESBL producers in these cases by not including the cefotaxime-clavulanate combination in addition to the ceftazidime-clavulanate concentration. The percentage resistance to ciprofloxacin was 45%. All isolates were susceptible to amikacin and the carbapenems. None of the executives gave a history of hospitalization in the last year or history of antimicrobial drug consumption in the last 6 months.\n\n【2】This trend in patients with no apparent risk factors for ESBL carriage calls for urgent attention. Unknown environmental factors are likely playing a key role in maintaining this selective pressure. Larger studies are required to substantiate these findings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bcc95c4b-d436-41b9-8c26-e5e48524b32b", "title": "Babesia sp. EU1 from Roe Deer and Transmission within Ixodes ricinus", "text": "【0】Babesia sp. EU1 from Roe Deer and Transmission within Ixodes ricinus\nBabesiosis is a zoonosis caused by intraerythrocytic piroplasms of the genus _Babesia_ , which are transmitted by ticks . In Europe, ≈30 human cases of babesiosis have been reported over the past 50 years and have been traditionally attributed to infection with the bovine parasite _B. divergens_ transmitted by _Ixodes ricinus_ . However, in 2003, Herwaldt et al. described the first molecular characterization of a new _Babesia_ species, _Babesia_ sp. EU1, isolated from 2 persons in Austria and Italy . Since this description, EU1 has been recovered from roe deer in Slovenia  and from _I. ricinus_ in Slovenia  and Switzerland .\n\n【1】_Babesia_ species EU1 merits increased attention as a potential agent of emerging tickborne disease in humans because its suspected vector, _I. ricinus_ , is the most prevalent and widely distributed tick in Europe and frequently bites humans. To evaluate the public health importance of EU1, its vector, animal reservoir hosts, and geographic distribution must be identified. We identified EU1 in roe deer and in _I. ricinus_ in western France.\n\n【2】### The Study\n\n【3】In January 2005 and January 2006, 89 roe deer at the Wild Fauna Reserve of Chizé (Deux Sèvres, France) were captured; blood was obtained through the jugular vein and analyzed for infection by _Babesia_ spp. Parasites were isolated from autologous roe deer erythrocytes as described , except that 20% fetal calf serum (FCS) was used. Cultures were monitored for parasitemia by examination of Giemsa-stained thin blood smears. Parasites were then adapted to culture in blood from deer ( _Dama dama)_ from the Jardin des Plantes of Nantes (Loire Atlantique, France) and from sheep reared in our laboratory. Adaptation was conducted as described , except that 20% FCS was also used.\n\n【4】A total of 150 μL parasite genomic DNA was then prepared from 10-mL cultures (10% parasitemia, 2.5% hematocrit) according to the protocol of a commercial extraction kit (Promega, Madison, WI, USA) on merozoite preparations obtained by Percoll gradient centrifugation (Amersham, Uppsala, Sweden) with a density of 1.08 g/mL in 0.15 M NaCl. PCR amplifications were performed with 10 μL DNA and BAB primers  as described . For positive samples, 5 μL DNA was subjected to a second amplification with EU1 primers  as described by Hilpertshauser et al. except that the annealing temperature was 63°C and uracil DNA glycosylase was not used.\n\n【5】During January 2005, 31 of 79 roe deer analyzed were infected with _Babesia_ spp. as shown by parasite culture and PCR amplification with BAB primers. Of 29 cultures tested for EU1 with the corresponding primers, 59% were positive, which indicated an estimated global EU1 prevalence of 23% . In January 2006, 5 of 10 cultures tested contained _Babesia_ spp. parasites. Sequencing of the complete 18S rRNA gene from subcultures in autologous deer and sheep erythrocytes amplified with the primer set CRYPTO   showed that 2 of these cultures (C210 and C201) had 100% identity with the EU1 human strain  . The unique sequence obtained has been deposited in GenBank .\n\n【6】In January 2005, a total of 106 engorged female adult _I_ . _ricinus_ were collected from the 31 roe deer harboring _Babesia_ spp. Ticks were then reared in the laboratory at 22°C and a relative humidity of 80%–90%. Forty-two ticks (from 22 roe deer) laid eggs from which larvae were analyzed for parasites with BAB primers as described ; 64% of larvae samples had a positive reaction. Amplification products from egg sample E177.3 and larva sample L177.3 that were sequenced showed 100% identity with the 18S rRNA gene of EU1 . The sequence has been submitted to GenBank . Among positive samples, 6 of 15 analyzed for EU1 with specific primers showed a positive reaction .\n\n【7】### Conclusions\n\n【8】Isolation of EU1 from roe deer in France confirms that these animals are reservoir hosts of the parasite and that EU1 is not restricted to 1 geographic area in Europe. A survey conducted in Slovenia showed that 21.6% of 51 roe deer tested were infected with EU1  and a similar prevalence (23%) was observed.\n\n【9】To our knowledge, this is the first isolation of EU1 in culture in homologous erythrocytes and erythrocytes from other ruminants. Until now, _Babesia_ sp. EU1 has only been detected in roe deer  and humans . It has also been detected in _I_ . _ricinus_ collected from sheep and goats in Switzerland ; however, the ticks in that study may have acquired the infection at a preceding stage during a blood meal taken on another host.\n\n【10】In a study in Slovenia in 1997, 2.2% of 135 _I_ . _ricinus_ collected by flagging vegetation were positive by PCR for EU1 . PCR studies in Switzerland that examined ticks collected from domestic and wild ruminants with unknown parasitologic status showed that 1%–2% contained EU1 DNA . In our study, 40% of larvae samples from female ticks collected on _Babesia_ \\-infected roe deer were infected with EU1. We assume that ticks do not necessarily become infected or transmit the parasite to the next generation after a blood meal on a EU1-infected host because 5 larval pools that originated from female ticks collected on EU1-infected roe deer were not infected .\n\n【11】DNA sequences of the 18S rRNA gene were identical in parasites isolated from roe deer (C201 and C210) or _I_ . _ricinus_ samples. This finding indicates that deer and ticks were infected with the same organism, which may be transmitted by the tick. In addition to _I_ . _ricinus,_ EU1 DNA has been isolated from _Haemaphysalis punctata_ ticks in Switzerland . However, during this survey, entire ticks or the apical part of fully engorged females were tested. Positive results from such samples indicated infection status only, not proof of the vectorial capacity of the tick .\n\n【12】We report that EU1 is transmitted within _I_ . _ricinus_ and that transovarial transmission occurs in this tick, as shown by detection of parasite DNA in eggs and larvae from females collected on roe deer. Some EU1-positive eggs and larvae can originate from adults engorged on EU1-uninfected roe deer, as observed in 3 roe deer (L128.3, L177.3, and L179.1). This finding suggests that the parasite was acquired during a preceding blood meal and that transstadial transmission occurred, at least from nymph to adult. Further investigations are needed to clarify the ability of _I_ . _ricinus_ to acquire and transmit _Babesia_ sp. EU1. This species, which has been isolated from 2 human cases of babesiosis, should be studied to determine other potential reservoir hosts because of its potential as an emerging zoonotic pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e31cb1e1-59ee-45de-a251-6f3d2ee0c7bc", "title": "Body Measurements", "text": "【0】Body Measurements\nData are for the U.S.\n\n【1】Measured average height, weight, and waist circumference for adults aged 20 and over\n\n【2】*   **Men:**  \n    Height in inches: 69.0  \n    Weight in pounds: 199.8  \n    Waist circumference in inches: 40.5\n*   **Women:  \n    **Height in inches: 63.5  \n    Weight in pounds: 170.8  \n    Waist circumference in inches: 38.7\n\n【3】Source: Anthropometric Reference Data for Children and Adults: United States, 2015–2018, tables 4, 6,10, 12, 19, 20 pdf icon \\[PDF – 1 MB\\]\n\n【4】Related FastStats\n-----------------\n\n【5】*   Obesity/Overweight\n\n【6】More data\n---------\n\n【7】*   Mean Body Weight, Height, Waist Circumference and Body Mass Index among Children and Adolescents: United States 1999-2018 pdf icon \\[PDF – 575 KB\\]\n\n【8】Related LInks\n\n【9】*   Body Mass Index table external icon\n*   CDC Growth Charts\n*   National Health and Nutrition Examination Survey\n*   National Center for Chronic Disease Prevention and Health Promotion – Overweight and Obesity", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e0060e89-6b9a-49c4-a60d-32fac5584519", "title": "Etymologia: Bayesian Probability", "text": "【0】Etymologia: Bayesian Probability\n### Bayesian Probability\n\n【1】Thomas Bayes   was a Presbyterian Minister, and how he become interested in statistics and probability is uncertain. Bayes presented his famous theorem on probability in “An Essay Towards Solving a Problem in the Doctrine of Chances,” which was published posthumously by his friend Richard Price in 1763. Bayes’s theorem provides a method of explicitly including prior events or knowledge when considering the probabilities of current events (for example, including a history of smoking when calculating the probability of developing lung cancer). Bayesian approaches use prior knowledge and information (e.g. probabilities) that may help reduce uncertainty in analysis and have therefore been increasingly adopted by analysts in public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "24fd404d-cf23-460c-8ec1-9c84a2b1c36b", "title": "Sequelae of Foodborne Illness Caused by 5 Pathogens, Australia, Circa 2010", "text": "【0】Sequelae of Foodborne Illness Caused by 5 Pathogens, Australia, Circa 2010\nFoodborne gastroenteritis is a major source of illness in Australia, causing an estimated 4.1 million (90% credible interval \\[CrI\\] 2.3–6.4 million) illnesses, 30,600 (90% CrI 28,000–34,000) hospitalizations, and 60 (90% CrI 53–63) deaths each year . In addition to the direct effects of these illnesses, infection with some pathogens can result in sequelae, which can be severe, require multiple hospitalizations, and be costly to society . We report on the effects of sequelae associated with Guillain-Barré syndrome (GBS), hemolytic uremic syndrome (HUS), irritable bowel syndrome (IBS), and reactive arthritis (ReA) from 5 pathogens acquired from contaminated food in Australia.\n\n【1】Each of these 4 sequel illnesses are preceded by different gastrointestinal infections and have unique characteristics. GBS, a rare but serious autoimmune illness, affects the nervous system and causes acute flaccid paralysis. GBS can occur as a sequel to _Campylobacter_ spp. infection 10 days–3 weeks after gastrointestinal illness . HUS is characterized by acute renal failure, hemolytic anemia, and thrombocytopenia and can result from infection with Shiga toxin–producing _Escherichia coli_ (STEC) ≈4–10 days after onset of gastroenteritis . IBS is a gastrointestinal disorder that causes abdominal pain and bowel dysfunction. It is not life threatening, but it can cause substantial health effects after illness with _Campylobacter_ spp. nontyphoidal _Salmonella enterica_ serotypes (hereafter referred to as nontyphoidal _Salmonella_ spp.), or _Shigella_ spp. ReA is a type of spondyloarthritis that can develop up to 4 weeks after an enteric infection from _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. or _Yersinia enterocolitica_ . We estimated the number of illnesses, hospitalizations, and deaths resulting from GBS, HUS, IBS, and ReA from selected foodborne pathogens in Australia in a typical year circa 2010.\n\n【2】### Methods\n\n【3】We estimated the effects of foodborne sequelae acquired in Australia circa 2010 using data from multiple sources in Australia and from international peer-reviewed literature. We defined foodborne sequelae as illnesses occurring after bacterial gastroenteritis caused by eating contaminated food. Sequelae were defined as the secondary adverse health outcomes resulting from a previous infection by a microbial pathogen and clearly distinguishable from the initial health event . Illness can be acute, such as with HUS, or chronic (lasting for many years), as with IBS. We estimated incidence, hospitalizations, and deaths with uncertainty bounds using Monte Carlo simulation in @Risk version 6 , which incorporates uncertainty in both data and inputs. Each stage of our calculation was represented by a probability distribution, and our final estimates of incidence, hospitalizations, and deaths were summarized by the median and 90% CrI. Similar to a recent study in the United States , we used empirical distributions for source distributions, such as the number of hospitalizations or deaths, to avoid assumptions about the expected shape of these distributions. All other inputs were modeled by using the PERT (project evaluation and review technique) distribution, which enables the input of a minimum, maximum, and modal value, or 3 percentile points, such as a median value and 95% bounds. We used this distribution widely in our analyses because it enables asymmetric distributions and can be produced from many data sources, including expert elicitation data. The Australian National University Human Research Ethics Committee approved the study.\n\n【4】##### Incidence of Sequelae\n\n【5】Several pathogens are associated with the development of sequelae. Community estimates of foodborne illness from Kirk et al. for _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. STEC, and _Y. enterocolitica_ were used for estimating the incidence of foodborne sequelae . Although _Shigella_ spp. and nontyphoidal _Salmonella_ spp. have been associated with HUS and STEC has been associated with IBS and ReA, data on which to base estimates are limited. In addition, although other pathogens, such as _Chlamydia trachomatis_ , _Clostridium difficile_ , _Giardia lamblia_ , and norovirus, have been associated with these sequelae , we assessed only pathogens commonly associated with sequelae, domestically acquired, and with a foodborne transmission pathway. A “sequelae multiplier,” which is the proportion of sequelae cases that develop after enteric infection with a specific bacterial pathogen, was applied to our estimates of domestically acquired foodborne gastroenteritis cases caused by that pathogen . For each sequel illness, we reviewed relevant studies published during 1995–2012 using systematic reviews and studies using Australian data where possible to estimate the relevant sequelae multipliers. We reviewed articles about sequelae after infection with _Campylobacter_ spp. _E. coli_ , nontyphoidal _Salmonella_ spp. _Shigella_ spp. and _Y. enterocolitica_ , and we estimated sequelae multipliers for GBS, HUS, IBS, and ReA after bacterial gastrointestinal infection on the basis of these reviews . Relevant articles and additional information are documented in Technical Appendix 1 .\n\n【6】Our sequelae multiplier for GBS was based on 30.4 (range 19.2–94.5) cases of GBS per 100,000 cases of campylobacteriosis using data from studies from the United Kingdom, Sweden, and the United States . For HUS, the sequelae multiplier used was 3% (95% CI 1.7%–5.4%) from a South Australian study on STEC and HUS notifications during 1997–2009 . On the basis of data from Haagsma et al. we assumed that 8.8% (95% CI 7.2%–10.4%) of foodborne disease caused by _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. and _Shigella_ spp. result in IBS. We used a separate sequelae multiplier for each pathogen that resulted in ReA. We assumed that 7% (range 2.8%–16%) of foodborne cases of _Campylobacter_ spp. 8.5% (range 0%–26%) of foodborne cases of nontyphoidal _Salmonella_ spp. 9.7% (range 1.2%–9.8%) of foodborne cases of _Shigella_ spp. and 12% (range 0%–23.1%) of foodborne cases of _Y. enterocolitica_ result in ReA . Total foodborne IBS and ReA cases reflect the sum of modeled IBS and ReA cases from these 3 and 4 pathogens, respectively. Details on the sequelae multipliers and incidence estimation methods are in online Technical Appendix 1 and Technical Appendix 2 .\n\n【7】We compared the incidence of sequelae circa 2010 to that of sequelae circa 2000 by applying the same sequelae multipliers to estimates of the incidence of acute gastroenteritis to specific pathogens in 2006–2010 and 1996–2000, respectively. The estimates of incidence of acute gastroenteritis were based on notification data for _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. STEC, and _Y. enterocolitica_ ,\n\n【8】##### Hospitalizations and Deaths\n\n【9】To estimate hospitalizations associated with IBS from foodborne _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. and _Shigella_ spp. and hospitalizations associated with ReA from foodborne _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. and _Y. enterocolitica_ , we used hospitalization data for 2006–2010 from all Australian states and territories, according to the International Classification of Diseases, Tenth Revision, Australian Modification (ICD-10-AM) codes. To estimate deaths for all 4 sequelae illnesses resulting from the respective foodborne pathogens, we used national death data for 2001–2010 from the Australian Bureau of Statistics, using ICD-10-AM codes . Principal diagnosis and additional diagnoses were included for hospitalizations, and underlying and contributing causes were included for deaths. Because we had only 1 year of hospitalization data for Victoria and 2 years for New South Wales, we extrapolated from these data to derive a distribution of the number of hospitalizations across all states, which was modeled as an empirical distribution. For these states, we assumed the same number of hospitalizations each year to adjust for missing data. Because of the severity of GBS and HUS, hospitalization estimates for these illnesses were not modeled, and all persons with estimated incident cases from contaminated food were considered to have been hospitalized.\n\n【10】We estimated incidences of hospitalization and death using a statistical model that incorporates uncertainty in case numbers and in multipliers using probability distributions , which is adjusted from the hospitalization estimation flow chart in Kirk et al. We assumed that all estimated incident foodborne _Campylobacter_ \\-associated GBS and STEC-associated HUS case-patients were hospitalized, so those cases were not modeled; however, multipliers were still needed for GBS and HUS to estimate deaths. Sequelae-associated deaths were estimated by using the same methods as for hospitalizations . Input data arose from the data sources discussed above or from multipliers that are discussed below.\n\n【11】##### Domestically Acquired Multiplier\n\n【12】The “domestically acquired multiplier” adjusted for the proportion of case-patients who acquired their infection in Australia. We estimated domestically applied multipliers for the antecedent bacterial gastrointestinal pathogens using notifiable surveillance data from each state, extrapolated to give national estimates . We adopted the domestically acquired multiplier for _Campylobacter_ spp. of 0.97 (90% CrI 0.91–0.99) for GBS and the domestically acquired multiplier for STEC 0.79 (90% CrI 0.73–0.83) for HUS . For IBS and ReA, a combined domestically acquired multiplier for _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. and _Shigella_ spp. for IBS and _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. and _Y. enterocolitica_ for ReA was calculated as a weighted average of the domestically acquired multipliers for each pathogen, weighted by the total number of IBS and ReA cases for each pathogen, respectively .\n\n【13】##### Proportion Foodborne Multiplier\n\n【14】For each of the 4 sequelae, we calculated the proportion of hospitalizations and deaths from foodborne pathogens using 2 multipliers: a “bacterial multiplier” to attribute the proportion of overall cases of each of the sequelae illnesses to specific pathogens and a “foodborne multiplier” to attribute illnesses to foodborne exposure. The bacterial multiplier, which was the proportion of sequel cases attributable to their antecedent bacterial pathogen, was extracted from systematic reviews for GBS and HUS  and multiplied by the foodborne proportion for _Campylobacter_ spp. and STEC, respectively. For IBS and ReA, from the literature we extracted a midpoint and range of the proportion of cases that resulted from infectious gastroenteritis . The IBS bacterial multiplier was then further multiplied by a foodborne multiplier for _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. and _Shigella_ spp. which was calculated as a weighted average of the foodborne multipliers for each pathogen, weighted by the total number of IBS cases for each pathogen. The ReA bacterial multiplier was then also multiplied by the foodborne multiplier for _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. and _Y. enterocolitica_ by using a weighted average of the foodborne multipliers for each pathogen as was done for IBS .\n\n【15】### Results\n\n【16】##### Incidence\n\n【17】We estimated that, circa 2010 in Australia, 70 (90% CrI 30–150) new cases of _Campylobacter_ \\-associated GBS, 70 (90% CrI 25–200) new cases of STEC-associated HUS, 19,500 (90% CrI 12,500–30,700) new cases of _Campylobacter_ \\-, nontyphoidal _Salmonella_ – and _Shigella_ \\-associated IBS, and 16,200 (90% CrI 8,750–30,450) new cases of _Campylobacter_ \\-, nontyphoidal _Salmonella_ \\-, _Shigella_ \\-, and _Y. enterocolitica_ –associated ReA were domestically acquired and caused by contaminated food . We estimated that 35,840 (90% CrI 25,000–54,000) domestically acquired sequel illnesses resulted from foodborne gastroenteritis—an incidence rate of 1,620 (90% CrI 1,150–2,450) sequelae cases per million population. _Campylobacter_ spp. infection resulted in the largest number of sequelae cases annually; ≈80% of the 36,000 sequel illnesses were attributable to _Campylobacter_ spp. alone.\n\n【18】##### Comparison with Estimates Circa\n\n【19】Using data circa 2000, we estimated that 50 GBS cases, 55 HUS cases, 14,800 IBS cases, and 12,500 ReA cases occurred each year. Elsewhere, we estimated that the rate of foodborne campylobacteriosis was approximately 13% higher in 2010 than 2000 ; this increase led to a 13% increase in _Campylobacter_ \\-associated GBS in 2010 over 2000. Similarly, we estimated that the rate of foodborne salmonellosis was 24% higher in 2010 than in 2000 . These factors combine to explain much of the increase in IBS and ReA. The rate of STEC-associated HUS remained about the same in 2000 and 2010 .\n\n【20】##### Hospitalizations and Deaths\n\n【21】We estimated that, circa 2010 in Australia, 1,080 (90% CrI 700–1,600) hospitalizations for sequelae illnesses occurred from domestically acquired foodborne gastroenteritis, equating to 50 (90% CrI 30–70) hospitalizations per million population per year . We estimated a total of 10 (90% CrI 5–14) deaths from sequelae to domestically acquired foodborne gastroenteritis—a rate of 0.5 (90% CrI 0.2–0.6) deaths per million population per year .\n\n【22】### Discussion\n\n【23】Our study demonstrates that foodborne gastroenteritis in Australia results in substantial severe and disabling sequelae. We estimated a yearly rate of 1,620 incident cases of sequelae illnesses, 50 hospitalizations, and 0.5 deaths per million population circa 2010. In addition, a comparison with estimates recalculated for 2000 indicates an increase in the rates of GBS, IBS, and ReA since 2000, which is consistent with and directly related to rising levels of antecedent foodborne illnesses caused by _Campylobacter_ spp. and nontyphoidal _Salmonella_ spp. during this period . This increase highlights the importance of quantifying sequelae when estimating the effects of foodborne disease and provides further impetus for reducing illness from foodborne bacterial pathogens.\n\n【24】The impact of _Campylobacter_ spp. infection in the community is high. Approximately 179,000 cases of foodborne campylobacteriosis occur in Australia each year , and _Campylobacter_ spp. was responsible for 80% of the foodborne sequelae illness estimated in this study. The reported rate of infection from _Campylobacter_ spp. in Australia has increased since 2010  and is higher than in many other industrialized countries. For example, the rate of _Campylobacter_ spp. for Australia was ≈10 times higher than that for the United States , double that for the Netherlands , and slightly higher than that for the United Kingdom . In the Netherlands, a lower rate of acute _Campylobacter_ spp. gastroenteritis has contributed to lower estimates of rates of sequel illnesses than our estimates for GBS, IBS, and ReA .\n\n【25】In New Zealand, food safety interventions have been effective in lowering campylobacteriosis rates and sequelae. In 2006, high campylobacteriosis notification rates (>3,800 cases per million population) prompted increased research on _Campylobacter_ spp. which resulted in the introduction of food safety and poultry industry interventions, including _Campylobacter_ spp. performance targets at primary processing plants and promotion of freezing all fresh poultry meat . By 2008, the rate of campylobacteriosis notifications decreased by 54% to 1,615 cases per million population . In addition, after these interventions in New Zealand, the rate of GBS hospitalizations decreased by 13% . The less dramatic decrease in GBS than in campylobacteriosis might be explained by the fact that _Campylobacter_ spp. is not the only cause of GBS. If Australia were to experience decreases similar to those in New Zealand, we would expect the rate of foodborne campylobacteriosis in the community to drop from approximately 8,400 to 3,864 cases per million population. Sequelae would decrease from 1,620 to 870 cases per million population per year. Furthermore, total GBS-associated hospitalizations, including GBS from all causes and readmissions, would decrease from ≈73 to 63 hospitalizations per million population annually.\n\n【26】A comparison of our foodborne _Campylobacter_ \\-associated GBS incidence estimates with raw hospitalization data showed many more hospitalizations than incident cases. This finding probably is attributable to repeat hospitalizations. We took a conservative approach by basing incidence estimates on community estimates of campylobacteriosis and assuming that all persons with incident cases were hospitalized. A yearly median of 1,536 (range 1,428–1,632) primary and additional GBS diagnoses occurred in Australian hospitals during 2006–2010 (including GBS from all causes and readmissions) and equates to a median rate of 73.1 (range 64.7–77.4) GBS-associated hospitalizations per million population each year. This rate is within the range from a New Zealand study, which found a median rate of 56.3 (range 42.1–75.9) GBS-associated hospitalizations during a 13-year period, with ≈41% of case-patients being readmitted, resulting in 23.2 (range 15.3–29.3) incident GBS hospitalizations per million population each year . If we assume that 41% of Australia’s 1,536 GBS hospitalizations are readmissions and apply the domestically acquired multiplier and foodborne proportion multiplier used to estimate GBS-associated deaths , we would estimate 170 (90% CrI 60–265) incident foodborne _Campylobacter_ \\-associated GBS hospitalizations. This point estimate is higher than our current estimate of 70, although the credible interval includes our estimate. A validation study of medical records of persons with GBS would enable us to better characterize readmissions for GBS.\n\n【27】Our approach has several limitations. First, our comparison of sequelae estimates for 2000–2010 assumes a constant rate of sequelae illness after gastrointestinal infection over time. Although our methods provide an indirect method of assessing changes in sequelae incidence over time, the approach is useful because it enables comparison of the population-level effect of sequelae at these 2 time points. Second, our study measured incidence and not prevalence of sequelae. We estimated the number of new cases every year and did not quantify the long-term effects of these sequelae. Third, our study does not estimate all sequelae illness from foodborne disease pathogens. We did not include sequelae, such as end-stage renal disease, inflammatory bowel disease, and encephalitis, in our estimates. We chose GBS, HUS, IBS, and ReA for this study because they were known, well studied, and well characterized in available data sources. These provide a good basis to begin to understand the effects of foodborne sequelae and the policy implications of reducing illness from preceding bacterial pathogens.\n\n【28】Our estimates for GBS, HUS, IBS, and ReA incidence relied heavily on the quality of the literature we reviewed. We used Australian data and systematic reviews wherever possible. The Australian hospitalization and deaths data we used were of high quality and included both principal and additional diagnoses from all states. However, because data were missing from some states in some years, we extrapolated from these data to the remaining years. Finally, ICD-10 and ICD-10-AM coding can be problematic when co-morbid conditions are present, when hospital transfers occur, or when diagnostic criteria are inconsistent. Therefore, our estimates for sequelae hospitalizations and deaths may be conservative because they do not account for these coding errors.\n\n【29】The sequelae estimates from this study showed that the impact of foodborne _Campylobacter_ spp. nontyphoidal _Salmonella_ spp. _Shigella_ spp. STEC, and _Y. enterocolitica_ was much greater then when consideration is given simply to the initial acute illness. _Campylobacter_ spp. infection, in particular, was highlighted as an increasing problem in Australia. Our estimates provide a basis for costing studies, which can be useful for developing food safety policies and interventions. Finally, our study highlights the need for better data from large population-based studies in Australia to further characterize sequelae, as well as foodborne pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bc817644-16be-4b3f-aec8-cc4eebf5e569", "title": "Seroprevalence of Antibodies against Chikungunya, Dengue, and Rift Valley Fever Viruses after Febrile Illness Outbreak, Madagascar", "text": "【0】Seroprevalence of Antibodies against Chikungunya, Dengue, and Rift Valley Fever Viruses after Febrile Illness Outbreak, Madagascar\nIn October 2009, the sentinel surveillance system for early outbreak detection in Madagascar  reported an increase of cases of fever with joint pain on the eastern coast. At the beginning of February 2010, chikungunya virus (CHIKV) infection was diagnosed in a patient from the Mananjary district. The International Federation of Red Cross and Red Crescent Societies reported 702 clinically diagnosed cases of chikungunya during February 9–February 15, 2010. Six hundred occurred in the coastal city of Mananjary, and 96 occurred in the small village of Irondro at the crossroads between the towns of Mananjary, Manakara, and Ifanadiana, indicating that the area was a focal point of the epidemic  .\n\n【1】Arthropod-borne viruses (arboviruses) such as CHIKV , dengue virus (DENV), and Rift Valley fever virus (RVFV)  are emerging pathogens in the southwestern Indian Ocean region. In 2005–2006, CHIKV caused outbreaks and epidemics on La Réunion, Mauritius, Mayotte, and the Seychelles , which caused considerable illness and death . Chikungunya appears to occur as an epidemic and an endemic disease in this region. The endemic disease affects mainly populations with high levels of IgG against CHIKV who live in rural areas in Africa . The epidemic disease occurs in Asia and the Indian Ocean region in populations in which herd immunity is weak, often in urban areas where _Aedes aegypti_ and _Ae. albopictus_ mosquitoes are the main transmission vectors. During epidemics, humans are the primary reservoirs. Monkeys, rodents, birds, and cattle have been identified as animal reservoirs . The onset of chikungunya epidemics is acute with high attack rates as seen in 2005–2006 on La Réunion . Concurrent epidemics of dengue and chikungunya have been reported from Asia  and Africa . In 2006, a combined outbreak of dengue fever and chikungunya fever occurred near the Madagascan city of Toamasina , but the rest of the country remained unaffected by this epidemic. An increased number of RVFV infections was noticed in Madagascar during the rainy seasons of 2008 and 2009 with 476 (19 fatal) and 236 (7 fatal) suspected cases, respectively .\n\n【2】The sudden emergence of chikungunya in the Mananjary area indicated a lack of herd immunity in the affected population. The previous outbreak of CHIKV infection in Madagascar in 2006 in the Toamasina region occurred in conjunction with DENV infection. Because of the recent reports of RVFV infections in animals and humans, our investigation of the recent outbreak in Madagascar included assessment of serologic parameters against CHIKV, DENV, and RVFV.\n\n【3】Approximately 2–3 months after the peak and 1–2 months after the decline of the outbreak of chikungunya, we retrospectively assessed the serologic markers and reported clinical features of women who came for routine pregnancy follow-up visits at 6 geographic locations. By focusing on pregnant women, we could reduce the need to stratify for age and sex and thus minimize the fragmentation of data. The focus could then to be placed on 1) assessing a possible inward spread of the epidemic, 2) evaluating whether the epidemic was limited to CHIKV or due to a simultaneous occurrence of DENV or RVFV infections, and 3) detecting factors associated with an increased risk of CHIKV infection.\n\n【4】### Methods\n\n【5】The cooperative project of the University of Antananarivo and the Bernhard Nocht Institute for Tropical Medicine was carried out during May–July 2010. Overall, 1,244 pregnant women were included from 6 different sites . Investigations were conducted in 2 coastal cities: Mananjary, the suspected epicenter of the chikungunya epidemic in February, and Manakara. Inland study sites included Ifanadiana, located at the ascending road from the above-mentioned cities to the highlands, the 2 highland cities of Tsiroanomandidy and Ambositra, and Moramanga, a highland city between Antananarivo and Toamasina. The study sites were chosen to include the coastal cities where the suspected chikungunya outbreak was reported (Manajary and Manakara), a city lying at the main road leading to the outbreak region on a moderate elevation level of 466 m (Ifanadiana), and 3 arbitrarily chosen cities in the highlands (Moramanga, Tsiroanomandidy, and Ambositra). Pregnancy follow-up services were chosen for a population comprehensive enough to allow the collection of ≈200 samples within a week. All women attending the routine pregnancy follow-up services were included.\n\n【6】The study was approved by the “Comité d’ethique de la Vice Primature Chargée de la Santé Publique” and discussed with representatives of the World Health Organization (WHO) during a meeting at the WHO office in Antananarivo on April 23, 2010. The study was explained to every participant, and informed consent was either signed (or, in the case of illiteracy, a fingerprint was obtained), and signature of a witness was acquired. Data on the course of pregnancy were obtained from interviews and review of the pregnancy follow-up booklet. To assess symptoms of CHIKV infections, researchers questioned the participants regarding current symptoms, symptoms that occurred since the time of their last menorrhea, or symptoms they recalled from a recent confirmed or suspected CHIKV infection.\n\n【7】A venous blood sample collected into EDTA was taken for measurement of IgG against CHIKV, DENV, and RVFV. To detect acute CHIKV or DENV infections, we measured levels of IgM against CHIKV and DENV. Immunofluorescence assays (IFAs) for CHIKV, DENV, and RVFV were performed with virus-infected Vero E6 cells as described . In brief, Vero cells were spread onto slides, air dried, and fixed in acetone. Plasma samples were serially diluted in phosphate-buffered saline, starting with an initial dilution of 1:10, added to the cells, and incubated for 90 minutes at 37°C. After slides were washed with phosphate-buffered saline, they were incubated with fluorescein isothiocyanate–labeled rabbit antihuman IgG and IgM (SIFIN, Berlin, Germany) at 37°C for 25 minutes . IgG or IgM titers \\> 100 were considered positive.\n\n【8】The serologic controls used for the IFA were the standard routine controls of the WHO Collaborating Centre for Arbovirus and Haemorrhagic Fever Reference and Research in Hamburg, Germany. Moreover, the controls used in this serologic survey were used as external quality assessment samples by the WHO Collaborating Centre for Quality Assurance and Standardization in Laboratory Medicine, Berlin, Germany. The sensitivity and specificity of the IFAs were demonstrated to be 100%, according to results of external quality assessments organized by the European Network for Diagnostics of Imported Viral Diseases . The DENV nonstructural protein 1 (NS1) antigen ELISA was performed according to the instructions of the manufacturer (Bio-Rad Laboratories, Hercules, CA, USA). The DENV NS1 antigen is a marker for DENV antigen circulation and thus detects acute infections up to day 21 after onset of symptoms.\n\n【9】The following retrospectively self-reported symptoms were assessed: fever, joint pain, and stiffness, skin symptoms and rashes, hip vibrations and pain, conjunctivitis, and stooped posture. For the 2 coastal locations where seroprevalence of CHIKV was highest, we analyzed potential risk factors for CHIKV infection, taking into account patient’s age, mosquito protective measures (bednet, fan, repellent, air conditioner), and because our study population consisted of pregnant women, we also considered pregnancy-related factors such as parity, weight, and trimester of pregnancy.\n\n【10】Statistical analysis was performed by using STATA version 10 . The risk factor analysis was done by using univariable and multivariable logistic regressions. If the odds ratios suggested a trend over several categories, a nonparametric test for trend was conducted in the univariable analysis. For multivariable models, a Mantel-Haenszel test for trend, adjusted for the other variables in the model, was performed.\n\n【11】### Results\n\n【12】A total of 1,244 women from 6 locations in Madagascar were included in the study. All locations, when data were presented graphically, showed a right-skewed age distribution. The median age was 25 years (range 12–50 years). The median age of women from the 2 coastal locations (Mananjary and Manakara) was not significantly different from that of women from higher altitudes (p = 0.95, by Wilcoxon rank sum test). No relevant difference was found regarding the proportion of primiparas by location (28.4% in the highlands compared with 27.4% at the coast, p = 0.70, by χ 2  test). None of the participants had an air conditioner at home; 3 women who lived on the coast reported using a fan. Two women who lived in the highlands reported using mosquito repellents. The only noteworthy mosquito protective measure used frequently was bednets (70.3%). Bednet use was significantly higher for women from the coastal cities of Mananjary (88.2%) and Manakara (90.8%) than for those living in the highland cities of Moramanga (65.7%), Ambositra (21.5%), and Tsiroanomandidy (56.7%). Of those from Ifanadiana, a coastal city at 450-m altitude, 94% used bednets.\n\n【13】A total of 154 of the 1,244 pregnant women (12.4%) were positive for IgG against CHIKV, and 116 of them (75.3%) had reported a history of symptoms of CHIKV infection since their last menorrhea. The highest rates of IgG against CHIKV (Mananjary 44.6%, Manakara 22.7%) and IgM against DENV (Mananjary 17.4%, Manakara 10.8%) were found in the 2 coastal cities . IgG against CHIKV and DENV were found in 7.2% and 2.4% in Mananjary and Manakara, respectively. Differences in seroprevalences at the other locations, all at higher altitudes, were negligible, apart from the high frequency of IgG against DENV in Moramanga (11.7%) and Tsiroanomandidy (3.9%). Among persons with IgG against CHIKV, the proportion of women who reported a history of symptoms related to CHIKV infection during the recent outbreak was the same in Mananjary and Manakara (79%). None of the women interviewed from Moramanga reported a history of recent symptoms, even if they had IgG against CHIKV or DENV.\n\n【14】According to the results, only persons with IgG against CHIKV from Mananjary and Manakara could be confidently assigned to the 2009–2010 outbreak. Therefore, reported symptoms and analysis of risk factors for CHIKV infection was confined to these 2 locations . Risk for previous CHIKV infection increased with body weight . This association persisted after adjusting for parity, bednet use, and age  and after additionally adjusting for the trimester of pregnancy .\n\n【15】### Discussion\n\n【16】In contrast to the outbreak in the Toamasina area of Madagascar in 2006, the outbreak investigated here appears to be exclusively caused by CHIKV infections without concomitant DENV infection. Although one third of the participants in coastal cities at the epicenter of the outbreak were infected with CHIKV, the epidemic did not spread to higher altitudes and further inland. The chikungunya epidemic curve reached its peak in Mananjary in February and abated in March 2010; in Manakara, the epidemic occurred ≈1 month later. The duration of the epidemic roughly corresponds to the rainy season in Madagascar from November to April. Anti-CHIKV IgG was detected in all samples positive for anti-CHIKV IgM, and we assumed that the outbreak had ended before the investigation described here was started. In the noncoastal locations, all samples were negative for IgM against CHIKV, which suggests that the epidemic was constrained to the coast.\n\n【17】By focusing on pregnant women, we achieved a relatively homogenous study population, which facilitated the comparison of differences between study sites by reducing differences between persons (e.g. age or sex) and overfragmentation of the data. In addition to DENV, the presence of RVFV was assessed because this virus has been recently reported in Madagascar . In 22 persons, IgG against CHIKV and IgG against DENV were detected. The major mosquito vectors of both viruses are equally susceptible to CHIKV and DENV, with simultaneous transmission being confirmed in experimental settings . Although a considerable number of the samples from Mananjary (17.4%) and from Manakara (10.8%) were positive for IgG against DENV, none was positive for DENV IgM or DENV NS1 antigen, which suggests past DENV infections, independent of the recent chikungunya epidemic.\n\n【18】A limitation of the study is the possibility that some women may have had prior CHIKV infections before the outbreak investigated in this study. A CHIKV outbreak was reported in the Toamasina region in 2006 . However, it seems unlikely that a relevant proportion of the resident population of the Mananjary region acquired CHIKV immunity during the Toamasina outbreak because this region is 400 km distant from Mananjary, and no tarred road exists along the coast. The elevated seroprevalance of IgG and IgM against CHIKV in the coastal city of Manakara, which lies 100 km south of Mananjary, suggests that the outbreak spread southwards along the coast.\n\n【19】The International Federation of Red Cross and Red Crescent Societies reported that the town of Irondo, where the inbound roads from Mananjary and Manakara meet, was also affected by the outbreak .The town is situated 30 km from the coastline at an altitude of ≈40 m. Although an altitude of 400–500 m does not avert mosquito survival, the low seroprevalence in Ifanadiana (altitude 466 m, 70 km inland) provides evidence against an upward and inbound spread of the epidemic. The long travel distance between Irondro and Ifanadiana, with serpentine roads and long uninhabited stretches between small settlements, may have interrupted transmission chains. Samples taken from the highland cities of Ambositra (1,280 m), Tsiroanomandidy (860 m), and Moramanga (920 m) indicate that the highland population of Madagascar remained unaffected by the recent chikungunya outbreak.\n\n【20】According to our data, 21% of the women from the coastal cities who were positive for IgG against CHIKV did not report symptoms corresponding to those of chikungunya during the outbreak. Similarly, the proportion of asymptomatic infections during a CHIKV outbreak in a naïve population from northeastern Italy in 2007 was 18% . In contrast, during the 2005–2006 chikungunya outbreak on La Réunion, only 5% of infections remained asymptomatic .\n\n【21】Entomologic data were not collected during the chikungunya outbreak on Madagascar. The main vectors for CHIKV are _Ae. albopictus_ and _Ae. aegypti_ mosquitoes _._ In a study from Gabon, _Ae. albopictus_ mosquitoes outnumbered _Ae. aegypti_ mosquitoes in most suburban areas, and in urban areas, where _Ae. aegypti_ mosquitoes were more commonly found, CHIKV and DENV were only found in _Ae. albopictus_  mosquitoes. _Ae. albopictus_ mosquitoes were the main vectors on La Réunion during the 2005–2006 epidemic.\n\n【22】Recent entomologic data for Madagascar were scarce in 2010, when the chikungunya outbreak took place. In a report from 1989, _Ae. albopictus_ was identified as the predominant _Aedes_ species on the eastern coast and on the central highland plateau, where all our study sites were located . A recent study provides evidence for an expansion of an invasive lineage of _Ae. albopictus_ that has spread throughout the island and possibly caused a decline of _Ae. aegypti_ at least in urban areas . More research on the vector dynamics of _Ae. albopictus_ and _Ae. aegypti_ mosquitoes in Madagascar is needed.\n\n【23】The analysis of risk and protective factors for infection was confined to the epidemic coastal cities. The finding that bednet use had no influence on the risk of CHIKV infection is in accordance with the fact that the chikungunya vectors _Ae. albopictus_ and _Ae. aegypti_ mosquitoes bite during the daytime. The results indicate a positive association between body weight and the risk for CHIKV infection but the causality could not be assessed in this cross-sectional study. Notably, a study from India found evidence of a higher risk for chronic sequelae among obese persons who acquired chikungunya . Future studies on risk factors for chikungunya should include body weight as a possible influence.\n\n【24】The 2009–2010 arboviral outbreak in coastal eastern Madagascar was an isolated CHIKV epidemic without relevant DENV co-transmission. With more than one third of all women affected in the epicenter, the infection rate in the population was high. Data from other locations suggest that the epidemic did not spread to higher altitudes and inland.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bb37086f-4973-4c58-a04f-cb75ecad7362", "title": "Investigation of the Disparity Between New York City and National Prevalence of Nonspecific Psychological Distress Among Hispanics", "text": "【0】Investigation of the Disparity Between New York City and National Prevalence of Nonspecific Psychological Distress Among Hispanics\nAbstract\n--------\n\n【1】**Introduction**  \nIn New York City, the age-adjusted prevalence of nonspecific psychological distress (NPD) among Hispanics is twice that of non-Hispanic whites; nationally, there is little Hispanic-white disparity. We aimed to explain the pattern of disparity in New York City.\n\n【2】**Methods**  \nData came from the 2006 National Health Interview Survey and 2006 Community Health Survey in New York City. Respondents with scores higher than 12 on the K6, a brief scale used to screen for mental health disorders, were defined as having NPD. Multivariate analyses controlled for Hispanic ancestry, socioeconomic status (education, employment, and income), nativity, language of interview, and health characteristics.\n\n【3】**Results**  \nIn New York City, the disparity between Hispanics and whites was fully explained after accounting for the disproportionate concentration of low socioeconomic status among Hispanics (odds ratio for NPD, 0.81; 95% confidence interval, 0.60-1.11). These factors also partially accounted for differences between Hispanics in New York City and the United States, but the prevalence of NPD overall in New York City remained elevated relative to the United States.\n\n【4】**Conclusion**  \nElevated NPD prevalence among New York City Hispanics was primarily attributable to large disparities in socioeconomic status; differences between New York City and the United States remained but were not specific to Hispanics. Interventions in New York City aimed at addressing racial/ethnic disparities in health may overlap with those addressing socioeconomic inequalities. Further study into the higher overall prevalence of NPD in New York City will be necessary to inform the design and targeting of interventions.\n\n【5】Introduction\n------------\n\n【6】Mental disorders are leading causes of illness and death in the United States . Nonspecific psychological distress (NPD) is a group of affective symptoms common to a range of psychiatric disorders but not specific to any single disorder . National and community-level surveillance of NPD is necessary for targeting interventions to prevent more serious disease.\n\n【7】Although the prevalence of chronic disease is higher among Hispanics, the largest minority group in the United States, compared with non-Hispanic whites, studies have noted the absence of a similarly patterned disparity in NPD. Many studies, for example, demonstrate similar or lower prevalence of NPD and other adverse mental health outcomes among Hispanics compared with whites . New York City is an exception to this pattern; NPD prevalence is almost twice as high among New York City Hispanics as among non-Hispanic whites .\n\n【8】These differences in prevalence remain poorly understood. The differential distribution of known correlates of NPD may provide one explanation. Female sex, low socioeconomic status (SES), marital disruption (divorced, separated, widowed), chronic illness, and Spanish language preference have been well characterized in the literature as risk factors for NPD . The prevalence of these correlates may be higher among Hispanics in New York City relative to non-Hispanic whites in New York City and Hispanics in the United States. Whether this higher prevalence translates to elevated distress is unknown.\n\n【9】Variation in distress and depressive outcomes among ethnicities within the broad “Hispanic” category may also provide an explanation for the disparity between New York City and US Hispanics. Puerto Rican ethnicity, for example, has been associated with a higher risk of adverse mental health relative to Cuban, Mexican, and other Hispanic groups . These differences may arise from social factors associated with discrimination, migration, and culture . Most US Hispanics are Mexican; by contrast, the New York City Hispanic population is more heterogeneous, composed predominantly of Puerto Ricans and Dominicans . In light of findings that highlight intragroup variation in mental health outcomes among Hispanics, we anticipated that different NPD prevalence estimates between New York City and the United States might be partly attributed to compositional differences in the Hispanic population.\n\n【10】Using nationally and city-representative data for the United States and New York City, we explored how these factors could explain the elevated NPD risk among New York City Hispanics relative to both New York City non-Hispanic whites and US Hispanics. These findings may guide the design and targeting of mental health interventions.\n\n【11】Methods\n-------\n\n【12】### Data sources\n\n【13】We used data from non-Hispanic white and Hispanic participants aged 18 years or older from the 2006 National Health Interview Survey (NHIS) and the 2006 Community Health Survey (CHS) in New York City. NHIS is conducted annually by the National Center for Health Statistics and represents the noninstitutionalized population of the United States. The survey collects detailed information on sociodemographic indicators and mental and physical health, through in-person household interviews conducted in English or Spanish . Because NHIS is publicly available and uses de-identified information, analyses using these data were exempt from review by an institutional review board. In 2006, the interviewed adult sample included 24,275 people. Among these, 496 did not respond to questions about distress. After restricting the sample to respondents who self-identified as non-Hispanic white or Hispanic, our US sample consisted of 18,405 adults.\n\n【14】CHS is an annual, random-digit–dialed telephone survey of approximately 10,000 noninstitutionalized New York City adults conducted by the New York City Department of Health and Mental Hygiene. Data collection and analyses were approved by the agency’s institutional review board. CHS is designed as a stratified random sample to provide neighborhood and citywide estimates for mental and physical health indicators . Data were collected through computer-assisted telephone interviews conducted in multiple languages. Of the 9,683 interviews conducted in 2006, 51 were excluded because of item nonresponse. Restricting this sample to respondents identifying as non-Hispanic white or Hispanic yielded 6,148 adults.\n\n【15】### Dependent variable\n\n【16】NPD was measured using the K6, a brief scale validated for use across racial/ethnic groups that is designed to screen the general population for mental health disorders . The scale, used by both the 2006 NHIS and the 2006 CHS, was developed using item response theory, a process that selects the best subset of items from a larger universe of items, and is characterized by high specificity and limited differential sensitivity across racial/ethnic groups. To characterize NPD, questions from different mental health surveys were entered into a model and the 6 best questions were selected. The K6 asks respondents how often in the preceding 30 days they felt “sad,” “nervous,” “restless,” “hopeless,” “worthless,” or that “everything was an effort.“ Responses are measured on a scale from 0 (“none of the time”) to 4 (“all of the time”) then summed (range, 0-24). By convention, respondents with K6 scores higher than 12 were classified as having NPD .\n\n【17】### Independent variables\n\n【18】Our analyses were limited to the following 6 racial/ethnic groups: non-Hispanic whites, Puerto Ricans, Dominicans, Mexicans, Central/South Americans, and Other Hispanics. Covariates were selected a priori and included those identified in the literature as traditional correlates of distress: age, sex, marital status (married, marital disruption, never married), nativity (United States, other), interview language (English, other), education (less than high school, high school graduate, some college, college graduate), employment status (employed, unemployed, homemaker, retired, student), and poverty-income ratio, which relates a household’s annual income to the federal poverty threshold for a household of its size and composition (<100%, 100%-199%, 200%-399%, ≥400%, unknown) . We also examined health indicators associated with distress in the literature: self-rated health status (excellent/good, fair/poor), diabetes status (ever having it or not), and current asthma (yes, no) \n\n【19】### Statistical analysis\n\n【20】All analyses were conducted using SUDAAN version 9 (Research Triangle Institute, Research Triangle Park, North Carolina) with Taylor series linearization methods to adjust for the complex survey design. Recommended weights were applied to both NHIS and CHS data to produce representative NPD prevalence estimates for each racial/ethnic category in the surveys. All estimates were age-adjusted to the 2000 US standard population . We conducted bivariate analyses of NPD for selected sociodemographic characteristics, stratified by survey and ethnic group. Comparisons among racial/ethnic groups and other sociodemographic characteristics within each survey were evaluated using the _t_ statistic. _P_ values of less than .05 were considered significant.\n\n【21】After comparing groups within each survey, we expanded our analyses to compare between surveys. Data from NHIS and CHS were pooled and reweighted to allow for statistical comparisons between the US and New York City populations. Using a series of multivariate logistic regression models, each of which included an interaction between race/ethnicity and survey source (ie, New York City population vs US population), we analyzed the role of known NPD correlates in accounting for the difference between the Hispanic-white gradient in New York City and that in the United States. Our analyses first treated Hispanics as a single group and then separated the various Hispanic ancestry groups. Base models adjusted only for age, and then in a series of richer models sequentially added other demographic characteristics, SES indicators (education, employment, and income), nativity and interview language, and health status measures. We also assessed the interaction of these covariates with survey source because their associations with NPD in New York City differed from their associations in the rest of the United States.\n\n【22】Results\n-------\n\n【23】Puerto Ricans (30.5%), Dominicans (25.9%), and Central/South Americans (22.8%) were the most common groups of New York City Hispanics, whereas in the rest of the United States, most Hispanics were Mexican (61.2%), followed by Central/South American (15.6%). In both New York City and the United States, Hispanics were disproportionately young, had lower SES, and were more likely to self-report fair or poor health . These patterns were generally consistent across Hispanic ancestry groups, although high unemployment appeared to be most concentrated among Puerto Ricans and Dominicans in both New York City and the United States. New York City Hispanics had significantly higher rates of marital disruption, low SES, non–English language interviews, and fair or poor health compared with US Hispanics. Many of these differences were concentrated among Puerto Ricans.\n\n【24】The distribution of risk factors differed among ethnicities in New York City compared with the rest of the United States. In New York City, we observed slightly higher, though nonsignificant rates of marital disruption among Hispanics compared with whites; in the United States, this pattern was reversed. The Hispanic-white disparity in SES and health was also considerably wider in New York City.\n\n【25】In New York City, NPD was twice as common among Hispanics (9.3%) as among whites (4.8%) . Prevalence was highest among Puerto Ricans (11.7%), and Dominicans (9.8%), the 2 largest ancestry groups in New York City, and Other Hispanics (9.9%), and all estimates were significantly higher than those of New York City whites (4.8%). In the United States, among Mexicans, prevalence was only 2.9%, and no Hispanic group showed significantly higher rates of NPD compared with whites (2.8%). Compared with US Hispanics overall (2.7%), New York City Hispanics had 3 times the prevalence of NPD. With the exception of Mexicans, every New York City racial/ethnic group (including whites) had significantly higher NPD estimates compared with its respective US racial/ethnic counterpart.\n\n【26】### Bivariate analyses\n\n【27】In most cases, NPD was especially prevalent among the middle-aged, women, and people of lower SES . A notable exception was among US Hispanics, where no education gradient was observed. For several risk factors, the association with NPD was more nuanced and likely to be context-specific. For example, prevalence was higher among non–English-speaking respondents in New York City, but not in the United States. For covariates associated with NPD, New York City Hispanics had a similar or higher prevalence than New York City whites, whereas in the United States, prevalence was higher among whites than Hispanics. At almost every level of each of the covariates examined, New York City Hispanics also had significantly higher NPD compared with US Hispanics.\n\n【28】### Multivariate analyses\n\n【29】In Model 1, New York City Hispanics overall had twice the age-adjusted odds of NPD as New York City whites . Both US whites and US Hispanics had similar odds of NPD — about half those of New York City whites. Taken together, these results demonstrate a large age-adjusted gap between whites and Hispanics in New York City and none in the United States. The interaction between race/ethnicity and survey source indicates not only that the Hispanic-white disparity was significantly larger in New York City compared with the United States, but that the difference between the gaps was also significant ( _P_ < .001).\n\n【30】When Model 1 was used to assess potential differences by Hispanic ancestry, underlying context-specificity in the Hispanic-white disparity became apparent. The results indicate that differences in the distribution of Hispanic ancestry groups between New York City and the United States are not sufficient to explain this context specificity; the disparity between each ancestry group and whites was significantly larger ( _P_ \\= .02) in New York City compared with the analogous gap in the United States.\n\n【31】In subsequent models, we accounted for an increasing number of risk factors. The overall gaps between New York City and the United States were robust; that is, accounting for these risk factors did not explain why NPD was more prevalent in New York City. However, these characteristics do explain the ethnicity gap in New York City. Comparing Model 3 to the previous 2 models, the disproportionate socioeconomic disadvantage of New York City Hispanics (and among Puerto Ricans, Dominicans, and Other Hispanics) compared with whites fully accounted for the higher odds of NPD. In the United States, racial/ethnic differences that had been absent emerged: Hispanics overall, and specifically, Dominicans and Mexicans, had lower odds of NPD than did US whites. The interaction between race/ethnicity and survey, however, was not significant, thus confirming that the contextual differences in the Hispanic-white disparity were fully explained by accounting for SES indicators.\n\n【32】Further adjustment for nativity and language of interview in Model 4 actually reversed the race/ethnicity gap in New York City and preserved it in the United States, although it was no longer significant. In New York City, this finding extended specifically to Dominicans, Mexicans, and Central/South Americans, whereas in the United States, Dominicans maintained this advantage. This regression indicated high NPD odds specific to non–English-speaking interviewees only in New York City (data not shown).\n\n【33】Additional controls for health status indicators (ever having diabetes, current asthma, and self-reported health) did not alter estimates from Model 4 and are therefore not shown.\n\n【34】Discussion\n----------\n\n【35】Our findings indicate that socioeconomic disadvantage is a key explanation for why the prevalence of NPD is so much higher among Hispanics than among whites in New York City, a pattern not found in the broader United States. We show that socioeconomic disparities between Hispanics and whites are larger in New York City than in the United States. Moreover, we provide evidence that NPD risk factors are more strongly correlated with disease in New York City compared with the United States. Accounting for these patterns fully explained the NPD gradient in New York City, whether we treated all Hispanics as a single group or categorized them by ancestry. After further accounting for nativity and language of interview, the disparity in New York City reversed and became consistent with that observed in the United States. In New York City, this pattern was observed most strongly among Dominicans, Mexicans, and Central/South Americans.\n\n【36】Our finding that NPD prevalence varies among Hispanics is consistent with the established literature . Previous studies have identified Puerto Ricans to be at especially high risk of NPD and other adverse mental health outcomes compared with other Hispanic subgroups, and Mexicans are among the subgroups with the lowest prevalence . The patterns we report are consistent with these studies. Previous research has suggested that Mexicans may be protected against the development of mental disorders despite socioeconomic disadvantage, and there is evidence that this protection may not extend to other Hispanic groups . These dynamics suggested that heterogeneity among Hispanics in New York City would have been a plausible explanation for its Hispanic-white disparity in NPD. We confirmed that in New York City, most Hispanics are of Puerto Rican ancestry, whereas in the broader United States they are of Mexican ancestry. However, in our empirical analysis we found that accounting for ancestry differences alone was insufficient to explain the disparity in New York City; further accounting for sociodemographic factors was still necessary.\n\n【37】Although we were able to explain the Hispanic-white disparity in New York City, our analysis could not account for the higher overall prevalence of NPD in New York City compared with the United States. Several factors may explain this result. Differences in survey administration may have played a role. CHS is a landline telephone survey, whereas NHIS is conducted in person; telephone surveys may allow respondents to feel less inhibited about expressing negative feelings. Dynamics of sample-selective attrition from the 2 types of surveys may also be different . Beyond these measurement dynamics, contextual factors may also be involved. A higher prevalence of psychiatric disorders is found in urban areas . Thus, factors associated with residence in New York City, including pace of life, cost of living, and heightened perception of and exposure to terrorism, may play a role in higher NPD prevalence. Future studies in other large metropolitan areas in the United States may shed light on this remaining unanswered question.\n\n【38】Our analyses had additional limitations. First, all data were self-reported; they may conflate cultural norms about affective expression with real psychological impairment. Many studies have documented that less acculturated Hispanic immigrants tend to express more idioms associated with mood and anxiety, known as “nervios.” This pattern is observed particularly strongly among Puerto Ricans. Expression of these idioms may not necessarily indicate psychological distress as it is clinically understood . Validation of the K6 scale for use across racial/ethnic groups was conducted in nationally representative samples . National samples of Hispanics consist mostly of Mexicans; validity of the scale may not extend to other Hispanic subgroups such Puerto Ricans and Dominicans. We also noted a higher NPD prevalence among non–English language respondents in New York City, including non-Hispanic whites. This finding may warrant deeper examination of other non–English language versions of the K6 scale.\n\n【39】Despite these limitations, we capitalized on the availability of large samples representing the populations of New York City and the United States. We accounted for differences in the ethnic composition of Hispanics as a possible factor influencing distress rates. Our analyses point to the central role of the Hispanic-white socioeconomic gradient in New York City in explaining the city’s racial/ethnic disparity in NPD. Our findings suggest that mental health interventions should be targeted to low-SES populations and should address socioeconomic risk factors directly. Future research is warranted to investigate the reasons underlying the elevated overall prevalence of NPD in New York City to inform design and targeting of interventions citywide and in other urban populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "043123c3-b40e-4935-b08b-5570c7977ffc", "title": "Routine Check-Ups and Other Factors Affecting Discussions With a Health Care Provider About Subjective Memory Complaints, Behavioral Risk Factor Surveillance System, 21 States, 2011", "text": "【0】Routine Check-Ups and Other Factors Affecting Discussions With a Health Care Provider About Subjective Memory Complaints, Behavioral Risk Factor Surveillance System, 21 States, 2011\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Most adults reporting subjective memory complaints (SMCs) do not discuss them with a health care provider and miss an opportunity to learn about treatment options or receive a diagnosis. The objective of this study was to describe correlates of discussing memory problems with a health care professional among adults reporting SMCs.\n\n【3】**Methods**\n\n【4】Data were from 10,276 respondents aged 45 years or older in 21 states reporting SMCs on the 2011 Behavioral Risk Factor Surveillance System (BRFSS). Odds ratios (ORs) adjusted for demographic and health-related measures were computed for discussing SMCs with a health care professional.\n\n【5】**Results**\n\n【6】Among all respondents aged 45 or older reporting SMCs, 22.9% reported discussing them with a health care professional; among those reporting a recent routine check-up, this rate was 25.2%. The largest adjusted OR for discussing SMCs with a health care professional was for respondents reporting that SMCs always (vs never) caused them to give up household chores (OR, 3.02) or always (vs never) interfered with work (OR, 2.98). Increasing age reduced the likelihood of discussing SMCs. Among respondents who discussed SMCs, 41.8% received treatment.\n\n【7】**Conclusion**\n\n【8】Routine check-ups may be a missed opportunity for discussions of SMCs that might lead to diagnosis or treatment. The Affordable Care Act requires a cognitive assessment for Medicare recipients during their annual wellness visit, but these results suggest that adults younger than 65 might also benefit from such an assessment.\n\n【9】Introduction\n------------\n\n【10】Although the US Preventive Services Task Force  does not recommend routine screening for dementia, which is often first manifested in memory problems, early diagnosis is important for numerous reasons. These include ruling out treatable causes of memory problems not related to dementia, providing a better chance for treatment to be effective, offering enrollment in clinical trials, and discussing community services . Another important consideration is identifying memory problems at a stage when the individual is still able to participate in decision making about his or her own future. The Affordable Care Act requires that Medicare recipients receive a cognitive assessment during their covered annual wellness visit , and recommendations for tools to use during this assessment are available .\n\n【11】The Healthy Aging Program at the Centers for Disease Control and Prevention (CDC) developed an optional module for use on the Behavioral Risk Factor Surveillance System (BRFSS) that addresses cognitive health and decline . The first question addresses confusion or memory loss that is happening more often or is getting worse during the past 12 months, defined hereinafter as subjective memory complaints (SMCs). Follow-up BRFSS questions address the need for help, functional difficulties, whether the respondent discussed the increased confusion or memory loss with a health care professional, and if so, whether or not treatment was received. Discussions with a health care professional about memory problems could serve some of the same purposes as assessment by providing an opportunity to learn about services and treatment. This study was conducted to determine how many adults reporting SMCs discussed their memory problems with a health care professional, the factors associated with that action, and the proportion who received treatment. Of special interest were respondents reporting a routine check-up within the previous year.\n\n【12】Methods\n-------\n\n【13】Data were from publicly available  BRFSS data collected in 2011 by 21 states (Arkansas, California, Florida, Hawaii, Iowa, Illinois, Louisiana, Maryland, Michigan, North Carolina, Nebraska, New Hampshire, New York, Oklahoma, South Carolina, Tennessee, Texas, Utah, Washington, Wisconsin, West Virginia) that included the cognitive impairment module . The BRFSS collects data from noninstitutionalized adults aged 18 or older through monthly random-digit–dial telephone surveys  and uses a complex sample design to randomly select respondents with either landline or cellular telephones. Data are weighted to account for the probability of selection of respondents and are further adjusted to be representative of the total adult population of each state by age, race/ethnicity, sex, marital status, education, home ownership, and type of telephone service. Data for this study were limited to the 121,304 landline surveys and excluded cellular telephone surveys, which were available for only 7 of the 21 states. Because 9 states used the cognitive impairment module on split surveys, data from different survey versions were combined and the weighting variables were renamed to be the same, as described by CDC . The median survey response rate for landline surveys in the 21 states in 2011 was 53.4%, ranging from 37.4% in California to 66.0% in Nebraska .\n\n【14】Respondents answering yes to the question “During the past 12 months, have you experienced confusion or memory loss that is happening more often or is getting worse?” were defined as reporting SMCs. Based on preliminary results of SMCs by age group for ages 18 to 24 (5.3%), 25 to 34 (6.9%), 35 to 44 (7.5%), 45 to 54 (12.2%), 55 to 64 (12.2%), and 65 or older (13.0%), data were restricted to the 95,147 respondents aged 45 or older. This group included 86.7% of all respondents reporting SMCs (10,583 of 12,205); the same age cutoff was used for these data in another study . A follow-up question for those reporting SMCs asked about the areas in which they need the most assistance as a result of their confusion or memory loss. Possible answers were 1) safety, 2) transportation, 3) household activities, 4) personal care 5) need assistance but not in those areas, or 6) no assistance needed in any area. Responses were dichotomized into need help or not. Respondents with functional difficulties were determined from responses to 2 questions and included those who responded that, during the previous 12 months, they always, usually, or sometimes (as opposed to rarely or never) gave up household activities or chores they used to do because of SMCs; or SMCs always, usually, or sometimes (as opposed to rarely or never) interfered with their ability to work, volunteer, or engage in social activities; or both. These measures were also each used separately in logistic regression as 5-level measures, with never as the referent. Additional questions asked whether their confusion or memory loss was discussed with a health care professional (yes/no), whether they received treatment such as therapy or medications for confusion or memory loss (yes/no), and whether a health care professional ever said they had Alzheimer’s disease or some other dementia (answers of yes to Alzheimer’s disease or other dementia were combined vs no diagnosis). Respondents who had not discussed their SMCs with a health care provider were not asked about treatment or diagnosis, so those records were coded as no.\n\n【15】Other measures included age (45–54, 55–64, 65–74, 75–84, and ≥85 y), sex, education (<high school, high school graduate, some college, and college graduate), annual income (<$15,000, $15,000–$24,999, $25,000–$49,999, $50,000–$74,999, and ≥$75,000), race/ethnicity (non-Hispanic white, non-Hispanic black or African American, Hispanic of any race, and other), disability (limited in any way in any activities or use special equipment \\[yes/no\\]), ever diagnosed with depression (yes/no), health status (excellent, very good, or good vs fair or poor), reporting any of 6 chronic diseases (arthritis, asthma, cardiovascular disease, cancer other than skin, chronic obstructive pulmonary disease, or diabetes \\[yes/no\\]), weight status (body mass index <18.5, 18.5–24.9, 25.0–29.9, 30.0–34.9, and ≥35.0), reporting a cost barrier to health care (Was there a time in the past 12 months when you needed to see a doctor but could not because of cost? \\[yes/no\\]), having one or more personal doctors (yes/no), health insurance (yes/no), reporting a routine check-up within the previous year (yes/no) (hereinafter referred to as a recent check-up), current smoking (yes/no), and state of residence.\n\n【16】Stata version 14.0 (StataCorp LP) was used for all analysis to account for the complex sample design of the BRFSS; analysis used the landline weight or the corresponding weight for states that had multiple survey versions. Pearson χ 2  tests of association (α = .05) from cross-tabulations of talking with a health care professional about their SMCs and demographic and health-related measures were used to select variables to include in the multivariate logistic regression model. Adjusted odds ratios (ORs) and 95% confidence intervals (CIs) are reported for the final logistic regression model after removal of variables that had _P_ values >.05. For all measures, refusal to answer or responses of “don’t know” were excluded. Excluding respondents with missing values for having a discussion with a health care professional reduced the sample size from 10,583 to 10,276.\n\n【17】Results\n-------\n\n【18】One in 8 respondents age 45 or older (12.5%; 95% CI, 12.0%–13.0%) reported SMCs, with minor (but significant) variation (11.9%–15.6%) across the 5 age groups. Compared with respondents aged 45 or older not reporting SMCs, those reporting SMCs were more likely to live in California (22.4% vs 17.1%); be Hispanic (14.4% vs 11.2%), aged 75 or older (17.6% vs 15.0%), and currently smoke (24.5% vs 15.2%); they were less likely to be white (68.1% vs 72.5%). They also reported less education and lower income, whereas sex distribution was similar for both groups.\n\n【19】Among the 10,276 respondents aged 45 or older reporting SMCs, 22.9% (95% CI, 21.1%–24.9%) reported discussing their memory problems with a health care professional. Significant differences in the percentages of adults who talked to a health care professional about their SMCs are shown in Table 1 . Of adults aged 45 or older reporting SMCs, the three-fourths who had a recent check-up were more likely than those who did not have a recent check-up to discuss their SMCs with a health care professional, but still nearly 75% had not discussed their memory issues with a health care professional. Adults reporting functional difficulties due to their SMCs were significantly more likely than those without functional difficulties to have talked with a health care professional about their memory problems. Among those with SMCs and functional difficulties, those who had a recent check-up were more likely than those not having one to have talked with a health care professional (37.4% \\[95% CI, 33.2%–41.7%\\] vs 25.2% \\[95% CI, 19.8%–31.5%\\]). Sex, race/ethnicity, income, insurance status, and reporting a cost barrier to health care were not significantly associated with talking with a health care professional so were excluded from Table 1 and the logistic regression model.\n\n【20】The largest adjusted ORs for talking with a health care professional were for respondents who reported that SMCs always (vs never) caused them to give up household chores (OR, 3.02) or always (vs never) interfered with work (OR, 2.98) and for respondents who were college graduates (OR, 2.42) . Compared with residents in California, residents in 7 states were more likely to talk to a health care professional. Others more likely to talk to a health care professional included respondents who had a recent check-up (compared with no recent check-up), had one or more personal doctors (compared with none), had a diagnosis of depression (compared with no such diagnosis), or had a disability (compared with no disability). The likelihood of talking with a health care professional decreased as age increased.\n\n【21】Among respondents reporting SMCs who talked with a health care professional, 41.8% (95% CI, 37.2%–46.5%) received treatment, representing 9.5% of all adults aged 45 or older reporting SMCs. Treatment was more likely among respondents who had a diagnosis of dementia (74.3% \\[95% CI, 61.1%–84.1%\\]) than among those with SMCs who did not have a diagnosis of dementia (7.4% \\[95% CI, 6.2%–8.8%\\]). However, because so few reported a dementia diagnosis, among study respondents with SMCs who had been treated (n = 900), only 22.6% (95% CI, 17.2%–29.0%) reported a diagnosis of dementia.\n\n【22】Among respondents aged 65 or older reporting SMCs, 98.0% (95% CI, 97.1%–98.6%) had health insurance and 84.2% (95% CI, 82.0%–86.3%) reported a recent check-up. Among respondents aged 65 or older reporting SMCs, 17.9% (95% CI, 15.8%–20.3%) of those who had a recent check-up versus 11.9% (95% CI, 8.2%–17.0%) of those not reporting a recent check-up reported talking to a health care professional about their SMCs. Adults aged 65 or older reporting SMCs represent 23.3% (95% CI, 21.9%–24.7%) of respondents of all ages with SMCs.\n\n【23】Discussion\n----------\n\n【24】These results agree with the results of previous studies  showing that few adults reporting memory problems are discussing them with a health care professional. The current results add to the body of information about such discussions in several ways. First, these results indicate that memory issues are often serious enough to affect daily activities such as household chores, work, or volunteering; however, even among those aged 45 or older most affected by SMCs, only about half reported such discussions. That the likelihood of such discussions decreased with increasing age indicates that even when adjusting for functional difficulties, older adults reporting SMCs are still less likely than adults aged 45 to 54 years to talk with a health care professional about them. This finding might have resulted from older adults feeling that their memory problems are a normal part of aging and nothing can be done about them, but the finding could have resulted from other causes. This finding also suggests that promoting assessments and discussions among adults younger than 65 might be beneficial. The increased likelihood of discussions among adults with disability and depression may indicate that adults reporting these conditions are more comfortable discussing issues with a health care professional or that these conditions create additional problems when combined with memory issues.\n\n【25】Having a recent check-up increased the likelihood of discussing memory problems with a health care professional, but only modestly (OR, 1.6). Even among those who had a recent check-up and reported that memory problems were affecting daily activities, only 37.4% had talked with a health care professional about their memory problems. Of course, any reported discussions could have taken place at some other time and with a different health care provider, but at least a routine check-up should have provided an opportunity for such a discussion. Although it is impossible to know what percentage of these adults reporting memory problems may progress to dementia or Alzheimer’s disease, a check-up could represent a missed opportunity for an assessment and discussion of available care, community services, and treatment options at their current stage of memory problems. Discussions could include a check for risk of falls, adherence with prescribed medications, review of exercise, smoking, dietary habits, level of social support, and consideration of future health care plans. Cognitive assessment tools have been developed  that could help in stimulating such discussions, and efforts are being made to raise awareness of the need for these activities . Because of the Affordable Care Act, those efforts currently focus on the Medicare population, estimated to include 93% of noninstitutionalized adults aged 65 or older . Based on these results for adults aged 65 or older with SMCs, there is room for improvement in discussing memory issues with a health care professional, although no information exists on adults who did not report SMCs. And, as mentioned above, results for adults younger than 65 suggest they might also benefit.\n\n【26】Also new in this study is that 41.8% of those who talked with a health care professional about their memory problems reported getting treatment. This percentage probably underestimates the percentage of those being treated because only those who said they had such discussions were asked that question, and some may be unaware that treatments they were receiving for other purposes also address memory problems. The data also provide no indication of when treatment occurred in relation to the discussion, what the treatment was, or if it was effective. However, the data suggest that at least in some cases the discussions might lead to treatment, especially because the treatments mentioned in the question were therapy or medications. The finding that those diagnosed with dementia were more likely than those not diagnosed to have been treated is not surprising given that the drugs currently available  treat Alzheimer’s and dementia rather than mild cognitive impairment. These drugs can temporarily help alleviate memory problems in some people who take them but cannot stop disease progression. New drugs in development may stop or slow the progression of the disease so that diagnosis at an earlier stage may be even more critical.\n\n【27】This study has several limitations. Results are subject to the usual limitations of telephone surveys, including possible bias due to nonresponse and errors caused by self-reporting and recall. Testing of other questions on the BRFSS suggests that the validity of measures in medical records (eg, a dementia diagnosis, recent check-up) is high , but the validity of the SMC measure is not known, nor has testing been done among respondents with cognitive impairment. People with dementia may be undiagnosed or unaware of their diagnosis, which might help explain respondents reporting treatment but not a diagnosis of dementia.\n\n【28】These results do not represent all noninstitutionalized adults because the BRFSS random selection process excludes a household if the selected respondent is physically or mentally unable to respond to the survey. A study using data from the cognitive impairment module in Wyoming  found that older nonrespondent adults in the household with SMCs for whom the respondent provided proxy responses were more likely to have functional difficulties (57% of proxied respondents vs 37% of respondents) and to have talked with a health care professional about their memory issues (60% of proxied respondents vs 23% of respondents) . Using cognitive impairment module data from 13 states , similar percentages were found for proxied responses only. Limiting studies to BRFSS respondents may be underestimating the extent of the impact of memory problems in the community. Another study limitation is the lack of information on respondents’ perceptions, beliefs, or family history of Alzheimer’s or dementia, factors that may affect discussions with health care providers . The generalizability of the results is unknown because of the state-by-state variation in these results and those reported elsewhere .\n\n【29】This study has several strengths. The BRFSS provides a large and representative sample; this study included more than 10,000 adults in 21 states reporting memory problems, and more than 2,200 reported talking with a health care professional about these problems. The study included a large number of variables, including information on demographics, self-reported behaviors, chronic conditions, access to health care, and health status.\n\n【30】These findings suggest that routine check-ups are a missed opportunity for assessing and discussing memory problems for the majority of adults aged 45 or older with SMCs. These results, based only on adults reporting SMCs, highlight the need for the cognitive assessment required in the Affordable Care Act for Medicare recipients and also suggest the need and potential benefits of cognitive assessment among adults younger than 65. Although memory problems could be discussed during any routine check-up, these results show that few adults reporting SMCs, even those with memory problems causing functional difficulties, are availing themselves of that opportunity. Public health officials and organizations can become involved by raising general awareness of the advantages of early detection of cognitive impairment and dementia and discussions with a health care professional. An example of such a message to health care providers is a letter from the New York State Department of Health that includes information from the Alzheimer’s Association on conducting assessments during the Medicare annual wellness visit . These actions could potentially result in greater numbers of older adults being diagnosed and treated for memory problems and made aware of available services.\n\n【31】Tables\n------\n\n【32】#####  Table 1. Self-Reported Discussions With a Health Care Professional About Memory Problems, Adults Aged ≥45 Years Reporting Subjective Memory Complaints, by Selected Characteristics a  , BRFSS in 21 states,\n\n| Characteristic | No. of Respondents | Respondents Who Reported Discussion, % (95% CI) |\n| --- | --- | --- |\n| **All adults aged ≥45 with SMCs** | 10,276 | 22.9 (21.1–24.9) |\n| **Age, y** | **Age, y** | **Age, y** |\n| 45–54 | 2,175 | 27.1 (23.3–31.2) |\n| 55–64 | 2,948 | 24.8 (21.6–28.3) |\n| 65–74 | 2,439 | 18.0 (15.2–21.2) |\n| 75–84 | 2,008 | 16.8 (13.7–20.4) |\n| ≥85 | 706 | 14.4 (10.3–19.7) |\n| _P_ value | <.001 | <.001 |\n| **Education** | **Education** | **Education** |\n| <High school | 1,523 | 19.2 (15.6–23.5) |\n| High school graduate | 3,414 | 21.9 (18.6–25.5) |\n| Some college | 2,790 | 26.2 (22.7–30.1) |\n| College graduate | 2,529 | 24.4 (21.2–27.8) |\n| _P_ value | .04 | .04 |\n| **Disability b** | **Disability b** | **Disability b** |\n| Any disability | 6,713 | 29.2 (26.8–31.8) |\n| No disability | 3,513 | 11.4 (9.3–13.8) |\n| _P_ value | <.001 | <.001 |\n| **Body mass index, kg/m 2** | **Body mass index, kg/m 2** | **Body mass index, kg/m 2** |\n| <18.5 | 202 | 24.6 (13.5–40.4) |\n| 18.5–24.5 | 2,932 | 20.2 (17.3–23.5) |\n| 25.0–29.9 | 3,401 | 20.4 (17.5–23.7) |\n| 30.0–34.9 | 1,950 | 23.5 (19.6–27.8) |\n| ≥35.0 | 1,469 | 31.5 (26.0–37.5) |\n| _P_ value | .002 | .002 |\n| **Any of 6 chronic diseases c** | **Any of 6 chronic diseases c** | **Any of 6 chronic diseases c** |\n| Yes | 8,365 | 24.5 (22.4–26.7) |\n| No | 1,809 | 17.0 (13.3–21.5) |\n| _P_ value | .004 | .004 |\n| **Current smoking** | **Current smoking** | **Current smoking** |\n| Yes | 2,054 | 27.1 (23.1–31.5) |\n| No | 8,174 | 21.7 (19.7–23.8) |\n| _P_ value | .02 | .02 |\n| **Had a routine check-up within the previous year** | **Had a routine check-up within the previous year** | **Had a routine check-up within the previous year** |\n| Yes | 7,909 | 25.2 (23.0–27.5) |\n| No | 2,191 | 16.2 (13.3–19.6) |\n| _P_ value | <.001 | <.001 |\n| **Has ≥1 personal doctors** | **Has ≥1 personal doctors** | **Has ≥1 personal doctors** |\n| Yes | 9,493 | 24.3 (22.4–26.3) |\n| No | 753 | 11.3 (7.7–16.4) |\n| _P_ value | <.001 | <.001 |\n| **Health status** | **Health status** | **Health status** |\n| Fair or poor | 4,928 | 28.2 (25.4–31.2) |\n| Excellent, very good, or good | 5,309 | 17.6 (15.4–20.2) |\n| _P_ value | <.001 | <.001 |\n| **Ever diagnosed with depression** | **Ever diagnosed with depression** | **Ever diagnosed with depression** |\n| Yes | 4,254 | 33.2 (29.9–36.7) |\n| No | 5,944 | 14.6 (12.8–16.6) |\n| _P_ value | <.001 | <.001 |\n| **Need help because of SMCs** | **Need help because of SMCs** | **Need help because of SMCs** |\n| Yes | 5,324 | 29.0 (26.3–31.8) |\n| No | 4,640 | 15.4 (13.1–17.9) |\n| _P_ value | <.001 | <.001 |\n| **Gave up household activities or chores** | **Gave up household activities or chores** | **Gave up household activities or chores** |\n| Never | 5,632 | 14.5 (12.7–16.5) |\n| Rarely | 1,608 | 24.3 (20.1–29.0) |\n| Sometimes | 2,104 | 32.1 (27.2–37.3) |\n| Usually | 428 | 41.9 (31.1–53.4) |\n| Always | 359 | 54.1 (43.8–64.1) |\n| _P_ value | <.001 | <.001 |\n| **SMCs interfere with working, volunteering, or engaging in social activities** | **SMCs interfere with working, volunteering, or engaging in social activities** | **SMCs interfere with working, volunteering, or engaging in social activities** |\n| Never | 5,183 | 12.5 (10.7–14.6) |\n| Rarely | 1,859 | 21.4 (17.8–25.6) |\n| Sometimes | 1,886 | 33.8 (29.1–38.9) |\n| Usually | 499 | 38.9 (29.7–48.9) |\n| Always | 728 | 51.3 (43.1–59.4) |\n| _P_ value | <.001 | <.001 |\n| **Functional difficulties d** | **Functional difficulties d** | **Functional difficulties d** |\n| Yes | 3,995 | 34.4 (30.9–38.0) |\n| No | 6,205 | 14.5 (12.7–16.4) |\n| _P_ value | <.001 | <.001 |\n| **State of residence** | **State of residence** | **State of residence** |\n| Arkansas | 531 | 22.2 (17.7–27.5) |\n| California | 498 | 14.8 (11.6–18.7) |\n| Florida | 970 | 25.2 (20.1–31.1) |\n| Hawaii | 483 | 24.4 (18.0–32.3) |\n| Illinois | 347 | 24.7 (18.2–32.6) |\n| Iowa | 356 | 22.4 (17.3–28.5) |\n| Louisiana | 499 | 32.4 (24.9–41.0) |\n| Maryland | 281 | 18.8 (13.7–25.2) |\n| Michigan | 310 | 24.4 (15.9–35.5) |\n| Nebraska | 858 | 18.2 (14.8–22.3) |\n| New Hampshire | 277 | 28.7 (21.7–36.9) |\n| New York | 210 | 25.3 (18.4–33.8) |\n| North Carolina | 661 | 25.2 (20.7–30.2) |\n| Oklahoma | 335 | 25.3 (20.1–31.2) |\n| South Carolina | 941 | 20.9 (16.4–26.2) |\n| Tennessee | 248 | 33.4 (26.1–41.6) |\n| Texas | 581 | 31.9 (25.4–39.1) |\n| Utah | 242 | 20.9 (14.9–28.6) |\n| Washington | 1,036 | 19.6 (16.6–23.1) |\n| West Virginia | 278 | 32.0 (25.8–38.8) |\n| Wisconsin | 334 | 18.4 (12.9–25.7) |\n| _P_ value | <.001 | <.001 |\n\n【34】Abbreviations: BRFSS, Behavioral Risk Factor Surveillance System; CI, confidence interval; SMC, subjective memory complaint.  \na  Characteristics are those that had _P_ values <.05 in Pearson χ 2  tests of association; these variables were included in the starting logistic regression model. Sex, race/ethnicity, income, insurance status, and reporting a cost barrier to health care were not significantly associated with talking with a health care professional.  \nb  Defined as limited in any way in any activities or use special equipment.  \nc  Chronic diseases are arthritis, asthma, cardiovascular disease, cancer other than skin, chronic obstructive pulmonary disease, and diabetes.  \nd  Determined from responses to 2 questions and included those who responded that, during the previous 12 months, they gave up household activities or chores they used to do because of confusion or memory loss that is happening more often or is getting worse always, usually, or sometimes (as opposed to rarely or never) or confusion or memory loss that is happening more often or is getting worse always, usually, or sometimes (as opposed to rarely or never) interfered with their ability to work, volunteer, or engage in social activities, or both.\n\n【35】#####  Table 2. Results of Logistic Regression Model a  for the Outcome Measure of Talking With a Health Care Professional About Memory Problems, Respondents Aged ≥45 Years Reporting Subjective Memory Complaints (N = 9,724 b  ), BRFSS, 21 states,\n\n| Group | Adjusted OR (95% CI) |\n| --- | --- |\n| **Age group, y** | **Age group, y** |\n| 45–54 | 1.0 \\[Reference\\] |\n| 55–64 | 0.80 (0.60–1.05) |\n| 65–74 | 0.68 (0.49–0.93) |\n| 75–84 | 0.66 (0.46–0.95) |\n| ≥85 | 0.58 (0.36–0.95) |\n| **Education** | **Education** |\n| <High school | 1.0 \\[Reference\\] |\n| High school graduate | 1.39 (0.98–1.99) |\n| Some college | 2.00 (1.39–2.87) |\n| College graduate | 2.42 (1.68–3.48) |\n| **Had a routine check-up within the previous year** | **Had a routine check-up within the previous year** |\n| No | 1.0 \\[Reference\\] |\n| Yes | 1.63 (1.20–2.21) |\n| **Has ≥1 personal doctors** |  |\n| No | 1.0 \\[Reference\\] |\n| Yes | 2.24 (1.41–3.57) |\n| **Ever been diagnosed with depression** | **Ever been diagnosed with depression** |\n| No | 1.0 \\[Reference\\] |\n| Yes | 1.66 (1.30–2.13) |\n| **Disability c** | **Disability c** |\n| No | 1.0 \\[Reference\\] |\n| Yes | 1.81 (1.38–2.37) |\n| **Gave up household activities or chores** | **Gave up household activities or chores** |\n| Never | 1.0 \\[Reference\\] |\n| Rarely | 1.38 (0.98–1.93) |\n| Sometimes | 1.52 (1.13–2.06) |\n| Usually | 1.62 (0.93–2.82) |\n| Always | 3.02 (1.81–5.05) |\n| **SMCs interfere with working, volunteering, or engaging in social activities** | **SMCs interfere with working, volunteering, or engaging in social activities** |\n| Never | 1.0 \\[Reference\\] |\n| Rarely | 1.41 (1.02–1.95) |\n| Sometimes | 2.19 (1.60–3.01) |\n| Usually | 2.21 (1.38–3.53) |\n| Always | 2.98 (1.89–4.70) |\n| **State of residence** | **State of residence** |\n| California | 1.0 \\[Reference\\] |\n| Arkansas | 1.29 (0.83–2.01) |\n| Florida | 1.43 (0.95–2.15) |\n| Hawaii | 1.83 (1.05–3.19) |\n| Illinois | 1.61 (1.00–2.58) |\n| Iowa | 1.49 (0.96–2.30) |\n| Louisiana | 1.85 (1.14–3.00) |\n| Maryland | 1.17 (0.65–2.10) |\n| Michigan | 1.33 (0.74–2.40) |\n| Nebraska | 1.03 (0.68–1.55) |\n| New Hampshire | 1.75 (1.07–2.85) |\n| New York | 1.51 (0.83–2.77) |\n| North Carolina | 1.35 (0.89–2.05) |\n| Oklahoma | 1.27 (0.81–1.98) |\n| South Carolina | 1.10 (0.69–1.73) |\n| Tennessee | 2.08 (1.21–3.59) |\n| Texas | 2.05 (1.34–3.12) |\n| Utah | 1.18 (0.67–2.07) |\n| Washington | 1.17 (0.80–1.71) |\n| West Virginia | 1.94 (1.22–3.08) |\n| Wisconsin | 1.22 (0.68–2.21) |\n\n【37】Abbreviations: BRFSS, Behavior Risk Factor Surveillance System; CI, confidence interval; OR, odds ratio; SMC, subjective memory complaint.  \na  The final logistic regression model excluded smoking, chronic diseases, body mass index, functional difficulties, and needing help due to SMCs because these variables were not found to be significant in the starting logistic model. The starting logistic regression model included all of the variables found in Table 1: age; education; disability; body mass index; chronic diseases; current smoking; check-up in previous year; one or more personal doctors; health status; ever diagnosed with depression; need help due to SMCs; gave up household chores or activities; SMCs interfere with work, volunteering, or engagement in social activities; functional difficulties; state of residence.  \nb  Sample size reflects the removal of 552 respondents for whom values for variables in the model were missing.  \nc  Defined as limited in any way in any activities or use special equipment.\n\n【38】Post-Test Information\n---------------------\n\n【39】To obtain credit, you should first read the journal article. After reading the article, you should be able to answer the following, related, multiple-choice questions. Credit cannot be obtained for tests completed on paper, although you may use the worksheet below to keep a record of your answers.org, please click on the “Register” link on the right hand side of the website to register. Only one answer is correct for each question. Once you successfully answer all post-test questions you will be able to view and/or print your certificate. For questions regarding the content of this activity, contact the accredited provider, CME@medscape.net . For technical assistance, contact CME@webmd.net . American Medical Association’s Physician’s Recognition Award (AMA PRA) credits are accepted in the US as evidence of participation in CME activities. The AMA has determined that physicians not licensed in the US who participate in this CME activity are eligible for _**AMA PRA Category 1 Credits™**_ . Through agreements that the AMA has made with agencies in some countries, AMA PRA credit may be acceptable as evidence of participation in CME activities. If you are not licensed in the US, please complete the questions online, print the AMA PRA CME credit certificate and present it to your national medical association for review.\n\n【40】Post-Test Questions\n-------------------\n\n【41】### Study Title: Routine Check-Ups and Other Factors Affecting Discussions With a Health Care Provider About Subjective Memory Complaints, Behavioral Risk Factor Surveillance System, 21 States,\n\n【42】**CME Questions**\n\n【43】1.  Your patient is a 62-year-old man with subjective memory complaints (SMCs). According to the surveillance study by Adams, which of the following statements about the percentage of older adults with SMCs who discussed memory problems with their clinician is _correct_ ?\n    1.  Among all respondents at least 45 years old who reported SMCs, more than half reported discussing them with a health care professional\n    2.  Among all respondents at least 45 years old who reported SMCs and had a recent routine check-up, more than two-thirds reported discussing them with a health care professional\n    3.  Among respondents 65 years or older reporting SMCs, 17.9% of those who had a recent check-up, vs 11.9% of those who did not, reported talking to a health care professional about their SMCs\n    4.  The results differed considerably from those of previous studies\n2.  According to the surveillance study by Adams regarding older adults with SMCs, which of the following variables is _most_ likely associated with these patients discussing their memory problems with their clinician?\n    1.  Reporting that SMCs never caused them to give up household chores\n    2.  Reporting that SMCs never interfered with work\n    3.  Increasing age\n    4.  Being a college graduate\n3.  According to the surveillance study by Adams, which of the following statements about the clinical implications regarding older adults with SMCs discussing their memory problems with their clinician is _correct_ ?\n    1.  Among respondents who discussed SMCs, one-quarter received treatment\n    2.  Routine check-ups may be a missed opportunity for discussions of SMCs that might lead to diagnosis or treatment\n    3.  The Affordable Care Act requires a cognitive assessment for Medicare recipients every 5 years\n    4.  Among study respondents with SMCs who received treatment, three-quarters reported a diagnosis of dementia\n4.  **Evaluation**\n\n    | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. |\n    | --- | --- | --- | --- | --- |\n    | Strongly Disagree |  |  |  | Strongly Agree |\n    | 1 | 2 | 3 | 4 | 5 |\n    | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. |\n    | Strongly Disagree |  |  |  | Strongly Agree |\n    | 1 | 2 | 3 | 4 | 5 |\n    | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. |\n    | Strongly Disagree |  |  |  | Strongly Agree |\n    | 1 | 2 | 3 | 4 | 5 |\n    | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. |\n    | Strongly Disagree |  |  |  | Strongly Agree |\n    | 1 | 2 | 3 | 4 | 5 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2fefef1b-cb2d-4ac9-9dec-abfa70667030", "title": "Genetic Characterization of Seoul Virus in the Seaport of Cotonou, Benin", "text": "【0】Genetic Characterization of Seoul Virus in the Seaport of Cotonou, Benin\nRodents are the most diversified order of wild mammals and are also the prevailing mammal lineage associated with human-inhabited socioecosystems. In addition to their destructive behaviors, rodents are involved in maintaining, disseminating, and transmitting zoonotic pathogens, impacting both animal and human health .\n\n【1】Hantaviruses are transmitted to humans by inhalation of aerosols contaminated by rodent excreta, including urine, feces, and saliva. They are the causative agents of hemorrhagic fevers and are considered emerging pathogens that impact public health worldwide. Hantaviruses are enveloped with a tripartite single-stranded RNA genome of negative polarity comprising small (S), medium (M), and large (L) segments.\n\n【2】Because hantaviruses are host-specific, their geographic distribution is tightly linked to that of their host. However, the emergence of hantaviruses in new geographic regions is still possible by the spread of the rodent reservoir . Transport-mediated dissemination of rodent-borne hantaviruses is of critical importance in their distribution and constitutes a critical health concern . Seoul virus (SEOV), an orthohantavirus first identified in South Korea in 1982, has had a particular impact on global human health attributable to its worldwide dispersal . Some outbreaks are hypothesized to be driven by the sporadic introduction of its now cosmopolitan host, the Norway or brown rat ( _Rattus norvegicus_ ), at seaports or from pet and laboratory rats . Little is known concerning the circulation of SEOV in Africa, although a recent study reported its presence in southeastern Senegal . We screened rats in the Autonomous Port of Cotonou, Benin, to determine the presence of SEOV in these rodents.\n\n【3】### The Study\n\n【4】We trapped rodents in the seaport of Cotonou using Sherman line capture traps and locally made wide mesh traps that were set for 3 consecutive nights in April 2018. We transported the animals to the Laboratoire de Recherche en Biologie Appliquée laboratory in closed containers and processed them the same day. We anesthetized the rodents with diethyl ether and subsequently euthanized them by cervical dislocation.\n\n【5】We screened blood samples from 32 brown rats and 37 house mice ( _Mus musculus_ ) for the presence of hantavirus-reactive antibodies by using an immunofluorescence assay as previously described . A total of 12 (37.5%) rats were seropositive, and the lungs of 2 rats were confirmed positive by reverse transcription PCR as previously described . This discrepancy cannot be explained by the presence of maternal antibodies because all seropositive rats were adults except for 1 subadult. However, the difference could be because of the limited sensitivity of the panhantavirus PCR or to a viral load decrease over time that fell below detection limits in rats that were infected several months earlier.\n\n【6】We treated reverse transcription PCR–positive samples with DNase I , and purified samples with Agencourt RNA Clean XP magnetic beads . We removed ribosomal RNA using a NEBNext rRNA depletion kit . We prepared the sequencing library with a NEBNext Ultra II RNA library prep kit and quantified it using a NEBNext Library Quant kit for Illumina . We sequenced pooled libraries on a MiSeq platform using a MiSeq v3 reagent kit with 300 bp paired-end reads. Raw sequence reads were trimmed and low-quality (quality score <15) or short (<36 nt) sequences were removed using Trimmomatic . The trimmed sequence reads were assembled against reference sequences (GenBank accession nos. NC\\_005237.1, NC\\_005236.1, NC\\_005238.1) using Bowtie2 algorithm  and some in-house scripts.\n\n【7】We deposited S, M, and L segment sequences of strain Benin1368 into GenBank (accession nos. MW561221–3) and analyzed them by using BLAST . BLAST revealed that the best matches were with SEOV strain CSG5 (accession nos. AB618112–30) from Vietnam for the S segment (97.51% nt identity) and the M segment (97.70% nt identity) and with SEOV strain Lyon/Rn/FRA/2013/LYO852  from France for the L segment (96.82% nt identity).\n\n【8】We performed phylogenetic analyses on 3 datasets composed of complete or nearly complete coding regions of S, M, and L segment sequences of SEOV from different geographic areas available in GenBank. We used sequences of Hantaan and Anjozorobe viruses as outgroups in all analyses. We performed phylogenetic analyses as previously described  using the general time-reversible plus gamma distribution plus invariant sites model (S and M segment) or the general time-reversible plus gamma distribution model (L segment).\n\n【9】The 3 datasets produced broadly concordant phylogenetic topologies . All datasets grouped the SEOV strains from Cotonou within a cluster that included strains from Europe (France and Belgium) and from Southeast Asia (Indonesia, Singapore, Vietnam, and Cambodia), referred to as SEOV lineage 7 . Variants of this lineage belonged to SEOV phylogroup A; this group originated in China and subsequently spread to other parts of the world . More specifically, SEOV lineage 7 may reflect the historical connections between regions of Southeast Asia and France through critical trade routes .\n\n【10】From Africa, only short SEOV sequences from conserved parts of S (226 nt) and L (347 nt) segments were available from wild black rats ( _R. rattus_ ) from Senegal . For this reason, we did not include them in our datasets using complete segments. However, phylogenetic analyses on the basis of datasets including these short sequences place them in SEOV lineage 4 (S segment) or 3 (L segment), with low branch support . This inconsistency is potentially attributable to the short length and high conservation of these sequences; although it could indicate a distinct introduction event from Benin, this interpretation must be considered with caution because of the low level of phylogenetic information provided by these sequences. We calculated estimates of evolutionary divergence between strains from Senegal and Benin using MEGAX . Analyses showed 95.7% nt homology for the S segment and 94.8% nt homology for the L segment. Analyses showed 97.85% amino acid-level homology for the nucleocapsid protein and 100% amino acid-level homology for the RNA polymerase.\n\n【11】### Conclusion\n\n【12】Because of insufficient testing for hantavirus infections and unreported mild cases , the exact circulation of hantaviruses on the continent of Africa is unknown. Whereas SEOV is not widely considered a public health issue in Africa by local health authorities, the presence of SEOV-like agents in humans and wild rats is strongly suspected in at least 17 different countries . Recent and unambiguous sequencing-based identification of SEOV in Senegal  and in Benin with our study confirms that SEOV should be anticipated as a possible cause of illness, such as hemorrhagic fever with renal syndrome. Seaports and ships have already been identified as potential entry points for hantaviruses , which is a likely cause in Africa, as our study shows. SEOV strains recovered from brown rats from the Cotonou seaport are phylogenetically similar to strains from Southeast Asia and Europe, regions where many maritime trade exchanges occur that could explain the presence of these strains in Cotonou. The accidental transportation of SEOV-carrying rats at seaports could lead to local emergence of SEOV infections among port workers. Regular sanitary control of rats within seaports could prevent rodentborne and arthropodborne pathogen dissemination through sea trade.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9dc585f2-ef62-4d2d-8d27-95d09a338dec", "title": "Increasing Evidence-Based Workplace Health Promotion Best Practices in Small and Low-Wage Companies, Mason County, Washington, 2009", "text": "【0】Increasing Evidence-Based Workplace Health Promotion Best Practices in Small and Low-Wage Companies, Mason County, Washington, 2009\nAbstract\n--------\n\n【1】**Introduction**  \nModifiable health risk behaviors such as physical inactivity, unhealthy eating, and tobacco use are linked to the most common chronic diseases, and chronic diseases contribute to 70% of deaths in the United States. Health risk behaviors can be reduced by helping small workplaces implement evidence-based workplace health promotion programs. The American Cancer Society’s HealthLinks is a workplace health promotion program that targets 3 modifiable health risk behaviors: physical inactivity, unhealthy eating, and tobacco use. We evaluated employers’ implementation of HealthLinks in small workplaces.\n\n【2】**Methods  \n**We targeted Mason County, Washington, a rural low-income community with elevated obesity and smoking rates. We conducted baseline assessments of workplaces’ implementation of program, policy, and communication best practices targeting the health risk behaviors. We offered tailored recommendations of best practices to improve priority health behaviors and helped workplaces implement HealthLinks. At 6 months postintervention, we assessed changes in best practices implementation and employers’ attitude about HealthLinks.\n\n【3】**Results  \n**Twenty-three workplaces participated in the program. From baseline to follow-up, we observed significant increases in the implementation of physical activity programs (29% to 51%, _P_ \\= .02), health behavior policy (40% to 46%, _P_ \\= .047), and health information communication (40% to 81%, _P_ \\= .001). Employers favorably rated HealthLinks’ appeal, relevance, and future utility.\n\n【4】**Conclusion  \n**When offered resources and support, small and low-wage workplaces increased implementation of evidence-based workplace health promotion best practices designed to reduce modifiable health risk behaviors associated with chronic diseases. Results also suggest that HealthLinks might be a sustainable program for small workplaces with limited resources.\n\n【5】Introduction\n------------\n\n【6】More than half of Americans have 1 or more chronic diseases such as heart disease, cancer, stroke, hypertension, and diabetes , and 39% of the working-age population have at least 1 chronic disease . Health risk behaviors such as tobacco use, unhealthy eating, and physical inactivity are linked to the most common chronic diseases, and chronic health conditions contribute to 70% of all deaths in the United States .\n\n【7】Employers face mounting health care and productivity costs from chronic illnesses among workers . Lost work days and lower worker productivity linked to the most common chronic health conditions may result in an annual economic loss in Washington State of $28.93 billion in 2011 . Low-income workers are concentrated in small workplaces, and they report more chronic diseases and higher smoking levels than other workers . Small workplaces are less likely than larger workplaces to provide workplace health promotion (WHP) programs , yet employers in small workplaces might be able to reduce modifiable health risk behaviors among their workers by implementing evidence-based WHP best practices that target health-related policies, programs, and communication .\n\n【8】The University of Washington Health Promotion Research Center (HPRC) and the American Cancer Society — Great West Division (ACS-GWD) collaborate to promote evidence-based chronic disease prevention to employers . Our research has shown that employers and human resources staff are motivated to implement evidence-based WHP programs and should be targeted in these efforts . ACS-GWD offers a WHP program that delivers free health promotion services to reduce modifiable health risk behaviors. The program, HealthLinks, is tailored for small workplaces with limited resources in that it provides in-person assistance from an ACS-GWD staff person who implements and supports the program through resources offered to the workplaces. This study’s objectives were to 1) improve small workplaces’ capacity to participate in HealthLinks, 2) increase employers’ implementation of evidence-based WHP best practices in small and low-wage workplaces, and 3) evaluate employers’ attitudes about WHP after participating in HealthLinks.\n\n【9】Methods\n-------\n\n【10】### Study design\n\n【11】We conducted a preassessment and postassessment (no comparison group) of employers’ implementation of evidence-based WHP best practices and their attitudes toward WHP after they received the HealthLinks intervention. We conducted the study from January 2009 through September 2009 in Washington State. The University of Washington Institutional Review Board exempted the study from further review after receiving a summary of the procedures and a copy of measures.\n\n【12】### Sample\n\n【13】We targeted Mason County, a largely rural community that has elevated health risk behaviors. The county reports a 29% obesity rate (2% higher than the state) and 29% current smoking rate (14% higher than the state) . The average income of residents in the county is 20% less than state levels, $45,417 versus $56,317 .\n\n【14】We recruited 23 small workplaces (defined as a workplace with fewer than 250 workers) in Mason County, Washington. We identified workplaces that met the inclusion criteria by using several approaches: 1) accessing a public database of businesses in the region ; 2) obtaining referrals from the Washington State Department of Health; and 3) identifying workplaces that had a prior relationship with ACS-GWD through participation in fundraising events or other activities.\n\n【15】### Program procedures\n\n【16】HealthLinks consists of 5 steps: 1) recruitment of workplaces, 2) assessment of baseline implementation of best practices, 3) recommendation of best practices, 4) implementation of recommended best practices, and 5) assessment of employer’s implementation of best practices at 6-months postintervention and their attitude about WHP. To recruit workplaces (step 1), an ACS-GWD interventionist telephoned the upper-level manager at each workplace and briefed the manager on HealthLinks. If the manager showed interest in participating in HealthLinks, the interventionist described the program in more depth. Information offered included 1) an outline of the relationships among missed work days, work productivity, and lost revenue, and 2) an outline of the relationship between WHP and return on investment. To assess baseline implementation of best practices (step 2), an hour-long, in-person assessment was conducted with the workplace manager. The ACS interventionist determined which among the WHP best practices (policy, program, and communication) were present or absent at the workplace (the best-practice instrument is in Appendix A ). Two weeks after the assessment, the interventionist prepared a tailored recommendation report (step 3) that listed each best practice and noted whether the practice was fully or partially implemented. The interventionist then recommended actions to fully implement the best practices (the list of resources and services that could be recommended are in Appendix B ). The ACS interventionist presented the recommendation report to the manager, who selected 2 to 4 recommended actions to implement. Working with the manager, the interventionist implemented the selected recommendations (step 4). For example, employers interested in a physical activity program might select the 10-week Active for Life (AFL) program . The interventionist and personnel from the Mason County Department of Health offered resources and training sessions for the workplace contact responsible for overseeing the AFL program. They also assisted in enrolling participants and tracking physical activity goals by using an electronic tracking system. Participants received incentives when they achieved their physical activity goals and completed AFL. Incentives included boxed lunches and gift cards to local grocery stores (eg, Top Foods, Trader Joe’s, Subway). Six months after the recommendation report, HPRC staff re-administered the assessment instrument to evaluate changes in best-practice implementation from baseline to follow-up. We also assessed employers’ attitudes about WHP.\n\n【17】### Outcome measures\n\n【18】We assessed employers’ implementation of evidence-based best practices before and 6 months after HealthLinks by using the Employer Practices Survey, a 50-item instrument consisting of closed-ended, nonscaled questions. The survey included 12 questions on tobacco use, healthy eating, and physical activity policies; 7 items on physical activity and tobacco use cessation programs; and 19 items on communication of health information. Primary outcomes included percentage of implementation of policy, program, and communication best practices and overall best practices.\n\n【19】We assessed employers’ perception of and satisfaction with HealthLinks at 6 months after the program by using an 8-item Employer Attitude and Satisfaction Survey comprising open- and closed-ended questions. The outcomes were 1) perceived barriers, 2) HealthLinks components employers liked most, 3) HealthLinks components most likely to affect future wellness activities, and 4) HealthLinks communication materials that were most helpful.\n\n【20】### Statistical analysis\n\n【21】We scored most of the Employer Practices Survey questions dichotomously, using a score of 1 for the practices in place and a score of 0 for those not in place. We evaluated employers’ implementation of tobacco policy by using 3 values, 0, 0.75, and 1. We assigned a score of 0 if the employer had no tobacco policy. We assigned a score of 1 if the employer had a complete tobacco ban policy (eg, tobacco use was not allowed anywhere on workplace grounds or in vehicles). We assigned a score of 0.75 if the employer did not allow using tobacco in the building(s). We assigned a score of 0.75 (rather than 0.50) because, by forbidding smoking indoors, employers were restricting most workers’ tobacco use for most of their working hours. For each best practice, we created a summary score by summing the values, dividing by the number of possible points, and reporting the result as a percent. We calculated an overall best-practice score for each employer by summing each best practice score and taking the mean.\n\n【22】We used Wilcoxon matched pairs tests to analyze significant differences in best-practice implementation from baseline to follow-up. We analyzed the data by using SPSS 14.0 for Windows (SPSS, Inc, Chicago, Illinois), and we calculated all reported significant differences at the 95% confidence level.\n\n【23】To assess employers’ attitudes and perceptions, we calculated frequency counts for employers’ responses to closed-ended questions on the Employer Attitude and Satisfaction Survey. For responses to open-ended questions, we looked for responses with similar themes and reported the most common themes.\n\n【24】Results\n-------\n\n【25】### Workplace characteristics\n\n【26】We contacted 69 eligible workplaces in Mason County, Washington, and intervened with 23 (33% participation rate); the workplaces had an average of 42 workers. Most (n = 20) workplaces had 200 workers or fewer. The top 5 industries were tribal centers, lumber and forestry, financial institutions, academic institutions, and public service agencies.\n\n【27】### Objective 1: Improve small workplaces’ capacity to participate in HealthLinks\n\n【28】Several factors affected workplaces’ capacity to participate in HealthLinks. More than half (n = 14) of participating workplaces had a previous relationship with ACS. Two factors most likely to influence workplaces’ decision to participate in HealthLinks were upper management support (n = 8) and concern about the health needs of workers (n = 7). The HealthLinks characteristics that drove employers’ participation included the reputation of ACS (n = 8) and the fact that HealthLinks was easy to implement, broad in scope, and free (n = 8).\n\n【29】Workplaces’ capacity to participate in HealthLinks depended on resources received. Our intervention tracking system documented which resources and educational presentations we delivered to the workplaces . The 3 resources that we delivered to more than half of workplaces were access to fightcancer.org, e-newsletters, and the Quit Line promotional posters. In addition, 10 workplaces and 173 workers from these sites participated in AFL; the Lunch and Learn topic that workplaces most frequently requested was physical activity (n = 9), followed by healthy eating and stress management.\n\n【30】### Objective 2: Increase implementation of the HealthLinks program\n\n【31】Overall, implementation of best practices increased significantly for all 3 practice types — policy, program, and communication . On average, workplaces implemented 36% of the best practices at baseline and 59% at follow-up ( _P_ < .001).\n\n【32】### Objective 3: Evaluate employers’ attitude about HealthLinks\n\n【33】At follow-up, 21 employers reported high satisfaction with HealthLinks . The most popular HealthLinks components were the e-newsletter and the Lunch and Learn presentations. The most helpful communication material was the e-newsletter. Employers favorably rated the Lunch and Learn topics for ease of promotion, relevance, and appeal. Employers rated sessions on physical activity, healthy eating, and stress management as easiest to promote; ratings for relevance and appeal were similar (data not shown). The HealthLinks component most likely to influence future WHP decisions was the Employer Practices Survey assessment of best practices.\n\n【34】Of the 23 participating workplaces, 12 identified at least 1 barrier to HealthLinks implementation. The most common barrier was workers’ not having the time to participate (n = 7) .\n\n【35】Discussion\n----------\n\n【36】In our study, we met the 3 proposed objectives: 1) improved the capacity of small workplaces to participate in the HealthLinks program, 2) implemented HealthLinks with on-site support from a respected community partner, and 3) evaluated attitudes about HealthLinks program components. Guidelines to aid employers in adopting WHP programs are available ; other researchers have identified characteristics that make WHP programs sustainable . The HealthLinks program for small workplaces is potentially sustainable over time.\n\n【37】Sustainable WHP programs target high-risk populations, involve upper management buy-in, increase program accessibility, offer incentives, and increase health awareness through effective communication . HealthLinks exhibited these key elements of sustainable WHP programs for small workplaces.\n\n【38】We effectively targeted high-risk populations (a community with elevated rates of obesity and tobacco use). Most workplaces selected the Quit Line promotional posters, and almost half participated in the intensive physical activity program, thus showing the importance of tobacco use and weight management to the targeted workplaces. In addition, employers appeared to support HealthLinks; they rated the HealthLinks resources and services as useful, relevant, and appealing.\n\n【39】HealthLinks increased workers’ access to AFL, with almost half of workplaces participating in the program. The high level of participation is likely attributable to the support provided by the ACS-GWD interventionist and Mason County Department of Health personnel who helped to identify incentives, managed competitive teams, and coordinated the program at workplaces. Our results demonstrate the importance of offering small workplaces hands-on support to improve workers’ participation in health promotion programs, thus increasing employers’ capacity to engage their workers. Without support and a champion to help promote AFL, many small workplaces may not have had the capacity to implement AFL _._\n\n【40】HealthLinks also helped employers promote the free Quit Line through on-site postings, thus enhancing access to a tobacco use cessation program. Research has shown that although most large and small workplaces rank smoking cession as a priority, only 2% offer cessation benefits  and less than 10% of small workplaces offer cessation programming . Like other researchers , we found that workplaces did not offer tobacco use cessation benefits; however, after HealthLinks, approximately two-thirds of employers promoted the state Quit Line through posters and other print materials, and 26% received information about instituting tobacco ban policies. These are encouraging results for small workplaces. The results demonstrate the willingness of employers in small workplaces to address cessation through policy and programs when they are offered resources.\n\n【41】Improving workers’ health education through effective communication is a key element of sustainable WHP programs  and enhances the sustainability of these programs. In our study, employers showed a high likelihood of implementing various communication strategies to improve workers’ health awareness. Improved health communication was most likely due to the availability of ready-to-use materials and regular distribution of a health-based e-newsletter, making it easier for employers to offer up-to-date health information. With ACS’s assistance, we helped employers establish a communication system that used diverse distribution channels (posters, e-newsletters, Lunch and Learn health education sessions) and offered health information covering multiple topics, with the intention that the workplaces would be able to sustain health awareness among their workers after the intervention ended.\n\n【42】This study has several strengths. The first is that we intervened in a community with elevated smoking and obesity rates. Second, we collected both process and outcome-level data, with process-level data corroborating outcome-level results. Finally, we collaborated with a known and respected community partner, ACS, which set in motion a community-based partnership that strengthened the recruitment and intervention-delivery processes and helped to sustain the relationships with the workplaces.\n\n【43】The study also has several limitations. We did not collect worker-level data, and this limits our understanding of how the HealthLinks program affected workers’ health behaviors and attitude. Second, our study used a preintervention and postintervention analysis without using a comparison group; however, our results demonstrated the feasibility of implementing WHP programs in small and low-wage workplaces and may potentially pave the way for future randomized controlled trials using the model of working with community partners and offering enhanced support.\n\n【44】Employers in small and low-wage workplaces can improve their workers’ health through evidence-based WHP best practices targeting specific modifiable health risk behaviors. The keys to working with small workplaces include making the WHP program easy to implement, collaborating with a respected community partner, and offering free resources and hands-on support. By targeting high-risk communities, obtaining employer buy-in, making the health programs accessible, and effectively communicating information to workers about health and wellness, WHP programs such as HealthLinks have the potential to be sustained over time. A recent report emphasized the need to disseminate “real-life” successful, WHP programs . Our study showcased a WHP program tailored to small and low-wage workplaces that increased employers’ implementation of evidence-based best practices. Furthermore, we targeted and reached small workplaces with workers at high risk for obesity and tobacco use.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b33a0d1f-5138-49a9-ade9-bae31f0746e1", "title": "West Nile Virus in Mexico: Evidence of Widespread Circulation since July 2002.", "text": "【0】West Nile Virus in Mexico: Evidence of Widespread Circulation since July 2002.\nDuring the summer of 2002, the Agricultural Ministry of Mexico (SAGARPA) received reports of encephalitis-like illness in horses from several different areas of Mexico, concurrent with reports of West Nile virus (WNV) encephalitis outbreaks in horses along the Texas border in the states of Coahuila, Tamaulipas, and Chihuahua. Other suspected cases were reported from several southern, tropical states. We report the results of an equine serosurvey conducted from July 2002 to March 2003 by the Office of Exotic Diseases of the Agricultural Ministry (CPA-SAGARPA). We also describe the first isolation of WNV in Mexico, in a Common Raven ( _Corvus corax_ ) from the state of Tabasco.\n\n【1】The Republic of Mexico is divided by the Tropic of Cancer, with temperate, arid climate zones in the north and at higher elevations and humid, subtropical, and tropical climate zones in the south. Our study encompassed most of these climatic zones, as equine serum samples were collected from 3 border states, 1 state on the Tropic of Cancer, and 10 states south of the Tropic . Sampled equine populations were chosen on the basis of a history of clinical encephalitis; medical history was provided by owners and corroborated by CPA-SAGARPA veterinarians. In total, 441 serum samples were analyzed for WNV antibodies.\n\n【2】Because most serum samples were collected late in the probable virus transmission season, all were first screened for immunoglobulin (Ig) G antibodies, using IgG enzyme-linked immunosorbent assays (ELISA) with a recombinant, envelope protein domain III antigen expressed and purified from Escherichia coli (D.W.C. Beasley, et al. submitted for pub.). Positive samples were confirmed by hemagglutination inhibition (HI) tests against WNV and St. Louis encephalitis virus (SLEV), by 90% plaque reduction neutralization tests (PRNT) against WNV , and by ELISA with WNV, SLEV, and Venezuelan equine encephalitis virus (VEEV) antigens and viruses. (The presence of several endemic arboviruses, including SLEV and VEEV, necessitated additional testing.) WNV infection was confirmed if the WNV antibody titer was \\> fourfold higher than the SLEV titer. To investigate evidence of recent WNV infection, 198 samples were also tested by using both IgG- and IgM-specific ELISA with WNV-infected cell culture antigens . Selected samples were tested by ELISA and PRNT for VEEV antibodies to determine if this virus was circulating in areas reporting equine encephalitis.\n\n【3】### Results\n\n【4】A total of 441 equine serum samples from 14 states of Mexico were tested . WNV-specific antibodies were detected in 97 (22%) of the samples. These data probably overestimate the true equine seropositivity rate because sampling focused on herds with a history of clinical encephalitis. Representative data from 22 of the WNV-positive samples obtained in five different states are presented in Table 1 . No evidence was obtained of SLEV infection in equines, but VEEV-specific antibodies were detected in serum samples from the states of Veracruz and Yucatán. (Horses are vaccinated against VEEV in the states of Chiapas and Oaxaca, so samples from these locations were not tested for VEEV antibodies.) The positive samples, several of which also contained WNV IgG, represent natural VEEV circulation and infection of horses, presumably with enzootic subtype IE strains .\n\n【5】On May 5, 2003, the CPA-SAGARPA received a report of a dead Common Raven from the El Yumka wildlife preserve in the city of Villahermosa, state of Tabasco. Although this species is native to Tabasco and other regions of Mexico, this bird was one of two ravens imported from the United States in 1999. A necropsy was performed, and virus isolation was attempted on tissue samples at the CPA-SAGARPA biosafety level 3 facility in Palo Alto, Mexico City. On May 16, 2003, cytopathic effects were detected in Vero cells injected with brain suspension. Viral RNA from the isolate was genetically characterized at the National Institute for Epidemiology and Diagnostics (InDre) in Mexico City and at the University of Texas Medical Branch in Galveston. A 2,004-nt genome portion, including the prM-E protein region, was amplified by using a reverse transcription–polymerase chain reaction as described previously ; the resulting amplicons were sequenced directly with the Big Dye sequencing kit and model 3100 sequencer (Applied Biosystems, Foster City, CA). The sequence of this WNV isolate  was aligned with all homologous WNV sequences of the same length available from the GenBank library , and phylogenetic trees were constructed by using maximum parsimony, maximum likelihood (incorporating empirical base frequencies, a general time-reversible substitution model with the following frequencies: A→C 1.34263; A→G 4.18575; A→U 1.55497; C→G 0.044980; C→T 15.08737; G→U 1.00000, a γ shape parameter of 0.228), and neighbor joining programs implemented in the PAUP 4.0 software package . All trees had nearly identical branching orders; the maximum parsimony tree is presented in Figure 2 . All trees placed the Mexican raven isolate as a sister to a clade that comprises most WNV strains isolated in Texas during 2002. Two other WNV strains from the Bolivar Peninsula, Texas , were positioned basally to a 1998 Israeli stork isolate that grouped with all other North American isolates, suggesting that the North American strains may not all have originated from a point source introduction into the New York area in 1999. This topologic finding was the result of a single synonymous, third codon position  U nucleotide (synapomorphy) shared by the 1998 Israeli and all North American strains except the Bolivar Peninsula isolates. However, the relatively poor support, represented by bootstrap values (1,000 replicates) of only 56–59 by using the three different phylogenetic methods, suggests that this topologic finding is not necessarily correct. More robust phylogenetic analyses using complete genomic sequences are needed to clarify these relationships.\n\n【6】Comparison of nucleotide sequences indicated mutations at 9 nt (0.45%) of the Mexican TM171-03 sequence compared to the prototype NY99 strain . Some of these nucleotide positions vary among other strains sampled worldwide, suggesting that they are not under strong purifying selection. Comparison with sequences of Year 2002 Texas strains showed only one shared mutation (genomic position 2466 C→U). WNV has remained genetically conserved in the New World; however, based on our limited sequencing, the Mexican strain appears to be the most divergent WNV isolate identified to date in the Americas . Two of the mutations resulted in amino acid changes at prM141 (Ile→Thr) and E156 (Ser→Pro), which have not been reported to date in North American isolates. The Ser→Pro amino acid substitution at residue E156 is of interest, as it abolishes a potential glycosylation site that is a putative WNV virulence determinant . This report is the first of a New World WNV isolate with probable altered E protein glycosylation. Further studies are in progress to assess the possible phenotypic effects of this mutation.\n\n【7】### Conclusions\n\n【8】Two recent publications reported serologic evidence of WNV infection among equines in the states of Yucatán and Coahuila from serum samples collected in October and December 2002, respectively . We obtained serologic evidence of earlier and more widespread circulation of WNV in five other Mexican states, dating back to July 2002. We also report the first isolation of WNV from Mexico from a dead Common Raven that resided in a wildlife preserve in Tabasco.\n\n【9】Genetic studies indicated that the Mexican WNV strain was likely introduced from the central United States. The level of genetic divergence (9 nt) of the Mexican isolate and the unique amino acid substitutions in the prM and E proteins when compared to all other North American WNV isolates suggest that the Mexican strain has been evolving independently for some time and did not simply enter Mexico recently from Texas. We speculate that this strain descended from a WNV strain introduced into the Yucatán peninsula by migrating birds. Nucleotide sequences of viruses isolated from Mexican states close to the U.S. border, once obtained, may more closely resemble strains isolated in Texas during 2002.\n\n【10】Of particular interest is the overlap in distribution of WNV and VEEV (serologic data for VEEV not shown) in the southern Mexican states of Veracruz and Yucatán; the presence of other flaviviruses like SLEV in these states is also likely. Both WVN and VEEV produce clinically similar neurologic disease in horses, and past, presumptive diagnoses of VEEV may have been inaccurate. Steps are now in place at the Mexico City headquarters of the Animal Health Division of SAGARPA to implement appropriate laboratory diagnosis for flaviviruses and alphaviruses. Additionally, field personnel are instructed to investigate epidemiologic signs of possible WNV infection including avian death and unusual human neurologic syndromes.\n\n【11】The biologic and epidemiologic consequences of mosquito-borne encephalitis viruses  cocirculating in the same ecosystem should be examined. The impact of WNV on human health in regions (such as Mexico) where inhabitants may have extensive prior exposure to other flaviviruses such as dengue, SLEV, Ilheus, Bussuquara, Jutiapa, and Yellow fever viruses may differ from that in regions (e.g. the United States and Canada) where human exposure to flaviviruses is very limited. WNV infection in persons with previous flavivirus immunity, which could either attenuate disease because of cross-protective antibodies  or potentially worsen disease because of immune enhancement , should be studied. Our ongoing VEEV surveillance in southern Mexico may identify differences in transmission habitats for VEEV and WNV and assist with optimizing virus containment efforts.\n\n【12】Dr. Jose G. Estrada-Franco is an assistant professor at the University of Texas Medical Branch. His research interests include the ecology and epidemiology of vector-borne diseases, their human impact, vector genetics, and vector-host-pathogen interactions of arboviruses and parasitic diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5c9a352a-cee7-487d-a750-bacdcf0eb192", "title": "Sun Smart Schools Nevada: Increasing Knowledge Among School Children About Ultraviolet Radiation", "text": "【0】Sun Smart Schools Nevada: Increasing Knowledge Among School Children About Ultraviolet Radiation\nBackground\n----------\n\n【1】Exposure to ultraviolet radiation (UV) is a risk factor for the development of skin cancer . The risk increases with the increasing amount of UV exposure over time , and exposure during childhood and adolescence can produce harmful long-term effects in adulthood , including an increased risk of melanoma. With roughly 300 sunny days per year and an average elevation of 5,500 feet, Nevada residents are at a particularly high risk for the development of skin cancer. Age-adjusted incidence is estimated at 15.6 per 100,000 , and age-adjusted deaths at 2.9 per 100,000 per year . However, these rates are probably much higher, because incidence of melanoma is underreported in the state (Unpublished master’s thesis, Lensch BT. Going for gold: analyzing death certification only cases in the Nevada Central Cancer Registry to merit registry certification. University of Nevada, Reno, Reno, Nevada; 2016).\n\n【2】Reducing UV exposure at the earliest possible age is essential to reducing skin cancer rates . Exposure can be reduced or avoided by appropriate use of sunscreen or protective clothing, by seeking shade, and by avoiding indoor tanning . The Community Guide recommends education and policy interventions in child care centers and primary and middle schools to increase skin cancer prevention behaviors in these vulnerable populations .\n\n【3】Community Context\n-----------------\n\n【4】To decrease the incidence of skin cancer in Nevada, the Nevada Cancer Coalition’s Sun Smart Schools pilot program was conducted during the 2015–2016 school year in 7 schools across Nevada. The goal of this program was to establish healthy sun safety habits among children and teens to prevent skin cancer development later in life by integrating sun safety education with school policy changes designed to support sun safe practices. Sun safe practices are not common in schools, despite their potential to change norms and increase sun safe behaviors that reduce UV exposure at little to no cost .\n\n【5】Program participants were students at public and private schools in urban and rural counties and spanned primary school and secondary school age groups. The program was implemented as a partnership between participating schools and the Nevada Cancer Coalition, a statewide nonprofit organization dedicated to cancer control. The program consisted of numerous essential elements but allowed participating schools flexibility in how best to implement these elements. Examples of physical sun-protective elements included asking parents to have their child apply sunscreen before coming to school, providing a 1-ounce bottle of sunscreen to each student for use while on school grounds, and allowing students to wear hats and sunglasses while outdoors during the school day. Teachers were given an age-appropriate curriculum about sun safety to incorporate into their lessons, daily UV index announcements to broadcast over the intercom system, and guest speaker presentations to educate students about UV and sun safety. Each school was required to use an age-appropriate, evidence-based curriculum (the Environmental Protection Agency SunWise for grades kindergarten through eighth, SunSmart U developed by the Skin Cancer Foundation for grades 6 through 12, and CATCH Global Foundation’s Ray and the Sunbeatables \\[for preschoolers and kindergarten through first grade\\]). All curricula offered were free, evidence-based, easy to use and teach, and met the schools’ common core requirements.\n\n【6】Schools were also asked to adopt a written policy for sun safety. The Nevada Cancer Coalition provided sample policies and technical assistance to participating schools to help ensure all policies would be effective in reducing UV exposure. The policies required adopting at least one of the following strategies: promotion of sunscreen use, approval of sun protective clothing on campus, and access to shade. We conducted an evaluation of this program via pre-intervention and post-intervention hard-copy surveys of students and parents to measure changes in attitudes and knowledge about sun safety. Parents were not direct participants in the program; however, because parents may be the most important influence in a child’s attitude and behavior about sun safety , we theorized that including them to even a limited degree could increase program effectiveness.\n\n【7】Methods\n-------\n\n【8】The Nevada Cancer Coalition used convenience sampling at participating schools to assess pilot program effectiveness. Four distinct subgroups (elementary, middle, and high school students and parents) completed a pre-intervention survey in August or September 2015. A follow-up post-intervention survey was conducted with the same subgroups in May 2016. Each survey was targeted to a fourth-grade reading level and reflected the age-appropriate, evidenced-based curriculum that teachers selected. All surveys were developed in partnership with the Office of Statewide Initiatives at the University of Nevada, Reno, and were validated by that office. A total of 1,441 people completed the pre-intervention survey, and 987 completed the post-intervention surveys for a total of 2,427 total survey responses.\n\n【9】Survey questions were identical at both pre-intervention and post-intervention to assess increased knowledge gain and behavioral and attitudinal changes. The only respondent identifier was the child’s grade, so pre-intervention and post-intervention surveys could not be linked to the same student, and we were able to assess only differences in overall score changes by age group. Responses were analyzed separately for elementary school students (fourth and fifth grade), middle school students (sixth, seventh, and eighth grade), high school students (tenth grade), and parents. Although parents did not receive formal sun-safety education, they received program updates in school newsletters and were encouraged to provide sunscreen and protective clothing for their children while at school.\n\n【10】Because surveys were designed for a reading level of fourth grade and above and were intended to be distributed only to children who met these inclusion criteria, elementary-school–aged students who were younger than this or who did not provide their grade level were excluded from analysis in both the pre-intervention surveys (n = 8) and post-intervention surveys (n = 194). Among middle school students, responses from students who did not provide their grade level were excluded from analysis in both the pre-intervention survey (n = 3) and post-intervention survey (n = 20). Among high school students, because the sun safety curriculum was implemented only in the tenth grade health class, and the pre-intervention survey was completed only by tenth grade students, students in other grades or those who did not provide their grade (n = 25) who took the post-intervention survey were excluded from analysis. A total of 2,177 survey responses were included in the analysis.\n\n【11】Results for the student surveys were weighted by dividing the total number of children enrolled in each grade at each school by the number of responses for that grade, providing a representative response distribution for each grade at each school. Parent surveys were weighted by dividing the total number of students enrolled at each school by the number of parent responses for each school. These weighted responses were used to calculate overall percentages of change among question responses, and χ 2  testing was conducted to determine if results were significant, meaning the result was not likely to occur randomly and was likely attributable to a specific cause. Additionally, Likert-scale responses were dichotomized in analysis. Survey questions with a Likert-scale response of “most of the time” and “sometimes” were equated with a positive response of “yes,” while those responses of “rarely” and “never” were equated with a negative response of “no.” A 95% confidence level ( _P_ value < .05) was used to interpret the significance of each result. All analysis was done using SAS version 9.4 (SAS Institute, Inc) software.\n\n【12】Outcomes\n--------\n\n【13】### Elementary school\n\n【14】Overall, elementary school students showed increased belief in the importance of sun safety practices from pre-intervention to post-intervention . Among these students, there was an increase of 8.1 percentage points ( _P_ \\= .047) from pre-intervention to post-intervention among those reporting yes to “Do you always or sometimes put sunscreen on when you go outside for a long time?” Additionally, there was an 18.5 percentage-point increase ( _P_ \\= .003) in the number students who reported wearing sunglasses post-intervention.\n\n【15】The elementary-school–aged students were asked one knowledge-based question reflecting the curriculum provided to their teachers: “Can you get a sunburn on a cloudy day?” There was a 10.1 percentage-point increase in yes responses from pre-intervention to post-intervention, and a 6.8 percentage-point decrease of “I don’t know” responses.\n\n【16】### Middle school\n\n【17】Middle school students showed an overall decrease in attitude toward the importance of sun safety practices from pre-intervention to post-intervention. However, middle school students showed a significant increase of 19.1 percentage points ( _P_ \\= .003) among those who reported wearing a hat, and a 29.5 percentage-point increase ( _P_ < .001) in those who reported wearing a long-sleeve shirt when outside in the middle of the day. Despite some negatively changed behaviors surrounding sun safety, there was a significant 7.1 percentage-point decrease in children reporting that they felt they and their friends looked better with a suntan, with only 11.9% of middle school students reporting they felt this way post intervention.\n\n【18】Middle-school–aged students were asked one knowledge-based question that reflected the curriculum provided to their teachers: “Can you get a sunburn on a cloudy day?” There was a significant increase of 20.1 percentage points in yes responses from pre-intervention to post-intervention, and a decrease of 12.4 percentage points in “I don’t know” responses.\n\n【19】### High school\n\n【20】High school students who participated in the Sun Smart Schools program showed an overall increase in practicing sun safe behaviors with a significant 14.9 percentage-point increase ( _P_ \\= .04) in sunscreen use and a nonsignificant increase of 13.3 percentage points ( _P_ \\= .07) when outside for a long time from pre-intervention to post-intervention. They also increased their use of hats when outdoors. Despite this, attitudes about the importance of protecting themselves from the sun did not change much from pre-intervention to post-intervention, with a minor increase from 65.5% to 66.3% of respondents saying it was important or very important and most high school students maintaining a positive attitude toward the appearance of tanned skin.\n\n【21】High school students were asked 3 knowledge-based questions reflecting the curriculum provided to their teachers. Responses to the question, “Can you get a sunburn on a cloudy day?” were surprising: 86.2% of pre-intervention respondents answered yes but only 77.6% of respondents answered yes at post-test. The second knowledge-based question asked if they thought spending a lot of time in the sun in childhood could lead to skin cancer when they were older. There was a nonsignificant increase in yes responses of 4.6 percentage points, and a slight decrease in both no and “I don’t know” responses. Lastly, students were asked if they thought a base tan helps protect their skin from sun damage. There was a nonsignificant increase of 9.3 percentage points in correct no responses in the post-intervention survey, and a decrease of 6.7% in “I don’t know” responses.\n\n【22】### Parents\n\n【23】Although parents were not directly exposed to the program, they received information and updates in newsletters and, potentially, through their children. Even with this limited exposure, there was a significant increase of 10.9 percentage points in parents who reported wearing a long-sleeve shirt when outside in the middle of the day. Responses to 3 knowledge-based questions showed some promising results, but only one, “Do you use a long-sleeve shirt when outside in the middle of the day,” showed a significant increase ( _P_ \\= .005). Parents also reported a decrease in the attitude-based question, “What do you think about protecting yourself from the sun”; however, most parents (83%) still found sun protection to be important even at post-survey. Opportunities to expand parent participation could be examined in the future.\n\n【24】### Notable outcome difference among subpopulations\n\n【25】Most elementary-school–aged children felt that protecting themselves from the sun was important in both the pre-intervention survey (87.1%) and post-intervention survey (90.7%). By high school, this dropped to 65.5% pre-intervention and 66.3% post-intervention. In addition, at post-intervention survey only 17.8% of elementary school students felt that they looked better with a tan, whereas 60.2% of high school students felt that they looked better with a tan.\n\n【26】Interpretation\n--------------\n\n【27】Behavior changes to reduce UV exposure were seen among all groups. Sunscreen use increased among elementary school and high school students. Middle school students and parents increased their use of long-sleeve shirts when outside in the middle of the day, high school students increased hat use, and elementary school students increased use of sunglasses. The Sun Smart Schools program helped facilitate the availability of sunscreen by asking parents to encourage sunscreen application each morning and to provide sunscreen for use while on the school campus. Additionally, each school was encouraged and supported by the Nevada Cancer Coalition to write new policies supporting the wearing of hats, sunglasses, and other protective clothing when outside, encouraging an atmosphere of sun safety and allowing students to protect themselves.\n\n【28】Students in elementary school were the only group that showed significant knowledge gains pre-intervention to post-intervention. Knowledge deficits among other age groups could be due to poor use of the curriculum provided and may point to the need for additional support from the Nevada Cancer Coalition to participating schools and classrooms. Research highlights the importance of partnerships and program advocates within schools and the community to create robust and lasting sun safety programs, so added support within Nevada’s school system may be needed to encourage full participation .\n\n【29】Changing attitudes about sun safety is challenging, especially among high school students. The Centers for Disease Control and Prevention has developed research-based recommendations for interventions to prevent skin cancer . Although education and policy approaches in primary school settings (kindergarten through eighth grade) are effective in changing behaviors, insufficient evidence is available to show that educational approaches or activities designed to influence behaviors or attitudes are effective for high school students and parents. The difference in effectiveness between age groups, and the fact that high school students maintain a positive attitude toward the appearance of tanned skin even when acknowledging the risk tanned skin represents, illustrates the conflict high school students may experience in desiring to adhere to social norms despite knowledge of risks. It also highlights a gap in the effectiveness of the existing curriculum to fully address social norm change.\n\n【30】Participation within each school varied by grade level and individual classrooms. Because of this, a true count of total participants reached in the pilot program is not available, and overall response rates to the pre-intervention survey and post-intervention survey cannot be calculated. An additional limitation to this pilot program was that no demographic information was captured for the survey respondents, so differences in participation by sex or race is not available.\n\n【31】Even with a decrease from pre-test to post- test, the majority of parents (83%) felt protecting themselves from the sun to be important. This suggests that as we age into adulthood, sun safety becomes more important. Results at post-intervention survey showed over 35% of parents felt they looked better with a tan, and although that percentage is much higher than that for elementary or middle school students, it is half of what high school students reported, again suggesting adults are more conscientious about skin health than high school students. A limitation to the analysis of the parent surveys was that they captured no identifiable information, and all responses were voluntary. Therefore, it is possible that the group surveyed post-intervention was different from the group surveyed pre-intervention. Additionally, parents were not given specific curricula and were outside the reach of school policy changes, which helps highlight the need for increased communication and participation with parents moving forward.\n\n【32】The pilot program had many limitations. Perhaps the most challenging was the flexibility allowed schools in the implementation of the various program elements. This likely added variability and created challenges in assessing the effectiveness of the program overall. It is also likely that selection bias occurred because of convenience sampling, and all statistics should be interpreted with caution. It must be considered that those who responded to the survey and those who did not respond represent 2 separate populations; thus, we cannot definitively say that results are representative of the general student–parent population. However, convenience sampling was still appropriate given the nature of the pilot study and large sample size to evaluate program effectiveness. Another limitation was the time at which the post-implementation surveys were conducted. This time period was selected to coincide with the end of the school year and was appropriate for assessing the short-term outcomes of knowledge, attitudes, and behaviors. However, a longer follow-up period would be more desirable to measure the program’s long-term effectiveness. Finally, the program began in the fall and was held throughout the winter with the post-implementation survey conducted in late spring. Some of the positive effects measured may be due to attitudinal changes occurring naturally with the season change and not due to the program’s influence.\n\n【33】Our results represent a starting point for developing, expanding, and tailoring the Sun Smart Schools curriculum and focus within a school-based setting. Although the interventions implemented among elementary and middle school students seemed to affect attitude, knowledge, and behavior, high school students and parents showed less behavior change associated with the program. This information indicates areas for improvement within the Sun Smart Schools program. Continuing to make policy changes within schools to improve accessibility of sun safety practices could influence better sun-protective behaviors from high school students while at school. Additionally, encouraging environmental changes within schools, such as adding shade structures or trees to the campus, are important. Clark County, which houses Nevada’s largest student population and experiences extreme heat, created an initiative to add shade structures to all playgrounds at elementary schools in their district , and similar actions should be encouraged in schools across Nevada. As the program goes forward, we will collect information on sex and race to find gaps in program elements and to address them as needed. One goal of this program is to provide long-term results to help bridge the behavioral and attitude gap between elementary school age and adulthood by building a solid foundation of knowledge and behaviors that change the way adolescents act as they age. By exposing children to a sun safety curriculum and by providing protection at school throughout primary and secondary education, we hope to create a culture of sun safety and thus reduce the incidence of skin cancer in Nevada.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a97f3b99-18a3-4bcd-9799-74cbbce246bb", "title": "Using Lessons Learned from Previous Ebola Outbreaks to Inform Current Risk Management", "text": "【0】Using Lessons Learned from Previous Ebola Outbreaks to Inform Current Risk Management\nConnecting Organizations for Regional Disease Surveillance (CORDS), together with the Southern African Centre for Infectious Disease Surveillance, organized an emergency meeting (September 1–2, 2014, in Dar es Salaam, Tanzania) to gather and collate first-hand experience from past Ebola outbreaks. The major aim was to identify key lessons that could inform current risk management. This meeting brought together a unique assembly consisting of scientists, policymakers, community and religious leaders, traditional healers, and media representatives from eastern and central Africa. They elucidated 3 major lessons that focus on improving communication, working with communities, and building and strengthening local capacity.\n\n【1】### Communication: Work with Communities, Not against Them\n\n【2】A major conclusion was that infectious disease management will work only when it is established with and within the community and not directed against it. This lesson requires community engagement in formulating infection control measures, as well as implementation, dissemination, and promotion of these measures. Infection control procedures are generally perceived as intrusive and, as such, often interfere with local social, cultural, and religious practices. For instance, the recommendation to avoid physical contact with a sick person is simply outside the behavioral norms in most communities. For example, from a community perspective, “protect yourself when caring for a sick person” would be a more suitable recommendation and a better alternative than a recommendation to avoid all physical contact. Building on this process of finding the right, appropriate containment measures, communication and health promotion work best when they involve community and religious leaders, traditional healers, and other advocates.\n\n【3】National and cross-border Ebola outbreaks are a new development, and engagement with various communities has presented a particular challenge throughout the current outbreak. A key aspect of this engagement is to devise and elaborate solutions for infection control that are consistent with local realities and practices. International health and aid organizations must strive to work in concert with communities to find adequate infection-control solutions.\n\n【4】### Communication: Share Early, Listen to Beliefs, and Read Rumors\n\n【5】Early sharing of information and surveillance data among professional groups was considered to lead to clear benefits. The countries of central and eastern Africa learned from previous outbreaks, and they have established infectious disease surveillance networks that operate in cross-border regions. Training in risk communication and One Health promote early communication among neighboring professionals across sectors and with the public. Multisectoral collaboration is key to early detection and faster response.\n\n【6】The risk communication approach also encourages public health professionals to engage with their communities to gain a better understanding of community values, opinions, and beliefs. For instance, a major lesson involved burial practices: participants explained that for local persons, a traditional burial is more essential than protecting themselves from an infectious disease they have not yet encountered. Therefore, the well-intended infection control messages went unheeded because they were not perceived to be relevant to completing a sacred ritual.\n\n【7】Traditional burial practices can hardly be stopped through the imposition of infection control measures (“Don’t touch/wash”), but they could be made safer by integrating protective steps into the rituals, such as using gloves and burying the deceased rapidly. International organizations might need to redesign their communication strategy to prioritize the communities’ needs, not their own requirements.\n\n【8】Communities have their own communication networks, and rumors spread fast and are influential. Regarding infectious disease, there are 2 kinds of rumors: 1) about possible cases (alerts) and 2) about community explanations of causes. Rumors distract from (compete with) the health message. Outcomes from this meeting suggest reframing rumors as useful guidance similar to pain, rumors provide often uncomfortable but useful feedback. The 2 main types of rumors are useful indicators to for 1) guiding case detection and 2) understanding where communication efforts go wrong. This reframing might enable health experts to detect cases earlier and to more effectively communicate with the public; it could also encourage a discourse about unusual disease events within communities.\n\n【9】Speaking from experience, public health experts advised communication specialists to consider using a case alert rumor book to provide alarm signals for quick response and follow-up response. In addition, a rumor book could serve as a record of the community explanations of infectious disease transmission and, as such, might help establish a starting point for community engagement. For instance, a common rumor was that Ebola is caused by witchcraft; the conventional response is to refer to Ebola as a virus. However, more useful would be to accept this alternative explanation and create recommendations consistent with community mind-set (e.g. don’t touch this person unprotected, but provide food \\[and prayers\\] as a token of empathy).\n\n【10】### Capacity Building: Avoid Blind Spots by Addressing the First Detectors\n\n【11】Cases typically appear before medical attention is sought and thus before cases can be understood as useful signals that trigger the activation of response. The initial detections of cases as alarm signals in a community represent the “blind spots” of capacity building: 1) capacity building on local level is often for health professionals, and 2) capacity building addresses a response mechanism (first responder training). Avoiding blind spots means shifting capacity building from reactive response to proactive detection involving the community.\n\n【12】Awareness raising and capacity building capacity at local levels must be continuous. For these purposes, a variety of training is now offered (World Health Organization and Centers for Disease Control and Prevention safety training). CORDS has designed an Intensified Preparedness Programme that offers short-term crisis response and longer term capacity building, consisting of training, advice, and background material for infectious disease management for affected and not-yet affected countries.\n\n【13】### Conclusions\n\n【14】The massive influx of international support addresses the foundational weaknesses of the public health systems. A key impairing factor, the engagement and communication with the community, is probably not yet addressed properly. The lessons learned in previous outbreaks identify major drivers of infection control as local realities and can advise the revision of the international response strategy with a focus on community engagement, communication, and capacity building. Effective community engagement during risk communication is a necessary and often underrated strategy to build trust and confidence for community health security and thereby global health security.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c09f12c5-a7fb-4968-ba0b-b7b9c4c108bb", "title": "Fatal Systemic Morbillivirus Infection in Bottlenose Dolphin, Canary Islands, Spain", "text": "【0】Fatal Systemic Morbillivirus Infection in Bottlenose Dolphin, Canary Islands, Spain\nSince the first morbillivirus outbreak affecting bottlenose dolphins ( _Tursiops truncatus_ ) along the mid-Atlantic Coast of the United States during the late 1980s , some mass die-off episodes affecting the global cetacean population have occurred . Three cetacean morbilliviruses have been identified: porpoise morbillivirus, isolated from harbor porpoises that died along the coast of Ireland ; dolphin morbillivirus (DMV), first identified in striped dolphins from the Mediterranean Sea ; and pilot whale morbillivirus, isolated from a long-finned pilot whale stranded in New Jersey, USA . Other members of the _Paramyxoviridae_ family that have affected various marine mammal populations worldwide are phocine distemper virus and canine distemper virus (CDV) . Since early July 2013, a widespread die-off of >500 bottlenose dolphins occurred along the US mid-Atlantic Coast that probably resulted from morbillivirus infection .\n\n【1】We report a unique morbillivirus that caused documented systemic infection in a bottlenose dolphin from the eastern North Atlantic Ocean. Although this case occurred nearly 9 years ago, the nucleotide sequence of a fragment of the morbillivirus phosphoprotein ( _P_ ) gene shows high homology with sequences recently reported for morbilliviruses from striped dolphins ( _Stenella coeruleoalba_ ) in the Mediterranean Sea. This information is of value because no other sequences of morbillivirus from bottlenose dolphins are available in GenBank, despite the seriousness of this fatal disease, which causes mass deaths of marine mammals worldwide.\n\n【2】### Case Report\n\n【3】On July 18, 2005, a juvenile female bottlenose dolphin was stranded alive in Arrieta, Lanzarote (Canary Islands, Spain). The animal was 250 cm long and in moderate body condition, according to anatomic parameters. The animal died shortly after stranding, and a complete standardized necropsy was performed within 6 hours after death. Required permission for the management of stranded cetaceans in the Canarian archipelago is issued by the environmental department of the Canary Islands’ government.\n\n【4】Tissues of all the major organs and lesions were collected and stored in neutral buffered 10% formalin fixative solution for histologic and immunohistochemical analyses. Fixed tissue samples were trimmed and then routinely processed, embedded in paraffin, sectioned at 5 μm, and stained with hematoxylin and eosin for examination by light microscopy. A monoclonal antibody against the nucleoprotein of CDV (MoAb CDV-NP, VMRD, Inc. Pullman, WA, USA) known to react with DMV was used as primary antiserum. Immunohistochemical analysis was performed on selected samples of brain, intestinal, lymphatic, pancreatic, pulmonary, renal, and splenic tissues, as described . Samples of lung, spleen, and brain were collected and held frozen at −80°C until processed for molecular virology.\n\n【5】On gross examination, the most remarkable findings were moderate to severe multiorgan parasitic infection, mainly cirripedes ( _Xenobalanus_ sp.) in the caudal fin; larval cestodes ( _Phyllobothrium_ sp. and _Monorygma grimaldi_ ) within the subcutaneous and peritoneum tissues; nematodes ( _Anisakis_ spp.), trematodes ( _Pholeter_ sp.), and taeniform cestodes in the stomachs and intestine, and trematodes ( _Nasitrema_ sp.) and nematodes ( _Crassicauda_ sp.) within both pterygoid sinuses; bilateral serous-suppurative and proliferative arthritis at the scapula-humeral junction; and generalized lymphadenomegalia. Within the brain; the leptomeninges were enlarged and fibrotic at the cortex area.\n\n【6】Histologically, the lesions were consistent with findings from the gross examination. Other histopathologic findings were multifocal nonsuppurative hepatitis, adrenalitis, and bronchointerstitial pneumonia. In lung, a multifocal suppurative bronchitis associated with larval and adult nematode parasites (morphologically identified as _Halocercus_ spp. and _Stenurus_ spp.) were observed. Alveolar septa were expanded by macrophages and lymphocytes admixed with scattered karyorrhectic debris. Alveoli were often lined by hyperplastic type II pneumocytes and filled with large numbers of multinucleated syncytial cells. The immunohistochemical study demonstrated morbilliviral antigen in bronchiolar epithelium, type 2 pneumocytes, and multinucleate (syncytial) cells from lung .\n\n【7】The syncytia also were found within the lymph nodes, pancreas, spleen, kidney, and intestine, and they were characterized by a moderate amount of eosinophilic cytoplasm and 2 to >20 nuclei, often containing weak stained eosinophilic inclusion bodies. Lymphocytolysis was remarkable within the spleen and multiple lymph nodes and were characterized by loss of cellular detail and accumulation of karyorrhectic and eosinophilic cellular debris. Syncytia from lymph nodes, spleen, pancreas, and kidney also were immunostained against CDV antibody. Severe nonsuppurative meningitis (with >20 layers of lymphohistiocytic cells), perineuritis, and encephalomyelitis were found within the nervous system. Multiple microhemorrhages were a common associated lesion. The brain showed severe inflammatory changes, but only a few neurons were immunopositive, and they were limited to some lymphohistiocytic cells surrounding vessels, glial, and endothelial cells. The epithelial tropism of the virus was demonstrated immunohistochemically within the lung, intestine, kidney, and pancreatic duct epithelium.\n\n【8】Molecular detection of cetacean morbillivirus was performed by a 1-step reverse transcription PCR of a 426-bp conserved region of the _P_ gene, as described . All the samples tested from this dolphin were reverse transcription PCR positive for morbillivirus. Sequences were the same in all positive samples from the animal. We conducted a BLAST  search to compare sequenced products with sequences described in GenBank for morbillivirus. The common sequence of 411-nt fragment of the _P_ gene showed a 99% homology with those sequences obtained in 2007 and 2011 from striped dolphins stranded along the coastline of the Mediterranean Sea.\n\n【9】We used MEGA 5.0 software  to construct maximum-likelihood phylogenetic trees. A bootstrap resampling (1,000 replicates) was used to assess the reliability of the trees .\n\n【10】### Conclusions\n\n【11】We describe a systemic morbillivirus infection in a bottlenose dolphin in the Canary Islands. A previous morbillivirus infection had been described in a bottlenose dolphin found dead along the Atlantic coast of Mauritania in 1998; the virus was exclusively detected by hybridization in the stomach tissue sample .\n\n【12】Other cases that illustrate the systemic and neurologic features of the disease have been described in bottlenose dolphins in the Mediterranean Sea , and the first reported case of a fatal morbillivirus infection in cetaceans within the Southern Hemisphere (southwestern Pacific Ocean) has been described . No sequences from these studies were provided to GenBank, although the phylogenetic analyses showed high homology with other DMV. The _P_ gene fragment of the virus obtained from the current study is molecularly almost identical to that reported in striped dolphins in the Mediterranean Sea during the last 5 years. This fact supports the hypothesis that transmission occurs between species, as it was reported in the 2006–2007 Mediterranean epizootic between pilot whales ( _Globicephala melas_ and _G. macrorhynchus_ ) and striped dolphins , and demonstrates that dolphin populations of the Mediterranean Sea and the Atlantic Ocean are in contact through the Gibraltar Straits. This particular point is critical to better understand the epidemiology and transmission of morbilliviruses between cetaceans and could be essential to clarifying the infection source of the die-off of bottlenose dolphins alson the East Coast of the United States .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b7e57e30-8589-4aa5-99b1-9115b2d82260", "title": "Novel Chlamydiaceae Disease in Captive Salamanders", "text": "【0】Novel Chlamydiaceae Disease in Captive Salamanders\n**To the Editor:** Although 2 major diseases of amphibians, chytridiomycosis and ranavirosis, have been relatively well studied, enigmatic amphibian disease and death not attributable to any of the known amphibian diseases frequently occur . We describe an apparently new disease in salamanders that is associated with a novel genus within the family _Chlamydiaceae_ .\n\n【1】The salamanders seen in our clinic belonged to 1 of the following species: _Salamandra corsica_ , the Corsican fire salamander (5 animals from 1 collection); _Neurergus crocatus_ , the yellow spotted newt (11 animals from 3 collections); or _N. strauchii_ , Strauch’s spotted newt (6 animals from 2 collections). All salamanders were captive bred; housed in breeding colonies in private collections in Elsloo and Eindhoven, the Netherlands, Munich, Germany, and Brugge, Belgium; and 1–3 years of age.\n\n【2】Disease was characterized by anorexia, lethargy, edema, and markedly abnormal gait. Mortality rate was 100%. Animals in these collections had no histories of disease. All animals were in good nutritional condition. Necropsy did not yield any macroscopic lesions. All animals had mild intestinal nematode or protozoan infections. Results of real-time PCRs for iridoviruses in liver and skin  or _Batrachochytrium dendrobatidis_ fungus of skin  were negative for all animals.\n\n【3】We placed liver suspensions from the dead salamanders on Columbia agar with 5% sheep blood and tryptic soy agar and then incubated the samples up to 14 days at 20°C. No consistent bacterial growth was observed. Histologic examination of 2 Corsican fire salamanders and 1 yellow spotted newt revealed hepatitis in 1 of the Corsican fire salamanders and the yellow spotted newt. Hepatitis was characterized by high numbers of melanomacrophages and a marked infiltration of granulocytic leukocytes. Immunohistochemical staining for chlamydia (IMAGEN Chlamydia; Oxoid, Basingstone, UK) showed cell-associated fluorescently stained aggregates in liver tissue, suggestive of Chlamydiales bacteria. Transmission electron microscopic examination of the liver of a yellow spotted newt revealed intracellular inclusions containing particles matching the morphology of reticulate or elementary bodies of _Chlamydiaceae_ .\n\n【4】A PCR  to detect the 16S rRNA of all Chlamydiales bacteria, performed on liver tissue samples from all animals, yielded positive results in all 5 Corsican fire salamanders; in 4/7, 1/3, and 1/1 yellow spotted newts; and in 4/5 and 1/1 Strauch’s spotted newts. For taxon identification, the 16S rRNA gene of the Chlamydiales bacteria was amplified and sequenced from the livers from 2 yellow spotted newts (1 from the collection in Elsloo, the Netherlands and 1 from the collection in Munich, Germany), 1 Strauch’s spotted newt, and 5 Corsican fire salamanders.\n\n【5】The sequences shared >90% nt identity with the 16S rRNA gene of _C. abortus_ B577  and therefore can be identified as a member of the family _Chlamydiaceae_ . The closest 16S rRNA similarity (92%) was observed with _C. psittaci_ strain CPX0308 (AB285329). The sequence obtained from all spotted newt species specimens was identical  but differed slightly (1%) from that obtained from the fire salamander species specimens . These sequence differences point to the existence of multiple strains with possible host adaptation.\n\n【6】We determined the phylogenetic position of the novel taxon, named _Candidatus_ Amphibiichlamydia salamandrae , identified by using neighbor-joining analysis with Kodon software (Applied Maths, Sint-Martens-Latem, Belgium). The novel Chlamydiales forms a distinct branch in the well-supported monophyletic clade with the genera _Chlamydia_ and _Candidatus_ Clavochlamydia salmonicola (family _Chlamydiaceae_ ) . Maximum parsimony and unweighted pair group with arithmetic mean analyses yielded cladograms with the same topology (results not shown). Previous reports of members of the family _Chlamydiaceae_ in amphibians concerned species occurring in other vertebrate taxa as well: _C. psittaci_ , _C. pneumoniae,_ _C. abortus_ , and _C. suis_ . To our knowledge, this member of the family _Chlamydiaceae_ has been seen in amphibians, but not in other vertebrate hosts. The 16S rRNA analysis showed this taxon to belong to a clade with _Candidatus_ Clavochlamydia salmonicola, a taxon found in fish. The phylogenetic position of the novel taxon in the family _Chlamydiaceae_ thus roughly reflects the phylogenetic relation between the host species, providing evidence for host–bacterium co-evolution in the family _Chlamydiaceae_ .\n\n【7】Although the results obtained are not conclusive with regard to the pathogenic potential of this novel genus and species of Chlamydiales, we were not able to attribute the clinical signs to any known disease. We therefore suggest that we discovered a novel bacterial taxon with possible considerable impact on amphibian health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "67863592-bde6-4ce0-a2de-6d22889b83be", "title": "Transmission of Schmallenberg Virus during Winter, Germany", "text": "【0】Transmission of Schmallenberg Virus during Winter, Germany\n**To the Editor:** Schmallenberg virus (SBV), an orthobunyavirus, emerged in northern Europe in 2011 . SBV infection causes transient fever, diarrhea, and a reduced milk yield in adult ruminants but, most notably, stillbirths and severe malformations in lambs and calves . Insect vectors play an essential role in transmission; the viral genome has been detected in various field-collected biting midges ( _Culicoides_ spp.) .\n\n【1】During autumn 2012 and winter 2012–2013, blood samples were taken at several times from individual sheep on a farm located in the German federal state of Mecklenburg–Western Pomerania. The farm is surrounded by agricultural fields and meadows. Approximately 1,000 ewes and their lambs, a dog, and some cats were kept on the farm; most of the animals are outdoors year-round. Only dams with \\> 2 lambs are housed in open stabling in December and January. The dung is regularly cleared away and stored ≈10 m from 1 of the stable entrances. Repellents or insecticides were not applied in the monitored period. Blood samples were taken in September 2012 and in January and February 2013 and analyzed by an SBV-specific real-time quantitative reverse transcription PCR (RT-qPCR)  and by an SBV antibody ELISA (ID Screen Schmallenberg virus Indirect; IDvet; Montpellier, France) by using the recommended cutoff of 50% relative optical density as compared with the positive control (sample-to-positive ratio \\[S/P\\]).\n\n【2】In September 2012, blood samples from 60 sheep tested negative by the SBV antibody ELISA. Moreover, fetal malformations of the brain, spinal cord, or skeletal muscle, which might have suggested a previous SBV-infection of the dam, were not observed during the lambing season in December 2012.\n\n【3】On January 10, 2013, blood samples were taken from 15 sheep that had not previously been tested; samples from all animals tested negative by ELISA. However, 4 sheep (S01–S04) tested positive by RT-qPCR (quantification cycle values: S01: 31.6, S02: 39.9, S03: 37.6, and S04: 34.9). Four weeks later, antibodies against SBV could be detected. Each of the PCR-positive blood samples was injected into 2 adult type I interferon receptor-knockout mice on a C57BL/6 genetic background. Both mice that had received blood samples of sheep S01 were seropositive after 3 weeks (S/P: 207.0 and 207.2), which demonstrates the presence of infectious virus in the inoculated blood. Assuming that viral RNA remains in the blood for just a few days, as reported after experimental infection with SBV , the sheep tested in this study had most likely been infected in early 2013. During this period, the lowest temperatures rose above 5°C for several consecutive days, with a maximum of ≈9°C . Within this brief interval, when the temperature was higher, some biting midges ( _Culicoides_ spp.) become active . Indeed, at the end of January, a single female biting midge (Obsoletus complex) was caught in a trap equipped with ultraviolet light; the midge tested negative by the SBV-specific RT-qPCR.\n\n【4】On January 23 and February 20, 2013, blood samples were taken from 90 sheep that had not previously been tested . A viral genome was not detected in any animal at any time. However, antibodies were detectable in 9 animals on the first sampling day. In 2 additional sheep, the S/P was in the inconclusive range; 1 of the animals tested positive after 4 weeks. In the remaining 79 sheep, no SBV antibodies could be detected; after 4 weeks, 76 sheep still tested negative by ELISA. However, the S/P of 1 sheep had increased to the inconclusive range, and 2 sheep were seropositive. Because antibodies may be detectable 10 days–3 weeks after experimental infection for the first time , the presumed period of infection was between mid-January and mid-February. At this time, the highest temperatures again rose above 6°C for a few days .\n\n【5】Although the within-herd seroprevalence was >90% in ewes after confirmed or suspected SBV infection in 2011 , in this study, conducted during the cold season, only 12 (13%) of 90 tested sheep were positive by ELISA. Three animals seroconverted between mid-January and mid-February. Thus, SBV transmission appears to be possible at a low level, most likely because of the low activity of the involved insect vectors.\n\n【6】In addition to the SBV cases found on the sheep holding in Mecklenburg–Western Pomerania, an additional 52 confirmed SBV cases (defined as virus detection by qRT-PCR or isolation in cell culture) in adult ruminants were reported to the German Animal Disease Reporting System from January 1 through February 20, 2013 . Most affected animal holdings were located in Bavaria, but cases were also reported from Thuringia, Saxony, Brandenburg, Mecklenburg–Western Pomerania, Hesse, and Lower Saxony. In conclusion, transmission of SBV by hematophagous insects seems possible, even during the winter in central Europe, if minimum temperatures rise above a certain threshold for several consecutive days.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "39d11f81-2bbc-49f6-9296-74c815655dc3", "title": "The Curious Case of the Cephalopod Parasites", "text": "【0】The Curious Case of the Cephalopod Parasites\nA. E. d’Audebard de baron de Férussac  and Alcide Dessalines d’Orbigny . Plate 24 from Natural, general and particular history of the acetabuiferous cephalopods living . Ink on paper. Public domain image from Biodiversity Heritage Library. Holding institution: Smithsonian Libraries, Washington, DC, USA.\n\n【1】This month’s cover illustration of an octopus comes from the book _Natural, General and Particular History of the Acetabuiferous Cephalopods Living_ . This voluminous study of cephalopods was completed by a pair of 19th century French naturalists, though not exactly as collaborators. A.E. d’Audebard de baron de Férussac wrote the introduction and the first 11 parts; Alcide Dessalines d’Orbigny revised and completed the book. Férussac, a professor of geography and statistics at the École d'état-major in Paris, is now chiefly recognized for his studies of molluscs. D’Orbigny, a professor of paleontology at the Paris Muséum National d’Histoire Naturelle, collected natural specimens from South America and corresponded with Charles Darwin.\n\n【2】Completed in the 1830s, this detailed illustration does not identify the species of octopus represented, though it is most likely a common octopus, which would have been 1–3 feet long and weighed 10–20 pounds. Nonetheless, it displays the animal’s characteristic features: bulbous head, wide-angled eyes that provide a panoramic view, 8 whip-like arms festooned with suckers, and mottled and flecked colors. One companion image resembling a multipronged compass protractor reveals the scale and reach of the straightened arms, and on first glance, a second drawing of the creature’s underside could pass for a splayed umbrella.\n\n【3】An undeserved reputation for ferocity and belligerence has been foisted on the octopus. In the first century ce , Pliny the Elder decried, “no animal is more savage in causing the death of a man in the water.” The air of mystery attached to these intelligent, curious creatures befits them, no doubt spurred by their physical appearance. Anthropologist Roland Burrage Dixon references a Hawaiian creation myth that describes a primordial ocean in which “swims the octopus, the lone survivor from an earlier world.”\n\n【4】Given their outward appearance, labeling these creatures as alien or otherworldly does not seem farfetched. Examining their anatomy does not dispel those notions. Their arms, which contain more neurons than their brains, collect and convey an array of sensory information and may even have distinct personalities. Of their three hearts, two move blood from their gills and the third circulates their blood. Their blue blood is tinted by the copper-transporting protein hemocyanin, which is more efficient than hemoglobin for transporting oxygen in frigid and oxygen-poor ocean water.\n\n【5】Octopuses are acknowledged for their adeptness at solving intricate problems and using tools, curiosity, and even mischief. Able to mimic colors and textures and to squeeze into astonishingly compact spaces, these cephalopods excel as both hunters and survivalists. Science writer Katherine Harmon Courage writes, “The boneless octopus must avoid becoming lunch for sharks, eels, fish and even killer whales. But not all of the organisms that feed on octopuses are such charismatic megafauna.”\n\n【6】Parasitic organisms occur in all animal species, are as diverse as their host species, and derive their sustenance, during at least part of their lifecycles, at the expense of their hosts. This ubiquitous template of coexistence has persisted and evolved and given rise to euryxenous parasites that infect a spectrum of unrelated hosts; stenoxenous parasites that prefer closely related hosts; and oioxenous parasites that limit themselves to single species of host. Among the latter two types is a phylum of highly specialized parasites known as Rhombozoa or Dicyemida, which dwell only in the kidneys of cephalopods.\n\n【7】Some of the estimated 250–300 known species of octopus are so similar that researchers differentiate them by examining those parasites. Courage writes, “These microorganisms are often unique not just to the octopus but also to a particular species of octopus. In fact, these very specific, kidney-dwelling species can even be used to tell one octopus species from another. (With such variable physical attributes, octopus specimens can be difficult to parse.)”\n\n【8】Parasites can cause a range of diseases in humans, domestic animals, and wildlife. Myriad parasitic infections range from asymptomatic to mild to severe to fatal. They are encountered by hosts of every species on every continent and in every body of water. A confluence of factors―including deforestation, urbanization, and development, international travel and tourism, contemporary agricultural practices, and displacement of wildlife, sanitation problems, and growing antimicrobial and insecticide resistance―encumbers efforts to control parasitic infections. Discerning the abundance, diversity, specialization, and history of parasitic organisms, such as the curious case of the finicky Rhombozoa, may yield information with potential to improve our understanding of other parasitic infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "955310fc-6460-4469-9762-022bcba6233b", "title": "Hepatitis C Antibodies among Blood Donors, Senegal, 2001", "text": "【0】Hepatitis C Antibodies among Blood Donors, Senegal, 2001\n**To the Editor:** Prevalence of chronic hepatitis C virus (HCV) among blood donors has been assessed in a few West African countries; most recent estimates range from 1.1% to 6.7% . A recent meta-analysis of studies, including a confirmation test, yielded an average prevalence of HCV infection of 3.0% . Until 2001, no systematic screening of HCV infection occurred among blood donors in Senegal, and blood donation legislation is still pending. We report an assessment of the proportion of blood donors from the Hôpital Principal de Dakar who had HCV antibodies in 2001.\n\n【1】Blood donors were all volunteers, recruited independently from the hospitalized patients and registered in a local donors association. We screened for risk factors for bloodborne infections in potential donors through a clinical examination and a confidential questionnaire. Persons with a history of jaundice or a risk behavior were excluded. Serum samples collected from blood donors from June to December 2001 were screened for HCV antibodies by a third-generation enzyme immunoassay (EIA) (HCV Murex 4.0; Abbott Laboratories, Abbott, IL). Confirmation was performed by a recombinant-immunoblot assay (INNO-LIA HCV Ab III update; \\[Innogenetics, Gent, Belgium\\]). HCV RNA was detected by a qualitative reverse transcription–polymerase chain reaction (Roche Amplicor HCV test \\[Hoffman-LaRoche, Basel, Switzerland\\]). Genotype was determined by the INNO-LiPA HCV II assay (Innogenetics). Presence of hepatitis B surface antigen (HbsAg) and alanine-aminotransferase (ALAT) level are routinely assessed, as well as HIV and human T-lymphotropic virus type l infection.\n\n【2】The age of the 1,081 donors ranged from 18 years to 61 years (mean 35.6 years), and 81% were men. First-time donors accounted for 31% and were younger than repeat donors (mean 30.5 years vs. 37.8 years; p < 10 –4  ). EIA HCV antibodies were found in 18 donors (1.6%). Immunoblot assay was positive for nine, yielding an overall prevalence of 0.8% (exact 95% confidence interval 0.4% to 1.5%). Eight of the nine were repeat donors, but the difference in prevalence compared with first-time donors did not reach statistical significance (1.1% vs. 0.3%). HCV-infected donors tended to be older than uninfected donors (mean 42.3 years vs. 35.5 years, median 46.7 years vs. 34.6 years, Mann-Whitney test p = 0.04), and the trend with age was significant (18–29 years 0.3%; 30–39 years 0.6%; 40–49 years 1.5%; \\> 50 years 1.8%; chi-square trend = 4.39; p = 0.03). ALAT levels of infected study participants were in the normal range (17–55 IU). One participant had an ALAT level above normal. Genotype 2ac has been identified on line immunoassay–positive samples (three samples not tested). HBsAg was detected in 13% of the new donors. No co-infection with HCV and hepatitis B virus was found.\n\n【3】The prevalence of HCV antibodies in blood donors in Dakar in 2001 appears to be one of the lowest in West Africa, close to published estimates for Mauritania and Benin (1.1% and 1.4%, respectively) and lower than in other West African countries such as Ghana or Guinea, where prevalence ranges from 2.8% to 6.7% . This finding is in keeping with results of a hospital case-control study on HCV infection and liver cirrhosis or cancer, conducted in 1995 in Dakar. While that study did not identify HCV infection in 73 controls, 2 of 73 case-patients (2.7%) had HCV antibodies . Conversely, high HCV prevalence was found in groups at risk: antibodies were present in 12 of 15 hemodialysis patients, and HCV RNA was found in 6 of the 12 HVC antibody-positive patients (genotype 2ac, the same as in our study); 7% of a cohort of 58 HIV-1 patients receiving highly active antiretroviral therapy had a positive HCV serologic result .\n\n【4】In the urban setting of Dakar, HCV infection seems still to be confined to groups at risk. The contribution of HCV to chronic liver diseases has not been yet demonstrated. Approximately 15,000 blood donations are annually made in Dakar. A systematic screening of HCV antibodies in blood donors could prevent, on average, 120 bloodborne HCV infections each year. Given these data and the price of EIA and LIA, the screening cost per HCV-positive sample identified, and infection subsequently averted, is approximately 200,300 CFA (U.S.$305). This estimate is low since it includes only the marginal cost of the reagent kits. This screening cost could be reduced by discarding blood units that test positive after only one enzyme-linked immunosorbent assay (156,000 CFA or U.S.$237), at the price of nearly 3% of blood units wrongly discarded. France has demonstrated that this strategy has the best cost-effectiveness ratio, as long as the prevalence remains below 8% . This cost compares favorably with the cost per HIV infection averted through improvement of blood safety (range U.S.$20–U.S.$1,000), assessed in some highly HIV-prevalent southern African countries (Tanzania, Zambia, Zimbabwe) . The HCV-positive discarded blood units will be added to the blood units testing positive for hepatitis B surface (13%), HIV, and HTLV, which accounted for nearly one third of all donations in 2001. These findings argue in favor of maintaining a roster of regular, seronegative donors to save numbers of blood units.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ee10ffff-e3c4-4fdb-a9e0-5abed464e5cc", "title": "Fatal Exacerbations of Systemic Capillary Leak Syndrome Complicating Coronavirus Disease", "text": "【0】Fatal Exacerbations of Systemic Capillary Leak Syndrome Complicating Coronavirus Disease\nSystemic capillary leak syndrome (SCLS), also known as Clarkson disease, is a rare disease of unknown etiology that most commonly develops in adults 50–70 years of age . Since SCLS was first characterized in 1960, <500 cases have been described in the medical literature. The current prevalence of SCLS is estimated to be <250 cases worldwide , although the disease is likely underdiagnosed.\n\n【1】SCLS is diagnosed clinically on the basis of a characteristic symptomatic triad of hypotension, hemoconcentration (elevated hemoglobin or hematocrit), and serum hypoalbuminemia resulting from fluid extravasation. Patients with SCLS experience transient and reversible episodes of plasma leakage into peripheral tissues, which lead to the acute onset of hypotensive shock and the development of anasarca after intravenous (IV) fluid resuscitation. Severe SCLS flares commonly result in multisystem organ failure and peripheral compartment syndromes . Between episodes, patients are typically asymptomatic.\n\n【2】Minor infections, typically of the upper respiratory tract, are common triggers for SCLS, although an infection-related prodrome is identified in only 44%–65% of cases . Although >80% of SCLS patients have a monoclonal gammopathy of unknown significance (MGUS), the role of this finding in disease pathogenesis remains unclear; the absence of MGUS does not exclude a diagnosis of SCLS . Interventions for acute SCLS episodes are limited to supportive measures, but monthly prophylaxis with high dose (1–2 g/kg patient weight) IV immunoglobulin (IVIg) prevents attacks among >90% of patients and provides them with a statistically significant survival advantage compared with those patients who were not treated with IVIg . Here we report the cases of 2 patients who died of severe SCLS soon after seeking treatment for mild-to-moderate symptoms of COVID-19; neither of them had been receiving IVIg prophylaxis beforehand.\n\n【3】### Methods\n\n【4】Patients were referred to the National Institutes of Health (NIH) for evaluation of suspected SCLS. Where applicable, patients provided written informed consent to participate in a natural history protocol (09-I-0184) approved by the NIH institutional review board. We followed the CARE guidelines for writing medical case reports in the preparation of this manuscript.\n\n【5】##### Case Reports\n\n【6】##### Case\n\n【7】A 59-year-old woman with a history of hypertension but no known history of SCLS or prior episodes of peripheral edema was admitted to a hospital in January 2021 with a 6-day history of cough, shortness of breath, and lower extremity pain. She was hypotensive, with a blood pressure of 96/70 mm Hg, but her initial physical exam was otherwise unremarkable and included normal results for a pulmonary examination. Laboratory examination revealed hemoconcentration (hemoglobin 17.1 g/dL, hematocrit of 52%) and a lactic acid level of 3.7 mmol/L. Despite resuscitation with 5.5 L Ringer’s lactate and 2.5 L normal saline IV fluids over the first 3 days of admission, serum lactate levels remained elevated at 5.7 mmol/L on day 4 of hospitalization . Lower extremity pain worsened, and anasarca without hypoxemia developed. SARS-CoV-2 PCR testing of a nasal swab specimen taken at the time of admission returned positive results; daily treatment with 6 mg dexamethasone and 40 mg enoxaparin was initiated. Remdesivir was not administered because of a developing acute kidney injury. The patient’s lung examination results remained unremarkable, but she became intermittently hypoxemic (SpO 2  of 88% on 2L nasal cannula), requiring ongoing nasal cannula support. A chest computed tomography revealed bilateral scattered ground glass opacities consistent with mild-to-moderate SARS-CoV-2 infection . A transthoracic echocardiogram performed on day 3 of hospitalization showed a normal left ventricular ejection fraction (70%–75%), no left ventricular dilation or geometric changes, and no wall motion abnormalities. Other laboratory abnormalities included increased creatinine phosphokinase levels, peaking at 15,094 units/L; elevated lactate at 6.3 mmol/L; and hypoalbuminemia at 2.4 g/dL, which raised concerns for SCLS.\n\n【8】The patient was transferred to the intensive care unit for further monitoring on day 4 of hospitalization. Pain increased in her extremities and tense anasarca developed; however, because compartment pressures were 18 mm Hg in her right arm and 15 mm Hg in her left arm, she did not meet criteria for a diagnosis of compartment syndrome. Apart from edema and tachypnea while on 2 L nasal cannula, her cardiopulmonary examination was otherwise normal. On hospitalization day 5, she suffered a cardiac arrest; spontaneous circulation returned after 2 rounds of cardiopulmonary resuscitation. She was intubated at the time of cardiac arrest and empirically started on vancomycin and cefepime. Because of concern for microvascular thrombi in the setting of SARS-CoV-2 infection, an argatroban infusion was started. Although the patient had remained afebrile and hemodynamically stable up to this point, shock rapidly developed, and she required vasopressor support with norepinephrine, vasopressin, epinephrine, and stress-doses of hydrocortisone. Continuous renal replacement therapy was initiated for oliguric renal failure.\n\n【9】The next day, edema in the extremities intensified, and creatinine phosphokinase levels increased further to >45,000 units/L, but compartment pressures were 17 mm Hg in her right arm and 16 mm Hg in her left arm, suggestive of rhabdomyolysis without compartment syndrome. No MGUS was detected. Serum SARS-CoV-2 IgG was not detected, prompting treatment with convalescent plasma. Given the patient’s grave condition and lack of proven interventions for acute SCLS, empiric treatments were also administered, including IVIg; methylene blue, an agent that suppresses downstream effects of nitric oxide ; and icatibant, a bradykinin receptor antagonist used to treat vascular leakage associated with hantavirus infection . Bradycardia developed and ultimately required transvenous pacing. Within hours, her hemoglobin decreased to 6.8 g/dL although there was no obvious source of hemorrhage. Because she was anticoagulated with argatroban and had a prolonged activated partial thromboplastin time in the setting of acute liver failure, the patient was transfused with packed erythrocytes. She suffered cardiac arrest, and ventricular fibrillation deteriorated into pulseless electrical activity. Transthoracic echocardiography performed during ACLS revealed no pericardial effusion. The patient received additional units of erythrocytes, IV fluids, fresh frozen plasma, and prothrombin complex concentrate during the cardiac arrest. However, spontaneous circulation was not restored, and the patient died on day 6 of her hospitalization.\n\n【10】##### Case\n\n【11】A 36-year-old man with no notable medical history first sought treatment in 2015 for transient hypotension and severe bilateral lower-extremity edema; laboratory testing showed hemoconcentration (hemoglobin 20.2 g/dL, hemocrit 59.4%) and a serum hypoalbuminemia level of 2.2 g/dL after several days of fevers and upper respiratory symptoms. SCLS was diagnosed on the basis of characteristic clinical presentation and an IgG lambda MGUS. His course was complicated by acute kidney injury and compartment syndromes in both legs, which required bilateral fasciotomies. He also had deep vein thromboses in the right internal jugular and left cephalic veins. He was treated transiently with anticoagulants, but a full evaluation for hypercoagulability was negative. Muscle biopsies taken at the time of fasciotomies provided no evidence of inflammatory myositis. All symptoms resolved before he was discharged from the hospital, although a bilateral sensorimotor neuropathy developed, presumably as a residual effect of compartment syndrome.\n\n【12】The patient was treated with IVIg (2 g/kg) within 24 hours of the 2015 hospitalization and monthly thereafter with no recurrence of his SCLS-related symptoms. In November 2016, IVIg prophylaxis was discontinued at the patient’s request. In 2017 and 2018, the patient experienced 2 episodes of bilateral lower extremity swelling, in both cases after several days of upper respiratory symptoms. Blood pressure and laboratory tests were normal at the time of these episodes, and swelling resolved without further treatment.\n\n【13】In February 2020, the patient sought treatment for a several-day history of fevers (40°C) and productive cough. He was noted to be hypoxemic (SpO 2  of 87% on room air). _Mycoplasma_ pneumonia was diagnosed on the basis of chest radiographic evidence of lung infiltrates and positive _Mycoplasma_ serologic testing. He was hospitalized and treated with IV antimicrobial drugs and fluids (10.5 L total). Although the fevers and respiratory symptoms resolved, bilateral leg swelling and serum hypoalbuminemia developed, prompting treatment with 1 dose of IVIg (2 g/kg) for empirically presumed SCLS-related edema in the absence of any other proven treatment options. He ultimately recovered and experienced no residual symptoms.\n\n【14】In January 2021, the patient was transported to the emergency department because of disorientation after several days of upper respiratory symptoms, fever (40.6°C), and back pain. Upon arrival, he was hypotensive, with a blood pressure of 94/20 mm Hg, and tachycardic at 140 beats/min. Upon initial examination, he showed no signs of apparent dyspnea (respiratory rate of 19 breaths/min) or respiratory distress (SpO 2  of 99% on room air). A PCR nasal swab test for SARS-CoV-2 performed on the day before he sought treatment was reported by the family to be positive. Laboratory tests taken at the time of hospital admission revealed severe hemoconcentration (hemoglobin >25 g/dL, 75% hemocrit), leukocytosis (leukocyte count 43.3K/μL blood), and lactic acidosis (lactate level 9.2 mmol/L). Other laboratory values included brain natriuretic peptide at 12 pg/mL, troponin at <0.02 ng/mL, and a fingerstick glucose level of 197 mg/dL. There was no ST elevation suggesting myocardial infarction or dysrhythmia on his electrocardiogram.\n\n【15】In the emergency department, his condition deteriorated rapidly. His blood pressure became unobtainable manually, and he exhibited an Sp0 2  of 73% on a 15-L nonrebreather mask. A bedside echocardiogram revealed a flat inferior vena cava, intact bilateral ventricular function, and pericardial effusion with no evidence of right heart strain or tamponade. He was intubated emergently but ultimately experienced cardiac arrest with pulseless electrical activity. Despite aggressive cardiopulmonary resuscitation and pharmacological interventions including boluses of epinephrine, bicarbonate, calcium, and magnesium, refractory ventricular tachycardia developed, followed by Torsade de Pointes, and he died shortly thereafter.\n\n【16】### Discussion\n\n【17】We report 2 cases of SCLS associated with mild-to-moderate COVID-19 infection. Case-patient 1 exhibited the clinical diagnostic triad for SCLS: hypotension, hemoconcentration, and hypoalbuminemia. Case-patient 2 carried a prior diagnosis of SCLS. These findings suggest that patients with SCLS may be at high risk for exacerbations or even death if they contract COVID-19.\n\n【18】Information from our studies complements the findings of several recent case reports of severe SCLS attacks associated with mild-to-moderate SARS-CoV-2 infection . In each of the cases in those studies, patients exhibited all the hallmarks of severe SCLS exacerbations after experiencing mild-to-moderate symptoms of COVID-19; 2 of 3 patients died from SCLS-related complications.\n\n【19】Because COVID-19 had not previously been associated with secondary capillary leak syndrome and there were no other obvious triggers for the episode in case-patient 1 in our study, it is highly likely that she carried latent SCLS even though MGUS was not detected at the time of the episode. Although MGUS is an important clue, it is not detected in all patients with SCLS. In the most recent comprehensive reviews of the literature, which included 290 cases reported during 1960–2016, MGUS was detected in only ≈75% of case-patients . Although MGUS was detected in 34 (91%) of 37 patients in a 2017 study , the authors emphasized that “the three monoclonal gammopathy-negative patients had typical severe SCLS flares.” Several studies failed to establish any functional role for these monoclonal paraproteins in disease pathogenesis . Finally, immunofixation may be negative during acute SCLS flares because of IgG extravasation and transient hypogammaglobulinemia .\n\n【20】Several unique clinical and laboratory features of SCLS can be used to differentiate disease flares from the sequelae of severe COVID-19 . Most notably, edema of the trunk and extremities is a prominent feature of acute SCLS and can lead to the development of compartment syndromes. Edema in COVID-19 infection is typically peripheral and frequently confined to the fingers and toes in association with chilblains, painful erythematous lesions . Pulmonary edema is a feature of SCLS rarely noted at initial observation but is a frequent characteristic of acute COVID-19–associated acute respiratory distress syndrome . Furthermore, both patients exhibited severe hemoconcentration despite aggressive fluid resuscitation. Patients who are critically ill with COVID-19 typically exhibit anemia, which is a predictor of a poor clinical outcome . Finally, although mildly decreased serum albumin levels of ≈3 g/dL have been reported in patients with severe COVID-19 , hypoalbuminemia is typically much more severe in SCLS flares, with an albumin level usually <2 g/dL.\n\n【21】Of note, SCLS had not previously been diagnosed in case-patient 1, and neither patient was receiving IVIg prophylaxis at the time of COVID-19 infection. Fortunately, we observed no COVID-19–associated SCLS flares in any of the >70 patients in the study cohort who were receiving IVIg prophylaxis (range 0.75–2.00 g/kg/mo). However, a previously published report  documented the case of a patient whose disease had been well-controlled by IVIg prophylaxis (0.5 g/kg/mo) but who died of refractory SCLS soon after COVID-19 infection. Nonetheless, in agreement with the authors of that study, we strongly recommend that SCLS patients receive IVIg prophylaxis indefinitely, at the highest recommended dose (2 g/kg/month), until the COVID-19 pandemic is under better control. Because no treatments for acute SCLS flares, including IVIg, have been proven effective, interventions are limited to supportive measures such as IV fluids and albumin, vasopressors, renal replacement therapy, and intubation. However, as noted in recent surveys of critically ill SCLS patients , fluid administration must be limited to avoid development of compartment syndromes and limb ischemia. Although case-patient 1 received several empiric treatments, including IVIg, icatibant, and methylene blue, the treatments appeared to have had no effect on her intermediate clinical outcomes.\n\n【22】Although the genetic basis of SCLS is not well understood , our work has provided evidence that patients with SCLS experience intrinsically exaggerated endothelial barrier dysfunction in response to otherwise mundane proinflammatory mediators . Severe COVID-19 and acute SCLS are both characterized by transient increases in the levels of proinflammatory cytokines in circulation; some of these cytokines, including C-X-C motif chemokine ligand 10, C-C motif chemokine ligand 2 and 3, interleukin -6, and tumor necrosis factor–α , directly provoke endothelial barrier disruption . These results suggest that the cytokine storm associated with mild-to-moderate COVID-19 may lead to an SCLS flare. Alternatively, as suggested elsewhere , it is also possible that SARS-CoV-2 may be directly toxic to endothelial cells. This hypothesis suggests that viral factors synergize with host-intrinsic mechanisms to provoke severe SCLS flares. Endothelial cell infection, diffuse inflammatory endothelitis, and microvascular thrombosis are common in COVID-19, although these responses are typically associated with prominent lung involvement . Because neither of these patients exhibited prominent pulmonary abnormalities, these mechanisms may not be substantial components of COVID-19–associated SCLS.\n\n【23】Further characterization of the immune and inflammatory responses to SARS-CoV-2 will be needed to elucidate its effects on SCLS pathophysiology at the molecular level. However, clinicians should be aware that patients carrying a diagnosis of SCLS or another relapsing-remitting and inflammation-related disease (e.g. autoimmune or autoinflammatory rheumatological diseases) may be at increased risk for severe disease and require increased vigilance for this rare but potentially fatal potential complication of COVID-19 as the pandemic continues.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fd62c6d4-b031-46d1-87b8-d5689324fb8b", "title": "Nonpharmaceutical Measures for Pandemic Influenza in Nonhealthcare Settings—International Travel-Related Measures", "text": "【0】Nonpharmaceutical Measures for Pandemic Influenza in Nonhealthcare Settings—International Travel-Related Measures\nFrom time to time, novel influenza A virus strains emerge and cause global influenza pandemics . Pandemics occurred 3 times in the 20th century and 1 time so far in the 21st century . The recognition that influenza pandemics can have substantial social and economic effects in addition to the impact on public health, along with the emergence of highly pathogenic strains of avian influenza virus in the past 20 years, has stimulated greater attention in preparing for future influenza pandemics . Given the delays in the availability of specific vaccines and limited supplies of antiviral drugs, nonpharmaceutical interventions (NPIs) form a major part of pandemic plans .\n\n【1】A range of NPIs can be applied at international, national, and local levels, with the objectives of delaying the arrival of infected persons, slowing the spread of infection, delaying the epidemic peak, and reducing the size of the peak . This article focuses on the use of measures related to international travel, including entry and exit screening of travelers for infection, travel restrictions, and border closures . We aimed to review the evidence base assessing the effectiveness of these travel-related NPIs against pandemic influenza and to identify the barriers to implementation of these interventions.\n\n【2】### Methods and Results\n\n【3】We searched for literature reporting or estimating the effectiveness of NPIs related to international travel and movement, including entry and exit screening travelers, travel restrictions, and border closures on pandemic or interpandemic influenza. We conducted literature searches on PubMed, Medline, Embase, and Cochrane Library for peer-reviewed articles published from January 1, 1946, through April 28, 2019. The search terms used were identified from relevant systematic reviews and research reports . We collected additional studies from secondary references from included studies or other relevant searches. Articles were eligible for inclusion if they reported or estimated the effectiveness of international travel–related NPIs for pandemic influenza using quantitative indicators such as delaying the introduction of infection, delaying the epidemic peak, or reducing the size of the peak. We excluded articles if they did not investigate the quantitative effectiveness of international travel–related NPIs or were editorials, reviews, or commentaries without primary data. Furthermore, we restricted articles to those published in English. Two independent reviewers (S.R. and H.G.) screened titles and abstracts and assessed full-text articles for eligibility. A third reviewer (B.J.C.) adjudicated any disagreements between the 2 reviewers.\n\n【4】We extracted the information on the effectiveness of NPIs from included studies by using a structured data-extraction form. Information of interest included the study setting, specific measures implemented, timing of intervention implementation, study results regarding effectiveness indicators, and potential barriers to implementation. The assessment of quality of evidence considered study design and assigned generally higher quality to randomized trials, lower quality to observational studies, and lowest quality to simulation studies. We provide full search terms, search strategies, selection of articles, and summaries of the selected articles .\n\n【5】##### Screening Travelers for Infection\n\n【6】We identified 4 relevant studies that considered the effect of screening on influenza transmission, including 2 epidemiologic studies from the 2009 pandemic  and 2 simulation studies . The epidemiologic studies estimated that entry screening delayed the arrival of influenza A(H1N1)pdm09 virus to previously unaffected areas by an average of 7–12 days  and delayed the epidemic in China by 4 days by reducing imported cases by 37% from border entry screening . The simulation studies predicted that entry screening would delay the arrival of infection into a country by a few days or 1–2 weeks at most . We did not identify any studies on exit screening; in the 2009 influenza pandemic, exit screening was not implemented by Mexico , nor by most other countries.\n\n【7】We did not systematically review studies of the technical performance of various screening tools (e.g. screening case definitions and thermal scanners) but identified in an informal search 4 studies that discussed the challenges of screening travelers for infection, which include limited screening sensitivity , an incubation period of 1–7 days for influenza A(H1N1)pdm09 virus (meaning some infected travelers might not show symptoms until after arrival at their destination) , limited local capacity of influenza surveillance , and limited public health resources, such as laboratory capacity and funding .\n\n【8】Screening inbound travelers for infection is a very visible public health intervention and can reduce the number of infectious persons entering the country . Infrared thermometers are currently used in many ports of entry in Asia because of the instantaneous and noninvasive nature of their use. Several simulation studies  included in this review estimated that this intervention helped to delay the introduction of infected persons. However, the sensitivity of screening travelers has been largely reliant on the sensitivity of detection of fever. Epidemiologic studies  conducted during the 2009 influenza pandemic demonstrated the low detection rate of entry screening that used the infrared thermal scanner and health declaration form at the airport; the sensitivity of screening travelers for infection was 5.8% in New Zealand and 6.6% in Japan. In addition to the lack of sensitivity for detecting febrile travelers (e.g. some travelers with febrile illness might take antipyretic medicine and evade detection), some infected travelers might travel during the incubation period, which is typically 1–2 days, and thus would not be identified as infected at departure or arrival . Once infection begins spreading in a local community, identifying additional inbound travelers with infection will do little to limit local spread. In addition, entry screening consumes considerable public health resources, including trained staff, screening devices, and laboratory resources, and thus might not be justifiable .\n\n【9】##### Travel Restrictions\n\n【10】We identified 1 epidemiologic study and 9 simulation studies that estimated or predicted the effectiveness of international travel restrictions  . An epidemiologic study estimated that the peak in the number of influenza-associated deaths was delayed by 2 weeks when international flight volume was reduced by 27% . Simulation studies predicted that 90%–99% of travel restrictions could delay international spread of cases by 2–19 weeks , delay the importation of the first case-patients by 1–8 weeks , and delay the epidemic peak by 1–12 weeks .\n\n【11】A simulation study predicted that selectively restricting the travel of children could delay the spread of infection by 35 days (R 0  \\= 1.2–2.0) , and another simulation study assessing the probability of escaping 1918–19 influenza pandemic among 17 Pacific Island countries and territories estimated that 4–5 countries avoided influenza pandemic (R 0  \\= 1.5–3.0) by strict limitation (79% or 99% restriction) of incoming travelers . Three studies explored the barriers to travel restrictions, which included the threat of economic loss  and lack of compliance among the public .\n\n【12】Because the volume of transportation is associated with the spread of influenza , travel restrictions have been considered as a measure to reduce international spread . Although previous expert survey and reviews suggested that travel restrictions are less likely to be effective , international travel restrictions are still included in some national pandemic plans . Several of the studies we reviewed  predicted that international travel restrictions might delay the importation of new infected persons from other affected areas, slow the international spread of the epidemic, and delay the epidemic peak . However, simulation studies estimated that travel restrictions after 5 months of the international arrival of the first infected persons would not be effective  and that only strict travel restriction was likely to be effective ; thus, the time of implementation of this measure should be considered with strict travel restrictions at the early stage of a pandemic. Some barriers exist to implementation of travel restrictions against pandemic influenza, most notably the potential economic consequences of restricting business travelers, as well as legal and ethical issues regarding mobility restrictions , discrimination of persons from influenza-affected area , and lack of public compliance.\n\n【13】##### Border Closures\n\n【14】One study investigated the effectiveness of border closures in 11 South Pacific Island jurisdictions during the 1918–19 influenza pandemic. We identified 4 islands where strict border control, including 5–7 days of maritime quarantine, substantially delayed the importation of influenza from 3 to 30 months and reduced the mortality rate compared with the other islands that had not implemented border control .\n\n【15】Because travel can drive cross-border transmission of infectious diseases, complete border closure could in theory prevent or delay the spread of influenza or its introduction in previously unaffected countries . However, in practice, complete border closure is likely to be unfeasible, even on isolated islands, because of the need to import food and medical supplies , and would result in substantial economic and social disruption .\n\n【16】### Discussion\n\n【17】We reviewed the effectiveness of each international travel–related NPI and the barriers to its implementation to provide scientific evidence to public health authorities. Our review found that the effect of screening travelers on entry to a country or region is very limited and unlikely to be a rational use of resources. However, this intervention has a potential role to inform travelers about the risk for infection and provide travel advice on avoiding travel to certain regions after departure or how to seek treatment after arrival . Furthermore, such screening can be seen by policy makers and politicians as a visible public health measure to help assure the public that action is being taken .\n\n【18】Our review identified the potential threat of economic consequences as a major barrier to implementation of travel restrictions. A simulation study demonstrated that children-selective travel restriction during a pandemic is less likely to affect economic impact compared with nonselective travel restrictions . A more structured epidemiologic study is needed to examine the cost and benefit of travel restriction by different risk groups of influenza transmission. A previous study demonstrated that successful border closure for 6 months in an island country provided a net societal benefit of USD 7.3 billion . However, this extreme measure is unlikely to be implemented unless required by national law in extraordinary circumstances during a very severe pandemic. The literature on border closure included in our review was based on the historical scenario of the 1918–19 influenza pandemic in isolated islands; this research might have limited relevance given the current and ever increasing levels of globalization.\n\n【19】Although international travel–related NPIs are not likely to be able to prevent importation of pandemic influenza to a country or region, NPIs implemented at the early phase might delay the start of a local epidemic by a few days or weeks , which is important if such delay can contribute to reducing the effect of the epidemic (e.g. by buying time to prepare healthcare providers and the public before the arrival of the epidemic, to plan and coordinate social distancing measures, and to purchase additional pharmaceuticals such as antiviral drugs or vaccines) . Once an epidemic has started, travel restrictions might also be used to delay the peak of the epidemic in an isolated location where heavy seeding by incoming infected persons could accelerate local transmission. International Health Regulations could play a role in decisions on whether to implement certain international measures .\n\n【20】We identified several knowledge gaps that could be filled by further research. Most fundamentally, information is still lacking on some aspects of the basic epidemiology of influenza, including the dynamics of person-to-person transmission (e.g. Can a person be infectious before the onset of symptoms? Can transmission occur from an asymptomatic or pauci-symptomatic case-patient? What fraction of infections are asymptomatic?). In terms of specific research on the effectiveness of travel-related NPIs, it is difficult to envisage how intervention studies could be done, but epidemiologic studies could be planned in advance of influenza pandemics or perhaps severe influenza epidemics. Studies could answer questions such as how many infections are imported from overseas or whether travel advisories might encourage infected persons not to travel.\n\n【21】Our review needs to be interpreted in light of some limitations. First, although international travel or trade of infected animals might have a role in the international spread of influenza, the study that assessed the movement restriction of animals was not included in this review. Second, mathematical models are useful tools for investigating the advantages and disadvantages of different interventions, but the results often depend on key modeling assumptions that are difficult to verify . The assessment of the quality of evidence was considered weak overall, given that most of the epidemiologic studies included in our review were ecologic studies. Third, only a few studies on the ethical and economic considerations regarding travel-related measures during influenza epidemics and pandemics were available .\n\n【22】Many countries continue to update their influenza pandemic plans on the basis of the latest available evidence. We found that international travel–related NPIs could delay the introduction of influenza and delay the start of local transmission; however, limited evidence exists to inform the use of these NPIs for controlling pandemic influenza. The evidence that we identified in our review does not support entry screening as an efficient or effective measure, and travel restrictions and border closures are likely to be too disruptive to consider. Additional prospective research on the effectiveness of travel-related NPIs would be valuable to support evidence-based decisions for future influenza pandemics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b3dd201e-bb9f-4516-8f21-357b1b3d9158", "title": "Parallels and Mutual Lessons in Tuberculosis and COVID-19 Transmission, Prevention, and Control", "text": "【0】Parallels and Mutual Lessons in Tuberculosis and COVID-19 Transmission, Prevention, and Control\nIn addition to having devastating effects on the economies of the world, the pandemic of coronavirus disease (COVID-19) itself and the responses entailed in containment and mitigation efforts could have disastrous consequences for existing public health programs, with the impacts being most pronounced in high-burden, low-income settings . Modeling of the impact of the COVID-19 pandemic conducted by Imperial College London (London, UK) suggests that in high-burden settings, disease-related deaths over 5 years might be increased by up to 10% for HIV, 20% for TB, and 36% for malaria .\n\n【1】To minimize the adverse consequences of COVID-19 on overall public health services, synergies between COVID-19 response and traditional public health programs should be sought and the lessons and resources developed in any of the programs should be used for the benefit of the others. In this regard, approaches to TB control might hold lessons for the public health response to COVID-19 and vice-versa.\n\n【2】### Synergies and Commonalities for COVID-19 and TB\n\n【3】Several commonalities exist between COVID-19 and TB, most notably transmission of their etiologic agents, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and _Mycobacterium tuberculosis_ . Both pathogens are transmitted through secretions from the respiratory tract . Moreover, protecting healthcare workers and other susceptible patients, and contact identification and evaluation are key components of the public health response to both infections. An understanding of the routes of and factors influencing transmission is necessary to develop effective and efficient measures to control the diseases. For TB, many years of clinical and experimental studies have provided a wealth of information on which to base contact identification, prioritization, and evaluation . Investigations of TB outbreaks have been especially informative . Not surprisingly, this level of understanding of SARS-CoV-2 transmission does not exist, and the relative contributions to transmission of large respiratory droplets, fomites, and aerosols remain controversial . Notably, transmission of both pathogens has been associated with superspreader events .\n\n【4】The clinical manifestations of COVID-19 were initially described as mainly involving the respiratory tract, with cough as a predominant symptom along with fever, but knowledge of its full natural history, with both immediate and potential long-term consequences, is still increasing rapidly . Both the degree of infectiousness and the severity of SARS-CoV-2 infection dictate rapid and effective implementation of healthcare facility infection prevention and control to minimize transmission. These measures include administrative, engineering, and personal measures (i.e. personal protective equipment) and community-based public health activities, even without strong empirical evidence on which to base these interventions.\n\n【5】### Seeking COVID-19 Mitigation and Control Strategies\n\n【6】Unquestionably, the package of community-based mitigation measures put into place for the current pandemic has had a major effect in reducing cases and deaths, as shown by Hsiang et al. However, uncertainties remain concerning the most effective individual or combinations of measures. These uncertainties preclude the ability to readily identify more targeted and efficient control strategies. Thus, an urgent need exists for a more detailed understanding of SARS-CoV-2 transmission routes and patterns.\n\n【7】Of particular importance is the implementation of monitoring and rapid case identification as current mitigation measures are relaxed. General agreement exists that rapid case identification through PCR-based testing quickly followed by contact identification and evaluation (generally called contact tracing in the context of COVID-19) is the key strategy in reducing transmission in settings where the epidemic curve is flattened or declining . Earlier in the pandemic, after the spring 2020 surge subsided, this approach, which closely resembles strategies used for TB, was being scaled up and implemented rapidly. The core actions involve identifying persons with the disease (index case-patients) and identifying and evaluating persons exposed to the index case-patient (contacts) to find additional cases and offer contacts preventive interventions. However, rapid increases in cases in late fall and winter 2020 made contact tracing impractical, simply because of volume. Now, as the pandemic wanes, a trend that we hope will continue, contract tracing is again becoming feasible.\n\n【8】### Value of Contact Identification and Evaluation\n\n【9】Contact identification and evaluation have been key components of TB-control measures in most low TB–incidence countries for at least the past 75 years, and a strong scientific basis exists for most, but not all, elements of this activity . Although the same information and approaches apply in generally resource-poor, high TB–incidence countries and although international guidelines exist, implementation of routine contact investigations has been very limited . In the setting of TB, effective contact investigations have addressed stigma, community engagement, training of interviewers, and use of specific operational guidelines . These same elements will likely prove crucial to the effectiveness of COVID-19 contact tracing.\n\n【10】At least 3 important differences exist between factors that should be considered when engaging in contact identification and evaluation for COVID-19 compared with TB. First, because of the short interval between exposure and disease onset, estimated to be a median of 4.1 days for COVID-19, the timeframe for contact identification and evaluation is much shorter than for TB . In addition, infection with _M. tuberculosis_ in immunocompetent hosts most commonly results in latent infection, which can last decades and in most cases never progresses to active TB disease. Second, persons with COVID-19 are most infectious in the immediate presymptomatic and early symptomatic phases, when the viral titers are at their peak, again indicating the need for speed in the contact process for maximal effectiveness . Third, SARS-CoV-2 clearly is transmitted from person to person predominantly through respiratory secretions that may be inhaled, settling on the mucosal lining of large airways, or be self-inoculated onto nasal mucosa or into the eyes . Unlike TB, the droplets with the SARS-CoV-2 viral cargo might also contaminate and persist on surfaces, although the role played by surface or fomite transmission is not well-quantified . However, increasing controversies and concerns exist as to the relative contribution of aerosols to overall transmission .\n\n【11】### The Role of Droplet Nuclei and Acquisition of Infection\n\n【12】_M. tuberculosis_ is transmitted nearly exclusively by aerosolized droplet nuclei, particles <5 µm in aerodynamic diameter . Large droplets per se are not effective vehicles for transmission of _M. tuberculosis_ ; however, as the water content of large droplets evaporates, droplet nuclei are formed. The closeness and duration of exposure to a person with infectious TB, as well as the ventilation of the space in which the exposure occurs, influence the likelihood of transmission. Nevertheless, TB outbreaks have been documented with more casual exposures in churches, schools, nursing homes, prisons and jails, and long airplane flights, as well as in other congregate settings, many of which have also been locations of documented SARS-CoV-2 transmission .\n\n【13】Direct and indirect evidence that SARS-CoV-2 may also be transmitted by aerosols with droplet nuclei (i.e. fine particles that remain suspended in air) carrying infectious particles  is increasing. A description of an outbreak of COVID-19, associated with a restaurant in Guangzhou, China, strongly suggested transmission through an airborne route , as did case distribution and additional studies of air circulation, also in this restaurant in Guangzhou .\n\n【14】For both TB and COVID-19, cough is a predominant symptom, and airborne droplets are produced by any forced expiratory maneuver, especially coughing; at least for TB, the severity of cough is an indicator of transmission risk. For TB, several additional indicators assist in quantifying the risk for transmission from the index case and, thus, in assigning priority to a contact investigation. These indicators include the bacillary burden, as indicated by the radiographic extent of the disease in the lungs and the presence or absence of cavitary lesions and qualitative sputum smear positivity . No such assessment is routinely used for COVID-19, although quantification of viral load in nasal or pharyngeal swab specimens and an assessment of the severity and duration of respiratory symptoms could provide such information . Reduction in viral inoculum by widespread wearing of masks has been postulated to result in less severe manifestations of SARS-CoV-2 infection .\n\n【15】For TB, because of the increasing risk for acquisition of infection with the closeness and duration of exposure to persons with this disease, contact evaluation can be structured, beginning in the home, workplace, or school, and places of leisure and working outward in a manner that conceptually resembles concentric circles. The number and percentage of close contacts with evidence of disease, or recent infection, inform the need to expand the investigation to contacts in outer ring circles. This iterative approach optimizes the use of resources for investigations and testing . For SARS-CoV-2, data strongly suggest that the virus is highly transmissible even with casual contact, so the duration of exposure might not be relevant .\n\n【16】All of the foregoing indicates that in conducting contact identification and evaluation for persons exposed to persons with COVID-19, a wide net must be cast. Moreover, given the incubation period and pace of the disease, the process must be accomplished much more quickly than is necessary for TB. Unfortunately, much of the knowledge base that is used to guide TB contact identification and evaluation does not yet exist for COVID-19. To generate the necessary information, investigators studying the epidemiology of COVID-19 and, in particular, those charged with investigating outbreaks and conducting contact tracing, should be certain that the data being collected will enable analyses directed toward identifying factors that influence viral transmission. A recent report of nationwide contact tracing for COVID-19 in South Korea indicated both the need to investigate »10 contacts per index case and that 11.8% of household contacts had COVID-19, >6 times the 1.9% prevalence of COVID-19 in nonhousehold contacts .\n\n【17】### Using the Investigation of TB on the USS Byrd as a Template\n\n【18】Essentially all infection control and public health measures for TB are based on the understanding, backed by strong empirical and experimental evidence, that _M. tuberculosis_ is transmitted nearly exclusively by aerosols . Some of the strongest evidence of _M. tuberculosis_ transmission through aerosols has been derived from several TB outbreak investigations. Perhaps the most notable and informative outbreak investigation was conducted in response to a single crew member who was found the have cavitary pulmonary TB during the course of a long sea tour by the US Navy vessel the USS Richard Byrd in 1965 . A thorough assessment of the patterns of air circulation and their relationship to new cases and infections was conducted aboard the ship. The investigation found that all new cases and infections occurred in crew members who had either direct personal contact with the index case-patient or were exposed through recirculated air in a closed ventilation system. The investigators were able to establish what might be viewed as a dose-response curve based on the exposure to different amounts of recirculated air and the proportion exposed crew members who were infected . Of particular note, several of the newly infected sailors (indicated by a new positive tuberculin skin test) who were asymptomatic and had negative chest radiographs were found to have _M. tuberculosis_ in their sputum, raising the possibility of transmission from persons without the usual symptoms of TB, as is the case with COVID-19 . This finding is consistent with findings from national TB prevalence surveys of a substantial proportion of study subjects who were found to have _M. tuberculosis_ in their sputum but had no symptoms (e.g. cough >2 weeks) .\n\n【19】Outbreaks of COVID-19 on a cruise ship (Diamond Princess) in late January 2020 and the USS Theodore Roosevelt in March 2020 provide unique opportunities, similar to those provided by the USS Byrd, to gain a more detailed understanding of transmission patterns for SARS-CoV-2. To date, published assessments of COVID-19 outbreaks in these 2 separate settings consist of initial assessments, 1 documenting the occurrence of 700 cases of COVID-19 among nearly 3,700 passengers and crew members in the cruise ship . The investigation identified that 15 of 20 cases in crew members were in food workers, and 16 of these 20 persons slept in cabins on deck 3. No details were provided for the distribution of COVID-19 cases in passengers, nor of the ventilation system in this cruise ship . A follow-up assessment was limited to 215 Hong Kong passengers after quarantine and disembarkation; 9 tested positive for SARS-CoV-2 . No berthing information is available for those passengers. The USS Roosevelt outbreak investigation was a serostudy of a convenience sample of 382 crew members . Although the sample was not representative of the entire crew, 60% of the participants had antibodies to SARS-CoV-2, indicating prior infection. Notably, 20% of the seropositive group denied having symptoms. Also, as is the case with asymptomatic TB, the degree to which these asymptomatic persons transmitted the infection is not known. Examination of crew member duty rosters and assessment of ventilation patterns in areas inhabited by infected and noninfected persons could provide important information concerning aerosol transmission and the role of spread of the virus by asymptomatic persons. Although the outbreak on the USS Byrd occurred >50 years ago, its assessment is a model for advancing knowledge by thorough investigations, including environmental studies to examine the role of air circulation. With increasing speculation and uncertainty about basic questions such as relative importance of different transmission modes for SARS-CoV-2 , the Diamond Princess and USS Roosevelt outbreaks present opportunities, similar to that provided by the USS Byrd, that should not be overlooked.\n\n【20】As noted, although contact identification and evaluation are widely used in high-income, low TB–incidence countries, implementation is limited in low- and middle-income countries. Given the experience with TB, considerable patience, skill, and ingenuity are needed in the implementation of contact tracing for COVID-19. Digital and other automated technologies have been applied to COVID-19 contact tracing in different country settings . This new thinking, coupled with innovative tools, will likely hold lessons and examples for improvements in TB prevention and control.\n\n【21】### Avoiding Past Mistakes and Seizing Present Opportunities\n\n【22】In response to COVID-19, countries are having to reassign or recruit and train staff, as well as to establish a robust laboratory diagnostic testing capacity to deliver timely quality-assured results. Early reports from the United States have documented that the COVID-19 response has diverted resources away from essential TB services . This scenario must be avoided; investments required should be used to improve all public health programs and be sustained over time. Thirty-five years ago, TB provided a dramatic example of the impact of inattention to, and disinvestments in, basic public health programs. During 1985–1992, a reversal of longstanding downward trends occurred as well as and 20% increase in cases .\n\n【23】We now have a rare opportunity to seize the moment and use the attention garnered by this novel virus pandemic to ensure that new investments contribute not only to the control of COVID-19, but also to the strengthening of older, yet very relevant public health programs, and to recognize that lessons learned from those programs benefit those at risk for COVID-19. In the United States and in other parts of the world, TB served as the impetus for the establishment of public health programs, and these programs were geared to deal with TB as a public health problem . Public health approaches to COVID-19, relying as they do on accelerated responses, digital technologies, and large numbers of trained community-based contact investigators, could establish a new more comprehensive paradigm for the public health programs of the future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "374862b4-efbf-487a-9477-dbfa4d6f4446", "title": "Shewanella spp. Bloodstream Infections in Queensland, Australia", "text": "【0】Shewanella spp. Bloodstream Infections in Queensland, Australia\n_Shewanella_ spp. most commonly _S. algae_ and _S. putrefaciens_ , are infrequent but occasionally severe causes of human infection associated with exposure to warm marine environments . Cases of otogenic and skin and soft tissue infections caused by _S._ ( _Pseudomonas_ ) _putrefaciens_ were described in the 1960s, and case series of bacteremic infections were reported in subsequent decades . Vignier et al. reported 16 cases of _Shewanella_ spp. infection that occurred in Martinique during 1997–2012 and identified an additional 239 cases in review of the published literature during 1973–2011 . That study observed that otogenic, skin and soft tissue, abdominal/biliary tract, and respiratory tract foci of infection were most common, and that 71 (28%) cases were bacteremic . _Shewanella_ spp. are frequently coisolated with other organisms, most notably Enterobacterales and _Aeromonas_ and _Vibrio_ spp. and occasionally might develop major antimicrobial drug resistance .\n\n【1】As a result of their rarity, the epidemiology of _Shewanella_ spp. bloodstream infections (BSIs) is poorly defined. The existing body of literature is limited to case reports and small series. In addition, as a result of developments in genomic and phenotypic testing, it has been recognized that before the current millennium many reports of _S. putrefaciens_ infection might have been actually caused by _S. algae_ . The objective of this study was to determine the contemporary incidence of and risk factors for development of _Shewanella_ spp. BSIs in the population of Queensland, Australia.\n\n【2】### Methods\n\n【3】The study population was all residents (2019 population ≈5 million) of Queensland, Australia. Queensland is a large state with diverse geography that includes subtropical and tropical coastal regions and inland dry desert areas. Approximately two thirds of the population is concentrated around the Greater Brisbane/Gold Coast/Sunshine Coast areas in the southeastern corner of the state, and the remainder is distributed predominantly along the eastern coastal areas. Healthcare within the publicly funded system is administered through 16 hospital and healthcare service regions . All Queensland residents with incident BSIs caused by _Shewanella_ spp. identified by Pathology Queensland during January 1, 2000‒December 31, 2019, were included in this study. Approval of the health research ethics committee at the Royal Brisbane and Women’s Hospital was granted with a waiver of individual consent (LNR/2020/QRBW/62494).\n\n【4】During the study, we used different methods to identify _Shewanella_ spp. Before 2008, VITEK 1, API20E, API20NE, (all from bioMérieux), and Microscan  were used to identify _Shewanella_ spp. but these methods were not able to identify _S. algae_ . From ≈2008 onward, the VITEK database for gram-negative identification was upgraded to improve differentiation of _S. algae_ . After matrix-assisted laser desorption/ionization time-of-flight mass spectrometry was introduced during 2012, differentiation of the 2 _Shewanella_ spp. was possible. In some instances in which earlier methods could not reliably distinguish between _S. putrefaciens_ and _S. algae_ , growth in nutrient broth with and without 6% NaCl was used to confirm the species identification. In instances in which methods were inadequate to reliably differentiate species, isolates are reported as _Shewanella_ spp. We performed antimicrobial drug susceptibility testing by using an automated method (i.e. VITEK AST Card, bioMérieux) and disc diffusion according to recognized standards  at the time of testing.\n\n【5】All blood cultures that had growth of _Shewanella_ spp. were retrospectively identified by the Clinical Information Systems Support Unit, Queensland Health, during the study period. We defined incident BSIs by the first isolation of a _Shewanella_ spp. for a patient; all subsequent isolations of the same species from that patient within 30 days were deemed to represent the same episode. Polymicrobial infections were those from which a _Shewanella_ spp. was coisolated with \\> 1 other major pathogens within a 48-hour period. Two independent sets of cultures were required for common contaminants to define significance .\n\n【6】Once incident episodes were identified, we obtained additional clinical and outcome information by using linkages to statewide databases. We identified all healthcare encounters with private and public institutions within the 2 years before 1 year after index blood cultured within the Queensland Hospital Admitted Patient Data Collection. We used this collection to determine healthcare encounters and hospital admission and discharge dates, discharge survival status, as well as all diagnostic codes (International Classification of Diseases, 10th Revision, Australian modification). Multiple admission episodes occurring within a continuous time period (such as with interhospital transfers) were deemed to represent 1 hospital admission for purposes of length of stay. We queried the Registry of General Deaths  as of December 31, 2020, to identify all deaths.\n\n【7】We classified BSIs as hospital-onset if the index blood culture was obtained 2 calendar days after admission or within 2 calendar days of hospital discharge . BSIs infections diagnosed in the community or within the first 2 calendar days of stay in hospital were classified as community-onset. Healthcare-associated BSIs were those community-onset BSIs that fulfilled \\> 1 criteria: nursing home resident, encounter at a healthcare institution within 30 days, or admission to a hospital for >2 days within 90 days before index blood culture . We classified community-onset BSIs that did not fulfill criteria for healthcare-associated infections as community-associated. We defined comorbid medical illnesses by using Charlson comorbidity index and established these illnesses by using validated coding dictionaries . We assigned a clinical focus on the basis of review of diagnosis-related group and primary hospital discharge codes.\n\n【8】We analyzed data by using Stata version 16.1 . The primary unit of analysis was incident BSI episodes, reported as crude incident rates per million persons annually. We excluded cases identified among persons who were not Queensland residents and obtained denominator data stratified by age, sex, and hospital and health service area from Queensland Health . We obtained the total annual number of sets of blood cultures performed by Pathology Queensland to calculate the overall culturing rate per population as described . We obtained average monthly mean peak and low temperatures and rainfall from Queensland weather stations from the Australian Bureau of Meteorology . Incidence rate differences were expressed as incidence rate ratios and reported with exact 95% CIs. p values <0.05 represented statistical significance.\n\n【9】### Results\n\n【10】During >86 million person-years of surveillance, 86 _Shewanella_ spp. BSIs occurred to give an incidence of 1.0 cases/1 million Queensland residents/year. We identified 4 additional episodes of _Shewanella_ spp. BSIs for persons residing in other states in Australia, and we excluded them from analysis. No second episodes of incident _Shewanella_ spp. BSIs occurred. Most (65, 76%) cases of BSIs were caused by _S._ _algae_ , 4 (5%) were caused by _S. putrefaciens_ , and 17 (20%) were not identified to the species level.\n\n【11】There was moderate year-to-year variability in the overall incidence of _Shewanella_ spp. BSIs . Occurrence of cases of _Shewanella_ spp. BSIs varied according to the month of the year; there was a peak in the warmer, wetter months and nadir in the drier, cooler months .\n\n【12】Incidence of _Shewanella_ spp. BSIs varied considerably among the regions of the state . No cases were observed within the western outback regions, and low rates were observed in the Greater Brisbane area. The highest rates occurred in the tropical coastal Torres and Cape area .\n\n【13】The median age of case-patients was 71.4 (interquartile range \\[IQR\\] 60.3–82.8) years, and 72 (84%) incident episodes were in male patients. There was an increased risk for development of _Shewanella_ species BSIs with advancing age, particularly among male patients . Male patients had an overall 5-fold increased risk compared with female patients (1.7 cases/1 million vs. 0.3 cases/1 million; incidence rate ratios for male patients 5.2 \\[95% CI 2.9–9.0\\]; p<0.0001).\n\n【14】Most BSIs were community-onset; 25 (29%) were classified as healthcare-associated, 54 (64%) as community-associated, and 6 (7%) as hospital-onset. Of the 25 healthcare-associated case-patients, 21 (84%) had hospital visits within 30 days, and 10 (40%) had hospital admissions within 90 days before the index episode. None were nursing home residents. The median Charlson comorbidity index was 2 (IQR 1–3), and only 21 (25%) patients had no underlying conditions documented. The most common focus of infection was soft tissue (35 cases, 41%) cases. Seven (8%) case-patients had an abdominal focus, 6 (7%) had a lower respiratory focus, 2 (2%) had an endovascular focus, and 1 case each had a bone/joint, head/neck, or genitourinary focus of infection. No focus was identified for 33 (38%) case-patients.\n\n【15】Most (73, 70%) infections were monomicrobial. Among the 13 polymicrobial infections, 4 patients had 3 isolates, and 2 patients had 4 isolates. The co-isolated organisms included _Escherichia coli_ in 7 cases, and 1 each of _Candida_ spp. _Corynebacterium diphtheriae_ , _Enterobacter cloacae_ , _Klebsiella oxytoca_ , _Proteus mirabilis_ , _Sphingomonas paucimobilis_ , _Pseudomonas oryzihabitans_ , _Pseudomonas aeruginosa_ , _Stenotrophomonas maltophilia_ , group G streptococcus, _Vibrio vulnificus_ , and 1 unidentified gram-negative bacillus. Four isolates (n = 65; 6%) were meropenem resistant and 1 (n = 70; 1%) was ceftazidime resistant. No resistance to gentamicin (n = 76) or ciprofloxacin (n = 74) was observed among _Shewanella_ spp. isolates tested.\n\n【16】All but 1 patient was admitted to a hospital for management (median length of stay 8 \\[IQR 5–16\\] days). Four patients required admission to an intensive care unit. Twelve (14%) patients died during the index hospitalization, and 13 (15%) died within 30 days of BSI diagnosis. Among the 13 patients who died, 6 (46%) had no focus identified, 3 (23%) had an abdominal focus, and 2 (15%) had soft tissue and lower respiratory tract infections. Eleven (85%) patients who died had \\> 1 Charlson comorbidity.\n\n【17】### Discussion\n\n【18】We describe the epidemiology of _Shewanella_ species BSIs in a large population in Australia. We confirm these infections as rare infections and provide novel incidence data. In addition, although usually considered community-associated pathogens, 29% of our cases were healthcare-associated and 7% hospital-onset. We also observed that the most patients had underlying medical illnesses and that the elderly were at highest risk. This finding is useful given that that the Queensland population, like those in many other high-income countries, is aging and showing an increased prevalence of chronic illnesses. Therefore, we might expect that the burden of _Shewanella_ species BSIs will increase in the coming years.\n\n【19】There are few previous contemporary studies for which to compare our results. Although case series have been published since the turn of the millennium, they have included small numbers of cases, of which only a minority have been associated with BSIs . Our observation of a temporal relationship between seasonal temperature and rainfall is much more pronounced than when similarly examined in a previous study conducted in Reunion Island . Climate factors probably play a major role in these infections given that they are usually identified in warm regions. However, these infections probably involve a complex interaction between environmental and human activity‒related factors.\n\n【20】Vignier et al. reported a case series from Martinique and summarized the world literature on _Shewanella_ species infections during 2013 . Although that study provided useful results, similar to all summaries of case reports, these results must be interpreted with caution because unusual or atypical cases might be more likely to be submitted and accepted for publication. Therefore, summaries of such reports might not reflect the average or usual characteristics of such infections. Our study has the benefit of comprehensive and consistent identification of all cases identified by a statewide laboratory such that selection biases are minimized and secular changes over time might be observed.\n\n【21】Nearly all of the _Shewanella_ species BSIs occurring in our population during years when we had adequate means of identification were caused by _S. algae_ . It has been recognized that many _S. putrifaciens_ cases previously reported were probably _S. algae_ because the clinical characteristics of these infections might be confused . Unfortunately, we do not have all of our original isolates for retesting.\n\n【22】Our study benefited from surveillance of a large population in Queensland. Including BSI data from public hospital and community collection sites over a 20-year period resulted in >86 million person-years of observations. The relatively small number of BSIs attributed to _Shewanella_ species  limited the statistical significance of further analysis of the data, such as differences between species, underlying conditions, and outcome. A strength of our study is that our laboratory surveillance was statewide in scope and included specimens sent from hospital and community collection sites.\n\n【23】However, our study was limited to the publicly funded system. Thus, cases identified within private hospitals and collection sites were not included. Although we speculate that this represents a small proportion of missed cases, our incidence rates should be interpreted as potential underestimates of the true population rate. A second limitation was that our study was retrospective; therefore, we were restricted in data variables for analysis. It would have been informative to examine environmental exposures, such as risk factors and the effect of specific antimicrobial drug treatments on outcome . A third limitation was that as a result of small numbers, we did not age and sex standardize our incidence rates. The possibility exists that at least some of the increased incidence observed in recent years of the study could have been related to demographic changes. Finally, as is the case for any study examining BSIs, our ability to detect a case is dependent on whether clinicians obtain blood for culturing from patients who have suspected infections. In addition, only a portion of all disease attributable to _Shewanella_ species is associated with positive blood cultures. Accordingly, the true rate of disease attributable to _Shewanella_ species in our population is likely higher than we detected .\n\n【24】In conclusion, we present a novel study of _Shewanella_ species BSIs that details the epidemiology of these infections in a large population in Australia. We observe that older persons are at highest risk and that their incidence is increased in association with higher environmental temperatures. Although _Shewanella_ species BSIs are rare, there is a major potential for large increases in coming years as a result of aging populations and climate change. Ongoing surveillance is warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "063ae00c-ef9e-40d5-85e2-6641f77ffbbe", "title": "Dose–Response Association Between High-Density Lipoprotein Cholesterol and Stroke: A Systematic Review and Meta-Analysis of Prospective Cohort Studies", "text": "【0】Dose–Response Association Between High-Density Lipoprotein Cholesterol and Stroke: A Systematic Review and Meta-Analysis of Prospective Cohort Studies\nAbstract\n--------\n\n【1】**Introduction**\n\n【2】Studies investigating the effect of high-density lipoprotein cholesterol (HDL-C) on stroke and stroke subtypes have reached inconsistent conclusions. The purpose of our study was to clarify the dose–response association between HDL-C level and risk of total stroke and stroke subtypes by a systematic review and meta-analysis.\n\n【3】**Methods**\n\n【4】We performed a systematic search of PubMed, Embase, and Web of Science databases through July 30, 2020, for prospective cohort studies that reported the HDL-C–stroke association and extracted the estimate that was adjusted for the greatest number of confounding factors. Restricted cubic splines were used to evaluate the linear and nonlinear dose–response associations.\n\n【5】**Results**\n\n【6】We included 29 articles, which reported on 62 prospective cohort studies including 900,501 study participants and 25,678 with stroke. The summary relative risk per 1-mmol/L increase in HDL-C level for total stroke was 0.82 (95% CI, 0.76–0.89; _I_ 2  \\= 42.9%; n = 18); ischemic stroke (IS), 0.75 (95% CI, 0.69–0.82; _I_ 2  \\= 50.1%; n = 22); intracerebral hemorrhage (ICH), 1.21 (95% CI, 1.04–1.42; _I_ 2  \\= 33.4%; n = 10); and subarachnoid hemorrhage (SAH), 0.98 (95% CI, 0.96–1.00; _I_ 2  \\= 0%; n = 7). We found a linear inverse association between HDL-C level and risk of total stroke and SAH, a nonlinear inverse association for IS risk, but a linear positive association for ICH risk. The strength and the direction of the effect size estimate for total stroke, IS, ICH, and SAH remained stable for most subgroups. We found no publication bias with Begg’s test and Egger’s test for the association of HDL-C level with risk of total stroke, IS, and ICH.\n\n【7】**Conclusion**\n\n【8】A high HDL-C level is associated with reduced risk of total stroke and IS and an increased risk of ICH.\n\n【9】Introduction\n------------\n\n【10】Stroke is highly prevalent worldwide, and the number of people who experience stroke increased to more than 104.2 million in 2017 . From 1990 through 2017, the disability-adjusted life-years for stroke were about 132.0 million in 195 countries . Moreover, stroke is the second leading cause of death in the world, accounting for 6.2 million deaths globally in 2017. Of these deaths, about 2.7 million were due to ischemic stroke (IS), 3.0 million to intracerebral hemorrhage (ICH), and 0.5 million to subarachnoid hemorrhage (SAH) . However, much of the stroke burden could be prevented by managing and controlling modifiable risk factors.\n\n【11】Many prospective cohort studies reported that a high-density lipoprotein cholesterol (HDL-C) level protected against the development of stroke . However, the “good cholesterol” label for HDL-C has been challenged by several recent randomized controlled trials demonstrating that HDL-C–elevating therapy increased the risk of cardiovascular diseases . Thus, a full understanding of the effect of HDL-C level on stroke and stroke subtypes is warranted. Only one systematic review, conducted in 2008, examined the association between HDL-C level and risk of total stroke . Another meta-analysis in 2013 investigated the association between HDL-C level and risk of hemorrhagic stroke . However, up to 10 more cohort studies have been published recently on the association of HDL-C level with total stroke, ICH, and SAH, showing inconsistent results . No meta-analysis has been performed on the association of HDL-C level with IS, and a dose–response meta-analysis on the association of HDL-C level with total stroke and IS is lacking. We therefore performed this systematic review and dose–response meta-analysis of prospective cohort studies to quantitatively evaluate possible linear or nonlinear associations between baseline HDL-C level and risk of total stroke, IS, ICH, and SAH.\n\n【12】Methods\n-------\n\n【13】### Data sources and searches\n\n【14】We followed the protocol for the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) Statement for our meta-analysis . We conducted a systematic literature search of PubMed, Embase, and Web of Science databases for all reports of prospective cohort studies that examined the association between HDL-C level and stroke and were published through July 30, 2020, with no restriction on language. We also searched the reference lists of all related articles and reviews.\n\n【15】### Study selection\n\n【16】Two authors (R.Q. and M.H.) independently searched articles, selected relevant studies based on their title and abstract, then evaluated these articles by reviewing the full text. Inclusion criteria for prospective cohort studies were as follows: 1) study participants were aged ≥18 years; 2) the study investigated the association between HDL-C level and risk of stroke or stroke subtypes; 3) the study reported the effect estimates, relative risks (RRs), or hazard ratios (HRs), with 95% CIs for ≥3 HDL-C categories or per-unit increase in HDL-C level; and 4) the study reported the number of cases, exposed person-years, or participant numbers in each category of HDL-C level. We excluded cross-sectional and case-control studies, commentaries, letters, reviews, meta-analyses, and studies with unusable data. If data from the same study were reported more than once, only the most recent and complete data were included.\n\n【17】### Data extraction and quality assessment\n\n【18】R.Q. and L.L. independently extracted the following information from each study: first author, publication year, study name, study location, follow-up period, age range, sex, stroke and HDL-C assessment method, baseline levels of HDL-C, case number of per-category HDL-C exposure, total persons or person-years of per-category HDL-C exposure, reported RRs or HRs and 95% CIs for each HDL-C category, and adjusted covariates. Included studies were assessed for quality according to the 9-point Newcastle–Ottawa Quality Assessment Scale (NOS) . Any discrepancy was resolved by discussion with a senior investigator (D.H.).\n\n【19】We classified stroke, which included embolic infarction, large-artery occlusive infarction, lacunar infarction, and unclassified, as ICH, SAH, and IS . Some studies include all types of stroke for analysis and we call it total stroke in this meta-analysis. The lowest HDL-C category was the reference. For studies that did not choose the lowest category as the reference category, we reformulated RRs to set the lowest HDL-C category as the reference . When HDL-C levels were reported in milligrams per deciliter (mg/dL), we used the scaling factor of 38.67 to translate 1-mg/dL HDL-C to 1-mmol/L HDL-C. Studies that provided results separately for men and women or reported multiple stroke subtypes within an article were treated as independent studies. For studies reporting results separately for fatal and nonfatal stroke, we combined the RRs and then included the pooled RR in the meta-analysis.\n\n【20】### Data synthesis and analysis\n\n【21】We considered the RR and 95% CI of the effect size for all studies. The reported HRs in the primary studies were considered equal to RRs . We first used the DerSimonian and Laird random-effects model, which considers both within-study and between-study variation, to calculate summary RRs and 95% CIs for high versus low HDL-C level . Studies reporting only a continuous risk estimate of stroke were excluded from our analysis. We then pooled the study-specific dose–response RRs and 95% CIs per 1-mmol/L increase in HDL-C level .\n\n【22】We used generalized least squares regression to estimate the study-specific dose–response association . The natural RRs and CIs across categories of HDL-C level were used to compute study-specific slopes (linear trends) and 95% CIs. A generalized least squares regression model estimates the linear dose–response coefficients and considers the covariance for each exposure category within each study because they are estimated relative to a common referent HDL-C level category. In this method, the distribution of cases and person-years, or cases and noncases, with the RRs and estimates of uncertainty (eg, CIs) for ≥3 quantitative categories of exposure were required. If studies reported only the total number of cases or person-years, the number of person-years or cases in each category was obtained from the total number of person-years or cases divided by the number of reported categories. We assigned the mean, median, or midpoint of HDL-C level in each category to the corresponding risk estimate. When the lowest or highest categories were open-ended, we assumed the width of the category to be the same as the closest category when estimating the midpoint . For the studies already reporting a linear dose–response trend for per n-mmol/L increase in HDL-C level, we calculated the dose–response RRs per 1-mmol/L increase in HDL-C level with this formula: RR 1  \\= EXP (LN (RR n  )/n\\*1), where RR 1  represents the dose–response RRs for each 1-mmol/L increase in HDL-C level and RR n  represents the dose–response RRs for each n-mmol/L increase in HDL-C level (EXP: exponential function; LN: log base e) . All study-specific dose–response RR estimates were then pooled by using the DerSimonian and Laird random effects model . With heterogeneity ( _I_ 2  ) ≥50%, a random-effects model was used to calculate the summary RRs and 95% CIs; otherwise a fixed-effects model was used, which considered both within- and between-study variation. The Hartung-Knapp-Sidik-Jonkman method was used to evaluate the stability of results for N <10 . A potential nonlinear association was examined by modeling HDL-C level by using restricted cubic splines with 3 knots located at the 25th, 50th, and 75th percentiles of the distribution . The _P_ for nonlinearity was calculated by testing the null hypothesis that the coefficient of the second spline is equal to zero .\n\n【23】Heterogeneity was assessed by Cochran Q and _I_ 2  statistics . For the Q statistic, _P_ < .10 was considered significant. For the _I_ 2  statistic, _I_ 2  values of 0%, 25%, 50%, and 75% were considered to reflect no, low, moderate, and high heterogeneity, respectively. We also performed subgroup analyses by sex, region, follow-up period, publication year, sample size, and the covariates (alcohol drinking, education, body mass index, systolic blood pressure, physical activity, lipid-lowering medication use, and other lipid profile parameters) adjusted in the analysis.\n\n【24】A sensitivity analysis was performed to assess the influence of each individual study by omitting 1 study at a time and calculating a pooled estimate for the remainder of the studies . Potential publication bias was assessed with Egger’s and Begg’s tests . Conversion from DerSimonian-Laird results to Hartung-Knapp-Sidik-Jonkman results involved using Microsoft Excel software (Microsoft Corp). Other analyses were conducted with Stata 12.1 (Stata Corp), and all tests were 2-sided with a significance level of _P_ < .05.\n\n【25】Results\n-------\n\n【26】**Literature search and study characteristics** . Our literature search identified 7,366 articles; 1,113 were duplicates, leaving 6,253. After screening the titles and abstracts, we selected 201 potentially eligible articles. After detailed evaluation, we included 29 articles describing 62 prospective cohort studies in our meta-analysis with a total of 900,501 study participants of which 25,678 had stroke .\n\n【27】Eleven studies were conducted in Asia (including Iran and Israel) , 9 in the United States , 7 in Europe , and 2 in Australia . Three prospective cohorts included only men , another 3 included only women , and the rest included both sexes . The mean NOS score was 8.24, which indicates the high quality of the articles included in the meta-analysis.\n\n【28】**HDL-C level and risk of total stroke** . To explore the association between HDL-C level and risk of total stroke, we examined 18 studies that included 256,427 participants overall and 12,328 people with stroke. We excluded 8 studies in comparing the highest versus lowest category of HDL-C because they provided only a continuous risk estimate. The pooled RR was 0.79 (95% CI, 0.72–0.87; _I_ 2  \\= 46.4%; _P_ heterogeneity  \\= .05) . The 18 studies were included in the dose–response analysis; the pooled RR for total stroke was 0.82 (95% CI, 0.76–0.89) per 1-mmol/L increase in HDL-C level, with low heterogeneity ( _I_ 2  \\= 42.9%; _P_ heterogeneity  \\= .03)  . We found a linear dose–response association between HDL-C level and risk of total stroke ( _P_ nonlinearity  \\= .96)  . No evidence of heterogeneity was detected between subgroups . We observed an inverse association for most subgroups, except a nonsignificant association in studies of women, with a follow-up period of less than 10 years, without adjustment for physical activity or without adjustment for other lipid profile parameters  .\n\n【29】**  \nLinear dose–response association between high-density lipoprotein cholesterol and risk of stroke and stroke subtypes modeled with restricted cubic splines. Graph A shows total stroke; B, ischemic stroke; C, intracerebral hemorrhage; and D, subarachnoid hemorrhage. \n\n【30】**HDL-C level and risk of IS** . We included 10 studies consisting of a total of 706,482 participants and 19,047 people with stroke in the binary analysis of the association of IS risk with HDL-C level. The pooled RR was 0.75 (95% CI, 0.68–0.82; _I_ 2  \\= 44.3%; _P_ heterogeneity  \\= .06). Another 12 studies provided only a continuous risk estimate, so 22 studies were included in the dose–response analysis of IS risk. The pooled RR for IS was 0.75 (95% CI, 0.69–0.82) per 1-mmol/L increase in HDL-C level, with low heterogeneity ( _I_ 2  \\= 50.1%; _P_ heterogeneity  \\= .004) . We found a nonlinear dose–response association between HDL-C level and IS risk ( _P_ nonlinearity  \\= .13) . No evidence of heterogeneity was detected between subgroups . Subgroup analyses showed a nonsignificant association in studies with a sample size of less than 10,000.\n\n【31】**HDL-C level and risk of ICH** . Ten studies consisting of 246,607 participants overall and 1,467 people with ICH were included in the analysis of HDL-C level and risk of ICH. The summary RR was 1.13 (95% CI, 0.93–1.36; _I_ 2  \\= 29.9%; _P_ heterogeneity  \\= 0.17) in the binary analysis . The pooled results showed that risk of ICH was increased 26% per 1-mmol/L increase in HDL-C level (RR 1.21; 95% CI, 1.04–1.42), with low heterogeneity ( _I_ 2  \\= 33.4%, _P_ heterogeneity  \\= 0.14) . We found a linear dose–response association between HDL-C level and risk of ICH (P nonlinearity  \\= 0.28) . The effect size and direction of the pooled estimates were robust for most subgroups.\n\n【32】**HDL-C level and risk of SAH** . Data from 7 studies that included a total of 127,935 participants of which 551 had SAH provided information on the association between HDL-C level and risk of SAH. The pooled RR was 0.69 (95% CI, 0.50–0.95; _I_ 2  \\= 30.7%; _P_ heterogeneity  \\= 0.19)  in the binary analysis. With a per-1–mmol/L increase in HDL-C level, the pooled RR was 0.98 (95% CI, 0.96–1.00; _I_ 2  \\= 0%; _P_ heterogeneity  \\= 0.61) . Hartung-Knapp-Sidik-Jonkman results showed that risk of SAH was decreased 14% per 1-mmol/L increase in HDL-C level (RR 0.86; 95% CI, 0.75–0.98). We found a linear dose–response association between HDL-C level and risk of SAH ( _P_ nonlinearity  \\= 0.94) . The pooled estimates remained relatively stable on subgroup analyses.\n\n【33】**Sensitivity analyses and publication bias** . In sensitivity analyses, the results were robust when excluding one study at a time in the analysis of total stroke, IS, ICH, and SAH. We found no publication bias with Begg’s test for risk of total stroke ( _P_ \\= 0.10), IS ( _P_ \\= .15), and ICH ( _P_ \\= .86), and Egger’s test for risk of total stroke ( _P_ \\= .10), IS ( _P_ \\= .31), and ICH ( _P_ \\= .63). Publication bias was not assessed for the association between HDL-C level and SAH because of limited studies.\n\n【34】Discussion\n----------\n\n【35】We aimed to clarify the association between HDL-C level and risk of total stroke and stroke subtypes and found an inverse linear association between HDL-C level and risk of total stroke and IS. For each 1-mmol/L increase in HDL-C level, the risk of total stroke decreased by 18% and that of IS decreased by 24%. For ICH, we found a positive linear association, with the risk of ICH increased 21% per 1-mmol/L increase in HDL-C level. In addition, we found a marginal inverse linear association between HDL-C level and risk of SAH.\n\n【36】Results of previous reviews and meta-analyses evaluating the association between HDL-C level and total stroke, ICH, and SAH were consistent with our study . However, previous research suggesting a negative association between HDL-C level and total stroke was based on a review of 8 cohort studies and 3 case-control studies . Our review did not report the association between HDL-C level and stroke subtypes because of the limited data on that relationship . In the current meta-analysis, we quantitatively evaluated the possible linear or nonlinear association of HDL-C level with total stroke, IS, ICH, and SAH.\n\n【37】We found an inverse linear association between HDL-C level and risk of total stroke. The reduced risk of total stroke may be due to the anti-atherosclerotic effects of HDL-C . The oxidation of LDL is thought to play an important role in the development of atherogenesis. HDL is a powerful antioxidant that exists in the subintimal space of the artery at a concentration 20 times greater than that of LDL and thus plays an important role in preventing atherosclerosis by inhibiting LDL oxidation in the artery wall . Additionally, HDL-C may play a central role in the reverse transport of cholesterol, thereby preventing the accumulation of excess cholesterol in peripheral tissues and the processes that initiate atherogenesis . However, subgroup analyses by sex showed significantly decreased risk of total stroke in men but not in women. The reason behind such inference remains unknown, and future experimental studies are needed to explore the potential mechanism.\n\n【38】Among the 22 studies included for the association between HDL-C level and IS risk in the current meta-analysis , 16 showed an inverse association , 10 of which reached a significant level  while the remaining 6 showed no statistical significance . After pooling the 22 studies with a larger sample size, we observed a significant inverse nonlinear association between HDL-C level and IS. The main cause of IS is the formation of atherosclerotic plaque on the carotid artery wall . The anti-atherosclerotic effects and potent anti-inflammatory properties of HDL-C could explain our finding of a significant inverse association between HDL-C level and risk of IS . The main protein in HDL-C, apolipoprotein A-1, had a direct protective effect on atherosclerosis in several animal experiments . Besides, Kotur-Stevuljevic et al suggested that the increase in oxidative stress of HDL in patients after IS contributed to a decrease in the activity of the anti-oxidant enzyme paraoxonase 1 . Further research should confirm whether increasing HDL-C level through lifestyle changes or pharmacologic therapies will affect IS risk.\n\n【39】Compared with a previous meta-analysis of HDL-C level and hemorrhagic stroke , 5 cohort studies were additionally included in our meta-analysis of the association of HDL-C level and ICH risk. We found a positive linear association of HDL-C level and ICH risk, which agreed with the previous meta-analyses. The possible mechanisms are as follows. First, HDL also has an antithrombotic function. A high HDL-C level can increase the risk of ICH by promoting fibrinolysis , which was found to be associated with the inhibition of coagulation cascade and the stimulation of blood clot fibrinolysis . In addition, HDL attenuates platelet function by stimulating endothelial cells to produce nitric oxide and prostacyclin .\n\n【40】Results of a previous meta-analysis reported a significant positive association between HDL-C level and SAH based on 2 cohort studies . Five cohort studies were additionally included in our meta-analysis of HDL-C level and SAH risk. We found a marginal inverse linear association between HDL-C level and SAH risk. More large-sample cohort studies are needed to firmly establish this association.\n\n【41】Our meta-analysis has several strengths. To our knowledge, this is the first meta-analysis to systematically examine the association between HDL-C level and risk of major stroke subtypes by using both binary and dose–response analyses. Also, all included studies had a prospective design, large sample size, and long follow-up. In addition, the high mean NOS score, 8.24, indicated a relatively high quality of the articles included.\n\n【42】Our meta-analysis also had several limitations. First, IS is a mixed term, including lacunar infarction, large-artery occlusive infarction, and embolic infarction. Only 1 study explored the distinction between IS subtypes, so we could not explore the association between HDL-C level and each IS subtype . Second, most included studies did not exclude participants using medication, which may have confounded the association of HDL-C level with risk of total stroke and stroke subtypes. Third, HDL-C level was measured only at baseline, so we could not consider the effect of HDL-C changes during follow-up. Finally, all included studies were observational, and we need further analyses based on randomized clinical trials for assessing the causality of HDL-C level on stroke.\n\n【43】The effects of HDL cholesterol levels on stroke risk vary by type of stroke. A high HDL-C level was associated with reduced risk of total stroke and IS, but an increased risk of ICH. Reasonable control of HDL-C level will prevent and control incident stroke. However, because the HDL particle is so complex, we do not know whether the particle size, number, HDL-C content, or functionality is the best marker of stroke risk. Future studies with information on potential mechanisms are needed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6df27185-d7a8-4a92-b0de-dfac6006124e", "title": "Participation in Older Adult Physical Activity Programs and Risk for Falls Requiring Medical Care, Washington State, 2005-2011", "text": "【0】Participation in Older Adult Physical Activity Programs and Risk for Falls Requiring Medical Care, Washington State, 2005-2011\nAbstract\n--------\n\n【1】**Introduction**  \nPhysical activity is known to prevent falls; however, use of widely available exercise programs for older adults, including EnhanceFitness and Silver Sneakers, has not been examined in relation to effects on falls among program participants. We aimed to determine whether participation in EnhanceFitness or Silver Sneakers is associated with a reduced risk of falls resulting in medical care.\n\n【2】**Methods**  \nA retrospective cohort study examined a demographically representative sample from a Washington State integrated health system. Health plan members aged 65 or older, including 2,095 EnhanceFitness users, 13,576 Silver Sneakers users, and 55,127 nonusers from 2005 through 2011, were classified as consistent users (used a program ≥2 times in all years they were enrolled in the health plan during the study period); intermittent users (used a program ≥2 times in 1 or more years enrolled but not all years), or nonusers of EnhanceFitness or Silver Sneakers. The main outcome was measured as time-to-first-fall requiring inpatient or out-of-hospital medical treatment based on the _International Classification of Diseases, 9th Revision, Clinical Modification_ , Sixth Edition and E-codes.\n\n【3】**Results**  \nIn fully adjusted Cox proportional hazards models, consistent (hazard ratio \\[HR\\], 0.74; 95% confidence interval \\[CI\\], 0.63–0.88) and intermittent (HR, 0.87; 95% CI, 0.8–0.94) EnhanceFitness participation were both associated with a reduced risk of falls resulting in medical care. Intermittent Silver Sneakers participation showed a reduced risk (HR, 0.93; 95% CI, 0.90–0.97).\n\n【4】**Conclusion**  \nParticipation in widely available community-based exercise programs geared toward older adults (but not specific to fall prevention) reduced the risk of medical falls. Structured programs that include balance and strength exercise, as EnhanceFitness does, may be effective in reducing fall risk.\n\n【5】Introduction\n------------\n\n【6】Falls affect 30% to 40% of community-living adults over age 65 every year. Half of falls result in some type of injury, and falls may contribute to declining function, loss of independence, illness, and death . Fall-related health care costs the United States nearly $30 billion annually . For many older adults, fear of falls and their physical and psychological aftermath are a serious concern, often leading to self-imposed restrictions on activity .\n\n【7】Fall prevention research has found that regular physical activity incorporating strength and balance exercise can reduce falls, fall-related injuries, and falls resulting in medical care . Community-based programs intended to enhance older adults’ access to age-appropriate exercise may be beneficial in preventing falls. In a large integrated health system in Washington State, Group Health Cooperative (GHC), Medicare-qualifying adult members aged 65 or older are eligible to participate in 2 nationally disseminated exercise programs at no additional cost. EnhanceFitness (EF), an evidence-based intervention funded by the Centers for Disease Control and Prevention, offers community-based group exercise classes led by qualified instructors. Another program, Silver Sneakers (SS), provides full membership to participating fitness centers nationwide in more than 10,000 locations . Previous investigations have suggested that specific fall prevention exercise programs can be effective in preventing falls among community-dwelling older adults . However, these programs targeted fall prevention and were not as widely disseminated as EF and SS are. Although general exercise programs for older adults reach a much wider audience, their impact on fall prevention has not been studied. Although EF participation has been shown to improve physical function , no previous studies have examined associations between participation in either EF or SS and fall-related outcomes . Preventing falls and the costs and system burden they create is a relevant outcome from both the societal and health plan perspectives.\n\n【8】Our objective was to examine the relationship between participation in EF or SS and risk for a fall requiring medical treatment (termed “medical fall” hereafter), in a sample of GHC Medicare plan enrollees. On the basis of existing evidence that physical activity plays an important role in fall prevention, we hypothesized that consistent users of either program would have lower risks of a medical fall compared with nonusers.\n\n【9】Methods\n-------\n\n【10】### Study design and sample\n\n【11】A retrospective cohort study was conducted among a demographically representative sample of the Seattle-area population of older adults. The exposure of interest was defined as participation in either the EF or SS program and the outcome was the first occurrence of a medical fall during the study period . Participants were members older than age 65 of GHC, which serves patients throughout the states of Washington and Northern Idaho. Participants were selected by using the following eligibility criteria: Integrated Group Practice members (receiving medical care primarily within the GHC system), continuous enrollment for at least 1 year, aged 65 to 98, and eligible for the Medicare EF and SS programs for some portion of 2005 through 2011. Individuals were excluded if they met any of the following specific criteria: residing in long-term care or nursing home setting, receiving hospice care (International Classification of Diseases, 9th Revision, Clinical Modification \\[ICD-9-CM\\] code V66.7), wheelchair-bound (V46 or V53.8), aged 99 or older, or having a diagnosis of a serious mental health or substance use disorder (290–319.99, not including depression \\[296.2, 296.3, 300.4, 311\\] anxiety \\[300.02\\], or dementia ). All demographic, health, and medical record data were extracted from GHC electronic health records and merged with participation data supplied by the EF and SS programs. GHC has complete records of health care use and costs, and these data have been validated and are frequently used for research . This research was approved by the institutional review boards of both Group Health Research Institute and the University of Washington.\n\n【12】EF, a nationally disseminated, evidence-based exercise program  for older adults, offers group-based exercise classes in community settings. Each class lasts 1 hour and follows a set format, including exercises targeting cardiovascular endurance (20–25 min), strength (20 min), and balance and flexibility (10 min), all of which are adaptable to individual ability level . SS is a benefit offered to Medicare Advantage enrollees that allows access to more than 10,000 fitness facilities nationwide. People who enroll in SS are granted access to exercise equipment and group exercise classes offered through selected fitness centers. SS also offers older adult fitness classes through these facilities that participants may choose to attend .\n\n【13】### Exposure and outcome assessments\n\n【14】Participation in either program was defined as documentation of attendance at either the EF or SS programs at least twice in a given year. Because specific attendance counts were not available in the data set, participation in the EF and SS programs was stratified into 3 levels in an effort to approximate regular exposure to the programs. These strata were 1) consistent users, who participated every year they were enrolled in GHC during the study period ; 2) intermittent users, who participated at least 1 but not all years they were enrolled; and 3) nonusers, who never participated in either program while enrolled. Mean duration of enrollment in GHC for this sample was 4.7 (standard deviation, 2.3) years.\n\n【15】Falls requiring medical treatment were identified through the use of both inpatient and outpatient recording of ICD-9-CM codes (805–829: fractures, including hip fracture; 830–839: joint dislocations; and 800–804 and 850–854: intracranial injury) and E-codes (880–888: accidental fall injury) indicating medical treatment of an injury related to having had a fall . The presence of an E-code or one of the listed ICD-9-CM codes was sufficient to define a study outcome consistent with definitions developed in previous research .\n\n【16】### Covariates\n\n【17】The following covariates were extracted from electronic health record data of both inpatient and outpatient treatment in the GHC system and were assessed as potential confounders to the relationship of interest (ie, the relationship between program participation and risk of a medical fall): age (continuous), sex (male/female), race/ethnicity (white, black, Asian, Hispanic, Native American), body mass index (BMI, kg/m 2  , calculated from most recent height and weight for each year; continuous), smoking status (yes/no), and Charlson Comorbidity Index score , a general measure of comorbidity based on the presence or absence of 19 conditions weighted for severity (continuous). Small amounts of missing data were noted in the variables for race/ethnicity (6.1%), BMI (19.7%), and smoking (2.3%); models including these variables excluded individuals with missing data. Because of their strong association with increased fall risk, pharmacy data were used to indicate whether participants used sedatives or sleeping medications (2 fills within 90 days for benzodiazepines or prescription sleep medications; yes/no).\n\n【18】The following comorbidities were identified through ICD-9 codes in the medical record and were also included in the analysis to account for their potentially confounding relationship with program participation and risk of a medical fall: diabetes status (249–251; yes/no); diagnosis of dementia (290, 294.1, 294.2, 331.0, 331.1, 331.82, 331.83; yes/no), walking disorder (719.7: difficulty walking; 781.2: abnormal gait; and 728.87: generalized weakness; yes/no), osteoarthritis (715, 721.0–721.9; yes/no), osteoporosis (733; yes/no), musculoskeletal conditions (712–719: arthropathy, rheumatoid arthritis, joint derangement; yes/no), and visual impairment (365: glaucoma; 366: cataract; 362.50–362.53, 362.55, 362.63: macular degeneration; and 362.01–362.03, 362.10, 362.11, 362.2, 363.31: retinopathy; yes/no). All comorbidity diagnosis variables were time-varying and constructed to reference a diagnosis for the given condition in the year before the year of interest.\n\n【19】### Data analysis\n\n【20】Initially, Kaplan–Meier survival curves were fit to the data to depict the time-to-medical-fall for each group. Time-to-event analyses were then conducted using days from entry into the study to the date of a fall, loss to follow-up, or the end of the study period (December 31, 2011). Individuals were censored if loss to follow-up occurred because of death or withdrawal from the GHC system. Specific dates for these events were unavailable and therefore defined as June 30 of the last year an individual appeared in the data set if that year was before 2011. A series of Cox Proportional Hazard models compared time-to-fall of nonusers to that of consistent and intermittent users of each program. Several models were constructed: a crude model (no adjustment for confounders), a demographic model (adjusted for age, race, and sex), and a full model (adjusted for age, race, sex, BMI, smoking, and the following comorbidities: dementia, walking disorder, osteoarthritis, osteoporosis, musculoskeletal conditions, and visual impairment). In fully adjusted models, individuals with missing values for a covariate were dropped from the analysis. All analyses were conducted using Stata 13 (StataCorp, LP).\n\n【21】Results\n-------\n\n【22】Compared with nonusers, consistent and intermittent users of either program were more likely to be female, less likely to smoke, and had lower Charlson comorbidity scores . However, all users of either EF or SS were more likely to have had a diagnosis for osteoarthritis, osteoporosis, visual impairment, and musculoskeletal conditions during the study period, whereas consistent users of either EF or SS were less likely to use sedatives or sleeping medications compared with nonusers and intermittent users of either program. EF users had a mean attendance of 65 times in the year (median of 67), while SS users’ mean attendance was 51 times in 2011 (median of 33). Survival curves  suggest that consistent EF users had the greatest proportion of the sample remaining without a medical fall at the end of the study period, followed by intermittent EF users, and, finally, nonusers. Both consistent and intermittent SS groups had similar curves, with both showing reduced time to falls compared with the nonuser group.\n\n【23】In hierarchically adjusted Cox regression models , a decreased risk of medical fall was found for both consistent and intermittent users of EF compared with nonusers, after adjusting for demographics. The same pattern remained in the fully adjusted model, showing consistent EF users to have a 26% decreased risk of a medical fall compared with nonusers, whereas intermittent EF users had a 13% decreased risk. The demographic model for SS use indicated a significant reduction for consistent and intermittent users. However, the full model yielded similar results for intermittent users (7% decrease in fall risk) but not for consistent SS users.\n\n【24】Discussion\n----------\n\n【25】To our knowledge, this is the first study to directly investigate the impact on the risk of medical falls of 2 exercise programs not specific to fall prevention. Consistent use of EF, our proxy for regular participation in the program over several years, was associated with the greatest reduction in risk of a medical fall, lowering risk by 20% to 30%. However, even intermittent use of the EF program also decreased the risk of medical falls. Although this finding corroborates previous evidence suggesting that strength and balance exercises, major components of the EF program, are essential to reducing fall and fall injury risk in older adults , further investigation involving more precise measure and categorization of participation is needed.\n\n【26】The results were less clear for the impact of the SS program. Unadjusted findings were suggestive of moderate risk reduction, but full adjustment yielded significant reduction only for intermittent users. This risk reduction was smaller than those seen for EF but was in the range of a 10% to 15% reduction in medical falls. However, the finding of significant impacts only for intermittent users runs counter to established findings in the literature, which suggest that consistent physical activity yields the strongest health impacts . This result makes drawing conclusions challenging and suggests that misclassification may be obscuring accurate interpretation of SS analyses.\n\n【27】Despite these limitations in the SS analysis, the magnitude of the association observed for SS participants was smaller than that seen with EF, a more structured program that includes balance and strength exercises that research suggests are critical to reducing fall risk . Because of the unstructured nature of the SS program in which participants can use whatever gym equipment or attend any classes they wish, we know very little about the type or intensity of exercise that participants engaged in or whether balance or strength exercises were performed. It is likely that balance exercises were not routinely practiced, as evidence suggests that most older adults do not engage in balance exercise . This likelihood may be part of the reason we did not see stronger associations between SS use and fall prevention.\n\n【28】EF has routinely scheduled classes multiple times per week and a strong social environment, promoting regular attendance at the EF program over many years. Users of the more free-form and independently driven SS program may be more likely to vary in the amount of use. On the basis of data supplied by each program, aggregate use statistics for this sample in 2011 support this differential attendance pattern. This finding suggests not only that EF users tend to use the program more frequently but also that the use pattern in the sample is more normally distributed (meaning that SS may have few very frequent users skewing the mean use statistic). This differential usage pattern may make the consistent and intermittent categories created for our analyses more problematic for SS, as the consistent category would be less likely to strongly parallel regular use. In short, although these results suggest that the EF program is more successful in reducing the risk of medical falls for this population, the nature of SS as an independently driven program with more sporadic attendance patterns lends itself more to misclassification under the participation definitions used in these analyses and makes conclusions about the program’s association with fall-related outcomes less robust.\n\n【29】This study has several limitations. First, it focuses only on falls resulting in medical care, as these appear in the inpatient or outpatient medical record, making it possible to measure without the use of self-report. This outcome definition excludes fall cases for which people do not seek medical attention and any injury truly due to a fall but not reported (and thus not coded) as such in the medical record. Despite this limitation, falls resulting in medical care are of high priority for risk reduction efforts given their adverse personal and societal effects .\n\n【30】Although missing data were not an issue for the outcome of interest, some data were missing for certain covariates, primarily race and BMI. Though these missing data resulted in small reductions in sample size for full adjustment models, they are not believed to have limited power. Participation in EF and SS is voluntary and, therefore, inherently self-selected. People who choose to participate in these programs may be systematically different from those who do not in ways that may affect their fall risk. Therefore, the potential for residual confounding remains. No information about engagement in physical activity outside either program was available for users or nonusers. The inability to adjust for baseline physical activity level in either group is a limitation; however, only 6% to 25% of older adults are estimated to regularly engage in the balance training and muscle-strengthening exercises requisite to fall risk reduction , suggesting that the impact of outside physical activity is likely low. Additionally, the threshold for participation in a given year was only 2 uses in that year, which is not indicative of regular physical activity through these programs. Although this finding limits our conclusions to the impact of 2 or more program uses per year, the low cut point used may have minimized the exposure of the group as a whole. This type of exposure misclassification would be expected to attenuate any association, rather than inflate it, so our results may be conservative. Future investigations should aim to use a continuous measure of participation to address this issue.\n\n【31】Despite these limitations, this study has several strengths. First, all outcomes and comorbidities were based on ICD-9-CM codes in the medical record rather than self-report. This procedure greatly reduces the potential for misclassification of comorbidity and outcome status, lending itself to a higher degree of accuracy in risk estimates. Furthermore, the use of this administrative data allowed for adjustment of many fall-related comorbidities that can be difficult to capture, including a history of gait and balance problems. Additionally, these analyses were based on a large, demographically representative sample, increasing power to detect associations and maximizing the generalizability of findings.\n\n【32】The results of this analysis provide evidence that participation in EF is associated with a reduced risk of medical falls. Furthermore, as hypothesized, this relationship shows a consistent pattern in which the strongest protective association was for consistent users of the program. Participation in SS may provide a moderate degree of fall protection, although findings were inconclusive. Overall, results suggest that evidence-based physical activity programs, particularly EF, should be more widely disseminated into communities not only for their general effects on fitness but also for their likely benefits on prevention of fall-related health care use, an important personal and societal outcome.\n\n【33】Tables\n------\n\n【34】#####  Table 1. Demographic Characteristics of EnhanceFitness Users and Nonusers, Washington State, United States, 2005–2011\n\n| Trait a | Nonusers, n = 55,127 | Consistent, n = 517 | Intermittent, n = 1,578 | _P_ Value b |\n| --- | --- | --- | --- | --- |\n| n (%) |\n| --- |\n| **Mean age, y (range)** | 74.1  | 73.7  | 75.0  | <.001 |\n| **Female** | 30,640 (55.6) | 381 (73.7) | 1,169 (74.1) | <.001 |\n| **Race c** | **Race c** | **Race c** | **Race c** | **Race c** |\n| White | 45,471 (90.6) | 436 (85.5) | 1,356 (88.6) | <.001 |\n| Black | 1418 (2.8) | 21 (4.2) | 60 (3.9) | <.001 |\n| Asian | 2,420 (4.8) | 40 (7.9) | 96 (6.3) | <.001 |\n| Other | 880 (1.8) | 7 (1.4) | 19 (1.2) | <.001 |\n| **Mean BMI, c kg/m 2 (range)** | 28.3 (7.4–77.1) | 26.9 (16.3–45.7) | 27.3 (14.9–49.4) | <.001 |\n| <18.5 | 531 (1.0) | 4 (0.8) | 12 (0.8) | <.001 |\n| 18.5–24.9 | 8,851 (16.1) | 142 (27.1) | 367 (24.0) | <.001 |\n| 25.0–29.9 | 11,142 (20.2) | 145 (28.1) | 428 (27.1) | <.001 |\n| \\>30.0 | 34,603 (62.8) | 228 (44.1) | 771 (48.9) | <.001 |\n| **Smoker c** | 4,359 (7.9) | 22 (4.3) | 59 (3.7) | <.001 |\n| **Charlson score d** | 0.92  | 0.63  | 0.60  | <.001 |\n| Diagnosis in study period | Diagnosis in study period | Diagnosis in study period | Diagnosis in study period | Diagnosis in study period |\n| Diabetes | 10,984 (19.9) | 87 (16.8) | 278 (17.6) | <.001 |\n| Dementia | 1,965 (3.6) | 9 (1.7) | 63 (4.0) | .002 |\n| Walking disorder e | 624 (1.1) | 9 (1.7) | 37 (2.3) | .09 |\n| Osteoarthritis | 22,801 (41.4) | 277 (53.6) | 990 (62.7) | <.001 |\n| Osteoporosis | 873 (1.6) | 22 (4.3) | 63 (4.0) | <.001 |\n| Musculoskeletal condition f | 27,254 (49.4) | 330 (63.8) | 1,151 (72.9) | <.001 |\n| Visual impairment g | 31,894 (57.9) | 371 (71.8) | 1,271 (80.5) | <.001 |\n| Coronary heart disease | 11,265 (20.4) | 99 (19.2) | 371 (23.5) | <.001 |\n| Hypertension | 28,598 (51.9) | 306 (59.2) | 1,053 (66.7) | <.001 |\n| **Use of sedatives or sleeping medication** | 7,106 (12.9) | 43 (8.3) | 208 (13.2) | <.001 |\n| **Fall resulting in medical treatment** | 16,834 (30.5) | 146 (28.2) | 672 (42.6) | <.001 |\n\n【36】Abbreviation: BMI, body mass index; EF, EnhanceFitness.  \na  Unless otherwise specified, traits are described at first enrollment. Analysis of variance (for continuous covariates) comparing trends in the covariate across levels of either EF or SS participation.  \nb  _P_ values correspond to a Pearson’s χ 2  analysis (for categorical covariates) or a 1-wayanalysis of variance (for continuous covariates) comparing trends in the covariate: race (6.1%), BMI (19.7%), and smoking (2.3%).  \nc  The following variables have missing values for some individuals: race (6.1%), BMI (19.7%), and smoking (2.3%). Percentages are approximate.  \nd  Charlson comorbidity score measures comorbidity based on the presence or absence of 19 conditions weighted for severity (continuous).  \ne  Includes difficulty walking (719.7), abnormal gait (781.2), and generalized weakness (728.87).  \nf  Includes arthropathy, rheumatoid arthritis, and joint derangement .  \ng  Includes glaucoma , cataract , macular degeneration (362.50–362.53, 362.55, 362.63), and retinopathy (362.01–362.03, 362.10, 362.11, 362.2, 363.31).\n\n【37】#####  Table 2. Demographic Characteristics of Silver Sneakers Users and Nonusers, Washington State, United States, 2005–2011\n\n| Trait a | Nonusers, n = 55,127 | Consistent, n = 3,953 | Intermittent, n = 9,623 | _P_ Value b |\n| --- | --- | --- | --- | --- |\n| n (%) |\n| --- |\n| **Mean age, y (range)** | 74.1  | 70.0  | 71.5  | <.001 |\n| **Female** | 30,640 (55.6) | 2,377 (60.1) | 5,879 (61.1) | <.001 |\n| **Race c** | **Race c** | **Race c** | **Race c** | **Race c** |\n| White | 45,471 (90.6) | 3,462 (91.6) | 8,571 (92.4) | <.001 |\n| Black | 1,418 (2.8) | 65 (1.7) | 210 (2.3) | <.001 |\n| Asian | 2,420 (4.8) | 202 (5.4) | 388 (4.2) | <.001 |\n| Other | 880 (1.8) | 49 (1.3) | 109 (1.2) | <.001 |\n| **Mean BMI, c kg/m 2 (range)** | 28.3 (7.4–77.1) | 27.9 (16.2–51.7) | 28.6 (10.0–111.0) | <.001 |\n| <18.5 | 531 (1.0) | 20 (0.5) | 31 (0.3) | <.001 |\n| 18.5–24.9 | 8,851 (16.1) | 862 (21.8) | 1,565 (16.3) | <.001 |\n| 25.0–29.9 | 11,142 (20.2) | 1,145 (29.0) | 2,253 (23.4) | <.001 |\n| \\>30.0 | 34,603 (62.8) | 1,926 (48.7) | 5,774 (60.0) | <.001 |\n| **Smoker c** | 4,359 (7.9) | 118 (3.0) | 485 (5.0) | <.001 |\n| **Charlson score (range) d** | 0.92  | 0.57  | 0.61  | <.001 |\n| **Diagnosis in study period** | **Diagnosis in study period** | **Diagnosis in study period** | **Diagnosis in study period** | **Diagnosis in study period** |\n| Diabetes | 10,984 (19.9) | 572 (14.5) | 1,968 (20.5) | <.001 |\n| Dementia | 1,965 (3.6) | 25 (0.6) | 214 (2.2) | <.001 |\n| Walking disorder e | 624 (1.1) | 34 (0.9) | 130 (1.4) | .09 |\n| Osteoarthritis | 22,801 (41.4) | 1,854 (46.9) | 6,034 (62.7) | <.001 |\n| Osteoporosis | 873 (1.6) | 108 (2.7) | 265 (2.8) | <.001 |\n| Musculoskeletal condition f | 27,254 (49.4) | 2,189 (55.4) | 6,893 (71.6) | <.001 |\n| Visual impairment g | 31,894 (57.9) | 2,446 (61.9) | 7,453 (77.5) | <.001 |\n| Coronary heart disease | 11,265 (20.4) | 561 (14.2) | 2,226 (23.1) | <.001 |\n| Hypertension | 28,598 (51.9) | 1850 (46.8) | 5,985 (62.2) | <.001 |\n| **Use of sedatives or sleeping medication** | 7,106 (12.9) | 344 (8.7) | 1,357 (14.1) | <.001 |\n| **Fall resulting in medical treatment** | 16,834 (30.5) | 861 (21.8) | 3,563 (37.0) | <.001 |\n\n【39】Abbreviation: BMI, body mass index; SS, Silver Sneakers.  \na  Unless otherwise specified, traits are described at first enrollment. Values are presented as n (%) unless otherwise indicated.  \nb  _P_ values correspond to a Pearson’s χ 2  analysis (for categorical covariates) or a 1-way analysis of variance (for continuous covariates) comparing trends in the covariate across levels of program participation.  \nc  The following variables have missing values for some individuals: race (6.1%), BMI (19.7%), and smoking (2.3%). Percentages are approximate.  \nd  Charlson comorbidity score measures comorbidity based on the presence or absence of 19 conditions weighted for severity (continuous).  \ne  Includes difficulty walking (719.7), abnormal gait (781.2), and generalized weakness (728.87).  \nf  Includes arthropathy, rheumatoid arthritis, and joint derangement .  \ng  Includes glaucoma , cataract , macular degeneration (362.50–362.53, 362.55, 362.63), and retinopathy (362.01–362.03, 362.10, 362.11, 362.2, 363.31).\n\n【40】#####  Table 3. Risk of a Medical Fall Among EF/SS Users Compared With Nonusers, Washington State, United States, 2005–2011\n\n| Group | N | Crude Model, HR (95% CI) | _P_ Value | Demographic Model, a HR (95% CI) | _P_ Value | Full Model, b HR (95% CI) | _P_ Value |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| EF | Nonusers | 55,127 | 1 \\[Reference\\] | — | 1 \\[Reference\\] | — | 1 \\[Reference\\] | — |\n| EF | Intermittent | 1,578 | 0.90 (0.84–0.98) | .009 | 0.84 (0.77–0.91) | <.001 | 0.87 (0.80–0.94) | .001 |\n| EF | Consistent | 517 | 0.73 (0.62–0.86) | <.001 | 0.71 (0.60–0.84) | <.001 | 0.74 (0.63–0.88) | <.001 |\n| SS | Nonusers | 55,127 | 1 \\[Reference\\] | — | 1 \\[Reference\\] | — | 1 \\[Reference\\] | — |\n| SS | Intermittent | 9,623 | 0.85 (0.82–0.88) | <.001 | 0.92 (0.88–0.95) | <.001 | 0.93 (0.90–0.97) | <.001 |\n| SS | Consistent | 3,953 | 0.83 (0.78–0.89) | <.001 | 0.93 (0.87–0.99) | .03 | 0.95 (0.89–1.02) | .18 |\n\n【42】Abbreviations: CI, confidence interval; EF, EnhanceFitness; HR, hazard ratio; SS, Silver Sneakers.  \na  Model adjusted for age, sex, and race.  \nb  Model adjusted for age, sex, race, and all covariates outlined in Tables 1 and 2.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5acff366-2958-4160-ae70-81648c10f04a", "title": "Serogroup W135 Meningococcal Disease, The Gambia, 2012", "text": "【0】Serogroup W135 Meningococcal Disease, The Gambia, 2012\nMeningococcal disease is endemic to the African “meningitis belt”; outbreaks occur regularly . _Neisseria meningitidis_ serogroup A causes most (80%) cases. However, during 2002–2003, serogroup W135 caused a major epidemic in Burkina Faso (attack rate \\[AR\\] 251 cases/100,000 population) . Thereafter, the incidence of serogroup W135 was low, with isolated cases and a small-scale outbreak in the meningitis belt . In 2010, serogroup A conjugate vaccine was introduced into the African meningitis belt and substantially reduced the incidence of meningitis .\n\n【1】In The Gambia, only 6 serogroup W135 cases were identified during 1990–1995; the most recent case had been reported in 1995 . In 2012, a large epidemic of serogroup W135 occurred throughout the meningitis belt, including The Gambia . Most risk factors identified in the meningitis belt concern serogroup A , and risk factors for serogroup W135 are little studied. Therefore, we report the investigation of this epidemic and the related risk factors.\n\n【2】### The Study\n\n【3】The Gambian Ministry of Health and the Medical Research Council Unit, The Gambia, investigated a serogroup W135 epidemic that occurred during February–June 2012 in the Central River Region (CRR) and Upper River Region (URR). Since 2008, surveillance of invasive bacterial diseases has been ongoing in Bansang Hospital in CRR and Basse Health Centre in URR . The peripheral health centers refer severely ill patients to these health facilities. Three approaches were used to recruit persons with suspected cases of serogroup W135: enhanced prospective surveillance in Bansang Hospital and Basse Health Centre, retrospective case identification from hospital records, and visits to households with confirmed case-patients serogroup W135 to identify other suspected cases. A suspected case was defined as a history of acute onset of fever and any of the following: altered consciousness, inability to eat, neck stiffness, seizures, petechial rash, or bulging anterior fontanel in a child <2 years of age. Cerebrospinal fluid (CSF) and/or blood samples were collected from hospitalized persons with suspected serogroup W135. A confirmed case was a suspected case in which serogroup W135 was identified by culture and/or an antigen-specific test. The alert threshold was defined as \\> 5 meningitis cases per 100,000 persons per week; the epidemic threshold was \\> 10 cases .\n\n【4】The investigation team administered 1 dose of ciprofloxacin to each close contact of confirmed case-patients and provided health information to raise awareness. At the end of the epidemic, The Gambian government deployed the tetravalent meningococcal polysaccharide vaccine.\n\n【5】CSF and blood samples were cultured for bacteria in BACTEC Medium (Becton Dickinson, Franklin Lakes, NJ, USA) and tested for serogrouping by latex agglutination by using BACTEC and Ramel (Thermo Fisher Scientific, Waltham, MA, USA) test kits. Antimicrobial drug susceptibility was tested.\n\n【6】We conducted a matched case–control (ratio 1:1) study to identify risk factors. Healthy controls were matched by age and village with confirmed case-patients, including those who died. Demographic, socioeconomic, and exposure (within 14 days before illness onset) data were collected by using a structured questionnaire. Risk factors were analyzed by conducting bivariate matched and multivariate conditional logistic regression analyses. The Joint Gambia Government/Medical Research Council Ethics Committee approved the study. All study participants or legal guardians provided written informed consent.\n\n【7】During February 1–June 25, 2012, a total of 469 suspected cases were identified, and 114 were confirmed to be serogroup W135. Thirty-one were co-primary or secondary cases in confirmed case-patients’ households. Most (67%) suspected case-patients were <5 years of age, and 56% of cases occurred in male patients. The overall case-fatality rate was 8%.\n\n【8】The overall AR was 111 cases per 100,000 persons but was much higher among younger children . The epidemic threshold was exceeded in the last week of February and continued until April for persons of all ages and until June for children <5 years of age . Among children <5 years of age, the peak AR attained 83 and 47 cases per 100,000 persons per week in CRR and URR, respectively . The epidemic peaked during the high temperature/driest months (March–May) and ended abruptly after the first rainfalls in June .\n\n【9】The most common signs and symptoms among the 113 confirmed serogroup W135 case-patients were weakness (96%), irritability (88%), neck stiffness (81%), and inability to eat (80%). Bulging fontanelle (74%), altered mental status (73%), and seizures (65%) occurred in a slightly lower proportion of case-patients.\n\n【10】Blood and/or CSF samples were collected from 301 (69%) of 438 hospitalized suspected case-patients, of which almost half  were positive for bacterial pathogens. Serogroup W135 was the major pathogen (114 \\[83%\\] of 138); followed by _Streptococcus pneumoniae_ (13%) and _Staphylococcus aureus_ (2%). Common antibacterial drugs used for meningitis were tested on 92 (81%) serogroup W135 isolates, which were susceptible to most of them (ampicillin, 100%; chloramphenicol, 99%; ciprofloxacin, 99%; penicillin, 95%; and tetracycline, 92%) but not to erythromycin (59% susceptible) and trimethoprim/sulfamethoxazole (100% resistant).\n\n【11】We enrolled 106 confirmed case–control pairs. Risk factors identified in the univariate analysis were male sex, students, >4 children 1–5 years of age in the household, contact with a meningitis case-patient, preceding history of respiratory illness (nasal discharge, difficult breathing), and itchy eyes . In the multivariate analysis, male sex (odds ratio \\[OR\\] 1.9; 95% CI 1.0–3.7), contact with meningitis case-patients (OR 4.8; 95% CI 1.3–17.8), difficult breathing (OR 6.8; 95% CI 1.4–33.4), and itchy eyes (OR 4.4; 95% CI 1.3–14.4) remained significantly associated with cases.\n\n【12】### Conclusions\n\n【13】Before the current cases, the most recent sporadic cases in The Gambia were reported in the early 1990s. These cases were part of a larger epidemic in the meningitis belt with a comparable predominance of serogroup W135 followed by _S. pneumoniae_ . After introduction of MenAfriVac (serogroup A conjugate vaccine), incidence and epidemics caused by serogroup A decreased substantially in the meningitis belt . The reemergence of epidemic serogroup W135 in this region requires a strategy for surveillance, epidemic detection and control, and revised vaccination policy.\n\n【14】Serogroup A outbreaks usually affect children >5 years of age and young adults . However, two thirds of the serogroup W135 cases occurred in children <5 years of age, for whom the AR was 5-fold higher than it was for older age groups, similar to the characteristics of the serogroups W135 outbreaks in Burkina Faso  and Niger  . Therefore, the current operational definition of alert and epidemic thresholds, drawn mostly from serogroup A data, should be revised because merging the AR for all age groups may delay, or result in nondetection of, a serogroup W135 epidemic in younger age groups.\n\n【15】Signs and symptoms of concurrent respiratory illness were more prevalent among case-patients than controls; itchy eyes and difficult breathing were associated with disease. The temporal sequence of these signs relative to the occurrence of meningococcal disease was not determined, and whether these factors facilitated the invasion of serogroup W135 carried in the nasopharynx or whether these symptoms were part of the initial serogroup W135 infection before onset of severe disease is unclear. Contact with confirmed serogroup W135 case-patients was a strong risk factor. These results are consistent with information available for the other serogroups and with the route of serogroup W135 transmission through droplet infection .\n\n【16】Our findings suggest that isolation of case-patients and prophylactic treatment of contacts may reduce transmission of meningococcal disease during epidemics. Enhanced surveillance for meningitis is recommended for early detection of epidemics. The occurrence of this large serogroup W135 outbreak suggests that multiserogroup conjugate vaccine should be deployed for control and prevention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bec76a8f-c60e-42c1-a071-22a3884a43a1", "title": "Academic Consortia: Untapped Resources for Preparedness, Response, and Recovery—Examining the Cholera Outbreak in Haiti", "text": "【0】On February 14, 2011, the Southeastern Center for Emerging Biologic Threats, the Southeastern Regional Center of Excellence for Emerging Infections and Biodefense, and the Emory Office of Critical Event Preparedness and Response convened a conference, Academic Consortia: Untapped Resources for Preparedness, Response, and Recovery—Examining the Cholera Outbreak in Haiti . The conference, one of a series hosted by Emory University (Atlanta, GA, USA), featured discussions of the response to and lessons learned from the 2010 cholera outbreak in Haiti and examined the role that US academic institutions can play in public health emergencies. The ongoing cholera outbreak in postearthquake Haiti  provided a framework for the conference. The objectives were to 1) explore challenges and identify gaps in responding to disasters and complex humanitarian emergencies (CHEs), 2) determine how consortia of academic institutions, with their range of resources, can augment planning, preparedness, response, and recovery efforts by emergency response agencies (e.g. United Nations and US government agencies), 3) explore the use of emerging technologies to augment responses, and 4) assess the contribution of workforce competencies in response and recovery. Challenges and opportunities faced during the cholera epidemic were highlighted, and lessons learned that could be broadly applied to other emergency settings were identified.\n\n【1】The opening speaker (Christopher Howard) discussed the differences between disasters and CHEs. Disasters are ecologic breakdowns in whichmost deaths result from trauma, whereas a CHE involves a complete breakdown of authority that goes beyond the mandate or response capacity of any single country or United Nations agency . Epidemics have been more frequently reported in association with CHEs; during 1995–2004, a total of 63% of large CHEs had >1 epidemic, compared with 23% of large disasters . The increased likelihood of epidemics during CHEs has been attributed to their longer duration and their larger, more prolonged population displacements. Understanding local context and patterns of endemic diseases is critical to organizing an appropriate response during a health crisis.\n\n【2】After a historical overview of cholera, presenters identified challenges encountered during the outbreak in Haiti. Foremost was the rapid, uncontrolled spread of cholera after its introduction because of preexisting factors exacerbated by the earthquake, such as deficient health care and sanitation systems, population displacement, poor or nonexistent clinical and public health infrastructure, and lack of political will. The situation was further aggravated by the lack of the availability of, acceptance of, and experience with using oral rehydration solution (ORS) therapy. The Haiti experience reinforced the need to rapidly prepare and train deployable teams in the United States before departure and locally and to provide guidance for provision of intravenous fluids and ORS.\n\n【3】The cholera outbreak is a reminder of the potential for infectious disease emergence during CHEs and disasters. Establishing infectious disease surveillance during CHEs and disasters—with an emphasis on diseases with outbreak potential, such as measles, other vaccine-preventable diseases, respiratory and enteric diseases (including cholera), malaria, and dengue—is crucial. At the conference, the necessity of establishing appropriate infection control protocols and creating a safe environment for staff and patients was emphasized. Several challenges and gaps in US workforce competencies were identified. Training was needed before responders arrived in the disaster setting; efficient ways to verify their credentials needed to be identified; and challenges in monitoring relief worker performance needed to be met. Steps would need to be taken to ensure that responders are prepared to minimize their risk for infections and trauma.\n\n【4】In keeping with the sustainability theme, presenters emphasized the need for partnerships with other organizations and education of in-country personnel by using a train-the-trainer approach. Understanding the target audience’s knowledge and knowledge gaps is crucial for creating an efficient and relevant educational model. The Haiti experience also highlighted the need for translators in the United States and in Haiti who had experience in disaster response and could interpret for patients and family members and deliver health education messages to communities. Academic institutions with activities and programs in Haiti before the earthquake were best positioned to contribute to the epidemic response.\n\n【5】Research priorities include development of user-friendly telemedicine programs; community-based research leading to sustainable outcomes; optimization of the use of modeling to project the course and magnitude of the epidemic and to help prioritize interventions (such as antimicrobial drug therapy and vaccination); conduct of molecular epidemiologic assessment of epidemic strains; identification of better early warning methods and systems; assessment of the acceptability, feasibility, and potential effectiveness of cholera vaccination; identification of better data collection and management approaches; and optimization of the availability of appropriate diagnostic tests, vaccines, and antimicrobial agents in urban setting and remote areas. Lack of funding was cited as the primary constraint on research; universities have the expertise but need rapid access to financial resources (e.g. short-term emergency grants) to support emergency responses.\n\n【6】Academic institutions and consortia have many opportunities to contribute to emergency responses and recovery through professional and public education, research, clinical care, and public health practice. Expertise exists, not only in infectious diseases, but also in business, management, engineering, logistics, and risk communication. Experts in these disciplines, in concert with behavioral and social scientists; water, sanitation, and hygiene experts; mental health professionals; and nutritionists can collaborate on multidisciplinary teams to augment the public health workforce. In addition, academics may have access to areas where foreign response agencies are not welcome.\n\n【7】In summary, the conference identified challenges and opportunities for US academic institutions and consortia to assist in preparedness, response, and recovery efforts. Although limited funding handicaps research into methods to improve health during response and recovery, the need for sustainability of interventions also is critical for strengthening health systems. Conference participants agreed that continued collaboration and development of long-term relationships are essential for coordination and avoidance of duplication of efforts. Development of distance learning courses sponsored by a consortium may help improve training of relief personnel and support for multidisciplinary teams during emergency responses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "48ef2acc-a74e-4510-b407-4c2a8a9e6f6a", "title": "Disseminated Mycobacterium abscessus Infection and Showerheads, Taiwan", "text": "【0】Disseminated Mycobacterium abscessus Infection and Showerheads, Taiwan\n**To the Editor:** Diseases caused by nontuberculous mycobacteria (NTM) in patients with Sjögren syndrome have rarely been reported . In addition, showerheads in residential bathrooms as a source of _Mycobacterium abscessus_ –induced disseminated disease have never been reported .\n\n【1】A 65-year-old woman with Sjögren syndrome sought treatment at National University Taiwan hospital with fever (38.6°C) and a 3-month history of lymphadenopathy over the left neck, left submandibular, and bilateral inguinal areas. Active Sjögren syndrome with lymphadenitis was considered because of progressive hypergammaglobulinemia (IgG 3,030 mg/dL, reference range 700–1,600 mg/dL) and high titers of anti–Sjögren syndrome (SS) A (561 AU/mL) and anti-SSB antibodies (220 AU/mL; positive >120 AU/mL). A chest radiograph obtained 1 month before admission showed no active lung lesions; however, cultures of 3 samples of sputum all yielded _M. abscessus_ bacteria (isolate A). Pathologic examination of excised lymph nodes of the bilateral inguinal area showed reactive lymphoid proliferation and granulomatous inflammation with multinucleate giant cell formation, suggestive of mycobacterial disease; however, there was no evidence of caseating necrosis or acid-fast bacilli. ELISA results were negative for antibodies to HIV-1, HIV-2, HTLV-1, and HTLV-2.\n\n【2】Parenteral antimicrobial drugs (imipenem, 500 mg every 8 h) and amikacin (250 mg 2×/d) along with oral clarithromycin (500 mg 2×/d) were administered. Fever subsided 3 days after lymph node excision with concomitant administration of antimycobacterial agents. The patient was treated successfully with intravenous antimicrobial drugs for a total of 14 days, followed by oral clarithromycin (500 mg 2×/d) and doxycycline (100 mg 2×/d) therapy for 4 months. Follow-up blood cultures 10 weeks after initiation of antimycobacterial agents were negative for the organism. _M. abscessus_ bacteria grew on cultures of the excised lymph nodes (isolate B) and 2 sets of blood cultures (isolate C) _._\n\n【3】A total of 6 swab specimens taken from the interior surface of the showerheads from the 6 bathrooms of the patient’s 2 houses (3 in each house), 1 in Taichung (central Taiwan) and the other in Taipei (northern Taiwan), and 6 shower water samples of the 6 bathrooms were submitted for mycobacterial cultures. Four of the 6 swab samples (isolates D–G), 2 (isolates D and E) from Taichung and 2 (isolates F and G) from Taipei, grew _M. abscessus_ bacteria. Cultures of shower water from the 6 bathrooms were all negative for the organism. These isolates were identified as _M. abscessus_ by conventional biochemical methods and confirmed by 16S rRNA gene sequencing analysis and a PCR–restriction fragment length polymorphism–based method targeting a 439-bp fragment of the 65-kDa HSP gene as previously described . Random amplified polymorphic DNA patterns of these isolates (isolates A–G) as determined by means of arbitrarily primed PCR using 3 different random primers were identical (i.e. they shared every band) . Three unrelated isolates of _M. abscessus_ recovered from cutaneous lesions of 3 patients who were treated at the same hospital in 2010 had distinct random amplified patterns that differed from those generated from isolates A–G .\n\n【4】A previous study in Taiwan showed that the incidence  of all pulmonary disease caused by NTM increased significantly from 2.7 (1.26) in 2000 to 10.2 (7.94) in 2008 . The most common organism in localized pulmonary infection and disseminated infection was _Mycobacteriam avium_ cellular complex, and _M. abscessus_ predominated in skin and soft tissue infection and lymphadenitis . The rise in pulmonary infections or colonization by NTM over recent decades, particularly among immunocompromised populations, is reported to be partly associated with the increased use of showers . Recently, a few studies have shown a link between pulmonary _M. avium_ complex infections and home showerhead water microbiology . Although pulmonary disease caused by _M. abscessus_ did not develop in the patient reported here, multiple respiratory specimens showed evidence of pulmonary colonization. The fact that cultures of the swabs taken from the interior surface of 4 showerheads were positive for _M. abscessus_ but that cultures of the shower water were negative for the organism support previous findings that assemblages of NTM can occur inside biofilm that forms on the interior surface of showerheads . The same strains of _M. abscessus_ isolated from different showerheads suggested the possibility of contamination in the environment by the aerosolized microorganism from respiratory secretions of the patient.\n\n【5】The mechanisms of susceptibility to mycobacterial infection in the patient with Sjögren’s syndrome remain unknown . Previous studies suggest that toll-like receptor 2, dectin-1, tumor necrosis factor–α, interferon-γ, leptin, T-cells, and possibly neutrophils are major components in the host defense of HIV-noninfected patients against rapidly growing mycobacterial infections, including those caused by _M. abscessus_ .\n\n【6】In summary, we report a case of bacteremic lymphadenitis caused by _M. abscessus_ in a patient with Sjögren syndrome. Our data provide evidence that the interior surface of showerheads may serve as a source of infection by this waterborne and aerosolized microorganism.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e0f2cf97-982b-44df-9cf4-a5dba504a187", "title": "Building Social Networks for Health Promotion: Shout-out Health, New Jersey, 2011", "text": "【0】Building Social Networks for Health Promotion: Shout-out Health, New Jersey, 2011\nBackground\n----------\n\n【1】Disseminating preventive health information through informal personal networks can reach at-risk community members and may create social networks that reinforce healthy behaviors in high poverty areas . When community members define the relevant health-related topics and take part in constructing and disseminating materials, the results are effective messaging and “community relevant” efforts to reduce health disparities . Community choice enhances the local relevance and contributes to an ecologic approach to health promotion .\n\n【2】Community-participatory and peer-led efforts for reducing health disparities benefit the community . In addition, community-participatory methods of disseminating preventive health behavior information may benefit community change agents. Opportunities for community members to learn about health and to master skills, such as providing health promotion, promotes self-efficacy . Self-efficacy can be seen as integral for health self-care with consequent health improvement or prevention. For example, if a woman living with human immunodeficiency virus (HIV) infection improves her self-care to include medication adherence, she will improve her immune status and health, and can decrease HIV transmission . Fostering skills to promote community health behaviors may empower change agents to improve their own health behaviors and increase the social impetus for them to do so because of increased pressure on change agents to “practice what they preach.” Therefore, a program that creates opportunity for community members to help others should prove mutually beneficial to participants and their communities. We developed the Shout-out Health program to engage women in community-designed relevant health promotion outreach.\n\n【3】Community Context\n-----------------\n\n【4】In 2011, our academic–community agency partnership between Rutgers University (formerly the University of Medicine and Dentistry of New Jersey) and Hyacinth AIDS Foundation tested the feasibility of women at risk for or living with HIV infection to provide health promotion within their informal social networks through the Shout-out Health program. Because HIV/AIDS trends in the United States depict concentrated “hot spots” — geographic areas where HIV/AIDS is the leading cause of death among women of color, with a significantly higher proportion of HIV infections occurring in areas with high poverty rates  — we tested the feasibility of Shout-out Health in Paterson and Jersey City, New Jersey. We chose these 2 because more than a third of residents in both cities are racial/ethnic minorities; 16% of residents in Jersey City and 27% of residents of Paterson earn an annual income below the federal poverty guidelines . Our academic community partner team consisted of members from a state university with experience in HIV care and treatment and a community-based agency that provides HIV prevention and treatment services within the targeted communities.\n\n【5】The objective of the Shout-out Health intervention was to offer opportunity and support for high-risk community members to identify, create, and provide health promotion messaging within their informal personal networks. Community engagement objectives for the project included developing the academic–community agency partnership to facilitate Shout-out Health and developing a community-driven health promotion approach based on the local realities of women at risk for or living with HIV infection.\n\n【6】Methods\n-------\n\n【7】The academic–community partner team developed the Shout-out Health program procedures. The François-Xavier Bagnoud Center Community Advisory Board reviewed the program, and the Rutgers University (formerly the University of Medicine and Dentistry of New Jersey) Institutional Review Board approved the program. Our community agency team members provided advice about the 2 cities to target for the Shout-out Health intervention, and the inclusion criteria included English-speaking, adult women. We recruited women from the community agency client base. The community agency serves women at risk for or living with HIV infection. The community agency definition for women at risk for HIV infection includes self-reported active engagement in behavior that places them at direct risk of HIV infection, such as needle sharing and unsafe sex. Community outreach workers recruited women and invited them to a meeting with the program team to learn more about participating in Shout-out Health.\n\n【8】Sixty-five women from the 2 cities consented to take part in Shout-out Health. Fifty-seven (87%) of these women completed both pretest and posttest questionnaires. Of the 8 participants who chose to stop participating, 5 were from the in-person meeting group and 3 were from the online meeting group.\n\n【9】Of the 57 women who completed Shout-out Health, 38 chose to take part in the in-person meeting group and 19 chose to meet in the online group. We offered the choice of the online or in-person meeting approach because we had already feasibility-tested Shout-out Health in Newark, New Jersey, with 14 women randomized to an online or in-person meeting group and there were no differences in participation by meeting approach. Therefore, for the participant’s convenience, they could choose the meeting format of preference but could not move back and forth between the 2 approaches.\n\n【10】Most participants were racial or ethnic minorities, and because of the nature of discussion about health self-care, they self-identified as HIV-infected. We collected data about psychosocial factors, indicating that the participants experienced substantial life challenges: 66.1% had at least a high school diploma; 76.8% were unemployed; 60% had problems with housing sometimes, most, or all the time; and 22.4% reported that they experienced a low mood or depression most or all the time.\n\n【11】We based the meeting process on empowerment education principles . Steps in the process were as follows:\n\n【12】1.  Identification of health issues each participant felt were important to women in their community. Women’s opinions of important health issues were not prioritized, though the groups discussed each topic when the participant explained why the health topic was important.\n2.  Discussion and development of the health promotion topic, including materials for distribution. Participant knowledge and information for dissemination were evaluated for accuracy and supported by the academic–community agency team.\n3.  Development and discussion about how the participant would provide health promotion in the community. This included role-playing and discussion about the participants’ personal action plans, strategies, and tracking plans for health promotion encounters in the community. We defined an encounter as a participant’s interaction with a community member to promote the health topic. Participants were not required to conduct any particular number of health promotion encounters.\n4.  Participants provided health promotion within their informal social networks and communities for 1 and one-half months.\n5.  Participants met once after the health promotion in the community concluded to discuss and report on the community health promotion experiences.\n\n【13】We designed this 5-step intervention process , with 2 groups: a traditional, in-person meeting group and an online meeting group. The in-person meeting group convened 4 times for 2 hours, over a 3-month period. During the same period, women in the online group worked asynchronously except for 2 conference calls. All participants provided health promotion outreach on their selected topics to women in their informal social networks over a 5- to 6-week period. Women received gift cards as reimbursement for their time, travel, and child care expenses: $50 at step 2 and $100 at each of step 3 and step 5.\n\n【14】Health promotion encounters were self-reported in activity logs and by accounting for print materials that were distributed. Encounter quantity was the primary productivity outcome measure, though the qualitative analysis of the participant’s discussions about health promotion encounters provided information about the context, time, and type of encounters. Quantitative preprogram and postprogram questionnaires included an empowerment score from an empowerment scale developed by Rogers et al , and participants reported preventive health behaviors at baseline and step 5. All meetings were audio-recorded for qualitative analysis. The 21-item survey has 9 questions about various self-care behaviors such as Papanicolaou screening and dental care; 5 questions related to influences on self-care behavior such as food security, substance abuse, and housing; 2 questions about health care provider relationships; 1 question about caring for children or families (a broad gender role and societal expectation); and 4 questions about demographics such as employment and level of education.\n\n【15】The second part of the pre-post questionnaire consists of the empowerment scale for assessing personal and collective empowerment outcomes . Rogers et al developed and validated the Empowerment Scale in a mental health population; we selected the tool because it consists of 4 domains that captured outcomes of interest for this program (self-esteem/self-efficacy; power/powerlessness; community activism and autonomy; and optimism or control over the future), and we did not find an alternative tool for the general community in the literature. We did not revalidate the tool with our population, nor did we calculate scores for participants who skipped an item in the empowerment scale. We used descriptive statistics, _t_ tests, and nonparametric tests to examine overall changes and group differences in the pretest and posttest Empowerment Scale score and for self-care behavior changes. Two team members sorted, coded, and analyzed the qualitative data for themes.\n\n【16】Participants also requested that the recipients of their health promotion evaluate the encounter, and we provided an information sheet and a telephone number to call us to complete the evaluation by telephone.\n\n【17】Outcome\n-------\n\n【18】The health promotion provided by the 57 participants reached 5,861 people in the 2 cities. The number of outreach encounters ranged from 1 to 450, with a mean of 103 outreach encounters per participant. We saw no significant differences in the productivity by group meeting format.\n\n【19】Participants identified 14 different topics as important for health promotion within their informal social networks; the most frequently chosen health topic was HIV and sexually transmitted disease (STD) prevention (n = 12), followed by depression prevention and treatment (n = 11), and substance abuse treatment (n = 9). Other topics chosen by 5 or fewer participants were stress management, discussions about the choices we make about health, smoking cessation, breast cancer screening and awareness, a church member health needs assessment, nutrition and food acquisition, family violence prevention, senior health, sleep hygiene, services for women who were recently released from prison or women living with HIV infection, and communicating with health care professionals. Nine participants in 1 city worked in groups of 2 or 3 to deliver their health promotion in the community, and the remaining 48 developed an individual approach to community health promotion for their topic.\n\n【20】Most of the participants targeted specific locations for their health promotion efforts, such as streets with active drug use or prostitution. One participant characterized the area she went to provide health promotion to friends about substance abuse treatment as the “devil’s mouth” — a street formally known to the participant. Likewise, many other participants targeted community members with known problems. Furthermore, because of their relationships within the community they also provided information to the targeted women’s relatives to attain family support for an issue.\n\n【21】Other action plans included striking up conversations at the beauty parlor, community events, family gatherings, and other community locations. Much of this health promotion deliberately capitalized on existing informal social networks and community connections — for example, an older Hispanic woman developed a plan to help her Hispanic peers learn to communicate more effectively with their children or grandchildren about HIV and STD prevention.\n\n【22】The individual contact for health promotion was widely variable and innovative. One participant provided information about how to conserve food dollars to people waiting in line at soup kitchens and while people dined at soup kitchens. Another participant gathered her group for a discussion about treatment of depression by providing text messaging outreach. One participant provided information and facilitated discussion about family violence prevention during a retail store staff meeting; this led to other retail store owners requesting that this participant provide information at their staff meetings. Four participants distributed information at health fairs; 1 of these participants convened her own health fair and arranged for free US Public Health Service health information materials to be sent to her senior center. She drew women to the event by offering manicures, and after the health fair she set up a schedule to remind herself monthly to complete the online order forms to have health information sent to her senior center regularly. This participant arose as a natural leader in the group, and she facilitated participant involvement and community health promotion even after she completed the Shout-out Health program.\n\n【23】### Box. Participant Quotations Illustrating the 2 Most Prominent Empowerment Themes, Shout-out Health, New Jersey,\n\n【24】**  \nCommunity Unity and Activism**\n\n【25】“I felt — I felt great about it. It was okay because it was some stuff in here like I really didn't know about. And then to pass it on to somebody else that probably didn't know about it, I felt good about it.”\n\n【26】“I'm so grateful for everybody, because I wish if I would’ve known the way I’m teaching my kids, my children in the young community about this disease, probably I wouldn't be HIV positive if I would’ve knew this stuff, what I know now.”\n\n【27】“There were about 40 people there and we spoke about nutrition and then everybody put their piece in. It was a wonderful meeting. But after that, the word spread. I was invited to a women’s group.”\n\n【28】“We all came together to make a difference in the community.”\n\n【29】“The project was real women taking action and helping others just like themselves.”\n\n【30】“For some women health and wellness takes third or fourth place when faced with more pressing social issues such as poverty, unemployment, etc. This community campaign helped encourage women to take better care of themselves.”\n\n【31】“Knowledge is power and by sharing ideas we become more informed.”\n\n【32】**Self-efficacy and Confidence**\n\n【33】“I like that I created my own ideas and used them to help parents and teens become more aware.”\n\n【34】“The experience of brainstorming a message of our own choice and the ability to share that message with others was amazing.”\n\n【35】“Participating in the group had helped me to stay focused and committed on my own journey to health and fitness.”\n\n【36】“I found out information I did not know before and was able to relate to people. Before this, I thought I was on my own.”\n\n【37】“I like the fact that I got to help out people in need and got to talk to people and make new friends that I didn’t have before. It also helped boost my confidence to approach people and voice my information in a professional manner.”\n\n【38】“I enjoyed myself and had a lot of fun. This gave me confidence I did not have before and also it was an opportunity of a lifetime to do something for the community.”\n\n【39】The preprogram and postprogram questionnaires revealed no significant change in empowerment score from baseline to postintervention overall, by group meeting format, or by city . However, the participants we qualitatively categorized within self-determination theory constructs as autonomously motivated had significantly more encounters (mean \\[SD\\], 127 ) than participants in the controlled motivation group (51 ; _P_ < .001). Self-determination theory holds that autonomous motivation for an activity arises from eagerness because of a sense of importance, whereas controlled motivation for an activity is due to the compensation or expectations of others . In addition, 4 themes coded by 2 team members with 100% agreement during content analysis of the meeting transcripts and online discussion blogs arose: community unity and activism, self-efficacy and confidence, perceived power, and optimism. Examples of the passages exemplifying the 2 most prominent themes of community unity and self-efficacy are in the Box.\n\n【40】We observed no significant changes in participant’s self-care of health behaviors preprogram to postprogram. However, themes related to participants’ increased knowledge and awareness about preventive health arose in the content analysis of meeting transcripts and online written dialogue. We coded 27 passages with 100% agreement as indicative of increased self-care knowledge and awareness. Example passages follow:\n\n【41】> “I liked that I was able to participate in helping women with their health and I also learned valuable info for myself.”\n> \n> “By helping women in the community I am motivated to help myself with the information that was discussed and provided in the folder that was handed out in the beginning of the research. I was given an opportunity to read the information and educate myself on women’s health issues. Some of which I knew about, others I did not know more about.”\n\n【42】Seventy-four community members who received health promotion information from participants called the study team to evaluate the encounter. Close to half of these evaluation respondents (n = 31) were friends of the participant, 8 respondents were family members, 8 were classmates, and 8 were other. The remaining respondents were strangers, neighbors, or church members. These evaluations indicated that the recipients of the health promotion found the information to be helpful, and the evaluations provided us with preliminary information about the participant’s social networks.\n\n【43】Interpretation\n--------------\n\n【44】The primary objective of Shout-out Health was to offer opportunity and support for community members to develop and provide health promotion information within their informal social networks in 2 cities with the goal of reducing health disparities. Within 12 weeks, 57 women at risk for or living with HIV infection developed diverse health promotion messages and disseminated them to 5,861 people in their communities, demonstrating the feasibility and effectiveness of the Shout-Out Health program approach. The sheer scope of this health promotion, and women’s abilities to find community members with particular health needs, is noteworthy and has implications for interventions that build social network opportunities for healthy behavior promotion by at-risk community members within hot spot areas.\n\n【45】Other objectives related to community engagement included establishing a strong academic–community agency partnership to facilitate Shout-out Health and a community-driven approach to health promotion. This project’s success is in part attributable to participation from both the academic and community partners. The academic partner procured funding for the project and provided expertise on program design and evaluation. The community agency partner provided the knowledge and awareness of the targeted communities and enabled an approach based on local realities. Both partners participated in the facilitation and management of Shout-out Health and worked together with mutual respect. The final community engagement objective was to create a community-driven health promotion opportunity for women to take part in. Participants identified 14 health issues that they considered important for their community. Many of these approaches are innovative and resulted in sustainable health information dissemination networks as exemplified by the aforementioned description of the woman who set up a calendar and facilitated free US Public Health Service health information to be routinely sent to her senior center for health fairs. Our strong academic–community partnership is related to the success of this community-driven approach to health promotion, but the success is also attributable to the participants’ visions for improving health and reducing health disparities in their communities.\n\n【46】We recruited participants from our community agency partner centers where they serve women at risk for or living with HIV infection. With this targeted recruitment, women participants reported many of the problems experienced by others in their communities — unemployment, housing problems, chronic illness, and depression. Therefore, the personal, informal social networks of these participants included at-risk and difficult-to-reach community members. Participants reported they were close to their target populations, and the 74 evaluations completed by the recipients of the health promotion support this. This is consistent with findings from other community-participatory health promotion efforts where participants deliberately capitalized on existing informal social networks and community connections . These close relationships may foster trust and credibility  and health promotion messages that are relatable because participants understood their social network’s needs and norms. Not only did these participants provide widespread health promotion in their informal social networks but also the recipients of the health messaging were at-risk community members in need of preventive health information.\n\n【47】Shout-out Health provides preliminary evidence for assessing the personal gain of participants in community engagement projects. Findings from the Shout-out Health community project show that given support, discussion, training, and information, at-risk community members can successfully provide health promotion to other high-risk community members and may personally benefit by helping others in their community. Future programs like Shout-out Health should include the evaluation of participants’ personal gains, community-member health outcomes, and the long-term and sustainable effects of building social networks for health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e4335e1d-e79d-42c9-bf56-243cfb4aa6b8", "title": "Identifying and Quantifying Genotypes in Polyclonal Infections due to Single Species", "text": "【0】Identifying and Quantifying Genotypes in Polyclonal Infections due to Single Species\nSimultaneous infection with multiple pathogens of the same species occurs in human patients with HIV, hepatitis C, Epstein-Barr virus, dengue, tuberculosis, and malaria . However, available laboratory methods do not distinguish among pathogen genotypes in samples from individual patients. They do not permit the identification or quantitation of genotypes in samples with multiple pathogens of the same species, or the identification of size polymorphisms produced by insertions and deletions.\n\n【1】Conventional polymerase chain reaction (PCR) with agarose gel electrophoresis permits the identification of pathogen allotypes (strains) in human blood and tissue and an assessment of the sizes of their amplicons but does not define allotype copy number or genotype copy number. Real-time PCR permits identification and quantitation of allotypes  but does not permit the identification of genotypes within allotypes.\n\n【2】From the epidemiologic perspective, a molecular strategy to define the allotypes and genotypes of human pathogens and their copy numbers would permit one to study the dynamics of simultaneous infection with multiple genotypes in ways that have been impossible. For example, this knowledge could be used to identify novel genotypes (size polymorphisms) resulting from insertions and deletions at polymorphic loci.\n\n【3】From the bioterrorism perspective, a strategy to identify size polymorphisms (insertions and deletions) in critical regions of pathogen genomes would be of immense value. This information could be used to test for deletions in regulatory (suppressor) regions and for the insertion of new genes in regions controlled by strong promoters. Available methods do not permit the rapid identification of size polymorphisms within allotypes or the quantitation of individual pathogen genotypes.\n\n【4】To address this challenge, we used real-time PCR and capillary electrophoresis. Real-time PCR with allotype-specific primers permits one to define the allotypes present and their copy number . Capillary electrophoresis permits one to define individual genotypes within allotypes and genotype copy number. The combination of real-time PCR and capillary electrophoresis also permits the identification of insertions and deletions in potentially critical regions of pathogen genomes.\n\n【5】### Materials and Methods\n\n【6】##### Collection of Patient Samples and Isolation of Pathogen DNA\n\n【7】Using fingersticks, filter paper blot samples (S & S #903 Blood Collection Cards, Schleicher & Scheuell Bioscience, Keene, NH, USA) containing 50 μL of blood were obtained from persons with _Plasmodium falciparum_ infection in Mali. These samples were obtained in a prospective study of asymptomatic _P_ . _falciparum_ infection in the village of Missira (160 km northwest of Bamako), after review and approval by the University of Bamako Institutional Review Board (IRB) in Bamako and the Tulane University IRB in New Orleans. Before obtaining informed consent from the participants (after IRB reviews and approvals), the protocol was reviewed with the chief and elders of the village and the women's council. After those additional reviews and approvals, informed consent was obtained from persons >18 years of age and from the parents and guardians of children <17 years of age before obtaining blood samples. DNA was isolated from filter paper blots and blood samples by using the QIAamp DNA Mini Kit (Qiagen, Valencia, CA, USA).\n\n【8】Control template DNA was obtained from cultured parasites  by using the QIAamp DNA Blood Mini Kit (Qiagen). Cloned isolates used as controls for the 4 known allotypes of the polymorphic block 2 region of merozoite surface protein 1 ( _msp1_ ) were Indochina I/CDC for MAD20, Haiti 135 for K1, 7G8 for RO33, and OK/JC 5 for MAD20/RO33 hybrid allotype parasites . DNA template concentrations were estimated from standard curves by plotting the fluorescence of 5 DNA standards with concentrations of 1 μg/mL (1,000 ng/ml), 100 ng/mL, 10 ng/mL, 1 ng/mL, and 0 ng/mL (blank or negative control) vs. the log 10  of template DNA concentration by using PicoGreen dye (Molecular Probes, Eugene, OR, USA) with the VersaFluor fluorometer (Bio-Rad, Hercules, CA, USA).\n\n【9】##### Primer and Probe Design for Real-time PCR\n\n【10】Primers and probes were designed by using the Beacon Designer software, version 2.03, Premier (Biosoft International, Palo Alto, CA, USA) , in combination with manual manipulation. The primers and probes used to amplify the K1, MAD20, RO33, and hybrid (MAD20/RO33) allotypes of the block 2 region of _msp1_ and the internal control gene (erythrocyte-binding antigen 175, _eba175_ ) in _P_ . _falciparum_ are listed in Table 1 , together with information on fluorophores, melting temperatures, final reactant concentrations, and the observed ranges of amplicon sizes. Unlabeled primers and fluorophore-labeled probes were obtained from Integrated DNA Technologies (Coralville, IA, USA), LUX-labeled primers from Invitrogen (Carlsbad, CA, USA); and the Cy5-labeled probe for _eba175_ from Biosearch Technologies (Novarto, CA, USA).\n\n【11】##### Real-time PCR Amplification of Pathogen DNA\n\n【12】Real-time PCR was performed with the iCycler (Bio-Rad) by using the amplification conditions described below  with a 2× multiplex-specific master mix (Qiagen) and 3-μL aliquots of template DNA. Reaction mixtures were supplemented with 2.5 U recombinant Taq polymerase (Invitrogen) and subjected to an initial denaturation at 95°C for 15 min, followed by 45 cycles of denaturation at 94° C for 30 s, annealing at 53°C for 90 s, and extension at 72°C for 90 s. Fluorescence measurements were obtained during the annealing step with TaqMan probes (K1 and MAD20/RO33 hybrid allotypes, and the _eba175_ internal control), and during the elongation step with LUX primers (MAD20 and RO33 allotypes). Each sample was tested in quadruplicate. Two samples were used to define the allotypes present and their copy number with the iCycler; the other 2 samples were removed from the iCycler during the exponential (logarithmic) stage of amplification for capillary electrophoresis to define the genotypes present and genotype copy number.\n\n【13】##### Template Specificity and Optimization of Multiplex PCR\n\n【14】Template specificity was tested for each primer probe set with the 4 control template DNAs (from Indochina I, Haiti 135, 7G8 and hybrid MAD20/RO33 parasites). Amplicons of the expected sizes were obtained with matched template and primer probe sets; no amplicons were obtained with unmatched template and primer probe sets. Negative controls likewise yielded no amplicons. DNA extracted from specimens without parasites was used to control for primer-primer and primer-probe interactions, and other potential causes of false-positive PCR results. After establishing specificity, the reaction conditions were optimized by defining the efficiencies of each primer probe set using a series of 10-fold dilutions with each control template DNA. These efficiencies were then matched to the efficiencies obtained with the multiplex PCR to adjust the final primer and probe concentrations so the efficiencies of the multiplex PCR matched those of the individual PCRs.\n\n【15】##### PCR Amplification and Allotype Quantitation\n\n【16】Standard curves were generated by using 10-fold dilutions of template DNA (3-μL aliquots) from each of the control parasites to estimate the initial copy numbers of the 4 allotypes in each sample. The standard curves (regression lines) for each allotype, the resulting reaction efficiencies, threshold cycle (C T  ) values and estimates of initial copy numbers were calculated by using the iCycler Software (Bio-Rad).\n\n【17】##### Capillary Electrophoresis and Genotype Quantitation\n\n【18】To estimate amplicon size (base pairs) and copy number for each genotype, 2 replicates were removed from the iCycler for each sample during the logarithmic amplification stage, as determined by real-time relative fluorescence unit (RFU) data, and stopped with 0.5 mol/L EDTA. For each reaction, two 1-μL aliquots of the real-time PCR reaction mixture were loaded onto a DNA 500 Lab Chip (Agilent Technologies, Waldbronn, Germany) and run on the Bioanalyzer 2100 (Agilent Technologies), according to the manufacturer's instructions. With capillary electrophoresis using the DNA 500 Lab Chip, a linear relationship was shown between amplicon size and elution time ( _r 2 _ \\>0.998, p<0.001 for amplicons from 25 bp to 400 bp; data not shown). The copy numbers for each genotype were calculated from the molarities provided by the Agilent software. These calculations are based on the observation that the concentration of each amplicon is proportional to its peak area on the electropherogram.\n\n【19】### Results\n\n【20】##### Real-time PCR To Identify Pathogen Allotypes\n\n【21】Real-time PCR with allotype-specific primers permits the amplification of individual allotypes in specimens from infected human subjects (first 3 columns of Table 2 , and Figure 1 , panel A). Based on control specimens containing only 1 allotype and on negative controls, this strategy is specific. Based on filter paper blots for specimens containing >100 parasites/μL, it is sensitive. However, real-time PCR with allotype-specific primers does not distinguish among (identify) genotypes within allotypes . This is because real-time PCR cannot identify size polymorphisms, whether they result from natural events such as the spontaneous addition and deletion of tripeptide repeats in malaria parasites  or deliberately malevolent manipulation of microorganisms in the laboratory as potential agents of bioterrorism.\n\n【22】##### Optimization of Real-time PCR\n\n【23】Estimates of efficiency (the degree to which replication increases the number of amplicons by the expected 2-fold increment \\[100% efficiency\\] in each cycle) indicate that the efficiency of the real-time PCR assays performed in these studies was excellent (90%–100%). In addition, efficiencies of reactions in multiplex did not differ significantly from individual reaction efficiencies, or from other reaction efficiencies in multiplex.\n\n【24】##### Reproducibility of Copy Number (Threshold Cycle, C T  ) Estimates\n\n【25】Data for estimates of copy number were based on the amplification of block 2 of _msp1_ from _P_ . _falciparum_ malaria parasites . The reproducibility of C T  estimates was examined separately for exemplary 93- and 154-bp amplicons, and found to have means of 27.99 and 28.62 cycles, respectively, with standard deviations of 0.34 and 0.13 cycles (i.e. coefficients of variation \\[CVs\\] of 1.2% and 0.5% for these 2 amplicons, n = 10 for each).\n\n【26】##### Capillary Electrophoresis To Identify Pathogen Genotypes\n\n【27】In contrast to real-time PCR, which identifies only allotypes, capillary electrophoresis identifies genotypes within allotypes (based on size polymorphisms) in samples from persons . Across participants (in groups of samples), this method permits one to identify the spectrum (range) of genotypes in the population (data for samples from 10 persons infected with _P_ . _falciparum_ are presented as an example in Figure 2 and Table 3 ). The reproducibility of capillary electrophoresis is sufficient to separate amplicons that differ by >5 bp. This conclusion was based on a comparison of amplicons containing 148 and 153 bp with elution times on electropherograms of 60.51 and 61.11 s, respectively .\n\n【28】##### Reproducibility of Genotype Copy Number Estimates\n\n【29】Based on the electropherograms, the reproducibility of peak area measurements and estimates of genotype copy number was excellent. CVs varied from 0.13% to 0.45% for amplicon concentrations between 10 nmol/L and 80 nmol/L (n = 12 replicates at each of 4 template concentrations of a 95-bp amplicon from 10 nmol/L to 80 nmol/L, data not shown).\n\n【30】##### Real-time Fluorescence in Relation to Peak Area\n\n【31】The slopes of increasing fluorescence based on real time PCR with the iCycler were indistinguishable from the increasing peak areas on the electropherogram . The similarity of these slopes (based on different parameters) indicates that increases in RFUs are directly proportional to increases in amplicon concentration (molarity). This result permits one to extrapolate from allotype copy number to genotype copy number based on peak area.\n\n【32】##### Field Samples from Persons with Polyclonal Infections\n\n【33】Three of the 4 known _P_ . _falciparum_ allotypes have size polymorphisms within block 2 of _msp1_ . K1, MAD20, and hybrid MAD20/RO33 allotype parasites have size polymorphisms because they contain tripeptide repeats within block 2 of _msp1_ ; RO33 does not have size polymorphisms because it does not have tripeptide repeats . These size polymorphisms are evident for K1 in a sample from a single person  and for K1, MAD20, and hybrid MAD20/RO33 parasites in samples from 10 persons .\n\n【34】### Discussion\n\n【35】##### Simultaneous Infection and Detection of Genetically Modified Organisms\n\n【36】Studies by a number of investigators have shown that simultaneous infection with multiple pathogens (genotypes) of the same species occurs in patients with HIV, hepatitis C, Epstein-Barr virus, dengue, tuberculosis, and malaria  and have identified deletions and insertions (genotypes) due to tandem repeats in cytomegalovirus . Because pathogen genotypes based on insertions and deletions are common, the strategy reported here is potentially applicable to all microbial human pathogens. This complexity of infection is likely to be important in the pathogenesis and transmission of many emerging infectious diseases. For example, epidemiologically and clinically meaningful events such as severe disease and antimicrobial drug resistance are likely to be driven by competition among pathogen genotypes in vivo (by the virulence and antimicrobial susceptibility/resistance determinants of the predominant genotypes) and may also affect transmission.\n\n【37】In addition to block 2 of _msp1_ in _P_ . _falciparum_ , other examples of natural sequence variation detectable by using real-time PCR and capillary electrophoresis (variations >5 nucleotides/bp) include duplications and deletions in the 3´ noncoding regions (NCRs) of dengue  and yellow fever  and insertions and deletions in the _env_ gene of HIV . For _Mycobacterium tuberculosis_ , examples include variation in the tandem repeats within _IS_ 6110 , variable numbers of tandem repeats (VNTRs) , and genomic deletions . For select agents, examples include variation in VNTRs (multiple locus VNTR analysis) in _Bacillus anthracis_ , similar differences in _Yersinia pestis_ , and insertions, deletions, and variation in the inverted terminal repeat region and the coding region of the smallpox virus  .\n\n【38】In addition, disease-producing agents may be modified in the laboratory to increase their virulence or to introduce antimicrobial drug resistance for bioterrorist events . However, available methods are inadequate to rapidly diagnose and quantitate simultaneous infection with multiple pathogens (genotypes) of the same species or identify insertions and deletions in critical regions of pathogen genomes. The results reported here provide a strategy to address these issues based on real-time PCR and capillary electrophoresis.\n\n【39】##### Real-time PCR To Identify and Quantitate Pathogen Allotypes\n\n【40】As demonstrated here and elsewhere, allotype-specific primers permit one to identify the pathogen allotypes in a specimen . In addition, real-time PCR may be used to quantitate the numbers of microorganisms in a specimen. Because the relationship between the number of cycles necessary to reach the C T  and the log 10  of copy number is linear, real-time PCR can be used to estimate the initial amount of template DNA (copy number) .\n\n【41】##### Capillary Electrophoresis To Identify and Quantitate Pathogen Genotypes\n\n【42】In contrast to real-time PCR (in which all amplicons \\[genotypes\\] are examined together in the same well once each cycle), capillary electrophoresis detects the amplicons from each genotype as they pass a fluorescence or absorbance detector. This is accomplished by separating dsDNA amplicons based on their size (base pairs) by using a charged electrical field to drive the dsDNA polyanions to the detector at the anode. Because this separation is driven by the ratio of the electrical driving force to the mass of each amplicon, the rate of movement to the anode is inversely proportional to mass (size in base pairs). Thus, smaller amplicons travel faster and have shorter retention times on the electropherogram .\n\n【43】##### Detection of Artificial-size Polymorphisms\n\n【44】The results reported here demonstrate that capillary electrophoresis is sufficiently sensitive to detect insertions and deletions >5 bp in size. This finding means that capillary electrophoresis is more than sufficiently sensitive to detect biologically significant insertions and deletions in genetically modified organisms . Thus, it provides an open-ended strategy to test for genetically modified organisms, by testing for size polymorphisms at critically important sites in the pathogen genome, e.g. at sites related to pathogenicity (virulence) or antimicrobial resistance.\n\n【45】##### Advantages, Limitations, and Potential Pitfalls\n\n【46】The advantages of real-time PCR followed by capillary electrophoresis are that it can be performed without waiting days or weeks for cultures to grow and that it detects pathogens that do not grow in conventional culture media or under standard conditions . In addition, as noted above , sequence information is enormously helpful in selecting loci within the genome likely to have insertions and deletions and interpreting the results obtained. Although insertions, deletions, and single nucleotide polymorphisms (SNPs) produce detectable changes in melting curves , melting curves are qualitative rather than quantitative. In addition, melting curves alone cannot identify specific insertions, deletions, or quasispecies (SNPs) without the addition of probes for the affected target region of the genome or the use of PCR . Finally, because the strategy reported here tests for size polymorphisms, it does not require prior knowledge of the specific sequences that may have been introduced into (or deleted from) the pathogen genome to identify genetically modified organisms. However, this strategy does have 3 limitations.\n\n【47】First, sequences identical to (or cross-reactive with) host sequences cannot be used as targets because blood and tissue specimens are inevitably contaminated with host DNA (this issue can be resolved by searching the GenBank database). Second, the threshold of detection for genetically modified organisms is the addition (or removal) of sequences >5 bp (based on the sensitivity of capillary electrophoresis), i.e. point mutations (SNPs = quasispecies)  cannot be detected with this strategy. As a result, this method is likely to be of greater value for organisms with dsDNA genomes such as bacteria, eukaryotic parasites, and dsDNA viruses (in which quasispecies are less common because of more accurate replication) than for organisms with single negative-stranded RNA genomes (in which quasispecies are more common because their replication depends on the error-prone reverse transcriptase—HIV, hepatitis C, hepatitis B) . Third, capillary electrophoresis may need to be performed separately for each allotype to avoid confusion between amplicons of similar size from different allotypes .\n\n【48】### Conclusions\n\n【49】The strategy reported here can be used for epidemiologic studies of simultaneous infection with multiple pathogens (genotypes) of the same species in emerging infectious diseases and for the rapid identification of select agents that have been genetically modified to increase their virulence or antimicrobial drug resistance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e0506aab-7f57-458b-b53f-f27553b430f1", "title": "Rickettsia felis in Fleas, Germany", "text": "【0】Rickettsia felis in Fleas, Germany\n_Rickettsia felis_ , the causative agent of the flea-borne spotted fever rickettsiosis, is pathogenic for humans . Since the first detection of _R_ . _felis_ from midgut epithelial cells of the cat flea, _Ctenocephalides felis felis_ , in 1990 , interest in the role of this flea species as its main vector has increased. _R_ . _felis_ has been found in cat fleas on all continents . Because _R_ . _felis_ is not lethal for cat fleas and is transmitted transovarially by these fleas , _C_ . _felis_ could be a vector and a reservoir of this pathogen. For these reasons, the cat flea was considered the only flea species with a major role in the epidemiology of flea-borne spotted fever rickettsiosis. However, _R_ . _felis_ has been reported in other flea species , and flea-borne spotted fever rickettsiosis is now considered an emerging human infectious disease. We analyzed the presence of _R_ . _felis_ in different flea species collected from naturally infested cats and dogs in different locations in Germany.\n\n【1】### The Study\n\n【2】A total of 310 fleas were collected from 49 dogs and 54 cats in 11 widely distributed locations in Germany (Berlin, Munich, Brandenburg, Leipzig, Chemnitz, Rostock/Laage, Bremen, Osnabrück, Münster, Freising, and Schongau)  in 2007. Specimens collected were recorded and kept at –20°C. Samples were shipped on dry ice to our laboratory, and species identification was performed by using light microscopy and following the determination key of Hopkins and Rothschild . Because of infestation variations (1–150 fleas per animal), 3 fleas per animal host were chosen randomly for species differentiation.\n\n【3】Fleas were homogenized individually in 80 μL of phosphate-buffered saline with a RETSCH Tissue Lyser Mixer Mill 300 (QIAGEN, Hilden, Germany) by using 5-mm steel beads. A 100-μL volume of ATL buffer and 20 μL of proteinase K (QIAGEN) were added, and homogenates were incubated at 56°C in an Eppendorf Thermomixer (Eppendorf, Hamburg, Germany) until tissues were completely lysed. DNA was extracted from each flea by using a QIAamp DNA Mini Kit (QIAGEN) according to the manufacturer’s instructions (tissue protocol) and stored at –20°C until used.\n\n【4】PCR amplification of rickettsial DNA was performed by using oligonucleotide primer pairs Rp CS.877p/Rp CS.1258n  generated from the rickettsial citrate synthase ( _gltA_ ) gene. Positive samples were analyzed for a 530-bp portion of the outer membrane protein A ( _ompA_ ) gene with primer pair Rr 190.70p/Rr 190.602n  and for a 765-bp portion of the _ompB_ gene with primer pair 120–1278/120–3599 . PCR amplification was accomplished in 50-μL volumes containing 5 μL DNA, 30 μL distilled water, 10 μL 5× _Taq_ buffer (Roche, Mannheim, Germany), 3 μL 25 mmol/L MgCl 2  (Roche), 1 μL 10 mmol/L dNTP (Roche), 0.25 μL each primer (100 μmol/L), and 0.5 μL _Taq_ polymerase (5U/mL; Roche). Conditions for the _gltA_ and _ompA_ PCRs were as described by Bertolotti et al. Negative and positive controls were included in all PCRs.\n\n【5】All PCR products were separated by electrophoresis on 1.5% agarose gels at 100 V for 60 min and examined under UV light. Positive samples for both genes were purified by using the QIAquick PCR purification kit (QIAGEN) and sequenced by the MWG Biotech Company (Martinried, Germany). Sequences obtained were compared with those of characterized rickettsia in GenBank by using BLAST analysis .\n\n【6】Five species of fleas were identified in the study. The most prevalent species was _C_ . _felis_ (93% of fleas in cats and 78% in dogs) . _Archaeopsylla erinacei_ (hedgehog flea), was the second most abundant species, with 26 specimens collected from dogs and 8 from cats. A few specimens of _Ctenocephalides canis_ (dog flea), _Pulex irritans_ (human flea), and _Ceratophyllus gallinae_ (hen flea) were also identified . Eight dogs had mixed populations of fleas; 5 had _C_ . _felis_ and _A_ . _erinacei_ , 2 had _C_ . _felis_ and _C_ . _gallinae_ , and 1 had _C_ . _felis_ and _C_ . _canis_ . Mixed populations of fleas were also detected in 3 cats; 2 were infested with _C_ . _felis_ and _A_ . _erinacei_ , and 1 with _P_ . _irritans_ and _C_ . _felis_ .\n\n【7】Thirty-six (25%) of 146 fleas collected from dogs and 24 (15%) of 164 fleas collected from cats were positive for the _gltA_ gene. Positive fleas were found in 6 of 11 sampled locations. Proportions of infected fleas collected from dogs ranged from 25% (Berlin) to 56% (Münster), and proportions of infected fleas collected from cats ranged from 10% (Freising) to 100% (Münster) .\n\n【8】Of 60 fleas positive for the _gltA_ gene (for dogs and cats), only 2 were negative for the _ompA_ and _ompB_ genes. Sequencing analysis of the _gltA_ gene for these 2 samples showed that 1 sequence (from _C_ . _gallinae_ ) was 99% homologous with part of the _Rickettsia helvetica_ _gltA_ gene (AM418450.1) from an _Ixodes persulcatus_ tick isolated in Russia; the other sequence (from _C_ . _gallinae_ ) was 94% homologous with the _Rickettsia_ sp. citrate synthase gene (U76908.1). Thus, we report _R_ . _helvetica_ in _C_ . _gallinae_ ticks.\n\n【9】Of the other 58 _gltA_ \\-positive samples, 2 were positive for the _ompA_ gene in the first round; 56 fleas were positive for the _ompB_ gene. The 2 _ompA_ \\-positive samples were sequenced, and sequences matched the _ompA_ gene from _R_ . _felis_ (AJ563398.1; 99%–100% similarity). The 56 positive _ompB_ samples were sequenced, and sequences matched with _ompB_ gene from _R_ . _felis_ (CP000053.1; 98%–100% similarity). All hedgehog fleas (34 specimens) collected were infected with _R_ . _felis_ . Moreover, these 34 specimens were collected from 5 locations within a large area from Berlin (northeastern Germany) to Munich (southeastern Germany). Our findings indicate that _A_ . _erinacei_ may play a major role in the transmission of _R_ . _felis_ in Germany. Recent studies reported _R_ . _felis_ in 1 hedgehog flea in Portugal  and in 4 hedgehog fleas in Algeria .\n\n【10】### Conclusions\n\n【11】Our study confirms that _C_ . _felis_ remains the most common flea species infesting cats and dogs in Germany. Nevertheless, only 24 of 266 cat fleas collected were infected with _R_ . _felis_ . Infected cat fleas were found only in 4 of 11 studied sites, in contrast with a recent study in France, where _R_ . _felis_ –infected _C_ . _felis_ were present in all locations studied . In the 4 positive sites in Germany, 3 had positive _A_ . _erinacei_ specimens and 1 had positive _C_ . _gallinae_ . In the other sites where no positive fleas where found, only _C_ . _felis_ was present either alone or in association with _P_ . _irritans_ and _C_ . _canis_ .\n\n【12】Although _C_ . _felis_ seems to be the main vector of _R_ . _felis_ , our findings indicate that _A_ . _erinacei_ may be a vector for human flea-borne rickettsiosis in Germany. Because hedgehogs may act as a reservoir of pathogens , further studies will be conducted to investigate the role of hedgehogs and hedgehog fleas in maintenance and transmission of _R_ . _felis_ in Germany.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "da921bac-e937-4fde-a5fe-ed8d7c471cf2", "title": "Effectiveness of Seasonal Influenza Vaccine against Pandemic (H1N1) 2009 Virus, Australia, 2010", "text": "【0】Effectiveness of Seasonal Influenza Vaccine against Pandemic (H1N1) 2009 Virus, Australia, 2010\nAfter the emergence and rapid global spread of pandemic influenza A (H1N1) 2009 virus, development of a pandemic (H1N1) 2009–specific vaccine began . A candidate reassortant vaccine virus, derived from the A/California/7/2009 (H1N1)v virus as recommended by the World Health Organization, was used to produce a monovalent, unadjuvanted, inactivated, split-virus vaccine for Australia . The national monovalent pandemic (H1N1) 2009 vaccination program in Australia ran from September 30, 2009, through December 31, 2010, and vaccination was publicly funded for all persons in Australia \\> 6 months of age .\n\n【1】In September 2009, the World Health Organization recommended that trivalent influenza vaccines for use in the 2010 influenza season (Southern Hemisphere winter) contain A/California/7/2009 (H1N1)–like virus, A/Perth/16/2009 (H3N2)–like virus, and B/Brisbane/60/2008 (of the B/Victoria/2/87 lineage) virus . Since March 2010, the Australian Government has provided free seasonal influenza vaccination to all Australia residents \\> 65 years of age, all Aboriginal and Torres Strait Islander persons \\> 50 years, all Aboriginal and Torres Strait Islander persons 15–49 years with medical risk factors, persons \\> 6 months with conditions that predispose them to severe influenza, and pregnant women . Influenza vaccination is also recommended, but not funded, for persons who might transmit influenza to those at high risk for complications from influenza, persons who provide essential services, travelers, and anyone \\> 6 months of age for whom reducing the likelihood of becoming ill with influenza is desired. Individual industries are also advised to consider the benefits of offering influenza vaccine in the workplace . Because pandemic (H1N1) 2009 was expected to be the dominant strain in 2010, the monovalent vaccine continued to be used despite the availability of the seasonal vaccine, particularly by persons who were not eligible for funded vaccine . However, in 2010, there were no published data on the relative use of monovalent and seasonal vaccines at that time.\n\n【2】The need for rapid implementation of programs results in initial studies using immunogenicity, rather than efficacy, to assess performance of influenza vaccines. After 1 dose of monovalent pandemic (H1N1) 2009 vaccine containing 15 µg hemagglutinin without adjuvant, seroprotection was estimated to be 94%–97% in working-age adults  and 75% in children . Observational studies provide a practical way to calculate vaccine effectiveness under field conditions . Effectiveness of monovalent pandemic (H1N1) 2009 was estimated to be 72%–97% by 3 studies in general practice and community-based settings in Europe , 90% in a hospital-based study in Spain , and 100% in a community-based study of children in Canada . These studies were conducted in populations for which the respective local or national pandemic vaccination program primarily used vaccine without adjuvant.\n\n【3】We assessed effectiveness of the 2010 seasonal influenza vaccine against laboratory-confirmed pandemic (H1N1) 2009 influenza infection in Victoria, Australia. Data came from an established test-negative case–control study in a general practitioner sentinel surveillance network .\n\n【4】### Methods\n\n【5】##### Sentinel Surveillance\n\n【6】Victoria is the second most populous state in Australia; it has a temperate climate, and the annual influenza season usually occurs during May–September. Each season, on behalf of the Victorian Government Department of Health, the Victorian Infectious Diseases Reference Laboratory conducts surveillance for influenza-like illness (ILI; defined as history of fever, cough, and fatigue/malaise) and laboratory-confirmed influenza. General practitioners within the network provide weekly reports on case-patients with ILI as a proportion of total patients seen and send swabs from patients with ILI to the laboratory for testing. In 2010, a total of 87 practitioners participated in the program, which operated for 25 weeks, from May 3 (week 19) through October 24 (week 43). Practitioners were asked to collect nose and throat swabs from patients with an ILI  within 4 days after onset of the patient's symptoms. Samples were collected by using Copan dry swabs (Copan Italia, Brescia, Italy) and were placed in virus transport medium. Practitioners were also asked to provide data on the patient's age, sex, date of symptom onset, vaccination status, type of influenza vaccine (monovalent or trivalent/seasonal) received, and date of vaccination. Type of vaccine and date of vaccination were ascertained from medical records and patient report.\n\n【7】##### Laboratory Testing\n\n【8】RNA was extracted from clinical specimens by using a Corbett extraction robot (Corbett Robotics, Brisbane, Australia), followed by reverse transcription to cDNA by using random hexamers. PCR amplification and detection selective for the type A influenza virus matrix gene was performed by using primers and a Taqman probe on an ABI-7500 Fast Real-Time PCR system (Applied Biosystems, Foster City, CA, USA). Samples determined to be positive by this assay were confirmed as positive or negative for pandemic (H1N1) 2009 in a second real-time PCR that incorporated primers and probes specific for the hemagglutinin gene of the pandemic (H1N1) 2009 virus. Influenza B viruses were identified by a separate PCR. One practitioner chose to send samples to the state reference laboratory in South Australia for testing with equivalent diagnostic assays.\n\n【9】##### Ascertainment of Case-patients and Controls\n\n【10】Case-patients and controls were sampled prospectively throughout the study period. A case-patient was defined as a person with ILI for whom test results for pandemic (H1N1) 2009 were positive; a control was defined as a person with negative test results for influenza virus. Analysis of vaccine effectiveness against other influenza subtypes was not undertaken because of the almost exclusive circulation of pandemic (H1N1) 2009 virus during the season; therefore, patients with positive test results for other influenza viruses were excluded. A control could become a case-patient if another illness developed during the season, but a case-patient was no longer at risk and could not be included again.\n\n【11】##### Data Analysis and Calculation of Vaccine Effectiveness\n\n【12】All analyses were conducted by using Stata version 10.0 (StataCorp LP, College Station, TX, USA). The χ 2  test was used to compare proportions, and the Mann-Whitney U test was used to compare time from vaccination to time seen by practitioner; p<0.05 was considered significant. Patients were excluded from the vaccine effectiveness analysis if vaccination status was unknown, if the date of symptom onset was unknown, or if the interval between symptom onset and specimen collection was >4 days (because of decreased likelihood of a positive result after this time) . Patients were considered not vaccinated if time between date of vaccination and symptom onset was <14 days. If only the month of vaccination was reported, the date of vaccination was conservatively estimated to be the last day of the month. To avoid overestimation of vaccine effectiveness arising from recruitment of controls when influenza was not circulating in the population, analysis was restricted to case-patients and controls detected within the influenza season, defined as the period during which influenza-positive case-patients were detected (weeks 26–40).\n\n【13】Vaccine effectiveness was defined as (1–odds ratio) × 100%; the odds ratio is the odds of laboratory-confirmed pandemic (H1N1) 2009 case-patients having been vaccinated divided by the odds of controls having been vaccinated. In the test-negative case–control design, the odds ratio estimates the incidence density (rate) ratio because controls are selected longitudinally throughout the course of the study (i.e. by density sampling) . The odds ratio in test-negative case–control studies has also been shown to approximate the risk ratio under conditions of varying attack rates and test sensitivity and specificity . Logistic regression was used to calculate odds ratios and 95% confidence intervals (CIs) for having laboratory-confirmed pandemic (H1N1) 2009, which were adjusted for the variables of age group and month of specimen collection against the following: seasonal vaccine, monovalent vaccine, both vaccines, and any (either or both the seasonal and monovalent) vaccine. Sensitivity analyses were conducted to determine the effects of the following on vaccine effectiveness: not censoring for specimens collected from ILI patients >4 days after symptom onset, including controls recruited outside the defined influenza season, and assuming that patients with unspecified type A influenza had pandemic (H1N1) 2009.\n\n【14】##### Ethical Considerations\n\n【15】Data in this study were collected, used and reported under the legislative authorization of the Victorian Public Health and Wellbeing Act 2008 and Public Health and Wellbeing Regulations 2009. Thus, the study did not require Human Research Ethics Committee approval.\n\n【16】### Results\n\n【17】A total of 172,411 patients were seen by participating practitioners during the study period, of whom 678 (0.4%) had ILI. After a nadir ILI rate of 0.2% in week 21, the rate gradually increased to 0.4% in week 31 before increasing more sharply to a peak of 0.9% in week 36. Swabs were collected from 478 (71%) ILI patients, among whom 170 (36%) had positive influenza test results and the remainder were negative. Influenza-positive patients were detected during weeks 26–40, which was defined as the influenza season . A total of 142 patients were excluded from further analysis because vaccination status was unknown (n = 11), symptom onset date was unknown (n = 33), time between symptom onset and specimen collection was >4 days (n = 43), or the specimen was collected outside the influenza season (n = 82). A significantly higher proportion of influenza-negative patients (13%) than influenza-positive patients (4%) were excluded because >4 days had elapsed between symptom onset and specimen collection (p = 0.001). No significant difference was found by age group for whether study participants had a specimen collected within 4 days after symptom onset (p = 0.10).\n\n【18】Of the remaining 336 patients, 156 (46%) had positive influenza test results. Most (89%) influenza case-patients had pandemic (H1N1) 2009, 6% had unspecified type A influenza, 4% had influenza A (H3N2), and 1% had influenza type B . After exclusion of the other influenza patients, 139 pandemic (H1N1) 2009 case-patients and 180 controls were included in the study analysis. Most (57%) participants were 20–49 years of age, and case-patients were significantly younger than controls (p = 0.001); no case-patient was \\> 65 years of age . No statistically significant difference was found between male and female study participants by case or control status (p = 0.60) or by vaccination status (p = 0.09). The high proportion of case-patients detected in August resulted in a significant difference between case-patients and controls by month of swab collection (p<0.001).\n\n【19】Overall, 59 (18%) study participants were reported as vaccinated with any vaccine, but the proportion was higher among controls (26%) than among case-patients (9%; p<0.001). The proportion of controls, who were mostly older, who had received the trivalent seasonal vaccine was higher than the proportion of controls who had received the monovalent vaccine . Similarly, controls who had received both vaccines were all \\> 20 years of age. Only case-patients who were 5–19 and 20–49 years of age were reported as vaccinated. Influenza vaccine type was not specified for 1 case-patient and 1 control, each of whom was reported as vaccinated.\n\n【20】Reflecting the availability of each vaccine, the median period between vaccination and visit to a general practitioner was significantly shorter for those who received seasonal vaccine (114 days) than for those who received monovalent vaccine (223 days; p<0.0001). No significant difference in the time from vaccination to practitioner visit was found between case-patients and controls for seasonal (p = 0.70) or monovalent vaccine (p = 0.95).\n\n【21】In general, point estimates of vaccine effectiveness adjusted for patient age and month of specimen collection differed little from crude estimates . A significant protective effect was observed for seasonal vaccine only (adjusted vaccine effectiveness 79%; 95% CI 33%–93%) and seasonal and monovalent vaccines (adjusted vaccine effectiveness 81%; 95% CI 7%–96%). The adjusted vaccine effectiveness for receipt of any (either or both the seasonal and monovalent) vaccine was lower at 67% because of the 47% vaccine effectiveness for monovalent vaccine. The absence of vaccinated case-patients and controls meant vaccine effectiveness could not be estimated for several of the 5 age groups ; therefore, age was collapsed into 3 variables: children (0–19 years), working-age adults (20–64 years), and elderly persons ( \\> 65 years). Estimates of vaccine effectiveness for working adults were 0%–14% higher than the overall adjusted estimates; estimates for children were either undefined because no controls were vaccinated or were without a significant protective effect. Vaccine effectiveness could not be calculated for elderly persons because there were no case-patients in this age group.\n\n【22】Sensitivity analyses to determine the effects of certain assumptions resulted in variations in the adjusted vaccine effectiveness point estimates of 0%–3% and no changes to their relative significance. The effects considered were as follows: assumption that those patients with unspecified influenza type A had pandemic (H1N1) 2009, no exclusion of patients if >4 days had elapsed between symptom onset and specimen collection, and no exclusion of patients if they were identified outside the defined influenza season.\n\n【23】### Discussion\n\n【24】Our results indicate that the 2010 seasonal trivalent influenza vaccine is >80% effective against pandemic (H1N1) 2009 virus, regardless whether given by itself or in addition to monovalent vaccine. Groups in Europe and Canada have estimated the effectiveness of monovalent seasonal influenza vaccine against pandemic (H1N1) 2009 virus to be 72%–100% . However, the effectiveness of any vaccine (monovalent, seasonal, or both) against pandemic (H1N1) 2009 virus was lower (67%, 95% CI 33%–84%) because effectiveness for monovalent vaccine only was 47% (95% CI –62% to 82%). The lower effectiveness of monovalent influenza vaccine against pandemic (H1N1) 2009 virus compared with seasonal trivalent influenza vaccine is difficult to explain. Both vaccines contain the same quantities (15 µg) of hemagglutinin; and although the monovalent vaccine does not contain adjuvant and was available ≈6 months before the seasonal vaccine, it has been shown to be strongly immunogenic . Immunogenicity does not necessarily correlate directly with vaccine effectiveness, and we cannot exclude waning immunity as an explanation for the lower effectiveness of monovalent vaccine in our study. Waning immunity after receipt of monovalent vaccine has been suggested after an interim study from the United Kingdom for the 2010–11 influenza season . The finding could also be a product of the relatively small number of case-patients and controls who received only the monovalent vaccine, given that vaccine effectiveness estimates can change considerably by the inclusion or exclusion of 1–2 vaccinated study participants.\n\n【25】When stratified by age, estimates of vaccine effectiveness for working-age adults were higher and more precise than those for children. We previously demonstrated that the sentinel practitioner surveillance program in Victoria is well suited for estimating vaccine effectiveness among working-age adults, who account for most of the surveillance population , and the 2010 results were consistent with this observation. The relatively few participants in the young (childhood) age groups meant the study had insufficient power to produce defined or significant estimates of vaccine effectiveness. At the other end of the age spectrum, 2% of study participants (5 controls and 0 case-patients) in 2010 were \\> 65 years of age compared with an average of 7% in this age group during 2003–07 . Although the absence of pandemic (H1N1) 2009 case-patients \\> 65 years of age is not surprising, given that older adults have been shown to have relatively higher levels of cross-reactive antibodies to pandemic (H1N1) 2009 virus , the reason for the low proportion of controls in this age group remains unclear. Among the several explanations are a true lower rate of ILI in older persons during 2010, a lower rate of visits to practitioners for ILI by persons in this age group (or treatment at other health services such as hospitals), or preferential sampling of younger persons by practitioners (and perhaps awareness that pandemic \\[H1N1\\] 2009 was the predominant circulating influenza virus subtype).\n\n【26】In addition to having a sample size large enough to provide vaccine effectiveness estimates by age group and influenza type, several other considerations with regard to design of case–control studies of influenza vaccine effectiveness have been proposed: 1) whether the control group best represents the vaccination coverage of the source population and 2) whether collection and confounding variables have been adjusted for, particularly underlying chronic conditions for which vaccine is recommended and previous influenza vaccination history . A 2010 survey of pandemic vaccination suggests that monovalent vaccine coverage in the control group was generally consistent with that in the general population and that use of monovalent vaccine was ≈17% among those from Victoria, compared with 13% among controls . No equivalent survey of 2010 seasonal vaccine usage was available for comparison.\n\n【27】Data about concurrent conditions of study participants that would indicate need for influenza vaccination were not collected during the 2010 influenza season; thus, adjustment of the vaccine effectiveness estimates for this potentially confounding variable could not be conducted. Such confounding by indication (or negative confounding), in which persons at higher risk for influenza are more likely to be vaccinated, underestimates effectiveness of influenza vaccine but may be counteracted by healthy vaccinee bias (or positive confounding), which overestimates effectiveness . The extent to which these biases occur is likely to vary and may explain the positive and negative variation of crude influenza vaccine effectiveness estimates after adjustment for chronic conditions in several similar test-negative case–control studies . Speculation about the relative effects of these biases on how many received monovalent vaccine is also difficult; vaccination was funded for the entire population of Australia, but at the end of February 2010, only 18% had been vaccinated .\n\n【28】Similar methods using test-negative controls to assess seasonal and pandemic vaccine effectiveness against both seasonal and pandemic influenza viruses have been applied in North America and Europe . Observational studies provide a convenient and timely way to assess influenza vaccine effectiveness without the ethical, practical, and financial stringencies associated with clinical trials for vaccine efficacy, but they also have limitations. Modeling suggests that the test-negative case–control design generally underestimates true vaccine effectiveness under most conditions of test sensitivity, specificity, and the ratio of influenza to noninfluenza attack rates , although quantifying the extent of this effect in this study is difficult because the precise sensitivity and specificity of the test are not known. We attempted to limit ascertainment bias by censoring records that indicated specimen collection >4 days after symptom onset and restricting the analysis to case-patients and controls tested within the influenza season only, although sensitivity analyses indicated little effect if these restrictions were relaxed. Of note, these findings apply predominantly to working-age adults receiving medical care in the general practice setting; the study did not include those who did not seek medical care for ILI. Thus, the study measured effectiveness of vaccine against illness severe enough to require a visit to a practitioner; the results cannot necessarily be generalized to other parts of the population, in particular young children and elderly persons. We were also unable to determine whether participants had previously been infected with pandemic (H1N1) 2009 virus, which may result in overestimation of vaccine effectiveness.\n\n【29】In conclusion, we applied a test-negative case–control study design to an established sentinel surveillance system to estimate effectiveness of a trivalent seasonal influenza vaccine, which included an A/California/7/2009 (H1N1)–like virus, the pandemic (H1N1) 2009 influenza virus strain. This strain is also a component of the trivalent influenza vaccine for the 2010–11 Northern Hemisphere influenza season . The trivalent vaccine provided significant protection against laboratory-confirmed pandemic (H1N1) 2009 virus infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3eaf1bd5-2b40-4293-8421-d1319a0fe41d", "title": "Mycobacterial Aerosols and Respiratory Disease", "text": "【0】Mycobacterial Aerosols and Respiratory Disease\nHypersensitivity pneumonitis is an occupational hazard of workers in two different industries, automobile manufacturing (e.g. metal working) and leisure (e.g. indoor swimming pools). Pulmonary illness and infection have also been a consequence of exposure to aerosols generated by hot tubs, spas, and coolant baths. Respiratory problems have also been associated with exposure to water-damaged buildings during reconstruction, and mycobacteria isolated from materials from such buildings have been shown to provoke inflammatory reactions. The outbreaks share the common feature of aerosol exposure and respiratory illness. I propose that exposure to aerosols containing mycobacteria is a common feature of the outbreaks and that mycobacteria or their products could be responsible for the respiratory symptoms.\n\n【1】Epidemiologic studies have established that the workers in such outbreaks were exposed to aerosols generated in the workplace from water that was either a work tool (e.g. metalworking fluid) or an integral part of the workplace or household (e.g. swimming pools and hot tubs) . Outbreaks of respiratory disease occurred in spite of disinfectant treatment of the waters or fluids to reduce the number of microorganisms. Living or working in water-damaged buildings or as a consequence of reconstruction of water-damaged buildings has also been associated with outbreaks of respiratory problems . Respiratory disease has been associated with mycobacteria in reservoirs, aerosols, or structural material in a number of cases .\n\n【2】### Hypersensitivity Pneumonitis in Workers Exposed to Metalworking Fluid\n\n【3】An estimated 1.2 million workers in the United States are exposed to aerosols generated by metal grinding . Metalworking fluids are widely used in a variety of common industrial metal-grinding operations to lubricate and cool the tool and the working surface. Metalworking fluids are oil-water emulsions that contain paraffins, pine oils, polycyclic aromatic hydrocarbons, and heavy metals . Exposure to metalworking fluid aerosols can lead to hypersensitivity pneumonitis and chronic obstructive pulmonary disease . Mycobacteria were recovered significantly more frequently from metalworking fluid samples collected from facilities where hypersensitivity pneumonitis was found; compared to facilities that did not have hypersensitivity pneumonitis . In one study, exposure to metalworking fluid mist resulted in hypersensitivity pneumonitis in 10 workers . Acid-fast microorganisms identified as mycobacteria were present in the reservoir at 10 7  CFU/mL . A mycobacteria in the reservoir was considered to be a likely cause of the hypersensitivity pneumonitis because one patient was infected by a _Mycobacterium_ sp. and had antibodies against the reservoir fluid .\n\n【4】Hypersensitivity pneumonitis appeared in spite of disinfection of the metalworking fluid with morpholine, formaldehyde, or quaternary ammonium-based disinfectants , and mycobacteria were recovered from the metal working fluid . Mycobacteria are resistant to formaldehyde and quaternary ammonium disinfectants  and the heavy metals in metalworking fluids . Further, mycobacteria can grow on the organic compounds in metalworking fluid, including the paraffins, pine oils, and polycyclic aromatic hydrocarbons  and can degrade the disinfectant morpholine . Mycobacteria present in the water  can likely grow on the organic compounds in metalworking fluids in the absence of competitors after disinfection. Cleaning would not be expected to eradicate mycobacteria because of their ability to form biofilms . Adding disinfectant and cleaning the reservoir in one facility did not prevent the reappearance of mycobacteria (7 x 10 5  CFU/mL by 2 weeks ). Further, disinfectant treatment would likely result in selection of mycobacteria remaining after the cleaning.\n\n【5】### Hypersensitivity Pneumonitis in Swimming Pool Attendants\n\n【6】Granulomatous pneumonitis has been reported in lifeguards (“lifeguard lung”) who worked at an indoor swimming pool that featured waterfalls and sprays . Affected lifeguards with symptoms worked longer hours than unaffected lifeguards , which demonstrated a dose-response effect. The waterfalls and sprays increased the number of respirable particles fivefold and the levels of endotoxin eightfold . Based on the presence of endotoxin in the aerosol samples, endotoxin exposure was suggested as the cause of the pneumonitis in lifeguards . However, subsequent data provided evidence of a possible second factor resulting in hypersensitivity pneumonitis; aerosols containing mycobacteria were shown to cause granulomatous lung disease . Others have reported high numbers of mycobacteria in swimming pools and whirlpools  and in hot tubs . Further, amoebae were reported in the indoor swimming pool where lifeguards reported pneumonitis . Mycobacteria, including _M. avium_ and _M. intracellulare_ , can survive and grow in phagocytic amoebae  and protozoa . In fact, _M. avium_ grown in amoebae or protozoa are more virulent . Mycobacteria are resistant to chlorine  and preferentially aerosolized from water .\n\n【7】### Mycobacterial Disease after Exposure to Aerosols Generated by Hot Tubs\n\n【8】Hypersensitivity pneumonitis and mycobacterial pulmonary disease has been reported after exposure to hot tubs . The mycobacteria isolated (e.g. _M. avium_ ) were likely responsible for the infections based on the identity of patient and hot tub mycobacterial isolates by either restriction fragment length polymorphism analysis  or multilocus enzyme electrophoresis . Further, exposure was followed closely by the onset of symptoms, and the extent of symptoms was related to the length of exposure (i.e. time spent in the hot tub) . Although these reports do not document the use of disinfectants in the hot tubs, the waters had been heated. Mycobacteria are relatively resistant to high temperature  and concentrated in hospital hot water systems .\n\n【9】### Hypersensitivity Pneumonitis in Occupants of Water-Damaged Buildings\n\n【10】Inflammatory reactions—including eye irritation, respiratory infections, wheeze, bronchitis, and asthma—in workers in water-damaged or “moldy” buildings have been associated with the presence of high numbers of microorganisms . Mycobacteria were recovered from materials collected from water-damaged buildings, as well as from microorganisms normally associated with building materials . During reconstruction, those mycobacteria could be aerosolized in the dust. Although other microorganisms could be responsible for the respiratory problems, both saprophytic (e.g. _M. terrae_ ) and pathogenic (e.g. _M. avium_ ) strains isolated from moldy buildings were capable of inducing inflammatory responses in a mouse macrophage cell line . The mycobacteria elicited dose-dependent production of cytokines interleukin-6 and tumor necrosis factor-α, nitric oxide, and reactive oxygen species from the murine macrophage . Because whole mycobacterial cells were used in the assays , whether cell metabolites, which are likely easily aerosolized, were responsible for the induction of inflammatory reactions is not known. Heat-shock proteins from a number of mycobacterial species have been shown to generate Th1-type responses, airway inflammation, and airway hyperresponsiveness . This evidence suggests that mycobacteria or their metabolites are possible causes of respiratory disease in persons exposed to water-damaged buildings.\n\n【11】### Ecology of Mycobacteria\n\n【12】The unique combination of physiologic characteristics that distinguish the environmental opportunistic mycobacteria make them likely agents for causing respiratory disease in these diverse settings. Mycobacteria are found in a great variety of natural and human-influenced aquatic environments, including treated drinking water  and aerosols . Mycobacteria in drinking water are associated with the presence of particulates . Although these microbes are grown in rich media in the laboratory, they are oligotrophic and capable of substantial growth in low concentrations of organic matter. For example, _M. avium_ and _M. intracellulare_ can grow in natural and drinking water over a temperature range of 10°C to 45°C . Mycobacteria are relatively resistant to high temperatures. For example, 10% of cells of a strain of _M. avium_ survived after 1 h at 55°C . Mycobacteria are slow growing as a consequence of their fatty acid- and wax-rich impermeable cell wall . The resulting cell surface hydrophobicity permits adherence to solid substrates (e.g. pipes and leaves) in aquatic environments, which results in mycobacteria’s persistence and resistance to being washed away at high flow rates . Further, hydrophobicity is undoubtedly associated with the ability of these bacteria to metabolize a wide variety of nonpolar organic compounds  that are constituents of metal working fluids .\n\n【13】### Resistance of Mycobacteria to Disinfection\n\n【14】Mycobacteria are very resistant to the disinfectants used in water treatment, including chlorine and ozone . For example, _M. avium_ is almost 500 times more resistant to chlorine than is _Escherichia coli_ . Mycobacteria are also quite resistant to agents used for surface and instrument disinfection, including quaternary ammonium compounds, phenolics, iodophors, and glutaraldehyde  and can degrade the disinfectant morpholine . Hydrophobicity and impermeability are undoubtedly factors contributing to the disinfection resistance of mycobacteria . Chemical or enzymatic removal of surface lipid, while not reducing viability, reduces surface hydrophobicity and alters cell charge . Because of their inherent impermeability, mycobacteria grow relatively slowly compared to other bacteria. The slow growth is not necessarily a disadvantage because it correlates with increased resistance to antimicrobial agents , including chlorine (Falkinham JO, unpub data).\n\n【15】Exposure of a mixed microbial population to disinfectants results in selection of a disinfectant-resistant or tolerant population . The persistence and growth of mycobacteria in drinking water systems  are due, in part, to their disinfectant-resistance  and ability to grow under oligotrophic conditions . Disinfection of swimming pools, therapy pools, and spas or hot tubs with chlorine is expected to kill nonmycobacterial flora and to permit the growth of even the slowly growing mycobacteria in the absence of competitors for nutrients. High temperature would also be expected to result in enrichment of mycobacteria . Resistance to disinfectants could also lead to the proliferation of mycobacterial populations in metal working fluid and coolants after disinfection .\n\n【16】### Aerosolization of Mycobacteria\n\n【17】Although _M. tuberculosis_ is transmitted between patients through aerosols, little information exists on aerosolization of the environmental opportunistic mycobacteria (e.g. _M. avium_ and _M. intracellulare_ ). Patient-to-patient transmission of environmental opportunistic mycobacteria does not occur . _M. avium_ and _M. intracellulare_ are readily aerosolized from aqueous suspension . Transfer of mycobacteria occurs as a result of binding of mycobacterial cells to air bubbles and ejection of water droplets after the air bubbles reach the liquid surface . Aerosolization can result in >1,000-fold increase in numbers of viable mycobacterial cells per milliliter of water droplets ejected from water . Mycobacteria in natural aerosols are found in particles and droplets (i.e. <5 μm) that can enter the alveoli of the human lung . Cell surface hydrophobicity, not surface charge, is a major determination of enrichment in ejected droplets . Transfer of mycobacteria from water to air is subject to prevailing physiochemical conditions and can be manipulated. Salts (e.g. NaCl) or detergents reduce the rate of transfer of mycobacteria from water to air by ejected droplets. The influence of the components of metalworking fluid or of chlorine or other disinfectants in water upon aerosolization mycobacteria is unknown.\n\n【18】### Mycobacteria and Immune Responses and Airway Inflammation\n\n【19】Mycobacterial cells and cellular components provoke inflammatory responses. Cells of mycobacterial strains isolated from material collected from water-damaged buildings provoke inflammatory responses in macrophages . Mycobacterial heat-shock proteins generate Th1-type responses, airway inflammation, and hyperresponsiveness . The mycolic acid-containing glycolipids, mannose-containing phospholipids, glycopeptidolipid mycosides, phenolglycolipid mycosides, and sulfatides that are unique to mycobacteria have all been reported to stimulate immune responses in animals . Further, mycobacteria produce a variety of extracellular primary and secondary metabolites  that could be aerosolized and trigger immune responses, including hypersensitivity pneumonitis. Some of these immunostimulatory compounds are produced in response to growth on polycyclic aromatic hydrocarbons . Unfortunately, the studies of inflammatory responses provoked by mycobacteria have been limited to whole cells grown under a single condition  or single proteins . The influence of growth conditions (e.g. growth in metalworking fluid or chlorinated water) or cell fractions (e.g. membranes) or metabolites to stimulate inflammatory responses has not been measured.\n\n【20】### Conclusion\n\n【21】Contemporary reviews of airway dysfunction all describe the need for information concerning microbial agents of workplace and household exposure . Although many more studies are needed, the evidence points to a role of environmental opportunistic mycobacteria in provoking hypersensitivity pneumonitis, respiratory disease, and respiratory infection in both the workplace and home. In addition to the recovery of identical species and types of mycobacteria from reservoirs and patients, physiologic characteristics of mycobacteria are consistent with their presence in the sources, transmission by means of aerosols, and illnesses. Identifying the factors that influence the presence of mycobacteria in aerosols in these workplaces would have an impact on workers in a variety of occupational settings.\n\n【22】On the basis of several physiologic and ecologic characteristics of mycobacteria, several approaches to reduce the impact of mycobacteria in these settings are possible. Because mycobacteria are associated with particulates , their numbers in reservoirs can be reduced by removal of particular matter (e.g. filtration). UV light can be used to reduce mycobacterial numbers. Disinfection of mycobacteria at high temperatures (e.g. 40°C) is more effective at reducing numbers, especially if cells were grown at lower temperatures (e.g. 30°C). Agents or combinations with surfactant or detergent-like and disinfectant activity would increase permeation in cells and biofilms and kill more mycobacteria. Finally, aerosolized or waterborne mycobacteria may be trapped in filters coated with hydrophobic compounds (e.g. paraffin) and thereby intercepted before inhalation or ingestion.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "51ea0287-7275-4bff-9406-6b61290b6dfe", "title": "Outbreaks of Tilapia Lake Virus Infection, Thailand, 2015–2016", "text": "【0】Outbreaks of Tilapia Lake Virus Infection, Thailand, 2015–2016\nViral diseases are common causes of illness and death in cultured fish; such viruses include infectious salmon anemia virus, infectious hematopoietic necrosis virus, and viral hemorrhagic septicemia virus . With regard to tilapia, some viral pathogens, including betanodavirus, iridovirus, and herpes-like virus , reportedly cause severe disease. In recent years, Thailand has experienced extensive losses of tilapia; most losses occurred 1 month after transfer of fish from hatchery to grow-out cages in public rivers or reservoirs (1-month mortality syndrome). During routine investigation of this syndrome, multiple bacterial and parasitic infections were identified. However, no association was established between the outbreaks and any primary causative agent(s). Most deaths occurred within 2 weeks after the first dead fish were found. Similar observations of extensive losses of raised tilapia and wild fish in Israel and Ecuador have been reported . These outbreaks led to identification of a virus affecting tilapia, called tilapia lake virus (TiLV). The epidemiologic pattern and clinical signs for infected fish in Thailand led to suspicion of an illness of unknown etiology that was similar to TiLV infection.\n\n【1】During 2015–2016, we investigated 32 outbreaks involving a large number of deaths of unknown cause among Nile tilapia ( _Oreochromis niloticus_ ) and red hybrid tilapia ( _Oreochromis_ spp.). The outbreaks occurred at fish farms in central, western, eastern, and northeastern Thailand . Affected fish were commonly found within 1 month after transfer from the hatchery facility to grow-out ponds or cages. In general, clinical signs and high mortality rates were associated with fish weighing 1–50 g . Mortality rates among tilapia farms were 20%–90%; higher rates were associated with secondary bacterial and parasitic infections. Mortality rates peaked within 14 days after the first dead fish were found.\n\n【2】As part of the outbreak investigation, samples of brain tissue were taken from fish at each of the 32 outbreak locations (each with a mortality rate >1%/day for 3 consecutive days): 10–30 moribund fish and 5–10 apparently healthy fish from the same culture areas. In total, 325 samples were collected and tested for etiologic agent(s) . Samples from fish involved in 22 of the 32 outbreaks were positive for TiLV .\n\n【3】For our study, we selected a field sample positive for TiLV (designated TiLV/Tilapia/Thai/TV1/2016) and processed it for whole-genome sequencing. Another 6 TiLVs were selected for sequencing of the putative polymerase basic 1 (PB1) gene . TiLV genome sequencing was conducted by using newly designed primers based on reference TiLVs available in the GenBank database . Nucleotide sequences of 7 TiLVs from Thailand were submitted to GenBank (accession nos. KX631921–36).\n\n【4】Comparison of the TiLVs from Thailand with those from Israel showed high nucleotide and amino acid identities (95.18%–99.10%). Among TiLVs from Thailand, nucleotide and amino acid identities for segment 1 or the putative PB1 gene of the virus were high (99.61%–100%) . Genetic analysis of the putative PB1 protein of TiLVs from Thailand and the viruses of the family _Orthomyxoviridae_ showed that TiLVs from Thailand possessed motifs preA, A, B, C, D, and E similar to those of _Orthomyxoviridae_ viruses, including influenza A, B, and C viruses; infectious salmon anemia virus; Dhori virus; and Thogoto virus  . Phylogenetic analysis showed that TiLVs from Thailand were closely related to TiLVs from Israel and grouped with the viruses of the family _Orthomyxoviridae_ but not _Arenaviridae_ and _Bunyaviridae_ . This result suggests that the genetic composition of this emerging virus was similar to that of orthomyxoviruses and homologous with previously published TiLV sequences.\n\n【5】Our PCR and whole-genome findings demonstrate genetic homology between TiLV from Thailand and the etiologic agent of a novel RNA virus infection of tilapia in Israel and Ecuador . Furthermore, the clinical signs and pathological presentation of infection with TiLV from Thailand are similar to those of infection with TiLV from Israel . The clinical signs, gross lesions, and histopathologic lesions combined with virus identification and characterization highlight emerging TiLV in Thailand as the primary cause of the outbreaks. We also found that fish that survived massive die-offs rarely showed clinical signs, suggesting the development of specific immunity against the virus. It should be noted that the TiLVs from Thailand possessed 10 gene segments encoding 10 proteins, including segment 1 or putative PB1 protein. The pattern of protein motifs for this putative PB1 was similar to that for influenza viruses. To our knowledge, TiLV has infected tilapia only, no other aquatic or terrestrial animals.\n\n【6】Our results emphasize that the virus isolated from Thailand shares high sequence similarity with TiLV from Israel, suggesting that this virus spreads across continents. Given that tilapia are the main aquaculture species, control of TiLV will be improved by further efforts such as strict biosecurity, vaccine development, and selection of resistant tilapia breeds.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4222f88f-2c72-49f9-9395-d294dd3097fb", "title": "Opportunities for Policy Interventions to Reduce Youth Hookah Smoking in the United States", "text": "【0】Opportunities for Policy Interventions to Reduce Youth Hookah Smoking in the United States\nAbstract\n--------\n\n【1】Preventing youth smoking initiation is a priority for tobacco control programs, because most adult tobacco smokers become addicted during adolescence. Interventions that restrict the affordability, accessibility, and marketing of cigarettes have been effective in reducing youth cigarette smoking. However, increasing numbers of youth are smoking tobacco using hookahs. Predictors of smoking tobacco with hookahs are the same as those for smoking cigarettes. Established interventions that curb youth cigarette smoking should therefore be effective in reducing hookah use. Potential policy interventions include equalizing tobacco tax rates for all tobacco types, requiring warning labels on hookah tobacco and accurate labeling of product contents, extending the cigarette flavoring ban to hookah tobacco, enacting smoke-free air laws and removing exemptions for hookah lounges, and expanding shipping restrictions on tobacco products.\n\n【2】Introduction\n------------\n\n【3】Preventing youth smoking is a priority for tobacco use control programs, because most tobacco users become addicted during adolescence . A range of public policies restricting the affordability, accessibility, and marketing of cigarettes has helped cut youth cigarette smoking nearly in half between 1998 and 2009 . Although fewer children and adolescents are smoking cigarettes, an increasing number are smoking tobacco using a hookah, also known as a waterpipe, nargile, goza, or hubble bubble . In 2011, 18.5% of 12th-grade students reported having smoked a hookah in the past year .\n\n【4】Hookahs are instruments used to smoke flavored tobacco, which is called _shisha_ , or _maassel_ . Shisha is a wet mixture of tobacco, sweetener, and flavorings. Shisha comes in various flavors, including strawberry, cappuccino, and cotton candy. A typical hookah has a fired-clay head that holds the shisha, a glass or acrylic bowl that is filled with water, and leather or plastic hoses through which users inhale the smoke. A metal screen or a piece of aluminum foil with holes punched in it covers the shisha, and a piece of burning charcoal is placed on top. When the smoker inhales through the hose, the smoke is drawn through the water, which cools the smoke.\n\n【5】Many hookah smokers believe that smoking a hookah carries less risk of tobacco-related disease than cigarette smoking . However, hookah smoke contains many of the same toxins as cigarette smoke and has been associated with lung cancer, respiratory illness, low birth weight, and periodontal disease . Studies of youth and young adults have found that predictors of smoking hookah are the same as those for smoking cigarettes, including social acceptability , having friends and family members who smoke , and perceiving that smoking a hookah is not harmful . Because predictors of cigarette and hookah smoking among youth are consistent, established interventions to reduce youth cigarette smoking should be effective for reducing hookah smoking. This article highlights potential policy interventions to reduce youth hookah use.\n\n【6】Price\n-----\n\n【7】The single most powerful intervention to reduce youth smoking is raising the price of tobacco; adolescents and young adults are 2 to 3 times more price sensitive than adults . Hookahs are a type of pipe, and shisha, the flavored tobacco smoked in hookahs, is a variety of pipe tobacco. The federal tax on pipe tobacco is $2.83 per pound, which is nearly $22 per pound less than the tax on cigarette tobacco .\n\n【8】Equalizing the federal tax rates between loose pipe tobacco and loose cigarette tobacco would increase the price of shisha and likely reduce youth hookah smoking. In Oregon, a 50-gram package of shisha retails for about $3.65, which includes federal and Oregon state tobacco tax. Increasing the federal tax on loose pipe tobacco from $2.83 per pound to $24.78 per pound would add $2.42 in federal tax and at least $1.57 in Oregon state tax to a 50-gram package of shisha (Oregon’s 65% tax rate is applied to the wholesale price, which includes the federal tax). This would more than double the retail price of shisha. For every 10% increase in the real price of cigarettes, the number of youth who smoke drops by 6% to 7% . Assuming the same relationship holds for shisha, a 100% increase in price would lower youth consumption by at least 60%.\n\n【9】Health Warnings\n---------------\n\n【10】On the basis of research on youth cigarette smoking , youth who do not perceive hookah smoking as harmful are more likely to do it . Young people view hookah smoking as safer than smoking cigarettes and incorrectly believe that hookah smoke has less nicotine and is less toxic than cigarettes because of the filtering mechanism of the water in the base of the hookah . These findings suggest that an opportunity exists to reduce youth hookah use by educating youth about the adverse health effects associated with hookah smoking.\n\n【11】One way to educate youth about the dangers of tobacco use is to require warning labels on tobacco products and advertisements. Graphic warning labels are effective at raising awareness of the dangers of tobacco and increasing intentions to quit use . Federal law requires warning labels on cigarettes and smokeless tobacco but not on pipe tobacco. Warning labels on all tobacco products (including shisha) would inform consumers that hookah smoking is not safe. In addition to health warning labels, hookah tobacco products also need accurate product labeling. Some shisha brands have labels with incorrect information. One study  found that labeling of hookah tobacco did not reflect actual nicotine delivery to smokers; those who smoked a brand of hookah tobacco labeled 0.05% nicotine had greater plasma nicotine levels than those who smoked a brand of hookah tobacco labeled 0.5% nicotine. These inaccurate labels may perpetuate users’ beliefs that hookah tobacco is safer than other tobacco products .\n\n【12】Regulation of Flavored Tobacco\n------------------------------\n\n【13】Shisha is available in dozens of candy, fruit, coffee, and cocktail flavors. Flavors mask the harshness of tobacco and make it easier for new users to start using tobacco . The fruit flavors of hookah tobacco and the sweet smell of hookah smoke also contribute to the perception that hookah smoking is safer than cigarettes . The 2009 Family Smoking Prevention and Tobacco Control Act banned flavored cigarettes with the exception of menthol . However, the flavor ban does not extend to cigars, smokeless tobacco, or pipe tobacco. Extending the cigarette flavor ban to pipe tobacco would likely make hookah less appealing, particularly to youth. The Food and Drug Administration has the authority to extend a flavor ban to any tobacco product without an additional act of Congress . State and local governments can also pass laws banning the sale of flavored tobacco, which would further limit shisha sales.\n\n【14】Smoke-Free Environments\n-----------------------\n\n【15】As of December 2010, 25 states and the District of Columbia had comprehensive smoke-free laws prohibiting smoking in worksites, restaurants, and bars . Some states, including Oregon, have comprehensive smoke-free laws with exemptions that permit indoor smoking in cigar bars and smoke shops. Hookah lounges in Oregon are allowed to operate under the smoke shop exemption. Although three-fourths of the largest cities in the United States ban cigarette smoking in bars, hookah tobacco smoking may be permitted in nearly 90% of these cities via exemptions in clean indoor air laws that permit hookah smoking in smoke shops . The air inside hookah lounges contains high concentrations of secondhand smoke, creating hazardous conditions for patrons and employees . Oregon’s smoke-free law took effect in 2009; in the following 30 months, 30 applications for hookah lounges were submitted to the state . As of April 2012, 15 hookah lounges were operating in Oregon with certification to allow indoor smoking. Oregon Healthy Teens, Oregon’s youth behavioral risk factor survey, indicated that the prevalence of hookah use increased significantly from 2.7% in 2008 to 5.1% in 2009 among 8th-grade students in counties with hookah lounges. In counties without hookah lounges during the same time, the prevalence of hookah use by 8th-grade students increased from 1.6% to 1.9% .\n\n【16】In addition to protecting people from exposure to secondhand smoke, smoke-free laws help decrease the perception of smoking as an acceptable behavior , which in turn promotes cessation and discourages youth initiation of tobacco use. Conversely, the presence of hookah lounges creates and reinforces a community norm accepting of hookah smoking. Nearly 30% of high school students in San Diego learned about hookah smoking from seeing a hookah lounge, and current hookah users were more likely to know of a hookah lounge in their community . Furthermore, lounges reinforce pro-hookah messages in advertisements and on social networking sites. A content analysis of 144 hookah lounge websites across the United States found that only 4% included a tobacco-related warning on any page ; the word “tobacco” appeared on 58% of the websites. Hookah lounge websites most commonly focused on flavorings, pleasure, relaxation, product quality, and the cultural and social aspects of hookah smoking; information on age limits, health warnings, and the involvement of tobacco in hookah smoking was limited . Smoke-free laws without exceptions for hookah lounges can protect more workers from secondhand smoke, limit hookah misperceptions perpetuated on hookah lounge websites, and create community norms that discourage hookah smoking.\n\n【17】Internet and Mail-Order Access\n------------------------------\n\n【18】Dozens of Internet sites sell shisha tobacco for home delivery. Online sales make it easier for youth to access tobacco. Major credit card companies agreed not to process online payments for cigarettes  but have not stopped processing payments for shisha. The Prevent All Cigarette Trafficking Act stops the US Postal Service from shipping cigarettes, roll-your-own tobacco, and smokeless tobacco but does not prohibit shipping shisha or other pipe tobacco products . Expanded restrictions on credit processing for Internet purchases and on shipping tobacco products would make hookah smoking less accessible to youth.\n\n【19】Conclusion\n----------\n\n【20】Predictors of youth hookah smoking are similar to predictors of youth cigarette smoking. Therefore, successful strategies for reducing cigarette use among youth and young adults should also work for hookah use. There are many opportunities for policy interventions to reverse rising rates of youth hookah smoking. Equalizing tobacco tax rates would make hookah smoking less affordable for youth, and product warning labels would inform both youth and other users about the dangers of hookah smoking. Flavor bans for pipe tobacco may reduce hookah’s appeal to youth, and expanding shipping restrictions on tobacco products would make hookah smoking less accessible to youth. Smoke-free laws without exemptions would prevent hookah lounges from opening, and thereby decrease youth exposure to environments that normalize hookah smoking.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "80f084bb-7704-48f3-8944-d1627fec65c2", "title": "Community-Acquired Clostridium difficile Infection, Queensland, Australia", "text": "【0】Community-Acquired Clostridium difficile Infection, Queensland, Australia\n**To the Editor:** In Queensland, Australia, a steady increase in community-acquired (CA) _Clostridium difficile_ infections (CDI) during 2003–2012 could not be explained by patients’ demographic characteristics or environmental factors . Several risk factors have been implicated in the increased rates of CA-CDI, primarily exposure to antimicrobial drugs, gastric acid–suppression drugs, and corticosteroids . Given the recent rise in prescription of corticosteroids and proton pump inhibitors in Australia, we hypothesized that the observed increase in CA-CDI was associated with increased drug prescriptions.\n\n【1】To test our hypothesis, we analyzed a subset of data used in a previous study , which included fecal samples from patients seen by general practitioners in the community from January 2008 through December 2012. The samples were submitted to Sullivan Nicolaides Pathology (Taringa, Queensland, Australia) for _C. difficile_ toxin gene detection. After samples submitted from healthcare facilities and nursing homes were excluded, the final dataset contained data from 14,330 fecal samples. We aggregated the data by patient sex, age categories, year, and statistical area level 4 (SA4). For each sex-age-year-SA4 group, we used as numerators the numbers of CA-CDI cases identified and as denominators the numbers of samples submitted for microbiological testing.\n\n【2】The Australian Department of Human Services provided data from the Pharmaceutical Benefits Scheme. The quantities of 11 anatomic therapeutic chemical drugs were accessed by patient sex, age group, year, and SA4. Corresponding with the CA-CDI data, medication data to be analyzed were then aggregated by sex, age group, year, and SA4.\n\n【3】For each medication, we built binomial logistic regression models, using CA-CDI status as the outcome, in a Bayesian framework, incorporating fixed effects for sex, age group, quantity of drug prescribed, year , and spatially unstructured random effects at the SA4 level. After performing an initial burn-in, we stored and summarized 1,000 values from the posterior distribution of each parameter by using descriptive statistics (posterior mean, 95% posterior credible interval \\[95% CrI\\], and p value). We examined multiple pairwise comparisons of CA-CDI and medication exposure; thus, we used the Holm adjustment for p values to avoid α inflation and to control the familywise error rate.\n\n【4】Of the 14,330 fecal samples tested, 1,430 (10%) were positive for _C. difficile_ . The proportion of positive fecal samples increased over the 5-year period, from 7.10% in 2008 to 12.72% in 2011 and 11.48% in 2012 (p<0.001). After adjusting the regression models for sex, age group, temporal pattern, and spatial distribution, we found that exposure to antimycobacterial drugs (odds ratio \\[OR\\] 1.09; 95% CrI 1.02–1.16) and anthelmintic drugs (OR 1.07; 95% CrI 1.01–1.13) were associated with increased odds of CA-CDI. After post hoc Holm adjustments, no statistically significant association between medication exposure and CA-CDI was observed .\n\n【5】Our findings suggest that the increase in CA-CDI proportion was not associated with population-level medication exposure in Queensland during 2008–2012. CA-CDI epidemiology in Queensland might be driven by a group of factors other than medication exposure, such as transmission of the pathogen from food, animals, or hospitals into the community. Studies have confirmed the risk for foodborne and animalborne spread of _C. difficile_ into the community . In Australia and New Zealand, importation of onions and garlic from the United States and Mexico might be responsible for increased CDI cases during Southern Hemisphere summers , and high prevalence of _C. difficile_ colonization in piglets has been identified . However, the role of these factors in leading to CA-CDI cases remains unknown. Currently, there is no evidence of a reverse-infection route (healthcare-acquired CDI being transmitted to persons in the community). However, Sethi et al. documented environmental shedding of _C. difficile_ by inpatients for several weeks after resolution of symptoms . Therefore, the possibility that asymptomatic patients might be a source of transmission after hospital discharge needs to be examined. In recent years, epidemiologic models exploring the role of CDI coming from the community into the hospital have become increasingly popular ; however, to the best of our knowledge, only 1 modeling study described CDI dynamics within the wider community . Although this approach is innovative, we acknowledge some limitations. Medication exposure was used as a proxy, based on the average prescription in the community, and it cannot be applied to the individual patient. In addition, we were unable to adjust the regression model for the presence of concurrent medical conditions and other unmeasured confounders.\n\n【6】Exposure to medications, particularly antimicrobial drugs, probably influences CA-CDI pathogenesis . However, our community-based assessment indicates that a more holistic exploration is needed to identify alternative factors driving increases in CA-CDI cases in the wider population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "202e9aeb-0b01-4304-8f1f-5e646c2dfd2d", "title": "SARS-CoV-2 Omicron BA.1 Challenge after Ancestral or Delta Infection in Mice", "text": "【0】SARS-CoV-2 Omicron BA.1 Challenge after Ancestral or Delta Infection in Mice\nThe SARS-CoV-2 Omicron variant (B.1.1.529, BA.1 sublineage) emerged nearly 2 years after the ancestral strain was identified . The Omicron BA.1 variant contains ≈50 mutations in the spike protein , resulting in substantial antigenic change. The strain was more infectious than prior variants of concern (VOCs) and escaped immunity, causing infections in persons who were previously vaccinated with ancestral strain–based vaccines  or infected with the ancestral virus or Delta (B.1.617.2) VOC. Since January 2022, additional Omicron sublineages (BA.2 to BA.5) have been detected worldwide. BA.4/BA.5 have identical spike proteins, most similar to BA.2, with additional spike mutations .\n\n【1】We sought to mimic the human scenario and selected a mouse model from available animal models  to assess the cross-reactivity of neutralizing antibody elicited by ancestral, Delta, and BA.1 viruses and to assess the effect of primary homologous and heterologous infection on secondary infection with the Omicron BA.1 strain. We also compared antibody cross-reactivity to BA.2 and BA.5 in serum samples from mice infected with ancestral, Delta, and BA.1 strains.\n\n【2】We first compared the associated illness, mortality rates, and kinetics of replication of 10 4  50% tissue culture infectious dose (TCID 50  ) of SARS-CoV-2/Australia/Vic/01/20 (ancestral strain–like), SARS-CoV-2/Australia/Vic/18440/2021 (Delta), and SARS-CoV-2/Australia/NSW/RPAH-1933/2021 (Omicron BA.1) strains in 7- to 9-week-old female K18hACE2 transgenic mice . We infected groups of 15 K18hACE2 mice with intranasally delivered ancestral, Delta, or Omicron BA.1 strains by using a low dose of each virus (10 2  TCID 50  ), selected so that the mice would survive primary infection . We mock-infected 15 mice with phosphate-buffered saline (PBS). We collected blood on day 27 after primary infection and then challenged mice with 10 4  TCID 50  of Omicron BA.1 virus. We collected lungs and nasal turbinates (NTs) 2 and 4 days after challenge; we weighed and monitored 5 mice per group for clinical signs for 14 days . We collected blood samples on day 28 after Omicron BA.1 challenge (day 56 from primary infection).\n\n【3】After primary infection, all Omicron BA1–infected mice survived without major weight loss, but 1 ancestral strain–infected and 5 Delta-infected mice died during days 8–13. After challenge with 10 4  TCID 50  of Omicron, all mice, including the PBS group (naive control), survived without weight loss. The control group had mean virus titers of 10 2.6  (day 2) and 10 2.7  (day 4) in NTs and 10 3.7  (day 2) and 10 3.5  (day 4) TCID 50  /organ in lungs after Omicron BA.1 challenge.\n\n【4】Consistent with other reports , we found the titers of BA.1 to be lower than those for ancestral and Delta viruses . Virus was not recovered from the tissues of mice challenged with BA.1 that had prior primary infection with ancestral, Delta, or BA.1 viruses , except 1 mouse in each of the ancestral and Delta primary infection groups.\n\n【5】The homologous responses were strongest to ancestral (geometric mean titer \\[GMT\\] 709), followed by Delta (GMT 129), and were lowest to BA.1 (GMT 83) . The low titer neutralizing antibody response to Omicron BA.1 infection is probably attributable to less robust replication of BA.1 virus in mouse tissues . Mice recovered from primary BA.1 infection were fully protected from rechallenge with the higher dose of BA.1, and no boost in homologous neutralizing antibody titers occurred (day 56 GMT 62).\n\n【6】Primary Omicron BA.1 infection did not induce heterologous neutralizing activity against ancestral, Delta, BA.2, or BA.5 viruses . In contrast, primary ancestral infection elicited an 8-fold reduced titer against Delta and 21-fold reduced titer against the BA.1 virus, and primary Delta infection elicited a 2-fold reduced titer against ancestral strain. None of the mice first infected with BA.1, ancestral, or Delta viruses developed neutralizing antibodies against BA.5.\n\n【7】Despite the absence of detectable BA.1 virus in the respiratory tract tissues after secondary infection in mice previously infected with ancestral or Delta , we observed a boost in homologous GMTs 1,338 (ancestral) and >453 (Delta), and cross-reactive neutralizing antibody titers GMTs >440 (ancestral) and 124 (Delta), and vice versa (GMTs of 27 and 60, respectively), with no improvement in cross-reactivity to BA.1. Mice first infected with Delta and rechallenged with BA.1 had low but detectable neutralizing antibody titers against BA.5 .\n\n【8】Our observations are consistent with BA.1 being antigenically distinct from the ancestral and Delta strains . A boost occurred in preexisting SARS-CoV-2 neutralizing antibodies to ancestral and Delta but not in cross-reactivity to Omicron, probably because more epitopes are shared between ancestral and Delta than between those strains and Omicron. Serologic data from humans suggest that \\> 3 exposures to ancestral strains as infection or vaccination or a combination are needed to induce cross-reactive antibodies to BA.1 . Although data from antigenic cartography using human serum suggest that BA.2 is antigenically closer to the ancestral and Delta strains , we did not detect cross-reactive neutralizing antibodies after primary infection with ancestral and Delta strains. Protection from replication of the Omicron BA.1 strain despite the lack of cross-reactive neutralizing antibodies may be attributable to mucosal immunity or T-cell responses in ancestral strain–infected and Delta-infected mice .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4e3f09ae-006b-44ea-b37f-7b03dbe0164f", "title": "Performance of 2 Commercial Serologic Tests for Diagnosing Zika Virus Infection", "text": "【0】Performance of 2 Commercial Serologic Tests for Diagnosing Zika Virus Infection\nZika virus belongs to the Flaviviridae family, genus _Flavivirus_ , and is an arbovirus transmitted mainly by mosquitoes of the genus _Aedes_ ( _Stegomyia_ ). Initially isolated in 1947 from a sentinel monkey during yellow fever surveillance in Uganda, Zika virus was reported as causing only sporadic human infections, associated with asymptomatic or mild, self-limiting illness, until 2007 . In 2007, Zika virus spread first to Pacific islands and then throughout the Americas, resulting in large outbreaks in several regions of the world. Zika virus infection is estimated to be symptomatic in 18%–73% of cases ; severe complications have been reported, including neurologic disorders, such as Guillain-Barré syndrome, and congenital Zika virus syndrome, which is characterized by severe microcephaly, brain and ocular anomalies, congenital contractures, and neurologic impairment in the fetuses and newborns of mothers infected during pregnancy .\n\n【1】The mild signs and symptoms of Zika virus infection include fever, rash, joint pain, conjunctivitis, headache, and myalgia  _._ These manifestations are difficult to distinguish clinically from those caused by other arboviral infections, such as dengue or chikungunya, which are often observed in the same geographic areas. Therefore, specific, reliable diagnostic tools are needed.\n\n【2】Several commercial kits are available for direct viral detection by nucleic acid–based testing, which enable diagnosis during the acute phase of the disease : up to 7 days after symptom onset in serum samples, up to 20 days in urine, and even longer in semen . This virologic window, combined with the high proportion of asymptomatic forms, makes the monitoring of Zika virus infection difficult, especially in pregnant women. Therefore, serologic tools for diagnosis of Zika virus infection are urgently needed. This challenge remains because of cross-reactivity among flaviviruses, especially in a context of secondary flavivirus infection or previous immunization.\n\n【3】The first objective of this study was to evaluate the performance of two commercial serologic kits for the detection of Zika virus–specific IgM and IgG in serum samples from patients with an arboviral-like syndrome in a region where other arboviruses are known to circulate, including dengue and chikungunya viruses. The 2 commercial kits (4 assays) studied were the Euroimmun Zika virus IgM and IgG ELISAs  and Dia.Pro Zika virus IgM and IgG ELISAs . The second objective was to determine the kinetics of Zika virus IgM and IgG induced after infection, as defined by these kits.\n\n【4】### Material and Methods\n\n【5】##### Clinical Samples and Study Design\n\n【6】The clinical samples used in this study were selected according to the standards for reporting diagnostic accuracy requirements. Two thirds came from the serum collection of the National Reference Centre (NRC) for Arboviruses in French Guiana and one third from samples collected for a descriptive prospective study of Zika virus disease in the French military community in French Guiana (ZIFAG) . The NRC collection comprises clinical specimens received during 2002–2017 for routine diagnosis and expertise. The protocol of the ZIFAG study received ethics approval from the Comité de Protection des Personnes Sud-Méditerranée I (ID RCB: 2016-A00394–47); all participants provided written informed consent. All the selected specimens were obtained from patients with an arboviral-like syndrome and an etiologic diagnosis confirmed by real-time reverse transcription PCR (RT-PCR) on acute-phase serum or urine samples. A commercial qualitative RT-PCR kit  was used for Zika virus detection and NRC in-house real-time RT-PCR for dengue and chikungunya viruses.\n\n【7】We first evaluated the performance of the 2 commercial immunoassays against a panel of 199 serum samples collected from days 3 through 20 after the onset of symptoms (onset defined as day 0—that is, within the first 24 hours) . We evaluated the sensitivity of the assays in a subgroup of 90 serum samples from 90 patients with confirmed Zika virus infection diagnosed from the end of 2015 through 2016, during the outbreak in French Guiana (Zika subgroup). We evaluated the specificity of the assays in a subgroup of 109 serum samples with a strong potential for causing flavivirus cross-reactions (non-Zika subgroup). This subgroup comprised serum samples obtained from 35 patients with confirmed dengue virus infection sampled during 2002–2013 dengue epidemics; 29 patients with confirmed chikungunya virus infection sampled during the chikungunya outbreak in French Guiana in 2014–2015, just after the dengue outbreak in 2012–2013; and 45 patients with neither dengue virus, chikungunya virus, nor Zika virus infection sampled just before the Zika virus outbreak. We peformed Zika microneutralization tests on the Zika virus, dengue virus, and chikungunya virus–negative subgroup of serum samples, enabling confirmation of the absence of Zika neutralizing antibodies.\n\n【8】To determine the kinetics of Zika virus antibodies, we used a panel of 300 serum samples collected from 124 patients with confirmed Zika virus infection from day 0 through day 300 after the onset of symptoms . We collected 1–8 samples from each Zika virus–infected patient. The distribution of samples according to time since onset was as follows: 76 samples collected during days 0–4, 50 during days 5–14, 55 during days 15–30, 19 during days 31–60, 32 during days 61–90, 44 during days 91–180, and 24 during days 181–300.\n\n【9】##### Euroimmun and Dia.Pro Zika Virus IgM and IgG ELISAs\n\n【10】We tested all samples with the Euroimmun ELISAs according to the manufacturer’s recommendations and calculated signal-to-cutoff (S/CO) ratios; values <0.8 were regarded as negative, \\> 0.8 to <1.1 as equivocal, and \\> 1.1 as positive. We tested all serum specimens with the respective Dia.Pro ELISAs for qualitative determination of IgM and IgG, according to the manufacturer’s instructions; we interpreted results as positive if the S/CO ratio was \\> 1.1, negative if <0.9, and equivocal if 0.9–1.1. Recombinant Zika virus nonstructural protein 1 was the antigen in both the Euroimmun and Dia.Pro assays.\n\n【11】##### Statistical Analysis\n\n【12】Continuous variables were expressed as mean (± SD) or median with interquartile range (IQR) and discrete variables as percentages and 95% CIs. We calculated the sensitivity and specificity of the assays with 95% CIs. The differences in the S/CO ratios of the IgM and IgG assays at each time were compared with the Student _t_ test. We performed all statistical analyses using R 3.4 statistical software .\n\n【13】### Results\n\n【14】##### Patient Characteristics\n\n【15】The 199 samples used to evaluate the performance of the serologic kits came from patients with a mean \\+ SD age of 36 \\+ 16 years (range 1–74 years; IQR 27–46 years). This panel was composed of the Zika virus–positive subgroup (n = 90), 51 (57%) female and 39 (43%) male, with a mean age \\+ SD of 39 \\+ 12 years; and the Zika virus–negative subgroup (n = 109), 62 (57%) female and 45 (43%) male, with a mean \\+ SD age of 34 \\+ 18 years .\n\n【16】The 300 samples used to determine the kinetics of Zika virus IgM and IgG came from 124 patients, 82 (66%) female and 42 (34%) male. The age range of these patients was 8–74 years (mean \\+ SD 37 \\+ 11 years).\n\n【17】##### Performance of Zika Virus IgM ELISAs\n\n【18】We evaluated the sensitivity and specificity of the 2 Zika virus IgM tests against 90 Zika virus subgroup samples and 109 non-Zika subgroup samples . Each test gave inconclusive results for 6 of the 199 samples: with the Euroimmun test, we obtained 6 inconclusive results from the 90 samples from the Zika virus subgroup, and with the Dia.Pro test, we obtained 4 inconclusive results of the 90 samples from the Zika virus subgroup and 2 for samples collected on day 9 after onset of disease from patients with confirmed dengue virus infection.\n\n【19】The sensitivity of the Euroimmun Zika virus IgM test was 49% (41/84; 95% CI 38%–60%); sensitivity of the Dia.Pro test was 69% (59/86; 95% CI 59%–79%). Only 1 of the 109 non-Zika subgroup serum samples was positive in the Euroimmun test, indicating 99% specificity (95% CI 97%–100%). Four non-Zika subgroup samples were detected as positive by the Dia.Pro IgM test, including 3 samples from patients with acute dengue virus infection, indicating a specificity of 96% (103/107; 95% CI 92%–100%).\n\n【20】##### Performance of Zika Virus IgG ELISAs\n\n【21】We used the same panel to evaluate the performance of Euroimmun and Dia.Pro Zika virus IgG kits . With the Euroimmun test, 16 (8%) of the 199 samples gave inconclusive results, 8 among positive Zika virus samples collected on days 5 (1 sample) and 9 (7 samples) of disease onset; 8 negative Zika virus samples also gave inconclusive results (5 chikungunya virus–positive samples and 3 from the group negative for dengue virus, chikungunya virus, and Zika virus). The sensitivity of this assay was 71% (58/82; 95% CI 92%–100%) and the specificity 70% (71/101; 95% CI 61%–79%). With the Dia.Pro IgG test, 8 samples gave inconclusive results: 5 samples from the Zika virus–positive group (2 collected before day 5 and the others on day 9 or later after the onset of illness) and 3 samples from the Zika virus–negative group (collected on days 4 to 6 after onset) . The sensitivity of this assay was 79% (67/85; 95% CI 70%–88%) and the specificity 62% (66/106; 95% CI 53%–71%).\n\n【22】The false positivity rate of the 2 Zika virus IgG assays varied according to the subpanel used. These rates were 40%–58.6% for positive dengue virus or chikungunya virus sample subgroups and 11.9%–21.4% for the negative dengue virus, negative chikungunya virus, and negative Zika virus sample subgroup .\n\n【23】##### Combined Performance of IgM/IgG Assays\n\n【24】The sensitivity of the combined Euroimmun Zika virus IgM and IgG assays was 82% (71/87; 95% CI 74%–90%) and the specificity was 69% (70/101; 95% CI 60%–78%). The sensitivity of the combined Dia.Pro Zika virus IgM and IgG assays was 87% (75/86; 95% CI 80%–94%) and specificity was 62% (66/106; 95% CI 53%–71%).\n\n【25】##### Time-Course Analysis of Zika Virus IgM and IgG, Days 0–300 after Onset of Symptoms\n\n【26】We used the panel of sequential samples from patients with confirmed Zika virus infection to determine the kinetics of Zika virus IgM and IgG over 10 months after clinical onset . For the Euroimmun Zika virus IgM test, maximum percentage detection (71%) was 15–30 days after the onset of disease. After this time, the percentage of detectable IgM decreased rapidly, to only 21% of samples collected in days 31–60 and <9% for those collected >60 days after infection . The Dia. Pro Zika virus IgM test was more sensitive, with higher rates of positive samples observed over a longer time: 29% positive samples on days 0–4 after clinical onset, increasing to a maximum of 93% positivity during days 15–30. The positivity rate decreased more slowly than with the Euroimmun test, and 29% of samples were still positive for IgM during days 181–300 after infection. For Zika virus IgG, both assays detected the antibody in >40% of samples collected during the acute phase of disease (days 0–4) and in 100% of samples collected during days 31–180 after onset of disease . A slight decrease in the positivity rate for IgG observed with the Euroimmun assay before day 300 suggests a possible lack of sensitivity over time.\n\n【27】We also evaluated the evolution of the overall mean S/CO ratios by time and the test used . We observed similar kinetics for the mean S/CO ratios for the 2 assays: the maximum mean S/CO ratio peaked during days 15–31 for the IgM assays and during days 91–180 for the IgG assays. Nevertheless, the differences between the mean S/CO ratios for both the IgM and the IgG assays at each time after infection class were significant (all p<0.05) . The Dia.Pro assays gave higher S/CO ratios for the same threshold values, explaining the greater sensitivity of these tests. We obtained the kinetics of Zika virus IgM and IgG with both assays for patients for whom we had \\> 5 sequential samples are .\n\n【28】### Discussion\n\n【29】In this assessment of the performance of Euroimmun and Dia.Pro Zika virus IgM and IgG ELISAs for diagnosis of Zika virus infection, we had a large panel of well-characterized samples from areas endemic for arboviruses, the dates of symptom onset, and infection confirmed by real-time RT-PCR. Although the Dia.Pro IgM assay was more sensitive (69%) than the Euroimmun IgM test (49%), the performance of both IgM assays was suboptimal. The sensitivity of a screening test is a major factor in determining its usefulness, because poor sensitivity of the first test used in a diagnostic algorithm could lead to false-negative results that would not be further evaluated.\n\n【30】The sensitivity of the Euroimmun Zika virus IgM test was significantly lower than that reported previously  _._ The differences might be caused by differences in study design, selection criteria, and the few positive samples in the previous studies . The lower sensitivity we found for this assay might also be the result of the larger proportion of secondary flavivirus infections in the panel used, as the samples were taken in an area endemic–epidemic for dengue and with mandatory vaccination against yellow fever. A significant lower sensitivity of the Euroimmun Zika virus IgM assay has also been reported in travelers from Israel compared with travelers from Europe and Chile, possibly related to the West Nile virus background immunity of the population of Israel . More recent evaluations have also reported low sensitivity (39.5% and 37%) of the Euroimmun assays  _._\n\n【31】Sensitivity is essential for a frontline diagnostic test, and specificity should also be carefully evaluated. In our study, we assessed the specificity of all the tests with various non–Zika virus samples to evaluate potential cross-reactivity. A first subpanel of samples from patients with confirmed acute dengue virus infection was formed because of the high potential for flavivirus cross-reactivity; a second subpanel consisted of samples from patients with confirmed chikungunya virus infection; and a third subpanel consisted of samples from patients with no dengue virus, chikungunya virus, or Zika virus infection. The specificity of the Euroimmun Zika virus IgM assay was 99% and that of the Dia.Pro test was 96%, with cross-reactivity varying according to the subpanel. Most cross-reactions were observed in the subpanel of acute dengue samples, in which 3 of 33 samples were false positive for Zika virus IgM, whereas 1 of 74 samples collected >1 year after the dengue epidemic (chikungunya virus subpanel and dengue virus, chikungunya virus, and Zika virus negative subpanel) was false positive. Maximum cross-reactivity of IgG tests was seen in the subpanel collected in the post–dengue epidemic period in 2014–2015. The false-positivity rate was 40% (14/35) in the acute dengue subpanel for both commercial tests; the rate grew to 45.8% (11/24) for the Euroimmun Zika virus IgG test and 58.6% (17/29) for the Dia.Pro Zika virus IgG test in the subpanel of acute chikungunya virus samples collected right after the dengue epidemic period in 2014–2015. The IgG cross-reactions tended to decrease with time: 2 years after the end of the latest dengue epidemic in French Guiana, 5/42 (11.9%) samples were falsely positive by the Euroimmun IgG test and 9/42 (21.4%) samples were falsely positive with the Dia.Pro test. All samples except 2 with a false-positive result for Zika virus IgG were positive for dengue virus IgG by our in-house IgG antibody capture ELISA technique. The predictive positive value of both Zika virus IgG assays therefore largely depends on the epidemiologic situation of other flaviviruses, like dengue virus.\n\n【32】A study performed in Martinique during March–June 2016 during the Zika virus epidemic showed a good correlation between a high Euroimmun Zika virus IgG ratio and a positive Zika virus seroneutralization result. Ratios >4 were associated with positive seroneutralization in >95% of cases, whereas ratios >5 were associated with seroneutralization in 100% of cases  _._ At the time of the study, only sporadic dengue cases were reported in Martinique, as the previous epidemic occurred in 2013–2014, >2 years earlier. In our assessment, the IgG S/C ratio value of false Zika virus IgG–positive samples varied from 1.1 to 6.7 (mean 3.9) for acute dengue samples to 1.2 to 3.7 (mean 2.0) for other non–Zika virus samples. These results underline the importance and potential efficacy of using selected panels to evaluate performance and, especially, the specificity of serologic assays. These results also indicate that when there is major cocirculation of dengue virus and Zika virus, the interpretation of serologic assays could be increasingly complex.\n\n【33】The overall specificity of the Euroimmun Zika virus IgG assay was 79% and that of the Dia.Pro assay was 62%. These results indicate suboptimal specificity, which is lower than that reported previously , and is a concern for serologic diagnosis of Zika virus infection. Seroneutralization is the classical reference test for confirming contact with Zika virus in cases of positive results with ELISA assays in regions where flaviviruses cocirculate; however, even seroneutralization tests could be difficult to interpret, leading the US Centers for Disease Control and Prevention to change its guidance for interpretation of Zika virus antibody test results in May 2016 . In cases of secondary flavivirus infection, a microneutralization test might not discriminate between the past and recent infecting viruses, leading to an assumption of just a recent flavivirus exposure.\n\n【34】As reported previously by others, when IgM and IgG results were combined, sensitivity increased to 82% for Euroimmun and 87% for Dia.Pro assays, whereas specificity decreased to 69% for Euroimmun and 62% for Dia.Pro  _._ However, according to this combined analysis, 42% (30/71) of samples positive by the Euroimmun assays and 21% (16/75) for the Dia.Pro assays correspond to IgM negative/IgG positive samples, for which distinction between recent and past infections is not possible. Thus, a combined interpretation is not suitable for Zika infection diagnosis in the context of endemic–epidemic circulation.\n\n【35】We not only assessed the performance of Euroimmun and Dia.Pro Zika virus IgM and IgG tests but also evaluated the kinetics of the antibodies through 300 days after the onset of symptoms. A major concern in serologic diagnosis of Zika virus infection is determining the date of infection, due to the high proportion of asymptomatic forms. Data on the duration of IgM persistence after Zika virus infection are still limited, but our results indicate that this antibody could persist for at least several months, as described for other arboviruses  _._ Such persistence could preclude determination of the recent nature of an infection, and other assays should be evaluated. The analysis of individual Zika virus antibody kinetics revealed distinct patterns. In some patients (such as patients 4, 5, 10, 16, 53), high IgM ratios during the acute phase were associated with delayed and moderate increases in Zika virus IgG ratios, possibly reflecting a primary flavivirus or Zika virus infection; for other patients (such as patients 20, 34, 41, 58, 62), an early increase in the IgG ratio was combined with a low or even negative Zika virus IgM ratio throughout follow-up, indicating secondary flavivirus infections. These 2 types of kinetics showed contrasted signal intensities, which are not observed with our in-house ELISA assays, in which whole Zika virus is used as an antigen (data not shown).\n\n【36】A limitation of our study is that the Zika virus–positive samples were confirmed by RT-PCR and thus were all from symptomatic cases. If antibody levels are different in symptomatic and asymptomatic infections, as described for dengue, the performance and antibody kinetics observed in this study might be considerably different in samples from asymptomatic infections .\n\n【37】This study highlights the complexity of interpreting serologic assays in areas where various arboviruses cocirculate and demonstrates the importance of evaluating serologic assays with serum specimens from persons living in endemic–epidemic areas and use of parallel testing antibodies to maximize the reliability of diagnosis. Further studies are also needed to identify specific biomarkers of each flavivirus infection for diagnosis after the acute phase of disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "95c57d70-7023-42bc-ab5a-03b22ca2a051", "title": "Development and Validation of a Tool for Assessing Glucose Impairment in Adolescents", "text": "【0】Development and Validation of a Tool for Assessing Glucose Impairment in Adolescents\nAbstract\n--------\n\n【1】**Introduction**  \nChildhood obesity is associated with an increased risk for type 2 diabetes. Early identification of adolescents at risk for impaired fasting glucose may lead to earlier and more comprehensive evaluation and intervention. Because widespread glucose testing of adolescents is not recommended, community-based tools are needed to identify those who could benefit from further testing. One such tool, developed for adults, was the Tool for Assessing Glucose ImpairmenT (TAG-IT). Our objective was to validate whether a similar tool could be useful for community-based screening of glucose impairment risk among adolescents.\n\n【2】**Methods**  \nOur study sample consisted of 3,050 adolescents aged 12 to18 years who had participated in the 1999-2008 National Health and Nutrition Examination Survey (NHANES). Half of participants were female and 40% were nonwhite. NHANES measured fasting glucose and height, weight, and resting heart rate. We used Pearson correlations and regression analysis to determine key variables for predicting glucose impairment. From these measurements, we created a composite TAG-IT score for adolescents called TAG-IT-A. We then applied the TAG-IT-A model to 1988-1994 NHANES data, using linear regression analysis and receiver operating characteristic analysis to determine how well the TAG-IT-A score predicted a fasting glucose at or above 100 mg/dL.\n\n【3】**Results**  \nWe determined that age, sex, body mass index, and resting heart rate were important predictors of impaired fasting glucose and that TAG-IT-A was a better predictor of impaired fasting glucose than body mass index alone (area under the curve, 0.61, _P_ < .001 vs 0.55, _P_ \\= .10, respectively). A TAG-IT-A score of 3 or higher correctly identified 50% of adolescents with impaired fasting glucose, while a score of 5 or higher correctly identified 76% .\n\n【4】**Conclusion**  \nThe TAG-IT-A score is a simple screening tool that clinicians and public health professionals could use to easily identify adolescents who may have impaired fasting glucose and need a more comprehensive evaluation.\n\n【5】Introduction\n------------\n\n【6】Obesity rates have tripled in the past 20 years among children and adolescents, leading to an obesity epidemic in this population . Concurrent with this increase in obesity has been an increase in the incidence of type 2 diabetes in this same population . An estimated 1 in 3 children born in the United States in 2000 will develop type 2 diabetes at some point in their lifetime .\n\n【7】Because few children have impaired fasting glucose, widespread use of glucose screening in this population is not recommended. In addition, many adolescents seek health care infrequently and only for acute problems and thus may go years without contact with a health care provider. New tools are needed for community-based screening to identify those at increased risk of glucose impairment and in need of further evaluation or interventions.\n\n【8】In adults, the Tool for Assessing Glucose ImpairmenT (TAG-IT) was developed for use in the community to assess risk of glucose impairment . Compared with using body mass index (BMI) alone, TAG-IT was much better at predicting glucose impairment. Furthermore, a TAG-IT score of 5 or higher correctly identified those with glucose impairment 87% of the time . In addition to TAG-IT, He et al  developed the Abnormal Glucose Risk Asessment-6 (AGRA-6), which includes 6 models for assessing risk for abnormal glucose levels. These models examine different measures of abnormal glucose, such as impaired fasting glucose, impaired glucose tolerance, undiagnosed diabetes, and all other types of glucose impairment. The area under the curve for all 6 models was 0.72 to 0.80. The authors also reported that the AGRA-6 models accurately predicted about 70% of those with abnormal blood glucose .\n\n【9】Although both TAG-IT and AGRA-6 are valid community screening tools to assess glucose impairment risk in adults , to our knowledge, a similar glucose impairment screening tool for adolescents (age 12-18 y) does not exist. Adolescents are growing and transitioning through puberty, which may affect body composition, physical activity levels, resting heart rate, and insulin resistance. Therefore, methods for predicting insulin resistance or glucose impairment in adults may not be effective in adolescents.\n\n【10】Our objective was to develop and validate a community-based screening tool specifically for adolescents. The tool is based on TAG-IT, the adult screening tool, and is called Tool for Assessing Glucose ImpairmenT among Adolescents (TAG-IT-A). We hypothesized that the TAG-IT-A tool would effectively predict risk of impaired fasting glucose levels in adolescents.\n\n【11】Methods\n-------\n\n【12】The Centers for Disease Control and Prevention (CDC) conducts the National Health and Nutrition Examination Survey (NHANES) annually. NHANES uses complex probability sampling with a cluster sample design to assess the health and nutritional status of adults and children in the United States. For our study, we used multiple NHANES data sets. We used the 1999-2008 NHANES data to develop TAG-IT-A because NHANES oversampled adolescents and racial/ethnic minorities during these years to provide more precise population estimates. Furthermore, given the rise in the prevalence of type 2 diabetes, there were more adolescents with elevated glucose levels in later years  compared with earlier NHANES data sets . After using the 1999-2008 NHANES data to develop TAG-IT-A, we used NHANES III  data to validate the TAG-IT-A score in a manner similar to that described by Koopman . The NHANES protocols were approved by CDC’s institutional review board. We obtained written informed consent from legal guardians and assent from minors.\n\n【13】### TAG-IT-A development sample\n\n【14】To develop the TAG-IT-A score, we used data from adolescents aged 12 to 18 who participated in the fasting subsample of the NHANES 1999-2008 survey. We excluded participants who did not have a fasting blood sample, were currently taking diabetes medication, or had incomplete data on key variables used in the data analysis.\n\n【15】### TAG-IT-A variables\n\n【16】The purpose of the screening tool is to use demographic and noninvasive clinical variables to identify adolescents with a fasting blood glucose at or greater than 100 mg/dL, defined by the American Diabetes Association as impaired . Key variables used in the development of the screening tool for adolescents were modeled after those used to develop the adult screening tool, TAG-IT . The variables considered for inclusion in the adolescent screening tool were age, race, sex, body mass index (BMI), resting heart rate, hypertension diagnosis (measured resting blood pressure or physician diagnosis), and fasting blood glucose. The only variable investigated in the adult tool that was not explored with the adolescent screening tool was a family history of diabetes, because NHANES did not ask adolescents about this.\n\n【17】Age, race, and sex were all self-reported, and race was categorized as non-Hispanic white, non-Hispanic blacks, Hispanic, or other (Asian and American Indian). Age was then divided into 2 groups, 12 to 14 and 15 to 18. BMI was calculated from height and weight, which was measured by trained technicians who used standard NHANES methods . We used BMI values from the CDC growth charts for US children to categorize each adolescent as normal weight (≤ 84th percentile), overweight (85th-94th percentile), or obese (≥95th percentile) . Resting heart rate and blood pressure were measured by using standard NHANES procedures . Resting heart rate was grouped into 10-beat intervals (eg, <60, 60-69, 70-79.). We classified hypertension as a resting blood pressure at or greater than the 95th percentile adjusted for age, sex, and height or participant self-report of a physician diagnosis of hypertension .\n\n【18】### TAG-IT-A validation sample\n\n【19】To validate TAG-IT-A we used data on adolescents who participated in the fasting glucose subsample of NHANES III . We excluded adolescents who did not have a fasting blood sample, were currently taking diabetes medication, or had incomplete data on key variables used in the data analysis. The same variables used to develop the TAG-IT-A tool were used in the validation sample. Age, race, sex, and BMI were used as the predictive variables of the population from the NHANES III database, and fasting glucose was used as the outcome variable to validate the TAG-IT-A score.\n\n【20】### Statistical analysis\n\n【21】In all analyses, we used NHANES weights to accommodate the complex sampling design of NHANES. The weights account for geographic probability sampling methods and oversampling of specific groups, such as minorities and adolescents, and are designed to ensure that the weighted findings are representative of all US adolescents.\n\n【22】We used scatter plots to examine the relationship between fasting glucose levels and age, race, sex, BMI, resting heart rate, and hypertension. Pearson product moment and point biserial correlations were used to examine the relationships among these key variables. Following the approach taken by Koopman et al  in adults, we used multiple regression analysis to identify the variables associated with a fasting glucose of 100 mg/dL or higher. Potential independent variables that we entered into the regression model were age, race, sex, BMI, resting heart rate, and hypertension diagnosis; fasting glucose was the dependent variable entered. If the _P_ value was less than or equal to .05, the variable was associated with an impaired fasting glucose level. Once we identified significant variables, we used logistic regression analysis to examine the relationship of these variables to fasting glucose levels. We also used the logistic regression model to generate weighted scores for each key factor that the TAG-IT-A score comprised. The method for developing the weighted scores was modeled after the widely used Charlson Comorbidity Index , in which odds ratios (ORs) from 1.0 to 1.19 were assigned 0 points, ORs from 1.2 to 1.49 were assigned 1 point, ORs from 1.5 to 2.49 were assigned 2 points, and so on. After the TAG-IT-A scoring system was created, receiver operating characteristic (ROC) curves were constructed to examine the area under the curve for TAG-IT-A’s ability to predict fasting glucose impairment in the NHANES III sample. For comparison, an ROC curve was also generated to examine the ability of BMI alone to predict fasting glucose impairment in the NHANES III sample. We used SAS-callable SUDAAN version 9 (RTI International, Research Triangle Park, North Carolina) to conduct all data analysis except for ROC generation, for which we used SAS version 9 (SAS Institute, Inc, Cary, North Carolina). We set statistical significance at _P_ <.05.\n\n【23】Results\n-------\n\n【24】A total of 3,050 adolescents from NHANES 1999-2008 with complete data were included in the analysis to develop TAG-IT-A. Most participants were white (63%); 51% were male, 32% were either overweight or obese, and 16% had a fasting glucose value of 100 mg/dL or higher . A total of 2,564 adolescents from the NHANES III sample were analyzed for validation of TAG-IT-A. The sample was primarily white (66%); 51% were male, 27% were either overweight or obese, and 11% had a fasting glucose value of 100 mg/dL or greater.\n\n【25】Correlations among the variables examined for the TAG-IT-A score were low, but we found a positive and significant relationship between fasting glucose, age, BMI, and resting heart rate . Age was positively related to BMI but negatively related to resting heart rate. Sex was positively related to BMI and resting heart rate. Finally, BMI was positively related to hypertension status.\n\n【26】The variables related to impaired fasting glucose included being aged 12 to 14, male, obese, and having a resting heart rate of 70 beats per minutes or higher . The odds of having an impaired fasting glucose level for each of these variables were at least 1.66 times as great as those of the reference group.\n\n【27】Twenty-nine percent of the adolescents in NHANES III had a TAG-IT-A score of 3 or higher, and 14% had a TAG-IT-A score of 7 or higher . As scores increased from 3 or higher, the sensitivity improved; a TAG-IT-A score of 5 correctly identified 76% of participants with impaired fasting glucose .\n\n【28】The area under the curve was 0.61 for the TAG-IT-A tool compared with 0.55 for BMI alone, which suggests that the TAG-IT-A tool is modestly more useful than BMI alone in identifying adolescents who may need additional evaluation for impaired fasting glucose .\n\n【29】Discussion\n----------\n\n【30】We found that fasting glucose levels were positively related to BMI. Eisenmann et al  reported that various markers for insulin sensitivity, including fasting glucose levels, were higher among obese children than among normal-weight children. We have reported similar findings in our patient population . In addition, we found that BMI was positively correlated with hypertension status. Obese girls were 6 times more likely to have hypertension than normal-weight girls . Falaschetti et al  reported that a 1 kg/m2 increase in BMI in children was associated with a 1.4 mm Hg higher systolic blood pressure. To our knowledge, the positive relationship between fasting glucose and resting heart rate has not been examined in the adolescent population. Among adults, elevated resting heart rates are linked to higher all-cause and cardiovascular disease mortality  and higher odds for elevated blood pressure (systolic and diastolic), abnormal fasting glucose, hypertriglyceridemia, and obesity . Although only 4% of adolescents in our sample had a heart rate higher than 100 beats per minute, measuring and tracking resting heart rate may be important because those with a resting heart rate above 70 beats per minute were twice as likely to have impaired fasting glucose.\n\n【31】Age, sex, BMI, and resting heart rate were the only predictors included in the final model of the TAG-IT-A score. These variables are different from what was included in the adult TAG-IT score. Koopman et al  found that, in addition to these 4 predictors, it was necessary to include family history of diabetes and history of hypertension in the model. Our analysis was limited by the lack of available data on family history of diabetes in the NHANES data sets. Including family history of diabetes in the model may have improved its predictive value as measured by ROC analysis . Others have developed screening tools for assessing abnormal blood glucose values in adults and reported that in addition to age, BMI, and race, hypertension medication use was also an important predictor . A group from Denmark reported that identification of adolescents at risk for developing type 2 diabetes was improved when blood glucose values and triglyceride concentrations were added to systolic blood pressure and parental diabetes .\n\n【32】We hypothesized that the TAG-IT-A score would be an effective predictor of impaired fasting glucose among adolescents; however, the TAG-IT-A tool was only modestly better than BMI alone (0.61 vs 0.55, respectively). The area under the curve for the impaired glucose tool in adolescents is less than what has been found in adults (0.61 vs 0.74-0.80). One reason this tool may not be as predictive in adolescents is that other variables, such as lipid aerobic fitness, physical activity levels, or puberty and sexual development, may also be important . Unfortunately, many of these variables were not consistently available in the NHANES data sets for significance testing and inclusion in the model. Also, the goal of this study was to develop a tool for assessing risk of impaired fasting glucose that uses noninvasive variables easily measured in nonmedical settings. Although some lipid measures are available in NHANES, they were excluded from inclusion in the model in order to investigate its applicability for community use. Beyond community use, the parameters necessary for calculating the TAG-IT-A score may also be routinely available in electronic medical records that could be programmed to automatically calculate this score for appropriate patients.\n\n【33】A logical question is what TAG-IT-A score public health or medical practitioners should use to determine if an adolescent needs further screening. If a score of 3 is used, then sensitivity is 50%, but sensitivity improves with higher criteria. The economic costs of additional tests need to be considered. Furthermore, health care practitioners need to decide if they want to maximize the number of cases identified correctly (sensitivity); if so, a value of 5 or higher might be used. In contrast, if health care practitioners are more concerned about minimizing the number of false positives, then a lower score with a higher specificity could be used (eg, 3 or higher, 65% specificity).\n\n【34】Few children and adolescents have fasting glucose values above 100 mg/dL . For this reason, using fasting glucose measurements to screen the entire population of adolescents is not recommended. New tools are needed to help identify at-risk adolescents who may benefit from additional testing to detect abnormal fasting glucose concentrations. Community-based screening of adolescents using the TAG-IT-A tool instead of BMI alone may provide a feasible and somewhat more sensitive way of identifying adolescents at increased risk of glucose impairment and in need of further evaluation or interventions. Having easily assessed measures (eg, heart rate and blood pressure) that are related to illness and death in adults  suggests that similar measures may indicate health concerns in adolescents as well.\n\n【35】The limitations of this study include the cross-sectional design of NHANES; adolescents who have fasting glucose values of 100 mg/dL or higher may not go on to develop type 2 diabetes. Nguyen et al , however, reported that even children who had normal values but were at the higher end of the normal range had an increased risk for developing type 2 diabetes as adults. Based on our findings and those of Nguyen et al , it appears that a longitudinal study is warranted to evaluate the effectiveness of the TAG-IT-A tool. Finally, we did not perform subgroup analysis by race because of the small sample size of those with impaired fasting glucose (only 11% of the total NHANES III adolescent sample). Therefore, it is unknown whether the tool performs similarly in different races. Similar sample-size issues were observed when sex was explored separately.\n\n【36】The TAG-IT-A tool is a simple measure that uses variables that can be obtained in community settings, and it is modestly better than BMI alone in predicting risk for impaired fasting glucose. The usefulness of this screening tool for identifying adolescents who have impaired fasting glucose should be further evaluated in community-based settings. Longitudinal studies are also needed to determine if adolescents with a high TAG-IT-A score subsequently develop type 2 diabetes in adulthood.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7b945a7-76a3-4ec7-a230-263a66c96f7a", "title": "Shedding of Marburg Virus in Naturally Infected Egyptian Rousette Bats, South Africa, 2017", "text": "【0】Shedding of Marburg Virus in Naturally Infected Egyptian Rousette Bats, South Africa, 2017\nThe genus _Marburgvirus_ , family _Filoviridae_ , comprises 1 species, _Marburg marburgvirus_ , which comprises 2 marburgviruses, Marburg virus (MARV) and Ravn virus (RAVV) . Marburgviruses cause sporadic but often fatal MARV disease in humans and nonhuman primates . The Egyptian rousette bat ( _Rousettus aegyptiacus_ ) has been implicated as the primary reservoir for marburgviruses , but the mechanisms by which they are maintained in these bats remain elusive. Evidence of marburgvirus circulation was reported from countries where MARV disease outbreaks have not been recorded . Determining the risks for spread and developing evidence-based public health strategies to prevent zoonotic transmission requires up-to-date knowledge about marburgvirus geographic range; genetic diversity; and transmission mechanisms, including natural ports of entry and shedding patterns. To clarify which marburgviruses are circulating and how they are maintained in Egyptian rousette bat populations in South Africa, we tested oral and rectal swab samples and blood samples collected during a previously identified peak season of marburgvirus transmission in a local Egyptian rousette bat population .\n\n【1】### The Study\n\n【2】We conducted this work in accordance with approved protocols by animal ethics committees of the National Health Laboratory Service (Johannesburg, South Africa; AEC 137/12) and the University of Pretoria (Pretoria, South Africa; EC054–14). During February–November 2017, a total of 1,674 Egyptian rousette bats (February, 107 bats; April, 600; May, 563; September, 214; November, 190) were captured, aged, and sampled at Matlapitsi Cave, Limpopo Province, South Africa, as described previously . We collected and processed oral and rectal swab samples from each bat and blood from a subset of 423 bats, as described previously . Swab samples were collected and pooled by aliquoting 4 × 25 μL of the media of each sample into a microcentrifuge tube, yielding a total of 416 pools containing 100 μL of pooled rectal swab samples and 4 pools containing 50 or 75 μL of pooled oral swab samples. We conducted serologic, virologic, and molecular tests as described previously . In addition, we conducted real-time quantitative reverse transcription PCR (qRT-PCR) for individual swab samples when the pool tested positive. We prepared sequencing libraries using the TruSeq RNA Access Kit  with MARV-specific bait enrichment, followed by sequencing on an Illumina NextSeq, genomic alignment and phylogenetic analysis . We calculated the significance of differences in several positive swab samples and seropositivity using the Fisher exact test in Stata version 13 .\n\n【3】Seven rectal swab pools (5 from April 2017 and 2 from September 2017) were qRT-PCR positive; the remaining rectal swab samples collected during February–November 2017 were all negative. The number of individual positive rectal swab samples ranged from 1 to 3 per positive pool, totaling 11 positive samples. Only 1 oral swab sample pool, from April 2017, yielded a positive qRT-PCR result, containing a single positive oral swab sample . Of 600 rectal swab samples collected during 3 nights in April, 9 (1.5%) were positive; of 215 rectal swab samples collected during 2 nights in September, 2 (0.9%) were positive. We found no significant difference between the number of positive rectal swab samples collected in April and the number collected September. The number of positive rectal swab samples differed significantly from the number of positive oral swab samples collected in April (p = 0.02). Attempts to culture marburgvirus from qRT-PCR–positive swab samples were unsuccessful. Identical results from specimens with cycle threshold (C t  ) >30 were obtained in other studies .\n\n【4】We obtained sufficient marburgvirus-specific sequence data only from 1 of the 12 individual positive swab samples for phylogenetic analysis: a rectal swab sample, collected from a juvenile female (bat 8095) in September 2017, from which we recovered 79.2% (15.1/19.1 kb) of the genome. We merged sequencing reads from replicate sequencing runs and mapped 2,472 reads to the MARV reference genome. Maximum coverage per base obtained was 291 reads; some regions had no coverage. The average coverage per base across the genome was 18.5 reads (when we included 0 coverage regions), and the average coverage when we excluded 0 coverage regions was 40 reads. We obtained near-complete coding sequences for the viral protein (VP) 35 (972/990 nt; 98.2%) and VP40 (898/912 nt; 98.5%) genes; coverage ranged from 49.7% (VP24) to 89.3% (glycoprotein) in other open reading frames of the genome.\n\n【5】The marburgviruses sequence (strain RSA-2017-bat8095) detected from the rectal swab sample of bat 8095 shared a common ancestor with all other RAVV complete or near-complete genome sequences, including 3 human isolates from Kenya , Uganda , and the Democratic Republic of Congo  and several bat isolates from Uganda  . The RSA-2017-bat8095 nt sequence shared »77% identity with the MARV SPU191–13bat2764 Mahlapitsi strain  that was collected and characterized from Matlapitsi Cave 4 years earlier.\n\n【6】Of 423 bats tested, 143 (33.8%) were positive for antibodies against marburgviruses (73 adults and 70 juveniles). Lowest overall seroprevalence occurred in April 2017 (17.78%) and ranged from 8.3% in juveniles (9/108) to 55.6% in adults (15/27) . Overall seropositivity did not differ significantly between male and female bats, but the overall seropositivity differed significantly between juvenile (forearm <89 mm; <1-year-old) and adult bats (p = 0.03). We detected seroconversion in 6 (33.3%) of 18 recaptured bats .\n\n【7】### Conclusions\n\n【8】The period of the lowest seropositivity in juveniles (April–May) resulting in the highest number of potentially susceptible bats at Matlapitsi Cave was the same as previously identified . This finding coincides with demonstrable seroconversions and virus shedding and represents a period of increased exposure. The significantly higher number of marburgvirus-positive rectal than oral swab samples we detected contrasts with results from experimentally infected Egyptian rousette bats and field studies in Uganda . Experimental data on marburgvirus shedding were obtained from subcutaneously inoculated and colonized Egyptian rousette bats . Whether this mode of infection represents a natural portal of entry for marburgviruses in Egyptian rousette bats and to what extent viral shedding patterns in colonized bats can be extrapolated to natural settings are unknown.\n\n【9】Our findings highlight the risk for marburgvirus fecal environmental contamination and for Egyptian rousette bat roosting sites as a possible source of virus spillover. Roosting behavior enabling direct physical contact suggests that fecal–oral transmission of marburgviruses in bats can occur. Biting among animals or biting by hematophagous ectoparasites might result in inoculation of wounds with contaminated feces or exposure to contaminated fomites.\n\n【10】Our findings, combined with earlier detection of an Ozolin-like MARV in Egyptian rousette bats roosting at Matlapitsi Cave , suggest local co-circulation of multiple marburgviruses genetic variants. Detection of RAVV in South Africa, closely related to East African isolates, indicates that long-distance movement of Egyptian rousette bats contributes to widespread geographic dispersion of marburgviruses. Moreover, it implies that more virulent strains, such as the MARV Angolan strain , might be co-circulating. Entering caves and mining have been associated with MARV spillover  and detection of viral RNA in rectal swab samples, highlight the potential route of transmission. Confirmation of the period for the highest virus exposure risk further highlights the value of biosurveillance and demonstrates that marburgviruses continue endemic circulation in South Africa. This circulation represents a potential threat that needs to be communicated to at-risk communities as a part of evidence-based public health education and prevention of pathogen spillover.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "85235524-5735-475f-8ebd-d205a1b0271f", "title": "West Nile Virus Infection and Conjunctival Exposure", "text": "【0】West Nile Virus Infection and Conjunctival Exposure\n**To the Editor:** Corvids (crows, blue jays, magpies, and their relatives) are particularly susceptible to West Nile virus (WNV) . Birds are useful indicators of the spread of WNV , and Canada has implemented WNV surveillance strategies that use these species as sentinels.\n\n【1】Direct acquisition of WNV through percutaneous injuries has been reported in 2 laboratory circumstances, involving a blue jay and a mouse . We describe a conjunctival exposure to WNV that occurred in the field and probably resulted in infection in the exposed person.\n\n【2】As part of the local WNV bird surveillance activities in 2003, an animal control officer collected sick and dead corvids at the Canadian Forces Base, Suffield, Alberta. He had a protective suit on, but he wore no mask or face shield. While killing an injured crow ( _Corvus brachyrhynchos_ ), the officer struck the struggling bird on a nearby horizontal pipe gate, which resulted in fracture of the skull, causing brain tissue and cerebrospinal fluid to spray onto his head, face, neck, and right shoulder. Body fluids and brain material of the bird entered his eyes, but not his mouth; he had no known open lesions on the exposed area. His co-workers immediately wiped off visible material, and a few hours later he showered.\n\n【3】The dead crow was sent for analysis to the Fish and Wildlife Division, Government of Alberta, where laboratory tests using the VecTest assay (Medical Analysis Systems, Inc. Camarillo, CA, USA), indicated that the crow was positive for WNV antigen. This test has been validated for detecting viral antigen in oropharyngeal and cloacal swabs in crows .\n\n【4】Seven days after exposure, the animal control officer became unwell and sought medical assistance. His symptoms included headaches, dizziness, spiking fevers, and sweats; on examination, mild otitis was noted, but he did not display meningismus or other neurologic signs. A whole blood sample with EDTA and a serum sample were collected, together with a throat swab for viral culture to exclude a possible enteroviral infection, as part of a standardized provincial protocol for investigating suspected WNV infections in Alberta. Betahistine dihydrochloride was prescribed for the dizziness and a cephalosporin for otitis. A cerebrospinal fluid sample was not collected, since his clinical signs did not suggest neurologic involvement.\n\n【5】At the Provincial Laboratory, WNV RNA was detected in the plasma by nucleic acid sequence-based amplification, with primers described by Lanciotti and Kerst , which was confirmed by the Artus RealArt RT-PCR assay (artus biotech USA Inc, San Francisco, CA, USA) in a Roche LightCycler. The serum sample, collected at the same time as the plasma sample, was negative for immunoglobulin M (IgM) antibody by enzyme immunoassay using 2 kits (Panbio, Windsor, Queensland, Australia; and Focus Technologies, Cypress, CA, USA). Fourteen days after exposure, a convalescent-phase serum sample showed IgM antibody to WNV in both kits; the plasma sample was negative for viral RNA. Hemagglutination inhibition assay on the acute- and convalescent-phase serum samples, collected 7 days apart, showed rising titers, from <1:10 on the acute-phase serum, to 1:40 for dengue virus (serotypes 1–4), 1:40 for St. Louis encephalitis virus, and 1:80 for WNV on the convalescent-phase serum. Preliminary data from our laboratory indicate that in ≈40% of cases of acute West Nile fever, the acute-phase plasma sample shows viral RNA before IgM antibody develops, after which viral RNA is no longer detectable . Two weeks after culture was initiated for virus isolation, the throat swab was negative for enteroviruses.\n\n【6】The patient's severe fever, sweats, headaches, anorexia, fatigue, and diminished concentration and memory continued. His symptoms peaked 2 weeks after the initial exposure. Three months later, his symptoms of fatigue, dizziness, headaches, and poor memory were severe enough to prevent him from returning to fulltime work. Eight months after exposure, he continues to have fatigue and headaches.\n\n【7】We believe this is the first reported case of apparent conjunctival transmission of WNV in an occupational setting. As the officer spent considerable time outdoors in areas where WNV transmission was relatively high in 2003 and repeatedly handled infected birds, we cannot eliminate the possibility of a mosquito bite or other percutaneous route of transmission. However, the nature of the exposure and the time to symptom development strongly suggest that infection occurred after conjunctival exposure. Persons who dispatch sick wildlife are encouraged to use appropriate, humane methods and should take precautions against exposure to tissues and body fluids.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8d993f2c-b872-4a18-b056-d5a1b6a564d8", "title": "What Is a Mammogram?", "text": "【0】What Is a Mammogram?\nRegular mammograms are the best tests doctors have to find breast cancer early.\n\n【1】A mammogram is an X-ray picture of the breast. Doctors use a mammogram to look for early signs of breast cancer. Regular mammograms can find breast cancer early, sometimes up to three years before it can be felt.\n\n【2】**Are you worried about the cost?** CDC offers free or low-cost mammograms. Find out if you qualify.\n\n【3】How is a mammogram done?\n------------------------\n\n【4】You will stand in front of a special X-ray machine. A technologist will place your breast on a plastic plate. Another plate will firmly press your breast from above. The plates will flatten the breast, holding it still while the X-ray is being taken. You will feel some pressure. The steps are repeated to make a side view of the breast. The other breast will be X-rayed in the same way. You will then wait while the technologist checks the X-rays to make sure the pictures do not need to be redone. Keep in mind that the technologist cannot tell you the results of your mammogram. Each woman’s mammogram may look a little different because all breasts are a little different.\n\n【5】What does having a mammogram feel like?\n---------------------------------------\n\n【6】Having a mammogram is uncomfortable for most women. Some women find it painful. A mammogram takes only a few moments, though, and the discomfort is over soon. What you feel depends on the skill of the technologist, the size of your breasts, and how much they need to be pressed. Your breasts may be more sensitive if you are about to get or have your period. A doctor with special training, called a radiologist, will look at the X-ray for early signs of breast cancer or other problems.\n\n【7】Tips for Getting a Mammogram\n\n【8】*   Try not to have your mammogram the week before you get your period or during your period. Your breasts may be tender or swollen then.\n*   On the day of your mammogram, don’t wear deodorant, perfume, or powder. These products can show up as white spots on the X-ray.\n*   Some women prefer to wear a top with a skirt or pants, instead of a dress. You will need to undress from your waist up for the mammogram.\n\n【9】When will I get the results of my mammogram?\n--------------------------------------------\n\n【10】You will usually get the results within a few weeks, although it depends on the facility. A radiologist reads your mammogram and then reports the results to you and your doctor. If there is a concern, you will hear from the mammography facility earlier. Contact your health care provider or the mammography facility if you do not receive a report of your results within 30 days.\n\n【11】An example of a normal mammogram. Each woman’s mammogram may look a little different because all breasts are a little different.\n\n【12】What happens if my mammogram is normal?\n---------------------------------------\n\n【13】Continue to get mammograms according to recommended time intervals. Mammograms work best when they can be compared with previous ones. This allows the radiologist to compare them to look for changes in your breasts.\n\n【14】What happens if my mammogram is abnormal?\n-----------------------------------------\n\n【15】An abnormal mammogram does not always mean that there is cancer. But you will need to have additional mammograms, tests, or exams before the doctor can tell for sure. You may also be referred to a breast specialist or a surgeon. It does not necessarily mean you have cancer or need surgery. These doctors are experts in diagnosing breast problems. Doctors will do follow-up tests to diagnose breast cancer or to find that there is no cancer.\n\n【16】Where can I get a mammogram and who can I talk to if I have questions?\n----------------------------------------------------------------------\n\n【17】*   If you have a regular doctor, talk to him or her.\n*   Contact the National Cancer Institute.\n*   For Medicare information, you can call 1-800 MEDICARE  or visit The Centers for Medicare & Medicaid Services.\n*   CDC’s National Breast and Cervical Cancer Early Detection Program works with health departments and other groups to provide low-cost or free mammograms to women who qualify. Find out if you qualify.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "679da0ab-1eeb-43ac-84c5-7ff1de9ac3a9", "title": "Classification of Older Adults Who Have Diabetes by Comorbid Conditions, United States, 2005–2006", "text": "【0】Classification of Older Adults Who Have Diabetes by Comorbid Conditions, United States, 2005–2006\nAbstract\n--------\n\n【1】**Introduction**  \nOlder adults who have diabetes vary widely in terms of comorbid conditions; these conditions help determine the risks and benefits of intensive glycemic control. Not all people benefit from intensive glycemic control. The objective of this study was to classify by comorbid conditions older American adults who have diabetes to identify those who are less likely to benefit from intensive glycemic control.\n\n【2】**Methods**  \nWe used latent class analysis to identify subgroups of a nationally representative sample of community-dwelling older adults (aged 57–85 y) who have diabetes (n = 750). The subgroups were classified according to 14 comorbid conditions prevalent in the older population. Using the Akaike Information Criterion, the Bayesian Information Criterion (BIC), the sample-size adjusted BIC, and the χ 2  goodness-of-fit statistic, we assessed model fit.\n\n【3】**Results**  \nWe found 3 distinct subgroups. Class 1 (63% of the sample) had the lowest probabilities for most conditions. Class 2 (29% of the sample) had the highest probabilities of cancer, incontinence, and kidney disease. Class 3 (9% of the sample) had the highest probabilities (>90%) of congestive heart failure and myocardial infarction. Class 1 had only 0, 1, or 2 comorbid conditions, and both class 2 and class 3 had 6 or more comorbid conditions. The 5-year death rates for class 2 (17%) and class 3 (33%) were higher than the rate for class 1 (9%).\n\n【4】**Conclusion**  \nOlder adults who have diabetes, cardiovascular disease, and 6 or more comorbid conditions may represent a subgroup of older adults who are less likely to benefit from intensive glycemic control.\n\n【5】Introduction\n------------\n\n【6】According to the American Diabetes Association (ADA), the glycemic target for most adults who have diabetes is a glycosylated hemoglobin (HbA1c) of less than 7.0% . Although the strategy of intensive glycemic control (defined as an HbA1c of <7%) may benefit many people who have diabetes, it may not benefit many older adults . Older adults who have diabetes are more heterogeneous than younger adults in terms of diabetes duration, functional ability, and comorbid conditions. The number and type of comorbid conditions may determine the risks and benefits of intensive glycemic control. The Action to Control Cardiovascular Risk in Diabetes (ACCORD) trial heightened concerns about the harms of very intensive glycemic control (defined as an HbA1c <6.5%) among older adults . In light of these concerns, the current challenge is to identify older adults who are likely to benefit from intensive glycemic control.\n\n【7】Clinical markers may help identify these subgroups of older adults. For frail adults and adults who have a life expectancy less than 5 years, the American Geriatrics Society recommends less intensive glycemic control than that recommended by ADA . The 2010 ADA guidelines identify comorbid conditions, diabetes duration, hypoglycemia risk, and previous failures at intensive control as considerations for less intensive glycemic control . Despite these recommendations, no simple approach for identifying subgroups of older adults who would benefit from intensive glycemic control has been well accepted. For health systems, the identification of subgroups is especially difficult because many relevant markers (eg, hypoglycemia risk, mortality prediction, functional status) are not readily available from electronic medical records.\n\n【8】Classifying older adults who have diabetes according to comorbid conditions may be a practical strategy for identifying subgroups. Unlike methods requiring additional assessments (eg, functional status), data on comorbid conditions are readily available. Comorbid conditions are also associated with life expectancy and the degree to which intensive glycemic control provides benefits . The objective of this study was to classify by comorbid conditions the population of older adults in the United States who have diabetes in order to identify older adults who are less likely to benefit from intensive glycemic control.\n\n【9】Methods\n-------\n\n【10】### Study design\n\n【11】We used latent class analysis (LCA) , the categorical analog to factor analysis, to identify subgroups by comorbid condition. We also compared their clinical characteristics and 5-year death rates. Institutional review boards at the Division of the Social Sciences at the University of Chicago and National Opinion Research Center approved data collection procedures. All respondents provided informed consent before participation in the study.\n\n【12】### Study population\n\n【13】We used data from Wave 1 of the National Social Life, Health, and Aging Project (NSHAP), a longitudinal, population-based study of health and social factors of older, community-dwelling Americans . Wave 1, conducted between July 2005 and March 2006 in English and Spanish, consisted of 3,005 interviews of adults aged 57 to 85; it oversampled men, black and Hispanic men and women, and men and women aged 75 to 84 at the time of screening. The sampling design also took into account urbanicity, defined as the probability-proportionate-to-size (PPS) selection of US Metropolitan Statistical Areas (MSAs) and non-MSA counties. NSHAP included an in-home interview, a biomeasure collection, and a self-administered postinterview questionnaire. Details on the NSHAP survey are available elsewhere . The Wave 1 survey had a weighted response rate of 75.5%, and the postinterview questionnaire had a weighted response rate of 84% .\n\n【14】We analyzed a subsample of 750 survey participants who self-reported diagnosed diabetes or who had undiagnosed diabetes. The following question was used to determine whether participants had diagnosed diabetes: “Has a medical doctor ever told you that you have any of the following conditions: \\[a list of conditions includes ‘diabetes or high blood sugar’\\]?” Participants who had an HbA1c of 6.5% or more and who did not self-report diabetes were considered to have undiagnosed diabetes . HbA1c was measured via dried blood spots during the biologic sample collection; details on the blood spot collection are available elsewhere . We chose not to determine diabetes diagnosis by examining data on prescribed medications because diabetes medications are also used to treat glucose intolerance.\n\n【15】The survey also collected self-reported data on age, sex, race/ethnicity, marital status, education, and number of visits with health care professionals in the previous year. We dichotomized number of health care visits to fewer than 4 visits in the previous year and 4 or more visits. Prescribed medications, including insulin, were cataloged during the in-home interviews . We measured self-rated physical health by using the single-item question, “Would you say your health is excellent, very good, good, fair, or poor?” Self-rated mental health was measured by a similar question. Responses to both questions were dichotomized (excellent/very good/good vs fair/poor). During the in-home interviews, participants also self-reported whether they had difficulty with each of the following activities of daily living for more than 3 months: walking 1 block, walking across a room, dressing, bathing/showering, eating, transferring (getting in/out of bed), and toileting. We collected data on 5-year death rates for the Wave 1 sample during Wave 2 of NSHAP during 2010 and 2011.\n\n【16】### Latent class analysis\n\n【17】For LCA, we used data from all 750 survey participants. We included 14 chronic conditions that are prevalent in the older population : arthritis, cancer, congestive heart failure, dementia, depression, emphysema, falls (in the previous 12 months), hypertension, incontinence (urinary or fecal), kidney disease, myocardial infarction, obesity, stroke, and thyroid disease. All conditions were self-reported except depression and obesity, and all were assessed during the in-home interview except fall history. Depression was measured through the 11-item Iowa form of the Center for Epidemiological Studies Depression scale (CES-D). A score of 9 points or more on the CES-D was identified as depression . Obesity (body mass index \\[BMI\\] ≥30 kg/m2\\] was calculated from interviewer-measured height and weight . Fall history was assessed in the postinterview questionnaire.\n\n【18】We created separate variables for each unique combination of comorbid conditions among participants who had reported data for all conditions; 508 participants, 84% of the participants who returned the postinterview questionnaire, had complete data on all comorbid conditions. At most, 508 unique combinations of comorbid conditions were possible, one for each survey participant.\n\n【19】### Statistical analyses\n\n【20】We fit latent class models successively, starting with a 1-class model and then adding another class for each successive model. Using the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), and the sample-size adjusted BIC (ABIC), we determined the optimal number of latent classes; lower values indicated better fit. Although the BIC is the most widely used criterion for assessing LCA model fit, multiple information criteria are often used in combination to select LCA class number . We also determined model fit by a χ2 goodness-of-fit _P_ value greater than .05. We estimated models using full information maximum likelihood (FIML), which computes a case-wise likelihood function over the observed data by using all available information to estimate model parameters . FIML provides less biased and more efficient estimates than pair-wise deletion, case-wise deletion, or similar response-pattern imputation . We compared model fit between 1-, 2-, 3-, and 4-class models. We used Mplus version 6 (Muthén & Muthén, Los Angeles, California) to conduct the analyses.\n\n【21】We used χ2 analysis and 1-way analysis of variance to describe class differences in sociodemographic characteristics, self-rated health, health care use, clinical characteristics, including the prevalence of multiple comorbid conditions, and 5-year death rates. We also compared classes by frequency and combinations of comorbid conditions. We used STATA version SE10.1 (StataCorp LP, College Station, Texas) to conduct these analyses. All analyses were weighted by using population weights that adjusted for the intentional oversampling of black and Hispanic participants and incorporated a nonresponse adjustment based on age and urbanicity. We adjusted standard errors for sample stratification (sampling strata independently) and clustering (sampling individuals within each of 100 primary sampling units).\n\n【22】Results\n-------\n\n【23】### Model fit\n\n【24】The AIC, BIC, and ABIC decreased as the number of classes increased from 1- to 4-class models . The AIC, BIC, and ABIC were all lowest with the 4-class model, but the AIC and ABIC only marginally decreased between the 3- and 4-class models, and the 4th class had only 7 members. The _P_ value for the χ2 goodness-of-fit statistic was greater than .99 for all 4 models. Thus, we selected a 3-class model to distinguish between population subgroups.\n\n【25】### Class characteristics\n\n【26】The estimated probability of having obesity, hypertension, or arthritis was at least 45% in all 3 classes . Most participants (n = 470, 63%) were in class 1, which was characterized by the lowest probability for nearly all conditions. The estimated probability of having dementia, congestive heart failure, emphysema, kidney disease, or stroke was less than 5% in class 1. Approximately 29% of participants (n = 215) were in class 2. Class 2 had the highest estimated probabilities of cancer, hypertension, incontinence, kidney disease, and obesity. Class 3 (n = 65, 9%) had the highest estimated probabilities of arthritis, congestive heart failure, depression, emphysema, falls, myocardial infarction, stroke, and thyroid disease. The estimated probability of congestive heart failure or myocardial infarction was more than 90% for class 3.\n\n【27】Class 2 had the lowest average HbA1c (Class 1, 7.24%; class 2, 6.86%; class 3, 7.28%) . A higher percentage in class 2 and class 3 reported 4 or more visits with health care professionals in the previous year than in class 1. More than half of class 2 (59%) and class 3 (78%) reported fair or poor physical health, compared with 26% of class 1. In addition, class 2 and class 3 had more difficulty than class 1 with all activities of daily living. For example, 54% of class 2 and 66% of class 3 reported difficulty walking 1 block, compared with 24% of class 1. The 5-year death rate of class 3 (33%) was almost twice the rate of class 2 (17%) and 3 times the rate of class 1 (9%).\n\n【28】### Prevalence of multiple comorbid conditions\n\n【29】Class 2 and class 3 had more comorbid conditions than class 1 (mean number of conditions \\[range\\]: class 1, 2.6 ; class 2, 5.5 ; class 3, 6.7 ) . Only class 1 had 0, 1, or 2 comorbid conditions, and only class 2 and class 3 had 6 or more comorbid conditions.\n\n【30】We found 266 combinations of comorbid conditions. Class 1 had 116 combinations, class 2 had 118 combinations, and class 3 had 32 combinations; we found no overlap of combinations between classes. The most frequent combinations of conditions were arthritis, hypertension, incontinence, and obesity (n = 18) and arthritis, hypertension, and obesity (n = 15).\n\n【31】Discussion\n----------\n\n【32】In this nationally representative sample of community-dwelling older adults who have diabetes, we found 3 classes of people, based on their comorbid conditions: people in class 1 (63% of the sample) had 2 or fewer comorbid conditions and were relatively healthy; people in class 2 and class 3 were sicker; they had 6 or more comorbid conditions, a higher 5-year death rate, more health care visits, and higher rates of insulin use, functional disability, and fair or poor self-rated health. Classes were not distinguished by any single comorbid condition. However, in class 3, the estimated probability of myocardial infarction was 93% and congestive heart failure, 100%; one-third of respondents in class 3 died within 5 years of participating in the first wave of the study.\n\n【33】The ADA  recommends that clinicians use information on comorbid conditions to identify older adults who are less likely to benefit from intensive glycemic control. However, because comorbid conditions co-occurred in many different patterns across all classes, the task of identifying patients by comorbid conditions in clinical practice would be challenging. Instead, we found a potentially simpler strategy. Using 2 categories based on the number of comorbid conditions (high \\[≥6\\] and low \\[≤2\\], one could assign nearly half of the participants in our study (n = 232, 46%) to the correct class. Thus, older adults who have diabetes and 6 or more comorbid conditions would be unlikely to benefit from intensive glycemic control.\n\n【34】The presence of cardiovascular disease may raise concerns about the effects of intensive glycemic control. In the ACCORD trial, the presence of cardiovascular disease appeared to decrease the benefits of intensive glycemic control . In the Veterans Affairs Diabetes Trial, the presence of coronary calcification was associated with higher rates of cardiovascular events during very intensive glycemic control . These recent findings would have direct implications for our class 3 participants, for whom the estimated probability of cardiovascular disease was more than 90%. Although most participants in class 3 had cardiovascular disease, participants in class 1 and class 2 also had cardiovascular disease. Thus, decisions about intensive glycemic control should take into account both counts of comorbid disease and the presence of sentinel conditions, such as cardiovascular disease.\n\n【35】This research has several strengths and limitations. It demonstrates that distinct subgroups of older adults who have diabetes share patterns of comorbid conditions. It partially simplifies the complex heterogeneity of comorbid conditions among older adults who have diabetes. Our classification system, based on 2 categories, could guide future geriatric diabetes treatment recommendations. One limitation to this work is that we were unable to account for the likely important effects of diabetes duration. Differences in duration have been proposed to explain the differences between the United Kingdom Prospective Diabetes Study  and more recent trials. Although too late for our study, NSHAP now collects self-reported data on diabetes duration. One limitation to using self-reported data on diabetes duration, however, is the delay between diabetes onset and diabetes diagnosis; previous trials have defined the onset of diabetes by clinical diagnosis . Future longitudinal data would allow examination of the development of new comorbid conditions in subgroups of older adults who have diabetes. Finally, because information on fall history was collected through the postinterview questionnaire and not during the in-home interview, this information may have been systematically underreported, and the lack of these data reduced the number of people in our analysis.\n\n【36】Classifying the older US population who have diabetes on the basis of comorbid conditions produces clinically distinctive subgroups; however, these subgroups overlap in the predicted probability of comorbid conditions. Relying on classes based on comorbid conditions alone may not provide a definitive classification system for this population. However, using information on cardiovascular disease history and counts of comorbid conditions could provide strategies to clinicians and policy makers for distinguishing clinically important subgroups. Future research is needed to evaluate alternative strategies for identifying the subgroup of older adults who have diabetes in whom the risks of intensive glycemic control outweigh its benefits.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3c16719a-33ca-4a96-abc9-f010bb985421", "title": "Quinine-Resistant Malaria in Traveler Returning from Senegal, 2007", "text": "【0】Quinine-Resistant Malaria in Traveler Returning from Senegal, 2007\nResistance of _Plasmodium falciparum_ to antimalarial drugs is one the most worrisome problems in tropical medicine. Quinine remains the first-line antimalarial option for treatment of patients with complicated malaria in Europe and Africa. However, emergence of quinine resistance has been sparsely documented . Maximizing the efficacy and longevity of quinine as a drug to control malaria will critically depend on pursuing intensive research into identifying in vitro markers and implementing active in vitro and in vivo surveillance programs such as those supported by the World Antimalarial Resistance Network. Such molecular markers are needed to monitor temporal trends in parasite susceptibility . We report quinine-resistant _P._ _falciparum_ malaria in a patient who returned to France from Senegal.\n\n【1】### The Patient\n\n【2】A 17-year-old white man from France spent ≈2 months (April and most of May) in 2007 in Dielmo, Senegal, where malaria is highly endemic and shows intense perennial transmission . He did not use antimalarial prophylaxis or protection against mosquitoes. After returning to France, he was admitted to the Bordeaux University Hospital Center on May 27, 2007 (day 0). The patient had _P_ . _falciparum_ parasitemia level of 7% and a 2-day history of fever, myalgia, vomiting, and rapid deterioration of consciousness into an arousable coma. A diagnosis of severe malaria with cerebral involvement was confirmed.\n\n【3】Intravenous quinine formiate (loading dose 17 mg/kg) was administered, followed by a maintenance dose (8.3 mg/kg 3×/day for 7 days). The patient was afebrile on day 3, and his thin and thick blood films became negative for _P_ . _falciparum_ on day 6. He was discharged from the hospital on day 7. However, on day 26, he relapsed and had fever and vomiting. He was hospitalized again on day 27 with a core temperature of 40°7C, deterioration of consciousness, and a _P_ . _falciparum_ parasitemia level of 4%. He received the same regimen of quinine formiate plus intravenous clindamycin (10 mg 3×/day) for 7 days. His serum quinine level (free and bound drug assayed by high performance liquid chromatography) taken immediately before the fourth drug dose was low (7 mg/L). The patient was then given quinine (10 mg/kg 3×/day from day 30 through day 34). Serum quinine levels then increased and fever cleared within 72 hours. However, a blood smear was positive on day 34.\n\n【4】Because of the treatment failure with quinine and clindamycin, the patient was treated with oral co-artemether (20 mg artemether and 120 mg lumefantrine, given as 4 tablets, followed by 4 tablets after 8 hours, and 4 tablets 2×/day for 2 days; total = 24 tablets). Parasitic clearance was observed within 48 hours. Blood smears and results of a PCR for _P_ . _falciparum_ were negative from day 36 through day 62. No further recrudescence occurred over the next 12 months.\n\n【5】The isotopic microdrug susceptibility tests used have been described . The chloroquine-susceptible 3D7 _P_ . _falciparum_ clone (Africa) and the chloroquine-resistant W2 clone (Indochina), after 2 rounds of sorbitol synchronization, were used as controls. The 50% inhibitory concentration (IC 50  ) values for 12 antimalarial drugs for the study isolate and these 2 controls are shown in the Table . The strain isolated on day 27 showed reduced susceptibility to quinine (IC 50  829 nmol/L, threshold 800 nmol/L) and chloroquine (472 nmol/L, threshold 100 nmol/L). The IC 50  for clindamycin was 39 μmol/L (the in vitro resistance cutoff value was not determined). The isolate was susceptible to all other antimalarial drugs tested. Phenotypes and genotypes were assessed only for parasites obtained on day 27.\n\n【6】We concurrently screened blood samples for resistance-associated point mutations. A sequence containing the ms4760 microsatellite was amplified as described . The observed ms4760–18 profile was composed of 2 DNNND repeats and 2 DDDNHNDNHNN repeats. Genotyping of the _P_ . _falciparum_ chloroquine resistance transporter ( _Pfcrt_ ) gene, which encodes a transport protein involved in chloroquine resistance (K76T), and the dihydroopteroate synthase gene, which encodes the sulfadoxine target (A437G), identified the resistant allele in our isolate . There was no mutation in codon 268, which encodes the atovaquone target . The isolate had only 1 copy of the _P_ . _falciparum_ multidrug resistance ( _Pfmdr1_ ) gene and a mutation in codon 184, which suggested in vitro susceptibility to mefloquine . Amplification of DNA from parasites obtained on day 0 and preserved on fixed and stained thin blood films by a modification of the procedure of Edoh et al. was not successful.\n\n【7】### Conclusions\n\n【8】Quinine remains a reliable treatment for patients with complicated or severe _P_ . _falciparum_ malaria outside southern Asia. Clinical failure with quinine used alone or in combination with clindamycin is common in Africa. In our case-patient, a correlation between the results of the in vivo and in vitro assessments was demonstrated at day 27. Because of the lack of reliable data on the correlation between quinine IC 50  and clinical failure, arbitrary IC 50  cutoff values were chosen for in vitro quinine resistance (300 nmol/L, 500 nmol/L, or 800 nmol/L) .\n\n【9】Quinine resistance appears to share common characteristics with chloroquine resistance. It is associated with mutations in the _pfmdr1_  and _pfcrt_  genes. Nevertheless, the mechanism of quinine resistance is still unknown. In addition to the _pfmdr1_ and _pfcrt_ genes, other genetic polymorphisms such as microsatellite length variations in the _P_ . _falciparum_ sodium/hydrogen exchanger ( _pfnhe-1_ ) gene  and mutations in the _P_ . _falciparum_ multidrug resistance protein gene may contribute to quinine resistance .\n\n【10】We report an association of clinical failure of quinine treatment with an IC 50  of 829 nmol/L, a mutation in codon 76 of the _pfcrt_ gene, and an ms4760–18 profile for _pfnhe-1_ composed of 2 DNNND repeats. Isolates of _P_ . _falciparum_ with \\> 2 DNNND repeats may be associated with reduced susceptibility to quinine. Henry et al. reported that 2 DNNND repeats were associated with quinine IC 50  values ranging from 300 nmol/L to 700 nmol/L, and that 3 repeats were associated with an IC 50  \\>600 nmol/L. However, the 3 strains with IC 50  s >800 nmol/L had \\> 2 DNNND repeats . Our results are consistent with these data.\n\n【11】_P_ . _falciparum_ resistance levels may differ depending on malaria transmission and drug pressure. Data from Senegal are fragmentary and were obtained by in vitro susceptibility studies conducted with isolates reported to have decreased in vitro susceptibility to quinine . Our patient had traveled to Dielmo, Senegal, where in vitro surveillance of antimalarial drug susceptibility has been conducted since 1996. During 1996–2005, the overall prevalence of isolates with IC 50  \\>800 nmol/L for quinine was <6%: 1% in 1996, 4% in 1997, 0% in 1998, 6% in 1999, and 0% in 2005 . Quinine was used for 96.4% of the treatments administered in Dielmo during 1990–1995 . This drug has since been replaced by chloroquine, sulfadoxine-pyrimethamine, and artemisinin-based combination therapies.\n\n【12】We report a patient with clinical failure associated quinine resistance in a traveler to Senegal. Our results are consistent with those of a recent review of the Uganda Malaria Surveillance Project that reported a higher risk for selecting quinine-resistant parasites associated with a 7-day quinine treatment course . Thus, resistance to quinine should be monitored in West Africa. Although such clinical failure of therapy is rare, increased vigilance is required during treatment follow-up, and surveillance of the parasite population should also be increased.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e5c068c8-d65c-4758-b823-6489184e6fa1", "title": "Reach of Supplemental Nutrition Assistance Program – Education (SNAP-Ed) Interventions and Nutrition and Physical Activity-Related Outcomes, California, 2011-2012", "text": "【0】Reach of Supplemental Nutrition Assistance Program – Education (SNAP-Ed) Interventions and Nutrition and Physical Activity-Related Outcomes, California, 2011-2012\nAbstract\n--------\n\n【1】**Introduction**  \nThis study combined information on the interventions of the US Department of Agriculture’s Supplemental Nutrition Assistance Program–Education with 5,927 interview responses from the California Health Interview Survey to investigate associations between levels of intervention reach in low-income census tracts in California and self-reported physical activity and consumption of fruits and vegetables, fast food, and sugar-sweetened beverages.\n\n【2】**Methods**  \nWe determined 4 levels of intervention reach (low reach, moderate reach, high reach, and no intervention) across 1,273 program-eligible census tracts from data on actual and eligible number of intervention participants. The locations of California Health Interview Survey respondents were geocoded and linked with program data. Regression analyses included measures for sex, age, race/ethnicity, and education.\n\n【3】**Results**  \nAdults and children from high-reach census tracts reported eating more fruits and vegetables than adults and children from no-intervention census tracts. Adults from census tracts with low, moderate, or high levels of reach reported eating fast food less often than adults from no-intervention census tracts. Teenagers from low-reach census tracts reported more physical activity than teenagers in no-intervention census tracts.\n\n【4】**Conclusion**  \nThe greatest concentration of Supplemental Nutrition Assistance Program–Education interventions was associated with adults and children eating more fruits and vegetables and adults eating fast food less frequently. These findings demonstrate the potential impact of such interventions as implemented by numerous organizations with diverse populations; these interventions can play an important role in addressing the obesity epidemic in the United States. Limitations of this study include the absence of measures of exposure to the intervention at the individual level and low statistical power for the teenager sample.\n\n【5】Introduction\n------------\n\n【6】Obesity is a precursor to numerous chronic diseases, including coronary heart disease, diabetes, hypertension, some cancers, gall bladder disease, osteoarthritis, sleep apnea, and respiratory problems . In the United States, in 2011–2012, the prevalence of adult obesity was 34.9%, more than double that of 1980 . Also during 2011–2012, the prevalence was 16.9% among children and adolescents aged 2 to 19 years, tripling during the same period . Obesity rates have plateaued, showing no significant change for adults or youths between 2003 and 2012 .\n\n【7】The US Department of Agriculture (USDA) promotes behaviors that can reduce the impact of the obesity epidemic among low-income populations through its Supplemental Nutrition Assistance Program–Education (SNAP–Ed) program, which is designed to increase the nutritious food choices of, and physical activity by, more than 46 million SNAP participants and people eligible for SNAP .\n\n【8】Peer reviewed studies demonstrating that SNAP–Ed improves desired behaviors are limited . USDA demonstration projects have found significant increases in fruit and vegetable consumption among children and seniors . Although the results look promising, these projects should be interpreted within the context that follow-up measures were taken right after the interventions ended; they were high-dose, direct-interaction interventions not representative of the diversity of SNAP–Ed interventions approved and promoted by the USDA; and they included randomized or quasiexperimental designs that are difficult to carry out under the usual conditions of program delivery.\n\n【9】The aim of our study was to test the external validity of SNAP–Ed interventions as implemented by various organizations with diverse populations in California by using an ecological approach. Specifically, using survey data from a random sample of SNAP–Ed-eligible individuals, we assessed by census tract whether level of SNAP–Ed intervention reach was associated with increased consumption of fruits and vegetables, less frequent consumption of fast food and sugar-sweetened beverages, and more physical activity.\n\n【10】Methods\n-------\n\n【11】This study was reviewed and approved by the California Health and Human Services Agency, Committee for the Protection of Human Subjects.\n\n【12】### Study population\n\n【13】The study population was SNAP–Ed-eligible Californians. The Healthy, Hunger-Free Kids Act of 2010 defines SNAP–Ed eligibility at the household and population level. In federal fiscal year (FFY) 2011, SNAP–Ed-eligible people were those in households with income at or below 185% of the federal poverty level (FPL) and people in locations such as schools or geographic areas (eg, census tracts) where 50% or more of the population resided in households at or below 185% of the FPL.\n\n【14】We used 2 databases of SNAP-eligible people. One database included data on people documented as participating in a SNAP–Ed intervention (and thereby SNAP–Ed-eligible) in FFY 2011. The second database included data on participants in the California Health Interview Survey (CHIS) in 2011–2012 who met the household income criterion for SNAP–Ed. Individual responses to selected items on the CHIS questionnaire were linked with estimates of the reach of the SNAP–Ed intervention, calculated by census tract. We used an ecological approach because the CHIS questionnaire did not ask about participating in a SNAP–Ed intervention. Relying on self-reports would have resulted in an invalid operationalization of SNAP–Ed participation and exposure. Most SNAP–Ed contractors do not explicitly identify their interventions as SNAP–Ed or USDA funded, and such questions would be subject to recall and social desirability biases.\n\n【15】### Independent variable\n\n【16】The independent variable, intervention reach, was defined as the number of SNAP–Ed participants divided by the number of SNAP–Ed-eligible people in each California census tract that met the population eligibility requirements for SNAP–Ed in FFY 2011. The number of SNAP–Ed-eligible people in SNAP–Ed-eligible census tracts was determined by using standardized procedures implemented annually by the Nutrition Education and Obesity Prevention Branch (NEOPB) of the California Department of Public Health and is based on data from the US Census and the American Community Survey.\n\n【17】The numerator used to calculate intervention reach (the number of participants in SNAP–Ed interventions in FFY 2011) was obtained from the USDA’s Education and Administrative Reporting System (EARS). In FFY 2011, EARS data were available from 120 NEOPB-funded contractors. NEOPB staff train and provide technical assistance to contractors on collecting and entering data, and they clean and summarize data for program planning and annual reports to the USDA.\n\n【18】Both unduplicated and duplicated counts of SNAP–Ed participants are entered by contractors into EARS for direct education (eg, structured learning interventions facilitated by a trained educator or through interactive media) and indirect education (eg, interventions involving the distribution of information and resources to groups of people in settings such as community fairs and cooking demonstrations). Data on the same individuals can appear in EARS for school-based interventions consisting of multisession classes. Different contractors could also record separately the same individuals for 1 SNAP–Ed intervention.\n\n【19】To eliminate the possibility of duplicate counts of people who participated in a SNAP–Ed intervention with multiple sessions, we counted people in the first session only. Interventions were also sorted by site name in cities, and duplicates were deleted. These procedures resulted in 6.6 million presumed unduplicated individuals who participated in a SNAP–Ed intervention in FFY 2011.\n\n【20】Intervention reach was calculated at the census-tract level by geocoding (ArcGIS version 10.1; Esri) the address of each SNAP-Ed intervention site for each unduplicated SNAP–Ed participant.\n\n【21】### Dependent variables\n\n【22】Data related to 3 intended SNAP–Ed outcomes (healthful eating, healthful beverage consumption, and recommended levels of physical activity) came from the 2011–2012 CHIS, conducted from June 2011 through December 2012 .\n\n【23】CHIS is an ongoing stratified random-digit–dialed health survey. Interviews in 2011–2012 were conducted in English, Spanish, Cantonese, Mandarin, Korean, and Vietnamese. Adults, teenagers, or children were randomly selected from each sampled household. Teenagers were interviewed after permission was obtained from their parent or legal guardian, who may or may not have been the adult respondent for the household. Interview data for children were provided by the adult that was identified as most knowledgeable about the child’s health.\n\n【24】The dependent variables for this study were fruit and vegetable consumption, fast food consumption, sugar-sweetened beverage consumption, and physical activity. Proxy questions related to consumption behaviors excluded children younger than 2 years; physical activity questions pertained to children 5 years or older.\n\n【25】Answers to CHIS questions about eating fruit were combined with questions about eating vegetables to develop 1 variable on fruit and vegetable consumption for adults, teenagers, and children. The questions asked of adults were “During the past month, how many times did you eat fruit? Do not count juices.” and “During the past month, how many times did you eat any other vegetables like green salad, green beans, or potatoes? Do not include fried potatoes.” Reported number of times were recorded as “per day,” “per week,” or “per month” on the basis of how respondents chose to answer the questions, and values were summed and converted to a per-day unit.\n\n【26】For teenagers, the open-ended responses to the questions “Yesterday, how many servings of fruit, such as an apple or banana, did you eat?” and “Yesterday, how many servings of other vegetables like green salad, green beans, or potatoes did you have? Do not include fried potatoes.” were combined. Child proxy interviews with adults included the questions “Yesterday, how many servings of fruit, such as an apple or a banana, did (child) eat?” and “Yesterday, how many servings of other vegetables like green salad, green beans, or potatoes did (child) have? Do not include fried potatoes.”\n\n【27】The same question was used to assess fast food consumption among adults, teenagers, and children: “Now think about the past week. In the past 7 days, how many times did you (“he/she” for children) eat fast food? Include fast food meals eaten at work, at home, or at fast-food restaurants, carryout or drive through.” The number of times was recorded.\n\n【28】The types of sugar-sweetened beverages on the market today include regular (nondiet) soda, sweetened fruit drinks, and sports and energy drinks. The 2011–2012 CHIS survey of adults focused on consumption of regular sodas only: “During the past month, how often did you drink regular soda or pop that contains sugar? Do not include diet soda.” Responses were converted to a per-week basis. Interviewers clarified with respondents whether their answers were based on per day, per week, or per month.\n\n【29】The answers to the following 2 open-ended questions for teenagers were combined to assess consumption of sugar-sweetened beverages: “Yesterday, how many glasses or cans of soda that contain sugar, such as Coke, did you drink? Do not include diet soda.” and “Yesterday, how many glasses or cans of sweetened fruit drinks, sports, or energy drinks, did you drink?” The following question was asked to assess consumption among children: “Yesterday, how many glasses or cans of soda, such as Coke, or other sweetened drinks, such as fruit punch or sports drinks did (he/she) drink? Do not count diet drinks.”\n\n【30】Physical activity was measured differently for adults than for teenagers and children. Minutes of walking per week for adults was assessed with a series of questions that asked about number of times per week and number of minutes per day of walking for transportation versus relaxation or exercise. Respondents answered these questions with number of minutes or hours.\n\n【31】Physical activity for teenagers was assessed with the open-ended question “Not including school PE, in the past 7 days, on how many days were you physically active for at least 60 minutes total per day?” Proxy interviews for children included the similar question “Not including school PE, on how many days of the past 7 days was (child) physically active for at least 60 minutes total?”\n\n【32】### Sociodemographic variables\n\n【33】Age was assessed on CHIS through reported date of birth or, if respondent refused, a categorical variable. Adult participants, parents or legal guardians of teenagers, and the “most knowledgeable adult” for children were asked to indicate the highest school grade completed. Sex and race/ethnicity were determined through self-report. FPLs were calculated from responses to questions about household income and number of persons in the household supported by this income.\n\n【34】### Analysis\n\n【35】Data on CHIS participants with an FPL greater than 185% were excluded. The physical addresses of the remaining SNAP–Ed eligible CHIS respondents were geocoded to a census tract and coded for level of intervention reach so that the record for each adult, teenager, and child included a variable (ranging from 0 to 1) that represented the proportion of intervention reach in the census tract where he or she lived.\n\n【36】A dichotomous variable of intervention reach was created to examine (using χ 2  tests) whether age, sex, education, race/ethnicity, or FPL significantly differed between census tracts that had an intervention and census tracts that had no intervention.\n\n【37】All dependent variables were examined for outliers. Responses of more than 750 minutes of walking per week (among adults) were removed. The distribution of this variable was also skewed to the right (skewness = 4.53). A Box–Cox transformation was performed, and a log transformation was determined to be the most appropriate method for obtaining a normal distribution.\n\n【38】Regression analyses (SAS version 9.3; SAS Institute Inc) were used to examine the relationships between intervention reach and the dependent variables. For these analyses, we established 4 categories of reach based on the distribution of the proportions: 1) no SNAP–Ed interventions; 2) low reach (0.01%–39.99% of the target population reached); 3) moderate reach (40%–89.99%); and 4) high reach (90%–100%). These models controlled for age, sex, education, and race/ethnicity. Data on age were categorized into 7 groups (0–4 y, 5–11 y, 12–17 y, 18–24 y, 25–44 y, 45–64 y, and ≥65 y). Educational attainment was recoded as less than high school or high school or more.\n\n【39】Negative binomial models were developed for outcomes based on counts (fruit and vegetable consumption, fast food consumption, sugar-sweetened beverage consumption, and physical activity for teenagers and children). Linear modeling (ordinary least squares) was used for the continuous outcome of physical activity (minutes per week of walking) among adults.\n\n【40】The models took the following forms: log _it_ (μ) = α + _X_ β + _CT_ (negative binomial model for count outcomes) and _Y_ \\= α + _X_ β + _CT_ (linear model for continuous outcomes), where in both models, α is the intercept; _X_ is the design matrix of the adjusted characteristics age, sex, race/ethnicity, and education; and β is a vector of the regression coefficients associated with those confounders. _C_ is a set of indicators for levels of intervention reach; the reference level is the comparison group (no intervention). _T_ is the regression coefficient of the intervention reach. For goodness of fit for the linear models, normality of the residual distributions was checked through Q–Q plots and scatter plots.\n\n【41】We hypothesized that SNAP–Ed interventions have a positive impact on the targeted population, and therefore a 1-sided _P_ value was selected to determine significance at the .05 level.\n\n【42】Results\n-------\n\n【43】### Characteristics of the sample\n\n【44】In 1,273 SNAP–Ed-eligible census tracts, CHIS interview data were available for 4,245 adults, 465 teenagers, and 1,217 children. The proportion of the sample by level of intervention reach was similar among the 3 age groups: roughly 34% to 36% in the no-intervention group, 36% to 40% in the low-reach group, 10% to 13% in the moderate-reach group, and 15% to 17% in the high-reach group .\n\n【45】Overall, 59.6% of adults and most teenagers (87.1%) and children (83.6%) were Hispanic. Two-fifths (40.4%) of adults had less than a high school education, and 56.4% had an FPL of less than 100%. We found no significant differences between the intervention and no-intervention census tracts for age, sex, education, race/ethnicity, or FPL .\n\n【46】Higher levels of intervention reach were related to more healthful eating behaviors among adults . Adults from high-reach census tracts ate fruits and vegetables more often than adults in low-, moderate-, or no-reach census tracts. Adults from census tracts with low, moderate, and high levels of reach ate fast food less often than adults in no-intervention census tracts. Contrary to expectations, teenagers living in census tracts with SNAP–Ed interventions ate fast food more often than those from no-intervention census tracts.\n\n【47】Children from high-reach census tracts ate more fruits and vegetables than children from no-intervention census tracts. Levels of intervention reach were not related to levels of consumption of sugar-sweetened beverages in any of the 3 age groups.\n\n【48】Teenagers from low-reach census tracts reported more physical activity than teenagers in no-reach census tracts.\n\n【49】Discussion\n----------\n\n【50】The greatest concentration of SNAP–Ed interventions was related to eating more fruits and vegetables among adults and children and eating fast food less often among adults only. The finding of increased fruit and vegetable consumption among children and adults confirms the findings of USDA demonstration projects designed with an emphasis on internal validity . Our study results suggest that such outcomes are generalizable to low-income people throughout California and that different types of SNAP–Ed interventions implemented by different types of organizations in diverse populations can lead to greater intake of fruits and vegetables. Novel to the scant research on the positive impacts of SNAP–Ed was our finding that adults from SNAP–Ed areas improved dietary behaviors by eating fast food less often.\n\n【51】SNAP–Ed interventions include messages to adults on the health benefits of fruits and vegetables and preparing meals at home, the provision of healthful recipes, and demonstrations on how to prepare fruits and vegetables. These educational messages and newly learned skills may have been responsible for changes to the snacks and meals made and eaten by parents at home, which in turn translated into increased fruit and vegetable consumption by their children. Many FFY 2011 interventions were school-based, directly targeting children. Lower levels of fast food consumption among adults may be explained by behavior changes during the day, when parents were more likely to rely on the convenience of fast food. SNAP–Ed interventions may have prompted parents to alter their choices away from fast food when out of the house for work or errands, for example, while their children were attending day care or school.\n\n【52】Contrary to our hypotheses, we found that census tracts with higher reach were associated with greater fast food consumption among teenagers. Teenagers from census tracts with SNAP–Ed interventions may have opted to use their disposable income on fast food in direct response to more healthful snacks and meals being offered at home that resulted from SNAP–Ed interventions directed to their parents. Alternatively, teenagers in the intervention census tracts may attend schools with nearby fast food restaurants.\n\n【53】Teenagers in low-reach census tracts had higher levels of physical activity than teenagers in no-intervention census tracts. Limited statistical power may be responsible for the lack of significant findings for teenagers in moderate- and high-reach areas.\n\n【54】One advantage of this study is that all census tracts from which EARS and CHIS data were obtained met the same criteria for SNAP–Ed eligibility. Nonsignificant differences between intervention groups (no vs low, moderate, or high) compared by age, sex, race/ethnicity, education, and FPL strengthen the case that SNAP–Ed interventions may explain more healthful behaviors among adults and children. However, this study is limited in that we do not know the extent to which CHIS participants in the low-, moderate-, or high-reach groups actually participated in an intervention; we know only that greater levels of reach heightened the probability that a CHIS respondent was also a SNAP–Ed participant. In addition, this study did not examine how the unique characteristics of the census tracts may have differed among the reach groups. The high-reach census tracts, for example, may be located in cities or counties that are more likely to have adopted policies or have environmental supports that encourage more healthful eating. Alternatively, the low-reach census tracts may have had fewer fast food establishments.\n\n【55】For many participants in this study, there was an established time order between presumed SNAP–Ed intervention exposure and behavior change. SNAP-Ed interventions used to calculate the independent variable (intervention reach) occurred during 8 months (October 2010 through May 2011) before the assessment of the dependent variables (June 2011 through January 2012). Moreover, CHIS was administered 4 months after FFY 2011 ended on September 30, 2011. However, the overlap in EARS and CHIS data collection subjects this study to the limitation of the cross-sectional design in establishing a true temporal relationship between the independent and dependent variables.\n\n【56】Finally, it is unclear whether the CHIS participants in this study were exposed to non-SNAP–Ed interventions or other factors that may have influenced their behaviors. Other organizations also target in-need populations in our high-reach census tracts to implement interventions or campaigns. The CHIS questionnaire does not ask about the Special Supplemental Nutrition Program for Women, Infants, and Children, and therefore we could not control for participation in this program in our analyses. Given these limitations, one should interpret our findings of significant relationships between SNAP–Ed interventions and more healthful dietary intake with caution.\n\n【57】The Healthy, Hunger-Free Kids Act of 2010 requires an annual reduction in SNAP–Ed funding by 10% so that by 2018 states will receive half of the funding that was available in FFY 2013 . Our study provides support to maintain and ideally expand SNAP–Ed interventions as a means to address the obesity epidemic in the United States. Currently funded SNAP-Ed contractors as well as those developing new programs should look to alternative sources of funding, and they could use our findings to justify continued or new support of direct-service interventions. Support is particularly important in California because of the state’s growing Latino population, a group that is at increased risk of obesity and its health consequences . Future research should address the limitations of this study, including the lack of neighborhood factors that could affect behaviors that prevent or reduce unhealthy weight gain.\n\n【58】Tables\n------\n\n【59】#####  Table 1. 2011–2012 California Health Interview Survey Participants by Age Group and Levels a  of Intervention Reach for California Supplemental Nutrition Assistance Program–Education (SNAP–Ed) Among SNAP–Ed-Eligible Census Tracts, Federal Fiscal Year 2011 b  \n\n| Age Group | No Intervention (n = 529) | Low Reach (n = 401) | Moderate Reach (n = 134) | High Reach (n = 209) | Total (n = 1,273) |\n| --- | --- | --- | --- | --- | --- |\n| Adults aged ≥18 y | 1,507 (35.5) | 1,522 (35.8) | 482 (11.4) | 734 (17.3) | 4,245  |\n| Teenagers aged 12–17 y | 160 (34.4) | 185 (39.8) | 48 (10.3) | 72 (15.5) | 465  |\n| Children aged 2–11 y | 409 (33.6) | 465 (38.2) | 156 (12.8) | 187 (15.4) | 1,217  |\n\n【61】a  Levels of reach defined as low (0.01%–39.99% of the target population reached); moderate (40%–89.99% reached); high (90%–100% reached).  \nb  All values are number (percentage).\n\n【62】#####  Table 2. Characteristics of 2011–2012 California Health Interview Survey Participants by Age Group and SNAP–Ed-Eligible Census Tracts With and Without SNAP–Ed Interventions, Federal Fiscal Year 2011 a  \n\n| Characteristic | Census Tracts That Had Interventions (n = 744) | Census Tracts That Had No Interventions (n = 529) |\n| --- | --- | --- |\n| Adults Aged ≥18 y | Teenagers Aged 12–17 y | Children Aged 2–11 y | Adults Aged ≥18 y | Teenagers Aged 12–17 y | Children Aged 2–11 y |\n| --- | --- | --- | --- | --- | --- |\n| **Age, no. (mean, y)** | 2,738 (49.4) | 305 (14.4) | 808 (5.5) | 1,507 (49.3) | 1,507 (49.3) | 160 (14.4) | 409 (5.7) |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 1,043 (38.1) | 148 (48.5) | 451 (55.8) | 560 (37.2) | 82 (51.2) | 82 (51.2) | 222 (54.3) |\n| Female | 1,695 (61.9) | 157 (51.5) | 357 (44.2) | 947 (62.8) | 78 (48.8) | 78 (48.8) | 187 (45.7) |\n| **Education b** | **Education b** | **Education b** | **Education b** | **Education b** | **Education b** | **Education b** | **Education b** |\n| <High school | 1,123 (41.0) | 182 (59.7) | 410 (50.7) | 591 (39.2) | 87 (54.4) | 87 (54.4) | 200 (48.9) |\n| ≥High school | 1,615 (59.0) | 123 (40.3) | 398 (49.3) | 916 (60.8) | 73 (45.6) | 73 (45.6) | 209 (51.1 |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Hispanic | 1,609 (58.8) | 260 (85.2) | 666 (82.4) | 919 (61.0) | 145 (90.6) | 145 (90.6) | 351 (85.8) |\n| White | 558 (20.4) | 21 (6.9) | 62 (7.7) | 262 (17.4) | 2 (1.2) | 2 (1.2) | 17 (4.2) |\n| Asian | 244 (8.9) | 12 (3.9) | 33 (4.1) | 192 (12.7) | 4 (2.5) | 4 (2.5) | 18 (4.4) |\n| African American | 206 (7.5) | 6 (2.0) | 22 (2.7) | 98 (6.5) | 7 (4.4) | 7 (4.4) | 13 (3.2) |\n| Other race | 121 (4.4) | 6 (2.0) | 25 (3.1) | 36 (2.4) | 2 (1.2) | 2 (1.2) | 10 (2.4) |\n| **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** | **Federal poverty level b** |\n| 0%–99% | 1,561 (57.0) | 184 (60.3) | 476 (58.9) | 835 (55.4) | 99 (61.9) | 99 (61.9) | 250 (61.1) |\n| 100%–185% | 1,177 (43.0) | 121 (39.7) | 332 (41.1) | 835 (44.6) | 61 (38.1) | 61 (38.1) | 159 (38.9) |\n\n【64】Abbreviation: SNAP–Ed, Supplemental Nutrition Assistance Program–Education.  \na  All values are number (percentage) unless otherwise indicated.  \nb  Assigned for teenagers based on consent of parent or legal guardian; assigned for children based on adult identified as most knowledgeable about the child’s health.\n\n【65】#####  Table 3. Relationships Between Reach of California SNAP–Ed Interventions in Federal Fiscal Year 2011 and Healthful Eating Behaviors, Consumption of Sugar-Sweetened Beverages, and Physical Activity of Adults, Teenagers, and Children Participating in the 2011–2012 California Health Interview Survey\n\n| Age Group/Level of Reach a | Fruit and Vegetable Consumption | Fast Food Consumption | Consumption of Sugar-Sweetened Beverages | Physical Activity |\n| --- | --- | --- | --- | --- |\n| Adults Aged ≥18 y | No. | Times Per Day, _z_ Value b | Times Past Week, _z_ Value b | Regular Soda, Times Per Week, _z_ Value b | Walking, Minutes per Week, _t_ Value c |\n| --- | --- | --- | --- | --- | --- |\n| No intervention | 1,507 | — | — | — | — |\n| Low | 1,522 | 0.85 | −1.67 d | −0.14 | −0.53 |\n| Moderate | 482 | 0.39 | −2.13 d | 0.40 | −1.57 |\n| High | 734 | 1.79 d | −2.08 d | −1.15 | 1.05 |\n| **Teenagers Aged 12–17 y** | **No.** | **Servings Yesterday, _z_ Value b** | **Times Past Week, _z_ Value b** | **Regular Soda, Fruit, Sports, or Energy Drinks, No. of Glasses/Cans Yesterday, _z_ Value b** | **Physically Active ≥60 Minutes, Days Last Week, _z_ Value b** |\n| No intervention | 160 | — | — | — | — |\n| Low | 185 | −1.14 | 2.78 e | 1.00 | 1.81 d |\n| Moderate | 48 | −1.26 | 2.44 e | 0.39 | 1.26 |\n| High | 72 | −0.55 | 3.28 e | 1.05 | 1.13 |\n| **Children f** | **No.** | **Servings Yesterday, _z_ Value b** | **Times Past Week, _z_ Value b** | **Regular Soda, Fruit, Sports, or Energy Drinks, No. of Glasses/Cans Yesterday, _z_ Value b** | **Physically Active ≥60 Minutes, Days Last Week, _z_ Value b** |\n| No intervention | 409 | — | — | — | — |\n| Low | 465 | 0.60 | 0.07 | 0.65 | 0.15 |\n| Moderate | 156 | 1.08 | −0.15 | −0.25 | −0.14 |\n| High | 187 | 2.07 d | 0.04 | −0.44 | 1.47 |\n\n【67】Abbreviation: SNAP–Ed, Supplemental Nutrition Assistance Program–Education.  \na  Levels of reach defined as low (0.01%–39.99% of the target population reached); moderate (40%–89.99% reached); high (90%–100% reached).  \nb  Negative binomial regression analyses controlling for sex, race/ethnicity, education, and age for adults (18–24 y, 25–44 y, 45–64 y, and ≥65 y).  \nc  Linear regression analyses controlling for sex, race/ethnicity, education, and age for adults (18–24 y, 25–44 y, 45–64 y, and ≥65 y).  \nd  _P_ < .05, 1-sided, based on hypothesized direction.  \ne  _P_ < .05, 2-sided, based on nonhypothesized direction.  \nf  Consumption questions asked about children aged 2 through 11 years; physical activity questions were answered by most knowledgeable adults of children 5 through 11 years.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "29eb32a1-dfbb-4a64-9610-957227396c84", "title": "Possible Association between Obesity and Clostridium difficile Infection", "text": "【0】Possible Association between Obesity and Clostridium difficile Infection\nInflammatory bowel disease (IBD) is a risk factor for _Clostridium difficile_ infections (CDIs). Because of similar disruptions to the intestinal microbiome found in IBD and in obesity, we conducted a retrospective study to clarify the role of obesity in CDI. We reviewed records of patients with laboratory-confirmed CDIs in a tertiary care medical center over a 6-month period. Of 132 patients, 43% had community onset, 30% had health care facility onset, and 23% had community onset after exposure to a health care facility. Patients with community onset had higher body mass indices than the general population and those with community onset after exposure to a health care facility, had higher rates of IBD, and lower prior antibacterial drug exposure than patients who had CDI onset in a health care facility. Obesity may be associated with CDI, independent of antibacterial drug or health care exposures.\n\n【1】_Clostridium difficile_ infections (CDIs) have a profound economic effect on the health care system; estimated costs range from $496 million to >$1 billion . _C. difficile_ is a leading cause of infectious diarrhea in hospitalized patients: the annual number of diagnoses of CDI on discharge has more than doubled, from ≈139,000 to 336,600 during this decade . The epidemiology of CDI has also shifted. A greater number of community onset cases have been recorded in traditionally low-risk populations , raising the concern for whether there are unidentified risk factors increasing the probability of CDI among this subset of persons. Association of CDI with novel risk factors can contribute to improved clinical surveillance of persons at highest risk for infection in the hospital setting or the community.\n\n【2】Inflammatory bowel disease (IBD) has been identified as an independent risk factor for _C. difficile_ colonization and disease; patients with IBD have increased severity of illness and death rates from CDI . This relationship appears to be modulated by a dysbiosis of intestinal microbiota . Similar to studies of antibacterial drugs and IBD, studies have shown that obesity may be associated with decreased diversity and changes in composition of the intestinal microbiome . Given the similarities in derangements of the intestinal microbiome seen secondary to antibacterial drug use, IBD, and obesity, obesity may also predispose persons to CDI.\n\n【3】Before 2010, the Society for Healthcare Epidemiology of America and the Infectious Diseases Society of America guidelines (SHEA-IDSA guidelines) defined CDIs as having community onset (CO) or inpatient health care facility onset (HO). Reflecting the changing epidemiology of CDI, the definition was expanded by the 2010 update of clinical practice guidelines to include an additional category of disease: community-onset health care facility–associated (CO-HCFA) . This category, CO-HCFA, is defined as onset of disease in CDI patients in the community who had exposure to health care facilities during the previous 4 weeks. We believe the introduction of this category has removed cases from the CO cohort who had recent exposure to health care facilities and may help detect associations between CDI and novel risk factors in patients with few other traditional exposures.\n\n【4】This study aims to identify possible demographic and risk factor differences between patients who develop community onset CDI compared with their HO and CO-HCFA counterparts. In particular, we examine whether obesity is overrepresented in patients with community onset infections who did not have exposure to health care facilities, antibacterial drugs, or the diagnosis of IBD. Furthermore, we examine the health care delivery sites represented among patients with CO-HCFA infections. The identification of these sites will facilitate targeted training and education of staff and improved allocation of infection control resources to decrease future incidence of disease.\n\n【5】### Methods\n\n【6】This study was a retrospective analysis of the infection control databases, microbiology results, and medical records of all patients who had laboratory proven CDI at Boston Medical Center (BMC) that serves as a regional safety net hospital. At the time of the study, the 508-bed academic medical center had a network of 15 community health centers. The study was approved by the BMC Institutional Review Board.\n\n【7】Our institution adopted the 2010 SHEA-IDSA guideline classifications for CDI in November 2011. All CDI cases in adults during November 2011–April 2012 were reviewed. Case-patients were defined as persons who had fecal samples positive for _C. difficile_ by using the C. Diff Quik Chek Complete enzyme immunoassay (TechLab, Blacksburg VA, USA) or GeneXpert PCR (Cepheid, Sunnyvale, CA, USA) during the study period. At BMC, only non-formed stools are accepted for microbiological analysis for CDI. Samples are tested by enzyme immunoassay for toxins A and B; if the result is inconclusive or clinical suspicion of disease is high, PCR is used.\n\n【8】By using the former classification, the case-patients with laboratory proven CDI were first categorized as having either community or nosocomial onset disease. Patients were then recategorized by using the new SHEA-IDSA guidelines as having CO, CO-HCFA, or HO disease. The CO category included patients who had symptoms or a positive fecal sample test, and no exposure to health care facilities or associated sites for >30 days before the clinic visit or hospital admission. CO-HCFA case-patients were exposed to health care facilities within the previous 30 days. Case-patients with HO disease had onset of symptoms >48 hours after admission and had positive results for CDI laboratory tests. Patients who had new symptoms and a positive assay and a previous positive test for _C. difficile_ \\>30 days but <8 weeks prior to examination were classified as having recurrent disease. Because of the small sample size of this group, recurrent case-patients were excluded from analysis to facilitate statistical comparison.\n\n【9】An exposure to hemodialysis centers, day surgery, chemotherapy suites, or long-term care facilities was considered an encounter with the health care system. Demographic data extracted from the patient chart included age, sex, race and ethnicity, height, and weight. Factors that have been identified as risk factors for CDI were also documented and included the presence of certain coexisting medical conditions, use of anti-ulcer medications, admission to a hospital intensive care unit, duration of hospital stay, and antibacterial use during the preceding month . IBD was cataloged separately from other immunocompromising conditions. Obesity was defined as body mass index (BMI) >30, calculated as weight (kg)/height (m) 2  .\n\n【10】Statistical analyses were performed by using SPSS software version 20 (SPSS, Inc. Chicago, IL, USA). Descriptive statistics, including student t-test, 1-way analysis of variance, and χ 2  statistics, were acquired for the data where appropriate. The proportion of CO case-patients diagnosed with obesity was compared with data gathered during 2011 in Massachusetts: population statistics provided by the US Census Bureau  and weight classification by BMI data provided by the Centers for Disease Control and Prevention Behavioral Risk Factor Surveillance System . Age was used as a continuous variable to calculate the means in univariate analysis but categorized as either <65 or \\> 65 years for multivariable regression. All reported p values were 2-sided; results with a p value <0.05 were considered significant.\n\n【11】Three binary regression analyses were performed with either CO and CO-HCFA, CO and HO, or CO-HCFA and HO as outcomes. CO-HCFA was used as the reference category for the first model, while HO served as the reference for the second and third models. A stepwise backward elimination method and likelihood ratios were used to find the best fitted model that also contained clinically relevant variables. All 2-way interaction terms were examined; none were found to have a substantial impact. A p value of 0.10 was used for exclusion from the regression model of nonclinically relevant covariates.\n\n【12】### Results\n\n【13】A total of 137 cases of CDI were identified in patients at BMC during the study period. Five patients had recurrent disease and were excluded, and the remaining 132 cases were analyzed. According to the former definitions of location of onset of CDI, 91 cases were CO and 41 were HO. By using the definitions described in 2010, 35.2% (32/91) of the CO cases were found to be HCFA-CO . Of these, 62.5% (20/32) had a prior hospital admission as a risk factor, while 28.1% (9/32) were from a long-term care facility. Other risk factors (accounting for those with >1 risk factor) included recent surgery (12.5%), hemodialysis (9.4%), or outpatient chemotherapy (3.4%). Results for patient demographics among the 3 CDI categories are shown in Table 1 . Among hospitalized patients from each category (109/132), patients with nosocomial infections had a longer length of stay (p<0.001) and were more likely to have been admitted to an intensive care unit (ICU) (p = 0.002) .\n\n【14】In univariate analysis testing for differences across the 3 groups, there were lower percentages of patients with IBD in the HO and HCFA categories compared with the CO group (p = 0.018). A higher percentage of patients in the CO category were noted to be obese, and this finding approached statistical significance (p = 0.08). The percentage of patients in the CO group who were obese (34%) was statistically higher than the state average (23%) (odds ratio 1.7, 95% CI 1.02–2.99). HO cases were more likely to have had prior exposure to antibacterial drugs compared with the CO and HCFA groups (p<0.001). The use of antiulcer medication and coexisting conditions such as immunosuppressive conditions, diabetes, and end stage renal disease were identified with statistically similar frequency in the 3 groups. CDI in HO group was associated with longer hospital stays and higher likelihood of an ICU stay than that in the CO or HCFA groups (p<0.01 for both).\n\n【15】In binomial logistic regression, the CO cohort was noted to be younger (p = 0.03) and 4 times more likely to be obese (p = 0.03) compared with the CO-HCFA group . Obesity was not observed at a substantially higher rate in the CO group compared with the CO-HCFA group. The CDIs in CO group were >5 times more likely to be associated with IBD compared with CO-HCFA and ≈6.5 times more likely when compared with the HO group; only the latter comparison approached significance (p = 0.094). Compared with HO patients (p>0.001), CO and CO-HCFA patients were statistically less likely to have had antibacterial drugs before symptom onset (p = 0.01).\n\n【16】### Discussion\n\n【17】This study demonstrates relationships between CDI, IBD, and obesity. By comparing a relatively low-risk group of patients with CDI to those with more traditional risk factors, we sought to identify an association between obesity and CDI. This association was underscored by the hypothesis that in a group without exposure to health care facilities, the statistical significance of other risk factors such as obesity and IBD may be increased. Under the categories created by SHEA-IDSA guidelines, case-patients with CO CDI were 4 times more likely to be obese compared with the community-onset health care facility–associated group, and almost 2 times as likely to be obese as the general population of Massachusetts. Like IBD, obesity may be associated with a higher risk of CDI.\n\n【18】The relationship between IBD and CDI is evolving. Issa et al. and Rodemann et al. demonstrated that ≈80% of IBD patients who acquired CDI did so in outpatient settings, and another series of inpatients showed IBD patients had CDI onset within an average of 1 day of admission, compared with 4 days for other CDI case-patients. IBD patients received a greater number of antibacterial drugs, had greater exposure to health care facilities, and were frequently administered immunosuppressive drugs that could have increased their risk of infection . However, there is biologic plausibility that IBD may create an intestinal environment hospitable for CDI, independent of antibacterial drugs and immune modulators.\n\n【19】Studies have demonstrated that the increased incidence of CDI and colonization in IBD patients may be mediated by a derangement of gut flora . Evolving literature suggests that the community of microorganisms living in symbiosis with the human host affects energy metabolism, alters responses by innate immunity, and can determine outcomes of host pathogen interactions . The diversity and the composition of the gut microbial community determine the effectiveness of its symbiosis with the host . Changes in fecal microbiomes have been demonstrated in recurrent cases of CDI associated with antibacterial drug use . This defect is also noted in obese patients with IBD . The similarities in alterations of normal microbial symbiosis in both IBD and obesity may explain why obese patients may be at risk for acquiring CO CDI. Greenblum et al analyzed fecal samples from a cross section of volunteers and examined gene-level and network-level topological differences in intestinal microbiomes associated with obesity and IBD. Obesity and IBD were linked with enzyme level variations and topographical changes, suggesting low diversity environments .\n\n【20】Aside from the overall decrease in richness of phylotypes of bacterial species, certain host conditions appear to be associated with specific changes in the intestinal microbiota and up or down-regulation of certain bacterial species. The development of CDI appears to be linked to the loss of the ability of the indigenous intestinal species to resist colonization by additional invasive pathogens . In particular, a decrease in the relative proportion of the phylum Bacteroidetes to that of Firmicutes has been associated with CDI. Manges et al. found that these changes could be driven by antibacterial drugs and health care exposure . IBD and obesity manifest similar changes in the fecal microbial community . Obesity may provide a milieu with increased susceptibility for invasion and infection by _C. difficile_ .\n\n【21】Higher BMI has been associated with a greater chance by trauma patients of acquiring health care–associated infections, including CDI. In a recent retrospective case control study, Bishara et al. demonstrated a higher BMI in all hospitalized patients with CDI compared with inpatient controls (p<0.001) . This observation was particularly notable because case-patients and controls had above average BMIs, suggesting that there may be an even more drastic association in the general population. In addition, Bishara et al. noted this relationship between BMI and obesity without differentiation in the probable sites of acquisition . We were unable to show a difference in obesity between the CO and HO groups, implying that either there is an inherent difference between patients with health care–associated onset and those with hospital onset, or that our study was not statistically powered to detect the association. Because we could show no notable differences between CO-HCFA and HO in regression analysis except for antibacterial drug use, we believe that our study was limited by our sample size.\n\n【22】Use of antiulcer medication has been identified as a risk factor for CDI in the community . There was no difference in the rate of antiulcer medication use among the 3 subgroups in this cohort. This may reflect local prescribing practices of physicians, because inpatients and outpatients were equally likely to be exposed to these medications. Case-patients who had HO CDI were more likely to have been in an ICU during the study admission. Overall, this trend and the increased incidence of prior antibacterial drug use in this group may represent a higher severity of illness in this cohort. The greater likelihood of nosocomial acquisition of disease could be caused by longer lengths of hospitalization . Most CO–HCFA case-patients had a history of prior hospitalization. Long-term facilities, day surgery centers, and outpatient hemodialysis sites appear to also serve as potential sites of increased transmission of CDI outside the hospital.\n\n【23】The main limitation of this study is related to the lack of data for true prevalence of risk factors in each group, because we compared cases with each other on the basis of the location of onset and not to controls. Hence, the trends observed require further validation with prospective analysis to establish whether there is a true association between obesity and CDI as noted in the CO cohort. The analysis is also limited by the retrospective design and, as mentioned before, the relatively small sample size. In addition, data collection was dependent on chart extraction, hence dependent on provider documentation. Since cases were defined by patient samples with positive diagnostic assays, this study did not differentiate between patients who were colonized and those with active disease. This may have overestimated true disease prevalence, as has been demonstrated . However, because only non-formed fecal samples are accepted for analysis at our laboratory, it is likely that the majority of the cases represented true disease.\n\n【24】### Conclusions\n\n【25】Translational research could help elaborate the dimensions of the interaction of the intestinal microbiota with _C. difficile_ in obese patients. It would also be of interest to establish if there is a dose response between BMI and risk for CDI acquisition. Further, it is critical to establish whether obesity is a risk factor for high rates of _C. difficile_ colonization, as is IBD; if that risk factor is established, prospective observations would improve understanding of whether obesity plays a role in the acquisition of CDI, or alters severity of disease and risk for recurrence. Last, the examination of the CO–HCFA group in this study underscores the importance of increased infection control at ancillary health care facilities and surveillance for targeting high-risk patients who were recently hospitalized.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "94a09f14-2bc9-4697-96d1-7eade709bdb5", "title": "Recurring Methicillin-resistant Staphylococcus aureus Infections in a Football Team", "text": "【0】Recurring Methicillin-resistant Staphylococcus aureus Infections in a Football Team\nFootball-related skin infections have gained national notoriety and public interest . Media coverage of high-profile athletes and teams with skin and soft tissue infection (SSTI) has provided more impetus for research of these infections. Annually, 60,000 college football players compete among 600 teams . The community-associated methicillin resistant _Staphylococcus aureus_ (CA-MRSA) strains have been a cause of SSTI outbreaks among athletes participating in football, wrestling, rugby, soccer, fencing and canoeing . SSTIs (pustules, “insect bites,” boils, and abscesses) are the hallmarks of CA-MRSA infections . CA-MRSA causes disease in young, otherwise healthy persons without the usual risk factors for MRSA infections . In addition, CA-MRSA has unique molecular markers (SCC _mec_ IV and Panton-Valentine leukocidin) and fewer resistance genes to non β-lactam antimicrobial drugs than healthcare-associated MRSA strains .\n\n【1】In August 2002, the Los Angeles County Department of Health Services (LACDHS) received reports of 2 college football players (players X and Y) on team A hospitalized for SSTIs due to MRSA, which was later identified as a community-associated strain (USA300) . No other MRSA SSTI was reported on team A until 1 year later. On August 25, 2003, an infectious disease physician notified LACDHS of the hospitalizations of 4 different players on team A with MRSA SSTIs. Despite the lack of background SSTI data on this team, the recurrence of infections prompted an investigation with objectives of identifying players with MRSA SSTIs and nasal carriage, conducting epidemiologic studies, implementing outbreak-control measures, and determining the genotype of the outbreak strain.\n\n【2】Team A was a college football program with 107 players on the roster at the time of the outbreak. The team practiced and played 11 of their 13 games on grass fields. Players began their football season with training camp from August 5 to 18, 2003. In camp, players were sequestered and lived together, in suites of 4 per dormitory, to foster camaraderie among teammates. Rigorous practices were held twice daily in the hot, summer weather.\n\n【3】### Methods\n\n【4】##### Case Finding\n\n【5】Case-players were defined as team A members with MRSA culture-confirmed SSTIs or SSTIs presumably caused by the USA300 strain in the outbreak period August 5 to September 5, 2003. Because we suspected that disease exposure occurred during camp, we chose the study period from the start of training camp to ≈2 weeks after the end. Our experience with other SSTI outbreaks found that in most persons lesions develop within 2 weeks postexposure to CA-MRSA. To find case-players, we reviewed the trainer’s treatment log to identify players with skin lesions who required medical or surgical interventions. We asked the staff to conduct skin inspections of all players. Players were encouraged to report any skin lesion. In addition, we queried the student health center to determine if these infections were prevalent on campus.\n\n【6】##### Nasal Carriage Study\n\n【7】As soon as the current outbreak was recognized, a returning player (player X) was suspected to be the source of infection. Player X had 1 of the 2 cases of CA-MRSA SSTIs discovered in 2002. His locker was directly across from the index case-player, and he was a roommate, during camp, of another case-player. Trainers obtained a nasal culture from player X on August 25. On September 3, trainers obtained cultures from the anterior nares of 99 available team members for a nasal carriage study.\n\n【8】##### Laboratory Study\n\n【9】MRSA isolates from case-players and nasal carriers were characterized by using pulsed-field gel electrophoresis (PFGE) with the _Sma_ I and _Eag_ I restriction enzymes . PFGE patterns of the isolates were compared with the USA300 strain responsible for other SSTI outbreaks in Los Angeles County . This strain was previously determined to contain SCC _mec_ IV by the Centers for Disease Control and Prevention . We also characterized a sample of methicillin-susceptible _Staphylococcus aureus_ (MSSA) isolates from players’ nasal cultures.\n\n【10】##### Case-Control and Carrier-Control Studies\n\n【11】On the basis of anecdotal reports of players sleeping in the locker room on used towels and delaying treatment of cuts and abrasions, we hypothesized that poor hygiene habits and compromised skin integrity might predispose players to infection. We designed a standard questionnaire to collect data on player demographics, living situation, football activities, exposure to persons with skin infections, hygiene practices, histories of skin lesions, and clinical symptoms. Trained health department employees administered the questionnaires in person.\n\n【12】We conducted unmatched case-control and carrier-control studies. Controls were selected by jersey numbers, by using a random-number generator, from asymptomatic teammates without nasal carriage of MRSA. Teammates with positive nasal cultures for MRSA were considered carriers. Carrier-players were defined as carriers with matching PFGE pattern to the USA300 strain. We excluded non-USA300 strains carriers to remove players who might represent the background prevalence of MRSA in the community. Players who were not available for interviews were not included. Bivariate analysis was completed by using Fisher exact test in Epi Info version 3.3 (CDC, Atlanta, GA, USA). Statistical significance was defined as p values <0.05. Because of the small sample size and zero-valued cells, similar risk factors from the bivariate analysis were grouped into categories. Multivariate analysis was completed by using the conditional exact test in SAS version 8 (SAS Institute Inc. Cary, NC, USA).\n\n【13】##### Outbreak Control Interventions\n\n【14】Upon recognition of the outbreak on August 25, team A instituted daily hexachlorophene showers for all players, increased the frequency of cleaning the facilities and athletic gear, disinfected the whirlpool tubs, provided more towels, and posted hand-hygiene signs in the locker room. Once nasal culture results were available, team physicians attempted to decolonize carriers with intranasal mupirocin . We recommended improving the timeliness of wound care, barring case-players from playing unless wounds were covered, discouraging the sharing of personal items and tubs, prohibiting sleeping in the locker room, and checking laundry procedures. We also disseminated CA-MRSA educational materials to staff and team members .\n\n【15】### Results\n\n【16】##### Characteristics of Case-players\n\n【17】We identified 11 case-players out of 107 team members for an attack rate of 10%. Cases were diagnosed during or within 2 weeks of the end of training camp . The first case was diagnosed on August 15, the last on September 1. With 1 exception, infections occurred before the first scheduled game on August 30. The most common sign was a boil . The elbow was the most common body site infected. No infection was at a current site of skin trauma or occurred at >1 body location simultaneously. Before hospitalization, the index and second case-players were given cephalexin and levofloxacin, respectively, for their infections without any clinical improvement. In total, 4 case-players were hospitalized and treated with parenteral vancomycin. Subsequent nonhospitalized case-players were treated with doxycycline and rifampin. Lesions of 9 players required surgical incision and drainage. All case-players ultimately responded to treatment with resolution of their infections. The median age of case-players was 20 years, with a median tenure of 2 years on team A. Linemen had the highest attack rate (18%) among all field positions . No quarterbacks, wide receivers, or special team players (kickers, punters) were affected. All were healthy men without underlying illnesses. Eight (80%) case-players interviewed reported having never worn elbow pads, and 6 (60%) usually did not have cuts or abrasions covered until >1 hour postinjury.\n\n【18】##### Characteristics of Carriers\n\n【19】Nasal cultures were obtained from 99 (93%) of 107 team members. Twenty-six (26%) cultures were positive for _Staphylococcus aureus_ , among which 8 (8%) were positive for MRSA, including player X. Player Y’s nasal culture was negative. The median age of carriers was 20 years (range 18–21 years), and median tenure on the team was 2.5 years (range 1–5 years). MRSA carriage was highest in linemen (38%). We identified 1 case-player with nasal carriage of MRSA. However, trainers obtained nasal cultures after all case-players had begun antimicrobial treatment. Locker room assignments showed clustering of case-players and carrier-players, notably the proximity of the potential source player (player X) to the index case-player . Among MSSA carriers (n = 18), no clustering of locker locations was seen. MSSA carriage was highest among linemen (28%) and cornerbacks/safeties (28%).\n\n【20】##### Laboratory Results\n\n【21】Four (57%) of seven MRSA isolates from culture-confirmed case-players were available for PFGE analysis. All were indistinguishable from each other, the USA300 strain found in Los Angeles County, and the isolates from 2 cases (players X and Y) in 2002. We denoted this genotype as strain A. Of 6 (75%) available MRSA isolates from 8 carriers, 4 (67%) were indistinguishable from strain A. Two carriers had unique MRSA genotypes (strains B and C) with \\> 7 bands difference between them and between strain A. Strains A, B, and C, player X and Y’s isolates, demonstrate community-associated antimicrobial susceptibility phenotypes . Among 5 MSSA isolates characterized, all had \\> 7 bands difference among themselves as well as from the USA300 strain.\n\n【22】##### Case-control and Carrier-control Study Results\n\n【23】Ten of 11 case-players were enrolled in the study; 1 was unavailable for interview. During camp, case-players were 15 times more likely than controls to have shared bars of soap with teammates and more likely to have had preexisting cuts or abrasions .\n\n【24】Five of 6 carrier-players were available for interviews. Carrier-players were 60 times more likely than controls to have had a locker adjacent to or across from a teammate with an SSTI and 47 times more likely to have shared towels with teammates . Carrier-players were more likely than controls to lived on campus in a dormitory or fraternity house. Among carrier-players and controls, players who lived on campus had a higher mean number of roommates than those who lived in off-campus apartments (2.3 vs. 1.5, p = 0.046).\n\n【25】Potential risk factors were grouped into 3 categories: “sharing” (sharing soap/towels with teammates), “skin injury” (cuts, abrasions), and “close contact” (locker adjacent to case-players, living on-campus). Multivariate analysis including these categories indicated that sharing was a significant risk factor for CA-MRSA infection (OR 12.1, 95% CI 1.83–108, p = 0.006) and carriage (OR 17.4, 95% CI 1.03–undefined, p = 0.047).\n\n【26】##### Postintervention Surveillance\n\n【27】Daily hexachlorophene showers were in use from August 25 to September 19. No new infections were reported during the 4 weeks after the discontinuation of the hexachlorophene showers. From October 20 to November 9, MRSA SSTI developed in 4 players: a lineman with a chin abscess, a linebacker (player Y from 2002) with an elbow boil, a quarterback (player Z) with folliculitis on a leg, and a tight end with a gluteal boil. Three MRSA isolates (except from the tight end) were available for PFGE; all matched strain A. The lineman in this cluster shared bars of soap with his roommate, a case-player.\n\n【28】Because of ongoing disease transmission and to identify potential reservoirs of MRSA, all 28 staff and student trainers and managers were nasally cultured on November 3; 11 (39%) were positive for MSSA. None was positive for MRSA. On November 22, we observed an official game. Previously unidentified lapses in hygiene practices occurred on the sidelines. We observed that student trainers reused hand towels between players, and players shared towels among themselves. Subsequently, the team switched to single-use towels on the sidelines. No new infections were reported for the remainder of the 2003 season. In the following season (August–December 2004), no MRSA SSTI outbreak occurred on team A. However, player Z had a recurrence of MRSA pustules on the forearm and leg in October 2004. He responded to outpatient treatment with doxycycline, rifampin, and incision and drainage of the lesions. His MRSA isolate was not available for PFGE. Throughout the last 3 football seasons, we received no reports of SSTI outbreaks among opposing athletes after playing this team.\n\n【29】### Discussion\n\n【30】This report is the first of recurring CA-MRSA SSTIs in a football team during consecutive seasons. From 2 cases in 2002 to an outbreak involving 11 players in 2003 and then 1 case in 2004, we have shown that eradicating these infections is difficult once they become established in a football team. Infections were likely propagated year to year from previously infected players, and they appear to be susceptible to recurring colonization and infection themselves.\n\n【31】Consistent with other reports, our findings implicate sharing personal items and improper wound care as risk factors for CA-MRSA infections . While the concept is counterintuitive, soap sharing was also associated with MRSA infections in a prison outbreak . Therefore, teams should consider switching to liquid soaps in an outbreak situation and always provide prompt wound care.\n\n【32】Linemen were identified as a high-risk subgroup. They engage in frequent and aggressive skin-to-skin contact during games, similar to hand-to-hand combat maneuvers as reported in a military MRSA outbreak . In addition, linemen tend to be physically larger than their teammates. Increased body mass index and lineman position were risk factors for CA-MRSA infection in another football team outbreak .\n\n【33】Two recent reported CA-MRSA outbreaks in football teams detected no nasal carriage in their combined cohort of 182 football players . In contrast, we document a high MRSA nasal carriage rate (8%) among team A players even while hexachlorophene showers were provided. The actual carriage rate might be higher, since we obtained nasal cultures after all case-players had begun antimicrobial treatment. Additional case-players may have been carriers as well, but they may have been decolonized before culture. Further research is needed to study the association between nasal carriage of CA-MRSA and SSTI to develop decolonization guidelines. The data facilitated a carrier-control study. Similar to risk factors for infection, nasal acquisition of CA-MRSA is associated with sharing personal items, particularly in the locker room.\n\n【34】Crowded living conditions during training camp appear to facilitate the acquisition of CA-MRSA, which then propagates in on-campus housing. Investigators of an outbreak among military recruits found an association between having a roommate with an SSTI and MRSA infection . Consequently, players’ living arrangements should be as dispersed as possible.\n\n【35】Unique to our investigation are 1 confirmed and 2 presumed community-associated strains of MRSA. We presented laboratory results indicating that the outbreak strain was likely the USA300 genotype. Since we do not have PFGE results from 6 case-players, different strains could have caused those infections. However, a multiclonal outbreak is unlikely, since other MRSA SSTI outbreaks in Los Angeles County among soccer players, men who have sex with men, jail inmates, and newborns have been exclusively due to the USA300 strain . In contrast, our limited data do not suggest a clonal spread of MSSA on this team. Multilocus sequence typing was not available locally, which prevented further characterization of the isolates.\n\n【36】Selection bias of case-players and controls is a limitation of this study. Enrollment of players with uncultured infections and those without PFGE results introduces the possibility of misdiagnosis and misclassification. Most football teams assign jersey numbers on the basis of field position. Therefore, our control selection method might not have captured a representative sample of the team. However, the distribution of field positions among controls and the entire team appears similar . The small sample size produces less precise (wide confidence intervals) results and prohibits more in-depth multivariate analyses. Reporting bias is possible, since players and the team fear negative publicity, and we do not have data on risk factors during the off-season. In order to maintain confidentiality, we were unable to interview several players because of high media scrutiny.\n\n【37】As CA-MRSA strains become more prevalent in the community , SSTIs will likely continue to afflict football players. Despite comprehensive infection control interventions, sporadic cases of MRSA SSTIs continue to occur on this team. However, a recurrent outbreak was averted in the latest season likely because of increased vigilance to proper hygiene practices and awareness of this disease among the staff and players.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "475c2849-06a6-46f8-ba5e-7b18b0591c0d", "title": "Livestock Density as Risk Factor for Livestock-associated Methicillin-Resistant Staphylococcus aureus, the Netherlands", "text": "【0】Livestock Density as Risk Factor for Livestock-associated Methicillin-Resistant Staphylococcus aureus, the Netherlands\n_Staphylococcus aureus_ is a zoonotic and human pathogen that can cause a range of health outcomes in humans from minor to life-threatening infections of the skin, bloodstream, respiratory system, urinary tract, and surgical sites . An increasing proportion of _S. aureus_ infections involve drug-resistant strains, including methicillin-resistant _S. aureus_ (MRSA) . In 2007, 171,200 MRSA infections occurred in European Union member states plus Iceland and Norway, resulting in 1,050,000 extra days spent in the hospital  This translates into excessive hospital inpatient and outpatient costs because of the need to isolate patients and because patients require longer stays and more extensive treatments . In 2005 in the United States, an estimated 94,000 MRSA infections resulted in >18,000 deaths .\n\n【1】MRSA has, in the past, been largely associated with hospitals and other healthcare facilities, but since 2000, the majority of MRSA infections in most countries are acquired in the community outside of healthcare settings . These strains of community-acquired MRSA are vital public health concerns, but less is known of their origins and routes of transmission. Among these strains of community-acquired MRSA, livestock-associated (LA) strains have been detected in several regions of the world .\n\n【2】Originally, the LA-MRSA strain studied here was denoted as nontypeable MRSA because of the inability to type it by using standard methods of pulsed-field gel electrophoresis . It was first detected in the Netherlands in 2003  and, as of 2010, accounts for >40% of the MRSA cases in that country . LA-MRSA has now been identified largely as a single clonal complex on the basis of multilocus sequence typing (ST398), and this clonal complex has a demonstrated association with pigs, cows, and other animals , although other clonal complexes have also been shown to be associated with livestock as well. In several European countries, increased risks of carriage have been reported in persons in contact with pigs and veal calves, including farmers, veterinarians, and slaughterhouse workers . In the Netherlands, among these occupational groups, the prevalence of ST398 carriage is roughly 42% , whereas the prevalence of any strain of MRSA in the general population is <1% .\n\n【3】The emergence and transmission of LA-MRSA among humans and animals (such as poultry, horses, companion animals, pigs, and cattle) have recently been reviewed . Most epidemiologic studies have focuses on identifying individual and farm level characteristics associated with LA-MRSA carriage and on studying those in direct contact with livestock. In 1 study among pig farmers and their household members, 30% were carriers of MRSA ST398, and the risk for carriage was related to direct exposure to pigs . A study among veterinarian field workers found that after short-term occupational exposure to pigs, 17% of them carried MRSA. However, >90% of those lost this carriage the next day . Another study found a clear association between carriage and the duration of contact with veal calves and that carriage was strongly reduced after a period of absence from animal contact . These studies, which indicate a high risk for carriage from livestock contact but little persistence of carriage after interruption of animal contact, bring into question the ability of LA-MRSA to spread into the population. Only 1 study examined the role of living in a livestock-dense region as a risk factor as well but did not find it to be a risk factor . This study used a random mailing in the 3 most pig-dense municipalities in the Netherlands. Of the 534 adult respondents without livestock contact, 1 person was positive for MRSA (0.2%), compared with 13 of 49 persons who worked or lived on a livestock farm (26.5%).\n\n【4】In 2007, risk factors for LA-MRSA carriage in the Netherlands were investigated by van Loo and colleagues . They found that risk factors for increased odds of LA-MRSA carriage included contact with livestock, acquiring MRSA through known risk factors such as travel to a foreign country or recent interaction with the healthcare environment, and living in a rural area. Our study, conducted during 2008–2011, built upon this work to test the hypothesis that persons living in areas of high pig density may be at increased risk for carrying LA-MRSA. We did this by combining information about where persons lived and what the livestock density was in these areas for which existing information on risk factors had been determined in the 2007 study.\n\n【5】### Methods\n\n【6】##### Data\n\n【7】We used data from van Loo et al. which consisted of records of all index case-patients with nontypeable MRSA carriage (now referred to as LA-MRSA) from the Netherlands from its emergence in 2003 through September 2005, before the country adopted active surveillance of high risk populations. Information on case-patients and controls was obtained from a national MRSA surveillance program through the country’s Institute for Public Health and the Environment . Case-patients were LA-MRSA index patients, that is, the first in a cluster of persons who tested positive for LA-MRSA from a given reference laboratory. Each LA-MRSA case-patient was matched with 2 controls from the same laboratory. The controls were also index patients, but had tested positive for a typeable strain of MRSA (T-MRSA) instead of to LA-MRSA. Further details on subject selection can be found in the original article .\n\n【8】The same variables used in the 2007 study were used in this current study: contact with pigs, contact with cows, the probable source of MRSA, and whether one lived in a rural area. Probable source of MRSA was placed by the original authors into the following categories: healthcare setting, foreign source (such as travel to another country), other source, or an unknown source . Our study added municipality level variables of livestock and human population densities and location of residence of study participants. (Municipalities are administrative boundaries in the Netherlands that comprise provinces. In 2005, there were 498 municipalities.) To accomplish this, our inclusion criteria were residence in the Netherlands, availability of information on individual contact with livestock, and geographic information sufficient to support mapping each person to the 6 digit postal code of his or her residence.\n\n【9】We then assigned spatial coordinates to LA-MRSA case-patients and T-MRSA controls on the basis of their 6-digit postal code using the geographic information system software, ArcGIS version 9.3 . When this method was not sufficient, we used Google Earth .\n\n【10】We downloaded municipality level statistics of population; land area; and numbers of swine, veal calves, and cows in 2005 from CBS StatLine . Livestock densities and population density were calculated as the number of animals (pigs, cows, and veal calves) per hectare of land in a municipality. In ArcGIS, we determined in which municipality participants lived, and assigned to them municipality-level characteristics of animal and population densities. We determined counts of case-patients and controls for each of the municipalities using ArcGIS. This study was reviewed and approved by the Institutional Review Board at Johns Hopkins Bloomberg School of Public Health.\n\n【11】##### Statistical Analysis\n\n【12】Summary statistics for all relevant variables were reported by using STATA version 10 . We explored the spatial variation in risk for LA-MRSA—or the concept that the risk or odds of acquiring LA-MRSA varies geographically—using spatial methods available in the R statistical package . We estimated spatial intensity of case-patients and controls for locations across the study area, defined respectively as the expected numbers of case-patients and controls per km 2  . Spatially varying intensity provides an estimate of regions of high and low densities of case-patients and controls. Spatial intensity is often measured as weighted counts as described . We used the quartic kernel as our weighting scheme in this study. Using the intensity estimates, we calculated the spatial odds of LA-MRSA to compare the geographic variation of case-patients and controls across the study area. The spatial odds of LA-MRSA per km 2  compared with those for T-MRSA per km 2  were calculated as the ratio of estimated case-to-control intensities .\n\n【13】Contact with pigs, contact with cows, and rural (versus urban) residence were modeled as binary variables. Probable MRSA source was a categorical variable. We compared probable acquisition of MRSA from a foreign country, acquisition from another source, or acquisition from an unknown source with the referent group of healthcare-related acquisition.\n\n【14】We determined goodness of fit of the models using Akaike information criteria and the Hosmer-Lemeshow goodness-of-fit test. Likelihood ratio tests were used to compare multivariate nested models. The densities of livestock were right skewed; thus, we log-transformed the variables to create a more linear relationship between animal density and log odds of LA-MRSA. For ease of interpretation, instead of the 1 log increase in livestock densities, we used a doubling of livestock densities, which is calculated by raising 2 to the power of the β of the density coefficient in the logit model . Variograms were used to diagnose possible spatial variation in regression residuals, with inference on regression parameters adjusted accordingly.\n\n【15】In a separate but related analysis, we identified specific clusters of LA-MRSA. We used SaTScan version 9.0  to conduct this cluster detection analysis with a Poisson model of counts of case-patients per municipality after adjusting for population size as described . SaTScan is a software package that is used to analyze spatial and temporal patterns in data. It uses a moving window method (in this application, over a set of contiguous municipalities) and determines the presence of a cluster on the basis of whether the estimated risk within a window is significantly greater than the estimated risk outside of the window. Statistical significance is based on the null hypothesis of Poisson constant risk . We created maps showing the identified clusters after adjustment of population density per municipality in ArcGIS. We made similar maps after further adjusting for pig, cow, and veal calf densities per municipality.\n\n【16】### Results\n\n【17】##### Study Population Characteristics\n\n【18】Descriptive statistics of the study population are shown in Table 1 . From the total population used in the Van Loo analysis of 111 persons (35 case-patients, 76 controls) , 87 persons (27 case-patients, 60 controls) were included in our study after we excluded persons who lived outside of the Netherlands (n = 4), persons for whom spatial information was insufficient (n = 3), and persons for whom information about individual contact with livestock was lacking (n = 17).\n\n【19】Of the 87 subjects with complete case information, most of those who had contact with pigs (10/13, 76.9%) and cows (7/8, r 87.5%) were LA-MRSA case-patients. Three subjects had contact with both pigs and cows, 2 of whom were case-patients. Twelve of 27 persons without any direct contact with livestock were LA-MRSA positive (44.4%).\n\n【20】Specific locations of case-patients and controls are plotted against municipality level population , cow density , pig density  and veal calf density . Case-patients and controls had significant differences in human and livestock densities per municipality .\n\n【21】##### Spatial Odds\n\n【22】Relatively high concentrations of controls are seen in general areas of high population density while higher spatial concentrations of case-patients are seen in the more agricultural areas of the country . We estimated spatial odds to give a visual assessment of the spatial variation in risk across the Netherlands . It is evident that the greatest differences in odds between case-patients and controls are in general areas of high pig density, as was originally reported by van Loo and colleagues . The elevated spatial odds in the northern part of the country are a spurious result because of small numbers of case-patients and controls and not something that we put forth as a valid result.\n\n【23】##### Univariate Logistic Regression\n\n【24】Univariate models results are reported in Table 2 . Persons who have contact with pigs have 11.18 times the odds of carrying LA-MRSA (compared with odds of carrying T-MRSA) than persons without pig contact (95% CI 2.76–45.30; p<0.001). A similar relationship is seen when persons with and without contact with cows are compared (odds ratio \\[OR\\] 20.65; 95% CI 2.39–178.31; p <0.006). Living in a rural area rather than living in an urban area is associated with 11.2 times the odds for LA-MRSA compared with T-MRSA (95% CI 3.15–39.76; p<0.000). Carriage of MRSA from an unknown or “other” source, as compared to healthcare settings (a priori known to be associated with typeable MRSA), was significantly (p<0.05) associated with the odds of LA-MRSA carriage as compared to T-MRSA.\n\n【25】We found that when pig density per hectare is doubled within a municipality, the odds of acquiring LA-MRSA in a univariate model are increased by 29.5% over the odds of acquiring T-MRSA (p<0.003). Similarly, doubling cow and veal calf densities increases the odds of acquiring LA-MRSA compared with those for acquiring T-MRSA by 75.4% (p<0.001) and 19.8% (p<0.001), respectively.\n\n【26】When the inclusion criteria required by our study were used, 16% of original data were lost. We conducted sensitivity analyses by coding all persons with missing livestock contact information as either all having contact or as all not having contact to produce what might be bounds for high and low extremes. In all cases, livestock densities remained significant independent risk factors at the 0.05% level .\n\n【27】##### Multivariate Models\n\n【28】Multivariate model results are reported in Table 4 . Model 1 is based only on the original individual level variables from van Loo’s study : contact with pigs and cows, rural versus urban residence, and information on patient’s probable source of MRSA. In model 1, both contact with pigs and rural residence remain significant predictors as they were in the univariate models, even when adjusting for contact with cows and probable MRSA source. Odds for LA-MRSA compared with those for T-MRSA were 11.6 times higher for a foreign source of MRSA than they were with a healthcare source (95%CI 1.04–129.63; p<0.046). Similarly, the odds were 9.56 when persons with an unknown source were compared with those with a healthcare source (95% CI 1.76–51.93; p<0.009). Acquiring MRSA from another (other) source compared with healthcare acquisition also had increased odds, but this result was not significant (OR 4.3, 95% CI 0.55–33.56; p<0.164).\n\n【29】Models 2–4 build on model 1 (the base model) by adding in the logs of pig, cattle, and veal calf densities per municipality, respectively, with the same individual level variables used in model 1 . Model 2 builds on model 1 by adding a term for the log of pig density. The odds ratio comparing LA-MRSA to T-MRSA for a 1 log increase in pig density per hectare after adjusting for the individual risk factors (the variables in model 1) for LA-MRSA was 1.37 (95% CI 1.01–1.87, p<0.041). A doubling of the pig density per municipality increases the odds of LA-MRSA carriage compared with T-MRSA carriage by 24.7% after adjustment for individual level risk factors. Model 3 builds on model 1 by incorporating the log of the cow density per municipality. Adjusting for the individual level predictors, a 1 log increase in cow density yields a 2.28 increase in odds for LA-MRSA compared with T-MRSA (95% CI 1.17–4.45, p<0.016). Here, a doubling of cow density in a municipality increased the odds of LA-MRSA compared with T-MRSA by 76.9%. The odds ratio of carrying LA-MRSA compared with those of carrying T-MRSA in model 4 for a 1 log increase in veal calf density after adjustment for individual variables was 1.37 (95% CI 1.08–1.72, p<0.009). Thus, a doubling of the veal calf density per municipality yields a 24.09% increase in the odds of carrying LA-MRSA compared with carrying T-MRSA.\n\n【30】The Hosmer-Lemeshaw goodness-of-fit tests indicate that all models fit the data sufficiently well. The Akaike information criteria and likelihood ratio values for models 2–4 indicate that adding area-level animal density variables improves the original model (model 1). Variograms of residuals from the 4 models did not reveal any substantial spatial variation.\n\n【31】##### Cluster Detection\n\n【32】Cluster detection analysis results indicate that after adjusting for the size of the population in a given municipality, 1 significant cluster of LA-MRSA cases (relative risk 5.2, p<0.014) was found when a maximum of 20% of the population at risk was designated as the maximum spatial cluster size. Figure 3 (panels A–C) shows the cluster detection results mapped on top of veal calf density, all cattle density, and pig density for visual identification of associations.\n\n【33】To test whether accounting for livestock density at the municipality level would eliminate the existence of this hot spot of LA-MRSA, we ran additional analyses in SaTScan with adjustment for the density of each animal population separately . These results indicated that adjusting for animal densities eliminated the presence of the cluster, further supporting the hypothesis that livestock densities per municipality are key risk factors for LA-MRSA carriage.\n\n【34】### Discussion\n\n【35】Our findings indicate that regional density of livestock is a notable risk factor for nasal carriage of LA-MRSA for persons with and without direct contact with livestock. This finding has been emphasized in recent research that found LA-MRSA carriage in persons without connections to the farm environment . A recent study indicated that proximity to farms is a potential risk factor, even in absence of direct contact between humans and animals . In addition, MRSA has been found in meat; diet may therefore provide another route of exposure for the general population .\n\n【36】We observed in the multivariate analysis that living in a region with high cattle density conferred higher odds of LA-MRSA carriage than did living in a region of high pig or veal calf density. We are not certain what may explain this association, but it does warrant further investigation. We found in our multivariate models that some of the risk factors previously identified by univariate analysis by van Loo and colleagues  dropped out as being less significant when regional livestock density was included in a multivariate model, such as direct contact with pigs and cows and living in a rural location. Intriguingly, acquiring MRSA from an “unknown” source remained highly significant in all of the multivariate models. These results highlight the value of considering these individual-level variables, together with regional level data, as an update of the univariate analysis conducted by van Loo and colleagues in 2007.\n\n【37】This analysis is limited by the small size of the dataset. However, even with such a small dataset, and after adjusting for known and supposed LA-MRSA risk factors, the densities of livestock per municipality remain strong and independent risk factors for LA-MRSA carriage.\n\n【38】A second limitation of the study is that the case-patients were initially restricted to index case-patients, which inherently selected against detecting secondary transmission. Conducting a future study that includes non–index case-patients would produce a more accurate picture.\n\n【39】A final limitation is the possibility of recall bias in the participants’ reports of exposures to livestock, leading to a misclassification of exposure. Such a nondifferential information bias may have biased our results toward the null hypothesis.\n\n【40】This work has potential policy implications for MRSA surveillance in countries where a substantial percentage of total MRSA cases are LA-MRSA, such as the Netherlands. Starting in 2006, health policy in the Netherlands has required testing for MRSA carriage on admission to the hospital for persons living or working on pig farms. This study suggests that this screening program may need to be expanded to include other persons from municipalities where livestock densities are high.\n\n【41】Although research has indicated that LA-MRSA is not readily transmitted from person to person , cases continue to be reported with no identified livestock-associated risk factors. Some possible modes of exposure could involve contact with other domesticated animals, person-to-person contact, and contact with contaminated meat or, in some cases, environmental pathways such as air or waste releases from farms to the surrounding community. Future research should assess these factors in terms of their relationship to living in livestock-dense areas and the likelihood of exposure to MRSA with a larger sample sizes. Information from the statistically significant cluster in the cluster detection analysis can be used to target interventions in the Netherlands. Future work should investigate more recent cases, specifically those without direct links to livestock farming.\n\n【42】We confirm what has been suggested in other studies that veal calf farming (not just pig farming) is a risk factor for LA-MRSA. We also demonstrate a relationship between nontypeable MRSA and all cattle, not just veal calves. The hypothesis that a relationship exists between other types of cattle farming and LA-MRSA carriage should also be explored in further research.\n\n【43】These findings also have the potential to affect countries beyond the Netherlands. Although pig farming is an important industry in the Netherlands, its scale there is greatly overshadowed by the density of pig-farming operations in the United States. In the United States, in 2007, there were 75,442 pig farms, 8,206 of which have >2,000 pigs on them (10.9%) . For comparison, in the Netherlands in 2000, of the 14,524 pig farms, only 983 housed >2,000 swine (6.8%). Future work could investigate the relationship between these more intensive livestock operations and drug-resistant microorganisms, especially LA-MRSA, which at present has not been widely detected in the United States. These research findings will be useful for generating hypotheses regarding the epidemiology of LA-MRSA in the Netherlands and can provide a warning that where one lives may play a critical role in one’s risk of disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f12aa28f-73da-4e68-b22e-039a2317d477", "title": "Severe Acute Respiratory Syndrome Coronavirus 2 Infection among Returnees to Japan from Wuhan, China, 2020", "text": "【0】Severe Acute Respiratory Syndrome Coronavirus 2 Infection among Returnees to Japan from Wuhan, China, 2020\nWith the emergence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) in Wuhan, China, several countries, including Japan, repatriated their nationals . During January 29–31, 2020, a total of 566 Japanese nationals were repatriated via 3 chartered flights from Wuhan (206, 210, and 150 passengers). After passengers disembarked in Tokyo, Japan, quarantine officials assessed them for signs/symptoms (e.g. fever, respiratory illness) of coronavirus disease (COVID-19) . A total of 28 symptomatic passengers were transferred to select hospitals for isolation. The remaining 538 were transported to a designated hospital, where another 35 were found to be symptomatic and were hospitalized there or transferred to other hospitals, leaving 503 asymptomatic persons for observation in quarantine .\n\n【1】### The Study\n\n【2】We conducted day 1 entry screening by testing oropharyngeal swab samples collected from all 566 returnees at the hospitals to which they were initially transported for SARS-CoV-2 ; all tests were based on the real-time reverse transcription PCR developed by the National Institute of Infectious Diseases . Hospitalized patients in isolation and asymptomatic returnees in quarantine were monitored daily for 14 days. If any signs/symptoms developed in a quarantined person, that person was transported to a designated hospital and oropharyngeal swab samples were collected for testing. We conducted exit screening for quarantined persons who remained illness-free by collecting oropharyngeal swab samples on day 14. The National Institute of Infectious Diseases Ethics Committee approved the study , and all 566 returnees who provided specimens gave written informed consent.\n\n【3】Among the 63 passengers who were symptomatic at entry screening, 2 (3.2%) were positive by PCR ; test results were subsequently positive for 2 more. For 1 of these patients, pneumonia was diagnosed on day 1 and a sputum sample was positive on day 3; the other patient had fever and cough on day 1, pneumonia diagnosed on day 2, and a positive oropharyngeal swab sample on day 6. Excluding 1 patient who remained hospitalized for stroke, the remaining 58 patients were transferred to designated quarantine facilities after confirmation of good health and negative PCR results; all 58 remained asymptomatic after discharge, and PCR results were negative at exit screening.\n\n【4】For the 503 asymptomatic/subclinical passengers, entry-screening PCR results were positive for 5 (1.0%) ; 3 remained asymptomatic, but mild signs/symptoms (fever, headache, sore throat) developed for 2 persons (1 on day 2, 1 on day 4). Of the remaining 498 persons with negative PCR results, 484 were quarantined at designated facilities and 14 at home. During quarantine, fever developed in 1 facility-quarantined and 1 home-quarantined person on day 10; both were confirmed positive by PCR, and pneumonia subsequently developed in both. The facility-quarantined case-patient was in a single room; no other person from this facility acquired COVID-19 or had a positive test result at exit screening. One person who remained asymptomatic had a positive test result at exit screening. Exit-screening results are pending for the patient hospitalized for stroke and the remaining 13 home-quarantined persons.\n\n【5】Among the 566 returnees, 12 cases of SARS-CoV-2 infection were detected; 540/541 facility-quarantined persons were confirmed negative by PCR performed on days 1 and 14 (197/197, 199/199, and 144/145 for the 3 flights). Entry screening detected 7 infections, for an infection point prevalence of 1.2%; infection period prevalence was 2.2% (12/552 returnees with complete follow-up). Despite universal testing, entry screening captured only 7/12 cases (58.3% sensitivity). Although screening symptomatic passengers (3.2%) was more efficient than screening all passengers (1.2%), screening only symptomatic passengers missed 5/7 prevalent infections at entry. Among symptomatic passengers, with 2 initially negative persons subsequently testing positive, entry-screening sensitivity was 2/4 (50%). Among asymptomatic passengers, with 3 initially negative persons subsequently testing positive, entry-screening sensitivity was 5/8 (62.5%).\n\n【6】### Conclusions\n\n【7】Testing all returnees—with follow-up for disease onset and course—enabled us to evaluate the spectrum of severity for SARS-CoV-2 infections . From least to most severe, 4 patients experienced asymptomatic infection, 2 mild illness, and 6 pneumonia. Prospective monitoring proved essential because of the 7 prevalent infections at entry, 5 were asymptomatic, 1 mild, and 1 pneumonia. Even with potential underascertainment of asymptomatic cases because of a lack of serologic assessment  (i.e. interval-censoring during screening tests), it is noteworthy that 4/12 persons with infections were asymptomatic. Although numbers are small, severity seemed to be age dependent . No infections were detected among the 100 persons <30 years of age; of the 2 infections detected among the 138 persons 30–39 years of age, both persons were asymptomatic. Although no person in this study died, only 1 was >69 years of age. Regarding sex, excluding 1 returnee for whom sex was unknown and 14 for whom exit-screening results are pending, of the remaining 551 returnees, 9 (1.8%) of the 506 male passengers (2 asymptomatic, 2 mild, 5 pneumonia) and 3 (6.7%) of the 45 female passengers (2 asymptomatic, 1 pneumonia) were infected.\n\n【8】Our findings have public health implications. As recently reported , we found that symptom-based screening performed poorly, missing asymptomatic and presymptomatic cases. Even with universal screening, nearly half of cases were missed. Because an asymptomatic case was detected at exit screening, limiting testing of quarantined persons to those with signs/symptoms would have missed such a case; with exit-screening results pending for 14 returnees, sensitivity could be lower. The poor sensitivity of single-point testing highlights the challenges of detecting SARS-CoV-2 infections.\n\n【9】The potentially long incubation period of COVID-19 was consistent with that recently reported  and contributed to the large proportion of missed cases. Active daily monitoring ensured that specific illness-onset times were captured, protected from the limitations associated with patient recall of symptom onset . Although exposure to SARS-CoV-2 occurred at some time before quarantine (i.e. left-censored), our setting enabled us to estimate the minimum incubation period for each incident symptomatic case by taking the return date as the exposure time. Determining the specific exposure time can be difficult and is conditional according to the definition of contact. Given such qualifications, a conservative minimum incubation period of 10 days obtained prospectively in a clean quarantine setting, without recall or assumptions regarding transmission modes, is noteworthy.\n\n【10】Testing and follow-up of all returnees provided valuable information about the spectrum of SARS-CoV-2 infection. Most reported data have been from medically attended patients, skewed toward symptomatic patients and more severe cases, limiting our knowledge of the clinical spectrum of infection . In our setting, we could remove the influence of patients’ health-seeking behaviors and clinicians’ diagnostic practices and found that 4/12 case-patients were asymptomatic. At the same time, of the 8 case-patients who experienced symptoms, pneumonia developed in 6. Our findings were also consistent with the reported age-dependent nature of COVID-19 ; infection and clinical attack rates were lower among younger persons. Shedding light on the severity pyramid among those infected—not only among those who sought care—provides an evidence base for risk communication, healthcare planning, and public health response. Combined with reports suggesting transmissibility of SARS-CoV-2 from asymptomatic/subclinical case-patients , our findings suggest that controlling COVID-19 through the usual tools of syndrome-based surveillance and contact tracing alone may be difficult.\n\n【11】When confronted with an emerging pathogen, researchers can generate critical epidemiologic information by studying quarantine populations. As with the First Few X study , our design is protected from the usual biases of passively reported surveillance data. Aggregating high-quality data from these types of investigations can build a larger severity pyramid, enabling reliable estimation of various severity measures (e.g. symptomatic proportion of infected case-patients, case severity proportion among those who are symptomatic). We recommend using similar assessments to help elucidate the epidemiology of SARS-CoV-2 and inform public health response.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4d6bc50f-e727-41c1-b2f9-cd5f24bb78bf", "title": "Veterinary Safety & Health", "text": "【0】Veterinary Safety & Health\nOverview\n--------\n\n【1】Veterinary medicine and animal care workers provide medical, surgical, preventive health or animal care services for a variety of animal species in many different workplace settings. These workers are exposed to biological, chemical, physical, and psychological hazards depending on their workplace setting, species of animals worked with, and type of tasks performed.\n\n【2】In the United States there are more than 455,000 veterinary medicine and animal care workers, including 79,600 veterinarians, 102,000 veterinary technologists and technicians, 83,800 veterinary assistants and laboratory animal caretakers, and 190,520 non-farm animal caretakers \\[BLS 2017a-d\\].\n\n【3】The NIOSH National Occupational Research Agenda (NORA) , Healthcare and Social Assistance Sector Program is working with industry, labor, other stakeholders, and academics to identify and address the priority workplace safety and health hazards of these workers.\n\n【4】Veterinary Workers and Workplaces\n\n【5】Veterinary medicine and animal care workers include:\n\n【6】*   Veterinarians, veterinary technologists and technicians, veterinary assistants, and laboratory animal caretakers.\n*   Zoo and aquarium workers, including animal caretakers and grounds keepers.\n*   Animal shelter and animal control workers.\n*   Stable and kennel workers.\n*   Groomers.\n*   Animal trainers.\n\n【7】More\n\n【8】Hazard Prevention and Infection Control\n\n【9】Physical Safety\n\n【10】Chemical Safety\n\n【11】Biological Safety\n\n【12】Other Hazards\n\n【13】NIOSH Publications and Other Resources\n\n【14】* * *", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ee08ea25-a3a3-483f-94c3-59a99ed3dd72", "title": "Use of Blood Donor Screening Data to Estimate Zika Virus Incidence, Puerto Rico, April–August 2016", "text": "【0】Use of Blood Donor Screening Data to Estimate Zika Virus Incidence, Puerto Rico, April–August 2016\nZika virus, a flavivirus transmitted primarily by _Aedes aegypti_ mosquitoes, has rapidly spread in the Americas since it emerged in the region in 2015 . Although most infections are asymptomatic, Zika virus has been identified as a cause of adverse outcomes of pregnancy, including microcephaly and other congenital brain defects , and has been linked to Guillain-Barré syndrome  and severe thrombocytopenia . Zika virus also has been recognized as a potential threat to blood safety . In other arbovirus outbreaks, related mosquitoborne flaviviruses, such as West Nile virus and dengue virus, have been transmitted through blood transfusion; the high percentage of asymptomatic infections was a contributing factor . Retrospective nucleic acid testing (NAT) of blood donations after a large Zika virus outbreak in French Polynesia during 2013–2014 found detectable Zika virus RNA in 2.8% of blood donations , and cases of likely transfusion-transmitted Zika virus infection (through whole blood–derived platelets) were documented in Brazil .\n\n【1】Puerto Rico first reported local transmission of Zika virus in December 2015  and has since been heavily affected. As of October 17, 2016, a total of 25,355 cases of locally acquired Zika virus infections had been reported from Puerto Rico to the Centers for Disease Control and Prevention (CDC) national arboviral surveillance system (ArboNET) .\n\n【2】To reduce the risk for transfusion-transmitted Zika virus infection, in February 2016, the Food and Drug Administration (FDA) recommended that all US areas with active Zika virus transmission cease blood collections unless donations are screened by NAT or treated with approved pathogen-reduction technology . Blood safety interventions in Puerto Rico were limited to importation of blood units from unaffected US areas and treatment of plasma and apheresis platelets with pathogen-reduction technology until early April 2016, when FDA authorized use of an individual donation NAT test (ID-NAT; cobas Zika, Roche Molecular Systems, Inc. Pleasanton, CA, USA) under an investigational new drug application .\n\n【3】Data from blood donor screening have been used during previous arbovirus outbreaks to supplement surveillance and guide the implementation of public health interventions. For example, in 2003, blood donor screening data were used to estimate the seasonal incidence of West Nile virus among the general US population . We describe the use of cobas Zika testing of blood donations from the 2 largest blood collection organizations in Puerto Rico to estimate the total number of incident Zika virus infections in Puerto Rico during April 3–August 12, 2016.\n\n【4】### Methods\n\n【5】Since April 3, 2016, all blood donations collected in Puerto Rico have been screened for Zika virus by using the cobas Zika ID-NAT, which uses PCR amplification to detect Zika virus RNA in plasma specimens. A blood donor with a reactive cobas Zika test result on initial donation is considered to be a presumptive viremic donor (PVD). In this study, we used data on PVDs to estimate Zika virus incidence.\n\n【6】For these analyses, we used data from blood donations collected by the Banco de Sangre de Servicios Mutuos (BSIS; San Jose, PR) during April 3–August 12, 2016, and by the Banco de Sangre del Centro Médico de la Administración de Servicios Médicos (ASEM; San Jose, PR) during April 4–July 31, 2016. These organizations collect most blood donations in Puerto Rico , with collections throughout the main island. Information collected and reported to CDC included a unique donor identification number, donor sex and age, city and ZIP code of donor residence, date of donation, and cobas Zika test result. City and ZIP code of donor residence were used to identify a donor’s municipality (i.e. county) and then health region as defined by the Puerto Rico Department of Health: Aguadilla, Arecibo, Bayamón, Caguas, Fajardo, Mayagüez, Metro/San Juan, and Ponce .\n\n【7】Because the minimum amount of time donors are required to wait between whole blood and plasma donations at blood centers is 56 and 28 days, respectively, the maximum number of donations per donor during the study period was 5. To estimate Zika virus incidence, all donations from any 1 donor were included in these analyses, except for repeat donations from donors who had a previous cobas Zika-reactive donation because such results could indicate infection and thus immunity. We also excluded donations from donors residing outside Puerto Rico.\n\n【8】To calculate the total number of incident Zika virus infections and the population incidence during the study period, we first calculated the proportions of cobas Zika-reactive donations to estimate the point incidence of Zika virus infection at the time of donation. The point incidence of cobas Zika-reactive donations, which we report aggregated to the week of collection, was then scaled to give estimates of Zika virus incidence during the referenced time frame. Estimates and 1-at-a-time 95% CIs of the number of incident Zika virus infections were computed weekly and cumulatively by week beginning April 3. The weekly values are estimates of the number of incident Zika virus infections during the given week; the weekly cumulative incidence values are aggregated estimates of the number of incident Zika virus infections from April 3 to the given week. The Zika virus incidence estimation process for April 3–August 12, 2016, followed the method of Busch et al. although this approach was modified to incorporate the fact that donors are necessarily asymptomatic at time of donation. In brief, proportions of cobas Zika-reactive donations were multiplied by a factor given as the ratio of the duration of the period of collection to the average viremia duration, whereas Zika virus–infected persons are asymptomatic. Parameters used to characterize the average asymptomatic viremia duration were the overall average viremia duration, the average incubation period (i.e. duration from infection to symptom onset), and the proportion of asymptomatic infections. We used statistical computer simulation to account for uncertainty in these parameters.\n\n【9】Because demographic or geographic factors might have affected transmission rates across Puerto Rico, we compared the proportions of cobas Zika-reactive donations across these factors using Fisher exact test. Factors statistically significant at the 5% level were incorporated into the estimation procedure by simultaneously stratifying the donation and population data by these factors, using the procedure outlined earlier  to compute separate estimates of the numbers of incident Zika virus infections during the period of interest for each stratum and summing these values for an estimate of the total number of incident Zika virus infections. We divided this summation by the total size of the population at risk to give the estimated incidence of Zika virus infection for this population during the 5-month study period .\n\n【10】We used US Census estimates for 2014 for population totals by stratum . For the primary analyses, the estimates of the parameters used were 9.9 days (95% CI 6.8–21.6 days) for mean Zika virus viremia duration , 6.2 days (95% CI 5.3–7.1 days) for the mean Zika virus incubation period , and 0.79 (95% CI 0.73–0.90) for proportion asymptomatic . The key parameter was the mean duration of Zika virus viremia. We performed a sensitivity analysis to evaluate the influence of the specification of this parameter by computing estimates for the total number and percentage of Zika virus infections in the population for different values of Zika virus viremia duration, ranging from 7 to 21 days. Analyses were performed and graphics created in the R version 3.3.1 statistical software package  by using purpose-written routines, and we used StatXact version Eleven  for Fisher exact test.\n\n【11】This study involved analyses of data collected as part of public health response activities. Therefore, the Office of the CDC Associate Director for Science considered it exempt from institutional review board review.\n\n【12】### Results\n\n【13】Data on 21,643 blood donors from BSIS and ASEM were reported to CDC for April 3–August 12, 2016. Of these donors, 21,468 (17,850 from BSIS and 3,618 from ASEM) were included in the analysis; 175 were excluded because of invalid data or residence outside of Puerto Rico. Included donors made 22,028 total blood donations during the study period. Of all included donors, 190 (153 BSIS and 37 ASEM) were PVDs; 21,278 had cobas Zika-nonreactive screening test results; 20,912 were first-time donors; and 14,407 (67%) were men . Reported donor residence included all of the municipal health regions in Puerto Rico . Among the 190 PVDs, 181 had reactive cobas Zika test results on their first donation, and 9 had nonreactive results at first donation but reactive results on repeat donation. Also among PVDs, 142 (75%) were men, 67 (35%) were 45–59 years of age, and 129 (68%) resided in either Metro/San Juan (44%) or Bayamón (24%) . The overall rate of cobas Zika ID-NAT donor reactivity during the 5-month period was 89/10,000 donors.\n\n【14】Combining donation data from all health regions, we found no statistically significant difference in cobas Zika test reactivity by age group (p = 0.32), but the proportion of reactivity (number of reactive donations/number of donations) significantly differed by donor sex (women, 48 \\[0.67%\\] of 7,125; men, 142 \\[0.95%\\] of 14,903; risk ratio 1.41, 95% CI 1.02–1.96; p = 0.036) and by health region (p<0.001). By health region, the association between reactivity and sex was significant in only 1 (Ponce, in which all of the 14 reactive donations were from men).\n\n【15】Based on the 2014 US Census Puerto Rico population estimate of 3,639,000 residents and using a mean viremia duration of 9.9 days (SD ± 3.94 days) and stratifying by health region and sex, we estimated the number of incident Zika virus infections for April 3–August 12, 2016, to be 469,321 (95% CI 401,477–559,126). This number represents a Zika virus cumulative incidence of 12.9% (95% CI 11.0%–15.4%) for Puerto Rico for the 5-month period . The estimated number of Zika virus infections for reproduction-aged women (16–44 years) was 69,675 (95% CI 48,226–117,578), which represents 9.7% (95% CI 6.7%–16.3%) of the total population of women of reproduction age in Puerto Rico.\n\n【16】Estimates of the total number and percentage of the population infected with Zika virus during the study period are given using mean viremia durations of 7–21 days  Estimates for percentage of the population infected with Zika virus declined with increasing viremia duration, ranging from 16.1% for 7 days viremia duration to 5.9% for 21 days. The incidence estimate would be lower if we had used an estimated mean viremia duration of >9.9 days in our calculations .\n\n【17】### Discussion\n\n【18】In this analysis of routine blood donation screening data from the 2 largest blood collection centers in Puerto Rico, we estimated that 469,321 persons were infected with Zika virus during April–August 2016, assuming a mean viremia duration of 9.9 days. The estimated cumulative incidence of Zika virus infection for the study period was 12.9%.\n\n【19】Among the parameters used in this estimation, mean duration of Zika virus viremia is most influential because it is inversely related to the overall estimate of the number of persons infected with Zika virus in Puerto Rico. To our knowledge, the mean duration of viremia in serum is still unknown but has been shown to range from 4–10 weeks in gravid women  to 3–18 days in asymptomatic, nonpregnant persons . We used the value of 9.9 days (95% CI 6.8–21.6 days) on the basis of a literature review of 25 cases that provided doubly interval-censored data . The wide 95% CI for the mean viremia duration estimates reflected the current paucity of data on viremia duration. To evaluate the influence of this key parameter in our analyses, we included a sensitivity analysis by varying the assumed mean viremia duration and computing corresponding incidence estimates of Zika virus infection.\n\n【20】Using the mean viremia duration of 9.9 days gave a substantially higher total number of incident Zika virus infections than the number of new laboratory-confirmed infections reported from Puerto Rico to ArboNET during the same period (≈10,000 infections) . However, because of limitations in general population testing, this system reflects only symptomatic persons and a subset of asymptomatic pregnant women. One advantage of using blood donor screening as a surveillance tool is that it can rapidly capture real-time, cumulative incidence data from a large, diverse convenience sample of the general population; this information might otherwise be unattainable during a public health emergency. As observed during previous outbreaks of arbovirus diseases (e.g. West Nile, dengue, chikungunya) in the continental United States and territories, blood donation screening conducted during outbreaks can identify persons who are acutely infected and asymptomatic, which can aid in active case surveillance and enable characterization of viral and immunologic dynamics of clinical illness . Detection of Zika virus–infected asymptomatic blood donors is important not only for preventing transfusion-transmitted infections but also because the infection can be sexually transmitted and might result in adverse birth outcomes, even among pregnant women who do not have signs or symptoms. As US blood centers implement updated FDA recommendations for universal Zika virus blood donation screening , the coupling of prompt communication of reactive blood donor screening results to public health authorities with appropriate prevention messages and other public health interventions will become increasingly important in helping to mitigate the spread of Zika virus.\n\n【21】The findings of this study are subject to several limitations. First, the number of persons residing in Puerto Rico (estimated at 3.4 million in 2016 by the Puerto Rico Department of Health) might differ from the 2014 US Census population estimate of 3.6 million in our model. Second, the demographic composition of blood donors, specifically sex and age, does not match that of the general population. Data from this study show that men represented >67% of blood donors. Furthermore, data from persons <16 years of age were unavailable because of blood donor age restrictions, so the estimates we give for the whole population include an extrapolation to this age group. Although the data do not indicate a substantial difference in Zika virus incidence by age, whether the lack of data from the 0–15-year age group substantially affected our population incidence estimates is unknown. Alternatively, with regard to sex and infectivity, few data are available to support a predisposition for Zika virus infection in men; nevertheless, the statistically significant study finding of a male-to-female ratio of infectivity of 1.41 among donors suggests the need for further exploration of any possible interplay between sex and the length of viremia from Zika virus infection or Zika virus susceptibility. Third, blood donors are subjected to a medical examination and questionnaire to ascertain signs and symptoms of illness, and potential donors who are feeling ill are excluded from donation. Consequently, blood donor screening data might underestimate infection incidence because of the exclusion of symptomatic persons. Because our model adjusted for the exclusion of these persons, this limitation should not affect our analysis; however, this factor is an important consideration when blood screening data are used as a surveillance tool. Last, the duration of Zika virus viremia is unknown, and assumptions made for this model were based on limited data. Important research priorities will be to determine viremia duration through longitudinal follow-up of infected blood donors and studies of acute infection in animal models, resulting in more precise calculation of viral kinetics.\n\n【22】In summary, the findings of this study suggest that a much larger proportion of the population in Puerto Rico was infected with Zika virus during April–August 2016 than reported through surveillance. Although Puerto Rico mandates reporting of Zika virus infections, the conveyance of arboviral surveillance data across local, state, and national levels is often delayed and can affect strategic planning and interventions. Blood donation screening data can augment clinical Zika virus surveillance data to provide real-time communication of Zika virus incidence estimates to enable better ascertainment of the extent of outbreaks and improved targeting of prevention and response efforts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "654a0466-f60d-4ad5-ba30-11f571b15084", "title": "Using Death Certificate Reports to Find Severe Leptospirosis Cases, Brazil", "text": "【0】Using Death Certificate Reports to Find Severe Leptospirosis Cases, Brazil\nAlthough pulmonary involvement has been well recognized as a major component of leptospirosis , worldwide attention to severe pulmonary hemorrhagic syndrome (SPHS) emerged after an outbreak in rural Nicaragua in 1995 in which most cases lacked jaundice and renal failure, the classic manifestations of severe leptospirosis . A report from Peru, where leptospirosis is endemic, pointed out that SPHS can complicate human leptospirosis without jaundice or renal failure (a condition known as Weil syndrome) . Although not well recognized globally in general clinical practice, SPHS is considered a major clinical problem in some leptospirosis-endemic regions, ranging from the Andaman and Nicobar Islands  to urban coastal Brazil .\n\n【1】Epidemic leptospirosis most commonly occurs after flooding in densely populated centers in developing countries, and it continues to be an important clinical problem in urban coastal Brazil. São Paulo is the most populous Brazilian state (population ≈40 million). The annual reported incidence of leptospirosis from 1969 through 1996 was 0.53–1.13 cases per 100,000 inhabitants . In the São Paulo metropolitan area (population 10.5 million), which accounts for 68% of all reported cases, the annual incidence of leptospirosis was 1.9–3.7 cases per 100,000 inhabitants in the period 1998–2004. During the same period, case-fatality rates varied from 11.0% to 18.0% . In the São Paulo urban setting, SPHS is a common feature of leptospirosis. A case series of São Paulo patients with leptospiral SPHS found that 23 (55%) of 42 patients died from this disease from 1994 through 1997 .\n\n【2】### The Study\n\n【3】In 2004, PRO-AIM (Program for Improvement of Death Cause Information in São Paulo, abbreviated from the Portuguese name) was initiated in the São Paulo metropolitan area to provide better reporting of causes of death. Leptospirosis is a major infectious disease in São Paulo, and we started active surveillance for cases of acute renal failure and jaundice, pulmonary hemorrhage, or a combination of both. Detailed questionnaires on clinical and epidemiologic features were completed, and necropsies were performed to improve identification of fatal leptospirosis cases in São Paulo.\n\n【4】The central component of this program is a death certificate system. Fatal cases of leptospirosis are a component of this registry as a major category. Death certificates with any clinical features suggestive of leptospirosis or isolated SPHS are referred to the Municipal Health Secretariat of São Paulo. The Secretariat analyzes and attempts to confirm such cases. Confirmation protocols include laboratory analysis, clinical and epidemiologic assessment, and verification by necropsy of case-patients who had pathologic features characteristic of leptospirosis in lungs, kidneys, and liver. If fatal leptospirosis is confirmed within 72 hours, public health authorities can be alerted. In a setting of high leptospirosis transmission, this step may forestall expansion of an epidemic. Furthermore, the Brazilian experience, as well as that of other countries , includes a high rate of misdiagnosis of leptospirosis and other clinically indistinguishable acute febrile illnesses such as dengue or scrub typhus fever. Thus, rapid identification and confirmation of fatal leptospirosis are important tools for surveillance, public health interventions, and alerting clinicians.\n\n【5】We describe our initial experience of actively seeking, identifying, and reporting fatal cases of leptospirosis. Using data from January 1, 2004, through August 31, 2006, we used a cross-sectional approach to analyze death certificate data from the São Paulo metropolitan area. Fatal cases were confirmed in the laboratory by serologic or immunohistochemical examination, supplemented by epidemiologic and clinicopathologic evidence. Pathologic criteria were an important adjunct to confirm cases because ≈30% of deaths of patients hospitalized for leptospirosis occur within 24 hours, which may preclude serologic diagnosis because of delayed antibody responses. Pathologic examination can identify constellations of known complications as well as unexpected features. In fulminant infection, histopathologic findings confirm involvement of typical targeted organs such as acute tubular necrosis or interstitial nephritis, acute loss of cohesion of hepatocytes, and pulmonary hemorrhage, which is important because, in leptospirosis-endemic regions, potential leptospirosis patients may have had previous infection and thus preexisting antileptospiral antibodies . We emphasize, however, that pathologic criteria are only confirmatory if clinical and epidemiologic criteria for infection are fulfilled. In the experience reported here, 5 cases (which were included in the laboratory confirmation group) were confirmed by immunohistochemical examination. Leptospiral antigen detection in postmortem samples is important for confirming the diagnosis  but is limited by tissue deterioration if there is a prolonged period between death and necropsy.\n\n【6】In 2004, 42 (15%) of 285 cases of reported leptospirosis cases were fatal; in 2005, 28 (11%) of 262 cases were fatal; and from January 1 through June 2006, 31 (19%) of 167 cases were fatal. The Table shows the distribution of fatal cases from January 2004 through August 2006. Of 101 fatal cases, 62 were confirmed by a combination of serologic and immunohistochemical testing; 15 cases were suggested on the basis of clinical, pathologic, and epidemiologic findings; and 24 were suspected on the basis of strong circumstantial clinical and epidemiologic evidence.\n\n【7】### Conclusions\n\n【8】Isolation of leptospires in culture from clinical specimens is the standard for diagnosis but faces logistical obstacles in real-world settings. In our study, only 6 blood samples were collected for culture, and they were uninterpretable because of contamination. Although not definitive for identifying infecting leptospiral serovars, microscopic agglutination test titers suggested that the most frequently reacting serogroups in São Paulo are Icterohemorrhagiae (72%) and Autumnalis (14%).\n\n【9】Of the 101 fatal cases, necropsies were performed for 42. Of these 42 necropsies, 27 cases were confirmed by a combination of necropsy and positive serologic test results. Fifteen fatal cases were suggested by a combination of necropsy findings plus clinical and epidemiologic evidence.\n\n【10】Of all fatal cases confirmed by necropsy, Weil syndrome with concomitant pulmonary hemorrhage was documented in 86% of cases in 2004, in 67% in 2005, and in 69% in 2006. Less common manifestations included Weil syndrome without pulmonary hemorrhage, and isolated pulmonary hemorrhage .\n\n【11】The frequency of clinical manifestations of severe pulmonary disease (with or without hemorrhages) was 76% among fatal cases and 26% among nonfatal cases. Importantly, 46% of all patients with severe pulmonary symptoms died, a finding that is consistent with the literature .\n\n【12】The system of active death notification we describe can be an important tool to evaluate the emerging complications of leptospirosis and the global extent of disease due to leptospirosis. Nonetheless, this, as well as any system of active death certification notification, has several limitations. A major problem is increasing the number of necropsies, which may be limited by logistics or cultural norms. Increasing the necropsy rate can be aided by prompt response through the death certificate notification system and involvement of public health authorities. Another problem is the lack of serologic and cultural confirmation. Molecular diagnosis would be ideal and could readily be introduced into this system.\n\n【13】The incidence of SPHS seems to have changed elsewhere, e.g. in Salvador, another urban area of Brazil, where the Copenhageni serovar predominates. Pulmonary hemorrhage seems to have newly emerged since the year 2000 . An active surveillance, death certificate–based reporting system represents an important tool that should provide further insights into the natural history and changing clinical manifestations of leptospirosis infection in diverse geographic regions in which the disease is endemic and epidemic. Systematic reporting focused on the most severe cases of leptospirosis has the potential for providing a data-driven basis for ministry-level policy development, institution of public health control measures, and quantitative assessment of the global severity of leptospirosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ff8fe814-0774-453e-a3c1-adf1ee9fb0e2", "title": "Increasing Prevalence of Borrelia burgdorferi sensu stricto–Infected Blacklegged Ticks in Tennessee Valley, Tennessee, USA", "text": "【0】Increasing Prevalence of Borrelia burgdorferi sensu stricto–Infected Blacklegged Ticks in Tennessee Valley, Tennessee, USA\nIn the United States, Lyme disease caused by tickborne bacterium _Borrelia burgdorferi_ sensu stricto occurs primarily in the Northeast and upper Midwest . In eastern Tennessee, which is considered nonendemic for Lyme disease, most of the human population resides in a low-elevation swath of the Tennessee Valley bordered to the west by the Cumberland Plateau and the east by the Great Smoky Mountains. The vector of Lyme disease, the blacklegged tick _Ixodes scapularis_ , was unreported in this area before 2006; in this year, uninfected adult ticks were collected from hunter-harvested deer in 8 Tennessee Valley counties  . This finding, plus uninfected _I. scapularis_ ticks detected in Knox County in 2013, were later incorporated into the national distribution map for _I. scapularis_ ticks .\n\n【1】During 2000–2014, human Lyme disease cases expanded southward along the eastern foothills of the Appalachian Mountains in nearby Virginia . In the winters of 2012 and 2013, _B. burgdorferi_ –infected adult _I. scapularis_ ticks were detected in Pulaski County, Virginia . This report of abundant infected _I. scapularis_ ticks only 100 km from the Tennessee border motivated us to investigate whether _Borrelia_ \\-infected ticks might now be present in the Tennessee Valley.\n\n【2】### The Study\n\n【3】In late 2017, we sampled host-seeking _I. scapularis_ ticks at 70 forested sites in 26 low-elevation counties in the upper Tennessee Valley . To find tick habitats (hardwood or conifer forests <800 m in elevation) accessible for sampling (i.e. trails through public forests or margins of public roads through private forests), we reviewed Google Earth  satellite imagery. We sampled each site once during the peak of adult _I. scapularis_ tick activity (late October–January). We recorded site elevation and geo-coordinates and collected host-seeking ticks using a standardized drag-cloth method; in brief, we dragged a 1-m 2  white corduroy cloth across leaf litter and checked every 10 paces for attached ticks. We dragged cloths 30–60 minutes per site and described tick tallies as number collected per hour to correct for variations in effort per site. We did not conduct drag-cloth collections during periods of rain, strong wind, low air temperatures (<8°C), or low relative humidity (<40%).\n\n【4】We placed ticks in 70% ethanol, identified species using a morphologic key , and tested ticks for _Borrelia_ spirochete infection by DNA extraction and quantitative multiplex real-time PCR using differential probes targeting the 16S rDNA of Lyme group _Borrelia_ and relapsing fever group _Borrelia_ . We then subjected a random subset of negative samples and samples positive by the 16S assay (maximum 6 samples/site) to PCR amplification of the 16S–23S rDNA intergenic spacer region  and Sanger sequencing for species-level identification.\n\n【5】No previous tick drag-cloth counts existed for the counties in our survey area, except for a 1,050-m transect of land in a forest in Anderson County, which we have drag-cloth sampled annually each December since 2012. To assess a trend in adult _I. scapularis_ tick abundance, we applied linear regression modeling to the tick tallies from that transect of land.\n\n【6】In late 2017, we collected 479 adult _I. scapularis_ ticks from 49 of 70 sites in the upper Tennessee Valley. Two adult _Amblyomma americanum_ ticks collected during the survey were excluded from analysis. We detected _I. scapularis_ ticks in all 26 counties surveyed, 23 of which met the criterion used by Eisen et al. for established _I. scapularis_ populations  . Site elevations were 210–730 m; the highest elevation at which _I. scapularis_ ticks were found was 570 m. The average number of adult ticks collected per hour during drag-cloth surveys was 8.8 (range 0–48). At the Anderson County site that had been drag-cloth sampled annually, a highly significant increasing trend in _I. scapularis_ ticks was evident (p = 0.003); the count in 2017 (24.8 ticks/hour) was 3.5× higher than that in 2012.\n\n【7】We tested all _I. scapularis_ ticks collected (N = 479) for _Borrelia_ spp. infection; 46 ticks (9.6%) from 7 sites in 4 counties (Anderson, Claiborne, Hamilton, and Union, panel B) tested positive for Lyme group _Borrelia_ by 16S rDNA PCR screening. We tested 26 samples for the intergenic spacer region by PCR; all were positive for this sequence and identified as _B. burgdorferi_ sensu stricto by sequencing. Most infected ticks came from 2 Union County sites, which had prevalences of 44% (14/32) and 78% (18/23). No ticks were found to be infected with _B. miyamotoi_ or other relapsing fever group borreliae.\n\n【8】### Conclusions\n\n【9】In eastern Tennessee, public awareness and concern about ticks focuses primarily on the abundant lone star ticks ( _Amblyomma americanum_ ) and American dog ticks ( _Dermacentor variabilis_ ) encountered during the spring and summer. Both species can spread pathogens , but neither are vectors of _B. burgdorferi_ spirochetes. Immature _I. scapularis_ ticks are similarly active in the summer, but in southern states, these ticks typically avoid host-seeking above leaf litter and are rarely seen on humans or drag-cloths . For this reason, assessment of _I. scapularis_ distribution in southern states is best achieved by acquiring adult life-stage ticks during cool season drag-cloth surveys (as reported here) or by collecting ticks from deer harvested in the fall. Inspection of hunter-harvested deer is efficient for the detection of low-density _I. scapularis_ ticks . Thus, our drag-cloth sampling for _I. scapularis_ ticks in 14 counties where none were found on deer a decade ago  suggests that tick abundance in these counties has increased. This suggestion is supported by a >3-fold increase in _I. scapularis_ tick counts at the Anderson County site where we have 6 consecutive years of drag-cloth counts.\n\n【10】This study documents emergence of _B. burgdorferi_ senso stricto in tick populations in eastern Tennessee. Infected ticks were predominantly found in high-prevalence hot spots in Union County (36.39°N). Relative to Lyme disease–endemic areas in the north, _B. burgdorferi_ prevalence in the study area was low (10%) and had a patchy distribution (7/49 sites had positive ticks). This distribution could reflect host barriers of _B. burgdorferi_ transmission in the South , or more concerning, the hot spots in Union County might reflect the beginning of an infection surge, similar to that seen in southwestern Virginia during the past decade .\n\n【11】In the United States, Lyme disease is primarily a summertime disease associated with bites from nymphal _I. scapularis_ ticks. In southern states, detection of _B. burgdorferi_ bacteria in adult ticks does not necessarily imply risk to humans; for example, _B. burgdorferi_ cycles in _I. scapularis_ populations on the Outer Banks of North Carolina, yet nymphs in that area cannot be collected on drag-cloths and no locally acquired cases of Lyme disease have been reported . In contrast, infected nymphs have been found on drag-cloths from surveys in Virginia, where Lyme disease incidence has spiked . We speculate that _Borrelia_ \\-infected _I. scapularis_ populations emerging in southwestern Virginia include immigrant ticks from the North, with some nymphs in these populations exhibiting host-seeking behaviors that lead to contact with humans. A similar invasion process might be under way in eastern Tennessee; the surveillance data reported here provide a baseline for investigating this possibility. Health officials and practitioners need to be vigilant for increasing Lyme disease incidence in Tennessee.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ca677d4e-9d08-4e3a-bded-408963f2caf0", "title": "Summer dreams", "text": "【0】Monsoon rains\n\n【1】Have come and gone\n\n【2】Quintessential\n\n【3】Remnants of summer\n\n【4】Rambutans and Durians\n\n【5】Discarded peels\n\n【6】Bustles with activity\n\n【7】Lives of other kinds\n\n【8】Wriggling and swimming\n\n【9】Summer dreams\n\n【10】Of warm humid nights\n\n【11】Fanning away the creatures of nature\n\n【12】That annoying bustle of wings\n\n【13】Of lives that emerged from the discards\n\n【14】Creatures of nature\n\n【15】That fly in search of meals\n\n【16】Meals of red\n\n【17】Within the meal\n\n【18】Enveloped\n\n【19】In a protective sheath\n\n【20】Lies a being\n\n【21】Is it alive or is it not?\n\n【22】We argue in the summer’s heat\n\n【23】How to define life?\n\n【24】This being is a villain\n\n【25】A villain of a kind\n\n【26】Minute yet mighty\n\n【27】Astute and elusive\n\n【28】Adapting and evolving", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c94dd58e-d4f1-44ac-b6de-0787fa251546", "title": "Impact of the Arthritis Foundation’s Walk With Ease Program on Arthritis Symptoms in African Americans", "text": "【0】Impact of the Arthritis Foundation’s Walk With Ease Program on Arthritis Symptoms in African Americans\nAbstract\n--------\n\n【1】**Introduction**  \nInadequate program design and lack of access to evidence-based programs are major barriers to the management of chronic diseases such as arthritis, particularly for African Americans. This study evaluates the effectiveness of the Arthritis Foundation’s Walk With Ease Program (WWE) in a subsample of African Americans who were part of a larger study that established evidence of the program’s efficacy.\n\n【2】**Methods**  \nParticipants were African Americans (N = 117) with self-reported arthritis who chose to participate in either a self-directed (n = 68) or group (n = 49) 6-week WWE program. Arthritis-related symptoms (ie, pain, fatigue, stiffness; measured using visual analog scales) were assessed at baseline, 6 weeks, and 1 year. Independent samples _t_ tests were conducted to examine group differences (ie, self-directed vs group) in arthritis-related symptoms at baseline, and paired sample _t_ tests were conducted to examine differences over time (ie, baseline to 6 weeks and baseline to 1 year) in symptoms. Satisfaction was examined by descriptive statistics.\n\n【3】**Results**  \nYounger, more educated individuals chose the self-directed format ( _P_ < .001, _P_ \\= .008; respectively). After the 6-week intervention, participants reported a decrease in pain ( _P_ < .001), fatigue ( _P_ \\= .002), and stiffness ( _P_ < .001). At 1 year, the decrease in pain ( _P_ \\= .04) and stiffness ( _P_ \\= .002) remained constant. Overall, participants were satisfied with both program formats.\n\n【4】**Conclusion**  \nThe individualized and group formats of the WWE program improved arthritis-related pain, fatigue, and stiffness in African Americans. Culturally appealing arthritis interventions ultimately may increase the use of existing arthritis interventions.\n\n【5】Introduction\n------------\n\n【6】Arthritis, the leading cause of disability in the United States, affects almost 50 million Americans  and continues to be a growing public health concern. Arthritis, like most chronic conditions, disproportionately affects African Americans . Individuals with arthritis often experience pain, fatigue, stiffness, and activity limitations resulting in poor quality of health. African Americans experience these symptoms at a higher rate and with greater severity than whites . Therefore, identifying ways to minimize the negative impact of arthritis among African Americans is critical.\n\n【7】Evidence-based arthritis interventions (eg, Arthritis-Self Management Program \\[ASMP\\], Arthritis Foundation Aquatics Program \\[AFAP\\], Walk With Ease \\[WWE\\]) are effective in reducing arthritis-related symptoms such as pain, fatigue, and stiffness and are a useful treatment option for people with arthritis . However, most research on intervention programs for people with arthritis has been conducted predominantly among white populations with limited focus on African Americans . Few arthritis intervention studies that include African Americans examined preferences , effectiveness , cultural relevance, acceptability , satisfaction, and uptake specifically among this target population . Because of the paucity of research in this area, this study was undertaken to further the research by examining the effectiveness of the Arthritis Foundation’s Walk With Ease program among African Americans.\n\n【8】Specifically, our study is focused on a subset of the parent study population that was used to establish evidence of effectiveness for WWE in general . To date, no studies have focused on African Americans in this particular intervention program. Currently, WWE is delivered in 2 formats (self-directed or instructor-led group) . The primary objectives of this study were to 1) examine the effectiveness of the WWE program on improving the most common arthritis symptoms (pain, fatigue, and stiffness) in African Americans irrespective of delivery format, and 2) assess the target audience’s acceptability of and satisfaction with both the self-directed and instructor-led group WWE program. Although data from the parent study preclude the ability to examine effectiveness of WWE by delivery format in this population, our secondary objective is to examine differences in intervention delivery format selection across demographic variables and arthritis symptoms.\n\n【9】Methods\n-------\n\n【10】The Arthritis Foundation’s WWE program, currently offered in 2 formats (ie, self-directed or instructor-led group), is a community-based 6-week program for adults with arthritis. The program focuses on setting goals, developing action plans, identifying motivational strategies, and building one’s confidence to increase physical activity as a way to minimize arthritis-related symptoms. Participants in the self-directed program are provided with the WWE workbook as a guide to navigate the 6-week program on their own. Participants in the instructor-led group program attend a class that meets 3 times a week for 1 hour, taught by lay leaders who have received a certification after attending a 1-day Arthritis Foundation Leader Training course.\n\n【11】Participants in the parent study included 462 community-dwelling adults recruited from senior centers, aging councils, public health departments, medical centers, rheumatology clinics, fitness/wellness centers, retirement communities, colleges and universities, churches, a service sorority, recreation centers, and various employers across urban and rural North Carolina counties . Eligible participants included individuals aged 18 years or older, self-reporting a diagnosis of arthritis, with no serious medical conditions (eg, uncontrolled hypertension or diabetes) or cognitive decline, and able to speak English. Participants chose either the self-directed or group program. Baseline assessments were collected starting in June 2008. Performance and self-reported outcomes were assessed at baseline and 6 weeks, and self-reported outcomes again at 1-year follow-up. The focus of this substudy is on the 117 participants who self-identified as African American and completed the self-reported assessments. The method used to select participants from the original WWE cohort for the purpose of this study is delineated . Additional details of the total sample have been reported elsewhere .\n\n【12】The parent study was initially performed at 33 community sites throughout North Carolina, 20 of which had participants who reported being African American. Sites included senior centers, churches, community/health wellness centers, employers, and departments or councils on aging. To protect the privacy of participants where only 1 or 2 participants signed up, we do not provide detailed information by site. At baseline, each participant who provided written informed consent completed a self-report survey (on paper or computer-based), and a series of performance-based tests administered by a trained research team member was included in the study. Six-week follow-up assessments were conducted at each of the community sites, where participants completed the same initial tests and survey and a written satisfaction survey comprising closed and open-ended questions about their experience. One year after completion, participants were mailed surveys assessing their self-reported outcomes only, to evaluate long-term effects of the program. For the purposes of this study, we focus on self-report survey data only. All procedures for the parent study were approved by the University of North Carolina, Chapel Hill Biomedical Institutional Review Board, and by the Institutional Review Board of George Washington University for the substudy.\n\n【13】### Measures\n\n【14】**Demographics** . Demographic information included race, age, education, sex, body mass index (BMI), health status, and comorbid conditions. Race was assessed by asking participants, “What is your race/ethnicity?” Age was measured as a continuous variable by using participant self-reported date of birth and later stratified into 3 categories (<60 y, 60–74 y, ≥75 y). Education was assessed on the basis of the response to “What is the highest degree or level of school you have completed?” Responses were later dichotomized (having a high school education or less and having more than a high school education). BMI (kg/m 2  ) was a continuous measure that was calculated by using self-reported height and weight and later dichotomized as less than 30 kg/m 2  or 30 kg/m 2  or more. Health status was assessed by asking participants to rate their health in general as excellent, good, fair, or poor . Participants were asked to report to each condition they had from a list of 16 common chronic conditions. Comorbid conditions represent the sum of all self-reported conditions not related to arthritis. To account for factors that may influence differences, demographic variables were included as covariates . All baseline and 6-week data were collected at the Assessment Center, a component of the National Institutes of Health Patient-Reported Outcomes Measurement Information System (PROMIS) initiative .\n\n【15】**Arthritis symptoms.** Pain, fatigue, and stiffness were measured by using visual analog scales (VAS) . VAS is a validated tool used to measure self-reported pain in participants and shows significant, reproducible findings across various longitudinal studies . Participants were asked to indicate a spot on a 100-mm line corresponding to their pain experience during the preceding 7 days. For each arthritis-related symptom (ie, pain, fatigue, and stiffness), the VAS is anchored with descriptors “no (symptom)” and “(symptom) as bad as it could be or is a major problem.” The VAS was then measured in millimeters from the left anchor to the point marked by the participant. Higher VAS scores  indicate more pain, fatigue, or stiffness. With pain, fatigue, and stiffness serving as the most common arthritis-related symptoms, these VAS variables were chosen to account for effectiveness of this substudy.\n\n【16】**Satisfaction.** Participants in both self-directed and group delivery formats were surveyed about their overall satisfaction with WWE. The survey assessed satisfaction and acceptability of information, tools, workbook, program presentation, value, or benefit. The survey questions were formatted differently depending on the type of program (ie, self-directed or group); however, answers were standardized for comparison of overall satisfaction. For example, in the group format, participants were asked: “Overall, to what extent are you satisfied with this program? (1 = Very well, 2 = Fairly well, 3 = A little, 4 = Not at all).” In the self-directed format, participants were asked, “Overall opinion of WWE program: I was satisfied with my experience doing the Walk With Ease program (1 = Strongly disagree, 2 = Disagree, 3 = Agree, 4 = Strongly agree).” Although the self-directed and group satisfaction survey results cannot be quantitatively compared, they are described in this study.\n\n【17】### Statistical analysis\n\n【18】To gauge the sensitivity of the statistical analyses used in this study, a power analysis was conducted on one of the main variables, pain, which was measured using the VAS. We found a minimal detectable difference (MDD) of 6.9 units with 80% power (n = 117; α = 0.05, standard deviation \\[SD\\], 26.4). This difference approaches the recommended VAS pain MDD range of 7 to 37 units and detects smaller differences as baseline pain scores decrease .\n\n【19】All analyses were conducted using SAS version 9.2. Descriptive statistics for demographic and outcome variables (ie, pain, fatigue, stiffness) were calculated separately for both the self-directed and group walking formats to provide a summary of the sample distribution at baseline. Differences in delivery formats were examined by using _t_ tests and χ 2  tests. Pearson correlations were conducted to assess if any of the demographic factors served as covariates when comparing the 2 program delivery formats.\n\n【20】We assessed the effectiveness of the intervention by comparing the arthritis symptom outcome scores (ie, pain, fatigue, and stiffness) at 6 weeks and at 1 year (independently) with baseline scores using a paired _t_ test. Following the standard set forth in the parent study , effect sizes were calculated by using Cohen’s _d_  to determine if the change over time is considered meaningful. A change score resulting in an effect size of 0.1 to 0.3 indicates a modest effect, and a change score resulting in an effect size of 0.3 to 0.5 indicates a moderate effect . Analyses were restricted to the 98 African American participants who completed follow-up at 6 weeks (84%) and at 1-year follow-up (72%). However, no significant differences were found in demographics between people who dropped out of the program and people who continued the study. Descriptive statistics are presented to summarize the satisfaction of both the self-directed and group WWE program at 6 weeks and 1 year.\n\n【21】Results\n-------\n\n【22】Of the 117 African American participants enrolled in the parent study at baseline, 68 were enrolled in the self-directed format and 49 were enrolled in the group format. Participants’ average age was 62; approximately 33% had a high school diploma or less. Most participants were female and reported being in excellent or very good health. More than 50% were classified as obese .\n\n【23】Significant differences were found in the age and education level of people selecting the self-directed versus the group format . Individuals who selected the self-directed format were significantly younger and more educated than those who chose the group format ( _P_ < .001; _P_ < .008, respectively). No significant differences were found in average number of comorbidities. However, a large number of participants in each format reported having high blood pressure (n = 70, 75%) and diabetes (n = 23, 25%) at baseline (data not shown). No significant group differences were found for BMI, health status, pain, fatigue, or stiffness.\n\n【24】Arthritis symptoms at baseline and 6-week follow-up were assessed to determine if there was improvement, irrespective of delivery format. A significant difference was found for all 3 arthritis symptoms, with the most significant changes occurring in pain and stiffness (effect size \\[ES\\] = 0.46 and 0.43, respectively) . Participants reported less pain (ES = 0.31, _P_ \\= .04) and stiffness (ES = 0.42, _P_ \\= .002) from baseline to 1 year . We found no significant difference for fatigue, despite a small difference in means of 4.7.\n\n【25】Of the 71% of African American participants (n = 48) who completed the satisfaction survey after participating in the self-directed format at 6-week follow-up, 92% (n = 44) agreed or strongly agreed that they were satisfied with the overall WWE program (data not shown). In surveying the group format participants at 1-year follow-up, data were collected anonymously; therefore, satisfaction specifically for African Americans could not be determined. However, overall, participants enrolled in the group format were indeed satisfied. Of the 109 group participants who completed follow-up surveys in the original study, 100% reported that they would recommend the WWE program to a friend. Nearly all of the group format participants also reported that their instructors kept them interested in the program at a fairly or very well level (99%), that they were satisfied with the way the instructor presented the topics (100%), and that the program fulfilled their expectations (99%).\n\n【26】Discussion\n----------\n\n【27】This study evaluated the effectiveness of WWE in African Americans with arthritis, who are disadvantaged in the impact and prevalence of the most common symptoms associated with arthritis . Results indicate that both pain and stiffness improved, with stiffness improving significantly at both 6-weeks and 1-year. Moreover, fatigue was significantly reduced at 6 weeks. However, the difference in fatigue from baseline to 1 year was not significant and fell below our indicated range of a meaningful change. Although findings were similar, the effect sizes found in our study examining only African Americans were slightly higher than those presented in the parent study, suggesting a greater impact. Findings in the parent study were presented by program delivery format whereas our findings are irrespective of delivery format. Because knowledge about the effectiveness of arthritis behavioral interventions for African Americans is limited , our results are promising. They contribute to the public health literature by providing a foundation for future studies focused on designing and examining the effectiveness of culturally appealing interventions that could aid in mitigating existing arthritis-related disparities.\n\n【28】WWE not only reduced arthritis-related symptoms but was also perceived as acceptable by African American participants. Specifically, most participants in the self-directed program reported being satisfied and having completed the program with lessons learned. Because differences in demographics were negligible by program format, it is plausible that African Americans’ overall satisfaction with the self-directed format is similar to their overall satisfaction with the group format. Previous research has found that African Americans with arthritis respond favorably to programs beneficial for their condition that could be delivered to them independently or in a group setting . Therefore, offering a program with optional delivery formats may be a way to engage an African American population that has consistently been underrepresented in interventions of this type. However, having the option of an independent versus group program may not affect acceptability, satisfaction, and participation among this underrepresented population. Other structure and delivery factors (eg, encouraging participation of others similar in race, sex, and age, including instructor of same race) may act as interconnecting facilitators or barriers to participation in arthritis behavioral interventions, influencing acceptability and satisfaction. Therefore, future research should consider additional delivery and structure components that may improve the appeal, cultural relevance, acceptability, and satisfaction of behavioral interventions among this underrepresented population.\n\n【29】Findings from this study have implications specifically for African Americans. Having a physical activity program that minimizes disease-related symptoms and is deemed acceptable among this target population provides a foundation for future research. Not only do African Americans have worse arthritis outcomes than other races/ethnicities , they are also overrepresented in being diagnosed with multiple chronic conditions  and underrepresented in meeting physical activity recommendations . Reducing disparities in chronic disease, specifically in arthritis, and in physical activity are national efforts included in Healthy People 2020 . Walking reduces the risk of various chronic conditions and minimizes symptoms . Increasing African Americans’ participation in physical activity programs that have multiple health benefits could decrease the chronic disease health disparities. Future research should further examine the effectiveness of programs like WWE and identify innovative ways to increase their public health impact.\n\n【30】Our study has limitations. Although our study was adequately powered to detect the recommended MDD for our primary outcomes (pain, stiffness, and fatigue), the small sample size prevented additional analyses that could determine differences in symptoms according to delivery format. However, the findings still provide novel information about the effectiveness of WWE among African Americans. Using 2 different surveys and following 2 different methodologies may have limited our ability to determine any differences in satisfaction according to format. Because we used participant self-report to measure arthritis symptoms, responses may have been biased due to social desirability. However, the scale used for this study has been used in similar research as a valid measure for pain, fatigue, and stiffness. In addition, future research examining the effectiveness of a behavioral arthritis intervention should include a control group to provide stronger evidence that the effects found are due to participation in the intervention. Notably, this sample of African Americans was highly educated, predominantly female, and reportedly in good health. Demographics of this type limit the external validity of our findings. Lastly, a mixed-method (quantitative and qualitative) design may have been a stronger methodological approach for assessing satisfaction and acceptability.\n\n【31】Despite these limitations, our study provides practical lessons. For example, we used targeted strategies (eg, recruiting from senior centers in neighborhoods with a higher percentage of African Americans) in an effort to overcome barriers to recruiting and retaining African Americans. Recruitment efforts resulted in 25% of the WWE parent study population being African American ; this percentage exceeds the national average (14.2% identify as African American) . More than 70% of the participants remained in the study over time. Relationships between obesity, other chronic conditions, and behavioral walking interventions should be examined through further research.\n\n【32】From a public health perspective, a community-based walking program for African Americans with arthritis has the potential to increase the currently limited culturally sensitive self-management opportunities now offered . Our findings provide the foundation for beginning to address arthritis-related health disparities through culturally sensitive, acceptable effective behavioral interventions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5799d88a-eab6-4f3e-9910-fc4c97b4e4db", "title": "Invasive Group A Streptococcal Disease in Nursing Homes, Minnesota, 1995–2006", "text": "【0】Invasive Group A Streptococcal Disease in Nursing Homes, Minnesota, 1995–2006\n_Streptococcus pyogenes,_ or group A _Streptococcus_ (GAS), is most commonly associated with noninvasive conditions such as pharyngitis and impetigo but can also cause severe invasive GAS infections such as necrotizing fasciitis and streptococcal toxic shock syndrome (STSS) . Risk factors for invasive GAS disease include advanced age, diabetes mellitus, cardiac disease, chronic obstructive pulmonary disease, cancer, immunocompromising conditions, and varicella . Most nursing home residents have at least one of these risk factors, which makes this population especially vulnerable to invasive GAS disease. An estimated 8,950 to 11,500 (3.5/100,000 population) invasive cases and 1,050 to 1,850 deaths occur in the United States annually . The incidence among persons \\> 65 years of age of 9.4/100,000 population is almost 3 times that of the general population .\n\n【1】Most of the literature about GAS disease in nursing homes has focused on acute outbreaks with little attention paid to sporadic disease in this setting . Factors contributing to these outbreaks include GAS-infected caregivers, inadequate infection control measures, resident-to-resident spread, and the presence of a chronically infected or persistently colonized resident . On the basis of our experience with GAS surveillance in Minnesota, we suspect that a lack of recognition of GAS disease occurrence within nursing homes may also be a contributing factor.\n\n【2】We describe the occurrence of invasive GAS disease among residents of nursing homes in Minnesota over 11 years, the challenges we encountered with surveillance, and our efforts to prevent and control the spread of disease in this setting. Our findings will be useful in the development of guidelines for the prevention and control of GAS disease in nursing homes.\n\n【3】### Methods\n\n【4】##### Surveillance\n\n【5】We began active, statewide, population- and laboratory-based surveillance for invasive GAS disease in April 1995 through Active Bacterial Core surveillance (ABCs), part of the Emerging Infections Program of the Centers for Disease Control and Prevention (CDC) . The population of Minnesota was 4.9 million in 2000. Invasive GAS disease is defined as GAS isolated from a normally sterile site such as blood or cerebrospinal fluid or from a wound when accompanied by STSS or necrotizing fasciitis . To ensure complete case capture, laboratories either submit computerized lists of all GAS-positive cultures from normally sterile sites at least monthly or are contacted twice monthly by Minnesota Department of Health (MDH) staff, and audits are completed routinely. Hospital infection control practitioners then complete standard report forms for cases of invasive disease, and all GAS isolates are sent to the MDH Public Health Laboratory. All GAS isolates undergo pulsed-field gel electrophoresis (PFGE) with _Sma_ 1 by methods described elsewhere, with the exception that an _Enterococcus_ isolate was used as the standard . PFGE patterns are evaluated both visually and with BioNumerics software (Applied Maths, Kortrijk, Belgium) by using the dice coefficient. For patterns to be considered indistinguishable, they must visually appear identical and the DNA patterns must differ by <1.5% with respect to molecular weight. Isolates are also sent to CDC for _emm_ typing .\n\n【6】Beginning in 1995, information about a case-patient’s residence was collected on the case report form, including street address, city, state, and ZIP code. In 1998, a question was added about whether the case-patient had been a resident of a nursing home, long-term care facility (LTCF), or other chronic care facility for at least 30 days before the date the culture was collected. Persons living in group homes, prisons, rehabilitation hospitals, or who were going to facilities for daily outpatient therapy were not included. Beginning in 2002, the name of the facility was collected. Addresses for case-patients \\> 55 years of age were also checked by a reverse address directory to see whether they corresponded with that of an LTCF. Information about size, location, and classification of LTCFs was collected from a directory of Minnesota licensed, certified, and registered healthcare facilities.\n\n【7】For the purposes of this study, only case-patients who could be confirmed as residents of nursing homes were included because accurate denominator data were only available for this group. In Minnesota, a nursing home is defined as a facility that provides nursing care to \\> 5 persons who are not in need of acute care facilities but require nursing supervision on an inpatient basis. Denominators for calculating incidence were derived from the 2000 US Census data as reported by the Minnesota State Demographic Center, which describe the population living in group quarters by age and type of quarters .\n\n【8】##### Cluster Identification\n\n【9】We defined a nursing home cluster as \\> 2 cases in residents of a nursing home in which isolates were nearly identical as determined by PFGE (PFGE patterns within a 3-band difference ) during a 12-month period. PFGE patterns were used for cluster identification because they were more discriminating than _emm_ types (e.g. several PFGE patterns were typically found to correspond to 1 _emm_ type, while PFGE patterns were not found to have multiple _emm_ types), and PFGE was readily available in our laboratory. We chose 12 months because we observed that invasive cases with indistinguishable patterns sometimes occurred many months apart within a facility. Beginning in 1995, case reports were reviewed regularly by address, facility name, and PFGE pattern to look for clusters.\n\n【10】EpiInfo version 6.0  was used for statistical analysis. χ 2  test was used to determine statistical significance of differences in proportions for discrete variables, and a _t_ test was used to determine whether the difference in means was significant for continuous variables.\n\n【11】##### Intervention\n\n【12】Since 1997, whenever a cluster was identified through ABCs, the nursing home was contacted by MDH and encouraged to conduct retrospective and enhanced prospective surveillance for invasive and noninvasive GAS infections. This surveillance included reviewing culture logs to identify noninvasive GAS infections and residents with chronic or recurrent infections and reviewing reported staff illnesses to identify a possible source of GAS. Clinical examples of possible GAS infections were provided so the facility could consider the possibility of earlier undiagnosed GAS disease and recognize new suspect cases. For prospective surveillance, the nursing home was encouraged to obtain cultures for any suspected infection and to ask laboratories to save GAS isolates for PFGE and _emm_ typing. A follow-up letter and packet of information about GAS and infection control measures were also sent to the nursing home. We offered assistance of further investigation but most often had no further contact with the facility. In 2004, after noting that we seldom observed another case at a facility after we contacted them regarding a cluster, we began to contact a facility any time we received a report of a single case.\n\n【13】In Minnesota, MDH staff cannot conduct an investigation in a nursing home without an invitation from the facility. Only 2 facilities with clusters requested our assistance. These investigations have been described in detail elsewhere , but in both instances cultures were collected from residents and staff (all residents and staff at facility G; all but 1 resident who refused and 3 staff members in the affected unit at facility B). Those with positive GAS cultures were treated with antimicrobial agents (clindamycin at facility G and penicillin and rifampin at facility B). Formal infection control educational sessions were provided for staff on the same day that cultures were collected.\n\n【14】### Results\n\n【15】##### Surveillance\n\n【16】From April 1995 through 2006, 1,858 cases of invasive GAS disease were reported among Minnesota residents; 642 (35%) were in persons \\> 65 years of age. One hundred seventy-five case-patients were identified as LTCF residents on their case report forms. Twenty-three of these case-patients resided in non–nursing home settings such as assisted living or group homes, and we were unable to determine the type of setting for 18 of those designated as LTCF residents. One hundred thirty-four (7%) of our case-patients were known to be nursing home residents. The number and percentage of all cases associated with nursing homes fluctuated over time; from 6 to 21 cases were identified among nursing home residents annually, representing 3%–12% of all case-patients each year . Seasonal variation of invasive GAS infections was noted among both the general population and nursing home residents , with peak incidence in the winter and spring and little disease noted in late summer and early fall.\n\n【17】The age of nursing home case-patients ranged from 36 to 100 years of age (median 84 years); 58% were women, 87% had positive blood cultures, 36% had bacteremia without another focus of infection, 32% had cellulitis, and 12% had pneumonia. The case-fatality ratio of all case-patients with invasive GAS was 12%. Among case-patients \\> 65 years of age, the case-fatality rate of nursing home resident case-patients (n = 121) was 35% compared with 18% of case-patients who were not nursing home residents (n = 521).\n\n【18】In 2000, 37,542 Minnesota residents \\> 65 years of age lived in nursing homes, and 556,724 lived in their own homes or other group quarters. During 2000, the incidence of invasive GAS infections among Minnesota nursing home residents \\> 65 years was 18.6 cases/100,000 population compared with 6.8 cases/100,000 among those \\> 65 years who did not reside in nursing homes. Estimated annual incidence for nursing home residents \\> 65 years varied from 13.3 cases/100,000 in 1997 to 50.6 cases/100,000 in 2003, while the estimated incidence for non–nursing home residents \\> 65 years was 8.6 and 9.3 cases/100,000 during the same years.\n\n【19】_Emm_ type was available for 117 (87%) of the nursing home case-patient isolates. Of 21 different _emm_ types identified, 4 ( _emm_ 1 \\[21%\\], _emm_ 89 \\[15%\\], _emm_ 28 \\[13%\\], and _emm_ 03 \\[11%\\]) accounted for 60% of the isolates. Among 1,416 (82%) non–nursing home case-patients, 5 _emm_ types accounted for 62% of the isolates ( _emm_ 1 \\[24%\\], _emm_ 28 \\[13%\\], _emm_ 03 \\[11%\\], _emm_ 12 \\[10%\\], and _emm_ 89 \\[4%\\]). Although total numbers of cases varied considerably from one year to the next, the proportion of disease caused by the most common _emm_ types fluctuated little.\n\n【20】##### Cluster Identification\n\n【21】Of the 444 licensed nursing homes in Minnesota, 91 (20%) were known to have at least 1 case of invasive GAS disease during the study period. Sixty-seven (74%) of these facilities had a single case; 13 facilities had 2 cases; and 11 facilities had \\> 3 cases. Of 24 facilities that had \\> 2 cases, 13 (54%) met the definition of a cluster as previously defined . We found that PFGE patterns for isolates from the same facilities were either indistinguishable from each other or distinctly different (>3 bands different).\n\n【22】Four nursing homes that had clusters also had additional cases that did not fit the definition for inclusion in the cluster, either because a case isolate had a distinctly different PFGE pattern, a case did not occur within 1 year of the other cases, or both. One of these facilities had 4 such cases. In addition, 11 other nursing homes each had 2–3 cases that did not fit the definition of a cluster.\n\n【23】All but 2 clusters were caused by the most common _emm_ types. _Emm_ 1 was the most common cause of invasive disease (causing 24% of all cases) and also the cause of 5 (38%) clusters.\n\n【24】Eighteen of 21 pairs (86%) of consecutive cases occurring within 12 months of each other in the same facility had matching PFGE patterns, while 13 (93%) of 14 pairs of consecutive cases occurring within 3 months had matching patterns. The occurrence of a third case in a nursing home was not dependent on the first 2 case isolates having the same PFGE patterns; 6 (46%) of 13 facilities in which the first 2 case isolates had different PFGE patterns and 5 (45%) of 11 facilities in which the first 2 case isolates had matching PFGE patterns subsequently had more cases.\n\n【25】No significant difference was found for age, sex, or type of infection between cluster and sporadic cases. Forty-one percent of case-patients with cluster-associated cases died, compared with 32% of patients with sporadic cases, but this difference was not significant (p = 0.33).\n\n【26】##### Intervention\n\n【27】In 32 (86%) of 37 encounters with nursing staff, the person contacted was not aware of a diagnosis of invasive GAS disease among their residents before our call. In addition, even when our contacts (usually either directors of nursing or nurses designated to oversee infection control for the facility) did know of the diagnosis, they generally had little knowledge about GAS disease. Before 2004, we noted that 9 of 12 nursing homes did not identify additional cases of invasive disease after our call. In 2004, we began notifying nursing homes after we identified single cases in their facilities. Since then, we are aware of only 1 facility with a cluster, and that facility had 2 cases 4 days apart.\n\n【28】We collected throat and skin lesion cultures from staff and residents for a unit with a cluster of invasive GAS disease at facility B and from staff and residents of the entire nursing home at facility G. At facility B, 2 (2.7%) of 75 throat cultures from staff and 2 (5.9%) of 34 throat cultures from residents were positive for GAS; 5 (6.2%) of 81 throat cultures from staff and 2 (4.5%) of 44 throat cultures of residents were positive for GAS at facility G. All of those with positive throat cultures were asymptomatic at the time of culture. All except 1 isolate from a staff person at facility G who did not provide direct patient care had PFGE patterns indistinguishable from those associated with the invasive cases at the facility. Those with positive cultures were each treated with a course of antimicrobial drugs, and no additional cases were detected at either facility.\n\n【29】### Discussion\n\n【30】Minnesota has had a unique opportunity to conduct active, population-based surveillance for invasive GAS disease for >11 consecutive years with nearly complete case reporting plus further molecular characterization of associated GAS isolates. Findings from this statewide surveillance, our review of the strengths and weaknesses of GAS surveillance specific for nursing homes, and further evaluation of factors associated with clusters of GAS in this setting provide information to aid in the development of effective national guidelines for the prevention and control of GAS infections in this vulnerable population.\n\n【31】Although <2% of Minnesota’s population resides in nursing homes, at least 7% of invasive GAS cases occurred among this population. As noted in other studies , we also found that the case-fatality rate was higher for nursing home residents than for the rest of the population. Much of this increase in illness is likely due to the frequency of risk factors for invasive GAS disease among this population (e.g. advanced age and underlying diseases such as diabetes and chronic obstructive pulmonary disease); however, the increase may also be due to difficulties of limiting the introduction and transmission of GAS in this or any institutional setting or in a closed population.\n\n【32】The true incidence of GAS disease in nursing homes and the occurrence of clusters are likely higher than detected by our surveillance system. Collection of specimens from febrile nursing home residents is limited when infections in nursing home residents are treated empirically. In addition, our early surveillance methods likely misclassified the residence of GAS case-patients. We found that the street addresses for patients that were obtained from hospital admission records were often not the addresses of the nursing homes where case-patients resided but were instead the home address of a spouse or other family member. The percentage of case-patients with invasive GAS disease identified as living in nursing homes rose markedly in 1998 when a specific question about LTCF residence was added to the ABCs case report form. In 2002, we began collecting the name of the facility where potential case-patients resided, enabling nursing home residence to be confirmed. Because of these improvements in methods over time, we cannot appropriately compare our early nursing home disease rates to those calculated from more recent data to draw conclusions about changes in trends.\n\n【33】Prevention and effective control of GAS infections in nursing home residents can be improved with changes in surveillance. Knowledge of the initial case in a facility may help prevent a second case through review and improvement of infection control in the facility, the identification of and treatment for a colonized or infected staff member, or segregation of infected patients. We found that nursing homes were frequently unaware that their hospitalized residents had invasive GAS disease until notified by public health officials. All GAS infections identified by referring hospitals must be reported back in a timely manner to the nursing homes from which a patient was transferred. Surveillance for noninvasive GAS infections may also be needed. Because these infections are not reportable, nursing home staff and public health personnel may not be aware of the first introduction of GAS into a facility or ongoing transmission when the onsets of invasive GAS cases are separated by long periods.\n\n【34】Given the current limitations of public health surveillance, nursing home staff, especially those responsible for infection control, must be educated specifically about GAS disease and its transmission. Hospital infection control practitioners may be in the best position to find cases and to inform nursing homes when they review culture results for hospital surveillance.\n\n【35】We found further characterization of GAS isolates helpful when confronted with multiple cases of GAS disease in a facility. Even if laboratory resources are scarce, nursing home isolates of GAS should be saved for future testing if additional cases occur. Both e _mm_ typing and PFGE are useful tools when attempting to determine whether \\> 2 cases are related. A high percentage of temporally related cases had isolates with indistinguishable PFGE patterns, which suggests that continued transmission of a single strain is occurring in a facility, although reintroduction of a similar strain from the community cannot be excluded. In half of the situations in which a nursing home had 2 invasive cases, additional cases occurred regardless of whether the GAS isolates from the first 2 case-patients had matching PFGE patterns, which suggests a failure of infection control in these facilities. Although knowledge of GAS strain relatedness identified through PFGE or _emm_ typing can help identify the source of the GAS infection, circulating within a facility or introduced from the community, we conclude that a thorough investigation is warranted when >1 case has occurred in a facility within a few months. _Emm_ typing is not readily available in most public health laboratories; however, the results of _emm_ typing of GAS isolates from ongoing ABCs is important for researchers currently developing multivalent GAS vaccines. The types most common among our nursing home case-patients are included in a 26-valent vaccine that has completed a phase II trial .\n\n【36】Although most invasive GAS disease cases occurring in nursing homes are sporadic, our experience suggests that the time of first awareness of any GAS disease in a nursing home is also the time to assess the extent of spread and institute infection control measures. Clinical syndromes of GAS should be reviewed with staff, and the importance of excluding staff and visitors with illness should be emphasized. Hand hygiene among staff, visitors, and residents needs to be emphasized. We also recommend that surveillance for GAS disease, including noninvasive disease, be implemented and that cultures be obtained from patients with potential cases. If ongoing transmission and disease continue, additional measures, such as performing screening cultures for GAS, can be helpful.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2cf712f4-82bc-4a77-be89-aa2a5316a8fe", "title": "Community Engagement of African Americans in the Era of COVID-19: Considerations, Challenges, Implications, and Recommendations for Public Health", "text": "【0】Community Engagement of African Americans in the Era of COVID-19: Considerations, Challenges, Implications, and Recommendations for Public Health\nAbstract\n--------\n\n【1】African Americans, compared with all other racial/ethnic groups, are more likely to contract coronavirus disease 2019 (COVID-19), be hospitalized for it, and die of the disease. Psychosocial, sociocultural, and environmental vulnerabilities, compounded by preexisting health conditions, exacerbate this health disparity. Interconnected historical, policy, clinical, and community factors explain and underpin community-based participatory research approaches to advance the art and science of community engagement among African Americans in the COVID-19 era. In this commentary, we detail the pandemic response strategies of the Morehouse School of Medicine Prevention Research Center. We discuss the implications of these complex factors and propose recommendations for addressing them that, adopted together, will result in community and data-informed mitigation strategies. These approaches will proactively prepare for the next pandemic and advance community leadership toward health equity.\n\n【2】Introduction\n------------\n\n【3】Racial/ethnic minority populations have historically borne a disproportionate burden of illness, hospitalization, and death during public health emergencies, including the 2009 H1N1 influenza pandemic and the Zika virus epidemic . This disproportionate burden is due to a higher level of social vulnerability — “individual and community characteristics that affect capacities to anticipate, confront, repair, and recover from the effects of a disaster” — among racial/ethnic minority populations than among non-Hispanic White populations . These characteristics include, but are not limited to, low socioeconomic status and power, predisposing racial/ethnic minority populations in general and African Americans in particular to less-than-optimal living conditions. Some racial/ethnic minority populations are more likely than non-Hispanic White populations to live in densely populated areas, overcrowded housing, and/or multigenerational homes; lack adequate plumbing and access to clean water; and/or have jobs that do not offer paid leave or the opportunity to work from home . These factors contribute to a person’s ability to comply with the mitigation mandates of the coronavirus disease 2019 (COVID-19) pandemic established to reduce risk for infection, such as physical distancing and sheltering in place .\n\n【4】The COVID-19 pandemic presents new challenges for public health evaluators, policy makers, and practitioners, yet it mirrors historical trends in health disparities and poor health outcomes among African Americans. African Americans are more likely to contract, be hospitalized, and die of COVID-19–related complications . Social vulnerability is often compounded by preexisting health conditions, exacerbated during times of crisis .\n\n【5】Public health leaders are now at a critical juncture to advance health equity among vulnerable African Americans. To advance this health equity, we must first have a comprehensive understanding of the factors that create health disparities and the factors that can contribute to an effective, multilevel response. With this understanding, we can then deploy effective mitigation strategies based on a community-based participatory research framework that fosters and sustains community leadership in the assessment and implementation of culturally appropriate and evidence-based interventions that enhance translation of research findings for community and policy change . The objective of this commentary is to 1) detail the interconnected historical, policy, clinical, community, and research challenges and considerations central to comprehensively advancing the art and science of community engagement among African Americans in the COVID-19 era; 2) describe The Morehouse School of Medicine Prevention Research Center (MSM PRC) pandemic response strategies, driven by community-based participatory research (CBPR); and 3) discuss community-centered implications and next steps for public health action.\n\n【6】Challenges and Considerations\n-----------------------------\n\n【7】### Historical context\n\n【8】Racial/ethnic health disparities have always existed in the United States. Differential health outcomes between African Americans and non-Hispanic White Americans have been part of the American landscape for more than 400 years . Many measures of health status have been used to assess differences among racial/ethnic groups; more recently, health researchers have advanced concepts and constructs of health equity and social determinants of health . Reaching back to the mid-20th century, the US government documented that African Americans were far more likely than non-Hispanic White Americans to have a wide range of potentially fatal illnesses, including noncommunicable diseases such as type 2 diabetes, asthma, end-stage renal disease, and cardiovascular disease . In 1985, the US Department of Health and Human Services published the landmark _Report of the Secretary’s Task Force on Black and Minority Health_ , better known as the Heckler report . The report documented an annual excess 60,000 deaths among African American and other racial/ethnic minority populations. These underlying determinants can only result in disproportionately adverse health outcomes for racial/ethnic minority populations during the COVID-19 pandemic.\n\n【9】The COVID-19 pandemic is intensified by the long-standing income inequality between non-Hispanic White people and racial/ethnic minority populations. Economists use the Gini coefficient to measure income inequality. Values for this measure range from 0 to 1, with higher values representing greater income inequality. From 1990 to 2018, the Gini coefficient in the United States rose from 0.43 to 0.49 — an increase in income inequality. When income disparities exist along with other disparities (eg, health insurance, employment, education, social justice, access to quality health care), public health pandemics marginalize racial/ethnic minority groups, and this marginalization requires a strong and strategic response .\n\n【10】### Policy landscape\n\n【11】Racial/ethnic minority populations are disproportionately affected by COVID-19 , as they are by many diseases. In the United States, African Americans, Hispanics/Latinos, Native Americans, Native Hawaiians, and Pacific Islanders are more likely than other racial/ethnic groups to die of COVID-19 . The pandemic has not affected all populations equally for several reasons, including social, behavioral, and environmental determinants of health. In addition, economic and social policies have not benefitted all populations equally. Obesity, asthma, depression, diabetes, heart disease, cancer, HIV/AIDS, and many other disorders that put vulnerable populations at greater risk of dying of COVID-19 can often be linked to a policy determinant . Air pollution; climate change; toxic waste sites; unclean water; lack of fresh fruits and vegetables; unsafe, unsecure, and unstable housing; poor-quality education; inaccessible transportation; lack of parks and other recreational areas; and other factors play a large role in overall health and well-being . These factors increase a person’s stress and limit opportunities for optimal health . Too often, public health researchers and practitioners stop at the social determinants of inequities. These social determinants do, indeed, play an outsized role in these human-made inequities, but underlying each one is a policy determinant that should be addressed to improve health equity.\n\n【12】Consider, for example, the problem of asthma among many racial/ethnic minority populations. One community, in East Harlem, one of Manhattan’s poorest neighborhoods, found that a bus depot caused the high rates of asthma among children who lived near it . Six of 7 bus depots in Manhattan are located in East Harlem, and East Harlem has the highest rate of asthma hospitalizations in the country . In another community, the exhaust and dust from the vehicles traveling a major highway that cut through the middle of the community was found to contribute to the high rates of asthma among residents who lived near it . In both of these examples, an underlying policy determined the placement of the bus depots and the highway, which led to the eventual health inequities.\n\n【13】Examples of how legislative and policy change can immediately affect the social determinants of health are demonstrated in government and public responses during the first 3 months of the COVID-19 pandemic in the United States. Federal, state, and local policies were implemented to stimulate local economies and infuse communities with free food and direct revenue, including increases in SNAP (Supplemental Nutrition Assistance Program) benefits and expanded unemployment benefits. These initiatives have helped communities and individuals during the crisis. Despite these programs, however, some marginalized African American communities have not benefitted. As the nation adjusts to the “new normal,” it is imperative that the social, economic, and health gaps in these communities also conform to a “new normal” that is driven by new or expanded _and_ sustained policies.\n\n【14】### Clinical mechanisms, chronic conditions, and increased risk of COVID-19\n\n【15】African Americans are twice as likely as non-Hispanic White Americans to die of heart disease and 50% more likely to have hypertension and/or diabetes . This elevated risk increases the likelihood of other complications and death from COVID-19 . Let us consider, for example, people living with diabetes. Their immune system is depressed overall, because their blood glucose is not well controlled (hyperglycemia) . It is hypothesized that hyperglycemia causes an increase in the number of a particular receptor in the lungs, pancreas, liver, and kidneys; this increase impairs the function of white blood cells, which are designed to fight off infections . This impairment predisposes the person living with diabetes to an increased risk of bacterial and viral infections. When severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) enters the lungs by way of this particular receptor, it overwhelms the alveoli (air sacs) in the lungs and disables the exchange of oxygen and carbon dioxide . As a result, some people with diabetes may need supplemental oxygen, intubation, and/or admission to an intensive care unit . Hyperglycemia in combination with a disease such as COVID-19 makes recovery difficult . People with diabetes who are in good mental health, know the names and dosages of their medications, and know their blood pressure, blood glucose, and other laboratory values, such as hemoglobin A 1c  , tend to have better control of their disease and have lower levels of illness and death . Emphasizing the importance of good blood glucose control to prevent diabetes complications and associated COVID-19 risk is more important now than ever . Mental health plays a major role in a person’s ability to maintain good physical health and optimally manage their chronic conditions, and mental illnesses may affect the ability to participate in health-promoting behaviors .\n\n【16】### Mental and behavioral health\n\n【17】The constellation of stressors triggered by the COVID-19 pandemic undermines the nation’s mental health . Various disruptions in daily life, coupled with the threat of contracting the deadly virus, is leading some people to experience anxiety and depression, sometimes to the extreme. Reports of family violence and use of suicide prevention hotlines have increased . Physical distancing, shelter-in-place orders, business and school closures, and widespread unemployment have radically changed ways of life and contributed to a sense of hopelessness, isolation, loneliness, helplessness, and loss . Pandemic-related factors, including quarantine, have led to posttraumatic stress disorder, confusion, and anger . One study indicated that a constant consumption of media reports had detrimental psychological effects on some people . If interrelated mental, behavioral, and emotional issues are not adequately addressed, disorders among racial/ethnic minority populations and other vulnerable populations (eg, the medically underserved, homeless, and disabled; inmates in the criminal justice system) will surge and exacerbate disparities .\n\n【18】Interrelated COVID-19–related stressors include childcare and safety, elder care, food insecurity, and interpersonal relationships . These stressors may trigger aspects of unresolved trauma. Poor coping mechanisms (eg, use of illicit drugs, excessive alcohol consumption, overeating, inadequate sleep) may develop or worsen. In addition to facing chronic stressors, communities of racial/ethnic minority populations often deal with the stigma associated with seeking mental and behavioral health care. A Surgeon General’s report, _Mental Health: Culture, Race, and Ethnicity_ , concluded that racial/ethnic minority populations, compared with the non-Hispanic White population, have less access to mental health care, are less likely to receive treatment, and when treated, often receive poorer quality of care . As a result, racial/ethnic minority populations often have a greater burden of behavioral disorder–related disability . Addressing the multifaceted mental and behavioral health needs of racial/ethnic minority populations in the United States is a complex issue that warrants attention from clinicians, researchers, scientists, public health professionals, and policy makers. It is imperative to recognize the significant role of community leaders in exploring solutions to COVID-19–related mental and behavioral health problems among racial/ethnic minority communities. Their lived experiences are central to the co-creation of pandemic response strategies for these populations.\n\n【19】### Perspectives of community leaders\n\n【20】The realities of research, evaluation, and clinically focused community engagement after the COVID-19 pandemic may change for the foreseeable future. Efforts to initiate and sustain culturally competent engagement of racial/ethnic minority groups previously relied on face-to-face interactions in homes, churches, and other community settings. Social or physical distancing has nearly stopped communities and their collaborators from real-time gathering. These changes challenge the human need for connection and in-person exchange. Although the adjustment has been difficult, the pandemic has resulted in new modes of engagement. Webinar and digital technology are now accessible for most people at low or no cost. Many community residents have newfound capacities to use technology for social and professional interactions as part of daily life.\n\n【21】Current health communication and messaging require community-informed improvements. The use of terms like _sheltering in place, social distancing_ , and _flattening the curve_ do not naturally resonate with many people. For some, these terms foster anxiety and distrust of systems perceived to separate communities rather than promote COVID-19 mitigation strategies. Community leaders, as well as business and faith leaders, have found themselves in a space of terminology and descriptions that are understood mostly by public health practitioners. Therefore, health literacy and the interpretation of current health conditions are vital.\n\n【22】The pandemic has intensified the economic strains among low-income and moderate-income people and families . Low-wage workers, many on the frontlines of the pandemic since it began, have had little to no increase in income . African American families who struggled to make ends meet before COVID-19 are now facing dire economic circumstances in making the best decisions for their families. Stressors include, but are not limited to, deciding how to pay rent or a mortgage, paying for food, assisting children with virtual learning, and protecting themselves with minimal or no health care benefits. The mental and behavioral health implications of these problems, along with the economic and practical challenges, have made a fragile ecosystem even more unstable. Low-wage workers in hospitality, food service, and retail industries cannot work from home. Workers who depend on employer-provided health insurance now have the additional burden of how to maintain health insurance coverage . Ultimately, lack of adequate access to health care, along with the complex realities of the COVID-19 pandemic, will increase health disparities for socially vulnerable African American employees and their families.\n\n【23】### Local examples of COVID-19 response strategies driven by community-based participatory research\n\n【24】The MSM PRC relies on a deeply rooted, community partnership model that responds to the health priorities of vulnerable African American residents before, during, and after public health emergencies such as the COVID-19 pandemic. For more than 20 years, the MSM PRC has applied dynamic CBPR approaches that focus on prevention, establish partnerships between communities and research entities, and are culturally tailored .\n\n【25】The MSM PRC capitalizes on community wisdom through a community coalition board (CCB) that has governed the center since its inception. The CCB is composed of 3 types of members: neighborhood residents (always in the majority), academic institutions, and social service providers . Neighborhood residents hold the preponderance of power, and all leadership seats and are at the forefront of all implemented approaches. Neighborhood resident members are intentionally recruited from census tracts with a high incidence and prevalence of chronic and infectious diseases. The communities served by the MSM PRC are majority (87%) African American, have an average household income of $23,616, and rank lowest among other local communities in other socioeconomic conditions and community neighborhood health factors .\n\n【26】The MSM PRC has strategically partnered with the CCB and the community to facilitate health research and related interventions based on a comprehensive understanding of historical, political, clinical, and community considerations. The community governance model was developed to address CBPR challenges that exist when academics are not guided by neighborhood leaders in understanding a community’s ecology, when community members do not lead discussions about their health priorities, and when academics and neighborhood leaders do not work together as a single body with established rules to guide roles and operations .\n\n【27】The MSM PRC conducts a recurring (every 4 years) community health needs and assets assessment (CHNA 2  ) process through the CCB, empowering community members to take on roles as citizen scientists who develop locally relevant research questions and identify priority health strategies . The recently completed CHNA 2  (February 2018) was co-led by neighborhood residents to advance a community health agenda. Survey development, data analyses, and response strategies are reviewed, monitored, and evaluated by the CCB and its Data Monitoring and Evaluation Committee . This 7-member committee, established in 2011, is designed to extend the CBPR engagement of CCB members in the work of the MSM PRC. It exists through academic–community co-leadership (a CCB neighborhood resident member and the MSM PRC assistant director of evaluation) of a group of CCB members tasked with leading assessments. For CHNA 2  , members met bimonthly (every other month, when the CCB did not meet) to discuss and inform evaluation and data collection activities and prepare for reporting of evaluation findings and interim results to the broader CCB to determine corresponding respond strategies. CHNA 2  primary data included surveys administered to 607 community residents. The most frequently cited community health concerns were diabetes, nutrition, high blood pressure, overweight/obesity, and mental health. County-level, top-ranking causes of illness and death, including cardiovascular disease, diabetes, and mental health disorders, align with these community perspectives .\n\n【28】CHNA 2  is relevant, despite being administered before the outbreak of COVID-19. The chronic conditions and health problems identified are those exacerbated by COVID-19 (diabetes, cardiovascular disease, and mental health), thereby making their focus even more relevant to the community.\n\n【29】The mental and behavioral health components of CHNA 2  were amplified to address the stress and anxiety caused by the pandemic. First, during National Mental Health Awareness Month (May 2020), the MSM PRC convened a virtual forum, _Our Mental and Behavioral Health Matter_ s. It was strategically designed to address the culturally bound mental health stigma in racial/ethnic minority communities that is due, in part, to the schism between religion and therapy. The forum also addressed challenges related to social isolation. Concerns centered on how to navigate a virtual mental health checkup and support for parents seeking to help their children process the realities of the pandemic and minimize childhood trauma. Featuring psychologists, researchers, and community- and faith-based pioneers, the forum engaged more than 230 local and national participants. Second, a CCB member representing Fulton County’s Department of Behavioral Health and Developmental Disabilities helped the MSM PRC to develop and disseminate an infographic on mental and behavioral health services for insured and uninsured residents. Third, the MSM PRC will offer annual Mental Health First Aid  trainings to community residents and professionals over the next 4 years.\n\n【30】The MSM PRC leads the Georgia Clinical and Translational Science Alliance’s Community Engagement Program, which is designed to advance community-engaged clinical and translational research . The Program is led by a community steering board adapted from the CCB model and includes co-leaders (faculty and staff, including a community health worker) from Emory University, the Georgia Institute of Technology, and the University of Georgia. The program conducted a webinar, _Community Engagement in the Era of COVID — Opportunities, Challenges and Lessons Being Learned_ , in May 2020. The webinar addressed the challenges and opportunities associated with initiating or sustaining community-engaged research during physical-distancing and shelter-in-place mandates. Clinicians, scientists, and community leaders from Atlanta, Athens, and Albany, Georgia, discussed uniquely nuanced issues for urban and rural community engagement and the basic need for social connectedness through virtual navigation of community engagement strategies (eg, via Zoom) and newly expanded access to telehealth medical visits . The webinar emphasized the importance of being a credible source of COVID-19 information and linkage across social and economic services, given heightened community anxiety and preexisting mistrust of medical research.\n\n【31】The MSM PRC is a central collaborator in a national initiative led by the National Center for Primary Care at Morehouse School of Medicine and the Satcher Health Leadership Institute, also at Morehouse School of Medicine. The National COVID-19 Resiliency Network is designed to mitigate COVID-19 in racial/ethnic minority, rural, and socially vulnerable communities. The initiative will work with community organizations to deliver education and information on resources to help fight the pandemic. The information network will strengthen efforts to link communities to COVID-19 testing, health care services, and social services through the institution’s leadership in policy, community engagement, and primary care. The MSM PRC’s CCB model will be scaled to collaborate with community organizations in highly affected geographic areas to assess and inventory community assets for COVID-19 testing, vaccination, and other health care and social services through a national community coalition board. The MSM PRC CHNA 2  model will also be scaled to inform mitigation approaches implemented by community-based organizations through establishment of a centralized inventory of culturally appropriate COVID-19 response strategies, by geography and population vulnerability. Approaches will engage community health workers, who are mission-critical stakeholders, nationally galvanized, and locally deployed.\n\n【32】These MSM PRC activities are founded on long-standing, community-partnered, and informed relationships in response to preexisting health priorities that are simply heightened by the COVID-19 pandemic. Ideally, this CBPR framework is established before a public health crisis. This framework and the practice of identifying community needs and mobilizing strengths are now poised, adapted, and scaled up in response to the COVID-19 pandemic. The continued evolution of the pandemic means that these approaches and solutions must be flexible in response to changing needs and new data.\n\n【33】Implications for Public Health\n------------------------------\n\n【34】Public health practitioners, evaluators, policy makers, researchers, and clinicians with a community-engaged mindset have long understood, grappled with, and proclaimed the complexities of health disparities in the context of historic and current social determinants . When considered together, the challenges and realities detailed in this commentary create opportunities for new approaches to intentionally engage socially vulnerable African Americans. The response strategies proposed below reflect the complex web of historical and current policy and clinical, mental and behavioral, and community factors. Use of a CBPR framework undergirds all response strategies proposed.\n\n【35】**Promote local community leadership to proactively inform mitigation strategies.** The importance of CBPR and related needs assessments and response strategies are heightened during the COVID-19 era. Health promotion for chronic conditions such as diabetes, obesity, and cardiovascular diseases may have previously been structured to result in poor health or premature death for racial/ethnic minority populations through reduced or nonexistent access to health care; these conditions now require more immediate attention because they increase vulnerabilities and risks that can lead to poor health outcomes or death. Community knowledge, perceptions, and approaches to culturally responsive mitigation strategies must be prioritized. Carefully constructed local community governance boards that include multidisciplinary leadership (clinical, policy and social service, and research, among others), should be formed to lead assessments toward community and data-informed COVID-19 mitigation strategies for vulnerable populations in highly affected geographic areas.\n\n【36】**Strategically engage public health and community-attuned policy leaders and prioritize community stimulus strategies.** The political landscape calls for public health leadership by mitigation response teams . These teams are key informants from the beginning of public health initiatives designed to mitigate the pandemic, and their engagement is essential. They will provide another lens through which to examine the structures and processes that enable inequities to systematically develop and flourish or be eradicated through community co-created responses.\n\n【37】The essential areas of policy for optimal community health are in prioritized economic development, food security, and access to health care protection for vulnerable African American communities. Collectively, these areas present opportunities for intervention in response to chronic disease self-management (clinical), economic strains (community), and health care protections (policy) associated with the COVID-19 vulnerabilities of many African American communities. These essential policy areas represent a proposed foundation that rests on 4 “Es” hypothesized to narrow disparity gaps and offer opportunities for self-sufficiency and community resiliency.\n\n【38】*   **E** mploy trained/certified, compensated community health workers, coaches, and ambassadors who are charged with cultural messaging and education, contact tracing, and surveillance toward increased adherence to policies on physical distancing and sheltering in place.\n*   **E** xpand SNAP programs with vouchers to include the purchase of household and personal care items rather than encouraging recipients to barter for basic care products.\n*   **E** nhance school lunch programs so that all children receive high-quality, balanced meals throughout the year, regardless of the ability to pay.\n*   **E** nsure universal broadband internet access to reduce education, health care, and information barriers.\n\n【39】**Cultivate community-informed public health disaster health literacy.** Health literacy concepts, modes, and education must be reframed. The media have newly exposed the lay public to the realities of unequal treatment and unequal pandemic risk. The public is, thereby, witnessing the more rapid connection between who they are, where they live, and who is more likely to suffer from and die of COVID-19. Marketing frameworks for community-based prevention can be used to position community leaders to inform and lead health communication strategies. These marketing frameworks will ensure that messages resonate, engage, and foster action with objectivity and community/cultural sensitivity.\n\n【40】**Foster culturally tailored behavioral and mental health dialogue and response** . Multidimensional prevention education strategies that encourage resilience (positive adaptation to adversity) must be promoted in African American communities. This promotion should involve advocating for proactive self-care, reducing stigma, and encouraging integrated health care. These strategies should be promoted and proactively integrated as cross-cutting components of _any_ research and health initiative.\n\n【41】**Prioritize patient-centered medical homes and neighborhood models.** Patient-centered medical home infrastructures that include models of integrated care (mental and behavioral health care services in primary health care settings) can help overcome barriers to comprehensive health care and overall wellness. This model engages comprehensive resources to care for a patient, regardless of race/ethnicity, sex/gender, sexual orientation, language, socioeconomic status, or health insurance coverage. Primary care providers are encouraged to incorporate this model into their practices to decrease illness and death among African Americans at heightened risk of COVID-19 .\n\n【42】**Redefine essential workers.** Although the accomplishments of first responders — physicians, nurses, scientists, and other people fighting to preserve life — are laudable and undeniable, many African American nonclinical frontline workers, such as maintenance, janitorial, or food processing workers, are excluded from the definition of essential workers. The social vulnerability of nonclinical frontline workers, who often have chronic health conditions that place them at particular risk for contracting COVID-19, should be acknowledged and considered in planning.\n\n【43】Community and public health leaders in health care, behavioral health, and policy must consider the implications of health inequities among racial/ethnic minority populations, seriously tackle their root causes, and develop culturally responsive COVID-19 strategies for socially vulnerable African Americans. CBPR-driven approaches that elevate marginalized communities as senior partners in planning, implementing, and evaluating strategies will promote community leadership and increase adherence to health communication messages as the COVID-19 pandemic evolves. Efforts should be characterized by strong data (research or evaluation), contextually relevant community engagement strategies, and action (policy, systems, and environmental change approaches). The COVID-19 pandemic has presented an optimal opportunity to reprioritize and sustain approaches toward advancing community engagement of vulnerable African Americans. These new approaches will prepare us for the next pandemic. More importantly, they will foster CBPR leadership in advancing health equity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "362b4743-f3ac-43ff-b725-d133c0fd35f0", "title": "Postmortem Diagnosis of Invasive Meningococcal Disease", "text": "【0】Postmortem Diagnosis of Invasive Meningococcal Disease\nInvasive meningococcal disease is nationally reportable in the United States, and its case-fatality rate is 10%–14% . In New York City, New York, USA, every suspected or confirmed case is investigated to rapidly identify and recommend antimicrobial prophylaxis to close contacts. The risk for disease is highest during the initial 1–4 days after exposure. Fatal invasive meningococcal disease may go undiagnosed, which impairs prevention efforts and understanding of transmission. We describe 2 cases of serogroup C meningococcal disease diagnosed post mortem by PCR from vitreous humor and immunohistochemical (IHC) staining of tissues collected at autopsy.\n\n【1】### The Cases\n\n【2】##### Case\n\n【3】In the fall of 2012, a man in his early 30s was found dead in his room by his family. He had attended a party and the next morning reported fever, chills, and general malaise (day 1). He was not seen again until day 3 when a family member found him unresponsive in his room. Emergency medical services personnel pronounced him dead. The case was reported to the New York City Office of the Chief Medical Examiner, and the medicolegal investigator noted that the man was in an early state of decomposition. No suspicious circumstances or evidence of external trauma or alcohol or drug use were present.\n\n【4】An autopsy performed on day 4 was remarkable for a purpuric rash more pronounced on the legs, arms, hands, and soles of feet . Early decomposition, commented on by the medicolegal investigator, was also noted at autopsy. Internal organs had a markedly soft consistency, but no other substantial abnormality was noted. The heart and valves were within normal limits without evidence of vegetation. The only substantial internal finding was a thin layer of purulent exudate on the leptomeninges. Cerebrospinal fluid (CSF) and blood samples could not be obtained. A limited amount of vitreous humor was collected; brain and other organ tissue samples collected were fixed in formalin.\n\n【5】The body was brought to the funeral home and embalmed on day 6. The medical examiner reviewing the case on day 6 became concerned that the patient might have died of meningococcal disease, and the examiner notified the New York City Department of Health and Mental Hygiene (DOHMH). DOHMH recommended that the medical examiner collect additional samples, specifically skin, for IHC. Three hours after embalming, skin samples were collected from areas of purpuric rash on the leg and stored in saline. Vitreous humor and skin samples were sent to Wadsworth Center laboratory of the New York State Department of Health (Albany, NY, USA) for testing by PCR, and tissue specimens (brain, lung, heart, liver, and kidney collected at autopsy and fixed in formalin and skin collected after embalming) were sent to the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA) for IHC staining and PCR .\n\n【6】Skin and vitreous humor specimens were positive for _Neisseria meningitidis_ serogroup C DNA by real-time PCR at Wadsworth Center . Gram-staining of brain and skin tissue (examination performed at CDC) showed gram-negative cocci and gram-positive bacilli in leptomeninges and vascular lumens. Widespread gram-positive bacilli were present in brain parenchyma without corresponding inflammation. Therefore, the gram-positive bacilli were deemed likely to represent postmortem bacteria overgrowth and did not contribute to an etiologic diagnosis. IHC assays that used a polyclonal anti– _N. meningitidis_ group Y antiserum that is broadly reactive with serogroups A, B, C, W, and Y, and a specific monoclonal anti– _N. meningitidis_ serogroup C antibody, revealed immunostaining in the leptomeninges, lung, and skin tissues . Results confirmed _N. meningitidis_ serogroup C as the etiologic agent causing acute meningitis and systemic infection.\n\n【7】Clinical, PCR, and IHC findings were all consistent with invasive meningococcal disease. All close contacts of the patient were located and given prophylaxis. No secondary cases were identified.\n\n【8】##### Case\n\n【9】Approximately 2 months after the first case occurred, a man in his mid-30s was found dead in his apartment by his friends and police after he was reported missing from work. He had last spoken to friends 4 days previously when he reported a sore throat (day 1). The man had a history of HIV infection and crystal methamphetamine use, which was confirmed by urine toxicology at autopsy.\n\n【10】Autopsy was performed on day 6. The patient had marked putrefactive skin changes on the left side of the body, consistent with his postmortem position, and visible purpura on the right side of the torso and lower extremities but not on palms or soles. Flattening of cerebral gyri with apparent focal subarachnoid purulent exudate was also reported. Cerebral cortex and leptomeninge tissues were cultured and grew gram-positive rods and mixed flora. Tissue samples of lung, liver, spleen, kidney, and pancreas were fixed in formalin and sent to CDC. Vitreous humor and a swab of the leptomeninges collected at autopsy were sent to Wadsworth Center.\n\n【11】At Wadsworth Center, testing of the vitreous humor by real-time PCR showed _N. meningitidis_ serogroup C DNA, _Haemophilus influenzae_ DNA, _Streptococcus agalactiae_ DNA, and _Staphylococcus aureus_ DNA . As in case 1, the bacteria other than _N. meningitidis_ were thought to represent postmortem bacteria overgrowth and did not contribute to etiologic diagnosis. At CDC, _N_ . _meningitidis_ DNA was extracted and amplified by PCR from formalin-fixed paraffin-embedded brain, liver, lung, spleen, and kidney tissue . Although multiple tissues showed marked autolysis, distinct and specific IHC staining for _N. meningitidis_ serogroup C was identified within blood vessels and in neutrophilic infiltrates in the leptomeninges  . The results confirmed _N. meningitidis_ serogroup C as the etiologic agent causing acute meningitis and systemic infection. DOHMH did not learn about the case until after it was too late to administer prophylaxis to close contacts, but no known secondary cases were identified.\n\n【12】### Conclusions\n\n【13】Suspecting and diagnosing meningococcal disease early is critical for initiating timely prophylaxis and preventing secondary cases. The 2 cases described here posed multiple challenges for diagnosing invasive meningococcal disease. In both cases, CSF and blood samples, where _N. meningitidis_ is typically identified, were unavailable. Given the patients’ unattended deaths and delayed discovery, putrefaction had set in and tissue samples were fairly decomposed. Additional bacteria were identified through real-time PCR or culture, further complicating the diagnosis for both cases.\n\n【14】_N. meningitidis_ is a fastidious organism that frequently undergoes autolysis, although it has been found in CSF by PCR up to 10 days postmortem . Vitreous fluid has been described as a useful specimen for postmortem analysis because the eye is isolated and the fluid is less subjected to contamination or purification . _N. meningitidis_ has been isolated from vitreous humor of living patients, usually in conjunction with symptoms of meningococcal endopthalmitis . We have demonstrated that postmortem diagnosis of _N. meningitidis_ from vitreous humor and IHC staining < 3 days after death is possible. This testing might prove a useful option for medical examiners and public health officials for diagnosing suspected meningococcal disease when blood and CSF are unavailable for testing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "abab72ba-95a2-4620-b0b3-1102aca8e1c5", "title": "Crow Deaths Caused by West Nile Virus during Winter", "text": "【0】Crow Deaths Caused by West Nile Virus during Winter\nIn the northern United States, West Nile virus (WNV) is thought to overwinter in hibernating mosquitoes . Because reports of birds dying of WNV infection during the winter are rare, we investigated the cause of crow deaths in New York during the winter of 2004–2005.\n\n【1】### The Study\n\n【2】Dead crows from a roost were reported to the Dutchess County Department of Health in December 2004 . The roost was located in coniferous and deciduous trees at the east end of the Mid-Hudson Bridge, Poughkeepsie, New York, USA. Because winter surveillance in Poughkeepsie had not previously confirmed WNV, the crows were not collected for testing.\n\n【3】However, after the third dead crow in January was reported, ground surveillance of the roost was initiated . Thereafter, carcasses were collected 4–5 times per week at a radius of 1/4 mile around the roost and were transported for necropsy to the New York State Department of Environmental Conservation. On March 1, 2005, the roost, culverts, and areas under the bridge were examined for overwintering mosquitoes. Temperature data from December 1, 2004, to March 31, 2005, were obtained from the National Oceanic and Atmospheric Administration, Silver Spring, Maryland, USA.\n\n【4】Oral swabs were collected from carcasses and screened by using VecTest (Medical Analysis Systems, Freemont, CA, USA) and Rapid Analyte Measurement Platform (RAMP; Response Biomedical Corp, Burnaby, British Columbia, Canada) . Brain tissue was submitted to the New York State Department of Health (NYSDoH) for testing by TaqMan reverse transcription–PCR (RT-PCR) and standard RT-PCR . When possible, blood clots were collected from heart chambers for antibody testing by ELISA . Ectoparasites were collected from some carcasses before necropsy and tested for WNV by TaqMan RT-PCR .\n\n【5】To characterize this WNV genotype, RNA was extracted from the homogenate of a WNV-positive crow kidney (strain 05000918) by using RNeasy (QIAGEN, Valencia, CA, USA). The envelope coding region was amplified in 3 overlapping fragments by using QIAGEN One-Step RT-PCR core kit. DNA was sequenced at the Wadsworth Center Molecular Genetics Core facility by using ABI 3100 or 3700 automated sequencers (Applied Biosystems, Foster City, CA, USA). We generated the sequence  by using the SeqMan module within Lasergene (DNASTAR, Madison, WI, USA) and compared it with previously characterized North American strains by using MegAlign within Lasergene.\n\n【6】We collected 45 fecal specimens from 12 sampling points in the roost and 10 from beneath 2 carcasses. Specimens were tested for WNV RNA by using TaqMan and standard RT-PCR  with minor modifications; 100 mg of each specimen was diluted in 1.0 mL of BA-1, homogenized, centrifuged, and sterile filtered. RNA was extracted from the filtrate by using RNeasy (QIAGEN), and RT-PCR was conducted.\n\n【7】From February 10 to March 29, 98 carcasses were collected from the roost area; of these, 12 (12.2%) were WNV-positive according to VecTest and RAMP and 13 (13.3%) were positive according to TaqMan RT-PCR . The crow isolate was characterized as the WN02 genotype .\n\n【8】Necropsy and histopathologic findings on WNV-positive crows (n = 13) were consistent with previously reported pathologic findings . Necropsy findings included low body weight (84.6%), enlarged spleen (23.1%), and enlarged liver (30.8%); histopathologic findings included slight to moderate encephalitis with mild, diffuse gliosis and occasional small foci of necrosis in the gray matter of the brain. Meningoencephalitis, characteristic of WNV-positive birds , was not observed. WNV-negative crows (n = 85) died from traumatic injuries (51.8%), predation (16.5%), avian pox (14.1%), pneumonia (11.8%), and poisoning (5.9%). Two pools of >20 lice ( _Philopterus_ spp. _Mallophaga_ ) from 6 WNV-positive birds and 1 pool from 1 WNV-negative bird were tested; 6 positive pools were detected from 4 positive birds.\n\n【9】All 56 blood clots collected were seronegative by ELISA for flavivirus antibodies. Of the 45 fecal samples, 3 were WNV-positive; 2 of these (1 collected from beneath a WNV-positive crow; 1 from a random roost sampling point) had >800 pfu/mL, according to extrapolation from TaqMan RT-PCR.\n\n【10】No mosquito hibernacula were located in the areas examined, and no mosquito activity was observed by field workers. Maximum daily temperatures were \\> 10°C for 6 days in December, 4 days in January and February, and 5 days in March; mean temperatures were <10°C throughout the epizootic .\n\n【11】### Conclusions\n\n【12】How WNV crow infections occurred during winter in New York when mosquito activity would have been limited is unclear . Reporting of crow carcasses can be as low as 10%; therefore, additional carcasses may have been observed and not reported before ground surveillance began . Initial crow infections could have occurred in November, when mean monthly temperature was \\> 10°C and mosquito infection was more probable. Maximum daily temperatures \\> 10°C occurred sporadically from December through March. However, mean temperatures remained at <10°C  and photoperiods at <12 h/day. Laboratory studies of wild-captured _Culex pipiens_ L. females, the primary WNV vector in the northeastern United States, have shown that _Cx. pipiens_ are unlikely to terminate diapause with photoperiods of <12 h/day and temperatures <10°C . Field studies in New York have shown that _Cx. pipiens_ remain in overwintering locations until mid-April, at which time photoperiods are \\> 12 h/day and mean temperatures \\> 10°C .\n\n【13】These winter deaths suggest a pattern of crow-to-crow transmission. WNV has been detected in blood–feather pulp of crows , and WNV-positive lice ( _Philopterus spp._ ) were collected from 4 WNV-positive crows. Research is needed on the risk for bird-to-bird viral transmission posed by ectoparasites, particularly to roost mates and nestlings. Scavenging of infected birds as a risk factor is supported by laboratory studies demonstrating WNV infection in crows after they ingested infected house sparrows ( _Passer domesticus_ )  and by chronic WNV infection in house sparrows and other bird species . Chronic infection in crows is unlikely given that laboratory studies have demonstrated 100% mortality rates within 5 days of infection .\n\n【14】Crow-to-crow transmission of WNV is supported by laboratory findings of fecal-shed WNV and contact transmission  and by WNV-positive results from oral and cloacal swabs used in VecTest and RAMP . In laboratory studies, crows shed WNV fecal titers as high as 10 8.8  pfu/g . Our study provides the first evidence of fecal-shed WNV in the wild. In Illinois, healthy and WNV-infected crows roosted communally in summer ; however, no additional evidence linked viremic crows and subsequent crow infections. Further study is needed on the role of summer and winter roosts and feces in the WNV transmission cycle. No human cases are known to be related to exposure to crow feces, although avoiding feces and wearing gloves when handling live or dead birds are recommended.\n\n【15】The role of birds in arbovirus overwintering and dissemination during migration has been suggested but is poorly understood. The last WNV-positive crow in this study was collected on March 29 as the roost was dispersing. Additional crows could have been infected before migrating to home territories. Radio-marked crows infected with WNV have traveled up to 4 km per night during the 5 days before they died . Thus, infected birds could transport the virus to new areas with active mosquitoes and contribute to the beginning of the WNV transmission cycle. We recommend additional study of winter WNV activity in crows and other bird species to determine their potential roles in arbovirus overwintering and the initiation of transmission when mosquitoes become active.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bf6fb502-5cd3-41cb-a4de-66311110ad92", "title": "Lineage 1 and 2 Strains of Encephalitic West Nile Virus, Central Europe", "text": "【0】Lineage 1 and 2 Strains of Encephalitic West Nile Virus, Central Europe\nGeographically, West Nile virus (WNV) is the most widespread member of the Japanese encephalitis virus (JEV) complex within the genus _Flavivirus_ and the family _Flaviviridae_ . The first strain (B 956) was isolated from a human patient in the West Nile district of Uganda in 1937; later the virus was also detected in several mosquito species, horses, humans, and other hosts in Africa, Europe, Asia, and Australia (where it has been named Kunjin virus) . WNV was introduced into the United States in 1999, and it spread quickly over large parts of North America and reached Mexico . The clinical impact of WNV varies in different regions. In the Old World, WNV causes relatively mild infections with influenzalike symptoms or no apparent disease ; encephalitis and fatalities in the human population, horses, or poultry are spasmodic . In the New World, WNV exhibits increased virulence among the local wild bird populations and causes more frequent severe central nervous system symptoms and deaths in humans and horses . Although exactly how WNV was introduced into New York is unclear, phylogenetic comparison of the viral nucleic acid sequences has shown a close relationship between the American WNV isolates and strains isolated from encephalitic geese and storks in Israel in 1998 . Experimental infections of rodents indicated that the neurovirulence of WNV correlates with its genotype, and the North American strains are highly neurovirulent for mice .\n\n【1】WNV shows relatively high levels of sequence diversity. Comprehensive studies on the phylogenetic relatedness of WNV strains show that they form at least 2 main lineages . Lineage 1 is composed of WNV strains from different geographic regions, and it is subdivided into at least 3 clades. Clade A contains strains from Europe, Africa, the Middle East, and America; clade B represents the Australian (Kunjin) strains; and clade C contains Indian WNV isolates. Lineage 2 contains the B 956 prototype strain and other strains isolated so far exclusively in sub-Saharan Africa and Madagascar. In addition to the 2 major WNV lineages, we recently proposed 2 lineages for viruses that exhibited considerable genetic differences to the known WNV lineages: lineage 3 consists of a virus strain isolated from _Culex pipiens_ mosquitoes at the Czech Republic/Austria border (named Rabensburg virus), and lineage 4 consists of a unique virus isolated in the Caucasus. These 2 viruses, however, may also be considered independent flaviviruses within the JEV complex .\n\n【2】WNV has been known to be present in central Europe for a long time. Seroprevalence in humans was reported in several countries, including Hungary, and WNV strains were isolated from mosquitoes, humans, migrating birds, and rodents during the last 30 years . Until 2003, however, WNV infections in Hungary have never been associated with clinical symptoms, although a severe outbreak of West Nile encephalitis in humans was reported in 1996 and 1997 in neighboring Romania.\n\n【3】In late summer 2003, an outbreak of encephalitis emerged in a Hungarian goose flock, resulting in a 14% death rate among 6-week-old geese ( _Anser anser domesticus_ ). Based on histopathologic alterations, serologic investigations, and nucleic acid detection by reverse transcription–polymerase chain reaction (RT-PCR), WNV was diagnosed as the cause of the disease . Chronologically and geographically related to the outbreak in geese, a serologically confirmed WNV outbreak was also observed in humans, which involved 14 cases of mild encephalitis and meningitis .\n\n【4】One year later, in August 2004, a goshawk ( _Accipiter gentilis_ ) fledgling showed central nervous system symptoms and died in a national park in southeastern Hungary. When histopathologic methods and RT-PCR were used, WNV antigen and nucleic acid were detected in the organs of the bird. Furthermore, the virus was isolated after injection of suckling mice. Here we report the sequencing and phylogenetic results of these 2 encephalitic WNV strains that emerged recently in central Europe.\n\n【5】### Materials and Methods\n\n【6】Brain specimens from one 6-week-old goose, which died during the encephalitis outbreak in a Hungarian goose flock, and brain samples from a goshawk, which also died from encephalitis, were used for WNV nucleic acid determination. The brain samples were homogenized in ceramic mortars by using sterile quartz sand, and the homogenates were suspended in RNase-free distilled water. Samples were stored at –80°C until nucleic acid extraction was performed.\n\n【7】Viral RNA was extracted from 140 μL of brain homogenates by using the QIAamp viral RNA Mini Kit (Qiagen, Hilden, Germany) according to the manufacturer's instructions. First, a universal JEV-group specific oligonucleotide primer pair designed on the nonstructural protein 5 (NS5) and 3´-untranslated regions (UTR) of WNV (forward primer: 5´-GARTGGATGACVACRGAAGACATGCT-3´ and reverse primer: 5´-GGGGTCTCCTCTAACCTCTAGTCCTT-3´ ; ) was applied on the RNA extracts in a continuous RT-PCR system employing the QIAGEN OneStep RT-PCR Kit (Qiagen). Each 25-μL reaction mixture contained 5 μL of 5× buffer (final MgCl 2  concentration 2.5 mmol/L), 0.4 mmol/L of each deoxynucleoside triphosphate, 10 U RNasin RNase Inhibitor (Promega, Madison, WI, USA), 20 pmol of the genomic and reverse primers, 1 μL enzyme mix (containing Omniscript and Sensiscript Reverse Transcriptases and HotStarTaq DNA polymerase) and 2.5 μL template RNA. Reverse transcription was carried out at 50°C for 30 min, followed by a denaturation step at 95°C for 15 min. Thereafter, the cDNA was amplified in 40 cycles of heat denaturation at 94°C for 40 s, primer annealing at 57°C for 50 s, and DNA extension at 72°C for 1 min, and the reaction was completed by a final extension for 7 min at 72°C. Reactions were performed in a Perkin-Elmer GeneAmp PCR System 2400 thermocycler (Wellesley, MA, USA) and in a Hybaid PCR Sprint thermocycler (Thermo Electron Corporation, Waltham, MA, USA).\n\n【8】After RT-PCR, 10 μL of the amplicons was subjected to electrophoresis in a 1.2% Tris acetate-EDTA-agarose gel at 5 V/cm for 80 min. The gel was stained with ethidium bromide; bands were visualized under UV light and photographed with a Kodak DS Electrophoresis Documentation and Analysis System using the Kodak Digital Science 1D software program (Eastman Kodak Company, Rochester, NY, USA). Product sizes were determined with reference to a 100-bp DNA ladder (Promega).\n\n【9】Where clear PCR products of the previously calculated sizes were observed, the fragments were excised from the gel, and DNA was extracted by using the QIAquick Gel Extraction Kit (Qiagen). Fluorescence-based direct sequencing was performed in both directions on PCR products. Sequencing of PCR products was carried out with the ABI Prism Big Dye Terminator cycle sequencing ready reaction kit (Perkin-Elmer), according to the manufacturer's instructions, and an ABI Prism 310 genetic analyzer (Perkin-Elmer) automated sequencing system. Nucleotide sequences were identified by Basic Local Alignment Search Tool  search against gene bank databases. Based on the sequence information obtained from the amplification products, complete WNV sequences that exhibited the highest nucleotide identities with the Hungarian genotypes were selected from the GenBank database to design primers that amplify overlapping RT-PCR products covering the entire genome of the strains. Oligonucleotide primers were designed with the help of the Primer Designer 4 for Windows 95 (Scientific and Educational Software, Version 4.10; Microsoft, Redmond, WA, USA) and were synthesized by GibcoBRL Life Technologies, Ltd. (Paisley, Scotland, UK). Detailed information on all primers is in the Tables A1 and A2 . PCR amplification products were directly sequenced in both directions; the sequences were compiled and aligned to complete genome sequences of selected representatives of WNV lineages 1a, 1b, 2, and putative lineages 3 and 4 (listed in Table ). Phylogenetic analysis was performed by using the modified neighbor-joining method (ClustalX ; ), and trees were constructed to demonstrate the relationship between the Hungarian WNVs and other WNV strains .\n\n【10】The nucleotide sequences of the Hungarian WNV strains goose-Hungary/03 (Hu03) and goshawk-Hungary/04 (Hu04) were submitted to the GenBank database. They are available under accession numbers DQ118127 and DQ116961, respectively.\n\n【11】### Results\n\n【12】In this study, the complete genome sequences of WNV strains derived from a 6-week-old goose, which died in 2003 during an outbreak of encephalitis in a Hungarian goose flock (strain goose-Hungary/03), and from a goshawk, which also died from encephalitis in the same region 1 year later (strain goshawk-Hungary/04), were determined, aligned, and phylogenetically analyzed. The genome of the goose-Hungary/03 strain is composed of 10,969 nucleotides (nt) and contains 1 open reading frame between nucleotide positions 97 and 10,398, coding for a 3,433 amino acid (aa)–long putative polyprotein precursor. The complete genomic sequence of the virus was subjected to a BLAST search against gene bank databases. The highest identity rates (98% at the nucleotide and 99% at the amino acid level) were found with WNV strains isolated in 1998 in Israel and in 1999 in the United States. In addition, phylogenetic analysis was performed to indicate the relationships between the Hungarian goose–derived WNV strain and selected representatives of WNV clades and clusters. The resulting phylogenetic tree  confirmed the results of the BLAST search, i.e. the Hungarian goose–derived WNV strain is clustering close to the previously mentioned WNV strains isolated in the United States and Israel, which belong to lineage 1a of WNV. Other European WNV strains (isolated in Italy, France, and Romania) are more distant to the Hungarian strain; they form a separate cluster consisting of a Romanian/Russian and a French/Italian subcluster.\n\n【13】The complete nucleotide sequence of the goshawk-Hungary/04 WNV strain is composed of 11,028 nt and contains 1 open reading frame between nucleotide positions 97 and 10,401, coding for a 3,434-aa putative polyprotein precursor. In BLAST search, the strain showed the highest (96% nt and 99% aa) identity to the WNV prototype strain B 956. Consequently, as the phylogram also indicates , this virus belongs to lineage 2 of WNV. Alignments of the available partial sequences from the E protein coding regions of other representatives of this cluster showed even higher identities (97%–98% nt and 100% aa) with WNV strains isolated in central Africa in 1972 (AnB3507, AF001563) and in 1983 (HB83P55, AF001557), respectively .\n\n【14】More recently (in early August 2005), additional lethal cases of encephalitis occurred in birds of prey in the same place in which the goshawk died of West Nile encephalitis in 2004, involving up to a total of 3 goshawks and 2 sparrow hawks ( _A. nisus_ ); 2 of the goshawks and 1 sparrow hawk died. Preliminary investigations detected WNV-specific nucleic acid in the brains of the birds. The partial nucleotide sequence of the 2005 virus (1,000 bp at the NS5´–3´-UTR regions) showed 99.9% identity with the goshawk-Hungary/04 strain (only 1 substitution at nucleotide position 9,376 \\[g→a\\] has been observed, which did not influence the putative amino acid sequence). Additional observation of the outbreak and investigations of the cases are in progress.\n\n【15】### Discussion\n\n【16】The primary aim of our investigations was to show the genetic relatedness of the WNV strains detected in Hungary in the last 2 years and to estimate their clinical and epidemiologic impact. The phylogenetic analysis emphasizes the close genetic relationship of the goose-Hungary/03 strain with a WNV strain isolated in Israel in 1998 and the WNV strain introduced in New York in 1999, since the 3 WNVs form 1 single cluster within clade 1a of lineage 1. These strains caused outbreaks in birds, humans, and horses. Previous European WNV isolates exhibited lower identity values, e.g. the strain that was responsible for the Romanian outbreak(s) in 1996 and 1997 showed only 96% nt identity with the Hungarian goose-2003 strain, and in the phylogenetic tree the other European isolates form a separate cluster consisting of 2 subclusters . The earliest representatives of the Israel/USA/goose-Hungary/03 cluster were reported by Malkinson et al. from ill and dead white storks ( _Ciconia ciconia_ ) in Israel in 1998. These storks, however, had hatched in central Europe, and during their autumn migration southwards, strong winds had blown them off course, from their usual route to Africa, to southern Israel. Malkinson et al. suspected that these birds introduced the neurovirulent genotype of WNV to Israel from their hatching place. The wetlands of southeastern Hungary are foraging and nesting habitats for storks and many other wild bird species, and the goose farm, where the WNV outbreak occurred in 2003, is located in this region. These facts, together with the close phylogenetic relatedness of the Israeli/US/Hungarian WNV strains, strongly support the theory that storks carried the neurovirulent WNV strain from central Europe (that is, from Hungary) to Israel, which sheds new light on the introduction of WNV to New York. This virus could have originated in Israel (which is the generally accepted although not proven theory) or central Europe. In both cases, however, the virus seems to have its true origin in Europe. In a recent publication, Lvov et al. suggested that WNV could have been introduced into New York by ships traveling from Black Sea ports .\n\n【17】When a WNV infection was detected in 2004 in a goshawk fledgling, which died from encephalitis in the same region of Hungary in which the outbreak in geese and humans occurred during the previous year, we anticipated a WNV strain more or less identical to the genotype detected there in 2003. The genomic sequence of this strain was not closely related to the sequence of the WNV strain detected in geese in the year before, however, but belonged to the group of central African lineage 2 WNV strains. A closely related strain from this cluster (ArB3573, AF001565, and AF458349) was identified as a neuroinvasive strain of WNV in a mouse model . To our knowledge, this report is the first on the emergence of a lineage 2 WNV strain outside Africa. Migratory birds that had overwintered in central Africa probably introduced this exotic strain to the wetlands of Hungary. On the other hand, as the goshawk is not a migratory species, and infection occurred in August, the African WNV strain must have already successfully adapted to local mosquito vectors. Consequently, this neurotropic, exotic WNV strain may become a resident pathogen in Europe with all the possible public health consequences.\n\n【18】Our results indicate that the WNV strains that emerged in 2 consecutive years and caused avian deaths in Hungary are epidemiologically unrelated. Genetically distinct WNV strains are circulating simultaneously yet independently in local birds and thus most likely also in local mosquito populations within the same region. They cause sporadic cases of encephalitis and also raise the possibility of spreading to other European countries or even to other continents, as happened in 1999 with another WNV strain, which resulted in a public health catastrophe in America.\n\n【19】In addition to the above 2 novel WNVs, we recently characterized another novel flavivirus of so far unknown human pathogenicity named Rabensburg virus, which has been isolated from _Culex pipiens_ mosquitoes in 1997 and 1999 at the Czech Republic–Austria border, only a few hundred kilometers from the region where the Hungarian WNVs emerged. After the entire genome was sequenced, Rabensburg virus turned out to represent either a new (third) lineage of WNV or a novel flavivirus of the JEV group . Thus, several distinct WNV strains seem to circulate in central Europe. In 2001 another flavivirus of the JEV group, Usutu virus, which has never previously been observed outside Africa, emerged in Austria and resulted in deaths in several species of birds, especially Eurasian blackbirds ( _Turdus merula_ ) . This virus became a resident pathogen in Austria and continues to disperse and cause deaths in blackbirds and other species of birds .\n\n【20】The snowy winter and rainy spring of 2005 resulted in serious floods in the area in which the Hungarian WNV strains were identified. Since the floodplains and polders were under water, the conditions for mosquito development were ideal. The summer was also very rainy, which resulted in more floods in the region and continuous mosquito gradation. The most recent data imply that the lineage 2 WNV strain may have overwintered in Hungary, causing several clinical cases of encephalitis in _Accipiter_ species in 2005 as well.\n\n【21】The routine diagnostic techniques in most of the European public health and veterinary laboratories are designed to detect lineage 1 WNV strains. In a recent PCR external quality assurance multicenter test, <40% of the involved laboratories could detect lineage 2 WNV strains . Therefore, a major goal of this article is to increase the scientific and public awareness of this potential public health threat for Europe and, perhaps, America. Furthermore, comprehensive investigations on the occurrence, ecology, and epidemiology of the different WNV strains circulating in central Europe, as well as the development of monitoring and surveillance programs, must be of highest priority. One may also speculate on environmental factors, such as climate change or global warming, that may have enhanced the recent emergence of viruses, which had previously been restricted to Africa, in new habitats and continents. Improved observation, reporting, and detection methods have also contributed to the apparent increasing emergence of these viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
