{"seq_id": "b6c05ece-04a8-4bd0-9539-9d4ba3c5b7f6", "title": "Landscape", "text": "【0】Landscape\nIsn't it plain the sheets of moss, except that\n\n【1】they have no tongues, could lecture\n\n【2】all day if they wanted about\n\n【3】spiritual patience? Isn't it clear\n\n【4】the black oaks along the path are standing\n\n【5】as though they were the most fragile of flowers?\n\n【6】Every morning I walk like this around\n\n【7】the pond, thinking: if the doors of my heart\n\n【8】ever close, I am as good as dead.\n\n【9】Every morning, so far, I'm alive. And now\n\n【10】the crows break off from the rest of the darkness\n\n【11】and burst up into the sky—as though\n\n【12】all night they had thought of what they would like\n\n【13】their lives to be, and imagined\n\n【14】their strong, thick wings.\n\n【15】**From Dream Work, copyright 1986 by Mary Oliver.**\n\n【16】Used by permission of Grove/Atlantic, Inc. and the author.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1563ca03-d20a-4af5-bae8-c86010c62a7d", "title": "Enterovirus A71 Subgenotype B5, France, 2013", "text": "【0】Enterovirus A71 Subgenotype B5, France, 2013\n**To the Editor:** We report the detection of human enterovirus A71 (EV-A71) subgenotype B5 in France, 6 years after it was first detected in Europe. EV-A71 belongs to the _Enterovirus A_ species (genus _Enterovirus_ , family _Picornaviridae_ ) and is a major cause of hand, foot and mouth disease (HFMD), sometimes associated with severe neurologic complications  _._ EV-A71 strains are classified in 6 genotypes, A–F, ( _2_ ) but most of the circulating strains belong to genotypes B and C and to 11 subgenotypes (B0−B5, C1−C5) . Genotypes B and C have been reported in HFMD epidemics in the Asia-Pacific region; different subgenotypes cause nationwide epidemics that usually occur every 2−3 years . During 1963−1986 in Europe, Australia, and the United States, enterovirus infections were caused by viruses from subgenotypes B0, B1, and B2, but since 2000, infections with C1 and C2 viruses have begun to predominate . The other subgenotypes have been reported rarely in Europe; the C4b and C4a strains were identified in France, Germany, Austria, and Denmark in 2004 and 2012, respectively , and the B5 subgenotype was reported in Denmark in 2007 .\n\n【1】In November 2013, a 3-week-old boy was admitted to the emergency unit of a hospital in Compiègne, France, with a 48-hour history of fever and irritability. He was born at term after an uneventful pregnancy and delivery. On admission, he had normal vital signs. He had had contact with a cousin with oral ulcerations, but no information was available about the source of the cousin’s infection.\n\n【2】Laboratory testing revealed moderate cytolytic hepatitis. Complete blood count results were within reference values. Cerebrospinal fluid showed pleiocytosis (38 leukocytes, 81% polymorphonuclear cells) with protein and glucose levels with reference ranges. Bacterial cultures of blood and cerebrospinal fluid were negative. Enterovirus genome was detected in serum samples and cerebrospinal fluid by reverse transcription PCR. The infant made a steady recovery and was discharged 10 days after admission, with no apparent adverse outcome. Final diagnosis was neonatal enterovirus infection with meningitis.\n\n【3】Genotyping was performed on the serum specimen by using seminested reverse transcription PCR amplification and sequencing of the viral protein 1 gene. Phylogenetic investigation with sequences of reference strains representing all subgenotypes indicated that the isolate from the patient, designated PAR024102\\_FRA13, belonged to the EV-A71 B5 subgenotype. We investigated the putative origin of the strain by comparing 248 nonredundant complete 1D sequences of EV-A71 B5 strains following a Bayesian phylogenetic approach. PAR024103\\_FRA13 shared a most recent common ancestor (posterior probability = 1) with virus strains sampled in China in 2009 and in Taiwan during the 2011–2012 outbreak, ( _8_ ) but was only distantly related to them (data not shown).\n\n【4】Further analyses with 274 partial 1D gene sequences from GenBank (on March 19, 2014) indicated close genetic relationships (posterior probability = 1) with strains isolated in Thailand in 2012  . The complete genome of PAR024103\\_FRA13 was determined by nucleotide sequencing of 4 overlapping segments obtained by gene amplification . Sequence comparisons were performed with 13 available EV-A71 B5 complete genomes; the virus strain isolated in France exhibited 92%−99.5% nt similarity (98.9%−99.4% aa similarity) throughout the genome.\n\n【5】The EV-A71 B5 subgenotype was first detected in 1999 in Malaysia and spread to several other countries in Asia during the 2000s. Outbreaks causing severe illness and deaths were reported in Japan , Brunei , and Taiwan (2008 and 2012) . The first detection of subgenotype B5 in Europe was associated with a recrudescence of EV-A71 infections associated with meningitis and HFMD in Denmark in 2007 . The overall phylogenetic data are consistent with an introduction of EV-A71 B5 in France by importation of a strain from Asia, possibly from Thailand. Transmission of EV-A71 strains has been shown to occur in Europe as discrete and temporally defined virus introductions, occasionally followed by sustained dissemination .\n\n【6】The emergence of the Asiatic lineage EV-A71 C4a in Denmark in 2012 is a recent event . The reemergence of EV-A71 subgenotype B5 in 2008 in Taiwan resulted in the largest outbreak of EV-A71 infection in the past 11 years . To our knowledge, the B5 subgenotype has not previously been detected in Europe. Global herd immunity produced by circulation of the C2 genotype may protect the European population from the spread of other subgenotypes . However, given that most countries in Europe do not perform specific surveillance for HFMD and most enterovirus infections are asymptomatic, this particular subgenotype could be circulating more widely without detection.\n\n【7】Enterovirus infections in neonates and infants are a frequent cause of hospitalization, which may contribute to EV-A71 detection . However, the development of a national syndromic surveillance targeting HFMD would enable early detection of HFMD outbreaks and any new EV-A71 subgenotype. Attention should also be paid to the potential risks of epidemic spread of EV-A71 outside Asia posed by international travelers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1ffdce83-6daf-4454-a2ee-a4bd60aa42db", "title": "Phylogenetic Analysis of West Nile Virus Genome, Iran", "text": "【0】Phylogenetic Analysis of West Nile Virus Genome, Iran\n**To the Editor:** West Nile virus (WNV) is a single-stranded, positive-sense RNA virus (≈11 kb) that is taxonomically classified within the family _Flaviviridae_ , genus _Flavivirus_ . WNV is found in Africa, Eurasia, Australia, and North America .\n\n【1】Comprehensive studies on phylogenetic relatedness of WNV strains have showed that WNV can be grouped into 5 lineages. Lineage 1 contains WNV strains from different regions, including northern, western, and central Africa; southern and eastern Europe; India; and the Middle East. Lineage 1 is subdivided into 3 clades. Clade 1A contains strains from Europe, northern Africa, the United States, and Israel, clade 1B contains Kunjin virus from Australia. Lineage 2 contains isolates from west, central, and eastern Africa and Madagascar. There is evidence that lineage 2 circulates in some regions of Europe (e.g. Italy, Austria, and Greece) . Lineage 3 contains Rabensburg virus 97–103, which was isolated in 1997 from _Culex pipiens_ mosquitoes in South Moravia in the Czech Republic. Lineage 4 contains a new variant of WNV (strain LEIVKrnd88–190), which was isolated in 1998 from _Dermacentor marginatus_ ticks in a valley in the northwestern Caucasus Mountains of Russia. Lineage 5 contains an WNV isolate from India (strain 804994) . In this study, we compared the phylogenetic relationships of WNV circulating in Iran to other WNV strains by using a partial WNV sequence isolated from an Iranian patient.\n\n【2】WNV was obtained from a blood sample from an Iranian patient who had encephalitis and was hospitalized in 2009 in Isfahan in the central highlands of Iran. The patient reported no history of animal contact, insect bites, blood transfusions, transplantations, and travel. He exhibited fever, headache, hypertension, and vomiting. On initial examination, he had a body temperature of 40°C. Laboratory investigations on the day of admission showed a leukocyte count of 240 cells/μL, a protein level of 52 mg/dL, and a glucose level of 50 mg/dL in a cerebrospinal fluid sample.\n\n【3】Further examinations were undertaken, and samples were sent to the Arboviruses and Viral Hemorrhagic Fevers Laboratory at Pasteur Institute of Iran in Teheran. For an IgG ELISA, wells in test plates were coated overnight with mouse hyperimmune ascitic fluid. Native antigen was added, and wells were incubated and washed. Test samples and peroxidase-labeled anti-human or anti-animal immunoglobulin were added. After incubation for 10 min, optical densities were read .\n\n【4】Viral RNA was extracted by using the QIAmp Viral RNA Mini Kit (QIAGEN, Hilden, Germany) from serum of the patient. A reverse transcription PCR was conducted by using a One-Step RT-PCR Kit (QIAGEN). Samples were subjected to 1 cycle at 50°C for 30 min to synthesize cDNA; 95°C for 15 min; and 95°C for 30 s, 54°C for 30 s, and 72°C for 60 s; and a final extension at 72°C for 5 min . The serum sample was positive for IgG against WNV. Molecular tests showed positive results for WNV.\n\n【5】The PCR product was sequenced by using the Big Dye Terminator V3.1 Cycle Sequencing Kit (Applied Biosystems, Foster City, CA, USA), the modified Sanger sequencing method, and an ABI Genetic Analyzer 3130 (Applied Biosystems) . Multiple alignments of nucleotide sequences were made by using ClustalW . A phylogenetic tree was constructed by using 27 representative sequences of WNV and Japanese encephalitis virus (available in GenBank) and a 358-nt sequence of WNV , from the patient, which corresponds to nt 259–616 in the late region of the capsid gene and the early region of the membrane gene. Phylogenetic status of the sequence for the WNV strain from the patient was assessed by using the neighbor-joining algorithm in MEGA5 . Reliability of phylogenetic groupings was evaluated by using the bootstrap test (1,000 replications). Japanese encephalitis virus SA14 sequence was used as an outgroup in phylogenetic analysis of partial genome sequences .\n\n【6】The phylogenetic tree identified clustering of isolates in 5 lineages. Lineage 1 had 2 sublineage (clade 1A and clade 1B). All sequences in lineage 1 were geographically distinguishable. Clade 1A contained strains from Europe, Africa, the Middle East, and the United States; clade 1B included the Australian strain NSW 2011 (JN887352). The WNV sequence from Iran (KJ486150) was grouped into lineage 2 and had 99% identity with the 358-bp region of WNV strain ArB3573/82 from the Central African Republic. Although it was believed that lineage 2 strains circulate only in Africa, reports of their emergence primarily in Balkan countries  and in our study support the presence of a lineage WNV 2 strain in the Middle East, particularly in western Asia .\n\n【7】Although the patient did not report any mosquito bites, this infection route cannot be excluded because he was a farmer and spent most of his time outdoors. Previous studies have demonstrated that nearly all human WNV infections were a consequence of mosquito bites . Our study should increase awareness of WNV infections as a public health threat. In future studies, priority should be given to investigations of the ecology, occurrence, and epidemiology of the different WNV strains circulating in Iran.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9af035b0-99c3-4251-b35a-2fef3dd4a95b", "title": "West Nile Virus Aseptic Meningitis and Stuttering in Woman", "text": "【0】West Nile Virus Aseptic Meningitis and Stuttering in Woman\n**To the Editor:** West Nile virus (WNV), a mosquito-borne flavivirus, is closely related to St. Louis encephalitis virus and Japanese encephalitis virus (JEV). Most cases of WNV have been mild, but neuroinvasive disease has been observed, especially among older persons and immunocompromised persons . The most common neurologic manifestations of WNV are aseptic meningitis, meningoencephalitis, and encephalitis with or without acute flaccid paralysis . Other less common neurologic manifestations include Guillain-Barré syndrome, chorioretinitis, stroke-like symptoms, and unilateral brachial plexopathy .\n\n【1】We report a case of WNV aseptic meningitis in a 39-year-old immunocompetent woman who had severe headache with new-onset stuttering. Her medical history included lumbar disc herniation and migraines, for which she was taking sumatriptan. Her symptoms started ≈2 weeks before hospitalization and included a severe generalized headache initially thought to be a migraine, but sumatriptan resulted in no improvement. A few days later, she had fever and was intermittently stuttering. She denied recent travel or animal exposure but admitted to having received multiple mosquito bites during the preceding weeks.\n\n【2】At admission, she had a temperature of 101.3°F, pulse rate of 92 beats/min, blood pressure of 130/80 mm Hg, and respiratory rate of 16 breaths/min. She appeared mildly ill but was alert and oriented with no nuchal rigidity, photophobia, rash, or limb weakness. Results of a physical examination were unremarkable, and results of a neurologic examination were notable only for stuttering. Laboratory test results included a leukocyte count of 12,300 cells/mm 3  (63% neutrophils, 29% lymphocytes, 7% monocytes, 1% basophils) and a platelet count of 204,000 cells/mm 3  . Other laboratory values were unremarkable, and levels of serum transaminases and creatinine phosphokinase were within reference ranges. Cerebrospinal fluid (CSF) was clear and contained 37 leukocytes/mm 3  (2% neutrophils, 78% lymphocytes, 20% monocytes), 2 erythrocytes/mm 3  , a glucose level of 68 mg/dL, a protein level of 36 mg/dL, and a lactic acid level of 2.1 meq/L. No abnormalities were found on a cranial computed tomography scan.\n\n【3】The patient began treatment with acyclovir, 10 mg/kg intravenously, every 8 hours for 3 days. On hospital day 2, she underwent magnetic resonance imaging of the brain; results were within reference limits. On hospital day 3, her headache began to improve and she became afebrile, but she still stuttered occasionally. Results of CSF tests for enterovirus, herpes simplex viruses 1 and 2, and varicella zoster virus and PCR for human herpesvirus 6 were negative, and acyclovir was discontinued. On hospital day 5, she was discharged. Three days later, serum and CSF ELISA results for WNV were positive. A WNV ELISA was performed at ViroMed Laboratories (Minnetonka, MN, USA) by using a Focus Test Kit (Focus Diagnostics, Cypress, CA, USA), and the result was positive. The patient subsequently reported that her stuttering had ceased.\n\n【4】A high degree of clinical suspicion for WNV infection should be considered in patients with a recent history of mosquito bites and an acute febrile illness associated with neurologic signs and symptoms . Typical CSF findings of infection with WNV include lymphocytic pleocytosis, elevated protein level, reference glucose and lactic acid levels, and no erythrocytes .\n\n【5】The clinical presentation of WNV infection varied widely from asymptomatic seroconversion to fatal encephalitis. It is possible, but unlikely, that the stuttering in the patient was an indication of a migraine aura. Initially, the patient reported that the headache might have been a migraine, but later reported that its associated symptoms, e.g. photophobia, were not as severe and did not last as long as her usual migraines. Further argument against migraine aura is the lack of response to her migraine medication and the fact that the stuttering continued after the headache resolved.\n\n【6】Because WNV resembles JEV, it is interesting to note that a case of stuttering in a young adult infected with JEV has been reported . However, the mechanism of stuttering associated with WNV is unknown. One possible explanation is myoclonic contractions of the tongue, i.e. vocal myoclonus.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cf9f75a6-8a62-48c9-bf57-19f4efe8f1e6", "title": "Incidence and Transmission Dynamics of Bordetella pertussis Infection in Rural and Urban Communities, South Africa, 2016‒2018", "text": "【0】Incidence and Transmission Dynamics of Bordetella pertussis Infection in Rural and Urban Communities, South Africa, 2016‒2018\nDespite high vaccine coverage with either the whole-cell or acellular vaccine in many countries, the incidence of pertussis has increased globally during the past 20 years . Disease increase has been attributed to several factors, including increased awareness by clinicians, more sensitive molecular diagnostic techniques , serologic markers for identification of infection in adolescents and adults who are commonly asymptomatic carriers of _Bordetella pertussis_ , pathogen adaptation, or waning immunity .\n\n【1】Infection indicates that bacteria are in or on the body and make a person sick. Colonization indicates that bacteria are on the body but do not make a person sick.\n\n【2】In South Africa, pertussis is a notifiable medical condition . The whole-cell pertussis vaccine was introduced in South Africa in 1950 and was replaced by the acellular pertussis vaccine in April 2009. The vaccine is given to infants at 6, 10, and 14 weeks of age, and a booster dose is given at 18 months of age. In 2019, according to the World Health Organization/United Nations Children’s Fund, coverage for the first dose of the acellular vaccine in South Africa was 84%, and coverage for the third dose was 77% .\n\n【3】Data on pertussis epidemiology in low- and middle-income countries, particularly in Africa, are lacking; as a result, the epidemiology of the disease is poorly understood . In South Africa during 2013‒2018, among persons hospitalized because of pneumonia, annual _B. pertussis_ incidence was 17 cases/100,000 population, and the highest incidence of disease and most deaths were in children <1 year of age . In 2018, clusters of pertussis were observed in several provinces in South Africa, and 54% of cases were detected in infants <3 months of age . Infants <1 year of age were at highest risk for severe pertussis and death. It is essential to understand infection in older children and adults because these age groups are likely sources of transmission to infants . This study reports the incidence, factors associated with infection, duration of infection, and transmission dynamics in persons of all ages in an urban community and a rural community in South Africa.\n\n【4】### Materials and Methods\n\n【5】##### Study Population\n\n【6】During 2016–2018 (May–October 2016, January–October 2017, and January–October 2018), we conducted a community cohort study in 2 communities in South Africa: 1 rural (Agincourt, Mpumalanga Province) and 1 urban (Klerksdorp, North West Province). We enrolled consenting members of randomly selected households each year. During the household enrollment process, at least 80% of the household members within each household had to consent to be included in the study for a household to be enrolled. A detailed account of the study methods and the cohort profile has been previously reported .\n\n【7】##### Household Visits\n\n【8】Demographic and baseline health status (including HIV) were collected for all participants at enrollment. Participants were considered HIV infected if they had 1 of the following during the follow-up period: 2 positive rapid HIV test results, evidence of a positive HIV laboratory result, or evidence of receiving antiretroviral treatment. Vaccination status was obtained for children <5 years of age from Road-to-Health Cards, which serve as a child’s formal health record in South Africa. Households were visited twice a week for nasopharyngeal specimen collection and symptom determination by using a structured questionnaire. Symptom data included fever (self-reported or measured tympanic temperature \\> 38°C), cough, difficulty breathing, sore throat, nasal congestion, chest pain, muscle aches, headache, vomiting, or diarrhea. In addition, during 2018, persons who tested PCR positive for _B. pertussis_ were retrospectively interviewed (interviews were conducted immediately after a positive PCR result) to confirm if the persons experienced any pertussis-specific symptoms (cough, inspiratory whoop, posttussive vomiting, or apnea) during the infection.\n\n【9】##### Specimen Collection and Testing\n\n【10】At each visit, a nasopharyngeal swab specimen was collected and placed in PrimeStore Molecular Transport Medium  and transported to the laboratory within 48–72 hours for testing. Specimens were received in the laboratory in real-time and then batched for further processing. Total nucleic acids were extracted by using the Roche MagNA Pure 96 Instrument  and the MP96 DNA and Viral NA SV Kits (Roche Diagnostics). Extracts were tested by using an internally validated IS _481_ and human ribonuclease P (RNaseP) duplex real-time PCR. Any specimen that tested positive for IS _481_ with a cycle threshold (Ct) value < 45 was repeated (from extraction using a fresh aliquot) and retested (2 replicates) by using a second, multitarget real-time PCR targeting _B. pertussis_ , _B. parapertussis_ , and _B. holmesii_ . A specimen was considered positive for _B. pertussis_ if IS _481_ or _ptxS1_ targets was detected with Ct values < 45 in at least 2/3 replicates and negative for hIS _1001_ ( _B. holmesii_ ) and pIS _1001_ ( _B. parapertussis_ ).\n\n【11】##### Study Definitions and Data Analysis\n\n【12】We defined an episode of _B. pertussis_ infection as \\> 1 consecutive visits in which _B. pertussis_ was detected. A new episode was one that began after a period of at least 12 consecutive visits (i.e. 6 weeks) in which _B. pertussis_ was not detected. We determined the incidence of _B. pertussis_ infection by dividing the number of PCR-positive episodes by the person-time under observation, expressed as 100 person-weeks. We assessed factors associated with incidence by using Poisson regression accounting for the person-time under observation. For analysis of incidence, we considered all identified episodes of infections, including multiple episodes in the same person.\n\n【13】We estimated the duration of a _B. pertussis_ infection as the date of the last positive specimen minus the date of the first positive specimen (within the same episode of infection) plus 2.5 days to account for gaps between visits; values are expressed as mean \\+ SD. We performed analysis of factors associated with the duration of infection (time-to-event outcome) by using accelerated time failure Weibull regression and analysis of _B. pertussis_ bacterial loads by using IS _481_ Ct as a proxy for bacterial load. We characterized IS _481_ bacterial load as follows: Ct < 34, higher bacterial load; Ct 34–39, intermediate bacterial load; Ct 40–44, lower bacterial load.\n\n【14】We defined an episode of symptomatic _B. pertussis_ infection as illness in a person who had \\> 1 symptoms reported from 1 visit before to 1 visit after the _B. pertussis_ episode. Signs and symptoms included fever, cough, difficulty breathing, sore throat, nasal congestion, chest pain, muscle aches, headache, vomiting, and diarrhea. In 2018, we used the pertussis-specific symptom data collected to determine if positive cases could be classified as pertussis by using the South Africa notifiable medical condition pertussis case definition (cough lasting \\> 14 days, or cough of any duration for children <1 year of age, along with \\> 1 of the following symptoms: paroxysms of coughing, inspiratory whoop, posttussive vomiting, and apnea \\[with or without cyanosis; only for infants <1 year of age\\]).\n\n【15】A cluster of cases included all _B. pertussis_ infections in a single household that occurred within an interval of <7 days. The index case was the first person testing positive within a cluster. We estimated cluster duration as the time from the first day of positivity of the first person in a cluster to the last day of positivity of the last person. We defined household cumulative infection risk (HCIR) as the proportion of subsequent infections within a household after the first PCR-positive case and evaluated HCIR among all households in which \\> 1 household member had _B. pertussis_ infection. The formula for HCIR was the number of infected persons within a household (excluding the index case) divided by number of enrolled persons living within the household. We excluded households that had coprimary cases from the HCIR analysis. We defined crowding as >2 persons sharing a sleeping room within the household.\n\n【16】We used logistic regression to analyze factors associated with symptomatic fraction and HCIR. Pertussis vaccine status was based on number of vaccine doses received by age. For all analyses, we accounted for clustering by study site and households within site by using hierarchical mixed effect models. For the multivariable models, we assessed all variables that were significant at p<0.2 by univariate analysis and removed nonsignificant factors (p \\> 0.05) by using manual backward elimination. We set statistical significance at p<0.05 and performed all statistical analyses in Stata version 14.1 .\n\n【17】##### Ethics\n\n【18】Ethics approval for the Prospective Household cohort study of Influenza, Respiratory Syncytial virus, and other respiratory pathogens community burden and Transmission dynamics in South Africa (PHIRST) was obtained from the University of the Witwatersrand Human Research Ethics Committee, South Africa . Written informed consent was obtained from all enrolled persons, or parents/guardians in the case of minors. A separate ethics application for the pertussis component of the PHIRST was obtained from the University of the Witwatersrand Human Research Ethics Committee, South Africa .\n\n【19】### Results\n\n【20】##### Study Population\n\n【21】During 2016–2018, a total of 1,684 persons from 327 households were enrolled: 542 persons in 2016, 577 in 2017, and 565 in 2018 , as described . Overall, 16.6% (279/1,684) of the participants were <5 years of age, of whom 97.3% (214/220) were fully vaccinated for age with the acellular _B. pertussis_ vaccine. HIV prevalence for the cohort was 15.3% (249/1,628).\n\n【22】##### Specimen-Level Data\n\n【23】There were 122,133 possible individual follow-up visits, from which 105,687 (86.5%) nasopharyngeal swab specimens were collected and tested for _Bordetella_ species . Of the specimens tested, 276 (0.3%) were PCR positive for _B. pertussis_ and 413 (0.4%) were PCR positive for _B. holmesii_ . _B. parapertussis_ was not detected. Of the specimens positive for _B. pertussis_ , 20.3% (56/276) were positive for both IS _481_ and ptx _S1_ , and 79.7% (220/276) were positive for IS _481_ only.\n\n【24】##### Individual-Level Data\n\n【25】##### Incidence and Factors Associated with Incidence\n\n【26】Among 1,684 study participants, 118 episodes of _B. pertussis_ infection were detected in 107 participants; 11 persons had 2 episodes of _B. pertussis_ infection. There were no demographic/clinical differences observed between persons who had 1 or 2 episodes of infection. The incidence of _B. pertussis_ infection was 0.21 (95% CI 0.17–0.25)/100 person-weeks . The highest incidence of _B. pertussis_ infection was observed in persons 5–14 years of age (0.27 \\[95% CI 0.20–0.35\\]/100 person-weeks). Children <5 years of age who had incomplete vaccination for age were more likely to be infected with _B. pertussis_ than were vaccinated children (adjusted incidence rate ratio 4.48, 95% CI 1.38–14.59). Among the 36 enrolled infants, 5 _B. pertussis_ infections were identified in those 9–11 months of age. Of those infants, 4 were fully vaccinated for age; 1 infant (incomplete vaccination) had symptoms (runny nose) during the _B. pertussis_ infection.\n\n【27】##### Duration of PCR-Positive _B. pertussis_ Infection\n\n【28】The mean ± SD duration of _B. pertussis_ infection was 12.0 \\+ 19.1 days. The duration of _B. pertussis_ infection was longer when \\> 2 symptoms were reported (adjusted hazard ratio 0.26, 95% CI 0.18–0.67)  when compared with colonized persons. Duration of infection was longer for participants who had higher and intermediate bacterial loads (Ct values < 34 and 35–39) when compared with persons who had lower bacterial loads (Ct values 40–44; adjusted hazard ratio for Ct < 34, was 0.16 \\[95% CI 0.06–0.44\\] and for wCt 35–39 was 0.41 \\[95% CI 0.22–0.74\\]).\n\n【29】##### Factors Associated with Symptomatic _B. pertussis_ Infection\n\n【30】Of the 107 persons positive for _B. pertussis_ , 34 (31.8%) reported symptoms during any given episode of infection . Common respiratory signs and symptoms reported during an episode of _B. pertussis_ infection were runny nose (55.9%, 19/34), cough (82.4%, 28/34), fever (29.4%, 10/34), and sore throat (14.7%, 5/34). Other, less commonly reported, symptoms were muscle aches, difficulty breathing, chest pain, and vomiting. In 2018, a total of 47.1% (24/51) of persons testing positive for _B. pertussis_ were retrospectively interviewed . A total of 83% (20/24) of these persons reported \\> 1 symptom consistent with pertussis (cough, inspiratory whoop, posttussive vomiting, or apnea) and 60.0% (12/20) of symptomatic persons met the pertussis clinical case definition. Among the symptomatic _B. pertussis_ –positive persons <5 years of age, 67.0% (4/6) were fully vaccinated for age. Persons positive for _B. pertussis_ were 4 times more likely to have symptoms if they were infected for \\> 7 days than persons infected for <7 days (adjusted odds ratio \\[aOR\\] 4.12, 95% CI 1.70–9.98) .\n\n【31】##### Household Cumulative Infection Risk\n\n【32】The overall household cumulative infection risk was 14.4% (43/298 susceptible exposed persons). Transmission was more likely to occur from male index case-patients than female index case-patients (aOR 12.20, 95% CI 1.57–94.96) and persons who had \\> 7 days episode duration than <7 days (aOR 24.79, 95% CI 2.74–224.30) . Within households with confirmed _B. pertussis_ transmission, 38.8% (14/36) of index case-patients transmitting to household contacts were colonized.\n\n【33】### Discussion\n\n【34】In this study population, we found a high incidence of _B. pertussis_ and most infected persons were colonized. We detected 118 episodes of _B. pertussis_ infection in 107 participants, of which 11 persons had 2 episodes. The overall HCIR was 14%, and 39% of index cases in the household were colonized with _B. pertussis_ . The mean duration of PCR-positive _B. pertussis_ infection was 12 days. Persons positive for _B. pertussis_ were more likely to report symptoms if they were infected for \\> 7 days. Transmission was more likely to occur from male index case-patients and persons who had longer episode duration.\n\n【35】In this community cohort study of healthy persons in South Africa, the incidence of _B. pertussis_ was 0.21 cases/100 person-weeks. Incidence in this study was higher than the mean annual incidence risk of 17 cases/100,000 population previously reported in South Africa for persons hospitalized because of pneumonia during 2013–2018 . This difference was probably caused by the fact that the study conducted by Wolter et al. was a cross-sectional study enrolling hospitalized patients who had respiratory illness, whereas our study was a community cohort study enrolling healthy persons. It is difficult to compare our incidence with those of other studies that report incidence data because our study was a community cohort study, whereas other studies focus on populations with _B. pertussis_ disease or populations that had outbreaks of _B. pertussis_ . Thus, more data describing the incidence of _B. pertussis_ among healthy persons over time, and additional longitudinal community data are required for meaningful comparisons to be made. Assessing factors associated with _B. pertussis_ incidence, children <5 years of age who did not receive the full schedule of pertussis vaccine for age were more likely to be infected with _B. pertussis_ than were vaccinated children. Similarly, other studies in South Africa showed that risk for pertussis and risk for hospitalization caused by pertussis decreased when persons were vaccinated .\n\n【36】Data on duration of PCR-positive _B. pertussis_ infection among persons are limited. In our study, the average duration of naturally acquired _B. pertussis_ infection was 12 days and episode duration was longer for participants with higher bacterial loads, similar to findings for influenza virus infection in the same cohort . A human challenge study that induced _B. pertussis_ colonization in healthy adults showed that _B. pertussis_ persisted within the nasopharynx for up to 16 days postinoculation in some of the study participants . However, it should be noted that in the human challenge study, adults had received vaccination within the previous 5 years and were treated for colonization from day 14. In our study, data on vaccination of persons \\> 5 years of age and treatment were missing, and that information could have potentially affected the duration of _B. pertussis_ within the nasopharynx of study participants.\n\n【37】Symptoms associated with _B. pertussis_ PCR-positive cases vary from mild, nonspecific respiratory symptoms to pertussis-specific symptoms. In this study, symptomatic _B. pertussis_ –infected persons reported nonspecific respiratory symptoms, such as runny nose, cough, fever, or sore throat. For the 2018 subset of symptomatic persons who were investigated for _B. pertussis_ –specific symptoms, 60% met the notifiable medical condition pertussis case definition. A study in Spain determining factors influencing the spread of _B. pertussis_ in households found that participants of all ages experienced _B. pertussis_ –specific signs and symptoms, such as cough lasting >2 weeks, paroxysmal cough, posttussive vomiting, inspiratory stridor, apnea, and fever . In South Africa, common signs and symptoms reported in children hospitalized with laboratory pertussis were nonspecific cough and fever .\n\n【38】Evidence suggests that colonization is a major trait of the _B. pertussis_ pathogen. In our study, two thirds of persons infected with _B. pertussis_ did not report any symptoms. _B. pertussis_ incidence data from the United States and the United Kingdom provided evidence of colonization of _B. pertussis_ . de Graaf et al. showed that colonization could be induced by the intranasal inoculation of _B. pertussis_ in a human challenge model . Warfel et al. found that persons previously vaccinated with the acellular pertussis vaccine can become colonized with _B. pertussis_ . There is evidence in the literature supporting colonization of _B. pertussis_ in the nasopharynx of persons, which was also observed in our study. In addition to colonization, we found a high proportion of index case-patients transmitting _B. pertussis_ to other household contacts. However, it is difficult to draw any conclusions from that result because additional longitudinal community data are required for meaningful comparisons to be made.\n\n【39】The transmission dynamics of _B. pertussis_ within households is poorly understood and requires additional research. Within the PHIRST, the overall HCIR was 14%. In 2001, a pertussis outbreak in New South Wales in Australia showed a 22.3% secondary household attack rate  and increased risk for household transmission when the index case-patient began antimicrobial drug treatment >7 days after onset of symptoms . Transmission in our study was associated with male sex, and the index case-patient had a long episode duration ( \\> 7 days). The association between prolonged episode duration and increased transmission is biologically plausible and has been demonstrated for other pathogens, such as influenza virus . Previous studies have not documented a difference in transmission between male and female patients , so this finding should be further explored.\n\n【40】The first limitation of our study is that it was conducted at only 2 sites in South Africa, and results might not be generalizable to other areas and communities within this country. Over the study period, specimens were collected from each person twice a week for a period of 6–10 months. Because we did not collect samples for a full year and therefore could not accurately estimate the incidence over the period of a year, we presented incidence as per 100 person-weeks. Vaccination status was confirmed only for children <5 years of age, so we could conduct analyses regarding vaccination only for this age group. Symptom data collection was poor for 2016 but improved in 2017 and 2018; therefore, the symptom data presented could be an underestimate. The number of _B. pertussis_ cases detected in this study is small, so some statistical analyses were underpowered.\n\n【41】The PHIRST was unique in that it was a community cohort study investigating _B. pertussis_ infection among healthy persons. A major strength of the study was the repeated, longitudinal sampling of persons over time according to a specified schedule, irrespective of symptoms. This study method ensured that the data collected over time were robust against sampling bias and enabled us to identify the large proportion of persons who were colonized with _B. pertussis_ .\n\n【42】In conclusion, in 2 disparate communities in South Africa, we found a high incidence of _B. pertussis_ , and two thirds of cases were colonizations. We have demonstrated that persons who are colonized with _B. pertussis_ can transmit this bacteria to other household members and thus represent a source of transmission to susceptible age groups.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3ef9bf20-d8c6-4e61-89a4-b3ac292c69f9", "title": "Epidemiology of Imported Infectious Diseases, China, 2005–2016", "text": "【0】Epidemiology of Imported Infectious Diseases, China, 2005–2016\nThe advance of globalization, frequent personnel exchanges, and close international trade cooperation make it possible for infectious diseases from all over the world to be imported into China. The number of persons entering and leaving China has been steadily increasing, from 302 million person-times in 2005 to 570 million person-times in 2016, and the number of visitor arrivals in Yunnan Province has increased from 1.5 million (1.0 million foreigners) in 2005 to 6.0 million (4.5 million foreigners) in 2016 . According to the China Statistical Yearbook , the number of inbound tourists from Africa has grown from 238,000 in 2005 to 589,000 in 2016, whereas the number of inbound tourists from Asia has grown from 12.5 million in 2005 to 18.0 million in 2016. The number of migrant workers leaving China rose in the same period, from 272,900 to 494,200. Some infectious diseases have been reemerging, such as epidemic hemorrhagic fever and malaria, which previously had basically been controlled, and wild poliovirus, which had been eliminated in China in 2000. After >10 years without a case of wild poliovirus infection in China, an outbreak of wild poliovirus infection occurred in 2011 in Xinjiang Uyghur Autonomous Region . A few emerging infectious diseases, such as Middle East respiratory syndrome (MERS), yellow fever, and Zika virus disease, have also been imported into China.\n\n【1】Since 2010, annual numbers of autochthonous malaria cases in China have fallen to unprecedentedly low levels; only hundreds of cases now occur in limited areas , whereas the number of imported cases has risen substantially . Malaria is a mosquitoborne infectious disease that caused a heavy health and economic burden in the past. _Anopheles sinensis, An. lesteri_ , _An. minimus_ , and _An. dirus_ mosquitoes are considered to be 4 major malaria vectors in China. As a main vector of transmitting malaria, _An. sinensis_ mosquitoes are present in all provinces and regions except Qinghai and Xinjiang; _An. lesteri_ mosquitoes are found in areas south of 34°N latitude, _An. minimus_ mosquitoes in mountainous and hilly areas south of 33°N latitude, and _An. dirus_ mosquitoes mainly in the tropical jungles of Hainan. In China, _Aedes albopictus_ and _Ae. aegypti_ mosquitoes are 2 major vectors that can transmit flaviviruses and alphaviruses, which cause diseases such as dengue fever, chikungunya, and yellow fever. According to a recent investigation by the Chinese Center for Disease Control and Prevention (CDC), _Ae. aegypti_ mosquitoes are found in 10 cities or counties in Yunnan Province, 7 cities or counties in Hainan Province, 1 city in Guangdong Province, and areas south of the Tropic of Cancer in Taiwan. _Ae. albopictus_ mosquitoes are widely present in 25 provinces in China, in the southeastern parts of a line between Shenyang in Liaoning Province and Motuo County in Tibet .\n\n【2】This study describes the epidemiologic characteristics of imported infectious disease cases during 2005–2016 in mainland China. It also provides scientific information for prevention and control in the future.\n\n【3】### Materials and Methods\n\n【4】##### Surveillance System\n\n【5】In China, since the establishment in the 1950s of the Notifiable Disease Reporting System (NDRS), which was the main communicable disease surveillance system, many disease-specific surveillance systems have been developed as complements to NDRS. In 2004, the National Information Reporting System of Infectious Diseases, an internet-based real-time information reporting technique, was integrated into NDRS to improve case-based reporting. The workflow includes data collection, data management, data use, and report dissemination. Once a case of notifiable infectious disease or emerging infectious disease is identified, it is mandatory for doctors in all hospitals and clinics to report to the network within the statutory period. CDCs at various levels (e.g. provincial) are in charge of data checking, utilization, and timely information feedback by uploading the reports on the websites. All the hospitals and CDCs at different levels can download these reports and know the national status in a timely manner, which makes the epidemic information transparent. The completeness and timeliness of case reporting have improved dramatically since 2004 . Therefore, this study takes 2005 as the first year of analysis, considering data quality and stability.\n\n【6】Once an emerging infectious disease is found that is not yet included in statutory reporting but is of considerable interest, the National Health Commission of the People’s Republic of China usually organizes relevant experts in clinical, epidemiologic, etiologic, and other areas to assess its risk of becoming epidemic, spreading range, influence extent, and social burden to determine whether the disease should be added to the list of notifiable infectious diseases. The results are submitted to the National People’s Congress for deliberation and adoption. At the same time, the emerging infectious disease is required to be reported under “other categories of infectious diseases” in the National Information Reporting System of Infectious Diseases.\n\n【7】##### Data Collection\n\n【8】Using data from the National Information Reporting System of Infectious Diseases (which does not include Hong Kong, Macao, and Taiwan), we performed a retrospective analysis of imported infectious diseases in mainland China during January 1, 2005–December 31, 2016. We selected data according to reporting date, reporting area, and final confirmation. The cases reported in the National Information Reporting System of Infectious Diseases, including laboratory-confirmed cases and clinically diagnosed cases, were diagnosed by clinicians according to the unified national diagnostic criteria .\n\n【9】##### Population and Case Definition\n\n【10】We divided the population into citizens of China and foreign citizens according to their native countries, so the imported cases included not only citizens of China returning from migrant work or other travel abroad but also foreign visitors, expatriates, and migrant workers in China. The major occupational categories of patients were migrant workers, doctors, teachers, students, and retirees.\n\n【11】Local CDC staff ascertained a potential imported case according to the field investigation results after diagnosis. A case in a patient who had visited or lived in an endemic or epidemic area outside China within the longest incubation period before the date of onset was classified as an imported case. Conversely, a case would be classified as a domestic case if there was no evidence of an infection acquired abroad.\n\n【12】##### Statistical Analysis\n\n【13】We entered and managed the data using Microsoft Excel 2010 . We used SPSS 18.0  to describe characteristics of imported infectious diseases regarding geographic and temporal distribution, gender, age, region, or country of origin. We created distribution maps of imported cases using ArcGIS 10.3 .\n\n【14】### Results\n\n【15】During 2005–2016, a total of 31,740 cases of infectious diseases were imported into China. These included 27,497 cases of malaria, 3,351 cases of dengue fever, 773 cases of influenza A(H1N1), 24 cases of Zika virus disease, 18 cases of chikungunya fever, 11 cases of yellow fever, 2 cases of acute flaccid paralysis caused by poliomyelitis, 1 case of MERS, and 65 cases of other infectious diseases . The study revealed that Yunnan Province witnessed the largest number of imported cases of infectious diseases during the study period.\n\n【16】##### Epidemiologic Profile of Imported Cases in Mainland China, 2005–2016\n\n【17】##### Demographic Characteristics of Imported Cases\n\n【18】Most persons with imported cases were male. The median age of patients with imported cases of malaria was 39 years; of patients with dengue fever, 32 years; and of influenza A(H1N1), 20 years . Among these imported cases, 2,470 were reported in foreigners and 29,270 in travelers from China; 18,932 cases were reported in migrant workers from China and 12,808 in persons with other occupations. Migrant workers from China accounted for 65.2% of imported malaria cases and 28.1% of imported dengue fever cases.\n\n【19】##### Trends in Imported Cases in Mainland China, 2005–2016\n\n【20】In 2005, only 2 types of imported infectious diseases were reported in mainland China: malaria and dengue fever. Chikungunya was imported into China in 2008 and influenza A(H1N1) in 2009; Lyme disease has been imported since 2012. Since 2013, the number of types of imported infectious diseases reported has increased each year. A total of 6 types of diseases, including scrub typhus, visceral leishmaniasis (VL), Japanese encephalitis (JE), epidemic hemorrhagic fever, loiasis, and Crimean-Congo hemorrhagic fever, started to be imported in 2013. Imported cases of _Escherichia coli_ O157:H7 infection, schistosomiasis, and MERS were reported in 2015. In 2016, 24 cases of Zika and 11 cases of yellow fever were imported. The types of reported imported infectious diseases increased dramatically after 2013 and reached 11 types in 2016, compared with only 2 types in 2005.\n\n【21】We found that imported cases of malaria showed an upward trend from 2005 to 2016. The imported cases of dengue fever increased year by year; increased sharply in 2013 and 2015, with a peak in 2015 (1,094 cases); and declined slightly in 2016 .\n\n【22】##### Seasonal Distribution of Imported Cases\n\n【23】Many of the main imported diseases in mainland China exhibited seasonality. Most yellow fever cases were imported in March. There was usually a higher incidence of imported malaria during April–August, reaching a peak in May and June. All the JE cases were imported during June–September, whereas incidence of imported dengue fever usually peaked in October . In particular, the cases of Zika were imported mainly through international airports around major festival events, such as the Spring Festival (celebrating the lunar new year), May Day, the Mid-Autumn Festival, and the National Day (October 1) .\n\n【24】##### Spatial Distribution of Imported Diseases\n\n【25】##### Provinces with Imported Diseases\n\n【26】All 31 provinces across the country reported imported malaria ; 36.1% (9,931 cases) were reported in Yunnan Province. The number of imported malaria cases in Yunnan Province was generally decreasing, however, whereas it had been slowly increasing in Sichuan, Henan, Jiangsu, and Zhejiang Provinces and other southeastern provinces year by year. In 2013, imported malaria increased sharply in Guangxi Province, which had the largest number of imported malaria cases in China for that year (1,261/4,067; 31.0%), accounting for 52.7% (1,261/2,394) of the total imported malaria cases in this province during 2005–2016 (data not shown).\n\n【27】Twenty-seven provinces across the country, all except Shanxi, Qinghai, Ningxia, and Tibet, reported dengue fever during 2005–2016. Among all imported dengue fever cases, 42.9% (1,439/3,351) were reported in Yunnan Province (data not shown). Influenza A(H1N1) was imported mainly into Beijing, Guangdong, and other major port cities (data not shown).\n\n【28】##### Origin Region/Country of Imported Diseases\n\n【29】According to our analysis, Africa and Asia were the main regions of origin of imported cases; 15,021 (47.3%) patients came from Africa and 12,581 (39.6%) from Asia . Asia was the main origin of imported dengue fever (3,097 cases, 92.4%), chikungunya, VL, JE, and other diseases. Africa was the main region of origin for imported malaria (14,854 cases, 54.0%), and others, especially _Plasmodium falciparum_ , yellow fever, loiasis, and other diseases .\n\n【30】Myanmar was the main country of origin for 5 imported diseases. These diseases were malaria (7,888 cases, 28.7%), dengue fever (1,384 cases, 41.3%), scrub typhus (6 cases, 85.7%), JE (13 cases, 76.5%), and acute flaccid paralysis (2 cases, 100.0%) (additional data not shown).\n\n【31】Malaria was imported mainly from Africa (14,854 cases, 54.0%) and Asia (9,160 cases, 33.3%). _P. vivax_ came mainly from Asia, especially Myanmar (peaked in 2011), and was introduced into Yunnan Province during 2005–2016. There was an exception, however: Ghana exported more cases of _P. vivax_ malaria to China in 2013. After that introduction, the number of imported cases of _P. vivax_ malaria from Ethiopia, Angola, Equatorial Guinea, and other countries in Africa began to increase slightly. _P. falciparum_ malaria also came mainly from Asia, especially from Myanmar, which exported it into Yunnan Province until 2013. In 2013, the largest number of cases of _P. falciparum_ malaria came from Ghana; thereafter, most cases were imported from Angola, Nigeria, Equatorial Guinea, Ghana, and other countries in Africa, mainly into Guangxi, Jiangsu, and other southeastern provinces in China (data not shown).\n\n【32】Imported dengue fever cases were mainly from Asia, especially from Myanmar; most of them were brought into Yunnan and Guangdong Provinces. Influenza A(H1N1) was imported mainly from the United States (141 cases, 18.2%) and Australia (111 cases, 14.4%). The 1 case of MERS was imported from South Korea in 2015. All 11 imported cases of yellow fever came from Angola. Of the 24 imported Zika cases, 70.8% (17 cases) were from Venezuela.\n\n【33】### Discussion\n\n【34】Many infectious diseases, such as _P. vivax_ malaria, dengue fever, influenza A(H1N1), epidemic hemorrhagic fever, JE, chikungunya, Lyme disease, scrub typhus, and VL, are found frequently in the indigenous population of China, but some are found more often as imported diseases, such as _P. falciparum_ malaria, yellow fever, Zika virus, and MERS. Imported _P. falciparum_ malaria is a major obstacle to achieving malaria elimination in China . This study showed that malaria was the most frequent imported infectious disease during 2005–2016, and Yunnan was the province with the greatest number of cases of imported malaria, which was consistent with other relevant studies in China . The reason for the large number of imported cases is that Yunnan Province has long international borders with Myanmar, Laos, Vietnam, and other countries of the Greater Mekong Subregion that show a high incidence of malaria . The persons who cross these borders to enter or leave China increase opportunities for infectious diseases to be imported from adjacent countries , especially from Myanmar .\n\n【35】_P. falciparum_ malaria was imported mainly from Ghana and _P. vivax_ malaria mainly from Asia, which was consistent with Zhou's findings . The number of cases of _P. falciparum_ malaria imported into Guangxi Province increased significantly in 2013; this increase can be attributed mainly to cases imported from Ghana in 2013, which was related to people working in Ghana and other Africa countries during a gold rush in that year .\n\n【36】Africa and Asia were the main origins of imported malaria and other mosquitoborne diseases, findings consistent with those of Tian et al. First, as a result of the rapid development of international economic exchange, trade, and travel, the number of migrant workers from China in Africa and Asia has increased annually in recent years. Second, climate and sanitary conditions in Africa and Southeast Asia are suitable for mosquitoes. Migrant workers from China in these areas are engaged mostly in outdoor field work, and their working and living environments are not mosquito preventive. Therefore, they have greater risk for infection . In addition, their health education level and self-protection awareness are low. All these factors have resulted in an increased number of malaria and other mosquitoborne diseases . Therefore, the main challenges of eliminating _P. falciparum_ malaria are curtailing border malaria and imported cases from Myanmar and countries in Africa . Cooperation between China and neighboring countries has played an important role in improving malaria control at cross-border areas and should be further strengthened.\n\n【37】This study revealed that imported cases of infectious diseases such as malaria, dengue fever, and chikungunya were more common in male youths, which is consistent with the findings of Jiao et al. Industries that use labor from China generally include construction, manufacturing, and transportation; therefore, the proportion of young men was high among migrant workers from China, to meet the needs of these industries.\n\n【38】Because of the distributions of mosquitoes and other vectors in China, diseases such as dengue fever, _P. vivax_ malaria, JE, epidemic hemorrhagic fever, and chikungunya are acquired primarily locally. However, the numbers and the types of imported infectious diseases reported have increased in recent years. We indicate 4 main reasons for this increase. First, globalization is a major factor. With growing economic globalization, ongoing development of international trade, and convenient and fast transportation in recent years, factors such as microbial mutation, global warming, floods, droughts, and natural migration of animals as disease vectors have increased the risk of disease introduction into China . Second, these increases are related to the increases in the number of migrant workers from China, as well as increases in the number of tourists. This study showed that migrant workers from China accounted for a large proportion of imported malaria. Migrant workers and tourists are more at risk than local residents because of lack of preexisting immunity that dramatically increases the chances that they could become infected with these diseases and bring them in from abroad . Third, the level of surveillance, diagnosis, and detection of emerging or reemerging infectious diseases improved in China during the study years, which potentially contributed to the increase in the number and the types of cases with a confirmed diagnosis. Finally, some of the increases were caused by newly discovered or emerging infections, such as MERS, which was discovered in 2012 and confirmed in 2013; the 1 case imported into China was a result of the large outbreak in South Korea in 2015.\n\n【39】This study has analyzed the epidemiologic characteristics of all imported infectious diseases in mainland China over a recent 10-year period to provide scientific guidance for control and prevention of imported diseases in China. The study also has limitations. First, there is currently no special reporting system for imported infectious diseases in China. The imported cases that were analyzed in this study were reported through the National Information Reporting System of Infectious Diseases and identified by local CDC staff through epidemiologic investigation; therefore, not all imported cases may have been identified. A second limitation is that the data of this study were obtained from the monitoring system and there are few denominator data to use for calculating the rates of imported infectious diseases, so conclusions are limited and risks cannot be calculated. Finally, the factors concerning imported infectious diseases are complex, whereas our data are limited. Some demographic characteristics; trip information, such as purpose and duration of travel; and climate, environmental, and other relevant information could not be obtained. In subsequent research, we will focus on the collection of more information and continue to conduct our in-depth study of imported infectious diseases.\n\n【40】To control and prevent imported infectious diseases, we recommend several measures. First, pretravel education targeting infectious diseases that are endemic or prevalent in the destination countries is key for the prevention of imported diseases, especially imported malaria. Such education includes pretravel special training sessions; distribution of pamphlets or leaflets; and posttravel tips via notices, banners, or scrolling electronic screens at international entry and exit ports. Also important are dispensing of free preventive medications for malaria prophylaxis, self-treatment of severe travelers’ diarrhea, and vaccination against vaccine-preventable diseases such as JE and yellow fever during international travel and residence .\n\n【41】Second, border screening should be strengthened and improved. The effectiveness of border screening is controversial , but in recent years, China’s screening practices have proven to be a crucial measure for discovering imported infectious diseases in travelers who are ill at the time they cross a border. Screening has played a major role in prevention as the first line of defense against importation of foreign infectious diseases. For example, among 25 cases of Zika virus disease imported into mainland China in 2016, border screening recognized 9 , and of 11 cases of imported yellow fever reported in 2016, border screening recognized 6 . Border screening should be strengthened and improved to prevent those epidemic diseases and others from entering China .\n\n【42】Third, as a second line of defense against imported diseases, fever clinics and primary clinicians need to play an active role in identifying patients with imported infectious diseases. At present, many problems exist in fever clinics set up in medical institutions, such as failure to meet requirements, unqualified procedures, or nonstandardized management, and some locations may even be out of service. Clearer management standards are needed to provide a better chance for infectious disease screening. Training of medical staff, especially primary clinicians, should be reinforced to improve the ability to identify, diagnose, and treat emerging or reemerging infectious diseases and to ensure that imported cases can be diagnosed and control measures can be implemented as early as possible to prevent these diseases from further spreading in China.\n\n【43】Fourth, multisectoral and regional cooperation mechanisms, especially international cooperation mechanisms in the border areas, should be further enhanced . We suggest that the relevant departments should intensify cooperation by using well-defined responsibilities and should improve communication regarding all aspects of public information sharing, training, monitoring, and control. It is critical that, in the border areas, neighboring countries develop the management of persons entering and leaving, improve the control of mosquitoes, and jointly respond to infectious diseases.\n\n【44】Fifth, it is necessary to improve the early warning and response capacity for emerging infectious diseases. We recommend establishing a special system of surveillance, risk assessment, and early warning. Spatiotemporal models linking disease data and different environmental factors are also urgently needed .\n\n【45】In summary, our study found that the numbers of emerging infectious diseases imported into China have increased year by year. Therefore, we must pay closer attention to prevention and control of imported cases, while preventing and controlling indigenous cases. These factors are crucial for preventing and controlling infectious diseases, such as _P. falciparum_ malaria, that have a large number of imported cases and seriously hinder the process of China eliminating malaria and other diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5e39622e-32e1-4a24-8fc1-84f9f98e3b76", "title": "Mosquitoborne Sindbis Virus Infection and Long-Term Illness", "text": "【0】Mosquitoborne Sindbis Virus Infection and Long-Term Illness\nMosquitoborne viruses such as chikungunya virus, Ross River virus, and Sindbis virus (SINV) are members of the genus _Alphavirus_ (family _Togaviridae_ ) and cause human arthritic diseases . SINV has mainly been reported in northern Europe and South Africa ; Sweden has an average of 3 SINV cases per year, with occasionally more cases in a previously defined endemic region in central Sweden  . Birds are the reservoir for SINV, and there is no evidence of human-to-human transmission. SINV infection in humans, called Ockelbo disease in Sweden, causes rash, arthritis, and mild fever . Most patients recover within weeks or months, but arthralgia and myalgia can persist for years following infection, suggesting inflammatory response or a persistent infection .\n\n【1】In mid-August 2013, several patients with rash, arthralgia, and fever visited the healthcare center in the small village of Lövånger in Västerbotten County, Sweden . The university hospital laboratory in Umeå received 172 blood samples from patients with suspected SINV infection; 50 patients had SINV-specific IgM and IgG . SINV infections have been believed to be almost exclusively confined to the central part of Sweden , but the 2013 outbreak occurred north of the endemic area . The distribution of verified cases by sex in this outbreak showed a higher proportion of female patients (62%) than male patients (38%) with acute SINV infection .\n\n【2】Previous reports suggested that joint symptoms might persist for years in SINV infections . To evaluate long-term consequences, we contacted 46 SINV patients by telephone 3–4 months, 6–8 months, or in both periods after acute disease . Our study was approved by the Regional Ethics Review Board (2014-102-32M), and we obtained written informed consent from all participants. We include details of the study results summarized here in the Technical Appendix . In total, 18/46 (39%) of the patients reported persistent musculoskeletal pain (arthralgia and myalgia) and restriction in their daily activity 6–8 months after the onset of acute symptoms . We invited these patients for a standardized examination by a rheumatologist, including a health assessment questionnaire, patient assessments of their pain and their global health using a visual analog scale, and a blood sample . Of 17 symptomatic patients who participated, 1 had arthritis in the ankle, 14 had \\> 1 tender joint, and 10 had enthesitis, tendinitis, or tenosynovitis at examination. Large joints (knee, hip, shoulder, wrists, ankles) and small joints (toes, fingers) were affected, with a predominance for the lower extremities, in contrast to other studies in which small and peripheral joints were mainly affected . Patients graded their global symptoms as more severe than the examining doctor did, indicating that joint function was only mildly affected whereas the pain was perceived as restricting. Test results did not detect citrulline antibodies, and the single patient with positive rheuma factor had no arthritic symptoms. A notable finding was that 4 patients (24%) had psoriasis, a condition present in <4% of the northern European population , raising the question whether psoriasis makes SINV patients more vulnerable to long-term arthralgia.\n\n【3】We asked the 28 patients at the 6- to 8-month follow-up who had recovered to complete a questionnaire and donate a blood sample at their local healthcare provider; 23 did so. Symptomatic patients reported more pain and impaired health, compared with patients who were asymptomatic 6–8 months after acute disease. We detected SINV-specific IgM in patients with and without persistent symptoms, as previously reported .\n\n【4】We isolated a new SINV strain from a mosquito caught in the area during the outbreak; it was most closely related to a SINV strain from Finland . However, no increased incidence was recorded in Finland either the year before or concomitantly with the Swedish outbreak . In addition, only 1 case was reported from the endemic area of central Sweden in 2013, suggesting that local factors such as weather conditions may determine an outbreak. June 2013 stood out with a high mean temperature and precipitation, which have been shown to be associated with a high incidence of SINV infection later in summer .\n\n【5】A recent study in northern Sweden revealed that in a randomly selected population-based cohort, 2.9% had SINV IgG , indicating that the virus was present in the region, although not recognized . More research is warranted regarding the long-lasting joint pain caused by a previous SINV infection; patients with undiagnosed SINV may visit a healthcare facility with such symptoms even several months postinfection. Our report illustrates how a vectorborne zoonotic disease can result in a large, unexpected outbreak. The key factors for outbreaks of SINV or other alphavirus-caused diseases are generally unknown, which warrants further investigations, especially in light of the global emergence of alphaviruses .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a9d160d3-9490-4417-b9ad-38dd05acfcde", "title": "Human Bocavirus in Patients with Encephalitis, Sri Lanka, 2009–2010", "text": "【0】Human Bocavirus in Patients with Encephalitis, Sri Lanka, 2009–2010\nEncephalitis is a serious infection causing high rates of illness and, in industrialized countries, has a case-fatality rate of 6.5%–12% . However, the situation in developing countries is largely unknown. Globally, the causes remain unrecognized in 60%–85% of encephalitis cases . Recently, human bocavirus (HBoV) has been implicated in causing life-threatening encephalitis in Bangladeshi children . In Sri Lanka, information about the causative agents of encephalitis is scarce. The aim of this study was to determine the occurrence of HBoV and other possible pathogens in children and adults with encephalitis admitted to a tertiary care hospital in Sri Lanka.\n\n【1】### The Study\n\n【2】The study was conducted at Colombo North Teaching Hospital, Ragama, Sri Lanka, during July 2009–November 2010. A total of 233 patients (110 adolescents/adults \\> 12 years of age and 123 children) were enrolled. Adolescents and adults were admitted to adult wards. Cerebrospinal fluid (CSF) samples were available from 191 patients. Criteria for enrolment were as follows: any combination of the triad of fever, headache, and vomiting, along with altered level of consciousness, seizures, focal neurologic deficits, altered behavior, and signs of meningeal irritation. Clinical and laboratory information was available for 164 patients. The male:female ratio for adolescents/adults was 1.3:1; ages ranged from 12 to 90 years (mean 42 years); For children, the male:female ratio was 0.7:1; ages ranged from 2 to 144 months (mean 48 months). The ethics committees of the University of Kelaniya and Oita University approved this study.\n\n【3】CSF samples were subjected to macroscopic examination, total and differential leukocyte counts, bacterial culture, Gram staining, and measurement of protein and glucose. Blood was cultured for bacteria and examined for total and differential leukocyte counts, erythrocyte sedimentation rates, and hemoglobin and C-reactive protein levels.\n\n【4】Classical encephalitis-causing pathogens  and diarrheagenic viruses, such as HBoV, rotavirus, astrovirus, norovirus, parechovirus, and human adenovirus (HAdV), were determined in CSF by PCR  . Anti- n \\-methyl-D-aspartate receptor (NMDAR) encephalitis was diagnosed by on-cell Western analysis . For HBoV PCR-positive patients, HBoV types 1–4-specific IgG and IgM responses in CSF samples were measured by enzyme immunoassays .\n\n【5】Nucleotide sequences of all amplicons were determined to confirm the PCR products, to distinguish genotypes, and to perform phylogenetic analysis . BLAST analysis  was used to identify the viruses and genotypes. Multiple sequence alignment was conducted by using ClustalW2 . The phylogenetic analysis was done with a neighbor-joining tree by using MEGA5 . A bootstrap analysis of 1,000 replicates was performed to test the reliability of the branching pattern.\n\n【6】The causes of encephalitis were type 2 dengue virus in 1 (0.5%) patient, human echovirus (HEcoV) type 9 or 25 in 2 (1%), HBoV  in 5 (3%), and HAdV 41 in 7 (4%): all were sole detections. None of the other viruses and no bacteria were detected. Samples positive for HBoV by primers designed from viral protein 1/2 also were positive by primers designed from nonstructural protein (NP) 1 gene. HEcoV was detected in 2- and 9-year-old children. HAdV 41 was not confined to children; ages of infected patients ranged from 13 months to 55 years. Of 81 CSF samples, anti-NMDAR encephalitis was detected in 2 (2%) adults (42 and 72 years of age). All patients in this study recovered and were discharged, except for one 13-month-old boy with HAdV 41 encephalitis who left the hospital against medical advice.\n\n【7】The severity of symptoms in the HBoV-positive patients did not differ from those of patients with other infections. None of the patients who had positive PCR results for HBoV1–3 had corresponding HBoV1–4 IgM or IgG in their CSF. Phylogenetic analysis  of the viral protein 1/2 gene showed that the Sri Lanka HBoV1 strains did not subcluster with encephalitis-associated Bangladesh strain, although they had 97%–98% nt identities. The Sri Lanka HBoV1 strains had 98%–99% nt identities among themselves and with other HBoV1 strains. The Sri Lanka HBoV2 strain was closely related to the Tunisia strain (96% nt identity). The Sri Lanka HBoV2 had 90%–91% nt identities with the Bangladeshi encephalitis-causing strains and 90%–96% nt identities with other HBoV2 strains. The Sri Lanka HBoV3 strain was closely associated with the cluster formed by viruses from the United Kingdom, Australia, Tunisia, and China and had 96%–97% nt identities with those strains. The sequence of NP1 gene is conserved and had 98%–100% nt identities among the Sri Lanka strains.\n\n【8】### Conclusions\n\n【9】The study in Bangladesh suggested that HBoV-associated encephalitis might be restricted to malnourished children . However, our study demonstrates that HBoV also can be detected in well-nourished children and adults with encephalitis. How HBoV might trigger encephalitis is unclear. HBoV viremia has been documented, and the virus might therefore have the potential to cross the blood–brain barrier. The NP1 of HBoV inhibits interferon-β production by the host, suggesting evasion of the innate immune response during infection .\n\n【10】Unlike the Bangladesh study, where 2 of 4 encephalitis patients in whom HBoV was detected died , all patients in our study recovered. In addition to HBoV1 and HBoV2, we detected HBoV3 in a child with encephalitis, which to our knowledge, has not been reported as a cause of the disease. Although HBoV infections occur mainly in children, among the 5 Sri Lanka patients with HBoV encephalitis, 3 were adults or adolescents. None of the patients with HBoV encephalitis had HBoV IgM or IgG in their CSF, indicating how rapidly disease onset occurred and how little time the immune system had to respond. Generally, the specific seroprevalence rate of HBoV1 antibodies in infected persons is 59%, followed by HBoV2, 3, and 4 (34%, 15%, and 2%, respectively) .\n\n【11】Our detection rate of viruses as a cause of encephalitis was 7.5%, and adding anti-NMDAR encephalitis, the detection rate increased to 10%, which is similar to that of another study . Anti-NMDAR encephalitis is becoming a dominant cause of encephalitis in certain population ; however, in Sri Lanka, it is 1%–4%, similar to other studies .\n\n【12】Dengue virus is the leading endemic cause of encephalitis in Brazil . This infection is also endemic to Sri Lanka and, before our study, dengue encephalitis was suspected but unconfirmed in the population. Enteroviruses frequently cause CNS infection, and the HEcoV 9 and 25 found here are known to cause encephalitis .\n\n【13】Among the HAdVs, serotype F is mainly responsible for gastroenteritis, whereas encephalitis is caused mainly by serotypes B, C, and D . The large number of HAdV 41 encephalitis cases indicates a unique epidemiology in Sri Lanka.\n\n【14】Herpes simplex and varicella-zoster viruses are implicated as the major causes of encephalitis. However, these viruses were not responsible for encephalitis in our study or in the studies in Bangladesh. HBoV is dominant in both Bangladesh and Sri Lanka. The limitation of our study is that causation could not be proven by the presence of HBoV antibody during infection or the absence of HBoV DNA in the CSF when recovered. The HBoV DNA detected in our study may represent persistent DNA from past infection; however, history of recent respiratory or diarrheal infection was absent. Future studies using quantitative PCR and serology are warranted to better establish the etiologic role of HBoV infection and encephalitis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "02fa7d99-6d5a-4911-b6ad-5839a89ec0c0", "title": "Considering Trends in Sodium, Trans Fat, and Saturated Fat as Key Metrics of Cardiometobolic Risk Reduction", "text": "【0】Considering Trends in Sodium, Trans Fat, and Saturated Fat as Key Metrics of Cardiometobolic Risk Reduction\nThe 2 articles by Urban and colleagues published this week in _Preventing Chronic Disease_ report 15-year trends in sodium, _trans_ fat, and saturated fat, 3 food components associated with increased risk for cardiovascular disease and obesity, in frequently ordered meal items (French fries, cheeseburgers, grilled chicken sandwiches, and regular cola) from leading US national fast food chain restaurants . These longitudinal findings track these 3 food components in foods that are frequently consumed by Americans. In recent surveys, almost half of Americans report eating fast food at least weekly , and similarly, nearly half report drinking soda daily . The findings by Urban et al confirm a substantial reduction in the content of _trans_ fat and saturated fat in French fries but not in cheeseburgers or chicken sandwiches. Changes were inconsistent in sodium, saturated fat, and calories among food products, with the exception of sodas, where there was an increase in portion size. The authors conclude that, unlike the reduction observed in artificial _trans_ fat in French fries, the content of sodium, saturated fat, and calories in the selected foods did not change much. Taken together, these findings indicate that little improvement has been made in the quality or energy density of popular fast food products and suggest the need for interventions to improve population health.\n\n【1】It is important to consider these findings in the larger context as public health researchers, practitioners, and policy makers develop and implement interventions to reduce intake of excessive calories, saturated fat, and artificial _trans_ fat. Cheeseburgers, French fries, and a soda represent a quintessential part of American culture. Banter about them was central to the _Saturday Night Live_ skit made famous by the late John Belushi. Similarly, songs made popular by performers such as Jimmy Buffett, Charlie Pride, the Gang of Four, and the Village People are all about having a cheeseburger, French fries, and a soda. These staples of the American diet are unlikely to disappear. However, central to American food choices is an unacceptably high prevalence of diet-related risk factors that compromise the health of Americans and contribute to the high costs of chronic disease. During the period examined by Urban and colleagues, the late 1990s through 2013, the US prevalence of chronic disease risk factors such as overweight, obesity, and hypertension have remained high, cardiovascular disease remains the leading cause of death, and prevalence of prediabetes and diabetes continues to increase . The continued popularity of fast food restaurants and continued high prevalence of diet-related risk factors remind public health researchers, practitioners, and policy makers that there is much that needs to be done.\n\n【2】The findings of the 2 studies by Urban and colleagues present concrete evidence on an issue that many may consider obvious: the paradigmatic meal of a cheeseburger with French fries and a soda still contains components whose consumption is associated with increased risk of adverse health outcomes. Changes in portion size can change the amount of these components consumed, as with the increasing portion size observed for regular soda, which can increase energy density.\n\n【3】In the past several years, increased public health efforts have sought to implement and evaluate interventions to increase access to healthy food options through farmers markets, community gardens, and other venues that make the healthier choice the easier choice whenever possible . Changes in subsidy programs and other structural changes have been implemented to remove access barriers, such as remediation of food deserts or placement of farmers markets in high-traffic areas, such as near subway stops. Many evaluation efforts have focused on measuring consumer awareness and purchasing behavior as surrogates for consumption . Few studies have evaluated changes in key outcomes, including consumption patterns, changes in risk factors, or changes in health outcomes. These 3 outcomes are challenging to study because they are hard to measure, the time horizon to document an effect is long, and the attribution to a specific intervention is difficult to make. Researchers, practitioners, and policy makers need to work together to develop creative methods to study these important outcomes. Evaluating effect and return on investment of these interventions is critical to identify which interventions will make a lasting difference in the health of the population. The experience with _trans_ fat suggests that regulatory interventions, including menu labeling in restaurants and vending machines, to be implemented in 2015 , will improve the food environment. New federal nutrition policy, including the 2015 update of the _Dietary Guidelines for Americans_ , will help shape the environment to support healthier choices. At the same time, there is increased public concern about the healthfulness of fast food and growing support for efforts to increase access to healthy food options, particularly in schools. Taken together, expanded policy levers in concert with increased consumer awareness and demand for healthy food products will accelerate improvement in the US diet and food environment and support reductions in the population’s cardiometabolic risk.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "537b0929-f59f-4e66-8efc-61c632d0da7b", "title": "“How Comes It, Rocinante, You’re so Lean?” “I’m Underfed, with Overwork I’m Worn”", "text": "【0】“How Comes It, Rocinante, You’re so Lean?” “I’m Underfed, with Overwork I’m Worn”\nBartolomé Esteban Murillo  The Young Beggar (c. 1650) Oil on canvas (1.34 m × 1 m) Photo: C. Jean. Louvre, Paris, France. Réunion des Musées Nationaux/Art Resource, New York, NY\n\n【1】“There are only two families in the world, the Haves and the Haven’ts,” wrote Miguel de Cervantes  at a time he viewed as “no golden age” in his native Spain . A brilliant satirist, Cervantes ridiculed the socially divisive mores of a gilded imaginary past held onto for too long and seeming all the more incongruous amidst the poverty and oppression of his own life.\n\n【2】Spain in the 17th century, its empire collapsed and population ravaged by three plague epidemics, was embroiled in conflict abroad and royal mismanagement at home. Its misadventures, chronicled in the plays of Lope de Vega and Calderón, also fueled the genius of Diego Velázquez, Francisco de Zurbarán, Jusepe de Ribera, and Bartolomé Esteban Murillo. They, too, advanced the cultural front by constructing from the rabble a Spanish school of painting for the ages.\n\n【3】Spanish baroque, as the school came to be known, expanded on similar art movements in the Netherlands and elsewhere in Europe. Immortalizing royals and street peddlers alike, it left a vivid record of the times. Among the greats of this period, Murillo, a native of Seville known as much for his good character as his artistic talent, painted the indigent and populated lofty religious scenes with ordinary human faces.\n\n【4】Orphaned in childhood, Murillo was raised by relatives . He apprenticed under Juan del Castillo, a leading painter of the 1630s and 1640s who, along with Alonso Cano, influenced his early work. He left this apprenticeship to go into business for himself, creating popular pictures for quick sale. As a street artist, he saw the poorest in the city, so his early paintings were sympathetic portraits of the ragged boys and flower girls of Seville. His break came with a commission to paint 11 works for a local Franciscan monastery. These paintings brought him fame, and soon he was affluent enough to marry well.\n\n【5】He traveled to Madrid, where he likely became familiar with the work of Velázquez, fellow Sevillian and painter to the king, and the work of Rubens, Titian, Veronese, and Tintoretto in the royal galleries. He returned to his hometown to found the Seville Academy of Art, paint major works, and enjoy great popularity, even outside Spain . His mature paintings were mostly religious. Some were genre scenes with children, a novel theme. A few were fine portraits. All attained such lightness and refinement they were labeled “vaporous.”\n\n【6】Even now, a “Murillo” stands for a good painting in Spain. The artist made an impression, although his fortunes faltered over time. His works sold so well that the king limited their export. But they were copied too often. Poor imitations in a flooded market damaged his reputation, especially since he never signed or dated his paintings.\n\n【7】One legend has it that Murillo died poor; another, that he gave his wealth to charitable causes. His last work, the Espousal of St. Catherine, commissioned for a Capuchin monastery in Cadiz, was not completed. The painter fell from the scaffold and was severely injured. He was taken back to Seville, where his death was attributed to these injuries.\n\n【8】The Young Beggar, on this month’s cover, exemplifies Murillo’s technique. It showcases the masterful brushwork, use of chiaroscuro, meticulous attention to naturalism, and gentleness toward his subjects that attracted the attention of later artists, among them 18th- and 19th-century’s Sir Joshua Reynolds, John Constable, and Édouard Manet.\n\n【9】“Masterless children,” a common sight in Seville, were pitied and loathed by local society, which gave them alms with one hand and dismissed them with the other. Survival and socialization on the street suited them for servile tasks, which inevitably led to adult criminal opportunities, gambling, and prostitution . In 1593, the city was “full of small boys who wander about lost and begging and dying of hunger and sleep in doorways and on stone benches by the walls, poorly dressed, almost nude, and exposed to many dangers…and others have died of freezing by dawn” .\n\n【10】An unpublished parish-by-parish survey of the “honorable poor” of Seville in 1667 found that untended children aggregated in courtyards and abandoned buildings. Exposed to weather and pests, unwashed and malnourished, they were susceptible to infectious diseases, from ringworm to bubonic plague. When, against all odds, it snowed twice in 1624 and 1626, they were decimated. One study in the parish of San Bernardo showed that 27% of burials in 1617 to 1653 were of children.\n\n【11】Seville’s population declined in the 17th century. A 1679 silk merchants’ quarter survey showed that 40% of the buildings were vacant. Two serious plague epidemics and extensive emigration to the New World reduced the number of children. Charity, documented in literary works and prominent in the paintings of Murillo, was thought as noble and pious, but the ugliness of want (festered wounds, parasites, filth) so vivid in these paintings exposed the dark side. The poor had to register for licensed begging to be protected from impostors that would take “alms from the truly poor who cannot work” . City authorities used charitable funds to create “hospitals” for vagrants to be put away from the public eye.\n\n【12】A slant of light reminiscent of Caravaggio  illuminates Murillo’s Young Beggar, allowing a glimpse of his life. Weary and resigned, the boy leans against the wall, his belongings strewn along with the remnants of a meal, scraps of fish and rotting fruit. He is delousing himself. The mild demeanor of the child is punctuated by the furrowed brow and soiled feet. Like Rocinante, he is underfed, overworked, and riddled with pests.\n\n【13】“The Haves and the Haven’ts” are still at it. The Haves contributing to charity, the Haven’ts flooding the streets, some 100 million of them around the globe: migrant workers in People’s Republic of China and elsewhere, homeless in Los Angeles, Bristol, or Marseille. Authorities still sort the truly and deserving poor from impostors, while the ugliness of want (lice, tuberculosis, flu, HIV/AIDS, hepatitis, diphtheria) is not shrinking from public view .\n\n【14】The pathos in Murillo’s Young Beggar lies in the child’s complete abandon and his charm, unblemished by the circumstances. Well addressed in literature and art, the plight of the poor, its abatement and elimination, is a main concern of public health. Armed with time-honored public health practice: health education, prescription and device distribution, tuberculosis screening and treatment, improved hygiene, ivermectin use against scabies and body louse infestation, and systematic immunization, the public health worker can now join other dreamers championed by Cervantes in declaring, “ ‘My armour is my only wear, / My only rest the fray’ ” .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "923b85fb-8a2b-418e-8d95-8bf77edf3547", "title": "Achieving Excellence in the Practice of Chronic Disease Epidemiology", "text": "【0】Achieving Excellence in the Practice of Chronic Disease Epidemiology\n*   CDC’s State Chronic Disease Epidemiology Assignee Program\n*   Accomplishments of Chronic Disease Epidemiology Field Assignees\n*   Past, Present, and Future Capacity-Building Efforts\n*   Acknowledgments\n*   Author Information\n*   References\n\n【1】Heart disease, diabetes, cancer, arthritis, and other chronic diseases are the leading causes of death and disability and the leading drivers of health care costs in the United States . Health disparities and inequalities exist across chronic diseases, behavioral risk factors, environmental exposures, social determinants, and health care access by sex, race and ethnicity, income, education, disability status, and other social characteristics . A white paper developed by the Council of State and Territorial Epidemiologists’ (CSTE’s) Chronic Disease Epidemiology Capacity Building Workgroup stated that for 3 of the Essential Public Health Services — surveillance, communication, and consultation — chronic disease epidemiologists (CDEs) perform functions that are critical to health departments . Collecting, analyzing, interpreting, and disseminating data on chronic diseases and related risk factors is vital to understanding and raising awareness about morbidity, mortality, associated costs, and disparities. These data are also vital inputs throughout the process of implementing evidence-based public health approaches to reduce the burden of chronic diseases in the United States.\n\n【2】Chronic disease surveillance is changing, with new priorities that are more upstream, more clinical, more cross-cutting, and more granular than previous priorities; new data sources, such as electronic health records, to supplement traditional sources; and new technologies. Today’s state, territorial, local, and tribal CDEs increasingly need to be strategic, innovative, collaborative, and efficient while wearing many hats and taking on leadership roles: statistician, informaticist, demographer, cartographer, evaluator, communications specialist, privacy officer, strategist, convener, and others. CDEs need to expand partnerships across multiple sectors to leverage data and resources to address social, environmental, and economic conditions that affect health and advance health equity. Timely and locally relevant data, metrics, and analytics are of utmost importance in this work to guide, focus, and assess the effect of prevention initiatives, including those targeting the social determinants of health and enhancing equity . Concurrently, chronic disease surveillance is challenged by data gaps, limitations in data access and timeliness, increases in data collection costs, decreases in funding, and inadequate staffing. The CSTE’s _2017 Epidemiology Capacity Assessment Report_ enumerated 304 CDEs in all 50 states and the District of Columbia . Survey respondents from the 51 jurisdictions indicated a need for 137 additional CDEs (a 45% increase) to reach full capacity, and most (88%) jurisdictions indicated a need to improve capacity in the Essential Public Health Services in chronic disease epidemiology .\n\n【3】The public health structure varies across states, and many state public health agencies provide epidemiological technical assistance and resources to local public health agencies. The size, resources, and other demands of local public health agencies might prohibit the hiring of dedicated CDEs or even the ability to have general epidemiologists perform chronic disease epidemiology and surveillance services. In 2016, the National Association of County and City Health Officials conducted a study on the funding, workforce, programs, and partnerships at local public health agencies; 1,930 local public agencies responded to the study survey . The survey showed that 49% of local public health agencies directly provided chronic disease epidemiology and surveillance services in the past year; this percentage ranged from 44% to 65% according to the size of the population served: 44% for small populations (<50,000), 56% for medium populations , and 65% for large populations (≥500,000) . Increasing the number of CDEs to build capacity and enhance expertise in surveillance, communication, and consultation is critically important. The Centers for Disease Control and Prevention’s (CDC’s) State Chronic Disease Epidemiology Assignee Program aims to address the workforce shortage of CDEs in states.\n\n【4】CDC’s State Chronic Disease Epidemiology Assignee Program\n\n【5】Since 1991, the CDC’s State Chronic Disease Epidemiology Assignee Program has helped states build chronic disease epidemiology capacity by placing a CDC employee (hereinafter referred to as field assignee) in a state or local public health agency. Field assignees assist states by providing epidemiologic consultation and leadership for surveillance systems; offering expertise in designing epidemiological studies, analyzing data, evaluating chronic disease prevention and health promotion programs, and disseminating findings; providing data and identifying priority populations for public health program planning; and mentoring and training entry-level and mid-level CDEs and other staff members in epidemiologic methods and data interpretation.\n\n【6】To date, CDC’s State Chronic Disease Epidemiology Assignee Program has benefited 36 states and New York City during its 28-year history . Field assignees have served in their state position for up to 12 years. Currently, the program has 4 field assignees; they are in Arizona, Colorado, Illinois, and Indiana. The field assignees’ work has directly enhanced chronic disease epidemiology capacity, and CDC has provided forums and training (eg, introduction to CDC surveillance systems, Evaluation 101, geospatial data methods, Behavior Risk Factor Surveillance System weighting methodology, public health law, legal epidemiology) for the field assignees and state CDEs. Field assignees serve as a liaison between the state or local public health agency and CDC. As a CDC employee, field assignees have access to CDC subject matter experts, training, data sets, analytic software, and an electronic library for broad access to the scientific literature, which can help supplement state resources and further contribute to statewide capacity in the practice of chronic disease epidemiology.  \n\n【7】States that have hosted an assignee through the Centers for Disease Control and Prevention’s State Chronic Disease Epidemiology Assignee Program, 1991–2018. The program has benefited 36 states and New York City. \n\n【8】Accomplishments of Chronic Disease Epidemiology Field Assignees\n\n【9】Field assignees have contributed to capacity building in their states in numerous ways (Box). In recent years, field assignees have focused on analyzing and disseminating state and local data on health disparities and improving data-informed decision-making processes to target public health interventions for chronic disease prevention and management. Colorado’s field assignee has worked to enhance data usage for chronic disease program planning. This field assignee collaborated with a state chronic disease grant program to develop a new data-driven approach to scoring grant applications. This new approach was designed to increase the effect of grantee programs on health disparities by elevating scores of applications proposing to serve areas of greater need. To develop the new approach, a county ranking was created by using a principal components analysis of county data on the burden of disease and the social determinants of health, and a new methodology was developed to apply the results of the county rankings to the scores of grant applicants. Arizona’s field assignee contributed to several state reports to inform program priorities, including the _Arizona American Indian Health Status Summary Report for Data Year 2015_ , which was shared statewide with tribal leaders and partners working with tribal communities. The report informed targeted interventions and focused on health disparities. Indiana’s field assignee contributed to several quality improvement initiatives for chronic disease programming and surveillance, such as the development and implementation of out-of-hospital and telemedicine programs for heart disease and heart failure patients in rural areas that lacked both primary care providers and specialists. Illinois’s field assignee applied a novel approach for the state to better understand implementation of evidence-based interventions for high blood pressure and glycemic control among Federally Qualified Health Centers, organizations that serve approximately 1.2 million of Illinois’s most vulnerable citizens. The field assignee is also leading efforts to assess feasibility of a statewide quality improvement collaborative.\n\n【10】Box. Examples of Responsibilities and Expectations of Assignees in the Centers for Disease Control and Prevention’s State Chronic Disease Epidemiology Assignee Program\n\n【11】• Provide general epidemiological consultation and assistance to the state public health agency, local public health agencies, and partners as appropriate.\n\n【12】• Ensure collaboration across chronic disease programs and with internal and external stakeholders for epidemiology, surveillance, and evaluation activities.\n\n【13】• Consult with chronic disease program managers about how data can be used to support and target chronic disease prevention efforts and develop strategies for strengthening those efforts.\n\n【14】• Enhance data collection, analysis, interpretation, and dissemination.\n\n【15】• Mentor, develop resources for, and conduct trainings for state and local chronic disease program staff members and epidemiologists to strengthen epidemiology capacity and enhance data usage.\n\n【16】• Serve as preceptor and mentor for student interns, fellows, and preventive medicine residents.\n\n【17】• Build partnerships with other agencies and stakeholders across multiple sectors to increase data sharing and usage.\n\n【18】• Develop and implement chronic disease surveillance plans.\n\n【19】• Develop and implement chronic disease program evaluation plans.\n\n【20】• Contribute to the development of chronic disease and related state plans.\n\n【21】• Provide technical assistance in writing chronic disease-related grant applications, cooperative agreements, and requests for proposals.\n\n【22】• Make presentations at national and local conferences and meetings on behalf of the state public health agency.\n\n【23】• Publish state reports and articles in peer-reviewed scientific journals.\n\n【24】• Participate in Council of State and Territorial Epidemiologists subcommittees and workgroups.\n\n【25】Past, Present, and Future Capacity-Building Efforts\n\n【26】Many other chronic disease epidemiology capacity-building efforts have occurred or are ongoing. Formal state and local capacity-building programs have included, but are not limited to, CDC/CSTE’s Applied Epidemiology Fellowship and CDC/National Association of Chronic Disease Directors’ Applied Chronic Disease Epidemiology Mentoring Program . These have been successful programs, but expanded efforts are needed. Governmental agencies, foundations, universities, and others committed to chronic disease–related public health capacity building should collaborate with those working in other subject areas to build capacity on cross-cutting competencies. Examples of topics include those identified by CSTE’s chronic disease epidemiology capacity assessment: using informatics tools in support of epidemiologic practice; understanding institutional review board processes; using systems thinking in epidemiologic planning and policy development; leading community public health planning processes; practicing culturally sensitive epidemiologic activities; conducting program evaluations; and others .\n\n【27】Future efforts should build on past and current efforts, be informed by national assessment results, and target jurisdictions with subpar levels of chronic disease epidemiology capacity. Training efforts should be tailored to address changes occurring in public health and chronic disease surveillance. To achieve excellence in chronic disease epidemiology and to build capacity, the following are needed: 1) identify champions for enhancing capacity, 2) continually review and update the essential roles of CDEs, 3) expand the skills and competencies of the current and future workforce, 4) develop and enhance partnerships to improve data sharing, 5) leverage and link existing data sources, 6) improve the availability of local data, 7) fill data gaps to better measure determinants of health and health disparities, and 8) make data more actionable. Strong commitment is vital to building and maintaining capacity-building efforts in chronic disease epidemiology and surveillance in state, territorial, local, and tribal public health agencies. Throughout these capacity-building efforts and across all chronic disease epidemiology and surveillance efforts, the default view must be through a health equity lens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "21c21322-754d-43b3-b264-fdafb3fb15a3", "title": "Terrestrial Bird Migration and West Nile Virus Circulation, United States", "text": "【0】Terrestrial Bird Migration and West Nile Virus Circulation, United States\nWest Nile virus (WNV) is a mosquitoborne virus that can cause severe and even fatal disease in humans. After WNV introduction into New York, NY, USA, its geographic range expanded quickly, reaching the West Coast in 2003. Previous studies have shown that the spread of WNV occurred faster than could be explained by contiguous diffusion  and demonstrated that its expansion occurred heterogeneously, consisting of contiguous diffusion and long distance translocations . Since then, phylogeographic studies have reported frequent mixing of WNV strains from local and distant locations. The most notable exception is California **,** where several genetic studies have shown limited movement into and out of the state .\n\n【1】The rapid expansion of WNV in the United States probably cannot be attributed to the movement of humans because humans are dead-end hosts. However, in nature, WNV is maintained in an enzootic transmission cycle involving mosquito vectors and highly mobile avian reservoirs. _Hyalomma marginatum_ ticks have also been implicated in the transmission of WNV .\n\n【2】Although evidence of WNV infection has been identified in many species of birds, deaths and disease among birds vary greatly, ranging from asymptomatic to fatal infections; peak viremia potentially reaches >10 12  PFU/mL . WNV RNA has been detected in bird spleen and kidneys as long as 36 weeks after infection  and in brains of _Nestor notabilis_ kea up to 72 months after infection .\n\n【3】Although phylogenetic evidence of geographic clustering by location is limited, a recent study reported that WNV isolates clustered according to avian flyway . Because birds are the primary reservoirs for WNV, this finding was not surprising, but it is relevant because bird migration has also been implicated in the movement of influenza A virus , _Borrelia burgdorferi_ (Lyme disease agent) , other pathogenic organisms , and even invasive invertebrate organisms . Several serologic studies (e.g. ELISA, plaque reduction neutralization test) have been used to determine the direction of WNV movement within the Atlantic, Mississippi, and Pacific flyways and demonstrated WNV in birds migrating southward, whereas evidence of the virus in birds during northward migration is limited .\n\n【4】Studies of virus movement associated with avian hosts in the United States have concentrated on the migration of waterfowl and excluded terrestrial birds, largely because the migratory patterns of waterfowl have been thoroughly characterized by banding studies. However, passerine birds, the primary reservoir for WNV, are terrestrial birds, not waterfowl. Terrestrial birds and waterfowl fly along similar but distinct flyways. Although waterfowl follow regular paths bounded by mountains and rivers, terrestrial birds often follow looped routes that enable them to maximize tail winds, avoid head winds, and correlate with seasonal fluctuations in food availability . Although looped migration paths have been described for several species of birds ( _Selasphorus rufus_ hummingbirds , _Circus aeruginosus_ western marsh harriers , _Falco eleonorae_ Eleonora’s falcons , _Cuculus canorus_ common cuckoos ), the general flyways of terrestrial birds have been inadequately studied. However, in 2014, La Sorte et al. provided a general description of terrestrial bird flyways in North America . They defined 3 flyways: the single distinct Western flyway and 2 overlapping flyways, the Central and Eastern flyways. A similar 3-flyways system (Pacific, Central, and Atlantic flyways) has been described for waterfowl ; however, most studies have relied on the more common 4-flyways system (Pacific, Central, Mississippi, and Atlantic flyways). In this study, we used phylogeographic approaches to investigate the relationship between WNV circulation in the United States and the flight paths of terrestrial birds.\n\n【5】### Methods\n\n【6】##### Generation of Alignments\n\n【7】We identified all unique sequences of natural and laboratory WNV strains by using the Virus Variation Resource . Virus sequences meeting the following criteria were included in this study: 1) the nucleotide sequence spanned the complete open reading frame, 2) the sequence was derived from natural isolates and not laboratory strains, 3) the sequence was unique (i.e. all sequences differed by \\> 1 nt), and 4) the sequence contained no degenerate nucleotides. All sequences were manually aligned in BioEdit version 7.1.3  or MEGA7 , and noncoding regions were removed when necessary (i.e. the open reading frame was used for analyses).\n\n【8】##### Isolation of Viral RNA and Next-Generation Sequencing\n\n【9】We obtained additional WNV isolates from the World Reference Center for Emerging Viruses and Arboviruses at the University of Texas Medical Branch at Galveston (Galveston, TX, USA) . Isolates were originally collected from Virginia, Georgia, Texas, and Colorado. We extracted viral RNA from the supernatant of infected Vero cells by using a QIAamp Viral RNA Mini Kit (QIAGEN, Germantown, MD, USA) according to the manufacturer’s instructions. We generated libraries with a TruSeq RNA version 2 kit (Illumina, San Diego, CA, USA) and samples sequenced by the University of Texas Medical Branch at Galveston Next Generation Sequencing Core on an Illumina 1500 Hi-Seq platform. Adaptor sequences and poor quality reads (Q score <20) were removed with Trimmomatic . Reads were aligned with Bowtie2  under the sensitive local parameter against the prototypical strain of WNV . Consensus sequences were generated by using SAMtools .\n\n【10】##### Phylogeny\n\n【11】To evaluate temporal structure, we generated a time-naive phylogeny (i.e. a maximum-likelihood phylogeny) to enable determination of the patristic distance between all isolates on the phylogeny. We generated maximum-likelihood trees with RAxML-HPC Black Box on Cyberinfrastructure for Phylogenetic Research version 3.3  and determined automatic halting by bootstrapping. We determined the root-to-tip distance, which is a phylogenetic measure of genetic distance, for each isolate of the maximum-likelihood phylogenies by using TempEst (formerly Path-o-gen) . We evaluated the correlation (Pearson method) between root-to-tip distance and collection date in R .\n\n【12】We used a Bayesian Markov chain Monte Carlo (MCMC) approach to infer phylogeographic relationships and selected the most appropriate phylogenetic model by using standard path sampling and stepping-stone approaches. XML files were generated in BEAUti version 1.8.3 or 1.8.4 and run with BEAST version 1.8.4  on Cyberinfrastructure for Phylogenetic Research . We used the GTR+Γ+I (general time reversible with gamma rate distribution and invariable sites) model to infer nucleotide substitution frequencies, an uncorrelated lognormal clock model to infer the mutation rate, and a Bayesian Skyline tree prior to model changes in population size. The evolution rate mean was restricted to 10 −4  through 9 × 10 −4  substitutions/site/year, consistent with previously reported rates for WNV evolution .\n\n【13】We ran trees with an MCMC length of 100 million and sampled every 5,000 steps. Log files were reviewed in Tracer  to determine burn-in, which ranged from 5% through 10%. We ran multiple independent MCMC chains until effective sample size values exceeded 200. Log and tree files were combined in LogCombiner version 1.8, and a maximum clade credibility tree was generated in TreeAnnotator . Locations were inferred by using ancestral state reconstruction with an asymmetric discrete trait substitution model .\n\n【14】##### Analysis of Migration\n\n【15】After the XML files were generated in BEAUti, we manually edited them to enable counting of all Markov jumps (MJ) (which described the relative magnitude of migration out of source locations and into sink locations) for 2001 through 2009 . This method for evaluating migration, first described by Minin and Suchard , has been used to characterize migration of several major pathogens including rabies virus , dengue virus , HIV , influenza virus , and Rift Valley fever virus .\n\n【16】As expected for an emerging zoonotic disease, the annual West Nile neurologic disease (WNND) incidence and sample collection efforts varied dramatically among states over time, adding substantial complexity to the model. To mitigate the effects of inconsistent sampling and to confirm the observed results, we applied a stricter inclusion criterion to confirm the results obtained by using the full dataset. The sequences were randomly down-sampled such that the number of sequences used correlated (p<0.05 by Pearson method) with the incidence of WNND reported to the Centers for Disease Control and Prevention (CDC) in a particular year (the most accurate record of relative WNV activity). Our analysis ensured that the dataset was representative of the WNV activity of each region in a particular year.\n\n【17】We calculated incidence by using the number of WNND cases reported to CDC from each state during each year and dividing that number by the estimated population of each state. The population estimates were obtained from the Time Series of Intercensal State Population Estimates available at the Population Division of the US Census Bureau . States with insufficient sequences to represent the WNND incidence were excluded. Down-sampling was undertaken in at least duplicate to ensure that reduction in sample size and diversity did not remove important relationships.\n\n【18】### Results\n\n【19】##### Sequence Collection\n\n【20】All previously published sequences of natural WNV isolates collected in the United States were obtained from GenBank on January 1, 2016. The number of WNV sequences varied substantially over time and among locations, which presented statistical challenges. In particular, although GenBank has >900 WNV open reading frames, most come from a few states where laboratories were actively involved in WNV surveillance and research: California, New York, and Texas. The ability to compare multiple isolates over multiple years was critical to the analysis. Only a few states had sufficient numbers of WNV sequences available in GenBank to enable analysis for multiple consecutive years: New York, Connecticut, Illinois, North Dakota, South Dakota, Texas, and California.\n\n【21】To mitigate the influence of sampling bias, we obtained additional WNV isolates from the World Reference Center for Emerging Viruses and Arboviruses for 3 states and sequences to support the analysis: Virginia (n = 39), Georgia (n = 20), and Colorado (n = 31) . Given that previous studies have demonstrated limited WNV movement into or out of California , we did not include isolates from California in the analysis. Similarly, because of the proximity of New York and Connecticut, we chose New York to represent WNV in the Northeast because Connecticut is a small state. Last, to ensure that each location was represented across a similar time frame, we included only isolates collected during 2001–2009 in the Bayesian phylogeny and migration analysis. Table 2 shows the states and availability of yearly isolates.\n\n【22】##### Model Selection\n\n【23】We compared 203 nucleotide substitution models by using the Bayesian and Akaike Information Criteria in JModelTest2 ( https://github/com/ddariba/jmodeltest2 ) and found the GTR+Γ+I model to be the most appropriate. For assessing temporal signature, we used a maximum-likelihood tree with sequences of WNV strains from New York, Virginia, Georgia, Illinois, North Dakota, South Dakota, Texas, and Colorado (n = 379) . We identified a statistically significant positive correlation (r = 0.93, 95% highest posterior density \\[HPD\\] = 0.92–0.94; p<2.2 × 10 −16  ) between the root-to-tip distance and the date of isolation in Temp-Est (formerly known as Path-O-gen) . The mutation rate was estimated to be 4.05 × 10 −4  substitutions/site/year, and the most recent common ancestor (MRCA) was in 1997. Together these results indicated a strong temporal signal in the dataset. Finally, we evaluated Bayesian tree priors (skyride, skygrid, and skyline) and uncorrelated clock models (lognormal and exponential) by using path-sampling and stepping-stone approaches. The uncorrelated lognormal clock model with the Bayesian skyline tree prior was the most appropriate.\n\n【24】##### Phylogeographic Analysis for the United States\n\n【25】Analysis of all WNV sequences collected from New York, Virginia, Georgia, Illinois, North Dakota, South Dakota, Texas, and Colorado during 2001–2009 provided estimates of the introduction date of the MRCA and mean evolution rate that were consistent with the estimates of the root-to-tip distance analysis . The date of MRCA introduction was estimated as 1997, and the average evolution rate was 3.92 × 10 −4  substitutions/site/year.\n\n【26】We used MJ between reconstructed ancestral states to estimate the magnitude of relative migration out of, or into, each of the 8 regions . Frequent migration (>2 MJ) was detected from Illinois to Colorado (8.38 MJ), Illinois to Georgia (8.23 MJ), Illinois to North Dakota (10.43 MJ), Illinois to New York (29.97 MJ), Illinois to South Dakota (6.69 MJ), Illinois to Texas (22.87 MJ), Illinois to Virginia (11.45 MJ), New York to Colorado (4.36 MJ), New York to Georgia (7.04 MJ), New York to South Dakota (2.18 MJ), New York to Texas (4.56 MJ), New York to Virginia (4.24 MJ), Texas to Colorado (9.78 MJ), Texas to North Dakota (5.18 MJ), Texas to South Dakota (7.56 MJ), and Virginia to Georgia (3.62 MJ).\n\n【27】Overall, 3 major sources of WNV circulation (New York, Illinois, and Texas) seemed to be the origin of 88.5% of the total migration events observed . Southward and westward movements were detected along the East Coast, but only northward movement was observed within the central United States. A notable exception was observed in Illinois, where evidence of WNV movement in all directions was demonstrated.\n\n【28】##### Incidence-Controlled Phylogeny\n\n【29】To mitigate the effects of inconsistent sampling, we applied a stricter inclusion criterion to ensure that the dataset was representative of WNV activity in each region in a particular year. In this approach, the sequences were randomly down-sampled by using the sample command in R, such that the number of sequences was proportional to the incidence of WNND reported to CDC . Illinois, North Dakota, and South Dakota were not included in the down-sampled datasets because there were insufficient sequences to represent WNND incidence in these states. To ensure that reduction in sample size and diversity did not remove important relationships, the down-sampling was independently performed twice.\n\n【30】According to the 2 incidence-controlled datasets, the MRCA was ≈1997 in both down-sampling exercises (95% HPD 1996.00–1998.52 and 95% HPD 1995.7–1998.25), and the overall mutation rates were estimated to be 4.02 × 10 −4  and 3.83 × 10 −4  substitutions/site/year . As with the full dataset, the Markov analysis demonstrated that New York and Texas were strong sources of WNV circulation. Significant movement (mean >2 MJ) was detected from Texas to Colorado (20.42 and 20.44 MJ); Texas to New York (12.36 and 11.77 MJ); Texas to Georgia (8.28 and 9.55 MJ); Texas to Virginia (7.14 and 7.732 MJ); New York to Georgia (6.1 and 5.38 MJ); New York to Virginia (4.95 and 3.65 MJ); New York to Colorado (4.04 and 2.66 MJ); New York to Texas (2.66 and 2.73 MJ); Virginia to Georgia (1.55 and 3.62 MJ); and, in dataset 2 only, Virginia to Colorado (1.31 MJ) .\n\n【31】Together, the MJ analyses of the incidence-controlled dataset and the full dataset illustrate a consistent pattern of WNV circulation. All southward movement originated in the eastern United States (New York and Virginia), and most of the northward movement originated in the central United States (Texas) .\n\n【32】### Discussion\n\n【33】In recent years, emerging zoonotic diseases caused by Ebola, Zika, Nipah, Middle Eastern respiratory syndrome, and influenza A viruses have become major public health problems, devastating communities and costing millions for public health interventions. Decisive, evidence-based approaches are critical for managing emerging infectious diseases, but effective and efficient countermeasures will be possible only after the relationships between pathogens and their hosts have been thoroughly characterized.\n\n【34】Bird migration has been implicated in the movement of a variety of pathogens . In particular, characterization of the relationship between avian influenza virus movement and waterfowl migration has supported surveillance and early warning programs . However, studies of avian hosts in the Americas have mainly concentrated on the migration of waterfowl to the exclusion of terrestrial birds because waterfowl are easily tracked with banding; thus, their migration has been thoroughly characterized.\n\n【35】The introduction and subsequent spread of WNV into the Americas underscores the invasive potential of emerging pathogens in the New World, as has been recently exemplified by Zika virus, another mosquitoborne flavivirus. Dramatic variations in the location, timing, and intensity of WNV strain collection and sequencing has left the field with a limited understanding of virus circulation patterns and no reliable way of predicting the geographic spread of WNV outbreaks. We have addressed this knowledge gap by characterizing the movement of WNV with regard to the migratory patterns of its natural hosts, terrestrial birds. We compiled 379 virus sequences for analysis, including 289 previously reported sequences from New York, Virginia, Georgia, Illinois, North Dakota, South Dakota, Texas, and Colorado, plus 90 novel sequences from Virginia, Georgia, and Colorado.\n\n【36】Phylogeographic analysis revealed that 3 locations— New York, Illinois, and Texas—accounted for 88.5% of the total WNV MJ inferred. Because New York is the presumed original introduction point for WNV into the United States, its role as a major source of WNV movement was expected. However, 74.2% of the observed MJ originated in Illinois and Texas only. Of note, North Dakota and South Dakota, which are 2 of the states with the highest annual WNND incidence, seem to be strong sinks for WNV moving out of Illinois and Texas.\n\n【37】The contributions of Illinois and Texas to WNV circulation are not surprising because both locations are situated at major convergence points between the Eastern and Central flyways. In the case of Texas, birds from both flyways may avoid long-distance flights across the Gulf of Mexico by traveling along the circa-Gulf route that follows the Gulf Coast through Texas into Mexico. In the case of Illinois, seasonal shifts in terrestrial bird migration routes ensure that Illinois supports birds from the Eastern and Central flyways during annual migrations.\n\n【38】Of note, although mosquito and WNV activity occurs earlier in the southern than in the northern United States, southward migration was detected along the East Coast during our sampling period, 2002–2009, indicating that the southeastern United States is probably a dead end for WNV circulation. Indeed, low-level transmission probably occurs during the winter in warmer locations such as Florida, Texas, and Louisiana. This possibility is supported by isolations of WNV from mosquitoes and birds during December and January in Harris County, Texas , and suggests that ecologic factors not related to mosquito abundance and WNV activity drive WNV movement along the East Coast. Instead, movement of WNV into the northeastern United States (New York) from Illinois and Texas was observed (in the incident-controlled analysis). These results suggest that introduction of WNV into the northeastern United States originated from the central United States.\n\n【39】Overall, we have defined the pattern of WNV circulation in the United States  and demonstrated looped virus movement patterns in the Eastern and Central flyways that are bridged by Illinois, a region shared between the 2 flyways. This specific pattern correlates with the looped migration patterns of terrestrial birds. Although other geographic regions may contribute to virus movement, there were insufficient virus sequences available from other states to incorporate into this analysis. Thus, on the basis of available information, 3 of the 8 locations considered (New York, Illinois, and Texas) seem to be the preferred sites for efficiently monitoring ongoing WNV evolution.\n\n【40】As new WNV sequences become available, similar phylogeographic methods can be used to develop more detailed information about WNV circulation in the United States. For example, on the East Coast, WNV circulation occurs southward, so surveillance efforts in the Northeast are likely to be more informative than surveillance in the Southeast. Conversely, WNV in the central United States travels northward, so surveillance in the south-central United States is more likely than surveillance in the north-central United States to be informative. Last, the region of overlap between the Eastern and Central flyways is the most likely location for deriving surveillance information because WNV in this area travels in multiple directions.\n\n【41】Collectively, the results of this study illustrate the value of using multidisciplinary approaches to surveillance of infectious diseases, especially zoonotic diseases. Animal migration is shaped by a delicate balance of ecologic factors and anthropomorphic barriers. Natural and manmade events (e.g. climate change, atmospheric fluctuations, habitat destruction) can drastically alter host behavior, which in turn affects the circulation patterns of infectious agents such as WNV. In this study, we defined the patterns of WNV circulation and key areas for surveillance and correlated them with the migratory patterns of their primary reservoir, terrestrial birds. Although this information does not enable prediction of the size of annual WNV outbreaks, these advancements support the construction of targeted surveillance and vector mitigation strategies to predict the annual flow of WNV strains and to enable public health officials to anticipate changes in WNV circulation resulting from altered bird migration.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "53b56414-a144-4d16-b87e-a829589a84b2", "title": "Multisystem Inflammatory Syndrome after SARS-CoV-2 Infection and COVID-19 Vaccination", "text": "【0】Multisystem Inflammatory Syndrome after SARS-CoV-2 Infection and COVID-19 Vaccination\nMultisystem inflammatory syndrome (MIS) in children (MIS-C) and adults (MIS-A) are febrile syndromes with elevated inflammatory markers that usually manifest 2–6 weeks after a severe acute respiratory syndrome 2 (SARS-CoV-2) infection . The Brighton Collaboration Case Definition for MIS-C/A was recently published to be used in the evaluation of patients after SARS-CoV-2 immunization ; some scientists are concerned that vaccination against SARS-CoV-2 can trigger MIS-C/A. We report 6 cases of MIS from a large integrated health system in Southern California, USA; 3 of those patients received SARS-CoV-2 vaccination shortly before seeking care for MIS. All 6 patients met the Brighton Collaboration Level 1 of diagnostic certainty for a definitive case and had MIS illness onset between January 15–February 15, 2021. The Chief Compliance Officer for the Southern California Permanente Medical Group reviewed this case series and confirmed that it was compliant with the Health Insurance Portability and Accountability Act for publication.\n\n【1】### The Study\n\n【2】Patient 1 was a 20-year-old Hispanic woman who sought care for 3 days of a diffuse body rash, tactile fever, sore throat, mild neck discomfort, and fatigue. There was no cough, congestion, headache, or abdominal pain. She had vomiting and diarrhea, which had subsided 8 days before admission. She received her first dose of SARS-CoV-2 vaccine 15 days before admission. She had no known coronavirus disease (COVID-19) exposure but was SARS-CoV-2 PCR and nucleocapsid IgG positive. She was hypotensive at arrival to the emergency department, requiring inotropic support. She had elevated troponin and brain natriuretic peptide (BNP) with a left ventricular ejection fraction initially mildly reduced at 45% but 30%–35% the following day. She responded well to therapy with intravenous immunoglobulin (IVIG) and methylprednisolone .\n\n【3】Patient 2 was a 40-year-old Hispanic man who sought care after 6 days of episodic fevers up to 101.7°F. Associated symptoms included dyspnea on exertion, headache, neck pain, lethargy, abdominal pain, and diarrhea. No chest pain was present. He had a history of SARS-CoV-2 vaccination and laboratory-confirmed mild to moderate COVID-19, both within 48 days before seeking care . His exam was notable for sweats, diffuse abdominal pain on palpation, tachycardia, and tachypnea. Patient 2 fulfilled Brighton Level 1 criteria for MIS-A with documented fevers, gastrointestinal and neurologic symptoms, elevated inflammatory and cardiac markers, and electrocardiogram changes that were concerning for myocarditis . He responded well to treatment with dexamethasone .\n\n【4】Patient 3 was an 18-year-old Asian American man who sought care at the emergency department with a history of 3 days of fever as high as 104°F with headache, vomiting, diarrhea, and abdominal cramping . He denied any upper respiratory symptoms. He had a history of a laboratory-confirmed COVID-19 infection 6 weeks before the onset of symptoms and received the first dose of the SARS-CoV-2 vaccine 18 days before the onset of symptoms. In the emergency department, he was found to be hyponatremic and hypotensive . His examination was notable for tachycardia and abdominal tenderness. He had elevated inflammatory markers, thrombocytopenia, and lymphopenia. Echocardiogram revealed mild to moderate reduced systolic function with an ejection fraction of 40%–45%. He responded well to therapy with methylprednisolone, IVIG, and anakinra.\n\n【5】Patient 4 was a 62-year-old Asian American man who sought care at the emergency department for fever lasting 5 days. For 6 days he had had nausea and vomiting, which developed 23 days after a laboratory-confirmed mild to moderate acute COVID-19 illness that subsided after 1 week. He also had 4 days of bilateral hearing loss. He was hypotensive, requiring inotropic support. He had thrombocytopenia, elevated inflammatory markers, and elevated troponin with diffuse ST elevations on electrocardiogram . He responded well to treatment with methylprednisolone, including improvement in his hearing loss.\n\n【6】Patient 5 was a 29-year-old Hispanic woman who experienced fever, chills, headache, and nausea 28 days after a laboratory-confirmed acute COVID-19 illness. She sought care at the emergency department with hypotension requiring ionotropic support. Clinicians diagnosed MIS-A on the basis of conjunctivitis, evidence of colitis on abdominal imaging, elevated inflammatory markers, lymphopenia, and elevated BNP. She responded well to treatment with methylprednisolone and IVIG .\n\n【7】Patient 6 was a 23-year old Hispanic man who experienced fever and abdominal pain 38 days after a laboratory-confirmed mild to moderate acute COVID-19 illness. He was hypotensive, requiring inotropic support. He had mesenteric adenitis on abdominal imaging. He had elevated inflammatory markers, neutrophilia, lymphopenia, and a left ventricular ejection fracture of 20% on echocardiogram. He was treated with IVIG and methylprednisolone . He died 12 days after admission.\n\n【8】### Conclusions\n\n【9】At the time of our study, our medical group was only vaccinating healthcare workers and patients \\> 75 years of age. The 3 patients that were immunized qualified for early vaccination because they either worked or volunteered in a healthcare setting. These cases occurred ≈1 month after the peak surge of COVID-19 cases in Southern California. At the time these patients sought care, only ≈7% of the adult ( \\> 18 years of age) population who were members of the Kaiser Permanente patient group (≈3,776,000 members) had received \\> 1 SARS-CoV-2 vaccine, whereas 3 of the 6 patients in this study who had MIS were vaccinated. These 6 patients were hospitalized at 5 of the 15 Kaiser Permanente medical centers across Southern California. We believe the temporal association after SARS-CoV-2 immunization is worth noting, given the theoretical concern of MIS-C/A after vaccination . We did not identify any patients with MIS after vaccination who did not have recent SARS-CoV-2 infection. It is possible that other case-patients in our member population were hospitalized outside of our 15 medical centers and thus were not captured for this case series.\n\n【10】Overall, MIS is rare in adults. In comparison we treated >50 children with MIS-C during January 2021–February 2021 and >100 since May 2020 among a pediatric population of 960,000.\n\n【11】The Centers for Disease Control and Prevention (CDC) allows for vaccination after a SARS-CoV-2 infection after recovery from the acute illness and after the isolation period, with no recommended minimal interval between infection and vaccination . Most cases of MIS-C/A occur 2–6 weeks after an exposure or infection , although we have seen several children brought for care as late as 8–10 weeks after a confirmed infection or exposure. We need to continue to monitor for MIS-C/A after SARS-CoV-2 infection and immunization as more of the population are vaccinated, especially as vaccines are administered to children who are at higher risk for MIS. CDC and the US Food and Drug Administration co-manage VAERS (the Vaccine Adverse Event Reporting System), which is being used to monitor for adverse events after COVID-19 vaccines. MIS-C/A is listed as a postvaccination adverse event of special interest  and should be reported to VAERS .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "77d9fed9-205a-4c28-9d90-5a48e2f24288", "title": "Incidence, Etiology, and Healthcare Utilization for Acute Gastroenteritis in the Community, United States", "text": "【0】Incidence, Etiology, and Healthcare Utilization for Acute Gastroenteritis in the Community, United States\nIn the United States, the incidence of acute gastroenteritis (AGE) is high. AGE is estimated to cause 179 million illnesses annually . Precise data are limited on the occurrence and characteristics of sporadic AGE, particularly because the illnesses are generally mild and usually do not require medical care; may not have had diagnostic testing even if care was sought; and, depending on the pathogen, may not be reportable through public health surveillance systems. Previous US publications, using data from the US Foodborne Diseases Active Surveillance Network (FoodNet), have reported AGE prevalence ranging from 7.7 to 11%, equivalent to roughly 0.7–1.4 illnesses/person/year, depending on the recall period (i.e. 7 or 28 days) and symptom profile (i.e. diarrheal illness alone or with the presence of additional symptoms) . These studies have been essential in establishing estimates of AGE incidence in the community and highlighting the substantial burden of disease. However, differences in AGE case definitions have complicated efforts to compare findings across studies and time periods, and robust estimates of occurrence across the age spectrum remain limited. Consequently, there is a need to obtain all-age, population-based estimates of AGE within the United States.\n\n【1】Even assuming the lowest reported AGE prevalence of 7.7%, there is potential for substantial disease burden on the local healthcare systems and on society, such as through lost productivity . Among persons with AGE, 12%–20% have reported visiting a healthcare provider to manage their symptoms, and AGE has been estimated to contribute to 2–3 million ambulatory visits and 900,000 hospitalizations per year in the United States . However, these data have relied on samples of persons within a geographic area who may differentially seek care depending on if they have medical insurance or access to an affordable care source. As a result, these studies may not accurately estimate the true potential burden on a healthcare system.\n\n【2】Clarifying the etiology of AGE illness within communities and healthcare systems can help to effectively target prevention efforts. Sporadic cases of AGE are largely attributable to viral pathogens; norovirus is the most common cause of AGE across the age spectrum. Evidence in the literature suggests that intensity of viral shedding among those with asymptomatic norovirus infections is similar to that of symptomatic infections ; however, according to transmission modeling of a healthcare-associated outbreak, symptomatic shedders are more likely to transmit norovirus to others than those without symptoms .\n\n【3】To better characterize the incidence of AGE in the community, the associated healthcare utilization, and the prevalence of viral enteropathogens among both symptomatic and asymptomatic persons, we conducted the Community Acute Gastroenteritis (CAGE) Study among the membership population of a large, integrated healthcare system. The aims of the CAGE Study were to generate 30-day prevalence and annualized incidence estimates of AGE occurrence across the age spectrum, describe the proportion of symptomatic persons seeking healthcare, and calculate the prevalence of enteric viral pathogens among those who did and did not report AGE. To contextualize our results with previously reported literature, we report our findings here using 2 validated case definitions .\n\n【4】### Methods\n\n【5】##### Study Population\n\n【6】We conducted the CAGE Study within Kaiser Permanente Northwest (KPNW), an integrated health care delivery system with >600,000 current members. This network comprises 24% of, and is demographically similar to, the underlying population of northwest Oregon and southwest Washington, USA .\n\n【7】##### Sampling and Recruitment\n\n【8】We targeted enrollment to ≈3,000 members of all ages over a 12-month period. To achieve this goal, we selected age-stratified, simple random weekly samples of KPNW members from September 26, 2016, through September 19, 2017. Sampling was conducted without replacement through automated abstraction of health plan enrollment records, updated monthly, and was unrelated to AGE illness status or healthcare utilization. We excluded members who were in hospice care, non-English speaking, decisionally or cognitively impaired, previously recruited for the study, or had opted out of all KPNW research activities. Within the randomly sampled population, we targeted enrollment of an age-stratified subset of 500 members to complete a survey and provide a stool sample for virologic testing (SS cohort); the remaining 2,500 targeted enrollees were asked to complete a survey only (SO cohort). Although we describe the prevalence of viral pathogens among both cohorts, we sampled 500 in the SS cohort to have adequate power to detect an estimated 5% prevalence of these pathogens among asymptomatic persons.\n\n【9】Every week, we first invited our selected sample to participate by mailing recruitment postcards containing information about the study and a link to an online survey. Three days later, we sent recruitment email invitations to sampled members with active email addresses on file; we sent a reminder email invitation 1 week later. For participants within the SO cohort, we made no further recruitment efforts. To sampled members within the SS cohort, study staff made recruitment phone calls beginning 1 week after email invitations were sent; staff made \\> 3 phone call attempts over the course of 1 week.\n\n【10】To compensate for their time, we provided enrolled participants who completed only the survey (all SO participants and those SS participants who did not provide a stool specimen) a $10 gift card. We compensated SS participants who completed a stool specimen with a $20 gift card.\n\n【11】##### Survey\n\n【12】The 34-item survey administered to participants comprised questions on demographic characteristics and about AGE symptoms in the previous 30 days. For those reporting AGE symptoms, we collected the frequency of vomiting/diarrhea for the most recent illness and information on any related medical encounters. We assessed encounter types separately and included inpatient hospitalizations; urgent care, emergency department, and outpatient visits; and telephone and email encounters. We defined telephone and email encounters as remote and defined the remaining encounter types as in-person. We also asked survey respondents to self-report medical conditions associated with the occurrence of chronic diarrhea as a major symptom (i.e. Crohn’s disease, ulcerative colitis, inflammatory bowel disease, or abdominal or colorectal cancer).\n\n【13】##### Case Definitions\n\n【14】Our primary AGE case definition included participants who reported any vomiting ( \\> 1 episode within 24 hours) or diarrhea ( \\> 3 loose stools in any 24-hour period) . Participants with <3 loose stools in a 24-hour period and no vomiting were not considered to have AGE. Persons with medical conditions associated with chronic diarrhea were considered to have AGE if they reported vomiting; otherwise, they were categorized as noncases, regardless of diarrhea episodes.\n\n【15】For incidence and healthcare utilization analyses, we separately considered a second case definition limited to all persons reporting acute diarrhea, which we defined as having \\> 3 loose stools in any 24-hour period . Participants with <3 loose stools in a 24-hour period, those reporting vomiting only, and those with medical conditions associated with chronic diarrhea were categorized as noncases.\n\n【16】##### Stool Collection and Laboratory Testing\n\n【17】For SS participants, we employed the same method for stool sample self-collection as previously described for our medically attended acute gastroenteritis (MAAGE) study; stool sample kits were sent to responders by overnight courier within 1 day of survey completion . Once returned, the Oregon State Public Health Laboratory (OSPHL) conducted laboratory testing of stool specimens submitted by study participants to detect norovirus, rotavirus, astrovirus, and sapovirus, using TaqMan real-time quantitative reverse transcription PCR (qRT-PCR) protocols developed by the Centers for Disease Control and Prevention (CDC), also as previously described . OSPHL forwarded stool specimens testing positive for rotavirus to the CDC for confirmatory testing by qRT-PCR and enzyme immunoassay (EIA).\n\n【18】##### Statistical Analyses\n\n【19】We conducted all analyses using weights to account for the age-stratified probability sampling. In brief, we calculated a base weight to account for the initial probabilities of selection within each age stratum and week of sample selection. For the SO sample, we calculated a nonresponse adjustment factor using 10 strata defined by age group (0–4, 5–17, 18–44, 45–64, and \\> 65 years) and sex to reduce potential bias due to nonresponse; for the SS sample, we calculated the nonresponse adjustment factor using the 5 age strata. Last, we raked the weights using the iterative proportional fitting algorithm  so that the marginal totals by age and sex matched known KPNW population totals from September 2017, when the CAGE survey was completed. We obtained SEs using Taylor series linearization and calculated 95% CIs by using exact (Clopper-Pearson) formulas.\n\n【20】We report population characteristics using unweighted counts and weighted means or proportions. We estimated 30-day point prevalence with 95% CIs by using weighted proportions. We calculated prevalence estimates overall, by age group, and by month. For monthly estimates, we included responses to the survey occurring before the 15th of the previous month; we included responses that occurred on or after the 15th in the calculation of prevalence for the current month. Using the prevalence estimate, we then calculated an annualized rate by multiplying the prevalence by 365/30, which yields and estimate of the average number of AGE cases per person per year.\n\n【21】For prevalence and reported healthcare encounter estimates, we report calculations using the AGE and acute diarrhea case definitions. Because of the small sample of persons meeting the acute diarrhea case definition from among those submitting a stool specimen for virologic testing, we report results using only the AGE case definition. We conducted all analyses in Stata version 15.1 .\n\n【22】##### Ethics Statement\n\n【23】This project was reviewed and approved by the KPNW Institutional Review Board (FWA00002344). Participants provided informed consent to participate in this study.\n\n【24】### Results\n\n【25】In our 52-week study period, our sex- and age-stratified random sample comprised 28,217 KPNW members; 3,167 were selected for the SS cohort and 25,050 for the SO cohort. From this sample, we received a total of 3,894 surveys and 574 stool specimens. On an unweighted basis, we observed a higher proportion of responses for those <5 years of age, >45 years of age, female, and of non-Hispanic ethnicity; we observed a lower proportion of responses among those 18–44 years of age . After weighting, we observed a similar distribution across demographic characteristics between survey responders and the KPNW membership. The weighted mean age of participants was 40.1 years; weighted percentage by sex was 52% female and by race was 81% White .\n\n【26】Overall, 395 participants met our primary AGE case definition, resulting in a 30-day AGE age-weighted prevalence of 10.4%, equivalent to a rate of 1.27 cases/person/year. Among those participants, 23% reported both diarrhea and vomiting, 50% reported only diarrhea, and 27% reported only vomiting. A total of 289 participants reported having acute diarrhea, resulting in a 30-day diarrheal prevalence of 7.6%, equivalent to a rate of 0.92 cases/person/year . A total of 124 participants (3.2%) reported having had 1–2 loose stools but did not meet criteria for either case definition.\n\n【27】We observed no significant difference in prevalence estimates between male and female participants (p = 0.264). When examined by age, the prevalence of AGE illness was highest among the youngest age group (0–4 years, 13.5%) and lowest among the oldest (>65 years, 6.4%). For acute diarrhea, the highest prevalence occurred among those 18–44 years of age (10.2%) and those 5–17 years of age (3.3%). Those 0–4 years and 5–17 years of age had comparatively low prevalence (7.8% and 3.3%, respectively) when using the diarrhea-based definition, compared with a prevalence of 13.5% and 10.6%, respectively, when using the primary case definition . We observed monthly variability in the occurrence of AGE, but we observed no statistically significant seasonal patterns .\n\n【28】##### Healthcare Encounters\n\n【29】Overall, 80 (19%) persons with AGE had \\> 1 AGE-related healthcare encounters within KPNW. Most of those (63 \\[79%\\]) had an in-person encounter, 37 (46%) of whom also had a remote encounter; 17 (19%) had only a remote encounter . The percentage of participants seeking AGE-related medical care was slightly lower among persons reporting only acute diarrhea; 17% had \\> 1 encounter overall, of which 77% had an in-person visit and 23% had a remote encounter only.\n\n【30】##### Pathogen Testing\n\n【31】In total, 574 SS study participants (with and without AGE) returned stool samples. On average, stool samples were collected within 10 days of survey completion; 83% of samples were collected within 2 weeks of survey completion. Of the samples, 570 (99%) were usable and tested at OSPHL, 70 collected from persons who met our primary definition for AGE and 500 collected from those who did not. As a weighted percentage, those totals yield an estimated 30-day AGE prevalence of 12.3% (95% CI 8.8%–16.5%) in this group. Overall, norovirus and rotavirus were the most commonly detected viral pathogens . Using this sample, we estimated that 22.4% (95% CI 9.6%–40.8%) of our total KPNW population with AGE would test positive for \\> 1 viral pathogen. An estimated 7.2% (95% CI 2.0%–17.5%) would test positive for norovirus alone, 11.5% (95% CI 2.4%–30.1%) for rotavirus alone, 3.5% (95% CI 0.1%–19.0%) for sapovirus alone, and <1% for both norovirus and rotavirus. Using the sample of 500 specimens from persons without AGE, we estimate that 8.2% (95% CI 5.5%–11.7%) would test positive for \\> 1 viral pathogen: 1.4% (95% CI 0.4%–3.4%) for norovirus alone, 5.8% (95% CI 3.5%–9.0%) for rotavirus alone, 0.98% (95% CI 0.32%-2.32%) for sapovirus alone, and 0.59% (95% CI 0.12%-1.74%) for astrovirus alone.\n\n【32】Of the 23 total stool specimens that tested positive for norovirus, 5 were determined to be genogroup I and 18 to be genogroup II. Forty of the 41 stool specimens (98%) testing positive for rotavirus by qRT-PCR at OSPHL were available for confirmatory testing at CDC. Of those, 1 (3%) was determined to be rotavirus vaccine shedding, and 3 (8%) asymptomatic persons tested positive by rotavirus EIA (however, all 3 had high Ct values, indicating lower viral load, and could not be genotyped).\n\n【33】### Discussion\n\n【34】Our study results confirm that use of an expanded case definition that includes persons reporting only vomiting increases prevalence estimates by ≈50% compared with a definition that includes only diarrhea . Our 30-day AGE prevalence estimates are broadly consistent with the range observed in previous literature, although the use of differing case definitions make direct comparisons more complex. Reporting our results in 2 ways improves our ability to compare our findings to those of others; however, doing so reduces the consistency observed between our prevalence estimates. For our AGE case definition, we observed a prevalence estimate of 10.4%, compared with Canada’s Foodbook estimate of 5.7% in 2014–2015 . For our diarrhea-only case definition, our estimate of 7.6% was lower than FoodNet’s estimate of 10%–11% from 1996–1999 . Those differences highlight the inherent variability in estimates of AGE, which may reflect variations in occurrence due to geography, time, or other factors.\n\n【35】Previous work has argued for use of the expanded AGE case definition . Excluding symptoms of vomiting has been associated with decreased sensitivity for identifying norovirus infections . Further, research has shown age-related differences in AGE symptom profiles, particularly with vomiting . For instance, 1 study found that vomiting was reported for 37% of AGE patients <5 years of age, compared with only 17% of those 65–74 years of age . This finding is further supported by our data, where we saw a lower prevalence of AGE among participants 0–4 years of age when we used the case definition that did not include vomiting (7.8%) compared with the definition that did (13.5%). Considering that norovirus is the most common cause of AGE among children younger than 5 years , use of an expanded AGE surveillance case definition will yield more complete estimates of norovirus burden.\n\n【36】Nearly 1 in 5 (19%) of our respondents sought medical care for their AGE symptoms, consistent with behaviors reported in other US studies, even where 8%–9% did not have insurance coverage . Our study is unique in describing AGE-related healthcare seeking behavior that includes not only telephone consultations with a clinician for illness management but also other remote encounters, such as email exchange via patient portal and video appointments. One previous FoodNet publication asked whether respondents made a call to a medical provider, but it is unclear whether the calls were used to discuss management of clinical symptoms (in place of in-person visits) . KPNW encourages members to use those remote technologies for initial access to medical providers to make healthcare more accessible; to reduce the burden of in-person visits to medical offices; and to reduce the risk for transmission of communicable diseases to other members, staff, and clinicians. Whereas most clinical care was in-person, 19% of our population exclusively used remote encounters. As healthcare delivery systems increasingly expand access to virtual care, more research is needed to determine how this shift affects the burden of AGE on the healthcare system. If our population were generalizable to the US population, there would be an estimated 415 million AGE illness episodes per year, with an associated 5.4 million in-person AGE-related encounters. Our novel findings of an extrapolated 1.2 million remote AGE-related healthcare encounters indicate a potential higher burden on the healthcare system due to AGE that has not been well-captured in previous studies.\n\n【37】We observed an overall pathogen positivity of roughly 10% from among all submitted stool specimens; differences observed in pathogen positivity between those who did and did not have AGE were not statistically significant. This finding is likely because of the small numbers within our SS cohort and the potential time lag between occurrence of symptoms and collection of stool sample, as well as the high rate of rotavirus detection by qRT-PCR. Further work in this area is needed, but our study was powered to calculate the prevalence of viral pathogens among asymptomatic persons, rather than to detect significant differences between those with and without symptoms of AGE. Among those not meeting the AGE case definition, we observed a norovirus prevalence of 1.4% and a rotavirus prevalence of 5.8% by qRT-PCR; both values are slightly lower than previously published estimates of 4% and 11%, respectively . The prevalence of rotavirus detection is higher when using qRT-PCR versus EIA testing, because of the increased sensitivity of PCR tests for detecting viral pathogens at a lower viral load , which is supported by our findings. Because lower levels of shedding may be less clinically relevant, EIA continues to be preferred for routine surveillance rotavirus testing . However, the detection of both norovirus and rotavirus from persons without recognized AGE highlights a potential reservoir for sporadic AGE within the community, although asymptomatic infections are believed to be less contagious. Even so, interventions designed to reduce the transmission of AGE-related pathogens are important to follow for both symptomatic and asymptomatic persons.\n\n【38】A key strength of our study is that participants were selected via an age-stratified, representative sample of KPNW enrollees, which is reflective of the underlying population base. Consequently, we have been able to more accurately calculate the estimated number of AGE episodes per person per year when compared with other studies, and our findings are generalizable to the target population of this area. Further, conducting this study within our population reduces the likelihood that access to care is a barrier to seeking treatment in the healthcare system. Conversely, our study may have been limited by a low overall participation rate of 13.8%. This percentage is higher than reported in a comparable study from the United Kingdom , and we exceeded our sample size goal by obtaining 3,874 completed surveys and 574 stool specimens; however, the percentage is lower than those for other published studies of comparable design, such as FoodNet . We attempted to minimize the effects of this bias by using weighting that incorporated a nonresponse correction factor to improve the generalizability of our findings. Although we recognize the potential for recall bias, because our findings are based on reports of AGE within the previous 30-days, previous work examining the effects of a 7-day versus 1-month recall period found no difference in monthly prevalence estimates . Therefore, we believe any effect on our findings will be minimal. Our study may also have been limited by a delay between survey completion and stool sample collection, resulting in an underestimate of the prevalence of viral pathogens among those meeting our AGE case definition during the 30-days before survey completion. Because our study was powered to detect the prevalence of viral pathogens among those asymptomatic for AGE, we believe the effects on our findings would have been minimal.\n\n【39】In conclusion, AGE continues to exert a substantial burden of disease within the population, as well as upon healthcare delivery systems. This effect is particularly notable when vomiting is considered as part of the AGE case definition; prevalence estimates were nearly 50% higher when including this symptom. Our findings also provide key estimates of the prevalence of asymptomatic shedding of AGE-related viral pathogens, which can help contextualize viral prevalence data from AGE cases to assess disease burden. General interventions designed to reduce the transmission of AGE-related viral pathogens (e.g. hand hygiene) continue to be crucial as a means to reduce the extent of AGE in the population, even among persons without symptomatic disease. However, the high number of AGE cases in the community, particularly when including vomiting-only symptoms, leads to a heavy burden on the healthcare system. Additional targeted interventions, such as vaccines, could help reduce AGE in the community and, thus, reduce strain on healthcare systems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d1bd294f-1db8-4e4d-bee2-d59368b53605", "title": "Whole Grain Intake and Impaired Fasting Glucose in Adolescents, National Health and Nutrition Examination Survey, 2005–2014", "text": "【0】Whole Grain Intake and Impaired Fasting Glucose in Adolescents, National Health and Nutrition Examination Survey, 2005–2014\nAbstract\n\n【1】**Introduction**\n\n【2】Large prospective cohort studies show a lower risk of developing type 2 diabetes among adults with higher whole grain consumption. Less is known about the relationship between whole grain consumption and precursors for diabetes risk in adolescents. We examined whether intake of whole grains was associated with impaired fasting glucose (IFG) in adolescents.\n\n【3】**Methods**\n\n【4】We analyzed data on dietary intake from an average of two 24-hour diet recalls from fasting, nondiabetic adolescents aged 12–18 years (N = 2,286) across 5 cycles of the National Health and Nutrition Examination Survey (NHANES 2005–2014). We used logistic regression to calculate the odds of having IFG (100–125 mg/dL) with respect to servings of whole and refined grains, as well as percentage of whole grains, adjusting for sex, age, race/ethnicity, annual household income, obesity, total energy, and diet quality.\n\n【5】**Results**\n\n【6】IFG was present in 17% of participants. After adjusting for covariates, number of servings per day of whole grains was significantly associated with lower odds of IFG, but there was no relationship between IFG and servings of refined grains or percentage of whole grains. Consuming at least 1 ounce-equivalent serving (16 g) of whole grains daily, compared with consuming no whole grains, was associated with a 40% reduction in the adjusted odds of having IFG (adjusted odds ratio = 0.60; 95% CI, 0.38–0.93).\n\n【7】**Conclusion**\n\n【8】Analysis of 10 years of national cross-sectional data suggests that US adolescents whose daily diets consist of a minimum threshold amount of whole grains may be less likely to have IFG, a finding that has implications for diabetes prevention in adolescents.\n\n【9】Introduction\n\n【10】Observational and experimental data suggest a protective relationship between whole grain consumption and type 2 diabetes. Decades of prospective cohort data with the Nurses’ Health Study (with more than 1 million person years of follow-up) demonstrated a 28% lower relative risk of type 2 diabetes incidence among women who consume more whole grains, even after adjusting for weight status, which is a strong predisposing factor for development of diabetes . Among men in the Health Professionals Follow-up Study, consumption of whole grains exerted a stronger protective effect against incident diabetes than other components of the examined “prudent dietary pattern” (ie, fruit, vegetables, and fish) . Recent evidence from European cohort data suggests that consuming 3 servings per day of whole grain foods (compared with consuming a half serving) could reduce risk of incident type 2 diabetes by 20% . Evidence about whole grain cereal fibers in particular shows a high protective effect. A recently published meta-analysis of 8 cohort studies with an average of 12.6 years of follow-up (adults aged 35–79 years) demonstrated a 32% lower risk of developing type 2 diabetes  that was nearly identical to the estimate from a meta-analysis published more than a decade ago .\n\n【11】However, considerably less is known about the relationship between whole grain consumption and risk for type 2 diabetes in children and adolescents. This discrepancy represents a gap in knowledge and a critical opportunity for more investigation, given the rising prevalence of type 2 diabetes among US youth . To our knowledge, the only study that has examined whole grain intake and diabetes risk in youth examined 285 Minnesota adolescents and demonstrated that whole grain intake was associated with greater insulin sensitivity (measured with euglycemic clamp). This relationship was strongest among adolescents with high body mass index (BMI, kg/m 2  ), although it was still significant even after adjustment for adiposity, which independently increases diabetes risk . Although type 2 diabetes is rising in adolescents, it is still a rare outcome (estimated in 2012 to be 12.5 cases per 100,000 youth) . However, although insulin sensitivity is a relevant marker for predicting type 2 diabetes, not all clinically meaningful diagnostic end points have been considered. Fasting glucose is collected widely in large population samples such as the National Health and Nutrition Examination Survey (NHANES), and having a fasting glucose between 100 and 125 mg/dL meets diagnostic criteria for prediabetes .\n\n【12】Despite the demonstrated health benefits associated with habitual whole grain consumption, daily intake in the United States is inadequate and estimated to be approximately 1 ounce-equivalent serving (1 oz-eq) per day for adults  and as low as a half serving (0.5 oz-eq) per day for adolescents . An ounce-equivalent serving of whole grains contains at least 16 g of whole grains . Studies in adults suggest that there is lower risk of developing type 2 diabetes when consumption of whole grains is 2 to 3 servings per day . Evidence from the aforementioned experimental study of adolescents demonstrated that consumption of 1.5 oz-eq per day of whole grains was associated with lower BMI and greater insulin sensitivity , but further exploration using diagnostic clinical end points is warranted to more fully understand the diabetes risk spectrum.\n\n【13】This analysis was conducted to address 2 gaps: 1) characterizing the unexplored relationship between whole grain intake and a clinical marker of diabetes risk in adolescents, and 2) evaluating a potential threshold at which a minimum level of intake confers benefit. This study aimed to test our central hypothesis that whole grain intake would be positively associated with decreased odds of impaired fasting glucose (IFG) among adolescents.\n\n【14】Methods\n\n【15】### Study population\n\n【16】We examined data on adolescents aged 12 to 18 years who participated in NHANES, a complex, multistage probability, cross-sectional survey designed to be representative of the US civilian, noninstitutionalized population. We examined five 2-year data cycles (2005–2006, 2007–2008, 2009–2010, 2011–2012, and 2013–2014) and limited the sample to participants who were selected to be in the fasting subsample (morning visit) and had fasted for at least 8 hours. Pubertal status was added to the continuous NHANES questionnaire only starting in 2011–2012, with provided Tanner staging drawings, and data have not yet been released to the public.\n\n【17】Individuals were excluded if they had a self-reported or laboratory-confirmed (glucose ≥126 mg/dL) diagnosis of diabetes (n = 14); were pregnant (n = 12); or were missing data on height and/or weight (n = 27), complete diet recall (n = 87), income (n = 152), or laboratory data (n = 5). This yielded a final sample of 2,286 participants.\n\n【18】### Measures\n\n【19】Demographic information obtained during the survey included the adolescent’s age and sex. Race/ethnicity were used to categorize participants into 4 categories: Hispanic, non-Hispanic White, Non-Hispanic Black, and other. Annual household income and family size are used with each cycle of NHANES to calculate the index of family income to the federal poverty level (FPL) in accordance with the poverty guidelines of the US Department of Health and Human Services .\n\n【20】Height and weight were measured by trained personnel at the mobile examination center using standardized protocols . Age- and sex-specific BMI percentiles were calculated using codes provided by the Centers for Disease Control and Prevention, classifying participants as normal weight (BMI <85th percentile), overweight (BMI 85th to <95th percentile), obesity (BMI ≥95th percentile to <120% of the 95th percentile), and severe obesity (BMI ≥120% of the 95th percentile) .\n\n【21】All participants included in the subsample had been fasting for at least 8 hours . Plasma fasting glucose was examined using hexokinase-mediated reaction, which is the reference analytic method. Per recommended analytic guidelines, we used Deming regression equations to be able to include fasting glucose data from 2005–2006 with data from 2007–2008 . According to the American Diabetes Association definition, fasting glucose between 100 mg/dL and 125 mg/dL is considered impaired and meets the criteria for prediabetes .\n\n【22】A 24-hour diet recall was conducted in person at the mobile examination center, and a second recall was collected via telephone. The Food Pattern Equivalents Database (FPED) was used to convert foods and beverages reported in NHANES to US Department of Agriculture food pattern components . The FPED database reports grain consumption in terms of ounce-equivalents to align with the food group serving definitions used in the 2005 Dietary Guidelines for Americans (eg, one slice of bread; a cup of cereal or a half cup of hot cereal, cooked pasta, rice, or other grain such as bulgur, oatmeal, and cornmeal) . The average of the 2 recalls was used for this analysis.\n\n【23】The primary dietary outcome of interest was consumption of whole grains, reported in the FPED database as ounce-equivalents of whole grains. Whole grains were also examined as a proportion of total grain intake, calculated as the ratio of ounce-equivalents of whole grains divided by the sum of whole grains and non-whole (refined) grains. The mean intake of refined grains (oz-eq/d) was also examined. The Healthy Eating Index 2010 (HEI 2010), which is scored from 0 to 100 points and quantifies adherence to the 2010 Dietary Guidelines for Americans, was calculated using a publicly available scoring algorithm .\n\n【24】We also tested to determine a meaningful threshold of whole grain consumption. We used 0 oz-eq (<31st percentile), 0.5 oz-eq (57th percentile), and 1.0 oz-eq (77th percentile) as cutoff points to approximate the 25th, 50th, and 75th percentiles in the following 4 categories: 0 oz-eq/d, 0 to less than 0.5 oz-eq/d, 0.5 oz-eq/d to less than 1 oz-eq/d, and 1.0 oz-eq/d or more.\n\n【25】### Statistical analyses\n\n【26】We used survey weights to account for the complex, multistage probability sampling design used in NHANES in accordance with recommendations from the National Center for Health Statistics . We used fasting subsample exam weights to appropriately reflect sampling methodology. Analyses were conducted using Stata 15.1 (StataCorp LLC).\n\n【27】For descriptive analyses, participants (all non-diabetic) with normal fasting glucose were compared with those with IFG by using design-based _χ_ 2  tests for categorical variables (eg, race/ethnicity) and survey-weighted univariate linear regression for continuous variables (eg, age).\n\n【28】Logistic regression was conducted to evaluate odds of prediabetes (defined here by IFG) associated with the following continuous variables: whole grain consumption, refined grain consumption, and percentage of total grains that were whole. Multivariable models were adjusted for sex, age, race/ethnicity, income, and BMI, as these are known to be associated with prediabetes and whole grain intake . Furthermore, models were adjusted for total energy intake (average kcals consumed per day) and overall dietary quality (using HEI 2010), as is commonly done in nutritional epidemiologic studies . Because pubertal status was not available and because the pubertal growth spurt is known to decrease insulin sensitivity , sensitivity analysis was conducted to evaluate the subset of participants who were aged 16 to 18 years and presumably all post-pubertal. We assessed obesity as a potential mediator between whole grain consumption and IFG, using the causal mediation approach (medeff with jackknife) by Hicks and Tingley . Significance for all results was set at _P_ < .05.\n\n【29】Results\n\n【30】### Demographics\n\n【31】Participants included in the final sample (N = 2,286) and the 297 excluded individuals differed by race/ethnicity ( _P_ \\= .01): those who were excluded had a smaller percentage of non-Hispanic White individuals (47% compared with 60%) and a larger percentage of Hispanic (25% vs 19%), non-Hispanic Black (17% vs 14%), and other/mixed (10% vs 7%) individuals. However, on every other characteristic, as well as the proportion with IFG, there were no significant differences between those included and those excluded.\n\n【32】The survey-weighted prevalence of IFG in the sample was 17.2%. The presence of impaired compared with normal fasting glucose varied significantly ( _χ_ 2  test, _P_ < .05) by sex (ie, 71% of those with IFG were male), by race/ethnicity (ie, Hispanics comprised 16.9% of those with normal glucose but 26.9% of those with IFG), and degree of obesity (ie, the proportion with IFG compared with normal glucose was double among those with severe obesity) . Individuals with IFG also had significantly higher mean daily caloric intake (2,098 kcal vs 1,974 kcal; _P_ < .05), and lower income level (231% of the FPL vs 266% of the FPL).\n\n【33】Whole and refined grain consumption also varied with respect to variables associated with IFG. Consumption of whole grains was associated with lower BMI percentile, higher income, higher diet quality (as measured by HEI 2010), and higher total energy intake ( _P_ < .05 for all). Compared with non-Hispanic Whites, Hispanics and non-Hispanic Blacks had lower intake of whole grains ( _P_ < .05). Consumption of refined grains was associated with lower diet quality (per HEI 2010) and higher total energy intake (ie, calories) ( _P_ < .05), but it was not associated with income or BMI. Compared with non-Hispanic Whites, Hispanics had higher intake of refined grains ( _P_ < .05), but they were the only group with this association.\n\n【34】### Whole and refined grains and odds of IFG\n\n【35】Individuals with IFG had lower whole grain intake (0.54 vs 0.69 oz-eq/d) and higher refined grain intake (6.93 vs 6.32 oz-eq/d) ( _P_ < .05 for both) . In unadjusted logistic regression, each incremental serving of whole grains was associated with a 19% reduction in the odds of IFG (odds ratio \\[OR\\] = 0.81; 95% CI, 0.69–0.96). This finding remained significant in a multivariable model that adjusted for sex, age, race/ethnicity, obesity category, total energy (kcal/d), and dietary quality index (per HEI 2010), with a 23% reduction in odds of IFG (aOR = 0.77; 95% CI, 0.65–0.91) . The adjusted odds of IFG among adolescents aged 16 to 18 years was of comparable magnitude and was also significant (aOR = 0.72; 95% CI, 0.54–0.96).\n\n【36】Unadjusted analysis indicated a small increase (4%) in odds of IFG with each additional serving of refined grains (OR = 1.04; 95% CI, 1.01–1.08), but the effect was no longer significant in the adjusted multivariable model (aOR = 1.02; 95% CI, 0.96–1.08) . Percentage of whole grains was not significantly associated with odds of IFG in either unadjusted or adjusted models .\n\n【37】Whole grains were associated with both obesity and IFG. The interaction between whole grain consumption and obesity (exposure–mediator) was small and not significant, and the percentage of the total effect of whole grain consumption mediated by obesity was less than 1%. Refined grains were not significantly associated with obesity in either univariate or adjusted analyses.\n\n【38】### Examining for threshold effect with whole grain intake\n\n【39】Odds of IFG by percentile of whole grain consumption (25th, 50th, and 75th) are presented in Table 3 . Compared with participants who had consumed no whole grains, participants whose whole grain consumption was less than 1 oz-eq/d did not appear to have any protective effect from their whole grain consumption. However, compared with peers with no whole grain consumption, adolescents who had consumed at least 1 oz-eq/d had a 40% reduction in odds of having IFG (aOR = 0.60; 95% CI, 0.38–0.93), even after adjusting for sex, race/ethnicity, diet (energy intake and diet quality index), and adiposity. Sensitivity analysis of the subsample of older adolescents aged 16 to 18 years also showed significant reduction in adjusted odds of IFG (aOR = 0.49; 95% CI, 0.24–0.97).\n\n【40】Discussion\n\n【41】Results from this study demonstrate a significant association between whole grain consumption and IFG among US adolescents, and the study was the first to examine a clinical end point, IFG, in a nationally representative sample of adolescents. The prevalence of type 2 diabetes among children and adolescents in the United States is rising , and as many as one-quarter of adolescents have prediabetes . This analysis highlights the need for consideration of whole grain consumption in education and intervention efforts regarding diabetes prevention in youth.\n\n【42】We aimed to determine whether a threshold of consumption exists above which adolescents experience meaningful benefit; we found lower odds of having IFG among adolescents who consume at least 1 oz-eq serving of whole grains per day. In 2000, around the time of the introduction of whole grains as a priority area in the Dietary Guidelines for Americans , whole grain consumption in the United States was approximately one-half serving per day for children aged 6 to 18 . Modest gains in consumption have occurred over time, with levels closer to three-quarters of a serving per day for children in 2011–2012 . However, socioeconomic disparities with respect to whole grain consumption are growing. An analysis of whole grain consumption in US adolescents from 2005–2012 showed that, although whole grain consumption rose during this interval, low-income (<200% of the FPL) adolescents continued to eat only 0.5 servings per day while their more affluent peers had doubled intake to 1.0 servings per day . This discrepancy highlights a priority for strategies to increase whole grain consumption among low-income adolescents to reduce future disparities in diabetes risks by socioeconomic status.\n\n【43】Many proposed mechanisms explain why whole grains may be beneficial for glucose control. Researchers have long recognized that fiber-rich whole grains increase satiety and fullness, are high in antioxidants, and have dietary fibers that are fermented in the digestive tract, leading to a slower and steadier rise in blood glucose compared with the rise from processed grains with a higher glycemic index (ie, ranking of carbohydrate-containing foods according to their effect on blood glucose levels). Whole grains are also high in bioactive compounds like magnesium (which is particularly high in whole grain cereals), which increases insulin secretion and glucose clearance from the blood . Newer hypotheses are focusing on examining how the many bioactive compounds found in whole grains may operate synergistically as a “whole grain package” .\n\n【44】Excess adiposity is a risk factor for diabetes, with improvement in risk seen with relatively small (5%–7%) decreases in weight . In this sample, odds of IFG increased proportionally with weight status, and adolescents with severe obesity had 2.5 times the odds of having IFG compared with their normal-weight counterparts. The inverse relationship between (higher) whole grain consumption and (lower) odds of IFG remained significant after adjustment for weight status (and for dietary quality and caloric intake). Furthermore, mediation analysis showed that, although whole grains were associated with both obesity and IFG, obesity was not mediating the relationship between whole grain consumption and IFG. This finding suggests that beyond the known benefits a diet high in whole grains can have with respect to weight status, there may be other independent benefits that consumption of whole grains has with respect to glucose regulation in adolescents.\n\n【45】Practitioners and clinicians should be mindful of the impact of their recommendations to patients regarding whole grain consumption. A simple message to merely add whole grain foods to an unhealthy baseline of refined grains increases the total amount of grain-based carbohydrates without replacing refined grains with whole grains and, thus, misses the mark. Further complicating the task of messaging is the lack of consistency in what makes it possible to market a food as being “whole grain” . Foods containing exclusively whole grains (ie, 100% whole grain) have the highest nutritional value and are preferentially recommended by the Dietary Guidelines for Americans . However, package labeling regulations with minimal requirements regarding whole grain content (eg, promotion of products with as little as one-half serving of whole grains) frequently draw consumer attention to processed foods containing some whole grains but which are higher in sugar and calories .\n\n【46】This analysis has limitations and strengths. First, the data are cross-sectional, so causal relationships cannot be determined. Higher whole grain consumption in adolescents appears to be associated with lower odds of having IFG, but we cannot conclude from these data that increasing one’s whole grain intake will directly lower the risk of developing IFG. To determine this, controlled experimental studies are required. Another limitation is that physical activity was not included as a covariate because the question items regarding moderate to vigorous physical activity (and screen time) changed, so there was no consistent variable during the time frame studied. Obesity was assessed with BMI, which does not provide differentiation between fat mass and lean mass, another limitation of the study. Strengths of this analysis are the complex sampling technique of NHANES, which allows for creating generalizable, nationally representative estimates. Although many large studies estimate whole grain intake with diet screeners that may include only a few whole grain foods such as cereal fibers, these data include quantification of whole grain intake with two 24-hour diet recalls.\n\n【47】Our analysis of a nationally representative sample of adolescents demonstrated that higher whole grain consumption is associated with lower odds of having IFG, which constitutes a clinical diagnosis of prediabetes, even after adjusting for obesity. Specifically, eating 1 serving of whole grains per day may reduce the odds of IFG by 40%. These findings suggest that a diet high in whole grains may be associated with lower diabetes risk in American adolescents.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cb87365b-8930-4bd0-a672-ac68460fa6bb", "title": "Chagas Disease in a Domestic Transmission Cycle in Southern Texas, USA", "text": "【0】Chagas Disease in a Domestic Transmission Cycle in Southern Texas, USA\n### The Study\n\n【1】Chagas disease is caused by the parasitic protozoan _Trypanosoma cruzi_ and affects an estimated 12 million persons throughout South and Central America and Mexico . In the United States, the disease exists almost exclusively as a zoonosis; only five autochthonous insect-borne cases have been reported in humans . The distribution of Chagas disease in the United States includes approximately the southern half of the country. Twelve species of triatomines are known to occur in the United States, the most important being _T. sanguisuga_ in the eastern United States, _T. gerstaeckeri_ in the region of Texas and New Mexico, and _T. rubida_ and _T. protracta_ in Arizona and California .\n\n【2】In the small community of San Benito, Texas , after three pet dogs died from Chagas cardiomyopathy, personnel from the Texas Department of Health, the Cameron County Health Department, Environmental Health Division, and the Centers for Disease Control and Prevention (CDC) inspected the owner’s home, garage, and grounds for potential triatomine insect vectors . Blood was drawn from four dogs and two persons residing on the property and tested for antibodies to _T. cruzi_ . A second site approximately 2 miles away was also inspected and blood drawn from three dogs, one of which had been diagnosed as positive for _T. cruzi_ by the original veterinarian. A follow-up serologic survey was conducted to determine the percentage of the stray dogs in Cameron County that would test positive for Chagas disease antibodies. Once a week, samples from stray dogs were shipped to CDC for testing. Each sample was issued an identification number; and information on the animal’s location, sex, age, health condition, and size was recorded. Serum specimens were tested for anti- _T. cruzi_ antibodies by indirect immunofluorescence (IIF) .\n\n【3】Ecologic niches and potential geographic distributions were modeled by using the Genetic Algorithm for Rule-set Prediction (GARP)  _._ In general, the procedure focuses on modeling ecologic niches, the conjunction of ecologic conditions within which a species is able to maintain populations without immigration. Specifically, GARP relates ecologic characteristics of known occurrence points to those of points randomly sampled from the rest of the study region, seeking to develop a series of decision rules that best summarizes those factors associated with the species’ presence. Recently, this method has been used to study the distribution of species complex members and vector-reservoir relationships with respect to Chagas disease .\n\n【4】Inspection of the residence where the three dogs lived indicated a substantial infestation with the triatomine species _T. gerstaeckeri_ . Triatomines were collected under cement slabs of a backyard patio adjacent to the house and from a garage located approximately 75 feet from the home . Of 31 live triatomines collected, including adults of both sexes and immature stages (i.e. two fifth-instar nymphs), 24 contained _T. cruzi_ \\-like parasites in their hindgut . Cultures were established from triatomine urine collected from insects that were fed in the laboratory and placed in 1.5-mL microcentrifuge tubes. Approximately 50 µL of clear urine was injected into Novy, Nicolle, & MacNeal medium culture . The cultures were positive for parasites confirmed to be _T. cruzi_ , on the basis of morphologic criteria. Inspection of the second residence failed to indicate a bug infestation; however, the pet owner recalled frequently observing both rats ( _Rattus_ spp.) and opossums ( _Didelphis virginiana_ ) on the premises. At the first site, three of the four dogs tested positive for _T. cruzi_ , with titers ranging from 1:128 to 1:256. Neither of the two persons had positive antibody titers against _T. cruzi_ . At the second site, only the previously diagnosed dog tested positive, with a titer of 1:256. The other two dogs tested negative, as did the pet owner. Serum samples from stray dogs from Cameron County, Texas, were tested for anti– _T. cruzi_ antibodies. Of 375 dogs tested, 28 (7.5%) were positive by IIF, with titers ranging from 1:32 to 1:512. The sensitivity of this test in humans is 98.8% . Because of the low specificity of serologic tests for distinguishing _T. cruzi_ from _Leishmania_ spp. all positive samples were tested for antibodies to _L. donovani_ . A low level of cross-reactivity was observed in 17 of the 28 samples. In each case, however, the titer was 1–2 dilutions less than the titer to _T. cruzi_ , indicating a primary response to _T. cruzi_ rather than to _Leishmania_ spp. Ecologic niche models for _T. gerstaeckeri_ were developed by using GARP, based on published and unpublished collection records from Mexico and the southwestern United States. The model predicted a distribution for this species that extends from central Mexico, through central Texas, the Texas panhandle, into northern Texas and southeastern New Mexico .\n\n【5】### Conclusions\n\n【6】_Triatoma gerstaeckeri_ is considered a sylvatic species, most frequently associated with pack rat ( _Neotoma_ spp.) burrows . Although individual triatome insects occasionally invade domestic dwellings throughout the southwestern United States and Mexico , this species has not been reported to colonize these habitats. In this investigation, colonization appears to have occurred, based on the observation of large numbers of bugs, including ones in immature stages. In the Chagas disease–endemic regions of South and Central America, the primary risk for insect transmission to humans is related to the efficiency with which local vector species can invade and colonize homes, resulting in a domestic transmission cycle for what is otherwise exclusively a zoonotic disease in the southern United States. In disease-endemic countries, higher house infestation rates generally result in a higher risk of transmission. At the first site in south Texas, six dogs either died or tested positive for _T. cruzi,_ and 24 of 31 bugs contained hindgut trypanosomes. These observations demonstrate the existence of a domestic transmission cycle for an insect species that is typically considered a zoonotic vector. Whether this observation represents an isolated case or actually occurs more frequently but remains unrecognized, indicating an emerging public health problem, remains to be determined. The serologic results in stray dogs are very similar to those reported in previous studies from the region, suggesting that the disease is stably maintained in this reservoir host . The distributional predictions based on GARP models indicate a potentially broad distribution for this species and suggest additional areas of risk beyond those previously reported , should this problem become of greater public health concern.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e956db7b-56c4-42c3-a1e3-4f3bf46f1281", "title": "Alligators as West Nile Virus Amplifiers", "text": "【0】Alligators as West Nile Virus Amplifiers\nThe primary enzootic cycle for West Nile virus (WNV) is between adult ornithophilic mosquitoes and birds, with these mosquitoes occasionally infecting incidental hosts such as horses and humans . Most research to date has focused on these endothermic vertebrate hosts. Other arboviruses infect a variety of ectotherms, including species of lizards , snakes , and turtles , but the knowledge of ectotherm involvement in the ecology of WNV is limited. In the lake frog ( _Rana ridibunda_ ), West Nile viremia capable of infecting mosquitoes  develops, and antibodies develop in Nile crocodiles ( _Crocodylus niloticus_ ) and other ectotherms after natural infection . Experimentally infected North American bullfrogs ( _R. catesbeiana_ ) and green iguanas ( _Iguana iguana_ ) sustain low viremia levels for a short period of time, which suggests that they do not transmit the virus to biting mosquitoes .\n\n【1】In North America, WNV infections in ectotherms were first reported in 2001 . In the years 2001 to 2003, U.S. alligator farms reported substantial economic losses and at least one human case of fever due to WNV outbreaks in juvenile American alligators ( _Alligator mississippiensis_ ) . These alligators were housed in crowded tanks at a constant temperature of 32°C. The mode of transmission, the risk posed to handlers, and role of alligators in secondary WNV transmission cycles are unknown. To assess the potential role of juvenile alligators in the ecology of WNV transmission, we evaluated routes of transmission, determined viremia profiles, evaluated viral persistence in organs, and examined the role of temperature on WNV replication in these animals.\n\n【2】### Materials and Methods\n\n【3】##### Acquiring and Housing Alligators\n\n【4】American alligators were transported to Fort Collins, Colorado, from two U.S. alligator farms: St. Augustine Alligator Farm, St. Augustine, Florida (N = 26, age = 1–2 years, weight = 1–3 kg) and Colorado Gator Farm, Mosca, Colorado (N = 22, age = 10 mo, weight = 200–400 g). Alligators were fed gator chow pellets (Burris Mill and Feed, Franklinton, LA) twice per week (food volume ≈5% of body weight) .\n\n【5】Alligators were divided between two rooms; one room was maintained at 32°C and the other at 27°C. Room temperature and humidity were monitored by HOBO data recorders (Onset, Bourne, MA). Within each room, alligators were placed in livestock tanks (2 m diameter) separated by plastic curtains to reduce cross-contamination between tanks. Each tank contained 15 cm of water at the corresponding temperature (27ºC or 32ºC) and an adequate basking surface. Water was heated with aquarium heaters and aerated with an aquarium water pump. Equipment was checked twice daily, and the water was changed and tanks were disinfected every other day. Rooms were kept dark to calm the alligators (a standard practice at some alligator farms).\n\n【6】##### Mouse Infection\n\n【7】The NY99-4132 strain of WNV, passaged 3–4 times in Vero cells, originally from crow brain provided by W. Stone, New York State Department of Environmental Conservation, Albany, New York, was used in this study. We injected 24 Swiss Webster mice (6–8 weeks of age) subcutaneously with ≈1,000–2,000 PFU of WNV. Mice that developed neurologic signs 7–8 days postinoculation were euthanized and frozen at –70ºC.\n\n【8】##### Alligator Infection\n\n【9】Six alligators in the 32°C room and six alligators in the 27°C room were subcutaneously injected behind the left front leg with ≈7,500 PFU of WNV with a volume of 0.15 mL. Another six animals from each room were fed WNV-infected mice (1/2 mouse per small alligator \\[<700 g\\] and 1 mouse per larger alligator \\[>700 g\\]). Two noninfected alligators were placed with each infected group to serve as tankmate controls. Eight noninfected alligators served as bleeding controls in each room.\n\n【10】##### WNV Isolation from Serum\n\n【11】Blood samples were collected from each alligator daily for 15 days postinfection for virus isolation (some tankmate alligators were bled daily through day 21). Blood (0.2 mL) was collected from the caudal vein and added to 0.9 mL of BA-1 diluent (composed of Hank’s M-199 salts, 1% bovine serum albumin, 350 mg/L sodium bicarbonate, 100 U/mL penicillin, 100 mg/L streptomycin, 1 mg/L amphotericin B in 0.05 mol/L Tris, pH 7.6), producing an approximate 1:10 serum dilution. Blood samples were centrifuged at 3,750 rpm for 10 min to separate serum from clotted blood and stored at –70°C.\n\n【12】WNV viremia was quantified by plaque assay. Blood samples were serially diluted 10-fold with BA-1 through 10 –8  , and 100 mL of each dilution was added in duplicate to Vero cell monolayers in six-well plates (Costar, Cambridge, MA). Samples were allowed to incubate on the cells for 1 h at 37°C. Cells were then overlaid with 3 mL per well of 0.5% agarose in M-199 medium, supplemented with 350 mg/L sodium bicarbonate, 29.2 mg/L L-glutamine, and antimicrobial drugs as in BA-1. After 48 h of incubation, a second 3-mL 0.5% agarose overlay containing 0.004% neutral red was added for plaque visualization. Plaques were counted on day 4 postinfection.\n\n【13】##### WNV Isolation from Other Samples\n\n【14】Cloacal swab samples were taken from each alligator daily for 15 days postinfection (some tankmate alligators were swabbed daily through day 21 postinfection). A cotton swab was inserted into the cloaca ≈2 cm, rotated, and then placed in a tube containing 1.0 mL BA-1. Virus content was quantified by plaque assay.\n\n【15】Nine alligators (two that died of infection and seven that recovered) were tested for virus in tissues. Tissue samples (≈0.5 cm 3  in size) were harvested from the lung, liver, spleen, heart, kidney, spinal cord, cerebrum, and cerebellum. Samples were trimmed as needed and ground in 1.5 mL BA-1 containing 20% fetal bovine serum with a Retsch MM300 mixer mill (Retsch GmbH & Co, Hann, Germany) (30 cycles/sec for 4 min). Each resulting homogenate was transferred to a 1.7-mL Eppendorf microcentrifuge tube and clarified by microcentrifugation at 7,500 rpm for 3 min. Each supernatant was transferred to a 1.8-mL cryovial (Nalge Nunc International, Rochester, NY) and stored at –70ºC. Virus content was quantified by plaque assay.\n\n【16】Water (0.5 mL) was taken from each tank daily (before cleaning) for 15 days postinfection and then twice per week through day 31 postinfection. Water samples were added to 0.5 mL BA-1 (containing 2x concentrations of antimicrobial drugs). Water samples were pooled according to tank. Half of each pool was used for virus isolation. Water samples were added to 25-cm 3  tissue culture flasks (Corning, Corning, NY) (1 mL per flask) containing Vero cell monolayers. Flasks were rocked every 15 min for 1 h at 37°C, and 10 mL of Dulbecco’s Modified Eagle Medium (Invitrogen, Carlsbad, CA), supplemented with 2% fetal bovine serum, was added to each flask. Flask media were replaced on day 6 postinfection. Flasks were checked daily for cytopathic effect (CPE) through day 10 postinfection. Remaining water samples were tested by Taqman reverse transcriptase–polymerase chain reaction (RT-PCR) .\n\n【17】##### Neutralizing Antibody Detection\n\n【18】Blood samples (0.4–0.6 mL) were collected from each alligator for neutralizing antibody detection twice per week from day 21 postinfection through day 31 postinfection. To detect neutralizing antibodies, 15-μL serum samples from day 21 to day 31 were mixed with 60 μL of BA-1 and 75 μL of a WNV preparation (200 PFU/0.1 mL) in a polypropylene 96-well plate (Costar, Cambridge, MA). The virus-serum mixtures were incubated at 37°C for 1 h to allow for virus neutralization. These mixtures were then tested by plaque assay. Controls employed BA-1 only (cell viability control), serum-free virus mixture with BA-1 only (to enumerate PFU in the challenge dose of virus), and West Nile hyperimmune mouse ascitic fluid (diluted 1:200) mixture with virus (to verify challenge virus identity). Specimens were considered positive for WNV neutralizing antibodies if they reduced a challenge dose of ≈100 PFU of WNV by at least 90% at a serum dilution of 1:10.\n\n【19】### Results\n\n【20】##### Viremia after Parenteral Infection\n\n【21】Every alligator injected with WNV became viremic from days 1 to 3 postinfection . Alligators housed at 32°C became viremic on day 1 or 2 postinfection, while those kept at 27°C became viremic on days 2 or 3 postinfection. Viremia in the 32ºC alligators persisted an average of 10 days with an average maximum WNV titer of 5.7 log 10  PFU/mL (maximum 6.7 log 10  PFU/mL). The alligators housed in 27°C conditions were viremic for ≈14 days and averaged a maximum WNV titer of 5.8 log 10  PFU/mL (maximum 6.1 log 10  PFU/mL). No injected alligators died of the infection.\n\n【22】Tankmates in the 32ºC injected group became viremic on days 10 and 12 postinfection, while the tankmates in the 27°C injected group failed to become viremic . Infection of tankmates in the 32ºC injected group persisted for ≈10 to 12 days, and neither died of the infection.\n\n【23】##### Viremia after Oral Infection\n\n【24】Viremia developed in two alligators from the 32°C room and five alligators from the 27°C room 3–6 days after they ate WNV infected mice . Alligators in the 32°C room remained viremic for \\> 9 days, while the alligators in the 27°C room remained viremic for ≈14 days.\n\n【25】Every alligator in the 32°C orally infected tank eventually became viremic during the experiment, with an average maximum WNV titer of 5.6 log 10  PFU/mL (max 6.2 log 10  PFU/mL) . Tankmate viremia onset ranged from 12 to 24 days after infection. Because we stopped routine daily bleeding after day 15 postinfection, the exact viremia onset days of two alligators in this group are unknown. Also, the average duration of viremia for these alligators cannot be calculated. Two alligators in this group died of WNV infection after 12 or 13 days of viremia.\n\n【26】Both tankmates from the 27°C orally infected group also became infected . One tankmate came into contact with a viremic mouse but did not eat it; this alligator became viremic on day 4 postinfection, and the infection persisted for \\> 14 days. Viremia developed in the other tankmate on day 16 postinfection. Because of the absence of daily bleeding, the duration of viremia is not precisely known.\n\n【27】##### Viral Loads of Cloacal Swabs\n\n【28】Of 29 viremic alligators, 24 had detectable viral loads in their cloacae . All five remaining infected alligators became viremic on the last 1 to 2 days of swabbing or after daily swabbing ceased, so no positive swabs can be reported from them. Viral shedding was detected within 3 days of detectable viremia and, in some instances, was detected on the same day as viremia onset. Duration of shedding lasted 6 to ≥12 days, with an average maximum viral load of 5.2 log 10  PFU/swab (maximum 6.2 log 10  PFU/swab).\n\n【29】##### Viral Isolation from Other Samples\n\n【30】Of 29 infected alligators, 2 died, and WNV was detected in their tissues . No virus was isolated from the seven alligators that recovered from infection. WNV neutralizing antibodies were detected in 100% of infected alligators within 25 days after virus detection. No infectious virus or viral RNA was detected in water samples. Sample volumes were each 0.00013% of the total tank water volume.\n\n【31】### Discussion\n\n【32】In some southern states, alligator farms contribute to the economy as agricultural producers and tourist attractions. A typical operation raises 3,000 alligators each year. The market value of raw products (e.g. meat, hides) from an average adult alligator is ≈$300, and alligator meat typically fetches ≈$5 per pound. In Louisiana alone, the total value of farm-raised alligators is >$16 million . Beginning in 2001, alligator farms in at least four different states suffered substantial economic losses due to WNV outbreaks in young alligators. Public health risks involved in these large outbreaks and the eventual culling of thousands of young alligators are also substantial.\n\n【33】We have shown that sick juvenile alligators carry high viral loads in tissues, which poses a threat to handlers, processors, and consumers, although this risk has not been quantified beyond one reported case in Idaho of human West Nile fever in a handler of imported Florida juvenile alligators. Furthermore, all infected alligators in our study shed WNV from the cloaca, which poses another possible threat to other alligators and to handlers. Although tankmates in our study became infected at a high rate, we cannot conclude with certainty that cloacal shedding is the cause of this direct transmission.\n\n【34】Direct transmission likely plays an important role in the epizootiology of WNV infection in farmed alligators but has not been documented in wild alligators . However, we now know that high levels of viremia develop in young alligators, so WNV infection could likely lead to mosquitoborne transmission as well. In general, viremia reached titers considered to be infectious to _Culex quinquefasciatus_ mosquitoes with the NY99 strain of WNV (5.0 log 10  PFU/mL) in all but three infected alligators . _Cx. quinquefasciatus_ is one of the principal vectors of WNV in the southeastern United States . Numerous species of mosquitoes feed on reptiles as well as birds and mammals and thus could be vectors from alligators to people . The primary WNV amplification cycle is believed to depend on birds and mosquitoes ; however, the maximum duration of viremia in juvenile alligators was >2 weeks, which is longer than that observed in birds (maximum duration 7 days) .\n\n【35】Because most alligator farms raise juvenile alligators at a higher temperature (32°C) than older alligators, the effect of temperature on WNV infection was of interest. The 5°C difference in temperature that we tested did not significantly alter infection rates (Fisher exact test, p = 0.11). In general, alligators housed at 27°C maintained detectable viremia 4–5 days longer than the alligators housed at 32°C, which could be due to an enhanced immune function at the higher temperature. In 1969, Tait et al. discovered that lizards ( _Egernia cunninghami_ ) housed at 30ºC produced higher titers of antibodies at a faster rate than those housed at 25ºC after injection with sheep red blood cells . In our study, WNV neutralizing antibodies developed in all infected alligators within a month of infection; these antibodies were detected in the alligators housed at 32°C an average of 5 days earlier than in the alligators housed at 27°C (data not shown). Although neutralizing antibody circulation is only one part of immune function, previous studies have suggested that multiple aspects of the ectothermic immune system may be affected by body temperature, which is directly affected by environmental temperature .\n\n【36】Transmission of WNV by means other than mosquitoes has been shown in humans , mice , and birds , although some modes of transmission are poorly understood. In our study, alligators were successfully infected by parenteral and oral routes, although infection rates between the parenteral and oral groups differed significantly (Fisher exact test, p < 0.05). All 12 injected and 7 of 12 orally inoculated alligators became viremic. Furthermore, high viral loads in the cloacal samples indicate a possible fecal-oral route of transmission, although no viral RNA was detected in our water samples, probably because of the dilution effect of ≈400 L per tank (a 10 –6  dilution factor). Other transmission routes could include bloodborne transmission, although wounds were observed on only two alligators during the experiment, or direct transmission by contaminated water droplets sprayed onto the conjunctiva or other mucous membranes. Although we apparently sampled water that was too dilute to detect WNV particles, at discrete moments, pockets of highly concentrated virus particles in the water could exist and lead to transmission. Infectious saliva could also contribute to direct transmission, but this factor was not examined in this study.\n\n【37】The only deaths observed in our study were two alligators housed at 32°C and infected by tankmate transmission. These data confirm the observations on the farms that WNV infection kills some alligators. Precise death rates on the affected farms are unknown, but we observed an overall death rate of 7% in this study (2 of 29 infected alligators) 2  . Because of infectious virus in their tissues, these dead alligators represent a potential health threat to handlers, alligator meat consumers, and other alligators. Infectious virus was not isolated from tissues of seven alligators that recovered from infection, which suggests that surviving alligators do not pose a health threat after viremia and cloacal shedding cease (within 4 weeks postinfection).\n\n【38】In summary, juvenile alligators may be competent hosts for WNV. This study showed that juvenile alligators have adequate viremia levels (high-titer and long-lasting) for viral transmission by mosquitoes. Coupled with multiple routes of infection, alligators may play a role in WNV ecology, especially in areas where the density of young alligators is high.\n\n【39】Kaci Klenk was formerly a research fellow with CDC’s National Center for Infectious Diseases, Division of Vector-borne Infectious Diseases, in Fort Collins, Colorado. She is currently a biologist at the United States Department of Agriculture, Animal and Plant Health Inspection Service, National Wildlife Research Center, in Fort Collins. Her main interests are zoonotic diseases with current focus on WNV ecology.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d68c532a-6d8a-4209-ac70-3a6e7eef1c54", "title": "Integrated Serologic Surveillance of Population Immunity and Disease Transmission", "text": "【0】Integrated Serologic Surveillance of Population Immunity and Disease Transmission\nThe potential to combine public health and environmental surveillance data with innovations in machine learning, statistical modeling, and data visualization has contributed to an emerging vision of precision public health, the idea that global health programs should use high-resolution data to guide interventions and direct scarce resources to those who would benefit most . Robust disease surveillance is a cornerstone of global health efforts that range from detecting emerging pathogens and epidemics to the control or elimination of vaccine-preventable diseases, HIV, malaria, and neglected tropical diseases (NTDs)  . Most infectious disease surveillance maintains a single-disease focus. In this perspective, we encourage an integrated approach to surveillance of population immunity and infectious disease transmission. First, we argue that antibody-based methods provide a unique opportunity to augment and integrate surveillance across diverse global health initiatives. Second, we highlight multiple areas for synergy through integration, where the combined result will add more value to public health efforts than independent disease monitoring through vertical programs. Finally, we draw on innovations in laboratory and data science to suggest key ingredients for an integrated serologic surveillance (serosurveillance) platform. Throughout, we show how information generated through an integrated platform can create new opportunities to more quickly and precisely identify public health program gaps, and we draw on examples from many global health programs that use serosurveillance to target and monitor their efforts.\n\n【1】### Serology for Integrated Surveillance\n\n【2】Antibodies are unique among biomarkers in their ability to identify persons with protective immunity to vaccine-preventable diseases and to measure past exposure to diverse pathogens that range from viruses to bacteria, parasitic protozoa, and nematodes. All pathogens leave behind immunologic footprints in the form of antibodies that last for months to years and can be detected by testing dried blood spots or serum samples against panels of well-defined antigens. Antibody response provides an objective and sensitive way to uncover immunization coverage gaps or waning immunity to vaccine-preventable diseases  and monitor a population’s exposure to malaria , enteric pathogens , and many NTDs . Antibody response can also be a key tool to monitor epidemics, such as HIV  and emerging pathogens . Antibody levels reflect past exposure over a period of months to years, so cross-sectional surveys contain an immense amount of information about past vaccination and pathogen exposure .\n\n【3】Enteric pathogen incidence estimated from serosurveillance in European countries is 2–6 orders of magnitude higher than the rates estimated from standard case-based surveillance . This surprising statistic illustrates a key advantage of serosurveillance: most pathogen infections have asymptomatic presentation or mild symptoms, so reported clinical cases typically represent the tip of the iceberg in terms of actual infection. When measured in young children, antibody levels reveal recent often subclinical infections and, when measured on the population level, reflect the degree of disease endemicity, a characteristic that makes serosurveillance extremely useful for monitoring transmission interruption or recrudescence in malaria and NTD elimination settings . Measuring antibodies to vaccine-preventable diseases in young children can track improvements in infant vaccination through routine health services and complement program coverage data .\n\n【4】The development of multiplex serologic assays and the geographic overlap of many communicable diseases creates an opportunity to extend scarce global health resources beyond single-disease testing. For example, in the past 10 years Uganda completed \\> 20 population-based serosurveys, including the Malaria Indicator Survey ; the AIDS Indicator Survey ; the Demographic and Health Surveys ; and NTD transmission assessment surveys in subsets of districts for lymphatic filariasis, onchocerciasis, schistosomiasis, soil-transmitted helminths, and trachoma (annually since 2008). Uganda’s highly monitored population illustrates how integrated serologic testing could potentially reduce the number of surveys required to monitor diverse global health programs. Alternatively, if disease-specific serosurveys remained in place but the number of antigens included in the tests were expanded, this strategy would increase the spatiotemporal resolution of information available across programs. In principle, integrated serosurveillance could include separate laboratory assays run on the same specimens, but multiplexed assays enable the highest efficiency in terms of cost and sample volume requirements. For example, multiplex bead assays on the Luminex platform (Luminex Corporation, Austin, TX, USA) enable the measurement of antibody responses to as many as 100 different antigens with just 1 μL of serum . When samples from a serosurvey in Cambodia were analyzed at the US Centers for Disease Control and Prevention (Atlanta, GA, USA), the cost of adding a tetanus toxoid–coupled bead to a multiplex assay with 19 other antigens was US $0.30/sample, and the total cost of the 20-plex assay was less than that of a double-antigen ELISA developed for tetanus  . In other surveys with the laboratory work performed in-country, our team at the Centers for Disease Control and Prevention estimated that the marginal cost of a 20-plex bead assay was US $20/sample , similar to the cost of the 2 separate ELISAs for measles and rubella.\n\n【5】### Synergy through Integrated Serosurveillance\n\n【6】##### Uncovering Public Health Program Gaps and Overlap in Disease Exposure\n\n【7】Age-structured serosurveys can provide information about age-specific immunity gaps for vaccine-preventable diseases that result from missed vaccination or waning immunity . For example, immunizing reproductive-age women through antenatal care visits is needed for boosting tetanus seroprotection and preventing neonatal tetanus in lower-income countries. Serologic data from Cambodia revealed the success of that country’s maternal and neonatal tetanus elimination program and identified that additional effort is still needed to reach young, first-time mothers  . This example illustrates how analyses of single antibodies can reveal public health program gaps, but the most novel and synergistic opportunities will most likely emerge from concurrent measurement of antibody responses to a broad pathogen panel.\n\n【8】The simultaneous measurement of antibody responses to multiple pathogens could uncover populations with high exposure to multiple infectious diseases and potentially multilayered health disparities. This, in turn, should create opportunities to integrate program delivery rather than relying on multiple vertical programs separately delivering interventions, a paradigm shift that aligns with calls for integrated global health systems . High pairwise correlation (r>0.5) of mean antibody responses to different antigens across geographic clusters in the Cambodia serosurvey suggests overlap in disease exposure and transmission . Cluster-level mean antibody responses reveal regional differences in exposure but also provide multilayered information that could help identify opportunities for coordinated response. For example, clusters in the western and northern regions of Cambodia with the highest _Plasmodium falciparum_ antibody levels also have high antibody levels to the causative agent of lymphatic filariasis ( _Wuchereria bancrofti_ ) and _Strongyloides stercoralis_ but low antibody levels to tetanus toxoid .\n\n【9】The Cambodia survey illustrates how a relatively small panel of antigens in a multiplex assay can extend the value of blood collection, and including a broader panel  could provide even more information in future surveys. Similar measurements among young children would more accurately reflect recent exposure than measurement in adults, which would reflect both short-term and long-lived antibody responses . For pathogens with exposure that begins at birth, measurements in young children capture the key period of age-dependent antibody acquisition that differentiates transmission across populations . High-resolution maps of overlapping serologic responses estimated through geostatistical predictive algorithms could contribute information to integrated surveillance-and-response systems (discussed later) . If extended in this way, multiplex serologic testing could create new opportunities for coordinated and appropriate response across diverse diseases.\n\n【10】##### New Opportunities to Measure Health Burden and Progress toward United Nations Sustainable Development Goals\n\n【11】Integrated serosurveillance would dramatically expand our knowledge of pathogens that are not currently part of routine surveillance in low- and middle-income countries. Serologic data combined with mathematical models were critical to developing global burden of disease estimates for rubella and hepatitis . Similar efforts could augment burden of disease estimates for enteric pathogens, NTDs, and other pathogens that have well-defined antigen targets but lack population-based information about incidence. For example, including antibody responses to diverse enteric pathogens in population-based surveys could improve burden of disease estimates and provide an objective indicator through which we could view progress toward achieving related United Nations sustainable development goals 3.3 (improving health and well-being target 3, which includes reducing waterborne diseases) and 6 (universal access to clean water and sanitation) .\n\n【12】##### New Opportunities for Immunoepidemiology\n\n【13】Integrating multiplex assays into geolocated, population-based surveys would enable an unprecedented characterization of population-level, multipathogen immunologic profiles. Large-scale studies drawing on this resource could lead to new biological insights about how disease occurrence is interrelated and provide clues for future experimental studies to examine mechanisms of immune modulation . For example, helminths infect >800 million persons worldwide, and growing evidence suggests these pathogens influence malaria infection, HIV infection, vaccine effectiveness, and even fecundity through complex immunoregulatory pathways . As another example, studies have linked seasonal El Niño atmospheric conditions to profound shifts in environmentally mediated infections, such as regional-scale cholera epidemics and mosquitoborne pathogen transmission (e.g. malaria, arboviruses) . Antibody titers in young children are a sensitive measure of recent pathogen exposure and have even been shown to fluctuate with seasonal changes in malaria transmission . Because antibody levels reflect symptomatic and asymptomatic infections, measuring antibody responses could reveal a more complete picture of seasonally driven changes in transmission compared with case-based surveillance.\n\n【14】### Path toward Integrated Serosurveillance\n\n【15】##### Specimen Collection\n\n【16】Serum samples and dried blood spots are already collected for programs such as the Demographic and Health Surveys, Malaria and AIDS Indicator Survey, and NTD transmission assessment surveys. With small changes in protocol, samples from these platforms could be used for routine multiplex testing. In most cases, protocol changes will amount to ensuring adequate consent to enable broad-based serologic testing and sending specimens to regional or national laboratories with the capacity for multiplex testing. Broader multiplex testing of routine clinical samples could supplement the information available through these population-based surveys, and expanded testing within other focused serologic surveys could contribute as well . In all cases, adequate safeguards for participant privacy and biosafety will be essential and should follow similar protocols already in place for existing periodic serosurveys (e.g. Malaria and AIDS Indicator Surveys) and global laboratory testing networks . Dried blood spots collected on filter paper from finger or heel pricks are easy to collect, transport, and preserve, and antibody results from dried blood spot eluates are comparable with those obtained directly from serum samples for malaria, some helminths, and a broad range of viral pathogens, including vaccine-preventable diseases and dengue . Additional validation studies would help ensure that field protocols are adequate for the broad set of antigens envisioned in an integrated platform .\n\n【17】A challenge to integrated serosurveillance is that different diseases have different ideal populations for monitoring, governed by disease-specific transmission dynamics. For example, children are a focus of most NTD and vaccine-preventable disease monitoring, but teens and adults are most relevant in surveillance of HIV, tetanus protection, and in some cases malaria (e.g. forest workers). Enteric pathogen transmission is often so intense in low-income settings that most population-level heterogeneity in antibody response occurs in the first few years of life , an age range that is only partly represented in population-based samples. Moreover, HIV surveillance programs have demonstrated that sampling methods that extend beyond population-based surveys and passive clinical surveillance are needed to capture the highest risk groups, and similar methods will probably be necessary for malaria and other pathogens in elimination settings . Aligning optimal sampling methods across diseases is a substantive challenge, but adaptive geostatistical design methods  could potentially start with the information collected during the initial integrated, population-based serosurvey and then augment the information needed for disease-specific control priorities through follow-on, enrichment sampling of persons in ideal age ranges and with high-risk characteristics.\n\n【18】##### Specimen Testing\n\n【19】From a large set of potential antigens , each survey could include a subset tailored to specific disease monitoring objectives, while new antigens could be added in response to emerging threats. For example, including a recombinant chikungunya virus antigen in multiplex testing of an existing filariasis surveillance cohort tracked the virus’s introduction to Haiti during 2013–2014 . Technologic advances on bead-based platforms, such as Luminex, will further extend the number of antigens included in a single assay, and new antigen discovery through high-throughput screening studies will further enrich integrated platforms . A current limitation of the technology is that individual antigen-coupled beads are generally not yet commercially available; future commercial development of these reagents would enable better standardization across laboratories. Bead-antigen coupling and bead degradation can introduce variability into multiplex assays, so the use of standardized operating procedures and a consistent bead preparation for each survey are essential. Another limitation is that global reference serum standards to translate arbitrary units in serologic assays into international unit values are mainly limited to vaccine-preventable diseases. Reference standards for other pathogens, such as non– _P. falciparum_ malarias and NTDs, would add immense value because they would enable direct comparison of quantitative results across surveys and laboratories. Finally, antibody measurements will be most useful if they are integrated into a coordinated repository and testing platform, such as the recently proposed World Serology Bank , which could further streamline laboratory protocols, accessibility to reagents, and funding. Surveillance laboratory networks for vaccine-preventable diseases provide a model for how globally standardized testing can work in practice .\n\n【20】##### Analysis Pipelines to Provide Actionable Information\n\n【21】Integrated serosurveillance will only reduce infectious disease transmission if it translates into actionable information and triggers a response by effective programs. In this context, information must be timely, accurate, and high resolution to be actionable from a programmatic perspective. Generating actionable information at spatial scales much smaller than national or district levels is commonplace in high-income countries and should be a near-term, attainable goal for the rest of the world . Efforts in precision global health exemplify how information could be integrated across serosurveys in space and time; high-resolution estimates of child growth failure, measles immunization gaps, and malaria mortality rates show how advances in computation, modeling, and data science have accelerated the development of new pipelines for processing, analysis, and visualization to support precision public health that spans from village to continental scales . Integrated serosurveillance will be best positioned to contribute to this endeavor if serology measurements flow into efficient data pipelines and analysis methods are general enough to accommodate diverse pathogens. The breadth of antigens incorporated into multiplex assays  means that a single integrated serosurveillance platform could potentially generate spatially explicit estimates of vaccine immunity, malaria transmission, NTD transmission, and HIV incidence.\n\n【22】For antigenically stable pathogens, force of infection can be estimated from cross-sectional surveys with general methods that range in approach from mathematical modeling to nonparametric survival analyses . For infections that lead to partial or transient immunity, it might be possible to extend existing approaches to estimate force of infection among young children, provided that antibody levels remain sufficiently elevated for multiple years. The distribution of infectious disease transmission in populations is often highly heterogeneous in space, and for this reason, malaria and NTD elimination efforts have led to the development of sophisticated data pipelines that aggregate, analyze, and map surveillance data with rapid updates . Mapping antibody response is a relatively underexploited opportunity, and existing platforms could be extended to include multiplex serologic data. Combining antibody levels, seroprevalence, or force of infection estimates with geospatial prediction algorithms could lead to high-resolution, richly layered maps of infectious disease exposure and immunity that would be an immense resource for precision guidance of global public health programs.\n\n【23】##### Financing\n\n【24】Integrated serosurveillance will generate information that is a global public good , and international financing will be essential to support coordination across programs for specimen storage, testing, analysis, and reporting. Coordinated financing would also help ensure harmonization across each step in the collection, testing, and analysis pipeline. As the global community prepares for a world after polio eradication, extending the polio surveillance infrastructure and integrating surveillance across vaccine-preventable diseases has been proposed . World Health Organization reference laboratory networks for vaccine-preventable diseases already support serologic testing for measles, rubella, yellow fever, and Japanese encephalitis and have the technical capacity to support high-throughput serologic assays. Additional financial support that builds from this existing laboratory infrastructure could reinforce investments that are already in place and extend the serologic testing platform beyond vaccine-preventable diseases. In an analogous example, the World Health Organization’s Global Rotavirus Laboratory Network tests fecal specimens for the presence of >20 enteric pathogens other than rotavirus using multiplex molecular assays . For data analysis and synthesis, the Institute for Health Metrics and Evaluation’s Local Burden of Disease Project provides an example of how coordinated financing can be used to aggregate, analyze, and disseminate information through sophisticated data pipelines to support diverse precision public health efforts .\n\n【25】### Conclusions\n\n【26】Integrated serologic surveillance is a concept with enormous potential. If deployed at scale through existing surveys and integrated into existing data pipelines, multiplex antibody testing will enable the global community to more quickly identify and respond to public health gaps, including undervaccination, emerging infectious diseases, persistent areas of high transmission, and recrudescence of NTDs or malaria in elimination settings. The key elements for building an integrated serosurveillance platform are within reach, and in our view, it is time to move beyond proof-of-concept studies toward a systematic, integrated platform.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bbcc9d27-8896-4b88-89ac-e501bef943f5", "title": "Genomic Definition of Hypervirulent and Multidrug-Resistant Klebsiella pneumoniae Clonal Groups", "text": "【0】Genomic Definition of Hypervirulent and Multidrug-Resistant Klebsiella pneumoniae Clonal Groups\n_Klebsiella pneumoniae_ is a frequent cause of nosocomial infections and has also emerged as an agent of severe community-acquired infections, including pyogenic liver abscess, pneumonia, and meningitis . The rise of antimicrobial drug resistance in _K. pneumoniae_ , a member of the ESKAPE group ( _Enterococcus faecium_ , _Staphylococcus aureus_ , _Klebsiella pneumoniae_ , _Acinetobacter baumannii_ , _Pseudomonas aeruginosa_ , and _Enterobacter_ species) of bacterial pathogens , raises serious therapeutic challenges. Most multidrug-resistant (MDR) _K. pneumoniae_ isolates, which produce extended-spectrum β-lactamases (ESBLs) and/or carbapenemases in combination with quinolone and aminoglycoside resistance, belong to particular clones . Invasive community-acquired isolates are predominantly of capsular serotypes K1 and K2 and appear to differ in clonal background from MDR isolates . Controlling the emergence of these 2 types of high-risk clones and mitigating the alarming prospect of strains that would combine high virulence with multidrug resistance requires a precise definition of clonal groups (CGs) and rapid identification of their medically relevant features. _K. pneumoniae_ clones have been recognized so far by using multilocus sequence typing (MLST) based on 7 housekeeping genes . However, MLST fails to draw clear discontinuities between CGs . Rapid, high-throughput sequencing promises to revolutionize medical microbiology and molecular epidemiology  by improving discriminatory power and providing access to the resistome and virulome of clinical isolates. However, it remains challenging to extract medically relevant information from genome sequences in a timely manner. The objectives of this work were to delineate precisely, based on genome-wide genotyping, CGs corresponding to highly virulent and MDR _K. pneumoniae_ isolates; extract the antimicrobial drug resistance and virulence-associated genomic features of those CGs by using a rapid and simple bioinformatics tool; and detect potential dual-risk isolates carrying virulence and resistance genes.\n\n【1】### Materials and Methods\n\n【2】##### Isolate Selection for Genome Sequencing\n\n【3】We sequenced 48 nonredundant _K. pneumoniae_ isolates . Forty-two of the isolates were of capsular serotype K1 or K2, the 2 serotypes predominantly associated with community-acquired invasive infections . To conduct genome sequencing, we used the HiSeq 2000 Sequencing System (Illumina, San Diego, CA, USA) with a 2 × 100 nt paired-end strategy .\n\n【4】##### Genomes from Sequence Databases\n\n【5】Genomic sequences available as of July 15, 2013, for the entries _Klebsiella_ , _K. pneumoniae_ , and _K. variicola_ , were downloaded from the NCBI ( National Center for Biotechnology Information ) genome sequence repository . The downloaded sequences comprised 7 complete genomes and 115 whole-genome shotgun sequences available as scaffolds or contigs. Of these 115 draft genomes, 2 were discarded because of the poor quality of assembly and 1 was discarded because it corresponded in fact to the more distant species _K. oxytoca_ . Thus, a total of 119 publicly available genome sequences were included .\n\n【6】##### Serotype Determination and MLST Data Generation\n\n【7】The capsular serotype was determined by PCR, using K1- and K2-specific PCR primers . The serotype of some comparative isolates was initially determined by using classical serology methods and/or by determining their C-pattern . MLST data were generated by using the international _K. pneumoniae_ MLST typing scheme .\n\n【8】##### Definition of MLST and Core Genome MLST (cgMLST) Schemes\n\n【9】A set of 634 genes, defined as the strict cgMLST set, was obtained by using stringent conservation and synteny criteria to maximize typeability and to minimize paralogous or xenologous loci . Two typing schemes, defined as sets of predefined loci, were implemented by using the BIGSdb genome database software  in a newly created database named BIGSdb-Kp . First, the MLST scheme, with its reference sequences and sequence types (STs), was imported from the international MLST database  into BIGSdb-Kp, which now acts as the reference. Second, a 694-gene cgMLST scheme was defined as the combination of the 7 MLST genes, the 53 ribosomal MLST genes , and the 634 strict cgMLST genes.\n\n【10】##### Genes Associated with Virulence and Heavy Metal Resistance or with Drug Resistance\n\n【11】Sequences of genes previously associated with virulence or heavy metal resistance were used to define virulence-associated loci, and all currently described variants of major antimicrobial drug resistance determinants (namely, β-lactamases, aminoglycoside resistance–conferring enzymes and fluoroquinolone-resistance loci) were included in the BIGSdb-Kp database. For selected strains, the antimicrobial drug resistance phenotype was analyzed by using conventional methods .\n\n【12】##### Data Analysis\n\n【13】Phylogenetic networks were constructed by using SplitsTree v4.13.1  based on comparison of allelic profiles using distance matrices corresponding to the percentage of distinct loci (excluding missing alleles), which were obtained using BioNumerics v6.6 (Applied-Maths, Sint-Martens-Latem, Belgium). Clonal complexes (CCs) were defined as groups for which MLST profiles showed only 1 allelic mismatch with at least 1 other member of the group. Phylogenetic reconstruction based on cgMLST genes was performed by using minimum evolution analysis after discarding putative homologous recombination biases . Selected genomes were annotated by using the MicroScope annotation and comparative genomics platform .\n\n【14】##### Data Availability\n\n【15】The annotated genomic sequences of strains BJ1-GA, T69, SA1, and cur15505 (SB2390) were deposited in the European Nucleotide Archive (ENA) and are available under accession nos. CBTU010000000, CBTV010000000, CBTW010000000, and CCBO010000000, respectively. The sequences of CIP 52.145 chromosome and plasmids are available under accession nos. FO834904 and FO834905 and FO834906, respectively. Sequence reads corresponding to the remaining 43 isolates have been deposited in the ENA Sequence Read Archive under study accession nos. ERS500935–ERS500961, ERS503301–ERS503315, and ERS508075. The 48 assembled genomic sequences generated in this study are accessible from the BIGSdb-Kp database through Institut Pasteur’s whole-genome MLST home page .\n\n【16】### Results\n\n【17】##### Diversity of Virulent and MDR _K. pneumoniae_ Isolates as Determined by MLST\n\n【18】The average number of contigs and the N50 (i.e. the length for which half of the bases of a draft genome are situated in contigs of that length or longer) of the 48 genomes generated in the present study were similar to those of the 112 publicly available draft genomes . These 2 characteristics quantify the level of fragmentation of the sequence assemblies. MLST allele sequences were retrieved from genomic sequences imported into the BIGSdb-Kp database. For 46 of the 48 sequenced strains, MLST alleles were also obtained by the classical Sanger sequencing method, which, in all cases, confirmed the genomic data. This result corresponds to <1 error in 138,552 nt (i.e. 0.00072%), demonstrating the high quality of consensus base calls in the assemblies.\n\n【19】Forty-three different STs were found in the entire dataset. Among the isolates we sequenced, the 20 K1 isolates represented 2 STs, among which ST23 (n = 18) predominated, whereas the 22 K2 isolates represented 10 distinct STs, among which ST380 (n = 5) and ST86 (n = 6) predominated . Isolates belonging to these 3 predominant STs, as well as ST57, ST65, and ST375, were previously reported from invasive infections . Reference strains NTUH-K2044 and CG43 were found to belong to ST23 and ST86, respectively. Publicly available genomes predominantly included (85/119, 71%) STs previously associated with multidrug resistance: ST11, ST14, ST15, ST258, and ST512 . Therefore, the dataset comprised multiple representative isolates of both types of high-risk _K. pneumoniae_ .\n\n【20】##### Definition of CGs based on cgMLST\n\n【21】Sequences corresponding to the 694 cgMLST loci were extracted from the 167 genomes by using BIGSdb, and allelic profiles were compared . Sharp discontinuities were apparent: groups of isolates were clustered in homogeneous branches that were well separated from other groups. These results demonstrate the existence of clearly delineated _K. pneumoniae_ clones. To define CGs nonarbitrarily, we analyzed the distribution of the number of allelic mismatches (loci at which sequences differ) among all pairs of genomes . The results showed a high number of genome pairs with <100 mismatches or 500–600 mismatches; almost no genome pairs had 100–300 mismatches. Therefore, we propose to define _K. pneumoniae_ cgMLST CGs as groups of cgMLST profiles having < 100 allelic mismatches (i.e. 14.4% of the 694 alleles) with at least 1 other member of the group.\n\n【22】By using this definition, we defined 14 CGs . The MDR-associated ST258 and its MLST single-locus variants belonged to a single well-demarcated group, designated CG258. On the basis of MLST results, ST258 was previously associated with many other STs in a single heterogeneous CC . We showed that most of these STs do not belong to CG258, demonstrating the power of cgMLST to discard spurious MLST associations. Within CG258, isolates of ST258 and ST512 formed a main cluster with, on average, only 2.1% allelic mismatches, whereas strains of ST11 and ST340 differed from this cluster by 11.0%, indicating genetic substructure within this CG. The other CC associated with multidrug resistance, CC14 , was split into 2 distinct CGs: CG14 and CG15 .\n\n【23】Most isolates of serotype K1, which is associated with pyogenic liver abscess, belong to ST23 and ST57 . All isolates of these 2 STs were placed together in CG23, a compact, clearly delineated CG. CG23 was highly homogeneous (3.0% distinct alleles on average), even though epidemiologically unrelated isolates from distinct continents were included . This result suggests that these strains recently emerged from a common ancestor, which is in agreement with epidemiologic evidence for the recent emergence of liver abscess caused by _K. pneumoniae_ . STs associated with serotype K2 were distributed into 3 main CGs: CG86, CG375, and CG380. In addition, K2 isolates of MLST-defined CC65 (ST25, ST65, and ST375) represented 3 distinct CGs, which differed by at least 32%. This result shows that MLST classification into CCs can conflate members of distinct CGs , and it illustrates the inability of MLST to reliably recognize _K. pneumoniae_ CGs. In our study, the use of cgMLST demonstrated that K2 isolates associated with severe community-acquired infections are genetically more diverse than K1 isolates. The relative prevalence of these serotype K2–associated CGs, and the differential clinical characteristics of infections they cause, remain to be determined by using prospective collections of isolates.\n\n【24】To determine the population structure of _K. pneumoniae_ , we performed phylogenetic analyses of cgMLST sequences . The results showed that most branches leading to CGs were long and branched deep in the tree. This pattern suggests an evolutionary radiation of most groups at approximately the same time, presumably when _K. pneumoniae_ expanded into a novel niche. Whereas CG23 and CG380 branched off early, CG375 and CG86 were placed on a single branch together with ST25 and CG65, suggesting a common ancestry for these 4 latter clones.\n\n【25】##### Reproducibility and Epidemiologic Relevance of cgMLST\n\n【26】One pair of public genomes in fact corresponded to the same strain: BAA-2146. This isolate, an MDR NDM-1–producing isolate belonging to ST11, was independently sequenced twice and was assembled by using 2 distinct methods  . The 2 genomic assemblies did not show a single allelic difference. This observation indicates that cgMLST is highly reproducible.\n\n【27】To explore the ability of cgMLST to cluster epidemiologically associated isolates and distinguish them from other genetically closely related isolates, we included in our analysis 20 published genomes of isolates from a 2011 outbreak at the National Institutes of Health Clinical Center (Bethesda, Maryland, USA) . The outbreak isolates formed a unique cluster nested within the diversity of CG258 , suggesting that cgMLST may be useful for short-term epidemiologic questions and outbreak investigation. Additional studies on well-defined sets of outbreaks will be needed to define levels of variation among epidemiologically related and unrelated isolates.\n\n【28】##### Detection of Virulence-Associated Genes in _K. pneumoniae_ Genomes\n\n【29】Genes previously associated with virulence and gene clusters coding for resistance to heavy metals, which can be encoded on the virulence plasmid , were searched by using BIGSdb . The type-3 fimbriae cluster _mrkABCDF_ was almost universally present, and plasmid-associated clusters _pcoABCDERS_ (copper resistance) and _silCERS_ (silver resistance) were widely distributed. In contrast, the other genes were mainly associated with particular CGs. The allantoinase cluster was present only in members of CG23 and ST25. Some or all isolates of the major CGs associated with community-acquired invasive infections (CG23, CG86, CG375, and CG380) harbored the 2 well-recognized virulence factors _iucABCDiutA_ (aerobactin synthesis) and _rmpA_ (the positive regulator of mucoid phenotype) in addition to the yersiniabactin siderophore cluster and (except for CG23) the 2-component system _kvgAS_ . These results reinforce the view that CG23, CG86, CG375, and CG380 represent hypervirulent CGs of _K. pneumoniae_ . Several other STs and CGs (ST25, CG65, ST66 \\[corresponding to the virulent K2 reference strain CIP 52.145\\], ST90, and ST382) harbored virulence genes, suggesting additional hypervirulent clones, which is also supported by the clinical origin of some isolates of these STs . In contrast, CG258 was almost entirely devoid of virulence genes. Microcin E492 and colibactin synthesis clusters were present in almost all isolates of CG23 and CG380. These 2 secreted molecules, which cause damage to eukaryotic cells in vitro, may contribute to the virulence of these 2 CGs.\n\n【30】##### Detection of Drug Resistance–Associated Genes in _K. pneumoniae_ Genomes\n\n【31】We investigated the presence of genes associated with resistance to β-lactams, aminoglycosides, and quinolones. Because single amino acid changes can have medically relevant phenotypic effects, we searched the known protein variants of the major β-lactamase families . At least 1 variant of the SHV enzyme was found in 160 isolates, among which 159 belonging to phylogroup KpI . The _K. variicola_ and KpII-B isolates harbored _bla_ LEN  and _bla_ OKP-B  , respectively, and 1 KpII-B isolate also contained the ESBL-encoding _bla_ SHV-18  gene. These results are consistent with previous associations of _K. pneumoniae_ phylogroups KpI, KpII, and KpIII with chromosomally encoded β-lactamase families SHV, OKP, and LEN, respectively . No gene corresponding to a chromosomal β-lactamase was detected in the genomes of CIP 52.145 (ampicillin susceptible) and 4\\_1\\_44.\n\n【32】In addition, _bla_ TEM  and _bla_ OXA  were detected in 45 and 80 genomes, respectively. In 48 of the latter, an internal stop codon was present within _bla_ OXA-9  , leading to a truncated protein. This truncation was previously described on the multiple antimicrobial drug resistance plasmid pRMH712 isolated from strain 4003 . Nine genomes contained a gene encoding the ESBL CTX-M-15. As expected, all 20 genomes from the 2011 outbreak at the National Institutes of Health Clinical Center were found to harbor a gene encoding the carbapenemase KPC-3 . A _bla_ KPC  gene was also detected in 62 other genomes. Several other β-lactamase–encoding genes, including _bla_ NDM-1  , were detected , consistent with earlier findings .\n\n【33】Regarding loci associated with resistance to quinolones, genes _qnrB_ and _qnrS_ were each present in 5 genomes, whereas a _qnrA1_ gene was found in the genome of strain 1162281, consistent with previous work  . Genes coding for the enzymatic targets of quinolones were detected in all genomes, except for gene _gyrB_ , which was absent in 2 genomes. The _gyrA_ and _parC_ genes were mutated in their quinolone resistance–determining region in all isolates of the MDR clone CG258, whereas no mutation of these genes was found among the hypervirulent clones. In addition, the _oqxAB_ locus encoding an efflux pump  was detected in 163 of the 167 genomes, demonstrating that this locus is highly conserved in _K. pneumoniae_ .\n\n【34】We identified 16 aminoglycoside resistance–associated loci in the genomes studied . The most frequently represented gene (100 genomes) was _ant(3′′)-Ia_ . The _aac(6′)-Ib-cr_ variant, coding for an aminoglycoside- and quinolone-modifying enzyme, was found in 6 genomes . Genes conferring resistance to aminoglycosides were mainly present in the MDR clones CG258 and CG14.\n\n【35】For all strains except cur15505, antimicrobial drug resistance phenotypes were highly concordant with detected genes . Strain cur15505 was resistant to ceftriaxone and ceftazidime, an antimicrobial drug resistance phenotype compatible with the production of an SHV-type ESBL. However, the _bla_ SHV  allele present in this strain’s genome could not be precisely identified by BIGSdb-Kp because it was situated at a contig extremity.\n\n【36】Although isolates of hypervirulent CGs harbored almost no resistance genes , we did identify 2 isolates of CG23 (BG130 and BG141) that displayed genomic features responsible for high-level resistance to β-lactams, aminoglycosides, and quinolones, as confirmed by analysis of the resistance phenotype . The first strain, BG130, which was isolated in Vietnam in 2008, contained an array of genes identical to a part of the Tn _6061_ transposon from _Pseudomonas aeruginosa_ and to the VEB-1-encoding region found in a _K. pneumoniae_ isolate from Greece ; these genes were _bla_ VEB-1,  _ant(2′′)-Ia_ ( _aadB_ ), _arr-2_ , _cmlA5_ , _bla_ OXA-10  , _ant(3′′)-Ia_ ( _aadA1_ ), _qacEΔ1_ , _sulIΔ_ , _cmlA9_ , _tetR_ , and _tetA_ . Moreover, downstream of this gene array, strain BG130 carried an additional region comprising gene _rmtB_ , encoding an rRNA methylase, and genes _bla_ TEM-1  and _bla_ CTX-M-15  . Strain BG130 also harbored gene _qnrS1_ ; the gene had the same surroundings as those in plasmid pHS8, which was described in a _K. pneumoniae_ isolate from China . The second strain, BG141, which was isolated in Madagascar in 2007, also carried numerous resistance genes. Those genes included _bla_ CTX-M-15  , _bla_ TEM-1  , _bla_ OXA-1  , _aac(6′)-Ib-cr_ , and _qnrB1_ , and the gene order was identical to that found in plasmids pKDO1 and pKPX-2, which were identified in MDR _K. pneumoniae_ strains from the Czech Republic and Taiwan, respectively . These results are evocative of horizontal gene transfer that may have occurred between virulent and MDR _K. pneumoniae_ strains.\n\n【37】### Discussion\n\n【38】_K. pneumoniae_ clinical isolates are evolving toward increasing levels of antimicrobial drug resistance, placing this species among the infectious bacterial pathogens that are most challenging to control . In parallel, even though most infections still occur opportunistically in debilitated patients, _K. pneumoniae_ is emerging as a cause of severe community-acquired invasive infections . Recognizing the resistant or hypervirulent CGs is a prerequisite to better understand and control their global emergence. We implemented a genome database for _K. pneumoniae_ , BIGSdb-Kp, and defined 694 genomic loci suitable for genotyping as well as loci associated with virulence and antimicrobial drug resistance. A genome-wide gene-by-gene approach based on the BIGSdb system  was previously applied to _Neisseria meningitidis_  and _Campylobacter_ species . The BIGSdb system provides a simple tool for rapidly extracting medically relevant information.\n\n【39】The detection of genes with the BIGSdb system faces 2 limitations. First, for multicopy genes, only 1 occurrence might be detected. In _K. pneumoniae_ , this problem is posed by the possible co-occurrence of chromosomal and plasmid-borne _bla_ SHV  genes. However, this issue has been recently addressed since version 1.8.0 . Second, the number of loci for which allelic number attribution fails is strongly dependent on the assembly fragmentation . Therefore, it seems advisable to exclude highly fragmented genomes from the analysis.\n\n【40】Several _K. pneumoniae_ international clones associated with multidrug resistance or hypervirulence were described by using MLST . However, the low level of nucleotide sequence divergence among _K. pneumoniae_ isolates  makes it difficult to define borders between clones . The 694-gene cgMLST genotyping scheme represents ≈100 times more sequence information than that provided by the 7-gene MLST. cgMLST demonstrated clear discontinuities in the _K. pneumoniae_ genotypic space and showed that high-risk CGs were remarkably distinct from their closest genotypic neighbors. This result is in striking contrast to the genotypic continuum obtained by using MLST. Retrospectively, the failure of MLST to disclose the sharp discontinuities among CGs can be attributed to a severe lack of resolution.\n\n【41】The discovery of recognizable _K. pneumoniae_ CGs opens the way to studying their genomic and biologic specificities. Different combinations of virulence and resistance genes were found among CGs, providing identification markers and hinting that these clones had distinct host–pathogen relationships and antimicrobial drug exposure. The genomic signatures of CGs show that MDR and hypervirulent populations of _K. pneumoniae_ are, so far, mostly nonoverlapping. However, genes encoding resistance to β-lactams, quinolones, and aminoglycosides were detected in 2 isolates of hypervirulent clone CG23: BG130 (Vietnam, 2008) and BG141 (Madagascar, 2007). These 2 isolates are among the first documented members of CG23 shown to harbor _bla_ CTX-M15  and genes conferring high-level resistance to aminoglycosides and fluoroquinolones. A CTX-M-15–producing ST23 strain was also reported from South Korea . These results show that the gloomy prospect of dual-risk _K. pneumoniae_ strains, combining virulence and multidrug resistance features, is becoming a reality.\n\n【42】Genome-wide genotyping is a powerful approach to address fine-scale epidemiologic questions . Our findings show that the discriminatory power of cgMLST, based on 694 genes, makes it possible to distinguish a subgroup of isolates involved in the 2011 _K. pneumoniae_ outbreak in Bethesda  from epidemiologically unrelated CG258 isolates. However, local epidemiologic investigations will require more resolution to decipher recent transmission events. Because the typical _K. pneumoniae_ genome harbors ≈5,300 genes, there is ample room for defining additional loci to improve discriminatory power. The BIGSdb-Kp database can host additional loci and schemes curated at a distance by distinct laboratories .\n\n【43】Our findings demonstrate the existence of clearly distinguishable _K. pneumoniae_ CGs and show that MDR and hypervirulent populations of this species are largely nonoverlapping. However, hypervirulent _K. pneumoniae_ are beginning to evolve toward increasing levels of antimicrobial drug resistance and may represent a new and serious threat to public health . The freely accessible BIGSdb-Kp database represents a novel tool for monitoring the emergence of high-risk clones and for global collaboration on the population biology, epidemiology, and pathogenesis of _K. pneumoniae_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "551e839e-8ceb-487a-a896-c876268ebbb6", "title": "Influenza Pandemic Periodicity, Virus Recycling, and the Art of Risk Assessment", "text": "【0】Influenza Pandemic Periodicity, Virus Recycling, and the Art of Risk Assessment\n\"I am sure that what any of us do, we will be criticized either for doing too much or for doing too little…. If an epidemic does not occur, we will be glad. If it does, then I hope we can say… that we have done everything and made every preparation possible to do the best job within the limits of available scientific knowledge and administrative procedure.\"\n\n【1】**—US Surgeon General Leroy Burney, Meeting of the Association of State and Territorial Health Officers, August 28, 1957** \n\n【2】In 1941, on the eve of US entry into World War II, concern about a repeat of the 1918 influenza pandemic and its effect on armed forces led the US military to establish the Commission on Influenza (later combined with other commissions to become the present Armed Forces Epidemiological Board) and place high priority on developing a vaccine . Pandemic influenza did not materialize, but the vaccine did. The first successful large-scale influenza vaccine field trials were completed in 1943 . In 1947, failure of the vaccine to provide protection against the epidemic influenza type A antigenic variant confirmed concerns of vaccine obsolescence and led to the term \"antigenic shift\"  and designation of the 1947 FM1 strain by the Commission on Influenza as subgroup A´ on the basis of the hemagglutination inhibition (HI) test.\n\n【3】In May 1957, with reports of a potential influenza pandemic in the Far East, risk assessment responsibilities of the Commission on Influenza were clear. The Department of Defense influenza immunization policy of 1954 mandated quick formulation and provision of a new vaccine. The Public Health Service had no such official policy and found risk assessment to be a challenging process that relied heavily on international sources for surveillance and the Influenza Commission for advice. \"There was no indication it would become a killer of the 1918 variety, but neither was there positive assurance it would not\" . Risk management was contingent on evidence of \"continued low mortality\" or \"increased virulence\" . The consensus by late June was probable sporadic local occurrences during the summer with an epidemic during fall or winter that would bring only a relatively small increase in deaths. On August 28, the Surgeon General recommended immunization through established physician-patient channels. The watchword was to \"alert but not alarm\" the public and to generate interest in receiving the vaccine .\n\n【4】The 1957 Asian virus pandemic simultaneously increased knowledge of influenza pandemics and the complexity of future pandemic risk assessments. The pandemic had appeared exactly 10 years after appearance of the A´ virus, which suggested pandemic periodicity . Preexisting HI antibodies to the 1957 A2/Asian virus in sera collected before the pandemic were reported for some persons >75 years of age, which suggested that human influenza viruses were recycling .\n\n【5】In July 1968, with reports of influenza epidemics again appearing in the Far East, the US Military Commission on Influenza quickly obtained strains and recommended a new vaccine . Risk assessment by the Public Health Service this time around was a much simpler process. Annual vaccine recommendations to physicians for persons at high risk for death or severe complications were by now a matter of course. The need for a new vaccine was apparent , but early reports consistently described the disease as mild , and the US epidemic was over before the A2/Hong Kong virus was recognized as an antigenic shift .\n\n【6】The 1968 pandemic added to the complexities of risk assessment. The new subtype had appeared, right on time, 11 years after the 1957 Asian pandemic and replaced the dominant influenza A2/Asian virus subtype, as had the viruses of 1947 and 1957. Further, most persons >85 years of age had preexisting antibodies to the 1968 virus, which suggested that the hemagglutinin of this virus, as well as that of the 1957 virus, had appeared previously in the human population .\n\n【7】In 1976, speculation was rife that a new pandemic strain was due in a few years. The concept of 10- to 11-year influenza A virus pandemic patterns, with disappearance of the predecessor virus, seemed entrenched in the influenza literature. Previous influenza pandemics had occurred in 1968, before that in 1957, and before that in 1947; carrying the logic further, pandemics also occurred in 1929, 1918, 1900, and 1890 . In addition, seroarcheologic findings had been interpreted as evidence that the swine virus had last appeared in 1918, the Hong Kong virus (now designated H3) in 1900, and the Asian (H2) virus in 1890, not exactly 10–11 years apart, but in the same order . To some, the next pandemic virus in the sequence was the swine virus of 1918 .\n\n【8】On February 13, 1976, the New York Times published a guest editorial to remind the public and policy makers that influenza pandemics had marked the end of every decade—every 11 years—since the 1940s. The editorial urged accelerated pandemic planning and coordinated vaccine research .\n\n【9】### Risk Assessment in\n\n【10】Coincidentally, on February 14, 1976, the day after the Times editorial was published, the Center for Disease Control (CDC) hosted an emergency meeting with the US Army, Food and Drug Administration, National Institutes of Health, and New Jersey State Health Department to assess the isolation of swine influenza virus from the late January outbreak at Fort Dix, New Jersey . Information was insufficient at the time to assess whether the swine influenza virus outbreak was a unique event in susceptible young recruits or the beginning of a pandemic, but the isolation of a predicted potential pandemic strain almost on schedule did not go unnoticed.\n\n【11】On March 10, the Army provided data to the US Advisory Committee on Immunization Practices that confirmed person-to-person transmission of swine influenza virus . The single swine influenza death loomed large, although most cases were mild. No one at the advisory committee meeting equated the disease potential of this virus with 1918, but the association of swine influenza virus with the most devastating pandemic in memory was widely speculated in the news media. Slightly more than a month after the outbreak, no evidence suggested that a pandemic would or would not occur; a situation such as the Fort Dix outbreak had never been encountered. On March 18, the action memo from the Assistant Secretary of Health to the Secretary, Department of Health, Education, and Welfare stated that \"severe epidemics, or pandemics, of influenza occur at approximately 10-year intervals\" and publicly linked swine flu with the pandemic of 1918 .\n\n【12】When WHO convened a meeting of consultants in Geneva on April 7 , 3 months had passed without evidence of further swine virus transmission anywhere in the world. The swine A/New Jersey strain had not replaced the current A/Victoria strain, which continued to circulate at Fort Dix well into February, and no evidence of swine/Victoria virus reassortants had been seen. Theories of what might happen were being overtaken by the realities of what was happening. The Fort Dix outbreak was beginning to look like an isolated event.\n\n【13】A report from the United Kingdom on the behavior of swine influenza virus in infected human volunteers would not appear in Nature for some weeks , but in April early rumors circulated that swine A/New Jersey virus was more infectious than classic swine virus but that the symptoms were mild to moderate. The report added little to risk assessment; the findings were consistent with events seen in the outbreak. But an accompanying editorial in Nature summarized the UK and likely European view, which urged caution in vaccine stockpiling and immunization programs and continuing assessment, \"until the shape of things to come can be seen more clearly\" .\n\n【14】Beginning in April and continuing into May, a group of US investigators used the Delphi technique to obtain an expert risk assessment with minimal bias . The 15 participating scientists and epidemiologists concluded that if swine influenza virus were to circulate in the United States, the epidemic would more likely resemble those of 1957 and 1968 than that of 1918. The probability of further swine influenza virus outbreaks was estimated at 0.10.\n\n【15】On August 1, a series of news reports began to appear on a fatal respiratory illness among American Legionnaires attending a convention in Philadelphia . Wide but inappropriate speculation that the cause of these unprecedented deaths might be swine influenza, accompanied by equally unprecedented national publicity, precluded further opportunity for rational risk assessment.\n\n【16】### Theory of Predictable Pandemics\n\n【17】Unknowingly, at the same time as the Fort Dix outbreak, the Working Group on Pandemic Influenza met in Rougemont, Switzerland, on January 26–28. Issues addressed included the growing body of evidence linking the origin of antigenic shift to animal reservoirs of influenza viruses , the questionable validity of predictable patterns of pandemic periodicity, and the appropriate classification of the 1947 strain .\n\n【18】When the 1947 epidemic occurred, only 13 years had passed since the first influenza virus was isolated. Available scientific knowledge was limited. No precedent existed for defining a pandemic strain or distinguishing antigenic shift (a complete change) from antigenic drift (point mutations resulting in accumulated amino acid changes). The 1957 Asian pandemic virus provided the first evidence of a true antigenic shift. The hemagglutinin and neuraminidase surface antigens were totally different from those of their 1956 predecessors. The 1968 Hong Kong pandemic virus provided evidence that antigenic shift can occur in the hemagglutinin independent of the neuraminidase, which was largely unchanged. The 1947 strain failed to meet the definition of an antigenic shift.\n\n【19】The 1971 revision of the system of nomenclature  recognized the independence of the 2 surface antigens and linked antigenic shifts with influenza A virus subtypes but further confounded the issue by designating the 1947 strain as a subtype for historical reasons. In the 1980 revision, which combined antigenically closely related subtypes regardless of source of isolation , the previous hemagglutinin designations of swine (Hsw1) and human H0 and H1 subtypes became H1N1, ending a misclassification of the 1947 strain that had endured for >30 years.\n\n【20】Thus, counting 1890, a total of 4 recognized pandemics have been separated by 28 years , 39 years , and 11 years . Excluding the emergence of the H1N1 virus in 1977, an additional 38 years have elapsed since the last pandemic. No predictable pattern of pandemic periodicity exists.\n\n【21】### Pandemic Virus Recycling\n\n【22】In 1935, high levels of antibodies to the newly isolated influenza viruses from humans  and swine  were commonly seen among persons >10 years of age, which suggested that the 1918–1920 pandemic had been caused by the same or a closely related virus. The birth dates associated with the peak prevalence of swine virus (H1) antibodies did not change in sera collected 12, 17, or 20 years later . The seroarcheologic findings were validated in 1999 by sequencing the HA gene recovered from persons who died of influenza during the pandemic . Thus, swine (H1) virus was present from 1918 to 1920 and left a lifelong immunologic imprint on most persons who were <25 years of age at the time. Validation of the H1 seroarcheologic model allowed reexamination of earlier reports of preexisting H2  and H3  antibodies in sera collected from elderly persons before the respective pandemics .\n\n【23】##### Serologic Findings Linking 1890 with H3\n\n【24】After 1957, preexisting H2 antibodies were not commonly observed. Three laboratories reported preexisting H2 antibodies among the elderly, while 3 other laboratories found no orientation of H2 antibody toward any particular age group. Further, peak antibody prevalence from the 2 primary laboratories  differed by nearly 8 years. The lack of agreement among investigators and the low levels and low titers of H2 antibodies suggest either differences in test specificity, sensitivity, or both. More recent application of the seroarcheologic model failed to confirm the proposed link of preexisting H2 antibodies with the 1890 pandemic .\n\n【25】In contrast, preexisting high levels of H3 antibodies among persons >85 years of age in 1968 were common findings in all serologic tests. Some investigators linked the origin of preexisting H3 antibodies to the minor 1900 pandemic , whereas others favored the 1890 pandemic . Observations from recent application of the validated H1 seroarcheologic model to published data linked preexisting H3 antibodies to the pandemic of 1890 .\n\n【26】We can reasonably conclude that the virus (H3) with the highest HI antibody titers and highest peak antibody prevalence (>90%) in the elderly resulted not from an epidemic  but a pandemic . The virus (H2) with the lowest HI antibody titers and seroprevalence (15%–29%) in the elderly is an unlikely candidate for the most severe influenza event of the late 19th century.\n\n【27】##### Epidemiologic Findings Linking 1890 with H3\n\n【28】Population immunity against the shared neuraminidase (N2) antigens between the 1968 H3N2 pandemic strain and its H2N2 predecessor is believed to have contributed to the low number of deaths observed in 1968 and 1969 . However, more dramatic was the selective sharp decrease in expected excess deaths among persons born before 1893. In the 1970 wave that followed, no excess deaths occurred in persons born before 1885 . Influenza infection rates in 1968 and 1969 among persons born before 1890 were two thirds lower than among persons born after 1899 , further linking H3 with 1890.\n\n【29】##### Unclear Evidence for H2 Recycling\n\n【30】No single, simple explanation has been proposed for the reported low levels of preexisting H2 antibodies before 1957. Whether these antibodies, if specific, represented cross-reactions stimulated by a related virus or by the H3 virus itself is uncertain. Evidence against specificity (or at least prevalence) of H2 antibodies is the absence of any obvious protective effect among persons >75 years of age during the 1957–1958 pandemic, which is in stark contrast to the strong correlation of prepandemic antibodies with protection in 1968 and 1969 (H3) and 1977 (H1) .\n\n【31】Linking H2 to 1890 and H3 to 1900 may have been a historical accident. The reports of preexisting low levels of H2 antibody in persons >75 years of age predated the H3 findings by 10 years. Thus, H2 antibodies were attributed to the 1890 pandemic, the only accepted pandemic around that period. When preexisting high levels of H3 antibody were recognized in essentially the same age cohort in 1968, the 1890 pandemic slot had already been taken.\n\n【32】##### Lack of Evidence for H1 Recycling\n\n【33】Researchers have long speculated  that preexisting H1 antibody among the elderly in 1918 accounted for the well-known \"W\" excess death curve . Theories of special protection of the population >40 years of age compete with theories of extraordinary vulnerability of young adults. But given the continued increase in the death rate curve (albeit dampened) among those >65 years of age in 1918  and the remarkably low death rate among those with preexisting antibodies in 1968 (H3) and 1977 (H1), evidence of H1 recycling in 1918 is not compelling. With the passage of time and the absence of sera collected from persons >40 years of age before the 1918 pandemic, the issue of H1 recycling is difficult to resolve.\n\n【34】H1 reappeared, of course, in 1977, but evidence suggests that the 1977 H1N1 virus reemergence was not a natural event . Transmission of the mild H1N1 for >25 years, primarily among those born after 1957, coupled with the previous natural transmission among persons born before 1956, completes the H1N1 immunologic experience of all age groups. If a natural recycling sequence ever existed, present population immunity precludes H1 as a pandemic candidate for years to come.\n\n【35】Solid evidence of recycling exists for a single subtype, H3, which (likely with an equine N8 neuraminidase ) caused the pandemic of 1890 and reemerged with the N2 neuraminidase in 1968. Thus, in the last 115 years, the influenza A virus hemagglutinin had recycled in humans at least once, after 79 years. Neuraminidase subtypes during this same period of time were N8 , N1 , and N2 (1957 and 1968) . No evidence of neuraminidase recycling has been seen.\n\n【36】### Lessons from\n\n【37】Swine influenza virus was isolated in the United States from humans for the first time in 1974, just 2 years before the Fort Dix outbreak . Additional swine virus infections of humans were confirmed by serologic evidence and virus isolation in 1975 and 1976, with a least 1 suggested incident of person-to-person spread other than the Fort Dix outbreak. Increased recognition of swine influenza infections may have been a matter of increased surveillance, number of susceptible humans, or swine virus transmissibility. Human experimental studies  and virologic findings  suggest the latter.\n\n【38】Influenza virus eradication in swine was recommended by WHO in 1976 , but such action was not taken because of major biologic challenges and absence of resources. Today, even if pandemic risk were absent, the economic loss from infected poultry and mounting human illness and death are compelling reasons in themselves to place highest priority on avian influenza virus control.\n\n【39】##### Risk Assessment Limited by Available Knowledge\n\n【40】The major lesson from 1976 was that increased animal-to-human transmission and major outbreaks of a novel influenza virus do not necessarily lead to pandemics, at least in the short term. However, knowledge of the Fort Dix outbreak and evidence that swine influenza virus/H3N2 reassortants could occur in pigs under conditions of natural transmission  likely would have generated concerns for years about swine influenza transmission to humans had not H1N1 virus reappeared in the human population in 1977.\n\n【41】Since 1976, available knowledge of the influenza A virus and its natural history has expanded greatly. Multiple experimental studies have better defined conditions for virus mutation and the creation of reassortants. Opportunities for human exposure and the current number of incidences of avian virus transmission to human are unprecedented in modern times, but in 2005, as in 1976, the precise conditions that lead to the emergence of a pandemic strain are unknown.\n\n【42】##### Concern of Virus Recycling\n\n【43】In recent history, influenza virus recycling has occurred twice, once through the natural process (H3 in1968) and once likely through human negligence (H1 in 1977) . If human influenza A epidemics are restricted to 3 subtypes, as some have speculated, and if H1 and H3 are presently in circulation, then only H2 remains. The risk of H2 reemerging in humans through an act of nature is theoretical. The risk of H2 reemerging through an act of human negligence is all too real.\n\n【44】In the published report of the April 7, 1976, WHO meeting of international experts, the final paragraph urged extreme caution in developing live vaccines from A/New Jersey strains (H1N1) because of the possible danger of spread to susceptible human or animal hosts . That paragraph was written specifically to respond to reports that several investigators outside Western Europe had plans to develop and test such vaccines. One year later, an H1N1 virus, identical to the laboratory strain from1950–1951, swept the world.\n\n【45】In an incident earlier this year, H2N2 virus was accidentally distributed in proficiency testing panels to laboratories in 18 countries. Recognizing the potential danger, CDC and WHO issued a health advisory on April 13, 2005, to destroy all such samples and followed on May 3 with recommendations to increase biosafety levels for H2N2. Laboratory containment of H2N2 strains is crucial. No one born since 1968, including many laboratory staff, is immune. The level of compliance with these biosafety recommendations in all areas of the world is unknown. Focusing on the theoretical risk for natural H2 emergence and ignoring the real risk in our own laboratories would be tragic.\n\n【46】##### Risk Assessment Separate from Risk Management\n\n【47】Internationally, influenza risk assessment and risk management are separate functions. WHO makes risk assessments in the form of annual recommendations on influenza vaccine composition. Nations may elect to accept WHO findings and recommendations or to have their own risk assessment bodies that incorporate WHO findings. Risk management, on the other hand, is the exclusive responsibility of national governments. Independent expert bodies may make recommendations, but risk management ultimately is a political process, performed and funded by federal and state governments.\n\n【48】Nationally, risk assessment should also be a separate scientific function, free from influence by perceived risk-management resource constraints, organizational capacities, or political aspirations. Pandemic risk management, itself an uncertain art, must independently weigh ongoing risk-assessment findings in the context of actions that best serve national and international interests.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "09013d50-8273-45d7-b203-b59d837722f4", "title": "Contribution of Sex-linked Biology and Gender Roles to Disparities with Trachoma", "text": "【0】Contribution of Sex-linked Biology and Gender Roles to Disparities with Trachoma\nBlindness is a major public health concern globally: approximately 50 million people are blind, and three times that number are visually impaired. Unless marked improvements are made in numbers of healthcare personnel, infrastructure, and use of services, 75 million cases of blindness will likely occur by 2020.\n\n【1】Recent evidence has shown that women account for approximately 64% of global blindness; an age-adjusted prevalence 39% higher than in men . Cataract, the leading cause of blindness in the world, occurs more commonly in women than men . Paradoxically, in developing countries the use of cataract surgical services among women is considerably lower than among men . Other leading causes of blindness include glaucoma, diabetic retinopathy, and trachoma.\n\n【2】Trachoma is often referred to as the leading cause of preventable blindness in the world. It is the leading infectious cause of blindness, followed by diseases such as onchocerciasis and measles. Systemic diseases such as leprosy and HIV/AIDS also lead to blindness, although much less frequently.\n\n【3】Trachoma is a chronic infectious eye disease affecting marginalized population groups throughout many countries of Africa, the Middle East, Asia, and a few settings in Latin America. _Chlamydia trachomatis_ , the infective agent, has no known animal reservoir. The manifestations of trachoma vary depending upon the number of episodes of infection, severity, and the persistence of infection. Trachoma generally occurs early in life through physical transmission of _C. trachomatis_ to the eye by hands, flies, or cloth. The pool of chlamydiae in the community generally resides in preschool-age children , and transmission is easily facilitated by poor hygiene, scarcity of water, and crowded living conditions. The highest prevalence of active trachoma in hyperendemic areas is found among children 1–3 years of age. Adult women are also more likely to have evidence of active disease and infection.\n\n【4】A single episode of infection will not lead to sequelae; however, repeated bouts of infection and active disease as well as persistent infection will lead to scarring of the upper tarsal conjunctiva. Approximately 10% of children have persistent infection and many are actively and consistently shedding infectious agent . The severity of active disease will generally dictate the severity of scarring. Among those with tarsal scarring, a small proportion will have thickening of the tarsus and deformation of the lid, whereby the malposition of eyelashes leads to abrasion of the cornea and, in some cases, corneal ulceration and scarring. Consequently, trachoma occurs throughout patients’ lives, exhibiting different signs and symptoms at different stages; Figure 1 gives an example of the changes in disease and its sequelae in trachoma-endemic countries.\n\n【5】Estimates of the number of people affected and blinded by trachoma have been recently revised; approximately 80–85 million people now have active trachoma, approximately 8 million have trichiasis, and 3 million are blind. Most of those affected are found in sub-Saharan Africa, the Middle East crescent, and parts of Asia. The distribution of trachoma corresponds with that of poverty in much of Africa and Asia. Conditions that facilitate the transmission of _C. trachomatis_ , such as household crowding and poor access to and use of water, define risk . Because of changes in disease endemicity, active disease and trachomatous trichiasis are less correlated than they were 50 years ago; for example, active trachoma has disappeared in most areas of China, where it was once endemic. There are, however, approximately 2.5 million people in China blind because of trachoma . Trichiasis also remains common in countries such as Vietnam, where active disease rates are relatively low. Trachoma has been eliminated as a public health problem in these settings and others (such as Tunisia, Saudi Arabia, and Oman) through improved socioeconomic status, improved environmental and public health infrastructures (water supply, latrines), and changed behavior. Nevertheless, because infections accumulated in the past, trichiasis is likely to develop in many persons in these countries.\n\n【6】Productivity losses attributable to trachoma are conservatively estimated at U.S.$2.9 billion annually. This figure is likely an underestimate of the social cost of the disease .\n\n【7】### Trachoma as a Gender-associated Health Issue\n\n【8】Survey data from virtually every trachoma-endemic setting have consistently shown that trachoma-related blindness is two to four times higher in women compared to men . Survey data also show that women have an excess risk for corneal opacity, trichiasis, and scarring; women account for 60% to 85% of all cases of trichiasis in the community . Tracing the excess risk for and complications of trachoma borne by women through the course of disease suggests that factors other than biology are involved. While evidence shows that, by the age of 40, women are more likely to have severe conjunctival scarring compared to men, the relative effect is not as high as expected, if a linear association is assumed. The further back along the course of the disease in a patient, evidence becomes less and less clear. In young children, the focus of most active trachoma in a community, the prevalence of active disease seems to vary, with just a slight excess risk for girls. In some surveys, such as recently in Dalocha District in Ethiopia, while 11 of 18 trichiasis cases were in women, in children <10 years of age, 21.1% of boys and 13.5% of girls had active trachoma . However, in another district in Ethiopia, girls and women were 1.63 times more likely to have active trachoma compared to boys and men . Preschool girls in Tanzania had slightly higher rates of active trachoma than boys; including adults in the findings showed that female patients had a twofold excess risk for active trachoma compared to male patients . In most settings, the differences in prevalence of active trachoma between girls and boys are not as striking as they are in women and men .\n\n【9】### Excessive Effect of Trachoma on Women\n\n【10】Understanding the reasons for the excess risk for and complications of trachoma in girls and women requires examining a number of issues. Are females more biologically susceptible to the consequences of infection with _C. trachomatis_ ? Could other factors (e.g. number of episodes, persistence of disease, higher bacterial loads) help explain the excess conjunctival scarring and trichiasis in women? Do gender roles affect the risk for trachoma and its consequences? Are women more likely to have recurrence after trichiasis surgery compared to men?\n\n【11】### Biologic Susceptibility to Consequences of Infection with _C. trachomatis_\n\n【12】Research in developing countries has not shown that women or girls are more biologically susceptible to the consequences of infection with _C. trachomatis_ . However, research in industrialized countries has demonstrated a higher prevalence of dry eye syndrome in women, likely due to hormonal factors. If these findings are applied to trachoma-endemic settings, they would suggest that corneal damage is more likely to develop in women with trichiasis than men. The paucity of longitudinal data analyzed by sex limits our ability to confirm this hypothesis.\n\n【13】### Other Factors Explaining Excess Conjunctival Scarring and Trichiasis in Women\n\n【14】Recent research in Tanzania suggests that, while girls have only slightly higher prevalence of active disease than boys, they account for most of the community load of the community load of _C. trachomatis_ infection . In a different population in Tanzania, when the group of children with a heavy bacterial load were compared to noninfected persons, those with a heavy bacterial load were more likely to be younger and to be female (odds ratio 1.64, 95% confidence intervals 1.1–2.5) and to live in a house with at least one other person with heavy infection . In a separate study in the same district, antimicrobial treatment was provided to all residents. At the 2-month follow-up, the predictors of infection were being younger and female and having trachomatous inflammation at baseline assessment before treatment. At 6 months, the strongest predictor of infection was infection at 2 months; 95% of those with a severe infection at 2 months were infected at 6 months . Despite high rates of antimicrobial coverage in this community, _C. trachomatis_ was reemerging, even among those treated.\n\n【15】These studies suggest that infection loads are higher in girls and women and that persistent infection is more common in girls. Longitudinally, this hypothesis is supported by the 7-year follow-up results on conjunctival scarring. Of girls with severe active trachoma at the beginning, 33% eventually had scarring compared to 22% of boys with similarly severe active trachoma .\n\n【16】Few studies have measured infection with _C. trachomatis_ in adults. In a study of women, infection in adulthood, although not common, increased the risk for trichiasis 2.5-fold . The few studies with information on infection show that adult women are more likely than adult men to be infected . These data justify treating adults, particularly women.\n\n【17】In a follow-up study of women in Tanzania, evidence for persistent infection existed, as several women were infected at follow-up with the same genovar . Whether men have more or less susceptibility to persistent infection is not known.\n\n【18】### Effect of Gender Role on Risk Factors for Infection\n\n【19】Women and girls are the primary caregivers in most societies in developing countries. Proximity to children exposes women to repeated infection more than men and is likely a primary reason for the greater effect of active disease . The close association of mothers with children makes them more likely to continue to acquire infection and exposes them to the risk for persistent infection. Ongoing infection and scarring may explain the almost four times greater risk compared to men of developing trichiasis. In central Tanzania, women with preschool-age children appeared more likely to have active trachoma than similarly aged women without preschool-age children. Young girls have the role of childcare in this setting, and active disease is associated with the status of mother or caretaker . However, a case-control study of trichiasis in women did not find a significant difference between childcare activities in those with trichiasis compared to those without trichiasis; the study design did not permit an evaluation of infection in the children themselves, which is the key factor . That study did find an increased risk in women whose mothers also reported trichiasis, which suggests a possible genetic or shared environmental component to trichiasis. The absence of rigorous behavioral research has limited our ability to understand how gender roles contribute to the progression from infection to clinical disease and its sequelae.\n\n【20】In many Islamic countries the use of kohl and other traditional products for preventing eye diseases, treating eye diseases, and “brightening” the eyes is common and can be implicated in disease transmission within the household and community . In addition, some of these traditional products are applied by everting the lid, moistening the product with saliva, and rubbing the compound directly onto the tarsus. This procedure may further scar the tarsus and spread bacteria from the nasopharynx. These products are used among both young boys and girls, but continued use in adulthood is limited primarily to women. This practice may be another way in which gender roles contribute to the excessive effects of trachoma in women.\n\n【21】### Recurrence after Trichiasis Surgery\n\n【22】Recurrence after surgery to correct trichiasis is not uncommon. Recurrence rates reflect both poor quality of surgery (recurrence usually occurring within 6 months of surgery) or other pathologic processes (e.g. continued contracture of the tarsal conjunctiva, reinfection). Recurrence, measured cross-sectionally, may reflect both of these conditions; measured prospectively, determining the apparent contribution of each factor is easier. Most studies of the outcome of trichiasis surgery had few patients enrolled and lacked adequate power to compare differences between men and women . Omani women and men, with 3- to 5-year follow-up data, had unequal rates of recurrence. Overall, 61.3% of surgery in women were associated with recurrences compared to 48.9% among men . West et al. in their large series of cases in Tanzania, found no difference by sex in recurrence rates up to 8 years . Cross-sectional studies indicate that women are more likely to have recurrence compared to men. In the recent Egyptian study, trichiasis recurred after surgery in 44.4% of women compared to 37.7% of men .\n\n【23】Possible reasons for the differences are multiple and may overlap: Women may have had more severe trichiasis before surgery, which is a known risk factor for recurrence . In some settings, women use surgical services less or may wait until trichiasis is more advanced before surgery. Finally, reinfection may play a role in recurrence. As noted, infection in adulthood increases the risk for trichiasis 2.5-fold . In the Omani study , recurrence was approximately three times higher in persons with infective conjunctivitis compared to those without conjunctivitis. All these potential explanations point to gender role–mediated factors more than sex-linked factors. Gender roles in most societies in trachoma-endemic countries account for the variation in prevalence of infection and sequelae of infection seen in men and women .\n\n【24】### Public Health Approaches to Trachoma Control and Gender Equity\n\n【25】Eliminating trachoma as a blinding disease is the goal of the Alliance for the Global Elimination of Trachoma by 2020. The Alliance has adopted a four-part strategy, referred to as the SAFE strategy, which includes surgery to correct trichiasis, antimicrobial agents to treat active trachoma, and face washing and environmental changes to prevent transmission. Adopting the SAFE strategy, while not explicitly gender-sensitive, would necessitate considering gender in implementation.\n\n【26】Few national trachoma control programs monitor use of trichiasis surgery separately for men and women. Fewer still have adequate information on the proportion of trichiasis patients who have had surgery). This lack of this information limits evaluation of program progress, gender based or not. Data from the Tanzania and Vietnam programs suggest that these countries have gender equity in trichiasis surgery . However, programs directed at increasing the number of persons receiving surgery to correct trichiasis have generally not been explicitly gender-sensitive, and publications on uptake of services have rarely compared men and women. Gender equity in use thus remains unknown in most settings.\n\n【27】Similarly, trachoma control programs have tended to measure antimicrobial coverage as a single, communitywide or districtwide measure, without assessing rates of compliance by age and sex. Research on compliance from Rombo district in Tanzania showed that compliance with azithromycin treatment was higher among women (80.2%) than men (73.3%). However, as pregnant women are not eligible to receive azithromycin and must use topical tetracycline, interpreting coverage findings is not straightforward . The authors suggested that, since childcare is primarily the concern of women, they were more likely to attend the distribution of azithromycin by bringing their children and thus are more likely than men to receive drugs for themselves. Furthermore, the authors suggested that patient or recipient expectations contributed to compliance and that the expectations of women were influenced to a greater extent than those of men by their children’s illness . Current recommendations promote a minimum antimicrobial coverage (whole population) of 80%; evaluation is needed to assess if women are more or less likely to get an antimicrobial agent.\n\n【28】Although the antimicrobial agent is currently donated free-of-charge, the cost of distribution remains high (approximately U.S$0.50 per dose) . This cost is borne by various governmental and nongovernmental agencies; however, how long this can continue is not clear. Willingness to pay for research in Tanzania has demonstrated that those at higher risk for trachoma were less willing to pay for future treatment. This group included female-headed households . Only higher maternal education was associated with willingness to pay.\n\n【29】Approaches to preventing trachoma in the SAFE strategy, namely, face washing and environmental changes, have not been researched to the same extent that surgery and antimicrobial use have. Research in the early 1990s demonstrated the effectiveness of face washing in trachoma reduction. Cross-sectional studies of risk factors have consistently showed a strong association between the presence of active trachoma and the absence of good sanitary conditions (primarily the absence of latrines and the high concentration of flies). Assessment of environmental conditions (including water supply) is household- or community-based, rather than individually based, thus limiting the ability to evaluate gender-specific characteristics. Nevertheless, the fact that women and girls are primarily responsible for water collection, face washing, and cleaning (if done) of latrines suggests that introducing improved infrastructures will have the greatest effect on women, both in terms of eliminating trachoma and improving quality of life. Women’s and girls’ roles also suggest that most hygiene and environmental components of trachoma control will fall to women as well . Thus, for example, in the absence of water in the villages, convincing women to use scarce water that they must collect for washing purposes is difficult.\n\n【30】Trachoma remains a major problem, particularly among girls and women, in much of sub-Saharan Africa, areas of the Middle East crescent, and pockets of Asia and South America. Eliminating trachoma, while possible, will require a rededication to prevention strategies and a focus on disease control as a gender-sensitive intervention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0cc984ee-7e86-4f4d-b70a-26c8ab374c13", "title": "Tuberculosis-Related Deaths within a Well-Functioning DOTS Control Program", "text": "【0】Tuberculosis-Related Deaths within a Well-Functioning DOTS Control Program\nIn both humanistic and economic terms, the cost of deaths due to tuberculosis (TB) is staggering. In 1990 alone, approximately 2.5 million people died of TB, accounting for >25% of avoidable adult deaths in the developing world . Directly observed treatment strategy (DOTS), a comprehensive approach to TB control, is one of the most cost-effective health interventions available . In the context of a well functioning DOTS program, cure rates in excess of 80% can be expected. While these outcomes are assumed to decrease mortality rates, the detailed epidemiology of deaths in a well-functioning DOTS program by using modern molecular techniques has not been described.\n\n【1】Since 1995, we have conducted a population-based molecular epidemiologic study of TB in a health district in southern Mexico. Previous reports have documented the TB control program approaches World Health Organization benchmarks  and drug resistance is considerable and has an important negative impact on treatment outcomes . We now report the short- and long- term mortality rates due to TB in this cohort of TB patients. These data suggest that, as has been described with other diseases, excess mortality may persist for months or years after treatment completion, default, or failure .\n\n【2】### Methods\n\n【3】The study site, described previously , is located in a predominantly urban region in the Orizaba Health Jurisdiction in the state of Veracruz, which encompasses 134 square km and has a population of 284,728 . The incidence rate of TB during the year 2000 for the state was higher than that for the nation (28.0 vs. 15.9 per 100,000 inhabitants) .\n\n【4】Community-based screening of chronic coughers (>2 weeks) was performed from March 1995 to October 2000. Patients with positive AFB sputum smears underwent epidemiologic, clinical (standardized questionnaire, physical exam, chest x-ray, and HIV test), and mycobacteriologic evaluation. Treatment was provided in accordance with official norms . Treatment outcomes were classified as previously described . Annual follow-up was performed for treatment outcome and vital status. Deaths were confirmed with death certificates. A close caregiver was interviewed to elicit signs and symptoms of the terminal illness and “probable cause of death” was assigned by two of the authors (JF, LF). Informed consent was obtained from participants. The study was approved by the appropriate institutional review boards.\n\n【5】##### Microbiologic Evaluation\n\n【6】Mycobacterial culture, identification, and susceptibility testing were performed on sputa from each enrolled patient. In brief, unconcentrated sputum was spread onto Lowenstein-Jensen media (DIFCO, Detroit, MI) at the local laboratory, and the remaining sputum was frozen at –70°C. The tubes were examined on a weekly basis until growth was detected. Cultures were reported as negative if no growth occurred after 8 weeks. Cultures with visible growth were forwarded to the department of Mycobacteriology at the Instituto de Diagnostico y Referencia Epidemiológicos (March 1995 to December 1997) or to the Clinical Microbiology Laboratory of the Instituto Nacional de Ciencias Médicas y Nutrición (INCMNSZ) (January 1998 to October 2000) for definitive biochemical identification at the species level . The frozen sputum sample was processed if the first sample was contaminated or had no growth. Identification and drug susceptibility tests were carried out using conventional methods and the BACTEC system (Becton Dickinson, Cockeysville, MD).\n\n【7】##### Mycobacterium tuberculosis Fingerprinting\n\n【8】Mycobacteria isolated from study patients were genotyped at Stanford University from March 1995 to February 1997 and at INCMNSZ from March 1997 to October 2000 with the internationally standardized IS6110-based restriction fragment length polymorphism (RFLP) technique and compared by using a computer-assisted visual approach (Bioimage AQ-1 analyzer and Molecular Fingerprinting Analyzer, version 2.0) . Isolates with identical IS6110 fingerprints that contained five or fewer hybridizing bands underwent spoligotyping at INCMNSZ as described . To assess transmission of M tuberculosis that rapidly progressed to disease, we established a 1-year period for defining clustering as described . Cases were considered “clustered” if two or more isolates from different patients were identified within a year that had 1) six or more IS6110 bands in an identical pattern, or 2) five or fewer bands with identical IS6110 fingerprints and matching spoligotypes. The first patient diagnosed in each cluster and those with unique fingerprints were classified as having reactivated disease.\n\n【9】### Analysis\n\n【10】Deaths were attributed to TB based on the death certificate (TB listed as the cause of death), interview of close caregiver (TB identified as probable cause of death), and active TB at the time of death (positive AFB smear after last treatment or during treatment if the patient did not complete treatment performed <6 months before death). A patient’s death was attributed to TB if two or more of these conditions were met. Bivariate and multivariate analyses were performed to describe association between sociodemographic (age, sex, household characteristics, occupation, ethnicity, years of formal education, place of residence, and social security), behavioral (usage of drugs and alcohol, and previous incarceration), clinical (previous diagnosis of TB; other associated diseases, previous hospitalizations; HIV infection, duration of symptoms previous to diagnosis; time elapsed between diagnosis and initiation of treatment and between initiation of treatment and smear conversion; and symptoms such as cough, hemoptysis, fever, night sweats, weight loss, and general malaise), bacteriologic (number of bacilli per microscopic field, drug resistance, RFLP, or spoligotype pattern), therapeutic (compliance, treatment outcome, and retreatment) variables, and death due to TB and other causes. Survival analyses included Kaplan-Meier curves and Cox proportional hazards model for TB and non-TB–related deaths using as reference time the period elapsed from diagnosis. Variables were entered into the models according to their statistical significance in univariate analysis and their biological relevance. The percentage of population attributable risk was calculated for variables that were included in the final model . DBASE IV and STATA 5.0 programs were used for data analysis .\n\n【11】### Results\n\n【12】During the study period, 454 patients were diagnosed with pulmonary TB. Most patients were men (270 \\[59.5%\\] of 454) and the median age was 42 years (range 12–97 years). Most came from lower socioeconomic status as indicated by household characteristics, formal years of education, and occupation. Of 438 patients, 74 (17%) lived in households with earthen floor, 214 (49.3%) of 434 had no access to potable water within the household, 293 (65.4%) of 448 had <6 years of formal education, and 104 (23.4%) of 445 were manual workers. The prevalence of HIV was 2.1% (9 of 429), combined resistance to isoniazid and rifampin was seen in 26 (6.7%) of 388 patients, and other resistance patterns in an additional 61 (15.7%) of 388. One hundred fifty-one (37%) of 413 patients had chest radiographs with evidence of pulmonary cavitation, and 72 (17.4%) of 413 patients had interstitial pulmonary infiltrates. Median time from initiation of symptoms to treatment was 101 days (range 3–2,307 days), median time from diagnosis to initiation of treatment was 5 days (range 0–594 days), and median time for sputum conversion was 42 days (range 15–1,228 days).\n\n【13】##### Treatment and Follow-Up\n\n【14】Of 11 patients who refused treatment, 4 died. Of 443 initiating treatment, 75.4% initiated treatment <10 days after diagnosis; 96.5% received supervised treatment. Outcomes for patients were as follows: 357 (80.6%) were cured of whom 314 (70.9%) were bacteriologically confirmed, 41 (9.3%) defaulted, and 20 (4.5%) failed treatment; 16 (3.6%) died during treatment, and 9 (2%) transferred out of study area. Patients were tracked for a median of 839 days (range 3–2,402). Sixty-one additional patients died during follow-up after treatment. Death was due to TB in 34 (41.9%) of 81 instances; 2 deaths were in patients who did not receive treatment, 11 in patients receiving treatment, and 21 after treatment. Tuberculosis mortality rates were higher during treatment versus after treatment (1.3/10,000 days vs. 0.7/10,000 days, p<0.01). Crude comparison of sociodemographic, bacteriologic, and clinical characteristics of patients who died from TB or from other causes and surviving patients are shown in Table 1 . Patients who died from TB had a higher probability of having been treated for a previous TB episode, showed more severe clinical symptoms, and had drug-resistant isolates. Less frequently they had other coexisting chronic conditions, such as HIV infection or hepatic cirrhosis. The patients who died from TB also had longer delays before diagnosis, treatment, and sputum conversion. These patients had higher probabilities of default, failure, and having subsequent TB episodes. The most common non-TB causes of death included diabetes , cirrhosis and other liver diseases , AIDS , cancer , and cardiovascular diseases . Kaplan Meier survival probabilities from TB deaths were 97.3% after the first 6 months, 95.8% after 1 year, 93.7% after 2 years, and 91% after 3 years.\n\n【15】##### RFLP and Spoligotyping Results\n\n【16】M. tuberculosis culture, drug test, and IS6110 RFLP and spoligotyping were available for 326 (72%) isolates. Comparison of patients whose isolates were available for genotyping to those whose isolates were unavailable indicated that patients for whom fingerprint analysis was not performed had a higher probability of being of native origin in Mexico (30 \\[24.2%\\] of 124 vs. 39 \\[12%\\] of 324, p= 0.001) and of living in households with earthen floor (31 \\[25.8%\\] of 120 vs. 43 \\[13.5%\\] of 318, p=0.002). Forty (12.3%) of the 326 evaluated cases were in clusters. The frequency of being members of clusters of recently transmitted disease was higher among patients dying from TB than among those dying from other causes or surviving (8 \\[28.5%\\] of 28 patients who died from TB vs. 32 \\[10.7%\\] of 298 patients who died from other causes and survivors, p=0.01).\n\n【17】##### Factors Associated with Mortality Rates\n\n【18】Predictors of death due to TB by Cox regression analysis included treatment default, resistance to isoniazid and rifampin, and recently transmitted TB controlling for time of occurrence of death, weight loss >15%, and years of formal education . The effect was not modified when gender, age, HIV infection, crowding in the household, household characteristics, occupation, ethnicity, previous treatment, delay in seeking treatment, specific symptoms, type of radiologic lesions, and other kinds of diseases were introduced into the model.\n\n【19】After controlling for age, predictors of non-TB death included HIV-infection, hepatic cirrhosis, and weight loss. Recently transmitted TB was not associated with other causes of death .\n\n【20】The proportion of death due to TB and to other causes attributable to the different categories of risk factors is shown in Table 2 . In the study population, 60% of deaths due to TB were attributable to drug resistance and treatment default.\n\n【21】Patients with recently transmitted disease had a lower probability of survival compared with patients with reactivated disease (p=0.007) . When causes of death were analyzed according to genotype, we found that TB was the cause of death in 8 (20%) of 40 patients with recently transmitted disease and in 20 (7%) of 286 patients with reactivated disease (p=0.01).\n\n【22】##### Sensitivity Analysis\n\n【23】Analysis of the distribution of the time between diagnosis dates of successive matching fingerprints indicated that 49.4% (95% confidence interval \\[CI\\] 44% to 54%) of all isolates with matching fingerprints patterns were identified within 1 year. When the interval was modified to 6, 18, 24, 30, and 36 months, we found that the proportion of clustered cases increased (8.3%, 13.8%, 13.8%, 14.4%, and 14.7%, respectively), (χ2 trend, p=0.02). The association between clustered cases and death due to TB continued to be positive for each of the other definitions of interval for clustering: 3.6 (1.1 to 10.5), p=0.01; 2.8 (1.0 to 7.3), p=0.03; 2.8 (1.0 to 7.3), p=0.03; 2.7 (0.9 to 6.8), p=0.04; and 2.6 (0.9 to 6.6) p=0.04.\n\n【24】### Discussion\n\n【25】This study describes high mortality rates from TB in a cohort of pulmonary TB patients who resided in an area with a low rate of HIV infection and were treated in the context of a well-functioning DOTS program. Patients were followed for an average of 2.3 years after diagnosis. Tuberculosis was associated with high rates of deaths both during treatment and after treatment completion, default, or failure. The main independent risk factors for death due to TB were treatment default and being infected with multidrug-resistant M. tuberculosis. Additionally, data indicate that cases due to ongoing transmission of TB may have higher mortality rates than cases due to reactivation of latent disease. These results suggest that current techniques underestimate death associated with TB and provide further impetus not only to treat but also to prevent TB.\n\n【26】Case completion rates are the standard by which the effectiveness of DOTS-based treatment programs are judged. A large body of evidence collected under diverse settings demonstrates completion rates of 81% and cure rates of 73% . The epidemiology of death in these programs is less well studied. Studies performed in HIV-endemic settings constitute an exception as the high rate of deaths occurring while patients are still on antituberculous therapy are cited as an impediment to achieving desired cure rates .\n\n【27】Our study reports TB mortality rates both during and after treatment completion, default, or failure. Of the 443 patients who were started on therapy, 16 died before completing therapy. This frequency (3.6%) compares favorably to that reported in other studies (4% in the Gambia , 15% in Bolivia , 25% in Malawi , and 28% in India ). However, in contrast to previous reports, we had a median of 833 days of follow-up after the completion of therapy, during which 21 additional patients (4.6%) died of TB, comparable with reports from the 1960s and 1970s . This mortality rate exceeds that expected according to age-adjusted state mortality statistics .\n\n【28】Although not novel, the risk factors for death due to TB identified in this cohort are noteworthy; these findings suggest that, even in a high-quality TB control program, additional efforts could yield important benefits. Treatment default has been previously described as associated with higher mortality rates in Mexico  and elsewhere . Concerted efforts to further reduce default may disproportionately decrease deaths. We also found that drug resistance, and particularly multidrug resistance, are associated with death . Although the most appropriate strategy for managing drug-resistant TB is debated, these data provide additional impetus for evaluating novel approaches, such as those recently introduced in Mexico, for managing drug-resistant TB . We estimate that 60% of deaths due to TB in this setting were attributable to treatment default and multidrug-resistance. Additional studies are needed to quantitate the presumably greater mortality rates that result from default and drug resistance in other settings.\n\n【29】Although relatively uncommon in this setting, HIV was associated with a high overall mortality rate. Most of these deaths were due to non-TB–related death. HIV/AIDS was the main predictor for non-TB deaths in this cohort. In our study, eight of the nine HIV-seropositive persons died, three of them a year or more after completing antituberculous therapy. Several studies have demonstrated excess mortality rates after successful TB treatment in HIV-infected patients ; the high rates have been attributed to HIV-related disease . These data emphasize the need to improve integration of quality treatment for both TB- and HIV-infected persons in populations that suffer from both diseases.\n\n【30】The most striking finding of the molecular epidemiologic component of this study is the association between clustering, which we interpret as indicative of recently transmitted disease, and TB-related death. Death due to TB was significantly more common among those who had recently transmitted disease than those with reactivated disease (28.6% vs. 7%, p=0.01). This association was independent of treatment default, multidrug resistance, time of occurrence of death, weight loss, and years of formal education. Furthermore, the effect was not modified when other variables indicative of sociodemographic level (such as occupation, characteristics of the household, or ethnicity) or clinical variables indicating other diseases, including HIV infection were introduced in the model. Inferring causality from such associations is difficult, it seems biologically implausible that being more likely to die made people more likely to acquire recently transmitted TB. A more plausible explanation is that rapidly progressing to disease contributes to deterioration of the health of these patients and thus increases their likelihood of death. The phenomenon is well described for other diseases such as measles. However, why this would be more pronounced for recently transmitted disease is unclear.\n\n【31】The association between clustering and death could be spurious because of limitations in molecular or conventional epidemiology. The validity of clustering as a proxy for recent transmission might be challenged by the fact that we did not perform fingerprint analysis on all isolates . Comparison between patients whose isolates were genotyped and those whose isolates were not genotyped showed that results of this study may not be generalizable to indigenous or lower socioeconomic groups. Our selection of the time interval allowed us to consider TB that was transmitted very recently and progressed to disease. The validity of using a 1-year interval was confirmed with the sensitivity analysis using different time periods (6, 18, 24, 30, and 36 months) as association between clustered cases and death persisted, despite modification of the time interval and by the fact that almost 50% of isolates with matching DNA fingerprint patterns occurred within 1 year of identification of the previous case. Conventional epidemiologic approaches to studying the cause of death are difficult. Death certificate data are notoriously unreliable, and whether patients died of TB or other causes is not certain . Therefore, we added other criteria that included the interview of a close caregiver and activity of TB at the time of death to validate our definition of death due to TB. The cause-of-death profile derived from interview of a close caregiver has been demonstrated to be useful for planning purposes . We consider that this definition adequately identified TB-related death as characteristics of patients dying from TB differed from patients dying from other causes in several important aspects (drug resistance, coexisting chronic conditions, and clinical severity) and allowed the identification of different risk factors for TB-related death and for death due to other causes.\n\n【32】If confirmed in other settings, the conclusions of this study have important implications for control programs. Most importantly, given that current surveillance data are collected at the conclusion of therapy, this method probably underestimates the true impact of TB on a population’s death. Given the increasing role of cost-efficacy modeling in setting health-care priorities, this oversight has important consequences. In addition, if recently acquired TB exhorts a greater mortality rate than that due to reactivated infection, the importance of interrupting TB transmission is further elevated.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cd838062-5d6a-4dc7-baf3-f16f512de8ec", "title": "Erratum, Vol. 19, October 27, 2022 Release", "text": "【0】Erratum, Vol. 19, October 27, 2022 Release\nBecause of a technical error, several lines of text were omitted in the online version of the first paragraph of the Results section of the article “Disparities in Influenza Vaccination Coverage and Associated Factors Among Adults with Cardiovascular Disease, United States, 2011–2020. We regret any confusion this error may have caused.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fc237074-e568-4a16-bd82-25d939318b55", "title": "Phylogenetic Analysis of Transmission Dynamics of Dengue in Large and Small Population Centers, Northern Ecuador", "text": "【0】Phylogenetic Analysis of Transmission Dynamics of Dengue in Large and Small Population Centers, Northern Ecuador\nDengue virus (DENV) is a vectorborne tropical disease transmitted by _Aedes aegypti_ and _Ae. albopictus_ mosquitoes. Globally, an estimated 390 million cases occur per year, and 3.9 billion persons are at risk for infection . Dengue is endemic to 100 countries; southeast Asia is the most severely affected region, followed by the western Pacific and the Americas .\n\n【1】Dengue is considered an urban disease because transmission is often reported in areas with high population density. However, human mobility and active commerce have increased in classically defined rural sectors where population density is generally lower, influencing DENV transmission. DENV transmission in rural sectors is often reported to occur at similar rates as urban areas . More remote rural areas can also serve as a source for emerging febrile diseases, often through cases introduced through cross-border movement. For instance, in Laos, surveillance of fevers in rural areas provided evidence that more DENV serotypes were circulating in rural areas than in urban ones . Those introductions highlight the need for highly coordinated and timely arbovirus surveillance at the local and national level to enable early warning of potential DENV outbreaks among neighboring countries . In Asia, cross-border surveillance systems and networks such as UNITEDengue have been established to share data on disease outbreaks to monitor and control the disease more effectively .\n\n【2】DENV has been endemic to South America since the 1980s and to Ecuador since 1988. Generally, only 1 serotype circulates episodically within a region in South America, but all serotypes have circulated and reemerged in cycles . During 2010–2019, explosive epidemics were reported in the Americas; cases peaked at 3.1 million in 2019  After a period of low prevalence in 2020, likely because of underreporting caused by the COVID-19 pandemic  and limited human movement between regions, Ecuador reported an increase of DENV cases in 2021.\n\n【3】Esmeraldas Province is located in the northern coast of Ecuador and shares a border with the Nariño department of Colombia. Fifty seven percent of the population lives in poverty and lacks basic needs such as potable water and garbage collection services . In a previous study conducted in rural communities in Esmeraldas during 2013–2014, we detected circulation of all DENV serotypes, whereas in the city of Esmeraldas, the largest city in the province, we detected only 2 serotypes (DENV-1 and DENV-2) . Those findings suggest that rural communities can act as a source of DENV transmission. High human mobility and levels of commerce reported in towns along the Colombia border suggest that DENV cases found in rural communities in Ecuador were likely introduced from Colombia .\n\n【4】On the basis of this previous evidence, the aim of this study was to extract DENV nucleotide sequences obtained from serum samples from active DENV cases collected during 2019–2021 in Esmeraldas Province. We investigated the phylogenetic relationship of those sequences to DENV nucleotide sequences from throughout Ecuador to learn more about the role of rural DENV transmission dynamics in northwestern Ecuador.\n\n【5】### Materials and Methods\n\n【6】##### Study Site\n\n【7】The study was conducted as part of an ongoing arboviral surveillance study in Cantón Eloy Alfaro, Esmeraldas Province, northwestern Ecuador. In brief, we selected 6 communities according to their gradient of remoteness: 2 remote riverine communities with no road access (Santa María and Santo Domingo), 3 communities with road access (Colón Eloy, Timbiré, and Maldonado), and 1 commercial center (Borbón) .\n\n【8】##### Sample Collection\n\n【9】We collected data during May 2019–December 2021 by using active fever surveillance at the individual level. Except in the town of Borbón, community residents >2 years of age were invited to participate in prospective active surveillance. In Borbón, we invited a random sample of 500 households located in the town center to participate. In total, 1,460 households and 5,957 participants provided \\> 1 month of active surveillance data from the study period of May 2019–December 2021. A group of community members trained to identify cases (brigadistas) conducted home visits weekly during the rainy season (June–October) and every 2 weeks during the dry season (November–December). The surveillance protocol began with brigadistas inquiring whether any household members had experienced fever, red eyes, or rash within the previous 7 days. When a symptomatic person was identified, the brigadista alerted the study nurse, who then followed up with a questionnaire asking about ≈21 symptoms associated with DENV and travel history. Participants experiencing diarrhea or upper respiratory symptoms were excluded. When a DENV-like illness was identified (fever plus rash, myalgia, arthralgia), a blood sample was immediately collected and a rapid diagnostic test performed (data not shown). Serum derived from the blood sample was stored in liquid nitrogen for transportation to the laboratory, where the samples were kept at −80°C until processing. The study was approved by the Bioethics committee at the Universidad San Francisco de Quito, the University of Michigan, and the Ecuadorian Ministry of Health.\n\n【10】##### RNA Extraction, Reverse Transcription PCR, and Sequencing\n\n【11】We extracted viral RNA from 488 febrile serum samples by using the QIAamp Viral RNA Mini Kit , according to manufacturer instructions. We used this RNA as a template for the triplex real time reverse transcription PCR (RT-PCR) Zika-Dengue-Chikungunya assay, which we performed to confirm infection and etiology . We serotyped 121 samples positive for dengue using a conventional RT-PCR with some modifications .\n\n【12】We considered 27 positive samples with a real-time PCR cycle threshold (Ct) value of <30 optimal for sequencing. We amplified a 20-µL aliquot of cDNA obtained using the Superscript IV protocol by multiplex PCR, combining 5 µL of 5X Q5 Reaction Buffer, 0.5 µL of 10 mM dNTPs, 0.25 µL of Q5 DNA polymerase, 15.25 µL nuclease-free water, and 1.5 µL primer pool A (10 µM). For DENV-1 and DENV-2, we added 1.5 µL primer pool B (10 µM) . We selected the samples that showed a 900 bp band in pool A and pool B mix for sequencing. Using 2.5 µl of PCR products from each pool diluted in 45 µl of PCR water, we prepared the cDNA MinION library by using a native barcoding kit (NB-114) with a ligation sequencing kit (LSK-109) following manufacturers’ instructions and loaded it into the MinION flow cell (FLO-MIN 106) . We performed demultiplexing and adaptor removal with Porechop version 0.2.3\\_seqan 2.1.1  and assembled the sequencing reads with a de novo assembly approach using Spades version 3.13.0 . Next, we mapped the reads in Minimap2 version 2.17-R941  against reference genomes for DENV-1  and DENV-2 . The consensus sequence was generated with Samtools version 1.7  and the serotype was confirmed using Genome Detective Arbovirus Typing software Tool version 1.137  and BLAST .\n\n【13】##### Phylogenetic Analysis\n\n【14】In total, we generated 9 DENV-1 and 13 DENV-2 genome sequences from our field site and used them for the phylogenetic analysis (22 samples). In addition, we sequenced 5 samples from the national surveillance system at the reference hospital of the city of Esmeraldas, the capital of the province (3 DENV-1 and 2 DENV-2) (accession nos. SRR1593089–SRR15793115). The hospital samples were selected for patients residing in Esmeraldas on the basis of the patient’s clinical record. To augment the Ecuador data for phylogenetic analysis, we incorporated 22 sequences from GenBank, 21 representing El Oro Province (located in southern Ecuador) and 1 sample from Esmeraldas. To enable cross-country analysis, we included available GenBank sequences from Argentina, Brazil, Perú, Nicaragua, Belize, Venezuela, and Colombia. Before generating an alignment, we identified genomes from the National Center for Biotechnology Information by randomly selecting the results of a BLAST of our whole-genome sequences for each serotype separately. We generated alignments by using MAFFT on XSEDE version 7.471 , then visually inspected and refined them in Aliview version 1.28 .\n\n【15】We constructed a maximum-likelihood phylogenetic tree for each serotype by using RaxML-HPC Blackbox version 8.2.12  under a general time-reversible substitution model with 1,000 bootstrap replicates and plotting the resulting tree in FigTree version 1.4.4 . We explored the temporal signal of the datasets by performing a root-to-tip genetic distance analysis using TempEst version 1.5.3  and rooted the trees using the R 2  method. To explore the temporal patterns of DENV spread, we constructed Bayesian time-calibrated trees by using BEAST version 1.10.4 under a Hasegawa-Kishino-Yano substitution model and an uncorrelated lognormal relaxed molecular clock . We used a Bayesian skyline tree prior  with 10 groups and a piece-wise constant reconstruction. We ran Markov chain Monte Carlo chains for 100 million steps and logged trees every 1,000 steps; we discarded the first 10% of the trees as burn-in by using TreeAnnotator version 1.10.4 . We assessed adequate mixing and convergence of model parameters, defined by effective sample sizes of >200, in Tracer version 1.7.1  and summarized maximum clade credibility (MCC) trees for each run using TreeAnnotator version 1.10.4, summarizing node ages as median heights.\n\n【16】### Results\n\n【17】##### Samples Collected and DENV Serotypes Identified\n\n【18】During May 2019–December 2021, we collected 488 serum samples from febrile patients during the household-based active fever surveillance study: 187 samples from remote riverine communities with no road access (146 from Santa María patients and 41 from Santo Domingo); 181 samples from communities with road access (61 from Colon Eloy, 78 from Maldonado, and 42 from Timbiré), and 120 samples from the commercial center, Borbón. Of the 488 samples, 121 were PCR-positive for DENV. A percentage of the samples were positive for chikungunya virus, as reported elsewhere . Common symptoms among patients were fever, headache, muscle aches, joint aches, chills, backache, and stomachache. Cases confirmed in June 2019 (19 cases) and July 2019 (16 cases) were all identified as DENV-1, whereas cases detected in January and March 2021 (12 cases) were all identified as DENV-2 . We did not obtain any samples during April 2020–November 2020 because of COVID-19 pandemic lockdown restrictions in Ecuador.\n\n【19】##### Phylogenetic Analysis of DENV-1 Serotype in Ecuador\n\n【20】The MCC tree for DENV-1 generated in BEAST software  using whole-genome sequencing indicated at least 2 instances of viral variant exchange (viral exchanges) between countries. Those viral exchanges, inferred from the genetic similarities highlighted in the phylogeny, suggest that the strains were introduced into Ecuador. In one instance, a single Ecuador sequence collected in 2014 from Esmeraldas province  shares a common ancestor with a sequence obtained in 2010 in Argentina. The theoretical most recent common ancestor of those 2 sequences was estimated to date back to 2006 and shared a common ancestor with viral sequences from Venezuela comprising the bulk of clade I . No additional sequences from this clade were detected in Ecuador, suggesting limited transmission after this potential introduction. The other DENV-1 viral exchange occurred no later than 2008 and was associated with the bulk of subsequent DENV-1 sequences from Ecuador. Those sequences formed 2 clusters, 1 from Esmeraldas Province and 1 primarily from El Oro Province that also included a sequence from Esmeraldas province . El Oro is located 600 km south of Esmeraldas Province . The introduction of those viruses into the rural communities of Esmeraldas Province is estimated to have occurred no later than 2012. Persistence of clade II lineage in Ecuador over time without the presence of other sequences from other countries suggests that Ecuador has sustained transmission that is continuing to evolve independent of external viral introductions.\n\n【21】##### Phylogenetic Analysis of DENV-2 Serotype in Ecuador\n\n【22】The MCC tree for DENV-2 whole-genome sequencing (along with some partial genomes) also suggests \\> 2 exchanges of distinct viruses among Ecuador, Colombia, and Venezuela. In one instance, a cluster composed of 20 sequences collected in 2014–2015 from Machala, a city in El Oro Province, shares a most recent common ancestor dating back to 2007 with sequences from Colombia . The closest related sequence to this Ecuador clade is a Colombia envelope gene sequence from Valle del Cauca collected in 2013. In addition, 2 envelope gene sequences from Colombia (Putumayo) sampled in 2013 clustered within the Ecuador clade II. The other instance of viral exchange is suggested by 2 clusters, a cluster of 15 Ecuador sequences collected from 2020–2021 active surveillance of rural communities in Esmeraldas Province, which share a most recent common ancestor dating back to 2013, and a cluster of sequences from Colombia . The sequences from Machala arrived in Ecuador earlier than those from Esmeraldas Province, possibly because the Machala study predates the Esmeraldas study .\n\n【23】##### Phylogenetic Analysis of DENV-1 Serotype from Esmeraldas Province\n\n【24】Focusing on the DENV-1 serotype from our study in Esmeraldas Province , we labeled sequences on the basis of their gradient of remoteness: remote communities with no road access (Santa María and Santo Domingo), communities with road access (Colon Eloy, Timbiré, and Maldonado), the commercial center (Borbón), and the large population center of Esmeraldas Province (Esmeraldas city) . This DENV-1 subclade tree does not show distinct clustering by gradient of remoteness, although it does support the hypothesis that the cities of Esmeraldas and Borbón are sources of the viruses circulating in the smaller communities in our study site. A sequence from the city of Esmeraldas collected in 2019 (root position posterior node probability 0.9999) and a sequence from the Borbón hospital collected in 2021 (root position posterior node probability 0.9092) shared a most recent common ancestor dating back to 2012 . The descendants of those 2 ancestral sequences are found in remote communities and communities with road access; however, an Esmeraldas city sequence collected in 2019 derived within this cluster is suggestive of a subsequent exchange back into the large population center.\n\n【25】##### Phylogenetic Analysis of DENV-2 Serotype from Esmeraldas Province\n\n【26】The DENV-2 Ecuador lineage from Esmeraldas Province demonstrates clustering by community . We also found strong support (0.9378) for the commercial center (Hospital of Borbón) and for those communities with road access (Colon Eloy and Timbire) being intermediate sources (0.992) of DENV-2 strains moving between the city of Esmeraldas and 2 remote communities (Santa Maria and Santo Domingo). The most recent common ancestor dates back to 2018 for a subset of the remote samples. Although much of the data support viral movement from cities and commercial centers to more remote communities, our phylogeny also supports movement from a remote community (Santo Domingo) to the commercial center of the region (Borbón) .\n\n【27】### Discussion\n\n【28】Human mobility is one of the main determinants of DENV transmission  because the flight range of _Aedes_ mosquitoes is minimal . The social dynamics of the region combined with our results suggest that Esmeraldas Province is an entry point for DENV strains arriving in Ecuador from Colombia. Legal and illegal commerce (mostly by sea) between Esmeraldas Province and Colombia is robust  and occurs primarily in small, mostly rural communities with weak customs controls that favor informal and illegal commerce and irregular human migration.\n\n【29】Our data suggest that DENV-1 entered Ecuador no later than 2009 and moved to at least 2 Coastal provinces in Ecuador (Esmeraldas and El Oro). Sequences found in those provinces were most closely related to nucleotide sequences from Colombia, but they also showed similarity to sequences from Venezuela . Another DENV-1 sequence shares a theoretical most recent common ancestor from 2006 with an Argentina sequence and also shares high similarity with sequences from Venezuela . The first DENV-2 introduction might have occurred earlier than 2009, and the virus circulated in Ecuador until 2015. This DENV-2 strain was most closely related to Colombia strains circulating until 2013 . The second introduction probably occurred no later than 2013, resulting in an Ecuador lineage that persisted at least until 2021. Those DENV-2 sequences showed high similarity to both Colombia and Venezuela sequences.\n\n【30】The perspective that large tropical cities are the primary source of DENV (and other _Ae. aegypti_ mosquito–transmitted viruses) has resulted in establishment of surveillance centers in large urban centers, but in Ecuador, social (and commercial) dynamics suggest that DENV is also entering through small rural community centers and subsequently disseminating to the cities. Although most of our data suggest that DENV is brought to remote communities from urban centers, we also found instances of DENV movement from remote communities to larger commercial centers (e.g. Esmeraldas and Borbon). Smaller communities with road access probably provide a bridge between more remote communities and larger commercial centers, such as Esmeraldas and Borbón, thereby serving as intermediate sources. Large population centers, such as Esmeraldas, are hubs for public transportation and therefore may become the epicenter of viral spread . Further studies are needed to understand how a heterogeneous landscape (including biological, social, and environmental factors) among communities in the region drive epidemic dynamics .\n\n【31】One reason that DENV transmission has remained endemic to regions such as Ecuador (in contrast to the diminished transmission of Zika and chikungunya viruses) is that there is continual serotype and lineage replacement to which cross immunity is only partial . For example, the Ministry of Health reported a 2.8-fold increase in the number of dengue cases from 2014 to 2015 (15,031 to 42,459 cases) . This increase was likely caused by introduction of a new lineage of DENV-2 from Colombia to Ecuador, leading to the replacement of the previously dominant lineage that had been circulating since 2012 . The replacement of lineages has been reported in several studies associated with outbreaks linked to the introduction of novel lineages  and has long been considered a part of DENV transmission dynamics in endemic and hyperendemic regions . Those replacements might be fueled by viruses displaying novel antigenic variations for which the local population lacks immunity  or by viral variants with higher fitness . Another reason for the observed increases might be that cases reported by the Ministry of Health are other _Ae. aegypti_ mosquito–transmitted arboviruses, such as chikungunya virus .\n\n【32】Several surveillance gaps should be considered when interpreting our data, including minimal historical specimens, lower resolution in rural sites than in urban sites, and limited country-specific data from countries in South America (including Ecuador). Although those gaps are reflective of working in a resource-limited setting, our data also reflect a unique contribution to dengue transmission dynamics in a resource-limited setting. Additional gaps in our data are a result of the COVID-19 pandemic, which might have resulted in our missing DENV cases and potential DENV introductions. Ecuador’s countrywide state of emergency (March 16–April 24, 2020) and subsequent restrictions forced us to stop or sharply curtail our active household-based surveillance study . This state of emergency similarly affected the government’s passive surveillance program.\n\n【33】We believe our conclusions are robust for several reasons. First, we are using complete genome sequences. Second, we obtained posterior probabilities by using a Bayesian Skyline analysis that provides us with a level of confidence for our phylogenetic relationships.\n\n【34】For example, evidence is ample that Colombian viruses are ancestral to and a potential source of Ecuador DENV . Colombia has numerous, very active Caribbean ports that likely serve as entry points to South America for the virus . In contrast to previous reports that suggest migration from Venezuela drives DENV lineage introduction into Ecuador , we believe its role is minimal. Ecuador does not share a border with Venezuela, and 75% of immigrants from Venezuela travel through Colombia (a 5-day journey). In addition, the main border crossing point in the highlands (Rumichaca, Ecuador)  is a region with no _Ae. aegypti_ mosquito activity. A recent report, however, shows that migrants from Venezuela play a vital role in the introduction of DENV to Colombia , so further introductions to Ecuador through the Colombia border are likely.\n\n【35】Although our findings are consistent with what is known about the social dynamics of the study region, samples collected through community-based surveillance studies and previously reported strains were opportunistically gathered, usually from clinical settings. DENV cases are likely underestimated in Ecuador. Viral genomic characterization from countries such as Colombia and Venezuela are similarly limited, further adding uncertainty to our capacity to estimate the precise source and direction of exchanges.\n\n【36】Cases in smaller population centers are increasingly recognized as playing a vital role in broader DENV transmission dynamics . Our phylogenetic analyses in Ecuador shed light on those dynamics. Esmeraldas Province might be a key site from which DENV is introduced, amplified through local transmission, and then spread throughout Ecuador by human travel and movement of infected mosquitoes. Our results underline the need for coordinated efforts in DENV strain monitoring and control across national borders. Ministries of Health should intensify dengue surveillance in neglected remote regions, especially along national borders. Surveillance can also elucidate how DENV diffuses and becomes endemic, which requires an understanding of how large population centers affect viral dynamics regionally, and whether smaller, more remote communities represent a silent reservoir of ongoing dengue circulation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8e7646e3-83dc-4648-8463-ed07b4364c58", "title": "Features and Initial Assessment of the Italian Behavioral Risk Factor Surveillance System (PASSI), 2007-2008", "text": "【0】Features and Initial Assessment of the Italian Behavioral Risk Factor Surveillance System (PASSI), 2007-2008\nAbstract\n\n【1】**Introduction**Surveillance systems for health status and behaviors of populations are fundamental for planning, implementing, and monitoring preventive interventions. In 2006, the Italian Ministry of Health provided funding to the National Institute of Public Health to develop an ongoing surveillance system for adult behavioral risk factors. We describe the main features of the system (known as PASSI) and provide a preliminary assessment of its activity. **Methods**PASSI is conducted by participating local health units, which use a common questionnaire and methods. Each month, local health unit staff conduct telephone interviews of a random sample of resident adults aged 18 to 69 years. Data are transmitted to the national coordinating center, where they are cleaned, managed, and made available for local, regional, and national analysis. Training, data analysis, and communications are centrally supervised, and data quality is routinely monitored. **Results**In 2007 and 2008, nearly 60,000 interviews were completed. The demographic characteristics of survey participants closely corresponded to census data in the surveyed areas. The response rate was 82%; the refusal rate was 10% or less. Communications activities have been conducted to disseminate the results and encourage their use. **Conclusions**PASSI is administered by the public health system with limited human and financial resources. In the first 2 years of activity, the data quality was good, and information collected was useful. The organizational model of PASSI may be of interest to countries that are developing surveillance systems as well as those with systems already in place.  \n\n【2】Introduction\n\n【3】Noncommunicable chronic diseases (NCDs) are responsible for most disease burden worldwide and have a high economic impact . In Europe, 86% of all deaths and 77% of disability-adjusted life years lost are attributable to NCDs . Given the role of lifestyle in the development of NCDs, the planning, implementation, and evaluation of interventions for their prevention and control rely on timely information about the population’s health status and behaviors , as well as its knowledge and perceptions of healthy habits. The most appropriate means of obtaining this information is through specific surveillance systems, which must conform with international standards . In Italy, the most recent National Health and Prevention Plans include multiple interventions for tackling the most important risk factors for NCDs, in accordance with the integrated approach proposed by the World Health Organization . Along with other European countries, Italy adheres to a common strategy for NCD control (Gaining Health)  and in 2007 adopted a program called Guadagnare Salute . Italy’s National Statistics Institute performs periodic national surveys on health determinants and the adherence to available preventive measures . However, the periodicity, timeliness, and local representativeness of these data are often insufficient for planning public health actions. For these reasons, in 2006 the Italian Ministry of Health commissioned the National Centre for Epidemiology, Surveillance and Health Promotion (Centro Nazionale Epidemiologia, Sorveglianza e Promozione Della Salute, or CNESPS) of the National Institute of Public Health to develop a system for the ongoing surveillance of major behavioral risk factors and preventive measures for NCDs. The system, Progress by Local Health Units Towards a Healthier Italy (Progressi Delle Aziende Sanitarie per la Salute in Italia, or PASSI), is based on ongoing nationwide collection of data using a standardized questionnaire. We describe the system and present the results of an initial assessment of its first 2 years of activity. We evaluate population representativeness, quality of the data collection process, acceptability, costs, and sustainability of the system. Our methods are similar to those adopted in previous assessments of the initial phase of surveillance systems in the United States .  \n\n【4】Methods\n\n【5】At the request of the Ministry of Health, in 2005 and 2006, pilot studies were conducted in most of Italy’s 21 regions to assess the feasibility of performing surveillance and providing local and regional data for public health interventions; the pilot studies were coordinated by CNESPS. A provisional protocol was developed by participants in the Field Epidemiology Training Program, in cooperation with regional public health professionals, based on the Behavioral Risk Factor Surveillance System of the US Centers for Disease Control and Prevention . In 2006, a national committee was established to develop the definitive protocol for PASSI. The committee consisted of staff from CNESPS, graduates of the Field Epidemiology Training Program who had performed the pilot studies, and outside experts. The protocol, which has been published elsewhere , was approved by the ethics committee of the National Institute of Public Health. In Italy, health care is provided by the regions, which have considerable economic and regulatory autonomy within the framework of the National Health System. The number of local health units (LHUs) per region varies between 1 and 22, with catchment populations ranging from 40,000 to more than 1 million. Because most public health interventions are planned and evaluated locally, we wanted to maintain the level of “information production” as close as possible to the level of “information use” by involving regions and LHUs directly in the surveillance process. In PASSI, LHU personnel receive training and support from regional coordinating groups. The national coordinating group, based at CNESPS, provides leadership on technical issues, supports the regional coordinating groups, and oversees the functioning of the system. Initially, the national coordinating group provided training on all aspects of the system to the regional coordinators. They in turn provided initial training on basic procedures to all PASSI supervisors and interviewers at the LHU level (more than 1,000 nationwide). The questionnaire was based on questionnaires used for similar surveys, in particular the Behavioral Risk Factor Surveillance System, to facilitate the comparison of PASSI results with those from other systems . Box. Questionnaire Topics, Italian Behavioral Risk Factor Surveillance System (PASSI), 2007-2008TopicTarget PopulationFixed core modulesSelf-perceived health and quality of lifeAllSmoking habitsAllAlcohol consumptionAllCervical cancer screeningWomen, 25-69 yBreast cancer screeningWomen, 40-69 yColorectal cancer screeningWomen and men, 50-69 yDiet and nutritional statusAllPhysical activityAllMental healthAllCardiovascular risk factorsAllInfluenza vaccinationAllRubella vaccinationWomen, 18-49 yPrevention of traffic accidentsAllPrevention of domestic injuriesAllSociodemographic aspectsAllRegional optional modules (since June 2008)Prevention of traffic accidents (supplement)AllHormone replacement therapyWomen, 45-60 yChild health promotion campaignAllAbbreviation: PASSI, Progressi Delle Aziende Sanitarie per la Salute in Italia (Progress by Local Health Units Towards a Healthier Italy).The questionnaire covers a variety of topics related to health behavior and prevention, all of which are considered priorities in the National Health Plan  and the National Prevention Plan  (Box). Particular attention is given to subjective aspects, such as the respondents’ perceptions, opinions, knowledge, and attitudes about health behaviors, and whether their doctors provide them with appropriate medical advice. Almost all questions are closed-ended, with multiple-choice answers. Many questions are administered only to specific population subgroups. The questionnaire has 4 parts: Fixed core component: a standard set of 114 questions, grouped into 15 modules (Box). These questions are asked by all participating LHUs and are meant to remain substantially unchanged for many years.Rotating core: questions on different topics that are asked in alternating years by all participating LHUs.Optional modules: questions that regions elect to use to satisfy specific regional information needs (eg, for health promotion campaigns).Emerging modules: a few questions, administered for brief periods, to gather timely information on important issues of a “late breaking” nature (eg, influenza).The questionnaire is revised annually so that new questions can be added (or existing ones reworded), based on public health policy and emerging evidence on the issues addressed by the surveillance system. In the first 2 years of data collection , the fixed core had 2 minor revisions, and 3 optional modules were added. In late 2009, an emerging module on the A-H1N1 influenza virus was implemented, and in 2010 a revised version of the questionnaire was introduced that included a rotating module and 2 new optional modules. In each LHU, a random sample is extracted each month from the enrollment lists of residents aged 18 to 69 years in the catchment area, stratified by sex and age group (18-34 y, 35-49 y, 50-69 y). These lists contain demographic data (eg, birthday, sex), home address, name of their general practitioner (GP), and often a telephone number. The sample is large enough (at least 25 people per month per LHU) so that annual estimates of the main variables can be obtained with acceptable precision at the LHU level, as well as more frequent estimates for some regions and the entire country. Inclusion criteria and other methodologic details are reported in the Appendix . After the sample is extracted, a letter is sent to the homes of the selected individuals. The letter explains the purpose of the system and informs the recipients that they will be contacted shortly; their GPs are also informed. Telephone numbers, if not recorded on the enrollment list, are obtained from telephone directories, from the GPs, or from the respondents themselves when they call to make an appointment after receiving the letter. Survey administrators attempt to call at least 6 times on different days of the week (including weekends) and at different times of the day; if a person cannot be reached, a substitute of the same sex and from the same age group is randomly extracted. The questionnaire is administered via telephone interviews conducted by specially trained personnel from the public health departments of the LHUs. After briefly explaining the objectives to the respondent, the interviewer obtains oral informed consent. The interviewer uses computer-assisted telephone interviewing or printed questionnaires and subsequent data entry on a personal computer. All data are self-reported. Because they are anonymous, they cannot be validated at the individual level. The ongoing surveillance process, with interviews conducted every month, provides the system with flexibility, since items can be modified over time; it also permits evaluation of trends and seasonal variations. For small areas, more precise estimates can be obtained by aggregating data over time. Data collected on the printed questionnaire are entered on a personal computer. All the records are encrypted and transmitted to a centralized database, where basic data-quality controls are performed. No personal identifiers appear in the database. PASSI interviewers and supervisors have protected access to the server through a web portal , with individual user names and passwords and differentiated access profiles. Uploaded records are accessible to the local coordinators, and input errors can be corrected online. The quality of the data collection process is automatically monitored using indicators modeled after international standards  . After all interviews collected in a calendar year are uploaded, data are checked and edited at the central level. To calculate regional and national pool estimates, the data from the different LHUs are aggregated. Because the LHUs differ considerably in terms of population size, and the sizes of the samples also differ substantially, a weight specific for each LHU stratum is added to each record to account for the number of interviews performed in each of the 6 strata of the LHU’s sample and the size of the corresponding strata in the LHU’s target population. Edited datasets are subsequently made available for downloading to regions and LHUs. CNESPS performs national-level analyses and provides the regions and LHUs with tools for obtaining local results, such as data analysis plans and codes to run statistical programs.  \n\n【6】Results\n\n【7】Data collection began in 2007 in 19 regions, and 20 regions participated in 2008. The number of participating LHUs was 124 (74%) in 2007 and 136 (84%) in 2008. Approximately 85% of Italy’s 18- to 69-year-old population was covered in 2008. The number of interviews performed rose from 22,006 in 2007 to 37,819 in 2008, of which 13% and 22%, respectively, were computer-assisted telephone interviews. The monthly number of interviews performed in each region varied widely (range, 25-500) because of differences in population size and number of LHUs. The median duration of interviews for both years was 20 minutes. The demographic composition of the PASSI national pool sample in 2007 reflected the distribution by age and sex of the population in the corresponding LHUs, as determined by the National Statistics Institute. In the PASSI sample, the younger strata were slightly underrepresented and the older strata overrepresented; the maximum difference was 1.1 percentage points compared with official demographic figures . The eligibility rates for 2007 and 2008 were 92% and 91%, respectively, and the response rate for both years was 82%. The refusal rate was 10% for 2007 and 8% for 2008. Partial interviews and break-offs  accounted for less than 1% of all registered interviews. The National Centre for Disease Prevention and Control (Centro Nazionale per la Prevenzione e il Controllo Delle Malattie, or CCM) provided the National Institute of Public Health with €1 million (US $1.27 million), used for the national coordinating activities and for supporting the regions in the first 3 years of the project. The participating regions and LHUs contributed part-time personnel, and some regions provided incentives for interviewers. The cost of an interview at the LHU level was an estimated €20 (US $25.46), including preparatory activities and data entry but excluding the coordinating costs. Preliminary results were presented at the 5th International Conference on Behavioral Risk Factor Surveillance in 2007, in Rome, organized by the National Institute of Public Health and the CCM. The national results of the first year of surveillance  were presented in 2008 during an official workshop in Rome. In September 2009, updated PASSI data were presented at a national conference of the Italian program Guadagnare Salute, sponsored by the Ministry of Health. General annual reports and brief reports on specific health topics have been produced and made available to stakeholders. One peer-reviewed article has been published . Most regions and many LHUs have already released their surveillance data and used them for local public health activities. National, regional, and local reports, documents, and other relevant material for PASSI are accessible online .  \n\n【8】Discussion\n\n【9】Although we are conducting a more complete evaluation of PASSI, following internationally accepted guidelines , and validation studies are being performed to compare the results with those provided by other information sources, this article nonetheless highlights some important characteristics of PASSI. First, the study sample appears to be representative of the target population. Second, acceptability by respondents also appears high, as suggested by low refusal rates and the negligible proportion of incomplete interviews. The response rate compares favorably with rates reported in the most successful international surveys . Furthermore, data collected by PASSI seem to be of practical use, as demonstrated by the fact that many public health agencies use the data for communication, policy making, and health intervention planning. Perhaps the most important feature of PASSI is that it is carried out by the public health system at the local level, which seems to have many advantages. In particular the sampled individuals were contacted and interviewed by LHU personnel, which seems to have contributed to the high level of participation; LHUs’ lists of residents were readily available for sampling; and direct involvement of LHU personnel in managing the system provided them with the opportunity and motivation to identify and monitor the needs of their populations and their perceptions of the preventive interventions offered. However, PASSI does have some limitations. In particular, as with any health survey, the reliability and validity of self-reported behavior may be problematic . Furthermore, people who are younger than 18 years or older than 69 years are not included in the survey. The age range of 18 to 69 years was chosen because minors cannot provide informed consent to be interviewed by telephone and because even mild cognitive impairment, which is present in a sizable proportion of older people , can reduce the reliability of the answers. Moreover, many preventive programs monitored by PASSI, such as tumor screening, are recommended for people up to age 70 years. To obtain information on the health status of the elderly, a different approach (known as PASSI d’Argento) is being piloted with assistance from the national coordinating group; this approach focuses more on health issues of the elderly and allows for face-to-face and, if necessary, proxy interviews. A critical aspect of PASSI is sustainability. The system is still considered experimental, without a stable institutional setting. Local surveillance is conducted by public health staff who have other routine responsibilities. This situation generates fatigue, which is exacerbated by insufficient political and financial support in some regions, where public health infrastructures are more fragile. Another challenge is maintaining a sense of usefulness when prevalences for many of the variables change little over time. Geographic comparisons have proved useful in sustaining the momentum of the US system , and as the number of collected interviews increases over time, subgroup analyses at the local level will be possible, improving targeting of health interventions. Despite these limitations, many regions and LHUs support the goal of creating an institutional setting for PASSI. Furthermore, in 2009, the CCM renewed funding for PASSI for 2 years so that it could be implemented throughout the entire country. Another substantial contribution to the improvement of PASSI has been its link to the international health promotion and surveillance community . Our experience shows that a surveillance system such as PASSI can operate with a reasonable amount of resources, producing useful data and gaining the support of its main stakeholders. We believe that PASSI is an interesting model both for countries intending to develop similar surveillance systems and those in which such systems are already in place.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e7673dd6-999f-4e78-8206-f29783d5e113", "title": "Detection and Genotyping of Coxiella burnetii in Pigs, South Korea, 2014–2015", "text": "【0】Detection and Genotyping of Coxiella burnetii in Pigs, South Korea, 2014–2015\nQ fever is a zoonotic disease caused by the extremely infectious bacterium _Coxiella burnetii_ . Humans can be infected by inhalation of infectious aerosols or contaminated dust from infected ruminants or through contact with infected animal products. Ruminants are known as the primary reservoirs for the bacterium. Wildlife may also serve as reservoirs . However, epidemiologic data on the occurrence of _C. burnetii_ in pigs are limited. Their susceptibility to _C. burnetii_ infection has been confirmed by the presence of serum antibodies , but strong evidence for pigs serving as reservoirs of _C. burnetii_ is lacking. In addition, transmission of _C. burnetii_ from pigs to humans has not been confirmed.\n\n【1】In the veterinary field, commercial immunologic methods are the easiest to interpret and are used at the herd level to detect _C. burnetii_ infection or exposure within a population of animals . In South Korea, there have been several studies on _C. burnetii_ in ruminants , but studies evaluating _C. burnetii_ in pigs are lacking. As first step toward understanding the epidemiology of _C. burnetii_ in pigs, we evaluated the prevalence and genotypes of this bacterium in pigs reared in Gyeongsang Province, South Korea.\n\n【2】### The Study\n\n【3】During 2015 in South Korea, a total of 10,332,000 pigs were raised, of which 2,338,521 (22.6%) were raised on 1,134 farms in Gyeongsang Province . For this study, we collected 1,030 blood and 97 tissue samples from pigs (645 breeding and 479 fattening pigs) reared on 209 pig farms in Gyeongsang Province during 2014–2015. Sample size was determined using a formula with an expected disease prevalence of 50%, accepted absolute er­ror of 5%, and CI of 99% in a simple random sampling design ; a minimum of 664 samples were required. Samples were collected by practicing veterinarians during treatment or regular medical checkups; ethical approval was not required. The number of samplings was based on the number of pigs and farms within each of the Province’s administrative districts .\n\n【4】Whole blood was used for PCR; the serum was separated for serologic testing. Lung, lymph node, liver, spleen, and kidney samples were collected for differential diagnosis of diseases in pigs that aborted or had a stillbirth, respiratory symptoms, or weakness.\n\n【5】To detect _C. burnetii_ –positive samples, we used 2 different assays and nested PCR. We used an indirect multispecies ELISA (ID Screen Q Fever Indirect Multi-species Kit; IDvet, Montpellier, France) according to the manufacturer’s instructions to detect _C. burnetii_ antibodies in samples; a sample optical density to positive-control optical density value of >50% was considered positive. We also performed an indirect immunofluorescence assay (IFA), using the _Coxiella burnetii_ (Q Fever) FA Substrate Slide (VMRD, Pullman, WA, USA), as recommended by the manufacturer; titers \\> 64 to phase-1 or phase-2 antigens were considered seropositive. We used the DNeasy Blood and Tissue Kit (QIAGEN, Hilden, Germany) according to the manufacturer’s instructions to extract DNA from whole blood and tissue samples. The _Coxiella_ 16S rRNA gene in extracted DNA was then amplified using nested PCR and sequencing primers . We sequenced amplification products with Macrogen (Seoul, South Korea) and analyzed results using sequence alignment programs and statistical methods .\n\n【6】Of the 1,030 sampled pigs, 70 (6.8%) were positive for _C. burnetii_ by ELISA ; these pigs were from 32 (15.3%) of the 209 sampled farms. Two of the 32 farms had 8 positive pigs each; the other 30 had 1–3 positive pigs each. Fifty-three (5.2%) sampled pigs had samples identified as phase-1 or phase-2 antigen seropositive by IFA; these samples were also seropositive by ELISA. An additional 17 samples seropositive by ELISA were seronegative by IFA. _C. burnetii_ seroprevalence was significantly higher (p<0.0001) in breeding than in fattening pigs by ELISA and IFA.\n\n【7】ELISA and IFA results were in agreement for 1,013 (98.4%) of the 1,030 samples; 53 (5.2%) samples were positive, and 960 (93.2%) were negative. The Cohen κ coefficient was 0.85 (i.e. very good agreement; 95% CI 0.79–0.92).\n\n【8】Three (0.3%) pigs were positive for _C. burnetii_ by PCR; all were breeding pigs and seronegative for _C. burnetii_ . One positive sample was lung tissue from a pig that appeared to have respiratory signs; other respiratory pathogens were also detected in the sample. Acute _C. burnetii_ infection with organ involvement was confirmed by PCR. However, the infection status of seropositive pigs cannot be determined on the basis of a single titer. _C. burnetii_ –seronegative pigs can, however, shed the organism and, thus, might serve as a reservoir for transmission of the bacterium to humans. 16S rRNA gene sequences for the 3 _C. burnetii_ PCR-positive samples (GenBank accession nos. KT945014–16) showed 100% identity with each other; nucleotide sequences showed high (96.6%–96.9%) identity with those of other _C. burnetii_ strains. Phylogenetic analysis showed that the 3 isolates belong to clade A, clustering with previously published _C. burnetii_ sequences .\n\n【9】### Conclusions\n\n【10】We found that 6.8%, 5.2%, and 0.3% of tested pig samples in Gyeongsang Province were positive for _C. burnetii_ by ELISA, IFA, and PCR, respectively. These rates of seropositivity are relatively low compared with the rate found in a study in Uruguay, in which 18.4% (83/479) of the blood samples were seropositive by layer microagglutination . In that study, the innate susceptibility of pigs to _C. burnetii_ was confirmed during a Q fever epidemic. Seropositivity in our study was, however, higher than that reported in blood tested by IFA in Japan (0/396 samples)  and by complement fixation in Bulgaria (0.05%; 1/1,809 samples) .\n\n【11】In _C. burnetii_ –positive animals, bacterial burden is highest in birth products. We did not test such tissues; however, the positivity rate in our study was similar to that (0/16) in a previous examination of pig placentas by real-time PCR in the Netherlands . In our study, the PCR-positive pig samples did not test positive by serologic methods.\n\n【12】Similar to results from a previous study , our results showed that IFA was less sensitive than ELISA at detecting _C. burnetii_ in serum. However, serologic diagnosis of coxiellosis in animals is complicated. Animals can maintain seropositivity after acute infection has cleared, and they can seroconvert without shedding ; thus, serologic methods are not useful for determining which animals currently pose a risk for transmission.\n\n【13】In our study, seroprevalence among breeding pigs was significantly high (p<0.05). In addition, only breeding pigs were positive for _C. burnetii_ by PCR. Because of pregnancy stress, breeding pigs probably experienced a recrudescent infection, making them more likely to shed the organism. A study on the epidemiology of Q fever suggested that breeding pigs can cause infection in humans .\n\n【14】The genus _Coxiella_ is divided into 4 highly divergent genetic clades (A–D); _C. burnetii_ belongs to clade A . Phylogenetic analysis showed that the 3 _C. burnetii_ isolates in our study were closely related to clade A strains from the United States, Japan, and Greenland, indicating a close epidemiologic link.\n\n【15】Although the number of _C. burnetii_ –positive pigs was low in our study, a previous study identified contact with pigs as a risk factor for _C. burnetii_ seropositivity in humans . Therefore, pigs may serve as potential reservoirs for _C. burnetii_ . However, several questions remain unanswered regarding the epidemiology of _C. burnetii_ infection in pigs and possible transmission to humans. Additional investigations of the infection prevalence in other animals are necessary to understand the epidemiology of _C. burnetii_ .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c21e4a28-c5d8-48e6-9af5-3de879fd558f", "title": "Canine Influenza Virus A(H3N2) Clade with Antigenic Variation, China, 2016–2017", "text": "【0】Canine Influenza Virus A(H3N2) Clade with Antigenic Variation, China, 2016–2017\nCanine influenza can be caused by a variety of influenza A viruses, including equine-origin H3N8 and avian-origin H3N2 viruses, which are both established lineages in dogs worldwide. Canine influenza virus (CIV) A(H3N8) has predominantly circulated in the United States since 2004 , and CIV A(H3N2) mainly prevails in China and South Korea . CIV H3N2 was first isolated in 2006 from Guangdong Province, China, and found to be genetically most closely related to H3N2 avian influenza viruses prevalent in aquatic birds in South Korea . Since 2006, H3N2 CIV has rapidly become prevalent in China and South Korea  and has also been isolated in Thailand and the United States .\n\n【1】CIV usually causes mild respiratory symptoms, and CIV-infected dogs often recover without treatment. As a consequence, animal owners and veterinarians often neglect treating CIV infections, creating an opportunity for CIVs to circulate and further adapt in dogs. Mutations leading to better growth in dogs could enhance infectiousness in other mammals (e.g. humans). Also, CIVs are antigenically novel to the human immune system and, thus, might pose a threat to public health. Therefore, we set out to characterize CIV H3N2 in dogs in China to assess the potential risk to the public.\n\n【2】### The Study\n\n【3】During October 2012–July 2017, we collected 399 throat swabs from dogs with respiratory symptoms in pet hospitals and kennels in China to monitor for CIV H3N2 epidemics and virus evolution. We amplified the matrix gene by real-time reverse transcription PCR using Influenza A Virus V8 Rapid Real-Time RT-PCR Detection Kit  and isolated and identified virus isolates using methods previously described . Of 399 samples, 54 (13.5%) contained CIV H3N2 isolates. Of these 54 isolates, 43 were from Beijing, 6 from Nanjing, 3 from Shanghai, and 2 from Xi’an.\n\n【4】To characterize the evolution of CIV H3N2, we sequenced the full genome of the 54 isolates (GenBank accession nos. MK212398–829) and performed genetic analyses using available sequences of related viruses from GenBank and the GISAID database . Phylogenetic analysis of worldwide CIV H3N2 isolates indicated that each genome segment of the H3N2 isolates after 2016 formed a separate clade, distinct from other isolates from China, which grouped with isolates from South Korea and the United States . Each genome segment of the 41 H3N2 CIVs isolated after 2016 shared high nucleotide sequence identities (99.62% ± 0.09% to 99.88% ± 0.10%). Among these isolates, the time to most recent common ancestor computed by molecular clock analysis  was similar for each genome segment; all ancestors dated back to early- to mid-2016 . Therefore, the introduction of this CIV H3N2 clade into China most likely occurred in 2016 as either a single event or multiple events involving genetically similar viruses. This clade was more closely related to earlier H3N2 CIVs than the ancestral H3N2 avian influenza viruses from South Korea , and viruses of this clade could have originated from H3N2 CIVs circulating in South Korea or the United States.\n\n【5】We then investigated the molecular characteristics of these viruses. Although all the CIV H3N2 isolates from this clade still possessed 226Q and 228G (which confer specificity to cell entry receptors in birds) in hemagglutinin, they also possessed the 4 amino acid substitutions 251R and 590S in polymerase basic 2 and 146S and 242I in hemagglutinin, which have frequently been identified in human influenza viruses. Of note, 251R and 590S in polymerase basic 2 are known determinants of adaptation to growth in mammals  .\n\n【6】Antigenic analysis with ferret antiserum against representative viruses of different clades demonstrated a diversity of reaction patterns that generally corresponded with phylogenetic relationships . H3N2 CIVs isolated during 2016–2017 reacted well with antiserum against viruses of the same lineage and less well with antiserum against viruses of other lineages. Numeric analysis of these hemagglutinin inhibition (HI) titers with AntigenMap  revealed that H3N2 CIVs isolated after 2016 had a distinguishable antigenic reaction pattern .\n\n【7】The CIV H3N2–positive dogs in this study generally had only respiratory symptoms and recovered within 10 days. However, the virus spread rapidly. Among dogs in a cohort, 1 displayed mild disease (cough, runny nose, lethargy) soon after being introduced into a kennel. Within 3 days, similar symptoms were observed in 16 more dogs within that kennel. Real-time reverse transcription PCR confirmed that all 17 dogs were CIV positive. We obtained 2 CIV H3N2 isolates (A/canine/Beijing/0512-133/2017 and A/canine/Beijing/0512-137/2017) from 2 German shepherd dogs in this kennel.\n\n【8】To determine the prevalence of CIV H3N2 in dogs, we randomly performed serologic surveillance for H3N2 virus among dogs visiting the Veterinary Teaching Hospital of China Agricultural University (Beijing, China) in 2017. Of 240 serum samples, 15 (6.3%) were positive for CIV H3N2 (HI titers against A/canine/Beijing/0512-137/2017 of ≥40). To evaluate whether humans can be infected by CIV H3N2, we collected serum samples from pet owners (n = 50), veterinarians (n = 5), and animal hospital staff (n = 23) who had contact with CIV-positive dogs. Serum from 1 pet owner tested positive for CIV H3N2 (HI titer 80), revealing that this virus is a potential threat to public health.\n\n【9】### Conclusions\n\n【10】CIV H3N2 originated from avian influenza viruses in aquatic birds. We found that H3N2 viruses of a novel genetic clade and antigenicity have prevailed in dogs in some areas of China since 2016, completely replacing the previous strains; this H3N2 clade might have originated from CIVs in South Korea or the United States. However, the sparse sequence data for isolates from South Korea and the United States and the absence of CIV H3N2 sequences from these countries after 2016 prevent identification of the ancestor of this clade. Unlike the geographic clustering of isolates observed during the spread of H3N2 CIVs in the United States , the H3N2 CIVs isolated during 2016–2017 in Beijing (northern China), Shanghai and Nanjing (southeastern China), and Xi’an (western China) have high genetic identities.\n\n【11】In 2017, the percentage of dogs treated at the Veterinary Teaching Hospital of China Agricultural University that were seropositive for CIV H3N2 was 6.3%, higher than the percentage during 2012–2013 (3.5%) . The wide prevalence and increased seropositivity of H3N2 variants suggest the lineage that emerged in 2016 might possess greater infectivity in dogs than earlier viruses, which might have resulted in clade replacement. The possibility of stochastic events leading to the disappearance of the previous clade should not be excluded. Considering that preadaptation of influenza A(H1N1)pdm09 virus genes to mammalian hosts through prior circulation for several decades in swine might have contributed to the emergence of viruses containing these genes in humans, the potential adaptation of this CIV H3N2 clade to mammals and its public health threat should be further evaluated.\n\n【12】Because dog competitions and trade involving different countries are frequent and the surveillance of CIV is limited, further studies should focus on determining whether viruses of this CIV H3N2 lineage are prevalent in other countries. Global active surveillance to monitor the spread of these viruses among dogs should also be enhanced. Such efforts could prevent further CIV spread and adaptation and will be critical for identifying public health threats that could emerge at the animal–human interface.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7039f064-6728-492b-8fa6-aa730904151a", "title": "Delftia tsuruhatensis, an Emergent Opportunistic Healthcare-Associated Pathogen", "text": "【0】Delftia tsuruhatensis, an Emergent Opportunistic Healthcare-Associated Pathogen\n_Delftia tsuruhatensis,_ a member of the _Comamonadaceae_ family, was first isolated from sludge in Japan in 2003 . Mainly studied for environmental purposes , _D. tsuruhatensis_ has rarely been identified in humans  _._ We present a case report of a respiratory infection caused by _D. tsuruhatensis_ in a premature infant.\n\n【1】A female infant, born premature at 36 weeks’ gestation, had a cardiac congenital pathology for which resection of the ductus arteriosus and pacemaker placement were performed at 4 months of age. During the immediate follow-up period, she developed acute renal failure, which was treated by peritoneal dialysis. Her undernutrition status required enteral and parenteral nutrition. Laboratory tests showed slight leukocytosis, with elevated neutrophils at 9.2 G/L (reference range 1.4–8.5 G/L), monocytosis at 2.1 G/L (reference 0.2–2.0 G/L), and an elevated C-reactive protein at 15.1 mg/L (reference 0–5 mg/L). Two days after surgery, the infant developed pneumonia associated with ventilator-associated hypoxia, which prompted bronchial aspiration sampling that was sent to the clinical microbiology laboratory for analysis, which was performed as previously described  _._ Colonies grew after 24 hours’ incubation on both Polyvitex and Columbia media (bioMérieux, Craponne, France) in pure culture at 10 7  CFU/mL. We correctly identified the isolate using matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry (Microflex, Bruker, Leipzig, Germany), as previously described . We maintain a custom MALDI-TOF mass spectrometry database that is updated regularly and enabled identification of the colony as _D. tsuruhatensis_ , with an identification score of 2.082. We confirmed identification of the strain using 16S rDNA amplification coupled to sequencing, as previously reported . We obtained an amplicon of 1,296 bp and identified it as _D. tsuruhatensis_ with a similarity of 99.70% with GenBank sequence no. KC572558. Because the strain was negative for d \\-mannitol assimilation as highlighted using API NE (bioMérieux), we excluded possible misidentification with _D. lacustris._\n\n【2】We tested for antimicrobial drug susceptibility according to EUCAST 2017 recommendations  using the Etest gradient method. We categorized the strain as resistant to amoxicillin (MIC >256 mg/L) and amoxicillin/clavulanate (MIC >256 mg/L) but susceptible to ceftriaxone (MIC 0.5 mg/L), ertapenem (MIC 0.5 mg/L), imipenem (MIC 0.5 mg/L), and ofloxacin (MIC 0.047 mg/L). We administered ceftazidime to the patient for 10 days. Further collected samples were negative on culture.\n\n【3】One month later, the infant became febrile (temperature 39°C); a chest radiograph revealed pneumonia, and testing showed a still-elevated C-reactive protein (15 mg/L). A new bronchial aspiration was obtained and inoculated, as described previously. Twenty-four hours after incubation, we observed 2 isolates, each growing 10 5  CFU/mL, and analyzed them by MALDI-TOF mass spectrometry. One isolate was identified as _D. acidovorans_ (score 2.207). Faced with the discrepancy with the previous results, we performed 16S rRNA PCR coupled with sequencing, as previously reported, enabling the identification of _D. tsuruhatensis_ with a sequence identity of 99.70% to GenBank sequence no. KC572558. We identified the other isolate as _Neisseria macacae_ , with a score of 2.033.\n\n【4】We initiated therapy with imipenem, vancomycin, and amikacin before we received the microbiology results, after which we readjusted the regimen, this time administering only tobramycin aerosol. The patient’s health gradually deteriorated; she developed bradycardia and refractory hypoxia. She died at 6 months of age, 12 days after the last isolation of _D. tsuruhatensis._\n\n【5】We report isolation of _D. tsuruhatensis_ in respiratory samples from a 6-month-old infant, born at 36 weeks’ gestation. Recurrent isolations of the microorganism from the same patient, including 1 time in pure culture, exclude potential contamination. In addition, clinical signs, such as pneumonia with ventilator-associated hypoxia, support infection rather than colonization. However, the patient had recurrent pneumonia, despite a successful first therapy with ceftazidime.\n\n【6】We also looked at the number of strains of _D. tsuruhatensis_ isolated in our university hospitals in Marseille, France, during 2008–2015 and in the literature . The microorganism has been isolated 13 times from 11 patients, including the case we describe here, mainly from blood cultures (5/11 cases) and respiratory specimens (5/11), but also from 1 urine sample. Overall, the underlying conditions were observed for 10 cases, including 2 transplant recipients. No information was available for 1 patient. Considering the presence of a vascular catheter, hospital stay longer than 48 hours, or both, all reported infections were healthcare associated. In addition, of the 6 patients in whom the bacterium had been isolated from blood cultures, all 6 had an intravascular device. These data are consistent with the 2 cases of bacteremia involving _D. tsuruhatensis_ already reported in the literature for which intravascular device–related and underlying conditions were found  _._ Bacterial identification systematically failed when using phenotypic methods. Since its implementation in routine laboratory tests, MALDI-TOF mass spectrometry has correctly identified _D. tsuruhatensis_ in 4 of 8 tested isolates. For the 4 other isolates, _D. tsuruhatensis_ was misidentified as _D. acidovorans_ in 3 cases. Accurate identification was definitively performed using 16S rDNA sequencing.\n\n【7】In conclusion, _D. tsuruhatensis_ is an opportunistic emergent healthcare-associated pathogen that can be easily misidentified. Clinicians should consider this bacterium particularly in immunocompromised patients and those with intravascular devices.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3dbdab10-d393-43e3-bdfb-c48a3edecb4c", "title": "Establishing Content Validity for the Nutrition Literacy Assessment Instrument", "text": "【0】Establishing Content Validity for the Nutrition Literacy Assessment Instrument\nAbstract\n\n【1】**Introduction**  \nIdentification of low levels of health literacy is important for effective communication between providers and clients. Assessment instruments for general health literacy are inadequate for use in nutrition education encounters because they do not identify nutrition literacy. The primary objective of this 2-part study was to assess content validity for the Nutrition Literacy Assessment Instrument (NLAI).\n\n【2】**Methods**  \nThis study included a 35-item online survey of registered dietitians (134 of whom answered all questions) and a pilot study in which 5 registered dietitians used the NLAI among 26 clients during nutrition education consultations. To assess agreement with the NLAI by survey participants, we used the following scale: “necessary” (70% agreement), “adequate” (80% agreement), or “good” (90% agreement); comments were analyzed by using content analysis. For the pilot, we made comparisons between subjective assessments, the Rapid Estimate of Adult Literacy in Medicine (REALM), and the NLAI. Registered dietitians also completed a postpilot–study survey.\n\n【3】**Results**  \nFor the online survey, we found good agreement (average, 89.7%) for including each section of the NLAI. All sections accomplished their purpose (average, 81.5%). For the pilot, REALM and NLAI correlation ( _r_ \\= 0.38) was not significant; the subjective assessment of clients by dietitians and NLAI lacked agreement 44% of the time, and registered dietitians provided instruction on deficient knowledge and skills identified by the NLAI 90% of the time.\n\n【4】**Conclusion**  \nThe NLAI is a content-valid measure of nutrition literacy. Additional validation of the NLAI is important because an objective instrument is needed for identifying nutrition literacy, a construct that appears to be different from health literacy.\n\n【5】Introduction\n\n【6】Health literacy is “the degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions” . Assessment instruments  provide guidance for choosing patient health information according to individual health literacy needs.\n\n【7】Nutrition is important for disease prevention, but increasing rates of chronic diseases suggest a need for nutrition education. However, nutrition information is complex and may require high levels of cognitive skills . Most registered dietitians do not assess health literacy, perhaps because current instruments do not assess nutrition literacy. Nutrition literacy requires knowledge of nutrition principles and skill in food-related tasks .\n\n【8】The Rapid Estimate of Adult Literacy in Medicine (REALM) is a valid and reliable instrument that predicts reading ability of health care–related words . Two instruments relate to nutrition literacy assessment: the Newest Vital Sign  and the Nutrition Literacy Scale . The food label used in the Newest Vital Sign purports to measure numeracy, but this instrument does not assess nutrition knowledge. The Nutrition Literacy Scale attempts to measure comprehension of nutritional information, but it is unclear whether it provides any measure beyond print literacy, and other uses have not been described in the literature.\n\n【9】Consequently, we developed the Nutrition Literacy Assessment Instrument (NLAI). Literature review and expert interviews  were the basis for including the following domains: appreciation of relationships between nutrition and health, knowledge of macronutrients, food measurement skill, numeracy and label reading, and skill in grouping like foods.\n\n【10】The objectives of this 2-part study were to 1) establish content validity of the NLAI and identify attitudes toward needs and constructs for a nutrition literacy instrument (survey) and 2) test the usability of the instrument in nutrition education settings and determine its impact on teaching delivery from the registered dietitian’s perspective (pilot).\n\n【11】Methods\n\n【12】### Survey: critique of instrument\n\n【13】A 35-item survey was designed to assess content validity for the NLAI and attitudes among dietitians toward nutrition literacy. Content validity reflects the relevance of the assessment instrument to its targeted construct  and usually requires expert review . Content validity should be determined before construct validity, which is the accuracy with which the assessment measures the targeted construct. The survey had 7 sections; the first section addressed whether survey participants found the instrument’s algorithm useful and understandable. For the next 5 sections of the survey (nutrition and health, macronutrients, household food measurement, food labels and numeracy, and food groups), participants were asked whether the instrument accomplished its purpose, whether the questions were appropriate in difficulty, whether anything important had been left out, and whether the section was important to include. For the section on nutrition and health, a question on length replaced a question on whether anything was left out. Each question of the survey allowed participants to provide additional comment. The seventh section addressed health literacy, and because another research question was whether dietitians would prefer to use the REALM instead of the NLAI, we asked 2 questions about the REALM.\n\n【14】All methods were approved by the University of Illinois institutional review board (IRB). Two dietetic practice groups (each with approximately 6,400 members) of the Academy of Nutrition and Dietetics (formerly the American Dietetic Association) were selected as participants in the survey because their practice areas involve nutrition education. Methods were approved by the dietetic practice groups and the Academy of Nutrition and Dietetics. All data were collected in December 2011.\n\n【15】The survey recruitment invitation was distributed by an e-mail that included a link to the survey, which was designed by using online survey software (SurveyGizmo, Boulder, Colorado). Data were automatically saved as an Excel (Microsoft Corp, Redmond, Washington) spreadsheet. Of 385 potential participants, 377 (98%) consented to the survey, 178 (46%) answered some questions, and 134 (35%) completed the entire survey. Of the 243 who consented but did not finish, 211 left the survey immediately after consent; this drop-off might be explained by the temporary web-browser incompatibilities on the day the survey was released.\n\n【16】For statistical analyses, we used PASW Statistics for Windows, version 18.0 (SPSS Inc, Chicago, Illinois). To evaluate the degree of agreement with the NLAI by survey participants, we compared our data with the following scale: agreement at or above 70% is necessary, agreement at or above 80% is adequate, and at or above 90% is good . Additionally, we analyzed comments by using content analysis, which involves identifying coherent and important examples, themes, and patterns . Two researchers analyzed the comments separately to develop a list of keywords and codes and assigned comments to categories. Then each reviewed the results of the other; they agreed on assignment of comments to categories for 363 of 377 (96%) comments and discussed differences until consensus was reached. Overarching themes were developed from the codes that had the highest frequency of similar response.\n\n【17】### Pilot study\n\n【18】Five registered dietitians were recruited from personal contacts for the pilot study; because they were viewed as coinvestigators, they completed human subjects training before participation. They were recruited from 4 participating sites: a regional bariatric surgical facility, 2 metropolitan outpatient clinics, and a private practice. The pilot study was implemented in the metropolitan areas of Chicago and Urbana-Champaign, Illinois. All methods received IRB approval, and all data were collected between May 2011 and May 2012.\n\n【19】Participants in the pilot study were trained by the first author (H.G.) on health literacy so they would know how to interpret and act upon the results of the NLAI. Health care providers who receive training on health literacy have greater intentions of identifying patients with low levels of health literacy and of checking for patient understanding of information provided  and more often use methods and educational materials designed for people who have low levels of health literacy . The training included 3 online educational modules, a packet of research materials, and a telephone conversation with the researchers to clarify the research process.\n\n【20】Adult clients (aged >18) were recruited through a flyer by clinic personnel using a convenience sample approach at selected outpatient clinics in the dietitians’ normal clinic area. We targeted 10 completed surveys per participating dietitian within 1 month. If the dietitian determined the client was not suitable because of cognitive impairment or an inability to read, the patient did not participate. A prescreening instrument was required by the IRB at 1 site to ensure competency for participation.\n\n【21】After consent forms were completed, the dietitian completed a subjective assessment (a short form to indicate whether the dietitian’s initial impression, based on general observation, of the client’s nutrition literacy was adequate, marginal, or inadequate); read the instructions for the REALM and administered the interview; gave the NLAI to the client to complete, and recorded the time required to complete the NLAI. The REALM is scored by totaling the number of words (of 66) read correctly, yielding a predicted age-level reading. On the basis of the number of correct answers, each section of the NLAI is scored as “suggests high likelihood of inadequate nutrition literacy,” “suggests marginal nutrition literacy,” or “suggests adequate nutrition literacy.” The dietitian then proceeded with the scheduled nutrition education and completed a postpilot–study survey. We used nonparametric correlation tests (Spearman ρ) to evaluate the relationship between the NLAI and the REALM.\n\n【22】Results\n\n【23】### Survey: critique of instrument\n\n【24】We found that the algorithm achieved adequate agreement (81.5%) on the basis of yes answers to “Is the algorithm easy to understand and follow?” (93.1%) and “Is this section important to include?” (83.1%) and no answers to “Are there decisions that are missing?” (68.1%) . Almost one-third (29.5%) of participants believed decisions had been left out of the algorithm; the most common themes identified as being left out were “language/cognitive barriers” (14 comments), “readiness to learn” (5 comments) and “ability to purchase and prepare food” (4 comments).\n\n【25】For yes answers to the question, “Is this section important to include in the instrument?” the sections achieving adequate agreement were nutrition and health (80.9%) and macronutrients (87.1%), whereas the sections achieving good agreement were household food measurement (95.2%), food label and numeracy (94.5%), and food groups (90.7%). With an overall score of 89.7%, these 5 sections achieved good agreement for importance.\n\n【26】Combining yes answers to whether the section accomplished its purpose and whether the section was appropriate in difficulty or in length and no answers to whether anything had been left out, we found that 5 sections achieved agreement. The sections on nutrition and health (76.3%) and macronutrients (78.5%) achieved the minimum required for agreement; the sections on household food measurement (80.3%) and food label and numeracy (80.3%) achieved adequate agreement; and food groups (92.1%) achieved good agreement.\n\n【27】For the section on nutrition and health, 15 participants commented that the reading level was too high, 13 commented that the section was too long or wordy, and 9 commented that the concepts were too advanced. For the section on macronutrients, 10 participants commented that the section was too difficult or “encourages guessing.” For the section on household food measurement, 19 participants commented that visual references in pictures are needed for better size estimation; 11 noted issues with the use of the word “portions”; 8 suggested modifying the “milk image”; and 39 made suggestions for including various alternative foods.\n\n【28】The section on food label and numeracy achieved a high score for importance (94.5%) and purpose (95.2%), but it did not score as well for the questions on whether anything had been left out (only 72.5% said no) and difficulty (26.7% said section was too hard). Fifteen participants commented that 1 question, which requires computation of percentages, is too hard, and 8 commented the section overall is too difficult.\n\n【29】Most participants agreed the section on food groups is important (90.7%), achieved its purpose (98.6%), and left nothing important out (80.6%). Six participants noted a need for an “others” category, which might include foods such as added sugars or oils, and 4 noted that a “combination food” (foods that are composed of more than 1 food group, such as tacos or lasagna) should be added as food categories.\n\n【30】Most (79.9%) participants did not prefer to use the REALM instead of the NLAI to assess nutrition literacy. Whereas 92.9% of participants indicated they do not use health literacy assessment instruments, 73.1% agreed that if an instrument designed for nutrition was available, they would use it in their practice. Almost all participants (96.4%) agreed that health literacy is important, and most (80.6%) agreed that an assessment of nutrition literacy is worth the time it would require. Of the 98 participants who indicated they would use an instrument if one was available, 10 indicated yes, if time is available. Of those who said they would not used such an instrument, 5 participants indicated there is not enough time to use an instrument during nutrition education encounters, and 4 participants said they prefer an interactive approach.\n\n【31】### Pilot study\n\n【32】Twenty-five clients completed the REALM, and 26 clients completed the NLAI . Most (n = 23) scored greater than 61 on the REALM (predictive of a reading level above 9th grade), but of those, 7 clients scored at marginal nutrition literacy and 1 client scored at inadequate nutrition literacy for at least 1 area of the NLAI. The Spearman ρ correlation between the REALM and NLAI was not significant ( _r_ \\= 0.38; _P_ \\= .06). Additionally, the subjective assessment and the NLAI indicated lack of agreement at a rate of 44%. For the 10 clients who received less than adequate scores on the NLAI, the dietitian provided instruction on the deficient knowledge or skill area 90% of the time. All 5 dietitians indicated that the time needed to complete the assessment is “about right” .\n\n【33】Discussion\n\n【34】The NLAI was valid as tested within this study, and dietitians preferred the NLAI over the REALM. However, nutrition literacy is a complex concept, and responses to some questions deserve further investigation after the instrument is refined. In addition, the methods used to evaluate nutrition literacy need to reflect this complexity. “Dimensionality . the number of theoretically and empirically distinct subcomponents of the broader construct”  is important to the design of nutrition instruments because gaining an understanding of one’s nutrition literacy is dependent upon the complex interplay of knowledge of related subtopics as reflected in the NLAI.\n\n【35】Several sections of the NLAI require further investigation. For instance, the responses using the cloze method for the section on nutrition and health indicated that the section accomplished its purpose but may be too long and difficult. Although others have used simpler and shorter assessment techniques , these techniques may not capture the essence of the questions’ rationale. Diet and health is complex, and requires a higher order of integrated conceptualization than perhaps multiple response items can divulge. However, the cloze method has been used in assessing the understanding of other health-related topics, such as prostate cancer , pharmacy instructions , and cardiovascular disease risk . Therefore, while the cloze method is used in health arenas other than nutrition, dietitians may be unfamiliar with, uncomfortable with, or lack knowledge about the applicability of this method.\n\n【36】Our approach to the section on macronutrients is similar to the approach used for the 23-item Nutrition Knowledge test , which asks participants to identify the nutrient content of foods. Ideally, an instrument should stratify participants into categories of nutrition literacy, requiring questions with varying levels of difficulty. Deciphering which questions to include or exclude according to difficulty is necessary and is determined by measures of construct validity. Because the section on macronutrients has not been widely evaluated with clients, the range in difficulty of questions or appropriate foods to include cannot be determined at this time.\n\n【37】Food photographs (like the photographs used for the food portions section of the NLAI) are a useful aid for estimating food portions  and have been studied with varying degrees of success for use in reporting portion sizes in food consumption studies . It is not known, however, whether food photographs labeled by common household measurements (such as those in the NLAI) improve accuracy of estimation . Our questions did not ask participants to estimate the amount seen in photographs because the amounts were identified in the questions. Rather, participants were asked to identify whether the stated amount for a given food is the “right” portion. In our study, the photographs served as a visual cue for proportionality, but they may not be necessary if knowledge of common food measurements is strong.\n\n【38】The section on food label and numeracy was adapted from the Newest Vital Sign, a reliable and valid measure of health literacy. Because it uses a nutrition label as the text reference, it is not on its own a measure of nutrition literacy but rather a measure of the ability to read food labels. Food labeling is an area with which the public has struggled — to the point where front-of-package labeling and healthfulness scores have been suggested . Another food label literacy tool has been developed for children; it uses 10 items to determine children’s ability to make healthful choices based on the Nutrition Facts label. Each item presents 2 labels and asks which choice is the more healthful . Future developments of the NLAI could consider similar items for an adult version.\n\n【39】Conceptually, food groups have been central to nutrition education since 1916 , so we were not surprised that our survey participants indicated food groups are important to evaluating nutrition literacy. Several participants commented that combination foods should be addressed in the NLAI; however, the US Department of Agriculture’s MyPlate food guidance graphic does not incorporate combination foods, nor did previous food guides. We question whether the general public recognizes that some foods can be classified into more than 1 food group. In addition, the concept of food groups may be different for the client than for the professional. For instance, a study among low-income African American women found the names of food groups may differ between the women and the professionals, as well as foods attributed to the food groups .\n\n【40】This study had several limitations. Dietitians were unable to recruit 10 clients within 1 month at any of the participating sites. Through personal communication, the dietitians provided the following reasons for this inability: summer vacations (for dietitians and their clients), lack of interest among potential clients in spending additional time required for assessment, and lack of incentives for potential clients. Time was extended for 1 location to increase participation, but because of the extended initial IRB review process, and because multiple IRBs were involved, others did not want to engage in an amendment process to extend the time for data gathering.\n\n【41】We did not identify whether the dietitians participating in the pilot study had educational materials targeting the spectrum of health literacy levels that were available for use in educational encounters. Lack of variability in educational materials makes it difficult to derive relationship between use of the NLAI and adjustment in teaching methods made by the dietitian as a result of the assessment. Additionally, although written materials are commonly used in nutrition education and were included in data assessment, oral instruction is an important method of education as well and is not investigated here.\n\n【42】Although most participants did not prefer the REALM over the NLAI, and most indicated they would use an assessment tool if it were available, we did not specifically ask whether the participants would use the NLAI. We interpreted the positive responses to the content of the NLAI and the minimal number of suggestions for additions to mean that the NLAI would serve their needs and they would use it. Further testing of the applicability in the clinical setting is warranted.\n\n【43】Another potential limitation is that we did not gather demographic information from our pilot participants. We did not compare age, socioeconomic status, race/ethnicity, or educational attainment of our sample with the characteristics of samples used to develop other instruments.\n\n【44】The lack of agreement between the subjective assessment and the NLAI in the pilot study suggests a need for more objective measures for determining nutrition literacy. Although the Newest Vital Sign has received some attention in nutrition literature  as an instrument that can reliably and quickly assess health literacy, and its use of the Nutrition Facts label may be preferable to other health literacy instruments for nutrition practice, our survey indicates that dietitians believe nutrition literacy requires skills and knowledge beyond an ability to read food labels. Additionally, testing of the Newest Vital Sign in an elderly African American population indicates its practicality may be limited .\n\n【45】Registered dietitians participating in our online survey found the NLAI to be content valid as a measure of nutrition literacy. Although participants gave numerous suggestions for instrument improvement, the reader should exercise caution in overemphasizing any suggestion because most participants agreed with the approach and methodologies of the NLAI. Future research should evaluate use of the NLAI in clinical settings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "99eec2e4-5547-43da-8f40-20818026ea92", "title": "Type 2 Diabetes Among Filipino American Adults in the Multiethnic Cohort", "text": "【0】Type 2 Diabetes Among Filipino American Adults in the Multiethnic Cohort\nAbstract\n\n【1】**Introduction**\n\n【2】Several Asian racial and ethnic groups, including individuals of Filipino ancestry, are at higher risk of developing type 2 diabetes than White individuals, despite their lower body mass index (BMI). This study examined determinants of type 2 diabetes among Filipino American adults in the Multiethnic Cohort Study.\n\n【3】**Methods**\n\n【4】Participants in Hawaii and Los Angeles completed questionnaires on demographics, diet, and anthropometrics. Generational status was determined according to birthplace of participants and their parents. Based on self-reported data and data on medications, type 2 diabetes status was classified as no, prevalent, or incident. We used polytomous logistic regression, while adjusting for confounders, to obtain odds ratios.\n\n【5】**Results**\n\n【6】Among 10,681 Multiethnic Cohort Study participants reporting any Filipino ancestry, 57% were 1st-, 17% were 2nd-, and 25% were 3rd-generation Filipino Americans. Overall, 13% and 17% of participants had a prevalent or incident type 2 diabetes diagnosis. Overweight and obesity and the presence of other risk factors increased from the 1st to subsequent generations. First-generation immigrants were less likely to report type 2 diabetes at cohort entry than immigrants of subsequent generations who were born in the US or whose parents were born in the US; only the prevalence of type 2 diabetes was significantly elevated in the 2nd generation compared with the 1st generation.\n\n【7】**Conclusion**\n\n【8】The results support the hypothesis that Filipino migrants adopt lifestyle factors of the host country and subsequent generations experience higher type 2 diabetes rates due to changes in risk factor patterns.\n\n【9】Introduction\n\n【10】According to the National Diabetes Statistics Report , more than 34 million people in the United States have been diagnosed with type 2 diabetes. Risk factors for type 2 diabetes include excess body weight and physical inactivity . People from many ethnic groups (eg, Asian, Pacific Islander, Latino, African American) are at higher risk of developing type 2 diabetes than White people . These include individuals of Filipino ancestry, despite having a lower body mass index (BMI) than other ethnic groups . In the Multiethnic Cohort (MEC) Study, an epidemiologic study of chronic disease risk among more than 200,000 residents of Hawaii and Los Angeles, Filipino American adults had a mean body mass index (BMI) of 23.9 kg/m 2  , compared with 24.6 kg/m 2  and 27.7 kg/m 2  in White and Native Hawaiian adults, respectively . According to 2017–2018 National Health and Nutrition Examination Survey data, Filipino American adults had the second highest type 2 diabetes prevalence (10.4%) among Asian Americans, following Asian Indians at 12.6%; the rate of type 2 diabetes for Chinese adults was 5.6% . A California report showed incidence rates of 14.7 cases per 1,000 person-years for Filipino American adults compared with 7.5 for Japanese adults and 6.5 for Chinese adults; only South Asian adults (17.6), Pacific Islander adults (19.9), and Korean adults (20.3) had higher rates . In the MEC, the risk of developing incident type 2 diabetes was 2.5-fold for Filipino American adults compared with White adults and was higher than for all other Asian Pacific Islander adults . Moreover, nonobese Filipino American adults were twice as likely to develop type 2 diabetes as nonobese non-Hispanic White adults in a population-based study . A geographic comparison showed a greater type 2 diabetes prevalence in San Diego (14.1%) and Hawaii (14.7%) than in the Philippines (11.8%) , just as higher rates were reported for multiple American Asian groups  in comparison to those in their native country .\n\n【11】A high prevalence of risk factors for chronic diseases, including obesity, smoking, and binge drinking, has also been reported for Filipino American people . In terms of diet, rice, fruits, vegetables, and fish or meat were reported as typical parts of daily meals , and intake of vegetables, fruits, plants, whole grains, and fiber was inversely associated with type 2 diabetes . With acculturation, intake of total energy and percentage of calories from fat increased, similar to increases in BMI . Although Filipino American people constitute 15.1% of Hawaii’s population and 0.9% of the US population as of 2019 , they are understudied because they are commonly aggregated under the racial and ethnic groups of Asian or Pacific Islander .\n\n【12】Migrant studies comparing risk factors and disease incidence across generations have made important contributions to the understanding of disease etiology, as generational status, shift in risk factors, prevalence, and age at migration have a role in understanding how changes in exposure affect disease risk . Selection bias — sometimes called the “healthy immigrant hypothesis,” which states that healthier individuals are more likely to migrate than those in poor health — may underlie the often-seen better health outcomes for 1st-generation immigrants . In the new country, however, immigrants and their descendants typically experience changes in risk factors . As shown for cancer incidence among Japanese  and Filipino  migrants, risks align more closely to the levels of the host country, often after adoption of behaviors prevalent in the new country. To explore possible reasons for the high type 2 diabetes incidence among Filipino American people despite their relatively low obesity rates, we compared risk factors among 1st generation (born in the Philippines), 2nd generation (born in the US to both parents born in the Philippines), and 3rd generation (born in the US to at least 1 US-born parent) Filipino American adults in the MEC.\n\n【13】Methods\n\n【14】### Study population\n\n【15】From 1993 to 1996, more than 215,000 men and women aged 45 to 75 years from Hawaii and Los Angeles participated in the MEC . The goal of this prospective cohort is to examine diet, lifestyle, and genetic risk factors in relation to cancer and other chronic diseases among individuals of White, African American, Native Hawaiian, Japanese American, and Latino ancestry. Driver’s license files were chosen as the primary sampling frame for recruitment and were supplemented with Medicare files . Enrollment targeted the 5 selected ethnic groups using ethnic identifiers, in particular surnames and selected zip codes. As actual ethnicity was not known at the time of the mailing, many persons with ancestries outside the 5 selected groups received and responded to questionnaires.\n\n【16】At cohort entry , participants were able to self-report as many ancestries as applicable. Individuals reporting several of mixed ancestry were assigned to 1 of the 5 groups, according to the following priority ranking: African American, Native Hawaiian, Latino, Japanese American, White, and Other. Approximately two-thirds of the Other category were Filipino adults; our final study population included these adults as well as individuals in 1 of the 5 major groups who also reported Filipino ancestry. Participants also reported place of birth for themselves and their parents, which was recoded as the US, the Philippines, and Other. Migration status was assigned according to birthplace of the participants and their parents as 1st, 2nd, or 3rd generation.\n\n【17】### Data collection\n\n【18】A 26-page, self-administered, mailed questionnaire was completed by all participants in English (except Latino participants in California, who were given the choice of an English or Spanish questionnaire); the questionnaire collected information on demographic characteristics, anthropometric characteristics, medical history, and dietary intake. Subsequent questionnaires were mailed approximately every 5 years (Q2 in 1998–2002, Q3 in 2003–2008, Q4 in 2008–2012, and Q5 in 2012–2016) to surviving cohort members and collected updated information on bodyweight and medical history.  All cohort members are followed passively until death through vital records and cancer registries. At baseline, data on years of education (≤12, 13–15, ≥16 y) and smoking status (never, past, current) were collected. Mean time spent in sleep, sedentary, moderate, and vigorous activities on a typical day as reported in the baseline questionnaire were used to compute summary variables for moderate to vigorous activity (<0.5 and ≥0.5 h/d) and sleep duration (<7, 7–8, and >8 h) . BMI was computed from self-reported height and weight and categorized into underweight (<18.5 kg/m 2  ), normal weight (18.5 to <25.0 kg/m 2  ), overweight (25.0 to <30.0 kg/m 2  ), and obese (≥30.0 kg/m 2  ).\n\n【19】The self-administered survey at cohort entry included a quantitative food frequency questionnaire (QFFQ) with more than 180 food items . The unique attributes of the QFFQ included ethnic-specific foods, reliance on a food composition table specific to the MEC, and use of a large recipe database. Although all foods central to a Filipino diet were not listed in the QFFQ, consumption for some (eg, chicken adobo under “roasted, baked, grilled, or stewed chicken”) by other ethnic groups was common enough to include them. Food mixtures were disaggregated into their components, and each ingredient was assigned to the relevant food item. Individual food items and foods from mixed dishes were also classified into food groups based on the My Pyramid Equivalents Database to compute the Healthy Eating Index (HEI)-2010, an indicator of diet quality reflecting the 2010 Dietary Guidelines for Americans, with higher scores indicating better adherence . It consists of 12 components, 9 focused on adequacy (total vegetables, greens and beans, total fruit, whole fruit, whole grains, dairy, total protein foods, seafood and plant protein, fatty acid ratio) and 3 focused on moderation (refined grains, salt intake, empty calories \\[ie, energy from solid fat, added sugars, and alcohol\\]) . Each component is calculated on a density basis per 1,000 kcal. The continuous HEI-2010 scores  that range from 0 to 100 (100 equals highest diet quality) were split into tertiles, with tertile 3 indicating the highest diet quality. From the QFFQ, alcohol intake was categorized as less than 1 drink per month, 1 or more drinks per month to less than 1 drink per day, and 1 or more drinks per day.\n\n【20】Information on type 2 diabetes was obtained by self-report in questionnaires at cohort entry and at 4 follow-ups (Q2–Q5). Each questionnaire asked, “Has your doctor ever told you that you had diabetes?” Information on medication for diabetes was collected starting at Q2 and on the biospecimen questionnaire administered as part of a biorepository among 68,740 cohort members established in 2001–2006. To maximize consistency across the study population, we used a self-reported diagnosis or type 2 diabetes medication in at least 1 questionnaire as criteria to define cases, as high validity of self-reports for type 2 diabetes has been demonstrated . Participants who reported type 2 diabetes at cohort entry were classified as “prevalent” cases, those who self-reported type 2 diabetes or medication in a follow-up questionnaire as “incident” cases, and all others were classified as “no” type 2 diabetes.\n\n【21】### Statistical analysis\n\n【22】We used SAS version 9.4 (SAS Institute Inc) to analyze all data. After exclusion of cohort members without Filipino ancestry and those with invalid dietary and type 2 diabetes information, the final data set included 10,681 cohort participants. Descriptive analyses were applied to show population characteristics by type 2 diabetes status. We applied multinomial logistic regression using the total study population to compute odds ratios (ORs) and prevalence ORs (PORs) to model more than 2 discrete outcomes . To assess the influence of acculturation, migration status was modeled for the 2nd and 3rd versus the 1st generation as the reference group in relation to risk factors. Similarly, we examined the association of type 2 diabetes status (prevalent and incident vs no type 2 diabetes as the reference group) with relevant risk factors (age at cohort entry, sex, BMI, education, smoking status, alcohol intake, physical activity, sleep duration, and HEI-2010 tertiles) that were associated with type 2 diabetes in previous analyses . Area (Hawaii vs Los Angeles) was not included because the variable did not substantially influence the results. Missing values for covariates were coded as a separate category and included into the statistical models. In addition, the 12 individual HEI-2010 components were examined individually using the same approach both for migrant and type 2 diabetes status.\n\n【23】Results\n\n【24】Among the 10,681 participants with Filipino ancestry, 70% did not have type 2 diabetes, 13% reported prevalent type 2 diabetes, and 17% were classified as incident type 2 diabetes . The percentage of any type 2 diabetes diagnosis was similar in Hawaii and Los Angeles (30% vs 27%). At cohort entry, the mean age of all 3 groups was 57.8 (SD, 8.5) years with no diabetes, 59.7 (SD, 7.9) years with prevalent diabetes, and 55.8 (SD, 7.3) years with incident diabetes. Participants with prevalent type 2 diabetes were diagnosed at a mean age of 60.2 (SD, 8.0) years, and those with incident type 2 diabetes at 67.5 (SD, 7.7) years.\n\n【25】Of all participants, 95% reported a father with Filipino ancestry, and 83% reported a mother with Filipino ancestry . Fifty-seven percent of participants were born in the Philippines (1st generation), and most others were born in the US (42%). Of the participants’ parents, 73% were both born in the Philippines, 20% of the participants had 1 parent who was born in the Philippines, and 6% were both born in the US. As a result, 17% of participants were classified as 2nd generation and 25% as 3rd generation. The respective proportions of 1st generation participants were lower in Los Angeles than Hawaii (8% vs 58%). Of 1st-generation immigrants, 74% did not have type 2 diabetes, 11% reported prevalent type 2 diabetes, and 15% reported incident type 2 diabetes. Among 2nd- and 3rd-generation Filipino American adults, only 66% had no type 2 diabetes diagnosis and in both groups the proportions of prevalent versus incident cases were similar. Participants with prevalent (27.2 \\[SD, 5.2\\] kg/m 2  ) and incident (27.1 \\[SD, 4.5\\] kg/m 2  ) type 2 diabetes had a higher BMI at baseline than those with no type 2 diabetes (24.8 \\[SD, 4.1\\] kg/m 2  ). The prevalence of overweight and obesity increased from 1st to the 3rd generation (36%, 58%, and 68%).\n\n【26】### Lifestyle factors by generational status\n\n【27】Substantial differences were found in demographics and lifestyle factors among participants by generational status . Compared with the 1st generation, 2nd- and 3rd-generation Filipino American adults were less likely to complete 16 or more years of education and were also 2 to 3 times more likely to be past or current smokers, to consume alcohol, and to report 0.5 or more hours per day of moderate to vigorous physical activity. Compared with 1st-generation Filipino American adults, 2nd- and 3rd-generation Filipino American adults were 2 to 3 more times likely to be classified as overweight (PORs of 2.08 and 2.88, respectively) and 5 to 11 times more likely to be classified as obese (PORs of 5.19 and 11.2, respectively). They were also more likely to report high diet quality as indicated by being in the 3rd HEI-2010 tertile, with higher scores for whole grains, dairy, total protein foods, fatty acid ratio, sodium, and refined grains and lower scores on total and whole fruits as well as empty calories (data not shown). In addition, the 2nd generation scored higher on greens and beans and the 3rd generation scored lower on seafood and plant protein, while total vegetable intake was similar for both generations.\n\n【28】### Predictors of type 2 diabetes status\n\n【29】After full adjustment, only 2nd-generation Filipino American adults were more likely to report prevalent type 2 diabetes at cohort entry (POR = 1.23; 95% CI, 1.04–1.46) than the 1st generation; migration status was not significantly related to incident type 2 diabetes . Among lifestyle factors, underweight participants were less likely to report prevalent (POR = 0.52; 95% CI, 0.29–0.95) and incident type 2 diabetes (OR = 0.39; 95% CI, 0.21–0.74). In contrast, overweight was associated with higher rates of prevalent (POR = 1.63; 95% CI, 1.42–1.87) and incident (OR = 2.24; 95% CI, 1.99–2.53) type 2 diabetes as compared with normal-weight participants. Obese participants were nearly 4 times as likely to have prevalent (POR = 3.82; 95% CI, 3.18–4.58) and incident type 2 diabetes (OR = 3.71; 95% CI, 3.13–4.38). Past smokers reported significantly more and those reporting alcohol consumption reported significantly less prevalent type 2 diabetes, and those who had 1 or more drinks per day were also less likely to be diagnosed with incident type 2 diabetes. Getting less than 7 hours of sleep (POR = 1.17; 95% CI, 1.03–1.33) and more than 8 hours of sleep (POR = 1.29; 95% CI, 1.02–1.64) was significantly associated with prevalent type 2 diabetes. Participants in HEI-2010 tertiles 2 (POR = 1.38; 95% CI, 1.20–1.58) and 3 (POR = 1.71; 95% CI, 1.46–2.00) had higher rates of prevalent type 2 diabetes than those in tertile 1; odds of incident type 2 diabetes with respect to HEI-2010 tertile were not significant. For the HEI-2010 components, participants who reported prevalent type 2 diabetes scored higher on total vegetables, greens and beans, whole fruits, whole grains, dairy, total protein foods, fatty acid ratio, and empty calories and lower on sodium. The only component to be associated with incident type 2 diabetes was a higher fatty acid ratio score (data not shown).\n\n【30】Discussion\n\n【31】First-generation immigrants were less likely to report type 2 diabetes at cohort entry than subsequent generations with self or parents born in the US. Rates of overweight and obesity increased across generations and were the strongest predictor of type 2 diabetes incidence. Participants with prevalent type 2 diabetes had higher scores for most HEI-2010 components (vegetables, greens and beans, protein food, fatty acid, whole grains, dairy, and empty calories) and lower scores for sodium, suggesting that participants with type 2 diabetes possibly adjusted their dietary intake according to recommendations for type 2 diabetes management. The lack of an association between diet quality and incident type 2 diabetes contradicts the MEC report of lower type 2 diabetes incidence associated with high diet quality reported among the entire population  but may be due to the relatively small number of cases per group.\n\n【32】The higher rates of type 2 diabetes in the 2nd and 3rd generations than in the 1st may be due to the healthy immigrant hypothesis , the younger age at cohort entry, or lack of screening and diagnosis among newly arrived immigrants. Results from a longitudinal study supporting the healthy immigrant hypothesis found that Filipino migrants self-reported significantly better health, less depression, larger hip circumference, and lower waist-to-hip ratio than residents in the Philippines . Our results align with the hypothesis that increasing BMI and other lifestyle behaviors may be responsible for the rising type 2 diabetes rates and agrees with findings among women with Filipino ancestry , young adults in the Philippines , Japanese migrants to the US , and a review of worldwide migrant populations . Once Filipino immigrants acculturate to the US, they may start to consume a more American diet and eat fewer Filipino foods , in addition to adopting other disease risk factors of the host country, as is often described in migration studies . Indeed, our results indicate that 2nd- and 3rd-generation Filipino American adults were more likely to smoke, consume alcohol, be overweight or obese, and score lower on fruit intake than 1st-generation individuals who had left their home country.\n\n【33】Although only 13% of the participants self-reported type 2 diabetes at cohort entry, 17% developed type 2 diabetes later in life. This finding indicates that, despite the ability of 1st-generation immigrants to migrate from their home country because they are in better health, they or their offspring may still develop adverse health outcomes later in life. We also found that 2nd- and 3rd-generation immigrants were less likely to have a college education than 1st-generation immigrants, even after controlling for other risk factors. Lower educational attainment contributes to increased risk of chronic diseases, including diabetes, and poorer health outcomes  and may be a factor at the population level in the development of type 2 diabetes among acculturated US-born Filipino American adults. The high rates of type 2 diabetes among Filipino American adults in this subset of the MEC are similar to those described in previous research, which found that Filipino American adults had rates second only to those of Native Hawaiian adults and Pacific Islander adults in Hawaii  and rates that were higher among Filipino adults in the US than in the Philippines .\n\n【34】Our study has several strengths. We had access to information about birthplace and ethnicity of parents among a large study sample of Filipino Americans living in 2 US states. Therefore, it was possible to compare a substantial number of individuals born in the US with those born in the Philippines. The long-term follow-up of more than 20 years, the repeated questionnaires asking for type 2 diabetes status and medication, and the extensive and validated diet questionnaire by QFFQ  were additional unique features of the analysis. At the same time, our study also had limitations. Our data were based on self-reports; however, high validity with 84% to 97% specificity, 55% to 80% sensitivity, and more than 92% reliability over time for self-reports of prevalent and incident type 2 diabetes have been demonstrated by other large cohorts  and considered sufficiently accurate to allow use in epidemiologic studies . Nevertheless, undiagnosed cases due to lack of health care access, possibly more common in the 1st generation, may have been missed. Another limitation was the lack of glucose or HbA 1c  measures or medical records available for this population. Because the QFFQ was not designed for Filipino individuals, some foods in a typical Filipino diet were not included and may have led to underreporting of total energy intake. This limitation may particularly apply to the 1st and 2nd generation of Filipino American adults, who are more likely to consume a traditional Filipino diet . Not all 1st-generation participants may have been fully fluent in English and may have had difficulties in completing the QFFQ. Alternatively, the sample of 1st-generation Filipino American adults in the MEC may not have been representative of this group since those without English skills may not have responded to the survey. Because the physical activity measure was based on recreational and not occupational activities, which are often high among recent immigrants, measurement bias may have occurred. Family history of type 2 diabetes may have influenced the results, but these data were not collected in the MEC. Finally, 24% of participants in this study reported mixed ethnicity and may have adopted food preferences and behaviors that are different from those of the Filipino community.\n\n【35】In conclusion, our results from a prospective cohort suggest that descendants of Filipino immigrants to Hawaii and California have adopted lifestyle factors of the host country, increased their body weight across generations, and developed higher type 2 diabetes rates as a result of these changes in risk factor patterns. Although the prevalence of overweight and obesity increased from the 1st to the 3rd generation, rates of type 2 diabetes were significantly higher only in the 2nd generation compared with the 1st. Therefore, type 2 diabetes incidence may stabilize in the 3rd generation as a result of more acculturation, a finding also reported in a Japanese American cohort . Our findings indicate that culturally appropriate interventions are needed to reduce lifestyle factors that result in high rates of type 2 diabetes among Filipino immigrants and their descendants.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "17607533-2f6e-44dd-aa6d-242d60dbfc2a", "title": "Candida dubliniensis Candidemia in Australia", "text": "【0】Candida dubliniensis Candidemia in Australia\n**Letter to the Editor:** We read with interest the recent publications of Meis et al. and Brandt et al. describing the first reported cases of _Candida dubliniensis_ fungemia from Europe and North America, respectively. To contribute to the growing recognition of the pathogenic role of this organism, we report the first case of _Candida dubliniensis_ fungemia from Australia.\n\n【1】A 68-year-old Caucasian woman with a long history of alcohol abuse visited the emergency department with a 5-week history of progressive weakness. On physical examination she was afebrile, cachectic, and confused. Poor oral hygiene with gingivitis and tooth decay was noted, but no oral candidiasis was seen. Stigmata of chronic liver disease, including palmar erythema and spider nevi, were present. Neurologic examination showed generalized muscle weakness, mild cerebellar ataxia, and peripheral neuropathy.\n\n【2】Abnormal laboratory tests included hemoglobin 8.7 g/dL with macrocytosis, neutrophils 0.2 x 10 9  /L, platelets 98 x 10 9  /L, alkaline phosphatase 229 U/L, gamma glutamyltransferase 128 U/L, calcium 1.62 mmol/L, and albumin 2.4 g/dL. Coagulopathy was identified, with a prothrombin time of 23 seconds and an automated partial thromboplastin time of 44 seconds. The HIV 1 and 2 antibody test was negative.\n\n【3】The patient was admitted to the hospital, where she was rehydrated through a peripheral venous cannula, which was removed on day 5. Ticarcillin-clavulanic acid and gentamicin were administered for 4 days until her neutrophil count increased to >1.0 x 10 9  /L. The cause of the transient neutropenia was not identified. On day 7, the patient was increasingly unwell, with confusion, postural hypotension, and a temperature of 38°C. Yeasts were isolated from a blood culture taken on day 9. Oral and vaginal cultures collected on day 10 did not grow yeast. Treatment with intravenous fluconazole (400 mg/day) was begun. The patient's fever resolved within 48 hours, and her clinical condition improved gradually. Fluconazole was ended on day 37. There was no evidence of metastatic candidemia.\n\n【4】Positive cultures were detected at 31 hours by the BacTAlert (Organon Teknika Corp. Durham, NC). blood culture system, and yeast were present on Gram stain. Subculture on ChromAgar (ChromAgar Candida, Paris, France) grew apple-green colonies, which were germ tube positive. The Analytical Profile Index 20C profile at 48 and 72 hours was 6 1 5 2 0 1 4 ( _C. dubliniensis_ \\- 99.9% certainty) and the API 32C profile was 7 1 4 2 1 4 0 0 1 5, which matches the reported profile for _C. dubliniensis_ . The isolate grew well at 35°C but did not grow at 42°C. Identification was confirmed by Professor T. Patterson, University of Texas (San Antonio) by Ca3 probe, contour clamped homogeneous electric field (CHEF) karyotype, and CHEF pulsed-field gel electrophoresis. The fluconazole MIC was <0.125 mg/mL by the National Committee for Clinical Laboratory Standards broth microdilution method .\n\n【5】The initial description by Meis et al. of _C. dubliniensis_ fungemia included three patients with chemotherapy-induced immunosuppression and hematologic malignancy. The subsequent publication of Brandt et al. broadens the spectrum of susceptible patients, with two of four patients having end-stage liver disease and one with HIV infection, although the CD4+ count was in the normal range. Our patient also had advanced liver disease but had few other recognizable risk factors for candidemia, with transient neutropenia and a short course of broad-spectrum antimicrobial therapy the only apparent predisposing factors.\n\n【6】Identification of _C. dubliniensis_ as the causative pathogen in cases of candida fungemia is important, as concerns have been expressed that resistance may develop rapidly in oral isolates of _C. dubliniensis_ in HIV-infected patients treated with fluconazole . However, all eight bloodstream isolates reported to date have been susceptible to fluconazole, and treatment with this agent was initiated in every patient. Our patient had a rapid and complete clinical response to fluconazole therapy. Fluconazole appears to be appropriate empiric therapy in patients with no prior azole exposure. However, susceptibility testing should be performed on each isolate to confirm azole sensitivity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "211767dc-0187-41ab-9064-4b1595dbea6b", "title": "Global Spread of Norovirus GII.17 Kawasaki 308, 2014–2016", "text": "【0】Global Spread of Norovirus GII.17 Kawasaki 308, 2014–2016\nNorovirus infections are a leading cause of acute gastroenteritis worldwide in persons of all age groups. Despite the broad genetic diversity, norovirus GII.4 has predominated during the past 20 years . During winter 2014–15, a new norovirus GII genotype 17 variant, known as Kawasaki 308–like 2014 (GII.17 Kawasaki), emerged and became the predominant genotype in Hong Kong, China , several major cities of mainland China , and Japan . This variant also was detected sporadically outside of Asia in countries such as Italy, Romania, and the United States . This new GII.17 Kawasaki variant is distinct from other GII.17 strains, including the co-circulating Kawasaki 323–like strains; it has 2 characteristic amino acid insertions in the most surface-exposed antigenic region of the major capsid viral protein 1 (VP1) . These changes have the potential to alter the antigenic properties or the virus–host cell binding preference, raising concern about the global spread of this variant and its replacement of GII.4 variants . To study the phylodynamic transmission pattern of norovirus GII.17 Kawasaki, we analyzed full-length VP1 nucleotide sequences collected worldwide during late 2014 through early 2016.\n\n【1】### The Study\n\n【2】We chose the region VP1 to analyze because it contained the most hypervariable protruding domain 2 across the norovirus genome and represented most sequences deposited in the public domain. The entire dataset comprised 254 complete VP1 sequences from 13 countries, and all were obtained from samples collected during September 2014–March 2016 . Among them, 129 sequences from 10 countries were determined for this study , and the remaining 125 sequences were retrieved from GenBank. These sequences were collected from diverse settings, including outbreaks in healthcare facilities and food-serving sites, sporadic community cases, and hospitalized patients .\n\n【3】GII.17 Kawasaki viruses were found in 13 countries across 4 continents: Canada, China, Germany, Hungary, Italy, Japan, the Netherlands, New Zealand, Russia, Slovenia, South Korea, Thailand, and the United States. Australia and South Africa reported no GII.17 Kawasaki as of mid-2015 and early 2016, respectively. Maximum-likelihood phylogenetic inference showed different genetic clusters within GII.17 Kawasaki, indicating rapid genetic diversification of viral population during spread . Sequences from the same continent scattered into different genetic clusters, inferring multiple introduction and frequent transmission events. To investigate the virus transmission pattern, we constructed a median-joining haplotype network based on complete VP1 nucleotide sequences . Overall, the 254 VP1 sequences comprised 207 different haplotypes . We identified a highly connected basal haplotype  that consisted of 8 identical VP1 sequences collected in the initial phase of the epidemic during November 2014–March 2015 from 6 cities mostly in Asia (2 from Hong Kong; 1 from Shanghai, China; 1 from Guangzhou, China; 1 from Taiwan; 2 from South Korea; and 1 from Russia). The same basal haplotype was concluded using integer neighbor-joining and tight span walker network models. The central node might represent a competent virus haplotype capable of replicating and spreading efficiently among humans and from which nearly all haplotypes originated. We found only 2 nucleotide differences without amino acid change between the basal haplotype and the first case-patient with GII.17 Kawasaki virus in this study (NS-405; collected in September 2014 from Hong Kong) . We determined complete genomes that comprised the basal haplotype for this study for the 2 Hong Kong strains and downloaded data for the 2 South Korea strains. These viruses had 4 unique amino acid substitutions distinct from NS-405: 2 in the nonstructural polyprotein (A187D in N terminal protein and N739S in protease) and 2 in VP2 (K58R and A89S; outside of the VP1-interacting domain) . Substitution in the protease might mediate changes in the cleavage efficiency of the polyprotein in norovirus replication . Although we noted no substitutions in the RNA-dependent RNA polymerase, N terminal protein and VP2 were previously implicated in modulating polymerase activity, virus tropism, and persistence . The 4 non-VP1 residues may affect viral fitness of the emergent GII.17 Kawasaki in humans; however, functional characterization is required .\n\n【4】We identified 3 important sublineages by topology . Viruses belonging to sublineage SL1  clustered closest to the first GII.17 Kawasaki isolate in this study. SL1 included strains from 6 countries outside of China across 3 continents: Thailand (collected in October 2014), United States (November 2014), Italy and the Netherlands (February 2015), Slovenia (August 2015), and Canada (December 2015–January 2016). The global spread of GII.17 Kawasaki viruses within a few months after the initial emergence in China in late 2014 highlights rapid transmissibility of these viruses. Despite the molecular evidence of early global presence of SL1, the apparent limited circulation of this sublineage is intriguing. SL1 was the only sublineage not originating from the basal haplotype but directly from the earliest NS-405. Sequence analysis of the other 2 SL1 complete genomes available, collected from the United States  and Taiwan , found none of the 4 non-VP1 substitutions observed in the basal haplotype. In this dataset, viruses belonging to sublineage SL2 had the most cases and widest geographic breadth . SL2 was detected in 6 countries outside of China across 3 continents (Germany, Japan, New Zealand, Russia, Slovenia, and Thailand) and most of the non-China sequences from 2014–15 (36%) and 2015–16 (41%) seasons belonged to this sublineage . The most successful SL2 might have an advantage to global spread, although we cannot rule out sampling bias. During the 2015–16 season, SL2 continued to circulate over a wide geographic area, although none of the sequences from China belonged to this sublineage. Instead, sublineage SL3, first detected in January 2015 as a minority (7%) in China in the 2014–15 season, became the predominant (63%) circulating GII.17 Kawasaki virus in both southern (Hong Kong) and eastern (Shanghai) parts of China during 2015–16 among sequences analyzed . No sequences from other countries clustered into SL3. This emerging sublineage highlights that GII.17 Kawasaki viruses were still circulating and, more important, rapidly evolving in various regions of China. Robustness of sublineage topology was confirmed in the phylogenetic tree .\n\n【5】### Conclusions\n\n【6】We determined the complete VP1 sequences of 129 GII.17 Kawasaki strains from 10 countries. Our analyses suggest that the new GII.17 Kawasaki originated from a single haplotype from which rapid genetic diversification into multiple sublineages occurred during global spread after the initial emergence in China in late 2014. Norovirus diversification into sublineages provides a preepidemic virus pool from which new pandemic GII.4 variants emerged . Although our study is limited by its focus on VP1 sequence analysis and not on virus genomes, it nevertheless is a good demonstration that a global network of norovirus laboratories sharing virus sequence information can delineate virus transmission pattern upon spread.\n\n【7】Dr. Martin C.W. Chan is an assistant professor in the Department of Microbiology of the Chinese University of Hong Kong. His research interest focuses on molecular epidemiology and pathogenesis of human noroviruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6f897757-1db7-4781-8366-2a8aecfef341", "title": "Patterns of Screen Time Among Rural Mexican-American Children on the New Mexico-Mexico Border", "text": "【0】Patterns of Screen Time Among Rural Mexican-American Children on the New Mexico-Mexico Border\nAbstract\n\n【1】**Introduction**\n\n【2】The prevalence of obesity is 26% among Hispanic children and teenagers and 47% among Hispanic adults. One contributor to obesity is sedentary behavior, such as using electronic screen devices (ie, screens). Low-income and Hispanic youths spend more time using such devices than other youths.\n\n【3】**Methods**\n\n【4】We interviewed 202 parents of Mexican-origin children aged 6 to 10 years in 2 rural communities near the US–Mexico border to determine screen use among children. We tested for associations between covariates and heavy screen use (≥4 hours/day) and calculated adjusted odds ratios (AORs) to identify independent, modifiable risk factors for such use.\n\n【5】**Results**\n\n【6】More than two-thirds (68.3%) of households had an annual income of less than $24,000, 89.1% spoke primarily Spanish, and 92.1% had internet access. The percentage of children with heavy screen use was 14.9% on weekdays and 25.2% on weekends. Smartphones were used by 62.4% of children, desktops or laptops by 60.9%; homework was the most common reason for use of these devices. One in 3 children used them for social media. Increased odds of heavy screen use were associated with having a television on while the child ate (weekday AOR = 3.02; 95% confidence interval \\[CI\\], 1.08–8.45 and weekend AOR = 2.38; 95% CI, 1.04–5.40) and using electronics to entertain (weekend AOR = 2.94; 95% CI, 1.15–7.51). More than 3 family meals per week (AOR = 0.40; 95% CI, 0.17–0.94 compared with ≤3 meals) and 2 or 3 family activities per week (AOR = 0.33; 95% CI, 0.12–0.87 compared with ≤1 activity) were associated with decreased odds of heavy weekend use.\n\n【7】**Conclusion**\n\n【8】Even in low-income, Spanish-speaking communities, children have access to electronic devices, social media, and the internet, and a substantial fraction of them are heavy users. Efforts to reduce screen time might focus on understanding and changing the social norms that promote it.\n\n【9】Introduction\n\n【10】In 2015–2016, Hispanic adults had a higher age-adjusted rate of obesity (47.0%) than non-Hispanic white (37.9%) or non-Hispanic black (46.8%) adults in the United States . Moreover, the prevalence of obesity in 2015–2016 among children and teenagers aged 2 to 19 years was 25.8% among Hispanics, 22.0% among non-Hispanic blacks, and 14.2% among non-Hispanic whites . The 2015 Youth Risk Behavior Survey showed that 16.2% of Hispanic and 13.3% of non-Hispanic white ninth-graders in New Mexico were obese, while an additional 16.8% of Hispanic and 15.5% of non-Hispanic white ninth-graders were overweight. Obesity rates in both populations are increasing .\n\n【11】Behaviors that contribute to obesity among children and teenagers include sedentary behavior and the consumption of excessive calories . Sedentary behavior is defined as any waking behavior that has a low level of energy expenditure (<1.5 metabolic equivalents) while in a sitting, reclining, or lying posture . The component of such behavior that is studied most often is screen time. Screen time is time spent on screen-based behaviors , such as watching television, playing video games, and using computers, smartphones, or other electronic devices with screens. Use of devices with screens other than televisions has increased dramatically in the United States in recent years .\n\n【12】Perhaps one reason the rate of obesity is higher among Hispanic children and teenagers than among their non-Hispanic counterparts is that the former spend more time using electronic screen devices . For example, in the 2015 New Mexico Youth Risk Behavior Survey, 27.3% of Hispanic and 21.7% of non-Hispanic white ninth-graders spent 3 hours or more watching television each weekday . And in the 2015 New Mexico Youth Risk and Resilience Survey in the largely Hispanic county of Otero, in the US–Mexico border region, 27.7% of middle-school students (sixth- to eighth-graders) watched 3 hours or more of television, and 28.5% used computers or video games for 3 hours or more on weekdays . Consistently, low-income and racial/ethnic minority children and teenagers report more time using electronic devices for recreational purposes than do their non-Hispanic white counterparts . Other demographic groups associated with greater screen time include boys, older children, younger mothers, and less-educated parents . Little is known about screen time in Hispanic subpopulations, especially Hispanic children in elementary school. Studying screen time in younger children is important because risk factors for obesity can begin to operate as early as infancy .\n\n【13】The _Salud Para Usted y Su Familia_ (Health for You and Your Family) project  is studying the determinants of obesity among Mexican American children in rural, low-income border communities in New Mexico. As a first step in designing an intervention to reduce the risk for childhood obesity in these communities, we collected data on the prevalence of risk factors, including screen time. The main objective of this study was to describe the demographic correlates of heavy screen use among Mexican American children in 2 small, rural communities on the New Mexico–Mexico border. A secondary objective was to assess the association of selected modifiable household norms with heavy screen use.\n\n【14】Methods\n\n【15】From July through December 2016, we conducted a cross-sectional survey of 202 mothers or primary caregivers of Mexican-origin children aged 6 to 10 years (in grades kindergarten through 4) in 2 _colonias_ (rural communities that lack adequate water, sewer, or decent housing) . We recruited study participants from the unincorporated community of Chaparral (population, 14,631) in Otero County and Doña Ana County and the village of Columbus (population, 1,244) in Luna County . Chaparral and Columbus are 20 and 3 miles from the Mexican border and 84% and 88% Hispanic, respectively .\n\n【16】We hired and trained _promotores de salud_ (promoters of health), bilingual indigenous community health workers, as promotor–researchers to recruit participants and collect data for the project . Promotor–researchers recruited a convenience sample from their communities by approaching potential respondents door-to-door and at schools, school bus stops, shopping centers, and community events. When 2 parents were available, mothers were preferred as participants. Promotor–researchers determined eligibility by administering a 9-item questionnaire. Eligibility criteria included having lived in the community for at least 1 year, being the primary caregiver for a Mexican American child aged 6 to 10 years, and living with a spouse or partner who shared childcare responsibilities. Mexican origin was defined as Mexican nativity in the child or 1 or more of the child’s biological parents or grandparents.\n\n【17】Our goal was a sample size of 200, 100 from each community. Promotor–researchers approached 1,093 individuals, of whom 1,091 (99.8%) completed the questionnaire. Among these, 260 (23.8%) were eligible. The modal reason for ineligibility was not having a child aged 6 to 10 years. Among eligible respondents, 202 (77.7%) signed informed consent agreements, and all those who signed completed interviews.\n\n【18】Promotor–researchers administered the informed consent and the study instruments in English or Spanish, depending on the respondent’s preference. The primary study instrument, an 88-item survey, took 45 minutes and was conducted at the time of recruitment or later at a convenient location. Study participants received a $5.00 gift card.\n\n【19】We asked participants with 2 or more children aged 6 to 10 years to choose 1 child and answer survey questions with that 1 reference child in mind. Interviewers prompted participants to respond about that child with phrases such as, “Going back to the child you were thinking about .”\n\n【20】### Variables\n\n【21】The 88 survey questions covered a range of factors associated in the literature with childhood obesity, including demographic variables, diet, and physical activity. It also included factors associated with screen time: 1) internet access; use of smartphones, computers or laptops, and other electronic devices among children, mothers, and fathers; 2) household norms related to screen use (Box); and 3) reasons for use and types of electronic devices used by all children in the household.\n\n【22】Box. Household Norms Related to Screen Use as Defined by the Questions Below.\n\n【23】• Is the TV on when your child eats?\n\n【24】• When eating together as a family, is there anyone who uses electronics (cell phone, games, etc.)?\n\n【25】• During a normal week, how often does your family eat a meal together?\n\n【26】• When your child misbehaves, do you ever take away his/her outdoor play time?\n\n【27】• When your child misbehaves, do you ever take away his/her electronics?\n\n【28】• Does it ever seem the only way to keep your child entertained is to encourage his/her use of TV, tablet, video games, or other electronics?\n\n【29】• How many times a week does your family do active things together?\n\n【30】We defined screen time as the number of hours per day that the child used electronic screen devices at home. Mothers were asked, “How many **hours** does your child spend at home on a normal day **during the week** using electronics (TV, videogames, computer games, cell phone)?” and “How many **hours** does your child spend at home on a normal day **on the weekend** using electronics (TV, videogames, computer games, cell phone)?” Possible responses were none, 1 or 2 hours, 3 hours, or 4 hours or more. The 2 outcome variables were heavy screen use on weekdays and heavy screen use on weekends. We defined heavy screen use as 4 hours or more per day .\n\n【31】### Analysis\n\n【32】To assess which variables should be included in a multivariate analysis, we first conducted individual tests of association between the outcome variables and potential risk factors. We used χ 2  and Fisher exact tests for unordered categorical variables and Cochran–Armitage tests for trend for ordered variables. Variables were included in the multivariate model if 1) the variable was associated ( _P_ < .25) in the weekday or weekend analysis, or 2) the variable was associated with screen time in the literature (ie, child’s age, child’s sex, maternal education, and income/Medicaid status). Internet access met the first criterion, but it was excluded because none of the heavy users lacked internet access. Weekday and weekend use were fit by using separate models.\n\n【33】Analysis was conducted by using SAS version 9.4 (SAS Institute Inc). The institutional review boards of the institutions with which the authors are affiliated reviewed and approved the study protocol.\n\n【34】Results\n\n【35】Among the 202 children in the study, 117 (57.9%) were aged 6 to 8 and 85 (42.1%) were aged 9 or 10 . Mean age was 8.1 years (standard deviation, 1.4 y). Among the parent respondents, 192 (95.0%) were female, 144 (71.3%) were born in Mexico, 181 (89.6%) had a high school education or less, and 143 (70.8%) had 5 or more members in their household. Among the 202 study households, 99 of 145 (68.3%) had a total monthly income of less than $2,000 (excluding “don’t know” responses); 180 (89.1%) had a member who receives Medicaid, and 180 (89.1%) spoke primarily Spanish. The children used Spanish-language electronic devices exclusively in 46 (22.8%) households; most used English exclusively or English and Spanish. Most (n = 108 \\[53.5%\\]) households had cell phone plans, and 92.1% had internet access.\n\n【36】Approximately one-quarter (53 of 202; 26.2%) of children used screens for more than 2 hours per day during the week at home, and 30 (14.9%) were heavy weekday users. On weekends, 84 (41.6%) children used screens for more than 2 hours per day, and 51 (25.2%) were heavy weekend users. Screen time was greater on weekend days ( _P_ \\= .002). Heavy use during weekdays or weekends was not significantly associated with child’s age, child’s sex, or any other demographic characteristic except household size . We found a trend toward less screen use on weekends as household size increased. We also found that a greater percentage of children in Columbus (32.0%) than in Chaparral (18.6%) were heavy weekend users.\n\n【37】The unadjusted analysis of 7 household norms  found that norms encouraging screen use were common. Six of 7 norms qualified for inclusion in the adjusted analysis, but we included all 7 norms. In the adjusted analysis , no demographic variables other than household size were associated with screen time. Larger households were less likely to report heavy weekend screen use. In contrast, 4 of 7 norms were associated with heavy weekday use, heavy weekend use, or both. Most (59.9%) families had the television on while the child ate, a practice associated with heavy screen use both on weekdays (AOR = 3.02; 95% confidence interval \\[CI\\], 1.08–8.45) and weekends (AOR = 2.38; 95% CI, 1.04–5.40) . Three of 4 families ate meals together more than 3 times per week, and heavy weekend screen use among children in these families was less prevalent than in families who ate meals together 3 times or fewer per week (AOR = 0.40; 95% CI, 0.17–0.94). Parents who used television or electronics for entertaining their child reported heavy weekend use more than twice as often (AOR = 2.94; 95% CI, 1.15–7.51) as parents who did not. Finally, families who were physically active together 2 or 3 times per week were associated with less weekend screen time than families active together at most 1 time (0 or 1) per week (AOR = 0.33; 95% CI, 0.12–0.87).\n\n【38】Parents reported multiple reasons why children (as a group) in their households used desktops or laptops and smartphones on weekdays . Two-thirds of all users used these devices for homework. Just more than half of all users used them for games and for internet/YouTube. No single reason for use was significantly associated with heavy weekday or heavy weekend use.  \n\n【39】Frequency of reasons for use of smartphones, desktops, or laptops by children on weekday in study households, according to level of use in the reference child, Chaparral and Columbus, New Mexico, 2016. Parents could indicate more than 1 reasons for use; thus, percentages do not sum to 100. \n\n【40】Among devices used by all children in study households, smartphones (62.4%) and desktop or laptops (60.9%) were dominant . Only 8.9% of children used none of the devices listed. No devices were significantly associated with heavy weekday or weekend screen time.  \n\n【41】Among mothers, 89.2% used smartphones, 25.0% used desktops or laptops, and 7.3% used game consoles. Paternal patterns of use were similar. Parental patterns of use were not significant predictors of heavy screen use among children.\n\n【42】Discussion\n\n【43】This study found that in 2 rural communities in New Mexico near the Mexico border, most families had cell phones and access to the internet in 2016. Among these families, one in 4 had a child aged 6 to 10 years who spent 2 hours or more per weekday using electronic devices at home, and one in 7 had a child who spent 4 hours or more per weekday using electronic devices at home. Most families reported that a television was on while children ate, and someone was using electronic devices during meals in one-quarter of the households. Social norms of television use during meals, not eating as a family frequently, encouraging children to entertain themselves with electronics, and not participating as a family in physical activities appear to be risk factors for heavy screen use in this study population.\n\n【44】In aggregate, the total screen time reported for many of these elementary-school–aged children exceed previous recommendations to limit screen time to 2 hours per day . Comparison with other populations of children is difficult because of differences in ages of study populations, outcome measures, and scope. In a population of Latino participants in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) in Oregon, 42% of children aged 2 to 5 years spent 2 hours or more per day on noneducational screen time . The National Health and Nutrition Examination Survey found that 47% of children aged 2 to 15 years spent more than 2 hours per day viewing television and video and using computers . Among Hispanic media users aged 8 to 12 years in 2015, mean daily screen time was 5 hours and 34 minutes nationally . The National Survey of Children’s Health (NSCH) reported weekday screen time for children who were more similar in age to those in our study population. For 2011–2012, NSCH reported that 7.8% of children aged 6 to 11 years old in New Mexico watched television or videos or played video games for 4 hours or more per weekday and that 2.9% spent 4 hours or more using computers, games, and other devices per weekday . Even if these percentages are summed (10.7%), the prevalence of heavy weekday screen use reported by NSCH is lower than the 14.9% reported in our study. Finally, comparison with screen time among Mexican children would be of interest, but the most comparable data available for Mexico, for children and teenagers aged 10 to 14 years, show that 27.7% have an average of more than 4 hours per day of screen time .\n\n【45】Our study suggests that it is important to measure screen time on both weekdays and weekends among school-aged children and that because weekend use is greater, measuring only weekday use might substantially underestimate total use. This finding is consistent with the findings of a 2006–2007 study of television viewing among mostly Mexican American fourth-graders in low-income schools along the Texas–Mexico border, where median television viewing was greater on weekend days than on weekdays (2.5 hours vs 1.5 hours) . Most studies do not distinguish between weekend use and weekday use .\n\n【46】Another study conducted in the US–Mexico border region found that parental rules or norms limiting television viewing were associated with less television viewing among children . This finding is consistent with our finding that having a television on during meals is associated with heavy screen use on both weekdays and weekends. Our finding on television viewing during meals is also consistent with the findings of other studies showing that children in homes where the television is on all or most of the time are more likely to have more screen time than other children have . Watching television during meals is associated with poorer diets among children . A study in Texas found that three-quarters of urban overweight or obese Mexican American children aged 6 to 8 years had televisions in their bedrooms .\n\n【47】For weekend use, several norms in addition to television use during meals were significant in the adjusted analysis. These same associations were suggested in weekday results but lacked significance. In general, it appears that eating meals and engaging in activities as a family limits screen time, while using electronic devices to keep children occupied increases it. Examinations of such family activities in relation to screen time were reported previously . The American Academy of Pediatrics has recommended positive parenting activities, such as playing together, as one way to decrease screen time .\n\n【48】Our study population’s access to computers and internet services can be compared with such access among the Hispanic population nationally. The 2015 American Community Survey established that 68.3% of Hispanic households had desktops or laptops and 70.9% had internet service; in limited–English-speaking populations, such as the one in our study, 53.0% of households had a computer . In our study, 108 (53.3%) households had cell phone plans, and 186 (92.1%) had internet access.\n\n【49】The extent to which our study population reflects the Mexican American population living in _colonias_ in New Mexico is not clear. In our study population, 68.3% of households had an annual income of less than $24,000. This percentage is comparable to the 69.1% of households of all races/ethnicities with an annual income of less than $25,000 in 2016 in Columbus, New Mexico, but it is different from the 49.5% of households with an annual income of less than $25,000 in Chaparral . Some aspect of how the study sample was collected might have resulted in the recruitment of families whose incomes are lower than the average income of residents in the 2 _colonias_ in our study. The study’s possible inclusion of low-income families who avoid participation in the census because of their undocumented status might account for this bias.\n\n【50】This study has several limitations. First, the study population was a convenience sample, and selection bias might have operated in the recruitment process and/or in the choice of the reference child by the parent when more than one child was eligible. No random sampling of households was considered possible in these communities. Consequently, the reported estimates might differ from those in these communities overall or in other New Mexico _colonias_ . Second, parental awareness of the more socially desirable responses to questions about use of electronic devices by children might have introduced a reporting bias toward underreporting screen time or household norms that encouraged it, such as choosing the child with less screen time as the reference child. Third, the sample size was small and may have been underpowered to detect associations between household norms and children’s screen time. The study’s strengths were the collection of data by trained, bilingual, local promotor–researchers and the 78% participation rate. To our knowledge, ours is the first assessment of total screen time, as opposed to television viewing , in _colonias_ along the US–Mexico border.\n\n【51】Although some Hispanic children of Mexican heritage live in poor, remote communities in the Southwest and their families might have limited skills in English, the assumption that their access to the internet or electronic devices is limited would be incorrect. Along with adopting an American diet and its attendant risk of obesity , Mexican American children whom we studied in these New Mexican communities have adopted the same levels of computer use and other electronic screen use as have non-Hispanic white children elsewhere in the United States.\n\n【52】Checking the epidemic of obesity among the Hispanic population in the United States in such communities will depend on making behavioral changes early in life and addressing the twin issues of diet and physical inactivity, including reducing screen time without cutting off access to screen time that might be beneficial . Strategies found to be effective in nonminority populations in reaching such goals need to be tested in Hispanic and other racial/ethnic minority populations.\n\n【53】Tables\n------\n\n【54】Table 1. Characteristics of Study Population and Their Association With Heavy Screen Use (≥4 Screen-Time Hours per Day) on Weekdays and Weekends Among Mexican-Origin Children Aged 6 to 10 Years, Chaparral and Columbus, New Mexico, 2016 a  \n\n| Characteristic | Overall, No. (%) b (n = 202) | Weekday (n = 30) | Weekend Day (n = 51) |\n| --- | --- | --- | --- |\n| No. (%) | _P_ Value | No. (%) | _P_ Value |\n| --- | --- | --- | --- |\n| **Child’s age, y** | **Child’s age, y** | **Child’s age, y** | **Child’s age, y** | **Child’s age, y** | **Child’s age, y** |\n| 6–8 | 117 (57.9) | 15 (12.8) | .34 c | 27 (23.1) | .40 c |\n| 9 or 10 | 85 (42.1) | 15 (17.6) | .34 c | 24 (28.2) | .40 c |\n| Mean | 8.1 | 8.1 | — | 8.3 | — |\n| **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** | **Child’s sex** |\n| Male | 108 (53.5) | 16 (14.8) | \\>.99 c | 31 (28.7) | .26 c |\n| Female | 94 (46.5) | 14 (14.9) | \\>.99 c | 20 (21.3) | .26 c |\n| **Maternal age, y** **d** | **Maternal age, y** **d** | **Maternal age, y** **d** | **Maternal age, y** **d** | **Maternal age, y** **d** | **Maternal age, y** **d** |\n| 20–29 | 51 (25.6) | 10 (19.6) | .25 e | 15 (29.4) | .44 e |\n| 30–39 | 95 (47.7) | 14 (14.7) | .25 e | 17 (17.9) | .44 e |\n| ≥40 | 53 (26.6) | 6 (11.3) | .25 e | 18 (34.0) | .44 e |\n| Mean | 35.7 | 34.2 | — | 37.5 | — |\n| **Maternal birth country** | **Maternal birth country** | **Maternal birth country** | **Maternal birth country** | **Maternal birth country** | **Maternal birth country** |\n| United States | 58 (28.7) | 8 (13.8) | \\>.99 c | 14 (24.1) | .86 c |\n| Mexico | 144 (71.3) | 22 (15.3) | \\>.99 c | 37 (25.7) | .86 c |\n| **No. of years of maternal education** | **No. of years of maternal education** | **No. of years of maternal education** | **No. of years of maternal education** | **No. of years of maternal education** | **No. of years of maternal education** |\n| 1–8 | 45 (22.3) | 7 (15.6) | .71 e | 15 (33.3) | .14 e |\n| 9–12 | 136 (67.3) | 21 (15.4) | .71 e | 32 (23.5) | .14 e |\n| \\>12 | 21 (10.4) | 2 (9.5) | .71 e | 4 (19.0) | .14 e |\n| **No. of household members** | **No. of household members** | **No. of household members** | **No. of household members** | **No. of household members** | **No. of household members** |\n| 3 | 20 (9.9) | 6 (30.0) | .051 e | 11 (55.0) | .002 e |\n| 4 | 39 (19.3) | 6 (15.4) | .051 e | 11 (28.2) | .002 e |\n| 5 | 74 (36.6) | 11 (14.9) | .051 e | 17 (23.0) | .002 e |\n| ≥6 | 69 (34.2) | 7 (10.1) | .051 e | 12 (17.4) | .002 e |\n| **Monthly household income, $** | **Monthly household income, $** | **Monthly household income, $** | **Monthly household income, $** | **Monthly household income, $** | **Monthly household income, $** |\n| <1,000 | 41 (20.3) | 9 (22.0) | .13 e | 12 (29.3) | .08 e |\n| 1,000–1,999 | 58 (28.7) | 9 (15.5) | .13 e | 14 (24.1) | .08 e |\n| 2,000–2,999 | 26 (12.9) | 4 (15.4) | .13 e | 3 (11.5) | .08 e |\n| ≥3,000 | 20 (9.9) | 1 (5.0) | .13 e | 3 (15.0) | .08 e |\n| Don’t know | 57 (28.2) | 7 (12.3) | .13 e | 19 (33.3) | .08 e |\n| **Household member receives Medicaid** | **Household member receives Medicaid** | **Household member receives Medicaid** | **Household member receives Medicaid** | **Household member receives Medicaid** | **Household member receives Medicaid** |\n| No | 22 (10.9) | 3 (13.6) | \\>.99 c | 8 (36.4) | .20 c |\n| Yes | 180 (89.1) | 27 (15.0) | \\>.99 c | 43 (23.9) | .20 c |\n| **Primary language in household** | **Primary language in household** | **Primary language in household** | **Primary language in household** | **Primary language in household** | **Primary language in household** |\n| Spanish | 180 (89.1) | 26 (14.4) | .75 c | 45 (25.0) | .80 c |\n| English | 22 (10.9) | 4 (18.2) | .75 c | 6 (27.3) | .80 c |\n| **Language child uses for electronics** | **Language child uses for electronics** | **Language child uses for electronics** | **Language child uses for electronics** | **Language child uses for electronics** | **Language child uses for electronics** |\n| Spanish exclusively | 46 (22.8) | 9 (19.6) | .59 c | 15 (32.6) | .36 c |\n| English exclusively | 84 (41.6) | 11 (13.1) | .59 c | 21 (25.0) | .36 c |\n| Both | 72 (35.6) | 10 (13.9) | .59 c | 15 (20.8) | .36 c |\n| **Internet access type** | **Internet access type** | **Internet access type** | **Internet access type** | **Internet access type** | **Internet access type** |\n| Cell phone subscription | 87 (43.1) | 14 (16.1) | .21 c | 22 (25.3) | .95 c |\n| DSL/cable subscription | 49 (24.3) | 6 (12.2) | .21 c | 15 (30.6) | .95 c |\n| Both cell phone and DSL/cable subscription | 21 (10.4) | 3 (14.3) | .21 c | 5 (23.8) | .95 c |\n| Other type of internet subscription | 15 (7.4) | 5 (33.3) | .21 c | 3 (20.0) | .95 c |\n| Access to internet without subscription | 14 (6.9) | 2 (14.3) | .21 c | 3 (21.4) | .95 c |\n| No internet access | 16 (7.9) | 0 | .21 c | 3 (18.8) | .95 c |\n| **Community of residence** | **Community of residence** | **Community of residence** | **Community of residence** | **Community of residence** | **Community of residence** |\n| Chaparral | 102 (50.5) | 16 (15.7) | .84 c | 19 (18.6) | .04 c |\n| Columbus | 100 (49.5) | 14 (14.0) | .84 c | 32 (32.0) | .04 c |\n| **Total** | 202 (100.0) | 30 (14.9) | — | 51 (25.2) | — |\n\n【56】a  Data collected from 88-item survey of 202 mothers or primary caregivers from July through December 2016. Participants with 2 or more children aged 6 to 10 years were asked to choose 1 child and answer survey questions with that 1 reference child in mind.  \nb  Percentages may not sum to 100 because of rounding.  \nc  Determined by Fisher exact test.  \nd  Values do not sum to 202 because 3 respondents did not answer question.  \ne  Determined by Cochran–Armitage 2-sided trend test.\n\n【57】Table 2. Household Norms and Their Association With Heavy Screen Use (≥4 Screen-Time Hours per Day) on Weekdays and Weekends Among Mexican-Origin Children Aged 6 to 10 Years, Chaparral and Columbus, New Mexico, 2016 a  \n\n| Household Norm | Category | Overall, No. (%)(n = 202) | Weekday (n = 30) | Weekend Day (n = 51) |\n| --- | --- | --- | --- | --- |\n| No. (%) | _P_ Value b | No. (%) | _P_ Value b |\n| --- | --- | --- | --- |\n| Is the TV on when your child eats? | No | 81 (40.1) | 6 (7.4) | .02 | 13 (16.0) | .01 |\n| Is the TV on when your child eats? | Yes | 121 (59.9) | 24 (19.8) | .02 | 38 (31.4) | .01 |\n| When eating together as a family, is there anyone who uses electronics (cell phone, games, etc.)? | No | 153 (75.7) | 20 (13.1) | .25 | 37 (24.2) | .57 |\n| When eating together as a family, is there anyone who uses electronics (cell phone, games, etc.)? | Yes | 49 (24.3) | 10 (20.4) | .25 | 14 (28.6) | .57 |\n| During a normal week, how often does your family eat a meal together? | ≤1 | 18 (8.9) | 2 (11.1) | .03 | 5 (27.8) | .08 |\n| During a normal week, how often does your family eat a meal together? | 2 or 3 | 32 (15.8) | 10 (31.3) | .03 | 13 (40.6) | .08 |\n| During a normal week, how often does your family eat a meal together? | \\>3 | 152 (75.2) | 18 (11.8) | .03 | 33 (21.7) | .08 |\n| When your child misbehaves, do you ever take away his/her outdoor play time? | No | 108 (53.5) | 14  | .44 | 28 (25.9) | .87 |\n| When your child misbehaves, do you ever take away his/her outdoor play time? | Yes | 94 (46.5) | 16 (17.0) | .44 | 23 (24.5) | .87 |\n| When your child misbehaves, do you ever take away his/her electronics? | No | 17 (8.4) | 2 (11.8) | \\>.99 | 1 (5.9) | .08 |\n| When your child misbehaves, do you ever take away his/her electronics? | Yes | 185 (91.6) | 28 (15.1) | \\>.99 | 50 (27.0) | .08 |\n| Does it ever seem the only way to keep your child entertained is to encourage his/her use of TV, tablet, video games, or other electronics? | No | 172 (85.1) | 22 (12.8) | .09 | 37 (21.5) | .006 |\n| Does it ever seem the only way to keep your child entertained is to encourage his/her use of TV, tablet, video games, or other electronics? | Yes | 30 (14.9) | 8 (26.7) | .09 | 14 (46.7) | .006 |\n| How many times a week does your family do active things together? | ≤1 | 80 (39.6) | 15 (18.8) | .48 | 29 (36.3) | .01 |\n| How many times a week does your family do active things together? | 2 or 3 | 65 (32.2) | 8 (12.3) | .48 | 10 (15.4) | .01 |\n| How many times a week does your family do active things together? | \\>3 | 57 (28.2) | 7 (12.3) | .48 | 12 (21.1) | .01 |\n| Total | — | 202 (100.0) | 30 (14.9) | — | 51 (25.2) | — |\n\n【59】a  Data collected from 88-item survey of 202 mothers or primary caregivers from July through December 2016. Participants with 2 or more children aged 6 to 10 years were asked to choose 1 child and answer survey questions with that 1 reference child in mind.  \nb  Determined by Fisher exact test.\n\n【60】Table 3. Adjusted Odds Ratios for Associations of Demographic Characteristics and Household Norms With Heavy Screen Use (≥4 Screen-Time Hours per Day) on Weekdays and Weekends Among Mexican-Origin Children Aged 6 to 10 Years, Chaparral and Columbus, New Mexico, 2016 a  \n\n| Demographic Variable or Household Norm | Adjusted Odds Ratio (95% Confidence Interval) |\n| --- | --- |\n| Weekday | Weekend Day |\n| --- | --- |\n| **Community of residence** |  |  |\n| Chaparral | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Columbus | 0.63 (0.24–1.63) | 1.22 (0.56–2.65) |\n| **Child’s age** | 1.17 (0.83–1.64) | 1.24 (0.92–1.66) |\n| **Child’s sex** | **Child’s sex** | **Child’s sex** |\n| Male | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Female | 1.01 (0.41–2.48) | 0.63 (0.29–1.38) |\n| **Maternal age** | 0.95 (0.89–1.02) | 1.04 (0.99–1.10) |\n| **Years of maternal education** | 0.96 (0.80–1.16) | 1.02 (0.88–1.17) |\n| **No. of household members** | 0.79 (0.57–1.10) | 0.73 (0.55–0.98) |\n| **Monthly household income, $** **b** | **Monthly household income, $** **b** | **Monthly household income, $** **b** |\n| <1,000 | 1 \\[Reference\\] | — |\n| Don’t know | 0.67 (0.18–2.51) | — |\n| 1,000–1,999 | 0.92 (0.28–2.98) | — |\n| ≥2,000 | 0.44 (0.11–1.69) | — |\n| **Household member receives Medicaid** **b** | **Household member receives Medicaid** **b** | **Household member receives Medicaid** **b** |\n| No | — | 1 \\[Reference\\] |\n| Yes | — | 0.33 (0.10–1.03) |\n| **Television on during meals** **c** | **Television on during meals** **c** | **Television on during meals** **c** |\n| No | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Yes | 3.02 (1.08–8.45) | 2.38 (1.04–5.40) |\n| **Someone uses electronics while eating** **c** | **Someone uses electronics while eating** **c** | **Someone uses electronics while eating** **c** |\n| No | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Yes | 1.32 (0.50–3.54) | 1.18 (0.49–2.88) |\n| **No. of meals eaten together during the week** **c** | **No. of meals eaten together during the week** **c** | **No. of meals eaten together during the week** **c** |\n| ≤3 | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| \\>3 | 0.44 (0.17–1.16) | 0.40 (0.17–0.94) |\n| **Outdoor play time limited for misbehavior** **c** | **Outdoor play time limited for misbehavior** **c** | **Outdoor play time limited for misbehavior** **c** |\n| No | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Yes | 1.25 (0.51–3.05) | 0.85 (0.39–1.84) |\n| **Use of electronic devices limited for misbehavior** **c** | **Use of electronic devices limited for misbehavior** **c** | **Use of electronic devices limited for misbehavior** **c** |\n| No | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Yes | 0.78 (0.14–4.23) | 5.84 (0.64–53.05) |\n| **Feels electronics are the only way to keep children entertained** **c** | **Feels electronics are the only way to keep children entertained** **c** | **Feels electronics are the only way to keep children entertained** **c** |\n| No | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Yes | 2.17 (0.75–6.30) | 2.94 (1.15–7.51) |\n| **No. of times per week family does active things together** **c** | **No. of times per week family does active things together** **c** | **No. of times per week family does active things together** **c** |\n| ≤1 | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| 2 or 3 | 0.58 (0.19–1.75) | 0.33 (0.12–0.87) |\n| \\>3 | 1.14 (0.36–3.63) | 0.96 (0.38–2.46) |\n\n【62】a  Data collected from 88-item survey of 202 mothers or primary caregivers from July through December 2016. Participants with 2 or more children aged 6 to 10 years were asked to choose 1 child and answer survey questions with that 1 reference child in mind.  \nb  Medicaid participation was used as a proxy for income in the weekend model because 28.2% of participants responded “don’t know” to the income question. For the weekday model, only 3 reference children were heavy users and were not in a Medicaid household, so we chose to include household income in this model, treating the “don’t knows” as a separate category and combining the $2,000-$2,999 and ≥$3,000 groups. Internet access was not included in the models because none of the heavy users were without internet access.  \nc  Household norms were rephrased for this table.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "746b4ba8-cb06-424f-b4d8-a5dd2719edb6", "title": "Impact of the Affordable Care Act on Access to Care for US Adults With Diabetes, 2011-2012", "text": "【0】Impact of the Affordable Care Act on Access to Care for US Adults With Diabetes, 2011-2012\nAbstract\n\n【1】**Introduction**  \nLack of health insurance is a barrier to medical care, which may increase the risk of diabetes complications and costs. The objective of this study was to assess the potential of the Affordable Care Act (ACA) of 2010 to improve diabetes care through increased health care access by comparing health care and health outcomes of insured and uninsured people with diabetes.\n\n【2】**Methods**  \nWe examined demographics, access to care, health care use, and health care expenditures of adults aged 19 to 64 years with diabetes by using the 2011 and 2012 Medical Expenditure Panel Survey. Bivariate descriptive statistics comparing insured and uninsured persons were evaluated separately by income above and below 138% of the federal poverty level (FPL), (a threshold for expanded Medicaid eligibility in select states under the ACA) using the _t_ test and proportion and median tests.\n\n【3】**Results**  \nUninsured adults reported poorer access to care than insured adults, such as having a usual source of health care (69.0% vs 89.5% \\[≤138% FPL\\], 77.1% vs 94.6% \\[>138% FPL\\], both _P_ < .001) and having lower rates of 6 key diabetes preventive care services ( _P_ ≤ .05). Insured adults with diabetes had significantly higher health care expenditures than uninsured adults ($13,706 vs $4,367, $10,838 vs $4,419, respectively, both _P_ < .001).\n\n【4】**Conclusion**  \nUninsured adults with diabetes had less access to health care and lower levels of preventive care, health care use, and expenditures than insured adults. To the extent that the ACA increases access and coverage, uninsured people with diabetes are likely to significantly increase their health care use, which may lead to reduced incidence of diabetes complications and improved health.\n\n【5】Introduction\n\n【6】In 2012, more than 29 million Americans were living with diagnosed diabetes . The serious health challenges facing people with diabetes include heart disease, stroke, hypertension, kidney disease, neuropathy, and blindness . Researchers estimate that the economic burden to society of diagnosed diabetes reached $245 billion in 2012 . Although private and public health insurance programs provide important access to health care for some people with diabetes, millions of working-age adults with diabetes lack health insurance . This suggests that a high proportion of the population with diabetes faces significant challenges in access to health care, which may lead to suboptimal care, increased rates of long-term complications, and greater health care expenditures.\n\n【7】The Affordable Care Act (ACA) of 2010 is designed to provide access to coverage for previously uninsured Americans. Adults with incomes below 138% of the federal poverty level (FPL) will gain access to Medicaid coverage in states that expand coverage  (28 states including the District of Columbia as of January 24, 2015). People with incomes above the poverty level in all states can obtain access to private insurance plans in health insurance “marketplaces.” In addition, premiums in these marketplaces are subsidized for people with household incomes between 100% and 399% of the FPL . An estimated 60% of the uninsured will obtain health insurance through one or the other of these 2 methods by 2019 . As of September 2014, ACA had reduced the number of uninsured by more than 9 million , although a separate breakdown for people with diabetes was not available.\n\n【8】Previous published work has shown that the uninsured face significant barriers to obtaining health care and face higher out-of-pocket health care costs than the insured . In addition, the uninsured can experience health problems as a result of the lack of access to medical care. Although much research has focused on the general uninsured population, few studies have focused on the population with diabetes. A study similar to ours focused on Medicaid and diabetes, although the authors used older data and did not include people with higher incomes , who are also affected by ACA. In addition, because health care reform, one of the most important social policy changes in the United States in decades, is now nearly fully implemented, no studies have taken a snapshot of the uninsured US population with diabetes and considered how their medical care may be changing under full implementation of ACA in 2014 and beyond.\n\n【9】The objective of this study was to gauge the potential impact of ACA on improving diabetes care through improved health care access by comparing health care and health outcomes of a large national sample of insured and uninsured adults with diabetes. Our results provide a straightforward comparison of the gap between the insured and uninsured before health care reform and insights about how indicators for these 2 groups may converge in coming years.\n\n【10】Methods\n\n【11】To obtain the latest pre-ACA snapshot of the US population with diabetes, we pooled data from the 2 most recent years of the Medical Expenditure Panel Survey (MEPS), the 2011 and 2012 household component full-year consolidated data files . (This period is “pre-ACA” because major provisions were not effective until 2014, although limited features such as expanded coverage for young adults began in 2010.) MEPS is an ongoing set of surveys sponsored by the Agency for Healthcare Research and Quality (AHRQ) that collects nationally representative data on health services and expenditures of the noninstitutionalized civilian population. MEPS is constructed from a subsample of households participating in the National Health Interview Survey and uses a stratified random sample design and computer-assisted in-person interviews . MEPS is well suited for this analysis because it provides detailed information on health care use and expenditures and also includes survey responses, capturing data on items such as health care access. Other national surveys and claims data provide one or the other type of data but rarely both.\n\n【12】MEPS respondents are interviewed 3 times during a calendar year and asked several questions about health care use, insurance, expenditures, access to care, preventive care services, and chronic diseases. People with diabetes are asked about health outcomes and health care specific to diabetes. For many measures on the full-year consolidated file, MEPS combines data from each respondent’s multiple interviews to create 1 calendar year variable.\n\n【13】We restricted the study sample to adults aged 19 to 64 years who reported that they had been diagnosed with diabetes (most people over age 65 with diabetes are unaffected by ACA coverage provisions, because they are eligible for Medicare.) We stratified analysis by household income at less than 138% of the FPL (hereafter, low income) and greater than 138% of the FPL (hereafter, high income), because these are the eligibility limits for Medicaid in states choosing to expand coverage.\n\n【14】We used edited variables when possible, which are cleaned for consistency by AHRQ across the multiple survey rounds in MEPS. These variables included age, sex, race/ethnicity, education, census region, and health insurance status. We collapsed the level of detail on variables such as employment, because these measures serve only as controls intended to identify systematic differences between the sample groups and are not primary aspects of diabetes care or outcomes. All outcomes, which are measured over a period of time (employment, insurance, access to care, diabetes care, health care use, and health care expenditures), were limited to the current year, 2011 or 2012, for the corresponding MEPS sample. For ease of exposition, we created the following mutually exclusive groups for the previous 12 months out of the monthly MEPS insurance indicators in the following order: dual eligible (Medicaid and Medicare), Medicaid, Medicare, private insurance, TRICARE and Civilian Health and Medical Program of the Department of Veterans Affairs (CHAMPVA), and other public insurance. People with more than 1 insurance type were grouped into the first applicable type in the insurance indicator list.\n\n【15】Diabetes-specific measures were from the MEPS Diabetes Care Survey (DCS), a paper-and-pencil survey module administered to those reporting that they had ever been diagnosed with diabetes. The DCS includes self-report of the year of diagnosis, of ever having had diabetes complications of the eye or kidney, and of receiving 6 preventive care measures in the past year (hemoglobin A1c blood test, feet checked for sores or irritations, dilated eye examination, blood cholesterol check, influenza vaccination, and blood pressure checks). We calculated the years since diabetes was first diagnosed as the difference between the respondent’s age at interview and the age at diagnosis. We used the DCS-specific survey weight when reporting DCS measures.\n\n【16】For physical health and comorbidities, we analyzed self-reported body mass index (BMI) (kg in weight/m 2  in height) and the prevalence of all adult priority conditions, as defined by AHRQ . In addition to diabetes, the conditions are hypertension, heart disease (coronary heart disease, angina, myocardial infarction, other unspecified heart disease), stroke, emphysema, chronic bronchitis, high cholesterol, cancer, joint pain, arthritis, and asthma. Several of these are either linked to diabetes as complications associated with diabetes management and duration; others are risk factors that present additional challenges to proper diabetes care.\n\n【17】We included 3 broad measures of access, measured as binary indicators. All respondents were asked if they had a usual source of medical care in the past year. Respondents were also asked if they were ever unable to access necessary medical care in the past year and if they were unable to get necessary prescription medications. For health care use in the past year, we studied the number of physician or clinic office visits and total number of prescriptions. We included binary indicators for any emergency department visit and any inpatient hospital nights, because these services are used infrequently and their statistical distribution is skewed. We examined total health care expenditures, out-of-pocket health care expenditures, and prescription drug expenditures, which included diabetes supplies.\n\n【18】Survey-specific procedures in Stata 13.1 (StataCorp LP) with weighted analyses and analytic subpopulations were used. To pool the 2011–12 data, we halved the survey weights so that the results equally represented people with diabetes for 2011 and 2012. Differences between insured and uninsured people were examined separately by income group using means, proportions, and crosstabs. Tests of significance were computed using survey-specific _t_ tests, proportions, and χ 2  tests. We also computed medians of health care expenditure and use data and tested differences by using the nonparametric k-sample test on equality of medians.\n\n【19】Results\n\n【20】We estimated from MEPS that from 2011 through 2012, more than 13 million adults in the United States aged 19 to 64 years were living with diagnosed diabetes, and nearly 2 million of them lacked health insurance . The prevalence of diabetes ranged from 4.8% among the uninsured with incomes above 138% of the FPL to 10.5% among the insured with incomes at or below 138% of the FPL ; in both income groups, insured persons were more likely to have diabetes than uninsured persons ( _P_ < .001).\n\n【21】Differences by insurance status suggest some patterns that may be related to both the likelihood of having insurance and the prevalence of diabetes. In both income groups, uninsured persons were more likely to be nonwhite ( _P_ \\= .007, low income; _P_ < .001, high income). Among the low-income groups, uninsured people with diabetes (42.7%) were more likely than the insured to be employed (26.5%) ( _P_ < .001). Significant regional differences by insurance status were also apparent among the low-income groups ( _P_ \\= .002); more than 55% of low-income uninsured adults and only 39% of insured adults resided in states in the southern census region.\n\n【22】High-income, insured adults with diabetes had a higher average BMI than uninsured adults (33.5 vs 31.5, _P_ \\= .002); this was also the case with overweight adults ( _P_ \\= .02) and those with high levels of morbid obesity (class II/III, _P_ \\= .01). Low-income insured adults had significantly higher rates of 7 chronic conditions (heart disease, stroke, emphysema, bronchitis, joint pain, arthritis, asthma) than those without insurance (all _P_ < .01 or smaller), and high-income people had higher rates of 2 conditions (high cholesterol and arthritis) (both _P_ < .05).\n\n【23】Significant differences in health care access were seen in both income groups, both in having a usual source of care ( _P_ < .001) and being unable to access necessary health care ( _P_ < .001). Low-income people were also much more likely to report that they were unable to get necessary prescription medications ( _P_ \\= .002). Significant differences by insurance status for all 6 recommended diabetes preventive care services (ie, Hemoglobin A1c \\[HbA1c\\] test, foot examination, eye examination, blood cholesterol check, influenza vaccine, and blood pressure check) were found across income groups ( _P_ ≤ .05).\n\n【24】In both income groups, insured adults with diabetes were much more likely to have used medical services in the past year than those without health insurance . For instance, in the low-income group, the mean number of annual office visits among the insured was nearly triple the mean number among the uninsured ( _P_ < .001) and nearly double that among the high-income group ( _P_ < .001). The mean number and the median number of prescriptions were also substantially higher among the insured in both income groups ( _P_ < .001). The likelihood of using emergency department services ( _P_ \\= .001) or having inpatient hospital nights ( _P_ < .001) in the past year was significantly greater ( _P_ < .001) among the low-income group than the high-income group.\n\n【25】Differences in health care use and differences in expenditures between the insured and uninsured in both income groups were large. Mean total expenditures were much greater among the insured, which probably reflects greater access to health care: nearly $6,400 higher for those with incomes above 138% of the FPL ( _P_ < .001) and more than $9,300 higher for those with incomes at or below138% FPL ( _P_ < .001). Median differences were slightly smaller but still significant for both groups ( _P_ < .001). Out-of-pocket expenditures were higher among the uninsured only in the low-income group. Prescription drug expenditures were a significant driver of total expenses and were much greater among the insured than uninsured for both income groups ( _P_ < .001).\n\n【26】Discussion\n\n【27】Our findings showed that from 2011 through 2012, shortly after passage of ACA, nearly 2 million working-age adults with diabetes lacked health insurance. We also showed that access to care was a significant barrier among this population and that proper diabetes care lagged among the insured on all indicators. Thus, the potential of ACA to improve health and health care for people with diabetes appears to be large. If the health care patterns of the uninsured in 2011–12 move toward those of the insured, our results suggest that expanded insurance coverage will likely increase health care costs in the short-term for people with diabetes. Although long-term effects were beyond the scope of our research, it is possible that these may be more favorable for health outcomes and expenditures. For example, a recent study found that weight loss among people with diabetes reduced health expenditures over 10 years , and other health care interventions have also been shown to reduce the burden of diabetes . Counterbalancing these findings are the findings from the Oregon Health Insurance Experiment , in which newly acquired Medicaid did not significantly reduce average HbA1c during the first 2 years of coverage. Our findings are consistent with that study in that we found higher rates of diagnosed diabetes and greater use of prescription medications among the insured than the uninsured samples.\n\n【28】Our findings showed that uninsured adults with diabetes undergo different patterns of care than those with health insurance. For instance, in our study the uninsured were much less likely to obtain prescriptions, make office visits to physicians, and to have a usual source of care. Rates of current multiple disorders, several of which are associated with diabetes, were greater among the insured than the uninsured in both income groups; differences were significant for 6 conditions among the low-income sample and for 2 conditions among the high-income sample. Possible explanations include self-selection, in which those who have a greater need for care seek insurance voluntarily, or improved access, in which the insured are diagnosed for these conditions at higher rates . The percentage of respondents reporting fair or poor health was nearly 10 points higher among the high-income uninsured group ( _P_ \\= .02), possibly supporting an access theory, although no significant differences were seen among the low-income group.\n\n【29】The observed disconnect between patterns of care provided and need for care suggests that as uninsured adults with diabetes obtain new health insurance under full implementation of ACA in 2014 and beyond, we should expect their use of medical care to increase significantly — more office visits, prescriptions, and use of inpatient care. Our results distinguish between people with incomes below 138% of the FPL who will become eligible for Medicaid (in states expanding Medicaid) and those people with incomes above 138% of the FPL; results also demonstrate that health care use should increase for both groups, although the gap in services is larger among low-income persons, suggesting greater potential demand among low-income persons. However, many other barriers to good diabetes care exist, besides health insurance, that are not directly targeted by ACA, including education, literacy, language, attitudes, beliefs, and social support .\n\n【30】Given the longstanding finding  that the uninsured face difficulties obtaining access to a usual source of health care, it is not surprising that uninsured adults with diabetes in both the high- and low-income groups we studied were significantly less likely to report having a usual source of care ( _P_ < .001). Greater total expenditures among the insured probably reflect access, or the ease of obtaining necessary medical care. Early observations in the Oregon studies indicate similar trends . The fact that our findings show that inpatient and emergency department services were greater among the insured parallels findings in Oregon  and recent qualitative evidence . Furthermore, among those with Medicaid, access to primary care providers may be limited in some areas, given low provider reimbursement rates.\n\n【31】Our results should be interpreted with caution. First, our study compares descriptive results. A more detailed comparison could use multivariate analysis to control for associations between variables presented here. The descriptive findings here serve as a starting point and guide for future research.\n\n【32】Second, our results are based on cross-sectional data, so we cannot assign causation to the differences in health status, outcomes, and health care use, access, or expenditures between uninsured and insured people. People with health insurance are likely to be different from those without health insurance in some important ways, both observable and unobservable. Multivariate models can control for the observables, although some of the differences we found probably reflect both observable and unobservable factors. Our goal with this analysis was not to estimate causal inference. Rather, we sought to address the research gap in the literature on differences between the uninsured and insured population of people with diabetes, above and below the average Medicaid eligibility cutoff of 138% FPL, by assessing numerous health outcomes and health care access and use measures. The fact that our demographic indicators show a high level of Medicaid use by the income group at or below 138% FPL and a high level of private insurance by the over-138% FPL group supports the value of such a comparison. That is, the uninsured in each group who become insured after ACA are likely to receive most of their coverage through the dominant form of insurance for that income group (Medicaid for low income, private insurance for high income). Although we expect some changes in patterns of health care and health care use to result from changes in coverage, measurement of the actual effects will need to be conducted in future research studies after additional years of post-ACA implementation data are available. We also see relatively few differences in demographics between the insured and uninsured, except for nonwhite race/ethnicity and region, which is probably due to the more restrictive Medicaid income criteria for low-income adults in Southern states.\n\n【33】Third, the study is limited to adults aged 19 to 64 years, so the findings here cannot be extended to children or the elderly, although they are less directly targeted by ACA. Fourth, the MEPS measure of the diabetes population reflects currently diagnosed adults. The population with undiagnosed diabetes is sizable , and some of these people may be newly diagnosed under ACA as a result of improved health care access. On the one hand, the highest rate of undiagnosed diabetes is found among the uninsured , so our findings may be viewed as underestimates of the potential number of uninsured people with diabetes who could be assisted by implementation of ACA. We could not identify adults in the diabetes population who may or will not be eligible for coverage under ACA (noncitizens or persons living in states that will not expand Medicaid); this nonidentification would overstate the number of uninsured people with diabetes who could be assisted by implementation of the ACA.\n\n【34】Tables\n------\n\n【35】#####  Table 1. Health and Demographic Characteristics of US Adults With Diabetes Aged 19 to 64 Years, Medical Expenditure Panel Survey 2011 and\n\n| Characteristic a | ≤138% Federal Poverty Level | \\>138% Federal Poverty Level |\n| --- | --- | --- |\n| Uninsured b (95% CI) | Insured b (95% CI) | _P_ Value c | Uninsured b (95% CI) | Insured b (95% CI) | _P_ Value c |\n| --- | --- | --- | --- | --- | --- |\n| Diabetes prevalence | 5.6 (4.8–6.6) | 10.5 (9.3–11.6) | <.001 | 4.8 (4.0–5.6) | 4.8 (4.0–5.6) | 6.7 (6.1–7.2) | <.001 |\n| Unweighted sample (n = 1,568) | 325 | 774 | — | 318 | 318 | 1791 | — |\n| Weighted sample (n = 13,084,968) | 770,404 | 2,529,894 | — | 1,024,650 | 1,024,650 | 8,704,030 | — |\n| **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** | **Demographic characteristic** |\n| Age (mean), y | 49.8 (48.4–51.1) | 50.8 (49.8–51.9) | .19 | .19 | 51.8 (50.2–53.5) | 52.3 (51.6–53.0) | .62 |\n| Female | 51.1 (43.6–58.6) | 53.9 (49.2–58.7) | .52 | .52 | 43.6 (35.0–52.2) | .464 | .57 |\n| Race/ethnicity nonwhite | 68.1 (58.9–77.3) | 53.2 (46.3–60.1) | .007 | .007 | 54.7 (45.2–64.3) | 35.4 (31.8–39.0) | <.001 |\n| Years of education, mean | 11.3 (10.7–11.9) | 11.6 (11.3–12.0) | .28 | .28 | 11.7(11.1–12.3) | 13.5(13.4–13.7) | <.001 |\n| Ever employed in calendar year 2011 or 2012 | 42.7 (35.1–50.3) | 26.6 (21.8–31.5) | <.001 | <.001 | 69.6 (61.7–77.6) | 73.8 (71.0–76.5) | .33 |\n| Census region: Northeast | 8.1 (4.4–14.7) | 20.8 (15.7–27.0) | .002 | .002 | 14.4 (8.7–22.7) | 14.6 (12.0–17.7) | .22 |\n| Census region: Midwest | 18.7 (12.7–26.6) | 18.4 (14.6–22.8) | .002 | .002 | 16.7 (11.2–24.1) | 23.8 (19.5–28.7) | .22 |\n| Census region: South | 55.8 (46.7–64.5) | 38.7 (33.0–44.7) | .002 | .002 | 44.0 (35.5–53.0) | 43.2 (39.2–47.3) | .22 |\n| Census region: West | 17.4 (11.4–25.7) | 22.1 (17.3–27.9) | .002 | .002 | 24.9 (18.3–32.9) | 18.4 (15.6–21.4) | .22 |\n| Urban/metropolitan statistical area | 82.8 (76.3–89.2) | 76.7 (70.1–83.4) | .15 | .15 | 87.7 (81.6–93.7) | 83.0 (79.3–86.6) | .16 |\n| **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** | **Type of health insurance in 2011 or 2012** |\n| Medicaid | — | 47.8 (41.4–54.4) | <.001 | <.001 | — | 6.9 (5.6–8.6) | <.001 |\n| Medicare | — | 12.9 (10.1–16.4) | <.001 | <.001 | — | 9.0 (7.2–11.2) | <.001 |\n| Both Medicaid and Medicare | — | 17.1 (13.2–21.8) | <.001 | <.001 | — | 2.6 (1.8–3.8) | <.001 |\n| Private insurance | — | 18.1 (13.7–23.5) | <.001 | <.001 | — | 78.8 (75.8–81.4) | <.001 |\n| TRICARE/CHAMPVA d | — | 3.1 (15.1–6.2) | <.001 | <.001 | — | 1.9 (1.3–2.9) | <.001 |\n| Other public insurance | — | 1.0 (0.4–2.4) | <.001 | <.001 | — | 0.8 (0.4–1.7) | <.001 |\n| **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** | **Diabetes and associated conditions** |\n| Age of onset, y | 40.3 (38.0–42.6) | 39.6 (37.9–41.2) | .58 | .58 | 41.8 (39.4–44.1) | 41.9 (40.8–42.9) | .95 |\n| Years since diagnosis | 9.5 (7.6–11.5) | 11.1 (9.8–12.3) | .14 | .14 | 10.2 (8.2–12.2) | 10.5 (9.6–11.4) | .82 |\n| Kidney problems | 12.9 (7.4–18.3) | 15.0 (11.2–18.8) | .48 | .48 | 12.5 (6.8–18.2) | 7.9 (6.2–9.7) | .12 |\n| Eye problems | 24.9 (17.6–32.2) | 28.8 (24.1–33.5) | .38 | .38 | 20.8 (13.8–27.8) | 16.7 (14.0–19.3) | .26 |\n| **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** | **Physical health and comorbidities** |\n| BMI (kg/m 2 ) | 32.7 (31.4–33.9) | 33.8 (33.0–34.7) | .14 | .14 | 31.5 (30.4–32.6) | 33.5 (32.8–34.2) | .002 |\n| BMI overweight (25.0–29.9) | 28.5 (21.7–36.3) | 22.4 (18.3–27.1) | .17 | .17 | 33.8 (26.8–41.5) | 24.7 (21.8–28.0) | .02 |\n| BMI obese I (30–34.9) | 26.5 (20.5–33.5) | 26.9 (22.5–31.9) | .91 | .91 | 29.7 (23.5–36.8) | 28.7 (25.9–31.7) | .78 |\n| BMI obese II/III e (≥35.0)) | 34.4 (27.0–42.5) | 38.9 (33.4–44.8) | .34 | .34 | 24.3 (17.8–32.4) | 35.4 (31.7–39.3) | .01 |\n| High blood pressure/hypertension | 72.0 (65.1–78.9) | 76.3 (71.8–80.7) | .31 | .31 | 72.9 (65.8–80.0) | 68.7 (65.4–71.9) | .27 |\n| Coronary heart disease, angina, acute myocardial infarction, other heart disease | 20.0 (12.9–27.1) | 32.2 (26.9–37.4) | .007 | .007 | 22.0 (15.0–29.0) | 21.9 (18.8–25.0) | .99 |\n| Stroke | 4.5 (0.6–9.7) | 13.0 (9.6–16.4) | .007 | .007 | 7.7 (2.5–12.8) | 6.2 (4.7–7.7) | .60 |\n| Emphysema | 1.2 (0.0–2.6) | 6.2 (4.7–7.7) | .002 | .002 | 1.8 (0.0–4.1) | 1.8 (1.0–2.6) | .97 |\n| Chronic bronchitis | 4.5 (1.5–7.6) | 12.4 (8.1–16.7) | .004 | .004 | 8.0 (2.9–13.1) | 5.5 (3.7–7.2) | .36 |\n| High blood cholesterol | 60.5 (52.1–69.0) | 68.8 (64.3–73.3) | .09 | .09 | 53.8 (44.5–63.1) | 67.3 (64.3–70.4) | .008 |\n| Cancer | 7.5 (3.5–11.4) | 11.6 (8.6–14.6) | .10 | .10 | 9.5 (2.4–16.6) | 13.0 (10.3–15.7) | .39 |\n| Joint pain | 57.2 (49.6–64.9) | 71.7 (67.4–75.9) | .001 | .001 | 60.0 (51.6–68.3) | 64.2 (60.7–67.6) | .36 |\n| Arthritis | 36.5 (29.0–44.0) | 54.6 (48.7–60.5) | .001 | .001 | 30.0 (21.9–38.2) | 39.8 (36.1–43.4) | .04 |\n| Asthma | 9.9 (5.3–14.5) | 26.5 (21.3–31.7) | <.001 | <.001 | 10.1 (4.4–15.8) | 11.6 (9.7–13.6) | .62 |\n| Self-rated general health as fair or poor versus excellent, very good, or good | 56.9 (48.9–64.8) | 59.2 (53.8–64.6) | .64 | .64 | 40.0 (32.5–47.6) | 30.2 (27.2–33.2) | .02 |\n\n【37】Abbreviations: BMI, body mass index; CI, confidence interval.  \na  Values are percentages unless otherwise noted.  \nb  Tests of insurance status are between >138% federal poverty level insured and ≤138% federal poverty level. All other tests are by insurance status within income groups. All results were weighted to be nationally representative.  \nc  Tests of significance were computed by using survey-specific _t_ tests for continuous variables, proportions tests for binary variables, and χ 2  tests for proportions.  \nd  TRICARE/CHAMPVA is coverage for military families and dependents through the TRICARE system and Civilian Health and Medical Program of the Department of Veterans Affairs (CHAMPVA).  \ne  Obese class II = BMI 35.0–39.9; obese class III = BMI ≥40.\n\n【38】#####  Table 2. Health Care Access, Use, and Expenditures of US Adults Aged 19 to 64 Years With Diabetes, Medical Expenditure Panel Survey 2011 and\n\n| Measure a | ≤138% Federal Poverty Level | \\>138% Federal Poverty Level |\n| --- | --- | --- |\n| Uninsured (95% CI) | Insured (95% CI) | _P_ Value b | Uninsured (95% CI) | Insured (95% CI) | _P_ Value b |\n| --- | --- | --- | --- | --- | --- |\n| **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** | **Access to and use of health care in 2011 or 2012** |\n| Had a usual source of care | 69.0 (60.6–77.5) | 89.5 (86.3–92.7) | <.001 | 77.1 (71.0–83.1) | 94.6 (93.1–96.1) | <.001 |\n| Unable to access necessary medical care | 18.5 (12.7–24.4) | 6.5 (4.1–8.8) | <.001 | 13.8 (7.9–19.6) | 2.5 (1.6–3.5) | <.001 |\n| Unable to get necessary prescription medications | 17.6 (11.1–24.2) | 6.4 (3.9–8.9) | .002 | 9.0 (3.9–14.1) | 4.0 (2.7–5.3) | .06 |\n| HbA1c test | 83.4 (76.4–91.3) | 95.6 (93.2–98.0) | .003 | 86.6 (79.2–94.0) | 97.5 (96.5–98.6) | .004 |\n| Feet checked | 48.5 (38.9–58.0) | 67.0 (62.7–71.3) | <.001 | 49.9 (40.1–59.7) | 69.6 (66.6–72.7) | <.001 |\n| Eye examination | 35.9 (27.5–44.2) | 58.1 (52.7–63.3) | <.001 | 38.8 (30.2–47.4) | 65.6 (62.1–69.1) | <.001 |\n| Cholesterol check | 66.2 (58.5–73.9) | 77.4 (73.2–81.6) | .015 | 69.5 (62.4–76.6) | 88.1 (85.8–90.2) | <.001 |\n| Influenza vaccine | 32.4 (25.3–39.6) | 57.2 (51.4–62.9) | <.001 | 37.8 (28.1–47.5) | 61.8 (58.6–64.9) | <.001 |\n| Blood pressure check | 84.4 (79.3–89.4) | 96.5 (94.4–98.6) | <.001 | 90.3( 86.7–94.0) | 98.1 (97.4–98.8) | <.001 |\n| Office visits, mean no. | 4.1 (3.0–5.2) | 11.0 (8.8–13.1) | <.001 | 4.6 (3.5–5.6) | 8.8 (7.9–9.7) | <.001 |\n| Office visits, median no. | 2 | 5 | <.001 | 3 | 5 | <.001 |\n| Total prescription fills in year, mean | 23.4 (18.7–28.0) | 51.3 (43.9–58.7) | <.001 | 23.5 (18.4–28.6) | 36.2 (34.0–38.6) | <.001 |\n| Total prescription fills in year, median | 17 | 37 | <.001 | 15 | 25 | <.001 |\n| Any emergency department visit | 19.3 (14.7–24.0) | 33.3 (28.3–38.2) | <.001 | 22.9 (16.4–29.4) | 21.9 (19.3–24.6) | .79 |\n| Any inpatient nights | 8.9 (5.2–12.6) | 25.6 (20.8–30.3) | <.001 | 10.9 (5.7–16.1) | 14.7 (12.3–17.1) | .22 |\n| **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** | **Expenditures in 2011 or 2012, $ c** |\n| Total, mean | 4,367  | 13,706  | <.001 | 4,419  | 10,838  | <.001 |\n| Total, median | 1,297 | 6,382 | <.001 | 1,483 | 4,767 | <.001 |\n| Out-of-pocket, mean | 1,177  | 755  | .021 | 1,490  | 1,288  | .43 |\n| Out-of-pocket, median | 432 | 225 | .005 | 795 | 808 | .09 |\n| Prescription drugs and diabetes supplies, mean | 1,194  | 4,296  | <.001 | 1,492  | 3,414  | <.001 |\n| Prescription drugs and diabetes supplies, median | 355 | 1,894 | <.001 | 346 | 1,744 | <.001 |\n\n【40】Abbreviation: CI, confidence interval.  \na  Values are percentages unless otherwise stated.  \nb  Tests of significance were computed using survey-specific _t_ tests for continuous variables, proportions tests for binary variables, and nonparametric _k_ \\-sample tests for medians. All results were weighted to be nationally representative.  \nc  Expenditures are as reported in current-year dollars. Measures refer to the full calendar year for respondents in either the 2011 or 2012 household component full-year consolidated Medical Expenditure Panel Survey data files. Samples were pooled as described in the methods section.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c4f7d40f-85a5-4982-a448-55ca8a959336", "title": "Indoor and Outdoor Rodent Hosts of Orientia tsutsugamushi, Shandong Province, China", "text": "【0】Indoor and Outdoor Rodent Hosts of Orientia tsutsugamushi, Shandong Province, China\nScrub typhus is an emerging infectious disease caused by _Orientia tsutsugamushi_ , which is transmitted through the bites of infected chiggers, the larvae of trombiculid mites of the genus _Leptotrombidium_ . Scrub typhus has been documented in southern China for thousands of years  and emerged in northern China during the 1990s . Several studies have investigated the animal hosts of _O. tsutsugamushi_ , but the major hosts and seasonality of _O. tsutsugamushi_ in northern China remain unclear. We collected small animals in Qingdao, a city in eastern China, to investigate the hosts and seasonality of _O. tsutsugamushi_ .\n\n【1】During December 2012–July 2016, we used indoor and outdoor mousetraps to capture 162 small mammals (154 rodents and 8 shrews) in 2 villages in Huangdao District, Qingdao (119°30′–121°00′E, 35°35′–37°09′N) . All animal samples were obtained in accordance with the Implementation Regulations of the People’s Republic of China on the Protection of Terrestrial Wild Animals . The collection of rodents for microbiological studies was approved by the Ethics Committee of Prevention Medicine of Shandong University .\n\n【2】We classified samples by morphologic characteristics. We captured all 7 _Cricetulus barabensis_ hamsters, 18 _Tscherskia triton_ hamsters, and 8 _Niviventer confucianus_ rats in the fields, as well as 98.6% (69/70) of _Apodemus agrarius_ mice. We captured most _Rattus norvegicus_ rats (22/24; 92%) and _Mus musculus_ mice (18/27; 67%) in indoor settings, as well as 25% (2/8) of _Crocidura lasiura_ shrews.\n\n【3】We extracted and screened DNA from rodent spleens for _O. tsutsugamushi_ by nested PCR selective for the 56-kDa type-specific antigen gene with outer primers (5′- TCAAGCTTATTGCTAGTGCAATGTCTGC-3′ and 5′-AGGGATCCCTGCTGCTGTGCTTGCTGCG-3′) and inner primers (5′-GATCAAGCTTCCTCAGCCTACTATAATGCC-3′ and 5′-CTAGGGATCCCGACAGATGCACTATTAGGC-3′) . Overall, 4.5% of 154 rodents but none of the 8 shrews were positive for _O. tsutsugamushi_ .\n\n【4】All infected rodents were captured during autumn (i.e. September–November); among rodents captured in autumn, the infection rate was 8.1% (7/86; p>0.05 by 1-sided Fisher exact test). None of the 68 rodents captured during spring, summer, and winter tested positive for _O. tsutsugamushi_ . The absence of _O. tsutsugamushi_ infection among rodents collected during spring, summer, and winter indicated that these rodents were not reservoirs but temporary amplifying hosts for _O. tsutsugamushi_ . The presence of _O. tsutsugamushi_ among rodents during autumn months is consistent with the seasonality of scrub typhus among patients in Shandong Province. Among humans, _O. tsutsugamushi_ infections occur during September–December and peak in October , in alignment with the reproductive season of _Leptotrombidium scutellare_ mites .\n\n【5】Overall, we found that 4.3% of _A. agrarius_ mice, 7.4% of _M. musculus_ mice, 12.5% of _N. confucianus_ rats, and 14.3% of _C. barabensis_ hamsters tested positive for _O. tsutsugamushi_ ; these results indicate the potential role of these rodents as animal hosts for _O. tsutsugamushi_ . None of the 24 _R. norvegicus_ rats, 18 _T. triton_ hamsters, or 8 _C. lasiura_ shrews tested positive, suggesting that these animals are not major hosts for _O. tsutsugamushi_ . _R. norvegicus_ rats, which were mainly captured indoors, were all negative for _O. tsutsugamushi_ ; these findings suggest that these rats might primarily stay indoors, thereby avoiding exposure to chiggers in the fields. We found 2 (11.1%) _O. tsutsugamushi_ –positive indoor house mice, possibly reflecting their travels between house and field.\n\n【6】The sequences of _O. tsutsugamushi_ from rodents identified belonged to 2 lineages, Kawasaki and STA-07 . We identified the Kawasaki strain in 4 rodent species collected in the same village during the autumns of 2013 and 2014 and the STA-07 strain in _A. agrarius_ mice in a village 20 km away during the autumn of 2014. These results suggest that _O. tsutsugamushi_ isolates from same geographic area are highly homologous regardless of host species. We deposited the 56-kDa type-specific antigen gene sequences obtained in this study in GenBank (accession nos. MT833389–95).\n\n【7】In conclusion, we documented _O. tsutsugamushi_ infection among outdoor _A. agrarius_ mice, _N. confucianus_ rats, and _C. barabensis_ hamsters, as well as indoor _M. musculus_ mice, in Shandong Province; these rodents might serve as animal hosts for _O. tsutsugamushi_ . The finding of _O. tsutsugamushi_ infection among indoor mice suggest that persons might be exposed to chiggers and _O. tsutsugamushi_ at home. Further study is needed to investigate whether scrub typhus patients in the area had a history of working or traveling in the fields and whether their houses were infested with mice and chiggers. Our results indicate that physicians should be attentive to patients who might have _O. tsutsugamushi_ infection, even if those patients have not worked in the field.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "33e58399-9951-43a7-a866-ef7a3dfa689f", "title": "Citywide Integrated Aedes aegypti Mosquito Surveillance as Early Warning System for Arbovirus Transmission, Brazil", "text": "【0】Citywide Integrated Aedes aegypti Mosquito Surveillance as Early Warning System for Arbovirus Transmission, Brazil\nThe _Aedes aegypti_ mosquito is the primary vector of arboviruses such as dengue (DENV), Zika (ZIKV), chikungunya (CHIKV), and yellow fever. This mosquito species is common in urbanized areas in the tropics because it is highly adapted to live in close association with humans, preferentially feeding on blood of human hosts and laying eggs in containers located around human dwellings . Estimates indicate that ≈3 billion persons live in areas with ongoing DENV transmission .\n\n【1】Traditional entomologic surveillance for _Ae. aegypti_ mosquitoes is based on periodic inspections of larvae and pupae in domestic breeding sites, which provide measures of infestation known as the house index (HI), the percentage of houses in which \\> 1 larvae or pupae was collected, and Breteau index (BI), the number of containers positive for larvae or pupae divided by the number of inspected houses. By using available infestation data, public health managers intensify control strategies in the areas with higher indices. Of note, indices based on collection of immature mosquitoes face many criticisms because surveys are costly to perform with the frequency required for adequate surveillance; indices are highly dependent on the agent’s motivation to effectively search for larvae in myriad container types, including cryptic and hard-to-access containers; surveys do not consider container productivity (i.e. these surveys might only provide measures of presence or absence immature mosquitoes); and larval density has proven to be a poor indicator of adult mosquito density .\n\n【2】Traps capturing adult mosquitoes could be a promising alternative to larval surveys because they sample the vector life stage that is directly responsible for transmission and provide qualitative (percent positive traps) and quantitative (number of captured mosquitoes per trap) indices . Adult traps provide relative measurements of the vector population, expressed in units of mosquitoes by area, mosquitoes per person, or mosquitoes per trap . Therefore, adopting of adult traps in an arbovirus-endemic setting likely would provide relevant information regarding the spatiotemporal dynamics of _Ae. aegypti_ mosquitoes.\n\n【3】Effective arbovirus surveillance should be able to accurately predict when and where an outbreak will occur. Routine virologic surveillance in field-caught _Ae. aegypti_ mosquitoes, (entomo-virologic surveillance) is one measure that could be adopted to enhance surveillance effectiveness . In DENV-endemic settings of developing countries, screening for natural infection in field-caught _Ae. aegypti_ mosquitoes has been performed in various situations but rarely as a component of a long-term routine surveillance to direct control interventions to critical areas . By adding entomo-virologic surveillance to routine surveillance based on large-scale adult mosquito trapping across an entire city, health managers ideally would be able to identify hotspots of disease transmission and intensify vector control in those regions before human cases arose .\n\n【4】We report on a 4-year integrated citywide vector surveillance approach that involved extensive use of adult mosquito traps, molecular diagnostic testing for natural arbovirus infection in live collected mosquito specimens, construction of transmission risk maps, and performance of timely vector control intervention < 48 h after mosquito collection. In this scheme, vector control was intensified in areas with higher risk for transmission instead of maintaining homogeneous vector control efforts over the landscape. In addition, we evaluated the correspondence of larval- and adult-based indices with the epidemiologic trend in the city of Foz do Iguaçu, Brazil, during 2017–2020.\n\n【5】### Methods\n\n【6】##### Study Site\n\n【7】We implemented an entomo-virologic surveillance system in the city of Foz do Iguaçu (25°30′58″S, 54°35′07″W), Brazil, which is located on the triple border with Argentina and Paraguay. Foz do Iguaçu has ≈250,000 inhabitants and an intense daily population movement across the 3 countries’ border cities. Foz do Iguaçu is divided into 73 urban areas of ≈1,500 premises each , plus 3 rural areas that were not included in this study. We defined premises as a property occupied by a residence or a business at ground level. According to the Brazil Ministry of Health, apartment buildings are not included, and surveillance and vector control interventions take place only at the foyer. The climate in Foz do Iguaçu is classified as humid tropical, according to the Köppen-Geiger system, and is characterized by hot and humid summers (mean temperature >27°C) and cold to mild winters (mean temperature <15°C), with an annual rainfall >1,850 mm.\n\n【8】##### Adult Mosquito Collection\n\n【9】During January 2017–December 2020, a total of 3,476 Adultraps  were installed in the city, and 1 trap could be found in the peridomestic environment for every 25 premises. This system was originally designed to capture gravid _Ae. aegypti_ female mosquitoes during oviposition because Adultraps use water as the principal attractant. These traps have an opening on the top where females enter, then are trapped in an interior chamber . Water remains confined in a compartment at the bottom of the trap that the mosquitoes cannot access, thus deterring egg laying. Local health agents visit all Adultraps every 2 months, within the first 4 days of the first week of odd months, when agents usually conduct larval surveys as part of traditional entomologic index. Therefore, during the study period, the 3,476 Adultraps were inspected 24 times in the same premises, a total of 83,424 trap inspections.\n\n【10】##### Entomologic Indices\n\n【11】Besides the traditional HI and BI based on larval surveys, the Adultrap inspections produced 3 entomologic indices based on adult collections. The trap positivity index (TPI) is the number of positive traps among the total number of traps inspected multiplied by 100; the adult density index (ADI) is the total number of _Ae. aegypti_ mosquitoes captured divided by the total number of inspected traps multiplied by 100; and the mosquitoes per inhabitant index (MII) is the total number of adult _Ae. aegypti_ mosquitoes collected, divided by the number of persons in each house with an Adultrap multiplied by 1,000. We calculated all entomologic indices every 2 months during 2017–2020, a total of 24 observations per index.\n\n【12】##### Entomo-Virologic Screening\n\n【13】Mosquitoes collected alive during each 2-month period were sent to the entomology laboratory for further taxonomic identification by using appropriate keys. Mosquitoes classified as _Ae. aegypti_ were placed in cryogenic tubes for diagnosis of arbovirus infection by quantitative real-time PCR (qPCR). Depending on the number of mosquitoes captured in traps on the same city block, we pooled < 10 mosquitoes per block, separating male from female mosquitoes. We calculated minimum infection rate (MIR) by dividing the number of positive pools by the total specimens tested, then multiplied by 1,000 . To estimate MIR, we used only data from DENV-positive pools because only a few pools were positive for ZIKV or CHIKV.\n\n【14】##### RNA Extraction and Real-Time qPCR\n\n【15】We extracted viral RNA from _Ae. aegypti_ mosquitoes by using the MagMAX Viral/Pathogen Nucleic Acid Ultra Isolation KIT , according to the manufacturer’s instructions. We added single or pooled mosquitoes to electromagnetic mixing beads (MagMAX Viral/Pathogen Binding Beads; Applied Biosystems) and macerated by using TissueLyser II . After RNA extraction, we separated an aliquot of 2 μL from each sample and used this to read the concentration of viral RNA recovered in a NanoDrop One C  Spectrophotometer (Thermo Fisher Scientific).\n\n【16】For arboviral genome amplification, we used the ZDC Biomol Kit  , which enables identification of ZIKV, CHIKV, and differentiation of DENV serotypes with an internal control (IC) of the reaction that uses probes specific to each molecular target. We used a 96-well QuantStudio 7 Flex Real-Time PCR System (Applied Biosystems) for PCR and analyzed results by using QuantStudioDesign and Analysis Software versions 1.3.1 and 1.5.1 (Applied Biosystems). We considered samples positive when the amplification plot curve exceeded the specific threshold for each target < 35 cycle threshold.\n\n【17】##### Epidemiologic Surveillance and Case Report\n\n【18】The health system in Foz do Iguaçu is composed of 30 basic health units, including 2 emergency care units, 3 private hospitals, and 1 public hospital. The Ministry of Health lists dengue, Zika, and chikungunya as diseases of compulsory notification that can be registered in any of the local health facilities. Epidemiologic surveillance for arboviruses is carried out passively after symptomatic persons seek care in the city health system. Zika and chikungunya cases were reported in the city before 2017, but no further large outbreaks were reported in Foz do Iguaçu. Thus, we restricted our analysis to suspected dengue cases reported during 2017–2020 . A suspected dengue case was reported whenever any person residing in Foz do Iguaçu received a clinical diagnosis of \\> 1 compatible dengue symptom, including fever, headache, myalgia, arthralgia, rash, nausea, retro-orbital pain, petechiae, or malaise, in the previous 14 days. Among suspected dengue cases, 33.2% were confirmed through laboratory diagnosis.\n\n【19】##### Geographic Information and Choropleth Maps\n\n【20】We recorded and stored all entomologic and epidemiologic field-derived data, such as the location and trapping history of the 3,476 adult mosquito traps, along with geocoded residential address of suspected dengue patients, in a single database . We used PostgreSQL version 9.5.7  for data storage and Quantum GIS version 3.10.2  to produce maps. We used geoprocessed information and Power BI version 2.85.985.0  to generate reports, graphs, and maps.\n\n【21】We created choropleth maps to help visualize the _Ae. aegypti_ mosquito population among the 73 areas of the city. We built the choropleth maps in accordance with guidelines provided by the Brazil Ministry of Health, which classifies HI < 1.0 as a low risk for dengue transmission, HI from 1.1–4.0 as a moderate risk, and HI \\> 4.0 a high risk.\n\n【22】##### Statistical Analysis\n\n【23】We evaluated the predictive ability of the entomologic indices by using 5 scenarios comprising comparison of dengue incidence (notifications per 100,000 inhabitants) in the same week and in 2, 4, 6, and 8 weeks after the entomologic surveys. We assessed each scenario in each index by using generalized linear mixed models (GLMM) with temporal pseudoreplication . In these models, we included HI, BI, TPI, ADI, and MII as the fixed effect in the explanatory variable indices of larvae and adult mosquitoes, and the incidence of dengue as the response variable. The continuous random-effect structure included each resampling date with 23 levels in each of the 73 areas of Foz do Iguaçu . We assumed a Gaussian distribution for the continuous response variable and implemented the models in the lme4 package in R . We obtained significance values for fixed effects in the ImerTest software package (R Foundation for Statistical Computing). We chose the best scenario by using Akaike information criteria (AIC) to rank models (ΔAICc), and calculated Akaike weights (wAICc) to evaluate the relative support of each model . We used ΔAICc to evaluate the differences in AIC score between the best model and the other models. We used Akaike weights to evaluate model selection uncertainty, which quantified the probability that the model was the best among those considered based on the data . We selected the best supported model based on rejection of GLMM null hypothesis (p<0.05), the lower AIC value, and an AIC weight >0.7 (70% confidence set) . We considered models with ΔAICc of <4.0 to have no differences . We implemented the ΔAICc and wAICc in the bbmle package (R Foundation for Statistical Computing).\n\n【24】### Results\n\n【25】##### Entomologic Survey of Larvae\n\n【26】In each 2-month period, an average of ≈4,883 (range 4,781–5,021) premises were inspected, which correspond to ≈6.25% of houses in Foz do Iguaçu. During 2017–2020, a range of 3.5%–17.7% of inspected Adultraps were positive for mosquitoes, and an average of 9.5% of traps had \\> 1 _Ae. aegypti_ mosquito. We used the number of positive houses to create HI and the number of containers to create BI and observed strong seasonal variation; values were 7 times higher during the wet summer (November–March) than in the dry winter (July–September) . The average HI of Foz do Iguaçu was 2.58% during the 24 observations of 2017–2020, and only twice was HI above the 4% alert level adopted by Ministry of Health, reaching 5.41% in March 2019 and 5.29% in May 2019.\n\n【27】We used the number of positive houses to estimate HI and number of positive breeding sites in each of larval survey to estimate traditional BI . Indices based on larval surveys showed an expected seasonal variation with higher values during the rainy summer (≈November–March), but HI and BI fluctuations were only partially in accordance with the dengue notification curve .\n\n【28】##### Entomologic Survey for Adult Mosquitoes\n\n【29】Adult _Aedes aegypti_ mosquitoes were collected on the same premises where larval surveys were performed. The average number of inspected traps was 2,468 (range 2,239–2,767). Therefore, a mean of 73% of adult traps were inspected bimonthly. A total of 11,962 adult _Ae. aegypti_ mosquitoes were captured in the adult traps, showing a massive predominance of female mosquitoes, 95.4% of all captured insects .\n\n【30】In contrast to the indices based on larval surveys, indices based on adult capturing corresponded more closely to the dengue notification curve . Ultimately, we observed high infestation levels based on adult indices in Foz do Iguaçu that aligned with dengue notification.\n\n【31】##### Entomo-Virologic Survey\n\n【32】Of the 11,962 adult _Ae. aegypti_ mosquitoes trapped during 2017–2020, a total of 1,563 (13.1%) were captured alive. In addition, 1,459 (93.3%) were screened for arbovirus infection through real-time qPCR. Subsequently, mosquitoes were screened for infection in 20/24 months of thorough monitoring that summed up 221 pools . From the 221 pools tested, 29 (13.1%) were positive for arboviruses, among which 22 (75.9%) pools were positive for DENV, 3 (10.3%) for ZIKV, and 4 (13.8%) for CHIKV. The average MIR for DENV was 42.6 (range 19.6–75.0), and MIR peaked in March 2020. The entomo-virologic results of natural DENV, ZIKV, and CHIKV infection in field-caught mosquitoes was available < 36 hours after Adultrap inspection.\n\n【33】##### Dengue Prediction of Entomologic Indices\n\n【34】All entomologic indices based on adult sampling (TPI, ADI, and MII) showed a statistically significant relationship with dengue incidence in Foz do Iguaçu during 2017–2020 . Indices based on larval surveys had limited statistically significant relationships with dengue incidence in the same week for both BI and HI and for BI after 2 weeks. Of note, indices based on adult trapping best predicted the incidence of dengue after 4 weeks, with emphasis on ADI and MII . Adult indices showed a stronger prediction of future dengue incidence than traditional larval surveys indices on the basis of GLMM results for each index in the 5 scenarios . Of note, coefficients for HI and BI were negative in most scenarios whereas positive coefficients were observed for TPI, ADI, and MII.\n\n【35】##### Choropleth Maps\n\n【36】We constructed maps for the 5 entomologic indices in each of the 24 months of collection during 2017–2020. We selected January 2019 to illustrate the differences between maps based on larval surveys (HI) from the one using adult trapping (TPI), and considered HI and TPI analogous . Of note, January 2019 marked the initial rise in dengue cases in the city, which peaked during March–May 2019, the moment in which a sensitive tool could foresee an increase in dengue transmission. In January 2019, HI classified 52 (71.2%) areas as being low and moderate risk for dengue transmission, whereas TPI estimated 25 (34.2%) areas under the same risk. The relative frequency of high-risk areas before the start of a dengue outbreak increased from 28.7% when measured by HI to 65.8% when measured by TPI .\n\n【37】### Discussion\n\n【38】Epidemiologic surveillance is defined as the systematic collection, analysis, and interpretation of determinants of disease activity to support the planning and implementation of further actions to mitigate disease burden. Systematic literature reviews have stressed a general lack of evidence for the usefulness of arboviral surveillance for early outbreak detection and emphasized the lack of indicators and alert signals to trigger response . When most arbovirus-endemic countries rely on passive surveillance with clinical but few laboratory diagnostics to confirm infection, epidemiologic data frequently are not able to provide a sensitive alert signal before an outbreak takes place. We report on implementation of a citywide integrated surveillance system using entomologic, epidemiologic, and entomo-virologic data gathered during a 4-year period. The extensive fieldwork provided a large dataset and enabled robust analysis. The entomologic indices based on adult trapping provided a more reliable alert signal of dengue outbreaks than widespread traditional indices based on larval surveys.\n\n【39】Dengue entomologic surveillance using larval inspections is a time- and resource-consuming activity widely used in many tropical countries . Larval surveys can identify key containers but often fail to provide fast or localized measurements of mosquito abundance. In this context, adopting adult mosquito traps as a complementary approach can improve dengue vector surveillance by providing information previously unknown in larval surveys, such as adult female mosquito abundance . Traditional larval surveillance indices often fail to demonstrate a strong correlation with adult mosquito density and dengue transmission. Thus, developing other indices to serve as indicators of an imminent dengue outbreak should be encouraged . Of note, TPI, ADI, and MII were developed by using Adultrap and thus should not be seen as universal adult indices. Although specific for Adultrap, our results highlight that analogous approaches, such as extensive time series data, traditional entomologic data, and high cover in a city, should be pursued by using other traps .\n\n【40】Ultimately, even though 73 areas of the city were tested, the HI rarely reached values above the 4% alert threshold; the only exceptions were in March 2019 (5.41%) and May 2019 (5.29%). Furthermore, the most intense dengue transmission peak in Foz do Iguaçu was recorded during January–May 2020. In this window, we had 3 HI estimates, 3.21 in January, 1.32 in March, and 0.7 in May. We observed the same pattern of poor correlation with local dengue transmission for BI, evincing criticisms directed to larval surveys as both entomologic and epidemiologic indicators. Instead, indices based on adult trapping showed low variation during the nonendemic years of 2017–2018 and peaked accordingly in the 2019 and 2020 dengue seasons.\n\n【41】Standard larval surveys were not sufficient to issue proper alerts in Foz do Iguaçu. By comparison, Adultraps detected increased mosquito infestation during the dengue transmission seasons, indicating the system’s ability to detect mosquito density variation and thus the likelihood of generating indices that could be used as part of an early warning system to trigger vector control response. The greater sensitivity of traps to mosquito density variation is probably because they can cover \\> 1 premises, whereas surveys of immature mosquitoes only encompass those houses included in the sample .\n\n【42】Comparing the predictive ability of traditional versus adult indices revealed that indices based on adult trapping consistently performed much better than indices based on larval surveys. In fact, we observed negative GLMM coefficients for HI and BI but saw positive estimates for indices based on adult trapping. In addition, MII, ADI, and TPI performed better as predictive indicators of dengue outbreaks 4 weeks after the trapping period. Therefore, local health managers would have ≈1 month after estimating the index values to promote and intensify vector control in areas with higher risk on a choropleth map. In addition, health managers could create additional criteria to prioritize areas for vector control in case the cost to cover all high-risk areas of a city becomes too expensive to be covered by health agencies.\n\n【43】One criterion that could be used to prioritize areas is the occurrence of _Ae. aegypti_ female mosquitoes naturally infected with DENV, ZIKV, or CHIKV. In Foz do Iguaçu, inspection of the 3,476 Adultraps took 4 days, and real-time qPCR results were available, on average, 36 h after all Adultraps were inspected and live mosquitoes collected. Thus, within 5 days of starting trap inspection, additional entomologic information, such as geographic position of traps, the infestation index, and the choropleth maps, were made available for local health managers. In the early hours of the next business day, the local health manager could meet with field supervisors to decide which area to prioritize and which vector control activities to perform considering local contexts . Therefore, a week after the start of Adultrap inspection, the dengue transmission risk among the 73 areas of Foz do Iguaçu would be known by the local health managers, triggering vector control interventions in prioritized areas.\n\n【44】In conclusion, traditional entomologic indices have shown a poor relationship with dengue transmission, if any . We conducted a 4-year citywide study to deepen the entomologic and epidemiologic features of dengue transmission in Foz do Iguaçu by focusing on developing indicators based on adult mosquito trapping. We demonstrated the process we used to develop the 3 adult trapping indices, all of which have a higher prediction behavior to foresee dengue outbreaks than the widely adopted traditional larval survey indices. Our proposed surveillance system can predict a dengue outbreak with high accuracy, and indices based on adult trapping are able to predict a dengue outbreak 4 weeks after DENV detection in adult mosquitoes. In addition, adoption of easily accessible technological resources makes it possible for the model to be replicated to other localities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5b82d88-0d6c-4e5f-a578-3d8e08fad5b2", "title": "Improving Adherence to Hand Hygiene Practice: A Multidisciplinary Approach", "text": "【0】Improving Adherence to Hand Hygiene Practice: A Multidisciplinary Approach\nHand hygiene is the simplest, most effective measure for preventing nosocomial infections . Despite advances in infection control and hospital epidemiology, Semmelweis' message is not consistently translated into clinical practice , and health-care workers' adherence to recommended hand hygiene practices is unacceptably low . Average compliance with hand hygiene recommendations varies between hospital wards, among professional categories of health-care workers, and according to working conditions, as well as according to the definitions used in different studies. Compliance is usually estimated as <50% .\n\n【1】Promotion of hand hygiene is a major challenge for infection control experts . In-service education, distribution of information leaflets, workshops and lectures, and performance feedback on compliance rates have been associated with transient improvement . No single intervention has consistently improved compliance with hand hygiene practices . This review summarizes factors influencing lack of adherence by health-care personnel to hand hygiene procedures and suggests strategies for improvement.\n\n【2】### Definitions\n\n【3】Two major groups of microorganisms are found on the skin: organisms that normally reside on it (resident flora) and contaminants (transient flora) . Unless introduced into body tissues by trauma or medical devices such as intravenous catheters, the pathogenic potential of the resident flora is low . Transient flora, which are easily removed by handwashing, cause most hospital infections resulting from cross-transmission .\n\n【4】The term hand hygiene includes several actions intended to decrease colonization with transient flora. This objective can be achieved through handwashing or hand disinfection. Handwashing refers to washing hands with an unmedicated detergent and water or water alone. Its objective is to prevent cross-transmission by removing dirt and loose transient flora . Hygienic handwash refers to the same procedure when an antiseptic agent is added to the detergent. Hand disinfection refers to use of an antiseptic solution to clean hands, either medicated soap or alcohol. Some experts refer to the action of \"degerming\" as the use of detergent-based antiseptics or alcohol . Hygienic hand rub is rubbing hands with a small quantity (2 mL to 3 mL) of a highly effective, fast-acting antiseptic agent.\n\n【5】### Hand Hygiene Agents\n\n【6】If hands are known to be or suspected of being contaminated, transient flora must be eliminated by washing or disinfecting the hands to render them safe for the next patient contact. Plain soap with water can physically remove a certain level of microbes, but antiseptic agents are necessary to kill microorganisms . Hand antiseptic agents are designed to rapidly eliminate most transient flora by their mechanical detergent effect and to exert an additional sustained antimicrobial activity on remaining flora. The multiplication of resident flora may be retarded as well, so that hand disinfection may be useful in situations in which microbiologically clean hands are required for extended periods.\n\n【7】Rotter showed that hand hygiene with unmedicated soap and water removed some transient flora mechanically; preparations containing antiseptic or antimicrobial agents not only removed flora mechanically but also chemically killed contaminating and colonizing flora, with long-term residual activity . Alcohol-based preparations have more rapid action than products containing other antiseptics (e.g. chlorhexidine gluconate or providone iodine) .\n\n【8】Semmelweis observed that normal handwashing did not always prevent the spread of fatal infection  and recommended hand disinfection in a solution of chlorinated water before each vaginal examination. Hand disinfection is substantially more efficient than standard handwashing with soap and water or water alone , particularly when contamination is heavy . Frequent handwashing may result in minimal reduction or even an increase in bacterial yield over baseline counts of clean hands .\n\n【9】Because alcohols have excellent activity and the most rapid bactericidal action of all antiseptics, they are the preferred agents for hygienic hand rubs, so-called \"waterless hand disinfection.\" In addition, alcohols are more convenient than aqueous solutions for hygienic hand rubs because of their excellent spreading quality and rapid evaporation. At equal concentrations, n-propanol is the most effective alcohol and ethanol the least . Alcohol-based hand rubs are well suited for hygienic hand disinfection for the following reasons: optimal antimicrobial spectrum (active against all bacteria and most clinically important viruses, yeasts, and fungi); no wash basin necessary for use and easy availability at bedside; no microbial contamination of health-care workers' clothing; and rapidity of action. After extensive reduction following hand disinfection with an alcohol preparation, it takes the resident skin flora several hours to become completely restored . Since alcohol alone has no lasting effect, another compound with antiseptic activity may be added to the disinfection solution to prolong the effect. These antiseptics have recently been extensively reviewed by Rotter .\n\n【10】Prevention of bacterial contamination and subsequent infection requires timely hand cleansing. Guidelines have delineated indications for hand cleansing  but without reliance on evidence-based studies of microbiologic contamination acquired during routine patient care. To provide such evidence, we studied the dynamics of bacterial contamination of health-care workers' hands in daily hospital practice . Our findings should help identify patient-care situations associated with high contamination levels and improve hand cleansing practices.\n\n【11】Structured observations of patient care were conducted by trained external observers, who took an imprint of the fingertips of the health-care worker's dominant hand to quantify bacterial colony counts at the end of a defined period of patient care . Bacterial contamination on ungloved hands increased linearly during patient care (mean 16 CFU per minute, 95% confidence interval \\[CI\\] _11-21_ ). Activities independently associated with higher contamination levels were direct patient contact, respiratory care, handling body fluids, and disruption in the sequence of patient care (all p <0.05). Contamination levels varied according to hospital location, with the medical rehabilitation ward having the highest levels (>49 CFU, p = 0.03). Both the duration and type of patient care influenced hand contamination. Furthermore, simple handwashing before patient care, without hand disinfection, was also associated with higher colony counts (>52 CFU, p = 0.03), which suggests that hand antisepsis is better than standard handwashing. These findings suggested that intervention trials should explore the role of systematic hand disinfection as a cornerstone of infection control to reduce cross-transmission in hospitals.\n\n【12】### Factors Influencing Noncompliance with Hand Hygiene\n\n【13】Risk factors for noncompliance with hand hygiene have been determined objectively in several observational studies or interventions to improve compliance . Factors influencing reduced compliance, identified in observational studies of hand hygiene behavior, included being a physician or a nursing assistant rather than a nurse; being a nursing assistant rather than a nurse; being male; working in an intensive care unit (ICU); working during weekdays rather than the weekend; wearing gown and gloves; using an automated sink; performing activities with high risk for cross-transmission; and having many opportunities for hand hygiene per hour of patient care.\n\n【14】In the largest hospital-wide survey ever conducted , we also identified predictors of noncompliance with hand hygiene during routine patient care. Variables included professional category, hospital ward, time of day or week, and type and intensity of patient care, defined as the number of opportunities for hand hygiene per hour of patient care. In 2,834 observed opportunities for hand hygiene, average compliance was 48%. In multivariate analysis, compliance was highest during weekends and among nurses (odds ratio \\[OR\\] 0.6, 95% CI 0.4-0.8). Noncompliance was higher in ICUs than in internal medicine (OR 2.0, CI 1.3-3.1), during procedures with a high risk for bacterial contamination (OR 1.8, CI 1.4-2.4), and when intensity of patient care was high (21 to 40 opportunities \\[OR 1.3, CI 1.0-1.7\\], 41 to 60 opportunities \\[OR 2.1, CI 1.5-2.9\\], >60 opportunities \\[OR 2.1, CI9 1.3-3.5\\]) compared with a reference level of 0 to 20 opportunities. In other words, compliance with handwashing worsened when the demand for hand cleansing was high; on average, compliance decreased by 5% (±2%) per increment of 10 opportunities per hour when the intensity of patient care exceeded 10 opportunities per hour. Similarly, the lowest compliance rate (36%) was found in ICUs, where indications for handwashing were typically more frequent (on average, 20 opportunities per patient per hour). The highest compliance rate (59%) was observed in pediatrics, where the average activity index was low (on average, eight opportunities per patient per hour). This study confirmed modest levels of compliance with hand hygiene in a teaching institution and showed that compliance varied by hospital ward and type of health-care worker, thus suggesting that targeted educational programs may be useful. These results also suggested that full compliance with current guidelines may be unrealistic  and that facilitated access to hand hygiene could help improve compliance.\n\n【15】### Perceived Barriers to Hand Hygiene\n\n【16】Several barriers to appropriate hand hygiene have been reported . Reasons reported by health-care workers for the lack of adherence with recommendations include skin irritation, inaccessible supplies, interference with worker-patient relation, patient needs perceived as priority, wearing gloves, forgetfulness, ignorance of guidelines, insufficient time, high workload and understaffing, and lack of scientific information demonstrating impact of improved hand hygiene on hospital infection rates.\n\n【17】### Risk Factors for Noncompliance\n\n【18】Some of the perceived barriers for the lack of adherence with hand hygiene guidelines have been assessed or even quantified in observational studies . The most frequently reported reasons associated with poor compliance, in addition to those mentioned above, are inconveniently located or insufficient numbers of sinks; low risk for acquiring infection from patients; belief that glove use obviates need for hand hygiene; and ignorance of or disagreement with guidelines and protocols.\n\n【19】Skin irritation by hand hygiene agents is an important barrier to appropriate compliance . The superficial skin layers contain water to keep the skin soft and pliable and lipids to prevent dehydration of the corneocytes. Hand cleansing can increase skin pH, reduce lipid content, increase transepidermal water loss, and even increase microbial shedding. Soaps and detergents are damaging when applied to skin on a regular basis, and health-care workers need to be better informed about their effects. Lack of knowledge and education on this topic is a key barrier to motivation. Alcohol-based formulations for hand disinfection (whether isopropyl, ethyl, or n-propanol, in 60% to 90% vol/vol) are less irritating than antiseptic or nonantiseptic detergents. Alcohols with added emollients are at least as well tolerated and efficacious as detergents. Emollients are recommended and may protect against cross-infection by keeping the resident skin flora intact, and hand lotions help protect skin and may reduce microbial shedding .\n\n【20】The value of easy access to hand hygiene supplies, whether sink, soap, medicated detergent, or waterless alcohol-based hand rub solution, is self explanatory. Asking busy health-care workers to walk away from the patient bed to reach a wash basin or a hand antisepsis solution invites noncompliance with hand hygiene recommendations . Engineering controls could facilitate compliance, but hand hygiene behavior should be carefully monitored to identify negative effects of newly introduced devices .\n\n【21】Wearing gloves might represent a barrier for compliance with hand hygiene . Failure to remove gloves after patient contact or between dirty and clean body site care for the same patient constitutes noncompliance with hand hygiene recommendations . Washing and reusing gloves between patient contact is ineffective, and handwashing or disinfection should be strongly encouraged after glove removal. In a study involving artificial contamination, organisms were cultured from 4% to 100% of the gloves and observed counts were up to 4.7 log on hands after glove removal .\n\n【22】Additional barriers to hand hygiene compliance include lack of active participation in promotion at the individual or institutional level, of a role model for hand hygiene, of institutional priority assigned to hand hygiene, of administrative sanctions for noncompliance; and of an institutional climate encouraging safety . A system change may be necessary for improvement in hand hygiene practices by health-care workers.\n\n【23】### Impact of Improved Hand Hygiene\n\n【24】Lack of scientific information on the definitive impact of improved hand hygiene on hospital infection rates has been reported as a possible barrier to adherence with recommendations. Hospital infections have been recognized for more than a century as a critical problem affecting the quality of patient care provided in hospitals. Studies have shown that at least one third of all hospital infections are preventable . A substantial proportion of infections results from cross-contamination, and transmission of microorganisms by the hands of health-care workers is recognized as the main route of spread . Seven quasi-experimental hospital-based studies of the impact of hand hygiene on the risk of hospital infections were published from 1977 to 1995  . Despite limitations, most reports showed a temporal relation between improved hand hygiene practices and reduced infection rates.\n\n【25】We recently reported the results of a successful hospital-wide hand hygiene promotion campaign, with emphasis on hand disinfection, which resulted in sustained improvement in compliance associated with a significant reduction in hospital infections and methicilllin-resistant Staphylococcus aureus cross-transmission rates over a 4-year period . The beneficial effects of hand hygiene promotion on the risk of cross-transmission have also been reported in surveys conducted in schools, day-care centers , and a community . Although additional scientific and causal evidence is needed for the impact of improved hand hygiene on infection rates, these results indicate that improvement in behavior reduces the risk of transmission of infectious pathogens\n\n【26】### Improving Adherence with Practices\n\n【27】In 1998, Kretzer and Larson  revisited hand hygiene behavioral theories in an attempt to better understand how to target more successful interventions. These researchers proposed a hypothetical framework to enhance hand hygiene practices and stressed the importance of considering the complexity of individual and institutional factors in designing behavioral interventions. Behavioral theories and secondary interventions have primarily focused on the individual, which is insufficient to effect sustained change . Interventions aimed at improving compliance with hand hygiene must be based on the various levels of behavior interaction . Thus, the interdependence of individual factors, environmental constraints, and institutional climate should be considered in strategic planning and development of hand hygiene promotion campaigns. Factors associated with noncompliance with recommendations are related not only to the individual worker but also to the group to which he or she belongs and, by extension, to the parent institution. Factors influencing compliance at the group level include lack of education and performance feedback; working in critical care (high workload); downsizing and understaffing; and lack of encouragement or role models from key staff. Factors operating at the institutional level include lack of written guidelines; lack of appropriate hand hygiene agents; lack of skin care promotion and agents; lack of hand hygiene facilities; lack of atmosphere of compliance; and lack of administrative leadership, sanctions, rewards, and support. Interventions to promote hand hygiene in hospitals should take into account variables at all these levels.\n\n【28】The complex dynamic of behavioral change involves a combination of education, motivation, and system change. Various psychosocial parameters influencing hand hygiene behavior include intention, attitude toward the behavior, perceived social norms, perceived behavioral control, perceived risk of infection, habits of hand hygiene practices, perceived model roles, perceived knowledge, and motivation . Factors necessary for change include dissatisfaction with the current situation, perception of alternatives, and recognition, both at the individual and institutional level, of the ability and potential to change. While the latter implies education and motivation, the former two necessitate primarily a system change.\n\n【29】Among reasons reported for poor adherence with hand hygiene recommendations, some that are clearly related to the institution (i.e. the system) include lack of institutional priority for hand hygiene, need for administrative sanctions for noncompliance or rewards for compliance, and lack of an institutional climate that encourages safety. Whereas all three reasons would require a system change in most institutions, the last would also involve management commitment, visible safety programs, an acceptable level of work stress, a tolerant and supportive attitude toward reported problems, and belief in the efficacy of preventive strategies .\n\n【30】### Strategies for Improvement\n\n【31】Improvement in infection control practices requires questioning basic beliefs, continuous assessment of the stage of behavioral change, interventions with an appropriate process of change, and supporting individual and group creativity . Because of the complexity of the process of change, single interventions often fail, and a multimodal, multidisciplinary strategy is necessary.\n\n【32】A framework for change should include parameters to be considered for hand hygiene promotion, together with the level at which each change must be applied: education, motivation, or system . Some parameters are based on epidemiologic evidence and others on the authors' and other investigators' experience and review of current knowledge. Some parameters may be unnecessary in certain circumstances and helpful in others. In particular, changing the hand hygiene agent could be beneficial in institutions or hospital wards with a high workload and a high demand for hand hygiene when waterless hand rub is not available . However, a change in the recommended hand hygiene agent could be deleterious if introduced during winter, when skin is more easily irritated.\n\n【33】Several parameters that could potentially be associated with successful promotion of hand hygiene would require a system change . Enhancing individual and institutional self-efficacy (the judgment of one's capacity to organize and execute actions to reach the objective), obtaining active participation at both levels, and promoting an institutional safety climate represent major challenges that exceed the current perception of the infection control practitioner's role.\n\n【34】More research is needed to determine whether education, individual reinforcement technique, appropriate rewarding, administrative sanction, enhanced self-participation, active involvement of a larger number of organizational leaders, enhanced perception of health threat, self-efficacy, and perceived social pressure , or combinations of these factors would improve health-care workers' adherence to recommendations. Ultimately, compliance with hand hygiene could become part of a culture of patient safety in which a set of interdependent elements interact to achieve a shared objective .\n\n【35】More readily achievable than major system change, easy and timely access to hand hygiene in a timely fashion and the availability, free of charge, of skin care lotion both appear to be necessary prerequisites for appropriate hand hygiene behavior. In particular, in high-demand situations, such as in critical care units, in high-stress working conditions, and at times of overcrowding or understaffing, having health-care workers use a hand rub with an alcohol-based solution appears as the best method for achieving and maintaining a higher level of compliance with hand hygiene. Alcohol-based hand rub, compared with traditional handwashing with unmedicated soap and water or medicated hand antiseptic agents, may be better because it requires less time , acts faster , and irritates hands less often . This method was used in the only program that reported a sustained improvement in hand hygiene compliance associated with decreased infection rates .\n\n【36】Finally, strategies to improve compliance with hand hygiene practices should be multimodal and multidisciplinary . It is important to note, however, that the proposed framework for such strategies needs further research before implementation.\n\n【37】### Future Research\n\n【38】Among key questions regarding the practices of hand hygiene in the health-care setting today, the following need to be addressed in controlled studies: What are the key determinants of hand hygiene behavior and promotion? Should hand disinfection replace conventional handwashing? What are the best hand hygiene agents? Should hand hygiene solution include a long-lasting compound? What are the most suitable skin emollients to include in hand hygiene solution? How can skin irritation and dryness from hand hygiene agents be reduced? How does skin care protection with hand cream affect the microbiologic efficacy of hand hygiene agents? and What are the key components of hand hygiene agent acceptability by health-care workers? Additional research questions include-- How can researchers generate more definitive scientific evidence for the impact of improved compliance with hand hygiene on infection rates? What is the acceptable level of compliance with hand hygiene (i.e. What percentage increase in hand hygiene results in a predictable risk reduction in infection rates?) and To what extent should the use of gloves be encouraged or discouraged? Finally, recognizing that individual and institutional factors are interdependent in terms of behavioral changes in health-care settings, what is the best way to obtain top management support for hand hygiene promotion? These questions are addressed to infection control practitioners, laboratory research scientists, and behavioral epidemiologists.\n\n【39】The challenge of hand hygiene promotion could be summarized in one question: How can health-care workers' behavior be changed? Tools for change are known; some have been tested, and others need to be tested. Some may prove irrelevant in the future; others have worked in some institutions and need to be tested in others. Infection control professionals should promote and conduct outstanding research and provide solutions to improve health-care worker adherence with hand hygiene and enhance patient safety.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dbaffeb4-c5e3-4b63-8a19-ad3904c0f28a", "title": "Global Health Security: A Blueprint for the Future", "text": "【0】##### Lawrence O. Gostin\n\n【1】##### Global Health Security: A Blueprint for the Future\n\n【2】##### Harvard University Press, Cambridge, Massachusetts, USA, 2021;\n\n【3】##### ISBN: 9780674976610 (cloth);\n\n【4】##### Pages: 352; Price: $45\n\n【5】Lawrence Gostin’s Global Health Security: A Blueprint for the Future comes along at an opportune time, as a pandemic reminds humankind of the importance of public health response to our wellbeing and security . The book addresses the types of infectious disease outbreaks and actions needed to prepare and respond, emphasizing the roles of multinational agreements and international cooperation. For readers knowledgeable about global health security, the content might serve as a refresher, for persons unfamiliar with the subject, as an introduction. Gostin, director of Georgetown Law School’s O’Neill Institute for National and Global Health Law, shaped by experiences as a lawyer and interactions with the World Health Organization, examines scientific and policy approaches. He discusses COVID-19 throughout the book and emphasizes health equity, drawing attention towards disadvantaged populations in low- and middle-income countries\n\n【6】The book is separated into 2 sections: “Growing Threats” encompasses topics as diverse as mosquitoborne diseases, climate change, and biosafety and biosecurity. “From Risk to Action” focuses on global health security governance, pathogen sharing, universal health coverage, and developing and using medical countermeasures. The chapter aptly titled “Humanity’s Biggest Killer” outlines the history, epidemiology, and clinical manifestations of mosquitoborne diseases. Gostin discusses interventions such as genetically engineered mosquitoes, long-lasting insecticidal bed nets, and vaccination, but cautions “optimism should be tempered…by the limits of traditional strategies, hurdles still to overcome for newer possibilities (from scientific challenges to special publics), and the weaknesses of the health systems.” In another chapter he discusses how behavior such as prophylactic use of antimicrobials in livestock and their overprescription by clinicians contributes to antimicrobial resistance and how lack of economic incentives for private sector investment inhibits development of antimicrobials. Gostin drives home potential dangers from complacency: “Imagine if we lived in a world where once fully treatable infections became life-threatening and routine surgeries posed lethal risks.”\n\n【7】The final chapter, “Global Medical War Chest,” focuses on regulatory considerations, market incentives, and clinical trial design for medical countermeasure development. Gostin argues that “financing, law, and ethics must be in place—not just when an outbreak strikes, but more importantly during periods of calm…Collectively, policies and processes that support all the building blocks of research and development can save millions of lives.”\n\n【8】Gostin might have highlighted certain topics in greater detail, such as misinformation and disinformation and transformative technologies. Additional information about cultural influences on health would have been beneficial, as would perspective on the history of combatting outbreaks, such as through use of face masks. On the other hand, given that global coordination is critical because infectious disease spread does not recognize geographic borders, information about multinational policies and governance is particularly insightful. The author’s use of first-person perspective and storytelling helps keep readers emotionally invested, and the tabletop exercise outbreak scenarios, list of health entity abbreviations, and headings used to guide the reader are valuable additions.\n\n【9】Global Health Security could be beneficial for policymakers reflecting on lessons learned from the COVID-19 pandemic and considering strategies to combat future outbreaks. Gostin’s perspective provides an important lesson: “When the world fully funds and thoroughly prepares for dangerous outbreaks, it is highly likely that dangerous pathogens can be rapidly brought under control. If we neglect the threat, wait until it is too large to stop, and then panic, many lives, and dollars, will be lost. Most of the human and economic suffering is preventable.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "617fca32-e394-43fb-9071-83e93897bcbc", "title": "Serologic Surveillance for West Nile Virus in Dogs, Africa", "text": "【0】Serologic Surveillance for West Nile Virus in Dogs, Africa\n**To the Editor:** West Nile fever is caused by the West Nile virus (WNV), a mosquito-borne member of the genus _Flavivirus_ . Birds are the natural reservoir of the virus, which is maintained in nature in a mosquito–bird–mosquito transmission cycle. WNV has been detected in many regions worldwide, including North America, Europe, Africa, the Near East, and Asia . WNV has been shown to cause meningoencephalitis in humans and horses. In the United States, seroconversion in dogs was detected 6 weeks before a human case was reported . Thus, dogs could be considered as sentinels for WNV infection, but their role as reservoir is unlikely because of short-term and low levels of viremia . In this study, we determined the seroprevalence of WNV in dogs living close to humans in different environments to assess their role as sentinels of this potentially severe zoonosis.\n\n【1】During 2003–2012, blood samples were collected from 753 adult dogs from France and 6 countries in Africa  . Samples were centrifuged within 24 h after collection, separated, frozen at –20°C, and sent to the virology laboratory of the Institut de Recherche Biomédicale des Armées (Marseille, France). Each sample was systematically tested for IgG against WNV by using an in-house ELISA with inactivated WNV as antigen. Serum samples were considered positive if the optical density at 450 nm was >3-fold the mean of that for negative antigen. Because of the antigenic cross-reactivity among flaviviruses, all positive samples were further tested by Western blot for WNV-specific antibodies ; seroprevalence was calculated on the basis of Western blot–confirmed cases only. For the statistical analysis, we used the exact binomial method to calculate 95% CIs of the proportions and the Fisher exact test to calculate p values and compare the seroprevalence rates between countries; significance was set at p<0.05.\n\n【2】Seropositive dogs were found in all portions of Africa and France surveyed except northeastern Gabon and Corsica . Seroprevalence of WNV in native dogs was significantly higher in Chad than in the Democratic Republic of the Congo (DRC) (p<0.001), Senegal (p<0.00001), Côte d’Ivoire (p<0.000001), and Gabon (p<0.000001). Seroprevalence was low in Kinshasa, DRC (12.5%), and Dakar, Senegal (11.1%), but in N’Djamena, Chad, all 5 native dogs tested had specific antibodies against WNV.\n\n【3】As part of the study, we tested 50 military dogs from France twice, before and after a 4-month mission in Chad; 12 (24.0%) became seropositive after the stay. In addition, 12.5% of military working dogs in France imported from Hungary were seropositive on initial testing. We also found that, in France, dogs are the sentinels of WNV circulation in the Var (12.0%) and Gard (9.1%) departments. All dogs we tested that were positive for IgG were negative for IgM, a finding that indicates infection by the virus did not occur recently.\n\n【4】The results and the statistical analysis reveal notable differences in the seroprevalence rates, according to the geographic area. N’Djamena, Chad, where all native dogs tested positive for WNV, is located at the confluence of the Chari and Logone Rivers and is an area with high densities of residential and migratory birds. In contrast, the northeastern region (Haut-Ogooué) of Gabon, where no native dogs tested positive for WNV, is an ecosystem of wet forests without migratory birds, unfavorable to virus circulation. In Dakar, 18.7% of native dogs were seropositive. In these central parts of Senegal, characterized by a semi-arid climate and vegetation composed of steppe plants and bluegrass, several WNV strains have been isolated from birds and mosquitoes. The seroprevalence was lower (0%–12.5%) in the sub-Saharan area, including Côte d’Ivoire, Gabon, DRC, and Senegal (Siné-Saloum and Casamance), where the humid or semihumid climate is linked with tropical rain forests or woodland savannah known to favor sedentary birds .\n\n【5】In a large proportion of the human and animal population of Africa, immunity to WNV has developed . A serologic survey of dogs from the Highveld region of South Africa showed that 37% (138/377) had neutralizing antibodies against WNV . Similarly, seroprevalence of antibodies against WNV is high among dogs in the United States, for example, 55.9% (218/390) in the Gulf Coast region . In Turkey, an area where many birds stop over during migration, seroprevalence among dogs was high (37.7%, 43/114)  _._\n\n【6】Our study highlights the role of dogs as sentinels for WNV circulation, particularly in southeastern France (Gard and Var departments), where WNV epidemics and epizootics occurred in 2000 and 2003. In addition, we observed that military working dogs purchased from Hungary, where WNV infection is common , may be seropositive _._ Seroconversion in dogs returning from short missions in WNV-endemic countries such as Chad was also observed. Therefore, our data emphasize the usefulness and convenience of WNV seroprevalence surveys in dogs for studying WNV epidemiology and circulation. It is possible that dogs living close to humans could attract infected mosquitoes, thereby reducing human infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5c1c2234-c03e-42ac-8cd9-27a69fb72449", "title": "Poultry Market Closures and Human Infection with Influenza A(H7N9) Virus, China, 2013–14", "text": "【0】Poultry Market Closures and Human Infection with Influenza A(H7N9) Virus, China, 2013–14\nA novel avian influenza A(H7N9) virus was first identified in China during March 2013, and by March 25, 2014, it had caused 390 laboratory-confirmed human cases in mainland China. The majority of patients with laboratory-confirmed cases reported recent exposure to live poultry markets (LPMs) in urban areas , and the H7N9 virus has been identified in LPMs in affected areas . Temporary closure of LPMs in Shanghai, Nangjing, Hanghzou, and Huzhou during the 2013 spring outbreak of influenza was associated with immediate and substantial reductions in incidence of confirmed H7N9 infection in those cities .\n\n【1】Although few confirmed human H7N9 infections were identified in the summer and autumn of 2013, the virus reemerged during the winter of 2013–14, and 251 confirmed cases were reported during December 1, 2013–March 25, 2014, mostly from cities in Guangdong Province in southern China and cities in Zhejiang Province in eastern China. In response to the identification of H7N9 in humans, poultry, or the environment, local authorities of affected cities implemented various control measures, the highest profile of which was closure of LPMs. The objective of our study was to estimate the effect of closure of LPMs in reducing incidence of human infections with H7N9 in the most affected cities of Guangdong and Zhejiang provinces during the 2013–14 winter outbreak.\n\n【2】### The Study\n\n【3】During December 1–March 25, 2014, a total of 93 and 89 confirmed human H7N9 cases were reported in Guangdong and Zhejiang provinces, respectively. In response to local cases, many urban areas introduced LPM closures for varying durations. To estimate the effect of these interventions, we used the same methods applied to estimate the effect of LPM closures in cities in eastern China in April 2013 . We selected 9 specific areas where LPMs were closed for ≥7 consecutive days, ≥3 H7N9 cases were reported in that local urban area during the study period of December 1, 2013–March 7, 2014, and \\> 1 case was confirmed before the local LPM closure . The dates of these interventions in relation to the dates of onset of illness of the 69 confirmed human cases in these 9 areas are shown in Figure 1 .\n\n【4】In our analysis we assumed the following: 1) the incidence rate of human infection with H7N9 was constant for the 2 weeks before the implementation of LPM closure in each area; 2) the incidence rate of human infection with H7N9 during the LPM closure period had a different rate from the pre-closure period and was also constant, so the ratio of incidence rates during closure versus before closure indicates the effect of LPM closure, and an incidence rate <1 indicated a reduction in incidence; and 3) illness onset in each human case-patient occurred after an incubation period based on a stochastic incubation period distribution. We further assumed that the incubation periods of cases among humans in all cities followed the same lognormal distribution. The start date of the study time horizon for a given area was either 14 days before the start date of LPM closure or the onset date of the first confirmed local H7N9 case in 2014, whichever was later. The end date of the study time horizon was either the last day of local LPM closure or March 7, 2014, whichever was earlier, to allow for a possible delay of case notification for 2–3 weeks.\n\n【5】Our model therefore included these parameters: the incidence rates before and after closure in each city and the parameters of the incubation period distribution. We estimated these parameters using Markov chain Monte Carlo method , using flat priors for the logarithms of the incidence rate before closure and the incidence rate ratio, a lognormal distribution with mean of 3.3 days and 97.5th percentile 5.7 days for the mean incubation period, and a lognormal distribution with mean 0.76 and 97.5th percentile 5.2 for the coefficient of variation of the incubation period corresponding to the posteriors from our previous analysis . After estimating separate effects of LPM closure in each urban area, we fitted an overall model, assuming the incidence rate ratio was the same across all areas.\n\n【6】Point estimates of the effectiveness of LPM closure varied from 61%–89% in the 4 areas in Guangdong Province, and from 70%– 89% in the 5 areas in Zhejiang Province with generally wide CIs, and the effectiveness was estimated to be 97% (95% CI: 89%, 100%) in the overall model, assuming the same incidence rate ratio associated with LPM closure in each area . In the latter model, the incubation period distribution had a mean of 3.4 days (95% CI: 2.2–5.0) and 95th percentile of 4.8 days.\n\n【7】As in our previous analysis of LPM closures in April 2013 , here we found that LPM closures were effective in reducing human risk for H7N9 infection in cities in Guangdong and Zhejiang provinces during the 2013–14 winter outbreak. The relatively short closure periods and the small number of cases in each city prohibited area-specific estimates of effectiveness that had narrow credibility intervals . In addition, the effect of LPM closure estimated here may incorporate other contemporaneous interventions as well as potential reductions in poultry consumption and population exposure to live poultry associated with the H7N9 outbreak , seasonal variation of avian influenza cases as observed in H5N1 infections, decline in media coverage, decline in seeking of health care by possible patients, and decreased laboratory testing output related to staff fatigue during the second wave of the epidemic.\n\n【8】Although our results support the effectiveness of LPM closure in protecting human health, closure of LPMs is a temporary and drastic measure that may be associated with substantial costs to society and the poultry industry . More sustainable interventions are needed. In the special administrative region of Hong Kong, LPM rest days, on which stalls are cleared of unsold poultry and disinfected, have been used since 2001 to reduce the amplification of avian influenza viruses in LPMs . Ideally, improved surveillance of avian viruses in poultry would enhance identification and closure of contaminated markets and uncontaminated markets could remain open.\n\n【9】Our findings are limited by the ecologic nature of our analysis. It is possible that the incidence rate of human infection with H7N9 in a city appeared to decline substantially at the same time LPMs were closed in that city for reasons other than the closure of LPMs or by chance. Information about the prevalence of H7N9 virus in poultry in different markets and the distribution network of poultry farms and markets would support construction of more complex models of the underlying transmission dynamics. Another limitation was that we could not include interventions used in locations where LPMs were not closed. Our analysis was designed to be self-controlled by comparing incidence of human cases before and after market closures. Finally, we could not examine the effect of LPM closures in other locations that had few cases during winter 2013–14. In general, our findings apply to urban areas, where live poultry purchases mainly occur in LPMs.\n\n【10】### Conclusions\n\n【11】Closure of live poultry markets was highly effective in reducing human risk for H7N9 infection during winter months of the 2013–14 influenza season. However, preventive actions such as enhanced surveillance of poultry and scheduling regular rest days could prevent the necessity of using this costly intervention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a82959af-598b-4f36-a3f8-7510e1f0627f", "title": "Enzootic Angiostrongyliasis in Shenzhen, China", "text": "【0】Enzootic Angiostrongyliasis in Shenzhen, China\n**To the Editor:** _Angiostrongylus cantonensis_ is a zoonotic parasite that causes eosinophilic meningitis in humans after they ingest infective larvae in freshwater and terrestrial snails and slugs, paratenic hosts (such as freshwater fish, shrimps, frogs, and crabs), or contaminated vegetables. With the increase of income and living standards, and the pursuit of exotic and delicate foods, populations around the world have seen angiostrongyliasis become an important foodborne parasitic zoonosis .\n\n【1】Shenzhen municipality is situated in the most southern part of mainland People’s Republic of China between the northern latitudes of 22°27′ to 22°52′ and eastern longitudes of 113°46′ to 114°37′; it shares a border with the Hong Kong Special Administrative Region, China, in the south. The climate is subtropical, with an average annual temperature of 23.7°C. The city is 1,952.84 km 2  and has a population of 10 million.\n\n【2】Since 2006, thirty-two sporadic cases of human eosinophilic meningitis caused by consumption of undercooked aquacultured snails have been documented in Shenzhen . To identify the source of these infections and assess the risk for an outbreak of eosinophilic meningitis, we conducted a survey to investigate whether _A. cantonensis_ occurs in wild rats and snails in Shenzhen.\n\n【3】To examine _A. cantonensis_ infection in intermediate host snails, 302 terrestrial snails ( _Achatina fulica_ ) were collected from 10 investigation sites across Shenzhen, and 314 freshwater snails ( _Pomacea canaliculata_ )were sampled from 6 investigation sites. We examined the snails for _A. cantonensis_ larvae by using pepsin digestion standardized procedures . To survey the prevalence of adult _A. cantonensis_ in definitive host rats, we collected 187 _Rattus norvegicus_ rats and 121 _R. flavipectus_ rats collected from 4 sites where positive snails positive for _A. cantonensis_ were found. These rats were examined for the presence of adult _A. cantonensis_ in their cardiopulmonary systems.\n\n【4】_A. cantonensis_ larvae were found in 96 (15.6%) of 616 examined snails. Of these, _P. canaliculata_ had an average infection rate of 20.7% (65/314), significantly higher (p<0.01) than that of _A. fulica_ (10.3%, 31/302), an indication that _P. canaliculata_ may be the principal intermediate host for _A. cantonensis_ in Shenzhen. _A. cantonensis_ adults were recovered from the cardiopulmonary systems of 37 (12%) of 308 examined rats. Infection rate for _R. norvegicus_ rats was 16.6% (31/187), significantly higher (p<0.01) than that for _R. flavipectus_ (4.9%, 6/121), an indication that _R. norvegicus_ may be the principal definitive host for _A. cantonensis_ in Shenzhen, possibly due to the rat’s preference for eating snails. Infection rates were higher for female rats (25.6% for _R. norvegicus_ and 7.8% for _R. flavipectus_ ) than for male rats (8.9% for _R. norvegicus,_ 2.9% for _R. flavipectus_ ), possibly because female rats eat more snails to supply proteins for reproduction. This report of enzootic _A. cantonensis_ infection in wild rats and snails in Shenzhen demonstrates the existence of natural origins of infection with _A. cantonensis_ for humans in this city.\n\n【5】Persons in Shenzhen eat raw or undercooked freshwater and terrestrial snails and slugs. This practice provides opportunities for infection with _A. cantonensis,_ particularly given that _P. canaliculata_ has been aquacultured intensively for human consumption. The prevalence of _A. cantonensis_ in wild rats and snails in Shenzhen poses substantial risk for future outbreaks of human eosinophilic meningitis. Moreover, public health officials, epidemiologists, researchers, clinical technicians, medical practitioners, parasitologists, and veterinarians, as well as the general public, should be aware of such risks, and integrated strategies should be taken to reduce or eliminate such risks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "99709af6-2de7-48ab-be7e-598db392331d", "title": "Geographic Association of Rickettsia felis-Infected Opossums with Human Murine Typhus, Texas", "text": "【0】Geographic Association of Rickettsia felis-Infected Opossums with Human Murine Typhus, Texas\nMurine typhus is a more common infectious diseases in south Texas. Often the disease is mild and unrecognized; however, it can be severe and even fatal. The severity of murine typhus infection has been associated with old age, delayed diagnosis, hepatic and renal dysfunction, central nervous system abnormalities, and pulmonary compromise. Up to 4% of hospitalized patients die . Murine typhus, which is endemic in many coastal areas and ports throughout the world, is one of the most widely distributed arthropodborne infections. Sporadic outbreaks of murine typhus have been reported in Australia and more recently in China, Greece, Israel, Kuwait, and Thailand .\n\n【1】Recent serosurveys have demonstrated a high prevalence of antibodies to typhus group Rickettsiae in humans living in Asia and southern Europe. In the United States, thousands of human cases were reported annually in the 1940s . A major public health measure consisting of a combination of environmental modification, rat, and vector-control programs greatly reduced human cases in the United States to <100 reported cases of murine typhus/year. As a result, most states no longer report murine typhus. However, murine typhus has been a reportable disease in Texas for the past 40 years.\n\n【2】Interest in this disease has been rekindled because of the resurgence of human cases of murine typhus in south Texas from 1980 through 1984, when 200 cases were reported to the Texas Department of Health. Twenty-eight percent of the patients resided in Nueces County, where the highest annual incidence rate, 4.2 patients/100,000 residents, was reported. Although onset of symptoms occurred throughout the year, 40% of cases were reported in April, May, and June. These studies (Boostrom et al. unpublished; 7–8) also showed that the maintenance and transmission of _Rickettsia typhi_ , the etiologic agent of murine typhus, did not occur by the classic cycle involving rats ( _Rattus rattus_ and _R. norvegicus_ ) and the rat flea, ( _Xensopsylla cheopis_ ). Detailed investigations of murine typhus in the Nueces County/Corpus Christi area have shown a cardinal role for the opossum ( _Didelphis virginiana_ ) and the cat flea ( _Ctenocephalides felis_ ) in the _R. typhi_ life cycle . In addition to _R. typhi_ , sampled opossums and their fleas were also infected with _R. felis_ (formerly known as ELB agent ). Furthermore, in 1994 _R. felis_ was detected by polymerase chain reaction (PCR) in a blood sample from a patient diagnosed with murine typhus. The presence of _R. felis_ , clinically masquerading as dengue fever, was documented recently in patients from Yucatan, Mexico, and four patients with fever and rash in France and Brazil . Our published data and these recent reports not only support the pathogenic role of _R. felis_ but also demonstrate its wide geographic distribution.\n\n【3】In this study, we report the presence of _R. typhi_ and _R. felis_ in opossums and their fleas collected during 1998 in south Texas. Data from our 1998 studies show that the rate of seropositive opossums and infected fleas, as well as the _R. typhi_ / _R. felis_ ratio, are comparable with those in our 1993 studies. In addition, we analyzed the reported cases of murine typhus in Corpus Christi in relation to opossum distribution and seroprevalence. We found a positive correlation between 1998 human murine typhus cases and the geographic distribution of seropositive opossums and their fleas.\n\n【4】### Materials and Methods\n\n【5】##### Review of Human Murine Typhus Cases\n\n【6】Historical data on cases of murine typhus are available through the Texas Department of Health and the Corpus Christi-Nueces County Health Department. Extant data fit the confirmed case definition of a fourfold rise in indirect immunofluorescence assay (IFA) titer or a single titer of \\> 1:128 with clinical symptoms. The 1997 data were extracted from cases reported to the Texas Department of Health. In 1998, data included passive and active surveillance of Spohn Hospital System records. In addition, a board-certified infectious disease specialist contacted area physicians about a human typhus study, which was running concurrently with the opossum study. We also included murine typhus cases reported by area physicians during May through July 1998. Data were analyzed for trends in yearly case rate and incidence by age groups. The 1997 and 1998 data were analyzed for sex, age, symptoms, and geographic distribution of cases.\n\n【7】##### Opossum Collection\n\n【8】The sera analyzed in this study came from the opossums trapped by Corpus Christi residents during an 18-day period in mid-June 1998. A total of 149 opossums were given to animal control officers for euthanization. Opossums were removed from traps, tagged, and transported to the Vector Control facility, where they were numbered and anesthetized with a ketamine/xylazine mixture. The opossums were weighed, identified by age and sex, processed for ectoparasites, and bled by cardiac puncture. Fleas and ticks were removed with a flea comb. The ectoparasites were collected and placed in vials containing 70% ethanol.\n\n【9】##### Rickettsial Seroprevalence in Opossums\n\n【10】Over 95% of trapped opossums were used for a seroprevalence study of rickettsial infections. Initial screening of opossum serum samples of antibodies to _R._ _typhi, R. rickettsii,_ _Coxiella burnetii,_ and _Ehrlichia chaffeensis_ was carried out at the University of Texas at San Antonio. Rickettsial diagnosis was performed with Multi-Test INDX R3E2 Dip-S-Ticks test strips (Integrated Diagnostics, Inc. Baltimore, MD). The assay uses a four-step enzyme-linked immunoassay dot technique for detecting both immunoglobulin (Ig) G and IgM antibodies. Serum samples from uninfected murine typhus patients were used as negative and positive controls. A titer >1:32 was considered positive for _R. typhi_ . Eighty samples with equivocal results were retested by the kit manufacturer (Integrated Diagnostics, Inc.). In addition, opossum sera were tested by IFA for antibodies to _R. typhi_ and _R. felis_ by IFA. Briefly, _R. felis_ \\-infected flea midguts (FleaData, Inc, Freeville, NY) were dissected and placed into individual wells of a 10-well Teflon-coated antigen slide at two midguts/well and allowed to air dry for 20 minutes. Slides were fixed in ice-cold acetone for 10 minutes, air dried, and incubated with individual opossum serum samples (diluted 1:64 and 1:128 in phosphate-buffered saline \\[PBS\\]), for 1 hour in a humidified chamber at 37°C. Serum was removed by aspiration, and wells were washed three times with PBS. Midguts were then incubated with secondary antibody (fluorescein isothiocyanate-conjugated goat anti-opossum IgG, \\[Bethyl Lab. Montgomery, TX\\], diluted 1:20 in PBS/0.01% Evan's blue) for 30 minutes at room temperature. After three PBS washes, slides were air dried and screened for seropositivity. _R. typhi_ \\-infected Vero cells were also used for the serologic screening. Murine typhus convalescent-phase serum, _R.typhi_ \\-positive opossum serum, negative control serum, and uninfected flea midguts (IFA and PCR negative) were used as positive and negative controls. The cat fleas, purchased from FleaData, Inc. were constitutively infected with _R. felis_ ( \\> 95% ) and used as positive controls and antigen sources for opossum serology. The IFA slides were screened by two readers for accuracy. Although attempts to isolate Rickettsiae from the serum samples during the acute phase of infection were unsuccessful, we extracted DNA from selected opossum serum samples. DNA was extracted from 200-µL serum samples by using QIAmp DNA Blood Mini Kit (Qiagen, Valencia, CA) and used for PCR with _Rickettsia_ \\-specific primers.\n\n【11】##### Detection and Identification of Rickettsiae in Fleas\n\n【12】Detection and identification of rickettsial species in fleas collected from opossums were carried out using PCR and restriction fragment-length polymorphism analysis of PCR products. Detection of _R. felis_ gene encoding 17-kDa protein antigen in fleas was done by PCR as described . Briefly, DNA from fleas was obtained by grinding the fleas with grinders containing 20 µL of sterile distilled H 2  O and boiling the lysate for 10 minutes. After centrifugation, 5 µL of the supernatant containing DNA was used for PCR. The DNA template was added to a solution containing 18 µL of PCR Master mix (Roche, Mannheim, Germany) and 1 µL each of forward and reverse primers (100 µmol). In a PCR thermal cycler (Thermo Hybaid, Franklin, MA), each sample was heated to 94°C for 3 minutes, followed by 30 cycles of 94°C for 45 seconds, 55°C for 45 seconds, 72°C for 45 seconds, and an additional incubation period of 72°C for 5 minutes on the final cycle. The target PCR product was visualized by electrophoresis on a 1% agarose gel stained with ethidium bromide and excised; DNA was recovered from the gel with a StrataPrep DNA extraction kit (Stratagene, La Jolla, CA) according to manufacturer's protocol. Enzymatic digestion of cleaned PCR product was done by incubating 8 µL of DNA in 1X enzyme buffer (10 mM Tris-HCl \\[pH 7.5\\], 50 mM KCl, 0.1 mM EDTA, 1 mM dithiothreitol, 200 µg/mL bovine serum albanin, and 50% glycerol), and 15 U of _Alu_ I (Stratagene) for 1 hour at 37°C. Digested products were visualized on 8% TBE gels (Novex, San Diego, CA) stained with ethidium bromide. For sequencing, the purified 17-kDa fragments were subcloned in TOPO TA cloning vector (Invitrogen, San Diego, CA) and were sequenced by the dye terminator method on a model 373 automated fluorescence sequencing system (Applied Biosystems, Foster City, CA). Sequence analysis was performed with the MacVector software package (Accelrys, Inc. Madison, WI), and the BLAST program (National Center for Biotechnology Information, Bethesda, MD) was used for comparison. Sequencing was carried out three times, in both directions, to ensure fidelity.\n\n【13】### Results\n\n【14】##### Human Murine Typhus Cases, Corpus Christi, Texas\n\n【15】Since the 1970s, the number of murine typhus cases has fluctuated around 20 cases/year in south Texas. In 1997, however, a record number of cases, 72, were reported in Texas, resulting in a statewide incidence of 0.4/100,000 population. Sixty-nine of the 72 cases occurred in Region 11 of the Texas Department of Health; most cases occurred in three counties, Hidalgo, Cameron, and Nueces. These three counties consistently register the majority of murine typhus cases in Texas. Data from January 1985 through December 1997 show that Nueces County has averaged the most cases. Cases are reported year-round; however, peak incidence occurs during May and June, which leads local physicians to call murine typhus “the summer flu.” Murine typhus cases from 1997 and 1998 , which occurred in residents of Corpus Christi, were reviewed. Patients ranged from 5 to 79 years of age (mean 40 years). The 1997 and 1998 murine typhus patients were analyzed for race, ethnicity, history of fleabite, exposure to cats and opossums, and presence of symptoms. Fifty-five percent of patients were Hispanic, and 62.2% were female. Symptoms included headache (56%), fever (100%), rash (27%), nausea/vomiting (51%), malaise/fatigue (44%), arthralgia/myalgia (22%), and diarrhea (20%). Fewer than 15% of patients reported a history of fleabite, and the exposure to cats or opossums at residences was associated with only 13% and 11% of cases, respectively. Nueces County/Corpus Christi had 14 of the 42 confirmed murine typhus cases reported in 1999 in Texas and 20 of the 52 reported cases in 2000.\n\n【16】##### Characteristics of Opossums Trapped for Typhus Studies\n\n【17】Opossums are nuisances for residents of Corpus Christi by inhabiting den sites in junk heaps, storage sheds, garages, and attics. Corpus Christi’s opossum population is controlled primarily by private citizens using personal traps. Fifty traps are available at nominal rental through the Corpus Christi Animal Control Program. In contrast, anecdotal information from the nearby Flour Bluff and Calallen areas suggests that residents in these areas tolerate opossum presence. Most opossums that cause problems for residents in these areas are destroyed privately; occasionally, they are used for food. Nevertheless, from 1996 through 1998, Corpus Christi Animal Control trapped and euthanized >18,000 opossums. The mean number of trapped opossums during this 3-year period was 6,324/year. Although data regarding opossum population size, based on the average number of trapped opossums/year, are not available for the study area, the trapped population may represent 20% to 30% of the total yearly population. If this is the case and assuming equal distribution of opossums’ ideal habitats throughout the city, the opossum population density in Corpus Christi could approach \\> 75 opossums/square mile or approximately 1 opossum/0.013 square mile.\n\n【18】Although opossums are collected continuously in the Corpus Christi area, the 1998 study focused on opossums trapped within approximately a 3-week period during the traditional peak of human murine typhus cases. The characteristics of the 149 opossums trapped during June 8–25, 1998, were as follows: 51% female (n=76); 49.0% (n=73) male; and 47.7% juveniles and sexually immature . Weight of the trapped opossums ranged from 5 oz to 8 lbs (mean weight 13 oz for juveniles; 4 lbs 14 oz for adults).\n\n【19】##### Rickettsial Seroprevalence in Opossums\n\n【20】In 1998, a seroprevalence study for _R. typhi_ showed a geographic association between human cases of murine typhus and ranges of seropositive opossums . Six (31.6%) of the 19 patients lived within the minimum home range, 0.02 square mile, of a seropositive opossum. Another five patients (26.3%) were within the maximum home range, 0.1 square mile, of a seropositive opossum. Initial studies on seroprevalence of rickettsial infections in opossums carried out by enzyme-linked immunoassay showed no seroreactivity to _C. burnetti_ , the agent of Q fever; _E. chaffeensis_ , the agent of monocytic ehrlichiosis; and _R. rickettsii_ , the agent of Rocky Mountain spotted fever. However, >25% of the 149 serum samples reacted with _R. typhi_ antigens . Seventeen (23.9%) of juvenile and 21 (26.9%) of adult opossums were seropositive . Reevaluation of the opossum serum samples by using IFA with both _R. typhi_ (Wilmington strain) antigens and the _R. felis_ \\-infected cat flea midguts, showed that 8% and 22% of opossum sera were reactive at \\> 1:128 with _R. typhi_ and _R. felis,_ respectively . Although _R. felis_ \\-infected flea midguts were used to identify non- _R. typhi_ seropositive opossums, these two rickettsial species could not be distinguished in some samples (n=6). Since both _R.typhi_ \\- and _R. felis_ \\-positive fleas were collected from opossums, the possibility of dual infections of opossum could not be ruled out, even though dual rickettsial infection in fleas has not been reported  .\n\n【21】##### Detection and Identification of Rickettsiae in Cat Fleas\n\n【22】A total of 3,401 fleas were collected from 147 opossums. Over 99% of the fleas collected were identified as _Ctenocephalides felis_ . The number of fleas per opossum ranged from 1 to 488 (mean 23 fleas/opossum). Initially, ricksttsial infection in fleas was assessed by using IFA with rat polyclonal anti- _R. typhi._ A total of 359 fleas collected from 50 opossums were sampled and tested individually by IFA; 20% of the fleas were positive. PCR was used to confirm rickettsial infection in fleas. Analysis of 529 individual fleas from 144 opossums showed an overall infection rate of 2.6% (14 confirmed positive). Restriction fragment-length polymorphism analysis of positive flea PCR products yielded a banding pattern representing 3 _R. typhi_ \\- and 11 _R.felis_ \\-infected fleas . The overall _R. felis_ infection rates for 1998 samples were lower than 1993 infection rates . Overall, 8% of the opossums had positive fleas when fleas were tested individually, compared with 21% when flea pools (50 pools; <20 fleas/pool/opossum) were used. The observed discrepancy between the results from pooled and individual flea samples reflects the variability in the DNA recoverable by PCR procedure. Although there was a positive correlation between the opossum age and the flea/opossum ratio, infected fleas came from both juvenile and adult opossums. Additionally, no correlation between the infected fleas and seropositive opossums existed.\n\n【23】### Discussion\n\n【24】Since 1946, the Annual Summary of Notifiable Diseases in Texas has included murine typhus. Historical data identifies 1,127 cases of murine typhus in 1946. However, the reported cases of murine typhus dropped rapidly with the advent of successful rodent and flea controls; by 1952, <100 cases/year in Texas were reported . Through 1960, the number of human cases steadily decreased, ranging from 12 to 50 and averaging 20 cases/year. The sudden increase in locally acquired cases in the 1990s presented a different reservoir-vector-rickettsia paradigm. Historically, murine typhus infection as an urban zoonosis has been maintained and transmitted in commensal rodents, in particular the Norway rat ( _R. norvegicus_ ), and the oriental rat flea ( _X. cheopis_ ) . However, in recent years the zoonotic cycle responsible for the documented human murine typhus cases in south Texas, as well as southern California, has been shown to involve opossums and cat fleas . The role of opossums and cat fleas in the transmission of _R. typhi_ in suburban focus of murine typhus in Los Angeles County has been well documented . As in our study, a high proportion of opossums collected in Orange County, California, was seropositive for rickettsia . Opossums, as a peridomestic animal, are frequent visitors of human habitations, where they search for both harborage and food and thus expose the occupants to cat fleas and consequently to rickettsial pathogens. Cat fleas are frequently found in large numbers on opossums and are avid feeders on humans and household pets. In addition to _R. typhi,_ the cat fleas also harbor _R. felis_ . In fact, the cat flea infection with _R. felis_ is more common than _R. typhi_ . Both rickettsial species are readily maintained transovarially in fleas , but in contrast to commercial cat flea colonies that usually maintain >80% _R. felis_ \\-infection rates, only 1% to 5% of wild-caught fleas are infected with this rickettsial species .\n\n【25】We have shown that cat fleas collected from opossums from Corpus Christi, Texas, were infected with either _R. typhi_ or _R. felis,_ and the infection rates remained <5% in both the 1993 and 1998 samplings. While we have found no evidence for dual infection in individual fleas, opossums fed on by infected fleas could have antibodies against both _R. typhi_ and _R. felis_ . Our initial opossum serosurvey results  using enzyme-linked immunoassay were directed against _R. typhi_ only. However, IFA results from _R. felis_ \\-infected fleas confirmed our earlier findings  that the cat flea/opossum cycle is responsible for the maintenance of both _R. typhi_ and _R. felis_ in Corpus Christi.\n\n【26】We have reported the importance of _R. felis_ as a component of murine typhus transmission cycles . Both _R. typhi_ and _R. felis_ were found in fleas and opossum tissues from the murine typhus–endemic areas of southern California and south Texas . Additionally, a retrospective investigation of five murine typhus patients from Texas demonstrated that four of the patients were infected with _R. typhi_ and the fifth had been infected with _R. felis_  . This documented human infection with _R. felis_ and its presence in opossums and their fleas, and possibly in other wildlife associated with human habitations, have raised concerns about _R. felis_ spillover into human populations. In addition, cat fleas infected with _R. felis_ have been identified not only in the United States  but also in Central and South America, Europe, and Australia .\n\n【27】Together, our published data and these recent reports not only support the pathogenic role of _R. felis_ but also demonstrate its wide geographic distribution. However, we know very little regarding the natural maintenance and transmission of this organism in areas of the world besides south Texas and southern California. The cat flea, known as an indiscriminate feeder, has an extremely broad host range. While it parasitizes cats, opossums, and other animals of the same size, the flea readily switches to different hosts, and it has been found on rats and mice. Because cat fleas are commonly found on household pets, we extended our studies to determine rickettsial seroprevalence in cats. Our pilot serologic studies showed >15% of 513 serum samples from the eastern USA were reactive at \\> 1/64 with _R. felis_ , as assessed by IFA . Sorvillo et al. in their Los Angeles study of a suburban focus of murine typhus, reported that 9 of 10 domesticated and 3 of 26 feral cats were seropositive to _R. typhi_ . Thus, domesticated cats and cat fleas, as well as peridomestic animals, may play an important role in the maintenance cycle of _R. felis_ and its transmission to humans. Our study further documents the involvement of the opossum/Rickettsia/cat flea triad in the flea-associated rickettsial transmission cycle of urban and suburban areas of south Texas and southern California. Similar host/parasite relationships may also operate in other parts of the world where recent _R. felis_ human cases have been documented . Recent attention to _R. felis_ , which already has resulted in reassignment of this organism to the spotted fever group rickettsiae , may further elucidate the other components involved in the maintenance of this rickettsiosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "428bec81-8b42-416f-a7ae-3981b21d0cf3", "title": "Brucellosis Outbreak Traced to Commercially Sold Camel Milk through Whole-Genome Sequencing, Israel", "text": "【0】Brucellosis Outbreak Traced to Commercially Sold Camel Milk through Whole-Genome Sequencing, Israel\nBrucellosis, caused by bacteria of the _Brucella_ genus, is a neglected zoonotic disease that affects marginalized populations worldwide . Human transmission occurs mainly through consumption of unpasteurized, contaminated dairy products from infected, domesticated animals. In Israel, brucellosis primarily affects Arab populations, especially the seminomadic Bedouin tribal communities in southern Israel .\n\n【1】Dromedary camels, which are capable of asymptomatic carriage of _Brucella_ , are valuable domesticated animals in Bedouin culture, and unpasteurized camel milk has gained international popularity because of its alleged medicinal properties. Few reported brucellosis outbreak investigations involving camel milk have described families infected by milk from privately owned camels and have used serologic testing to confirm diagnoses . The standard tool for molecular typing of _Brucella_ has been multilocus variable-number tandem-repeat analysis or multilocus sequence typing (MLST). However, whole-genome sequencing (WGS) is increasingly used for the study of genomic epidemiology of _B. melitensis_ .\n\n【2】We report an outbreak of human brucellosis with unique epidemiologic characteristics, which originated from commercial, single-brand, unpasteurized camel milk; infection was diagnosed in 19 patients with a common exposure history over 4 months. We demonstrate the utility of WGS for brucellosis outbreak investigations.\n\n【3】### The Study\n\n【4】From July–November 2016, the Israeli Ministry of Health noted an increase in brucellosis cases in non-Arab patients in central and northern Israel, raising suspicion of a common source . An epidemiologic investigation noted patients were exposed to the same brand of camel milk. A total of 20 isolates were obtained from 19 patients across Israel (nos. 1–20). Patients from shared households included 2 pairs of siblings and 1 married couple. We studied 2 isolates (nos. 1, 2) from an infant from whom _B. melitensis_ was isolated from blood and cerebrospinal fluid. We also included 1 person  who consumed camel milk of an unknown brand.\n\n【5】The suspected vendor obtained milk from a Bedouin camel farm in southern Israel . Field investigation of the farm revealed 32 female and 2 male camels. A total of 4 female camels had positive serologic test results for _Brucella_ , but none were available for further testing. We sampled 6 bottles of camel milk obtained from a natural food store carrying the suspected brand and recovered a few colonies of _B. melitensis_ from 3 of the bottles.\n\n【6】Clinical isolates were submitted to the National Brucellosis Reference Laboratory (Kimron Veterinary Institute, Beit Dagan, Israel) for confirmation, and milk samples were cultured at the same laboratory. In total, 10 _B. melitensis_ human isolates from the outbreak and 3 camel milk isolates (nos. 21–23) from the implicated source were available for sequencing. An additional 4 epidemiologically unrelated isolates were sequenced and used as outliers, including 1 isolate from camel milk , 2 isolates from patients with _B. melitensis_ with no camel milk exposure (nos. 26, 27), and 1 isolate from an unrelated patient with _B. melitensis_ acquired after consuming camel milk . Finally, we used a _B. melitensis_ reference genome sequence from the National Center for Biotechnology Information Sequence Read Archive database .\n\n【7】DNA was extracted from _Brucella_ isolates by heat killing (80 C°, 10 min) and by using the DNeasy Blood & Tissue kit . Genomic libraries were prepared with a Nextera Flex kit  and subjected to paired-end sequencing by using the Illumina Miseq or Nextseq platforms. Sequences have been deposited in the European Nucleotide Archive (BioProject PRJEB43660).\n\n【8】We used WGS to analyze a total of 18 isolates: 10 human isolates, 3 camel milk isolates, 4 outlier isolates, and 1 reference sequence. Raw sequences underwent quality control using fastQC version 0.11.8 . We assembled sequence reads that passed QC by the pipeline shovill version 1.0.4 with Trimmomatic version 0.39, SPAdes version 3.13.1, and Pilon version 1.23 with the parameters “–trim –opts –sc” . We implemented in silico MLST with MLST version 2.10 per the pubMLST _Brucella_ spp. Scheme . We conducted core genome single-nucleotide polymorphism (SNP) analysis of the WGS assemblies by using ekidna version 0.3.2 . The sequence from isolate 2 was selected for reference with complete assembly of 3,297,563 bases . We masked resulting core genome SNPs for recombination events using Gubbins version 2.3.4  and visualized the final SNP alignment as a minimum-spanning tree created by PHYLOViZ2 .\n\n【9】We assigned all isolates to sequence type 8 by using in silico MLST. Core genome SNP analysis found 2,443 SNP sites with the sequence from isolate 2 for reference. The phylogenetic tree demonstrated 1 main cluster including 8 human outbreak isolates and all 3 isolates from camel milk . Two human outbreak isolates (nos. 4, 9) did not belong to the main cluster, suggesting \\> 1 clone might have caused the outbreak. Isolate 12 clustered with outbreak isolates, confirming that the case-patient who consumed an unknown brand of camel milk was part of the outbreak. Outlier sequences (nos. 25, 27, 28) were genetically distinct from outbreak isolates, expectedly. However, outliers (nos. 26, 24) clustered with outbreak isolates, suggesting an unrecognized epidemiologic link between Bedouin camel farming and outbreak source.\n\n【10】### Conclusions\n\n【11】We describe an outbreak of brucellosis unique in its patient population, source of infection, chain of transmission, and use of WGS. Prior outbreak investigations of brucellosis have seldom used WGS to link the outbreak with samples from the implicated source. Here, WGS exposed genetic linkage between bottled camel milk and 8 isolates from 7 patients, providing conclusive evidence of this common source. Isolates obtained from the same person (nos. 1, 2) and that person’s sibling  clustered tightly, providing internal validity to the discriminative power of WGS. WGS also associated isolate no. 12 (unknown camel milk brand) with the outbreak.\n\n【12】Two isolates (nos. 4, 9) from cases clustered apart from the outbreak strains. Because 4 female camels at the implicated farm had positive _Brucella_ serologic test results, they might have harbored several different strains of _B. melitensis_ , which could explain these results. We acquired all bottles of camel milk sampled during a single day, representing just 1 batch, and other batches were not tested. Therefore, the presence of additional clones implicated in this outbreak could not be ascertained.\n\n【13】Surprisingly, isolates 24 and 26 clustered with the outbreak strains, suggesting an unrecognized epidemiologic chain of transmission. This finding could reflect patterns of unregulated animal trade in Israel in which domesticated animals, including camels, are trafficked from Hebron (origin of isolate 26) throughout the Negev region to Bedouin communities (in this case Rahat \\[origin of isolate 24\\], the largest Bedouin city). Clustering of these outliers suggests that the outbreak might have been more widespread than documented.\n\n【14】In contrast with traditional patterns of brucellosis cases and case clusters, which occur mainly in small communities where residents consume locally produced unpasteurized dairy products, this outbreak resulted from web-based commercial sales of an unregulated food product. This sales channel enabled the spread of _B. melitensis_ throughout Israel, similar to a recent United States report on commercially sold contaminated raw cow milk . Therefore, modern consumer trends in small agriculture and food production may result in new food safety risks.\n\n【15】The advantages of WGS in this outbreak investigation included the capacity to analyze both related and outlier isolates. This capability enabled us to resolve complex linkages between cases, suggesting unequivocal, temporal relationships that depict outbreak directionality and can aid future preventative measures.\n\n【16】In conclusion, we describe a unique _B. melitensis_ outbreak linking human cases with commercially sold, unregulated camel milk. These results demonstrate that WGS can be a powerful tool for investigating transmission of brucellosis, a neglected zoonotic disease, in a world of modernized commercialism.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0f4df311-2a2b-4154-82be-cdfac8948d6d", "title": "Cerebral Aspergillosis Caused by Neosartorya hiratsukae, Brazil", "text": "【0】Cerebral Aspergillosis Caused by Neosartorya hiratsukae, Brazil\n_Aspergillus fumigatus_ is the most common filamentous fungus to cause opportunistic infections in humans. Two close relatives of _A. fumigatus_ , classified in the ascomycetous genus _Neosartorya_ , have been documented to cause occasional opportunistic infections  . These species are _N. fischeri_ and _N. pseudofischeri_ . The former has been reported on two occasions as causing systemic infection in transplant recipients , as well as a mixed pulmonary infection in a patient with myeloma  . _N. pseudofischeri_ has been reported to cause different localized and invasive infections . The conidial states of these species are morphologically very similar to that of _A. fumigatus._\n\n【1】We describe the first cerebral infection caused by another species of _Neosartorya_ , _N. hiratsukae_ . This taxon has been described only in Japan, where it was isolated from air and from pasteurized aloe juice  .\n\n【2】### Case Report\n\n【3】A 75-year-old Brazilian woman was admitted to the Hospital do Servidor Público Estadual de São Paulo on May 5, 1999, with progressive memory loss, confusion, and involuntary movements in the upper right arm after a fall 1 year earlier. She had also developed gait disorder, with short steps and constant loss of balance that led to a diagnosis of Parkinson’s disease in a neurology consultation in March 1999; however, the condition did not respond to the usual treatment. On April 30, a computed tomographic (CT) scan of the brain showed multiple lesions in both brain hemispheres, after which the patient was referred to the hospital. Past clinical history showed an evaluation of productive cough in 1996, with bloody sputum, night sweats, and intermittent fever; she underwent bronchoscopy with pathologic examination, which showed vascular congestion and focal intra-alveolar edema, but no specific pathogen was identified.\n\n【4】On examination, the patient appeared chronically ill, mildly pale, disoriented, and confused, although she was able to follow simple commands. The lungs had decreased sounds in both lower thirds, with rales. The upper arms moved slowly and repetitively, with a loss of strength. Tendon reflexes were normal. The patient underwent surgical exploration, with drainage of the frontal and occipital lesions. Four samples of a yellowish, dense liquid were collected. Laboratory examination did not show neoplastic cells or neutrophils in the liquid. Direct microscopic examination showed septate hyphae in all the samples. Cultures were negative for aerobic and anaerobic bacteria and mycobacteria, and two samples were positive for a fungus, tentatively identified as _Aspergillus_ sp. The patient was initially treated for 21 days with ceftriaxone, oxacillin, and metronidazole without major improvement. When the fungus was isolated, she was treated with amphotericin B and underwent postdrainage magnetic resonance imaging (MRI) of the brain, which showed multifocal brain abscesses, including subtentorial and supratentorial lesions . A chest x-ray and a CT scan of the thorax showed a small, bilateral pleural effusion and a left pulmonary cavitary lesion. Left pleural drainage obtained 80 mL of a clear yellow pleural effusion with pH 7.5, 19,870 leukocytes/mL and 970 erythrocytes/mL, with negative cultures for mycobacteria, aerobic and anaerobic bacteria, and fungi. Refractory hypokalemia developed after the patient received a total dose of 1 g of amphotericin B, which led to discontinuation of this drug. A course of amphotericin B plus itraconazole (400 mg/day) was initiated.\n\n【5】On the first day of itraconazole therapy, another MRI of the brain was performed, which showed an increase in the volume of the abscesses. During the next few days, the patient’s mental state worsened, and she had another cerebral drainage 8 days later. A yellowish opaque liquid, with a white granular deposit, was obtained from the frontal and parietal abscesses. A cerebral fragment was obtained for analysis, which showed only reactional brain tissue. All cultures were negative for bacteria but positive for the same fungus that had been isolated previously. Her clinical and neurologic status improved, and she was discharged to a nursing home on July 20, 1999; itraconazole (400 mg/day) treatment was continued.\n\n【6】The patient was again admitted to the hospital on August 8, and she stayed for 21 days after being diagnosed with urinary sepsis caused by _Escherichia coli_ . She was successfully treated with intravenous ceftriaxone. Her neurologic condition had been stable since her previous hospital discharge. She was last seen at the outpatient clinic on September 29, with an unchanged neurologic condition. During the hospital stay, another MRI of the brain showed a decrease in the size of all lesions, which was interpreted as a good response to the itraconazole therapy. Because of the difficulties of maintaining home care, she was admitted to another nursing hospital. Her clinical state deteriorated, and she died in November 1999 from multiple organ failure. Clinical information on this last hospital admission is very limited, and a postmortem examination was not performed.\n\n【7】Cultures obtained on the two occasions yielded molds with identical morphologic features, and one isolate was referred to the Medical School of Rovira i Virgili University in Reus, Tarragona, Spain, for identification purposes.\n\n【8】The isolate was subcultured on Czapek agar and malt extract agar (MEA), and incubated at approximately 25°C in the dark. After 14 days, the colonies on Czapek agar were very restricted (12 mm–14 mm in diameter), velvety, irregularly folded, umbonate, and white to yellowish white, with a pale yellow reverse. Sporulation was absent. On MEA, the colonies developed rapidly, attaining a diameter of 40 mm–46 mm in 14 days. They were velvety, radially folded, white to greenish white, with a pale yellow reverse. Ascomata and conidial heads developed throughout the culture. The fungus grew restrictedly at 45°C.\n\n【9】The microscopic features of ascomata and conidial heads were examined from wet mounts prepared in lactic acid under a light microscopy. Ascomata were non-ostiolate, superficial, white to light cream colored, globose or subglobose, measuring 120 µm–600 µm in diameter, and covered with a white aerial mycelium. The peridium was thin and membranous. The asci were eight-spored, more or less globose, and measured 11µm–15 µm in diameter. The ascospores were hyaline, one-celled, and lenticular, with two closely pressed equatorial crests. They measured 6 µm–7.5 µm x 4 µm–5 µm, including the crests, and their convex walls showed a fine reticulate ornamentation. Numerous conidiophores of an _Aspergillus_ sp. were intermixed with the ascomata. The conidiophores consisted of green to bluish green, uniseriate, short columnar conidial heads over hyaline to light green, smooth and thick-walled stipes, which measured 100 µm–170 µm x 3 µm–4 µm. The conidia were pale greenish, globose or subglobose, 2 µm–2.5 µm in diameter and with smooth or delicately roughened walls.\n\n【10】On the basis of the above characteristics, and especially taking into account the ascospore ornamentation observed under scanning electron microscopy , we identified the isolate as _N. hiratsukae_ . The isolate was morphologically compared with the type strain of _N. hiratsukae_ (NHL 3008) and was proven to be the same species . In addition, _N. hiratsukae_ is the only species of _Neosartorya_ with reticulated ascospores that grow restrictedly on Czapek agar, a characteristic also shown in the case isolate. _N. pseudofischeri,_ the most common _Neosartorya_ species involved in human infections, is easily distinguished because its ascospore walls are ornamented with raised flaps of tissue resembling triangular projections or long ridge lines . Living cultures of the case strain are deposited in the Centraalbureau voor Schimmelcultures, the Netherlands (CBS 109356) and in the Institute of Hygiene and Epidemiology, Belgium (IHEM 18438).\n\n【11】The case isolate was tested to determine its susceptibility to five antifungal drugs. Tests were carried out by a microdilution method described previously  and adapted from the reference method for molds recommended by the National Committee for Clinical Laboratory Standards  , with RPMI 1640 medium buffered to pH 7.0 with 0.165 M morpholinepropanesulfonic acid, an inoculum of 9.1 x 10 5  CFU/mL, an incubation temperature of 30°C, a second-day reading (48 h), and an additive drug-dilution procedure. MICs and minimum fungicidal concentrations (MFC) were as follows: amphotericin B 1 and >16 μg/mL, flucytosine 64 and >64 µg/mL, itraconazole 0.25 and 0.25 µg/mL, voriconazole 0.25 and 0.5 µg/mL, and UR-9825 0.06 and 0.5 µg/mL, respectively. Results demonstrated good activity of the four-azole derivatives tested. UR-9825, a novel triazole not yet licensed, showed the lowest MIC. MFCs of amphotericin B and flucytosine were very high, indicating the ineffectiveness of these drugs. These data correlate with our clinical results since a total dose of 1 g of amphotericin B was unable to reduce the brain abscesses, while the patient responded well to itraconazole at daily doses of 400 mg.\n\n【12】This case report is important because such clinical isolates of _Neosartorya_ spp. that produce white colonies and do not become green, like colonies of _A. fumigatus_ , are often discarded as contaminants. Therefore, the real incidence of aspergillosis caused by _Neosartorya_ species could be underreported. Many _Neosartorya_ species, other than _N. fischeri_ or _N_ . _pseudofischeri_ , are thermotolerant and can grow at temperatures above 37°C, showing their inherent ability to invade the brain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b3a4f763-65fa-49a2-b408-5c0b7ac4d5ad", "title": "Clinical-Community Partnerships to Identify Patients With Food Insecurity and Address Food Needs", "text": "【0】Clinical-Community Partnerships to Identify Patients With Food Insecurity and Address Food Needs\nAbstract\n\n【1】**Introduction**\n\n【2】More than 42 million people in the United States are food insecure. Although some health care entities are addressing food insecurity among patients because of associations with disease risk and management, little is known about the components of these initiatives.\n\n【3】**Methods**\n\n【4】The Systematic Screening and Assessment Method was used to conduct a landscape assessment of US health care entity–based programs that screen patients for food insecurity and connect them with food resources. A network of food insecurity researchers, experts, and practitioners identified 57 programs, 22 of which met the inclusion criteria of being health care entities that 1) screen patients for food insecurity, 2) link patients to food resources, and 3) target patients including adults aged 50 years or older (a focus of this assessment). Data on key features of each program were abstracted from documentation and telephone interviews.\n\n【5】**Results**\n\n【6】Most programs (n = 13) focus on patients with chronic disease, and most (n = 12) partner with food banks. Common interventions include referrals to or a list of food resources (n = 19), case managers who navigate patients to resources (n = 15), assistance with federal benefit applications (n = 14), patient education and skill building (n = 13), and distribution of fruit and vegetable vouchers redeemable at farmers markets (n = 8). Most programs (n = 14) routinely screen all patients.\n\n【7】**Conclusion**\n\n【8】The programs reviewed use various strategies to screen patients, including older adults, for food insecurity and to connect them to food resources. Research is needed on program effectiveness in improving patient outcomes. Such evidence can be used to inform the investments of potential stakeholders, including health care entities, community organizations, and insurers.\n\n【9】Introduction\n\n【10】Nearly 13% of US households — 42.2 million people — were food insecure at some time during 2015 . The US Department of Agriculture (USDA) defines food insecurity as an economic and social condition characterized by limited or uncertain access to adequate food . Together with insecurity in housing and income, food insecurity is a social issue that affects health. Food insecurity is associated with a higher risk of chronic disease, including obesity , diabetes , depression , hypertension , and chronic kidney disease . Among individuals with diabetes, food insecurity is associated with poorer glycemic control  and greater health care use, including outpatient visits , emergency department visits  and hospitalizations .\n\n【11】As a result of the growing body of evidence linking food insecurity and health, health care entities, including hospitals, health systems, and clinics, are increasingly attempting to address food insecurity in the communities they serve. Some payment systems are shifting from traditional fee-for-service to value-based reimbursement, which has incented many hospitals to consider community factors and social issues that affect health, with a goal of reducing the demand for treatment of preventable conditions. Additionally, expansion of health insurance coverage models resulted for many health systems in a reduced need for provision of charitable care in nonprofit hospitals, and therefore increased the need to find other community benefit investments to maintain a tax-exempt status.\n\n【12】Although anecdotal evidence suggests a growing role of hospitals and health care systems in addressing food insecurity, a description of the range of programs being implemented in the United States is lacking. This article reports on the results of a landscape assessment of health care entity–based programs in the United States that screen patients, especially older adults, for food insecurity and then connect them with food resources.\n\n【13】Methods\n\n【14】As part of the Tackling Hunger to Improve Health in Americans project (Tackling Hunger) , from January through August of 2016, the Public Health Institute (PHI) conducted the first phase of an evaluability assessment, to identify US health care entity–based programs that screen patients for food insecurity and connect them with food resources. The methodology for this evaluability assessment was based on the Systematic Screening and Assessment Method, which identifies interventions and determines their potential for formal evaluation . The Centers for Disease Control and Prevention (CDC), which provided technical assistance during the initial phase of this project, reviewed the data collected to describe the landscape of health care entity–based programs engaged in addressing food insecurity.\n\n【15】The first step in the evaluability assessment was to identify relevant programs through a nomination process conducted by professionals and experts in the field, an internet search for reports and white papers, and a literature review in PubMed. There were 3 inclusion criteria for programs to be selected for this assessment: 1) health care entity conducts screening to identify patients with food insecurity through the use of assessment tools, 2) health care entity links the patient to food security resources or programs, and 3) target patient population includes adults aged 50 years or older. Health care entity–based programs that focus on seniors (aged ≥50 y) with chronic disease were identified as a population of interest, because of priorities of the project funders. An online nomination request form was distributed widely through the e-newsletters and electronic mailing lists of various professional networks involved in the areas of health care and food insecurity and was also sent to a targeted group of professionals working in this area; programs identified through the internet search and literature review were also sent the nomination form and invited to self-nominate.\n\n【16】Self-nominations and third-party nominations were accepted from March through June 2016, yielding 57 programs. Managers of nominated programs were then contacted to request additional information to verify whether the program met the criteria. The project team then reviewed the nomination forms and additional documentation. Three staff members were assigned to review each program and independently determine whether the program met each of 3 inclusion criteria. If the 3 reviewers did not independently reach the same conclusion for inclusion criteria, they discussed the program; if consensus among the 3 reviewers was not achieved, a fourth member of the team reviewed the program. Of the 57 nominated programs, 22 met all 3 inclusion criteria.\n\n【17】A telephone interview was conducted with each of the program managers in August 2016. The purpose of the interview was to gather additional information and to clarify information the program had previously provided, including program objectives, activities, target population and reach, the role of the hospital or health care system in food insecurity screening and program implementation, food insecurity screening tools and processes, program stakeholders and partners, funding, sustainability, data collection, and program evaluation activities.\n\n【18】This landscape assessment presents data on key features of the 22 health care entity–based programs; program names were blinded and identified by using letters of the alphabet. The programs featured in this landscape assessment are geographically diverse, with 7 programs from the northeast region, 5 from the western region, 4 from the midwestern region, 3 from the southwestern region, 1 from the mountain plains region, 1 from the mid-Atlantic region, and 1 from the southeast region . Most programs started between 2014 and 2016, and all but 3 of the programs began after 2010.\n\n【19】Results\n\n【20】The programs are implemented by nonprofit health care entities, and program implementation occurs in various settings, including nonprofit hospitals and health systems, hospital-affiliated primary care clinics, academic health centers, teaching hospitals, community hospitals, critical-access hospitals, acute-care hospitals, children’s hospitals, federally qualified health centers (FQHCs), mental health and substance abuse treatment centers, safety-net clinics and hospitals, county- or state-run health centers, large integrated care delivery health systems, and nonprofit dental clinics . Most programs (n = 18) target patients of all ages with food insecurity; however, several programs indicated a focus on adults aged 50 years or older (n = 7), and nearly half (n = 10) mentioned a special interest in addressing food insecurity among children while still capturing adults aged 50 years or older through screening and intervention. Most programs (n = 13) focus on addressing the needs of patients with food insecurity and chronic disease. The programs are most often led by the health care entity, and most patients being served by the programs are uninsured or insured through Medicaid or Medicare. More than half of the programs involve collaborations with a local or regional food bank, the most commonly reported program partner. However, a range of partners participate in program implementation, including local and regional food banks, universities and schools of medicine, city and county health departments, local health councils, farms, farmers markets, food and agriculture networks, social service agencies, schools, hunger networks, senior centers, and community nonprofit organizations focusing on health, food insecurity, seniors, and early childhood.\n\n【21】### Food insecurity screening\n\n【22】Nearly two-thirds of the programs (n = 14) conduct food insecurity screening using the Hunger Vital Signs screener questions: 1) “Within the past 12 months we worried whether our food would run out before we got money to buy more” and 2) “Within the past 12 months the food we bought just didn’t last and we didn’t have money to get more” . The remaining programs use screening questions developed specifically for the program or other metrics, such as income in relation to the federal poverty level. In all but one of the programs, patients are screened for food insecurity during the clinical encounter, which includes primary care clinics, prenatal visits, well child checkups, urgent care and emergency department visits, home-based medical care, hospital admissions or discharges, and specialized health care facilities (ie, for mental health, substance abuse, diabetes, congestive heart failure, or dentistry). Most programs (n = 18) screen patients of all ages for food insecurity; however, several programs indicated a focus on screening adults aged 50 years or older (n = 7) or children (n = 10) (in addition to including adults aged ≥50 years in screening). In most programs (n = 18), a positive screening result triggers referral to the program or other food resources in the community, although in some cases (n = 4) referrals are provided at the discretion of the health care professional.\n\n【23】Most commonly, programs routinely screen all patients (n = 14), and of those, many (n = 8) indicated that they integrate screening into the clinical workflow as a part of routine intake or examination procedures. However, several programs (n = 8) do not routinely screen all patients but instead conduct screening at the discretion of individual providers. More than half of the hospitals and health systems track food insecurity screening results in the patient’s electronic health record. Screenings are conducted by various people, including medical assistants, patient navigators, social workers, community health workers, administrative staff, home health care providers, dietitians, certified diabetes educators, nurses, physicians, and medical students serving as patient advocates.\n\n【24】### Food insecurity interventions\n\n【25】The stated goals of the programs include the following: reducing food insecurity (n = 17), increasing access to and consumption of healthy foods or fruits and vegetables (n = 13), reducing chronic disease and improving health outcomes (n = 9), providing education on healthy eating (n = 4), and reducing hospital readmissions (n = 2). Various strategies are used to achieve these goals. Of the 13 types of food insecurity interventions, the most commonly implemented intervention (n = 19) is a referral to or a list of food resources, including local food resources (eg, food banks, soup kitchens, fruit and vegetable vouchers) and federal benefit programs such as the Supplemental Nutrition Assistance Program (SNAP) and the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) . In 15 programs, a patient navigator, case manager, or social worker helps to connect patients with food insecurity to food resources, and in more than half of the programs (n = 14) they assist food-insecure patients in applying for federal benefits like SNAP, WIC, Medicaid, and Medicare. Most programs (n = 13) have an education component. Seven programs address a patient’s immediate need for food by hosting an onsite food pharmacy (n = 2) or food pantry (n = 5) in the health care setting. Seven programs provide medically tailored food, for example, food that is appropriate for helping patients manage their diabetes. Eight programs, commonly called fruit and vegetable prescription programs, provide vouchers or coupons for fresh fruits and vegetables that can be redeemed at farmers markets or food pantries. The most common combination of interventions, implemented concurrently by half of the programs (n = 11), is to provide a referral to or list of food resources, a patient navigator or case manager, and assistance with applications for federal benefits.\n\n【26】### Funding and data collection\n\n【27】The programs receive long-term and short-term funds from various sources. More than half of the programs (n = 13) were informed by findings from a community health needs assessment (CHNA); consequently, common funding sources are the operating budgets of the health care entities (n = 11) and hospital community benefit funds (n = 9). Other common funding sources include foundations (n = 12), private donations (n = 5), USDA grants (n = 3), food banks (n = 8), and other city, state, and federal government funds (n = 7). Most programs (n = 14) indicated that they were financially sustainable for the next 2 to 3 years, while the remainder (n = 8) expressed concerns about long-term sustainability of funding.\n\n【28】Half of the programs (n = 11) have begun to track data on patient health outcomes, including body mass index, waist circumference, blood pressure, blood glucose, hemoglobin A1c, cholesterol, and hospital readmissions. Other data collected by programs include process indicators (number of patients enrolled in the program, referred to resources, and receiving food resources) and patient survey data on changes in food insecurity status, diet quality, fruit and vegetable intake, and knowledge of and self-efficacy around healthy eating.\n\n【29】Discussion\n\n【30】This article describes 22 US health care entity–based programs that screen patients, including older adult patients, for food insecurity, and connect them with food resources. All but 3 of the programs were initiated after the enactment of the Affordable Care Act, which included changes in the tax code for tax years beginning after March 2012 that required nonprofit hospitals to conduct CHNAs. These changes influenced the community benefit investments made by nonprofit hospitals. Several key program features emerged from this review.\n\n【31】These programs were developed on the basis of strong partnerships between health care entities — often hospitals — and community-based organizations, with a focus on meeting needs identified in the community. Community partnerships are important, because partners may be abreast of the frequent changes in availability of food insecurity–related resources in the community that can meet the needs of both the patient and their household. More than half of the programs were informed by findings from a CHNA conducted by a nonprofit hospital that suggested that food insecurity was a key issue in the community. Several programs indicated that their food insecurity interventions were a part of a more holistic program that connects patients to resources for assistance with housing, transportation, utilities, education, vocational training, employment, child care, and English-language skills. A better understanding of the broader social needs of communities can help to target and tailor food insecurity interventions. Guidance for hospitals interested in assessing food systems and food security in their CHNAs, “Making Food Systems Part of Your Community Health Needs Assessment,” is available as a result of the Tackling Hunger project and accessible through the PHI . The guidance outlines key food insecurity indicators, data sources and measurement tools, and food security and food system stakeholders.\n\n【32】Screening is another important component of programs that leverage community–clinical linkages to address food insecurity. Most programs are using the Hunger Vital Signs screening questions  to screen patients for food insecurity. These 2 brief, easy-to-administer questions may be the most feasible screening tool for health care entity–based food insecurity programs. Research indicates that this tool provides a valid measure of food security status in various populations . Using the Hunger Vital Signs questions, the Hunger Safety Net Workgroup of the Nutrition and Obesity Policy Research and Evaluation Network has developed food insecurity screening algorithms to guide physicians in screening pediatric and senior patients for food insecurity and referring patients to emergency and long-term food resources in the community .\n\n【33】Many of the programs reviewed in this assessment have integrated screening into the workflow of the hospital or health care entity. Institutional policies to integrate screening may be important to ensure that screening is not conducted at the discretion of individual providers but is universal for all patients. Universal screening can be important to accurately identify and assist all patients with food insecurity. One way of integrating screening into the workflow is to track screening in electronic health records (EHRs), which is being done by more than half of the programs. A few programs have begun using software programs  to track referrals to and use of resources, with a desire to eventually assess impact on patient health outcomes. However, most of the programs are still in the early stages of such efforts, often attempting to use their EHR systems to link patient screening results with referrals, use of referred food resources, and health outcomes. This requires either 1) identifying one EHR software package that can track each of these program data elements as a patient moves through the program or 2) using applications that utilize technology based on FHIR (Fast Healthcare Interoperability Resources) and allow interoperability and exchange of patient data between multiple data collection systems. Moreover, considering that some programs aim to reduce hospital readmissions, identifying technology that is either programmed directly within the EHR or embedded within the EHR or other electronic data systems using FHIR-based resources will facilitate linkage of screening and referral data to readmission data at the patient level and will enable programs to determine their impact on health care use. A related challenge is complying with requirements to protect patient health information under the Health Insurance Portability and Accountability Act (HIPAA), including possible privacy issues when sharing patient referral data with program partners external to the health system.\n\n【34】The goal of many of the programs is to not only increase access to food in general but also improve access to healthy foods, including fresh fruits and vegetables, for chronic disease prevention. Some programs aim to support the local food system by increasing access to and consumption of fresh fruits and vegetables through farmers markets, subsidized community-supported agriculture shares, and food banks. These efforts are consistent with an increasing recognition among food banks that a large proportion of beneficiaries have chronic diseases and that food banks should provide nutritious foods that help prevent or manage chronic disease .\n\n【35】Many programs indicated that their funding was obtained from multiple short-term funding sources, and several program managers expressed concern about long-term sustainability. The sustainability of certain hospital-based food insecurity programs may depend on identifying secure funding, one example of which may be the USDA Food Insecurity Nutrition Incentive (FINI) Grant Program . FINI grants support projects that incentivize the purchase of fruits and vegetables among low-income consumers participating in SNAP and can help fund fruit and vegetable prescription programs and initiatives that provide SNAP matching funds at points of sale such as farmers markets.\n\n【36】Another potential source of funding could be support from public or private health insurance. Information that demonstrates that food security interventions can have a positive impact on patient health outcomes and reduce health care utilization will likely be important to this group. Although half of the programs have begun to track data on patient health outcomes, the other half have focused their data collection on measuring program enrollment, referrals to and use of resources, and changes in food insecurity status and diet quality. This focus could expand to include health outcomes and health care utilization to ensure program value and sustainability.\n\n【37】A particular area of interest in this assessment was describing health care entity–based food security programs that focus on older adults, and we found that nearly one-third of the programs we assessed had this focus. Food insecurity in older adults can have causes and consequences that are unique to this population. Seniors often have low levels of participation in food assistance programs — nearly 3 in 5 seniors who qualify for SNAP do not participate . Strong, sustainable clinical–community partnerships are needed to ensure that older adults who screen positive for food insecurity are referred to and participate in food assistance programs. Community and clinical interventions should be designed to reflect the unique needs and determinants of food insecurity in older patients and to address their barriers to participation in food assistance programs. Existing models for home and community-based nutrition and aging services can be expanded and combined with efforts to increase awareness of the needs of older adults.\n\n【38】To our knowledge, this is the first assessment of its kind to describe the range of health care entity–based programs in the United States that screen patients for food insecurity and connect them with food resources. A key strength of this assessment was that programs were identified by nominations through an extensive, national network of researchers, public health professionals, and health care entities involved in food security initiatives, and there was diverse geographic representation among the programs. Additionally, detailed information was gathered from programs in 3 separate steps using different methods — an online nomination form, formal document collection, and telephone interviews — which allowed for an in-depth assessment of key features of these programs.\n\n【39】However, this assessment has several limitations. First, the programs represent a convenience sample and were not a complete census of programs that meet the inclusion criteria. Additionally, the inclusion criteria narrowed the scope of programs that were reviewed, and useful approaches for screening patients for food insecurity and linking them to food resources may not have been captured in this assessment if formal screening was not conducted or the target population did not include older adults. The nomination process identified several established programs that screen for food insecurity only among children, during well child checkups or in pediatric hospitals, but these programs were not included in this assessment because the intervention excluded older adults. Future research could include a broader landscape assessment that identifies programs that focus only on children or programs that address food needs through a more general social needs assessment but that do so without conducting formal screening for food insecurity. For example, programs that use community health workers or case managers may address a patient’s food needs through a more comprehensive evaluation of social needs, and this type of program may represent a larger share of health care–linked food insecurity interventions in the United States. Lastly, this article describes health care entity–based food insecurity programs that were identified through a larger initiative to assess the readiness of these programs for a formal evaluation. It is important to note that these 22 programs have not yet undergone formal evaluation, and most had collected minimal data on effectiveness. Therefore, program effectiveness and best practices cannot be determined from this assessment.\n\n【40】Although many of the programs featured in this assessment are still in the early stages of implementation and are operating on a small scale with a limited reach, collectively they represent the momentum in the health care sector to focus on prevention and address the health impact of food insecurity in their communities. More evidence is needed about the effectiveness of health care entity–based programs in improving food security and health outcomes and decreasing health care utilization. Rigorous effectiveness research can provide information to potential stakeholders, including health care entities, community organizations, and public and private insurers, to guide their investments.\n\n【41】Tables\n------\n\n【42】#####  Table 1. Characteristics of 22 Programs Identified as Health Care Entity–Based Food Insecurity Interventions, Tackling Hunger Landscape Assessment, United States,\n\n| Program (Blinded Name) | Health Care Organization | Program Partners | Target Patients | Chronic Disease Component |\n| --- | --- | --- | --- | --- |\n| A | Nonprofit community health system | Community partner organization with patient navigators | Adults and families of children aged 18 years or younger who screen positive | None |\n| B | Seven nonprofit hospitals and health systems, including an academic health center, a children’s hospital, and an FQHC | Food bank | All patients aged 18 years or older | Diabetes |\n| C | Nonprofit community hospital and health system | Community nonprofit organization, college of osteopathic medicine, city department of public health | All patients | Congestive heart failure, type 2 diabetes, obesity |\n| D | County-run health system with FQHCs | Food bank, community nonprofit organization | Children and adults | None |\n| E | Nonprofit integrated care delivery health system | Statewide nonprofit organization | Children and seniors | Diabetes |\n| F | Eight hospitals and health systems, including nonprofit teaching hospitals, nonprofit community hospitals, and critical access hospitals, as well as FQHCs | Food network (farmers, policy makers, grocers), local health council, county health department, Head Start, social service agencies, senior centers, school health clinics | All community residents, pregnant women, children younger than 5 years, teenagers | Diabetes |\n| G | Nonprofit health system | Regional food bank | All patients, seniors, teenagers | Obesity, diabetes, and other chronic diseases |\n| H | Community safety-net clinics, including an academic health center and 3 county FQHCs | Nonprofit farm | All patients | Chronic diseases |\n| I | Community FQHC, nonprofit health system, and hospital network | Food bank | All patients, seniors, infants | Diabetes |\n| J | Nonprofit community critical-access hospital | Nonprofit senior services organization, local council on aging, community nonprofit organization for seniors | Seniors (aged ≥60 y) | None |\n| K | State-operated mental health center, onsite FQHC | University psychiatry department, citywide farms partnership | Adult patients aged 18 years or older | Mental illness, diabetes, cardiovascular disease |\n| L | Nonprofit academic health center medical school, FQHC, nonprofit residential substance abuse treatment facility | Nonprofit farm | All patients, youth, pregnant women | Diabetes, obesity, cancer, substance abuse |\n| M | Academic health center, nonprofit safety-net hospital | Food bank, community nonprofit organization | All patients | None |\n| N | Charitable nonprofit health care organization, acute-care nonprofit hospital | Food bank | All patients, with a focus on seniors | None |\n| O | Nonprofit health system, medical and dental clinic affiliated with a public research university, FQHC, community mental health center, senior center | CSA farm cooperative, early childhood alliance, community early childhood development nonprofit organization | All patients, families that have young children (aged 0–8 y) in the home | Chronic disease, diabetes, prediabetes, obesity |\n| P | FQHC with 8 clinics | Coalition of regional farmers markets | All patients, pregnant women | Type 1 diabetes, type 2 diabetes, hypertension, obesity |\n| Q | Nonprofit health system, safety-net hospital | Food bank | All patients, seniors | None |\n| R | Nonprofit integrated health delivery system, 13 hospitals, primary care practices | Local food initiative, regional food bank | All primary care patients | None |\n| S | State university hospital, academic health center and affiliated medical clinics, nonprofit health system, nonprofit community dental services | Food bank | All patients, adults and their families | Chronic disease |\n| T | Community nonprofit hospital, primary care departments | Community nonprofit organization, food bank, city hunger network | All patients, children, pregnant women | None |\n| U | Nonprofit hospital | National nonprofit organization supporting a network of food banks | All patients, seniors | None |\n| V | Medical school–affiliated health system, nonprofit academic health center | Community nonprofit organizations, food pantry | All patients, adults and children | Diabetes, obesity |\n\n【44】Abbreviations: CSA, community supported agriculture; FQHC, federally qualified health center.\n\n【45】#####  Table 2. Food Insecurity Intervention Types and Frequency of Implementation, 22 Health Care Entity–Based Programs, Tackling Hunger Landscape Assessment, United States,\n\n| Intervention Type | Description | Number of Programs Implementing |\n| --- | --- | --- |\n| Referral to outside resources | Patient is referred to or provided with a list of local or federal food resources (eg, referred to local food bank) | 19 |\n| Patient navigator | Patient navigator, case manager, or social worker connects the patient to food resources | 15 |\n| Federal benefit application assistance | Patient assisted in applying for federal benefits, either through a patient navigator or other case worker | 14 |\n| Patient education | Patient provided with group classes or individual counseling on cooking, gardening, nutrition, or disease self-management | 13 |\n| Fruit and vegetable vouchers | Patient provided vouchers or coupons for fresh fruits and vegetables that can be redeemed at farmers markets/food pantries (eg, a fruit and vegetable prescription program) | 8 |\n| Medically tailored food | Patient provided medically tailored food (eg, tailored to patients with diabetes), either through home delivery or patients pick up at the food pantry | 7 |\n| Onsite food pantry | Hospital or health care entity has an onsite food pantry that provides patients with emergency food | 5 |\n| Boxes of healthy food, fresh produce, or both | Patient provided healthy food boxes, fresh fruits and vegetables, or both | 4 |\n| Subsidized CSA shares | Patient given subsidized CSA shares to obtain fresh fruits and vegetables | 3 |\n| Onsite food pharmacy | Hospital or health care entity has an onsite food pharmacy, which is similar to a food pantry but typically focuses on providing healthy foods, often through a prescription written by the health care provider | 2 |\n| Onsite vegetable garden/farm or community garden | Patient provided produce from onsite vegetable garden/farm or community garden | 2 |\n| SNAP matching | SNAP matching program that provides additional dollars when SNAP benefits are used at farmers markets | 1 |\n| Subsidized healthy food for purchase | Patients provided healthy food for purchase at a subsidized (greatly reduced) price | 1 |\n\n【47】Abbreviations: CSA, community supported agriculture; SNAP, Supplemental Nutrition Assistance Program.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "122b0b7f-146c-4fe0-b71b-d116ede793fa", "title": "Novel Orthopoxvirus and Lethal Disease in Cat, Italy", "text": "【0】Novel Orthopoxvirus and Lethal Disease in Cat, Italy\nOrthopoxviruses (OPXVs; family _Poxviridae_ , subfamily _Chordopoxvirinae_ , genus _Orthopoxvirus_ ) are complex, double-stranded DNA viruses with ongoing interest because of their potential use as bioterrorism agents and in gene therapy. Variola virus (VARV), the causative agent of smallpox, has been eradicated in nature; however, there is still the possibility of accidental or intentional release, and it is currently classified as a category A biologic agent . Another concern is the zoonotic potential of some OPXVs, such as monkeypox virus, camelpox virus, buffalopox virus, and cowpox virus (CPXV) .\n\n【1】CPXV, which has a wide host range and a distribution restricted to the Eurasian continent, causes localized dermatitis in humans, although severe disease might develop in immunocompromised persons, occasionally with a fatal outcome. Natural hosts for CPXV are wild rodents , but the infection is acquired mainly through direct contact with cats, which are natural hosts, and rarely by exotic animals and wild species . Ectromelia virus (ECTV) is the causative agent of mousepox, a severe exanthematous disease of mice in laboratory colonies and has been reported worldwide in several outbreaks and causes high economic losses in biomedical research . ECTV has never been reported in humans, and little is known regarding its natural distribution and hosts .\n\n【2】Reports of OPXV infections in animals and humans have largely increased during recent decades, which has enhanced their zoonotic potential and led to the perception of an increasing risk for humans . For cats, there are several reports of poxvirus infections, but the causative agent has been characterized as CPXV  or has not been characterized .\n\n【3】We report a case of fatal infection with an OXPV in a household cat. This virus was more closely related to ECTV than to CPXV, putatively representing a novel OPXV species.\n\n【4】### Materials and Methods\n\n【5】##### Case Study\n\n【6】A domestic, short-haired, male, 6-month-old cat was brought to a veterinarian because of multicentric, nodular, ulcerative dermatitis . The cat was regularly vaccinated for common feline diseases (feline panleukopenia, rhinhotracheitis, calicivirosis, and chlamydiosis) and showed negative test results for retroviral infections. An antiparasitic product had been applied monthly (Frontline Combo Spot On; Merial, Ingelheim, Germany). The cat was fed a balanced commercial diet and lived indoors, but it had access to outdoors and had a hunting behavior.\n\n【7】The cat had multiple nodular, plaque-like, ulcerative lesions on its body, particularly on the feet and face. Results of diagnostic testing, including a wood lamp examination, skin scrapings, trichogram, and fungal culture, were negative. Cytological examination showed a mixed inflammatory population of cells with a relevant amount of eosinophils. A blood test showed only a mild leukocytosis with an increase in numbers of lymphocytes, neutrophils, and eosinophils. Because of rapid worsening of its clinical conditions, the cat was euthanized.\n\n【8】##### Histopathologic Analysis\n\n【9】We collected multiple skin biopsy specimens for histopathologic analysis by using an 8-mm biopsy punch and fixed these specimens in 10% buffered formalin. Samples were embedded in paraffin, sectioned, and stained with hematoxylin and eosin, according to standard protocols.\n\n【10】##### DNA Extraction and PCR Amplification\n\n【11】An OPXV infection was suspected on the basis of clinical presentation and histopathologic analysis. Therefore, we processed histologic preparations for molecular investigations to confirm the presumptive diagnosis. We purified total DNA from a thin section of ≈20 mg of formalin-fixed, paraffin-embedded tissue by using the DNeasy Blood and Tissue Kit (QIAGEN, Hilden, Germany) according to the manufacturer’s instructions.\n\n【12】We tested the DNA extract by using 2 panchordopoxvirus PCRs specific for the variable GC content of the genera included in the subfamily _Chordopoxvirinae_ and other unclassified chordopoxviruses . For the low GC content PCR, we used DNA from a laboratory vaccinia virus Western Reserve strain (VACV-WR) as a positive control. For the high GC content PCR, we used an Orf virus isolated during an outbreak of contagious ecthyma as a positive control. We initially conducted subsequent identification of OPXV by using a PCR specific for the gene coding A-type inclusion protein , followed by a second PCR specific for the hemagglutinin (HA) gene . In addition, we performed 2 species-specific PCRs, 1 for ECTV and 1 for CPXV, to further characterize the virus .\n\n【13】We conducted all PCR amplifications by using an LA PCR Kit (version r.2.1) (Takara Bio, Tokyo, Japan) in a 50-μL reaction containing 1 mmol/L of primers, LA PCR Buffer (Mg 2+  ), 8 μL of dNTP mixture (corresponding to 400 mmol/L of each dNTP), 2.5 units of TaKaRa LA Taq polymerase, and 1 μL of template DNA. The cycling protocol used for each assay was programmed as described . PCR products were subjected to electrophoresis on a 1.5% agarose gel containing a fluorescent nucleic acid marker (GelRed; Bio-Rad Laboratories, Hercules, CA, USA) at 80 V for 45 min and visualized under fluorescent light on the Gel Doc EZ Imaging System with Image Laboratory Software (Bio-Rad Laboratories). PCR products were directly sequenced by Eurofins Genomics GmbH (Ebersberg, Germany). We manually edited and analyzed sequences by using the Geneious platform version 10.1.3 (Biomatters Ltd. Auckland, New Zealand).\n\n【14】##### Virus Isolation\n\n【15】After diagnosis of OPXV infection, we collected additional biopsy specimens from skin lesions of the diseased cat intravitam and used them for subsequent virologic investigations. For virus isolation, we used African green monkey kidney fibroblast CV-1 cells and African green monkey kidney epithelial Vero cells. Cells were grown in Dulbecco’s modified minimum essential medium (DMEM) supplemented with 10% fetal bovine serum. Tissues were homogenized in DMEM (10%, wt/vol) and centrifuged at 8,000 × _g_ for 10 min. Supernatants were treated with antimicrobial drugs (penicillin 5,000 IU/mL, streptomycin 2,500 μg/mL, and amphotericin B 10 μg/mL) for 30 min, inoculated on partially confluent CV-1 and Vero cell cultures, and incubated at 37°C in a 5% CO 2  incubator. After an adsorption period of 45 min, DMEM was added. Cells were observed daily for cytopathic effects.\n\n【16】For hematoxylin and eosin staining and indirect immunofluorescence (IIF) assay, we grew cells on coverslips placed in 12-well plates. Cells were mock- or virus-infected and coverslips were harvested at 48 hours postinfection. For detection of inclusion bodies, we fixed cells in Bouin solution for 2 h and stained them with hematoxylin and eosin. For the IIF assay, cells were fixed with 80% acetone for 30 min. Coverslips were rinsed twice with phosphate-buffered saline and incubated 30 min in a humidified chamber at 37°C with a serum sample (diluted 1:50) collected from the ill cat. Coverslips were washed twice with phosphate-buffered saline and incubated with goat anti-cat IgG conjugated with fluorescein isothiocyanate (Sigma-Aldrich, Milan, Italy).\n\n【17】The homogenate of skin biopsy specimens was inoculated onto the chorioallantoic membrane of 12-day-old chick embryos. After 2 days of incubation at 37°C, membranes were collected from the eggs and pock morphology was observed.\n\n【18】##### Electron Microscopy\n\n【19】We performed negative staining and electron microscopic analysis of homogenates of skin punch biopsy specimens and supernatants of infected Vero cells that showed an evident cytopathic effect. Samples were frozen and thawed twice and centrifuged at 4,000 × _g_ for 20 min and at 9,300 × _g_ for 10 min to clarify the supernatant. The second supernatant (82 μL) was then ultracentrifuged in an Airfuge centrifuge (Beckman Coulter, Brea, CA, USA) for 15 min at 21 lbs/in 2  (82,000 × _g_ ). The Airfuge was fitted with an A 100 rotor that held six 175-μL test tubes containing specific adapters for 3-mm grids, which enables direct pelleting of virus particles on carbon-coated, formvar copper grids. These grids were stained with 2% sodium phosphotungstate, pH 6.8, for 1.5 min, and observed with a Tecnai G2 Biotwin Transmission Electron Microscope (Field Electron and Ion Company, Hillsboro, OR, USA) operating at 85 kV. We identified poxvirus particles, observed at magnifications of 11,000×–26,500×, on the basis of their typical morphologic characteristics.\n\n【20】##### Serologic Analysis\n\n【21】We tested the serum sample collected intravitam from the diseased cat for OPXV antibodies by virus neutralization and IIF assays. We used strains Italy\\_09/17 isolated from the same cat and VACV-WR in these tests.\n\n【22】For the virus neutralization test, we mixed 2-fold dilutions of heat-inactivated serum (starting at a dilution of 1:2) with 100 50% tissue culture infective doses of virus in 96-well microtiter plates. After incubation at room temperature for 60 min, 2 × 10 4  CV-1 cells were added to each well. Plates were read after 4 days of incubation at 37°C in a humidified atmosphere of 5% CO 2  .\n\n【23】For the IIF assay, we fixed confluent monolayers of CV-1 cells grown on coverslips and infected with strain Italy\\_09/17 or VACV-WR with 80% acetone. We tested 2-fold dilutions of heat-inactivated serum (diluted 1:20 to 1:5,120) by using 1 coverslip/dilution. Goat anti-cat IgG conjugated with fluorescein isothiocyanate was used as a secondary antibody (Sigma-Aldrich).\n\n【24】##### Next-Generation Sequencing\n\n【25】For DNA extraction, we obtained virus stocks from semipurified virus particles. In brief, we infected CV-1 cells with strain Italy\\_09/17. At 48 hours postinfection, the cell medium was collected and nuclei and cell debris were discarded by centrifugation at 1,000 × _g_ for 10 min at 4°C. We extracted virus DNA by using a QIAamp Cador Pathogen Mini Kit (QIAGEN) according to the manufacturer’s instructions.\n\n【26】We quantified DNA by using the Fluorometric Qubit dsDNA High Sensitivity Assay Kit (Thermo Fisher Scientific, Waltham, MA, USA). We prepared a genomic DNA library by using the Nextera DNA Sample Prep Kit (Illumina, San Diego, CA, USA) according to the manufacturer’s protocol and performed a size-selection step manually by using Ampure XP magnetic beads (Beckman Coulter). We performed quality control analysis of the sample library by using the QIAxcel Advanced System with QIAxcel ScreenGel Software 1.4.0 (QIAGEN). We normalized library samples as suggested by QIAGEN and performed sequencing by using a MiSeq instrument, version 2, and a MiSeq Reagent Kit (Illumina).\n\n【27】##### Genome Annotation and Comparison\n\n【28】We obtained 1,497,762 paired reads in next-generation sequencing (NGS) experiments (Illumina); these reads had an average length of 155.4 bp. We performed quality control of reads by using FastQC  and sequence trimming by using the plugin Trim Ends in Geneious software version 10.1.3 . We filtered NGS sequences by using the genome of African green monkey ( _Chlorocebus sabeus_ ), which yielded 217,236 unmapped reads. We used these unmapped reads for de novo assembling of the feline OPXV genome by using the Geneious Assembler.\n\n【29】We annotated the nearly full-length genome sequence of the Italy\\_09/17 isolate by using ECTV strain Naval as reference . We performed genome annotation by using FindORFs software in Geneious version 10.1.3 and a set of reference sequences, including ECTV Naval , ECTV Mos , CPXV BR , and VACV COP  for comparison. We further analyzed open reading frames that remained unassigned or with a lower similarity to the reference sequences by using MyOrfeome .\n\n【30】##### Phylogenetic Analysis\n\n【31】For characterization of the OPXV strain, we used the strategy proposed by Emerson et al. We selected 9 coding sequences from the genome of the feline OPXV strain Italy\\_09/17: A7L, early transcription factor/VETF large subunit; A10L, major core protein; A24R, RNA polymerase 132; D1R, messenger RNA capping enzyme, large subunit; D5R, DNA-independent NTPase (DNA replication); E6R, hypothetical protein; E9L, DNA polymerase; H4L, RNA polymerase-associated protein; and J6R, RNA polymerase 147. Gene designations refer to the VACV COP genome. We aligned concatenated genome sequences of OPXVs representative of North American and Old World (African and Eurasian) viruses by using Geneious version 10.1.3 and the MAFFT algorithm . After searching the GenBank database, we retrieved complete HA gene sequences of 2 feline-derived human OPXV strains (accession nos. EF612709 and FJ445747) and of an OPXV isolated from captive macaques  and aligned them with cognate OPXV sequences.\n\n【32】We performed phylogenetic analysis for concatenated DNA alignments with Bayesian inference by using 4 chains run for >1 million generations . We used ModelTest software  to identify the most appropriate model of evolution for the entire dataset and for each gene individually. The identified program settings for all partitions, under the Akaike Information Criteria, included 6 character states (general time reversible model), a proportion of invariable sites, and a gamma distribution of rate variation across sites. We deposited nucleotide sequences of strain Italy\\_09/17 used for phylogeny in GenBank (accession nos. MF578930–9).\n\n【33】##### Detection of Other Pathogens\n\n【34】We subjected nucleic acids extracted from freshly collected skin biopsy specimens and serum of the affected cat to a TaqMan assay for detection of canine parvovirus 2/feline panleukopenia virus  and to a minor groove binder probe assay for rapid discrimination between true feline panleukopenia virus strains and antigenic variants of canine parvovirus 2 . We also used DNA extracts to detect proviral DNA of feline immunodeficiency virus  and feline leukemia virus  and DNA of feline hemoplasmas  and feline herpesvirus . We screened RNA extracts by real-time PCR or conventional reverse transcription PCR specific for carnivore coronaviruses  and caliciviruses .\n\n【35】### Results\n\n【36】##### Histopathologic Analysis\n\n【37】Histopathologic analysis of multiple skin specimens showed mild hyperplasia of the epidermis and the follicular wall. Many roundish to oval brightly eosinophilic inclusion bodies were clearly evident in the cytoplasm of both epidermal and follicular keratinocytes, including in a few sebocytes. The morphology of cells suggested a possible OPXV infection . Nodular to diffuse dermatitis caused by mixed inflammatory cells was also present in dermis and hypodermis; those cells were mainly represented by eosinophils, histiocytes, and lymphocytes, together with few plasma cells and neutrophils.\n\n【38】##### Molecular Investigations\n\n【39】Molecular analysis of formalin-fixed, paraffin-embedded tissues showed positive results for the low GC panchordopoxvirus PCR and negative results for the high GC panchordopoxvirus PCR . This pattern of amplification was consistent with an OPXV infection. Amplification of the gene coding for the A-type inclusion protein generated an amplicon of ≈1,237 bp, and amplification of the HA gene  generated an amplicon of 846 bp, which are expected sizes for these genes in ECTV. All samples collected from the cat were negative for other pathogens by the molecular assays used.\n\n【40】Sequence analysis of the HA gene of strain Italy\\_09/17 showed high nucleotide identity (98%) with that of CPXV strain Germany  and to feline-derived human poxvirus IT1 . In addition, strain Italy\\_09/17 was also highly related to most of the ECTV strains in GenBank; the highest (97%) nucleotide identity was with ECTV strain Naval . Strain Italy\\_09/17 showed positive results in the ECTV-specific PCR and negative results in the CPXV-specific PCR  .\n\n【41】##### Virus Isolation\n\n【42】Virus isolation from freshly collected skin biopsy specimens was successful with Vero and CV-1 cells. We observed a cytopathic effect at 48 hours postinfection that showed rounding of cells, increased granularity, and detachment from the monolayer. In Vero cells, a cytopathic effect was less evident than in CV-1 cells. Cells stained with hematoxylin and eosin contained large eosinophilic cytoplasmic inclusion bodies that were compatible with infection by poxviruses, including CPXV  and ECTV  .\n\n【43】The IIF assay showed granular fluorescence areas that displayed the morphology of the inclusion bodies in cell cytoplasms . Both CV-1 and Vero cells showed positive IIF assay results, but there was no fluorescence staining in the negative control.\n\n【44】At 48 hours postinoculation on embryonated eggs, virus produced superficial pocks on the chorioallantoic membrane. Most of these pocks were small (diameter 1.0 mm), gray, and had central hemorrhages. Few (3%–5%) pocks were larger (1.8 mm in diameter), white, and without hemorrhages.\n\n【45】##### Electron Microscopy\n\n【46】Many typical brick-shaped virions (≈320 × 240 nm) morphologically related to the genus _Orthopoxvirus_ were observed by negative staining and electron microscopy. We observed these results for skin punch biopsy specimens and cell culture supernatants.\n\n【47】As in a previous study , few particles showed the characteristic ribbon structure of the M form of vaccinia virus  , which is usually prevalent in fresh preparations collected during acute-phase infections. Most virions were slightly larger, showed a uniform electron density, and had a thick capsule outlined by a ragged edge (i.e. the morphologic aspect known as the C form), which are less infective and prevalent during evolution of a chronic infection.\n\n【48】##### Serologic Analysis\n\n【49】The infected cat was negative by virus neutralization for strain Italy\\_09/17 and reference VACV isolates. However, the IIF assay detected antibody titers of 1:1,280 for virus Italy\\_09/17 and 1:640 for VACV-WR.\n\n【50】##### Identification of a Novel OPXV by NGS\n\n【51】We used 217,236 paired reads for de novo assembling and obtained 3 contigs (contig one, 195,015 bp; contig two, 21,014 bp; and contig three, 1,596 bp) and a quality score >99%. The mean coverage of the assembled contigs was 61×. The 9 open reading frames (A7L, A10L, A24R, D1R, D5R, E6R, E9L, H4L, and J6R) used for OPXV characterization were mapped in contig 1, and their sequences (total 27,228 nt) were concatenated and aligned with concatenated cognate sequences of selected OPXVs. In addition, because the HA gene of 2 feline-derived human virus isolates was available in the sequence databases, we performed an alignment based on the HA gene. We conducted phylogenetic analysis on the basis of the 9 concatenated sequences by using Bayesian inference. Posterior probabilities percentages were consistently high ( \\> 90%) for all clades on phylograms, which supported inferred phylogenetic relationships.\n\n【52】In the consensus phylogenetic tree , we found that strain Italy\\_09/17 was distantly related to other OPXVs, including all 10 CPXV lineages  and other recently identified, deep-branching OPXVs  and displayed a closer relatedness with ECTV prototypes, albeit forming a separate cluster. This cluster also included strain Abatino, which was recently isolated from a poxvirus outbreak in a captive colony of Tonkean macaques in Italy . Nucleotide identity of strain Italy\\_09/17 with strain Abatino was 99.66% and identity with reference ECTVs was 98.11%–98.13%. Higher nucleotide identities were found among ECTVs (99.97%–99.99%) and between VARV-Garcia1966 (variola minor) and VARV-India1967 (variola major) (99.68%). Thus, on the basis of current OPXV criteria of species demarcation, the cat and macaque isolates should be considered prototypes of a novel OPXV. Also, for the HA gene, strain Italy\\_09/17 appeared more closely related to strain Abatino (99.79% nt identity) than to feline-derived human OPXV strains (95.83%–95.99% nt identity) that were identified in Italy in 2009, for which a full-length genome and concatenated genes used for species demarcation using phylogeny are not available .\n\n【53】### Discussion\n\n【54】OPXV infection in cats is frequently observed, and OPXV transmission from cats to humans has been demonstrated or at least suspected on several occasions . Cats are susceptible to CPXV infection, for which they represent only incidental hosts, as are humans, cattle, horses, and dogs. The virus is usually transmitted to cats by hunted rodents; cat-to-cat transmission is apparently rare . In contrast, ECTV has a host range restricted to laboratory mice, and cat or human infections have not been reported .\n\n【55】Additional OPXVs, such as raccoonpox virus and skunkpox virus, have been reported in wildlife and infect carnivores . Conversely, cats have been found to be susceptible to members of the genus _Parapoxvirus_ , raccoonpox virus, and uncharacterized poxviruses . There are >400 reports of OPXV infection in domestic cats, but the total number of feline cases is considered to be much greater . Despite this large number of reports, genetic characterization of the detected poxvirus has been achieved in only a few instances. Thus, circulation in cats of other OPXVs cannot be ruled out.\n\n【56】We report detection of an OPXV strain that caused a fatal infection in a cat. The virus was not a classical CPXV, which is common in felids. Analysis of 9 concatenated genes showed that the poxvirus detected was only distantly related to all CPXV lineages currently known and formed a separate cluster with respect to ECTV, with which it was strictly related and grouped with an OPXV strain recently isolated from captive macaques in Italy . These 2 viruses had lower genetic identity with ECTV than that observed with reference ECTVs and between variola minor virus and variola major virus. Therefore, these ECTV-like poxviruses likely represent a novel OPXV species. However, the true animal reservoir of this novel OPXV needs to be assessed, and the idea that wild rodents can act as carriers for the new virus cannot be ruled out.\n\n【57】If one considers the close relatedness between strain Italy\\_09/17 and ECTV, which has been detected only in laboratory animals, it could be speculated that an ECTV-like virus circulating in wild rodents has resulted in ECTV strains adapted to laboratory mice. Alternatively, an ECTV strain might have escaped from laboratory mice and adapted to wild conditions. In addition, the zoonotic potential of the feline ECTV-like OPXV deserves an in-depth investigation. Feline poxvirus was also related to an unclassified OPXV, which was detected in a human in Italy almost 10 years ago and for which only partial HA gene has been identified . Consequently, this feline poxvirus could represent a threat to human health. Thus, veterinarians and cat breeders and owners should be aware of this additional risk associated with handling of cats with skin lesions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "86e843c3-6222-4178-b28d-440efdc4bd47", "title": "Laboratory-Confirmed Avian Influenza A(H9N2) Virus Infection, India, 2019", "text": "【0】Laboratory-Confirmed Avian Influenza A(H9N2) Virus Infection, India, 2019\nLow-pathogenicity avian influenza A(H9N2) viruses have a wide host range, and outbreaks in poultry have been recorded since the 1990s in China . In India, avian specimens indicated no serologic evidence of H5N1 and H9N2 during 1958–1981 ; however, 5%–6% persons with direct exposure to poultry had H9N2 antibodies . Human cases of influenza H9N2 virus infection have been observed in Hong Kong, China, Bangladesh, and Pakistan .\n\n【1】An institutional review board approved an ongoing community-based surveillance in 93 villages of Korku tribes in Melghat District, Maharashtra State, India, to determine incidence of respiratory syncytial virus (RSV)–associated deaths among children <2 years of age. A total of 2,085 nasopharyngeal swabs from children with severe or fatal pneumonia were transported to India’s National Institute of Virology to test for influenza, RSV, and other respiratory viruses. A nasopharyngeal swab from a 17-month-old boy received on February 12, 2019, tested positive by PCR for influenza A(H9N2) virus.\n\n【2】The child, a resident of Melghat, had fever, cough, breathlessness, and difficulty feeding for 2 days after illness onset on January 31, 2019. His high intermittent grade fever had no diurnal variation and no association with rash or mucocutaneous lesions. Examination revealed a conscious, restless child with a respiratory rate of 48 breaths/min and lower chest wall in-drawing with intermittent absence of breathing for \\> 20 seconds. He was fully immunized for his age, with bacillus Calmette–Guérin, diphtheria, hepatitis B, poliovirus, and measles vaccines. Both length and weight for age were less than −3 SD. History of travel with his parents to a local religious gathering 1 week before symptom onset was elicited. The father had similar symptoms on return from the gathering but could not undergo serologic testing because of his migrant work. No history of poultry exposure was elicited. The child received an antibacterial drug and antipyretics and recovered uneventfully.\n\n【3】We tested the clinical sample using duplex real-time PCR for influenza A/B, H3N2, and 2009 pandemic H1N1 viruses; RSV A/B; human metapneumovirus; parainfluenza virus types 1–4; rhinovirus; and adenovirus. The sample was strongly positive for influenza A virus (cycle threshold value 20) but negative for seasonal influenza viruses and all respiratory viruses. Real-time PCR analysis for avian influenza viruses H5N1, H7N9, H10N8, and H9N2 revealed positivity for H9N2 virus (cycle threshold value for H9 was 25). We confirmed this result by sequencing the matrix (M) and hemagglutinin (HA) genes of the isolate, designated A/India/TCM2581/2019/(H9N2); the M gene (260 bp) had 97.27% nucleotide identity with A/chicken/India/99321/2009(H9N2), and the HA gene (225,478 bp) had 96.93% nucleotide identity with A/chicken/India/12CL3074/2015(H9N2).\n\n【4】We then generated whole-genome sequences by using the Miniseq NGS Platform  and a de novo assembly program (CLC Genomics Software 10.1.1 ). We used MEGA7  with a Tamura-Nei nucleotide substitution model including 1,000 replicates bootstrap support  for evolutionary analysis of 8 genes of A/India/TCM2581/2019/(H9N2) (submitted to GenBank under accession nos. MK673893–900). The HA, neuraminidase, and nucleoprotein gene phylogeny of A/India/TCM 2581/2019/(H9N2) grouped with the dominant G1 lineage (h94.1.1) and clustered with poultry strains from India and human strains from Bangladesh . The M, nonstructural, polymerase basic 1, polymerase basic 2, and polymerase acidic genes were related to an H7N3 isolate from Pakistan  . We confirmed that the A/India/TCM2581/2019(H9N2) strain had low pathogenicity, showing a KSKR/GLF amino acids motif at the cleavage site of HA (335−341 \\[H9 numbering\\]). We observed 6 potential glycosylation sites (11, 87, 123, 280, 287, and 472 \\[H9 numbering\\]) and loss of 2 sites (208 and 218 \\[H9 numbering\\]) in the HA gene of A/India/TCM2581/2019(H9N2) with respect to G1 viruses.\n\n【5】The virus was susceptible to adamantanes with S31 and to neuraminidase inhibitor with R292 and E119 (N2 numbering) . A/India/TCM2581/2019(H9N2) had Q226L and I155T in HA gene, which promote the human receptor binding. Compared with G1 vaccine strain A/Hong Kong/1073/99, the study strain had multiple mammalian-specific mutations that already exist in poultry-adapted H9N2. The study strain had amino acid changes R207K, H436Y, and M677T in the polymerase basic 1 gene; A515T in the polymerase acidic 1 gene; N30D, T215A, and T139A (all H3 numbering) in the matrix 1 gene; and P42S in the nonstructural 1 gene, all of which are known to be associated with mammalian host specificity and increased virulence in ferrets and mice . Known markers for virulence and transmission (E627K and D701N) in the polymerase basic 2 gene in the study strain were absent .\n\n【6】Bayesian evolutionary analyses using BEAST version 1.8.1  of the HA gene of H9N2 poultry strains from India indicated 3 clusters of multiple introductions at the estimated node age of 2000–2001 . Human strain A/India/TCM2581/2019(H9N2) and the other poultry viruses from India evolved with 5.163 × 10 –3  substitutions/site/year.\n\n【7】In conclusion, multiple introductions of H9N2 viruses in poultry have been observed in India. The identification of a human case of H9N2 virus infection highlights the importance of systemic surveillance in humans and animals to monitor this threat to human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ec1d762e-70bc-4493-a86c-9c753f84739d", "title": "in Healthcare Settings", "text": "【0】in Healthcare Settings\nOn this Page\n\n【1】*   How common are these infections?\n*   Who is at risk?\n*   How is it spread?\n*   How can you avoid getting an infection?\n*   How are these infections treated?\n*   What is CDC doing to address _Acinetobacter_ infections?\n\n【2】_Acinetobacter_ is a group of bacteria (germs) commonly found in the environment, like in soil and water. While there are many types, the most common cause of infections is _Acinetobacter_ _baumannii_ , which accounts for most _Acinetobacter_ infections in humans.\n\n【3】_Acinetobacter_ _baumannii_ can cause infections in the blood, urinary tract, and lungs (pneumonia), or in wounds in other parts of the body. It can also “colonize” or live in a patient without causing infections or symptoms, especially in respiratory secretions (sputum) or open wounds.\n\n【4】See CDC’s report  \nAntibiotic Resistance Threats in the United States,\n\n【5】These bacteria are constantly finding new ways to avoid the effects of the antibiotics used to treat the infections they cause. Antibiotic resistance occurs when the germs no longer respond to the antibiotics designed to kill them. If they develop resistance to the group of antibiotics called carbapenems, they become carbapenem-resistant. When resistant to multiple antibiotics, they’re multidrug-resistant. Carbapenem-resistant _Acinetobacter_ are usually multidrug-resistant.\n\n【6】### How common are these infections?\n\n【7】In 2017, carbapenem-resistant _Acinetobacter_ caused an estimated 8,500 infections in hospitalized patients and 700 estimated deaths in the United States .\n\n【8】### Who is at risk?\n\n【9】_Acinetobacter_ infections typically occur in people in healthcare settings. People most at risk include patients in hospitals, especially those who:\n\n【10】*   are on breathing machines (ventilators)\n*   have devices such as catheters\n*   have open wounds from surgery\n*   are in intensive care units\n*   have prolonged hospital stays\n\n【11】In the United States, _Acinetobacter_ infections rarely occur outside of healthcare settings. However, people who have weakened immune systems, chronic lung disease, or diabetes may be more susceptible.\n\n【12】### How is it spread?\n\n【13】_Acinetobacte_ r can live for long periods of time on environmental surfaces and shared equipment if they are not properly cleaned. The germs can spread from one person to another through contact with these contaminated surfaces or equipment or though person to person spread, often via contaminated hands.\n\n【14】### How can you avoid getting an infection?\n\n【15】Patients and caregivers should:\n\n【16】*   keep their hands clean to avoid getting sick and spreading germs that can cause infections\n    *   wash their hands with soap and water or use alcohol-based hand sanitizer, particularly before and after caring for wounds or touching a medical device\n*   remind healthcare providers and caregivers to clean their hands before touching the patient or handling medical devices\n*   allow healthcare staff to clean their room daily when in a healthcare setting\n\n【17】In addition to hand hygiene, healthcare providers should pay careful attention to recommended infection control practices, including rigorous environmental cleaning (e.g. cleaning of patient rooms and shared equipment), to reduce the risk of spreading these germs to patient.\n\n【18】### How are these infections treated?\n\n【19】_Acinetobacter_ infections are generally treated with antibiotics. To identify the best antibiotic to treat a specific infection, healthcare providers will send a specimen (often called a culture) to the laboratory and test any bacteria that grow against a set of antibiotics to determine which are active against the germ. The provider will then select an antibiotic based on the activity of the antibiotic and other factors, like potential side effects or interactions with other drugs.\n\n【20】Unfortunately, many _Acinetobacter_ germs are resistant to many antibiotics, including carbapenems, which makes them difficult to treat with available antibiotics.\n\n【21】### What is CDC doing to address _Acinetobacter_ infections?\n\n【22】CDC tracks the germ, and the infections it can cause, through its Emerging Infections Program . Additionally, CDC works closely with partners, including public health departments, other federal agencies, healthcare providers, and patients, to prevent healthcare infections and to slow the spread of resistant germs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f5f37eb7-2c89-43eb-b0e1-aa288f2a4291", "title": "Q Fever Osteoarticular Infection in Children", "text": "【0】Q Fever Osteoarticular Infection in Children\nQ fever is a zoonotic disease caused by the intracellular bacterium _Coxiella burnetii_ . Persistent focalized Q fever infection in adults mainly manifests as endocarditis or as an endovascular infection. Cases of osteoarticular infection (OAI) have been scantly reported in the literature rarely in children . Disease severity varies, similar to the clinical variations reported in adult patients .\n\n【1】_C. burnetii_ infections are endemic to Israel. Because diagnosis requires a high level of suspicion, an increase in diagnoses over time may be partly related to physician awareness of the disease rather than true higher incidence . An observational study of 2,434 cases of _C. burnetii_ infection in France  reported 58 pediatric cases, among which 22 (38%) were OAIs. This large study described the clinical characteristics of Q fever, the less common manifestations of Q fever such as lymphadenitis and lymphoma, and identified risk factors and screening tools predicting complications and death.\n\n【2】Because _C. burnetii_ bacteria do not grow in standard laboratory cultures, serology is the first-line diagnostic method for _C. burnetii_ infection. Phase II antibodies are predominant during primary infection and Phase I antibodies in persistent infection. Cutoffs of titers considered positive are debated and vary in different countries . Immunofluorescence assay (IFA) remains the preferred serology test because of its simplicity and accuracy. Complement fixation test (CFT) is more widely used despite its lower sensitivity . Immunohistochemistry and quantitative PCR of _C. burnetii_ –infected tissues are also available .\n\n【3】Diagnosis may be aided by clinical criteria. One definite criterion, 2 major criteria, or 1 major and 3 minor criteria are needed for definitive diagnosis of persistent Q fever. Definite criteria include a positive result on culture, PCR, or immunochemistry of bone, synovial biopsy, or joint aspirate. Major criteria include positive blood culture or PCR, phase I IgG antibodies \\> 800, evidence of bone or joint involvement by computed tomography scan, ultrasonography, magnetic resonance imaging (MRI), or abnormal positron emission tomography scan or indium leukocyte scan. Minor criteria include phase I IgG titer of 400–800 mg/dL, temperature \\> 38°C, and mono- or polyarthralgia. These last diagnostic criteria have been proposed to enable diagnosis of _C. burnetii_ persistent infection in cases in which titers are below the serologic cutoff . Other studies use a higher serology cutoff of phase I IgG \\> 1,024 .\n\n【4】Optimal antimicrobial treatment for chronic Q fever OAI has not been well established. Pediatric treatment recommendations in Q fever OAI are based on treatment of Q fever endocarditis in adults . We describe 3 cases of Q fever osteomyelitis in children in Israel and a review of the related literature.\n\n【5】### Case\n\n【6】A previously healthy 3-year-old boy was admitted for care with a limp of his right leg and swelling of his right ankle that began 3 weeks before admission. He had no history of trauma and had no fever or other systemic signs of infection. Results of complete blood count (CBC), C-reactive protein (CRP), and radiographic studies at admission were normal. MRI was performed and showed a lytic lesion in the talus bone, suspected to be a malignant space-occupying lesion . Open-bone biopsy was thus performed. Pathology revealed an acute inflammatory process with neutrophil and lymphocyte predominance, giant cells, and an epithelioid granuloma without necrosis, suggesting an infectious process . A swab sample from the tissue was found to be sterile despite the lack of previous antimicrobial therapy. Fungal and mycobacterial PCR results from the paraffin-embedded specimen were negative. Repeated physical examinations revealed signs of cellulitis around the surgical wound with no other systemic manifestations. The patient was treated with a first-generation cephalosporin for 6 weeks, and his clinical signs and symptoms were resolved completely.\n\n【7】Six months after his discharge, the child experienced swelling and mild cellulitis around his right ankle with no other symptoms. Synovial fluid from the ankle was sampled; bacterial, fungal, and mycobacterial cultures were all negative. The patient then recovered without any treatment. At 1 year after his initial admission, the patient experienced cellulitis at the same site. He was in good general health with no systemic signs of infection. Fluid aspirated from the right ankle, identified as pus, was cultured and analyzed using 16SrRNA PCR. Antimicrobial therapy with a first-generation cephalosporin was reinitiated, with good clinical response. Later in the treatment period, PCR results were found to be positive for _C. burnetii_ . IFA confirmed the diagnosis with high titer for phase I IgG, 1:6,400. Transthoracic echocardiography performed 1 month after diagnosis showed no valve involvement.\n\n【8】In light of clinical and radiological evidence of chronic osteomyelitis along with laboratory evidence (positive 16Sr RNA PCR and positive serologic results for _C. burnetii_ ), the patient was treated for persistent focalized Q fever. Treatment regimen included ciprofloxacin and rifampin for 12 months; full clinical and complete radiological resolution resulted.\n\n【9】Six years after his first infection, after being asymptomatic for 4 years, the patient again experienced pain and tenderness of the contralateral (left) foot and ankle without fever or systemic signs. Results of laboratory studies were unremarkable, but MRI of the ankle showed a Brodie’s abscess of the distal tibia and sonography of the ankle showed a small amount of fluid. Results of testing for 16SrRNA from synovial fluid was negative. IFA revealed high titers for phase I IgG, 1:3,200. On the basis of the patient’s history, clinical signs, and serology, treatment was begun with doxycycline, hydroxychloroquine, and rifampin. Shortly after treatment was started, the patient reported a new elbow pain. MRI study of the right elbow showed synovitis of the elbow and Brodie’s abscess of the distal humerus. Based on the presumed diagnosis of Q fever multifocal recurrent osteomyelitis, triple antibiotic treatment was continued, and substantial improvement was seen by 4 months later. MRI study at the end of treatment revealed complete resolution of all pathological findings. Nearly 12 months after completing his treatment, the patient was asymptomatic.\n\n【10】### Case\n\n【11】A previously healthy 2-year-old boy experienced limping for >3 weeks and had a recurrent low-grade fever in the week before admission. He had no history of trauma, exposure to animals, or ingestion of unpasteurized dairy products. At the time of admission, a physical examination noted obvious limping on his left leg but no localized tenderness or focal inflammatory signs. Results of CBC, CRP, and radiographic imaging of his lower limbs were unremarkable. Nuclear imaging (Technetium bone scan) demonstrated an increased signal in the talus of his left ankle . MRI study of the left ankle showed an intramedullary lesion of the talus compatible with an abscess . Empiric treatment with intravenous first-generation cephalosporin was initiated. After 2 weeks of therapy, the patient was only mildly improved. Bone biopsy was not performed because the location of the lesion was unreachable; serology for _C. burnetii_ was performed, considering the prolonged symptoms and suboptimal response to therapy. After 6 weeks of antimicrobial therapy, although the patient no longer had symptoms or signs of infection, the _C. burnetii_ serology result was unexpectedly positive: phase I IFA IgG titer was 1:200.\n\n【12】Confirmatory serology performed 2 weeks later showed a phase I IFA IgG titer of 1:800. At that time, the patient remained asymptomatic. A repeated MRI of the ankle showed the same lesion with no major changes. Results of a transthoracic echocardiogram, performed 3 months after initial care because of concern for possible endocarditis, were unremarkable. Considering the chronic course of Q fever OAI and the risk for relapse or progression to other manifestations, combined therapy of rifampin and trimethoprim/sulfamethoxazole (TMP/SMX) was initiated. The patient completed 1 year of treatment, throughout which he continued to be asymptomatic. MRI 12 months after treatment showed complete resolution of the primary talar lesion. Phase I IFA IgG titer at the end of treatment was 1:200.\n\n【13】### Case\n\n【14】A previously healthy 3-year-old boy experienced swelling and tenderness over his left foot and calcaneus for 6 months before his admission. His body temperature was normal over that period, and he had no systemic signs of infection. Two falls and mild bruises of the same leg were reported around the time his symptoms began. The patient lived in a rural area and was exposed to livestock. Results of CBC, CRP, erythrocyte sedimentation rate (ESR), and radiographic studies conducted at the time of admission were unremarkable. MRI demonstrated synovitis of the small joints of the midfoot . The patient underwent fine-needle aspiration of the ankle, which was technically difficult. Synovial fluid from this aspiration was sterile despite no previous antimicrobial therapy; results of 16SrRNA and specific PCR for _C. burnetii_ were negative.\n\n【15】Serology for Q fever was positive with high titer of phase I IgG, 1:1600, and of phase II, 1:1600, in IFA. Transthoracic echocardiography performed 1 month later showed no cardiac valve involvement. On the basis of the suggestive clinical signs, potential exposure to farm animals, and positive serology for _C. burnetii_ , the patient was treated with rifampin and TMP/SMX for Q fever OAI. Repeated serology showed increasing titers: IgG phase II up to 6,400, and IgG phase I up to 3,200. Despite 8 months of antimicrobial treatment, the patient remained symptomatic with debilitating pain. MRI performed at that time revealed synovitis and osteomyelitis with intramedullary abscesses in the small bones of the midfoot, not amenable for drainage. Twelve months into his antimicrobial therapy, his health had improved overall, with decreasing pain and decreasing IgG phase I titers.\n\n【16】### Literature Review\n\n【17】A review of the literature revealed 8 articles describing 29 pediatric cases of OAI caused by _C. burnetii_  . One of the 3 cases we describe was also described briefly in a published case series . Of the 31 total patients described, 24 (77.41%) were male; 29 (93.54%) were <10 years of age. All were previously healthy except for 1 patient who had acute lymphoblastic leukemia. Exposure to livestock or household animals was reported in most cases. Only 5 of 10 patients in Israel, including 2 of our 3 case-patients, led urban lifestyles .\n\n【18】Clinical manifestations were subtle or occult for 20 (65%) of patients. In cases in which laboratory tests were reported, CBC and CRP results were mostly within reference ranges. ESR values, considered superior to other acute-phase reactants for assessment of chronic infections, were not reported for most cases of Q fever OAI.\n\n【19】In cases in which biopsies were performed, histology was consistent with inflammation and noncaseating granulomas. Results of bone or synovial biopsy and joint aspirate cultures were always negative; results for 16-S rRNA gene PCR for _C. burnetii_ were positive in most of the joint aspirate cultures. In 16 (52%) cases, bone lesions were surgically debrided.\n\n【20】Most patients received combined therapy for 6–36 months. One case-patient was treated for a limited duration of 6 weeks with good initial response, but no data regarding long-term recovery were available. Three patients received no antimicrobial therapy, but they all underwent surgical debridement. Evidence of relapse was noted in 1 of those patients; data were unavailable for the other 2 patients because the follow-up period for those was brief.\n\n【21】In 18/31 cases (58%), the disease was multifocal, and in most cases a relapsing-remitting course was observed regardless of appropriate antimicrobial treatment. Eleven (35%) of 31 children had \\> 1 recurrence; others had no reported recurrence. The observation of no recurrence may be biased; some cases were published shortly after completion of the patients’ course of antimicrobial treatment, which precluded a substantial follow-up period . In cases in which serology was monitored during and after treatment, serology remained unchanged or even increased during follow-up, which can be explained by the unreliable role of serology as a test of cure in this disease .\n\n【22】### Discussion\n\n【23】We describe 3 cases of definite _C. burnetii_ OAI in children and provide a review of the literature for these cases. Diagnosis for these 3 patients was defined by the prolonged clinical symptoms associated with microbiological evidence. Major criteria including clinical evidence of OAI, bone or joint involvement visible on MRI, and diagnostic serologic titer were found in all 3 cases. The first case also had a positive _C. burnetii_ PCR as a definite criterion .\n\n【24】Our review of the published cases with chronic Q fever OAI sheds additional light on this often underdiagnosed and easily missed infectious disease. A high index of suspicion is encouraged in cases of an unusual case manifestation, a chronic or subacute course, relapsing symptoms and signs, nonresolving or slowly resolving osteomyelitis, culture-negative osteomyelitis, exposure history to farm animals, or bone histology demonstrating granulomatous changes . Some cases improve spontaneously or during treatment with a β-lactam, which is not active against _C. burnetii_ , and have no obvious rural contact, making diagnosis more challenging. Optimal antimicrobial therapy for chronic Q fever OAI has not been well established.\n\n【25】Current treatment recommendations are based on case series, retrospective cohort studies, and in vitro data. In the 3 cases we describe, the choice of treatment was based on recent literature and the safety and availability of the suggested drugs for the specific patient. The patient in case 1 was treated with ciprofloxacin and rifampin. Doxycycline therapy is considered ill-advised in children <8 years of age because of its side effects of teeth staining and weakening of enamel, especially in prolonged treatment. Of note, the US Centers for Disease Control and Prevention (CDC) in 2013 recommended the of use of doxycycline at a dose of 2.2 mg/kg twice per day for 2 weeks in children <8 years of age for the treatment of acute and chronic Q fever in United States . However, Q fever OAI requires a much longer treatment period with doxycycline, with a higher likelihood and potential for long-term adverse effects. Doxycycline is not approved in Israel for chronic use in children <8 years of age. Hydroxycholoroquine was not available in a liquid form and required special pharmacy preparation which could interfere with treatment continuity.\n\n【26】We diagnosed OAI in the other 2 case-patients shortly after the recurrence of osteomyelitis in case-patient 1. Considering the recurrence occurred while this patient was under treatment with ciprofloxacin and rifampin, we considered treatment failure using this regimen and therefore chose rifampin and TMP/SMX for the other 2 patients. TMP/SMX has been used in combination with doxycycline, ciprofloxacin, or rifampin according to reports available at that time .\n\n【27】The standard treatment of Q fever–persistent focalized infection in adult patients is hydroxychloroquine and doxycycline; this regimen leads to fewer recurrences. Minimal treatment duration in persistent infections is 18 months; we have no evidence to determine the duration of doxycycline and hydroxychloroquine therapy . The effectiveness of this therapy is likely to result from the alkalizing effect of chloroquine and its derivative, the hydroxyl, on lysosomal compartments, thus enabling improved doxycycline activity .\n\n【28】For children <8 years old, treatment with TMP/SMX or rifampin is preferred because of the risk for dental staining with prolonged tetracycline therapy . In vitro studies showed complete susceptibility to rifampin, TMP/SMX, and tetracyclines; heterogenous susceptibility to fluoroquinolones and erythromycin; and a little inhibitory growth effect of β-lactam antimicrobial drugs . Studies show no consensus regarding optimal treatment duration of Q fever OAI in children. Minimum treatment duration was 6 months in most of cases reviewed. Appropriate drug therapy did not necessarily prevent recurrences; in 16 (53%) cases, patients required surgical intervention despite adequate antimicrobial therapy . In addition to antimicrobial and surgical treatment, immunomodulatory agents including interferon gamma were found to be effective in vitro and in some case reports of chronic Q fever .\n\n【29】The etiology of treatment failure and relapse in Q fever OAI is not well understood. We were not able to conclude which treatment regimen was associated with the lowest recurrence rate; the same treatment regimens led to cure in some cases and treatment failure or relapse in others. We observed clinical recurrence and increasing antibody titers despite extended combined antimicrobial drug therapy .\n\n【30】Whether recurrence is related to reduced drug levels is unknown. In most cases, drug levels in blood or hair samples was not considered. In our institution, these tests were not routinely available. Doxycycline hair level assay testing to determine long-term compliance was deemed unnecessary because most of our patients were too young for treatment with doxycycline. No documentation of compliance was described; however, in our 3 cases, parents reported orange-stained urine during follow-up visits, indicating good compliance for rifampin. Our physicians had also evaluated parents administering medications as caring, dedicated, and reliable. Another factor that may explain the nonnegligible rate of recurrence is the dormancy of the intracellular bacterium. These dormant infectious particles turn to metabolically active in response to acidification of the endosome and are associated with the development of the parasitephorous vacuoles that allow the organism to replicate repeatedly. ( _24_ , _25_ )\n\n【31】Tools for follow-up are lacking. Acute-phase reactants were absent or only mildly elevated in in 17/20 (85%) cases, in which laboratory data were documented. Radiographic resolution is not expected over a period of several weeks. Serology may not change throughout the first years following clinical cure . We did not user newer imaging tools such as the 18F FDG-PET/CT described in recent studies  for follow-up in our cases.\n\n【32】In summary, pediatric OAI with prolonged course or inadequate response to empiric treatment should raise the suspicion of unusual pathogens such as _C. burnetii_ . Further studies are needed to guide optimal treatment of Q fever OAI, including choice of antimicrobial drugs, duration of therapy, and methods of monitoring response to treatment. Diagnosis of Q fever OAI requires increased awareness and use of newer diagnostic modalities, and urban residence or lack of direct exposure to animals does not rule out infection, especially in countries in which Q fever is endemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f58f7186-8065-480a-8d6d-7fc2fdeee5d1", "title": "Eastern Equine Encephalomyelitis Virus Infection in a Horse from California", "text": "【0】Eastern Equine Encephalomyelitis Virus Infection in a Horse from California\nEastern equine encephalomyelitis virus (EEEV) is a mosquito-borne virus in the family Togaviridae, genus Alphavirus. EEEV, Western equine encephalomyelitis virus (WEEV), and Venezuelan equine encephalomyelitis virus (VEEV) are related but genetically distinct alphaviruses. EEEV and VEEV are lethal in up to 90% of recognized equine cases, whereas WEEV is least virulent in horses, which have a mortality rate of approximately 40%  . EEEV may also cause fatal encephalitis in humans (mortality rate 50%-75%)  . In the United States, enzootic EEEV occurs mainly from New England to Florida and along the Gulf Coast, with rare reports of foci as far inland as Michigan and South Dakota  . In North America, sylvatic populations and the mosquito Culiseta melanura maintain the virus in hardwood, salt-water swamp habitats. Large populations of this mosquito allow amplification of the virus by transmission among wild birds  . In wild birds indigenous to North America, the infection is usually innocuous, whereas in pheasants, cranes, and emus, the disease is often lethal. C. melanura feeds almost exclusively on passerine birds; however, spillover of EEEV from the enzootic vector into several other mosquito species (e.g. Aedes spp.), which feed on tangential hosts such as humans and equines, may result in large epizootics with high mortality rates . Our paper describes a sporadic case of EEEV infection in a horse outside the known geographic range of this virus and the ensuing investigation to determine the source of exposure.\n\n【1】### Materials and Methods\n\n【2】##### Case Report\n\n【3】In April 2000, a 14-month-old gelding quarter horse was seen at a veterinary referral hospital in southern California for sudden onset of quadraparesis and recumbency. The horse had no history of prior neurologic disease. He had been castrated approximately 90 days before the illness without complication. A multidose, multivalent vaccine containing formalin-inactivated EEEV and WEEV, influenza virus, and tetanus toxoid was administered to the affected horse and 27 stable mates 1 week before the onset of illness.\n\n【4】The horse appeared alert and healthy the night before onset of clinical signs. At 6:30 a.m. on April 21, he was found down in his stall and unresponsive to external stimuli. The referring veterinarian found a recumbent, comatose horse with spontaneous nystagmus and flailing, incoordinated movements. Initial therapy included intravenous corticosteroids, fluid therapy (including glucose to treat possible hyperkalemic periodic paralysis), and diazepam for intermittent seizures. The horse did not respond to therapy and was sent to the referral hospital.\n\n【5】On examination at the hospital, the horse was comatose with elevated heart and respiratory rates and a normal rectal temperature. A neurologic exam showed that pupillary light responses were absent bilaterally. Palpebral reflexes were present although weak. No organized motor movements occurred in response to stimuli. Initial emergency treatment consisted of intravenous fluids with dimethyl sulfoxide and flunixin meglumine. Cervical and skull radiographs were performed and were within normal limits. An atlanto-occipital cerebrospinal fluid tap was also performed, and no abnormalities were seen on gross observation. At this point, diffuse cortical disease was evident. Trauma appeared very unlikely, and an infectious process or toxicosis seemed more probable. Because of the grave prognosis, the owners elected to euthanize the horse. The carcass was sent to the Animal Health and Food Safety Laboratory System, San Bernardino Branch, School of Veterinary Medicine, University of California, Davis, for necropsy.\n\n【6】### Results\n\n【7】##### Pathology\n\n【8】Results of gross necropsy examination were unremarkable except for markedly hemorrhagic bladder mucosa. Histologic examination revealed lesions mainly confined to the cerebral cortex, thalamus, hypothalamus, and anterior portion of the spinal cord (C 1  \\-C 4  ). Lesions in the brain were characterized by a multifocal to diffuse neutrophilic response with gradual progression to mononuclear cell infiltrates in some areas. Vascular damage and fibrin thrombi were evident . Some blood vessels had swollen endothelium surrounded by a thick layer of mononuclear cells. A mild degree of meningitis was present, with pleocellular response containing mainly mononuclear and neutrophilic infiltrates. The neuropil showed fine vacuolation, indicating edema. Some axons were markedly shrunken. The remaining portion of the spinal cord was unremarkable. The urinary bladder had diffuse submucosal hemorrhages. The lung showed flooding of the alveoli with eosinophilic fluid. The remaining tissues were unremarkable.\n\n【9】##### Virology\n\n【10】Portions of brain tissue were collected and sent for rabies testing at the local public health laboratory. Results were negative. Fresh, frozen brain tissues and serum were submitted to the National Veterinary Services Laboratories (NVSL). Tests for equine encephalomyelitides included virus isolation and serology for WEEV, EEEV, VEEV, _Equid herpesvirus 1_ (EHV-1), and _West Nile virus_ (WNV). For virus isolation, a 10% suspension of brain sample was prepared and injected into flasks of RK13, equine dermal, and Vero-MARU cells (Vero M). This cell line was obtained by NVSL at the 135th passage level from the Middle America Research Unit (MARU) as a multipurpose cell line for virus isolation in 1980 and has been maintained by the NVSL since that time. Additional brain suspension was injected intracerebrally into 16 suckling 4-day-old mice (from 2 litters). Cytopathic effects were observed in the RK13 and Vero M cells at 2 days after injection. Examination using electron microscopy of the RK13 cell culture fluids showed particles with morphologic features compatible with alphaviruses.\n\n【11】Virus preparations from both the cell culture supernatant and suckling mouse brains of mice that died were identified as EEEV by a complement fixation test with reference antisera. In that test, virus reacted strongly with EEEV antiserum and weakly or not at all with WEEV and VEEV antisera.\n\n【12】Serum collected from the yearling horse on April 21 before it died was tested for antibodies to EEEV, WEEV, and VEEV by hemagglutination inhibition (HI) and plaque reduction neutralization testing (PRNT). The serum had a HI titer of 20 against both EEEV and WEEV. HI antibodies to VEEV were not detected. In the PRNT, the serum neutralizing antibody titer versus EEEV was ≥100 but was undetectable against either WEEV or VEEV. The serum was also positive in an immunoglobulin (Ig) M-capture enzyme-linked immunosorbent assay for EEEV with a titer of ≥1000. Additional tests for antibodies to equine herpesvirus 1 and WNV were negative.\n\n【13】The Center for Vector-Borne Disease Research at the University of California, Davis (CVBDR), U.S. Army Medical Research Institute of Infectious Diseases (USAMRIID), and California Department of Health Services (CDHS) also received homogenized brain suspension material. Isolation of EEEV by mouse inoculation and cell culture supported NVSL’s findings. The isolate was further characterized by USAMRIID by reverse transcriptase-polymerase chain reaction (RT-PCR) testing and sequencing as described , but with primers listed in Table 1 . A 1,165-nucleotide portion of the viral genome including parts or all of the E2, 6K, and E1 genes was determined. Sequencing of the isolate showed an 18-nucleotide difference (98.5% homology) from the reference PE6 EEEV strain. Comparison with sequences that have been submitted to GenBank indicated that the virus is a North American antigenic variety in subtype 1 of the taxonomic scheme recently proposed by Brault et al.\n\n【14】##### Field Investigation\n\n【15】Local, state, and federal agencies participated in a joint field and laboratory investigation to determine the source of infection. Four hypotheses were investigated to explain the occurrence of EEEV outside its usual range: 1) imported infection from an EEEV-endemic region, 2) autochthonous transmission by locally infected mosquitoes, 3) intentional inoculation of the horse with live EEEV by a person or by purposeful contamination of the vaccine and 4) incomplete inactivation of EEEV in a commercially inactivated viral vaccine.\n\n【16】##### Travel History\n\n【17】We found no evidence that this case was due to importation from an EEEV-endemic region. The horse was born in northern California and at 6 months of age was moved to southern California for training. The horse traveled as far east as Fort Worth, Texas, in July 1999 for showing purposes. The rest of the 1999 horse show season took place in southern California. The horse was last moved to a new stable (Farm A) in southern California during February 2000. He attended several shows in this area and as far east as Hurricane, Utah. A review of a list of participants at the horse shows recently attended showed no horses from EEEV-endemic areas and no reports of encephalomyelitis among other equine participants.\n\n【18】##### Surveillance for EEEV in Southern California\n\n【19】No evidence for autochthonous transmission of EEEV by local mosquito populations was found through surveillance in mosquitoes, sentinel chickens, wild birds, horses, and humans from the region. California has an extensive, long-established Arboviral Encephalitis Surveillance Program that is active from April through October each year . The program includes collection and testing of mosquito pools and sentinel chicken flocks for WEEV and _St. Louis encephalomyelitis virus_ and surveillance for encephalomyelitis cases among equids, ratites (e.g. emus, ostrich) **,** and humans. Following recognition of the equine EEEV case, CDHS began including EEEV screening in its routine testing program.\n\n【20】Coincidentally, a sentinel chicken flock was located on Farm A. Sera submitted from this flock in April were retrospectively tested for EEEV antibody by indirect immunoassay and found to be negative. The flock remained seronegative for EEEV from May through October. In addition, mosquitoes were collected at Farm A and within a 5-mile radius with carbon dioxide traps. A total of 74 mosquitoes, including the _Culex_ spp. _tarsalis_ , _quinquefasciatus_ , _erythrothorax_ , and _stigmatosoma,_ and _Culiseta particeps,_ were collected in 23 trap nights during May. Only 8 of 74 mosquito species were _Culex tarsalis_ , a known vector species of WEEV and a potential vector species of EEEV in California  . Surveys for resting adult mosquitoes in barns and other buildings yielded no mosquitoes. All mosquito pools were tested and found negative by virus isolation in tissue culture. Routine biweekly testing of sentinel chicken flocks and mosquito pools throughout California until the end of October showed no further evidence of EEEV activity in the state.\n\n【21】Despite enhanced surveillance, additional cases of EEV infection in local animal and human populations were not identified. Surveillance for encephalitis cases in horses and humans was heightened in southern California after the equine case was recognized. Veterinarians were alerted statewide through a newsletter published by the California Department of Food and Agriculture, and the local health department issued a press release. Following the publicity, a veterinarian reported three horses with acute neurologic disease during mid-May at another ranch, Farm B, approximately 50 km from Farm A. Necropsy and serologic testing of these cases performed at the California Animal Health and Food Safety Laboratory System, San Bernardino Branch, showed EHV-1 as the likely cause of the outbreak at Farm B; no evidence of EEEV infection was found. In addition, a brown-headed cowbird (Molothrus ater _)_ die-off at another horse ranch, Farm C, approximately 80 km from Farm A, was investigated. No laboratory evidence of EEEV infection was found in three dead cowbirds collected from Farm C, although the causes of their deaths were not determined.\n\n【22】No other horses at Farm A had encephalitis. To further assess potential equine exposures at Farm A, a serosurvey of 10 randomly chosen stable mates of the affected horse were tested for EEEV antibodies. The sample ranged in age from weanlings to elderly horses; each had been vaccinated with the multivalent vaccine against WEEV, EEEV, influenza viruses, and tetanus (Vaccine A) from the same lot on the same day as the case. These horses showed positive neutralizing antibody titers by PRNT ranging from <20 to 320; none had IgM antibodies to EEEV by an ELISA-capture test. Previous vaccination histories were not available for these horses or the case, but the findings in the stable mates were compatible with recent vaccination or the presence of maternal antibodies in younger horses without natural exposure.\n\n【23】##### Criminal Mischief\n\n【24】Although an intentional introduction of EEEV seemed highly unlikely, recent concerns about bioterrorism made this an important possibility to consider. We found no evidence of purposeful contamination of the vaccine or intentional inoculation of the horse. EEEV is not readily obtainable. Furthermore, no motive for such an act was found.\n\n【25】##### Vaccine Studies\n\n【26】An extensive evaluation of the final hypothesis, residual live EEEV in the vaccine, could not eliminate Vaccine A as the source of infection. The farm manager ordered the multivalent EEEV, WEEV, influenza viruses, and tetanus toxoid vaccine by mail from an out-of-state vendor and stored the vials at 6° C in a refrigerator at Farm A. The vaccine was a commercial, four-way, multidose product that was administered intramuscularly by farm personnel. The viruses in the vaccine were formalin-inactivated, adjuvant-type, and of tissue-culture origin.\n\n【27】##### Virus Isolation\n\n【28】Three unused vials and one partially used vial of Vaccine A were found in the refrigerator at Farm A. CDHS and CVBDR attempted virus isolation by mouse inoculation and cell culture by using the residual vaccine from Farm A. One-day-old mice were inoculated by either intraperitoneal or intracranial injections of the vaccine and were monitored for 18 days. Live EEEV was not isolated from any of the vials. Additionally, the Center for Veterinary Biologics Laboratory conducted safety tests on stored vaccine from the same lot as Vaccine A, which was available because of licensing procedures that require samples from each lot to be retained. Virus isolation attempts on these samples were also negative by cell culture and wet chick inoculations.\n\n【29】##### Molecular Comparison of Horse and Vaccine Strains\n\n【30】EEE viral RNA was extracted from the horse isolate and passaged once via BHK (baby hamster kidney) cell culture at CVBDR and directly from the residual vaccine and amplified and sequenced in two separate regions of the genome according to previously published protocols . The structural E1 region was amplified and compared with several other published EEEV E1 sequences in GenBank; 1,100-nucleotide sequences of the horse isolate and Vaccine A strain were compared with each other as well as with the 10 most closely matched published sequences in GenBank. The conclusions from this laboratory’s study were very similar to the initial gene sequencing of the horse isolate by USAMRIID. Table 2 illustrates a comparison of each GenBank sequence to the sequence of the horse isolate. The phylogram is depicted in Figure 2 .\n\n【31】The NSP3 (nonstructural) region was also amplified and cloned to check for variability within Vaccine A and in the horse viral isolate, as well as to compare with published EEEV NSP3 sequences in GenBank. Four horse and five vaccine viral RNA clones were sequenced and analyzed. Of the 508 nucleotides in each fragment, only one nucleotide difference was evident among the cloned vaccine sequences, and only one was found among the four sequences from horse isolates. These differences could be a result of taq polymerase errors. The consensus sequences for both the vaccine and the horse EEEVl RNAs were compared with each other and with the only two EEEV GenBank sequences that came up in a BLAST search .\n\n【32】### Discussion\n\n【33】The identification of EEEV in a horse in California was unprecedented and clearly represented a potential human and animal health threat. In other areas of the country, equine epizootics have been recognized as precursors to human disease . The rapid recognition and reporting of the case permitted an extensive investigation into the source of exposure.\n\n【34】Several factors must be met to sustain epidemics, including virulent viruses, adequate vectors, neighboring intermediate hosts, and populations of susceptible horses and people . Such isolated cases as the one mentioned are sure to increase veterinary attention to the possibility of neurologic patients having EEEV infections, as well as elevating public awareness of the disease and methods of prophylaxis.\n\n【35】A diagnosis of EEEV infection was made on the basis of the rapid clinical onset of neurologic signs, compatible histopathologic and serologic findings, and isolation and molecular characterization of EEEV from brain tissue. Several neurologic conditions were considered in the differential diagnosis, including other viral encephalomyelitides (rabies, Aujesky disease, Borna disease, EHV-1 myeloencephalopathy, WEEV, and WNV encephalomyelitis), bacterial meningitis, listeriosis, leukoencephalomalacia, lead poisoning, equine protozoal myeloencephalitis, nigropallidal encephalomalacia, botulism, and verminous encephalitis.\n\n【36】California’s Arboviral Encephalitis Surveillance Program is among the most comprehensive in the United States. The jurisdiction where the horse was stabled participated in the program, and a sentinel chicken flock was located adjacent to the farm. In this case, locally infected mosquitoes were apparently not the source of exposure. Furthermore, there was no evidence of spread from the infected horse to the local mosquito populations based on mosquito pool and sentinel chicken flock testing throughout the year. The likelihood of EEEV’s having become established in California following this isolated equine case is remote but still important to monitor because of the public health implications. First, the primary vector of EEEV in North America, _C. melanura_ , is not known to occur in California  . In addition, our equine case was diagnosed in April, when mosquito populations are low in southern California; particularly the vector species known to feed on both birds and horses. Second, this case had a rapid clinical course, with euthanasia in <24 hours after onset of clinical signs. Since horses are known to have a short viremia (1 to 3 days’ duration) it is unlikely that any mosquitoes acquired the infection from the horse during this short time period. However, if vector abundance were increased, this horse would have had the potential to amplify the virus  . Incidental infections could have occurred among barn personnel and susceptible horses at Farm A and nearby locations by transmission from mosquitoes that acquired the infection from the case. Of even greater concern, competent vectors could then spread the disease further by feeding on susceptible wild bird populations, potentially establishing an enzootic cycle in southern California.\n\n【37】After we excluded disease by natural infection, bioterrorism, and importation, incomplete formalin inactivation of the EEEV in the vaccine had to be considered a likely possibility. Previous reports of residual virus in formalin-inactivated vaccines exist. Documented outbreaks due to _Poliovirus_ (PV _),_ _Foot-and-mouth disease virus,_ and VEEV have been directly related to the use of formalin-inactivated vaccines . Attempts to isolate live EEEV from residual and stored vaccine were unsuccessful. However, this does not eliminate the possibility that the horse received live virus with its immunization. If inactivated viruses existed in the vaccine, they were likely present in undetectable levels during vaccine development and testing. Additionally, the live viruses were probably distributed sporadically throughout the vaccine lot, allowing for only an isolated recognized case. The situation could also be analogous to the 1955 “Cutter inactivated poliovirus incident,” when children became infected with PV after vaccination and follow-up investigation disclosed that several lots of Salk PV vaccine contained live PV, despite being produced with formalin inactivation in full compliance with federal regulations  . In the PV vaccine example, live virus was not uniformly distributed in that vaccine lot  .\n\n【38】We further explored the hypothesis of residual live virus in the vaccine through molecular epidemiologic studies. Similar studies were used to examine the role of the VEEV vaccine in the 1967-1972 VEEV pandemic in Central America  . Unfortunately, the North American variety of EEEV is the most genetically homologous of the alphaviruses and therefore the least conducive to molecular comparison of strains . In our study, the greatest nucleotide homology in the E1 region was among the horse virus isolate, Vaccine A virus, and the LA50 virus strain . Differences among sequences from Vaccine A EEEV and the horse viral isolate in the E1 region might represent mutations that occurred when virus passed through various hosts (horse brain/BHK cell culture/1) or genetic variants within the vaccine strain. However, we concluded on the basis of the limited number of clones analyzed that there were few to no other EEEV subclones in the horse viral isolate or vaccine virus. The NSP3 region proved to be more highly conserved and therefore less conclusive. Also, very few EEEV sequences that included the nonstructural regions have been published in GenBank, so comparison was limited. Regardless, the Vaccine A EEEV appears to be closely related to the horse viral isolate; thus, the possibility of live virus in the formalin-treated vaccine infecting the horse remains.\n\n【39】We are unaware of any reports of problems with this vaccine lot, despite notification of the manufacturer and other state veterinarians. If Vaccine A or portions of the lot contained live virus, many exposed horses may not have been susceptible because of previous immunization or presence of maternal antibodies. In addition, cases may have been unrecognized or unreported. If Vaccine A was the source of infection for this case or other cases, it was probably a rare event.\n\n【40】A definitive source of infection may never be revealed in this case. However, the case illustrates the need to maintain awareness that EEEV can occur outside its normal geographic boundaries; it also underscores the importance of prompt diagnosis, reporting, and surveillance for arboviral encephalomyelitides.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35ff192e-2d3e-4aaf-817e-8cd2e8604d5d", "title": "Metagenomics of Imported Multidrug-Resistant Mycobacterium leprae, Saudi Arabia, 2017", "text": "【0】Metagenomics of Imported Multidrug-Resistant Mycobacterium leprae, Saudi Arabia, 2017\nLeprosy is a chronic dermatologic and neurologic disease caused by the infectious agent _Mycobacterium leprae_ and can lead to severe disabilities; >200,000 new cases are reported annually worldwide, according to the World Health Organization. A total of 242 leprosy cases were reported in Saudi Arabia during 2003–2012; however, little is known about the subtypes and prevalence of drug resistance among these _M. leprae_ cases .\n\n【1】In May 2017, a 30-year-old woman from the Philippines sought treatment at the dermatology clinic of King Fahad Medical City (KFMC) Hospital in Riyadh, Saudi Arabia, for painful systemic skin nodules and joint pain without joint swelling. She had no medical history of leprosy. The initial clinical diagnosis of this patient was inconclusive, but her initial signs and symptoms were suggestive of a connective tissue disease such as systemic lupus erythematosus, and initial clinical improvement was recorded after a short course of empiric steroids and hydroxychloroquine treatment. Other suspected diagnoses included lepromatous leprosy with type 2 erythema nodosum leprosum reaction or other nontuberculosis mycobacterial infection.\n\n【2】We performed a punch skin biopsy of the extensor surface of the forearm and performed Ziehl-Neelsen staining; we observed a florid histiocytic proliferation containing numerous _Mycobacterium_ bacilli without an obvious granuloma . We referred the patient to the infectious disease clinic, which performed QuantiFERON-TB Gold  and took a biopsy for bacterial and fungal culture, and all test results were negative. Mycobacterial culture showed >9 acid-fast bacilli/high-power field on smears, and no growth was observed on Lowenstein-Jensen slants after 8 weeks of incubation.\n\n【3】Her treatment started with a daily regimen of clofazimine (50 mg), dapsone (100 mg), and rifampin (600 mg). Treatment with moxifloxacin (400 mg/d) and macrolides was briefly added (clarithromycin and azithromycin were both stopped because of gastrointestinal side effects) in case of possible nontuberculosis mycobacterial infection. The patient had multiple relapses during 12 months of follow-up and became steroid dependent (i.e. her skin lesions reappeared shortly after steroid treatment ended).\n\n【4】Because initial test reports were inconclusive and the etiologic agent was unconfirmed, we attempted to confirm the etiology by subjecting the patient’s skin biopsy sample to metagenomic sequencing; a DNA sequencing protocol without target DNA–enrichment steps  was needed to unambiguously identify the etiologic agent. From the metagenomics datasets, we reconstructed the near-complete genome of the _M. leprae_ species (which we named KFMC-1) at 99.2% completeness when compared with _M. leprae_ TN, a strain commonly used for reference . We assembled the 3.24-Mb genome of _M. leprae_ KFMC-1 in 19 DNA segments, and average coverage was 20.02× . A single-nucleotide polymorphism comparison of _M. leprae_ KFMC-1 with a globally representative set of _M. leprae_ revealed KFMC-1 was most closely related to 3K1 Ryukyu-2 , which was originally isolated in Japan .\n\n【5】We identified 158 polymorphic sites in the genome , which corresponded to 136 single-nucleotide polymorphisms and 22 insertion/deletions. In total, 53 of the 158 changes were new, and 63 appeared within gene-coding regions, a couple of which helped us predict the multidrug-resistance profile. We identified a G→T nucleotide change, which leads to a nonsynonymous change (Q438H) in the _rpoB_ gene . This substitution results in rifampin resistance , matching our clinical records. The C1414A mutation in the _rrs_ locus is predicted to confer capreomycin resistance, as observed previously in _M. tuberculosis_ .\n\n【6】After we confirmed the clinical diagnosis as an _M. leprae_ infection, we halted moxifloxacin treatment and kept the patient on 3 standard antimicrobial drugs (clofazimine, dapsone, and rifampin). Afterward, the patient left Saudi Arabia and continued her antimicrobial drug course in her country of origin.\n\n【7】The predominant genotypes of _M. leprae_ strains in the Middle East are subtypes 2 and 3 . Most 3K cases are found in countries of East Asia, such as China , Japan , Korea , and the Philippines . In addition, >37% of the leprosy cases in Saudi Arabia occur in persons from other countries . Our results suggest that this case of leprosy was imported from the patient’s country of origin. Saudi Arabia hosts a massive number of expatriates from all over the world, including persons from _M. leprae_ –endemic countries, and also hosts one of the largest recurring religious gatherings in the world. Therefore, genomics-guided infection control efforts are needed to monitor the potential importation and prevent the spread of _M. leprae_ infections in the region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5c0f918c-ec2e-4947-bcf8-fdbc21e8203a", "title": "West Nile Virus: A Newly Emergent Epidemic Disease", "text": "【0】West Nile Virus: A Newly Emergent Epidemic Disease\nWest Nile (WN) virus is a mosquitoborne virus of the genus _Flavivirus_ , family _Flaviviridae_ . WN, Japanese encephalitis, St. Louis encephalitis, Murray Valley encephalitis, and Kunjin viruses (along with other viruses) belong to the Japanese encephalitis serocomplex and are closely related to each other genetically and ecologically. The Japanese serocomplex of viruses has a near global distribution with some overlap. All the viruses are maintained in cycles involving birds as vertebrate hosts and mosquitoes (principally _Culex_ sp) as vectors.\n\n【1】WN virus has a wide geographic distribution in Africa, west and central Asia, the Middle East, and Europe. Historically, epidemics have been infrequent and not associated with severe disease. In the past decade, however, epidemics/epizootics have occurred in several countries, including Romania (1996, humans), Morocco (1996, horses), Tunisia (1997, humans), Italy (1998, horses), Israel (1997, 1998, 1999, domestic geese), Russia (1999, birds and humans), and the United States (1999, humans, birds, and horses). Severe neurologic disease and fatalities have occurred in all these outbreaks.\n\n【2】The 1999 epidemic/epizootic in New York affected humans (62 laboratory-positive cases of neurologic disease with 7 deaths), birds (thousands of bird deaths, with illness and death documented in 26 species), and horses (25 cases with 9 deaths). The epicenter of the outbreak was the Queens section of New York City, where more than half the laboratory-positive human cases occurred. The outbreak in humans peaked in late August, with the first patient experiencing onset of symptoms on August 2, and the last on September 22. Virus transmission became widespread, and WN virus-positive birds, mosquitoes, or both ultimately were documented in New York, Connecticut, New Jersey, and Maryland. Virus isolation data suggest the 1999 outbreak was transmitted by _Culex_ species mosquitoes, principally _Culex pipiens_ . Overwintering mosquitoes of this species collected in January and February 2000 were found to be positive for WN virus.\n\n【3】Comparison of sequenced virus isolates from birds, horses, and mosquitoes and viral sequences amplified from human brain tissue in the 1999 New York outbreak confirmed that the viruses infecting all species were identical. Moreover, these viruses were identical (99.9% nucleotide homology) to a virus isolated in 1998 from domestic geese in Israel. Analysis of amino acid and nucleic acid sequences for a portion of the envelope gene of 40 WN virus strains from wide geographic areas produced pylogenetic trees showing that WN viruses segregate into two lineages. Lineage I includes viruses from Africa, all strains from north Africa, Europe, Israel, the United States, and Kunjin virus from Australia. Lineage II is composed only of strains from west, central, and east Africa, and Madagascar. The data strongly suggest that the virus causing the 1999 New York epidemic/epizootic was introduced from Israel or the Middle East.\n\n【4】The epizootics in domestic geese in Israel over 3 years (1997, 1998, and 1999) and the strong genetic similarity among WN virus strains suggest that the virus may have persisted in the area in mosquitoes, ticks, or chronically infected birds. Alternatively, WN virus could have been reintroduced in migrating birds from Africa or from Europe. Israeli data provide strong evidence that WN virus is introduced in white storks. It is possible that both mechanisms are correct.\n\n【5】The recent epidemics/epizootics of WN virus in north Africa and Europe and the unprecedented epidemic/epizootic in the northeastern United States underscore the ease with which exotic pathogens can move between continents and regions today. These epidemics/epizootics also reinforce the need to rebuild the public health infrastructure to deal with epidemics of vector-borne diseases and to develop effective surveillance, prevention, and control strategies for these diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "af0aee89-91c7-4900-a5ec-38230ac7bd27", "title": "New Delhi Metallo-β-Lactamase from Traveler Returning to Canada", "text": "【0】New Delhi Metallo-β-Lactamase from Traveler Returning to Canada\nThe _Enterobacteriaceae_ , particularly _Escherichia coli_ and _Klebsiella pneumoniae_ , are among the most common causes of serious hospital- and community-acquired bacterial infections in humans. Resistance to antimicrobial agents in these species has become increasingly prevalent. Of special concern is the development of resistance to the carbapenems; this development is caused by bacterial carbapenemases. These drugs are often the last line of effective therapy for treating infections caused by multidrug-resistant _Enterobacteriaceae_ . Three types of β-lactamases inactivate the carbapenems: _K. pneumoniae_ carbapenemases, metallo-β-lactamases (MBLs), and oxacillinases. The 2 most reported MBLs are the VIM and IMP types, which until recently have been mostly associated with _Pseudomonas aeruginosa_ and _Acinetobacter_ spp. although VIM-2 has spread among _Enterobacteriaceae_ in Greece and, to a lesser extent, Italy .\n\n【1】Recently, a new type of MBL, New Delhi metallo-β-lactamase (NDM-1), in bacteria ( _K. pneumoniae_ and _E. coli_ ) recovered from a patient from Sweden who was hospitalized in New Delhi, India, was described .We characterized a carbapenem-resistant _E. coli_ isolate from the urine of a patient with pyelonephritis and prostatitis who returned to Canada after recent hospitalization while visiting India.\n\n【2】### The Study\n\n【3】A 32-year-old man was admitted to the medical ward of a hospital in Mysore, southwestern India, during 2010, with hyperglycemia and upper urinary tract infection (UTI). His underlying diabetes mellitus was stabilized, but his UTI did not improve after 5 days of ciprofloxacin. He was transferred to a hospital in Alberta, Canada. Prostatitis with pyelonephritis was diagnosed, and the patient was treated with ertapenem, 2 g/day. Culture of a clean-catch urine sample taken before the ertapenem was started yielded _E. coli_ MH01 at >10 5  CFU/mL urine. The patient improved clinically, and a urine culture taken after 7 days of therapy showed no bacterial growth. The patient received 1 dose of 3 g fosfomycin after completing the ertapenem.\n\n【4】Antimicrobial drug susceptibility was determined with the VITEK 2 instrument (Vitek AMS; bioMérieux Vitek Systems, Hazelwood, MO, USA). MICs of the following drugs were determined: amoxicillin/clavulanic acid, piperacillin/tazobactam, cefoxitin, ceftriaxone, ceftazidime, aztreonam, meropenem, ertapenem, amikacin, gentamicin, tobramycin, ciprofloxacin, and trimethoprim/sulfamethoxazole. Additional susceptibility tests for imipenem, meropenem, ertapenem, tigecycline, and colistin were performed by using Etest (AB BioDisk, Solna, Sweden) according to the manufacture’s instructions. Results were interpreted by using Clinical and Laboratory Standards Institute (CLSI) criteria for broth dilution . Fosfomycin susceptibility was determined by using CLSI disk methods .\n\n【5】The sample with _E. coli_ was screened for MBLs with the MBL Etest according to the manufacturer’s instructions. Isoelectric focusing was performed on freeze–thaw extracts on polyacrylamide gels as described . PCR amplification for _bla_ VIM  , _bla_ IMP  , _bla_ NDM  , _bla_ CTX-Ms  , _bla_ OXAs  , _bla_ TEMs  , and _bla_ SHV  was conducted on the isolate by using a GeneAmp 9700 ThermoCycler instrument (Applied Biosystems, Norwalk, CT, USA) and PCR conditions and primers as described . The _bla_ CTX-M  was sequenced by using PCR conditions and primers as described , and the _bla_ NDM  was sequenced by using the following primers and conditions: NDM-F1: 5′-CAGCGCAGCTTGTCG-3′, NDM-R1: 5′-TCGCGAAGCTGAGCA-3′. The PCR program consisted of an initial denaturation step at 95°C for 5 min; followed by 30 cycles of DNA denaturation at 95°C for 1 min, primer annealing at 52°C for 1 min, and primer extension at 72°C for 1 min; followed by a final extension at 72°C for 5 min.\n\n【6】The _qnrA_ , _qnrS_ , and _qnrB_ genes were amplified in MH01 by using multiplex PCR . The _aac(6’)-Ib_ and _qepA_ genes were amplified in a separate PCR by using primers and conditions as described . The variant _aac(6′)-Ib-cr_ was further identified by digestion with _BstF5_ I (New England Biolabs, Ipswich, MA, USA).\n\n【7】Multilocus sequencing typing (MLST) was performed on MH01 by using 7 conserved housekeeping genes ( _adk_ , _fumC_ , _gyrB_ , _icd_ , _mdh_ , _purA_ , and _recA_ ).\n\n【8】MH01 was assigned to 1 of the 4 main _E. coli_ phylogenetic groups (A, B1, B2, D) by using a multiplex PCR-based method . Plasmid sizes were determined by using protocols and conditions described  and assigned to plasmid families by PCR-based replicon typing . Conjugation experiment was performed by mating-out assays with a selection agar containing different β-lactams (IMP 2 µg/mL, ceftazidime 4 µg/mL respectively) and by using _E. coli_ C600N as recipient.\n\n【9】When we used Vitek 2, _E. coli_ MH01 was resistant to amoxicillin/clavulanic acid, piperacillin/tazobactam, cefoxitin, ceftriaxone, ceftazidime, aztreonam, meropenem, ertapenem, amikacin, gentamicin, tobramycin, ciprofloxacin, and trimethoprim/sulfamethoxazole. The MICs detected by Etest were meropenem 32 µg/mL, imipenem 32 µg/mL, ertapenem >32 µg/mL, tigecycline 0.5 µg/mL, and colistin 0.125 µg/ml. The zone size for fosfomycin was 26 mm. MH01 was susceptible only to tigecycline and fosfomycin; CLSI has not published colistin MICs for _Enterobacteriaceae_ .\n\n【10】_E. coli_ MH01 was positive for MBL production by MBL Etest. Isoelectric focusing showed that _E. coli_ MH01 produces 2 β-lactamases with isoelectric points of 5.2 and 8.9; PCR with sequencing identified these enzymes as NDM-1 and CTX-M-15, respectively. The isolate was positive for _aac(6′)-Ib_ (but not _aac(6′)-Ib-cr_ ) and belonged to MLST clone 101 and phylogenetic group B1. _E. coli_ MH01 harbored 4 plasmids of 75 kb, 165 kb, 300 kb, and 400 kb. _E. coli_ (MH01A) transconjugant with an MBL phenotype was obtained, and plasmid analysis showed that it harbored a 75-kb plasmid. PCR confirmed that the transconjugant contained _bla_ NDM  that was untypeable by PCR-based replicon typing. The _bla_ CTX-M-15  was identified on the 165-kb plasmid that belonged to incompatibility groups IncA/C and IncFII. These results were similar to those obtained by Poirel et al.\n\n【11】### Conclusions\n\n【12】Kumarasamy et al. recently provided evidence that NDM-producing _Enterobacteriaceae_ (mostly _K. pneumoniae_ and _E. coli_ ) are widespread in the Indian subcontinent. They also found that many patients in the United Kingdom infected with bacteria that produce NDM-1 had been hospitalized on the Indian subcontinent. The patients sought care for a variety of hospital- and community-associated infections; UTIs were the most common clinical infections. NDM-producing _Enterobacteriaceae_ also have recently been isolated from patients residing in the United States , Netherlands , and Australia ; all patients had received medical care while visiting India.\n\n【13】Our findings add Canada to the growing list of countries from which these bacteria have been isolated. An _E. coli_ isolate with NDM-1 and belonging to the same sequence type has been reported from Australia from a patient previously hospitalized in Bangladesh . Isolation of the same clone in 2 patients in different countries without any obvious contact underscores the probable acquisition of these bacteria during receipt of medical care in the subcontinent and suggests that _E. coli_ ST101 with NDM-1 may be widespread throughout the region. The recent pandemic caused by _E. coli_ clone ST131, which produces CTX-M types of β-lactamases, highlights the ability of certain clones to spread rapidly. _E. coli_ ST101 with NDM-1 may have the potential to cause a similar pandemic.\n\n【14】The worldwide spread of _Enterobacteriaceae_ \\-producing NDMs has serious implications for the empiric treatment of hospital- and community-associated infections because of the multiresistant nature of these bacteria, which severely limits treatment options. Worse, few antimicrobial drugs being developed have activity against gram-negative bacteria. If the emerging public health threat of international travel in the spread of antimicrobial resistance is ignored, the medical community may face carbapenem-resistant _Enterobacteriaceae_ that cause common infections such as UTIs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8c343b50-a7b4-4b0c-b189-9ee817942e92", "title": "Geographic Divergence of Bovine and Human Shiga Toxin–Producing Escherichia coli O157:H7 Genotypes, New Zealand", "text": "【0】Geographic Divergence of Bovine and Human Shiga Toxin–Producing Escherichia coli O157:H7 Genotypes, New Zealand\nShiga toxin–producing _Escherichia coli_ (STEC) O157:H7 and related non-O157 STEC strains are zoonotic pathogens that can cause severe gastrointestinal illness in humans; clinical signs and symptoms of disease range from diarrhea and hemorrhagic colitis to life-threatening hemolytic uremic syndrome . Ruminants, asymptomatic carriers of STEC, shed the pathogen in their feces, and are considered a primary source of foodborne and environmental outbreaks of STEC infection in humans .\n\n【1】The incidence of STEC infections in New Zealand has been among the highest in the world. In 2012, a total of 147 clinical STEC cases (3.3 cases/100,000 population) were notified, of which 142 were confirmed . Consistent with observations in previous years, the predominant serotype among the confirmed cases was O157:H7 (83.8%; 119/142). STEC became a notifiable disease in New Zealand in 1997, and since then, the annual number of notifications has increased steadily . Although the spatial distribution of STEC cases in New Zealand suggests an association with farming and other rural activities, limited epidemiologic data are available on the transmission pathways of STEC from cattle to humans.\n\n【2】The objectives of this research were to 1) compare the population structure and geographic distribution of different genotypes of STEC O157:H7 isolates from bovine and human sources in New Zealand; 2) assess evidence for localized transmission of STEC from cattle to humans in New Zealand; and 3) compare the genotype distribution of isolates from New Zealand with those from Australia, the predominant historic source of imported New Zealand cattle , and the United States. To investigate the molecular divergence of isolates, we used 2 molecular typing methods: Shiga toxin–encoding bacteriophage insertion (SBI) typing and pulsed-field gel electrophoresis (PFGE) profiling. Although PFGE can provide an indication of genomic similarities, it cannot provide a reliable measure of genetic relatedness of isolates, and the visual assessment of bands on an agarose gel to create PFGE profiles can result in misclassification bias . By using 2 methods and by examining the concordance between them, we could use the combined genotyping datasets to assess structuring and patterns of diversity among STEC O157:H7 isolates of bovine and human origin in New Zealand.\n\n【3】### Methods\n\n【4】##### Human Isolates and Data\n\n【5】For the study, we obtained a total of 363 human-derived STEC O157:H7 isolates from the national Enteric Reference Laboratory (Institute of Environmental Science and Research Ltd, Upper Hutt, New Zealand) along with the associated PFGE profiles (restriction enzyme _Xba_ I) and geographic data (North or South Island, New Zealand, and region on each island). Of the 363 isolates, 278 (76.6%) originated from the North Island. The isolates were from patients with clinical STEC infections that occurred in New Zealand during 2008–2011 and represent 71.3% (363/509) of the STEC O157 cases notified and confirmed during 2008–2011 . The cases were reported as sporadic cases or household clusters (i.e. 2 STEC infections in the same home) and were not associated with confirmed foodborne outbreaks.\n\n【6】##### Bovine Fecal Isolates and Data\n\n【7】Fecal STEC O157:H7 isolates (n = 40) used in the study had been collected from cattle in previous studies conducted at beef slaughter plants in New Zealand during 2008  and 2009–2011 . Data regarding the origin (North or South Island, region, farm location) of the cattle and the virulence profiles of the isolates (virulence genes _ehx_ A, _eae_ , _stx_ 1, _stx_ 2, and subtype _stx_ 2c) were available. The isolates were retrieved from feces samples collected from 26 calves and 14 adult cattle, most (80.0%, 32/40) of which were from the North Island; the animals originated from 35 farms.\n\n【8】##### Bovine Meat Isolates and Data\n\n【9】Bovine meat isolates (n = 235) used in the study were from test samples used in routine mandatory testing at beef-processing plants across New Zealand during 2008–2011. Only PFGE profiles ( _Xba_ I) of STEC O157:H7 isolates were available for this study; the profiles were obtained from the national Enteric Reference Laboratory. Geographic data associated with meat-sample location (regions in North and South Islands) were obtained from the Ministry for Primary Industries (Wellington, New Zealand). Most isolates (85.5%, 201/235) originated from beef-slaughtering plants in the North Island. Virulence profiles of the isolates were not available.\n\n【10】##### PCRs for Detection of Virulence Genes\n\n【11】All human isolates were regrown on Columbia Horse Blood Agar (Fort Richard Laboratories, Auckland, New Zealand). Bacterial DNA was extracted from 5 colonies by using 2% Chelex beads solution (Chelex 100 Resin; Bio-Rad, Richmond, CA, USA) and analyzed in 2 PCR assays by using an automated real-time thermocycler (Rotor Gene 6200HRM; Corbett Research, Mortlake, NSW, Australia).\n\n【12】A multiplex PCR assay was performed using previously published primer sequences to detect the presence of virulence genes encoding for enterohemolysin ( _ehx_ A) , intimin ( _eae_ ) , and Shiga toxins ( _stx_ 1 and _stx_ 2) . Primers for detection of genes _stx_ 1 and _stx_ 2 did not differentiate between subtypes of toxins. The final 25-μL PCR reaction volume contained 2× PCR buffer (Express qPCR SuperMix; Invitrogen, Carlsbad, CA, USA), 2 μmol/L of each primer, 2.0 μL of DNA, and 2.5 μL of sterile water. The amplification program included an initial enzyme-activation step of 5 min at 94°C, which was followed by 40 cycles of, 20 s at 94°C, 20 s at 64°C, and 20 s at 72°C, followed in turn by a final extension of 5 min at 72°C. The PCR products were detected by electrophoresis using a 2% (wt/vol) agarose gel (Agarose low EEO; AppliChem, Darmstadt, Germany) and then stained with ethidium bromide and visualized under ultraviolet illumination.\n\n【13】_stx_ 2-positive isolates were further tested to determine whether the _stx_ 2 gene that was present was the genetic subtype _stx_ 2c. The _stx_ 2c gene was detected by using previously published primer sequences . The final 20-μL PCR reaction volume contained 2× PCR buffer (Express qPCR SuperMix; Invitrogen), 2 μmol/L of each primer, 2.0 μL of DNA, and 6.0 μL of sterile water. The PCR included an initial enzyme-activation step of 5 min at 94°C, followed by 35 cycles of 20 s at 94°C and 20 s at 55°C; no extensions were used. The amplified PCR product was detected as described above.\n\n【14】##### Molecular Typing Methods\n\n【15】All human and bovine fecal isolates were genotyped by using SBI typing ; SBI typing is a multiplex PCR method for screening specific _stx_ \\-associated bacteriophage insertion sites and _stx_ genes ( _stx_ 1 and genetic subtypes _stx_ 2a and _stx_ 2c of _stx_ 2). The characters A, W, Y, S and 1, 2a, 2c represent bacteriophage insertion sites _argW_ , _wrbA_ , _yehV_ , _sbcB_ , and Shiga toxin genes _stx_ 1, _stx_ 2a, _stx_ 2c (2 subtypes of _stx_ 2), respectively . All bovine fecal isolates were subtyped by using PFGE ( _Xba_ I) according to the standardized laboratory protocol published by PulseNet International . The SBI typing was completed at Washington State University, Pullman, Washington, USA.\n\n【16】##### Location of Work and Ethical Approval\n\n【17】This work was completed at the Molecular Epidemiology and Public Health Laboratory, Infectious Disease Research Centre, Hopkirk Research Institute, Massey University, Palmerston North, New Zealand. The use of STEC isolates from clinical case-patients in New Zealand was approved by the Multi-region Ethics Committee, Wellington, New Zealand, on March 19, 2012; reference number MEC/11/04/043.\n\n【18】##### Data Management and Statistical Analysis\n\n【19】For initial analysis, SBI types were grouped into 4 categories of 3 predominant SBI types (AY2a, WY12a, and ASY2c/SY2c) and other, less common, SBI types (AS12c, AS2c, ASWY2c, ASY12c, ASY2a2c, AWY12a, AWY2a, SWY2c, and Y2c). SBI types SY2c and ASY2c were grouped together because both were relatively common and shared the same virulence gene profile.\n\n【20】Although bovine meat isolates were not SBI-typed, a close correlation between PFGE profile and SBI type was observed for the human samples  and the bovine fecal samples . On the basis of the PFGE/SBI clusters, the most likely SBI type was inferred from the PFGE profiles for the meat isolates by taking the following approach. First, BioNumerics software version 6.6 (Applied Maths, Sint-Martens-Latem, Belgium) was used to compare PFGE profiles of human and bovine fecal isolates by conducting an UPGMA (unweighted pair group method with arithmetic mean) cluster analysis using the Dice similarity coefficient, with a band matching tolerance of 1%. Second, the UPGMA cluster analysis was applied on PFGE profiles of bovine meat isolates. The dominant SBI types in human and bovine fecal isolates were used to assign SBI-like types (AY2a, WY12a, and ASY2c/SY2c) to clusters with similar PFGE band patterns in bovine meat isolates.\n\n【21】χ 2  and Fisher exact test for count data were used to evaluate associations between island and SBI type (AY2a, WY12a, ASY2c/SY2c, and other SBI types) for bovine fecal, bovine meat, and human isolates; R software  was used for statistical computing. p values for associations between SBI types and region and between SBI types and year for human and bovine meat isolates were computed by simulating 10 8  tables from the null hypothesis (independence) and comparing the results with the test statistic from the observed data.\n\n【22】Population differentiation among human and bovine meat isolates was assessed by using analysis of molecular variance (AMOVA) applied to haplotypes of isolates’ PFGE profiles (generated in BioNumerics) using Arlequin software version 3.5.1.2 . A multilevel hierarchy was used for the AMOVA model to assess population differentiation between island, between regions within island, and within regions. Regions with <5 isolates were excluded from the analysis. A matrix of pairwise F ST  values was computed by comparing the PFGE haplotype frequency distributions for each pair of regions (using Arlequin, version 3.5.1.2). F ST  is an index of population differentiation, measuring the variance between subpopulations relative to the total variance, and ranges from 0 (no divergence) to 1 (complete divergence). The computed pairwise F ST  matrix, representing genetic distances between the regional populations of STEC O157:H7, was illustrated graphically as a NeighborNet tree by using SplitsTree software version 4.12.6 .\n\n【23】To illustrate the molecular relatedness and genotypic clustering of isolates, we used Primer 6 software  to link distance matrices of PFGE profiles of human and bovine meat isolates (generated in BioNumerics) with explanatory variables (SBI type and region) to create multidimensional scaling plots. Regions with <5 isolates were excluded from the analysis.\n\n【24】To assess the population structure of New Zealand isolates, we compared published frequency distributions of SBI types in 205 cattle and 79 human STEC O157:H7 isolates sourced from Australia and in 143 cattle and 179 human STEC O157:H7 isolates sourced from the United States  with frequency distributions of SBI types among bovine and human STEC O157:H7 isolates from New Zealand. To evaluate genetic similarities of human and bovine fecal isolates, we computed proportional similarity indices (PSI) based on the frequency distributions of SBI types in humans and cattle from all 3 countries. PSI is a similarity measure that estimates the area of congruence between 2 frequency distributions ; measurements range from 0 (distributions with no common SBI types) to 1 (highest possible similarity between distributions). Bootstrapped 95% confidence intervals for PSI values were calculated according to the percentile method described by Efron and Tibshirani , using 2,000 iterations. No grouping of SBI types was applied for PSI calculations. To illustrate the international geographic divergence of isolates, we used differences in PSI values (1 − PSI) to construct a NeighborNet tree with SplitsTree software version 4.12.6.\n\n【25】### Results\n\n【26】##### Genotype Diversity\n\n【27】All 403 human and bovine fecal isolates were positive for _ehx_ A, _eae_ , and _stx_ 2 (except 1 _chx_ A-negative human isolate); of these, 61 (15.1%) were also positive for _stx_ 1\\. The different virulence profiles of isolates, each represented by a dominant SBI type, are shown in Table 1 . The predominant SBI types AY2a, WY12a, and ASY2c/SY2c accounted for 55.0% (22/40), 15.0% (6/40), and 22.5% (9/40) of the studied bovine fecal isolates, respectively. Similarly, in human isolates, SBI types AY2a, WY12a, and ASY2c/SY2c were detected in 57.9% (210/363), 13.8% (50/363), and 23.1% (84/363) of the isolates, respectively. The distributions of AY2a, WY12a, ASY2c/SY2c, and other SBI types varied by year (p = 0.037) . On the basis of the genotype calibration of PFGE profiles of bovine meat isolates, SBI-like types AY2a, WY12a, and ASY2c/SY2c were prevalent in 64.7% (152/235), 23.4% (55/235), and 11.9% (28/235) of the isolates, respectively. Association between SBI-like type and year was marginally nonsignificant (p = 0.052).\n\n【28】##### Between-Island Comparisons\n\n【29】The distribution of SBI types observed differed between North and South Islands in bovine fecal and human isolates; SBI types AY2a and WY12a were more common in the North Island, and ASY2c/SY2c was more common in the South Island . Similarly, a significant difference in the prevalence of SBI-like types between islands was observed in bovine meat isolates .\n\n【30】##### Within-Island Comparisons\n\n【31】By using a 3-level hierarchy of island, region within island, and within region for the AMOVA model, we found that most of the molecular variation (>98%) resided between isolates within regions (on the basis of PFGE haplotypes). However, for the human isolates, a small but highly significant proportion of the molecular variation was estimated to be between regions within islands (1.03% variation, p<0.001); this finding provided evidence for highly localized geographic structuring. After we allowed for between region variation in the model, island was no longer a significant source of variation for the human isolates (p = 0.212). In contrast, a very small but significant amount of molecular variation was apparent between islands among the bovine meat isolates (0.38% variation, p = 0.017), but the proportion of variation between regions within islands was nonsignificant (0.34% variation, p = 0.121).\n\n【32】The population differentiation and geographic clustering of genotypes of STEC O157:H7 isolates from human cases and bovine meat samples from regions of both islands of New Zealand are illustrated in Figure 2 . Consistent with the AMOVA results, we found evidence of within-island clustering of human isolates. Two main clusters were observed representing North and South Island regions, with the exception of Canterbury, which clustered with North Island regions, and Wellington, Taranaki, and Gisborne, which were North Island outliers. Among human cases, the highest population differentiation of genotypes of STEC O157:H7 isolates was observed between the regions of Wellington (15 isolates) and Gisborne (12 isolates) on the North Island (pairwise F ST  value of 0.071), followed by Wellington and Otago (10 isolates) (F ST  \\= 0.060); the isolates from Gisborne included 2 household clusters (2 human cases each). For bovine meat isolates, no obvious structuring was apparent; however, Auckland region (5 isolates) appeared as a strong North Island outlier. Consequently, the most distinct difference in genotypes was observed between the regions of Auckland and Otago (9 isolates) (F ST  \\= 0.060), followed by Auckland and Northland (26 isolates) (F ST  \\= 0.057).\n\n【33】The molecular relatedness between PFGE profiles of human isolates, considering SBI type and region of origin as explanatory variables, is shown in Figure 3 . PFGE profiles showed genotypic clustering that was strongly associated with SBI types AY2a, WY12a, and ASY2c/SY2c, even after stratifying by island of origin . Clusters containing SBI type AY2a and ASY2c/SY2c were the predominant genotypes in the Taranaki and Gisborne regions, respectively, on the North Island ; the association between SBI type and region of origin was statistically significant (p<0.001). A similar genotypic clustering of regions was observed in bovine meat isolates from the North and South Islands .\n\n【34】##### International Comparison\n\n【35】Within each country, similar frequencies of SBI types were observed in cattle and human cases, but there were distinct differences in the population structure of SBI types between countries . Bovine and human genotypes in New Zealand shared the highest similarity (PSI value 0.92, 95% CI 0.74–0.93), followed by those in Australia (PSI 0.69, 95% CI 0.57–0.79) and the United States (PSI 0.61, 95% CI 0.51–0.69) . The observed differences in proportional similarities of SBI types among isolates from cattle and humans in all 3 countries are shown in Figure 5 .\n\n【36】### Discussion\n\n【37】We assessed the molecular epidemiologic evidence for transmission of STEC from cattle to humans in New Zealand and the relationship between population structure and geography at multiple spatial scales. The molecular analysis of bovine and human STEC O157:H7 isolates showed a concordant geographic variation of genotypes (SBI types) in both populations. In addition, there were marked differences between isolates from New Zealand’s North and South Islands, a finding that is consistent with localized transmission of STEC between cattle and humans.\n\n【38】The evidence of localized transmission of STEC between cattle and humans in New Zealand has advanced our understanding of the epidemiology of sporadic STEC infections in the country and is consistent with environmental- or animal-associated sources of infection rather than more disseminated foodborne outbreaks . Measures to prevent direct contact with animal fecal material in the environment include the wearing of protective clothing, increased hand washing, and targeted education of the population at risk regarding possible sources of STEC infection.\n\n【39】The North and South Islands of New Zealand are separated by the Cook Strait, a geographic barrier of >20 km. This barrier might contribute to the island-associated differences in distribution of genotypes observed in this study, by restricting the movement of carrier animals between islands. Cattle populations on each island are large: ≈6.6 million on the North Island and ≈3.5 million on the South Island . Despite the islands’ large cattle populations, the number of livestock moved between the islands (i.e. from farm to farm or farm to slaughter) is relatively low: ≈42,400 cattle from North to South Island, and ≈64,600 cattle from South to North Island per year . Thus, the movement of cattle probably has a limited influence on the distinct distribution of genotypes across both islands.\n\n【40】Although none of the bovine meat isolates were SBI typed, the PFGE data showed a strong island-associated distribution of bovine STEC O157:H7 genotypes, which was equivalent to the patterns observed in fecal isolates from cattle and humans. Bovine meat isolates were retrieved from carcass swab samples and bulk meat samples collected at beef-processing plants, so it could be hypothesized that fresh beef meat might be an exposure pathway for humans. However, although various food sources (including beef) were considered as potential risk factors during a nationwide prospective case-control study on sporadic STEC infections in humans, food was not identified as a major exposure pathway of infections in New Zealand .\n\n【41】Significant genetic variation was observed among human isolates at the regional level, indicating a more localized spatial clustering of STEC O157:H7 genotypes. Strong regional variation in the prevalence of zoonotic diseases has been observed previously in New Zealand. For example, there is marked regional variation in the distribution of serotypes in human cases of salmonellosis: _Salmonella enterica_ ser. Brandenburg was associated with sheep and human infections in the southern regions of the South Island , whereas the wild bird–associated _S. enterica_ ser. Typhimurium DT160 was distributed more evenly across the whole country . _S. enterica_ ser. Brandenburg has not been found to be endemic in any other regions in New Zealand, and it is likely that the spatial pattern of disease is influenced by environmental factors, such as the presence and density of local maintenance hosts.\n\n【42】Cattle are considered the most likely maintenance host of STEC O157:H7, and the association between human cases and cattle density suggests that spillover from cattle to humans is the main pathway ; however, overseas, the pathogen has frequently been isolated from sheep  and deer . Cookson et al. identified STEC serotypes of public health concern in sheep from the lower North Island of New Zealand but did not isolate STEC O157:H7. No nationwide studies of sheep or deer have been undertaken in New Zealand, hence sheep cannot be ruled out as potential maintenance hosts for region-specific populations of STEC O157:H7.\n\n【43】The observed regional clustering of genotypes among human STEC O157:H7 isolates leads to another hypothesis: other, yet unidentified, hosts could be reservoir/maintenance hosts in the epidemiology of STEC, and cattle are possibly only serving as “bridging hosts” at the human–animal interface , transmitting STEC to humans. For example, starlings have been implicated as biologic vectors in the dissemination of STEC among dairy farms in Ohio, United States , indicating that wildlife might play a key role in the epidemiology and ecology of STEC.\n\n【44】A relatively high prevalence of SBI types AY2a and ASY2c was observed in human and bovine fecal isolates from New Zealand. These findings are in contrast to those from the Australian study by Mellor et al. in which SBI type AY2a was not identified (0/284 isolates; p<0.001) and accounted for only 8.1% (26/322) of the isolates from the United States (human and cattle combined); SBI type ASY2c was prevalent in <1.0% of combined isolates in both countries. These differences in frequency distributions of SBI types indicate marked differences in the population structure of SBI types between countries. Australia and New Zealand are neighboring countries but separated by the Tasman Sea, a distance of ≈1,250 km. On the basis of historic data, Australia has been the predominant source of imported New Zealand cattle, mainly in the 19th century . Hence, the distinct geographic divergence of STEC O157:H7 genotypes between the 2 countries is somewhat puzzling and would suggest a limited historic introduction of STEC O157:H7 from Australia or elsewhere into New Zealand and a subsequent evolution in the New Zealand host population. Alternatively, the observed divergence of genotypes between Australia and New Zealand could be the result of genetic drift and/or selection driven by different environmental factors, such as climate, types of feed, husbandry systems, or animal genetics.\n\n【45】In this study, the highest PSI was observed between cattle and human isolates from New Zealand, followed by that between isolates from Australia and the United States. These findings provide evidence for a close association between populations of isolates from cattle and humans, which is consistent with the transmission of STEC from cattle to humans. This finding is in agreement with the national case–control study on clinical STEC cases in New Zealand, which identified variables related to beef and dairy cattle as major risk factors .\n\n【46】The molecular analysis of STEC O157:H7 isolates from cattle and persons with STEC infection revealed that prevalences of bovine and human isolates in the North Island were distinctly different from those of the South Island, suggesting localized transmission of STEC between cattle and humans. Furthermore, a distribution of STEC O157:H7 genotypes different from that observed overseas suggests a historic introduction of a subset of the globally circulating STEC O157:H7 strains into New Zealand.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e4676416-4b74-49e2-8a1b-bf7bf654c9a0", "title": "Rickettsia felis Infection in Febrile Patients, Western Kenya, 2007–2010", "text": "【0】Rickettsia felis Infection in Febrile Patients, Western Kenya, 2007–2010\nRickettsioses are a major human health problem in many parts of the world, including sub-Saharan Africa . Awareness of rickettsiae as causes of public health problems has been increasing; several novel or emerging diseases caused by these pathogens have been recognized. In Kenya, recent reports have documented human infections with _Rickettsia conorii_  and _R. felis_  and tick infection with _R. africae_ . Our objectives were to assess previous human exposure to rickettsiae and to determine the incidence of rickettsial infections among febrile and afebrile persons in western Kenya.\n\n【1】### The Study\n\n【2】The study was conducted among patients visiting the Lwak Mission Hospital, a rural health care facility in western Kenya in the Asembo area, Rarieda District, in western Kenya . Lwak Mission Hospital serves as the field clinic for population-based infectious disease surveillance conducted by the Kenya Medical Research Institute and the US Centers for Disease Control and Prevention as described . The study was conducted with ethical approval from these institutions (protocol nos. 932 and 4566, respectively). To assess previous exposure to rickettsiae, we examined a randomly selected subset of 357 serum specimens collected January 2007 through October 2008 from patients participating in population-based infectious disease surveillance. Samples were screened at a dilution of 1:128 for IgG against spotted fever group (SFG) and typhus group (TG) rickettsiae by using an indirect fluorescence antibody assay (Fuller Laboratories, Fullerton, CA, USA).\n\n【3】Blood specimens were collected from the first 2 outpatients ≥5 years of age and the first 2 outpatients <5 years of age seen each day for acute febrile illness (recorded axillary temperature \\> 38.0°C without an obvious cause, defined as cough, difficulty breathing, chest pain, signs of meningitis, or bloody diarrhea) from November 2008 through February 2010. A positive malaria smear was not an exclusion criterion. During the same period, blood specimens were also collected from controls: a group of outpatients who did not have febrile, respiratory, or diarrheal illness during the preceding 2 weeks and asymptomatic persons who accompanied patients to the clinic.\n\n【4】To detect rickettsial DNA, we performed 3 quantitative PCRs: a genus-specific assay selective for a 74-bp segment of the citrate synthase ( _gltA_ ) gene, a group-specific assay that detects a 128-bp segment of the outer membrane protein ( _ompB_ ) gene for tick-borne rickettsiae, and a species-specific assay that detects a 129-bp segment of the _ompB_ gene for _R. felis_ . To identify which _Rickettsia_ sp. was present in the positive specimens, we PCR amplified and sequenced segments of 4 rickettsial genes— _17-kDa_ , _ompB_ , and 2 _R. felis_ plasmid genes ( _pRF_ and _pRFδ_ )—by using primers and procedures as described .\n\n【5】Overall, 205 (57.4%) of 357 specimens had antibodies against rickettsiae. Of 357 serum specimens tested, 200 (56.0%, 95% exact binomial CI 50.7%–61.2%) had detectable IgG against SFG antigen preparation. Antibodies against TG antigen preparation were detected in 52 (14.5%, 95% CI 11.0%–18.6%) of 357 specimens tested; 47 (90.4%) of these specimens that reacted to TG antigens were also positive for SFG antigens, and 5 (1.4%) of the 357 specimens were positive for TG antigens alone. Presence of antibodies against SFG or TG antigens was not associated with patient sex (p>0.05). In addition, patient age was not significantly associated with TG seropositivity (χ 2  for linear trend 3.41, df 1, p = 0.065). However, an incremental linear association was demonstrated between age and IgG seropositivity to SFG (χ 2  for linear trend 45.46, df 1, p<0.001) .\n\n【6】A total of 699 febrile patients who sought care at Lwak Mission Hospital from November 2008 through February 2010 and 236 afebrile persons enrolled during this same period were tested for rickettsiae . Overall, 50 (7.2%, 95% CI 5.4%–9.3%) of the febrile patients and 8 (3.4%, 95% CI 1.5% _–_ 6.6%) of the afebrile persons had positive rickettsiae results according to the genus-specific _gltA_ assay. Univariate logistic regression indicated that febrile patients were more likely than afebrile persons to have positive PCR results (odds ratio 2.20, 95% CI 1.03–4.70, p = 0.04). According to the _ompB_ assay, all specimens tested were negative for tick-borne rickettsiae. BLAST searches  for homologous sequences determined that the segments amplified from the 3 genes had 100% nt homology with _R. felis_ URRXWCal2 .\n\n【7】In addition to fever, the most common clinical manifestations among patients with positive PCR results for rickettttsiae were headaches (100%), chills (93.8%), muscle aches (68.8%), and joint pains (68.8%). Rash was reported for 4.4% of rickettsiae-positive patients. Among febrile patients, no statistically significant associations were found between specific signs or symptoms and positive PCR results for rickettsiae (p>0.05). Samples from all febrile patients were Giemsa stained and examined; malaria parasites were detected in 79.2% and 73.4% of samples from patients who had PCR-positive and PCR-negative results for rickettsiae, respectively.\n\n【8】### Conclusions\n\n【9】The 2007–2008 serosurvey found prevalence of IgG against rickettsiae to be high. Other countries in Africa have reported similar (28%–58%) seroprevalence . The finding that prevalence of IgG to SFG rickettsiae increased with age can, in part, be explained by cumulative exposure to the pathogen and lifelong persistence of IgG. The high number of patients seropositive for SFG and TG rickettsiae may be attributed to cross-reactivity between SFG and TG rickettsial antigens , although results from studies in animal models show that cross-reactivity between SFG and TG rickettsiae is not consistent . Znazen et al. speculate that antibodies against _R. felis_ may be the major cause of cross-reactions to TG-rickettsiae–specific and SFG-rickettsiae–specific antigens.\n\n【10】The identification of _R. felis_ DNA sequences in febrile patients confirms the previous finding of this pathogen among febrile patients in Kenya . Our findings are also similar to those from northern Tanzania, where acute rickettsiosis was serologically confirmed for 8% of febrile hospital inpatients , and from rural Senegal, where 6% of febrile patients without malaria had positive _R. felis_ test results . Our findings suggest that rickettsial infections should be considered in the differential diagnosis of febrile cases in western Kenya and that diagnostic capacity should be established. Clinicians in malaria-endemic areas should consider rickettsial co-infections in diagnostic protocols and treatment of patients with malaria and other febrile illnesses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f3cded20-d2f7-44b0-93d1-349907611b2d", "title": "Distribution of Japanese Encephalitis Virus, Japan and Southeast Asia, 2016–2018", "text": "【0】Distribution of Japanese Encephalitis Virus, Japan and Southeast Asia, 2016–2018\nThe locations of epidemics of arthropodborne viruses (arboviruses) are strongly associated with the distribution of their vectors. In general, the distribution of arboviruses can expand through the dispersal, transfer, and migration of their vector arthropods and reservoir animals. Mosquitoes transmit a variety of viral pathogens (e.g. dengue, Zika, and chikungunya viruses) and have caused a number of arboviral epidemics throughout the world . Japanese encephalitis virus (JEV; family _Flaviviridae_ , genus _Flavivirus_ ) is a mosquitoborne arbovirus that causes a severe form of encephalitis in humans. JEV is distributed across most of Asia, the western Pacific, and northern Australia . The World Health Organization has estimated that the annual number of Japanese encephalitis cases worldwide exceeds 60,000 . JEV is transmitted primarily by mosquitoes of the _Culex vishnui_ subgroup, principally _Cx_ . _tritaeniorhynchus_ Giles; pigs and wading ardeid birds, such as egrets and herons, are known to be the major amplifying hosts .\n\n【1】On the basis of their genome sequences, JEVs are classified into 5 genotypes . JEV genotype I (GI), which has been further classified into subgenotypes GIa and GIb, and JEV GIII are the dominant lineages and have been detected widely throughout Asia. JEV GII is the thirdmost common lineage and has been found in Indonesia, Singapore, South Korea, Malaysia, and Australia. JEV GIV and GV are rare lineages; only a few viruses of these genotypes have been isolated from Indonesia, Malaysia, and China as of October 2019. Over the past 30 years, JEV GIa has displaced GIII as the dominant lineage in many countries of Asia . Although the origin and spreading pattern of JEV genotypes across the world have been investigated in some reports , the exact mechanisms of JEV genotype shift remain unclear.\n\n【2】### The Study\n\n【3】To study the epidemiology of arbovirus infection, we, an international team of researchers in Japan, Thailand, the Philippines, and Indonesia, conducted arbovirus surveillance in our respective countries during 2016–2018 with the support of our governments. In each country, we collected mosquitoes in and around cattle or pig housing using sweeping nets and aspirators. We collected mosquitoes that had digested blood in their midguts. We identified their species and sorted them into pools, which we used for virus isolation. We also collected serum samples from pigs and wild boars from each country to use for virus isolation.\n\n【4】We passed homogenized mosquito or serum samples through 0.45-μm filters  and inoculated filtrates onto monolayers of 3 culture cell lines (mammalian cell lines Vero9013 and BHK-21 and mosquito cell line C6/36). We assessed cytopathic effect (CPE) daily and collected supernatants from cells that exhibited CPE. If we observed no CPE, we passaged the cells 5 times for 7 days each, after which, if virus was present, CPE should have become apparent. We extracted RNA from culture supernatants using the QIAamp Viral RNA Mini Kit  and subjected the resulting RNA to reverse transcription PCR (RT-PCR) using the QIAGEN One-Step RT-PCR Kit and 2 universal flavivirus-specific primer sets (MAMD and cFD2 or FU2 and cFD3)  to screen for flaviviruses. To determine genome sequences, we used the QIAGEN One-Step RT-PCR Kit, TaKaRa LA RT-PCR Kit version 2.1 , and Invitrogen 5′ RACE System for Rapid Amplification of cDNA Ends version 2.0  as needed in combination with several JEV-specific primers .\n\n【5】Of 945 pig serum samples, we selected 56 candidate samples for virus isolation on the basis of their RT-PCR results with the MAMD and cFD2 primers, and from these samples, we obtained the full or partial genome sequences of 5 JEV isolates . Out of a total of 22,277 mosquitoes comprising \\> 16 species, we obtained the full or partial genome sequences of only 2 JEV isolates . Overall, we obtained the full-genome sequence of 4 of the 7 JEV isolates and the partial genome sequence (envelope gene) of the remaining 3 isolates (DDBJ accession nos. LC461956–62).\n\n【6】A preliminary study we performed showed that many pigs in these countries possessed antibodies against JEV . We found that JEV was more often isolated from serum samples from JEV antibody–negative pigs in farms where JEV seroprevalence was low. Because JEV isolation seems to be difficult in endemic regions, we suggest selecting younger pigs, which are less likely to be JEV antibody positive, for virus isolation studies to increase the chances of success.\n\n【7】The 2 JEV isolates we recovered in Japan, JEV/MQ/Yamaguchi/803/2016 and JEV/MQ/Yamaguchi/804/2016, were GIa and not GIII. In a phylogenetic analysis, these viruses clustered with JEV isolates recovered from Japan (in 2013 and 2009), China (in 2007), and South Korea (in 2010) . The 2 viruses collected in Japan in 2016 were most closely related to viruses recovered in 2013 from the same site that had been sampled in our previous study , suggesting that this strain has been maintained in the Yamaguchi area of Japan since at least 2013.\n\n【8】The JEV isolate we obtained from Thailand, JEV/sw/Thailand/185/2017, was GIb and clustered with other JEV isolates detected in Thailand during 1985–2005 , as well as isolates from Myanmar in 2010, Cambodia in 2014 and 2015, and Singapore in 2014 . Thus, these JEVs have been maintained in Thailand and other parts of the southern peninsula of continental Asia for >30 years.\n\n【9】JEV is distributed extensively throughout the Philippines . However, only 3 JEV GIII isolates from the Philippines (which were obtained from pigs during 1984–1986) were available for genetic analysis. Our 2 Philippines-derived JEV isolates, JEV/sw/Mindanao/K3/2018 and JEV/sw/Mindanao/K4/2018, clustered with these isolates . Our 2 Indonesia JEV isolates, JEV/sw/Bali/93/2017 and JEV/sw/Bali/94/2017, were obtained from the island of Bali, where a JEV vaccination program began in March 2018 . These JEV isolates clustered with other Indonesia isolates obtained in the 1980s  .\n\n【10】### Conclusions\n\n【11】In summary, our phylogenetic analysis revealed that the JEV isolates we obtained from Japan were GIa, the isolate from Thailand was GIb, the isolates from the Philippines were GIII, and the isolates from Indonesia were GIV . These results indicated that JEV GIII and GIV are still active and being maintained in parts of Asia.\n\n【12】Our data demonstrate that a number of the JEV isolates we obtained in select countries of Southeast Asia during 2016–2018 were phylogenetically related to isolates reported in the same country in the 1980s, suggesting that some JEV strains have been maintained in their corresponding regions. Contrary to our expectation, the JEV transmission cycle seems to have been maintained indigenously. JEV strains are presumed to be transferred between JEV-endemic regions by movement of arthropod vectors and bird reservoirs. Nonetheless, we infer that fixation of an invading JEV strain into a new region is difficult unless the new strain possesses properties advantageous for virus growth and expansion . However, the genotype shift from GIII to GIa has occurred in East Asia since the 1990s, indicating that GIa must have had some sort of growth advantage over GIII that permitted its spreading to and expansion in these countries. Our findings that JEV strain invasion in Asia is infrequent could assist in public health decision-making regarding vaccine formulation and campaign strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4fe2cdc9-217a-4fc4-b135-2b734659f3c2", "title": "Emergence of New Non–Clonal Group 258 High-Risk Clones among Klebsiella pneumoniae Carbapenemase–Producing K. pneumoniae Isolates, France", "text": "【0】Emergence of New Non–Clonal Group 258 High-Risk Clones among Klebsiella pneumoniae Carbapenemase–Producing K. pneumoniae Isolates, France\nIn _Klebsiella pneumoniae_ bacteria, resistance to carbapenems results in 2 main mechanisms: the production of an extended spectrum β-lactamase or plasmid-borne cephalosporinase associated with a decrease in permeability of the outer membrane (especially through alteration of OmpK35 and OmpK36 porins), or the production of a carbapenemase . In France, these carbapenemases are Ambler’s class A KPC enzymes; class B metallo-β-lactamases of NDM-, VIM- and, to a lesser extent, IMP-type; and Ambler’s class D oxacillinases of OXA-48–like type .\n\n【1】_K. pneumoniae_ carbapenemase (KPC) was first identified in United States in the early 2000s . Since then, this carbapenemase has spread and has become endemic in several countries, including the United States, Israel, Greece, China, and Italy. It has also been sporadically described in many countries of Europe . The worldwide spread of KPC has been linked to the dissemination of a main clone of _K. pneumoniae_ (sequence type \\[ST\\] 258) and a single-locus variant (ST512) . In Asia (especially China), ST11, another single-locus variant of ST258, is mostly reported among _bla_ KPC  \\-harboring _K. pneumoniae_ isolates . In addition, a recently published study, conducted by the EUSCAPE working group in 2013 in Europe, revealed that the spread of carbapenemase-producing _K. pneumoniae_ was driven by only a few clones . The most prevalent carbapenemase was KPC (45.5% \\[311/684 isolates\\]), and 72.7% (229/311) of KPC-producing _K. pneumoniae_ (KPC- _Kp_ ) belong to the same clonal group (CG) 258, including ST258 and ST512. Whole-genome sequencing (WGS) analysis has suggested that ST258 and ST512 KPC- _Kp_ spread out in Europe from 2 KPC-endemic countries: Greece (ST258) and Italy (ST512) . However, that study described the epidemiology of KPC in Europe in 2013, whereas the aim of our study was to describe the genomic characteristics of KPC isolates from a more recent period.\n\n【2】Analysis of the genetic context of _bla_ KPC  has revealed that this gene is mostly localized into a class 2 transposon named Tn _4401_ . Several variants of this Tn _4401_ (Tn _4401a_ through Tn _4401i_ ) have been reported with deletions upstream of _bla_ KPC  within the promoter region . Consequently, expression of _bla_ KPC  genes is complex and might involve different promoters, depending on the specific genetic environment and bacterial species. The 2 main promoters are named P1, which is in the vicinity of _bla_ KPC  , and P2, a hybrid promoter located partly in the inverted repeat right of IS _Kpn7_ . In rare cases, _bla_ KPC  genes have been described in genetic structure not related to Tn _4401_ and are named non– T n _4401_ e lements (NTE) . However, in NTE, the expression of _bla_ KPC  is mediated by other promoters. Our study aimed to deeply characterize the epidemiology of KPC _\\-Kp_ circulating in France in 2018.\n\n【3】### Material and Methods\n\n【4】##### Strains Collections and Culture Conditions\n\n【5】We included all KPC- _Kp_ sent to France’s National Reference Center for Antimicrobial Resistance during January 1–December 31, 2018. As previously described, we used isolates that were recovered from clinical and screening specimens and sent on a voluntary basis by any type of laboratory related to any health facility, such as private and public hospitals, nursing homes, and community laboratories . These laboratories were located throughout France, including overseas territories. KPC- _Kp_ recovered by the National Reference Center for Antimicrobial Resistance represent ≈85%–90% of the KPC- _Kp_ infection cases reported to the French Public Health Agency . The collection used for WGS analysis represents a total of 63 nonduplicate isolates recovered from rectal screening (n = 45), urine (n = 12), blood cultures (n = 1), wound infections (n = 2), and respiratory samples (n = 2) and 1 isolate for which no recovery site information was available. Because the aim of the study was to evaluate the genetic diversity of KPC producers, we discarded from further analysis any duplicate isolates or isolates recovered from the same patient.\n\n【6】##### Antimicrobial Susceptibility Testing and Carbapenemase Detection\n\n【7】We performed antimicrobial susceptibility testing by using the disc diffusion method on Mueller-Hinton agar  and interpreted results according to European Committee on Antimicrobial Susceptibility Testing guidelines as updated in 2018 . We determined MICs for colistin by using broth microdilution . We performed carbapenemase detection by using Rapidec Carba NP , followed by immunochromatographic detection of the carbapenemase enzyme using NG-Carba5 test .\n\n【8】##### WGS and Bioinformatic analysis\n\n【9】We sequenced all KPC- _Kp_ isolates by using Illumina technology as previously described . We extracted total DNA from colonies by using the Ultraclean Microbial DNA Isolation Kit  according to the manufacturer’s instructions. We prepared the DNA library as previously described  and performed de novo assembly and read mappings by using CLC Genomics Workbench 12.0 . We identified the acquired antimicrobial resistance genes by using Resfinder 3.1  and the CARD database . We annotated the genomes by using RAST . We performed phylogenic analysis by using CSIphylogeny 1.4  and visualized the genomes by using FigTree 1.4.3 . We performed sequences alignments by using ClustalW . We analyzed single-nucleotide polymorphisms (SNPs) on the whole genome by using CSIphylogeny V1.4 with parameters as follows: select minimum depth at SNP position at 10×, minimum distance between SNPs at 10 bp, and minimum SNP quality score of 30.\n\n【10】We constructed the genetic contexts by using de novo assembly or by mapping with reference genomes from GenBank and verified by in-house PCR as previously described . We analyzed plasmid contents of clinical isolates by using PlasmidFinder 2.1 to search for the replicase gene and by conducting manual searches for genes showing homology with the replicase gene.\n\n【11】### Results\n\n【12】##### Low Prevalence of KPC Producers among Carbapenem-Resistant _K. pneumoniae_ in France\n\n【13】In 2018, a total of 3,931 carbapenem-resistant _Enterobacteriaceae_ were collected, including 1,259 carbapenem-resistant _K. pneumoniae_ , among which 1,010 were carbapenemase producers. OXA-48-like enzymes were the most prevalent carbapenemases (69.4%), followed by NDM (17.1%); 37 isolates (3.7%) had a combination of both of these carbapenemases. KPC enzymes represent only 3.0% of all carbapenemases, corresponding to 6.8% (69 isolates, including 6 duplicated isolates) of all carbapenemase produced by _K. pneumoniae_ . KPC also was produced by 13 non– _K. pneunomiae_ , including 5 KPC-2 producers (2 _Escherichia coli_ , 1 _Klebsiella oxytoca_ , 1 _Enterobacter cloacae_ , and 1 _Citrobacter koseri_ ) and 8 KPC-3 producers (5 _E. coli_ , 1 _C. freundii_ , 1 _E. cloacae_ , and 1 _K. aerogenes_ ). Accordingly, _K. pneumoniae_ is the most prevalent species (84.1%) among KPC producers.\n\n【14】##### Antimicrobial-Susceptibility of KPC- _Kp_\n\n【15】Susceptibility testing revealed that all KPC- _Kp_ were resistant to all broad-spectrum cephalosporins (ceftazidime, cefotaxime, cefepime) and monobactam . All KPC- _Kp_ were resistant to ertapenem. According to European Committee on Antimicrobial Susceptibility Testing breakpoints, 62.1% (41/66) KPC- _Kp_ isolates remained susceptible to imipenem and 30.3% (20/66) to meropenem . Ceftazidime/avibactam (98.5% susceptibility) and colistin (92.2%) remained the most potent agents . However, we identified 1 isolate resistant to ceftazidime/avibactam but susceptible to carbapenems . This isolate produces a new variant of KPC, named KPC-39, that has been reported to possess increased ceftazidime catalytic activity but also to have concomitantly lost its carbapenemase activity . Among other antimicrobial families, 84.9% KPC- _Kp_ isolates were susceptible to tigecycline, 30.3% to sulfamethoxazole/trimethoprim, and 19.7% to ciprofloxacin . Resistance to aminoglycosides varied from 40.9% for amikacin to 75.8% for tobramycin.\n\n【16】##### _bla_ KPC  Variants and Associated Acquired Resistance Genes\n\n【17】We performed WGS on 63 nonduplicate KPC- _Kp_ isolates and identified their resistomes by using Illumina technology. In this collection, 44 isolates possessed the _bla_ KPC-3  gene (69.8%), and 18 (28.5%) possessed the _bla_ KPC-2  gene. One isolate harbored a novel single-nucleotide variant of _bla_ KPC-3  , _bla_ KPC-39  . Two isolates produced 2 carbapenemases, including 1 isolate coharboring _bla_ KPC-2  and _bla_ VIM-1  and another 1 coharboring _bla_ KPC-2  and _bla_ NDM-4  . We identified additional antimicrobial-resistance determinants in all isolates . Approximately 46% of the KPC- _Kp_ isolates carried an extended spectrum β-lactamase encoding gene, including 22 isolates harboring _bla_ CTX-M-15  ; 4 isolates coharboring _bla_ CTX-M-14  and _bla_ CTX-M-15  ; and 3 isolates with _bla_ CTX-M-14  , _bla_ CTX-M-3  , or _bla_ CTX-M-65  . The other acquired β-lactam resistance determinants encoded for the narrow-spectrum β-lactamases OXA-9 and TEM-1. To decipher quinolone resistance, we analyzed the presence of plasmid-mediated resistance determinants and known mutations in gyrase and topoisomerases . Resistance to quinolones was mediated by either mutation in gyrase _gyrA_ affecting the residues S83 (p.S83I, n = 42; p.S83F, n = 7; and p.S83Y, n = 2) or D87 (p.D87N, n = 7; p.D87G, n = 3; and p.D87A, n = 2) or _parC_ affecting the residues p.S80 (p.S80I, n = 51) or the production of _Qnr_ (Qnr66-like, QnrB6, or QnrS1). Aminoglycoside resistance was caused by the production of the 16S RNA methylase RmtB (n = 7) or an aminoglycoside-modifying enzyme (encoding by _aac(3′)-IIa, aadA1, aadA2, aac(6′)-Ib-cr, or strA/strB_ ). Colistin resistance (n = 5) resulted systematically in chromosome-encoded resistance with alteration of the _mgrB_ gene. In these isolates, 2 possessed a nonsense substitution (p.Q30\\*), 1 missense involved in colistin resistance (p.C27W) , an IS _Kpn26_ \\-like inserted (with direct repeats \\[DRs\\] of 4 bp: TTAA), and a missense mutation leading to the disappearance of the start codon. No isolate had plasmid-encoded resistance _mcr_ \\-1.\n\n【18】##### Genetic Diversity of KPC- _Kp_\n\n【19】Phylogenetic analysis revealed that the 63 KPC- _Kp_ belonged to 15 different clones (STs) circulating in France . Although many studies have asserted that CG258 is responsible for the spread of _bla_ KPC  , in our collection, only 8 isolates (12.7%) belonged to CG258 (4 each for ST258 and ST512). Furthermore, epidemiologic investigations revealed no link between these isolates . Because KPC is not widely disseminated in France, we did not expect to observe such clonal diversity. Indeed, the epidemiology of KPC in France is not comparable to what was reported in nearby countries in Europe where KPC is endemic, such as Greece and Italy, and where the spread of _bla_ KPC-2/-3  is clearly linked to CG258 . Among the 8 isolates we identified that belonged to CG258, 3 were recovered from patients with travel history in Greece (isolate 175C3 and isolate 177H5) and Italy (isolate 160C2). In France, the 3 most prevalent clones are ST307 (with 15 isolates), ST147 (12 isolates), and ST13 (7 isolates). By using the 21 SNP cutoff value proposed by David et al. to identity a single hospital outbreak caused by a ST258 and ST512 cluster , we identified that the ST307 clone was overrepresented because of an outbreak that included 11 isolates . However, this ST307 also included 4 isolates that were not related to this outbreak, such as the 195I4 strain, which was isolated from a patient who traveled in Crete (Greece) and possesses an additional carbapenemase-encoding gene ( _bla_ NDM-4  ). The second most prevalent clone, ST147, seemed to have disseminated upon distinct events . Most of the ST147 isolates have been recovered from different areas with no epidemiologic link between the patients . A link with Portugal has been identified for most (9/11) patients infected or colonized with a KPC-3–producing _K. pneumoniae_ of ST147 . The same link with Portugal was observed for 4 patients infected or colonized with a KPC-3–producing _K. pneumoniae_ of ST231. Strains from ST383 represented a small outbreak for which cross contamination was evidenced (<20 SNPs between 4 isolates \\[ Appendix 2 \\]). One isolate (171J7) was distantly related to other clones and corresponded to _K. variicola_ . KPC-2-producing _K. pneumoniae_ isolates of ST11 were predominantly linked to patients who had a history of travel in Asia (China and Vietnam), where this ST is known to be the main vector of _bla_ KPC  dissemination.\n\n【20】##### Diversity of Genetic Vehicle Involved in Spread of _bla_ KPC\n\n【21】Analysis of the close genetic context of _bla_ KPC  highlighted diversity in the genetic structures at the origin of the acquisition of the carbapenemase-encoding gene. The well-known Tn _4401a_ (in 29 isolates) and Tn _4401d_ (in 26 isolates) were the most prevalent structures identified . The KPC- _Kp_ of the 2 main clones ST307 and ST147, _bla_ KPC  , is carried on Tn _4401a_ in ST307 and Tn _4401d_ in ST147. Two unrelated isolates (ST11–167I9 and the _K. variicola_ 171J7 isolate) harbored _bla_ KPC  in the Tn _4401b_ isoform. In the remaining 6 isolates (ST273–171J9, ST147–199D1, ST1788–189B3, ST11–171G8, ST11–190F6, and ST11–171J10), _bla_ KPC-2  is localized in an NTE element . Although 3 isolates belonged to ST11, they displayed 200–800 SNPs of differences along their core genome, indicating that they were unrelated . The links with 3 different countries (Portugal for ST11–171G8, China for ST11–190F6, and Vietnam for ST11–171J10) are consistent with this unrelatedness. Analysis of NTE elements revealed 4 different structures even if common features were observed . For instance, the presence of a fragment of IS _Kpn6_ downstream of _bla_ KPC  and a copy of IS _Kpn27_ upstream were always present . DRs of TATAGG bracketing IS _Kpn27_ indicated a transposition process that occurred inside the resolvase gene of Tn _3_ . Immediately upstream of _bla_ KPC-2  (74 bp), the presence of the inverted repeat right of Tn _3_ is present in all NTE, indicating that all these structures were related. However, the NTE differed by the size of the deletions that are present between IS _Kpn27_ and _bla_ KPC-2  (from 280 bp in NTE-190F6 to 940 bp in NTE-199D1). We could observe a remnant of _bla_ TEM-1  in longer structures, but it was not functional anymore. Analysis of the 4 NTE revealed that in NTE-199D1, several copies of IS _26_ bracketed the whole structure, indicating that this IS might be involved in its acquisition by transposition or a recombination event. IS _26_ has been recently demonstrated to be able to transpose and thus create a class I transposon by targeting another copy of IS _26_ . NTE-171J10 is inserted in the _fip_ gene of IncN-type plasmids with the presence of DRs surrounding the NTE-171J0 . The _fip_ gene has already been demonstrated to be an integration hot spot in IncN-type plasmids . DRs as well as putative inverted repeats of Tn _3_ \\-family transposon are present at the integration site . Moreover, the presence of the complete Tn _3_ transposase gene indicated that NTE-171J10 might be functional. In NTE-189B3, a new class I transposon carrying a protein of unknown function has been identified. DRs bracketed IS _Apu1_ and IS _Apu2_ , indicating a transposition process mediated by these close insertion sequences .\n\n【22】### Discussion\n\n【23】In France, KPC producers (84.1% of _K. pneumoniae_ ) represent only 6.8% of all carbapenemase producers, far away from the global 72.7% found in Europe in 2013 . This relatively low prevalence of KPC producers in France compared with OXA-48–like and NDM producers has been reported since 2012 . We demonstrated unexpected clonal diversity among KPC- _Kp_ isolated in France. A few overrepresented clones were identified (i.e. ST307, ST147, ST231, and ST13). However, ST307 was involved in a regional outbreak, whereas ST147 and ST13 were identified in different parts of France. Most of the patients colonized or infected with KPC- _Kp_ had a clear link with Portugal, where these 4 STs were recently described to be the more prevalent . The KPC-2–producing _K. pneumoniae_ isolates identified in France were predominantly recovered from patients with a history of travel in Greece (ST258) or Asia (ST11).\n\n【24】Regarding antimicrobial susceptibility of KPC- _Kp_ in France, the relative high susceptibility to imipenem (30.3%) and meropenem to a lesser extent (18.2%) are in agreement with previous reports from Italy, where ST512 is highly prevalent (26.6% susceptibility to meropenem) . Conversely, data from the United States and Taiwan indicated that KPC- _Kp_ are more resistant to carbapenems in those parts of the world, where ST258 is more prevalent .\n\n【25】Altogether, our results indicate that the KPC- _Kp_ epidemiology has changed in Europe during the past 5 years. In 2018, ST258 and ST512 _K. pneumoniae_ were no longer the main drivers of KPC resistance, at least in several non–KPC-endemic countries of western Europe, such as France and Portugal . KPC- _Kp_ epidemiology also appears to have begun changing in some countries, such as Italy and Colombia, where CG-258 KPC- _Kp_ was previously known to be endemic. This change is indicated by the reported emergence of ST307 and ST273 KPC- _Kp_ in Sicilia (Italy)  and ST307 and ST14 KPC- _Kp_ in Colombia . This change in the global epidemiology of KPC- _Kp_ might have an effect on the identification of these carbapenemase producers with the molecular methods dedicated to the identification of GC258 _K. pneumoniae_ .\n\n【26】In addition, our study highlights the dissemination of _bla_ KPC  genes in high-risk clones of _K. pneumoniae_ (ST307 and ST147), genetic features that might provide an advantage in adaptation to the hospital environment and the human host . These clones already convey several antimicrobial-resistance genes, including genes encoding other carbapenemases of NDM and OXA-48–like types . Accordingly, we might now fear the emergence of ST307 and ST147 high-risk clones of _K. pneumoniae_ that can co-produce multiple carbapenemases. A recent study demonstrated the importance of ST307 in the dissemination of _bla_ OXA-181  in South Africa . In that study, >600 isolates belonging to ST307 were recovered and analyzed, and the results demonstrated the importance of this clone as a carrier of carbapenemase genes in all continents. Another study used Bayesian analysis to demonstrate that ST307 emerged in the mid-1990s . ST307 had been strongly associated with the diffusion of _bla_ CTX-M-15   and now is associated with the dissemination of carbapenemase genes .\n\n【27】In conclusion, we found that the epidemiology of KPC- _Kp_ has changed in Europe, in particular, with emergence of non–CG258 KPC- _Kp_ isolates in France, linked to dissemination from Portugal. This change in epidemiology has to be considered by microbiologists because a few diagnostic assays specifically designed for the identification of ST-258 KPC- _Kp_ isolates will not be able to detect non–CG258 KPC- _Kp_ isolates.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5bd7927-6d87-4929-bb30-699a953c25bf", "title": "Tick-Borne Relapsing Fever Borreliosis, Rural Senegal", "text": "【0】Tick-Borne Relapsing Fever Borreliosis, Rural Senegal\nTick-borne relapsing fever (TBRF), caused by several species of _Borrelia_ spirochetes, is transmitted to humans through the bites of soft ticks of the genus _Ornithodoros_ (through infected saliva or entry of infected coxal fluid at the bite site) . Wild rodents and insectivores are common reservoir hosts. TBRF-endemic foci persist around the world, where each _Borrelia_ species causing relapsing fever appears to be specific to its tick vector. TBRF is responsible for recurring fever associated with spirochetemia. In recent years, the extent of relapsing fever caused by infection with _B. crocidurae_ , transmitted by _O. sonrai_ ticks and its effects on public health have only just begun to emerge. In Senegal, Mali, Mauritania, and the Gambia, where this tick is endemic, 2%–70% of animal burrows are inhabited by this tick vector, and an average of 31% of ticks are infected by _B. crocidurae_ .\n\n【1】In Senegal, TBRF caused by _B. crocidurae_ was recently determined to be the most common bacterial infection affecting the human population . A conventional diagnosis of TBRF is based on the detection of spirochetes in blood smears sampled during the acute febrile phase. However, TBRF is underdiagnosed in most disease-endemic areas, where blood smears are screened only for malaria parasites. Therefore, we used specific semiquantitative PCR to evaluate the role of TBRF as a cause of fever among malaria smear–negative patients in rural Senegal.\n\n【2】### The Study\n\n【3】During December 2008 through June 2009, we enrolled all patients with fever (axillary temperature >37.5°C) who had visited the dispensaries in Dielmo (13°43′N, 16°25′W; population 391) and Ndiop (14°33′N, 16°15′W; population 313) in Senegal . Informed consent was obtained from patients and from parents or legal guardians for children. Ethical clearance was granted by the national ethics committee of Senegal and the local ethics committee of Marseille, France.\n\n【4】If Giemsa-stained thick and thin blood smears were negative for malaria infection, then slides were checked again for _Borrelia_ spp. A first screening was made at the dispensary and a second in Dakar by highly trained microscopists. Patients whose results were positive for _Borrelia_ spp. received a 7-day regimen of oral doxycycline or erythromycin (for children <8 years of age and for pregnant women). In addition, 200 µL of whole blood was collected from each malaria-negative patient and used for DNA extraction with a QIAamp kit (QIAGEN, Hilden, Germany). Samples were washed and bound with QIAGEN columns and an adapted manual polyvinyl chloride pump (Fisher Scientific Inc. Strasbourg, France). The columns were stored at 4°C until final elution and PCRs were performed in Marseille.\n\n【5】_Borrelia_ DNA was detected by using specific semiquantitative real-time PCR with primers (Bor16S3F, 5′ **\\-** AGC CTT TAA AGC TTC GCT TGT AG-3′; Bor16S3R, 5′-GCC TCC CGT AGG AGT CTG G-3′), and a probe (Bor16S3P, 5′-6FAM- CCG GCC TGA GAG GGT GAA CGG-3′) that were designed for amplification of a 148-bp fragment of a 16S RNA–encoding gene. The specificities of the _Borrelia_ spp. detection systems had been tested on DNA samples from 347 bacterial species, as described . All real-time PCRs were performed by using LightCycler 2.0 equipment and software (Roche Diagnostics GmbH, Mannheim, Germany). Appropriate handling and DNA extraction were controlled by qualitative PCR of the β-actin gene. Negative controls (sterile water and DNA from a sterile biopsy specimen) and positive controls ( _B. crocidurae_ DNA) were included for each test. All positive samples with a cycle threshold level of log-based fluorescence <36 (≈10–20 copies of spacer) were used to amplify the 148-bp gene fragments, which were subsequently sequenced, by PCR . Monthly variations were analyzed by making autocorrelation correlograms with PASW software version 17 (SPSS, Chicago, IL, USA).\n\n【6】A total of 134 patients were included in the study, and 206 samples were obtained. Several patients were seen multiple times with fever during the study period, and 1 sample was obtained during each independent febrile event. Test results for all controls were as expected. A total of 27 samples from 25 (13%) patients (17 male) were positive for _Borrelia_ spp. by real-time PCR, including 26 (15.0%) of 172 from Dielmo and 1 (0.3%) of 34 from Ndiop. Most _Borrelia_ spp.–positive patients (13; 56%) were <10 years of age, including 6 who were <5 years of age. A total of 8 (32%) patients were 5–10 years of age, and 3 were >20 years of age. Incidence rates in Dielmo were from 0.26 in November 2008, 0.51 in December 2008 and January 2009, 0.77 in March and May 2009, and 1.79 in June and July 2009.\n\n【7】Among the 27 samples positive by real-time PCR, only 4 (15%) had been identified as positive by thick smears at the dispensary, and only 15 (56%) had been identified as positive by highly trained microscopists who conducted a second thick-smear screening in Dakar. After PCR results were known, thick smears were examined again, and 3 more were found to be positive.\n\n【8】Two patients, 1 man and 1 woman, had _Borellia_ spp.–positive results by real-time PCR and by thick smears when they were seen 2 times within 11 and 49 days, respectively. Among the group of 179 PCR-negative samples, no _Borrelia_ spp. had been observed on thick smears. All DNA sequences obtained from these samples were identical and showed 100% identity with _B. crocidurae, B. duttonii_ , _B. hispanica_ , and _B. parkeri_ .\n\n【9】### Conclusions\n\n【10】Our specific semiquantitative PCR results demonstrated that the sensitivity of thick blood smear analysis was dramatically low (15%) when performed in standard conditions in a dispensary and remained low (56%) even when performed by trained microscopists. Similar results have recently been shown for patients infected with _B. hispanica_ in Morocco  or TBRF patients in Tanzania and Togo . In the study reported here, a limited quantity of DNA was available because other causes of fever were also screened . Therefore, we did not amplify and sequence a larger portion of the 16S RNA gene to definitively distinguish the _B. crocidurae_ endemic to Senegal from other TBRF-causing borreliae.\n\n【11】This study highlights the endemicity of TBRF in this rural area of western Africa, where villagers are settled agricultural workers  . Rodent and insectivore burrows are found in almost all households; burrow openings were located inside the bedrooms of traditional huts built with mud and of houses with cement floors and walls. Transmission is mainly nocturnal. The bites are painless, and tick blood meals last from a few minutes to half an hour .\n\n【12】_B. crocidurae_ accounts for high fever, frequent neurologic complications , and up to 9 recurrences over several months, but the mortality rates and early delivery by pregnant women caused by _B. crocidurae_ seem to be lower than those caused by _B. duttonii_ . In addition to relapses, repeat infections in the same person are common , as found in 2 patients reported here. Molecular methods showed the proportion of TBRF in febrile patients to be as high as 15% in Dielmo and incidence rates to be up to 1.79% per month in June and July of 2009. This finding confirms the increased incidence of TBRF that was noted between 1996 and 2002 .\n\n【13】Ideally, patients living in Senegal with unexplained fever should be tested for TBRF by molecular methods. However, this technique is more readily available to travelers returning from this country  than to its citizenry.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "747d77ac-2c6a-40a4-a2a6-516dae7c6388", "title": "High Vancomycin MIC and Complicated Methicillin-Susceptible Staphylococcus aureus Bacteremia", "text": "【0】High Vancomycin MIC and Complicated Methicillin-Susceptible Staphylococcus aureus Bacteremia\nSeveral studies have established a relationship between high vancomycin MIC and a worse prognosis for patients with bacteremia caused by methicillin-resistant _Staphylococcus aureus_ (MRSA) . However, to our knowledge, the role that a high vancomycin MIC could play in the clinical course of a patient with methicillin-susceptible _S. aureus_ (MSSA) bacteremia, has not been investigated, although a high vancomycin MIC has been also reported for strains of MSSA .\n\n【1】### The Study\n\n【2】We retrospectively determined the MIC of vancomycin for the first MSSA blood culture isolate from a cohort of 99 adult patients with catheter-related bacteremia. These patients were consecutively evaluated from January 2002 through December 2004 (mean follow-up 3 years) in University Hospital 12 de Octubre in Madrid, Spain, a 1,000-bed university medical center.\n\n【3】We determined methicillin and vancomycin susceptibility by using broth microdilution according to Clinical Laboratory Standards Institute methods. Vancomycin MIC of was determined under blinded conditions by Etest in the first isolate by using a 0.5 McFarland inoculum streaked evenly with a swab onto Mueller-Hinton agar plates .\n\n【4】Complicated bacteremia was defined by one of the following events occurring after the first episode of bacteremia: 1) development of endocarditis, septic thrombophlebitis (defined by persistent MSSA bacteremia at least 72 hours after initiation of active antimicrobial drugs + documented thrombi), arthritis, spondylitis, as well as end-organ hematogenous spread of infection to other locations; or 2) infection involving vascular or osteoarticular prostheses (excluding intravascular catheter) not removed within 4 days. We also calculated the crude death rate in the first 30 days after the first positive blood culture (30-day mortality) and mortality rate attributable to _S. aureus_ bacteremia (attributable death rate).\n\n【5】The Student unpaired _t_ test was used to compare continuous variables, the Mann-Whitney U test to compare continuous variables with a nonnormal distribution, and the Fisher exact test to compare proportions. All statistical tests were 2-tailed and the threshold of statistical significance was p<0.05. To analyze the risk factors for development of complicated bacteremia, we performed a multivariate forward stepwise logistic regression model including all the clinically relevant variables with a p value of <0.05 and possible confounding factors with a p value of <0.1 detected in the univariate analysis (SPSS software version 15.0, SPSS, Chicago, IL, USA).\n\n【6】All 99 MSSA strains were susceptible to vancomycin (MIC < 2 µg/mL) by the broth microdilution method. Our data showed that, in 23/99 (23.2%) strains, MICs of vancomycin were \\> 1.5 µg/mL by Etest (range 1.5–1.7 µg/mL).\n\n【7】Comparative data of patients with or without a high vancomycin MIC MSSA strain are shown in Table 1 . The incidence of severe sepsis/septic shock was similar in both groups (21.7% vs. 14.5%; p = 0.69), but patients with high vancomycin MIC strains had complicated bacteremia more frequently (78.3% vs. 13.2%; p<0.0001). Attributable death rate was higher in patients with high vancomycin MIC strains with a difference that nearly achieves statistical significance (17.4% vs. 3.9%; p = 0.08).\n\n【8】Comparative data between the 28 patients in whom complicated bacteremia developed and the remaining cohort are shown in Table 2 . The percentage of isolates with vancomycin MIC \\> 1.5 µg/mL was significantly higher in patients with complicated bacteremia (18/28 \\[64.3%\\] vs. 5/71 \\[7%\\]; p<0.0001). Initial treatment with glycopeptides was more frequent in patients in whom complicated bacteremia developed (82.1% vs 57.7%; p = 0.042). Among the 64 patients treated initially with glycopeptides, the rate of complicated bacteremia was significantly higher in patients with high vancomycin MIC isolates (15/18 \\[83.3%\\] vs. 8/46 \\[17.4%\\]; p<0.0001), as occurred in the 25 patients treated initially with β-lactams, (3/5 \\[60%\\] vs. 2/20 \\[10%\\]; p = 0.064). Vancomycin MIC \\> 1.5 µg/mL was the only variable independently related to the risk for complicated bacteremia (OR 22.9, 95% confidence interval 6.7–78.1) in the multivariate analysis.\n\n【9】### Conclusions\n\n【10】The aim of our study was to evaluate whether vancomycin MIC has any influence on the death rates and outcomes of patients with catheter-related MSSA bacteremia. We chose a MIC \\> 1.5 µg/mL as interpretive criteria for diminished susceptibility on the basis of the reported treatment failure for infections caused by organisms who have exhibited this level of vancomycin MIC . A first relevant finding of our study was the relatively high incidence of high vancomycin MIC among MSSA strains producing bacteremia (23.2%), a result similar to the percentage found for MRSA strains in our hospital .\n\n【11】Although a previous study found that vancomycin MICs for MSSA strains recovered from hemodialysis-dependent patients with bacteremia who had been treated with vancomycin did not seem to be related to their clinical outcomes , recently published in vitro data suggest that isolates of _S. aureus_ with high vancomycin MICs could be less susceptible to cloxacillin or daptomycin . Our data showed that patients with MSSA bacteremia caused by strains with high vancomycin MIC were not related to a higher rate of severe sepsis/septic shock development but were associated with a higher rate of complicated bacteremia. In fact, complicated bacteremia was related to a vancomycin MIC \\> 1.5 µg/mL but not with other factors such as age, acquisition of infection, severity of underlying disease, or catheter management, which was confirmed in the multivariate analysis.\n\n【12】The initial treatment most frequently associated with complicated bacteremia, in patients with and without high vancomycin MIC, was the use of glycopeptides alone or followed by antistaphylococcal β-lactams. A possible explanation for this finding is that the first hours of antibiotic treatment are crucial to avoid complications. Nevertheless, in our opinion the greatest risk for complicated bacteremia related to strains of _S. aureus_ with high vancomycin MIC should not only be attributed to the fact that these strains are more resistant to vancomycin because patients infected with strains with high vancomycin MIC that were initially treated with β-lactams also had a clear tendency to develop more complicated bacteremia. Nevertheless, the scarce number of patients who were treated initially with β-lactams limit our results. We hypothesize that certain structural modifications might also occur in the cell wall of strains with high vancomycin MIC, including a thicker cell wall as it has been described in MRSA . Thickness of the cell wall should not only hinder the action of vancomycin, but also the arrival to the target (penicillin binding proteins of β-lactams). If this hypothesis is correct, a vancomycin MIC of 1.5–2 µg/mL in MSSA could be not only a marker of poor response to vancomycin but also a subrogate marker of suboptimal response to β-lactams and even pathogenicity, as has been recently suggested in MRSA isolates .\n\n【13】Some limitations of this study deserve specific consideration. For most patients a treatment schedule including glycopeptides and β-lactams was used, so it is difficult to analyze the role played by each antimicrobial drug. All our strains had a vancomycin MIC <2 µg/mL, so we do not know which would be the outcome of MSSA bacteremia caused by more resistant strains. We did not specifically test clonality of strains with high vancomycin MIC because we had previously demonstrated that MSSA strains isolated from patients with bacteremia at our institution (which coincided with most of the strains included in the present study) were polyclonal . Finally, some important variables such as the previous use of vancomycin or vancomycin serum levels were not included in the analysis.\n\n【14】A high level of resistance to vancomycin is related with the development of complicated complicated bacteremia caused by MSSA, independent of the type of initial antibiotic treatment. Failure of glycopeptides does not appear to be the unique factor for the development of complicated bacteremia in patients with high vancomycin MIC isolates, and therefore intrinsic characteristics of these strains could also explain MSSA’s pathogenic role in the development of complicated bacteremia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c6c50bac-469e-41dc-9e92-a9597453c623", "title": "Spotted Fever Group Rickettsiae in Questing Ticks, Central Spain", "text": "【0】Spotted Fever Group Rickettsiae in Questing Ticks, Central Spain\n**To the Editor:** The number of spotted fever group (SFG) rickettsiae that cause diseases in humans is rapidly increasing ; infections have been described in ticks and humans in Spain . However, in Castilla-La Mancha, central Spain, where recreational parks and hunting estates are abundant and humans may be exposed to infected ticks, information on such infections is not available. Therefore, it is worthwhile to characterize _Rickettsia_ spp. found in this area for epidemiologic studies and proper diagnosis of possible rickettsial diseases.\n\n【1】In this study, we obtained 148 questing adult ticks, representing the most abundant species in the area: 12 _Dermacentor marginatus_ , 26 _Rhipicephalus_ _bursa_ , 41 _Rh. sanguineus_ , 15 _Rh. turanicus_ , 8 _Rh. pusillus,_ 2 _Haemaphysalis punctata_ , 11 _Hyalomma lusitanicum,_ and 33 _Hyalomma marginatum_ . The ticks were collected from the vegetation at natural sites surveyed in Castilla-La Mancha by blanket dragging with a cotton flannelette during fall 2009 and spring–summer 2010  and classified .\n\n【2】Total DNA was extracted from dissected tick internal organs by using the DNeasy Blood & Tissue Kit (QIAGEN, Düsseldorf, Germany) and used to analyze _Rickettsia_ spp. DNA by PCR, cloning, and sequence analysis of the amplicons. At least 3 clones were sequenced for each amplicon. Genes targeted by PCR included fragments of adenosine triphosphate synthase α subunit ( _atpA_ ), heat-shock protein 70 ( _dnaK_ ), outer membrane protein A ( _ompA_ ), outer membrane protein B ( _ompB_ ), citrate synthase ( _gltA_ ), 16S rRNA, _recA_ , and initiator protein of DNA replication ( _dnaA_ ) . To characterize _Rickettsia_ spp. we compared nucleotide sequence identity to reference strains and carried out multilocus analysis using _ompA_ \\- _ompB_ sequences and in silico _PstI_ and _Rsa_ I restriction analysis of _ompA_ sequences .\n\n【3】Ticks were first screened by 16S rRNA PCR, and positive samples were analyzed for all targeted genes. The results showed that 27 (18.2%) of the 148 ticks analyzed were positive for _Rickettsia_ spp. Of these, 11 were confirmed as _R. massiliae_ in _Rh. sanguineus_ , _Rh. turanicus_ , and _Rh. pusillus_ , 3 as _R. raoultii_ in _D. marginatus_ , 2 as _R. slovaca_ in _D. marginatus_ , and 2 as _R. sibirica_ subsp. _mongolitimonae_ in _H. marginatum_ and _Rh. pusillus_ . These species had >99% pairwise nucleotide sequence identity to reference strains _R. massiliae_ MTU5 , _R. slovaca_ 13-B , and _R. sibirica_ subsp. _mongolitimonae_ HA-91  genome sequences for all genes analyzed, and the only _R. raoultii_ reported sequences (accession nos. JQ792107, JQ792166, JQ792134, and NR\\_043755 for _ompB_ , _ompA_ , _gltA_ , and 16S rRNA, respectively). The sequences obtained in this study were deposited in the GenBank under accession nos. KC427998–KC428040.\n\n【4】Multilocus sequence analysis of _ompA_ \\- _ompB_ sequences  and in silico _PstI_ and _Rsa_ I restriction analysis of _ompA_ sequences also confirmed the identity of the _Rickettsia_ spp. identified in this study. As previously shown , multilocus analysis with _ompA_ \\- _ompB_ sequences was highly informative about the phylogenetic relationship between _Rickettsia_ spp. with similar results for maximum likelihood, maximum parsimony, and neighbor-joining methods (data not shown). Furthermore, the results suggested the tick vectors for these _Rickettsia_ spp. in the study area  match those reported or suspected previously for these _Rickettsia_ spp. but for the first time, _R. sibirica_ subsp. _mongolitimonae_ was identified in _Hyalomma_ and _Rhipicephalus_ spp. ticks in Spain .\n\n【5】These tick species are frequently found in the same area feeding on Eurasian wild boar ( _Sus scrofa_ ) and red deer ( _Cervus elaphus_ ), which may act as hosts for these pathogens . To test this hypothesis, we determined the seroprevalence for SFG rickettsiae in these host species in Castilla-La Mancha. Serum samples from 235 red deer and 206 wild boar were analyzed for the presence of anti-SFG _Rickettsia_ antibodies by ELISA (Spotted Fever Rickettsia IgG EIA Antibody Kit, Fuller Laboratories, Fullerton, CA, USA). The ELISA was adapted to test ungulate serum specimens by substituting antihuman IgG-horseradish by protein G-horseradish peroxidase (Sigma-Aldrich, Madrid, Spain). Specific SFG- _Rickettsia_ antibodies were detected in 146 (70.9%) of 206 wild boar and 174 (74.0%) of 235 red deer, indicating a high seroprevalence in these species and thus the possibility that they can serve as hosts for these pathogens.\n\n【6】These tick species also infest humans, thus posing a risk for transmission of rickettsiae that are pathogenic in humans . In fact, Castilla-La Mancha is one of the regions in Spain where a high number of SFG rickettsioses are reported .\n\n【7】In conclusion, these results demonstrate that SFG rickettsiae with public health relevance are found in ticks in central Spain as in other regions in Spain. In central Spain, the widespread distribution of tick vectors and possible wildlife hosts, the presence of persons in tick-infested recreational and hunting areas, and the transstadial and transovarial transmission of the pathogen in ticks may favor transmission to humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f90aad4d-3ac9-402d-b11e-3b0acc17c20d", "title": "Look Local: The Value of Cancer Surveillance and Reporting by American Indian Clinics", "text": "【0】Look Local: The Value of Cancer Surveillance and Reporting by American Indian Clinics\nAbstract\n\n【1】**Introduction**  \nCancer incidence and mortality rates for American Indians in the Northern Plains region of the United States are among the highest in the nation. Reliable cancer surveillance data are essential to help reduce this burden; however, racial data in state cancer registries are often misclassified, and cases are often underreported.\n\n【2】**Methods**  \nWe used a community-based participatory research approach to conduct a retrospective ascertainment of cancer cases in clinic medical records over a 9-year period  and compared the results with the state cancer registry to evaluate missing or racially misclassified cases. Six tribal and/or urban Indian clinics participated in the study. The project team consisted of participating clinics, a state cancer registry, a comprehensive cancer center, an American Indian/Alaska Native Leadership Initiative on Cancer, and a set of diverse organizational partners. Clinic personnel were trained by project staff to accurately identify cancer cases in clinic records. These records were then matched with the state cancer registry to assess misclassification and underreporting.\n\n【3】**Results**  \nForty American Indian cases were identified that were either missing or misclassified in the state registry. Adding these cases to the registry increased the number of American Indian cases by 21.3% during the study period ( _P_ \\= .05).\n\n【4】**Conclusions**  \nOur results indicate that direct reporting of cancer cases by tribal and urban Indian health clinics to a state cancer registry improved the quality of the data available for cancer surveillance. Higher-quality data can advance the efforts of cancer prevention and control stakeholders to address disparities in Native communities.\n\n【5】Introduction\n\n【6】American Indians and Alaska Natives (AI/ANs) often bear a greater cancer burden than other population groups . Cancer incidence and mortality rates for American Indians in the Northern Plains region of the United States, which includes Wisconsin, are among the highest in the nation . Evidence also shows that many AI/AN cancer cases may be misclassified in medical records, underreported to cancer registries, or both . Studies suggest the discrepancies may happen during the hospital intake process when a patient’s race and ethnicity are being recorded, because cancer patients may not accurately report their race/ethnicity or clinic staff erroneously document a patient’s race/ethnicity on the basis of cues such as age, marital status, or language use . Regardless of how these inaccuracies in the data arise, such discrepancies can greatly affect surveillance efforts and present additional barriers to addressing cancer disparities, particularly in small populations.\n\n【7】The objective of this community-based participatory research project was to assess AI/AN cancer case underreporting and misclassification in Wisconsin and to quantify the extent of any inaccuracies. This article reports on the second phase of the Improving American Indian Cancer Surveillance and Data Reporting in Wisconsin study, which was supported through the Great Lakes Native American Research Center for Health (NARCH) and led by Spirit of EAGLES: AI/AN Leadership Initiative on Cancer, a national effort to decrease cancer disparities funded by the National Cancer Institute. The initial phase of this study compared state registry data with national data from the Indian Health Service and found a significant number of misclassifications . The partners in this project saw potential for additional improvements in data accuracy by fostering reciprocal reporting relationships between local tribal health facilities and urban Indian clinics and the state cancer registry. In the present phase of the study, cancer case records from clinics serving AI/AN populations were compared with Wisconsin Cancer Reporting System case data. We sought to answer 2 questions: 1) Does the state cancer registry capture all cancer cases identified in tribal/urban clinic records; and, if so, 2) Is the race of AI/AN cancer patients correctly identified in the case records?\n\n【8】Methods\n\n【9】Recognizing the importance of community participation , we conducted this research project collaboratively among multiple community and partner organizations. In cooperation with tribal and urban Indian clinic partners, the Great Lakes Inter-Tribal Epidemiology Center, the Wisconsin Cancer Reporting System, and the University of Wisconsin Carbone Cancer Center jointly assessed cancer case misclassification of AI/AN populations in Wisconsin. These partnerships were convened and facilitated by Spirit of EAGLES; the Carbone Cancer Center has had a Spirit of EAGLES subcontract since 2000. The overall objective was to develop long-term methods to improve the quality, completeness, and use of cancer data by American Indian tribes in Wisconsin.\n\n【10】Ten of 13 Wisconsin tribal and urban Indian clinics participated in the full study, and 6 clinics were involved in the case-matching endeavor described in this article. Clinics that participated in the matching phase of the study did not differ in any obvious ways from those who declined. Both participating and nonparticipating clinics were diverse in terms of tribal nation affiliations, community size, socioeconomic status, rurality, and geography. The most common reason given for not participating was a perceived lack of time on the part of the clinic staff.\n\n【11】In the initial phase, project staff from Spirit of EAGLES provided training to tribal and urban Indian clinic personnel at each of the participating clinic sites. The personnel were taught cancer case abstraction in accordance with the state cancer registry data collection standards. Training included instructing clinic personnel on how to use medical records to verify a diagnostic confirmation (eg, “microscopic confirmation”), identify the stages of the cancer, and identify treatment therapies, if applicable. The clinic staff was given additional training in the use of International Statistical Classification of Diseases and Related Health Problems cancer codes (ie, ICD-9 codes 140.0–208.9). After completing training, personnel searched clinic electronic medical manager systems for relevant ICD-9 codes, performed a manual chart audit of each case identified in the electronic medical manager systems, and collected abstracted data on project-specific forms. The forms were based on the standard state registry neoplasm record form that all Wisconsin hospitals and clinics were using at that time to report cancer cases. In keeping with the participatory nature of the project, staff from each clinic reviewed these forms for usability and added questions of local interest as desired.\n\n【12】Following the abstraction process, clinics forwarded the completed form and case information directly to the Wisconsin Cancer Reporting System. The data were then linked to the registry case records using unique patient identifiers. To accomplish this linkage, registry staff keyed the clinic data into electronic abstracting software that standardized and formatted it to allow for matching to the registry’s database. Next, registry staff searched for various combinations of identifiers (ie, name, date of birth, Social Security number, and home address) to obtain the best possible match between the clinics’ case information and the state registry. Exact data and “fuzzy” data (eg, year of birth, middle initial) were used to account for common keying errors in both incoming and registry data (eg, reversed month/day and reversed first/middle name). If there was a discrepancy in classification of race/ethnicity between the 2 data sources, the clinic data was typically given preference.\n\n【13】Clinic sites differed on which year services were first offered. Consequently, clinics had different time frames in which cancer cases were queried. For this study, data on cases diagnosed from 1995 through 2003 were chosen for analysis. This was the common interval of available cases across most clinic sites and was also the common interval available in the state registry at the time. Because of varying factors including data cleaning and cross-checking processes, case data in the state registry are typically 3 or more years behind at any given time. In late 2008 when this matching process was conducted, the registry had complete data available until 2003.\n\n【14】The 6 participating clinics submitted 419 cancer cases for linkage with the Wisconsin Cancer Reporting System. Fifty-six (13.4%) of the 419 cases had an unknown date of diagnosis, and 97 (23.2%) had a date of diagnosis either before 1995 or after 2003 and were excluded from analysis. The linkage with the state registry was conducted using the remaining 266 cases . Data from 1 clinic were excluded because all observations fell outside the time window or lacked dates of diagnosis.\n\n【15】Data analyses were completed using SAS 9.2 (SAS Institute, Inc, Cary, North Carolina). We used a Pearson χ 2  statistic, because it is designed to test whether the difference between an established distribution (ie, the prematched AI/AN cancer cases in the state registry) and a new sample distribution (ie, the postmatched numbers of AI/AN cancer cases) is due to chance alone. This test was used to assess whether the change in the number of AI/AN cases in the state registry after the match was significant. Significance was set at _P_ ≤ .05.\n\n【16】Results\n\n【17】Overall, 266 cases with a documented date of diagnosis were found in the records of the clinics . Of these, almost all (n = 254, 95.5%) were reportable by Wisconsin law, although a few were nonreportable (ie, skin cancers, hemangiomas, and intracranial lipomas \\[n = 12, 4.5%\\]) and were excluded from further analyses. Among the reportable cases, 2 categories of case status were created: 1) clinic cases matching a registry report (n = 226, 85.0%) and 2) reportable cases not found in the registry database (n = 28, 10.5%).\n\n【18】Of the 254 reportable clinic cases, 40 American Indian cases (15.7%) were either misclassified in the Wisconsin Cancer Reporting System (n = 16) or were missing from the registry altogether (n = 24). These 40 cases are indicated in the Figure by the 2 shaded cells. With these cases added to the state registry, the number of AI/AN cancer cases in the registry’s database increased from 188 to 228 for the study period, a significant change of 21.3% ( _P_ \\= .05).\n\n【19】Discussion\n\n【20】Our findings show that, before matching, the cancer cases reported to the state cancer registry provided an incomplete characterization of Wisconsin’s AI/AN cancer burden. Although the actual number of newly identified cases was not large — 40 cases from 5 clinics from 1995 through 2003 — it was relevant to this population, resulting in a significant increase in the number of AI/AN cancer cases in the state registry for this period. A small number of newly identified cases affects ongoing surveillance and evaluation. As our study illustrates, direct reporting by tribal/urban clinics can improve the quality of cancer data and may have value for tribes and registries in other states that wish to improve surveillance of AI/AN cancer or work to reduce cancer disparities.\n\n【21】Nationally, tribal and urban Indian centers have become more engaged in cancer surveillance activities through Spirit of EAGLES, the Indian Health Service Division of Epidemiology and Disease Prevention, Centers for Disease Control and Prevention (CDC)–supported Tribal Comprehensive Cancer Control programs, and through other regional efforts, such as those of the Northwest Portland Area Indian Health Board and the New Mexico Tumor Registry. Many cancer control–focused educational and research efforts are ongoing nationally. For example, in Wisconsin, Spirit of EAGLES has convened a coalition of multiple partners including various American Indian tribes and organizations, the Wisconsin Well Woman Program, the Wisconsin Comprehensive Cancer Control Program, and the American Cancer Society. This coalition sponsors an annual statewide American Indian cancer conference for tribal members and is now in its tenth year. Additionally, the Menominee Nation partnered with University of Wisconsin School of Medicine and Public Health in a first-of-its-kind, tribally driven clinical trial on smoking cessation . Participants in these and other efforts across the country have repeatedly recognized the need for ongoing access to AI/AN cancer data, particularly the need for high-quality data at the local level. This study suggests one potential route — that is, direct reporting by tribal/urban clinics to the state cancer surveillance system — to achieving the goal of high-quality local cancer data.\n\n【22】In states without tribal clinics, assessing underreporting may be difficult but remains an important objective. In these states, the first step to assessing potential disparities in reporting is to compare the proportional distribution of the states’ AI/AN populations with national Indian Health Service estimates of cancer burden for that state or region. If it seems reasonable to suspect discrepancies (eg, if AI/AN populations appear to have rates well above or below what would be expected), partnering with state-recognized tribes or local organizations that serve AI/AN populations is a potential next step. Organizations serving these communities can assist in creating strategies to find discrepancies in state data. For example, surnames common to the population of interest could be matched with state registry data to assess potential inconsistencies. Research on Asian American populations in California indicates that such surname list matching is a viable solution to racial/ethnic misclassification when other options are unavailable .\n\n【23】Although local cancer data are highly valued, development of the necessary infrastructure is challenging. This can be particularly true for smaller tribal and urban clinics that may lack personnel with the time or training to abstract cancer cases, have few external resources, and have competing priorities. However, recognizing the reciprocal benefits of improved AI/AN cancer data, local clinics and state cancer registries might partner to achieve this goal. Training and ongoing technical support from registry staff, as well as an efficient means for tribal/urban clinics to report their cases, could help mitigate local clinic limitations. In Wisconsin, electronic reporting systems are already in use at the Wisconsin Cancer Reporting System, opening the potential for integration with clinics across the state. Additionally, annual reports returned to the clinics by the registry could provide information to help the clinics monitor their population’s cancer burden, focus interventions, and build a local cancer registry, while simultaneously demonstrating the value of local reporting. For example, a new project led by the Medical College of Wisconsin is piloting such a reciprocal cancer surveillance relationship between the Wisconsin Cancer Reporting System and the Red Cliff Tribal health facility. Efforts like these are critical to ensure the local high-quality cancer data needed to conduct effective surveillance and reduce cancer health disparities.\n\n【24】This study has both strengths and potential limitations. First, the small number of total cases impeded analysis of additional variables that may have increased our understanding of factors associated with cases being missed or misclassified. For example, patient sociodemographic characteristics and the type of cancer and treatment location may be related to the likelihood that these cases were properly documented, reported, or both. Our small sample size prohibited such exploration. Moreover, it remains unknown why some cases were missing from the registry given mandatory reporting by treating physicians in the state of Wisconsin. However, because tribal and urban Indian clinics are less likely than larger medical settings to treat cancers that they have diagnosed, it may be that these cases were not legally required to be reported. Additionally, the closest facility for a cancer patient to seek treatment may have been in a bordering state, so these cases may have been missed because the treating physician was not governed by Wisconsin law. The study was also limited by the use of data from 1995 through 2003 rather than more recent data. Classification of race may have improved since that time; if so, we would not be able to see the effect of this in our data. Regardless, improving historical data is still useful to strengthen ongoing surveillance and evaluation efforts. An additional limitation of this study was that the generalizability of the findings to all of Wisconsin or to other states may be limited, because not all tribal/urban clinics in the state participated in the study. However, even with limited participation, the matching of clinic data with the Wisconsin Cancer Reporting System still led to a significant improvement in the state-level data.\n\n【25】Our study had strengths as well. The collaborative and participatory process involving multiple organizations allowed for a unique assessment. Moreover, this project demonstrated the feasibility of working with multiple stakeholders to accomplish mutually beneficial ends. By involving local clinics in the project and linking them to resources and training, this project built additional capacity in these communities, which may help them in accomplishing their future data-related goals. Finally, this project not only answered a relevant research question but also accomplished the practical task of improving the state-level cancer registry data, which will benefit future cancer research and surveillance in Wisconsin.\n\n【26】The reporting of new cancer cases to the state cancer registry by American Indian clinics improved the quality of Wisconsin AI/AN cancer data above and beyond standard reporting practices and the previous linkage with national Indian Health Service records. Ongoing reporting should continue to improve data completeness and accuracy. Both the Wisconsin Cancer Reporting System and local tribal and urban Indian clinics have a stake in quality cancer data. Better cancer data will provide a superior foundation for focused cancer prevention and control efforts, informed advocacy for appropriate funding from state and federal sources to address AI/AN cancer disparities, and more accurate data for identifying research questions and conducting epidemiological studies. This study also provides a potential model for other state cancer registries to work with local tribal/urban clinics to increase the accuracy of their cancer data.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f56dc522-1cae-47d4-86af-f103b301eb4d", "title": "Phylogeny of Yellow Fever Virus, Uganda, 2016", "text": "【0】Phylogeny of Yellow Fever Virus, Uganda, 2016\nYellow fever virus (YFV) remains a public health threat; outbreaks occur frequently in regions of Africa and South America to which it is endemic. Genetic analyses have identified 5 YFV genotypes circulating in Africa in distinct geographic regions . This information can be used to identify the origin of outbreaks.\n\n【1】In December 2015, a yellow fever outbreak was identified in Luanda, Angola . A rapid increase in the number of cases was observed in January 2016, and the outbreak subsequently spread to other areas of Angola and neighboring Democratic Republic of the Congo . In April 2016, yellow fever was identified in the southwestern district, Masaka, of Uganda . By June 2016, the Ministry of Health of Uganda had reported 68 suspected yellow fever cases, of which 3 probable and 7 confirmed cases were in the Masaka, Rukungiri, and Kalangala districts . The Uganda Virus Research Institute (Entebbe, Uganda) collaborated with the Centers for Disease Control and Prevention (CDC, Fort Collins, CO, USA) to confirm the presence of YFV RNA in human clinical samples and determine the molecular epidemiology of virus causing the Uganda outbreak.\n\n【2】Serum specimens from the Uganda 2016 outbreak were determined to be YFV RNA-positive by real-time reverse transcription PCR at the Uganda Virus Research Institute, and CDC confirmed the results using a previously published method . One serum sample was selected as the most viable candidate for next-generation sequencing because of its relative concentration of viral RNA, as determined by real-time reverse transcription PCR (cycle threshold <30). The sample was prepared for sequencing on the Ion Torrent Personal Genomics Machine system (Life Technologies, Carlsbad, CA, USA), as previously described . Initial sequencing did not result in any sequence reads aligning with a YFV reference template (SeqMan NGen; DNASTAR, Madison, WI, USA), suggesting that the YFV RNA in the sample was of low quality and/or quantity.\n\n【3】To enhance sequence coverage, we subjected RNA extracted from the selected serum sample to a targeted RNase-H (Epicentre, Madison, WI, USA) digestion to remove the contaminating carrier and ribosomal RNA, as previously described . Then we prepared a standard cDNA library and conducted Ion Torrent sequencing. Fastq files were again aligned to a YFV reference template in SeqMan NGen (DNASTAR). Targeted RNase-H treatment of the RNA sample resulted in 37,637 sequencing reads aligning to the reference template or 1.2% of all sequencing reads corresponding to 38% coverage of the complete YFV genome. Contigs representing partial sequences of 7 coding regions (capsid, membrane, envelope, nonstructural \\[NS\\] 1, NS2B, NS3, and NS5) of the 10 YFV genes were identified. The longest contigs and deepest coverage were identified in partial coding regions of the envelope , NS3 , and NS5 , which were subjected to BLAST analyses . The YFV Uganda 2016 strain envelope sequence was aligned with reference YFV genomes by using MAFFT through the EMBL-EBI server , and phylogenies were generated with BEAST 1.8.4 , as previously described .\n\n【4】BLAST analyses determined that the highest percentage identity (95%) is shared between the Uganda 2016 strain and strains from South Sudan 2003 in the envelope region (the only region for which data from the Sudan strain are available) versus 83% with Angola 2016 strains from the same region. Furthermore, the Uganda 2016 sequences corresponding to the NS genes NS3 and NS5 have the highest percentage identities (94% and 95%, respectively) with a Uganda 1948 strain relative to 85% and 84% with the Angola 2016 strains in the same regions. Together these BLAST analyses indicate that the Uganda 2016 YFV is most similar to strains in the East African genotype. Phylogenetic analyses confirm the BLAST analyses and place the Uganda 2016 YFV in a well-supported clade along with these East African genotype strains, whereas the Angola 2016 strains group with an Angola 1971 YFV , indicating that the Uganda outbreak in 2016 was not seeded by the Angola outbreak.\n\n【5】These findings reiterate the endemicity of YFV throughout the tropical regions of Africa because at least 2 concurrent yellow fever outbreaks of independent origins were identified in 2016. Our findings also highlight the importance of assessing the molecular epidemiology of the virus in outbreak investigations. These data improve our understanding of YFV epidemiology in Africa and support the previous studies of Mutebi and colleagues . In addition, removal of contaminating ribosomal RNA proved to be an effective method for unbiased enrichment of viral RNA in degraded samples to enhance sequencing sensitivity.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "582586d0-8ba1-4dfa-87bf-593a7749e480", "title": "Infection with Scedosporium apiospermum and S. prolificans, Australia", "text": "【0】Infection with Scedosporium apiospermum and S. prolificans, Australia\n_Scedosporium apiospermum_ and _S_ . _prolificans_ are saprophytic molds with a worldwide distribution. _S. apiospermum_ , the anamorph of _Pseudallescheria boydii_ , was described more than a century ago as a cause of Madura foot and subsequently mycetoma and otitis externa. Recently, it has been isolated from patients with chronic lung disease, particularly cystic fibrosis , where the spectrum of infection ranges from colonization to disseminated infection. _S. prolificans_ was first described as a human pathogen in 1984 . _Scedosporium_ spp. cause a broad spectrum of diseases, including soft tissue infections , septic arthritis , osteomyelitis , ophthalmic infections , sinusitis , pneumonia , meningitis and brain abscesses , endocarditis , and disseminated infection . Patients at risk include those immunocompromised because of advanced HIV infection , immunosuppressive therapy and neutropenia , or intravenous drug use .\n\n【1】In the final months of 1999 during construction work, an increased number of _Scedosporium_ spp. isolates and _S_ . _prolificans_ were noted at Alfred Hospital, a university hospital in Prahran, Victoria, Australia, that provides statewide trauma, burn, cystic fibrosis, heart and lung transplant, and HIV services. The records of all patients from whom _Scedosprium_ spp. were isolated from June 30, 1997, through December 31, 2003, were reviewed to describe the epidemiology, clinical features, and outcomes of these infections.\n\n【2】### Methods\n\n【3】#### Data Collection\n\n【4】Records of all patients with _Scedosporium_ spp. cultured between June 30, 1997, and December 31, 2003, were reviewed retrospectively, and demographics, primary illness, antifungal therapy, and presence of other pathogens, and outcomes were recorded. Immunocompromised patients were defined as those with impairment of either or both natural and specific immunity to infection . Those with localized impaired host defenses caused by underlying diseases such as cystic fibrosis were classified as immunocompetent. Invasive infection was defined as a tissue biopsy specimen with hyphae plus culture of the organism . Disseminated infection was defined as positive blood cultures or infection at \\> 2 noncontiguous sites within 7 days. Neutropenia was defined as an absolute neutrophil count <0.5 cells/mm 3  . Descriptive statistics and odds ratios were calculated by using STATA release 8.0 (Stata Corporation, College Station, TX, USA.)\n\n【5】##### Laboratory Methods and Epidemiology\n\n【6】_Scedosporium_ spp. were identified after culture onto horse blood agar and Sabaroud dextrose agar (SDA) and incubation at 30°C and 37°C. Colonies were rapid-growing, gray-white, and downy, with a gray-black reverse side. Species were differentiated by microscopic morphology . Susceptibility testing was performed according to Clinical and Laboratory Standards Institute methods  and synergy studies (2-dimensional 2-agent microdilution checkerboard method) . Synergy was a summation of the fractional inhibitory concentration < 0.500.\n\n【7】For comparison of temporal patterns of mold isolates, the number of persons with _Scedosporium_ spp. during the 3 years subsequent to this review  and _Aspergillus_ spp. for 1999–2005 were extracted from laboratory databases. Rates of detection were expressed as per 1,000 separations and per 100,000 inpatient-patient days.\n\n【8】##### Environmental Sampling\n\n【9】From March 2001 through July 2002, environmental samples were collected from hospital public access areas within and adjacent to the construction site on 7 separate occasions. Additional sampling was conducted in ward corridors, nursing stations, patient rooms, and the patient car park. Air sampling was conducted with a portable high-volume air sampler (MAS 100; Merck, Darmstadt, Germany). The volume of air sampled at each site was 1,000 L/10 min (1 m 3  ). Settle plates were placed in ward corridors and patient rooms for 60 min of exposure. Dust was collected from horizontal surfaces with sterile swabs moistened with sterile saline. Dust and soil specimens were directly placed on standard SDA and selective SDA containing amphotericin B and incubated at 27°C.\n\n【10】### Results\n\n【11】From June 1997 through December 2003, a total of 59 isolates of _Scedosporium_ spp. were cultured from 56 patients. _S_ . _apiospermum_ was isolated from 31 patients, and _S_ . _prolificans_ was isolated from 28 patients. Both species were isolated from 3 patients at separate times (12 days, 13 months, and 27 months apart, respectively). Demographic information, coinfections, and outcomes of the patients are shown according to the species isolated . During the period of this review, an average of 28 allogeneic stem cell, 35 lung, 27 heart, and 15 renal transplants were performed each year.\n\n【12】##### _S_ . _apiospermum_\n\n【13】_S_ . _apiospermum_ was isolated from 32 specimens collected from 31 patients. Twenty-nine isolates were from the respiratory tract; sputum (n = 22), bronchoalveolar lavage (BAL) (n = 5), sinus (n = 1), and lung tissue (n = 1). The remaining 3 isolates were from brain tissue, a central venous catheter tip, and an ear swab, respectively. No blood cultures were positive for _S_ . _apiospermum_ . _S_ . _apiospermum_ was isolated concurrently with _Aspergillus_ spp. from the respiratory tracts of 9 (29%) of the 31 patients.\n\n【14】Twenty-one (68%) of the 31 patients were immunocompromised. Thirteen had undergone solid organ transplantation (11 had lung transplants and 2 had heart transplants), 4 had malignancies (3 had metastatic cancer and 1 had chronic lymphocytic leukemia), 3 had advanced HIV infection, and 1 was receiving immunosuppressive therapy for rheumatoid arthritis. All 10 immunocompetent patients had chronic respiratory tract disease; cystic fibrosis (n = 5), bronchiectasis (n = 4) and chronic mastoiditis (n = 1).\n\n【15】Three patients were lost to follow-up after isolation of _S_ . _apiospermum._ The remaining 28 patients were followed up for a total of 981 months. The mean duration of follow-up was 35 months (median 16 months, range 1–96 months). Two (6%) of the 31 patients had invasive infections. The first patient was a woman who received a lung transplant 5 years earlier and was previously colonized with _Aspergillus_ spp. but not _S_ . _apiospermum_ . She had pulmonary and cerebral lesions and was treated with amphotericin B deoxycholate and itraconazole for presumed aspergillosis. _S_ . _apiospermum_ was isolated from postmortem lung and brain tissue. The second patient was a man who had advanced HIV infection with fungal sinusitis on biopsy. He was treated with voriconazole and surgery but died 3 months later from infection with cytomegalovirus. There were 6 deaths within 1 month of isolation of _S_ . _apiospermum_ . All deaths occurred in immunocompromised patients but only 1 was directly attributable to _S_ . _apiospermum_ . The other deaths resulted from the underlying condition.\n\n【16】Four patients with chronic lung disease were receiving itraconazole for treatment of _Aspergillus_ spp. infection at the time _S_ . _apiospermum_ was isolated. Three of these patients died of respiratory failure 7, 9, and 16 months, respectively, after isolation of _S_ . _apiospermum_ , and 1 was alive when last seen 96 months after fungal isolation. Seven patients received antifungal therapy after isolation of _S_ . _apiospermum_ from respiratory tract samples (4 received voriconazole and 3 received itraconazole). These 7 patients had HIV infection and sinusitis (n = 2), had undergone lung transplantation (n = 3) or had bronchiectasis or cystic fibrosis (n = 2). Both patients with HIV infection had sinusitis and died within 7 months of complications of HIV. Of the other 5 patients, 4 who received azole therapy remained well without invasive infection at 32, 41, 48, and 88 months, respectively, of follow-up. The remaining treated patient died 15 months later; death was not attributed to fungal infection. The median duration of follow-up of those treated with azoles at the time of fungal isolation or subsequent to isolation was 16 months (range 3–96 months); _S_ . _apiospermum_ was not subsequently detected in these patients. The median duration of follow-up for patients receiving no treatment after fungal isolation was 19 months (range 1–84 months); _S_ . _apiospermum_ was isolated from 4 of these patients 1, 18, 30, and 36 months, respectively, after initial fungal isolation.\n\n【17】##### _S_ . _prolificans_\n\n【18】_S_ . _prolificans_ was isolated from 46 specimens obtained from 28 patients. Fourteen (50%) of the 28 patients were immunocompetent. Most (12/14, \\[86%\\]) specimens from immunocompetent patients were from the respiratory tract; cystic fibrosis (n = 6), chronic airway disease (n = 3), nasal discharge and sinus aspirates with chronic sinusitis (n = 3). Patients with cystic fibrosis or airway disease were not considered to have invasive disease and none received antifungal therapy. All 3 patients with chronic sinusitis were treated with surgery, and 2 received itraconazole. One isolate was from knee cartilage of a patient with hemophilia, osteoarthritis, and a knee replacement; the patient underwent surgery and received itraconazole. _S_ . _prolificans_ was also isolated from a skin-biopsy specimen from a patient with multiple-trauma and cellulitis who was treated only with surgery. There were no cases of disseminated infection or deaths resulting from _S_ . _prolificans_ in these immunocompetent patients.\n\n【19】_S_ . _prolificans_ was isolated from 14 immunocompromised patients. Immunodeficiency was caused by lung transplantation (n = 6), hematopoietic stem cell transplantation (HSCT) (n = 6), acute myeloid leukemia (AML), and myelodysplastic syndrome (MDS). _S_ . _prolificans_ was isolated from BAL of 6 lung transplant recipients. One patient was receiving itraconazole for _Aspergillus_ spp. infection, 3 were receiving voriconazole, and 2 were receiving itraconazole and voriconazole with terbinafine. Invasive disease did not develop in any of the 4 lung transplant recipients who received antifungal treatment and none died. Two lung transplant recipients did not receive antifungal treatment. One of these patients was lost to follow-up and 1 survived >25 months without developing invasive infection, although _S_ . _prolificans_ was isolated again 1 year later.\n\n【20】_S_ . _prolificans_ was isolated from August 2000 through September 2002 from 6 patients undergoing allogeneic HSCT and from 2 patients with hematologic malignancy. Four of the 6 HSCT recipients had positive blood cultures, and 4 recipients had skin lesions (multiple, nodular). Findings for 8 patients in whom invasive disease developed are summarized in Table 2 .\n\n【21】Patients 1, 2, and 6 had HSCT complicated by chronic extensive refractory graft versus host disease (GVHD). All 3 died of invasive infection despite treatment with itraconazole or voriconazole and terbinafine; 2 had positive blood cultures. Infections developed in patients 3, 4, and 5 during neutropenia after HSCT. Two of them had positive blood cultures, and both died. Patient 4 received no antifungal treatment, and patient 5 died despite replacement of prophylactic itraconazole with empiric amphotericin B. Patient 3, described elsewhere , survived after treatment with voriconazole, terbinafine, surgery, and neutrophil recovery. Patient 7 had MDS and preceding idiopathic CD4-cell lymphocytopenia. _S_ . _prolificans_ was isolated from sputum, but not BAL, and this patient was not treated. One month later, _S_ . _prolificans_ sinusitis was diagnosed. Despite surgery and treatment with itraconazole, followed by voriconazole and terbinafine, the patient died. Patient 8, a woman, was neutropenic after remission-induction chemotherapy for AML. A swab from a Hickman catheter exit site was positive for _S_ . _prolificans_ . Subsequently, chest wall cellulitis, deep soft tissue infection, and multiple skin nodules developed. She was treated with surgery, voriconazole, and terbinafine. Her neutropenia resolved and she recovered.\n\n【22】After isolation of _S_ . _prolificans_ , 5 of 28 patients were lost to follow-up. The remaining 23 were followed up for a total of 517 months. Mean length of follow-up was 22.5 months (median 9 months, range 1–68 months). Invasive infection occurred in 10 (36%) of 28 infected with _S_ . _prolificans_ compared with 2 (6%) of 31 infected with _S_ . _apiospermum_ . Deaths caused by scedosporiosis occurred in 5 (18%) of 28 infected with _S_ . _prolificans_ and 1 (3%) of 31 infected with _S_ . _apiospermum_ . All deaths occurred in immunocompromised patients; 6 (43%) of 14 infected with _S_ . _prolificans_ and 1 (5%) of 21 infected with _S_ . _apiospermum_ (odds ratio 6.0, 95% confidence interval 0.78–45.62, p = 0.05).\n\n【23】##### Drug-Susceptibility Testing\n\n【24】Drug-susceptibility testing was performed on 7 _S_ . _prolificans_ isolates. MICs (mg/L) were >16 for amphotericin B, >64 for fluconazole, >8 for itraconazole, >64 for 5-fluorocytosine, and >16 for ketoconazole, 2–8 for voriconazole, and 1–4 for terbinafine. Terbinafine and itraconazole were synergistic for 3 of 6 isolates tested, and terbinafine and voriconazole were synergistic for 5 of 7 isolates tested. Four _S_ . _apiospermum_ isolates tested had MICs of 0.5–1 mg/L for itraconazole and 0.5 mg/L for voriconazole.\n\n【25】##### Epidemiology of _Scedosporium_ spp. and _Aspergillus_ spp.\n\n【26】Throughout the study, _S_ . _apiospermum_ was isolated at a constant rate  of 3–6 isolates per year with no seasonal clustering. In contrast, _S_ . _prolificans_ was first detected in December 1999, and thereafter ≈8 isolates were identified annually. Detection of _S_ . _prolificans_ continued in 2004 with 8 isolates but decreased to 6 in 2005 and 4 in 2006. Two clusters of infection occurred in autumn 2001 and 2003 during periods of hospital building that required deep excavation. Air, dust, and soil sampling did not detect an environmental fungal source. _Aspergillus_ spp. were detected during 1999–2005 (mean 191 persons per year, range 153–230). Detection rates for _Scedosporium_ spp. and _Aspergillus_ spp. for 2003–2005 were 0.1–0.23/1,000 separations and 2–3/100,000 inpatient-patient days and 2.4–3.0/1,000 separations and 53–75/100,000 inpatient-patient days, respectively.\n\n【27】### Discussion\n\n【28】This study describes the epidemiology of _Scedosporium_ infection in a cohort of contemporaneous patients at a university hospital. The single-center approach, with cases identified by laboratory detection, allows capture of the full spectrum of infection from colonization to invasive infection. _Scedosporium_ spp. were detected in a broad range of patients and clinical settings. However, there were distinct differences in epidemiology, clinical manifestations, antifungal susceptibility patterns, and outcomes between _S_ . _prolificans_ and _S_ . _apiospermum_ .\n\n【29】Previous reviews have focused on invasive cases reported , but this approach is limited by both selection and publication bias  and does not describe the natural history of infection or the prevalence of these infections. Our series enables the annual incidence of cases, ratio of colonized to infected patients, and natural history of colonization to be determined for each species. We showed that invasive infections accounted for only 6% of _S_ . _apiospermum_ isolates and for 46% of _S_ . _prolificans_ isolates, and that isolation of _Aspergillus_ spp. was 20–30 times more frequent than that of _Scedosporium_ spp.\n\n【30】_S_ . _apiospermum and S_ . _prolificans_ are colonizers of abnormal airways caused by bronchiectasis, cystic fibrosis, chronic obstructive pulmonary disease, or lung transplantation . _S_ . _apiospermum_ was an airway colonizer in < 10% of patients with cystic fibrosis in France and Australia , and similar proportions of lung transplant recipients were colonized with 1 or both species at 1 center in Australia . However, there are few reports that _S_ . _prolificans_ colonized patients with lung disease . In the present study, _S_ . _prolificans_ was identified as an airway colonizer only after December 1999. The onset of invasive infections in HSCT and neutropenic patients occurred in August 2000. _S. prolificans_ was first detected at the M.D. Anderson Cancer Center in Houston, Texas, after 2001 . Emergence of _S_ . _prolificans_ may be related to an environmental source that was not identified or selection pressure caused by changes in antifungal prophylaxis practices . This finding did not appear to be the explanation in our patients because fluconazole remains standard prophylaxis for neutropenia, and itraconazole was used in only 1 of 8 patients. Other possible explanations include use of more aggressive chemotherapy regimens in patients with acute leukemia and the advent of nonmyeloablative allografts, which change characteristics of patients selected for transplants. The reason for persistence of this organism over several years after its initial appearance is also unclear but has also been observed by others .\n\n【31】In patients with respiratory tract disease, both _Scedosporium_ spp. were isolated in comparable numbers. However, the only invasive infection in this diverse group was disseminated _S_ . _apiospermum_ 5 years after lung transplantion. The outcome of 19 patients who had undergone lung or heart lung transplantation and were colonized with this fungus was comparable to the 17 patients in the immunocompetent group with airways disease caused by cystic fibrosis, bronchiectasis, or chronic obstructive pulmonary disease. As in other reports, other opportunistic pathogens, especially _Aspergillus_ spp. (present in one third), were commonly cultured simultaneously . In lung transplant recipients, _Scedosporium_ spp. in BAL was associated with advanced bronchiolitis obliterans and airway stenosis , which emphasizes the difficulty of interpreting the role of _Scedosporium_ spp. in this group. Whether colonization follows airway damage, immunosuppression, or antifungal therapy to treat _Aspergillus_ spp. colonization or is itself an ongoing cause of airway damage such as bronchiolitis obliterans is unknown. In our study group, most patients with respiratory tract colonization had not received antifungal therapy. Although antifungal treatment was used in ≈50% of patients with _S_ . _apiospermum_ airway colonization, there appeared to be no survival advantage in patients treated; our study was not a randomized comparison. With _S_ . _prolificans_ respiratory tract colonization, too few patients were treated with antifungal drugs for valid conclusions to be made. There was no reduction in survival in 5 untreated patients with isolates of either _S_ . _apiospermum_ or _S_ . _prolificans_ . The role of _Scedosporium_ spp. respiratory tract colonization requires further evaluation.\n\n【32】Allogeneic HSCT and AML/MDS were the only settings in which _S_ . _prolificans_ was more common than _S_ . _apiospermum._ There were no _S_ . _apiospermum_ infections in these patients, although in at 1 US cancer center, _S_ . _apiospermum_ infections were more common . This may reflect different geographic distributions of _Scedosporium_ spp. In our study, _S_ . _prolificans_ caused illness with high mortality rates in HSCT and leukemia patients. In HSCT recipients, _S_ . _prolificans_ infections occurred equally in those with neutropenia (early after transplant) and GVHD (later after transplant). Thus, host factors unique to AML and HSCT, such as neutropenia, GVHD, associated macrophage dysfunction, and environmental exposure, may play a role in the propensity for invasive infection with _S_ . _prolificans_ in this group.\n\n【33】As in previous reports , neutropenic patients had sepsis, positive blood cultures, and overwhelming infection (often associated with disseminated rash). In nonneutropenic patients, infection was more likely to be localized initially in lungs, joints, or sinuses. The ability of _S_ . _prolificans_ to grow in blood cultures in AML and HSCT patients is well recognized , and this diagnosis, or infection with _Fusarium_ spp. should be considered when a mold is cultured from blood. In our study, _S_ . _prolificans_ was the only species to grow in blood culture, although others have reported _S_ . _apiospermum_ , albeit, less frequently . Although colonization with _S_ . _prolificans_ without progression to disease was observed in patients with respiratory disease and after lung transplantation, this was not observed in 3 patients with AML/MDS or HSCT. A nonsterile site swab or sputum culture yielding _S_ . _prolificans_ was soon followed by a diagnosis of invasive infection, which indicated that in these patients a culture result, even from a nonsterile site, should be viewed seriously.\n\n【34】The clinical findings of _S_ . _prolificans_ in our patients suggest that this fungus is more invasive than _S_ . _apiospermum_ , as has been found in mice . In addition to disseminated infection in AML patients and HSCT recipients, _S_ . _prolifican_ s also caused locally invasive disease in patients who were not immunocompromised, such as those with posttrauma cellulitis, septic arthritis in a damaged joint, and sinus disease. One proposed mechanism for virulence of _S_ . _prolificans_ is melanin in the cell wall .\n\n【35】Antifungal therapy is problematic with _S_ . _prolificans_ with high MICs for amphotericin B, echinocandins, and azoles. This finding has stimulated interest in combinations of voriconazole and terbinafine , voriconazole and echinocandin , or polyene and echinocandin . However, these infections are rare and experience is limited to case reports . In vitro treatment with interferon-γ and granulocyte-macrophage–colony-stimulating factor has improved neutrophil function against _S_ . _prolificans_ hyphae . Surgery appears to be an important factor in survival in our cases, as well as other series . Although surgery and antifungal therapy were successful in 2 neutropenic patients with invasive _S_ . _prolificans_ infections, these patients also had concomitant neutrophil recovery that could explain their survival.\n\n【36】Much needs to be learned about the epidemiology, transmission, pathogenesis, and environmental niche of _Scedosporium_ spp. especially _S_ . _prolificans_ . A source for emergence of _S_ . _prolificans_ at our hospital was investigated, but none was identified. Our inability to detect the source of emergent _S_ . _prolificans_ was puzzling, although exposure and colonization may have occurred by the time the first case was seen. Cultures may have been obtained too late to identify environmental contamination, although changes in immunosuppression in the population at risk may also have contributed. _S_ . _prolificans_ appeared first as a colonizer of abnormal airways 8 months before the first invasive infection in an HSCT recipient. Shifts in the epidemiology of colonizing organisms in this patient group during hospital construction may be worthy of closer attention and surveillance. Molecular typing methods such as PCR  and inter-simple sequence repeat PCR fingerprinting  have distinguished genotypes from outbreaks and different geographic regions. Recently, _S_ . _apiospermum_ complex has been shown to include several individual species indistinguishable morphologically . These methods will be used for further investigation of our isolates as part of a larger Australian surveillance study.\n\n【37】In conclusion, _S_ . _prolificans_ and _S_ . _apiospermum_ are pathogenic fungi that demonstrate distinct clinical features dependent on the immune function of the host and the type of species isolated. They are usually found in patients with underlying disease, although occasionally after trauma. _S_ . _prolificans_ emerged as a major pathogen in allogeneic HSCT recipients and as a colonizer of patients with underlying lung disease. Madura foot is now rare and was not observed in this series. The high attributable mortality rate of invasive infection with both _Scedosporium_ spp. limitations of antifungal therapy, necessity for aggressive and deforming surgery to treat infections with _S_ . _prolificans_ , and uncertainty over the role of airway colonization emphasize the need to better understand the epidemiology and pathogenesis of this infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6f29f7a5-1ffc-41c9-9cdf-e64d5eebfb8e", "title": "Prevalence of Cancer Screening Among Adults With Disabilities, United States, 2013", "text": "【0】Prevalence of Cancer Screening Among Adults With Disabilities, United States, 2013\nAbstract\n\n【1】**Introduction**\n\n【2】Many studies on cancer screening among adults with disabilities examined disability status only, which masks subgroup differences. We examined prevalence of receipt of cancer screening tests by disability status and type.\n\n【3】**Methods**\n\n【4】We used 2013 National Health Interview Survey data to assess prevalence of 1) guideline-concordant mammography, Papanicolaou (Pap) tests, and endoscopy and stool tests; 2) physicians’ recommendations for these tests; and 3) barriers to health-care access among adults with and without disabilities (defined as difficulty with cognition, hearing, vision, or mobility).\n\n【5】**Results**\n\n【6】Reported Pap test use ranged from 66.1% (95% confidence interval \\[CI\\], 60.3%–71.4%) to 80.2% (95% CI, 72.4%–86.2%) among women with different types of disabilities compared with 81.4% (95% CI, 80.0%–82.7%) among women without disabilities. Prevalence of mammography among women with disabilities was also lower (range, 61.2% \\[95% CI, 50.5%–71.0%\\] to 67.5% \\[95% CI, 62.8%–71.9%\\]) compared with women without disabilities (72.8% \\[95% CI, 70.7%–74.9%\\]). Screening for colorectal cancer was 57.0% among persons without disabilities, and ranged from 48.6% (95% CI, 40.3%–57.0%) among those with vision limitations to 64.6% (95% CI, 58.5%–70.2%) among those with hearing limitations. Receiving recommendations for Pap tests and mammography increased all respondents’ likelihood of receiving these tests. The most frequently reported barrier to accessing health care reported by adults with disabilities was difficulty scheduling an appointment.\n\n【7】**Conclusion**\n\n【8】We observed disparities in receipt of cancer screening among adults with disabilities; however, disparities varied by disability type. Our findings may be used to refine interventions to close gaps in cancer screening among persons with disabilities.\n\n【9】Introduction\n\n【10】Cancer is the second leading cause of death among adults in the United States . Some cancers are preventable with regular screening tests and can be cured if detected and treated early. However, disparities in the use of preventive health services exist. People with disabilities have lower cancer screening rates than people without disabilities . In 2013, approximately 1 in 5 US adults reported having a disability; disabilities in mobility and cognition were the most frequently reported types . People with disabilities may have numerous health-care access barriers, including inaccessible health communication formats, limited access to transportation and parking, and lack of height-adjustable examination tables, accessible mammography equipment, and medical staff trained on proper patient lifting, transferring, and positioning techniques .\n\n【11】People with disabilities are a heterogeneous group whose health needs vary with the types of limitations they have and by the nature of their disabilities . Definitions of disability and the level of functioning that qualifies for disability status vary. Many studies on cancer screening among this population used broad disability measures based on limitations in actions within the environment to compare prevalence of screening between people with and without disabilities, and a few disaggregated data by disability severity or type to look at subgroup differences .\n\n【12】In studies of up-to-date breast cancer screening, screening rates ranged from 67% to 79% among women with disabilities and from 70% to 83% among women without disabilities . Receipt of up-to-date cervical cancer screening among women with disabilities ranged from 77% to 82%, compared with a range of 83% to 87% among women without disabilities . Breast and cervical cancer screening rates are lower among women with intellectual and developmental disabilities, cognitive disabilities, and multiple disabilities than among women with other disability types . Numerous studies of colorectal cancer (CRC) screening among people with disabilities assessed whether a person was ever screened, rather than whether screening was guideline-concordant . Some studies reported that CRC screening rates are higher among people with disabilities than among those without disabilities; however, rates varied by disability type . Up-to-date CRC screening rates were recently published in a study examining receipt of tests from 1998 through 2010 among people with and without chronic disabilities; the authors reported that receipt was similar between both groups . The data in many earlier studies of cancer screening prevalence are now nearly a decade old or older and may not reflect recent trends. Additionally, few studies examined receipt of a health-care provider’s recommendation for screening .\n\n【13】In this study, we used a nationally representative sample to examine differences in receipt of guideline-concordant screening in 2013 for breast cancer, cervical cancer, and CRC by disability status and by type of disability. We report the prevalence of up-to-date breast and cervical cancer screening among women with and without disabilities by screening recommendation status and the prevalence of receiving a recommendation for screening among adults with and without disabilities who were not up-to-date with CRC screening. In addition, we describe perceived barriers to health-care access by disability status.\n\n【14】Methods\n\n【15】We used data from the 2013 National Health Interview Survey (NHIS), a continuous, cross-sectional survey of US households conducted in-person with noninstitutionalized civilians. NHIS uses trained US Census Bureau interviewers and monitors for quality control. Sampling is done through a complex survey design that involves stratification, clustering, and multiple stages. One sample adult is selected in the household to provide additional information, which in 2013 included information for a cancer control supplement. The final adult response rate for the 2013 survey was 61.2% .\n\n【16】We included adults aged 21 to 75 years who provided responses to questions in the NHIS cancer control supplement and the disability module. The disability module consists of 6 questions, the following 4 of which measure serious functional limitations pertaining to hearing, vision, cognition, and mobility : Are you deaf or do you have serious difficulty hearing? Are you blind or do you have serious difficulty seeing even when wearing glasses? Because of a physical, mental, or emotional condition, do you have serious difficulty concentrating, remembering, or making a decision? Do you have serious difficulty walking or climbing stairs? In the 2013 survey, these questions were administered to approximately half of sample adults; household members serving as proxies for those who were unable to respond were excluded. Even though survey respondents could report more than one limitation, we excluded those with a mobility disability (eg, difficulty walking or climbing stairs) from the hearing, vision, and cognition categories. We did this because most people with multiple disabilities also have a mobility disability, and we wanted to make our estimates comparable with those in previous disability studies. The hearing, vision, and cognitive categories were not mutually exclusive, and respondents could be in more than one of these categories. Adults who answered no to all disability questions were considered not to have a serious functional limitation. We excluded from the analysis adults who reported having difficulty with self-care (eg, bathing, dressing) or having limitations with community involvement (eg, shopping or running errands alone) but did not report other disabilities (n = 65).\n\n【17】We merged the disability questions file, the sample adult file, and the person file to form the final analytic data set. The merged file had data on 15,079 adults aged 21 to 75 years with known disability status, 12,499 adults without a serious functional disability, and 2,580 adults with at least 1 of the 4 types of disability.\n\n【18】We included the following demographic variables: sex, race/ethnicity, marital status, education level, and general health status. Variables measuring whether respondents had health insurance coverage or usual sources of health care were used to assess health-care access. Barriers to care, such as difficulties with scheduling appointments and reaching clinic staff by telephone, long waiting-room times, inconvenient clinic hours, and lack of transportation served as proxies for barriers to cancer screening. The cancer control supplement included questions about receipt of screening tests for breast cancer, cervical cancer, and CRC. If a survey respondent indicated that he or she had received a screening test, then a follow-up question was asked about the timing of the test. Additionally, women who had seen a clinician within the previous 12 months were asked whether a health-care professional recommended that they receive a mammogram or Papanicolaou (Pap) test. Adults who reported that they were not up to date with screening for CRC were asked whether a health-care provider had recommended that they be tested for problems in their colon or rectum within the previous 12 months. We used US Preventive Services Task Force recommendations that were in place in 2013 to define up-to-date cancer screening. For cervical cancer screening, we assessed whether women aged 21 to 65 years who did not report having had a hysterectomy had received a Pap test within the past 3 years. For breast cancer screening, we assessed whether women aged 50 to 74 years had received a mammogram within the past 2 years. Both men and women aged 50 to 75 years were considered appropriately screened for CRC and current with the screening recommendation if they had received either a fecal occult blood (FOBT) test within the previous year, sigmoidoscopy within the previous 5 years along with FOBT within the previous 3 years, or colonoscopy within the previous 10 years.\n\n【19】We used SAS 9.3 (SAS Institute Inc) and SAS-callable SUDAAN release 11 (Research Triangle Institute) to conduct all analyses. All prevalence estimates were weighted so that they represent the noninstitutionalized, civilian US population. The final survey weights account for race/ethnicity, sex, and age composition of that population.\n\n【20】We calculated the weighted prevalence estimates with 95% confidence intervals (CIs) for demographic characteristics, health-care access, and cancer screening (including by provider recommendation) among adults without a disability; separately for adults with hearing, vision, cognition, or mobility limitations; and for all adults with any of these disability types. We conducted multivariable logistic regression analysis to obtain adjusted odds ratios for the likelihood of being up-to-date for cancer screening for each type of disability (adults without disability served as the referent group for all models), while controlling for the influence of demographic characteristics and health-care access. In all models, we controlled for race/ethnicity, marital status, education level, health-care coverage, general health status (excluded in the hearing disability model for CRC screening because of statistical nonsignificance), and usual source of care. The cervical cancer screening model also controlled for age category, and the CRC screening model controlled for age category and sex. We used a backward elimination approach to select variables for the final models and assessed goodness of fit using the Hosmer–Lemeshow test. Variables with _P_ ≥ .10 from the Wald _F_ test were eliminated unless they traditionally appeared in cancer screening models.\n\n【21】Results\n\n【22】Overall, 16.9% of adults aged 21 to 75 years were identified as having at least 1 of the 4 disability types. The prevalence of each disability type among this sample was mobility, 57.9%; cognition, 18.2%; hearing, 18.8%; and vision, 11.2%. Thirty-two percent of persons had more than one disability, with most (82.8%) having mobility limitations in combination with another disability. Regardless of disability type or status, most adults were non-Hispanic white and had health insurance coverage and a usual source of care . Compared with persons with no disability (6.4%), the prevalence of reporting fair or poor health was substantially higher among persons with a mobility disability (63.3%), a cognitive disability (38.4%), a vision disability (29.4%), or a hearing disability (17.4%).\n\n【23】In unadjusted prevalence estimates , women with any of the disability types had a lower prevalence of reporting up-to-date cervical and breast cancer screening than women without a disability. However, men and women with any of the disability types had a slightly higher prevalence of up-to-date CRC screening than adults without disabilities. Women with mobility limitations had the lowest prevalence (66.1%) of receiving a Pap test within the past 3 years, whereas those with cognitive limitations had the highest (80.2%). The reverse was true for receipt of mammograms within the past 2 years; prevalence was lower among women with cognitive limitations (61.2%) and higher among those with mobility limitations (67.5%). When controlling for sociodemographic and health-care–related variables we found that compared with women without disability, the odds of receiving a Pap test within the previous 3 years were significantly lower among women with any of the disability types (AOR, 0.77; 95% CI, 0.60–0.99) and among women with a mobility limitation (AOR, 0.58; 95% CI, 0.42–0.80). The odds of receiving mammograms within the previous 2 years were also lower among women with any of the disability types; however, this finding was not significant.\n\n【24】The prevalence of up-to-date breast and cervical cancer screening was higher among women who reported receiving recommendations from their health-care providers for these tests than among women who said they had not received recommendations . Among women who reported receiving recommendations, those with any of the disability types were less likely than those without any of the disability types to report receiving Pap tests within the previous 3 years (84.1% vs 94.7%, respectively, _P_ < .001). Among women who indicated that they had not received recommendations, those with any of the disability types reported less use of mammography (32.6% vs 48.6%, _P_ < .001) and of Pap tests (55.2% vs 66.7%, _P_ \\= .003) than women without any of the disability types. Among adults who reported that they were not up-to-date with screening for CRC, the percentage who had received recommendations for CRC screening tests in the previous 12 months was 15.2% for those with any of the disability types and 11.9% for those without any of the disability types ( _P_ \\> .05) (data not shown). The prevalence of perceived barriers to accessing health care was higher among persons with any of the disability types than persons without any of the disability types; the most frequently cited barrier among both groups was difficulty getting a clinic appointment (9.1% vs 4.5%) .\n\n【25】Discussion\n\n【26】Overall, our findings on cancer screening among persons with disabilities are consistent with some previous research on this topic . We found a lower prevalence of breast and cervical cancer screening among women with disabilities, but only the disparity in Pap test use persisted in models adjusted for cancer screening-related variables. A similar pattern was reported in a study on use of mammography and Pap tests by disability status and severity . In a study examining receipt of these tests among women with and without limitations in activities of daily living, however, disparities remained significant in multivariable analyses for each test . Similar to previous research on CRC screening, we found slightly higher rates of screening among adults with disabilities than among those without disabilities . Even though persons with disabilities were reported to have higher CRC screening rates than persons without disabilities from 1998 to 2010, the rates reported in 2010 (59.2% vs 58.9%) and in our study (61.8% vs 57%) are below the 70.5% _Healthy People 2020_ objective  . The breast and cervical cancer screening rates in our study are also lower than national objectives for these tests (81.1% for breast cancer, 93.0% for cervical cancer). Future studies should explore the reasons why some persons with disabilities may avoid or delay cancer screening, including the extent to which factors such as disability type, severity of limitation, and confusion about screening guidelines affect decision making. Research on this topic is scarce, especially for cervical cancer and CRC.\n\n【27】We found that the prevalence of breast and cervical cancer screening was higher among women who reported receiving recommendations for these tests from their health-care providers, which is consistent with previous studies . Women with disabilities were slightly less likely than those without disabilities to report receipt of screening recommendations, but the differences were not significant. Less than 15% of persons who were not up-to-date with CRC screening reported receiving recommendations for these tests, regardless of disability status. We did not find other studies that examined receipt of recommendations for CRC screening among this population. Additional research is needed to help identify the reasons why health-care providers might not refer persons with disabilities for cancer screening. Some providers may prioritize managing the disabling condition and related illnesses to the exclusion of addressing preventive health needs . Tools have been developed to help clinicians identify recommended preventive services and increase their use among persons with disabilities . Information is also available for health insurers, community-based organizations, and health educators to help increase their knowledge about disability and health and identify service gaps .\n\n【28】We found that most people with disabilities had health insurance coverage and a usual source of health care. This finding is consistent with previous research and may be related to links between enrollment in federal and state disability benefits programs and eligibility for public health insurance . However, some insured people with disabilities reported unmet health-care needs and difficulty accessing care. These disparities may be related to disability type (eg, mobility limitations), or they may be unrelated to disability status (eg, out-of-pocket cost) . In our study, people with disabilities were 7 times as likely to report transportation barriers as those without disabilities and nearly twice as likely to report difficulty scheduling appointments, long wait times at clinics, and inconvenient clinic hours.\n\n【29】Our study has limitations. NHIS data are based on self-report and may be subject to recall bias, particularly among persons with serious cognitive disabilities, and subject to social desirability bias. The questions used to measure disability in the NHIS disability module, however, have been cognitively tested by the National Center for Health Statistics and the US Census Bureau . Additionally, self-reported data for mammography, Pap tests, and CRC screening are reported to be reliable . The small sample sizes for some disability subgroups limit the generalizability of our findings, but the sizes all met criteria for analysis; the data are also weighted to be representative of the US population. Nearly one-third of respondents had more than one type of disability; therefore, our results might not be generalizable to those with a single disability. Because we could not distinguish between tests received for diagnostic purposes and tests received for screening purposes, actual cancer screening rates may be lower than reported. We also could not assess the onset of disability in relation to receipt of screening tests. Using data based on disability status at the time of data collection rather than the time of screening could have resulted in overestimates of screening among those whose disabilities were diagnosed after they were screened. Furthermore, because the NHIS disability survey includes only noninstitutionalized adults living in the community, we could not assess the prevalence of cancer screening among adults living in group homes and other settings in which the prevalence of disability may be higher.\n\n【30】One of the strengths of our study is that we used population-based data. We also examined 2 cancer screening-related topics that have been rarely researched among persons with disabilities — reported receipt of guideline-concordant CRC screening and recommendations by health-care providers for cancer screening.\n\n【31】Disparities among persons with disabilities in receipt of preventive services and health-care access persist. Our study findings may be used to increase awareness about gaps in cancer screening among subgroups of this population, to inform development of interventions that educate people with disabilities about the importance of discussing preventive health services with their health care providers, and to remind providers about the critical roles they play in recommending use of these services.\n\n【32】Tables\n------\n\n【33】#####  Table 1. Prevalence of Selected Demographic Characteristics and Health Behaviors Among Adults Aged 21–75 Years (N = 15,079), by Disability Type a  , National Health Interview Survey, United States,\n\n| Characteristic | Type of Disability, % (95% Confidence Interval) |\n| --- | --- |\n| Hearing | Vision | Cognitive | Mobility | Any Type | None |\n| --- | --- | --- | --- | --- | --- |\n| **Total** | 454 | 304 | 471 | 1,512 | 2,580 | 12,499 |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 62.5 (57.2–67.6) | 47.9 (41.5–54.3) | 47.8 (42.2–53.4) | 43.4 (40.4–46.4) | 47.5 (45.3–49.7) | 46.8 (45.7–47.9) |\n| Female | 37.5 (32.4–42.8) | 52.1 (45.7–58.5) | 52.2 (46.6–57.8) | 56.6 (53.6–59.6) | 52.5 (50.3–54.7) | 53.2 (52.1–54.3) |\n| **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** |\n| 21–49 | 27.3 (23.0–32.1) | 42.7 (36.2–49.5) | 53.6 (48.2–59.0) | 21.3 (18.9–24.0) | 29.7 (27.6–31.9) | 60.9 (59.5–61.7) |\n| 50–64 | 35.7 (31.1–40.6) | 36.1 (30.2–42.4) | 33.0 (28.0–38.4) | 46.5 (43.7–49.4) | 41.4 (39.3–43.7) | 27.0 (26.0–27.9) |\n| 65–75 | 37.0 (32.2–42.0) | 21.2 (16.2–27.3) | 13.4 (10.4–17.0) | 32.2 (29.4–35.1) | 28.9 (26.9–30.9) | 12.4 (11.7–13.2) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 81.2 (77.3–84.5) | 60.5 (53.8–66.7) | 70.6 (65.9–74.9) | 66.1 (63.1–68.9) | 69.4 (67.2–71.5) | 66.7 (65.6–67.7) |\n| Non-Hispanic black | 5.8 (4.1–8.2) | 14.7 (10.8–19.6) | 14.8 (12.0–18.1) | 18.4 (16.1–21.0) | 15.1 (13.5–16.9) | 11.9 (11.2–12.6) |\n| Hispanic | 7.7 (5.6–10.6) | 18.6 (14.4–23.7) | 9.9 (7.4–13.2) | 10.7 (9.1–12.5) | 10.6 (9.3–12.1) | 14.0 (13.3–14.7) |\n| Other b | 5.3 (3.6–7.7) | — c | 4.8 (3.2–7.1) | 4.9 (3.8–6.2) | 4.9 (4.0–5.9) | 7.4 (6.9–8.0) |\n| **Marital status** | **Marital status** | **Marital status** | **Marital status** | **Marital status** | **Marital status** | **Marital status** |\n| Married/living together | 51.0 (45.4–56.5) | 37.9 (32.1–44.0) | 29.0 (24.3–34.3) | 40.1 (37.0–43.2) | 40.5 (38.2–42.9) | 54.8 (53.6–56.0) |\n| Single/never married | 17.5 (13.6–22.2) | 22.1 (16.9–28.4) | 35.9 (30.7–41.4) | 16.4 (14.3–18.7) | 19.9 (18.0–21.9) | 24.6 (23.5–25.8) |\n| Divorced/widowed/ separated | 31.5 (26.5–37.0) | 40.0 (34.1–46.2) | 35.1 (30.2–40.2) | 43.5 (40.6–46.5) | 39.6 (37.4–41.8) | 20.5 (19.7–21.4) |\n| **Education** | **Education** | **Education** | **Education** | **Education** | **Education** | **Education** |\n| Less than high school diploma | 18.4 (14.5–23.0) | 22.7 (18.0–28.3) | 22.4 (18.9–26.4) | 25.1 (22.4–28.0) | 23.2 (21.4–25.1) | 9.9 (9.3–10.5) |\n| High school diploma/GED | 24.9 (20.6–29.8) | 32.5 (27.0–38.5) | 31.3 (27.0–36.0) | 31.0 (28.0–34.3) | 30.0 (27.8–32.3) | 24.1 (23.1–25.1) |\n| Some college/ associates degree | 30.9 (26.1–36.2) | 26.3 (21.2–32.1) | 29.5 (24.9–34.5) | 31.1 (28.2–34.1) | 30.1 (28.1–32.2) | 31.2 (30.2–32.3) |\n| Bachelor’s degree or higher | 25.8 (21.1–31.3) | 18.5 (13.6–24.6) | 16.8 (12.9–21.5) | 12.8 (10.8–15.1) | 16.7 (15.0–18.5) | 34.8 (33.6–36.0) |\n| **General health status** | **General health status** | **General health status** | **General health status** | **General health status** | **General health status** | **General health status** |\n| Excellent/very good | 47.5 (42.0–53.2) | 37.3 (30.6–44.5) | 23.5 (19.1–28.7) | 11.7 (9.8–13.8) | 22.8 (20.7–25.0) | 68.6 (67.6–69.5) |\n| Good | 35.1 (30.2–40.4) | 33.4 (27.8–39.5) | 38.1 (33.3–43.0) | 25.1 (22.6–27.7) | 29.4 (27.5–31.5) | 25.1 (24.2–26.0) |\n| Fair/poor | 17.4 (13.4–22.2) | 29.4 (23.9–35.5) | 38.4 (33.3–43.8) | 63.3 (60.4–66.1) | 47.8 (45.3–50.2) | 6.4 (5.9–6.9) |\n| **Has insurance coverage** | **Has insurance coverage** | **Has insurance coverage** | **Has insurance coverage** | **Has insurance coverage** | **Has insurance coverage** | **Has insurance coverage** |\n| Yes | 86.5 (82.7–89.6) | 80.7 (75.7–84.9) | 81.9 (77.1–85.9) | 89.4 (87.4–91.1) | 87.0 (85.4–88.4) | 82.3 (81.5–83.1) |\n| **Has usual source of care** | **Has usual source of care** | **Has usual source of care** | **Has usual source of care** | **Has usual source of care** | **Has usual source of care** | **Has usual source of care** |\n| Yes | 85.9 (81.8–89.2) | 79.5 (72.6–85.1) | 88.3 (84.4–91.3) | 93.8 (92.2–95.1) | 90.3 (88.8–91.6) | 82.8 (81.9–83.6) |\n\n【35】Abbreviation: GED, general educational development.  \na  Respondents could report more than one limitation and were included in the analysis for each reported limitation, with the exception of mobility limitations. Regardless of any additional limitation, people with mobility limitations were only included in the mobility limitation subgroup.  \nb  American Indian/Alaska Native, Asian, multiple races, and race group not releasable.  \nc  Estimate suppressed because relative standard error was greater than 30%.\n\n【36】#####  Table 2. Prevalence and Adjusted Odds Ratios a  for Up-to-Date Cancer Screening Among Adults With a Disability Compared With Adults With No Disability, National Health Interview Survey, United States,\n\n| Characteristic | Disability Type b |\n| --- | --- |\n| Hearing | Vision | Cognitive | Mobility | Any Type | None |\n| --- | --- | --- | --- | --- | --- |\n| **Pap test** | **Pap test** | **Pap test** | **Pap test** | **Pap test** | **Pap test** | **Pap test** |\n| Pap test within past 3 years c , n | 90 | 94 | 166 | 392 | 700 | 5,184 |\n| Yes | 73.1 (61.9–81.9) | 76.6 (64.8–85.3) | 80.2 (72.4–86.2) | 66.1 (60.3–71.4) | 71.5 (67.4–75.2) | 81.4 (80.0–82.7) |\n| AOR for up-to-date status | 0.82 (0.44–1.53) | 0.99 (0.51–1.90) | 1.20 (0.71–2.03) | 0.58 (0.42–0.80) | 0.77 (0.60–0.99) | Reference |\n| **Mammogram** | **Mammogram** | **Mammogram** | **Mammogram** | **Mammogram** | **Mammogram** | **Mammogram** |\n| Mammogram within past 2 years d , n | 115 | 80 | 107 | 633 | 897 | 2,544 |\n| Yes | 66.5 (55.4–76.1) | 63.7 (50.4–75.2) | 61.2 (50.5–71.0) | 67.5 (62.8–71.9) | 66.7 (63.0–70.2) | 72.8 (70.7–74.9) |\n| AOR for up-to-date status | 0.89 (0.53–1.49) | 0.88 (0.48–1.61) | 0.91 (0.54–1.55) | 1.04 (0.77–1.40) | 0.97 (0.76–1.25) | Reference |\n| **Colorectal cancer screening** | **Colorectal cancer screening** | **Colorectal cancer screening** | **Colorectal cancer screening** | **Colorectal cancer screening** | **Colorectal cancer screening** | **Colorectal cancer screening** |\n| Colorectal cancer screening e , n | 320 | 175 | 216 | 1,136 | 1,746 | 4,726 |\n| Yes | 64.6 (58.5–70.2) | 48.6 (40.3–57.0) | 56.2 (47.9–64.2) | 63.1 (60.0–66.1) | 61.8 (59.1–64.5) | 57.0 (55.3–58.6) |\n| Received colonoscopy f , n | 93.1 (87.5–96.3) | 92.8 (84.5–96.8) | 93.6 (86.3–97.2) | 93.5 (91.2–95.3) | 93.5 (91.5–95.0) | 94.7 (93.7–95.6) |\n| AOR for up-to-date status | 1.41 (1.04–1.91) | 0.90 (0.60–1.35) | 1.25 (0.84–1.85) | 1.33 (1.12–1.58) | 1.29 (1.10–1.52) | Reference |\n\n【38】Abbreviation: AOR, adjusted odds ratio; Pap, Papanicolaou.  \na  Adjusted odds ratios are from logistic regression analyses that examined disability status in relation to being up-to-date on cancer screening tests while controlling for race/ethnicity, insurance, having a usual source of health care, general health status, marital status, and education. Colorectal cancer screening models also included sex and age category. General health status was not included in the hearing disability model because it was not significant. Pap test models also were controlled for age category.  \nb  Values are percentage (95% confidence interval) unless otherwise stated.  \nc  Women aged 21 to 65 years who had a Pap test within the past 3 years. Data on Pap tests were available for 5,884 women.  \nd  Women aged 50 to 74 years who had a mammogram within the past 2 years. Data on mammograms were available for 3,441 women.  \ne  Adults aged 50 to 75 years who had a high-sensitivity fecal occult blood test within the past 12 months, a sigmoidoscopy within the past 5 years with a high-sensitivity fecal occult blood test within the past 3 years, or a screening colonoscopy within the past 10 years. Data on colorectal cancer screening were available for 6,472 men and women.  \nf  Colonoscopy within the past 10 years among adults with known disability status who reported being up-to-date with colorectal cancer screening (n = 3,677).\n\n【39】#####  Table 3. Perceived Barriers to Health Care Access by Disability Status (N = 12,499 a  ), National Health Interview Survey, United States,\n\n| Barrier | Any Disability b | No Disability |\n| --- | --- | --- |\n| % (95% Confidence Interval) |\n| --- |\n| Difficulty getting through on telephone to reach clinic | 4.2 (3.4–5.2) | 1.6 (1.4–1.9) |\n| Difficulty getting clinic appointment | 9.1 (7.9–10.6) | 4.5 (4.1–4.9) |\n| Wait time at clinic too long | 7.3 (6.1–8.7) | 2.8 (2.5–3.2) |\n| Clinic not open at convenient times | 4.3 (3.5–5.4) | 2.3 (2.0–2.6) |\n| No available transportation to clinic | 6.5 (5.4–7.8) | 0.8 (0.7–1.0) |\n\n【41】a  The denominator for each survey question varies because of the exclusion of persons with unknown and missing responses.  \nb  Includes hearing, vision, cognitive, or mobility disability.\n\n【42】Post-Test Information\n\n【43】To obtain credit, you should first read the journal article. After reading the article, you should be able to answer the following, related, multiple-choice questions. Credit cannot be obtained for tests completed on paper, although you may use the worksheet below to keep a record of your answers.org, please click on the Register link on the right hand side of the website to register. Only one answer is correct for each question. Once you successfully answer all post-test questions you will be able to view and/or print your certificate. For questions regarding the content of this activity, contact the accredited provider, CME@medscape.net . For technical assistance, contact CME@webmd.net . American Medical Association’s Physician’s Recognition Award (AMA PRA) credits are accepted in the US as evidence of participation in CME activities. The AMA has determined that physicians not licensed in the US who participate in this CME activity are eligible for **_AMA PRA Category 1 Credits™_** . Through agreements that the AMA has made with agencies in some countries, AMA PRA credit may be acceptable as evidence of participation in CME activities. If you are not licensed in the US, please complete the questions online, print the AMA PRA CME credit certificate and present it to your national medical association for review.\n\n【44】Post-Test Questions\n\n【45】### Study Title: Prevalence of Cancer Screening Among Adults With Disabilities, United States,\n\n【46】#### CME Questions\n\n【47】1.  You are seeing a 53-year-old woman with a history of cerebral palsy and intellectual disability. She has chronic mild to moderate disabilities related to these conditions. According to the current study by Steele and colleagues, screening for which of the following cancer types was **_more_** likely to be up-to-date among adults with disabilities vs those without disabilities?\n\n【48】    1.  Colorectal cancer\n\n【49】    2.  Cervical cancer\n\n【50】    3.  Breast cancer\n\n【51】    4.  Lung cancer\n\n【52】2.  A review of the patient’s record indeed indicates that she is behind on screening requirements for breast and cervical cancer. How did the type of disability affect screening rates for these cancers in the current study?\n\n【53】    1.  Mobility limitations were associated with the lowest rates of cervical cancer screening; cognitive limitations were associated with the lowest rates of breast cancer screening\n\n【54】    2.  Cognitive limitations were associated with the lowest rates of both cervical and breast cancer screening\n\n【55】    3.  Mobility limitations were associated with the lowest rates of both cervical and breast cancer screening\n\n【56】    4.  Cognitive limitations were associated with the lowest rates of cervical cancer screening; mobility limitations were associated with the lowest rates of breast cancer screening\n\n【57】3.  You recommend that she receive breast and cervical cancer screening. According to the current study, which of the following statements regarding healthcare recommendations for cancer screening for adults with disabilities is **_most_** accurate?\n\n【58】    1.  Recommendations were not associated with higher rates of breast cancer screening\n\n【59】    2.  Recommendations were not associated with higher rates of cervical cancer screening\n\n【60】    3.  Recommendations were more effective for promoting cervical cancer screening among women with a disability compared with women without disabilities\n\n【61】    4.  Among women who did not receive a recommendation, women with disabilities had lower rates of mammography and Pap tests compared with women without disabilities\n\n【62】4.  The patient is hesitant to follow through on your recommendations. What was the **_most_** commonly cited barrier to healthcare access in the current study?\n\n【63】    1.  Too long of a wait at the clinic\n\n【64】    2.  Clinic not open at convenient times\n\n【65】    3.  No available transportation to clinic\n\n【66】    4.  Difficulty getting a clinic appointment\n\n【67】##### Evaluation\n\n| **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** | **1\\. The activity supported the learning objectives.** |\n| --- | --- | --- | --- | --- |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| **1** | **2** | **3** | **4** | **5** |\n| **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** | **2\\. The material was organized clearly for learning to occur.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| **1** | **2** | **3** | **4** | **5** |\n| **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** | **3\\. The content learned from this activity will impact my practice.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| **1** | **2** | **3** | **4** | **5** |\n| **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** | **4\\. The activity was presented objectively and free of commercial bias.** |\n| **Strongly Disagree** |  |  |  | **Strongly Agree** |\n| **1** | **2** | **3** | **4** | **5** |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "023814bf-9715-4a82-bca9-0a249e67c760", "title": "Identification of Potential Environmentally Adapted Campylobacter jejuni Strain, United Kingdom", "text": "【0】Identification of Potential Environmentally Adapted Campylobacter jejuni Strain, United Kingdom\nHuman campylobacterosis shows a marked seasonality with a peak during the early summer months in many countries . The driving factors for this seasonality are not understood. Studies have shown a coincident seasonality of infection in chicken, livestock, and humans, and the possibility of a common environmental trigger has been suggested . In a recent study of the influence of climate on seasonality in England and Wales, incidence of campylobacteriosis was correlated with air temperature (with higher temperature indicating more cases at key points of the year) . This finding may relate to animal husbandry practices, especially animal housing .\n\n【1】Studies have attempted to identify environmental reservoirs of infection in water sources; _Campylobacter_ organisms have been successfully cultured from surface water , and campylobacteriosis has been linked with exposure to untreated water . We were interested in identifying the factors driving the early summer increase of cases in the United Kingdom and in investigating the role of environmental reservoirs. Preliminary data identified multilocus sequence type (ST)–45 complex as a strain with possible transmission from environmental sources , and we have analyzed this complex in more detail.\n\n【2】### The Study\n\n【3】The study population was defined as all human cases of laboratory-confirmed _Campylobacter_ infection with onset from April 2003 through March 2006, reported by residents in 4 local authorities in northwestern England, as previously described . All case-patients were asked detailed questions about their illnesses and possible exposures.\n\n【4】Water samples were collected at least each fortnight from October 2003 through December 2005 as 2-L grab samples from sampling points on 2 rivers associated with the study area (River Mersey and River Wyre). Water samples were transported to the Food and Environmental Microbiology Laboratory, Royal Preston Hospital. _Campylobacter_ species were isolated by the addition of 10 mL of the water sample to 90 mL of warmed _Campylobacter_ enrichment broth (product CM0983, Oxoid Ltd, Basingstoke, UK) and incubated at 37°C for 24 hours, followed by incubation at 42°C for 24 hours. The enrichment broths were subcultured onto _Campylobacter_ blood-free selective agar (charcoal cefoperazone deoxycholate agar product CM0739, Oxoid Ltd) at 37°C for 48 hours microaerobically, by using a microaerobic gas generating kit (product CN0025, Oxoid, Ltd). _Campylobacter_ colonies were identified by morphologic features and confirmed by microaerobic and aerobic growth on blood agar. The colonies were then placed in Amies transport and sent to the laboratory Health Protection Agency regional laboratory in Manchester, UK, for DNA extraction and characterization. _C. jejuni_ isolates were identified to species and typed by multilocus sequence typing as previously described .\n\n【5】Case–case methodology was used to compare exposures between sequence types of _Campylobacter_ . Statistical analysis was performed by using STATA version 9.2 (StataCorp, College Station, TX, USA). Two-way tabulations and Fisher exact test were used to estimate the direction and size of association of individual variables with sequence types. Logistic regression was performed to estimate the significance of these findings and to examine them in a multivariate model. Data were collected from 2 distinct locations, and, thus, area of residence was controlled for in analysis as a stratified variable. Likelihood ratio tests were used to assess the significance of including or excluding variables from multivariate analysis.\n\n【6】Among the typed isolates of _C_ . _jejuni_ (n = 1,104), ST-45 (n = 49) was the third most prevalent sequence type reported in the study (after ST-257 and ST-21) and highly restricted to cases reported between late April (week 17) and early August (week 32) . When week of diagnosis was categorized into 2 seasons (early summer \\[weeks 17–28\\] and all other weeks), logistic regression of season by sequence type confirmed that, when compared with all other typed cases of _C. jejuni_ , cases of ST-45 were more likely to be reported in early summer than during the rest of the year (odds ratio 2.79, confidence interval 1.56–4.99, p = 0.001; 1,008 observations), and this relationship was not seen with other prevalent sequence types.\n\n【7】Seventy-four (37%) of 198 river water samples were positive for _C. jejuni_ , and among these, 28 sequence types  were identified, 11 of which were also reported among human cases in the study population. The most prevalent sequence type identified was ST-45, and the seasonality of isolation appears to closely correlate with the seasonality of human disease onset caused by this type .\n\n【8】Univariate logistic regression analysis identified that, in addition to illness during early summer, the statistically strongest associations (p<0.1) with ST-45 were being <5 years of age and living in the more rural part of the study area. These 3 factors remained significantly and independently more associated with ST-45 infection than with other sequence types when combined in a multivariate model (tested by using likelihood ratio tests) . When this model was used, subsequent estimation of the association of food and exposure variables showed that consumption of chicken at least once was less associated and consumption of home-delivered milk, and going fishing were more associated with ST-45 than with other types. This finding was sustained in a multivariate model (also including the demographic variables previously mentioned) . However, information on exposure variables was not available for all 1,008 cases of _C. jejuni_ , which resulted in a loss of statistical power.\n\n【9】### Conclusions\n\n【10】We have shown that a single prevalent human strain of _C. jejuni_ , ST-45, is strongly associated with the early summer seasonal peak of campylobacteriosis incidence described previously in northwestern England . This strain is also frequently isolated from recreational surface waters adjacent to the populations studied, from which other prevalent strains have rarely been isolated. The concordance between period of ST-45 isolation in water and reported incidence in humans is striking and suggests a relationship between the presence of this strain in the environment and human infection. When compared with case-patients infected by other sequence types of _C. jejuni_ , persons infected with ST-45 are more likely to live in rural areas, to be <5 years of age, to have gone fishing before illness, or to have consumed home-delivered milk, and were much less likely to have eaten any chicken in the 2 weeks before illness.\n\n【11】The coincident seasonal presence of ST-45 in both surface water and in case-patients may arise simply through seasonal excretion of ST-45, resulting from well-characterized seasonal infection in humans  or livestock. The river systems sampled were both urban and rural in character, and contamination through human sewage discharge or animal feces from adjoining pastures is possible. However, other common human and animal _Campylobacter_ sequence types were largely absent from the water sampled.\n\n【12】Although ingestion of untreated surface waters has previously been shown to be a risk factor for campylobacteriosis in a UK-wide case-control study , these new data are not sufficiently robust to demonstrate a causal link for ST-45 infection. Although data were collected on proxy exposures to water, reported exposures were very low. Furthermore, systematic water sampling would be required to confirm the apparent seasonal positivity of ST-45 in these data. The potential role of pets, and in particular dogs, in bridging the gap between exposure to surface water (for example, while being exercised) and domestic exposure settings remains to be investigated. Although cases of infection caused by ST-45 were no more associated with owning dogs than were cases of other sequence types (data not shown), the exposures of those dogs were not recorded. In a recent study, cases of ST-45 clonal complex were more associated with contact with pet dogs and cats than were other clonal complexes identified ; pet-mediated transmission of ST-45 might be supported in this study by the observed association of ST-45 with young children.\n\n【13】Despite the evidence presented of a potential environmental transmission route for ST-45, it is also a type well recognized to colonize poultry , and the incidence among humans may be a result of consumption of seasonally contaminated poultry . However, the absence of strong early summer seasonality among other recognized chicken-adapted sequence types suggests that this is not the case (data not shown). Also, the evidence from our study is strong that consumption of chicken (a common human exposure) is less associated with human ST-45 infection than with other types.\n\n【14】An explanation for these observations may be that ST-45 represents a strain of _Campylobacter_ that is comparatively well adapted to survival outside an animal host, as has been hypothesized for some strains of _C. coli_ . Other studies have reported that ST-45 is more widely distributed in terms of host and ecologic niche, including water, than other common sequence types . Evidence also has indicated that ST-45 is more resilient to physical stress than other sequence types . This would certainly support the hypothesis that ST-45 was more available to infect humans through transmission routes other than food, either through direct exposure to water or the countryside through outdoor activities or by indirect exposure through pets. The hypothesis is further supported by the association of human ST-45 with more rural area of residence in these data. The availability of ST-45 to humans due to its hypothesized adaptive survival outside animal hosts would apply even more so to poultry, because of their increased environmental exposure, and 1 study has demonstrated that contamination of a new flock with ST-45 arose from an isolate in a puddle outside a chicken house . Thus, through an as-yet-uncharacterized adaptation, ST-45 may be a strain of _C. jejuni_ that is able to bridge the various recognized environmental, livestock, and human transmission settings for disease, making it a key target for intervention in reducing _Campylobacter_ prevalence. It may also be a key driver for the early summer rise in human incidence, both through nonfoodborne exposure and contamination of food animals.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9acdefb5-9f38-416e-8a47-2407978cc6c0", "title": "Hepatitis A Virus Genotype IB Outbreak among Internally Displaced Persons, Syria", "text": "【0】Hepatitis A Virus Genotype IB Outbreak among Internally Displaced Persons, Syria\nHepatitis A virus (HAV) is the leading cause of acute hepatitis infections worldwide, infecting ≈1.5 million persons annually . Symptoms, which are usually mild, include nausea, vomiting, abdominal pain, restlessness, body weakness, myalgia, loss of appetite, and fever. However, HAV may progress into fulminant liver failure, necessitating liver transplant. Generally, HAV is self-limiting . HAV (genus _Hepatovirus_ , family _Picornaviridae_ ) is a nonenveloped virus with a single-stranded, positive-sense RNA linear genome (7.5 kb). The viral proten (VP) 1/2A junction (168 nt) is used to classify HAV into 6 genotypes: I–III (subgenotypes A and B) of human origin and IV–VI of simian origin . Genotype IA is the most commonly reported worldwide, whereas genotype IB is predominant in the Middle East .\n\n【1】On September 9, 2018, the governorate of Aleppo, Syria, informed the World Health Organization office in Syria that internally displaced persons (IDPs; displaced since early 2018) and local host community members in Tal Refaat, Fafin, and surrounding areas in the northwestern and western parts of Aleppo were experiencing a suspected hepatitis outbreak. The affected area included 17 locations in Azaz and Jabal Sem’an districts in western Aleppo . Outbreak field investigation found sporadic cases of the disease among IDPs starting July 21, 2018; as of November 8, a total of 638 cases of suspected acute hepatitis infection had been reported. Most patients (98.59%) were <15 years of age and the rest 16–54 years of age. A total of 105 patients (16.5%) were admitted into the Fafin hospital; no fatalities were reported. No field investigations were performed in the first half of 2018 because of the crisis that led to weakness in the routine surveillance system.\n\n【2】A total of 48 unidentified serum and plasma samples were collected from 24 IDP children with suspected hepatitis and sent to the laboratory on October 29. The specimens originated from 3 locations in Syria: 13 from Fafin camp in Aleppo, 6 from eastern rural Daraa, and 5 samples from rural Quneitra. Even though the main outbreak was in the Aleppo governorate, Daraa and Quneitra were also experiencing a notable upsurge in reported cases of suspected acute hepatitis infection. For this reason, additional samples were collected from these governorates.\n\n【3】We analyzed the serum specimens by serology (total HAV antibodies and HAV IgM) using the enzyme-linked fluorescent assay VIDAS  and the plasma specimens by real-time reverse transcription PCR (RT-PCR) for the detection of HAV (using the HAVNET protocol) and hepatitis E virus (HEV) . Seven samples had insufficient volume to perform both total HAV antibody and HAV IgM tests; thus, only the IgM test was performed. Overall, 19 plasma specimens were positive for HAV and none for HEV by PCR . Eighteen serum specimens had detectable HAV IgM. All the specimens with sufficient volume (n = 17) were positive for total HAV antibodies. Of these, 5 were from past infections, as indicated by the negative HAV PCR and HAV IgM results. One patient had detectable HAV vRNA but negative HAV IgM, which indicates the early start of acute infection . This patient’s serum was positive for total HAV antibodies, indicating a previous exposure with a current breakthrough infection.\n\n【4】We successfully sequenced the VP1/2A region for 6 specimens  and used ClustalW in BioEdit 7.0 to align the sequences . Sequence-based genotypes were inferred by comparing the obtained sequences with genotype reference and contemporary strains obtained from GenBank. The phylogenetic analysis indicated that all Syria specimens belonged to genotype IB.\n\n【5】The Office for the Coordination of Humanitarian Affairs reported that, as of September 3, 2018, a total of 23,279 families (107,083 persons) were displaced from Afrin to Tel Refaat, Fafin, and surrounding villages. These IDPs were in addition to 4,766 families (38,843 persons) in the host community. Tents, destroyed or empty dwellings, schools, mosques, and warehouses were used as collective shelters but are relatively distant from active frontlines. IDPs had restricted freedom of movement and no access to proper sanitation facilities as a result of infrastructure damage; 70% of the population rely on water trucking services, and 30% live on less than 20 L of water per day. Moreover, 88% of the respondents reported accumulation of solid waste in their areas . A health assessment in the Afrin district found that 153 of 180 (85%) assessed communities have no access to health services. Vaccination campaigns have been largely suspended since November 2016. These poor living and health conditions render the IDPs highly prone to vaccine-preventable diseases, including hepatitis A, measles, polio, and cholera . Although water testing for HAV was not possible in the affected areas, deterioration in water quality was reported down the supply chain and may have contributed to this outbreak .\n\n【6】In summary, we report a large outbreak of hepatitis among IDPs in Syria. Laboratory testing confirmed current HAV IB infection among most screened patients. The high displacement rate, deteriorated sanitary and health conditions, and poor water quality may have all contributed to the increased HAV reports among this population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5db75fa5-c8f9-48ea-8053-c21c568b2750", "title": "The European Commission’s Task Force on Bioterrorism", "text": "【0】The European Commission’s Task Force on Bioterrorism\nThe deliberate release of anthrax in the United States of America in September and October 2001 completely changed the international perception of the risk of bioterrorism. Before that time, public health preparedness for bioterrorism was not a political priority in the European Union (EU) member states. Preparedness and response plans and actions have since been given a higher priority on EU member agendas. Bioterrorism also initiated a discussion on the need to improve preparedness through reinforcing existing public health structures that are in charge of monitoring and controlling diseases.\n\n【1】On October 19, 2001, at the European Council in Ghent, Belgium, the Council of Ministers of the EU and the European Commission (EC) were asked by the heads of state and government to prepare a program to improve the cooperation between member states for the evaluation of risks, alerts, and intervention, the storage of the necessary means, and the collaboration in the field of research.\n\n【2】In parallel with these developments at the European level, many well-focused national activities were also carried out. Countries reexamined their preparedness plans and adapted them to the new threats, and legislation needed to implement countermeasures was reviewed. The national authorities also discussed the necessity of stockpiling medicines and other medical supplies and most upgraded their stocks of vaccines and antibiotics, especially smallpox vaccine. To improve the ability to recognize a deliberate release, countries developed training programs for different kinds of first responders.\n\n【3】On the initiative of the European Commissioner for Health and Consumer Protection, the Health Security Committee (HSC) was set up with representatives of the ministries of health from each member state. Following a proposal by the commission, the HSC agreed on December 17, 2001, on a program for cooperation on preparedness and response to biologic and chemical agent attacks (or Health Security Programme). The program comprises 25 actions grouped under four objectives . The overall goal is to improve cooperation between the member states with the aid of the commission and to facilitate collaboration between the different national authorities involved in public health preparedness for bioterrorism. During this process, gaps in resources and methods are identified and projects initiated to fill these gaps. As is standard practice in many sectors of activity at EU level, existing examples of preparedness in the member states are shared and can be extended or adapted for use throughout the EU. Projects and activities under the health security program are funded through existing funds, especially from the budget foreseen for the EU’s public health programs. Other funds such as from the sixth framework program for research could also be used for activities of relevance to the health security program.\n\n【4】Since the beginning of May 2002, a 15-member strong task force has been set up by the Directorate-General of Health and Consumer Protection, comprising nine national experts seconded from different institutions in the EU countries and six commission officials. The task force includes representatives from the Robert Koch Institute and the Paul Erhlich Institute (Germany), the Scientific Institute of Public Health (Belgium), the Swedish Institute for Infectious Diseases Control (SMI, Sweden), the National Institute of Public Health and the Environment (RIVM, Netherlands), the Hellenic Institute for Infectious Disease Control (Greece), the Italian National Institute for Health (Italy), as well as the Pitié-Salpêtrière Hospital in Paris (France). The task force’s main objective is to implement the health security program. To this end, members have initiated further activities with key players in the member states, some of which are described below.\n\n【5】One of the initial priority activities was the setting up of a mechanism for rapid information exchange, consultation, and coordination on all aspects dealing with bioterrorism. All designated authorities of the EU member states have now been connected through this network by means of email, fax, and telephone. This tool makes it possible for the task force and the nine member states to disseminate information rapidly about suspected or confirmed incidents and for national authorities to evaluate measures planned with their implications for EU and for other member states. The task force is available 24 hours a day, 7 days a week to facilitate the process. By May 2003, the system had been used in two major EU-wide exercises and for nine alerts about suspicious events communicated by the member states.\n\n【6】The early detection of a deliberate release or bioterrorist attack is an essential part of a program of preparedness. One essential tool to make this possible will be the regular surveillance activities already in place at national and European level. Certain modifications to these systems, such as the addition of certain diseases considered as having a high potential for deliberate release, would have to be done to make the systems better adapted to the new challenges of bioterrorism. To achieve this, the Community Network on Communicable Diseases (the already existing mechanism for EU collaboration in the areas of infectious disease surveillance and control) extended its list of diseases to be reported, to include additional threats such as tularemia, anthrax, Q-fever, and smallpox.\n\n【7】When the task force was established, several lists of agents that could be involved in a biologic attack and considered of major bioterrorist threat had been established and published. Against this background, an approach more suitable to the demands for focused action in Europe was developed though a matrix model designed to provide an evaluation of the public health impact of any given agent that could be used in bioterrorism. The matrix model may also be used to identify areas where the public health response needed strengthening for any given agent. This means that for a given type of action a different set of agents would emerge as deserving the highest attention. The matrix model is currently being validated and has produced lists targeting specific areas of public health preparedness, which are currently being reviewed by members of the HSC. Using this model, identifying the public health impact of new threats should be possible, such as severe acute respiratory syndrome (SARS) (once it has been better characterized), multidrug-resistant tuberculosis, or any other new or drastically changed disease-causing agent.\n\n【8】The clinical management, including recognition, of many of the diseases caused by the bioterrorist agents is likely to be difficult, since they are, to a large extent, unknown to the average clinician in Europe today. To provide clinicians with guidance and a source of generally agreed-upon information, 10 different articles on 48 major agents have been compiled ( _Bacillus anthracis_ (anthrax), _Yersinia pestis_ (plague), _Francisella tularensis_ (tularemia), smallpox and monkeypox viruses, _Clostridium botulinum_ (botulism), hemorrhagic fever viruses, encephalitis viruses, _Coxiella burnetii_ (Q fever), _Brucella_ species (brucellosis), _Pseudomonas mallei_ (glanders), and _Pseudomonas pseudomallei_ ). They have been sent for comments to experts in all member states, have been adopted by HSC, and will be made available to the European medical community. Their publication in a peer-reviewed medical journal is currently being considered.\n\n【9】The laboratory capacity required to diagnose most of the potentially toxic and infectious agents that might be used in case of deliberate release is believed to exist within the EU as a whole; however, no member state has the complete spectrum of the diagnostic capabilities needed. To improve the collective preparedness and to make more efficient use of the available resources, the laboratory capacities already in place are currently under review. As a long-term goal these capabilities would be made available to all member states of the EU. Additionally, more concrete collaboration between laboratories was initiated to share techniques and participate in common quality assurance schemes. Assisted by the task force, the P4 (U.S. BSL-4) laboratories in the EU are currently forming a network that will enhance their research activities through common projects. They have been asked to develop procedures to make P4 diagnostics available to all member states and to handle, under high safety level requirements, a substantial workload of analysis of environmental samples that might arise following a series of bioterrorist incidents or threats.\n\n【10】The European Agency for the Evaluation of Medicinal Products (EMEA) has already developed guidelines on the use of authorized pharmaceuticals in case of deliberate release of biologic and chemical agents . This includes guidelines for treatment and prophylaxis of the agents on the so-called CDC A-list of the most important threats as bioterrorist agents .\n\n【11】Stockpiling of medicines and vaccines (in particular, smallpox vaccines) has been extensively discussed between the different member states and the EC. Although the member states initially looked into the possibility of an EU-level vaccine stockpile, the member states opted to establish national stockpiles sufficient for their projected needs.\n\n【12】Smallpox has become the model for much of the work performed in the area of preparedness for bioterrorist incidents. Vaccination is certainly an essential countermeasure against possible smallpox threats, yet finding the right balance under different vaccine administration scenarios between personal and societal cost-benefit ratios has proved extremely difficult. To find optimal solutions, various attempts have been made to construct appropriate mathematical models. Currently a group of European modelers has been called by the task force to work together in a network to develop models for EU purposes. In addition, this group has been asked to give expert opinions on new models published and to establish an infrastructure for data-exchange that could be used for real-time modeling, should the need arise.\n\n【13】Many member states have developed (or are in the process of developing) specific preparedness plans for smallpox. The task force has put in place a compilation and review process, which includes the tabulation and comparison of these plans to improve the understanding of how and with what measures individual countries respond to specific triggers and events and how components of their plans, which are important for the whole EU, will interact.\n\n【14】National authorities had already developed substantial capacities to deal with chemical incidents. Much of the EU work on chemical incidents has been done in close collaboration with other organizations like World Health Organization (WHO) and Organization for the Prohibition of Chemical Weapons. A special working-group has been formed under the auspices of HSC, and a project will be initiated to take work on certain key aspects forward.\n\n【15】In most areas investigated, existing European expertise was identified that could ensure a high level of preparedness against biologic or chemical agents. Considerable effort is needed, however, to determine the exact kind of expertise needed for specific cases and to identify specialists and develop the modalities required for effective sharing of this expertise between countries. As a first step, a plan for the development of a common directory of experts has been developed, and countries have been asked to make their national expertise available on behalf of the EU. Further modalities will be developed when this first step is implemented.\n\n【16】Better preparedness should also include improved coordination between the commission and international organizations working in this area such as the WHO and the parties to the Global Health Security Action Initiative (Canada, France, Germany, Italy, Japan, Mexico, the United States, the United Kingdom, and the European Commission with WHO participating as a resource). Multiple initiatives regarding coordination of actions in this area are being pursued.\n\n【17】The EU program on health security is expected to be concluded in November 2003. However, this deadline likely does not provide enough time to complete the activities anticipated in many of the areas included in the 25-point action program. An extension of the task force’s duration is therefore being sought. In the areas where activities have taken place, a centrally launched initiative at EU level has been found to be beneficial both for the member states and for the EU as a whole. The work has opened new communication channels between member states, improving both the understanding of the preparedness in other countries as well as giving the possibility to learn from good practice and tried solutions. The EC has also gained experience in working with a group of experts who initiate activities with more operational effectiveness than has been the case in the past. This experience will be very useful in the continued discussion on the development of a European Centre for Disease Prevention and Control since the task force can be seen as a precursor to this centre, albeit in a narrow area of activity. During the remaining 6 months and the period of extension, if it is approved, the work in the area of bioterrorism will continue to strengthen the capabilities at EU level to respond to biologic and chemical threats. The capabilities to manage diseases in emergency situations in general will be improved, and knowledge gained will be most useful for other areas of emergency preparedness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c0e4c1f2-ee05-46e5-a98a-b2b087b22c5a", "title": "Spatial Clustering of Suicide and Associated Community Characteristics, Idaho, 2010–2014", "text": "【0】Spatial Clustering of Suicide and Associated Community Characteristics, Idaho, 2010–2014\nAbstract\n\n【1】**Introduction**\n\n【2】In 2015, Idaho had the fifth highest suicide rate in the United States. Little is known about the characteristics of areas in Idaho with high suicide rates. To aid suicide prevention efforts in the state, we sought to identify and characterize spatial clusters of suicide.\n\n【3】**Methods**\n\n【4】We obtained population data from the 2010 US Census and the 2010–2014 American Community Survey, analyzed data on suicides from death certificates, and used a discrete Poisson model in SaTScan to identify spatial clusters of suicide. We used logistic regression to examine associations between suicide clustering and population characteristics.\n\n【5】**Results**\n\n【6】We found 2 clusters of suicide during 2010–2014 that accounted for 70 (4.7%) of 1,501 suicides in Idaho. Areas within clusters were positively associated with the following population characteristics: median age ≤31.1 years versus >31.1 years (multivariable-adjusted odds ratio \\[aOR\\] = 2.4; 95% confidence interval \\[CI\\], 1.04–5.6), >53% female vs ≤53% female (aOR = 2.7; 95% CI, 1.3–5.8; _P_ \\= .01), >1% American Indian/Alaska Native vs ≤1% American Indian/Alaska Native (aOR = 2.9; 95% CI, 1.4–6.3), and >30% never married vs ≤30% never married (aOR = 3.4; 95% CI, 1.5–8.0; _P_ \\= .004).\n\n【7】**Conclusion**\n\n【8】Idaho suicide prevention programs should consider using results to target prevention efforts to communities with disproportionately high suicide rates.\n\n【9】Introduction\n\n【10】In the United States, suicide is the tenth leading cause of death; more than 44,000 suicides were reported in 2015 . From 2000 to 2015, the US age-adjusted suicide rate increased by 28%, from 10.4 per 100,000 population to 13.3 per 100,000 population . Suicide results in substantial medical and work-loss costs; lifetime costs were estimated to exceed $56 billion in 2015 ; this conservative estimate did not account for underreporting of suicides and other societal costs (eg, pain and suffering, justice system). Beyond the economic burden, suicide negatively affects families and community members, who may have long-lasting mental health problems and other life-changing difficulties .\n\n【11】Suicide rates vary in the United States by geographic location. During 2011–2015, the age-adjusted suicide rate was higher in the West than in the Northeast (14.0 per 100,000 population \\[West census region\\] vs 9.8 per 100,000 population \\[Northeast census region\\]) . Although suicide rates increased across all levels of urbanization in the United States during 1999–2015, rates were higher in less urban areas than in more urban areas . Because geographic differences are not fully explained by demographic patterns , they could be attributed to other factors, such as lack of access or poor access to quality mental health care, low socioeconomic status, and weak social cohesion in areas with high suicide rates . Increased access to lethal means could be another explanatory factor in areas with higher suicide rates .\n\n【12】A comprehensive public health approach to suicide prevention, in contrast to an approach that focuses on mental health treatment, can address multiple risk factors across the lifespan . Although a public health suicide prevention approach is warranted in communities nationwide , it is essential to focus on communities with disproportionately high suicide rates to eliminate geographic disparities and reduce suicide altogether . Furthermore, examination of suicide data at a fine-scale geographic level is needed to identify these communities for efficient planning and targeting effective prevention efforts, especially when resources are limited.\n\n【13】Several types of suicide clusters have been reported, including mass clusters, space–time clusters, and spatial clusters . Spatial cluster analysis has been used to identify communities with disproportionately high suicide rates, because spatial cluster analysis overcomes the “small numbers problem” (in which rates for areas with small populations have wider variability and less reliability than rates for areas with large populations) inherent in spatial analysis and allows for statistical assessment of rates across geographic units . A study in 2012 found 2 high-risk spatial clusters of suicide during 1999–2008 that comprised 15 of 120 counties in Kentucky . Another study, in 2017, found 24 high-risk spatial clusters of suicide during 2001–2010 that comprised 491 of 3,154 census tracts in Florida . Studies of suicide in Scotland, Australia, São Paulo, and Québec used the same methodology . To our knowledge, no study of suicide using spatial cluster analysis has been conducted in rural or western parts of the United States.\n\n【14】In Idaho, a northwestern rural state with a population of 1.7 million, suicide is a major public health problem . Idaho consistently ranks among the top 10 states with the highest suicide rates, with an age-adjusted suicide rate of 22.2 per 100,000 population, compared with 13.3 per 100,000 population nationally in 2015 . Eighteen of 44 counties in Idaho had an age-adjusted suicide rate of 22.0 per 100,000 population or more during 2010–2014 . However, these rates are likely unstable because of the small numbers problem . Because all of Idaho is federally designated as having a shortage of mental health providers , targeting Idaho communities with disproportionately high suicide rates at a more detailed level than the county level (because some counties are very large in area) is crucial. Therefore, we sought to identify and characterize areas with spatial clusters of suicide at the neighborhood level in Idaho. We examined whether there are geographic areas in Idaho that have statistically significant higher rates of suicide than expected, compared with other geographic areas in the state, and we explored their characteristics. For a complete representation of suicide in Idaho, we also described the epidemiology of residents who died by suicide.\n\n【15】Methods\n\n【16】We used a retrospective ecological study design to investigate suicides among Idaho residents during 2010–2014. We did not include suicides occurring in Idaho among out-of-state persons, because an objective of our study was to examine the characteristics of communities in which Idaho residents who died by suicide lived at the time of death. We used the census block group as a proxy for neighborhood. A census block group is a statistical division of a census tract that covers a contiguous area and generally has a population size of 600 to 3,000 people, whereas a census tract is a relatively permanent statistical subdivision of a county and generally has a population size of 1,200 to 8,000 people . Our study was deemed nonresearch public health practice by the Idaho Division of Public Health’s Research Determination Committee.\n\n【17】We obtained individual-level data on suicides from death certificates stored by the Idaho Bureau of Vital Records and Health Statistics, and for the spatial cluster analysis, we aggregated data on suicides to census block group. Although some suicide reporting systems and research exclude suicides among persons younger than 10 years, we did not exclude any age group, in accordance with the standard practice in Idaho . We identified suicides by the established _International Classification of Diseases, Tenth Revision_ , codes as follows: X60.0–X84.9, Y87.0, and U03.9 . Death certificates included information on sex, age, ethnicity, race, education, marital status, military status (based on the question “Ever in US Armed Forces?”), occupation, and mechanism of injury. We geocoded residential addresses from death certificates to obtain 15-digit census block group identifiers. We completed geocoding by using the Automated Geospatial Geocoding Interface Environment System . In total, 98.5% of residential addresses were matched to a census block group identifier; we excluded 23 suicides without a matched census block group identifier. We used census block group identifiers to merge suicide data with other data sources.\n\n【18】We obtained data on population estimates from the 2010 US Census and data on community characteristics from the 2010–2014 American Community Survey’s 5-year estimates . We measured the following community characteristics, suggested by previous studies , in proportions as appropriate: female; median age; American Indian or Alaska Native; Hispanic or Latino; persons never married; persons in single-parent families; persons with less than a high school education (ie, did not receive a regular high school diploma, GED, or alternative credential); unemployed persons, median household income; persons in poverty; persons in renter-occupied housing units; persons with disabilities; and persons with no health insurance. In addition to showing demographic patterns, these characteristics capture dimensions of social cohesion and economic deprivation that could be associated with suicide .\n\n【19】### Data analysis\n\n【20】Using information from death certificates, we first calculated descriptive statistics of residents who died by suicide and stratified these data by sex. We used the Pearson χ 2  test for categorical variables (or Fisher exact test for <5 expected cell counts) and _t_ tests for continuous variables. Next, we conducted spatial cluster analysis by using SaTScan version 9.4 (Martin Kulldorff and Information Management Services Inc), free software that uses scan statistics to identify clusters . We used the discrete Poisson model  to scan for nonoverlapping geographical areas (census block groups) with significantly high rates of suicide. In SaTScan, we used population size (default of 50% of the total population at risk) to specify the maximum spatial cluster size; circular spatial window shape, adjusted for sex and age distributions; and the default of 999 Monte Carlo replications. We selected the spatial clusters with _P_ < .10 for the subsequent analyses.\n\n【21】We used logistic regression models to examine associations between community characteristics and suicide clustering. Suicide clustering was constructed as a binary outcome variable indicating whether a census block group belonged to a spatial cluster of suicide (with _P_ < .10). To simplify interpretation and use of findings for a wider audience, we dichotomized each variable for community characteristics into high and low levels. Except for age and income, we constructed the variables to compare the highest quartile with the lowest 3 quartiles for each variable. For age and income, we constructed the variables to compare the lowest quartile with the highest 3 quartiles for each variable. We fit a series of univariable models to examine association of each community characteristic with suicide clustering. Community characteristics that were significant at _P_ < .05 in the univariable models were included in a multivariable model to identify the most important community characteristics related to suicide clustering. We performed model diagnostics, including goodness of fit and multicollinearity assessments, which did not indicate problems. We used SAS version 9.3 (SAS Institute Inc) for all statistical analyses other than spatial cluster analysis, and we used ArcGIS version 10 (Environmental Systems Research Institute, Inc) for cartographic displays of spatial clusters.\n\n【22】Results\n\n【23】During 2010–2014, 1,501 Idaho residents died by suicide. Most residents who died by suicide were male (78.5%), aged 35 to 64 years (53.7%), non-Hispanic (95.8%) and white (97.0%) . Overall, male and female residents who died by suicide did not significantly differ by the demographic characteristics examined. However, they significantly differed by marital status, military status, occupational status, and suicide method. The proportion of divorced persons was higher among females (32.6%) than males (25.0%), and the proportion of persons never married was higher among males (32.9%) than females (27.0%). The proportion of those who served in the military was higher among males (26.8%) than among females (3.1%). The proportion of those who were homemakers and those who had never worked or were disabled was higher among females (19.7% and 5.0%, respectively) than males (0.2% and 2.7%, respectively). For mechanism of injury, males (67.6%) were more likely than females (34.4%) to die by a firearm, and females (36.2%) were more likely than males (11.1%) to die by poisoning.\n\n【24】### Spatial clusters of census block groups with high suicide rates\n\n【25】SaTScan identified a “most likely” cluster and 9 secondary clusters . The 2 identified spatial clusters (with _P_ < .10) of census block groups with disproportionately high suicide rates during 2010–2014 accounted for 70 (4.7%) of 1,501 deaths by suicide . The “most likely” spatial cluster, comprising 25 census block groups and a population of 30,405, was found in southeastern Idaho. During 2010–2014, 54 suicides occurred in this spatial cluster, whereas 28 suicides were expected, indicating that the suicide rate was 90% higher inside the cluster than outside (relative risk = 1.9, _P_ \\= .04). A secondary spatial cluster with _P_ < .10 was identified in northeastern Idaho. This secondary spatial cluster, comprising 6 census block groups and a population of 4,391, had 16 suicides, whereas 4 suicides were expected. The suicide rate was more than 3 times higher inside this cluster than outside (relative risk = 3.6, _P_ \\= .06).  \n\n【26】Spatial clusters of Idaho resident suicides by census block group, 2010–2014. A dot is a centroid of a census block group (CBG); 1 dot might represent 1 or more suicides that occurred in that CBG during the study period. \n\n【27】### Characteristics of census block groups in spatial clusters\n\n【28】Compared with census block groups outside spatial clusters of suicide, census block groups in spatial clusters were more likely to have a higher proportion of females, American Indians or Alaska Natives, never married persons, and persons in poverty, and a lower proportion of persons with less than a high school education . Census block groups within spatial clusters had populations with a younger median age and a lower median household income. We observed no significant differences between census block groups within spatial clusters and outside spatial clusters in proportion Hispanic or Latino ethnicity, single-parent families, unemployment, renter-occupied housing, disability, or health insurance coverage. In the multivariable model that included significant characteristics from the univariable models, the following community characteristics remained significant: median age ≤31.1 years (multivariable-adjusted odds ratio \\[aOR\\] = 2.4; 95% confidence interval \\[CI\\], 1.04–5.6; _P_ \\= .04), >53% female (aOR = 2.7; 95% CI, 1.3–5.8; _P_ \\= .01), >1% American Indian or Alaska Native (aOR = 2.9; 95% CI, 1.4–6.3; _P_ \\= .006), and >30% never married (aOR = 3.4; 95% CI, 1.5–8.0).\n\n【29】Discussion\n\n【30】This ecological study identified geographic areas with disproportionately high suicide rates at the census block group level in 2 parts of Idaho. The communities in areas with suicide clustering had a unique demographic and socioeconomic profile. To our knowledge, this is the first study to investigate spatial clustering of suicide in the western region of the United States.\n\n【31】The 2 spatial clusters of census block groups identified were in 2 of the 18 counties where high rates of suicide had been reported . Identifying these clusters provides a more detailed view of geographic areas in these counties: 25 census block groups in a county with 60 census block groups, and 6 census block groups in a county with 18 census block groups . Our findings on spatial clusters of suicide at the census block group level cannot be fully compared with findings from previous studies, because those studies used different geographic units (counties and census tracts, not census block groups) . The proportion of geographic units that were part of the identified clusters was smaller in Idaho (3%) than they were in Kentucky (13%)  and Florida (16%) . Despite different levels of geography with varying population compositions, this finding might be attributed to differences in suicide risk levels in each state; a state where suicide risk has less geographic variation (eg, Idaho) is less likely to have many clusters. Our study spanned 5 years, which is half of the study period of other US studies ; a longer study including more suicides might have identified more or fewer areas or same or different areas within spatial clusters.\n\n【32】Our findings are generally consistent with findings of other studies reporting that areas of lower socioeconomic status are associated with higher rates of suicide . We found a positive association between suicide clustering and both low household income and high proportion of persons in poverty; however, we found a negative association between suicide clustering and low educational attainment. This finding is consistent with at least 1 previous study that found the proportion of the population without a diploma is less likely to be included in a suicide cluster . Our finding that suicide clustering was associated with a higher proportion of never-married persons is consistent with research on the influence of social support and family structure on suicide . Community characteristics related to housing, unemployment, disability, and health insurance coverage that were not significantly associated with suicide clustering in our study might be investigated in future studies to confirm our findings. Overall, the unique demographic and socioeconomic profile of areas with suicide clustering in Idaho should be viewed as a potential way to depict an environmental context that is conducive to suicide, rather than a direct cause of suicide clustering.\n\n【33】The literature identified 2 possible explanations for suicide clustering. First, concentrations of persons at high risk for suicide might live in areas that could be identified as a cluster (compositional effects) . Second, place of residence might influence suicide risk by being less supportive (eg, because of social or economic isolation) of persons at high risk (contextual effect) . Our objective was not to investigate causation, and we did not incorporate individual-level data to assess individual risk of suicide after controlling for contextual effect.\n\n【34】Our study demonstrates the feasibility of a state health department investigation of spatial clusters of suicide using multiple data sources. Strengths of this study include the use of population-based suicide data; use of the census block group as a granular, detailed unit of geographic analysis; and consideration of a broad range of community characteristics that covered the same period as the suicides. Spatial cluster analysis using SaTScan has many advantages, including adjusting for population inhomogeneity, adjusting for multiple comparisons, adjusting for covariates, and limiting preselection bias by not specifying cluster size a priori .\n\n【35】This study has several limitations. First, incorrectly not classifying suicide as a cause of death on death certification could have resulted in underreporting of suicide. Second, missing information on residential addresses resulted in incomplete geocoding; however, less than 2% of suicides were missing information on residential addresses. Third, we did not have information on how long the decedents lived in their homes; thus, we could not determine how duration of exposure to communities could affect results. Fourth, our cluster analysis was driven by the settings we selected in SaTScan; however, we followed the standard settings and those used in previous studies. Finally, our findings might not reflect current high-risk areas because data were from 2010–2014. However, retrospective analysis of mortality data is a fundamental tool for community health assessment, and we used the most recent available data. Although the contextual factors conducive to suicide in the identified clusters have probably not changed greatly since our study period, continuous evaluation and data triangulation to determine whether high-risk areas remain at high risk over time could increase confidence in public health programs that target prevention efforts to those areas. Although a study from Australia found that historical suicide clusters, detected during a 5-year period, predicted only 36% of suicide clusters detected during a subsequent 5-year period , our findings are better suited to inform current planning and response needs of suicide prevention programs rather than to predict future suicides.\n\n【36】Our findings could help public health practitioners and policy makers prioritize resources and target efforts for suicide prevention. The Centers for Disease Control and Prevention developed a technical package of prevention strategies to help communities use the best available evidence for suicide prevention . These strategies include strengthening economic supports; strengthening access and delivery of suicide care; creating protective environments; promoting connectedness; teaching coping and problem-solving skills; and identifying and supporting people at risk . A multicomponent public health suicide prevention approach should address the needs of communities at the highest risk of suicide, such as communities we found in our study. In Idaho, a public health approach that strengthens economic supports and strengthens access and delivery of suicide care in the identified areas might be most effective in preventing suicide.\n\n【37】Tables\n------\n\n【38】Table 1. Characteristics of Residents Who Died by Suicide, Stratified by Sex, Idaho, 2010–2014 a  \n\n| Characteristic | Total b (n = 1,501) | Male (n = 1,178) | Female (n = 323) | _P_ Value c |\n| --- | --- | --- | --- | --- |\n| **Age, mean (SD), y** | 45.6 (18.7) | 45.9 (19.4) | 44.4 (16.3) | .15 |\n| **Age group, n (%), y** | **Age group, n (%), y** | **Age group, n (%), y** | **Age group, n (%), y** | **Age group, n (%), y** |\n| <15 | 20 (1.3) | 18 (1.5) | 2 (0.6) | .008 |\n| 15–24 | 231 (15.4) | 182 (15.5) | 49 (15.2) | .008 |\n| 25–34 | 212 (14.1) | 171 (14.5) | 41 (12.7) | .008 |\n| 35–44 | 255 (17.0) | 192 (16.3) | 63 (19.5) | .008 |\n| 45–54 | 303 (20.2) | 226 (19.2) | 77 (23.8) | .008 |\n| 55–64 | 248 (16.5) | 186 (15.8) | 62 (19.2) | .008 |\n| 65–74 | 117 (7.8) | 99 (8.4) | 18 (5.6) | .008 |\n| 75–84 | 69 (4.6) | 62 (5.3) | 7 (2.2) | .008 |\n| ≥85 | 46 (3.1) | 42 (3.6) | 4 (1.2) | .008 |\n| **Ethnicity, n (%)** | **Ethnicity, n (%)** | **Ethnicity, n (%)** | **Ethnicity, n (%)** | **Ethnicity, n (%)** |\n| Hispanic | 63 (4.2) | 47 (4.0) | 16 (5.0) | .45 |\n| Non-Hispanic | 1,437 (95.8) | 1,130 (96.0) | 307 (95.1) | .45 |\n| **Race, n (%)** | **Race, n (%)** | **Race, n (%)** | **Race, n (%)** | **Race, n (%)** |\n| White | 1,456 (97.0) | 1,146 (97.3) | 310 (96.0) | .30 |\n| Black | 4 (0.3) | 4 (0.3) | 0 | .30 |\n| American Indian | 24 (1.6) | 15 (1.3) | 9 (2.8) | .30 |\n| Asian Pacific Islander | 8 (0.5) | 6 (0.5) | 2 (0.6) | .30 |\n| Other or mixed race | 9 (0.6) | 7 (0.6) | 2 (0.6) | .30 |\n| **Education, n (%)** | **Education, n (%)** | **Education, n (%)** | **Education, n (%)** | **Education, n (%)** |\n| <High school d | 264 (17.7) | 210 (18.0) | 54 (16.9) | .18 |\n| High school | 607 (40.8) | 488 (41.8) | 119 (37.2) | .18 |\n| \\>High school | 617 (41.5) | 470 (40.2) | 147 (45.9) | .18 |\n| **Marital status, n (%)** | **Marital status, n (%)** | **Marital status, n (%)** | **Marital status, n (%)** | **Marital status, n (%)** |\n| Married, including married but separated | 537 (35.9) | 423 (36.1) | 114 (35.4) | .03 |\n| Widowed | 87 (5.8) | 71 (6.1) | 16 (5.0) | .03 |\n| Divorced | 398 (26.6) | 293 (25.0) | 105 (32.6) | .03 |\n| Never married | 472 (31.6) | 385 (32.9) | 87 (27.0) | .03 |\n| **Military status, n (%)** | **Military status, n (%)** | **Military status, n (%)** | **Military status, n (%)** | **Military status, n (%)** |\n| Yes | 323 (21.6) | 313 (26.8) | 10 (3.1) | <.001 |\n| No | 1,170 (78.4) | 857 (73.3) | 313 (96.9) | <.001 |\n| **Occupational status, n (%)** | **Occupational status, n (%)** | **Occupational status, n (%)** | **Occupational status, n (%)** | **Occupational status, n (%)** |\n| Student | 127 (8.6) | 97 (8.3) | 30 (9.4) | <.001 |\n| Homemaker, housewife | 65 (4.4) | 2 (0.2) | 63 (19.7) | <.001 |\n| Never worked, disabled | 47 (3.2) | 31 (2.7) | 16 (5.0) | <.001 |\n| Other occupational groups | 1,246 (83.9) | 1,035 (88.8) | 211 (65.9) | <.001 |\n| **Mechanism of injury,** **e** **n (%)** | **Mechanism of injury,** **e** **n (%)** | **Mechanism of injury,** **e** **n (%)** | **Mechanism of injury,** **e** **n (%)** | **Mechanism of injury,** **e** **n (%)** |\n| Poisoning (X60–X69) | 248 (16.5) | 131 (11.1) | 117 (36.2) | <.001 |\n| Hanging, strangulation, suffocation, drowning and submersion (X70–X71) | 294 (19.6) | 216 (18.3) | 78 (24.2) | <.001 |\n| Firearm (X72–X74) | 907 (60.4) | 796 (67.6) | 111 (34.4) | <.001 |\n| Other methods (X75–X84, Y87) | 52 (3.5) | 35 (3.0) | 17 (5.3) | <.001 |\n\n【40】a  Individual-level data on suicides obtained from death certificates stored by the Idaho Bureau of Vital Records and Health Statistics.  \nb  The total number of participants for each variable varies because of missing values.  \nc  Based on the Pearson χ 2  test or Fisher exact test for categorical variables and _t_ test for continuous variables.  \nd  Did not receive a regular high school diploma, GED, or alternative credential.  \ne  Based on _International Classification of Diseases, Tenth Revision_ . No death using the U03.9 ICD-10 code was reported.\n\n【41】Table 2. Spatial Clusters of Suicide by Residential Location, Idaho, 2010–2014 a  \n\n| Cluster No. | Cluster | No. of Census Block Groups | Population b | Observed No. of Suicide Deaths | Expected No. of Suicide Deaths | Annual Deaths per 100,000 | Relative Risk | Log-Likelihood Ratio | _P_ |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 1 | Most likely | 25 | 30,405 | 54 | 28.4 | 35.9 | 1.9 | 9.4 | .04 |\n| 2 | Secondary | 6 | 4,391 | 16 | 4.5 | 67.6 | 3.6 | 8.9 | .06 |\n| 3 | Secondary | 11 | 14,084 | 28 | 13.3 | 39.8 | 2.1 | 6.3 | .55 |\n| 4 | Secondary | 22 | 25,347 | 44 | 25.3 | 32.8 | 1.8 | 5.8 | .69 |\n| 5 | Secondary | 1 | 1,600–1,700 | 7 | 1.7 | 78.7 | 4.1 | 4.7 | .95 |\n| 6 | Secondary | 3 | 2,947 | 10 | 3.2 | 58.1 | 3.1 | 4.5 | .97 |\n| 7 | Secondary | 3 | 2,040 | 8 | 2.3 | 67.1 | 3.6 | 4.4 | .98 |\n| 8 | Secondary | 5 | 4,896 | 12 | 4.4 | 51.0 | 2.7 | 4.4 | .98 |\n| 9 | Secondary | 30 | 60,471 | 72 | 51.0 | 26.6 | 1.4 | 4.0 | \\>.99 |\n| 10 | Secondary | 1 | 500–600 | 4 | 0.6 | 117.9 | 6.3 | 4.0 | \\>.99 |\n\n【43】a  Individual-level data on suicides obtained from death certificates stored by the Idaho Bureau of Vital Records and Health Statistics. Data on population estimates obtained from the 2010 US Census and data on community characteristics from the 2010–2014 American Community Survey’s 5-year estimates .  \nb  Adjusted for sex and age.\n\n【44】Table 3. Characteristics of Census Block Groups Within and Outside Spatial Clusters of Suicide, Idaho, 2010–2014 a  \n\n| Characteristic b | Census Block Groups Within Spatial Clusters Of Suicide, No. (%) (n = 31) | Census Block Groups Outside Spatial Clusters Of Suicide, No. (%) (n = 932) | Odds Ratio (95% Confidence Interval) c |\n| --- | --- | --- | --- |\n| **Proportion female** | **Proportion female** | **Proportion female** | **Proportion female** |\n| \\>0.53 | 14 (45.2) | 226 (24.3) | 2.6 (1.3–5.3) |\n| ≤0.53 | 17 (54.8) | 706 (75.8) | 1 \\[Reference\\] |\n| **Median age** | **Median age** | **Median age** | **Median age** |\n| ≤31.1 y | 16 (51.6) | 223 (23.9) | 3.4 (1.7–7.0) |\n| \\>31.1 y | 15 (48.4) | 709 (76.1) | 1 \\[Reference\\] |\n| **Proportion American Indian or Alaska Native** | **Proportion American Indian or Alaska Native** | **Proportion American Indian or Alaska Native** | **Proportion American Indian or Alaska Native** |\n| \\>0.01 | 15 (48.4) | 225 (24.1) | 3.0 (1.4–6.1) |\n| ≤0.01 | 16 (51.6) | 707 (75.9) | 1 \\[Reference\\] |\n| **Proportion Hispanic or Latino** | **Proportion Hispanic or Latino** | **Proportion Hispanic or Latino** | **Proportion Hispanic or Latino** |\n| \\>0.16 | 5 (16.1) | 235 (25.2) | 0.6 (0.2–1.5) |\n| ≤0.16 | 26 (83.9) | 697 (74.8) | 1 \\[Reference\\] |\n| **Proportion of never-married persons** | **Proportion of never-married persons** | **Proportion of never-married persons** | **Proportion of never-married persons** |\n| \\>0.30 | 19 (61.3) | 221 (23.7) | 5.1 (2.4–10.7) |\n| ≤0.30 | 12 (38.7) | 711 (76.3) | 1 \\[Reference\\] |\n| **Proportion of persons in single-parent families** | **Proportion of persons in single-parent families** | **Proportion of persons in single-parent families** | **Proportion of persons in single-parent families** |\n| \\>0.24 | 12 (38.7) | 228 (24.5) | 2.0 (0.9–4.1) |\n| ≤0.24 | 19 (61.3) | 704 (75.5) | 1 \\[Reference\\] |\n| **Proportion of persons with <high school education** **d** | **Proportion of persons with <high school education** **d** | **Proportion of persons with <high school education** **d** | **Proportion of persons with <high school education** **d** |\n| \\>0.16 | 2 (6.5) | 238 (25.5) | 0.2 (0.1–0.9) |\n| ≤0.16 | 29 (93.6) | 694 (74.5) | 1 \\[Reference\\] |\n| **Proportion of unemployed persons** | **Proportion of unemployed persons** | **Proportion of unemployed persons** | **Proportion of unemployed persons** |\n| \\>0.07 | 11 (35.5) | 229 (24.6) | 1.7 (0.8–3.6) |\n| ≤0.07 | 20 (64.5) | 703 (75.4) | 1 \\[Reference\\] |\n| **Median household income, $** | **Median household income, $** | **Median household income, $** | **Median household income, $** |\n| ≤35,345 | 14 (45.2) | 226 (24.3) | 2.6 (1.3–5.3) |\n| \\>35,345 | 17 (54.8) | 706 (75.8) | 1 \\[Reference\\] |\n| **Proportion of persons in poverty** | **Proportion of persons in poverty** | **Proportion of persons in poverty** | **Proportion of persons in poverty** |\n| \\>0.22 | 13 (41.9) | 227 (24.4) | 2.2 (1.1–4.7) |\n| ≤0.22 | 18 (58.1) | 705 (75.6) | 1 \\[Reference\\] |\n| **Proportion of persons in renter-occupied housing unit** | **Proportion of persons in renter-occupied housing unit** | **Proportion of persons in renter-occupied housing unit** | **Proportion of persons in renter-occupied housing unit** |\n| \\>0.41 | 12 (38.7) | 228 (24.5) | 2.0 (0.9–4.1) |\n| ≤0.41 | 19 (61.3) | 704 (75.5) | 1 \\[Reference\\] |\n| **Proportion of persons with disability** | **Proportion of persons with disability** | **Proportion of persons with disability** | **Proportion of persons with disability** |\n| \\>0.22 | 8 (25.8) | 232 (24.9) | 1.1 (0.5–2.4) |\n| ≤0.22 | 23 (74.2) | 700 (75.1) | 1 \\[Reference\\] |\n| **Proportion of persons with no health insurance coverage** | **Proportion of persons with no health insurance coverage** | **Proportion of persons with no health insurance coverage** | **Proportion of persons with no health insurance coverage** |\n| \\>0.23 | 7 (22.6) | 233 (25.0) | 0.9 (0.4–2.1) |\n| ≤0.23 | 24 (77.4) | 699 (75.0) | 1 \\[Reference\\] |\n\n【46】a  Individual-level data on suicides obtained from death certificates stored by the Idaho Bureau of Vital Records and Health Statistics. Data on population estimates obtained from the 2010 US Census and data on community characteristics from the 2010–2014 American Community Survey’s 5-year estimates .  \nb  Each variable for community characteristics was dichotomized into high and low levels. Except for age and income, we constructed the variables to compare the highest quartile with the lowest 3 quartiles for each variable. For age and income, we constructed the variables to compare the lowest quartile with the highest 3 quartiles for each variable.  \nc  Based on Wald method from univariable logistic regression models.  \nd  Did not receive a regular high school diploma, GED, or alternative credential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8d17a30e-e7c8-486b-ba84-ddb08eb0cce1", "title": "African Swine Fever Epidemic, Poland, 2014–2015", "text": "【0】African Swine Fever Epidemic, Poland, 2014–2015\nAfrican swine fever (ASF) is an infectious and notifiable disease of domestic and wild animals of the family Suidae . First described in Kenya in 1921, ASF was territorially restricted to Africa only until 1957, when it spread from Angola to Lisbon. From then on, ASF has been repeatedly detected in many countries of Europe, Central America, and South America. In some countries (e.g. France, Belgium, the Netherlands), ASF outbreaks were rapidly contained, but in others (e.g. Portugal and Spain) ASF virus (ASFV) persisted for >30 years. Another long-time infected region in Europe is Sardinia (Italy), where ASFV has been circulating since 1978 and where the disease has been maintained as endemic . In 2007, the most recent epidemic started in Georgia and thereafter moved to Armenia, Azerbaijan, and the Russian Federation . In 2012 and 2013, ASF occurred in Ukraine and Belarus, respectively, and in 2014, it crossed into the European Union. According to the World Organisation for Animal Health, >550 ASF cases among wild boar and outbreaks among domestic pigs were detected through 2015 in Estonia, Latvia, Lithuania, and Poland .\n\n【1】In Poland, the first cases of ASF were detected in wild boar in February 2014 in the northeastern part of the country, very near (<1 km) the border with Belarus . As of August 31, 2015, a total of 76 cases in wild boar and 3 outbreaks among domestic pigs had been found in 3 counties (basic administrative regions of Poland).\n\n【2】Extensive surveillance revealed a unique pattern of disease spread that did not fit the commonly perceived concept of ASF epidemiology. Our study objective was to describe the spatiotemporal spread of ASF in Poland during the first 18 months after detection of the first cases.\n\n【3】### Materials and Methods\n\n【4】##### Surveillance Design and Diagnostic Tests\n\n【5】After the first cases of ASF in Poland were confirmed, the affected area was differentiated into 3 levels of risk: area I (regions free from ASF but located near areas where ASF had been occurring in wild boar), area II (ASF detected in wild boar only), and area III (established after detection of ASF in pigs) . Despite differences with regard to animal movement restrictions, the surveillance strategy applied to areas I–III was the same: all wild boar found dead and those killed in road accidents (passive surveillance) and hunted wild boar (active surveillance) from all areas were submitted for testing. Samples collected from dead wild boar were whole blood, serum, marrow bones, kidneys, liver, spleen, lymph nodes, and lungs; samples from hunted wild boar were whole blood and serum. Homogenates (10% wt/vol) of individual tissues were prepared in phosphate-buffered saline. Clarified material was stored at −80°C or directly used for virus DNA extraction. Virus DNA was extracted directly from 200-μL aliquots of serum or tissue sample homogenates by using the commercial QIAamp DNA Mini Kit (QIAGEN, Hilden, Germany) according to the manufacturer’s recommended procedures. We used a PCR with the ASF diagnosis primers and a commercial probe , which generates an amplicon of 74 bp within viral protein 72, to confirm the presence of ASFV DNA. Specific primers and probes were added to a LightCycler 480 Probes Master Kit (Roche Applied Science), and reactions were performed in a Stratagene Mx3005P real-time PCR thermocycler (Agilent Technologies, Santa Clara, CA, USA) according to the protocol described by Fernández -Pinero et al.\n\n【6】Altogether, from February 2014 through August 2015, samples from 609 dead/road accident wild boar and 12,253 hunted wild boar from areas I–III , as well as from ≈35,000 domestic pigs, were tested by real-time PCR; detailed results and difficulties encountered during the diagnostic process are described elsewhere . According to terminology adopted in Poland, outbreaks were defined as the detection of DNA of ASFV in pigs (irrespective of the number of pigs in a holding), and cases were defined as the presence of viral DNA in \\> 1 wild boar found at the same time and in the same place. Thus, the number of infected animals outnumbered the number of cases or outbreaks. However, for the purpose of prevalence calculations, we took into account individual animals. To calculate the annual prevalence of ASF in wild boar during the first year of the epidemic and to analyze potential seasonal variations, we established prevalence rates (with 95% CIs) separately for wild boar tested within the scope of active and passive surveillance in quarterly intervals: spring (March–May 2014), summer (June–August 2014), autumn (September–November 2014), and winter (December 2014–February 2015). In addition, we calculated prevalence in monthly intervals to encompass the period from the beginning of the epidemic in February 2014 through August 2015. We mapped the locations of ASF outbreaks and cases by using sampling location coordinates in ArcGIS for Desktop software (Esri Inc. Redlands, CA, USA).\n\n【7】##### DNA Sequencing and Phylogenetic Analysis\n\n【8】We used the DNA of ASFV representing 64 cases and 3 outbreaks for phylogenetic analysis. So far we have failed to produce proper-length readable sequences for samples from case nos. 20, 24, 26–28, 32, 51, 56, 57, and 68. The primers specific to the MGF505-2R gene ASFV sequence were designed on the basis of the complete genome sequence of the BA71V strain  by using online Primer 3 Plus software . The primers were also 100% complementary to the Georgia 2007/1 sequence strain. The expected product length was estimated to be 1,173 bp. The primer sequences used for amplification and sequencing of the MGF505–2R fragment were LVR13F: 5′-GCAGAGGTATGATGTCCTTA-3′ and LVR13F: 5′-TTCCTGTTGAACAAGTATCT-3′. The PCR products were separated in a 1.5% agarose gel (Invitrogen, Grand Island, NY, USA) and then purified according to the procedure for the QIAquick Gel Extraction Kit (QIAGEN). The amplicons were sequenced on a GS FLX/Titanium sequencer (Roche Applied Science) by Centrum Badań DNA Service (Poznań, Poland). Each product was sequenced in forward and backward directions and then assembled into a single contig by using Geneious R7 software (Biomatters Ltd. Auckland, New Zealand). The ClustalW alignment calculation parameters in MEGA6  were as follows: gap opening penalty 15, gap extension penalty 6.66, transition weight 0.5, and delay divergent cutoff 30%. We plotted the phylogram by using the neighbor-joining algorithm in MEGA6 software and calculated the nucleotide similarity matrix providing the information about the sequence identity by using Geneious R7 software. The obtained nucleotide sequences of ASFV isolates were trimmed, assembled into contigs, and aligned by using Geneious R7 software. We also retrieved 2 sequences of ASFV representing genotype II (Georgia 2007/1 and Odintsovo/2014 Russia) from GenBank to use for comparison. The tree was rooted against ASFV strains Warmbaths South Africa and Malawi Lil 20/1, representing genotypes IV and VIII, respectively. We submitted the nucleotide sequences of ASFV successfully sequenced in Poland to GenBank under accession nos. KT366447–KT366459 and KT900042–KT900107.\n\n【9】##### Statistical Analyses\n\n【10】To evaluate correlations between the number of ASFV-positive wild boar and wild boar density in the forestry units in which ASF detections were notified during the first year after the beginning of the epidemic, we performed a Pearson and Spearman correlation analysis (significance level 0.05). The follow-up analysis was performed 3 months after detection of ASF in new areas, which led to enlargement of the infected zone in August 2015. To assess statistical differences between seasonal prevalence of ASF, we used the Fisher exact test with a Bonferroni correction for each single comparison (significance level 0.05).\n\n【11】### Results\n\n【12】The average annual prevalence of ASFV (based on positive PCRs) among hunted wild boar was 0.12% (95% CI 0.1%–0.2%) . Prevalence did not differ significantly by season. With regard to detection of ASFV in dead wild boar, the annual prevalence was 14.2% (95% CI 11.1%–17.9%) and ranged from 8.2% in spring to 24.3% in summer. The only significant difference (after taking the Bonferroni correction into account) was between summer and autumn (p<0.001). The monthly prevalence ranged from 0 to 0.7% among hunted wild boar and from 0 to 40.5% among dead wild boar . The overall 18-month prevalence in areas under animal movement restrictions was 18.6% (95% CI 15.7%–21.8%) according to passive surveillance and 0.2% (95% CI 0.1%–0.2%) according to active surveillance.\n\n【13】We found a correlation between the number of ASF notifications and the number of wild boar in the affected forestry units (Spearman rank correlation coefficient R = 0.90, p<0.05) in February 2015 (during the first year after detection of the first case). As of August 2015 (after detection of ASF in new areas in June 2015 and the enlargement of the infected zone), the correlation lost statistical significance .\n\n【14】The nucleotide and amino acid sequence identity of the MGF505-2R gene between ASFV isolates from Poland ranged from 99.47% to 100%. The largest cluster consisted of 42 sequences (41 from wild boar and 1 from pigs \\[outbreak 3\\]) exhibiting 100% homology between each other and indistinguishable from 2 references included for comparison: Georgia 2007/1 and Odintsovo 02/14 Russia . The second largest group containing 100% homologous sequences comprised 12 viruses (11 from wild boar and 1 from pigs \\[outbreak 2\\]) with 99.9% similarity to viruses of the previous group. The DNA fragment of the virus recovered from pigs identified as from the first outbreak differed slightly from those mentioned above, and the only identical sequence was from the virus from case no. 4. Sequences representing case nos. 15, 17, 41, 45, 55, and 72 formed a clearly separate and diverse cluster (within-group genetic diversity 99.5%–99.9%) .\n\n【15】### Discussion\n\n【16】After the emergence of ASF in Poland, the preliminary forecasts had predicted that the virus would deplete the population of wild boar in the region or would spread quickly to new areas because it is inherently so highly contagious. These predicted events, however, did not occur. Nor did the concept that ASFV cannot be sustained among wild boar without spillover from domestic pigs  apply to the situation in Poland. So far, the number of cases in wild boar in Poland has greatly outnumbered outbreaks among domestic pigs. The virus has been found almost exclusively in wild boar, which seem to be the sole mediator for virus dissemination. The total area of the infected region is only ≈1,500 km 2  . The slow spatial spread of ASF may be associated with the social behavior of wild boar, which has been studied quite extensively in Białowieża Primeval Forest, straddling the Poland–Belarus border . Wild boar show strong site fidelity, and most (≈70%) stay within 1–2 km of the center of their natal home ranges; only a relatively small percentage (5%–10%) of the population disperses from their natal range but not farther than 20–30 km. Spatial overlap of family groups is limited , which hampers transmission of the virus between groups by either direct contact between susceptible and sick animals or indirect contact with infected carcasses. In addition, the high virulence of the virus, which leads to the high case-fatality rate, prevents infected wild boar from long-distance movements. Therefore, long-distance dispersal of the virus by wild boar as carriers is assumed to be unlikely and mostly requires human involvement. However, specific socio-agricultural conditions in the affected region (i.e. low pig density, very few commercial farms, and small-scale national and international trade) create favorable barriers hitherto preventing the spread of the virus over long distances. It seems that the overall effect on the population was not significant and that, despite a high lethality rate of genotype II for wild boar  , the mortality rate  seems to not be very high. Therefore, the virus does not seem to be highly contagious, which can also be explained on one side by the inherent epidemiologic properties of ASF (no airborne transmission and required contact with blood or excretions of infected animals) and on the other side from the specific behavior of wild boar described above. The results obtained in our study provide grounds for redefining the role of wild boar, which after 18-months of observation can be considered as a reservoir host for ASFV.\n\n【17】The complete genetic identity between a large cluster of Poland ASFV isolates with Georgia 2007/1 isolates clearly shows that the examined region, although relatively variable, can remain highly conservative for a long time (8 years). On the other hand, the genetic divergence of up to 0.5% in viruses from Poland highlighted by the presence of separate clusters on the phylogenetic tree clearly indicates that Poland has experienced a few incursions of genetically distinct ASFVs of genotype II. This finding is also supported by epidemiologic observations: 29 of 76 cases were located no farther than 5 km from the border with Belarus. With respect to outbreaks among pigs, the phylogenetic analysis clearly indicates no direct link between the 3 outbreaks. Epidemiologic investigations showed that wild boar were the most likely source of infection for domestic pigs (mainly poor biosecurity of pig holdings, enabling contact with wild animals). Overall, results of phylogenetic studies demonstrate the dynamic nature of the ASF epidemic in eastern Europe and raise serious concerns for control of ASF. We emphasize that without close and transparent collaboration between ASF-affected countries, eradication goals will be difficult to achieve.\n\n【18】The statistical relationship between wild boar density and the number of ASF cases was found after the 12 months after the beginning of the epidemic. The correlation was not statistically significant a few months after the virus spread to new forestry units with high wild boar density, apparently because of substantial changes in the population size in areas II and III as a result of introduced control measures (according to the most current census, the population in the aforementioned areas decreased by ≈25%). Moreover, the analysis of combined data for Poland and the Baltic States, conducted by a panel of European Food Safety Authority experts, found no correlation between wild boar density and ASF case notifications . This issue requires clarification, and the analysis will be continuously updated. However, during the first year, all cases in wild boar were detected in areas with a wild boar density of \\> 1 animal/km 2  ; currently, in areas where substantial efforts have been undertaken to reduce wild boar populations, the number of ASF notifications has been reduced considerably. This finding raises potential implications for ASF control strategies (i.e. maintaining the wild boar population in the affected region at the level of ≈0.5–0.7 animals/km 2  ) and can be taken into account as a control option for reducing the number of cases among wild boar. Moreover, maintaining the population density in the surrounding regions at a low level may create a low-density barrier, preventing the virus from becoming established among wild boar in new areas, and infections, if they occur, can be expected to die out. As indicated in the most recent European Food Safety Authority report , the low density of wild boar can be achieved by female-targeted boar hunting and a feeding ban. However, this approach should be applied as a long-term control measure because intensive hunting is logistically demanding. It would also be desirable to significantly reduce the domestic pig population from backyard pig holdings, which do not fulfill biosecurity requirements. This process would thus create a chance to minimize the major risk for long-distance dispersal of the virus, which is attributed to human activity (e.g. transfer of contaminated pork, pig waste, or fomites to other, sometimes remote, regions). A new biosecurity regulation is being put in place in areas II and III, which, among other things, stipulates that holdings that do not fulfill the strict requirements will be closed.\n\n【19】In summary, during 18 months of ASF in Poland, we observed repeated introductions of ASFV into the country, slow spread of the disease in areas of dense wild boar populations, and a primary role of wild boar in virus maintenance. Enhancement of biosecurity practices at pig holdings is crucial for minimizing the risk for virus spillover virus from wild to domestic populations followed by long-distance spread of ASFV by human-related activities. Continuous and intensive surveillance enabling fast detection of ASF is needed, especially in previously disease-free areas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "82bae9e4-799c-4109-a09f-9b1c4954cd8d", "title": "Immune Restoration Disease in HIV Patient", "text": "【0】Immune Restoration Disease in HIV Patient\nDuring the first few months of highly active antiretroviral treatment, immune restoration may be complicated by clinical events in which either previously subclinical infections are found or preexisting partially treated opportunistic infections deteriorate. This condition, termed immune restoration disease (IRD), is thought to be caused by the improvement in the host's immune response to pathogens. We report what we believe is the first recorded case of IRD with confirmed _Mycobacterium microti_ infection. _M. microti_ is an unusual infection associated with small rodents. Novel genetic techniques have confirmed it as a human pathogen, but its true incidence remains unclear.\n\n【1】### Case Report\n\n【2】A 33-year-old man was admitted to the hospital in 2002 with a 3-month history of intermittent hemoptysis. He had previously visited hematology outpatient clinics for many years for hemophilia A (3%–5% factor VIII) and HIV-1 had been diagnosed in 1996 (a stored sample from 1989 retrospectively tested HIV-1–antibody positive). CD4 cell counts had been recorded as 434 × 10 6  /L in 1996 and 260 × 10 6  /L in 1998. After this time, he failed to attend any further clinic appointments.\n\n【3】In addition to worsening hemoptysis, he had minor weight loss but no night sweats. He appeared moderately unhealthy, with oral thrush and mild splenomegaly. A chest radiograph showed left upper lobe shadowing , sputum smears contained numerous acid-fast bacilli, and sputum was sent for culture, speciation, and sensitivity testing. Blood tests showed a CD4 cell count of 6 × 10 6  /L and HIV viral load of 272,000 (log 5.4) copies/mL. His hemoglobin was 12.1 g/dL, leukocyte count 4.9 × 10 9  /L, and platelets 367 × 10 9  /L. Renal and liver function test results were normal, apart from a γ-glutamyltransferase level of 200 U/L (normal <35 U/L). An ultrasound scan of his abdomen showed a diffuse increase in liver echogenicity and an enlarged (13.6-cm) spleen. He was naturally immune to hepatitis B, and tests for hepatitis C antibody and viremia were negative.\n\n【4】He was given broad-spectrum antimycobacterial therapy (rifampin, isoniazid, pyrazinamide, ethambutol, and azithromycin) as treatment for both _M. tuberculosis_ complex (MTB complex) and opportunistic mycobacteria. He improved throughout a 2-week hospital stay and highly active antiretroviral therapy (HAART) was begun. It consisted of zidovudine and lamivudine and an increased dose of efavirenz (800 mg daily) because of concomitant rifampicin therapy, together with cotrimoxazole for prophylaxis against _Pneumocystis jirovecii_ .\n\n【5】At a 3-week follow-up appointment, the patient had continued to improve. No further hemoptysis had occurred, and he had gained 2 kg body weight. After that, by telephone, he complained of occasional fevers and malaise but declined to be seen. Nine weeks after his initial visit, he came to the ward and with symptoms of breathlessness, nausea, and diarrhea. He had fevers spiking to >38°C, and newly palpable lymph nodes were evident in the right axilla and over the left parotid gland. A chest radiograph showed increased shadowing  and a computed tomographic (CT) scan of the chest showed a 6 × 3-cm solid mass in the upper left chest with visible air bronchograms and minor volume loss, reported as \"almost certainly solid neoplasm\" . A bronchoscopy, before which the patient received factor VIII, showed an area of irregular nodular tissue in the posterior segment of the left upper lobe. His CD4 count was 26 × 10 6  /L.\n\n【6】While we awaited histologic examination results, the patient's medication was withdrawn, and he improved within days. The lymph nodes reduced in size, and the spiking temperature resolved. Eight weeks after admission, the results of IS _6110_ polymerase chain reaction (PCR) testing on the slow-growing, positive mycobacterial culture confirmed the presence of MTB complex. The bronchial biopsy specimen showed necrotizing granulomatous bronchitis; acid-fast bacilli were not visible, and this specimen did not grow any mycobacteria.\n\n【7】The _Mycobacterium_ species was not growing well in the laboratory, but _M. microti_ was identified in the culture with spoligotyping methods . On extension of culture, the isolate was confirmed as _M. microti_ phenotypically and found to be sensitive in vitro to rifampin, isoniazid, ethambutol, and clarithromycin but resistant to pyrazinamide.\n\n【8】The antimycobacterial drugs were reintroduced without problems 4 weeks later, as was HAART after an additional 8 weeks. Thirty months after restarting antimycobacterial therapy, the patient remained well with a total weight gain of 19 kg, complete radiologic resolution of the pulmonary mass, undetectable HIV viral load (<50 copies/mL), and a progressive rise in his CD4 count to 249 × 10 6  /L.\n\n【9】### Conclusions\n\n【10】_M. microti_ is a slow-growing member of the MTB complex. It has most commonly been described in association with small rodents . Its role as a pathogen in humans has been proposed by a handful of case reports involving both immunocompetent and immunodeficient persons . Because of the slow growth of the bacterium, confirmatory diagnosis is difficult, and cultures may be wrongly discarded as negative after a routine 8- to 12-week culture period. In such cases several commercial tests can identify the organism to the level of the MTB complex. The members of the tuberculosis complex are not reliably distinguished on biochemical grounds. Restriction fragment length polymorphism, DNA fingerprinting, is a time-consuming, slow, and expensive method of distinguishing members. Spoligotyping (spacer oligotyping) is a PCR-based test that can be quickly performed, even on the small amounts of DNA in poorly growing cultures. Strains differ in the size of spacer regions that intersperse direct repeat regions. These polymorphisms create differences in the PCR products produced, a process that allows strain identification . Other PCR-based tests are being developed that may have advantages over spoligotyping for identifying some members of the complex . Culture remains the standard and allows drug sensitivity testing. We suggest that such cultures be extended for up to 6 months if acid-fast bacilli are noted in specimens from a patient with consistent pulmonary pathologic findings.\n\n【11】Severe immunosuppression associated with HIV-1 can allow coinfections to remain subclinical. After HAART is introduced, IRD can lead to the development of opportunistic infections with atypical and severe clinical signs and symptoms. In this case, necrotizing granulomatous inflammation occurred without isolation of mycobacteria, which is atypical in an immunodeficient HIV patient. These clinical manifestations have been described in many diseases, including MTB and other opportunistic mycobacterial infections , herpesvirus infections, and hepatitis B and C . Similar granulomatous endobrochial lesions have been described during immune restoration with _M. avium_ complex in HIV-infected patients . IRD has been described as occurring in 30% to 40% of those with very low CD4 counts when they begin HAART, developing within the first few months of treatment as the CD4 count rises. IRD is a diagnosis of exclusion: other processes such as drug fever, resistance, treatment failure, or other infections must be excluded. IRD may be a life-threatening condition, and clinical interventions, such as stopping medications, steroids, or both, may be indicated. An associated increase in markers of immune activation occurs, which seems to vary depending on the pathogen involved .\n\n【12】In MTB, the major risk factor for IRD is beginning HAART within 2 months of antituberculosis treatment. This fact has led many experts to recommend delaying HAART for 2 months, a strategy that may also reduce the incidence of adverse events. However, a recent retrospective review suggested that delaying HAART would, particularly in those with CD4 cell counts<100 cells × 10 6  /L, lead to significant increase in risk for death and new opportunistic infections . IRD should always be considered along with treatment failure if a patient's clinical condition deteriorates after HAART is introduced.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "783b0949-fcc2-4473-8c61-ec67a6f5da89", "title": "Parasitic Disease Surveillance, Mississippi, USA", "text": "【0】Parasitic Disease Surveillance, Mississippi, USA\nHuman populations in the state of Mississippi and the rest of the southeastern United States have historically been at risk for hookworm and other parasitic diseases . With improved sanitation and economic development, soil-transmitted helminths (STH), including the hookworms _Ascaris lumbricoides_ and _Trichuris trichiura_ , were presumed to have been eliminated. However, a recent report of continued hookworm and strongyloidiasis transmission in a community without access to proper sanitation in Alabama, USA, has challenged this assumption .\n\n【1】### The Study\n\n【2】To investigate the current prevalence of these infections, we conducted a pilot study to identify STH and other potentially endemic parasitic infections in convenience samples of specimens collected from patients in Mississippi. We deidentified fresh fecal samples submitted for diagnostic testing from patients at the University of Mississippi Medical Center (UMMC; Jackson, Mississippi, USA) during March 30, 2017–February 22, 2018, and serum samples submitted during October 28, 2017–March 29, 2018. This study was approved by the UMMC Institutional Review Board; the Centers for Disease Control and Prevention (CDC) was determined to be nonengaged and therefore did not undertake a separate institutional review board review.\n\n【3】We froze two 250-mg aliquots of feces for later DNA extraction. Where sample volume allowed, we performed microscopic examination using the saturated salt (specific gravity 1.2) passive flotation method as previously described . We extracted DNA by using the SurePrep Soil DNA isolation kit  after conducting initial bead beating for 3 minutes using zirconium beads. We stored DNA extracts at −80°C and sent them to the CDC for real-time PCR analysis. At CDC, each sample was initially tested for inhibition and poor DNA extraction by a real-time PCR assay targeting the human cytochrome B gene . Samples positive by this inhibition and extraction control were then tested by multiparallel real-time PCR for STH . A cycle threshold (C t  ) < 35 was considered to represent a positive result. Any positive PCR results were confirmed by duplicate testing.\n\n【4】We froze the deidentified serum samples at –80°C and sent them to CDC, where they were tested for antibodies to _Toxocara_ spp. _S. stercoralis_ , _Cryptosporidium_ spp. and _G. duodenalis_ using MAGPIX multiplex serology (ThermoFisher)  to detect evidence of prior exposure. For statistical calculations, we used Excel  and R version 3.3.1 .\n\n【5】A total of 650 fecal samples were obtained from UMMC patients. The median age of patients providing fecal samples for this analysis was 56 years (range 2–95 years). We obtained samples sufficient to perform saturated salt centrifugal flotation on 507 samples (80%). We found no samples to contain helminth eggs or larvae. Sufficient sample for DNA extraction was available for 631 (99.5%) samples. Of these fecal DNA extracts, a negative inhibition and extraction control excluded 37 samples. We tested 224 DNA extracts for _Ancylostoma_ spp. _N. americanus_ , _S. stercoralis_ , _A. lumbricoides_ , and _T. trichiura_ by real-time PCR. \n\n【6】Because prior work in Alabama  detected only _N. americanus_ and _S. stercoralis_ infections, we screened an additional 370 DNA extracts for these helminths only. Of these 370 samples, 2 DNA extracts yielded positive amplicons for _S. stercoralis_ (C t  29.57 and 30.48). The first of these samples (C t  29.57) yielded no amplification curve on repeat testing and was interpreted as representing an initial false-positive result. The second sample (C t  30.48) was positive upon confirmatory retesting (C t  28.52 and 30.49). \n\n【7】A total of 1,960 postdiagnostic serum samples from Mississippi residents were available for multiplex serologic testing. The median age of patients providing serum samples for this analysis was 38 years (range 0–94 years). Of the 1,960 samples, 646 (33.0%) reacted with the Cp17 antigen of _C. parvum_ (range 87–48,448 mean fluorescence intensity \\[MFI\\]), and 1,076 (54.9%) reacted with Cp23 (range 377–56,727 MFI). Of those samples, 538 (27.4%) reacted with both _C. parvum_ antigens , suggesting prior _Cryptosporidium_ species infection. A total of 111 samples (5.7%) reacted with the _G. duodenalis_ VSP3 antigen (range 84–48,547 MFI) . A total of 38 (1.9%) samples contained antibodies to the Cp17, Cp23 and VSP3 antigens , demonstrating prior exposure to both _Cryptosporidium_ and _G. duodenalis_ infections. A total of 172 (8.8%) samples contained antibodies to _Toxocara_ spp. Tc-CTL-1 antigen (range 23.2–33,814 MFI) . When _Toxocara_ \\-seropositive participants < 6 years of age were excluded, 167/1,814 (9.2%) of UMMC patient samples were seropositive. A total of 9 (0.4%) samples contained antibodies reacting with the recombinant _S. stercoralis_ NIE-1 antigen (range 16.2–11248 MFI) in MAGPIX serologic testing, of which 4 (0.2%) were positive in the confirmatory _S. stercoralis_ CrAg-ELISA (range 9.94–57.7 IU/mL) .\n\n【8】### Conclusions\n\n【9】The results of this limited pilot study suggest a low prevalence of STH infections in Mississippi but that rare infections with _S. stercoralis_ might be found in Mississippi residents. The single case confirmed by real-time PCR tests likely represents active infection. Because >80% of patients with strongyloidiasis serorevert within 18 months after successful treatment , the 4 confirmed antibody-positive serum samples also likely represent active cases of strongyloidiasis. No linked immigration or travel history data on patients providing these samples were available, so whether these infections were acquired within the United States is unknown. Combined with the recent finding of strongyloidiasis in a rural community from Alabama , these data should encourage more focused sampling of areas with poor sanitation and hygiene, high levels of poverty, and poor access to healthcare for potential residual foci of endemic STH and strongyloidiasis transmission in Mississippi and the wider southeastern United States.\n\n【10】The total _Toxocara_ spp. seroprevalence in all participants in this study was 8.8%, which is higher than the average prevalence reported by the most recent National Health and Nutrition Examination Survey study . Although these results are not directly comparable because of different sampling methods, the potentially high _Toxocara_ spp. seroprevalence in Mississippi warrants further investigation.\n\n【11】The seroprevalence results of this study suggest that prior exposure to _Cryptosporidium_ spp. is common in Mississippi. Only 5.7% of the postdiagnostic serum samples were found to have serologic evidence of prior exposure to _G. duodenalis_ infection. A small number of samples (1.9%) contained antibodies reacting with the 3 antigens Cp17, Cp23, and VSP3, indicating prior exposure to _Cryptosporidium_ spp. and _G. duodenalis_ infection. Further investigation of the epidemiology of waterborne protozoan infection in Mississippi, including determination of the actual prevalence and distribution using systematic sampling and determination of the species and subtypes infecting persons, is warranted.\n\n【12】The absence of any positive findings by microscopic examination or PCR for the STH suggests that such infections are uncommon in the general Mississippi population. We found high seroprevalence of antibodies to _Toxocara_ spp. in Mississippi. Although this finding could indicate increased exposure to this infectious agent compared with the national average, our data do not enable determination of the sources of increased infection or overall annual incidence of disease. Further studies on the epidemiology and prevalence of parasitic diseases in the state of Mississippi are indicated.\n\n【13】In conclusion, this convenience sampling study did not find evidence of high STH prevalence in Mississippi. However, we did identify several likely current cases of strongyloidiasis and relatively high rates of _Toxocara_ exposure. We recommend further investigation with larger sample sizes to more clearly define the true extent of STH infection in this region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7576da5d-a977-458a-8c4e-fb6c8a69f136", "title": "Development of Medical Countermeasures to Middle East Respiratory Syndrome Coronavirus", "text": "【0】Development of Medical Countermeasures to Middle East Respiratory Syndrome Coronavirus\nFrom September 2012 through April 27, 2016, a total of 1,728 laboratory-confirmed Middle East respiratory syndrome coronavirus (MERS-CoV) infections, leading to 624 deaths (36% case-fatality proportion), had been reported to the World Health Organization (WHO) . Most infections (75%) have been identified in Saudi Arabia . Zoonotic transmission from exposure to MERS-CoV–infected Arabian camels, known as dromedaries, or their raw milk and limited, nonsustained human-to-human transmission have been reported, including large outbreaks in healthcare facilities . The recovery of infectious MERS-CoV in virus cultures of specimens from bed sheets, bedrails, intravenous fluid hangers, and radiograph equipment indicates the potential for fomite transmission of the virus in hospitals providing care for MERS-CoV patients . However, sustained human-to-human transmission has not been documented, and some case-patients have no identified source of exposure to MERS-CoV. As of April 2016, a total of 26 countries had reported locally acquired or exported cases from the Arabian Peninsula, including 2 cases in the United States identified during May 2014 in healthcare personnel who became ill after working in Saudi Arabia . A traveler who visited Saudi Arabia, Qatar, the United Arab Emirates, and Bahrain and then returned to South Korea infected with MERS-CoV in mid-2015 triggered 184 MERS-CoV cases, resulting in 38 deaths in multiple health facilities and 1 additional case in a person who traveled to China .\n\n【1】Human infections with MERS-CoV are expected to continue to occur on the Arabian Peninsula because of the prevalence of MERS-CoV in dromedaries and the cultural importance of these camels (i.e. for food, milk, and racing purposes) in the region. During the 2003 outbreak of severe acute respiratory syndrome (SARS) in China, civet cats, the suspected reservoir of SARS coronavirus (SARS-CoV), were culled aggressively; no outbreaks were identified after 2004. In contrast, culling of camels is culturally impractical in the Middle East, and MERS-CoV zoonotic infections of humans have continued since 2012.\n\n【2】The potential for emergence of MERS-CoV mutations that could facilitate sustained community transmission and global dissemination cannot be predicted. No vaccines against or specific treatments for human infection with SARS-CoV, MERS-CoV, or other coronaviruses have been approved. Since 2013, efforts have focused on furthering development of animal models, vaccines, and therapies against MERS-CoV . In this report, we update the current state of development for MERS-CoV medical countermeasures, including regulatory challenges in the United States, and draw attention to areas in immediate need of increased infrastructure support for development of these countermeasures.\n\n【3】### Strategies for Potential Use of MERS-CoV Medical Countermeasures\n\n【4】MERS-CoV infection could theoretically be prevented by vaccination, pre- or postexposure antiviral chemoprophylaxis, or passive immunoprophylaxis of persons in affected countries at increased risk for MERS-CoV exposure (e.g. healthcare personnel, persons who work with camels) or persons at higher risk for more severe disease, including persons >65 years of age and those with chronic medical conditions. Therapeutic drugs with specific activity against MERS-CoV (e.g. antiviral drugs, immunotherapeutic treatments) or that target the host immune response could be used for treatment of human illness caused by MERS-CoV infection or for pre- or postexposure prophylaxis. Before human clinical trials of potential MERS-CoV medical countermeasures are started, proof-of-concept data must be obtained from in vivo studies of experimentally infected animals. Such data may indicate a product’s potential efficacy and provide a mechanism for selection of available medical countermeasure candidates. In addition, MERS-CoV vaccines could be developed for animals and used for vaccination of dromedaries on the Arabian Peninsula and in source countries for camel imports to the Horn of Africa to reduce MERS-CoV transmission among camels and possibly from camels to humans.\n\n【5】### Animal Models and Virus Strains\n\n【6】Preclinical development of MERS-CoV medical countermeasures has been hindered by several factors, including limited data on the natural history of MERS-CoV infection in humans; the lack of a small animal model that is naturally susceptible to MERS-CoV; and the inability to consistently replicate severe human disease in MERS-CoV–infected nonhuman primates (NHPs). Another factor is limited access to clinical samples and recent virus isolates; for example, a MERS-CoV strain isolated from a patient in 2012, rather than a more recently isolated strain, is currently used by most investigators worldwide.\n\n【7】Small animal and NHP models are useful for testing potential medical countermeasures for efficacy . Studies in mice, both dipeptidyl peptidase-4 (DDP-4 or cluster differentiation 26) transduced and transgenic, and in rabbits, hamsters, and ferrets have been reviewed elsewhere . These small animal models have been used for screening potential MERS-CoV medical countermeasures .\n\n【8】The major NHP models under development include rhesus macaques and common marmosets . Overall, common marmosets appear to be better suited than rhesus macaques for therapeutic studies designed to target severe disease because marmosets show slightly slower onset of illness and longer duration and severity of disease and their small size requires lower doses of therapeutic drugs. However, the marmoset model has not been standardized and is not consistent between laboratories . Furthermore, the size of marmosets substantially limits sequential blood sampling for virologic or pharmacokinetic testing. Challenges to the development of NHP models include determination and standardization of the optimal MERS-CoV challenge dose and of the volume and route of exposure, as well as the limited availability of NHPs, especially marmosets.\n\n【9】Large animal models in development include camels and camelids such as alpacas . These models may be vital in understanding the virology and immunology of MERS-CoV infection in dromedaries, a natural host. In addition, serologic evidence of MERS-CoV infection in alpacas has been reported in Qatar . Major gaps for all animal models include a lack of consensus and availability of the optimal animal model to replicate severe human illness from MERS-CoV infection; limited availability of currently or recently circulating MERS-CoV strains; the lack of understanding of clinically relevant symptoms that can be incorporated into clinical scores or used as a signal to begin treatment in animal models; and competition for funding, laboratory space, availability of animals, and expertise with other emerging or reemerging infectious diseases, such as Ebola virus disease and Zika virus disease.\n\n【10】### Diagnostic Devices\n\n【11】Critical issues for facilitating appropriate clinical management of MERS-CoV cases and for implementing infection prevention and control measures in healthcare facilities is the prompt diagnosis of MERS-CoV infection and the monitoring of prolonged viral shedding in severely ill patients and their healthcare and family contacts. Outside of the United States, several commercial and in-house academic laboratory reverse transcription PCR (RT-PCR) molecular assays are available for research, diagnostic, and viral load monitoring purposes. These assays can measure MERS-CoV RNA in samples from symptomatic patients and their asymptomatic contacts. Contributing factors to recent large clusters of MERS-CoV infection in hospitals in Saudi Arabia and South Korea may be linked to inadequate infection-control procedures and prolonged shedding of MERS-CoV. MERS-CoV RNA has been detected for 24–31 days after onset of fever in hospitalized patients .\n\n【12】The Secretary of the US Department of Health and Human Services declared a potential public health emergency on May 29, 2013, regarding MERS-CoV infection that could have a high potential to affect national security or the health and security of US citizens living abroad. The US Food and Drug Administration (FDA) subsequently issued an emergency use authorization to the Centers for Diseases Control and Prevention (CDC) for an in vitro molecular diagnostic test to diagnose MERS-CoV infection in multiple types of clinical specimens from symptomatic patients. The use of this test was later expanded to include the ability to test asymptomatic contacts of a person infected with MERS-CoV who traveled from Saudi Arabia to the United States. The CDC made this test available to multiple US public health laboratories, the US Department of Defense, and WHO laboratories worldwide. Although the test has been distributed extensively, it is limited in terms of the CDC’s ability to scale up the supply of reagents to support a surge in MERS-CoV cases in the United States and in other countries where the test has been made available. Therefore, an emergency use authorization was issued on July 17, 2015, for the commercially developed RealStar MERS-CoV RT-PCR Kit U.S. (Altona Diagnostics GmbH, Hamburg, Germany) for use in the in vitro qualitative detection of MERS-CoV RNA in tracheal aspirate or tracheal secretion samples . Although this commercial assay is a first step in bridging the diagnostic test availability gap in case of a surge scenario, the current coverage, at least in the United States, is insufficient until alternative, FDA-cleared commercial tests are available .\n\n【13】A worldwide gap exists in the lack of readily available, simple, rapid, and accurate diagnostic tests for use in outpatient and inpatient clinical settings where the ability of the facility to use currently available, higher complexity molecular tests is limited. The lack of commercial development of MERS-CoV assays may be partially related to the limited availability of clinical specimens and MERS-CoV isolates from infected patients. Availability of serum specimens from RT-PCR–confirmed MERS-CoV patients who survived can help facilitate development of serologic tests. If paired acute and convalescent serum samples are available, serologic tests can be used to confirm MERS-CoV infection when viral shedding is not detectable, and for surveillance purposes such as measuring population exposures and immunity to MERS-CoV infection.\n\n【14】### Therapeutic Drugs\n\n【15】No investigational therapeutic drugs have been evaluated for treatment of MERS-CoV patients in prospective randomized controlled clinical trials. Potential therapeutic drugs for MERS-CoV patients include available approved drugs with nonspecific properties, such as immunomodulators, small-molecule drugs with broad antiviral activity, repurposed FDA-approved small-molecule drugs that show activity against MERS-CoV in vitro  , and newly developed monoclonal or polyclonal antibody therapies with specific activity against MERS-CoV  .\n\n【16】One promising approach has been to investigate libraries of drugs approved by the FDA and the European Medicines Agency. Considering development times and manufacturing requirements for new products, repurposing of existing drugs might potentially facilitate a rapid response to outbreaks of emerging viruses . Other early-stage work on MERS-CoV therapeutics includes studies focusing on the essential viral replication steps of fusion, proteolysis, and RNA polymerization  .\n\n【17】Immunotherapeutics under evaluation consist of convalescent plasma and monoclonal and polyclonal antibodies. Most of the monoclonal antibodies in development have specific neutralizing activity against the MERS-CoV spike protein . Platforms are being developed to rapidly discover monoclonal antibodies, either from fully human convalescent blood or from transgenic animals, which can be manufactured on a large scale and are likely to have a good safety profile. The most advanced immunotherapeutic for MERS-CoV uses a transchromosomal bovine production system to produce fully human polyclonal MERS-CoV antibodies; a phase I study of this product was recently implemented . Preliminary results from immunoprophylaxis or treatment studies have shown efficacy of fully human monoclonal or polyclonal antibodies in MERS-CoV-infected mice and NHPs . Although fully human monoclonal antibodies typically have a good safety profile and a defined set of preclinical toxicology studies, challenges to development of immunotherapeutics include ensuring the absence of antibody-dependent enhancement of disease and reducing the risk for generation of escape mutant viruses that would be resistant to treatment.\n\n【18】### Vaccines\n\n【19】##### Human Vaccination\n\n【20】Development of MERS-CoV candidate vaccines was initiated by the National Institute for Allergy and Infectious Diseases at the National Institutes of Health, academic investigators, and several companies . Most candidate vaccines are still being evaluated in animal models. They have generally targeted the spike protein of MERS-CoV and are recombinant virus, subunit, DNA, or virus-like vector vaccines . One live-attenuated MERS-CoV candidate vaccine is in early development . Preliminary studies for several other MERS-CoV vaccine candidates have been initiated, and early results demonstrate immunogenicity; 2 have progressed to NHP challenge, and a phase 1 clinical study in adults of 3 different doses of a DNA plasmid vaccine that expresses the MERS-CoV spike protein was started in January 2016 . Ongoing assessment of antigenic evolution of circulating MERS-CoV strains is essential for informing vaccine development .\n\n【21】A concern that must be addressed in the development of MERS-CoV vaccines is the potential for causing antibody-dependent enhancement of disease upon virus challenge, such as what was observed with a SARS-CoV candidate vaccine upon SARS-CoV challenge . The lack of a precedent of coronavirus vaccines for humans poses another challenge for the evaluation of MERS-CoV vaccines for humans, although vaccines against other animal coronaviruses are safe and in use in animals.\n\n【22】##### Camel Vaccination\n\n【23】Considering the cultural importance of dromedaries on the Arabian Peninsula for meat, milk, and racing, prevention of camel-to-camel MERS-CoV transmission and reduction of spread from dromedaries to humans by camel vaccination is being investigated by government, academic, and commercial investigators . Young camels appear to be at high risk for MERS-CoV infection and could be a priority group for vaccination ; the loss of maternal MERS-CoV antibodies ≈5–6 months after birth suggests a short time window for vaccination . A major challenge to this approach is that dromedaries can be reinfected with MERS-CoV; a study by Farag et al. found no correlation between MERS-CoV RNA levels and neutralizing antibodies in camels , suggesting that antibodies may not be protective against infection. Because older camels can be reinfected, a camel vaccination strategy may require multiple dosing and booster vaccination to increase effectiveness over time. Experimental MERS-CoV infection studies and vaccine studies of a small number of dromedaries have been conducted in large animal Biosafety Level 3 facilities in the United States and overseas . In addition, 3 doses of a DNA vaccine containing the MERS-CoV spike protein induced humoral immunity in dromedaries . In a recent study, a modified vaccinia virus Ankara vaccine that expresses the MERS-CoV spike protein was administered intranasally and intramuscularly to dromedaries; when challenged intranasally with MERS-CoV, vaccinated dromedaries had fewer signs of respiratory infection and lower MERS-CoV titers in the upper respiratory tract compared with unvaccinated dromedaries . Alpacas (New World camelids) are being investigated as a suitable proxy for camels because of the lack of available dromedaries in the United States, the high cost of acquiring dromedaries, and the relatively smaller size of alpacas .\n\n【24】### Regulatory Considerations for Medical Countermeasures in the United States\n\n【25】Regulatory considerations for MERS-CoV medical countermeasures in the United States are focused on a pathway to human clinical trials for drugs and vaccines through submission of investigational new drug applications. Investigational new drug submissions must adhere to requirements set forth in the Code of Federal Regulations, Title 21, Part 312 . Several guidance documents exist on the FDA website related to virology, microbiology, pharmacology and toxicology, and clinical and medical considerations . The most appropriate approval pathway is likely to be product-specific and will require consideration of existing product data, proposed intended use and population for use, and validated endpoints for efficacy predictive of clinical benefit, if any. Likewise, data needed for consideration of an emergency use authorization, including dose finding and dose ranging, duration, and safety, can be obtained through sources such as investigational new drug clinical trials.\n\n【26】Repurposing of drugs approved by the FDA for other illnesses for a MERS-CoV indication can potentially be expedited or accelerated if 1) the mechanism of action for antiviral activity is defined, 2) there is no change to the approved final drug form and route of administration, 3) dosing does not exceed the currently approved dose and duration for the currently indicated population and adequate pharmacokinetics data support this dosing, and 4) the risk–benefit profile is acceptable for the intended population and indication. For example, the risk–benefit profile for an approved drug with an oncology indication may be unacceptable if the drug is repurposed for administration to a healthy population for MERS-CoV postexposure prophylaxis. However, data requirements to initiate human trials will depend on the characteristics of the drug product and its intended use against MERS-CoV. As such, sponsors should consider prioritizing drug development on the basis of the totality of scientific evidence and merit of the drug alone, not on whether the drug has been previously approved.\n\n【27】In the absence of a standardized and accepted animal model that simulates human disease from MERS-CoV infection, it is unclear how the FDA may be able to expedite licensure or approval when data are lacking. The best approach may be collection of preclinical safety data and implementation of adaptive human clinical trials. This approach was taken for medical countermeasures in response to the 2013–2016 Ebola virus disease outbreak.\n\n【28】For diagnostic devices, the current emergency use authorization pathway serves as a fast approach to make products available for emergency public health purposes. After an emergency has been terminated, Premarket Notifications for these products should be submitted to FDA for a more thorough evaluation as 510(k)s .\n\n【29】### Clinical Experience and Medical Countermeasure Trials\n\n【30】The overarching goal for clinical research of MERS-CoV patients is to optimize clinical management and to identify effective therapies to improve survival. Although clinical data on some MERS-CoV patients have been published in case series , there is a need for much more epidemiologic, clinical, virologic, and immunologic data to improve the limited understanding of the pathogenesis of MERS-CoV infection in humans. Gaps include information on viral load and duration of viral shedding in blood, urine, respiratory, and other clinical specimens from infected persons; understanding of the innate and adaptive immune response to MERS-CoV infection; pathology data on the distribution of MERS-CoV in respiratory and extrapulmonary tissues in fatal cases; information from autopsies of persons who died of MERS-CoV; and an overall improved understanding of the pathogenesis of MERS-CoV in humans. Only one study has investigated MERS-CoV infection in autopsy tissues of a patient who died from the disease . Collaborations are especially needed to pool and systematically collect serial clinical specimens from MERS-CoV patients for virologic, immunologic, and biomarker analyses to correlate with clinical illness, and to conduct long-term follow-up of survivors of severe disease . Detailed understanding of host factors and cofactors associated with disease severity from asymptomatic infection to fatal illness is needed. Efforts to promote international sharing of clinical specimens and MERS-CoV isolates are needed to foster development of diagnostics, therapeutics and vaccines.\n\n【31】Use of standardized clinical data collection instruments and common biologic sampling protocols for serial prospective data collection will facilitate data pooling from MERS-CoV cases and comparisons across clinical sites and countries. Global collaborations among clinical networks are also needed to implement clinical trials, preferably randomized controlled clinical trials, of MERS-CoV investigational therapeutics, Without an international agreement on protocols and systematic standardization of case reporting and data collection methods, haphazard or anecdotal reporting and analysis of disease course and outcome may continue. WHO and the International Severe Acute Respiratory and Emerging Infection Consortium are collaborating in adapting standardized protocols for controlled clinical trials for MERS-CoV .\n\n【32】### Timelines for Clinical Trials of Medical Countermeasures\n\n【33】Prospective controlled clinical trials (ideally randomized clinical trials) of potential MERS-CoV therapies and vaccines in humans are needed urgently; however, there is uncertainty in estimating timelines for the development of potential MERS-CoV medical countermeasures because of the need to further characterize existing and new animal models, the unpredictability of demonstrating a favorable risk–benefit outcome during preclinical testing, and competition for resources with other emerging infectious diseases. In addition, the risk for antibody-dependent enhancement of disease may interrupt the timeline for conducting human clinical trials of MERS CoV vaccines and immunotherapeutics. Researchers of all potential MERS-CoV medical countermeasures should have preclinical toxicology data available before initiating human clinical trials. Although animal efficacy data are not technically required before implementing human clinical trials of potential countermeasures, such data are considered important for identifying the most promising medical countermeasure candidates, justifying risk in human volunteers, and informing the design of future clinical studies. Timeframes for the production of specimen panels and repositories to aid commercial diagnostic development are also contingent on obtaining adequate funding and clinical samples.\n\n【34】### Conclusions\n\n【35】Although preclinical development and research on potential MERS-CoV medical countermeasures has achieved appreciable progress to date, such development is preliminary, and substantive challenges must be overcome before most potential countermeasures are ready for human clinical trials. The only clinical trials of MERS-CoV medical countermeasures to date are phase I studies of 1 candidate vaccine and 1 immunotherapeutic that were both implemented in 2016 and are ongoing. Prioritization of animal models, standardization of representative virus strains, and establishment of clinical trial capabilities in areas where the virus is endemic among dromedaries are viewed as critical elements of effective MERS-CoV medical countermeasures development. Results of substantial progress in establishing the infrastructure and platforms for preclinical and advanced clinical development of countermeasures can serve as a model to enable more timely response to other emerging infectious diseases of global public health concern in the future.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "86d35682-39e8-43a3-97dc-5d1b185072b8", "title": "A Participatory Regional Partnership Approach to Promote Nutrition and Physical Activity Through Environmental and Policy Change in Rural Missouri", "text": "【0】A Participatory Regional Partnership Approach to Promote Nutrition and Physical Activity Through Environmental and Policy Change in Rural Missouri\nBackground\n\n【1】Chronic diseases (eg, heart disease, cancer, stroke) account for most premature deaths . Although research demonstrates the association of risky behavior with chronic disease, particularly physical inactivity and poor nutrition, changing these behaviors is challenging, particularly in rural communities where residents are less likely to meet recommendations for these behavioral determinants .\n\n【2】Failure to meet the recommendations is due to factors across several ecological levels. At an individual level, rural residents have limited exposure to preventive health care messages . Rural residents also have limited access to environmental determinants of physical activity or healthy eating, such as safe, walkable communities, recreation facilities (including informal recreation areas such as hiking trails), parks, and healthful food outlets . Creating environmental supports and changing policy in rural communities is particularly challenging because rural communities have lower population density and, thus, fewer resources than their urban and suburban counterparts. Our case study describes how we created a regional partnership to leverage resources and enhance environmental and policy initiatives to improve nutrition and physical activity in rural Missouri.\n\n【3】Community Context\n\n【4】Healthier Missouri Communities (Healthier MO) is a community-based research project conducted by the Prevention Research Center in St. Louis (PRC-StL) and community partners from 12 counties in rural southeast Missouri . The PRC-StL began working with community partners in southeast Missouri in 1994 through county heart health coalitions. The PRC-StL built partnerships in southeast Missouri because of the high poverty rate (currently double the Missouri rate of 15%)  and significantly higher rates of chronic disease than the rest of the state . All counties are rural  and currently all but one are designated as Medically Underserved Areas . Southeast Missouri residents are less likely to be physically active than Missouri residents as a whole  and, although county data for fruit and vegetable consumption is unavailable, Missouri residents overall are less likely to meet recommendations for fruit and vegetable consumption than residents in the nation as a whole . Access to places to be physically active (eg, parks and recreational facilities) varies across the region, and residents have less access to healthful food than do residents in other regions in the state .\n\n【5】This article describes the partnership activities from 2010 to 2014. Before 2010, four of the initial heart health coalitions pilot-tested a regional partnership approach. For the 2010–2014 funding cycle, a decision to shift to a regional partnership approach with all counties was made by community partners and informed by interviews conducted with national key informants who were implementing environmental or policy change to promote nutrition and physical activity . The interview findings suggested that rural communities face a lack of human capital, professional training opportunities, and the perception that policy changes have a small impact on population health because rural populations are small (compared with suburban and urban populations). Informants recommended developing a broad-based partnership to leverage regional resources and increase the health impact of environmental or policy changes .The collective decision to move toward a regional partnership also marked a transition from community-based research to community-based participatory research (CBPR).\n\n【6】The programmatic objective of Healthier MO was to implement environmental and policy interventions to promote physical activity and healthy eating across southeast Missouri. The partnership engagement objective was to develop a regional partnership with representation from the 12 counties to design, implement, and evaluate interventions to promote these behaviors. The anticipated outcomes of the partnership engagement component were 1) increased resource sharing among partners and 2) a sustainable regional partnership focusing on chronic disease prevention.\n\n【7】Methods\n\n【8】### Evidence-based training\n\n【9】In spring 2010, thirty community partners representing the 12 Healthier MO counties participated in an evidence-based decision-making training, using a well-established training model based on _Evidence-Based Public Health_  . Community partners included those who participated in the county heart health coalitions before 2010 and others doing similar work in the region. The partnership included African American and white partners representing grassroots communities, community-based organizations, schools, and health departments.\n\n【10】The training focused on essential elements of evidence-based public health and advantages of adapting evidence-based programs for one’s community. Academic partners engaged community partners in an evidence-based decision-making process to determine which environmental or policy interventions addressing nutrition and physical activity were feasible and important for their region. The community partners identified community gardens and school wellness initiatives. Partners also learned about CBPR and the process of using the approach.\n\n【11】### Partnership engagement\n\n【12】The partners previously engaged in the county heart health coalitions recognized CBPR as a significant shift. For the first time community partners engaged in the design, implementation, and evaluation of the intervention, a common practice in a CBPR approach. Also, there was a shift in the way counties were compensated. Previously counties were provided with funding to engage in academically defined interventions and given small stipends to spend on nutrition and physical activity activities of their choosing. In Healthier MO, partners collectively decided on acceptable expenditures based on regional efforts with community gardens and school wellness initiatives.\n\n【13】The Healthier MO partnership was staffed by a research team that included academic staff and faculty and 2 community liaisons. All meeting times, locations, and topics, were determined by all partners, with the research team developing meeting agendas and managing meeting logistics. Communication was facilitated by the research team, including distribution of notes and documentation of decisions made by the partnership. Individual community partners disseminated information to their community constituents. A team of research and community partners worked with local media to disseminate information about events and successes with a larger regional audience.\n\n【14】### Action planning and implementation\n\n【15】Partners met monthly after community gardens and school wellness were identified as the intervention foci. During meetings in fall 2010 and spring 2011, the partners developed partnership principles to determine how they would work together  and an action plan to define the objectives; action steps to design, implement, and evaluate the chosen interventions; and the outcomes of interest. In summer 2011, the research staff conducted listening tours in each community to evaluate the action planning process and understand each community’s hopes for the regional partnership. The listening tours solidified participation in the partnership and buy-in from local community members who were not representing their communities at the monthly meetings. During the action planning phase, the composition of the partnership changed. Whereas some of the partners who participated in the heart health coalitions determined the process or purpose no longer fit their local community’s needs, new members joined upon invitation by community partners who remained committed. The partners decided to engage new partners as appropriate without extensive recruitment.\n\n【16】From 2011 to 2014, the partners implemented and evaluated a series of intervention activities. The monthly meetings were used to support the implementation, share information, provide training, and coordinate evaluation. Those involved in community gardens tracked planting and harvest data, and teachers implementing the school intervention tracked student participation and, in some cases, outcome data. At least once per year academic partners analyzed data and formally presented findings to the community partners, giving community partners an opportunity to discuss them. In 2012, partners used community garden evaluation data to develop a video as a marketing and dissemination tool.\n\n【17】In 2013, garden and school subcommittees were formed to focus on dissemination of the research findings and to plan for partnership sustainability when grant funding concluded. Healthier MO members volunteered to join the subcommittees and get involved in the work between partnership meetings. The subcommittees discussed ongoing funding and partnership structure and process. The school subcommittee developed a guidebook to disseminate the results of the school wellness interventions to increase physical activity during the school day.\n\n【18】### Assessment of partnership engagement\n\n【19】In spring 2014 two group interviews were conducted with Healthier MO community partners. Of the 20 community partners, 12 participated in the group interviews, and 3 partners participated separately by telephone or electronic survey. The purpose of the group interviews was to assess the benefits and challenges of the regional partnership approach and the perceived outcomes of the partnership. Partnership literature  informed the development of the interview guide, and focused coding was used to code data. The research protocol was approved by the academic partners’ institutional review boards.\n\n【20】Outcome\n\n【21】### Resource sharing\n\n【22】The partners discussed benefits and challenges of sharing resources in the form of social support, information, expertise, and skills as part of a participatory regional partnership. Healthier MO partners traveled up to 2½ hours to attend monthly meetings. Some participants noted that the benefit of the regional partnership was evidenced by the fact that people showed up each month. As one member stated, “This is just a very small part of my organization, but the benefits of coming far, far, far outweigh the challenge of finding the time.” One participant highlighted the social support received by bringing people together regionally. The partner shared that the regional partnership is energizing and helps when managing small struggling local efforts. Participants also noted that the partnership benefited them personally by helping to develop their professional networks, for example, with schools. Overall, partners expressed that time provided at meetings to communicate one-on-one created space “where we shared our struggles and our triumphs.”\n\n【23】Other participants noted the benefit of sharing information and expertise. One participant described the benefit as “being able to share best practices as far as what other groups are doing.” A local health department partner found it particularly beneficial to learn what other health departments were doing. Several partners noted it was not just about one-way information sharing but “we threw ideas or problems out and someone would say, ‘This is what I did for that.’” Another partner said that information helped “my direction, opened up ideas that I wouldn’t have come up with, gave me encouragement and it gave us confidence to go forward and have some kind of feedback from people who might be doing it differently or better.” In addition to informal information sharing, partners indicated the benefit of formal trainings on such topics as grant writing. The partners noted that because of information sharing in the meetings they called one another in between meetings for advice. One partner emphasized that the information and support received by the regional partners was a greater benefit than the grant funds.\n\n【24】The shift to a regional approach also presented challenges. Partners explained that the original county heart health coalitions got money and were not required to participate in the work of the partnership. One participant explained, “I think a lot of people were still in the mindset of the heart health coalition where they just got some money and then they could go do what they wanted with it, and they didn’t have to really participate.” A few partners also indicated that being rural made it harder than initially thought to share resources and information because of the geographical distance between partners.\n\n【25】### Sustainability\n\n【26】The second outcome was the sustainability of the Healthier MO partnership after the funding ended. The partnership prepared by developing subcommittees for each initiative (community gardens and schools) and developing a leadership structure for the next partnership incarnation. In the post group interviews, participants identified future areas of consideration, including funding, partnership structure and function, and relationship building.\n\n【27】Partners expressed that lack of funding would be a challenge to long-term partnership sustainability. Partners were concerned about how people will travel without mileage reimbursement. Another member explained that some partners are able to participate because of the funding and may not be able to justify the time to their organization without funding. The partners did identify ways to generate funds. One participant suggested partners can sell fruits and vegetables from the community gardens to sustain the partnership when grant funds end. A second participant noted that state funders may be more interested in funding Healthier MO because it is a regional initiative. However, members noted that the partnership will need to enhance their capacity to submit and manage grants.\n\n【28】Partnership structure and function were identified as main sustainability concerns. Partners were concerned about long-term facilitation and organization. Several partners shared that although they can participate, they cannot lead due to other organizational commitments. One partner suggested that a paid leader might be needed while several others suggested a group of partners is needed to lead. One participant acknowledged that delegating roles will be important, stating, “I do think it would be a good idea to have someone in charge of communicating, someone in charge of finances, and in charge of the meetings themselves, getting the agendas.” The partners determined that moving forward they will meet quarterly. As a result, one partner explained the meetings will have to produce results. She noted, “we can’t say, ‘oh, we’ll table that to next month,’ because then it’ll be too late.” Another noted that people need to show up to meetings, stating, “I think it’s very important that because we’re not having as many meetings that when we do have them, we need to come together, because if we don’t, that’s how it’s going to fall apart.” To come together and communicate between meetings, partners identified text messaging as a “plus” because some members do not have Internet access. Despite the challenges, partners expressed that they have “finally bonded” and they will participate, regardless of funding.\n\n【29】### Intervention outcomes of a regional rural partnership\n\n【30】The participants identified outcomes of the interventions, including those related to community gardens and schools. Partners noted that existing community gardens improved and the region gained new gardens. Another participant noted that the gardens allowed community members “to live a healthier lifestyle . eating healthier, growing their own produce” and provided access to fresh fruits and vegetables to folks that would not have access. Another mentioned that the community gardens increased physical activity for older adults in their community. A participant noted that the school initiative was important for children to learn physical activity at an early age to build the skills that they can maintain in the future. Another partner noted that the playground equipment and the walking tracks installed at 4 schools will have long-term benefits.\n\n【31】Interpretation\n\n【32】Healthier MO was a shift to a participatory regional partnership approach that intended to build the southeast Missouri regional capacity to promote physical activity and healthy eating through environmental and policy change. Partnership outcomes were increased resource sharing and partnership sustainability. Although there was change in how funds were distributed and used for interventions, the funding level did not increase. However, the partners did see an increase in intangible resources, including shared expertise, knowledge, and skills from Healthier MO beyond what was available in their local rural community. The acknowledged benefit of a regional approach and the relationships and structures developed were seen as supporting short-term sustainability. The absence of funding for future travel was identified as a challenge that could reduce long-term sustainability.\n\n【33】Most partners agreed that being highly engaged in the design, implementation, and evaluation increased a sense of ownership among all partners. Perhaps as a result of this sense of ownership, community partners noted a missed opportunity to plan ahead. Although Healthier MO initiated planning for partnership sustainability approximately 2 years before the formal funding end, partners agreed that planning should have started earlier. For example, one community partner suggested that a broader community–academic facilitation model be implemented initially. Because of the dramatic shift in community engagement at Healthier MO’s inception, the research team facilitated the partnership to reduce the burden on community partners during the transition to a new participatory and regional norm. The research team included 2 paid community liaisons who were regional residents. Distributing paid positions among academic and community partners facilitates a shared benefit, and paid staff is helpful to fledgling partnerships. One drawback of paid leadership is that it can limit nonpaid partner involvement and ownership in the long term . The community partners noted this tension when they discussed future leadership structure.\n\n【34】Community partners identified 2 lessons learned that may benefit other rural areas considering a regional partnership approach. First, community members identified the need to invite funders to the partnership to help develop regional capacity to successfully apply and manage grants. In an earlier study, national rural key informants observed that smaller population size was a barrier to attracting funders because the effect on a small rural population has only a small influence on state or federal policy . However, eliminating rural health disparities is a national priority, and rural communities recognize that funders are necessary partners in addressing rural health gaps . Second, partners noted that it is essential to commit meeting time to building relationships. One partner described it as “sealing the bonds . so that once the funding is gone, the bonds will hold and we’ll still push forward.” This advice is supported by literature on social capital that finds that communities with higher levels of social capital or relationships of trust and reciprocity have better health outcomes . Relationship has long been a hallmark of small communities. Recognizing relationship building as a key capacity and leveraging it to build regional networks may be an important lesson for other rural regions in the United States.\n\n【35】Healthier MO demonstrates that a participatory regional partnership approach to implementing environmental and policy interventions to promote nutrition and physical activity can be successful in rural areas. A CBPR approach allowed us to increase Healthier MO partner ownership and enabled partners to choose the interventions that were most important and feasible for the region. A participatory approach also allowed us to identify what partners valued (eg, interventions, relationships, sustainability). We recommend that other rural regional partnerships discuss how partners want to be engaged and what they value early in the partnership process. In the end, Healthier MO is successful because partners value the health of the people in their region and are committed to participate regardless of the required travel, time, or funding level.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fa4332d9-44fb-4a35-92bc-2a2875dd37ac", "title": "Mediterranean Fin Whales (Balaenoptera physalus) Threatened by Dolphin MorbilliVirus", "text": "【0】Mediterranean Fin Whales (Balaenoptera physalus) Threatened by Dolphin MorbilliVirus\nFin whales ( _Balaenoptera physalus_ ) living in the Mediterranean Sea belong to a population that is part of the Atlantic stock . For feeding purposes, these whales tend to concentrate in specific areas, one of which is Pelagos Sanctuary, the widest protected marine area for sea mammals in the Mediterranean Basin between Italy, France, and Monaco. In Pelagos Sanctuary and, in general, in the entire Mediterranean Sea, fin whales are considered vulnerable because of several anthropogenic threats, the most common of which are ship strikes, chemical pollution, and noise .\n\n【1】Postmortem investigations on well-preserved whale carcasses are necessary to gain evidence-based insight into the effect these threats have on fin whales; thus, the carcasses of all large cetaceans found stranded along the Italian coastline are systematically examined to determine the cause of death. Because morbillivirus infections have been detected during these postmortem investigations, we conducted a study to determine the effect of this natural threat on the Mediterranean fin whale population.\n\n【2】### The Study\n\n【3】During 2006–2014, a total of 23 fin whales were found stranded along the coast of Italy. We systematically conducted full necropsies and microscopy- and molecular-based analyses on 9 (39%) carcasses that were in good conservation status. Of these 9 fin whales, 2 were juveniles and 2 were calves found during January 2011–February 2013 along the coasts of Tuscany, Sardinia, and Liguria, Italy. These young whales showed pathologic, immunohistochemical (IHC), biomolecular, and/or serologic evidence of dolphin morbillivirus (DMV) infection  ; however, not all of these cases were spatially or temporally related to known fatal DMV-associated outbreaks that occurred during 2006–2014 . Moreover, in October 2013, a newborn male fin whale was found stranded alive on Elba Island (Tuscany); the whale died after a few hours, and postmortem investigations conducted within 24 hours of death yielded biomolecular and IHC evidence of DMV infection. Viral genome, antigen, or both were found in several tissues, along with a parasitic infection and a generalized lymphocytic depletion. Hyperimmune rabbit anti–rinderpest virus serum (provided by Pirbright Institute, Pirbright, UK)  was used to detect morbillivirus antigens; only circulating monocytes and tissue macrophages in brain and thymus stained positively .\n\n【4】DMV genome was detected in brain, lung, spleen, and thymus from the newborn whale; viral RNA was extracted from these tissues by using TRIzol reagent (Thermo Fisher Scientific, Waltham, MA, USA). Primer DMV2  and the RevertAid First Strand cDNA Synthesis Kit (Thermo Scientific) were used to synthesize cDNA; primer and viral RNA were incubated at 42°C for 60 min followed by 70°C for 5 min. Amplification was performed by using primers DMV-N1 and DMV-P2  and Phusion Hot Start II DNA Polymerase (Thermo Scientific) with the following PCR conditions: 30 s at 98°C; 35 cycles of 10 s at 98°C, 30 s at 62°C, 1 min at 72°C; and 10 min at 72°C. Next, the DNA fragments obtained from lung and cerebral cDNA were purified, cloned into the plasmid vector pCR-Blunt II-TOPO (Invitrogen, Life Technologies, Carlsbad, CA, USA) according to the manufacturer’s instructions, and then sequenced.\n\n【5】Sequences from 5 lung and 4 cerebral plasmidic colonies were analyzed. Programs in the DNASTAR Lasergene software package  were used to edit, assemble, and translate sequences. This technique enabled identification of a 1,355-bp DNA fragment from the newborn whale that encompassed partial nucleoprotein and phosphoprotein genes . This fragment, from lung and cerebral samples, showed 98.89% sequence homology with the complete DMV genome  and 99.85% sequence homology with a DMV strain identified in long-finned pilot whales ( _Globicephala melas_ ) that were affected by the 2006–2007 epidemic in the Mediterranean . Amino acid changes in KR337460, compared with AJ608288, are shown in Figure 2 . The same DNA fragment was recovered from the newborn whale’s spleen and thymus by using the PCR protocol mentioned above .\n\n【6】Viral hemagglutinin (H) protein mediates DMV entry into host cells by specifically binding with the whale's signaling lymphocyte activation molecule (SLAM)/CD150; thus, we also investigated variations in the H gene of the newborn whale by using the previously described cloning procedures for cerebral cDNA with 3 new overlapping primers pairs . This technique enabled identification of a 1,699-bp DNA fragment encompassing a partial H gene sequence; 116 bp at the beginning of the gene were missing. The H consensus fragment, obtained from the cDNA clone for each primer pair, showed 99.41% sequence homology with the complete DMV genome  and 99.94% sequence homology with the DMV strain identified in long-finned pilot whales . We also identified 2 aa changes: the previously reported p.Val164Ala  and a novel variation, p.Ala451Thr, located within the H protein region (residues 382–582) involved in SLAM binding  . This variation was previously reported in other related morbilliviruses  and does not control any change in the tertiary structure of H antigen, as determined by using the SWISS-MODEL  modeling program.\n\n【7】### Conclusions\n\n【8】The results of our direct (IHC and biomolecular) and indirect (serologic) testing provide evidence of DMV infection or exposure in 5 (55%) of 9 fin whales that were found stranded off the Mediterranean Coast during 2011–2013. These 5 infected whales correspond to 21.7% of the 23 whales stranded along the Italian coastline during 2006–2014. The other 4 examined whales showed no evidence of morbillivirus infection. The range of DMV-susceptible host species has progressively expanded , as highlighted by the recent report of DMV infection in a captive common seal ( _Phoca vitulina_ ) during the 2011 outbreak . This expansion, combined with spread of DMV through the transplacental route, resulting in virus colonization of the thymus in fetuses, could represent DMV survival strategies among cetacean populations. In addition, our data argue in favor of an epidemic cluster of fatal DMV among the Mediterranean fin whale population, even though, on the basis of the amino acid sequence of the SLAM/CD150 viral receptor, this species is not included among those susceptible to DMV epidemics .\n\n【9】Although the single amino acid substitution, p.Ala451Thr, did not cause substantial variations in the structure of H antigen, the effect of the variation on protein functions is unclear. Recent studies showed that similar amino acid changes could affect virulence and infectivity of different _Canine distemper virus_ (family _Paramyxoviridae_ , genus _Morbillivirus_ ) strains, but such changes are often neutralized by compensatory mutations that preserve the biologic activity of H protein . Furthermore, despite the high sequence homology observed between N, P, and H genes of the DMV strain identified in the newborn fin whale in our study and in the isolates recovered from DMV-affected cetaceans during the 1990–1992 and the 2006–2007 epidemics , we cannot exclude that more prominent differences occurred in virus genes encoding for both structural and nonstructural proteins responsible for virulence and pathogenicity (e.g. P/V/C and fusion genes) ; the simultaneous occurrence of primary structure differences, if any, in the SLAM/CD150 receptor should also be taken into account . In conclusion, although further studies are needed to elucidate the complex virus–host interaction dynamics and the putative influence exerted by environmental factors, DMV should be regarded as one of the major threats for the conservation of fin whales within the Mediterranean Sea.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7bb95d70-d23f-423f-a7e9-c18869701f5d", "title": "Vibrio cholerae O1 Isolate with Novel Genetic Background, Thailand–Myanmar", "text": "【0】Vibrio cholerae O1 Isolate with Novel Genetic Background, Thailand–Myanmar\n**To the Editor:** _Vibrio cholerae_ O1, a causative agent of cholera, was classified into 2 biotypes, classical and El Tor . However, accumulating evidence suggests that atypical El Tor _V. cholerae_ , which possesses traits of both classical and El Tor biotypes, has replaced the seventh pandemic prototypic El Tor _V. cholerae_ worldwide in recent years. Cholera outbreaks in Thailand during 2007–2010 were caused by atypical El Tor isolates carrying the classical type cholera toxin gene . Epidemiologic surveys in a Thailand–Myanmar border area during 2008 yielded more than 500 isolates of _V. cholerae_ O1. We identified an isolate that possessed the typical El Tor type cholera toxin gene (genotype 3) and designated it MS6 (later assigned strain number DMST28216). It does not belong to either the seventh pandemic prototypic biotype identified in 1961 or the group of atypical El Tor strains found during 1991–present .\n\n【1】MS6 was isolated from stool samples from a 26-year-old woman (migrant worker) from Myanmar who had been admitted to Mae Sot General Hospital in Tak Province, Thailand, for 3 days with vomiting, watery diarrhea, nausea, fever, and headache. The illness was considered mild to moderate. Acute gastroenteritis was diagnosed on the basis of the symptoms and laboratory results. The key virulence factors of _V. cholerae_ O1 include cholera toxin (CTX), which is responsible for profuse watery diarrhea, and a pilus colonization factor known as toxin-coregulated pilus (TCP). The virulence-related genes ( _ctxAB_ and _tcpA_ ) and the phage repressor gene ( _rstR_ ) of MS6 had identical sequences to those of the seventh pandemic prototypic El Tor _V. cholerae_ O1 N16961 strain. The isolate was found to be positive for enteric bacteria in the Voges-Proskauer test and resistant to polymyxin B (50 units). We further investigated 2 gene clusters, _Vibrio_ seventh pandemic island I (VSP-I) and II (VSP-II), associated with the seventh pandemic strains and absent in classical and pre–seventh pandemic strains . The common genes on the VSP-I island in N16961, including VC0175, VC0178, VC0180, VC0181, and VC0183, were detected by PCR  in MS6 but were lacking in VSP-II; 26.9 kb of VSP-II was originally found in N16961. Moreover, PCR analysis showed that the isolate did not possess the VC2346 gene, a specific marker of the seventh pandemic clone . However, we found a VC2346 homolog with 83.9% sequence identity with VC2346 at the nucleotide level and 97% at the amino acid level in the 624-bp region. The coding region of the homolog is considered to be shorter than VC2346 (684 bp) because it contains the stop codon, TAG. This homolog was identified in environmental, classical, or pre–seventh pandemic strains of _V. cholerae_ O1 , including MS6, and in 2740–80 (US Gulf Coast, 1980), 3569–08 (US Gulf Coast, 2008), BX33026 (environmental water in Australia, 1986), RC27 (classical, human isolate in Indonesia, 1991), O395 (classical, human isolate in India, 1965), MAK757 and M66–2 (pre–seventh pandemic, human isolate in Indonesia, 1937), and NCTC 8457 (pre–seventh pandemic, human isolate in Saudi Arabia, 1910), excluding seventh pandemic strains.\n\n【2】This conservation of the homologue of VC2346 in strains isolated over the course of a century and the geographic distribution of the strains suggest a notable biologic function and a specific marker. In addition, we determined the sequences of 15 housekeeping genes which exhibited sequence variations in toxigenic _V. cholerae_ . The results indicated that by comparison, MS6 is closely related to the US Gulf clones . However, 2 genes, _malP_ and _pepN_ , of MS6 are remotely related to them and are novel sequence types, based on results of a BLAST  search. Antimicrobial susceptibility testing by using the disk diffusion method revealed that MS6 was susceptible to chloramphenicol, ciprofloxacin, gentamicin, sulfamethoxazole/trimethoprim, tetracycline, streptomycin, furazolidone, doxycycline, and norfloxacin, and had intermediate susceptibility to ampicillin and erythromycin, suggesting that MS6 had not been exposed to several antimicrobial drugs. The _V. cholerae_ SXT element, which usually shows code drug-resistance markers, integrates into a specific site of the _prfC_ gene. In MS6, the complete _prfC_ gene was detected. The accession numbers of DDBJ for nucleotide sequences determined in this study are AB699244-AB699265. MS6 possesses unique properties in terms of the ribotype, pulsed-field gel electrophoresis pattern, and multiple-locus variable-number tandem-repeat analysis profile, compared with other _V. cholerae_ O1 isolates .\n\n【3】This case was probably an episode of sporadic cholera from indigenous _V. cholerae_ O1, such as US Gulf Coast and Australian clones, which are mainly associated with environmental sources. We have been unable to isolate another MS6-like clone, which could have escaped detection because of low prevalence or might exist in a dormant state in a rural area. Nevertheless, the transmission route and its pathogenicity must be of concern for public health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d1403fe3-8b47-46cf-a6ee-31be55854bbd", "title": "Rabies Immunization Status of Dogs, Beijing, China", "text": "【0】Rabies Immunization Status of Dogs, Beijing, China\n**To the Editor:** In the People’s Republic of China, >3,000 persons die of rabies each year; most were infected by dog bites . Since 2000, the dog population in Beijing has increased dramatically, and the exact vaccination coverage and immunization status of dogs are not known.\n\n【1】During 2006–2009, to assist with governmental rabies control, Fengtai District was selected as a geographically representative area in Beijing in which to conduct a survey of rabies antibody titers in domestic dogs. Blood samples were randomly collected from 4,775 dogs in Fengtai District, which account for 3% of all registered dogs in the district. Rabies virus neutralization antibody (VNA) titers were detected by fluorescent antibody virus neutralization . In brief, VNA titers \\> 0.5 IU indicated positive immunization, implying that the dog had an adequate level of antibody, and VNA <0.5 IU indicated negative immunization . The data were analyzed by 2-tailed χ 2  test; p<0.05 was considered significant. Vaccination coverage and antibody levels were categorized either by dog’s function (guard or pet) or residence (urban or suburban) .\n\n【2】Most dogs with a history of vaccination were positively immunized (68.1%) , compared with 16.4% in the unvaccinated group , demonstrating that compulsory immunization is crucial to rabies control . Of 944 dogs with unclear vaccination history, 221 (23.4%)  had adequate antibody levels, possibly from undocumented vaccination or contact with rabies hosts. However, for 2006, 2007, 2008, and 2009, immunization coverage in the district was 55.0%, 53.8%, 67.4%, and 54.4%, respectively, all below the >70% criterion recommended by the World Health Organization . The results imply that much work still needs to be done by the Beijing government, not only to meet the World Health Organization immunization baseline but also to keep risk for a rabies epidemic in Beijing low.\n\n【3】Immunization coverage ratios differed significantly (p<0.05) between guard (39.3%) and pet dogs (69.5%)  and between urban (81.7%) and suburban areas (27.6%) . Consequently, the number of negatively immunized guard dogs was 1.68× lower than that for pet dogs  (p<0.05), and the number of positively immunized dogs in urban areas was 2.5× higher than that in suburban areas  (p<0.05).\n\n【4】In Beijing, guard dogs are usually raised by villagers to protect the house, whereas pet dogs are usually raised by city dwellers who treat dogs as friends. As a result, in urban areas dogs are registered and vaccinated in a timely manner by authorized pet hospitals . In suburban areas, however, dog management is deficient. For example, guard dogs in suburban areas are sometimes not vaccinated because the owner or veterinarian cannot safely restrain the dog for vaccination.\n\n【5】According to our study, >10% of unregistered dogs with no clear history of vaccination are not vaccinated during yearly vaccination programs. In Beijing during 2007–2009, of 9 cases of rabies in humans, 6 were associated with stray dogs , and most stray dogs were found in suburban areas. Hence, strategies to either reduce stray dogs in the city or to get such dogs under official management (e.g. include stray dogs in compulsory annual vaccination programs) are urgently needed.\n\n【6】In our opinion, policies related to dog registration, vaccination recording, and vaccination strategies need improvement in Beijing, especially in suburban areas. Although our report only focused on the Fengtai District, the findings could be helpful for the Beijing government for establishing strategies to control the rabies epidemic in the entire city.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ef6c6f26-a27d-455a-a38a-a9a35ea35601", "title": "Annual Mycobacterium tuberculosis Infection Risk and Interpretation of Clustering Statistics", "text": "【0】Annual Mycobacterium tuberculosis Infection Risk and Interpretation of Clustering Statistics\nStudies are increasingly using levels of clustering of isolates from tuberculosis cases (proportion sharing identical DNA fingerprint patterns) to estimate the extent of disease attributable to recent transmission. To date, few studies have been conducted in unindustrialized countries, where the impact of tuberculosis and the proportion of disease attributable to recent transmission are greatest. Whether or not the properties and interpretation of clustering statistics in such settings are similar to those in industrialized populations is unclear.\n\n【1】Studies in industrialized countries have found relatively low overall levels of clustering (e.g. 30% to 40% during a 3-year period ) but much higher levels among younger versus older patients. This age differential probably reflects past trends in the annual risk for infection, which was high in the early 20th century (e.g. _\\>_ 2% per year before 1940 in the Netherlands ) and is currently very low. Thus, a large proportion of disease in older patients is attributable to reactivation of infections acquired many years ago, and, given the short half-life of DNA fingerprint patterns , only a small proportion of old patients share identical isolates with other patients **.** In some unindustrialized countries, on the other hand, where the annual risk for infection may not have changed much over time, the age differential in clustering might be small, given that a large proportion of disease even among older persons may be attributable to recent (re)infection. Understanding the effect of the magnitude of the annual risk for tuberculous infection on clustering frequency helps determine how molecular epidemiologic data can be best applied to estimate the extent of ongoing transmission of _Mycobacterium tuberculosis_ , and hence to identify optimal control strategies.\n\n【2】We explored how the magnitude and trend in the annual risk for infection influence the age-specific proportion of clustered cases and its relationship to the extent of disease attributable to recent transmission. We use a model of the transmission dynamics of _M. tuberculosis_ previously calibrated to data from the Netherlands , where isolates from all tuberculosis cases with onset since 1993 have been routinely DNA fingerprinted . We describe the general epidemiologic assumptions in the model and how it distinguishes between cases according to the DNA fingerprint pattern of the strain causing the disease episode, which is needed to calculate clustering statistics.\n\n【3】### Methods\n\n【4】Our analysis is based on a model developed recently to interpret data on clustering of DNA fingerprint patterns in the Netherlands . Equations describing the model’s formulation are provided in the Appendix.\n\n【5】##### Epidemiologic Assumptions in the Model\n\n【6】The model’s structure, parameters, and assumptions have been published . Persons are assumed to be born uninfected. Infected persons are divided into those in whom primary disease has not yet developed (defined by convention as disease within 5 years of initial infection ), and those in the “latent” class, who are at risk for endogenous reactivation or for reinfection, which can be followed by exogenous disease. Exogenous disease is here defined as the first disease episode within 5 years of the most recent reinfection; endogenous disease includes disease occurring >5 years after the most recent (re)infection event, and second or subsequent disease episodes occurring <5 years after the most recent (re)infection event. (These definitions differ slightly from those of Sutherland et al. to include the assumption that once persons have recovered from disease during the first 5 years after initial infection or reinfection, their risk of developing disease becomes the same as that of developing disease through reactivation, until they are newly reinfected.)\n\n【7】The infection and reinfection risks are assumed to be identical, but reinfection is less likely to lead to disease than is initial infection, due to some immunity induced by the prior infection . We explored the implications of four assumptions for the magnitude (and trend) in the annual risk for infection, namely, that the risk for infection 1) declined over time, as estimated for the Netherlands (from approximately 2% in 1940 to approximately 5/10,000 by 1979 ); 2) remained unchanged over time at a very low level (0.1%); 3) remained unchanged at 1%; or 4) remained unchanged at 3%. Infection risks of 1% have been found in several populations (e.g. Malawi ). Infection risks of 3% are uncommon today but have been reported in parts of South Africa . For simplicity, we assumed that persons cannot be reinfected during the period between initial infection (or reinfection) and onset of the first primary episode (or exogenous disease).\n\n【8】The risks of developing disease depend on age and sex ; they are based on previous analyses, in which we fitted predictions of disease incidence to observed notifications in the U.K . The risks of developing either a first primary episode or disease following exogenous reinfection also depend on the time since infection and reinfection, respectively . The probability that a disease episode is infectious (sputum smear/culture-positive) is age dependent  . The demography of the population described in the model is assumed to be that for the Netherlands. Analyses are restricted to respiratory (pulmonary) forms of tuberculosis, since these are far more likely than extrapulmonary forms to lead to transmission. Although additional factors such as immigration and HIV can influence the extent of clustering in complicated ways , these factors are not considered here, where the focus is upon the effect of the magnitude and trend in the annual risk for infection on clustering.\n\n【9】##### Derivation of Clustering Statistics\n\n【10】Recent studies suggest that the half-life of DNA fingerprint patterns based on IS _6110_ restriction fragment length polymorphism (RFLP, which has been used for the DNA fingerprinting conducted to date in most studies) is 2–5 years . If the molecular clock speed for IS _6110_ RFLP patterns of strains involved in latent infection (currently unknown) were to be similar, this relatively short half-life implies that most of the fingerprint patterns of the strains causing disease today differ from those that caused disease many years ago. Similarly, this short half-life implies that the _M. tuberculosis_ fingerprint types and cluster distributions in tuberculosis cases today depend only loosely upon those that existed 50 years ago. Based on this assumption, to derive clustering estimates for a given population for recent years, we designed the model to simulate the introduction and subsequent transmission of strains with new DNA fingerprint patterns from a sufficiently distant time in the past (taken to be 1950), so that a) all cases with onset in recent years involved a strain whose DNA fingerprint pattern had first appeared since then and b) no assumptions would be required about the distributions of strains that existed before 1950. The general steps in the calculations are outlined briefly below.\n\n【11】The numbers of persons of each age in each of the epidemiologic categories for 1950 were calculated by using the model, based on described equations . From 1950, each of these age-sex classes was stratified to distinguish between those who had, versus those who had not, been (re)infected since 1950. Those who had been (re)infected since 1950 were subdivided further according to the time of infection or reinfection. The transmission dynamics were tracked simultaneously for all persons with the equations described in the Appendix and elsewhere , by using time steps of 6 months and 1 year for calendar year and age, respectively.\n\n【12】In each interval, disease was assumed to develop in a proportion of infected persons, and a proportion of these disease episodes was attributed to a strain for which the DNA fingerprint pattern differed from that of the strain with which the persons were originally infected. This latter proportion depended on the time since infection , and each of the new DNA fingerprint patterns was assigned a unique identity number. Each infectious patient with onset at a given time was assumed to contact a different number of persons . The frequency distributions of the number of persons contacted by each patient were used to derive the total number of persons who were newly (re)infected at this time. The corresponding equations were then applied to this number to determine the total number of persons in whom disease developed at a later time, _T,_ among those who had been infected at time _t_ . The DNA fingerprint patterns of the strains in these diseased persons were then determined by using the frequency distribution of the number of persons contacted by each case-patient at time _t_ . These calculations are described further in the Appendix.\n\n【13】##### Estimating the Effect of the Annual Risk for Infection on Clustering as an Indicator of Recent Transmission\n\n【14】Our model was used to calculate the age-specific proportion of disease attributable to primary and exogenous disease from 1993 to 1997 for the Netherlands and for settings in which the annual risk for infection is assumed to have remained unchanged over time at 0.1%, 1% _,_ and 3% _._ Primary and exogenous disease involve disease occurring during the first 5 years after the most recent (re)infection event, although the majority of persons in whom primary or exogenous (reinfection) disease develops acquire the disease within 2–3 years . The clustering by sex and age for cases with onset in different periods between 1993 and 1997 for the Netherlands, and for settings in which the annual risk for infection is assumed to have remained unchanged over time at 0.1%, 1%, and 3%,was also calculated by using the age and sex distribution of the cases with onset in that period . For simplicity, we present age-specific levels of clustering for male patients only. Model predictions for male patients generally compared better against the observed data in the Netherlands than did those for female patients .\n\n【15】The predictive values of clustering for the identification of recent transmission were calculated as follows. The positive predictive value of clustering for identifying recent transmission in different age groups in different periods was calculated as the proportion of case-patients who were in a cluster in a given period who had been infected or reinfected <5 years before disease onset. The negative predictive value of clustering for identifying recent transmission in different age groups was calculated as the proportion of case-patients who were not in a cluster in a given period who had been infected or last reinfected >5 years before disease onset.\n\n【16】### Results\n\n【17】##### Model Predictions of the Extent of Clustering and Disease Attributable to Recent Transmission\n\n【18】As shown in Figure 3 , very different age patterns in the proportion of disease attributable to recent transmission were predicted for the Netherlands and for settings in which the annual risk for infection has remained unchanged over time. In the Netherlands, the proportion of disease attributed to recent infection decreased dramatically with age, e.g. from 100% in the young to approximately 50% and 10% for 45- to 54-year-old patients and persons >65 years of age, respectively. The proportion of disease attributed to recent reinfection was very low for all age groups (<3%). For constant infection risk settings, the predicted proportion of disease attributable to recent transmission (i.e. recent infection or reinfection) was very similar, falling from 100% in the young to 85%, 88% _,_ and 90% in the oldest age groups for the 0.1%, 1%, and 3% infection risk scenarios, respectively. On the other hand, large differences between settings were predicted in the proportion of disease attributed to initial infection or to reinfection. In all instances, the proportion attributed to reinfection was zero in the youngest age groups, but this proportion increased with age to 3%, 35%, and 80% for the 0.1%, 1%, and 3% annual infection risk assumptions, respectively. The proportion attributed to recent initial infection in these settings decreased from 100% in the young to 80%, 50%, and 15%, respectively, in old patients.\n\n【19】As shown in Figure 4A , for each setting, the overall clustering (i.e. that seen among all age groups) was predicted to increase with study duration, e.g. from 15% for the Netherlands for a 1-year period to approximately 25% for a 5-year period. The clustering predicted for all the constant infection risk scenarios was similar in magnitude for each study period and increased from 60% to 70% for a 1-year period to 75% to 85% for a 5-year period. Since the overall clustering was not predicted to increase much for study periods of more than 3 years, clustering is defined using a 3-year period in the remainder of these analyses (represented by 1993–1995). As shown in Figure 4B , the clustering predicted for each age group was similar for each of the settings in which the annual risk for infection remained unchanged over time, and declined only slightly with age, e.g. from 83% for the youngest age group to approximately 75% for the oldest age category. In contrast, for the Netherlands, the clustering was predicted to decrease dramatically with age, from approximately 75% among young case-patients to approximately 15% in very old patients. This prediction is consistent with observed data .\n\n【20】##### Reliability of Clustering as a Measure of the Extent of Recent Transmission\n\n【21】For settings in which the annual risk for infection remained unchanged over time at 0.1%, 1%, and 3%, the predicted clustering in each age group underestimated the proportion who had been recently infected or reinfected . In settings with an annual risk for infection of 0.1%, at least 90% of cases in each age group were predicted to have been recently (re)infected, whereas the proportion clustered decreased from about 85% in the youngest age group to approximately 70% for the oldest persons. For the Netherlands (described elsewhere ), clustering underestimated the proportion of disease attributable to recent transmission in the young (by up to 43%) and overestimated that for older patients (by up to 50%).\n\n【22】The positive and negative predictive values of being in a cluster, as an indicator of recent transmission, depended both on age and the study setting . For settings with a high annual risk for infection that had remained unchanged over time, model predictions suggested that most patients clustered in each age group were likely to have been recently (re)infected, corresponding to a positive predictive value of clustering for recent transmission of almost 100% in each age group . The positive predictive value was estimated to decrease with age in the Netherlands from 100% in the very young to about 20% for the oldest patients.\n\n【23】When unclustered cases were considered, the proportion of clinical case-patients who were estimated to have been infected >5 years previously was low (<5%) for young patients and increased with age for all settings, approaching 100% for patients ages >55 years in the Netherlands . Almost all case-patients of ages >55 years who were not in a cluster in the Netherlands were therefore estimated to have been infected >5 years previously and thus owed their disease to reactivation of latent foci. Of the adult case-patients who were not in a cluster in the other settings, the proportion who had been infected >5 years previously was <45%, 35%, and 20% if the annual risk for infection was 0.1%, 1%, and 3%, respectively.\n\n【24】### Discussion\n\n【25】The availability of DNA fingerprinting techniques has led to a large number of studies that measure clustering of isolates from tuberculosis cases . Most of these studies have been conducted in industrialized settings and have found relatively low levels of clustering (30% to 40%) and decreases in clustering with age. Our analyses indicate that those findings have been influenced strongly by the large secular decline in the annual risk for infection that occurred in industrialized settings during the 20th century and that very different findings are expected in settings where the annual risk for infection has changed little over time. The clustering predicted is high (>60% for 2-year periods) in such settings, similar for all age groups, and may nevertheless still underestimate the extent of disease that is due to recent transmission.\n\n【26】Our conclusions are based on a model of the transmission dynamics of _M. tuberculosis_ that includes several simplifications. The most obvious is our assumption that the risks for disease, given infection in settings in which the infection risk is high, are the same as those estimated for industrialized populations. HIV influences these risks , although its effect on clustering is not yet understood . Another simplification is our assumption that the half-life of DNA fingerprint patterns is identical for strains involved in active disease and in latent infection. If latent infections are associated with a slow rate of genetic change of the bacilli, our assumption would have led to an underestimate of clustering but would not have affected our conclusions for settings in which the annual risk for infection has remained unchanged over time, where only a small proportion of disease is attributed to reactivation of a latent infection . The effect of this assumption on clustering estimates for the Netherlands is discussed elsewhere .\n\n【27】Our finding that the overall azmount of clustering in populations with a low (constant) annual infection risk should be similar to that observed in populations with a high (constant) infection risk may appear paradoxical. Our finding follows from the fact that in such populations any decline in the proportion of disease attributable to recent primary infection with age is compensated by increases in the proportion attributable to recent reinfection with age . As a result, both the overall and age-specific predicted proportions of disease attributable to recent transmission in these populations are very similar; this finding leads to predictions that the overall and age-specific levels of clustering in these settings would also be similar.\n\n【28】Previous model-based analyses  have indicated that in industrialized settings such as the Netherlands clustering among young case-patients will underestimate the extent of disease attributable to recent transmission (because some sources of infection have onset outside the study period and because DNA fingerprint patterns can change between infection and disease onset), and clustering among old case-patients may overestimate recent transmission (because clustering among older case-patients is more likely to be attributable to their being sources of infection rather than their being recently reinfected). These analyses extend those findings and indicate that in settings in which the annual risk for infection has not changed much over time, the overall level of clustering in any given age group is likely to underestimate the extent of recent transmission . This underestimate follows from the fact that in these settings, most disease in all age groups is attributable to recent transmission, and some patients will have been infected or reinfected immediately before the study started and thus may not be in a cluster.\n\n【29】These analyses provide the first estimates of the positive and negative predictive values of clustering. Overall, these analyses highlight the fact that in settings in which the annual risk for infection has not changed greatly over time, most clustered case-patients are likely to have been recently infected or reinfected (i.e. the positive predictive value of clustering is high) . This finding suggests that in such settings, application of the “n-1” rule , which assumes that each cluster comprises an index case attributable to reactivation and the other cases result (in)directly from that case, will lead to even more unreliable estimates of the extent of recent transmission than those based on the “n” rule. Similarly, estimates of the proportion of disease attributable to reactivation will be unreliable if they are based on the proportion of patients who fail to be in a cluster in a given period.\n\n【30】Our analyses demonstrate that the properties and interpretation of clustering statistics depend strongly on the trend and magnitude in the annual risk for infection and thus will vary between settings. For example, in settings in which the annual risk for infection has remained unchanged at either a high or a low level, the age differential in clustering is likely to be small, in contrast with that in industrialized settings, and clustering is likely to underestimate the extent of recent transmission in all age groups. Given the growing importance of clustering studies, which, to date have been conducted in populations in which the annual risk for infection declined dramatically over time and is currently very low, these insights are important for an improved understanding of the natural history of tuberculosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5522579d-0ad1-4729-a11b-a16e09443dfd", "title": "Dissemination as Dialogue: Building Trust and Sharing Research Findings Through Community Engagement", "text": "【0】Dissemination as Dialogue: Building Trust and Sharing Research Findings Through Community Engagement\nAbstract\n\n【1】A fundamental feature of community-based participatory research (CBPR) is sharing findings with community members and engaging community partners in the dissemination process. To be truly collaborative, dissemination should involve community members in a two-way dialogue about new research findings. Yet little literature describes how to engage communities in dialogue about research findings, especially with historically marginalized communities where mistrust of researchers may exist because of past or present social injustices. Through a series of interactive community presentations on findings from a longitudinal study, we developed a process for community dissemination that involved several overlapping phases: planning, outreach, content development, interactive presentations, and follow-up. Through this process, we built on existing and new community relationships. Following each interactive presentation, the research team debriefed and reviewed notes to identify lessons learned from the process. Key themes included the importance of creating a flexible dissemination plan, tailoring presentations to each community group, establishing a point person to serve as a community liaison, and continuing dialogue with community members after the presentations. Core strategies for developing trust during dissemination included engaging community members at every step, reserving ample time for discussion during presentations, building rapport by sharing personal experiences, being receptive to and learning from criticism, and implementing input from community members. This process led to a deeper understanding of research findings and ensured that results reached community members who were invested in them.\n\n【2】Introduction\n\n【3】Sharing research findings with community members is a vital component of community-based participatory research (CBPR) for several reasons . First, community members deserve access to the knowledge they have made possible through participation or other forms of engagement in a study . Second, community dissemination creates opportunities to explore the implications of research findings from a local perspective . Third, dissemination allows providers to implement findings immediately and locally , potentially reducing the gap between research and practice . Finally, by fostering dialogue with those most affected by a given health issue, community dissemination aids in developing culturally relevant interventions .\n\n【4】Involving community members in discussions about new findings is particularly crucial for addressing health disparities. Those who work with or are members of a target population can shed light on factors that need to be addressed ; without such input interventions may be ineffective. For example, although research has identified culturally specific determinants of treatment adherence among black people living with human immunodeficiency virus (HIV) , interventions to improve adherence have rarely been tailored for this population; this lack of tailoring may account for the comparatively weak effects found in adherence intervention trials that have a substantial number of black participants . Unless health care providers and other community members are engaged in collaborative partnerships to generate insights about research findings, opportunities to render interventions responsive to culturally specific determinants may be missed.\n\n【5】To foster partnerships, community dissemination should involve “a two-way dialogue, not a one-way flow of information” . Two-way dissemination enables community interpretations of findings to be integrated as part of an iterative research process , and it is more likely to influence health care practice than unidirectional dissemination . However, the CBPR dissemination literature primarily emphasizes passive, unidirectional dissemination strategies (eg, press releases, policy briefs, newsletters, websites). Apart from brief allusions to workshops with community members , scant literature examines how to implement community dissemination as a two-way dialogue or address its challenges, which may include translating research terminology into lay language , and — when findings focus on historically marginalized communities — how to discuss research in ways that are sensitive to mistrust and concerns about being stigmatized .\n\n【6】This article presents lessons learned from implementing community dissemination through a series of interactive community presentations. We discuss strategies for facilitating two-way dialogue and developing trust with communities to strengthen partnerships, gain a better understanding of findings, and explore implications for culturally relevant interventions and public policy.\n\n【7】Methods\n\n【8】**The study and the Community Advisory Board.** Project Mednet was a longitudinal study that examined how social network characteristics are associated with health outcomes and behaviors of black men and women living with HIV . The study was based on CBPR principles and conducted in partnership with AIDS Project Los Angeles (APLA), a large community-based organization (CBO) with an on-site co-investigator (M.G.M.) and an in-house research program with a community advisory board (CAB). We approach CBPR as a process grounded in working relationships sustained over time rather than convened for individual studies. For example, in addition to holding an academic post, the on-site co-investigator has been employed by APLA for more than 15 years, which has enabled him to develop and strengthen trusting relationships with many local providers and clients. The CAB has been sustained across multiple studies, and it comprises 6 clients and 12 service providers from 4 local social service agencies and 4 community clinics that primarily serve black people living with HIV. The service providers include HIV treatment educators, social workers, outreach staff, and HIV/AIDS clinicians. Sixteen CAB members are black, one is white, and one is Latino. For Project Mednet, CAB members provided guidance on study planning, data collection, analysis, and dissemination, including the decision to conduct a series of interactive presentations as a core community dissemination strategy. It was also important to the team and the CAB that the research team include members of the population being engaged and that staff were hired from the local community.\n\n【9】**The community dissemination process.** We conceptualized community dissemination as an ongoing dialogue with the community involving several overlapping phases: 1) planning, 2) outreach, 3) content development, 4) presentations with discussions, and 5) follow-up. Interactive presentations were conducted during a 6-month period, concurrent with evaluation of the process with CAB members, reinterpretation of data, and revision of presentation structure and content. Eleven presentations were conducted, with an average of 20 attendees per event. Presentation venues included 3 standing community meetings of service providers and clients, such as the Los Angeles Commission on HIV. Three were held for clients at CBOs, such as an agency that provides addiction counseling and health education to black men living with HIV. Two were held for staff at CBOs, including APLA, where most of the data were collected. One presentation was conducted as continuing education training for nurses and physicians who work primarily with black people living with HIV in South Los Angeles. Finally, 2 smaller presentations were conducted for leadership at 2 CBOs that had expressed interest in the findings: a national think tank that advocates for black people living with HIV and a grass-roots organization that provides wellness-oriented social activities for people living with HIV.\n\n【10】**Identifying lessons learned.** Throughout the dissemination phase, notes were collected on the interactive presentations, including content of discussions, challenges encountered, and input on findings or the dissemination process itself. During CAB meetings and in research team debriefing sessions after the presentations, preliminary themes were identified and strategies to refine the dissemination process were developed. This process continued throughout the dissemination phase, including during later small-group meetings with community providers to identify targeted strategies for implementing findings within existing programs or upcoming grant proposals.\n\n【11】Planning a Community Dissemination Phase\n\n【12】**Build community dissemination into project aims.** A main study aim was to share findings with community stakeholders and, with their input, to identify novel intervention solutions to address medical mistrust and support adherence among black people living with HIV. Establishing this aim in the grant proposal made presentations a priority and fostered a sense of accountability among research team members. Having presentations as a formal aim also allowed us to budget funds for protected staff time to implement community dissemination. We found that funds were needed to cover staff time at multiple levels so that the research team could develop and maintain relationships with community members and CBOs. The principal investigator (L.M.B.) needed time to prepare and lead interactive presentations and engage in one-on-one communication with community members. Given her role as the research team leader, her active involvement was appreciated by community members because it demonstrated the value we placed on their input. The study coordinator (B.M.) was responsible for reaching out to CBOs, maintaining relationships, revising slide sets, coordinating logistics, conducting presentations, and facilitating discussions. A research assistant (K.A.N.) collected notes on discussions. A co-investigator (M.G.M.) also conducted interactive presentations, and an interviewer (S.J.L.) presented the study methods. Because we mainly held in-person presentations at standing meetings, expenditures for printing and space rental were minimal. However, at some presentations funds were needed for refreshments, which allowed clients and staff to attend during lunch breaks.\n\n【13】**Develop a plan with community members, but keep it flexible.** In collaboration with the CAB we developed an initial plan for dissemination, but because community partnerships should be flexible , our plan evolved iteratively in response to community feedback throughout the process. In addition to holding brainstorming sessions during CAB meetings on how to disseminate findings, we had in-depth conversations with key CAB members. We selected interactive presentations as our core dissemination strategy to create opportunities to engage community members in two-way dialogue. The research team and CAB developed a preliminary list of local venues and CBOs whose staff or clients might be interested in the findings, starting with CBOs where data had been collected or who had referred participants to us; this list evolved as new opportunities arose.\n\n【14】**Presenting at standing meetings is often more practical than planning special events.** Initially we considered conducting multi-hour Project Mednet public forums, but this approach was often impractical given the busy schedules of CBO staff members. CAB members suggested that instead we integrate our presentations into standing meetings, such as CBO staff meetings, continuing education trainings, or community forums where clients and staff from multiple CBOs gathered regularly. This approach resulted in strong attendance and produced in-depth discussions with a diverse range of groups, such as nurses working with people living with HIV or young gay men of color attending an HIV education program. We found that coordinators of many standing meetings were actively searching for relevant and timely content and that providing content for these meetings was a valued way of “giving back” to community members who had referred participants to us.\n\n【15】Reaching Out to Community Partners\n\n【16】**Designate a point person to coordinate dissemination outreach.** Once the initial plan was outlined, we reached out to CBOs to explore their interest in hosting presentations. The study coordinator served as a point person for this process. Although the whole research team was active in dissemination, having a point person allowed community members to communicate with us easily, facilitated our addressing their concerns, and helped us tailor presentations to audiences. This role required experience working in the community, an ability to convey findings accurately without jargon, and readiness to answer questions about study aims, history, challenges, and community feedback.\n\n【17】**Draw on established relationships.** Relationships are at the heart of CBPR , and the dialogues initiated through interactive presentations created opportunities to strengthen our existing relationships with CBOs and community members by demonstrating that their support — through participation, referrals, or guidance — had resulted in important research findings. Ideally, the research team should have established relationships with key community gatekeepers before the dissemination phase. Several team members had long histories of engagement with the local community, and these relationships were often deepened through CAB meetings and during data collection. For example, while obtaining medical records to track patients’ engagement in care, the study coordinator developed a stronger working relationship with a local physician who treated many of the patients. During the dissemination phase, this same physician recommended our presentation as content for a continuing education series and then helped to facilitate an engaging discussion.\n\n【18】**Conduct “pre-meetings” with gatekeepers.** As the first step in scheduling presentations, the study coordinator initiated conversations with CBO staff members to explain our findings, to explore their possible relevance for staff members or clients, and to offer to conduct a presentation and co-facilitate a discussion. The study coordinator and CBO staff members also planned logistics and strategized together about how to ensure a good fit between content and attendees. This process began by exploring the interests and expertise of likely attendees and identifying which findings fit those interests so that slides could be tailored. The conversation also addressed how to attract attendees who would benefit from the presentation and contribute to discussions. We found that CBO staff members who had close relationships with clients were often able to identify and invite individuals or groups who cared about the research topic and about fostering a productive dialogue.\n\n【19】Tailoring Content for Various Audiences\n\n【20】**Work with community members to select findings for presentation.** To select the overall content, we shared a broad set of findings with our CAB and asked what they thought would be of interest to community members. We then tailored this general set of findings to various audiences, such as community forums, client meetings, staff meetings, or continuing education trainings. We found that attendees at large community forums (eg, regional planning meetings where consumers and CBO representatives make recommendations on HIV-related services and funding) tended to be research-oriented and interested in precise descriptions of methods and policy implications. In contrast, meetings of CBO staff members or clients emphasized how findings could be pragmatically applied to services (eg, a one-page handout with tips for addressing clients’ mistrust). Continuing education trainings (eg, with nurses and physicians) allowed more time than other venues, enabling us to cover findings in more depth and facilitate more nuanced discussions. Continuing education attendees are regularly exposed to recent research findings at other trainings but rarely have opportunities to discuss the findings with the researchers who have conducted the studies.\n\n【21】Sharing the Findings and Building Trust\n\n【22】**Make presentations interactive.** To foster dialogue, we included community members and CBO staff members as co-facilitators and reserved as much time for discussion as for the presentation. Whenever possible, CBO staff members helped to facilitate discussions, which led to rich dialogues combining our familiarity with the findings with their experience in the community. A project interviewer presented the methods, which strengthened rapport with attendees because he had also established relationships with CBO staff members during the study’s recruitment phase.\n\n【23】**Share a personal story illustrating how the issue has affected your life.** Although our research team includes several black staff members, the principal investigator and the on-site co-investigator, who often led presentations, are white. During presentations they were sometimes asked why they were doing research with black communities. In exploring these questions with the CAB, it became clear that there were concerns that nonblack researchers might be motivated by factors other than the well-being of the community, such as professional opportunism or financial gain. The team understood how such concerns could arise from both the historical and ongoing marginalization of black communities in research, health care, and other contexts. To build rapport and trust, the CAB suggested that the primary presenter address mistrust at the beginning of the presentation by telling a story that conveyed why this area of research mattered personally. When we implemented this advice, the effect in the room was palpable, establishing a feeling of personal connection between the attendees and the speaker. The story was of an experience of HIV stigma that occurred when the principal investigator, as a teenager, had requested an HIV test from her doctor, who responded in a judgmental manner, saying “We don’t have patients like that here.” She shared with the audience how that experience deepened her commitment to understanding and addressing HIV stigma through research. In addition to clarifying the speaker’s motives, sharing this story helped to humanize the research topic.\n\n【24】**Cultivate a receptive attitude toward criticism.** Part of seeking community feedback on research involves receiving criticism on the study’s methods, interpretation of findings, or overall approach. At such times, it may be tempting to respond defensively, for example by suggesting that the concerns are somehow less applicable to the study or researchers in question. However, we sought to adopt a receptive stance toward such comments by responding in a respectful, nonconfrontational manner. Defensive responses may undermine trust and exacerbate community concerns about researchers by dismissing the validity and relevance of their comments. We found that it was important to inquire further, seek to better understand the concerns, respond to them, learn from them, and revise our research strategies accordingly. We also came to appreciate how such comments led to deeper and more authentic conversations about issues implicit in doing research with black communities in the United States. They demonstrated community members’ investment in the research and their community — an investment that is crucial to respect and nourish in CBPR. By the end of the project, many community members confirmed that our efforts to listen to, and learn from, criticism was among the most important factors that made the study successful from a community perspective.\n\n【25】Deepening Trust Through Ongoing Dialogue\n\n【26】**Follow up with one-on-one meetings.** After presentations, some attendees reached out to us to discuss parallels between their work as service providers and our findings. Setting up face-to-face meetings or conference calls helped to solidify new relationships and enabled us to engage in much more detailed and realistic explorations of how community partners’ work shed light on the findings, implications of the findings for their programs, and possible collaborations.\n\n【27】**Recognize the potential value of research findings for service providers.** Following presentations at community forums, some CBO leaders wanted to discuss how the findings related to their programs. One CBO used our preliminary data for a grant proposal, allowing for timely use of research findings (a key aim of CBPR). In another case, we presented data (from another study) to a local funder that was considering cutting funding for a community partners’ program — a program that our data suggested was yielding strong positive outcomes. For CBOs that develop new interventions, data can also suggest which aspects of their program may contribute most to improving outcomes. For example, staff members from one CBO discussed how their intervention, although developed intuitively through community input, was based on principles similar to those shown in our findings. To make findings most useful for providers, be prepared to develop brief reports or conduct tailored analyses.\n\n【28】**Demonstrate that suggestions are integrated into further research or programs.** In follow-up meetings, attendees and our CAB also told us that one of the most important aspects of the process was seeing that their input was implemented. Integrating feedback from presentations and follow-up conversations demonstrated to community members that the discussions were not merely academic exercises but represented mutual learning opportunities with positive effects on clients, programs, and further research.\n\n【29】Discussion\n\n【30】We developed a dissemination process for CBPR involving a series of interactive community presentations held at local CBOs. This process was intended to deepen our understanding of our study findings, support near-term implementation by providers, strengthen relationships with community partners, and elicit ideas for culturally relevant interventions. Although researchers frequently emphasize the importance of these types of aims , few report on implementation of strategies to achieve them. Through post-presentation research team debriefings and systematic review of notes from presentations, we identified a set of concrete strategies that researchers can use to engage communities in two-way conversations about research findings. These include creating a flexible dissemination plan, tailoring presentations to various community groups, establishing a point person to serve as a community liaison, and continuing dialogue with CBOs and attendees after presentations. Keys to developing trust during dissemination included engaging community members at every step, reserving ample time for discussion during presentations, building rapport by sharing personal experiences, being receptive to and learning from criticism, and implementing community members’ input.\n\n【31】We found these strategies beneficial, but our findings are limited because they reflect the dissemination process of only one project. Further research is needed to determine whether the strategies can consistently foster community trust, cultural tailoring of findings, or near-term implementation by community providers. Future research could also compare different strategies. For example, as an alternative to delivering interactive presentations at CBOs, community members could be invited to day-long forums , which have the potential advantage of increasing one-on-one dialogue between community members and researchers. However, briefer interactive presentations may be better for reaching a wide array of community members through integration into standing meetings at CBOs, continuing education trainings, and other existing settings.\n\n【32】In light of growing concern that communities do not always receive the benefits of new findings , researchers are encouraged to design studies with community dissemination in mind . Our study illustrates how community dissemination plans that include strategies for dialogue may yield benefits beyond those limited to one-way dissemination. These strategies can be incorporated into dissemination plans and research proposals, which should specify the overall approach to be used (eg, interactive presentations) and how the approach will be implemented (eg, with a point person for community relations). Plans should also be flexible to enable responsiveness to community input and reflect the concerns of the communities being studied. For example, with populations that are historically mistrustful of research, it is important to articulate how trust has been developed or will be fostered in meaningful ways.\n\n【33】Conducting dissemination as a dialogue with community members involves an investment of time and resources, but it can lead to understanding of findings that is well-grounded in community perspectives and their implications for existing and new community-based services. In addition, this type of dissemination can strengthen community–academic partnerships, which builds 1) community trust in research and 2) researchers’ understanding of community concerns. New research findings may be of interest to community members, particularly if they are able to use them in proposals and programs. Given the labor-intensiveness of community dissemination, funders who support CBPR should be prepared to designate adequate resources so that community members may genuinely take an active role in all phases of research and use data for practical purposes, such as improving their programs or influencing public policy at the community level . For example, the National Cancer Institute developed supplemental grants for dissemination that may serve as a model for targeted dissemination; and our experience suggests it is valuable for community dissemination to be incorporated into primary funding mechanisms to encourage “designing for dissemination” . Similarly, academic settings should reward efforts to facilitate community dialogue on research findings, which may yield benefits not only to individual studies but also to the overall reputation of research in diverse communities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bf6fb502-5cd3-41cb-a4de-66311110ad92", "title": "Lineage 1 and 2 Strains of Encephalitic West Nile Virus, Central Europe", "text": "【0】Lineage 1 and 2 Strains of Encephalitic West Nile Virus, Central Europe\nGeographically, West Nile virus (WNV) is the most widespread member of the Japanese encephalitis virus (JEV) complex within the genus _Flavivirus_ and the family _Flaviviridae_ . The first strain (B 956) was isolated from a human patient in the West Nile district of Uganda in 1937; later the virus was also detected in several mosquito species, horses, humans, and other hosts in Africa, Europe, Asia, and Australia (where it has been named Kunjin virus) . WNV was introduced into the United States in 1999, and it spread quickly over large parts of North America and reached Mexico . The clinical impact of WNV varies in different regions. In the Old World, WNV causes relatively mild infections with influenzalike symptoms or no apparent disease ; encephalitis and fatalities in the human population, horses, or poultry are spasmodic . In the New World, WNV exhibits increased virulence among the local wild bird populations and causes more frequent severe central nervous system symptoms and deaths in humans and horses . Although exactly how WNV was introduced into New York is unclear, phylogenetic comparison of the viral nucleic acid sequences has shown a close relationship between the American WNV isolates and strains isolated from encephalitic geese and storks in Israel in 1998 . Experimental infections of rodents indicated that the neurovirulence of WNV correlates with its genotype, and the North American strains are highly neurovirulent for mice .\n\n【1】WNV shows relatively high levels of sequence diversity. Comprehensive studies on the phylogenetic relatedness of WNV strains show that they form at least 2 main lineages . Lineage 1 is composed of WNV strains from different geographic regions, and it is subdivided into at least 3 clades. Clade A contains strains from Europe, Africa, the Middle East, and America; clade B represents the Australian (Kunjin) strains; and clade C contains Indian WNV isolates. Lineage 2 contains the B 956 prototype strain and other strains isolated so far exclusively in sub-Saharan Africa and Madagascar. In addition to the 2 major WNV lineages, we recently proposed 2 lineages for viruses that exhibited considerable genetic differences to the known WNV lineages: lineage 3 consists of a virus strain isolated from _Culex pipiens_ mosquitoes at the Czech Republic/Austria border (named Rabensburg virus), and lineage 4 consists of a unique virus isolated in the Caucasus. These 2 viruses, however, may also be considered independent flaviviruses within the JEV complex .\n\n【2】WNV has been known to be present in central Europe for a long time. Seroprevalence in humans was reported in several countries, including Hungary, and WNV strains were isolated from mosquitoes, humans, migrating birds, and rodents during the last 30 years . Until 2003, however, WNV infections in Hungary have never been associated with clinical symptoms, although a severe outbreak of West Nile encephalitis in humans was reported in 1996 and 1997 in neighboring Romania.\n\n【3】In late summer 2003, an outbreak of encephalitis emerged in a Hungarian goose flock, resulting in a 14% death rate among 6-week-old geese ( _Anser anser domesticus_ ). Based on histopathologic alterations, serologic investigations, and nucleic acid detection by reverse transcription–polymerase chain reaction (RT-PCR), WNV was diagnosed as the cause of the disease . Chronologically and geographically related to the outbreak in geese, a serologically confirmed WNV outbreak was also observed in humans, which involved 14 cases of mild encephalitis and meningitis .\n\n【4】One year later, in August 2004, a goshawk ( _Accipiter gentilis_ ) fledgling showed central nervous system symptoms and died in a national park in southeastern Hungary. When histopathologic methods and RT-PCR were used, WNV antigen and nucleic acid were detected in the organs of the bird. Furthermore, the virus was isolated after injection of suckling mice. Here we report the sequencing and phylogenetic results of these 2 encephalitic WNV strains that emerged recently in central Europe.\n\n【5】### Materials and Methods\n\n【6】Brain specimens from one 6-week-old goose, which died during the encephalitis outbreak in a Hungarian goose flock, and brain samples from a goshawk, which also died from encephalitis, were used for WNV nucleic acid determination. The brain samples were homogenized in ceramic mortars by using sterile quartz sand, and the homogenates were suspended in RNase-free distilled water. Samples were stored at –80°C until nucleic acid extraction was performed.\n\n【7】Viral RNA was extracted from 140 μL of brain homogenates by using the QIAamp viral RNA Mini Kit (Qiagen, Hilden, Germany) according to the manufacturer's instructions. First, a universal JEV-group specific oligonucleotide primer pair designed on the nonstructural protein 5 (NS5) and 3´-untranslated regions (UTR) of WNV (forward primer: 5´-GARTGGATGACVACRGAAGACATGCT-3´ and reverse primer: 5´-GGGGTCTCCTCTAACCTCTAGTCCTT-3´ ; ) was applied on the RNA extracts in a continuous RT-PCR system employing the QIAGEN OneStep RT-PCR Kit (Qiagen). Each 25-μL reaction mixture contained 5 μL of 5× buffer (final MgCl 2  concentration 2.5 mmol/L), 0.4 mmol/L of each deoxynucleoside triphosphate, 10 U RNasin RNase Inhibitor (Promega, Madison, WI, USA), 20 pmol of the genomic and reverse primers, 1 μL enzyme mix (containing Omniscript and Sensiscript Reverse Transcriptases and HotStarTaq DNA polymerase) and 2.5 μL template RNA. Reverse transcription was carried out at 50°C for 30 min, followed by a denaturation step at 95°C for 15 min. Thereafter, the cDNA was amplified in 40 cycles of heat denaturation at 94°C for 40 s, primer annealing at 57°C for 50 s, and DNA extension at 72°C for 1 min, and the reaction was completed by a final extension for 7 min at 72°C. Reactions were performed in a Perkin-Elmer GeneAmp PCR System 2400 thermocycler (Wellesley, MA, USA) and in a Hybaid PCR Sprint thermocycler (Thermo Electron Corporation, Waltham, MA, USA).\n\n【8】After RT-PCR, 10 μL of the amplicons was subjected to electrophoresis in a 1.2% Tris acetate-EDTA-agarose gel at 5 V/cm for 80 min. The gel was stained with ethidium bromide; bands were visualized under UV light and photographed with a Kodak DS Electrophoresis Documentation and Analysis System using the Kodak Digital Science 1D software program (Eastman Kodak Company, Rochester, NY, USA). Product sizes were determined with reference to a 100-bp DNA ladder (Promega).\n\n【9】Where clear PCR products of the previously calculated sizes were observed, the fragments were excised from the gel, and DNA was extracted by using the QIAquick Gel Extraction Kit (Qiagen). Fluorescence-based direct sequencing was performed in both directions on PCR products. Sequencing of PCR products was carried out with the ABI Prism Big Dye Terminator cycle sequencing ready reaction kit (Perkin-Elmer), according to the manufacturer's instructions, and an ABI Prism 310 genetic analyzer (Perkin-Elmer) automated sequencing system. Nucleotide sequences were identified by Basic Local Alignment Search Tool  search against gene bank databases. Based on the sequence information obtained from the amplification products, complete WNV sequences that exhibited the highest nucleotide identities with the Hungarian genotypes were selected from the GenBank database to design primers that amplify overlapping RT-PCR products covering the entire genome of the strains. Oligonucleotide primers were designed with the help of the Primer Designer 4 for Windows 95 (Scientific and Educational Software, Version 4.10; Microsoft, Redmond, WA, USA) and were synthesized by GibcoBRL Life Technologies, Ltd. (Paisley, Scotland, UK). Detailed information on all primers is in the Tables A1 and A2 . PCR amplification products were directly sequenced in both directions; the sequences were compiled and aligned to complete genome sequences of selected representatives of WNV lineages 1a, 1b, 2, and putative lineages 3 and 4 (listed in Table ). Phylogenetic analysis was performed by using the modified neighbor-joining method (ClustalX ; ), and trees were constructed to demonstrate the relationship between the Hungarian WNVs and other WNV strains .\n\n【10】The nucleotide sequences of the Hungarian WNV strains goose-Hungary/03 (Hu03) and goshawk-Hungary/04 (Hu04) were submitted to the GenBank database. They are available under accession numbers DQ118127 and DQ116961, respectively.\n\n【11】### Results\n\n【12】In this study, the complete genome sequences of WNV strains derived from a 6-week-old goose, which died in 2003 during an outbreak of encephalitis in a Hungarian goose flock (strain goose-Hungary/03), and from a goshawk, which also died from encephalitis in the same region 1 year later (strain goshawk-Hungary/04), were determined, aligned, and phylogenetically analyzed. The genome of the goose-Hungary/03 strain is composed of 10,969 nucleotides (nt) and contains 1 open reading frame between nucleotide positions 97 and 10,398, coding for a 3,433 amino acid (aa)–long putative polyprotein precursor. The complete genomic sequence of the virus was subjected to a BLAST search against gene bank databases. The highest identity rates (98% at the nucleotide and 99% at the amino acid level) were found with WNV strains isolated in 1998 in Israel and in 1999 in the United States. In addition, phylogenetic analysis was performed to indicate the relationships between the Hungarian goose–derived WNV strain and selected representatives of WNV clades and clusters. The resulting phylogenetic tree  confirmed the results of the BLAST search, i.e. the Hungarian goose–derived WNV strain is clustering close to the previously mentioned WNV strains isolated in the United States and Israel, which belong to lineage 1a of WNV. Other European WNV strains (isolated in Italy, France, and Romania) are more distant to the Hungarian strain; they form a separate cluster consisting of a Romanian/Russian and a French/Italian subcluster.\n\n【13】The complete nucleotide sequence of the goshawk-Hungary/04 WNV strain is composed of 11,028 nt and contains 1 open reading frame between nucleotide positions 97 and 10,401, coding for a 3,434-aa putative polyprotein precursor. In BLAST search, the strain showed the highest (96% nt and 99% aa) identity to the WNV prototype strain B 956. Consequently, as the phylogram also indicates , this virus belongs to lineage 2 of WNV. Alignments of the available partial sequences from the E protein coding regions of other representatives of this cluster showed even higher identities (97%–98% nt and 100% aa) with WNV strains isolated in central Africa in 1972 (AnB3507, AF001563) and in 1983 (HB83P55, AF001557), respectively .\n\n【14】More recently (in early August 2005), additional lethal cases of encephalitis occurred in birds of prey in the same place in which the goshawk died of West Nile encephalitis in 2004, involving up to a total of 3 goshawks and 2 sparrow hawks ( _A. nisus_ ); 2 of the goshawks and 1 sparrow hawk died. Preliminary investigations detected WNV-specific nucleic acid in the brains of the birds. The partial nucleotide sequence of the 2005 virus (1,000 bp at the NS5´–3´-UTR regions) showed 99.9% identity with the goshawk-Hungary/04 strain (only 1 substitution at nucleotide position 9,376 \\[g→a\\] has been observed, which did not influence the putative amino acid sequence). Additional observation of the outbreak and investigations of the cases are in progress.\n\n【15】### Discussion\n\n【16】The primary aim of our investigations was to show the genetic relatedness of the WNV strains detected in Hungary in the last 2 years and to estimate their clinical and epidemiologic impact. The phylogenetic analysis emphasizes the close genetic relationship of the goose-Hungary/03 strain with a WNV strain isolated in Israel in 1998 and the WNV strain introduced in New York in 1999, since the 3 WNVs form 1 single cluster within clade 1a of lineage 1. These strains caused outbreaks in birds, humans, and horses. Previous European WNV isolates exhibited lower identity values, e.g. the strain that was responsible for the Romanian outbreak(s) in 1996 and 1997 showed only 96% nt identity with the Hungarian goose-2003 strain, and in the phylogenetic tree the other European isolates form a separate cluster consisting of 2 subclusters . The earliest representatives of the Israel/USA/goose-Hungary/03 cluster were reported by Malkinson et al. from ill and dead white storks ( _Ciconia ciconia_ ) in Israel in 1998. These storks, however, had hatched in central Europe, and during their autumn migration southwards, strong winds had blown them off course, from their usual route to Africa, to southern Israel. Malkinson et al. suspected that these birds introduced the neurovirulent genotype of WNV to Israel from their hatching place. The wetlands of southeastern Hungary are foraging and nesting habitats for storks and many other wild bird species, and the goose farm, where the WNV outbreak occurred in 2003, is located in this region. These facts, together with the close phylogenetic relatedness of the Israeli/US/Hungarian WNV strains, strongly support the theory that storks carried the neurovirulent WNV strain from central Europe (that is, from Hungary) to Israel, which sheds new light on the introduction of WNV to New York. This virus could have originated in Israel (which is the generally accepted although not proven theory) or central Europe. In both cases, however, the virus seems to have its true origin in Europe. In a recent publication, Lvov et al. suggested that WNV could have been introduced into New York by ships traveling from Black Sea ports .\n\n【17】When a WNV infection was detected in 2004 in a goshawk fledgling, which died from encephalitis in the same region of Hungary in which the outbreak in geese and humans occurred during the previous year, we anticipated a WNV strain more or less identical to the genotype detected there in 2003. The genomic sequence of this strain was not closely related to the sequence of the WNV strain detected in geese in the year before, however, but belonged to the group of central African lineage 2 WNV strains. A closely related strain from this cluster (ArB3573, AF001565, and AF458349) was identified as a neuroinvasive strain of WNV in a mouse model . To our knowledge, this report is the first on the emergence of a lineage 2 WNV strain outside Africa. Migratory birds that had overwintered in central Africa probably introduced this exotic strain to the wetlands of Hungary. On the other hand, as the goshawk is not a migratory species, and infection occurred in August, the African WNV strain must have already successfully adapted to local mosquito vectors. Consequently, this neurotropic, exotic WNV strain may become a resident pathogen in Europe with all the possible public health consequences.\n\n【18】Our results indicate that the WNV strains that emerged in 2 consecutive years and caused avian deaths in Hungary are epidemiologically unrelated. Genetically distinct WNV strains are circulating simultaneously yet independently in local birds and thus most likely also in local mosquito populations within the same region. They cause sporadic cases of encephalitis and also raise the possibility of spreading to other European countries or even to other continents, as happened in 1999 with another WNV strain, which resulted in a public health catastrophe in America.\n\n【19】In addition to the above 2 novel WNVs, we recently characterized another novel flavivirus of so far unknown human pathogenicity named Rabensburg virus, which has been isolated from _Culex pipiens_ mosquitoes in 1997 and 1999 at the Czech Republic–Austria border, only a few hundred kilometers from the region where the Hungarian WNVs emerged. After the entire genome was sequenced, Rabensburg virus turned out to represent either a new (third) lineage of WNV or a novel flavivirus of the JEV group . Thus, several distinct WNV strains seem to circulate in central Europe. In 2001 another flavivirus of the JEV group, Usutu virus, which has never previously been observed outside Africa, emerged in Austria and resulted in deaths in several species of birds, especially Eurasian blackbirds ( _Turdus merula_ ) . This virus became a resident pathogen in Austria and continues to disperse and cause deaths in blackbirds and other species of birds .\n\n【20】The snowy winter and rainy spring of 2005 resulted in serious floods in the area in which the Hungarian WNV strains were identified. Since the floodplains and polders were under water, the conditions for mosquito development were ideal. The summer was also very rainy, which resulted in more floods in the region and continuous mosquito gradation. The most recent data imply that the lineage 2 WNV strain may have overwintered in Hungary, causing several clinical cases of encephalitis in _Accipiter_ species in 2005 as well.\n\n【21】The routine diagnostic techniques in most of the European public health and veterinary laboratories are designed to detect lineage 1 WNV strains. In a recent PCR external quality assurance multicenter test, <40% of the involved laboratories could detect lineage 2 WNV strains . Therefore, a major goal of this article is to increase the scientific and public awareness of this potential public health threat for Europe and, perhaps, America. Furthermore, comprehensive investigations on the occurrence, ecology, and epidemiology of the different WNV strains circulating in central Europe, as well as the development of monitoring and surveillance programs, must be of highest priority. One may also speculate on environmental factors, such as climate change or global warming, that may have enhanced the recent emergence of viruses, which had previously been restricted to Africa, in new habitats and continents. Improved observation, reporting, and detection methods have also contributed to the apparent increasing emergence of these viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1fa4464a-8ac9-4e2c-bda0-a6410f86e002", "title": "Epidemiology and Prevention of Pediatric Viral Respiratory Infections in Health-Care Institutions", "text": "【0】Epidemiology and Prevention of Pediatric Viral Respiratory Infections in Health-Care Institutions\nNote: Corrections made to abstract on 3/9/01.\n\n【1】Infection control professionals worldwide rely on the Guideline for Isolation Precautions in Hospitals promulgated by the Hospital Infection Control Practices Advisory Committee of the Centers for Disease Control and Prevention . This widely venerated document has assumed almost ecclesiastical authority. The guidelines have been framed carefully to reflect current evidence and opinion on the modes of transmission of nosocomial pathogens, and it is this rigorous evidence-based process that insures their credibility. However, scrutiny of guidelines addressing the nosocomial spread of viral pathogens reveals the fragile data on which many of the recommendations are based.\n\n【2】Evidence on modes of transmission of viruses tends to be the most fragmentary and unconvincing. When the first Decennial Conference was held, viral diagnostics was in its infancy, and few hospital clinical laboratories were equipped to assist infection control professionals in understanding the epidemiology of nosocomial viral disease. Moreover, our current knowledge about the spread of infection by droplets and droplet nuclei is a relatively recent phenomenon. It was not that long ago that all infections were thought to be spread by miasms, those putrid vapors emanating from decomposing organic matter and environmental filth. William Farr, an excellent epidemiologist and close colleague of Florence Nightingale, firmly believed that the 1849 cholera outbreak in London was caused by miasms rising from the fetid River Thames. Malaria (literally from the Italian root, mal aria, or \"bad air\") and yellow fever were attributed to miasms before their mosquito vectors were discovered near the turn of the century. Indeed, some authorities predicted with confidence that these diseases, which killed thousands of workers who were trying to dig the Panama Canal, would be eradicated as soon as the canal trench was filled with water, sealing over the miasm-generating tropical ooze. Not until mid-century did Wells et al. at Johns Hopkins demonstrate that tiny droplet nuclei could convey infectious microorganisms over long distances from patient to patient .\n\n【3】What, then, do we know about the transmission of common, clinically important nosocomial viruses? Studies of three viruses of importance to pediatric hospital epidemiologists (respiratory syncytial virus \\[RSV\\], influenza virus, and rhinovirus) illustrate that modes of transmission have been clarified somewhat but that serious gaps in our knowledge persist. Many of these studies should provide inspiration for young hospital epidemiologists and infection control professionals. Almost without exception, they were performed by hard-nosed investigators who had little, if any, external funding--investigators who exploited serendipitous events or devised and conducted original studies on a shoestring.\n\n【4】### RSV\n\n【5】RSV is the most important cause of respiratory infection in young children worldwide, infecting virtually every child in the first few years of life. Immunity is feeble and fleeting, and repeated infections are the rule. One in every 100 or 200 infected infants requires hospitalization, usually for bronchiolitis. Therefore, pediatric hospital wards are flooded with patients with community-acquired RSV every winter, and failure to follow fastidious infection control procedures inevitably leads to nosocomial transmission . RSV is, in fact, one of the \"perennial weeds\" on pediatric wards that Caroline Breese Hall discussed at this same conference 20 years ago . The consequences of RSV infection can be especially dire for children with underlying conditions such as prematurity, cardiac and pulmonary disease, or immunosuppression . Nosocomial RSV infection in immunocompromised adults results in prolonged, substantial illness and even death . RSV also takes a heavy toll on members of the nursing and medical staff, with attack rates in some studies approaching 50% . Bronchiolitis does not develop in health-care providers because, as adults, they have considerably larger airways than infants; however, severe colds and reactive airway disease do develop . Because winter is the busiest time of year on pediatric wards, ill staff members seldom take time off to recuperate, thus serving as efficient vectors in the chain of disease transmission.\n\n【6】Since RSV is a respiratory virus, one might be tempted to speculate that it is transmitted primarily by droplet nuclei or droplet contact. However, Hall et al. demonstrated clearly that contact transmission predominates . Freshly infected infants, who were producing copious secretions, were placed in a crib in a room reserved for the study. Volunteers were brought into the room and assigned to one of three groups. \"Cuddlers\" performed routine care, picked the baby up, and played with the child. \"Touchers\" had extensive contact with objects in the baby's environment, which had been contaminated heavily with secretions. \"Sitters\" sat right next to the crib for 3 hours but did not touch anything in the baby's environment. None of the 14 sitters developed RSV infection, but five of the seven cuddlers and four of the 10 touchers became ill.\n\n【7】Infants secrete enormous concentrations of RSV, often more than 10 7  /mL of nasal discharge, and the concentration of virus diminishes only slowly over a period of days . Moreover, RSV survives well on fomites; for example, virus can be cultured for >5 hours on impervious surfaces such as bed rails . Thus, care givers have numerous opportunities to contaminate their hands during routine care, and unless they wash their hands, virus will be transmitted by indirect contact to other infants. Furthermore, symptomatic infection has a high probability of developing in care givers who touch their eyes or nose with contaminated fingers.\n\n【8】Numerous studies have evaluated potential strategies to control nosocomial transmission of RSV. Gowns and masks were studied before the modes of transmission of RSV were understood fully . These studies, which were underpowered, did not detect a beneficial impact on the rate of cross-infection. Hall's group, recognizing that the eyes are an unprotected portal for inoculation of virus in health-care workers, evaluated especially designed eye-nose goggles that ward staff could wear when caring for infants infected with RSV . Although these goggles reduced the rate of infection in care givers and infants to 5% and 6%, respectively, the goggles were not well accepted by the staff and eventually were abandoned.\n\n【9】Studies at Children's Hospital, Boston, provide considerable support for the key role of contact with contaminated secretions in RSV transmission, as well as the value of wearing gowns and gloves when caring for infected patients . Surreptitious surveillance of compliance with gown and glove precautions on a general pediatric ward documented adherence in only 38.5% of encounters with ill infants. When open monitoring, education, and feedback of nosocomial infection rates were introduced, compliance reached levels as high as 95% and remained very good even after surreptitious surveillance was reintroduced. The rate of nosocomial RSV infection fell from 6.4 to 3.1 cases per 1,000 patient days. The magnitude of the effect was by far the greatest at the peak of the winter epidemic in the community, when the ward was crowded with infected infants. Thus, simple barrier precautions, including wearing gloves when touching contaminated objects, proved extremely effective in limiting RSV transmission. Of course, it is possible that excellent compliance with handwashing might obviate the need for gloves, as is the case for all nosocomial infections transmitted from patient to patient by contaminated hands. Isaacs et al. found that handwashing and cohorting were effective in reducing the nosocomial infection rate. For RSV, using a hand antisepsis agent that contains detergent or alcohol is critical. Aqueous chlorhexidine without detergent has poor activity against RSV .\n\n【10】Some investigators have advocated performing rapid tests for RSV on all symptomatic infants during the annual RSV season, cohorting RSV-positive patients, and placing them on gown and glove precautions. Madge found that this approach was more effective than gowns and gloves or cohorting alone , although compliance was not measured. Snydman noted a reduction in nosocomial infection in a newborn nursery when rapid testing was combined with cohorting, visitation restrictions, and gowns, gloves, and masks . However, the cost-effectiveness of routinely testing all symptomatic infants for RSV remains to be demonstrated conclusively. Once the virology laboratory has documented that the RSV season has started, a child with bronchiolitis will likely have RSV, and screening only children who have atypical symptoms may be sufficient.\n\n【11】Recently, investigators using polymerase chain reaction (PCR) to detect RSV RNA suggested that RSV might be transmitted over considerable distances by air . RNA was found in air samples taken as far as 7 m from the bedside of infected patients for up to day 7 of hospitalization. However, a positive PCR result does not prove that infectious virus is present, and it seems premature to use such data to refute excellent epidemiologic studies by several groups of investigators documenting the primary importance of contact transmission.\n\n【12】### Influenza\n\n【13】Influenza is a substantial threat to hospitalized patients despite the availability of a relatively effective vaccine and two classes of drugs (M2 ion channel inhibitors and neuraminidase inhibitors) shown to prevent infection in clinical trials . Although influenza is widely viewed as affecting primarily elderly patients and adults with coexisting illnesses or conditions, such as chronic pulmonary and cardiac disease, nosocomial transmission has been well documented in young children . Perhaps nosocomial disease is less frequently diagnosed in hospitalized children because infants are unable to articulate many of influenza's characteristic symptoms, and influenza often presents simply as an episode of fever in this population.\n\n【14】The proper isolation procedures for hospitalized patients with influenza are controversial. Infection can likely be transmitted by direct and indirect contact, as well as by droplet contact. Airborne spread by droplet nuclei has sparked controversy, since true airborne transmission would best be controlled by isolating patients in rooms with negative air pressure and requiring staff to wear masks on entering the room. Such precautions would be costly and difficult to implement at the height of an influenza outbreak.\n\n【15】What is the evidence for airborne transmission of influenza? The explosive nature of influenza outbreaks supports airborne transmission. Some investigators have even suggested that the rapid intercontinental transmission of influenza can be mediated by transport of aerosolized virus on air currents over hundreds to thousands of kilometers in low-pressure centers with frontal waves . However, data substantiating the airborne theory of transmission are relatively sparse. Perhaps the most compelling data come from animals models of influenza. Mice inoculated with influenza virus readily transmitted infection to susceptible animals from which they had been separated by double wire screens . The attack rate increased at low relative humidity, as would be expected, since virus suspended in aerosolized droplet nuclei survives much longer at lower humidity. Moreover, transmission occurred more frequently when the ventilation in the chamber housing the mice was poor, as Wells established is typical of diseases spread by the airborne route. In a ferret influenza model, infected ferrets transmitted influenza to uninfected ferrets separated by a 9-foot duct with two 90 bends . Large droplets certainly would not be able to negotiate such curves, whereas droplet nuclei typically can.\n\n【16】A natural experiment in patients at the Veterans Administration Hospital in Livermore, California, can be viewed as the human counterpart of these animal experiments . One building housing 150 patients with tuberculosis and chronic pulmonary disease was ventilated by UV light-irradiated air, whereas another part of the hospital housing 250 tuberculosis patients received nonirradiated air. During the 1957-58 influenza season, the attack rate in patients in the irradiated building (as confirmed serologically) was 2%, but the attack rates among patients and staff in the nonirradiated area were 19% and 18%, respectively.\n\n【17】Probably the most dramatic example of airborne spread in humans occurred during an airplane flight from Anchorage to Kodiak, Alaska . At an intermediate stop in Homer, Alaska, the plane had mechanical difficulty and remained on the tarmac for several hours with an inoperative ventilation system. A young woman had boarded the flight in Homer and within 15 minutes developed full-blown symptoms of acute influenza. A point-source outbreak of influenza ensued, and 72% of the 54 passengers became ill within 72 hours. The attack rate was highest in passengers who remained on the crippled plane the longest, and the six passengers who deplaned immediately remained well. Although the passengers who stayed on the plane moved about at will, influenza developed in few of those who had close contact with the index patient.\n\n【18】Since available evidence tends to support airborne transmission of influenza, attempting to place infected patients on precautions suitable for protecting susceptible patients and staff from virus-laden droplet nuclei seems prudent. Of course, improved compliance with current recommendations for immunizing health-care workers remains the key to influenza control in the hospital. Most facilities will be severely challenged if they try to isolate all patients with symptoms compatible with influenza.\n\n【19】### Rhinovirus\n\n【20】Although nosocomial rhinovirus infection is not as substantial a problem as RSV and influenza on pediatric wards, it can have serious sequelae in premature neonates and children with chronic diseases or immunosuppression . For example, in another session at this decennial meeting, Huskins and his colleagues at Children's Hospital, Boston, report an outbreak of rhinovirus infection at a pediatric chronic-care facility that was associated with considerable illness and death. However, there is another reason to discuss the transmission of rhinovirus-namely, that this pathogen demonstrates the difficulty in proving conclusively how respiratory viruses are transmitted.\n\n【21】The common cold is a profound nuisance in everyday life, although seldom a cause of serious illness. The average child can expect to have four to eight episodes per year, and adults three to five infections. Many viruses, such as parainfluenza, RSV, and coronavirus, can produce similar symptoms, but rhinovirus is by far the most frequent etiologic agent. Repeated colds are virtually guaranteed because there are >100 distinct rhinovirus serotypes, and infection with one serotype does not confer substantial immunity against the others.\n\n【22】A prodigious volume of work at the Common Cold Research Unit in Salisbury, England, following World War II established that colds could be produced by inoculating secretions into the nose or eye of volunteers . These rather crude experiments were replicated with nasal inoculation of small concentrations of rhinovirus once the specific viral agents that cause the common cold were elucidated . Presumably, therefore, persons might acquire rhinovirus by touching their nasal or ocular mucosa with contaminated fingers. A study by Hendley et al. at the University of Virginia demonstrated that health-care workers are not immune to practices that might promote self-inoculation . One third of grand-rounds attendees picked their nose, and one in 2.7 rubbed their eyes during a 1-hour lecture. Subsequent work demonstrated that it was difficult to transmit rhinovirus by kissing , and that exposure to cold did not increase the likelihood of \"catching a cold\" .\n\n【23】These studies could not answer the central question of whether rhinovirus is transmitted primarily by direct contact, indirect contact, droplet contact, or droplet nuclei. Unfortunately, considerable additional investigation has not resolved the issue completely . Essentially, two experimental approaches, both highly contrived, have come to different conclusions. Work by Hendley and Gwaltney at the University of Virginia generally has supported transmission by hand contact and self-inoculation, while experiments by Dick at the University of Wisconsin have favored spread by large droplets, droplet nuclei, or both.\n\n【24】The Virginia group demonstrated that adults with experimental rhinovirus colds readily contaminated their hands and that rhinovirus could be recovered from 43% of plastic tiles they touched with their contaminated fingers . Adults with natural rhinovirus colds contaminated their hands in 39% of cases, and virus was found on 6% of objects in their homes . Virus could survive from a few hours to as long as 4 days on nonporous surfaces, and for at least 2 hours on human skin . Volunteers who had contact with contaminated objects or with fingers of persons with rhinovirus colds had a high rate of infection when they intentionally touched their eyes or nose. Infection generally could be prevented by treating contaminated surfaces with disinfectant or applying iodine to fingers .\n\n【25】In a labor-intensive, randomized clinical trial, the Virginia group found that treating mothers' fingers with iodine reduced the rate of secondary infection . Specifically, as soon as a cold occurred in another member of the family, mothers were instructed to dip their fingers in iodine or placebo when they awoke in the morning, every 3 to 4 hours during the day, and after activities that might wash the iodine from the skin. The investigators counted on the well-established residual activity of iodine to kill virus on contact. Over the 4-year study period, the secondary attack rate for colds in the intervention group was 7%, versus 20% in the control group. In the iodine-treated group, no confirmed rhinovirus infection occurred in susceptible mothers who had been exposed to 11 index cases. In contrast, five infections occurred after 16 exposures in the placebo group, although this difference was not significant.\n\n【26】These studies provide considerable evidence for indirect contact transmission by contaminated fomites and fingers. In other experiments, the Virginia investigators found little support for transmission via large respiratory droplets or droplet nuclei. Exposure of susceptible volunteers to highly symptomatic volunteers across a small table (droplet contact and droplet nucleus transmission) or a double-wire barrier (droplet nucleus spread) resulted in infections in 1 of 12 and zero of 10 subjects, respectively . These rates of transmission were far less than the 11 infections among 15 persons (73%) who self-inoculated their mucous membranes with contaminated fingers.\n\n【27】Meanwhile, the Wisconsin group was developing models to study transmission of rhinovirus colds, building on observations showing high attack rates among men crowded together in a small hut in Antarctica . In one such model, symptomatic volunteers were housed with susceptible volunteers in a room approximately 12-by-6-by-3 m . The subjects played various board, card, and video games during the study period. Since viral titers in nasal secretions fall as symptoms diminish, volunteers were replaced with highly symptomatic persons as soon as they experienced reduced rhinorrhea or sneezing. The average length of exposure required for transmission was very high, 200 hours of exposure to achieve a 50% attack rate. Based on these results, Dick et al. suggest that exposure times in the Virginia studies were too short to exclude droplet and airborne transmission.\n\n【28】In additional experiments, the Wisconsin group extended these studies by having volunteers play poker for 12 hours while sitting at round tables . Three experiments were performed involving 24 symptomatic \"donors\" and 36 susceptible \"recipients.\" Half of the recipients were fitted with restraints, either arm braces that allowed them to reach their cards but not touch their face, or a plastic shield that left their hands free but did not allow them to reach their eyes or nose. Despite these barriers, the attack rates were 56% and 67%, respectively, strongly favoring transmission by air since self-inoculation was impossible. Moreover, when 12 additional susceptible volunteers were brought to a separate room to play poker with chips and cards that were literally soaked with contaminated secretions from donors, no rhinovirus infections occurred. In addition, little virus was found on the chips and cards. The Wisconsin group suggested that the relatively high attack rates seen in the self-inoculation studies conducted by the Virginia group might be attributable to intensive exposure to fresh wet secretions (e.g. the volunteers literally blew their noses into their hands).\n\n【29】The above studies provide only a glimpse of the extensive literature on the transmission of rhinovirus colds, but controversy still simmers. The prudent person probably will wash his or her hands after shaking hands with someone who has a cold or after touching environmental objects potentially contaminated with relatively fresh secretions. Alcohol-based, waterless antiseptics are ideal for this purpose. Although droplet contact or airborne transmission of rhinovirus infection is possible, prolonged and close exposure is apparently required.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4dd4e1aa-f250-45e9-8496-b7d08c080147", "title": "Bartonella quintana and Rickettsia felis in Gabon", "text": "【0】Bartonella quintana and Rickettsia felis in Gabon\nBartonellae are gram-negative bacteria that cause several human diseases and are transmitted by various arthropods, such as lice, ticks, and fleas . _Bartonella quintana_ is a worldwide fastidious bacterium that infects humans and belongs to the alpha subgroup of the _Proteobacteria_ . Recent reports suggest that humans are the natural reservoir of _B_ . _quintana_ and that the human body louse is the vector . However, we have recently reported molecular detection of several _Bartonella_ species, including _B_ . _quintana_ , in _Ctenocephalides felis_ fleas from France, which suggests that fleas may be important vectors of human disease . Fleas are found worldwide on mammals and are vectors of several major zoonoses, including plague caused by _Yersinia pestis_ , murine typhus caused by _Rickettsia typhi_ , and fleaborne spotted fever caused by _R_ . _felis_ . More than 2,000 species of fleas exist worldwide. While some species are highly host specific, others are more catholic and will feed on numerous hosts, especially in the absence of their preferred host . Several flea species, including _Pulex irritans_ , _C_ . _canis_ , _C_ . _felis_ , _Ceratophyllus gallinae_ , _Ceratophyllus columbae_ , and _Archaeopsylla erinacei_ , may infest humans. In this study, we collected _P_ . _irritans_ (human fleas) and _C_ . _felis_ fleas on a pet monkey in Gabon and report for the first time the molecular detection of _B_ . _quintana_ in _P_ . _irritans_ .\n\n【1】### The Study\n\n【2】Four fleas collected from a pet monkey ( _Cercopithecus cephus_ ) in Franceville, Gabon, were stored in 70% alcohol and sent to the World Health Organization (WHO) Collaborative Center for Rickettsial Reference and Research in Marseille, France, where molecular studies were performed in April 2005. Fleas were rinsed with distilled water for 10 min and dried on sterile filter paper in a laminar flow hood. Preliminary entomologic identification was performed by using reference taxonomic keys as previously reported .\n\n【3】Fleas were crushed individually in sterile Eppendorf tubes with the tip of a sterile pipette. DNA was then extracted by using the QIAamp Tissue Kit (Qiagen, Hilden, Germany) according to manufacturer's instructions. Rickettsial DNA was detected by polymerase chain reaction (PCR) with primers targeting the citrate synthase gene ( _gltA_ ) as previously described . _R_ . _montanensis_ DNA was used as positive control, and negative controls consisted of laboratory uninfected flea DNA. _Bartonella_ DNA was detected by PCR with 3 sets of primers targeting the intergenic spacer (ITS) gene, and the _B_ . _quintana_ spacers 336 and 894 as previously described . _B_ . _elizabethae_ DNA was used as positive control and uninfected fleas as negative controls. Additionally, fleas were identified at the species level after amplification and sequencing of a portion of the 18S rDNA gene as previously described . PCR products were purified, and DNA sequencing was carried out by using the d-Rhodamine Terminator cycle sequencing ready reaction kit with Amplitaq Polymerase FS (Perkin-Elmer, Coignieres, France) as described by the manufacturer. For all PCR products, sequences from both DNA strands were determined twice. Sequencing products were resolved by using an ABI 3100 automated sequencer (Perkin-Elmer). Sequence analysis was performed by using the software package ABI Prism DNA Sequencing Analysis Software version 3.0 (Perkin-Elmer). All obtained sequences were compared with those available in GenBank by using the nucleotide-nucleotide BLAST (blastn) program .\n\n【4】Using morphologic taxonomic keys, 3 fleas were identified as _P_ . _irritans_ and 1 flea as _Ctenocephalides felis_ . These findings were unambiguously confirmed when a 331-bp fragment of the 18S rDNA gene showed 99.7% and 99.4% homology with previous sequences of _C_ . _felis_  and _P_ . _irritans_ . When _gltA_ primers were used, _R_ . _felis_  was detected in the _C_ . _felis_ , whereas the _P_ . _irritans_ as well as negative controls were negative. Using the ITS primers for _Bartonella_ spp. we detected PCR products in the 3 _P_ . _irritans_ fleas, whereas the _C_ . _felis_ flea and negative controls were negative. By sequencing the ITS gene–amplified fragments from these 3 fleas, we identified _B_ . _quintana_ . Two PCR procedures targeting specific _B_ . _quintana_ spacers previously described  were carried out to confirm the results. By using these primers, _B_ . _quintana_ type 1 sequence was obtained for the spacer 336  and _B_ . _quintana_ type 2 sequence for the spacer 894 . Thus, according to current guidelines for _B_ . _quintana_ typing , we have amplified genotype 2 of _B_ . _quintana_ .\n\n【5】### Conclusions\n\n【6】We present here the first molecular detection of _R_ . _felis_ in sub-Saharan Africa, Gabon . To date, 4 species of fleas have been associated worldwide with _R_ . _felis_ including _C_ . _felis_ , _C_ . _canis_ , _P_ . _irritans_ , and _Archeopsylla erinacei_ . Thus, the amplification of _R_ . _felis_ in the _C_ . _felis_ flea from the monkey was not surprising but suggests that nonhuman primates may be infected as well as humans and may represent a reservoir of _R_ . _felis_ . The role of mammals, including rodents, hedgehogs, cats, dogs, and monkeys, in the life cycle and circulation of _R_ . _felis_ remains unclear and warrants further epidemiologic studies.\n\n【7】We report for the first time that the human flea _P_ . _irritans_ can be infected with _B_ . _quintana_ . Apart from the body louse, the natural vector of _B_ . _quintana_ in humans, we have previously detected _B_ . _quintana_ in _C_ . _felis_ fleas with a prevalence of 4.5% in a series of 309 fleas collected in various regions of France . Thus, our results confirm that _B_ . _quintana_ may be found in the human flea and may explain 2 clinical reports of chronic adenopathy attributed to _B_ . _quintana_ infection for which the only epidemiologic risk factor identified was the presence of fleas . Few reports of detection of other bartonellae in fleas have been made . Recently, the rodent flea _Ctenophthalmus nobilis_ has been found to be a competent vector of at least 2 _Bartonella_ species, _B_ . _grahamii_ , which has previously been associated with human infection, and _B_ . _taylorii_ . In contrast, no evidence of either horizontal or vertical transmission was seen in bank voles ( _Clethrionomys glareolus_ ) injected with _B_ . _taylorii_ maintained in an arthropod-free environment, which suggests that fleas may be essential for transmitting some _Bartonella_ spp . In the study of Stevenson et al, _Bartonella_ spp. were detected in 38 _Oropsylla hirsuta_ and 3 _Oropsylla tuberculatus cynomuris_ prairie dog fleas in United States . In addition, new _Bartonella_ genotypes, whose medical importance is not yet known, were detected in _Pulex_ fleas in Peru , in 5 _C_ . _felis_ collected from cats, and in a _Nosopsyllus fasciatus_ collected from a _Rattus surifer_ specimen in Thailand .\n\n【8】Although detection of _Bartonella_ DNA is often reported from several sources, including fleas, mammals, and human samples, isolation of bartonellae by culture remains infrequent . Culture media and procedures used for _Bartonella_ spp. have been highly variable and have questionable sensitivity . A novel chemically modified liquid medium that will support the growth of several _Bartonella_ spp. has been recently developed and may provide an advantage over conventional blood agar culture for the isolation of _Bartonella_ spp . The prevalence of _B_ . _quintana_ as well as other bartonellae in human fleas remains unknown, and this subject needs to be addressed to better define possible sources of _Bartonella_ infections in humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1537b453-bfbf-4709-a873-b92ad19a114a", "title": "Transmission of Severe Acute Respiratory Syndrome Coronavirus 2 during Border Quarantine and Air Travel, New Zealand (Aotearoa)", "text": "【0】Transmission of Severe Acute Respiratory Syndrome Coronavirus 2 during Border Quarantine and Air Travel, New Zealand (Aotearoa)\nNew Zealand (Aotearoa in Māori) has a goal of eliminating coronavirus disease (COVID-19), which has resulted in a low incidence of this disease in this country . Managed isolation and quarantine (MIQ) is the mainstay of postborder controls to minimize importation risk. With few exceptions, international arrivals to New Zealand undergo a mandatory 14-day period of MIQ in designated facilities before entering the community. MIQ facilities are repurposed commercial hotels used exclusively for isolation and quarantine of returnees.\n\n【1】During the MIQ period, regular health monitoring, as well as PCR testing on days 3 and 12, is undertaken to identify persons with COVID-19, whether symptomatic or asymptomatic, and measures are taken to control transmission. Subsequent to this study, a day 1 test has also been put in place, as have predeparture tests. Persons who complete their 14-day period, show negative PCR results for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), and remain asymptomatic are cleared to be released. We report a case of COVID-19 in a recent arrival to New Zealand in September 2020.\n\n【2】### Human Ethics\n\n【3】A review by the New Zealand Health and Disability Ethics Committees advised that that its approval was not required for this study. Nasopharyngeal samples that had positive results for SARS-CoV-2 by real-time reverse transcription PCR were obtained from public health medical diagnostics laboratories located throughout New Zealand. Under contract for the Ministry of Health, the Institute of Environmental Science and Research has approval to conduct genomic sequencing for surveillance of notifiable diseases.\n\n【4】### Index Case-Patient\n\n【5】On September 18, 2020, a COVID-19 case was identified in New Zealand. The case was in a person who was a recent international arrival from India who had completed 14 days in MIQ in Christchurch, New Zealand, had shown negative results twice for SARS-CoV-2 on days 3 and 12, and had subsequently been released. This case-patient is denoted as case-patient G.\n\n【6】Case-patient G flew from Christchurch to Auckland, New Zealand, on the day of release on a government-chartered flight with several other persons released from MIQ. This case-patient subsequently showed development of symptoms and showed positive results for SARS-CoV-2 four days later. Persons who had close contact with case-patient G were subsequently monitored and tested . All persons who were positive for SARS-CoV-2 as a result of this investigation have provided verbal consent to be included in this study.\n\n【7】### Travel from India to New Zealand\n\n【8】Case-patient G had been part of a cohort of 149 repatriated New Zealand citizens or permanent residents who had returned from India to New Zealand on August 27, 2020. The entire cohort who arrived in Christchurch had traveled on the same chartered flight (a Boeing 747) from Delhi, India, through Nadi, Fiji; all passengers disembarked from the flight in Fiji. Several passengers remained in Fiji, 3 of whom later showed positive results for SARS-CoV-2 during their quarantine period but who were not included in this investigation. Predeparture testing for SARS-CoV-2 was not mandatory at the time and no passengers reported having been tested.\n\n【9】Of the persons who arrived in Christchurch on this flight, 8 showed positive results for SARS-CoV-2 while in MIQ. Of these 8 case-patients, 3 were shown to be genomically linked and are denoted as case-patients A, B, and C . During the first (≈18 hour) flight from New Delhi to Nadi, case-patients A, B, and C sat within 2 rows of each other; all other case-patients observed physical distancing . The flight was at ≈35% occupancy, and passengers were evenly spaced throughout the aircraft.\n\n【10】The timing at which case-patient C experienced symptoms was consistent with transmission during the flight from India to New Zealand by case-patient A or B. Case-patients A or B might have been infected during or before the flight from a common source. All passengers were required to wear facemasks for the duration of the flight, and the flight crew followed infection prevention measures. The passengers in question did not travel together and did not know each other. On arrival in Christchurch, passengers were disembarked in groups of 10 to enable physical distancing to be maintained in the terminal, and each case-patient was provided with a fresh surgical mask. The cohort was transferred by bus to MIQ upon arrival in Christchurch. Physical distancing and surgical mask use were used while boarding and on board, but seating was not preallocated to specific passengers.\n\n【11】### Evidence of Transmission in Hotel-Managed Isolation and Quarantine\n\n【12】The MIQ facility was a repurposed commercial hotel, in which each room had its own bathroom and no balconies. Case-patient C was positive on day 12 and was relocated to the isolation section of the facility. Before their relocation, an adult and infant child, both of whom had returned from India on the same flight, were in the adjacent room . Both the adult and child completed their 14-day quarantine. Each person had 2 negative test results and no reported symptoms but later showed positive results for SARS-CoV-2 while in the community (these 2 case-patients are denoted as case-patients D and E). We consider that these 2 case-patients were infected while in MIQ.\n\n【13】Closed-circuit television review of the period between the arrival of case-patients C, D, and E and the transfer of case-patient C to the isolation section of MIQ showed that there were no instances where the 3 persons were outside of their rooms at the same time. Nevertheless, footage showed that during routine testing on day 12, which took place within the doorway of the hotel rooms, there was a 50-second window between closing the door to the room of case-patient C and opening the door to the room of case-patients D and E. Therefore, we hypothesized that suspended aerosol particles were the probable mode of transmission in this instance, and that the enclosed and unventilated space in the hotel corridor probably facilitated this event . A commissioned review of the ventilation system found that the rooms in question had a net positive pressure compared with the corridor. Fomite transmission through use of communal bins in the corridor was considered to be a less probable route of transmission because contact with the bin lid by case-patient D was >20 hours after it was touched by case-patient C.\n\n【14】### Domestic In-Flight and Household Transmission\n\n【15】Following their 14-day completion of MIQ, case-patients A (who was deemed to be recovered), D, E, and G boarded an 85-min government-chartered domestic flight (on a Boeing 737) from Christchurch to Auckland. All passengers were required to wear masks, and the flight was at ≈50% occupancy. Case-patient G sat directly in front of case-patients D and E, and case-patient A sat at a distance . On arrival at Auckland airport, case-patients D and E were met by a household contact, denoted as case-patient F, and case-patient G was met by household contacts (case-patients H and I). These household contacts had not been in MIQ because they had no recent history of travel outside New Zealand. However, both contacts subsequently tested positive for SARS-CoV-2 .\n\n【16】### Genome Sequencing of SARS-CoV-2\n\n【17】We generated the genomes of the 9 positive SARS-CoV-2 samples from case-patients A–I according to reported sequencing protocols  . These genomes were classified within the (now ancestral) PANGO  genomic lineage B.1.36.17. Because of the dynamic nature of this genomic nomenclature, this cluster from New Zealand is now classified as lineage F.1, which is now extinct .\n\n【18】We compared these data to virus genomes sequenced from New Zealand and those B.1.36.17 genomes from the global dataset that were available on GISAID  as of February 2021 (n = 1,994) . The 9 SARS-CoV-2 sequences from New Zealand, together with 500 B.1.36.17 genomes, uniformly sampled at random from the global population , were aligned by using MAFFT version 7 and the FFT-NS-2 algorithm . Ambiguous sites that have been flagged as potential sequencing errors were masked. We created a maximum-likelihood phylogenetic tree by using IQ-TREE version 1.6.8  and the Hasegawa-Kishino-Yano  nucleotide substitution model with a gamma-distributed rate variation among sites. We determined the best fit model by using ModelFinder . We assessed branch support by using the ultrafast bootstrap method .\n\n【19】We found a genomic link between virus isolated from all 9 case-patients and a maximum genomic distance of 4 single-nucleotide polymorphisms . Placing this cluster within the global context provides high confidence (100% bootstrap node support of 1,000 iterations) that it was a single introduction of the virus into New Zealand . Of the other 5 case-patients who were positive for SARS-CoV-2 and arrived on the same flight from India, 1 case-patient was definitively excluded from the cluster on the basis of virus genome being within a different (non-F.1) genomic PANGO lineage . Four samples did not contain adequate RNA for genomic sequencing.\n\n【20】### Conclusions\n\n【21】This case study of COVID-19 transmission demonstrates a multibranched chain of transmission involving numerous settings, supported by closed-circuit television observations, genomic sequence analyses, and epidemiologic investigations. Major aspects included a probable case of transmission without direct person-to-person contact by aerosol within MIQ; transmission in-flight, as well as within households; and use of genomic sequence analysis to confirm probable direction of transmission between cases. These findings reinforce the need for rigorous border control processes for countries pursuing COVID-19 elimination, as well as real-time integration of genomic and epidemiologic data to inform outbreak investigations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0fafa71e-aefb-41e1-ae87-5ba6dc0a82a9", "title": "Invasive Fusariosis in Nonneutropenic Patients, Spain, 2000–2015", "text": "【0】Invasive Fusariosis in Nonneutropenic Patients, Spain, 2000–2015\nInvasive fusariosis (IF) is a fungal disease that mostly affects patients with hematologic malignancies or who have received hematopoietic cell transplants. These patients often have prolonged and profound neutropenia, low levels of T cells, or both . Despite advances in early diagnosis and treatment, IF remains associated with high morbidity and death rates .\n\n【1】Most studies on this fungal disease have occurred in North and South America . However, the epidemiology of IF in Europe has not been fully characterized; within Europe, most multicenter studies on IF have occurred in Italy and France . A multicenter study by the European Confederation of Medical Mycology reported 76 cases of IF during 2007–2012 . Most (60%) of these IF cases occurred in Italy but none in Spain. Previous single-center studies in Europe have reported regional differences in the distribution of _Fusarium_ species and their susceptibilities to antimicrobial drugs, highlighting the importance of monitoring local epidemiologic data . We characterized the epidemiology of IF in Spain using a retrospective observational study that examined the effects of clinical and etiologic characteristics on outcomes in a cohort of 58 IF patients at hospitals in Spain.\n\n【2】### Methods\n\n【3】We conducted this study in 18 hospitals in Spain, 8 of which belong to the Spanish Network for Research in Infectious Diseases, Instituto de Salud Carlos III, in Madrid, Spain. The study was approved by Reina Sofía University Hospital Institutional Review Board (Córdoba, Spain), which waived the need to obtain written informed consent. During January 2000–December 2015, hospital staff reviewed microbiological and pathologic registries to identify cases of IF. Data were recorded in a password-protected, electronic clinical research file. We monitored the collected data for missing information, inconsistencies, and ambiguities; when necessary, we sent queries to the appropriate hospitals for clarification.\n\n【4】We conducted a blind review of reported cases of IF. In this study, we included only proven cases of IF according to the consensus definitions of invasive fungal diseases established by the European Organization for Research and Treatment of Cancer and the Mycoses Study Group Education and Research Consortium . IF can be proven in 4 ways: microscopic examination of a specimen obtained by needle aspiration or biopsy that documents hyphae and isolates _Fusarium_ spp. in the same tissue; blood culture yielding _Fusarium_ spp. alongside signs consistent with an infectious disease process; isolation of _Fusarium_ spp. in a normally sterile site (excluding bronchoalveolar lavage fluid, a paranasal or mastoid sinus cavity specimen, and urine) with accompanying signs of infection; or amplification and sequencing of _Fusarium_ DNA in formalin-fixed paraffin-embedded tissue .\n\n【5】The participating hospitals identified _Fusarium_ isolates using genetic sequencing, matrix-assisted laser desorption/ionization time-of-flight (MALDI-TOF) mass spectrometry, or morphologic characteristics. In addition, all biological samples available at the time of this retrospective study (i.e. _Fusarium_ isolates and specimens for biopsies) were sent to the Mycology Reference Laboratory at the Instituto de Salud Carlos III for confirmatory genetic sequencing. The isolates were freeze-dried on potato agar slants on arrival and stored in distilled water. Molecular identification was based on the translation elongation factor 1α gene . Isolates were cultured in GYEP medium  with 2.0% glucose  at 30°C for 24–48 h. Genomic DNA was isolated by using an extraction procedure described previously . Internal transcribed spacer region and a portion of the translation elongation factor 1α gene region were amplified as previously described . Alignment results were obtained and edited using Lasergene MegAlign Pro software from DNASTAR, Inc. All sequences were compared with reference sequences from GenBank and MycoBank  databases by using InfoQuest FP version 4.50 software . We also used an in-house database of the Mycology Reference Laboratory. When only biopsy specimens were available, we used real-time PCR specific for _Fusarium_ species as previously described . When identified cases did not have available isolates or biologic material for PCR, we reported the specific _Fusarium_ species only if obtained by MALDI-TOF mass spectrometry in conjunction with histopathology. We reported the remaining IF cases in patients with compatible clinical signs and symptoms as caused by _Fusarium_ spp.\n\n【6】We defined survival as the time between the date of diagnosis and death or end of follow-up care (i.e. 90 days). We defined the date of diagnosis as the day of the first _Fusarium_ \\-positive culture. We defined disseminated fusariosis as the involvement of >1 noncontiguous organ. We did not consider fungemia cases as disseminated disease unless >1 organ was involved (e.g. skin, lung, or sinuses) . We defined neutropenia as a blood neutrophil count <500 cells/mm 3  temporally related to the onset of fungal disease. We defined persistent neutropenia as cases with continued low neutrophil counts at the end of treatment or death. We defined resolution of neutropenia as a persistent recovery of blood neutrophil count >500/mm 3  as determined by available hospital records.\n\n【7】We calculated the incidence of IF using total admissions in the participating hospitals as the denominator; we expressed the incidence as the number of cases per 100,000 admissions. For statistical purposes, we arbitrarily defined 2 time periods: 2000–2009 and 2010–2015. We compared incidences between the different periods by χ 2  test using WinPepi version 11.65 . We considered p < 0.05 to be significant.\n\n【8】We collected variables describing patient demographic data, concurrent conditions, neutropenia, receipt of corticosteroids, clinical manifestations, diagnostic procedures, treatment, and outcome (i.e. 90-day survival). We compared categorical variables using χ 2  or 2-tailed Fisher exact test, and we compared continuous variables using the Mann-Whitney U test. We conducted all statistical tests with SPSS Statistics 16.0  and R version 3.5.0 ; we considered 2-sided p values <0.05 to be significant. We constructed unadjusted Kaplan–Meier curves and compared them using the log-rank test. In addition, we evaluated factors associated with 90-day survival. We evaluated the following variables by univariate and multivariable Cox regression analyses: age, sex, age-adjusted Charlson comorbidity index , underlying disease, receipt of antifungals and corticosteroids in the previous 30 days, neutropenia at diagnosis of fusariosis and at the end of follow-up, clinical manifestations, and primary treatment (monotherapy with a lipid formulation of amphotericin B, monotherapy with voriconazole, or combination treatment). We entered variables with p<0.1 by univariate analysis into the multivariate analysis; we included variables with p<0.05 by the multivariate analysis in the final model. We evaluated the prediction accuracy of the final Cox model using the area under the receiver operating characteristic curve.\n\n【9】### Results\n\n【10】##### Incidence of IF\n\n【11】We identified 75 patients with _Fusarium_ spp. isolated from clinical samples at 18 hospitals in Spain  during 2000–2015. We excluded 10 patients with superficial infections and 7 with probable cases. In this study, the overall incidence of IF during 2000–2015 was 0.55 cases/100,000 admissions, corresponding to 0.42 neutropenic and 0.13 nonneutropenic patients/100,000 admissions (p<0.01). The overall incidence of IF was 0.40 cases/100,000 admissions during 2000–2009 and 0.79 cases/100,000 admissions during 2010–2015 (p<0.01). Among neutropenic patients, the incidence of IF increased from 0.32 to 0.57 cases/100,000 admissions (p = 0.06). Among nonneutropenic patients, the incidence of IF increased from 0.08 to 0.22 cases/100,000 admissions (p = 0.05). We also determined the annual cumulative incidence curves for the 2 groups of patients .\n\n【12】##### Clinical Characteristics\n\n【13】We identified 58 cases of IF: 44 (75.9%) occurred in neutropenic patients and 14 (24.1%) in nonneutropenic patients . Most (59%) patients were male. The median age was 67 years (interquartile range \\[IQR\\] 38–19 years) in neutropenic patients and 45 years (IQR 28–65 years) in nonneutropenic patients (p = 0.05). In total, 36.4% of neutropenic and 42.9% of nonneutropenic patients had received corticosteroid therapy in the previous 30 days (p = 0.66); 72.7% of neutropenic and 35.7% of nonneutropenic patients had received antifungal therapy in the previous 30 days (p = 0.66). At the end of follow-up (i.e. 90 days after diagnosis), 23 (52.3%) neutropenic patients had persistent neutropenia.\n\n【14】In total, 41 (93.2%) neutropenic and 5 (35.7%) nonneutropenic patients had hematologic malignancies (p<0.01). In the neutropenic patient group, the most common hematologic malignancies were acute myeloid leukemia (43.2%) and acute lymphoid leukemia (18.2%). The proportion of patients who had received hematopoietic stem cell transplants (HSCTs) was 25% (11/44; 8 allogeneic, 2 autologous, and 1 cord blood haploidentical) in neutropenic patients versus 21.4% (3/14; 1 allogeneic and 2 autologous) in nonneutropenic patients (p = 1.00). Four (9.7%) neutropenic patients and 1 (7.1%) nonneutropenic patient had graft-versus-host disease (p = 1.00). Overall, 21.4% of nonneutropenic patients and 2.3% of neutropenic patients had a history of solid organ transplantation (p = 0.04). Finally, 35.7% of nonneutropenic and 2.3% of neutropenic patients had other underlying conditions (p = 0.04). Five nonneutropenic patients had chronic cardiac disease, T-cell prolymphocytic leukemia, rheumatoid arthritis, chronic obstructive pulmonary disease, or infantile respiratory distress syndrome; 1 neutropenic patient had hemophagocytic lymphohistiocytosis.\n\n【15】Neutropenic patients were more likely than nonneutropenic patients to have a fever at the time of diagnosis (90.9% vs. 57.1%; p<0.01). Neutropenic patients also were more likely to have skin lesions (65.9% vs. 21.4%; p<0.01). We did not observe a difference in the proportion of patients with lung involvement in the IF infection (72.7% of neutropenic vs. 64.3% of nonneutropenic patients; p = 0.74). We also did not observe a difference in the rate of concurrent infections (56.8% of neutropenic vs. 42.9% of nonneutropenic patients; p = 0.36) . Disseminated disease occurred more frequently among neutropenic patients (79.5%) than nonneutropenic patients (28.6%; p<0.01). In contrast, localized forms of IF, especially localized pneumonia (6.8% of neutropenic patients vs. 35.7% of nonneutropenic patients; p = 0.02) and fungemia (6.8% of neutropenic patients vs. 28.6% of nonneutropenic patients; p = 0.05), were more common among nonneutropenic patients. However, disseminated fungemia was more common among neutropenic than nonneutropenic patients (45.7% vs. 25%; p = 0.03). Among neutropenic patients, skin lesions were more common in persons with disseminated IF (77.1%) than localized IF (22.2%; p = 0.004); lung involvement also was more common among those with disseminated IF (82.9% vs. 33.3%; p = 0.007).\n\n【16】Nonneutropenic patients were more likely than neutropenic patients to have a diagnosis on the basis of positive culture alone (43.2% of neutropenic patients vs. 85.7% of nonneutropenic patients; p<0.01). Neutropenic patients were more likely to have a diagnosis on the basis of a positive culture and histopathology (52.3%) than nonneutropenic patients (14.3%; p = 0.01).\n\n【17】##### Isolated _Fusarium_ Species\n\n【18】Thirty-six patients (62.1%) received a species-level etiologic diagnosis using molecular methods: 26 (44.8%) by genetic sequencing and 10 (17.2%) by MALDI-TOF mass spectrometry . Among these patients, _F. solani_ species complex (SC) was the most common (18/58; 31.0%), along with _Gibberella fujikuroi_ SC (10/58; 17.2%), and _F. oxysporum_ SC (5/58; 8.6%). The remaining 22 (37.9%) cases were identified as caused by _Fusarium_ spp. Among disseminated infections, most (74.4%) were reported to species level; the most common were _F. solani_ SC (15/39; 38.5%), _F. fukikuroi_ SC (8/39; 20.5%), and _F. oxysporum_ SC (3/39; 7.7%). In the cohort, disseminated IF also was caused by 2 _F. brachygibbosum_ infections and 1 _F. dimerum_ SC infection. Most (63.2%) localized infections were caused by _Fusarium_ spp. _F. solani_ SC (2/39; 5.1%), _F. fukikuroi_ SC (2/39; 5.1%), and _F. oxysporum_ SC (3/39; 7.7%) were the most common etiologic agents.\n\n【19】##### Therapeutic Regimens\n\n【20】Overall, 62.1% of patients received monotherapy (29.3% azoles, 32.8% amphotericin B), 27.6% of patients received combination therapy (e.g. azoles plus amphotericin B, mostly liposomal amphotericin B plus voriconazole), and 10.4% of patients did not receive an active treatment (3 patients were not treated, 2 received empirical caspofungin, and 1 received empirical micafungin) . Nonneutropenic patients were more likely than neutropenic patients to receive voriconazole monotherapy (54.5% vs. 85.7%; p<0.01). We did not observe a difference in the proportions of patients who were treated with amphotericin B (36.4% of neutropenic patients vs. 21.4% of nonneutropenic patients; p = 0.35). Sixteen (36.4%) neutropenic patients received combination therapy: 14 received a lipid formulation of amphotericin B plus voriconazole and 2 received amphotericin B plus posoconazole. No nonneutropenic patients received combination therapy.\n\n【21】##### Outcomes\n\n【22】By 90 days after diagnosis, 56.9% of all patients had died: 28.6% of nonneutropenic patients and 65.9% of patients with neutropenia at IF onset (p = 0.01). We analyzed these results using unadjusted Kaplan–Meier survival graphs . The death rate of nonneutropenic patients (28.6%) was similar to that of patients who had recovered from neutropenia (38.1%; p = 0.56) and both rates were significantly lower than among patients with persistent neutropenia (91.3%; p<0.01).\n\n【23】After 90 days, 66.7% of patients with localized and 36.8% with disseminated disease had died (p = 0.03). Overall, 66.7% (2/3) of patients with localized disease of the skin/soft tissue, 50% (4/8) with lung involvement, and 14.3% (1/7) with fungemia died.\n\n【24】In the assessment of prognostic factors, we analyzed 50 IF patients who received active antifungal therapy. In univariate analysis, 90-day death risk was associated with neutropenia at onset of IF and persistent neutropenia. Age, sex, age-adjusted Charlson comorbidity index, corticosteroid therapy in the previous 30 days, administration of antifungal in the previous 30 days, hematologic malignancy, history of HSCT, fungemia, disseminated disease, and the treatment type (i.e. monotherapy with a lipid formulation of amphotericin B, monotherapy with azoles or combined therapy) were not associated with 90-day survival. In multivariate analysis, persistent neutropenia (hazard ratio \\[HR\\] 7.08, 95% CI 1.91–26.17; p<0.01) was significantly associated with 90-day death risk . We calculated adjusted Kaplan–Meier survival curves for the Cox regression model .\n\n【25】### Discussion\n\n【26】We described 58 cases of invasive _Fusarium_ infections at 18 hospitals in Spain during a 15-year period . During this time, IF incidence increased from 0.40  to 0.79  cases per 100.000 admissions (p<0.01). Incidence of IF in neutropenic patients increased from 0.32 to 0.57 cases per 100,000 admissions (p = 0.06). We observed a 3-fold increase in the incidence of IF in nonneutropenic patients, from 0.08 to 0.22 cases per 100,000 admissions (p = 0.05). This increase might have been caused by an increase in the at-risk population, environmental exposure to _Fusarium_ conidia, the increased use of antifungal prophylaxis, or a combination of these factors.\n\n【27】To date, the highest IF incidence rates worldwide are in healthcare centers in Brazil. A cohort of HSCT recipients at hospitals in Brazil during 1985–2001 had a 0.6% prevalence of IF . During 2007–2009, ≈10 years later, a prospective multicenter study by Nucci et al. reported a 3.7% IF prevalence among allogeneic HSCT patients and 3.4% among patients with acute myeloid leukemia . That study also found a 1-year cumulative incidence of 5.2% among those who had received an allogeneic HSCT and 3.8% among those with acute myeloid leukemia or myelodysplasia . A >10-fold increase, from 86 to 1,023 IF cases per 100,000 admissions, occurred among patients with hematologic malignancies at a healthcare center in Brazil from 2000–2005 to 2006–2010 . In the United States, a prospective multicenter study conducted by the Transplant-Associated Infection Surveillance Network during 2001–2006 reported a 1-year cumulative incidence of up to 0.3% of non- _Aspergillus_ mold infections . In Italy, a multicenter retrospective study reported a 0.2% incidence of IF among patients who had received an allogenic HSCT during 1999–2003 .\n\n【28】In this cohort in Spain, IF occurred frequently in patients with hematologic conditions and in HSCT recipients. IF occurs almost entirely in markedly immunosuppressed patients who usually are neutropenic; we identified IF in nonneutropenic patients, including patients with a history of solid organ transplants, chronic cardiac or lung disease, or rheumatoid arthritis. These findings are in agreement with Park et al. who studied 37 patients with IF and noted that 54.1% of IF patients were nonneutropenic, 83.8% had hematologic malignancies, and 16.2% had history of solid organ transplantation . Thus, nonneutropenia and certain nonhematologic conditions might not be uncommon among IF patients.\n\n【29】Lungs, the bloodstream, and the skin were the organs most frequently affected by IF. _Fusarium_ species produce aleuroconidia, yeastlike structures that can invade the bloodstream and might cause fungemia and metastatic skin lesions .\n\n【30】IF is associated with high death rates. This cohort had a 90-day death rate of 56.9%, similar to rates noted in other studies . The death rates of nonneutropenic and neutropenic patients who recovered from neutropenia were similar (28.6% of nonneutropenic patients vs. 38.1% of neutropenic patients; p = 0.56), consistent with previous reports that 70% of IF cases resolve when patients recover from neutropenia . In this cohort, patients with persistent neutropenia had a 91.3% death rate despite antifungal therapy. Persistent neutropenia was the single most predictive prognostic factor in IF, consistent with previous reports .\n\n【31】Researchers must identify effective therapeutic strategies to improve the prognosis of IF patients with persistent neutropenia. The treatment practices observed in our study align with guidelines from the European Society of Clinical Microbiology and Infectious Diseases Fungal Infection Study Group and European Confederation of Medical Mycology . We examined the effects of different therapeutic regimens on 90-day death rate. We found no differences in the outcome of patients treated with voriconazole and those receiving a lipid formulation of amphotericin B. Combination therapy also was not associated with outcome, consistent with results from the largest study series conducted so far . However, these results must be interpreted with caution. Nucci et al. found that patients receiving combination therapy had more severe disease , possibly influencing the clinician’s decision to administer 2 drugs. In our study, no nonneutropenic patients received combination therapy; further studies should examine the potential benefits of combination therapy in nonneutropenic patients. Finally, because we observed similar clinical responses in patients treated with a lipid formulation of amphotericin B or voriconazole as monotherapies, clinicians might not need to rely on antifungal susceptibility tests to guide treatment. Most clinically relevant _Fusarium_ isolates exhibit high minimal inhibitory concentrations to most antifungals, including azoles, echinocandins, and polyenes .\n\n【32】Our study is subject to the limitations of retrospective observational studies. For example, the sample size was limited to the cases with available data. Despite being a multicenter study during a 15-year period, the sample size might be insufficient to detect significant differences in some groups. Furthermore, clinicians might prescribe a more potent treatment regimen for patients with severe disease, possibly skewing our analysis of treatment outcomes. Although all IF cases in this retrospective study met standard criteria , we could not determine the causative species in every case. Thus, the _Fusarium_ species was only reported when determined by genomic sequencing or MALDI-TOF mass spectrometry, because these techniques have high agreement rates (89.8%–97.0%) . All remaining IF cases in our series were reported as _Fusarium_ spp. We also were not able to culture all _Fusarium_ isolates because of a lack of specimens, especially for cases occurring before 2010 in hospitals in which _Fusarium_ isolates were not collected as part of routine procedures\n\n【33】In conclusion, our data show that IF is an emerging infection in Spain. We report an increase in the incidence of IF among nonneutropenic patients, including those with hematologic conditions and other concurrent conditions, such as chronic cardiac or lung diseases, rheumatoid arthritis, or history of solid organ transplants. Our results support previous studies reporting that IF survival is critically dependent on resolution of neutropenia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "75fe4255-d998-4d69-a376-a503d51f7b03", "title": "SARS-CoV-2 Variants and Age-Dependent Infection Rates among Household and Nonhousehold Contacts", "text": "【0】SARS-CoV-2 Variants and Age-Dependent Infection Rates among Household and Nonhousehold Contacts\nSARS-CoV-2 has been spreading globally since 2019; new variants of concern (VOCs) caused several epidemic waves during 2020–2022. According to a meta-analysis, the overall household secondary attack rates were higher for the Omicron variant (42.7%) than for the Alpha (36.4%) and Delta (29.7%) variants . The transmissibility and age-dependent susceptibility for Omicron and Delta exhibited significant heterogeneity among studies , and children were identified as being more vulnerable than adults to new variants . Infection rates among close contacts, determined by SARS-CoV-2 diagnostic tests, can vary according to study design, site settings, nonpharmacological control measures, and contact patterns . Thus, assessing infection rates among household and nonhousehold contacts within the same geographic area and population by using consistent methods over time could provide more reliable and valid information about changes in the effects of age and VOCs on transmission risk. With this study, we aimed to analyze the effects of age and VOCs on SARS-CoV-2 transmission by using contact tracing data of index case-patients and household and nonhousehold contacts in a city in Toyama Prefecture, Japan.\n\n【1】### The Study\n\n【2】We analyzed COVID-19 cases recorded in a city in Toyama Prefecture, Japan, over 4 periods, dominated by each of the 4 main virus variants: July 1–October 31, 2020 (pre-VOC period), April 1–30, 2021 (Alpha period), July 3–August 15, 2021 (Delta period), and January 3–23, 2022 (Omicron period) . Health center staff conducted telephone interviews with all COVID-19 case-patients, including those who were asymptomatic, to collect clinical information and recent activity history. According to the contact tracing guidelines of the National Institute of Infectious Diseases (Japan Ministry of Health, Labour and Welfare), we defined a close contact as someone who had contact with a COVID-19 case-patient during the period from 2 days before symptom onset until diagnosis . Close contacts were divided into household contacts (those who resided in the same household) and nonhousehold contacts (others who had contact with a confirmed COVID-19 case-patient for \\> 15 minutes within a 1-meter distance without wearing any personal protective equipment). All contacts received SARS-CoV-2 PCR testing regardless of symptom status. If the PCR result for the first test was negative, contacts received PCR testing again if COVID-19–associated symptoms developed. We excluded from analysis close contacts with no PCR results.\n\n【3】All data management and analyses were conducted as part of the public health response in Toyama Prefecture and the National Institute of Infectious Diseases, and we used registered data collected according to the Infectious Diseases Law of Japan. Ethics approval was not required for this study.\n\n【4】First, we determined the baseline characteristics of the index case-patients and close contacts for each of the 4 periods. Second, we calculated infection rates stratified by the characteristics of index case-patients (age, sex, history of contact with COVID-19 case-patients before diagnosis, and symptom status) and close contacts (age, sex, and interval between diagnosis of index case-patients and PCR results of contacts). To adjust for clustering effects, we calculated infection rates as the total number of positive contacts divided by the total number of close contacts (with 95% CIs) by using the svyset command in Stata . To account for clustering among contacts exposed to the same index case-patients, we analyzed odds ratios of the infection rates (with 95% CIs) by using GEE (generalized estimating equations) logistic regression models with exchangeable correlations. We adjusted the models for the characteristics of both the index case-patients and their close contacts. Third, we described the contact matrix for the average number of contacts and infection rates based on the age of the index case-patients and contacts. We used Stata version 16.0 and R version 4.2.1  software to perform statistical analyses.\n\n【5】We enrolled 1,057 patients and 3,820 contacts: 123 index case-patients and 530 close contacts, in the pre-VOC period; 246 index case-patients and 988 close contacts in the Alpha period; 304 index case-patients and 984 close contacts in the Delta period; and 384 index case-patients and 1,318 close contacts in the Omicron period . We excluded close contacts without PCR results: 45 (8.5%) persons from the pre-VOC period, 29 (2.9%) from the Alpha period, 111 (11.3%) from the Delta period, and 173 (13.1%) from the Omicron period. Infection rates during the Omicron period were 35.0% (95% CI 28.3–42.2) for household contacts and 15.1% (95% CI 10.0–22.5) for nonhousehold contacts. After adjustment for age, symptoms, sex, contact history, interval from diagnosis of index case-patient to PCR test, and household size, the odds ratios for infection were 6.22 times higher among household contacts and 3.55 times higher among nonhousehold contacts during the Omicron period than during the pre-VOC period . The risk for infection among household contacts 0–19 years of age increased significantly, from 3% in the pre-VOC period to 38% during the Omicron period . In contrast, during the study period, infection rates for nonhousehold contacts in this age group were lower, despite a higher number of contacts compared with nonhousehold contacts in other age groups . Infection rates among household contacts \\> 60 years of age decreased during the Delta period (12%) but increased again during the Omicron period (29%). Regarding infectivity throughout all time periods, the risk for infection from index case-patients \\> 60 years of age was higher than that from index case-patients of other ages .\n\n【6】### Conclusions\n\n【7】Our study showed that odds of infection were 6.2 times higher for household contacts during the Omicron period than during the pre-VOC period and that children and adolescents were particularly vulnerable . Despite increased nonhousehold contact among persons 0–19 years of age, nonphysical contact  and nonpharmacological control measures  in school and daycare centers may have led to lower infection rates and fewer large outbreaks in schools.\n\n【8】In addition, infection rates for contacts \\> 60 years of age decreased during the Delta period but increased again during the Omicron period, potentially because of waning immunity associated with SARS-CoV-2 vaccination and the attenuated effect on the Omicron variant , even with high vaccination rates (93%) among persons >65 years of age during the Omicron period . In addition, the infectivity of elderly persons tended to be higher than that of persons in other age groups even after vaccine introduction , possibly because of close contact, such as caregiving and nursing care. The value of protecting those who care for elderly case-patients should thus be emphasized.\n\n【9】A limitation of this study was the varied timing and frequency of PCR testing. As the number of days from symptom onset to diagnosis decreased over time, infection rates were associated with the timing of testing and symptoms at the time of testing. We might have missed asymptomatic infections and potentially overcounted infected case-patients among contacts who might have been exposed to other places or infected persons.\n\n【10】Our finding of increased odds of infection among household contacts during the period of the Omicron variant, particularly among children and adolescents, highlights the need for periodic surveys to investigate comparative infectivity by epidemic strain as well as susceptibility and trends by age group over time in the same area and population. Such studies would account for variations in local conditions such as control regulation, contract tracing strategy, population age structure, and vaccination coverage.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "47ec07d4-f3e5-4e7e-8b23-a042ead1ecac", "title": "Hansen Disease among Micronesian and Marshallese Persons Living in the United States", "text": "【0】Hansen Disease among Micronesian and Marshallese Persons Living in the United States\nAt 11 cases per 10,000 population and 8 per 10,000, respectively, in 2007, the small Pacific Island nations of the Republic of the Marshall Islands and the Federated States of Micronesia have the highest prevalence of Hansen disease (HD), i.e. leprosy, in the world and have made little progress in the past decade toward the World Health Organization (WHO) leprosy elimination target of <1 per 10,000 . During the first quarter of 2010, 33 new cases were detected among the 54,000 residents of the Marshall Islands .\n\n【1】HD has been present in the United States for more than a century; the number of patients has remained relatively constant at 150–200 per year .The US National Hansen’s Disease Program (NHDP) has noted an increasing number of cases among US-resident Marshall Islanders and Micronesians, including several persons with advanced disease. In 1996, the Hawaii HD program reported a cluster involving 16 (5%) of 321 persons screened from a Marshallese migrant community . In 2002, the US Army noted 3 cases in 1 month in soldiers from this region . The recent reporting of multiple cases among the Marshallese community in northwestern Arkansas  has drawn attention in a region unaccustomed to leprosy, with its stigmatizing historical connotations .\n\n【2】Under the terms of the Compacts of Free Association (the legal documents governing the relationships between the United States, Federated States of Micronesia, and the Republic of the Marshall Islands), citizens of this former US Trust Territory of the Pacific Islands are not subject to usual immigration requirements but may freely enter, reside, and work in the United States for as long as they wish. They hold a unique legal status, are not classified as immigrants, and maintain their country-of-origin citizenship. Transportation data indicate net emigration of an average of 952 Marshallese and 1,443 Micronesians annually, with a total of 38,325 emigrants for 1991–2006; almost all of these persons are thought to have immigrated to the United States and its territories . The actual distribution of this population within the United States is unknown; a specific category included in the 2010 US Census should provide this information. As economic and climatologic pressure drive increasing emigration from this HD-endemic former US Trust Territory, the US HD case load is expected to continue to increase, worsening health disparities and requiring increased program and local resources, although this increased case load is unlikely to create a public health threat of transmission to the general population. Cultural and socioeconomic issues may affect case detection and long-term disease management in this population, including adherence to and completion of therapy.\n\n【3】The objective of this report is to describe, on the basis of secondary analysis of existing program data, the epidemiology of HD among Marshallese and Micronesian persons residing in the United States. The intent is to assist in providing resources to address a health disparity that disproportionately affects a group of a particular national origin, while at the same time avoiding worsening of ethnic and disease-related stigma.\n\n【4】### Materials and Methods\n\n【5】Demographic and disease-related data were abstracted for January 1990–October 2009 NHDP surveillance and clinical records of cases that identified patients’ place of birth as the former Trust Territory (Marshall Islands or Micronesia). To facilitate global comparison, US cases (reported by using the Ridley-Jopling classification system ) were reclassified into the WHO multibacillary/paucibacillary system . Country-of-origin HD data (case numbers or rates and age/sex/disease-classification) were abstracted from WHO reports . Country-of-origin demographic data, US Census reports, and published reports relevant to migration patterns were abstracted to obtain denominator estimates where possible . Data were analyzed by using the SPSS version 16.0 statistical analysis program (SPSS Inc. Chicago, IL, USA). The study was approved by the Tulane University Institutional Review Board and exempted from review by the University of Arkansas for Health Sciences Institutional Review Board.\n\n【6】### Results\n\n【7】The number of HD case-patients of Micronesian or Marshallese origin as a proportion of all US cases has risen over the past decade, with Micronesian and Marshallese patients constituting 90 (13%) of all 686 cases in the United States during 2004–2008 ; the total source population of these 2 countries is only ≈170,000. Data are summarized in Table 1 .\n\n【8】During January 2000–October 2009, 72 (55%) of US HD cases in Micronesian or Marshallese patients were reported in Hawaii. Of 29 Micronesian patients, 22 (76%) had multibacillary HD, 5 were female, and 3 were children. Of 43 Marshallese patients, 31 (72%) had multibacillary HD, 17 (17%) were female, and 1 was a child. Cases in Hawaii were not consistently reported to NHDP until the late 1990s; however, Bomgaars et al. reported that HD cases in 15 Micronesian and 23 Marshallese patients were noted there from 1990 through mid-1998, including the Marshallese cluster summarized in Table 2 .\n\n【9】Since 1990, 88 cases have been reported in 26 states of the US mainland , 66 of these since 2000. Cases are increasingly being reported in inland locations. In all groups, the majority of patients were young men with multibacillary disease.\n\n【10】Among Micronesians, 55 cases occurred in 22 states of the US mainland , 41 of these since 2000. All but 2 patients were <40 years of age, 42 (76%) had multibacillary disease, 10 (18%) were female, and 6 (10%) were children. Six patients reported having a previous HD diagnosis in the Federated States of Micronesia; 4 of these patients reported having completed the WHO-recommended 1 year of multidrug therapy (MDT) there. Of these patients, 14 (25%) originated in Chuuk State and 13 (24%) in Pohnpei State; for 28 (51%), the specific Federated State was not documented. During the 1990s, 11 of 14 US-mainland HD cases were reported in the western coastal states of California, Washington, and Oregon. Since 2000, a total of 28 of 41 cases were reported in midwestern and southern states (e.g. 6 in Iowa, 5 each in Texas and Florida, 3 in Missouri). Before moving to the reporting state, 21 patients had resided in other mainland states (often in multiple locations), 5 in Hawaii, and 8 in Guam. Of the 33 patients for whom clinical data were available, 25 (76%) had \\> 1 complication, including 15 patients with erythema nodosum leprosum (ENL) and 3 with severe facial involvement (lagophthalmos, blindness, nasopalatal destruction). One patient was pregnant when her HD was diagnosed. Of the 24 patients for whom pharmacy data were available (excluding 8 who had recently begun treatment), the complete US-recommended MDT course had been dispensed to 10 (42%) .\n\n【11】Of the 33 US mainland Marshallese patients since 1990 (25 of these since 2000), single cases were reported in 9 states, 2 in California, 5 in Washington, and 17 in Arkansas. All but 1 patient were <40 years of age, 25 (76%) had multibacillary disease, 11 (33%) were female, and 3 (10%) were children. Three patients reported that their HD had been diagnosed in the Marshall Islands; none had completed MDT there. Nine patients had resided in other mainland states before moving to the reporting state; 5 had first lived in Hawaii. Of the 16 patients for whom clinical data were available, 12 had \\> 1 complication, including 6 patients with ENL. Two patients were pregnant at the time of diagnosis. Of the 22 patients for whom pharmacy data were available (excluding 3 currently being treated), the complete US-recommended MDT course had been dispensed to 7 (32%) .\n\n【12】Beginning in 1996, 17 HD cases have been reported among the ≈8,000–10,000 Marshallese living in northwestern Arkansas, 10 of these since 2004. All patients were <40 years of age, 15 (88%) had multibacillary disease, 5 (30%) were female, and none were children. (Two 25-year-old men with multibacillary disease, reported in November 2009 after completion of data collection for this study, are not included in this analysis.) Four patients had previously lived in other states, none in Hawaii. Specific island origin within the Marshall Islands was not reported; however, the general Arkansas Marshallese population is known to come from several different atolls. Of the 10 patients for whom clinical data were available, 7 had \\> 1 complication, including 4 patients with ENL. Excluding the 2 patients currently being treated, the US-recommended MDT course had been dispensed to 3 (20%) .\n\n【13】HD prevalence reported in the countries of origin is summarized in Figures 4 and Figure 5 . Because of the epidemiologic characteristics of HD (e.g. lack of biomarker, prolonged latency), surveillance is generally based on program records rather than population surveys; prevalence and case-detection rate (an approximation of incidence) are highly dependent on operational factors . Active case-detection programs were done in the Federated States of Micronesia in 1996 and 2005 and in the Marshall Islands in 1996 and 1998–1999 . Patterns of age/sex/classification distribution differed markedly during these years, with more children and paucibacillary disease being identified than during the usual passive surveillance years ; this pattern was also noted in the 1998 Marshallese cluster in Hawaii.\n\n【14】### Discussion\n\n【15】This study was descriptive; thus, comparisons among the various population subgroups are not statistically valid because of small numbers, varied data sources, and unknown age/sex distribution and denominators for the overall migrant populations. However, because age and sex distributions for the Arkansas, Hawaii, and Republic of the Marshall Islands Marshallese populations are similar , some crude comparisons can be made for these groups. Unless an unidentified confounder is present, rates and distribution of cases would be expected to be similar in the 3 groups.\n\n【16】The Arkansas and Hawaii Marshallese populations were approximately the same size (3,000 each) and age/sex distribution (median age 20 years, M:F = 1) in 1998–2001; rough estimates place the current Arkansas population at 8,000–10,000 and Hawaii at 6,000–8,000. However, Hawaii has identified 2.6× more cases since 2000, more female patients, and more paucibacillary disease and has found disease in children. Estimated case detection rates for Arkansas Marshallese persons are approximately half those for the Marshall Islands  and more skewed toward multibacillary disease, male, and adult patients . This Arkansas and general US mainland pattern of predominantly multibacillary disease in men does not reflect disease distribution patterns in the countries of origin or in Hawaii, suggesting under-detection of child, female, and paucibacillary disease patients rather than a biologic disease pattern .\n\n【17】Of 49 mainland patients from both the Federated States of Micronesia and the Marshall Islands for whom clinical data were available, 37 (75%) had \\> 1 complication, such as leprosy reaction, neuropathy, or other tissue involvement (e.g. palatal, renal, testicular). Although this finding could certainly reflect reporting bias, with the most severely affected being more likely to seek care and to be reported, it does indicate the magnitude of the need for treatment resources for this population with limited health care access. The most severe cases of blindness and disfigurement occurred in Micronesians living alone or in very small ethnic communities, limiting the potential impact of ethnic-targeted services. The high number of ENL cases is of note because these patients may require corticosteroid treatment for many years. The low rates of MDT completion may contribute to poor clinical outcomes and possibly to drug resistance and ongoing transmission.\n\n【18】This study has several limitations. Hawaii did not report to the NHDP system until the late 1990s. Cases in Guam and Northern Marianas and among US military personnel are not reported to the NHDP. Reporting is not mandatory in all states but is likely to be fairly complete because of US physicians’ need for consultation on management of this unfamiliar condition. Since the 2004 US Food and Drug Administration designation of clofazimine as an investigational drug, any patients with multibacillary disease are likely to have been reported so they would be able to receive the recommended MDT, although paucibacillary disease may be underreported. The increase in case reports since 2004 could reflect this requirement.\n\n【19】Measures of adherence and completion of treatment, complications, and other clinical data were available only for mainland patients receiving direct or pharmacy care from the Baton Rouge NHDP facility. MDT is dispensed directly to active patients or provided to local physicians or health departments for supervision of treatment. Although adherence to and completion of treatment are monitored at national program level for HD patients attending clinics funded through contracts with the NHDP, for whom compliance exceeds 90% , this level of follow-up has not been possible with private physicians, who are the providers for all of the Marshallese and Micronesian patients. In this situation, dispensing of MDT by the NHDP pharmacy was the only treatment measure available; actual adherence to treatment is unknown but is likely to be much lower and could be a subject for future study. A variety of cultural factors are likely to contribute to the low rate of treatment completion in this group, but evaluation of these cultural issues is beyond the scope of this study\n\n【20】Since achievement of the global leprosy elimination target of <1 case per 10,000 persons, attention has waned in some regions in which this neglected tropical disease persists. Although prevalence in the Republic of the Marshall Islands and the Federated States of Micronesia fell during the 1990s, rates have remained stable at high levels for the past decade. In these nonindustrialized nations, leprosy receives low priority in relation to more urgent public health concerns (rising rates of multidrug-resistant tuberculosis, 30% adult prevalence of type 2 diabetes, high infant mortality, and many others). Under the Compacts of Free Association, since the 1986 independence of the Federated States of Micronesia and the Republic of the Marshall Islands most health funding to these former territories flows from the United States. Assistance with leprosy control in the countries of origin would, in addition to improving health conditions there, be the first step toward preventing importation of cases to the United States. Because of the unique status of migrants from these nations to the United States, Micronesian and Marshallese migrants receive neither the health screening required for immigrants (which is not effective at preventing importation of HD because of its decades-long incubation period) nor targeted health services often available to large immigrant or refugee populations. Although legally residing in the United States, they remain citizens of their home nations and are thus ineligible for US health programs such as Medicare and Medicaid, although NHDP services such as biopsy interpretation, MDT, and direct outpatient care at the Baton Rouge facility are available. Many remain uninsured (>50% of the Arkansas Marshallese population) . Mainland state and local health departments are not typically resourced to serve either this population or this otherwise-rare condition. Hawaii receives more than $15 million annually in Compact impact funds to partially address the multiple health issues of the Marshallese and Micronesian migrant populations, but these additional US funds have not been available to the mainland states .\n\n【21】With the goal of decreasing health disparity and preventing disability, case-finding and case-management interventions are needed in US-resident Marshallese and Micronesian communities that are integrated into general health services and avoid the stigma of leprosy-labeled activities. Special efforts may be necessary to increase case detection among women and children. Large populations, such as the Arkansas Marshallese community, may be easier to reach with targeted but nonstigmatizing efforts, as has occurred in Hawaii. The small, widely distributed Micronesian communities, where the most severe, disabling cases have been identified, may be more difficult to reach.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f8cd468d-1e39-4af0-b9ef-2af1a2556e7b", "title": "Mycobacterium riyadhense in Saudi Arabia", "text": "【0】Mycobacterium riyadhense in Saudi Arabia\nInfections caused by nontuberculous mycobacteria (NTM) appear to be emerging globally, but the definitive reasons for this are unclear. Advances in diagnostic technologies have led to the identification of >160 species of _Mycobacterium_ , including several human pathogens. _M. riyadhense_ is a slow-growing NTM identified as a cause of pulmonary and extrapulmonary illnesses in humans from Riyadh, Saudi Arabia . At least 8 clinical cases have been reported from France, Bahrain, Saudi Arabia, and South Korea, with 5 of the 8 cases in Saudi Arabia  . _M. riyadhense_ can be misidentified by commercially available line probe assays as _M. tuberculosis_ complex, mostly because of confusing banding patterns . A recent nationwide study of NTM prevalence in Saudi Arabia showed no suspected cases of _M. riyadhense_ , which could be due to limiting the screening to line probe assays .\n\n【1】To explore the presence of _M. riyadhense_ in clinical settings in Saudi Arabia, we conducted a prospective study on a nationwide collection of isolates. Suspected NTM isolates reported as _M. tuberculosis complex_ or _Mycobacterium_ species with nonspecific banding pattern by line probe assays were subjected to different conservative gene sequencing to identify _M. riyadhense._\n\n【2】During April 2014–September 2015, we collected 458 NTM isolates, with clinical and epidemiological data, from all 9 national referral laboratories in different provinces of Saudi Arabia. We formulated the isolate enrollment strategy to suspect _M. riyadhense_ on the basis of previous studies . In brief, we conducted primary identification of the isolates using line probe assay-Genotype MTBC (Hain Lifescience, Nehren, Germany). We further tested isolates that showed a nonspecific banding pattern  by using Genotype Mycobacteria CM and AS assays (Hain Lifescience). The Genotype Mycobacteria CM assay showed a specific banding pattern of 1,2,3,10,15,16 (1,2,3,10,16 in previous study) for a group of isolates; AS assay identified these isolates as _Mycobacterium_ species. We subjected all isolates to partial sequencing of 16S rRNA, _rpoB_ and _hsp65_ genes using BigDye Terminator chemistry (Applied Biosystems, Foster City, CA, USA) . We then subjected the assembled sequences of all 3 genes to analysis via BLAST  and the EzTaxon database. We followed stringent identification criteria, requiring similarity \\> 99% between isolate and reference strain for species confirmation.\n\n【3】We identified 14 isolates that fit the inclusion criteria; most were reported from the Central province, Riyadh, in Saudi Arabia, but the reason is unclear. Microbiological analysis showed slow-growing mycobacteria producing rough white colonies on LJ medium within 3–4 weeks of incubation at 37°C. Primary sequencing of the 16S rRNA gene showed 12 cases of _M. riyadhense_ had a 99%–100% match with 3 database strains (GenBank accession nos. JF896094, JF896095, and NR044449). On the other hand, _rpoB_ and _hsp65_ sequences also showed 99%–100% similarity with other sequences (accession nos. EU921671, EU27644.1, JF86095 and NR 04449.1). The other closest species observed during the analysis were _M. alsense_ , _M. szulgai_ , and _M. angelicum_ (98% similarity with 16S rRNA gene sequences); _M. genavens_ and _M. simulans_ (96% similarity with _hsp65_ gene sequences); and _M. lacus_ , _M. intracellulare_ , and _M. malmoense_ (94% similarity with _rpoB_ gene sequences). Two isolates that matched inclusion criteria could not be identified as _M. riyadhense_ ; BLAST analysis showed the closest matching species as _M. lacus_ DSM 44577(T), with 89% similarity. Two 16S rRNA gene sequences from this study were deposited in GenBank (accession nos. KX898970 and KX898971).\n\n【4】We identified 12 clinical cases of _M. riyadhense_ infection, including pulmonary and extrapulmonary invasive infections, over a period of 18 months. Demographically, Saudi citizens dominated; 11 of 12 case-patients were male, and mean age was 50 years. Geographic distribution of cases showed 10 cases from Riyadh (Central province) and 2 from Dammam (Eastern province). Clinical data revealed 9 cases with pulmonary involvement and 3, including a pediatric case, with lymphadenitis. Of note, 75% of the respiratory cases were clinically relevant according to American Thoracic Society criteria for NTM pulmonary disease. Most patients recovered with isoniazid, rifampin, and ethambutol therapy .\n\n【5】The lack of advanced molecular diagnostic tools in clinical laboratories in Saudi Arabia impedes the accurate identification of _M. riyadhense_ . Without an accurate diagnosis, treatment is delayed. In this study, most of the patients were treated with standard TB regimens; some of them received clarithromycin, which did not appear to be highly effective . To date, no standard treatment regimen for _M. riyadhense_ disease has been developed, likely due to its status as a rare species. In the cases reported here, patients generally responded well to the initial therapies, but drug resistance may challenge the empirical treatment used. A strain resistant to isoniazid is already reported from South Korea . We recommend that clinicians in Saudi Arabia be vigilant to the possible emergence of _M. riyadhense_ as a more common pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8526c1a5-af20-48b5-baa6-9ffec48d69b8", "title": "Psittacosis Outbreak among Workers at Chicken Slaughter Plants, Virginia and Georgia, USA, 2018", "text": "【0】Psittacosis Outbreak among Workers at Chicken Slaughter Plants, Virginia and Georgia, USA, 2018\nPsittacosis results from inhalation of aerosolized droppings or respiratory secretions of birds infected with _Chlamydia psittaci_ . During 2008–2017, a total of 60 cases of psittacosis, a nationally notifiable disease in the United States, were reported to the National Notifiable Diseases Surveillance System .\n\n【1】The most common source of psittacosis in the United States is believed to be pet psittacine birds (e.g. parrots, cockatoos). The most recent large poultry-associated outbreaks in the United States were reported 3 decades ago and were linked to turkeys . _C. psittaci_ prevalence in poultry in the United States is unknown, although it has been recently identified in turkeys in the United States  and turkeys and chickens overseas . Poultry can be infected but show no overt signs of illness .\n\n【2】During August 31–September 4, 2018, the Virginia Department of Health (VDH) received reports of 10 persons, all workers at the same chicken slaughter plant, hospitalized with fever, headache, cough, and radiographic evidence of pneumonia. Lower respiratory tract specimens (2 bronchoalveolar lavage and 1 sputum) from 3 hospitalized workers were positive for _C. psittaci_ by real-time PCR targeting the _C. psittaci_ locus tag CPSIT\\_RS01985 , performed at the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA). The Virginia plant suspended operations on September 8.\n\n【3】On September 12, the Georgia Department of Public Health (GDPH) was notified that 3 employees of a Georgia chicken slaughter plant owned by the same company were hospitalized with pneumonia. _C. psittaci_ was detected in sputum samples from all 3 patients. The Georgia plant suspended operations on September 15.\n\n【4】After plant closures, VDH and GDPH staff inspected the respective plants, which both slaughter only chickens, and collected environmental samples to test for _C. psittaci_ . Staff collected samples from areas where workers were close to or directly handled live chickens or carcasses. Environmental samples were tested for chlamydial species by using real-time PCR, followed by high-resolution melt analysis , at the University of Georgia Infectious Disease Laboratory (Athens, GA, USA).\n\n【5】The company held employee meetings in each state and invited VDH and GDPH representatives to provide outbreak information and conduct active case finding. VDH and GDPH initiated investigations of cases and potential risk factors. A case was defined as illness in a worker employed during August 1–September 7, 2018, at the Virginia plant, or during August 13–September 28, 2018, at the Georgia plant, who had either physician-diagnosed pneumonia, or fever or chills with ≥2 symptoms of headache, cough, or muscle aches. A confirmed case required PCR detection of _C. psittaci_ in a clinical specimen.\n\n【6】At the Virginia plant, 50 cases (including 5 confirmed) were identified; 30 cases (including 8 confirmed) were identified at the Georgia plant. PCR cycle threshold values for the 13 confirmed cases ranged from 26 to 37. Using sequencing of the outer membrane protein A ( _ompA_ ) gene, we identified genotype D of _C. psittaci_ in patient specimens; this genotype is most often found in poultry . Cases occurred during August 3–September 8 in Virginia and August 17–October 22 in Georgia.\n\n【7】We provide detailed characteristics for all patients (ill workers) . A total of 58% of patients were men (age range 19–58 years). Bird evisceration was the most common job duty or title (reported by 53% of ill workers), consistent with previous psittacosis outbreaks . Twenty-nine workers were hospitalized (3 in intensive care) and had stays from 1 to 37 days. No deaths were reported.\n\n【8】_C. psittaci_ was not detected in any of the environmental samples from the Virginia (n = 62) and Georgia (n = 46) plants. After extensive cleaning with sanitizers, including quaternary ammonia, chlorine solutions, and chlorine dioxide foam (all registered by the US Environmental Protection Agency as effective against _C. psittaci_ ), the Virginia and Georgia plants reopened on September 18 and 19, respectively. Georgia cases that occurred after the plant reopened were attributed to longer incubation periods. The incubation period for psittacosis is typically 1–4 weeks , but illness onset >30 days after exposure was reported in the 2 most recent poultry-associated outbreaks in the United States .\n\n【9】At the request of the US Department of Agriculture Food Safety and Inspection Service, the National Institute for Occupational Safety and Health conducted a health hazard evaluation of the Virginia plant on September 19–20. Recommendations to the plant included repositioning cooling fans, ensuring evisceration tools were working properly, and changes to other work practices to reduce bacterial contamination and aerosolization. A health hazard evaluation was not requested at the Georgia plant, but company management reported implementing or evaluating options to implement all applicable recommendations at the plants.\n\n【10】Clinicians evaluating poultry slaughter plant workers with febrile respiratory illness should consider psittacosis as a possible diagnosis. In the absence of a more likely diagnosis, clinicians should contact state health authorities to discuss whether _C. psittaci_ testing should be requested through CDC, which has the only laboratory in the United States in which PCR testing for human specimens is currently available.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "29d5f693-cbd5-420f-88a4-7b19dc7c6bd9", "title": "Canine Serology as Adjunct to Human Lyme Disease Surveillance", "text": "【0】Canine Serology as Adjunct to Human Lyme Disease Surveillance\nLyme disease is caused by _Borrelia burgdorferi_ and transmitted in North America by _Ixodes_ spp. ticks. Routine surveillance for human illness indicates that risk for infection within the United States is highly localized. Residents of 10 states accounted for >93% of the ≈248,000 cases reported to the Centers for Disease Control and Prevention (CDC) during 1992–2006 . Annual county-level incidence ranged from 0 to >1,000 cases per 100,000 population .\n\n【1】Accurate information about risk is necessary for targeting and motivating Lyme disease prevention efforts . In addition, health care providers require knowledge of local disease risk to properly interpret clinical and laboratory findings . Although risk often can be inferred from surveillance data, reporting practices are subject to bias. Independent measures of disease risk are therefore valuable for validating surveillance findings.\n\n【2】Like humans, domestic dogs are susceptible to opportunistic infection with _B. burgdorferi_ . These infections are often subclinical and pose no risk for direct transmission to humans. Nevertheless, they elicit a robust antibody response. Given the greater proclivity of dogs for tick exposure, canine seroprevalence has been proposed as a sensitive and independent measure of human Lyme disease risk . We compared US national surveillance data on Lyme disease with recently published data on _B. burgdorferi_ antibody seroprevalence in dogs  to determine the degree of concordance between these 2 measures of Lyme disease risk and to assess the potential for canine seroprevalence to predict areas of Lyme disease emergence among humans.\n\n【3】### The Study\n\n【4】State and territorial health departments report Lyme disease cases to CDC as part of the National Notifiable Diseases Surveillance System . Data on canine seroprevalence of _B. burgdorferi_ antibodies were obtained from a 2009 publication by Bowman et al. that reported results for 982,336 dogs tested throughout the United States by using a commercial C6-based assay during 2001–2006 . We obtained state-specific seroprevalence from Table 1 of this publication and county-specific seroprevalence as categorical values (0%, 0.1%–0.5%, 0.51%–1%, 1.1%–5%, \\> 5.1%) from Figure 2 of this publication after digital enlargement. We excluded counties too small for the value to be determined reliably. We calculated average annual human Lyme disease incidence for 2001–2006 and 2007–2009 using US Census Bureau population estimates for 2004 and 2008, respectively. To evaluate county-level emergence of Lyme disease among humans, we stratified counties by the mean observed annual incidence for all counties during 2001–2006 of 4.7 cases per 100,000 population. We defined an emergent county as a county in which incidence was below this value during 2001–2006 and above this value during 2007–2009.\n\n【5】Detailed canine seroprevalence data were available for 46 US states. In linear regression analysis, state canine seroprevalence and human Lyme disease incidence were positively correlated . On the basis of this relationship, human Lyme disease incidence was effectively zero when the canine seroprevalence was < 1.3%. States generally fell into 2 distinct categories according to canine seroprevalence . Median Lyme disease incidence was uniformly low (median 0.3 cases/100,000 population) and not correlated with canine seroprevalence (r 2  0.0, p>0.4) among 32 states with canine seroprevalence <5%. Among 14 states with canine seroprevalence >5%, median annual human Lyme disease incidence was ≈100-fold higher (24.1 cases/100,000 population) and positively correlated with canine seroprevalence (r 2  0.33, p = 0.03).\n\n【6】Categorical canine serologic data were available for 866 (28%) of 3,141 counties in the 46 states . Median population in 2004 was 85,699 for counties for which data were available, compared with 25,505 for all counties in the 46 states. As in the state-level analysis, human incidence and canine seroprevalence were positively associated at the county level. Median annual reported Lyme disease incidence for humans was 0.2 per 100,000 population in counties with canine seroprevalence < 1%, 1.4 in counties with canine seroprevalence 1.1%–5%, and 25.9 in counties with canine seroprevalence >5% (p<0.001). Five (1%) of 520 counties with canine seroprevalence < 1% had rates of human illness above the overall county mean of 4.7 cases per 100,000 population annually, compared with 171 (85%) of 201 counties with canine seroprevalence >5%.\n\n【7】Overall, 153 (5%) of 2,830 counties with average annual human incidence < 4.7 per 100,000 population during 2001–2006 met the criteria for emergence during 2007–2009. Emergence was more common in counties with higher canine seroprevalence . Eighteen (56%) of 32 counties with canine seroprevalence >5% met the criteria for emergence, compared with 6 (1%) of 519 counties with seropositivity < 1% (p<0.001). Among the 32 counties with canine seroprevalence >5%, a total of 12 (67%) of the 18 counties with emergent Lyme disease were immediately adjacent to a county with seroprevalence >5%, compared with 4 (29%) of the 14 counties with nonemergent Lyme disease.\n\n【8】### Conclusions\n\n【9】Our results confirm an overall correlation between canine seroprevalence and reported human incidence of Lyme disease as measured through national surveillance. Canine seroprevalence < 1% is associated with extremely low rates of human illness in both state- and county-level analyses. Because human cases are reported according to county of residence rather than county of exposure, infections acquired during travel will occasionally be reported from areas without local transmission. Similarly, low levels of canine seropositivity are expected on the basis of the specificity of assay (up to 2% false positivity ), data from field surveys , and relocation of dogs from areas of high endemicity . Low levels of canine seroprevalence or human incidence should not be misinterpreted as confirmation of local transmission of _B. burgdorferi_ . Conversely, the overall agreement between human and canine data support the conclusion that risk for _B. burgdorferi_ infection is generally low to nonexistent outside the highly Lyme disease–endemic areas of the Northeast, mid-Atlantic, and upper Midwest.\n\n【10】At the other end of the spectrum, canine seroprevalence >5% was invariably associated with above average Lyme disease incidence in state-level analyses. In county-level analyses, the situation was more nuanced. Although 85% of counties with canine seroprevalence >5% also had above average Lyme disease incidence, 15% did not. In more than half of these counties, incidence increased to above average rates in the following 3 years, suggesting some predictive potential for high canine seroprevalence, especially in counties geographically clustered with other high seroprevalence counties. In other counties, however, high seroprevalence appears to be an anomaly resulting from small sample sizes and local demographics. For example, Routt County, Colorado, is a small rural county in a state where locally acquired Lyme disease has never been documented. Although canine seroprevalence for the county was >5%, a survey of all county veterinarians indicated that 11 of 12 seropositive dogs had lived in or traveled to known Lyme disease–endemic areas . Selective testing of dogs with exposure histories may yield misleading results with respect to local endemicity.\n\n【11】Our findings suggest that canine seroprevalence >5% can be a sensitive but nonspecific marker of increased risk for human Lyme disease. Because dogs do not transmit infection directly to humans (or humans to dogs), this association reflects similar susceptibilities to tick-borne infection. In some circumstances, high canine seroprevalence appears to anticipate increasing rates of human infection at the county level. Conversely, canine seroprevalence < 1% is associated with little to no local risk for human infection. Canine seroprevalence is a useful adjunct to human surveillance for Lyme disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8ae1515f-6379-41e9-b823-008d63582ea7", "title": "Methicillin-Resistant Staphylococcus pseudintermedius in Rats", "text": "【0】Methicillin-Resistant Staphylococcus pseudintermedius in Rats\n**To the Editor** : _Staphylococcus pseudintermedius_ is a coagulase-positive species in the _S. intermedius_ group. Previously misidentified as _S. intermedius_ , _S. pseudintermedius_ is now recognized as a leading cause of opportunistic infection in dogs  and a cause of sporadic infections in other species, including humans . Additionally, evidence of zoonotic transmission of _S._ _pseudintermedius_ from dogs to humans has been reported . Although information regarding the pathogenic process of _S. pseudintermedius_ is limited, the bacterium is known to possess virulence factors similar to those found in _S. aureus_ , including a leukotoxin comparable to the Panton-Valentine leukocidase associated with community-acquired _S. aureus_ infection .\n\n【1】Of concern is the emergence and widespread international recognition of methicillin-resistant _S. pseudintermedius_ (MRSP) . One veterinary laboratory noted a 272% increase in MRSP cases from 2007–2008 through 2010–2011 . As with methicillin-resistant _S. aureus_ , MRSP resistance is conferred by the _mecA_ gene, making MRSP resistant to all β-lactam antimicrobial drugs and some other antimicrobial drug classes . Compared with methicillin-susceptible strains, MRSP seems better able to colonize humans .\n\n【2】The potential for zoonotic transmission and concerns that MRSP could be mistaken for other methicillin-resistant staphylococci  suggest the need for further investigation into the epidemiology of this pathogen. One question yet to be addressed is whether commensal pests, particularly rats ( _Rattus_ spp.), could serve as a source of MRSP because of their pervasiveness, their propensity toward close contact with humans, and the fact that they are the source of several other zoonotic diseases . We report MRSP carriage in wild Norway rats ( _R. norvegicus_ ) in Vancouver, British Columbia, Canada.\n\n【3】During September–November 2011, Norway rats were trapped in a random sample of alleys in Vancouver’s Downtown Eastside, an impoverished neighborhood with high levels of homelessness, intravenous drug use, and HIV infection. Immediately after the rats were euthanized, a sterile swab was used to sample the oropharynx and nares of each rat.\n\n【4】Swabs were placed in 2 mL of enrichment broth containing 10 g/L tryptone T, 75 g/L sodium chloride, 10 g/L mannitol, and 2.5 g/L yeast extract and incubated for 24 h at 35°C. Aliquots of 100 µL were streaked onto mannitol salt agar with 2 µg/mL oxacillin and incubated at 35°C for 48 h. Suspected staphylococcal isolates were subcultured onto Columbia blood agar and identified according to colony morphologic appearance, Gram staining, and catalase reaction. Tube coagulase-positive isolates were speciated by using a multiplex PCR specific for the thermonuclease ( _nuc_ ) gene . Methicillin resistance was confirmed by demonstrating penicillin-binding protein 2a antigen with the latex-agglutination test (Oxoid Ltd. Basingstoke, UK). Isolates were typed by sequencing of the _mec-_ associated direct repeat unit ( _dru_ typing) . Antimicrobial drug susceptibility was evaluated by broth microdilution (Sensititre; Trek Diagnostics, Cleveland, OH, USA), according to Clinical and Laboratory Standards Institute guidelines . The study was approved by the University of British Columbia Animal Care Committee.\n\n【5】MRSP was isolated from 5 (2.1%) of 237 rats trapped. However, lack of standardized screening methods for MRSP could have resulted in underestimation of MRSP prevalence. Of the 5 isolates, 3 were _dru_ type dt11a, a strain commonly found in dogs , and the other 2 were a novel _dru_ type (assigned dt7ac). All isolates tested demonstrated resistance to multiple antimicrobial drug classes .\n\n【6】Carriage of MRSP has not been identified in wild rats; therefore, the epidemiologic and public health implications of these findings are difficult to determine. However, the isolation of a common dog-associated _dru_ type from rats suggests that MRSP might be transmissible between dogs and rats. This possibility is not surprising given the potential for direct and indirect contact between these species. Indeed, rat-to-dog transmission of other bacterial pathogens has been recognized . Detection of a _dru_ type not previously detected in methicillin-resistant staphylococci suggests that these isolates might have evolved independently of methicillin-resistant staphylococci in other animal species.\n\n【7】Rat carriage of MRSP does not prove that rats are capable of transmitting the bacterium to humans; however, rats are a source of other zoonotic pathogens . These pathogens are most commonly transmitted from rats to humans indirectly, through contamination of the environment or foodstuffs . Of note, MRSP has been shown to survive for extended periods in the environment , suggesting a mechanism through which the bacterium could be passed from rats to humans and dogs or vice versa.\n\n【8】In 2011, MRSA was detected in bedbugs from Vancouver’s Downtown Eastside . We have identified another urban pest in this area as a potential source of a multidrug-resistant pathogen. As with bedbugs, rat infestations are most common in impoverished, inner-city neighborhoods, and residents of these neighborhoods are at greatest risk for rat-to-human pathogen transmission. These findings suggest that inner-city urban rats warrant further investigation as a potential source of MRSP and other contemporary zoonotic pathogens, including multidrug-resistant bacteria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "660a4717-89fd-45b3-843b-134338d7c544", "title": "Health and the Myrmidons", "text": "【0】Health and the Myrmidons\nSalvador Dalí  _Daddy Longlegs of the Evening―Hope!_  Oil on canvas (25.4 cm × 50.8 cm) Fundación Gala-Salvador Dalí, (Artists Rights Society), 2011. Collection of the Salvador Dalí Museum, Inc. St. Petersburg, FL, USA,\n\n【1】Audio Presentation\n\n【2】“I’ll be a genius, and the world will admire me,” Salvador Dalí wrote in his diary at age 15. These confident words marked a journey of greatness started during childhood, in Figueres, Spain. In this rural town in Catalonia, steeped in artistic heritage, he started to read Voltaire, Nietzsche, Kant, Spinoza, and Descartes, and these philosophers’ notions on the nature of reality created in his mind an abiding sense of purpose.\n\n【3】Much of Dalí’s boyhood was spent in his parents’ home at the coastal village of Cadaqués near Port Lligat, where he later made his own home. Rocks from the local beach found their way into many of his works. His early art education benefited from frequent visits with the family of artist Ramón Pichot and from studies at the Municipal School of Drawing in Figueres under engraver Juan Nuñez. Dalí’s earliest surviving works date from this period, during which he also wrote copiously and showed interest in cubism. Soon he entered the famed Academy of San Fernando in Madrid, where he met poet Federico García Lorca, who later published an ode in his honor, “O Salvador Dalí of the olive-colored voice / I do not praise your halting adolescent brush / or your pigments that flirt with the pigments of your times / but I laud your longing for eternity with limits.”\n\n【4】As a student in Madrid, Dalí got to know the Prado Museum and the cubist works of Picasso, Georges Braque, Juan Gris, and the metaphysical paintings of Giorgio de Chirico. Encouraged by the attention of his peers and association with Lorca, he ventured outside the restrictions and requirements of the school. Increasingly disenchanted, he was suspended and later expelled by the academy, as he was at some point to be expelled from the surrealist movement, again for unwillingness to color inside the lines.\n\n【5】In a long period of experimentation also marked by personal notoriety, Dalí took his work in many directions, from a strictly academic style to cubism. An astonishing draftsman, he drew from nature, the imagination, or classical tradition as he searched for a distinctive style. He traveled to Paris where he eagerly embraced surrealist concepts. A solo exhibition there in 1929 brought a glowing review from poet André Breton, psychiatrist and head of the surrealist group.\n\n【6】Surrealists, Dalí among them, wanted to shake up comfortable middle class values, preferring to depict life as lived by the mind. This approach, with its probing of the unconscious, dreams, memories, and psychological associations, formed the foundation of new art rooted in paradox and contradictions rather than mimesis.\n\n【7】_Daddy Longlegs of the Evening―Hope!_ on this month’s cover, was inspired by war, the carnage of which Dalí experienced from close up. The Spanish Civil War took him to Paris. Shortly after his departure, the family home in Cadaqués was bombed, his own place at Port Lligat ransacked and destroyed. His friend Lorca was executed; his sister, imprisoned and tortured. Chased to Italy and then again to Paris, he lived there until the outbreak of World War II, when once more he had to flee. His work of this time grew dark and intense, evoking tragedy, melancholy, and confusion. In 1940, he emigrated to the United States, where he was to live for 8 years.\n\n【8】In _Daddy Longlegs of the Evening―Hope!_ the landscape is denuded. On the right side, an olive tree, the perennial emblem of peace, is bare of leaves. On the horizon, two human figures are engaged in some macabre dance, their elongated shapes and ballooned sleeves mocking similar forms in the foreground. Evening shadows cast a mournful look over the scene. In the center, Dalí places his own image, a melting face drawn in the outline of his familiar beach rocks and joined to Creativity, a hollow figure draped over the tree that once nourished it.\n\n【9】Literary and music references, two inkwells and a prominent cello, complete the hollow figure, offering clues about Dali’s state of mind. On the upper left corner, the cannon from Giorgio de Chirico’s painting _The Philosopher’s Conquest_  is held by a crutch, the cannonball melting into distorted body parts, lifeless fluids, a spent sperm-like form. Out of this unlikely background emerges a bandaged _Nike of Samothrace_ , ill-defined and threatened by a decayed horse of the apocalypse leaping from the cannon. In the lower left corner, a winged putto, messenger of love and artistic pursuits, weeps at the spectacle. But despite the devastation, a symbol of hope appears on the artist’s head: daddy longlegs, a token of good luck, amidst swarming ants.\n\n【10】The Myrmidons, ant-men (from _μύρμηξ_ \\[ _murmex_ \\] “ant”) mentioned in The Iliad and described in some detail by Ovid in the Metamorphoses, were a prolific warrior race patterned after ants from whom they allegedly descended by divine transformation. The term, which has survived with various connotations, still at times denotes mindless masses or “hired ruffians,” who make up for unquestioning loyalty with sheer numbers. The term also still evokes somehow the ants of origin―the same plentiful arthropods Dalí sprinkled liberally in his paintings, along with flies and other insects―their glistening bodies clustered around the edges, foreshadowing decomposition.\n\n【11】“Beautiful as the chance meeting on a dissecting table of a sewing machine and an umbrella,” is how revelatory juxtapositions were viewed by the surrealists. And the marvelous was to be detected in the everyday, the discarded, the coincidental, and the unnoticed. These ideas drove surrealism and, no longer incongruous or farfetched, now find their way in all aspects of life and no less in science.\n\n【12】Among health threats, viruses are like the Myrmidons in their sheer numbers. Even though several thousand have been identified, the large masses remain at large. With each new identification, the public health burden increases. And this is where Dalí would insert daddy longlegs. For with each identification, the opportunity also arises for new vaccines or other prevention strategies and effective treatments, as in the case of many known viruses, including the formidable HIV.\n\n【13】Like a surrealist painting, emergence of viruses around the globe features realities that by all appearances have nothing to link them, often in settings that by all appearances are not linked. In this issue alone, a new strain of Andes virus associated with fatal human infection was found in central Bolivia; and a new human adenovirus, in Bangladesh. Adenovirus type 7 is emerging in Malaysia. A variant West Nile virus strain, most related to the indigenous Kunjin, was characterized in Australia. Lymphocytic choriomeningitis virus–associated meningitis is reported in southern Spain; and hepatitis E virus infection, in solid organ transplant recipients in the Netherlands. The chance meeting on a “dissecting table” still applies, and the marvelous resides in identifying and controlling the Myrmidons, one by one.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "52ee496c-6650-401f-aa8e-eb9dec7aa37d", "title": "Syndromic Surveillance for Influenzalike Illness in Ambulatory Care Setting", "text": "【0】Syndromic Surveillance for Influenzalike Illness in Ambulatory Care Setting\nRapid identification of a bioterrorism-related outbreak poses challenges to traditional public health disease surveillance . At the individual level, the nonspecific prodrome of many diseases caused by bioterrorism agents requires the disease to be recognized by an astute clinician . At the population level, an intentional release of a bioterrorism agent may require that disease clusters of syndromes, such as influenzalike illnesses (ILI), be recognized through recently developed nontraditional surveillance mechanisms, such as syndromic surveillance . For instance, increases in ILI shown by syndromic surveillance could indicate undiagnosed inhalation anthrax or pneumonic plague.\n\n【1】Various data sources can be used to construct syndromic surveillance systems. Existing patient data sources, such as emergency room chief complaints, ambulance dispatch data, and clinical diagnosis data, have been used . Metadata collection systems that incorporate emergency room syndromes, private practice billing codes grouped into syndromes, and veterinary syndromes also exist . Other existing data sources that are potentially suitable for syndromic surveillance include calls to poison control centers, over-the-counter and prescription medication sales , nurse help-line telephone logs , and absenteeism in schools . In this study, we evaluate the use of data from an ambulatory care clinic network to detect increases of ILI using a time-series autoregressive and cumulative sum (CUSUM)–based detection algorithm.\n\n【2】### Methods\n\n【3】##### Data Source\n\n【4】Data in this study are from the HealthPartners Medical Group (HPMG), which is a family of nonprofit Minnesota healthcare organizations that serves approximately 240,000 patients in the Minneapolis-St. Paul Metropolitan Area. HPMG is a current partner in the National Bioterrorism Syndromic Surveillance Demonstration Program .\n\n【5】Investments in technology infrastructure allow HPMG to digitally record International Classification of Diseases, Revision 9 (ICD-9) data from patient visits to network clinics within approximately 24 hours of a patient’s initial visit. The need to develop validated and standardized methods for syndromic surveillance is a current challenge to the field. A framework for evaluating syndromic surveillance systems has been developed, which provides a general approach to comparing and contrasting aspects of syndromic surveillance systems . We have adapted some of these recommendations and have identified the following six criteria that should be satisfied before developing syndromic surveillance detection algorithms: 1) data are collected and should exist for reasons other than bioterrorism surveillance; 2) data should be recorded and accessible in a recognized, consistent, and electronic format; 3) data should be available for analysis shortly after the patient’s initial visit; 4) sufficient historical data sources should be available that represent a reasonably static and definable population; 5) syndromes should be validated against existing traditional data sources; and 6) thresholds set for these systems should achieve high sensitivity and positive predictive value. Based on the framework for evaluating syndromic surveillance systems, we assess the appropriateness of ambulatory-care encounter data from HPMG.\n\n【6】To satisfy the first criterion, these data are collected at HPMG shortly after the patient’s initial visit; then identification is removed and the data transferred to the Minnesota Department of Health for analysis without additional work for physicians, clinic staff, or patients. For each scheduled, same-day, or urgent care patient encounter, ICD-9 codes are collected, recorded, and stored in a standardized electronic format, which fulfills the second criterion. Multiple ICD-9 codes may be recorded for each patient (e.g. 786.2 cough and 780.6 fever). The ICD-9 format consistency allows for reliable syndrome classification and analysis. Other possible sources of data that have been reliably classified into syndrome categories and could be used in this type of time-series analysis include chief complaint text fields and Health Level 7 (HL-7) messaging data . ICD-9 encounter data and nonidentifying demographic information are queried daily from the HPMG central patient database and sent to the Minnesota Department of Health through secure file transfer protocol. Fulfilling the third criterion, an advanced electronic patient-tracking system allows for >90% of the encounter data at HPMG to be available in HPMG databases within 24 hours of the clinic encounter. In addition to the daily transmitted data, historical encounter data beginning in April 1999 are used in the analysis. These historical data represent a consistently insured population within the clinic network with minimal immigration into or emigration from the HPMG network, thus satisfying the fourth criterion for this type of autoregressive time-series analysis. Had HPMG experienced substantial changes in its insured population, the underlying statistical assumptions necessary for this analysis would have been violated . Validation against a traditional data source, the fifth criterion, and the selection of an appropriate threshold, the sixth criterion, will be discussed below in the Validation section.\n\n【7】##### Models and Analysis\n\n【8】We used an autoregressive model (PROC AUTOREG) to model the square root of the daily counts of ILI to the HPMG clinics in a 3-year historical period . Sample SAS code with tests for autocorrelation and stepwise autoregression is provided in the Appendix. The model closely resembles ordinary linear regression, but instead of the usual regression model, the following autoregressive error model is used:\n\n【9】The notation\n\n【10】indicates that each ε _t _ is normally and independently distributed with mean 0 and variance σ 2  . By simultaneously estimating the regression coefficients β and the autoregressive error parameters ϕ _i _ , the model corrects the regression estimates for autocorrelation, a common problem in time-series data.\n\n【11】In the model, we include an indicator for weekend or weekday, an indicator of the day as a regular or national holiday, and indicators for a sine and cosine function for seasonal adjustment. The model also includes a seventh-order autoregressive error model, selected by stepwise regression. In each case, the terms contribute significantly to the fit of the model (p < 0.05).\n\n【12】The predicted residuals from this model are then analyzed by using the cumulative sums method (PROC CUSUM) . Initially used in the manufacturing industry, CUSUM has been used for _Salmonella_ surveillance in the United States and for influenza surveillance in the United Kingdom . The method has properties making it well suited for disease outbreak detection. It can quickly detect small shifts from the process mean, provide estimates of when the change occurred, and estimate the magnitude of change .\n\n【13】##### Validation\n\n【14】Because syndromic surveillance attempts to identify disease outbreaks before a definitive diagnosis is made, assessing the validity of the ILI syndrome is difficult. The actual cause of many signals generated by this system may never be known because many patients are never requested to submit specimens for laboratory testing.\n\n【15】We assessed the validity of the HPMG ILI syndrome category by comparing ILI visits in the HPMG network to deaths from pneumonia and influenza in the core seven-county Minneapolis-St. Paul metropolitan area over the same time period. ICD-9 codes that describe ILI were selected . We also associated increases in ILI with the known onset of influenza season by comparing influenza isolates and hospital laboratory data.\n\n【16】##### System Testing\n\n【17】Data pertaining to the incubation period for inhalational exposure to most potential bioterrorism agents are limited. Therefore, evaluation of this system used hypothetical scenarios in which additional ICD-9 counts were added to existing clinical data to determine the number of excess cases necessary to trigger a signal. In an approach adapted from Goldenberg, our hypothetical scenario uses data gathered from the only documented large-scale aerosol release of weapons-grade _Bacillus anthracis_ spores .\n\n【18】In April and May 1979, an unusual outbreak of inhalational anthrax occurred in the city of Sverdlovsk in the former Union of Soviet Socialist Republics. The outbreak was originally ascribed to consuming contaminated meat, but investigation by Soviet and international scientists subsequently linked the outbreak to an accidental aerosolized release of anthrax spores from a nearby military facility . The inadvertent release, which may have contained as little as several milligrams of spores, caused 77 confirmed inhalational anthrax cases and 68 deaths in 43 days.\n\n【19】To test our model, we constructed three hypothetical scenarios on the basis of data available from the Sverdlovsk release. We made several assumptions in constructing these scenarios. First, a point-source release pattern similar to that observed in Sverdlovsk occurs in the downtown area of the Minneapolis-St. Paul metropolitan area during a weekday when most of the population is at work. This scenario will effectively disperse exposed persons to most clinics in the HPMG network, if one assumes that exposed persons will seek care at a clinic near their residence. Second, a subset of those exposed to the release will visit an HPMG clinic on the date of symptom onset and in a manner identical to those exposed in the Sverdlovsk release. This assumption effectively replicates the incubation periods experienced by those in the Sverdlovsk release and adds these additional cases to the daily totals of ILI observed in HPMG clinics. The third assumption increases the overall numbers of patients seeking care in the HPMG network from this exposure to 310 during a 43-day period, four times the number of confirmed ill in the Sverdlovsk release. The anthrax release and the resulting increase in ILI were modeled for three different time periods to determine the effect of season, day of the week, holidays, and naturally occurring ILI on the ability to detect the outbreak.\n\n【20】### Results\n\n【21】##### Validation\n\n【22】The seasonal variation in HPMG ICD-9 ILI counts is similar to the variation in deaths from pneumonia and influenza in the core seven-county Minneapolis-St. Paul metropolitan area, as reported by the Minnesota Department of Health . To satisfy the fifth criterion for establishing syndromic surveillance, i.e. validating syndromes against existing traditional data sources, death data from April 10, 1999, to December 29, 2000, were compared to ILI ICD-9 counts over the same period. Visual comparison of these data in Figure 1 suggests that ICD-9 ILI counts rose several weeks before the peak in deaths. ILI syndrome validity was determined to be acceptable, as Pearson correlation results were significant between weekly influenza and pneumonia deaths and ILI clinical encounters in the same week (0.41) and the previous week (0.41).\n\n【23】Formal calculations of sensitivity and positive predictive value were not conducted in this study. Calculation of the appropriate threshold used in the detection algorithm was determined qualitatively by adjusting the model parameters to detect the onset of influenza season. Figure 2 illustrates a large and continuous signal that was retrospectively observed beginning on December 12, 2000, and continuing until December 25, 2000. This alarm corresponds to a large ILI outbreak in the Minneapolis-St. Paul metropolitan area and was possibly associated with increased influenza A and respiratory syncytial virus infection. A hospital in the HPMG network reported above average submission and testing of isolates corresponding to these organisms during December 2000 and January 2001.\n\n【24】##### System Testing\n\n【25】The hypothetical anthrax release was modeled at three different time periods beginning June 26, 2001, December 17, 2001, and April 1, 2002. The threshold for CUSUM in each scenario was calculated as 1.1812, which resulted in an average-run-length of 50. The outbreak that began in June was detected on June 30, 4 days after the release, with a CUSUM value of 3.09 and after 30 outbreak-associated ILI patients (11.9% increase above expected) visited the HPMG clinic network . The December outbreak was detected 7 days after the release with a CUSUM value of 2.33 and 130 outbreak-associated ILI patients (12.4% increase above expected) . The April outbreak was detected 5 days after the release with a CUSUM value of 2.00 and an additional 45 outbreak-associated patients with ILI (11.7% increase above expected) recorded in the clinic network .\n\n【26】### Discussion\n\n【27】Based on the six criteria we propose, we have attempted to construct a time-series syndromic surveillance system capable of detecting a bioterrorism or other public health event against the background of normal ILI clinic visits. Patient use patterns and seasonality have a considerable effect on the distribution of the dataset, an effect that must be considered when designing the autoregressive model.\n\n【28】Because the HPMG network offers same-day scheduling for its members, many patients do not seek care on the weekend, when only urgent care facilities are open. This delay results in an increased caseload on Monday, a situation that is further exacerbated on a 3-day weekend. The distribution of data is also affected by limited clinic access associated with holidays. The HPMG clinic network operates at a reduced capacity on New Year’s Day, Memorial Day, Independence Day, Labor Day, Thanksgiving Day, Christmas Eve Day, and Christmas Day. These holidays often occur on different days of the week from year to year, and therefore generate lower-than-expected counts in the dataset. Additionally, ILI events occur with greater frequency in the winter, which generates a seasonal effect associated with the HPMG ICD-9 data.\n\n【29】Influenza season in Minnesota is variable; onset ranges from early October through mid-January. Figure 2 illustrates a large, sustained increase of ILI beginning December 12, 2000. The Minnesota Department of Health Public Health Laboratory confirmed the season’s first positive influenza isolate on December 13, 2000. This signal suggests that the rapid detection of ILI in the community is attainable by monitoring ICD-9 counts representative of ILI in a clinic network. When persons \\> 65 years of age were separated into a distinct ILI syndrome category, a statistically significant signal is observed from November 18 to November 20. This increase in the \\> 65-year category precedes the relatively large signal in the general population by approximately 3 weeks, demonstrating the utility of analyzing subsets of the patients as possible sentinel populations.\n\n【30】The ability of the system to detect additional bioterrorism-related cases is apparent in the hypothetical scenarios illustrated in Figure 3 , Figure 4 , and Figure 5 . When background levels of ILI are relatively low, the system quickly detected additional cases associated with the anthrax release. At best, the system detected the outbreak only 2 days after the first case-patients began to visit the clinics. In winter months, when background ILI is higher, the system was slower to detect the outbreak-associated cases. In December 2001, a 5-day delay occurred between the appearance of symptomatic patients to the clinics and the recognition of the outbreak by the system. Twenty-five additional patients were seen at clinics on December 24, 2001, a holiday, and the system calculated a significant CUSUM alarm of 4.48. The ability of this system to detect the outbreak-associated cases at different times of the year, on weekends, and on holidays shows that the autoregressive model adequately controls for variance and autocorrelation in the dataset.\n\n【31】These scenarios demonstrate that the system possesses the ability to detect the cumulative sum of a small amount of additional counts. The practical success of this surveillance system is limited only by the availability and quality of the source data.\n\n【32】### Conclusion\n\n【33】We have established criteria necessary for initiating syndromic surveillance for ILI and have demonstrated the effectiveness of our detection algorithm by using proxy data for a bioterrorism agent release and historical data for influenza. We believe that this approach to syndromic surveillance is useful in detecting increases in ILI.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d3353309-7b8f-46b3-8158-455e4b5d8296", "title": "Peste des Petits Ruminants Virus, Mauritania", "text": "【0】Peste des Petits Ruminants Virus, Mauritania\n**To the Editor:** Peste des petits ruminants virus (PPRV; genus _Morbillivirus_ , family _Paramyxoviridae_ ) causes severe infectious disease in sheep and goats in Africa and Asia. Pneumo-enteritis clinical signs are dominated by ocular and nasal discharge, and mortality rates are high . Four distinct lineages of PPRV have been described on the basis of a phylogenetic analysis of a cDNA fragment of the nucleoprotein (NP) gene : lineages I and II are found in western Africa , lineage III in eastern Africa and the Middle East, and lineage IV in Asia. Recent studies have shown changes in this distribution , including the emergence of PPRV lineage IV in northeastern and northern Africa . Sparse serologic results  are available regarding PPRV spread in Mauritania or genetic features of circulating PPRV strains.\n\n【1】A seroprevalence survey was implemented in October 2010 to assess PPRV spatial distribution in Mauritania. The study was limited to 8 southern provinces ( _wilayas_ ), which covered 99.3% of the national sheep and goat stocks . Of 40 districts ( _mougataas_ ), 21 were randomly selected. A single geographic point was randomly sampled within each of the selected _mougataas_ , and 100 small ruminants were sampled in a 7-km radius around the coordinates. None of the _mougataas_ in which sampling occurred had a PPRV vaccination program.\n\n【2】In addition, field veterinary officers from Trarza and Tagant Provinces were asked to report suspected outbreaks of peste des petits ruminants disease (PPR) during January–March 2012. All outbreaks were investigated, and biologic samples were collected for laboratory diagnostics.\n\n【3】All serum samples from the 2010 and 2012 surveys were analyzed by using antibody ELISA ID Screen PPR competition (IDvet Innovative Diagnostics, Grabels, France). Optical density values were converted to inhibition percentages; according to the ELISA cutoff value, inhibition percentages of < 45% were considered positive. A logistic beta-binomial regression model was used to analyze prevalence rates within mougataas. Swab samples were tested by using reverse transcription PCR (RT-PCR) adapted to a 1-step format (OneStep RT-PCR Kit; QIAGEN, Hilden, Germany) and based on nucleoprotein (NP) 3–NP4 PPRV-specific primers targeting the 3′ end of the NP gene . Amplicons of 351 nt were extracted, and after sequencing, nucleic acid segments were aligned with PPRV sequences stored in the database of the Centre de coopération internationale en recherche agronomique pour le développement (Montpellier, France) or retrieved from GenBank .\n\n【4】A total of 1,190 sheep and 714 goat serum samples were collected during the 2010 survey; the estimated serologic PPRV prevalence rate was 43% (n = 1,904; 95% CI 38%–47%). PPRV infection was widespread: prevalence rates ranged from 3% (Guerou) to 98% (Kobeni) . No significant difference was found by species or animal age (p = 0.28 and p = 0.92, respectively), but an increasing gradient in prevalence rates was observed from north to south; the effect of latitude was significant (p<10 −6  ) . The increasing prevalence rates moving from the north to the south might be related to higher small ruminant density in southern Mauritania (pastoral resources), which might increase between-herd transmission. Moreover, the movement of livestock between Mauritania and 2 countries to the south, Mali and Senegal , was favorable for PPRV exchanges over the years.\n\n【5】Three suspected outbreaks of PPR were reported during January–March 2012 . Both sheep and goats were affected; the animals, particularly young animals, had signs typical of acute PPR. Illness rate ranged from 11% to 17% and case-fatality rates from 39% to 58%. Clinical signs lasted 27–39 days. A total of 43 animals were sampled for virus detection, and 12 animals from 2 sites tested positive by RT-PCR. Seroprevalence rates were estimated on larger samples, including recovering animals in the 3 outbreak locations; these rates were high for all 3 sites: 61%, 70%, and 75% (n = 87, 31, and 12, respectively).\n\n【6】N-gene sequences were obtained from 2 sheep swab specimens collected in Trarza during the outbreak survey in early 2012 (deposited in the GenBank under accession nos. KF483658 \\[Mauritania1\\_2012\\] and KF483659 \\[Mauritania5\\_2012\\]). These isolates were placed in a phylogenetic tree built from PPRV sequences recently collected in western (Senegal, Mali) and northern Africa (Morocco), as well as isolates from other parts of the world retrieved from GenBank. Phylogenetic analysis involved 255 nt located on the C terminus end of the NP gene of the virus (84 aa). The PPRV strain from Mauritania belonged to lineage II . Sequences were close to, but distinct from, those collected in Senegal and distinct from those identified in Morocco and northern Africa (lineage IV).\n\n【7】Our study results highlight 2 PPRV epidemiologic systems: northern Africa, where all identified PPRVs belonged to lineage IV and were closely related to PPRV initially identified in Sudan ; and western Africa, where all identified PPRVs belonged to lineages I and II . This information might be useful for the design of regional control strategies. Ongoing monitoring of PPRV in Mauritania is needed to watch for the possible spread of PPRV lineage IV from northern Africa.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f2b3b55d-6480-4459-86f7-76f9ab3c5f33", "title": "Co-infections with Plasmodium knowlesi and Other Malaria Parasites, Myanmar", "text": "【0】Co-infections with Plasmodium knowlesi and Other Malaria Parasites, Myanmar\n_Plasmodium_ species are co-endemic to regions of Southeast Asia . This finding is believed to be underestimated because of insufficient sensitivity of microscopic detection of parasites. The prevalence of mixed infections with malaria parasites in the border regions between Thailand and Myanmar was recently found to be < 24% . Identification of _P. knowlesi_ as the fifth human malaria pathogen, which is prevalent in countries in Southeast Asia, has complicated this situation. _P. knowlesi_ is a parasite that infects mainly long-tailed macaques ( _Macaca fascicularis_ ) and pig-tailed macaques ( _M. nemestrina_ ) in Southeast Asia . The parasite has developed the capacity to naturally infect humans, and infections in some persons have been life-threatening . Furthermore, infections with _P._ _knowlesi_ in travelers to this region have been increasing .\n\n【1】_P. knowlesi_ isolates obtained from humans have been frequently misidentified as _P. falciparum_ or _P. malariae_ because of the morphologic similarities of these parasites . Use of PCRs specific for 18S small subunit (SSU) rRNA genes of malaria parasites has identified suspected cases . _P. knowlesi_ infection in humans in the border area between the People’s Republic of China and Myanmar has been reported , but the prevalence is unknown. We investigated the frequency of co-infections with _P. knowlesi_ and other _Plasmodium_ spp. in this region.\n\n【2】### The Study\n\n【3】The study was reviewed and approved by the Ethic Committee of the Institute for Parasitic Disease Control of Yunnan Province and local administration authority in Myanmar. One hundred forty-six blood samples were obtained in 2008 from randomly selected patients with uncomplicated malaria in southern Myanmar near Yunnan Province of China, where pig-tailed macaques are also present. Written consent was obtained from each person before blood samples were obtained. A drop (20 µL–50 µL) of fingerprick blood was placed directly on premarked filter paper. Malaria infection was identified by microscopic analysis of Giemsa-stained blood films made from blood spotted on the paper.\n\n【4】DNA templates for a nested PCR were prepared from whole blood spots on filter paper according to a previously reported method . Genomic DNA of _P. falciparum_ was obtained from in vitro–proliferated 3D7 clone. Genomic DNA of _P. vivax_ was obtained from a patient from the study region previously identified by using PCR and the primer sets used in this study. As reported , DNA (3 μL/sample and 1 ng of each positive control DNA) from each sample was amplified with the _Plasmodium_ genus–specific primer pair rPLU1 and rPLU5.\n\n【5】Two microliters of PCR product from each amplification was subjected to a second PCR amplification with species-specific primer pairs rFAL1 and rFAL2 for _P. falciparum_ , rMAL1 and rMAL2 for _P. malariae_ , rVIV1 and rVIV2 for _P. vivax_ , rOVA1 and rOVA2 for _P. ovale_ , and Pmk8 and Pmkr9 for _P. knowlesi_ . PCR products amplified with nested primers were analyzed by agarose gel electrophoresis. DNA bands were removed from the gel, purified by using the QIAquick Gel Extraction Kit (QIAGEN, Valencia, CA, USA) and ligated to T-cloning vector (Invitrogen, Carlsbad, CA, USA) according to protocols provided by the manufacturers. Plasmid inserts were then sequenced.\n\n【6】Sequence identity was confirmed by random basic local alignment search tool analysis of sequences in GenBank . Novel sequences were deposited in GenBank with accession nos. GU816242–GU816250. Phylogenetic relationships of unique sequences amplified by using nested primers with corresponding reference sequences were constructed by using the neighbor-joining method in MEGA version 4.0 . All sequences clustered with reference sequences of _P. falciparum_ , _P. vivax_ , or _P. knowlesi_ , which suggested that all sequences were species specific .\n\n【7】### Conclusions\n\n【8】Three parasite species ( _P. falciparum_ , _P. vivax_ , and _P. knowlesi_ ) were identified in 146 infected persons. Phylogenetic analysis showed that amplified products were species specific . Monoinfection with _P. falciparum_ , _P. vivax_ , and _P. knowlesi_ accounted for 34.9% (51/146), 36.3% (53/146), and 2.7% (4/146), respectively, of the infections. Mixed infections of _P. knowlesi_ with _P. falciparum_ or _P. vivax_ accounted for 6.9% (10/146) of the infections, and mixed infections with _P. knowlesi_ and either _P. falciparum_ or _P. vivax_ accounted for 8.9% (n = 13 in both groups) of the infections. Only 2 samples (1.4%) had mixed infections with _P. falciparum_ , _P. vivax_ , and _P. knowlesi_ . Thus, the prevalence of mixed infections in southern Myanmar was lower than that in northern Myanmar near the border with Thailand . The prevalence of _P. knowlesi_ was 21.9%. In most cases, this parasite showed co-infection with either _P. falciparum_ or _P. vivax_ , which indicated that _P. knowlesi_ may have not fully adapted to the human host or that humans who were infected with other malaria parasites may be more vulnerable to _P. knowlesi_ infection.\n\n【9】Our study also emphasizes the need for improvement of current methods for detecting _P. knowlesi_ infection . A recent report found that the primer pair Pmk8 and Pmkr9, which is specific for the 18S SSU rRNA gene of _P. knowlesi_ , can cross-hybridize with the corresponding sequence of _P. vivax_ . We observed weak amplifications in 16 samples (11%); all were from _P. vivax_ –infected blood. All amplicons made by using Pmk8 and Pmkr9 primers were sequenced and compared with the homologous sequences. Because of the high similarity of 18S SSU rRNA gene sequences among these parasites, more specific sequences are needed for establishing a reliable PCR-based method for routine diagnosis of _P. knowlesi_ infection .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3b2501e2-d93f-4859-b2a2-86c888425034", "title": "Sporotrichosis Cases in Commercial Insurance Data, United States, 2012–2018", "text": "【0】Sporotrichosis Cases in Commercial Insurance Data, United States, 2012–2018\nSporotrichosis is an infection caused by the fungus _Sporothrix_ . The infection typically follows cutaneous inoculation and involves the skin, subcutaneous tissue, and lymph nodes; pulmonary or disseminated disease occurs less frequently and usually affects immunocompromised persons . _Sporothrix_ exists nearly worldwide in soil and decaying plant matter, but many unanswered questions remain about its precise ecologic niche . In the United States, its geographic distribution is poorly understood. Knowledge about where sporotrichosis is most likely to occur can help healthcare providers recognize and treat it earlier and help public health officials focus prevention messages.\n\n【1】We used the MarketScan Research Databases  to examine the geographic distribution of sporotrichosis in the United States. These databases comprise health insurance claims data from outpatient visits, prescriptions, and hospitalizations for employees, dependents, and retirees throughout the United States. In 2018, the databases contained records for »27 million persons. MarketScan data are fully de-identified; thus, the Centers for Disease Control and Prevention institutional review board did not need to approve this study.\n\n【2】To query the database, we used Treatment Pathways (IBM), a web-based platform, that comprises data from persons with health insurance plans that contributed prescription drug information to the MarketScan databases. We used data from February 1, 2012–December 31, 2018, to identify sporotrichosis patients using code 117.1 from the International Classification of Diseases (ICD), Ninth Revision, Clinical Modification and code B42 from the ICD, 10th Revision, Clinical Modification (ICD-10-CM). We used the primary beneficiary’s state of residence to calculate average annual state-specific rates per 1 million MarketScan enrollees. We evaluated underlying conditions on or in the month before sporotrichosis diagnosis, demographic features, and type of sporotrichosis.\n\n【3】Of ≈76 million unique patients during 2012–2018, 1,322 had a sporotrichosis diagnosis code. For 1,236 (93.5%) of those, information was available about state of residence. The average annual rate of sporotrichosis cases per 1 million enrollees was highest in Oklahoma (6.1), Michigan (3.9), Kansas (3.5), and Kentucky (3.5) . Nationwide, the average annual rate was 2.0 cases/1 million enrollees.\n\n【4】For the 1,252 patients continuously enrolled during the month before their diagnosis, median age was 54 years; most (62%) patients were female . The most common underlying conditions we evaluated were diabetes (7%), immune-mediated inflammatory disease (3%), and chronic obstructive pulmonary disease (2%). Among 514 patients with sporotrichosis ICD-10-CM codes, specific types included lymphocutaneous in 13% and unspecified forms in 69%. Thirty-seven (3%) patients were hospitalized at diagnosis.\n\n【5】Although sporotrichosis occurred rarely in this large sample of privately insured patients, it was most common in the southern and south-central United States. The US geographic distribution of sporotrichosis has not been well described since the 1940s, when most cases were observed in the Mississippi River basin, with the highest frequencies in North Dakota, Nebraska, Wisconsin, Kansas, and Missouri . Reasons for the low rates we found in North Dakota and Wisconsin and the high rates in Michigan are unknown. Historically, Wisconsin-grown sphagnum moss has been the most common source of sporotrichosis outbreaks. At least 8 published outbreaks implicated Wisconsin moss shipped to other states for use in topiaries or packing material for tree seedlings . Since the late 1990s, sporotrichosis outbreaks appeared absent from published literature . Industry changes in harvesting sources or processing methods might play a role in the absence of outbreaks . Public health officials might not detect or investigate these outbreaks, as no routine surveillance for sporotrichosis exists. Our observation of higher sporotrichosis rates in Oklahoma and Kansas is consistent with past outbreaks linked to contaminated hay in those states .\n\n【6】In our analysis, sporotrichosis more frequently affected older women. These results possibly reflected differences in care-seeking behavior or exposures. Underlying conditions were uncommon, suggesting that most cases occurred in previously healthy persons. This finding was consistent with lymphocutaneous disease resulting from traumatic inoculation.\n\n【7】Our findings are subject to several limitations. The primary limitation was that patients’ states of residence might not represent the exposure location or the original environmental source of _Sporothrix_ . Undetected cases and potential case misclassification are inherent limitations of administrative data. Diagnosis codes might substantially underestimate the true number of cases because affected persons might not seek care for self-limiting sporotrichosis. In addition, sporotrichosis might not have been correctly diagnosed or coded. Furthermore, administrative data often lack detail (i.e. differentiating between different sporotrichosis forms is not possible in ICD’s Ninth Revision, Clinical Modification, and was not frequently used in ICD-10-CM). However, MarketScan is one of the few data sources large enough to provide a sufficient number of sporotrichosis cases to analyze at a subregional level.\n\n【8】Knowledge about where sporotrichosis is most likely to occur is essential for increasing clinician awareness of this rare disease. Increased awareness might lead to faster diagnosis and treatment with antifungal medication, which most sporotrichosis patients need . Understanding the distribution of sporotrichosis is also essential for understanding emerging sources of infection. Parts of Latin America are experiencing a large and growing epidemic of severe sporotrichosis caused by cat-transmitted _S. brasiliensis_ . This emerging infection provides further evidence of the need for ongoing monitoring.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c420d77a-7e67-42d9-9de7-fdccd867a3f3", "title": "Identifying Postpartum Intervention Approaches to Reduce Cardiometabolic Risk Among American Indian Women With Prior Gestational Diabetes, Oklahoma, 2012-2013", "text": "【0】Identifying Postpartum Intervention Approaches to Reduce Cardiometabolic Risk Among American Indian Women With Prior Gestational Diabetes, Oklahoma, 2012-2013\nAbstract\n\n【1】**Introduction**  \nInnovative approaches are needed to reduce cardiometabolic risk among American Indian women with a history of gestational diabetes. We assessed beliefs of Oklahoma American Indian women about preventing type 2 diabetes and cardiovascular disease after having gestational diabetes. We also assessed barriers and facilitators to healthy lifestyle changes postpartum and intervention approaches that facilitate participation in a postpartum lifestyle program.\n\n【2】**Methods**  \nIn partnership with a tribal health system, we conducted a mixed-method study with American Indian women aged 19 to 45 years who had prior gestational diabetes, using questionnaires, focus groups, and individual interviews. Questionnaires were used to identify women’s cardiometabolic risk perceptions and feasibility and acceptability of Internet or mobile phone technology for delivery of a postpartum lifestyle modification program. Focus groups and individual interviews were conducted to identify key perspectives and preferences related to a potential program.\n\n【3】**Results**  \nParticipants were 26 women, all of whom completed surveys; 11 women participated in focus group sessions, and 15 participated in individual interviews. Most women believed they would inevitably develop diabetes, cardiovascular disease, or both; however, they were optimistic that they could delay onset with lifestyle change. Most women expressed enthusiasm for a family focused, technology-based intervention that emphasizes the importance of delaying disease onset, provides motivation, and promotes accountability while accommodating women’s competing priorities.\n\n【4】**Conclusion**  \nOur findings suggest that an intervention that uses the Internet, text messaging, or both and that emphasizes the benefits of delaying disease onset should be tested as a novel, culturally relevant approach to reducing rates of diabetes and cardiovascular disease in this high-risk population.\n\n【5】Introduction\n\n【6】American Indians (AIs) experience type 2 diabetes mellitus (DM) and cardiovascular disease (CVD) at twice the rate of the US general population, and cardiometabolic disparities in morbidity and mortality rates are increasing in AIs . High CVD burden is partially attributable to DM prevalence, which increased from 9.3 to 20.2 per 1,000 population among childbearing AI women younger than 35 years from 1994 to 2004 . Women with a history of gestational diabetes mellitus (GDM) have a 20% to 50% chance of developing DM in the 5 to 10 years following GDM , and cumulative incidence approaches 70% in AI women . These women are also more likely to experience CVD events, even in the absence of concurrent DM . Diabetes is in itself a major risk factor for CVD, and women with DM who experience a CVD event have worse survival and poorer quality of life than men . A post-hoc analysis of self-reported women with prior GDM in the Diabetes Prevention Program demonstrated that women, including AI women, randomized to an intensive lifestyle program had a 53% risk reduction for DM .\n\n【7】Because GDM diagnosis may heighten a woman’s risk perception for her health and that of her offspring, the childbearing years present a unique window of opportunity for prevention . Despite the promise of lifestyle change to decrease cardiometabolic risk, implementing interventions to reduce weight and increase physical activity is challenging because of childbearing women’s busy lives . Furthermore, AI women with prior GDM may face challenges adopting healthy behaviors because of individual, interpersonal, and sociocultural influences . Little is known about AI women’s risk perceptions and prevention beliefs or their perspectives about barriers and facilitators to risk-reducing behaviors postpartum. Given the cardiometabolic risk GDM confers and the potential impact of early intervention, we sought to elicit perspectives of AI women with prior GDM to inform the development of a postpartum lifestyle modification intervention to be tested in this tribal community.\n\n【8】Methods\n\n【9】We conducted a cross-sectional study of AI women with prior GDM using questionnaires and focus groups and individual interviews from December 2012 to September 2013, in partnership with a large tribal health system in south-central Oklahoma. The institutional review boards of the University of Massachusetts Boston and the Chickasaw Nation of Oklahoma approved this study, and all women provided informed consent.\n\n【10】To recruit participants, providers from the Chickasaw Nation Department of Health (CNDH) and the study team, including staff from the Chickasaw Nation Nutrition Services, which administers the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC), displayed study fliers and distributed business cards with the research team’s contact information in the medical center and community clinics. Eligibility criteria were the following: 1) self-identified as AI, 2) aged 19 to 45 years, 3) health care obtained through CNDH, and 4) diagnosed with GDM within past 10 years (confirmed by chart review) but not currently in a first-GDM pregnancy. Of 49 women who expressed interest in participating and were considered eligible, we scheduled 28 for participation. Of those who expressed interest but did not participate, many were either unable to schedule a focus group or interview or did not return follow-up telephone calls or emails. Five of the 28 participants had been diagnosed with DM since their GDM pregnancy; these women did not complete the Risk Perception Survey for Developing Diabetes (RPS-DD) but their data were included in all other analyses.\n\n【11】### Data collection procedures\n\n【12】We offered focus groups at various times of the day and week over a period of months, and only 12 of 28 women interested in participating in focus groups attended. During one focus group, we learned a participant was in her first GDM pregnancy and not eligible for the study; therefore, we present focus group data from 11 women. We invited the other 16 women to complete face-to-face, individual interviews; 13 women completed these, and several of these interviews were rescheduled to accommodate the women. Two women completed surveys or interviews by telephone, because they lived far from the tribal headquarters. One woman did not participate in a focus group or interview due to an unanticipated surgery that complicated her schedule in the study time frame. Therefore, for these analyses, we present data from a final sample of 26 women.\n\n【13】We (S.P.P. T.J.) conducted 4 comoderated focus groups consisting of 2 to 5 participants and lasting approximately 60 minutes. Interviews ranged from 25 to 45 minutes and were conducted by 2 team members (T.J. S.M.). Participants completed surveys before focus groups and interviews. All focus groups and interviews were audio-recorded and transcribed. Participants received a $20 gift card for focus group and interview participation and a $10 gift card for survey completion.\n\n【14】### Measures\n\n【15】Participants completed a demographic questionnaire on personal and family health history and technology (Internet, mobile telephone) feasibility and acceptability. To measure risk perception for DM, we used 2 risk perception questions from the RPS-DD . We adapted these questions to measure risk perception for CVD (termed “heart disease”); only women who already had DM answered the CVD-related questions.\n\n【16】The focus group/interview moderator’s guide included questions about women’s risk perceptions and health beliefs regarding DM and CVD prevention, barriers and facilitators to healthy lifestyle behaviors postpartum, and modes of participant engagement to inform a lifestyle modification intervention. We asked women to reflect on their postpartum experiences following GDM, including barriers and facilitators to lifestyle change, and to describe characteristics of a program that might have helped them to carry out risk-reducing behaviors. Researchers with extensive familiarity in the subject matter and experience conducting qualitative research in this community reviewed the guide for face validity.\n\n【17】### Data analysis\n\n【18】We analyzed and interpreted focus group and interview data by using inductive content analysis to identify codes, subcategories, categories, and overarching themes . Two team members (E.J.J. E.W.S.) independently examined the transcripts and manually divided text into meaning units reflecting words or sentences containing aspects related to each other through their content and context. We condensed meaning units into codes, sorted these into categories and subcategories, and interpreted the underlying meaning of the categories into themes. We conducted a subset analysis of women with DM to identify additional themes. The 2 coders met to compare codes, resolve discrepancies, and validate the coding scheme. A third team member (S.E.L.) met with the 2 coders to review coding and categorization and reach consensus on representative data and a final set of themes. We selected quotes to illustrate major themes. For survey items, we calculated frequencies, means, and standard deviations (SDs).\n\n【19】Results\n\n【20】### Demographics\n\n【21】The mean age of the 26 participants was 32 (SD, 4.8) years, and participants had a mean number of 2.3 (SD, 0.7) children. The average length of time since most recent GDM was 3.7 (SD, 3.1) years (1 woman whose data were included in analyses was in her second GDM pregnancy at time of focus group). DM had been diagnosed in 5 of 26 women in the previous month to 5 years, and 11 of 26 reported a history of depression. All women reported a family history of DM, and most reported hypertension or CVD in a first-degree family member .\n\n【22】Most participants (24 of 26) reported accessing the Internet daily from home or another convenient location “always” or “most of the time”; only 2 of 26 reported occasional poor connectivity. All participants reported access to a mobile telephone with texting plan; 25 of 26 reported sending and receiving text messages daily, and 24 of 26 reported having an unlimited plan. Nineteen of 26 reported engaging in social networking daily; only 2 women reported never engaging with social networking. There were no significant differences in demographic characteristics between women who participated in focus groups and women who were interviewed.\n\n【23】### Risk perception\n\n【24】Among participants without DM, 15 of 19 reported a moderate to high chance of developing DM in the next 10 years, and the number increased (17 of 19) when asked to assess risk in the absence of lifestyle behavioral change. Nearly half of all participants (12 of 26) reported having a moderate to high chance of developing CVD in the next 10 years, and 19 of 26 reported a moderate to high chance without lifestyle changes. Among the subset diagnosed with DM, 4 of 5 reported having a moderate to high chance of developing CVD in the next 10 years with no lifestyle changes; 1 woman with DM perceived her risk as slight .\n\n【25】### Qualitative findings\n\n【26】Major themes and representative quotes related to women’s prevention beliefs, perceived barriers and facilitators to postpartum lifestyle modification, and perspectives about a potential lifestyle modification program were consistent across focus groups and interviews and spanned individual, relational, community, and sociocultural domains .\n\n【27】#### Beliefs about prevention and delay\n\n【28】Most participants expressed high risk perception for DM and believed they were more likely to delay DM onset than prevent it altogether. They frequently attributed this belief (delay vs prevention) to their strong family histories. Discussion related to genetics, a term not appearing in the moderator’s guide, surfaced frequently, and it was usually expressed synonymously with family history. Although most participants expressed doubt that preventing DM was possible, most women highlighted many benefits to delaying disease, emphasizing longevity, being healthier and more active in their children’s lives, and controlling the severity of future DM. Several women believed DM prevention was possible for their children and that being a role model was important.\n\n【29】Most participants also expressed high risk perception for CVD and believed that delay was more likely than prevention. The belief that CVD was inevitable, related to family history, was common. The few women who did not have a first-degree family member with CVD expressed that they were more likely to prevent CVD than DM, and several women stated that delay is possible with lifestyle changes, even with a family history. Many women expressed uncertainty and a lack of knowledge about CVD prevention compared with DM, and one woman referred to it as a “silent killer,” more elusive than DM. Several women stated that they presumed that the risk factors were similar for both diseases and that lifestyle behaviors that would delay one would delay the other.\n\n【30】When asked how they could prevent DM and CVD, many women mentioned the importance of consuming a diet high in whole grains, proteins, and fresh fruits and vegetables; being physically active; and not smoking. Several women noted that information related to healthy lifestyle behaviors was easily accessible in this tribal community.\n\n【31】Women with prior GDM who had been subsequently diagnosed with DM attributed this to having a strong family history, being overweight, and not reducing weight after GDM. The women with DM reported currently attempting to eat healthfully and exercise to minimize severity, improve self-management, avoid insulin, and reduce risk for CVD.\n\n【32】#### Perceived benefits and barriers of participating in a postpartum intervention program\n\n【33】All women who participated in focus groups or interviews stated that they would have been, or would be, interested in participating in a risk-reducing lifestyle program for women with prior GDM. Perceived barriers and facilitators to postpartum lifestyle change affected women’s preferences for program design. Common barriers to preventive behaviors included competing priorities, exhaustion, childcare duties, and time-related, financial, and geographic constraints. Across focus groups and interviews, many women expressed a lack of social support as a major barrier to eating healthy postpartum, and in interviews, 3 women expressed experiencing a sense of sabotage from family members when they attempted to eat healthier at home. Facilitators of lifestyle change included the perceived value of role modeling healthy behaviors in the family, social support, nutritional education, and access to gyms with childcare. We also identified common themes related to perceived benefits and barriers of a potential lifestyle modification program .\n\n【34】#### Preferences for program design\n\n【35】Most women expressed that the ideal lifestyle program would provide motivation and promote accountability while accommodating women’s competing family and work demands. Many women felt that a mode of program delivery that could maintain a sense of social support and promote family participation while not requiring face-to-face time would best facilitate involvement. Several women also mentioned it would be helpful to initiate such a program during the GDM pregnancy or immediately postpartum to promote smoother transitions and to help women think ahead toward prevention. Several women stated they would be interested in a program that would help them better organize their daily routines to prioritize healthy meal planning and exercise postpartum. In 1 focus group and 1 interview, women mentioned that a program might address potential postpartum depression and increase women’s confidence to carry out healthy lifestyle behaviors.\n\n【36】When asked to consider potential program designs (eg, face-to-face, Web-based, and text-messaging) in an individual or group setting, the most common preference was for a program that primarily used text-messaging with the potential for supplemental sessions with a lifestyle coach, either online or face-to-face. However, in general, women did not feel they would be able to participate face-to-face unless the program was combined with another scheduled appointment for the mother or the baby. Many women mentioned that the timing of text messages would be critical so they would not be disregarded, and they thought that text messages could be educational and motivational and could promote accountability. Women indicated interest in receiving reminders or tips through messaging, even group texts.\n\n【37】Discussion\n\n【38】To our knowledge, this is the first study to elicit perspectives of AI women with prior GDM to inform the development of a postpartum lifestyle program to reduce cardiometabolic risk to be implemented in a tribal community. We found that most Oklahoma AI women believed they would inevitably develop DM, CVD, or both. Although they were optimistic that they could delay onset or decrease severity of disease, women discussed many individual, relational, and social barriers to postpartum lifestyle change. Most women expressed enthusiasm for a lifestyle change program after pregnancy, incorporating facilitators and using the Internet and text messaging, thereby reducing the need for face-to-face contact, a major barrier to participation.\n\n【39】In our study, Oklahoma AI women expressed moderate to high risk perception for DM and CVD following GDM. In contrast to findings from a study that examined DM risk perception among a sample of predominantly white women , a larger proportion of women in our study correctly considered themselves at moderate or high risk for DM. This finding is probably due to women’s pervasive family histories but also may have been, in part, because of the GDM education received through the tribal health system. We also found that AI women with a family history of CVD considered themselves at moderate or high risk for CVD, although many women generally perceived a lack of knowledge and familiarity with CVD compared with DM. This finding should be considered when designing programs for AI women with prior GDM, because these women are at increased risk for CVD .\n\n【40】Congruent with findings from an earlier study in this tribal health care system , most women in this study believed that the development of DM or CVD was inevitable. When asked in greater depth about their beliefs, women were optimistic that they could delay onset or decrease severity of disease with lifestyle changes. This is a crucial finding that should inform future lifestyle modification interventions in this community. It is possible that the public health message of diabetes prevention is not compatible with women’s beliefs and lived experiences, and messages related to delay may be more effective for certain populations than for others. Further research is needed to assess the value of such tailored messaging.\n\n【41】Although their risk perception was high, women in our study reported similar barriers to adopting lifestyle changes as women with prior GDM in other studies . Building on the lifestyle changes many women adopt during GDM pregnancies, the best strategy to reduce DM may be to implement tailored, relevant, postpartum lifestyle modification interventions . Adding to our 2010 study findings  and similar to the findings of a recent study in Montreal among women primarily of European origin , we found that women had a high perceived need for social support, particularly from partners and family members, and they expressed enthusiasm for a postpartum lifestyle change program incorporating facilitators to promote health behaviors. However, in contrast to the Montreal study’s finding that in-person interactions with peers and professionals were deemed essential, many Oklahoma AI women felt this approach would be too challenging and potentially not feasible for them, unless it was with an existing appointment. Notably, most participants could access the Internet daily from home and reported engaging daily in social networking. All women had a mobile telephone, and most reported having an unlimited texting plan; all but 1 participant reported sending and receiving text messages daily, a finding similar to trends found in a recent survey of this age group . This finding is important given the rural location of participants and other time-constraint–related challenges inherent in a face-to-face program; although women thought such a program would be motivating, they stated numerous barriers to attending it. Similar to findings of recent studies , women in our study were challenged to attend a single focus group, even when offered numerous options at various times of the day and week over a period of months and when several interviews were rescheduled to accommodate them. Additionally, many women who expressed interest in participating did not return telephone calls or emails from research team members or were unable to schedule an in-person meeting. This finding reflects a critical challenge in translating prevention strategies, as interventions must account for women’s competing priorities and time constraints .\n\n【42】One important finding, which should prompt further study, was the overall high rate of self-reported depression in this group of AI women with prior GDM. Tailoring postpartum lifestyle interventions to address depression could be critical to promoting healthy lifestyle change and reducing cardiometabolic risk among women who live with depression.\n\n【43】This study builds on our previous work in that it provides AI women’s perspectives about a postpartum lifestyle change program that would be feasible and acceptable in this community. Our findings suggest that a program using text messaging or the Internet that is tailored to address family based changes and that encourages family involvement may be effective for promoting lifestyle change in women with prior GDM in this tribal community. A recent systematic review described 9 lifestyle intervention studies to reduce DM risk in women with prior GDM, and many of these used a combination of in-person and technology-based modes of delivery; although most were pilot or feasibility studies, preliminary findings suggest lifestyle interventions can reduce DM risk . These interventions should be tested in larger, well-designed, randomized controlled trials and tailored to be culturally relevant for high-risk populations.\n\n【44】Our study has limitations. The regional, purposive sample of Oklahoma AI women may limit the generalizability of our findings. Furthermore, our final participation rate (26 of 48 eligible women, or 54%) limits findings, because the 22 women who did not participate may have held different views than participants. In addition to potential self-selection bias, recall and social desirability biases may have affected responses.\n\n【45】A family friendly mobile health or technology-based program that provides motivation and promotes accountability for lifestyle behavioral change, while accommodating women’s competing family and work demands, should be tested to reduce rates of DM and CVD in this high-risk group of AI women with prior GDM. Programs that will effectively delay or prevent DM and CVD in this population require an understanding of the greater sociocultural context, with culturally and situationally relevant tailoring of interventions . For this group of AI women who largely perceived the development of DM and CVD as inevitable, a tailored approach emphasizing the benefits of delaying disease onset, rather than preventing disease, may be the best approach. Because AI women are usually the primary stewards of family health , translating DM and CVD delay or prevention in this group is critical to prevention across the life course, the goal being to eliminate cardiometabolic health disparities in AI communities.\n\n【46】Tables\n------\n\n【47】#####  Table 1. Characteristics of Participants in an Exploratory Study to Identify Postpartum Intervention Approaches to Reduce Cardiometabolic Risk in American Indian Women With Prior Gestational Diabetes, Oklahoma, 2012–2013 a  \n\n| Characteristic | Overall Sample(N = 26) |\n| --- | --- |\n| **Age, y, mean (SD)** | 32 (4.8) |\n| **Number of children, mean (SD)** | 2.3 (0.7) |\n| **Length of time since GDM, y, mean (SD)** | 3.7 (3.1) |\n| **Marital status** | **Marital status** |\n| Single | 2 |\n| Married or living with partner | 23 |\n| Divorced | 1 |\n| **Education** | **Education** |\n| Some high school (9th through 11th grade) | 1 |\n| High school graduate or GED | 9 |\n| Some college or vocational training | 6 |\n| Associate degree | 3 |\n| Bachelor’s degree or higher | 7 |\n| **Employment b** | **Employment b** |\n| Currently employed | 15 |\n| Out of work and looking for work | 1 |\n| Homemaker | 5 |\n| Student | 1 |\n| Unable to work | 1 |\n| **Self-reported personal health history c** | **Self-reported personal health history c** |\n| Type 2 diabetes | 5 |\n| Depression | 11 |\n| Smoked at least 1 cigarette in previous 18 months | 12 |\n| **Self-reported family history of disease (in a first-degree family member) c** | **Self-reported family history of disease (in a first-degree family member) c** |\n| Diabetes mellitus | 26 |\n| Heart disease | 22 |\n| Hypertension | 22 |\n| Stroke | 16 |\n| **Daily Internet access** | **Daily Internet access** |\n| Always | 17 |\n| Most of the time | 7 |\n| Some of the time | 2 |\n| **Own mobile phone with text messaging plan** | 26 |\n| **Frequency of sending and receiving text messages** | **Frequency of sending and receiving text messages** |\n| Daily | 25 |\n| Weekly | 1 |\n\n【49】Abbreviation: SD, standard deviation; GED, general educational development certificate; GDM, gestational diabetes mellitus.  \na  Data presented are whole numbers unless otherwise indicated.  \nb  n = 23 due to missing data.  \nc  Respondents could choose more than 1 answer.\n\n【50】#####  Table 2. Risk Perception for Developing Type 2 Diabetes and Heart Disease Among American Indian Women With Prior Gestational Diabetes, Oklahoma, 2012–2013\n\n| Risk Perception | No. a |\n| --- | --- |\n| **For developing diabetes over the next 10 years b (n = 19)** | **For developing diabetes over the next 10 years b (n = 19)** |\n| Almost no chance | 1 |\n| Slight | 3 |\n| Moderate | 12 |\n| High | 3 |\n| **For developing diabetes over the next 10 years without changing lifestyle behaviors b (n = 19)** | **For developing diabetes over the next 10 years without changing lifestyle behaviors b (n = 19)** |\n| Almost no chance | 0 |\n| Slight | 2 |\n| Moderate | 9 |\n| High | 8 |\n| **For developing heart disease over the next 10 years c (n = 26)** | **For developing heart disease over the next 10 years c (n = 26)** |\n| Almost no chance | 5 |\n| Slight | 9 |\n| Moderate | 9 |\n| High | 3 |\n| **For developing heart disease over the next 10 years without changing lifestyle behaviors c (n = 26)** | **For developing heart disease over the next 10 years without changing lifestyle behaviors c (n = 26)** |\n| Almost no chance | 1 |\n| Slight | 6 |\n| Moderate | 9 |\n| High | 10 |\n\n【52】a  Data presented are whole numbers unless otherwise indicated.  \nb  n = 19 due to 2 cases of missing data among participants without diagnosed type 2 diabetes (n = 21).  \nc  Heart disease risk perception questions were administered to all participants (n = 26).\n\n【53】#####  Table 3. Qualitative Themes Identified During Focus Groups and Individual Interviews Discussing Prevention Beliefs and a Potential Lifestyle Modification Program for American Indian Women With Prior Gestational Diabetes, Oklahoma, 2012–2013\n\n| Theme | Representative Quote |\n| --- | --- |\n| **Domain 1: Beliefs About Prevention and Delay of DM and CVD** | **Domain 1: Beliefs About Prevention and Delay of DM and CVD** |\n| High risk perception for DM and CVD | • “I could do a lot of things to help delay \\[DM\\], maybe prevent it. Maybe I’m wrong and I just need to work harder . but I think it’s one of those things that we’re just genetically unlucky.” (FG4)• \\[Regarding heart disease\\] “I think no matter how much you make yourself toe the line, eating right and everything else, inadvertently genetics is always gonna come and kind of pull out the ace and trump you.” (II6) |\n| Delay of DM and CVD more likely than prevention | “I don’t think it’s so much as preventing \\[DM\\] as much but there is delaying it. You can delay the process . like it’s gonna take its time, but if you slow the process down it will take that much longer before you actually do \\[develop DM\\].” (FG1) |\n| Inevitability of DM related to family history | “I think I will have diabetes in the future because all of my family does.” (FG3) |\n| Importance of attempting to change lifestyle behaviors to minimize severity of future disease | “If you just say I’m destined or doomed for \\[heart disease\\] just because you’re genetically predisposed, you’re in essence cutting your life short altogether. To delay it you’re gonna have longevity. I think it’s really important that you attempt to prevent it.” (II1) |\n| Knowledge-behavior gap | “I know what the right choices are, I just choose not to make them sometimes . but I’m making an informed decision.” (FG4) |\n| Perceived lack of knowledge about heart disease | “I don’t know much about heart disease or anything like that, but it’s a muscle and if you don’t work your muscle, it’s not gonna work for you, so diet and exercise should help prevent a lot of things.” (FG4) |\n| **Domain 1a: DM and CVD Beliefs — Perspectives of Women Who Had Developed DM** | **Domain 1a: DM and CVD Beliefs — Perspectives of Women Who Had Developed DM** |\n| Attempting to minimize DM severity and reduce heart disease risk | “Now that I have diabetes I’m walking . so I don’t gain more weight and get any heart disease or heart problems.” (II14) |\n| **Domain 2: Facilitators of and Barriers to Postpartum Lifestyle Change** | **Domain 2: Facilitators of and Barriers to Postpartum Lifestyle Change** |\n| FacilitatorsSupport of family and friendsBreaking the cycle of poor health in the family Motivated by fear of developing DM and/or CVD  | • “My husband is great . he’s got high blood pressure, so he has to eat healthier also. So we’re the team with it . we support each other.” (II15)• “If I’m eating right, then they see me eating right . they’re getting those skills and getting the idea it’s important to exercise, to eat right. So hopefully to kind of break the cycle in a way.” (II6)• “I’m going to regret it if I don’t change certain things . I’m scared that I’m gonna have to go through that \\[managing DM\\], so I’m gonna do whatever I can to try and change it.” (II15) |\n| BarriersCompeting priorities and demandsSocial sabotage or a perceived lack of support | • “Once you have your baby it’s all about caring for them and what they need, not yourself . I don’t have any memory of ‘Did I eat right or did I exercise?’ . any of that.” (FG3)• “They’ll tease you about how you can’t eat this food, and they put it in front of you . try to get you to eat it, and most people go ahead and eat that unhealthy food.” (II10)• “\\[N\\]ot to have the support in your home can get to you . that person wants to do better for themselves.” (II1) |\n| **Domain 3: Benefits of and Barriers to Participating in a Postpartum Intervention Program** | **Domain 3: Benefits of and Barriers to Participating in a Postpartum Intervention Program** |\n| BenefitsFocus on health of entire familyBeing a role model for children Foster continuation of healthy diet following GDM Increase social support between women in family and community  | • “If you’re looking at prevention of diabetes, teaching kids at a very young age to eat healthy and to exercise \\[is key\\] . so \\[you need\\] to draw the kids in.” (II1)• “We play ball, we ride bikes together . so I do feel like I’ve taken some steps \\[as a role model\\] . but could still do a better job with them \\[her children\\] right now.” (II1)• “I was just going to my 6-week checkup because the diabetes clinic really didn’t need me anymore because I didn’t need them \\[after management of GDM pregnancy\\]. So yeah, a program after I delivered would have been awesome.” (FG3)• “It would help other women to see that someone else can do it \\[be successful with healthy lifestyle changes\\] and would also be able to help someone that needs that help . because, you know, ‘I care and I know how you are and I can help you do it.’” (II13) |\n| BarriersLack of time and childcareTime away from being with their families Rural residences made traveling to the tribal health headquarters inconvenient and expensive  | • “I’d rather spend time with my kids and just hang out with them and run around the backyard, as opposed to taking time away from them, so . it’s kind of like a tug, a moral tug in a way, so I put my stuff on the back burner.” (II6)• “Obviously it would not be very good to have to drive all the way into town, sit down, talk to someone for 30 to 45 minutes, and then have to drive all the way back in less than an hour. I think location and scheduling would probably be the two biggest challenges.” (II2) |\n| **Domain 4: Preferences for Program Design** | **Domain 4: Preferences for Program Design** |\n| Increase social support while promoting accountabilityProvide motivation by connecting with someone relatable Limit face-to-face time to accommodate competing family and work demands Provide smooth transition from pregnancy to postpartum Promote family participation and women’s central role as primary stewards of family health  | • “We could text motivational things like, ‘so-and-so reached her goal,’ and then we could be happy for her, and that’s gonna make her feel good because we’re all in that group together.” (FG4)• “\\[A\\] relatable person \\[in the program\\] . they probably cave too . they’re not picture perfect . they’re trying as well and they believe that you can do it, and they’re very invested in you as much as you’re invested in a program.” (FG2)• “\\[S\\]ay it’s during the day, to set up the time from work, to get there, to get back, you have to factor that in. How long are you gonna be there? What if it runs over after work? What to cook for dinner? And things like that.” (II4)• “I think initiating this even while they’re still in the hospital. From giving birth, saying, ‘Hey, this is an option out there,’ and starting it right off the bat. And keeping in mind there’s ways to modify it, ’cause you need more calories and things when you’re breastfeeding.” (FG4)• “There’s a sense of being the matriarch. You’re a provider, whether it’s financially \\[or as\\] a homemaker . so \\[women need\\] to have guidelines to help change their lifestyle as well as trickle down to their family.” (II11) |\n\n【55】Abbreviation: CVD, cardiovascular disease; DM, type 2 diabetes mellitus; GDM, gestational diabetes mellitus; FG, focus group; II, individual interview.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "46f80bf3-1f76-497d-9091-957adb1e09fa", "title": "Infection with Mansonella perstans Nematodes in Buruli Ulcer Patients, Ghana", "text": "【0】Infection with Mansonella perstans Nematodes in Buruli Ulcer Patients, Ghana\nBuruli ulcer, caused by _Mycobacterium ulcerans_ , is a neglected tropical disease common in rural parts of West Africa. Infection with _M. ulcerans_ causes disfiguring skin ulcers, mainly in children. The disease is highly focal, and in Ghana, cases are reported mainly from the humid and tropical southern regions, including Ashanti and Greater Accra  _._ Recent studies suggest that aquatic invertebrates serve as a reservoir for _M. ulcerans_ , although complete transmission pathways remain unknown  _._ Aquatic insects infected with _M. ulcerans_ can establish infection in mice by biting , but it is not clear that this is the cause of human infection  _._ In southeastern Australia, evidence has been found linking infected mosquitoes with human cases , but proof of transmission is lacking.\n\n【1】Residents of regions in which Buruli ulcer is endemic are frequently exposed to parasitic infections such as filariasis. In Ghana, lymphatic filariasis caused by _Wuchereria bancrofti_ nematodes is found in several regions to which Buruli ulcer is endemic, such as the Upper Denkyira District in the central region of Ghana, but its prevalence is unknown . The filarial nematode _Mansonella perstans_ is endemic to countries in central and western Africa; its distribution overlaps that of other filarial nematodes _W. bancrofti_ , _Loa loa_ , and _Onchocerca volvulus_ . Infective _M. perstans_ larvae are transmitted through the bite of _Culicoides_ midges (Diptera: Ceratopogonidae); the larvae develop over the course of months into adult worms that reside in serous cavities, particularly in the abdomen. _M. perstans_ infection is not associated with a specific set of clinical signs and symptoms, but those attributed to this infection include acute swelling in the forearms, hands, and face that recedes in a few days and often recurs; itching with or without rash; arthralgia; and eosinophilia .\n\n【2】During an investigation into the immunopathogenesis of Buruli ulcer, we observed _M. perstans_ nematodes in preparations of peripheral blood mononuclear cells from a patient. This finding led us to consider whether this organism was involved in the transmission or pathogenesis of _M. ulcerans_ disease or if the finding was incidental. We then conducted a small case–control study to investigate the frequency of _M. perstans_ co-infection in patients with _M. ulcerans_ disease and the effect of this co-infection, if any, on patient response to antimicrobial drug therapy.\n\n【3】### The Study\n\n【4】During August 2010–December 2012, we recruited all patients who had clinically suspected _M. ulcerans_ infection and had attended a clinic in the Buruli ulcer–endemic Asante Akim North District in Ghana. Age- and sex-matched household contacts of patients were also asked to participate; all study participants were \\> 5 years of age. The study protocol was approved by the ethics review committee of the School of Medical Sciences, Kwame Nkrumah University of Science and Technology (CHRPE/91/10).\n\n【5】Whole blood samples were taken at baseline, at week 6, and at week 12 from 66 patients in whom the diagnosis of Buruli ulcer disease had been confirmed by PCR for the IS _2404_ repeat sequence specific for _M. ulcerans_ ; samples were also obtained from 20 household contacts at the same intervals. The samples were heparinized, and peripheral blood mononuclear cells were separated from 10-mL samples. Filarial infection was confirmed on a blood film stained with Giemsa and Delafield hematoxylin and examined for microfilariae at ×10 and ×40 magnification (the Knott technique; _10_ ). _M. perstans_ nematodes were distinguished from _L. loa_ and _W. bancrofti_ nematodes by their small size and the absence of a sheath .\n\n【6】Patients in whom _M. ulcerans_ infection was found were treated with 10 mg/kg oral rifampin and 15 mg/kg intramuscular streptomycin, administered daily at village health posts under direct observation for 8 weeks (RS8 treatment). The patients were followed up every 2 weeks in the clinic and monitored for complete healing or recurrence of skin lesions. We compared the proportion of household controls versus the proportion of Buruli ulcer patients infected with _M. perstans_ nematodes and the time to complete healing of _M. ulcerans_ lesions in co-infected versus monoinfected patients. Categorical variables such as sex, clinical form of _M. ulcerans_ lesion, and category of _M. ulcerans_ lesion were compared by using the Fisher exact test, and cumulative healing was compared by using the log-rank test.\n\n【7】We found all forms of _M. ulcerans_ disease among the group of patients; proportions of each type and category are shown in the Table . Of 66 patients with _M. ulcerans_ disease, 15 (22.7%) were co-infected with _M. perstans_ nematodes, whereas 4 (13%) of 30 household controls had _M. perstans_ infection (p = 0.4 by Fisher exact test). Three patients in the co-infected group and none in the _M. ulcerans_ –monoinfected group reported pruritus. No other clinical signs of _M. perstans_ infection were found.\n\n【8】All 66 patients completed RS8 treatment, but 9 were lost to follow-up during the 12-month follow-up period. Buruli ulcer lesions healed completely in 14 co-infected patients by 58 weeks (median 20 weeks, 95% CI 14.6–30.2) and in 43 monoinfected patients by 50 weeks (median 21 weeks, 95% CI 16.7–25.5). We found no difference in cumulative time to healing for co-infected versus monoinfected patients (p>0.05 by log-rank test) . Buruli ulcer patients who had _M. perstans_ nematodes co-infection were treated with doxycycline (200 mg) and ivermectin (150 μg/kg) daily for 6 weeks, starting during the second to fourth week of RS8 treatment. Viable microfilariae were still visible in peripheral blood mononuclear cell cultures from all co-infected patients after ivermectin and doxycycline treatment, but pruritus subsided in the 3 patients who had reported it.\n\n【9】### Conclusions\n\n【10】We found co-infection with _M. perstans_ in 23% of Buruli ulcer patients in a disease-endemic district in Ghana, but this prevalence was not significantly difference from prevalence among household contacts who served as controls (13%). As with Buruli ulcer, _M. perstans_ filariasis is predominantly found in rural populations and infection begins in childhood; the highest infection rates are found in children 10–14 years of age , similar to those for children at highest risk for _M. ulcerans_ infection. _M. perstans_ infection occurs in Ghana and was seen in the Volta region of Ghana around Hohoe during the 1990s, but its prevalence is unknown , and no information is available about the average number of worms per infection. In Uganda, prevalence of _M. perstans_ infection has been found to range from 0.4% to 50% .\n\n【11】_M. perstans_ nematodes are transmitted by the bites of _Culicoides_ midges, but it is not known whether _M. perstans_ –infected midges can be co-infected with _M. ulcerans_ . In a guinea pig model, skin penetration was shown to be a requirement for establishment of _M. ulcerans_ disease , and it has been postulated that mosquito bites cause _M. ulcerans_ disease in Australia  _._ These organisms might share a common route of transmission, but our findings in this small study do not support this concept.\n\n【12】Our findings suggest that _M. perstans_ nematodes are common in rural Ghana and coincidentally infect patients with _M. ulcerans_ disease, necessitating the consideration of these organisms in the management plan of Buruli ulcer patients. Although often asymptomatic, _M. perstans_ infection may cause eosinophilia, subcutaneous swellings, aches, pains, and skin rashes in a considerable proportion of patients  _._ Because filarial nematodes are known to polarize the host immune responses from T-helper type 1 cells needed for protection against mycobacterial infections, toward humoral and T helper type-2 mediated immunity, we plan to undertake a study to investigate this interaction.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "20e4af46-33fa-4b24-aca3-b671be2cbd85", "title": "Epidemiology of Pulmonary Nontuberculous Mycobacterial Disease, Japan", "text": "【0】Epidemiology of Pulmonary Nontuberculous Mycobacterial Disease, Japan\n**To the Editor:** Incidence of pulmonary nontuberculous mycobacterial disease (PNTMD) is reportedly increasing globally . Although such an increase is expected in Japan , the epidemiologic situation is unclear. The most recent survey, which used the 1997 American Thoracic Society diagnostic criteria, reported that the incidence rate for PNTMD in 2007 was 5.7 cases per 100,000 person-years . To update the data, we performed a nationwide hospital-based survey in Japan.\n\n【1】After a preliminary survey of 20 hospitals, we developed and disseminated questionnaires to all 884 hospitals in Japan that were certified by the Japanese Respiratory Society. The surveys asked about the number of newly diagnosed cases, from January through March 2014, of PNTMD, pulmonary _Mycobacterium avium_ disease, _M. intracellulare_ disease, or _M. avium_ complex (MAC; the combination of the first 2 species listed); pulmonary _M. kansasii_ disease; pulmonary _M. abscessus_ disease; and tuberculosis (TB) for inpatients and outpatients. Hospital respondents returned the completed questionnaires by mail, fax, or Internet. To avoid potential reporting bias and misclassification, we counted only cases that met the 2007 American Thoracic Society/Infectious Diseases Society of America statements  and excluded cases diagnosed at other hospitals. Because the source population can be ascertained by using the epidemiologic data for TB as a reportable disease, to estimate the incidence rate of PNTMD, we used the ratio of TB to PNTMD cases. The PNTMD incidence rate was calculated as the national incidence rate of TB multiplied by the ratio of new PNTMD to TB cases reported by the responding hospitals .\n\n【2】To clarify the chronologic changes in incidence, we followed the same method for comparing TB and PNTMD used in a prior epidemiologic study in Japan . We established methods for maximizing survey response rates and facilitating ease of completion by offering extensive support to survey recipients .\n\n【3】We achieved a high response rate of 62.3% (551 hospitals), and in all regions the response rate exceeded 50% . The numbers of newly diagnosed cases were 2,327 for TB and 2,652 for PNTMD. Because the incidence rate for TB was reported to be 12.9 cases per 100,000 person-years, that of PNTMD was estimated to be 14.7 cases per 100,000 person-years, which is ≈2.6 times the incidence rate reported in 2007 . By using the same method, we found the incidence of pulmonary MAC, _M. kansasii,_ and _M. abscessus_ disease to be 13.1, 0.6, and 0.5 cases per 100,000 person-years, respectively . The ratio of pulmonary _M. avium_ disease to MAC was higher in the northern and eastern parts of Japan, whereas the ratio of pulmonary _M. intracellulare_ disease to MAC was higher in the southern and western parts of Japan .\n\n【4】From this survey, we observed that the incidence rate of PNTMD may exceed that of TB and that incidence rates of PNTMD in Japan may be among the highest worldwide . This finding implies that the prevalence of PNTMD as a chronic infection is estimated to be much higher than that of TB.\n\n【5】We assume that the high rates of PNTMD in Japan are consistent with data suggesting that Asians are particularly susceptible to PNTMD . Other factors contributing to the increase might be the simplified diagnosis according to the 2007 American Thoracic Society/Infectious Diseases Society of America statements, increased awareness by medical staff, population aging, and increased frequency of medical checkups with computed tomography of the chest.\n\n【6】Another finding was the characteristic gradient clustering of the ratios of _M. avium_ and _M. intracellulare_ . This finding supports the widely accepted belief that environmental factors strongly affect the epidemiology of PNTMD; therefore, the role of factors such as soil, humidity, temperature, and saturated vapor pressure should be seriously considered .\n\n【7】We also found dramatic increases in incidence of pulmonary _M. abscessus_ disease and pulmonary MAC disease, whereas incidence of pulmonary _M. kansasii_ disease was stable. Although we did not distinguish _M. massiliense_ from _M. abscessus_ , the incidence rate for pulmonary _M. abscessus_ disease increased from 0.1 cases in 2001 to 0.5 cases per 100,000 person-years in 2014. This epidemiologic tendency should be monitored .\n\n【8】This study has several limitations. First, differing characteristics between the responding and nonresponding hospitals could cause bias. Second, we did not collect data outside of hospitals. Third, incomplete reporting could undermine the accuracy of our estimates . Therefore, the epidemiologic data should be verified by using other approaches .\n\n【9】The dramatic increase in incidence rates for PNTMD warrants its recognition as a major public health concern. Because the prevalence rates of this currently incurable lifelong chronic disease are estimated to be high, the effect on the community could be enormous. Further investigations are needed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "89d8dcc5-6300-4def-85e5-c94c0e13739d", "title": "Underreported Threat of Multidrug-Resistant Tuberculosis in Africa", "text": "【0】Underreported Threat of Multidrug-Resistant Tuberculosis in Africa\nGlobal control of tuberculosis (TB) has been jeopardized by 2 major threats: HIV/AIDS and multidrug-resistant TB (MDR TB). MDR TB is defined as strains of _Mycobacterium tuberculosis_ that are resistant to at least isoniazid and rifampin . Drug resistance has reached alarming levels with the emergence of strains that are virtually untreatable with existing drugs. The recent report of an outbreak of extensively drug-resistant TB (XDR TB) in South Africa , with its extremely high case-fatality rate, has drawn wide attention. However, the more general problem of MDR TB, with an estimated 450,000 cases worldwide annually, has been recognized since the first World Health Organization (WHO) global survey on drug resistance in the late 1990s .\n\n【1】According to the WHO Global Report on Anti-tuberculosis Drug Resistance in the World , MDR TB strains have emerged in all regions of the world. However, despite the dramatic increase in TB rates in Africa, MDR TB appears nearly absent from this continent, which, until recently, reported the lowest median levels of drug resistance.\n\n【2】Two explanations have been most commonly put forward to explain these reported low levels of MDR TB in Africa. The first explanation is the presence of well-functioning control programs in Africa. Eighty-nine percent of the population in the WHO-defined region of Africa is covered by directly observed therapy, short course (DOTS), which is similar to the global average . However, there is discordance between purported DOTS coverage and national TB program (NTP) efficacy. Each year, countries with the lowest case detection and cure rates are clustered in the WHO-defined Regional Office for Africa (AFRO) region . This suggests that NTPs in Africa are not performing better than their Eastern European or South American counterparts, where MDR TB rates have already reached alarming levels. The functional status of many programs in Africa is difficult to assess with certainty, and the high incidence rates on that continent indicate that programs may not be functioning well. Low case-detection rates alone may not lead to development of MDR TB. Other factors that might favor development of MDR TB include the availability of drugs on the open market and a private sector that delivers drugs to the population in an unregulated fashion.\n\n【3】The second explanation is the recent introduction of rifampin in Africa. It is often stated that because rifampin was only recently introduced in Africa on a large scale, there has been relatively little time for resistance to develop. However, development of rifampin resistance may be spurred by HIV infection, and such resistance appears to develop rapidly .\n\n【4】Our research described in this report indicates the possibility of a third and straightforward explanation: field results for Africa are still incomplete, despite the recent publication of the WHO Fourth Global Report . This report is the result of independent drug resistance surveys (DRSs) conducted throughout the world following strict guidelines. The number of countries participating in the overall survey was low, especially in the Africa region. Furthermore, one might speculate that those countries capable of providing DRS data may have been the ones most likely to have a well-functioning NTP, laboratory structures, and transport networks, which would bias overall reporting. However, even if this is not the case, overall low levels of reporting make findings of the report questionable.\n\n【5】Consequently, the low level of reported drug resistance in Africa may not be an accurate reflection of reality, and the limited evaluation may be responsible for this skewed portrayal. Lack of comprehensive national DRS data from all countries in Africa is a barrier to understanding the magnitude of prevalence and incidence of MDR. In light of the added threat of XDR TB within the African context of high HIV prevalence, a thorough assessment of TB drug resistance and evaluation of data-specific deficiencies is urgently needed.\n\n【6】Our report aims to provide a more accurate assessment of MDR TB in Africa by incorporating all the latest available data and published estimates. In addition to suggesting that the effect of MDR TB in Africa is higher than previously thought, our report explores trends and relationships among case detection, treatment success, retreatment failure, and HIV rates relative to MDR TB. We attempted to identify population-based factors associated with MDR TB. Given the limited healthcare funding and substantial incidence of HIV in Africa, even a relatively low but increasing tide of MDR TB can lead to disastrous consequences for this continent.\n\n【7】### Methods\n\n【8】##### Search Strategy\n\n【9】PubMed and Medline databases were searched for reviews and original reports based on primary studies by using the keywords tuberculosis, _Mycobacterium tuberculosis_ , multidrug-resistant, and Africa. English and French language articles were reviewed.\n\n【10】##### Study Selection\n\n【11】Data were assembled into a comprehensive composite on the basis of several criteria. Published MDR TB rates from WHO were prioritized because of their reliability and because they are the result of DRSs conducted throughout the world following strict guidelines: new patients were clearly distinguished from those with previous TB treatment, and optimal laboratory performance was ensured and maintained through links with supranational reference laboratories.\n\n【12】Second, data on MDR TB were collected from peer-reviewed journals. Articles were considered only if specific criteria were met: 1) new patients needed to be distinguished from patients with prior TB treatment; 2) the number of confirmed TB cases needed to be statistically significant; and 3) the study needed to be nationwide or the population covered needed to be greater than one fourth of the total population.\n\n【13】For countries where data were unavailable through WHO reports or recent peer-reviewed journals, published formulaic estimates by Zignol et al. were used . The formula considers 9 independent variables considered likely to be associated with drug resistance and used in regression analyses. Resulting estimates compare favorably with known MDR TB values.\n\n【14】##### Data Extraction\n\n【15】The following data were tabulated for all forms of TB for all countries under scrutiny. Presurvey and postsurvey variables were annual country-specific indicators averaged from 1995 to the year of the drug resistance survey (presurvey) and from year after drug resistance survey to 2005 (postsurvey). These variables included TB incidence rate, case detection rate, treatment rates (category I: cured, completed, failure, default and success), and retreatment rates (category II: cured, completed, failure, default, and success). Year 2005 variables were published MDR TB estimates for new cases of pulmonary TB, HIV prevalence among adults 15–49 years of age, TB-related death rate, HIV/TB co-infection rate, male-to-female ratio of case notification rate, and health funding per capita (US dollars) in 2002 obtained from the United Nations Development Program .\n\n【16】Combined MDR TB rates were compiled for 39 of the 46 countries in the AFRO region. For 21 of those 39 countries, MDR TB rates were gathered from either WHO reports or peer-reviewed articles. For the remaining 18 countries, MDR TB rates were reported by using a formula described by Zignol et al. No DRS data or formulas were available for 6 countries, Cape Verde, Comoros, Liberia, Mauritania, Sao Tome and Principe, and the Seychelles, which account for only 20,475 of the 2,528,915 TB cases in Africa . Additionally, the accuracy of annual data from Mauritius was deemed questionable and thus excluded from further analysis.\n\n【17】##### Regression Analysis of Continuous Variables\n\n【18】The unadjusted associations between continuous independent variables and MDR TB rates were evaluated by using a linear correlation matrix. Independent variables highly (p<0.05) or marginally (p<0.10) predictive of the outcome in unadjusted models were included in further linear multivariable modeling. In addition, interactions between case detection, treatment failure, and retreatment failure rates were tested by using univariate linear regression analyses. All analyses were conducted in Stata Statistics version 9.2 (StataCorp, College Station, TX, USA).\n\n【19】##### Analysis of Presurvey and Postsurvey Continuous Variables\n\n【20】The change between presurvey and postsurvey averages (average value from 1995 to year of drug resistance survey, and year of drug resistance survey to 2005, respectively) for each continuous independent variable was evaluated by using the nonparametric Wilcoxon signed rank test. Given the small sample size, these variables were not assumed to be normal. A change between presurvey and postsurvey was deemed statistically significant if the p value was <0.05 and marginally significant if <0.10.\n\n【21】##### Categorical Trend Analysis\n\n【22】In addition to relationships between continuous MDR TB rates and independent factors, a categorical analysis of MDR TB was conducted. Countries were divided into 3 categories (tertiles) on the basis of MDR TB rates: (0.0–1.7, 1.8–2.1, and \\> 2.2 defined as low, middle, and high MDR rates categories, respectively). For each TB-related indicator, a univariate trend across the 3 MDR TB categories (low, middle, and high) was assessed. Trend analysis was conducted by using the nptrend function in Stata Statistics version 9.2 (StataCorp), which performs a nonparametric trend test of rank sums across ordered groups. A trend was deemed statistically significant if the p value was <0.05 and marginally significant if <0.10.\n\n【23】### Results\n\n【24】##### MDR TB Rates in Africa\n\n【25】Data on prevalence of MDR TB among all TB cases collected from various WHO publications and published peer-reviewed articles are shown in Table 1 . The 39 countries considered in this study encompass ≈99% of total estimated incident or prevalent TB cases (all forms) in the AFRO region. The proportion of MDR TB among all TB cases varies from 5.8% in the Democratic Republic of Congo to virtually 0% in Kenya. The median MDR TB rate was ≈1.9% .\n\n【26】The situation in Africa for MDR TB among all combined TB cases is depicted in a series of related maps . The first map is reproduced and translated from the WHO Third Global Report published in 2004  . The coloring scheme was translated into the 3 categories used in this report, and the continent appears particularly devoid of MDR TB.\n\n【27】Panel B of the Figure includes data from various recent WHO publications, peer-reviewed journal articles, and the Fourth Global Report published in 2008 . Indications of geographic clustering are apparent; countries with a high prevalence of MDR TB are clustered in the southeastern and central regions of Africa. Panel B of the Figure also shows the 11 countries that have MDR TB rates >2.0 of all combined TB cases, including Democratic Republic of Congo, Rwanda, Equatorial Guinea, and Senegal, which were not included the WHO Third Global Report.\n\n【28】Panel C of the Figure includes formulaic estimates. On the basis of these estimates, high levels of MDR TB are not only located in the southeastern and central regions, but also in West Africa, which differs from results of the WHO Third Global Report.\n\n【29】##### Effect of Year of Introduction of Rifampin\n\n【30】Dates of rifampin introduction were available for 11 countries . Dates of introduction are clustered around the mid-1980s. The earliest and latest known introductions of rifampin were 1979 in South Africa and 1989 in Zambia. Zambia has a middle-level rate of MDR TB, South Africa has a high rate, and other countries have low rates. No clear association between date of rifampin introduction and MDR rates could be discerned.\n\n【31】##### Indicators for Continuous MDR Rates\n\n【32】Presurvey independent variables were evaluated through 3 types of linear regression analysis: correlation , univariate, and multivariate regression (data not shown). Regarding associations with MDR rates, correlation analysis showed a significant association with retreatment failure rate (p = 0.043, r 2  \\= 0.119). Several other univariate and multivariate linear models, such as treatment failure × retreatment failure and case detection × treatment failure × retreatment failure, were marginally significant. However, no other models that used presurvey data were more predictive than retreatment failure rate alone. For postsurvey variables, univariate analysis  also showed a statistically significant association between MDR rates and retreatment failure rates.\n\n【33】##### Comparison of Presurvey and Postsurvey Indicator Averages\n\n【34】Using the Wilcoxon signed rank test, we determined that 5 of 14 indicators showed a significant (p<0.05) change between presurvey and postsurvey years. Treatment default showed a decrease; incidence rate, treatment cured, treatment success, and retreatment death rate increased between time intervals.\n\n【35】##### Indicators for MDR TB Categories\n\n【36】When considered through nonparametric trend analysis, there was a marginally significant positive association between MDR TB categories and retreatment failure rates (data not shown). Median retreatment failure rates were higher within relatively higher categorized MDR TB countries.\n\n【37】### Discussion\n\n【38】Our visual mapping indicates that MDR TB is likely to be more prevalent in Africa than previous reports indicated. The latest WHO Global Report on Anti-tuberculosis Drug Resistance in the World published in March 2008  indicated rates of drug-resistant TB in Africa to be among the lowest worldwide. However, as of April 2008, 25 of the 46 countries in the AFRO region still had not completed a national study to investigate levels of MDR TB. By incorporating the latest national studies on MDR TB in Africa, and classifying countries on the basis of MDR TB rates in combined cases of TB, we found worrisome trends: 6 countries that were not included in the WHO Third Global Report published in 2004 have MDR TB rates >2.0% of all combined TB cases. This finding suggests that completing DRSs for all or most countries in the AFRO region is urgently needed and that the MDR TB threat in Africa could be much higher than originally assessed by WHO in its previous report in 2004. Drug-resistant strains, along with HIV/AIDS, are causing the biggest challenge to efficient management and control of TB.\n\n【39】The lower rates of MDR TB in Africa, when compared with rates in Eastern Europe or South America, could be related to the fact that for many years Africa was neglected and TB was not treated. Alternatively, later introduction of rifampin in drug regimens is often cited as an explanation for these low rates. However, for the few countries for which data were available, we did not find a statistically significant relationship between year of rifampin introduction and level of MDR TB.\n\n【40】We subsequently set out to investigate linear associations between country-specific factors as well as categorical trends in TB management to discriminate levels of MDR TB in Africa. Retreatment failure rate was the most predictive indicator of MDR TB rates; other variables such as average case detection rate , average TB incidence rate , TB prevalence , and HIV/TB co-infection  did not show a linear relationship. Categorical analysis showed similar results.\n\n【41】One can speculate about the reasons that retreatment failure rates may be associated with higher rates of MDR TB. The relationship may simply be an association without causation, i.e. that retreatment fails because a given patient may already have MDR TB. In that sense, retreatment failure is simply a marker for preexisting MDR TB. However, it is also possible that retreatment may be a cause of MDR TB. Current WHO recommendations for retreatment regimens could lead to development of MDR TB or XDR TB in many instances because suggested retreatment regimens potentially amount to addition of 1 drug to a failing regimen, thereby intensifying the level of drug resistance.\n\n【42】We subsequently investigated whether combining the retreatment failure rate with other indicators in our model could generate a predictive combination for MDR TB rates. Whereas the univariate linear and the multivariate regression model showed a marginally significant association with either treatment failure, case detection rate, or both, retreatment failure as a sole indicator remained the most significant. This finding may indicate that, regardless of the successful record of a given NTP in detecting cases and subsequently successfully treating them with a category I regimen, the highest MDR TB rates were found in the countries with the highest retreatment failure rates (category II).\n\n【43】We also investigated levels of variables reported in years after a DRS to determine whether the result of the survey affected these variables. Five of the 14 variables measured showed a statistically significant change over time: 4 variables (case detection rates, treatment cure rates, treatment success rates, and retreatment death rates) increased after the survey, and treatment defaulters decreased. Unfortunately, we cannot determine the effect of those 5 indicators on MDR TB rates because most countries surveyed reported only 1 DRS. Retreatment failure, which was shown to be the most predictive indicator of MDR TB rates, did not change over time. This finding suggests that MDR TB rates have increased in the years after the survey. To draw a parallel with physics, if the acceleration of an object remains constant over time, its speed increases. Therefore, it seems critical for countries who have already reported DRS results to undergo a second DRS to monitor the evolution of the MDR TB rate originally reported and help determine the effect of newly installed healthcare and TB treatment systems on drug resistance. For example, the only DRS reported from Kenya (conducted in 1995) reported an MDR TB rate of 0.0%. In 2008, the validity of this result is highly questionable given the recently published high MDR TB rates of neighboring countries .\n\n【44】This research was limited by the differing country-level data estimates and the ecologic study design. Twenty one of the 39 countries analyzed have TB estimates from WHO or peer-reviewed journals. However, the other 19 countries required formulaic estimates of MDR TB rates. The effect of these differing estimation methods depends on the congruity of future country-level estimates relative to the figures accepted in this study. Therefore, analyses should be repeated as more definitive estimates become available. Second, the ecologic study design, which analyzes these TB-related factors on a population level, limits the transference of any apparent relationships to the individual level. However, evidence shows that an effective DOTS program can limit the development of drug resistance on the individual TB patient level . Lastly, the small sample size of 39 countries limits the power to statistically determine differences. As a final note, this report hoped to realize 2 goals: to provide a composite of MDR TB in Africa and to identify relationships and trends.\n\n【45】Trends and relationships aside, MDR TB in Africa is a burgeoning obstacle that shows no signs of regression. The prevalence of MDR TB remains below the levels seen in Central Europe and parts of Latin America. However, in Africa, the tragically high HIV prevalence and limited funds and infrastructure dedicated to healthcare are serious factors. As the outbreak of MDR TB/HIV co-infection in New York City in the mid-1990s taught the medical community, this co-infection is pernicious, difficult, and overwhelmingly expensive to treat. This situation begs the question: if the continent is finding difficulty addressing TB, a well-defined disease caused by a well-defined agent, which is fully treatable with effective and affordable drugs through internationally recommended guidelines, then how will the continent fare against MDR TB and XDR TB?", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dd01caa2-6e07-4cd2-ade6-a6bc58155a80", "title": "The Pandemic Century: One Hundred Years of Panic, Hysteria and Hubris", "text": "【0】Audiobook narrated by John Lee; 13 h 40 min ;  \nHighBridge, a division of Recorded Books ; $23.70 – $33.09\n\n【1】Long before microbes were discovered, a sudden occurrence of disease in a society would trigger fear, accompanied by rational and irrational responses. A mysterious, highly contagious disease that spread across the Mediterranean and killed a quarter of the population of Athens is the first recorded pandemic .\n\n【2】_The Pandemic Century: One Hundred Years of Panic, Hysteria and Hubris_ by Mark Honigsbaum, a medical historian at the City University of London (UK), focuses on large-scale outbreaks since 1900. The author uses 9 examples to shine a light on epidemiological blind spots to avoid in future investigations. For example, he suggests that the initial lackluster response to the 1918 influenza pandemic can be attributed in part to false confidence in Richard Pfeiffer’s 1892 discovery of _Bacillus influenzae_ , now known as _Haemophilus influenzae._ We later learned that _H. influenzae_ occasionally is found in persons with influenza but was not the cause of the pandemic.\n\n【3】Through colorful and engaging narrative, Honigsbaum probes the social context of early 20th Century America that led investigators to attribute differences in pneumonia rates among African Americans to racial factors compared with white soldiers at Camp Funston, Kansas, thought to be an early site of the 1918 influenza pandemic. He details how public hysteria, fear, and conspiracy theories hindered public health responses in other outbreaks, such as in the 1976 Legionnaires’ disease outbreak in Philadelphia, Pennsylvania. He includes laboratory and epidemiological investigations that led to identification of _Legionella pneumophila_ in 1977 . The film, Influenza 1918, which aired on the Public Broadcasting Service, also profiles the pandemic .\n\n【4】The book at times offers a gloomy assessment of how, in today’s era of international travel, ecological disturbances and human behaviors have tipped the scales in favor of pathogens. It also highlights lapses that occurred during outbreak investigations, such as how mishandled laboratory samples delayed recognition of severe acute respiratory syndrome (SARS) in 2002 and how cross-border travel and the lack of appreciation for local cultural practices by investigators facilitated spread of Ebola virus in West Africa during 2014–2016.\n\n【5】The text concludes with a description of global efforts to anticipate pandemics, such as Resolve, a public health initiative supported by philanthropic institutions including the Bill and Melinda Gates Foundation. Honigsbaum notes uncertainties and gaps here too, such as Madagascar’s ineligibility to receive support to respond to a massive _Yersinia pestis_ outbreak from a World Bank fund. The fund was established after the Ebola outbreak in West Africa and only can be used for diseases caused by viruses that have certain characteristics and threaten to spread internationally .\n\n【6】This book appealed to me because it condenses events spanning a century into readable segments told by an author known for in depth historical accounts. _The Pandemic Century_ could appeal to diverse categories of readers, including epidemiologists, public health workers, students, and anyone interested in understanding why, despite impressive gains in global public health preparedness and advances in disease prevention and control, pandemics continue to surprise and terrify us.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9ade0f83-024a-4807-96be-828948122c1b", "title": "Resource Allocation during an Influenza Pandemic", "text": "【0】Resource Allocation during an Influenza Pandemic\n### To the Editor\n\n【1】Considerable progress has been made in the United Kingdom to prepare for an influenza pandemic. After public consultation, an updated national framework  was recently published, along with new guidance on ethics , surge capacity, and clinical prioritization .\n\n【2】As Paranthaman et al. pointed out , difficult ethical choices must be made during a pandemic. Therefore, the UK Committee on Ethical Aspects of Pandemic Influenza published the ethical framework  designed to assist with and support the ethical aspects of policy and clinical decision making during and after an influenza pandemic. The fundamental principle underpinning the ethical framework is equal concern and respect, and it is expected that this principle, supported by 7 others listed in the guidance, will be used by clinicians, managers, and healthcare planners to develop policies on clinical issues for use during a pandemic. It is recognized and acknowledged within the document that the weight of a given principle will vary according to the circumstance.\n\n【3】Equally relevant is the interim guidance on surge capacity and prioritization in health services , which sets out a framework for the health service response in the United Kingdom during a pandemic and which advocates the wider use of the clinical triage criteria described for critical care by Christian et al . The proposed use of clinical triage at the primary care/secondary care interface starts to address the issue raised by Paranthaman et al. of who should be admitted to a hospital. This guidance expands early UK guidelines on the management of influenza-like illness during a pandemic.\n\n【4】We agree with Paranthaman et al. that early surveillance data are needed to rapidly inform clinical care guidelines in a pandemic. Therefore, efforts are ongoing to increase the resilience of health surveillance data gathering systems in the United Kingdom and to develop clinical systems for specific use during a pandemic. At the onset of a pandemic, it is intended that data will be gathered on the first few hundred patients by using a modification of the Web-based avian influenza management system of the Health Protection Agency. These data will provide important virologic and epidemiologic information to characterize the pandemic virus and inform modeling assumptions to validate “now casting” or real-time mathematical models  being developed in the United Kingdom and Europe to estimate the likely spread and impact of the pandemic. Furthermore, pilot projects are in preparation to develop clinical data collection systems in secondary care to assess treatments and outcomes during a pandemic.\n\n【5】In conclusion, contingency decisions outside normal patient pathways will be needed; the UK guidance, based on current knowledge and understanding, will help clinicians make difficult decisions on patient prioritization, plan surge capacity, build resilience into existing surveillance systems, and develop new systems that seek to inform the best use of resources to deliver optimal clinical care during an influenza pandemic. These decisions will be revised and modified to reflect new developments in the science.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dc31e3a2-2e26-4345-bb82-34ce2fa1a66b", "title": "CTX-M-65 Extended-Spectrum β-Lactamase–Producing Salmonella enterica Serotype Infantis, United States", "text": "【0】CTX-M-65 Extended-Spectrum β-Lactamase–Producing Salmonella enterica Serotype Infantis, United States\nA leading cause of bacterial foodborne disease in the United States is nontyphoidal _Salmonella_ . _Salmonella enterica_ serotype Infantis (hereafter called _Salmonella_ Infantis) is one of the most common _Salmonella_ serotypes in the United States and 1 of 3 serotypes for which incidence has substantially increased (by 60%) in the past 10 years . _Salmonella_ Infantis has been identified in a variety of foods, animals, and environmental settings.\n\n【1】Salmonellosis usually causes a self-limited gastroenteritis; however, current guidelines recommend that antimicrobial therapy be considered for groups of persons at increased risk for invasive infection. For those patients, treatment with ceftriaxone, ciprofloxacin, trimethoprim/sulfamethoxazole, or amoxicillin is recommended . Extended-spectrum β-lactamases (ESBLs) confer resistance to most third-generation cephalosporins and penicillins, including ampicillin.\n\n【2】Some _Enterobacteriaceae_ produce CTX-M ESBLs, which are encoded by _bla_ CTX-M  genes that were discovered in 1989 . Since then, their prevalence has increased dramatically  and they have been isolated worldwide, primarily from _Escherichia coli_ . Identification of ESBLs, including the CTX-M types, in _Salmonella_ in the United States is relatively rare .\n\n【3】In the United States, the National Antimicrobial Resistance Monitoring System (NARMS) is a surveillance system that tracks changes in the antimicrobial susceptibility of certain enteric bacteria isolated from ill persons, retail meats, and food animals. In July 2015, the US Food and Drug Administration (FDA) notified the Centers for Disease Control and Prevention (CDC) of a CTX-M-65–producing _Salmonella_ Infantis strain isolated from retail chicken meat in December 2014 . The isolate had a rare pulsed-field gel electrophoresis pattern, JFXX01.0787 (pattern 787). To clarify the sources and potential effects of this strain on human health, we analyzed data from several CDC surveillance systems to describe the prevalence, epidemiology, antimicrobial drug resistance, and molecular phylogenetics of _Salmonella_ Infantis pattern 787 isolates from humans.\n\n【4】### Methods\n\n【5】##### Background Rates\n\n【6】To determine the expected demographics, rates of hospitalization, and international travel among patients with _Salmonella_ Infantis infections compared with patients with infections caused by other common nontyphoidal _Salmonella_ serotypes, we analyzed data collected through the Foodborne Disease Active Surveillance Network  during 2012–2015. Begun in 1996, FoodNet has conducted active, population-based surveillance for culture-confirmed cases of infection caused by 9 pathogens transmitted commonly through food, including _Salmonella_ . FoodNet is a collaboration of CDC, 10 state health departments, the US Department of Agriculture Food Safety and Inspection Service (USDA-FSIS), and the FDA. The FoodNet surveillance area includes 15% of the US population; these data are used to estimate the burden of US foodborne illnesses, hospitalizations, and deaths . We defined other common nontyphoidal _Salmonella_ as the top 20 _S. enterica_ serotypes (excluding Infantis) isolated from humans: Typhimurium, Enteritidis, Newport, Heidelberg, Javiana, Saintpaul, Montevideo, Agona, Oranienburg, Muenchen, Thompson, Hadar, Braenderup, Derby, I 4,12:i:-, Paratyphi B var. L(+) tartrate+, Blockley, Anatum, Mississippi, and Panama. These 20 serotypes represented 69% of nontyphoiodal _Salmonella_ isolates reported to FoodNet in 2015.\n\n【7】##### Case Finding\n\n【8】We looked for pattern 787 isolates reported to the National Molecular Subtyping Network for Foodborne Disease Surveillance  through 2017. The PulseNet database contains pulsed-field gel electrophoresis patterns from state and local public health laboratories and food regulatory agencies. Only the first isolate from each patient was included in case counts.\n\n【9】CDC also requested patient data and clinical isolates from state and local public health departments for any case with pattern 787 reported through October 2015. Patient data included age, sex, date of symptom onset, hospitalization, and recent international travel (defined as travel outside of the United States in the 7 days before symptom onset). Isolate data included specimen collection date(s) and source.\n\n【10】##### Isolate Characterization\n\n【11】We used the NARMS standard broth microdilution protocol (Sensititer; Thermo Fisher Scientific, Oakwood Village, OH, USA) to determine the MICs for 14 antimicrobial agents: gentamicin, streptomycin, ampicillin, amoxicillin/clavulanic acid, ceftiofur, ceftriaxone, cefoxitin, azithromycin, sulfasoxazole, trimethoprim/sulfamethoxazole, chloramphenicol, ciprofloxacin, nalidixic acid, and tetracycline . These agents were categorized into 9 classes defined by the Clinical and Laboratory Standards Institute guidelines ; where available, CLSI interpretive criteria were used to define resistance. Transformation studies to confirm plasmid-associated genes were conducted by using plasmid purification and electroporation as previously described .\n\n【12】Whole-genome sequencing was performed according to the PulseNet protocol for the Illumina MiSeq (Illumina, La Jolla, CA, USA) . Closed PacBio genomes were generated as part of a previous study and used as references where indicated . Genome assemblies for short-read data were generated de novo and analyzed for acquired antimicrobial drug–resistance determinants and plasmid replicons by using ResFinder and PlasmidFinder . To confirm the absence of certain genes, we performed read mapping in CLC Genomics Workbench version 8.5 . To identify mutational resistance, we extracted the _gyrA_ and _parC_ genes from genome assemblies by using a perl script . To identify mutations in the quinolone resistance–determining regions of these genes, we aligned gene sequences in CLC Workbench. To assess isolate relatedness, we generated high-quality single-nucleotide polymorphism (hqSNP) phylogenies. In brief, isolates were aligned to the closed chromosomal sequence of 2014AM-3028 (omitting the plasmid contig) by using Lyve-SET-v1.1.4f . Genome alignments were processed by using Gubbins  to omit areas of recombination, uninformative sites were removed, and the resulting SNP alignment was used to calculate pairwise differences and generate hqSNP trees by using scripts bundled with Lyve-SET . We performed a phylogeographic analysis (a type of molecular clock analysis) by using Bayesian Evolutionary Analysis Sampling Trees .\n\n【13】To sample more diverse _Salmonella_ Infantis isolates, we obtained sequence data on isolates from sources other than CDC NARMS assigned to the same National Center for Biotechnology Information (NCBI) SNP cluster PDS000003955.192 as our study isolates on the NCBI Pathogen Detection page . These additional genomes were from isolates recovered from hospitalized patients, meat, and environmental samples from Peru during 2010–2014 and collected by the Center for Food Safety and Applied Nutrition at FDA and the US Naval Medical Research Unit , and isolates from chicken samples obtained at slaughter by USDA-FSIS. These isolates were used to produce a time-measured maximum clade credibility tree to estimate the dates that study isolates in addition to all isolates (study isolates and isolates from Peru and cecal isolates from USDA-FSIS) shared most recent common ancestors (MRCAs), and the geographic location of these MRCAs . The date of the MRCA was estimated by reporting the height of the most ancestral node of the maximum clade credibility tree and the 95% highest posterior density (HPD) intervals for these estimates. More details on genetic analyses are available in the Technical Appendix .\n\n【14】##### Statistical Analyses\n\n【15】We used the χ 2  test (or Fisher exact test when cell counts were ≤5) for statistical comparisons. All denominators exclude persons for whom data were missing. We considered p<0.05 to be significant. All p values were 2-tailed. We used SAS 9.3 or 9.4 (SAS Institute, Cary, NC, USA) to conduct our analyses.\n\n【16】### Results\n\n【17】The first pattern 787 _Salmonella_ Infantis isolate from a human in the United States was collected from a patient in June 2012. By the end of 2017, PulseNet contained 312 _Salmonella_ Infantis pattern 787 isolates from persons living in 43 states; Washington, DC; and Puerto Rico. The number of cases detected each year increased from 5 in 2012 to 174 in 2017 and represented 8.4% of all _Salmonella_ Infantis isolated that year .\n\n【18】State health departments submitted 34 pattern 787 isolates from humans to CDC; 29 (85%) had resistance phenotypes consistent with ESBL-conferred resistance to ceftriaxone, ceftiofur, and ampicillin . All 29 isolates with ESBL-resistant phenotypes had the _bla_ CTX-M-65  gene. In addition to ESBL-conferred resistance, these isolates were also resistant to chloramphenicol, sulfisoxazole, tetracycline, nalidixic acid, and trimethoprim/sulfamethoxazole and intermediately susceptible to ciprofloxacin and gentamicin. Resistance to these drugs was plasmid mediated and transferable to _E. coli,_ except for nalidixic acid and ciprofloxacin, for which resistance was caused by a chromosomal mutation in _gyrA_ (D87Y). Transferable resistance resulted from a large IncFIB-like plasmid containing the resistance genes _aph(3′)-Ic_ , _aph -Ia_ , _aadA1_ , _aac -IVa_ , _bla_ CTX-M-65  , _fosA_ , _floR_ , _sul1_ , _tetA_ , and _dfrA14_ . These observations are consistent with the organization of these genes on a single IncF1B-like plasmid closed by using long read sequencing, which was completed as part of a previous study by Tate et al. who recently published a further detailed description of this plasmid and a comparative genomics analysis of _bla_ CTX-M-65  –positive IncFIB-like plasmids from _Salmonella_ Infantis in the United States .\n\n【19】We focused our analysis on the 29 patients with _Salmonella_ Infantis isolates containing the _bla_ CTX-M-65  gene (hereafter called CTX-M-65 Infantis). We compared patients infected with CTX-M-65 Infantis with patients infected with all strains of _Salmonella_ Infantis and with patients infected with common _Salmonella_ serotypes other than Infantis. The median age of patients with CTX-M-65 Infantis was 25 years (interquartile range \\[IQR\\] 15–50 years), and 69% were female . The median age of patients with any strain of _Salmonella_ Infantis was 37 years (IQR 12–58 years), and 56% were female. The median age of patients infected with other common _Salmonella_ serotypes was 29 years (IQR 6–54 years), and 52% were female.\n\n【20】Of 18 patients infected with CTX-M-65 Infantis for whom outcome data were available, 8 (44%) were hospitalized, compared with a hospitalization rate of 29% among patients in both comparison groups (those with Infantis and those with other _Salmonella_ serotypes). Patients with CTX-M-65 Infantis infection were more likely (17%) to have the organism isolated from urine compared with all patients with _Salmonella_ Infantis (9%, p = 0.14) and patients with other common serotypes (5%, p<0.01).\n\n【21】For 20 CTX-M-65 Infantis–infected patients with information about symptom onset date and isolation date(s), _Salmonella_ Infantis was recovered from 12 who provided fecal samples >2 weeks after their reported date of symptom onset. A total of 8 patients were still reporting symptoms and had positive cultures >30 days after symptom onset, including 3 patients who sought care and had multiple isolates recovered from 50 days to 8 months after symptom onset.\n\n【22】Twelve (63%) patients with CTX-M-65 Infantis infections reported international travel in the 7 days before symptom onset. All reported travel to South America, 10 to Peru, and 2 to Ecuador . Travel was less common among all _Salmonella_ Infantis–­­infected patients (7%, p<0.01) and those infected with other common serotypes (8%, p<0.01).\n\n【23】A maximum-likelihood hqSNP phylogeny of the 34 isolates from humans and that from retail chicken meat revealed that isolates from patients with travel-associated infections formed a well-supported clade (clade A) with isolates from patients with infections not associated with travel . Isolates in clade A differed by only 2–47 pairwise hqSNPs, suggesting that these isolates recently evolved from a common ancestor. Included in clade A was the isolate from retail chicken meat collected in 2014.\n\n【24】Of the 5 CTX-M-65-negative isolates, 2 grouped within clade A and contained a multidrug-resistance plasmid that was lacking the _bla_ CTX-M-65  gene. An additional 3 CTX-M-65–negative isolates lacked the IncFIB-like plasmid replicon, lacked resistance determinants, and differed by 96–273 pairwise hqSNPs from isolates in clade A. Absence of the _bla_ CTX-M-65  gene in these 5 isolates was confirmed by read-mapping procedures and supported by the observed phenotypic susceptibility to β-lactam antimicrobial drugs.\n\n【25】A phylogeographic analysis of the 32 isolates in the main clade and additional genomes from NCBI generated a time-measured phylogenetic tree of isolates from the United States and Peru . These isolates last shared a common ancestor around 2006 (95% HPD interval 2003–2008). This analysis also suggests that the MRCA of these isolates existed in Peru with a probability of 98.7%. The probability of the tree being rooted in the United States was ≈1.3%. Clinical isolates from humans in the United States sequenced as part of our study last shared a common ancestor sometime around 2009 (95% HPD interval 2008–2009), before the first isolation of pattern 787 in the United States in 2012.\n\n【26】### Discussion\n\n【27】A new strain of _Salmonella_ Infantis, which has pattern 787 and frequently carries a multidrug-resistant plasmid with a CTX-M-65 ESBL, has emerged in the United States. This strain possesses clinically important resistance associated with higher hospitalization rates. Using epidemiologic and phylogenetic methods, we demonstrated that the earliest cases of CTX-M-65 Infantis infection were among travelers returned from South America, whereas subsequent infections were acquired domestically.\n\n【28】The demographic and clinical characteristics among patients with CTX-M-65 Infantis infections differed from those of patients infected with all strains of _Salmonella_ Infantis or other common _Salmonella_ serotypes. CTX-M-65 Infantis–infected patients were younger and more frequently female, and rates of hospitalization were 50% higher for these patients than for those in the other 2 groups. Among patients with CTX-M-65 Infantis infections, 63% reported recent travel to South America, predominantly to Peru, compared with <10% in the comparison groups reporting travel. This finding is consistent with those of other studies that found foreign travel to be a risk factor for CTX-M–type ESBLs . Studies have shown that CTX-M-65 has recently emerged in commensal _E. coli_ in Bolivia  and in _Salmonella_ Infantis in Ecuador , 2 countries to which patients in our study also traveled.\n\n【29】Our phylogeographic analysis, which included many isolates from Peru, suggested that isolates from patients in the United States last shared a common ancestor that existed in Peru in 2009. This finding makes sense, given the high proportion of patients in our study who reported travel to Peru and the known circulation of CTX-M-65–positive _Enterobacteriaceae_ in South America. In our study, patients with CTX-M-65 Infantis infection and a history of travel were reported every year from 2012 through 2015; patients with no history of travel were first reported in 2014.\n\n【30】One caveat is that genetic information for CTX-M Infantis from countries in South America other than Peru was not available; therefore, we could not distinguish the role that other countries in South America may have played in the spread of this _Salmonella_ Infantis strain to the United States. We cannot definitively determine how patients with this strain of _Salmonella_ Infantis who did not report travel to South America became infected; however, our analysis does show a close genetic relationship between clinical isolates from these patients and isolates collected by USDA-FSIS from chickens in the United States.\n\n【31】Poultry consumption may be a source of CTX-M-65 Infantis infection in the United States and abroad. In our study, more than one third of patients with CTX-M-65 Infantis infections did not report recent international travel and thus were exposed via a domestic source. CTX-M genes have been linked to poultry, particularly broiler chickens . A study of 14 chicken farms in Henan Province, China, conducted during 2007–2008, was the first to describe detection of CTX-M-65–producing _E. coli_ in chickens . Recently, CTX-M-65 Infantis was found to be transmitted from broiler chickens and chicken meat to humans in Italy .\n\n【32】The results of our initial hqSNP analysis demonstrated that the original isolate collected from retail chicken in 2014 was genetically related to isolates from humans. In addition, our data show a clear partition between the dates of specimen collection from the first travel-associated infections detected in the United States  and the dates of the first domestically acquired infections detected  . Sampling in poultry production plants and sequence data from USDA-FSIS demonstrate that this strain was present in domestic food processing plants during the latter part of the study period  . We cannot, however, determine precisely when or how this strain was introduced into poultry stock in the United States because enhanced poultry plant sampling was not conducted before 2014. International distribution of infected breeder stocks, chicken feed, or feed additives contaminated with CTX-M-65 Infantis could help explain how these broiler-associated infections have spread globally during the same period.\n\n【33】CTX-M-65, in comparison with more well-characterized CTX-M enzymes, differs by only 2 substitutions (A77V, S272R) from CTX-M-14, one of the more commonly detected CTX-M variants worldwide . The _bla_ CTX-M-65  gene was transmitted on a large IncFIB-like plasmid containing multiple resistance genes. The presence of multidrug resistance in CTX-M-65 Infantis isolates in our study suggests that a variety of antimicrobial drugs could provide positive selection pressure and thus promote persistence of this strain. These characteristics suggest that the potential for spread of the gene is high. Research conducted in Bolivia has shown that even in the absence of selective pressure from antimicrobial drug use, plasmid transfer of CTX-M-65 from _E. coli_ to other pathogens was achieved at high frequency and shown to be stable .\n\n【34】Additional studies on antimicrobial drug use and management practices in food animals may help us understand which factors contribute most to the emergence, persistence, and spread of resistance genes such as _bla_ CTX-M-65  . New efforts to perform whole-genome sequencing on all _Salmonella_ isolates at public health laboratories nationwide will help determine whether plasmid-mediated _bla_ CTX-M-65  has spread to other _Salmonella_ serotypes. One limitation of our study was that we were unable to obtain epidemiologic data for all 312 cases. The data available from FoodNet enabled us to compare variables such as patient demographics, travel, and hospitalizations; however, we did not have a control population for evaluating other variables of interest to determine potential domestic sources of transmission.\n\n【35】The spread of CTX-M-65 is concerning because the presence of ESBLs eliminates 2 recommended treatment options, ceftriaxone and ampicillin, for the management of salmonellosis. Given the multidrug-resistant profile of CTX-M-65 Infantis, potential for plasmid-mediated transmission, increased hospitalization rate, and evidence of this strain in domestic poultry, action is needed to prevent widespread dissemination in the United States. Enhanced surveillance and additional studies in humans and food animals may help pinpoint the sources of infection for implementation of prevention and control measures. Meanwhile, travelers and healthcare providers should be aware of the risks and implications of infection with this strain, including the potential for antimicrobial treatment failure.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5cf37ff5-9c09-47ec-862f-efbd59399579", "title": "Tula Virus as Causative Agent of Hantavirus Disease in Immunocompetent Person, Germany", "text": "【0】Tula Virus as Causative Agent of Hantavirus Disease in Immunocompetent Person, Germany\nHantavirus disease, also called hemorrhagic fever with renal syndrome and hantavirus cardiopulmonary syndrome, is a zoonosis; hantaviruses are transmitted from their reservoirs (rodents) to humans. The clinical course is characterized by initial high fever and body pain, potentially proceeding to renal, pulmonary failure, or both. The case-fatality rate depends on the causal virus species and can reach up to 50% .\n\n【1】Tula virus (TULV), a member of the family _Hantaviridae_ , genus _Orthohantavirus_ , has been isolated from common voles ( _Microtus arvalis_ ) . TULV, a broadly distributed virus in different parts of Eurasia, is hosted by common voles but has also been found in related vole species . Clinical findings of TULV pathogenicity are very rare. In 2003, a case of TULV-associated hantavirus disease was diagnosed by serologic and molecular epidemiologic means . So far, direct molecular evidence for TULV infection has only been found in 2 cases (1 in an immunocompromised patient who had severe hantavirus disease , the other in an immunocompetent person without preexisting illness who had mild hantavirus disease ).\n\n【2】We report molecular evidence of TULV infection in a 21-year old immunocompetent man who originated from a small village near Hamburg, northern Germany. He was admitted to hospital for sudden fever, sickness, severe headache, abdominal pain, and limb pain since the day before. His medical history was unremarkable. The patient worked as a sanitary and heating engineer in the northern part of Germany. Except for elevated body temperature, the physical examination did not reveal any abnormalities. Blood testing at the day of admission revealed thrombocytopenia (63 platelets/nL), markedly elevated C-reactive protein (63.6 mg/L), and borderline leukocyte (9.8 cells/nL) and serum creatinine (1.2 mg/dL) values. Because of biochemical signs of infection, ultrasound findings of a moderate enlargement of spleen and cervical lymph nodes as well as hints of a small lung infiltration, an antibiotic regime (ampicillin/sulbactam and clarithromycin) was initiated. Over the next few days, the fever resolved.\n\n【3】At day 4, biochemical signs of disturbed retention function indicated acute kidney injury. An enhanced serum creatinine value (1.6 mg/dL) and impaired (59 mL/min) glomerular filtration rate (GFR), determined using the CKD-EPI method, were noted; thrombocytopenia and an elevated C-reactive protein level persisted. The GFR, determined using the cystatin method, as measured at day 5, was reduced to 48 mL/min. The patient did not observe signs of oliguria. Under calculated intravenous fluid intake, the creatinine, GFR, and platelet values normalized until day 8, and the C-reactive protein declined to 12.1 mg/L. The patient was discharged from hospital at day 8 in good general condition.\n\n【4】Initial laboratory diagnostics were based on hantavirus serologic test results (on day 4 of hospitalization) using the Hantavirus Profile 1 immunoblot . This assay does not contain TULV antigen, but the patient’s serum showed strong IgG reactivity with Dobrava–Belgrade virus (DOBV), Hantaan virus (HTNV), and Puumala virus (PUUV) antigens as well as strong band intensity on PUUV in the IgM assay. The recomLine HantaPlus IgG and IgM assays  revealed strong IgG reactivity to PUUV and in the IgM blot strong band intensities on PUUV, Sin Nombre virus, and DOBV. On the basis of these serologic findings, a PUUV infection of the patient was suspected.\n\n【5】At day 5, serum was obtained for molecular virus detection. Using the primers of the Pan Hanta PCR , a 392-nt long region of the large genome segment was amplified and sequenced. The molecular phylogenetic analysis of the large segment sequence demonstrated TULV, but not PUUV, infection of the patient. Within the phylogenetic tree, the new sequence  clustered with vole-derived TULV sequences from Germany and, in a more refined analysis, with those from the central north (CEN.N) clade, which consists of sequences from the northern, eastern, and central parts of the country .\n\n【6】Previous studies from northeast Germany have shown that TULV is able to infect humans. Out of 563 serum samples from forest workers investigated, 22 samples (3.9%) reacted exclusively with TULV diagnostic antigen . In a survey of 6,537 serum samples representing the average population of Germany, 1 sample showed its highest neutralizing titer to TULV compared with PUUV, DOBV, HTNV, and Seoul virus .\n\n【7】A reason for the rare finding of TULV-associated hantavirus disease might be the close genetic and antigenic relationship to another vole-associated hantavirus, PUUV, which is carried by bank voles ( _Myodes glareolus_ ) and is a well-known pathogenic agent broadly distributed in Europe and parts of Asia . Because anti-TULV and anti-PUUV seroreactivities cannot be distinguished using the usual serologic techniques (i.e. without neutralization assays) , TULV infections might be misdiagnosed as PUUV infections during routine diagnostics.\n\n【8】The ability of TULV to cause hantavirus disease even in a previously healthy person shows the pathogenic potential of this virus. Therefore, the vole species hosting TULV should be considered as infection sources in their respective geographic ranges. Because of the broad distribution  and mass reproduction of common voles in several parts of Europe, TULV should be considered as a threat to human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aef7ad65-7b85-470f-91d1-896c5f86f91a", "title": "Frequently Asked Questions", "text": "【0】Frequently Asked Questions\n### About the Foodborne Disease Outbreak Surveillance System (FDOSS)\n\n【1】Open All Close All\n\n【2】What is FDOSS?\n\n【3】FDOSS is CDC’s program for collecting and reporting data about foodborne disease outbreaks in the United States. It is a part of the National Outbreak Reporting System (NORS) , which also includes data on illnesses resulting from contact with animals, environmental contamination, spread by person-to-person, waterborne transmission, and other enteric illness outbreaks.\n\n【4】What information does FDOSS collect?\n\n【5】FDOSS collects information about foodborne disease outbreaks, such as\n\n【6】*   Date and location of the outbreak\n*   Number of people who became ill and their symptoms\n*   Food or drink implicated\n*   Setting where the food or drink was prepared and eaten\n*   Pathogen (germ, toxin, or chemical) that caused the outbreak\n\n【7】How does this information get into FDOSS?\n\n【8】State, local, and territorial public health departments are responsible for identifying and investigating foodborne outbreaks and reporting them to CDC. A team within CDC investigates multistate outbreaks and also reports these to FDOSS.\n\n【9】Why does CDC collect this information?\n\n【10】CDC estimates that each year in the United States, about 9.4 million people get ill from 31 known foodborne germs. These illnesses lead to about 56,000 hospitalizations and 1,350 deaths. While most foodborne illnesses are not part of a recognized outbreak, outbreaks provide important information on the agents (germs, toxins, and chemicals) that cause illness, the foods responsible, and the settings that lead to transmission.\n\n【11】why outbreak information is collected\n\n| What outbreak information can provide | Why this information is important |\n| --- | --- |\n| Foods associated with outbreaks | Identifies foods most commonly reported in outbreaks, and changes in how frequently certain foods are implicatedProvides information on the causes of food contaminationIdentifies germ-food pairs most often linked to outbreaksOffers insight into how well prevention measures are working, which can lead to new policies and practices |\n| Germs associated with outbreaks | Identifies germs reported in outbreaks, and changes in how frequently certain germs are implicatedProvides information on how specific germs affect people and the significance of these germs to the public’s healthIdentifies germ-food pairs most often linked to outbreaksOffers insight into how well prevention measures to reduce those germs are working, which can lead to new best practices and industry standards |\n| Where germs contaminate food | Identifies points in the food production chain (for example, at a farm or production plant) and post-production (for example, during transportation, or at a grocery store, restaurant, or home) where germs contaminate foodOffers insight into how well prevention measures to reduce those germs are working, which can lead to new best practices and industry standards |\n| Places outbreaks happen, including where food is made and eaten | Tells us where outbreaks are more likely to occur, and if those places are changing over timePoints out food preparation and handling practices that can lead to outbreaksOffers insight into how well prevention measures are working, which can lead to new best practices and industry standards |\n\n【13】More Information:\n\n【14】*   Foodborne illness estimates\n*   Attribution of foodborne illness\n*   Podcast: “Knowing which foods make us sick will help guide food safety regulations”\n\n【15】How does CDC use this information?\n\n【16】CDC uses this information to better understand the germs, foods, settings, and contributing factors (for example, food not kept at the right temperature) involved in outbreaks. CDC also uses the information to identify emerging foodborne disease threats and to shape and assess outbreak prevention measures.\n\n【17】How can I get this information?\n\n【18】You can get information about foodborne disease outbreaks reported to CDC by\n\n【19】*   Reading CDC’s annual surveillance reports >  \n    (1966 through the most current year of available data)\n*   Searching NORS Dashboard >  \n    (1998 through the most current year of available data)\n\n【20】### Background Information\n\n【21】What is a foodborne illness?\n\n【22】A foodborne illness occurs when someone gets sick after consuming a contaminated food or drink. It is also called foodborne disease, foodborne infection, or food poisoning.\n\n【23】More than 250 agents are known to cause foodborne illness. These agents include germs (such as bacteria, viruses, and parasites) and chemicals (such as ciguatoxin ).\n\n【24】The most common symptoms of foodborne illness include upset stomach, abdominal cramps, nausea, vomiting, diarrhea, fever, and dehydration. Symptoms may range from mild to severe and differ depending on the agent that caused the illness. Some illnesses lead to long-term health problems or death.\n\n【25】More information:\n\n【26】*   Foodborne germs and illness >\n*   Prevention information >\n\n【27】What is a foodborne disease outbreak?\n\n【28】A foodborne disease outbreak occurs when two or more people get the same illness from the same contaminated food or drink. Nearly all of the more than 250 agents known to cause foodborne illness can cause an outbreak.\n\n【29】More information:\n\n【30】*   Investigating foodborne disease outbreaks (with infographic) >\n*   List of selected multistate foodborne disease outbreak investigations >\n\n【31】What are the roles of public health departments and CDC in foodborne disease outbreaks?\n\n【32】CDC has a long history of working with state, local, and territorial public health departments on foodborne illness investigations.\n\n【33】Which public health department participates in an investigation depends on the size and scope of the outbreak. Sometimes one department starts an investigation and then more illnesses are found across county or state lines.\n\n【34】*   **Local health departments** : Most foodborne outbreaks are local events. Public health officials in just one city or county health department investigate these outbreaks.\n*   **State health departments** : Typically, a state health department investigates outbreaks that spread across several cities or counties. This department often works with the state department of agriculture and with federal food safety agencies.\n*   **CDC** : A state may invite CDC to assist in the investigation of outbreaks that involve large numbers of people or severe or unusual illness. CDC also usually leads investigations when the food exposures occur in more than one state. States communicate regularly with one another and with CDC about outbreaks and ongoing investigations. Learn more about CDC’s role in outbreak response. >\n\n【35】### Some Foodborne Germs Commonly Linked to Outbreaks\n\n【36】_C. perfringens_\n\n【37】_Campylobacter_\n\n【38】_E. coli_\n\n【39】Norovirus\n\n【40】_Salmonella_", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "82e5469d-33d0-4982-b6f8-eb99019438e8", "title": "Risk-Based Estimate of Human Fungal Disease Burden, China", "text": "【0】Risk-Based Estimate of Human Fungal Disease Burden, China\nFungal diseases constitute a growing problem worldwide, causing a large, but poorly quantified, impact on public health . The incidence of fungal infections varies according to geographic region, socioeconomic conditions, and the number of persons with underlying conditions. China is one of the largest countries in the world (largest population and third largest land area). It has almost every type of weather niche, from the Pacific coast in the south to the snowy mountains in the Qinghai–Tibet Plateau, and even tropical rain forest. Many endemic fungal infections are present in China, along with globally distributed fungal pathogens. Although China has become the world’s second largest economy, it is still a developing country, with millions of impoverished citizens who are susceptible to fungal infections. Fungal keratitis, one of the major causes of avoidable blindness, has been relatively neglected . Moreover, old pathogens such as _Histoplasma_ and _Talaromyces marneffei_ (talaromycosis) have expanded ; new hosts contributing to new therapies for malignant and autoimmune disease have increased ; and new patterns, including aspergillosis in pulmonary tuberculosis (PTB) and chronic obstructive pulmonary disease (COPD), are emerging . In addition, the lack of effective drugs, shortages of well-trained medical care personnel, and unaffordable antifungal drugs result in severe outcomes. Therefore, an estimation of fungal disease burden is needed for China to increase public health awareness and facilitate effective interventions.\n\n【1】As in most other countries, fungal infections are not reportable in China, and the incidence and prevalence are difficult to calculate because of the lack of population-based surveillance data and few high quality epidemiology studies. The Chinese National Fungal Diseases Surveillance System  was established on May 18, 2019, but no data have been released yet. Even with this dearth of data, we have attempted to estimate the burden of fungal disease in China.\n\n【2】### Materials and Methods\n\n【3】##### Study Procedures\n\n【4】We conducted a literature review for published epidemiology papers that discussed fungal infections in China. If no epidemiological data existed for a particular fungal disease, we estimated the burden based on fungal infection incidence or prevalence and the specific populations at risk .\n\n【5】##### High-Risk Population Data\n\n【6】We obtained population statistics, including China’s total, child, and female populations 14–49 years of age, from the United Nations Population Division . We derived data on HIV/AIDS in China from the Joint Nations Program on HIV/AIDS (UNAIDS) ; we used the same source to calculate the proportion of HIV patients on antiretroviral therapy (ART). We consulted the World Health Organization (WHO) tuberculosis report to obtain data on tuberculosis patients; we assumed that 5.6% of these patients died . The numbers of lung cancer and hematological malignancy cases were derived from Global Cancer Observatory (GLOBOCAN) reports ; data related to transplant recipients were derived from China Organ Transplantation Registration System (COTR) . For other high-risk populations, we extracted data from relevant published reports .\n\n【7】##### Selection of Studies for Incidence or Prevalence Data\n\n【8】We conducted a systematic literature review and identified published epidemiology papers. We searched Web of Science, PubMed, and Embase databases for all the eligible studies published during January 1, 1950–October 7, 2019. Studies selected for this analysis were published in English; we included population-based incidence studies, population-based surveillance systems, and national investigation data. If no available incidence or prevalence data from China were found, we considered published global or international data. All search strings are listed in Appendix Table 2 and studies contributing to estimates for each fungal disease are listed in Appendix Table 3. All the assumptions and calculations for different fungal diseases are detailed in Table 2 .\n\n【9】### Analysis of Data\n\n【10】##### Candidiasis Burden Estimations\n\n【11】We estimated burdens of invasive candidiasis, _Candida_ peritonitis, _Candida_ peritonitis as a complication of chronic ambulatory peritoneal dialysis (CAPD), and recurrent vulvovaginal candidiasis (RVVC). To estimate invasive candidiasis, we first assessed candidemia incidence in intensive care units (ICUs). Because »20% of candidemia episodes in Asia occur in ICUs , we used these data to estimate annual incidence for all units. We then estimated _Candida_ peritonitis by assuming that there were 2 episodes of candidemia per episode of intraabdominal candidiasis in the ICU, based on a large prospective study . In addition, we estimated _Candida_ peritonitis in CAPD using data from the First Affiliated Teaching Hospital in Tianjin . For RVVC, when prevalence data were not available, we used the rate of women with RVVC from a recent global estimate . We assumed that esophageal candidiasis occurred in 20% of patients with HIV who were not on ART and 5% of those taking ART annually . Oral candidiasis was estimated only in patients with HIV; we assumed that it occurs in 45% of patients with AIDS annually .\n\n【12】##### Aspergillosis Burden Estimations\n\n【13】We calculated burdens of invasive aspergillosis (IA), chronic pulmonary aspergillosis (CPA), allergic bronchopulmonary aspergillosis (ABPA), and severe asthma with fungal sensitization (SAFS). We estimated the annual incidence of IA in hematological malignancy, solid and hematopoietic stem cell transplant (HSCT) recipients, lung cancer, COPD, and deaths from AIDS. We estimated that acute myeloid leukemia accounted for 40% of the annual incidence of all leukemias and multiple myeloma in 2018 . We took the rate of IA of 13% in hematological malignancy from a study from Taiwan , where mold-active prophylaxis was not given, and an equal number of cases were seen in all other leukemia and lymphoma cases. Among allogeneic HSCT recipients, we assumed an IA rate of 10% and rates in solid organ transplant recipients of 2% (renal), 6% (heart), 4% (liver), and 20% (lung) . For patients with lung cancer, we used a rate of 2.6% from a large study from China . To estimate the annual incidence of IA in patients with COPD, we used a recent study in Guangzhou Province, which found that the rate of IA in hospitalized patients with COPD was 3.9% . The annual hospitalization rate was a mean of 20.9%, and the number of patients with COPD came from a systematic review , from which we estimated the hospitalized patients with COPD. Although this information was not reported from China, we assumed a 4% autopsy incidence of IA in patients with AIDS .\n\n【14】We used the WHO 2017 figures for PTB to calculate CPA . We calculated CPA incidence after PTB based on our previous estimate, assuming that 22% of patients are left with a pulmonary cavity and that 22% of these patients develop CPA each year, as did 2% of those without a cavity . This calculation derives an annual incidence of CPA, which we converted to a 5-year period prevalence by assuming a 15% annual death or surgical resection rate. Given that PTB is one of several underlying causes of CPA, we conservatively assumed that PTB was primarily responsible for 33% of all CPA cases .\n\n【15】The reported rate of asthma in adults in China has increased from 1.42% in 2012 to 4.2% in 2019 . Ma et al. ascertained that 2.5% of these patients had ABPA (in secondary care) . Severe asthma proportion of adult asthmatics was estimated at 10%, as in other country estimates . We used a conservative fungal sensitization rate of 33% (as in other countries) to estimate the number of SAFS . No estimation was made about cystic fibrosis in China because few patients currently survive to adulthood.\n\n【16】##### HIV-Related Infection Burden Estimation\n\n【17】We estimated burdens of cryptococcal meningitis (CM), _Pneumocystis_ pneumonia (PCP), talaromycosis, and histoplasmosis. We ignored other HIV-related infections because of the lack of population-based data. We derived data for patients with AIDS from those who had a 5-year decline in CD4 counts to <200 cells/mL in the total population of HIV patients. We estimated the annual incidence of CM at 8% in patients with AIDS (CD4 count <200 cells/mL)  and estimated the overall CM annual incidence based on the assumption that the proportion of HIV-positive patients with CM was 21% . In children, 23 cases of CM were diagnosed over a 5-year period  in Shijiazhuang, giving an annual incidence of 0.43/100,000 . We conservatively estimated the 2-year incidence of PCP at 22% of patients with AIDS , which comprises 70% of total cases . Only 20% of the HIV population was assumed to be geographically at risk for infection with _T. marneffei_ ; the attack rate was 15% in patients with AIDS . Disseminated histoplasmosis was assumed to occur in 5% of the geographically exposed population (estimated at 67%) of patients with AIDS .\n\n【18】##### Mucormycosis, Fungal Keratitis, and Onychomycosis Burden Estimation\n\n【19】Mucormycosis is a rare fungal infection; the prevalence rate is 0.2–140.0/1 million population . We used a global prevalence rate (2.0/1 million population) to estimate the burden. To estimate the burden of fungal keratitis, which is usually caused by _Fusarium_ spp. and _Aspergillus_ spp. in China, we used the overall prevalence of 0.007% according to a multicenter study . We used the global prevalence rate (2.6%) from 11 population-based studies to estimate the onychomycosis burden .\n\n【20】##### Epidemiology Maps\n\n【21】For talaromycosis and histoplasmosis, which showed new endemic trends, we prepared epidemiology maps according to the number of reported cases in China. We searched the PubMed database for articles published in China during January 1, 1950–October 7, 2019. Reports published in English were included. Search strings and references contributing to the talaromycosis map are listed in Appendix Table 4, and those contributing to the histoplasmosis map are listed in Appendix Table 5.\n\n【22】##### Prediction of HIV-Related Invasive Fungal Burden\n\n【23】We made a simple prediction model to estimate the HIV-related fungal burden by 2050. We derived the prediction data for total population in the next 50 years from UN data , and we collected data on HIV and AIDS cases during 2012–2017 to make a linear regression model to predict the number of HIV cases in 2050. If early testing and ART are at the current level, we estimate that 20.4% of HIV patients will develop advanced HIV disease over time . Based on our assumptions, we also predicted the burdens of invasive fungal diseases, including CM, PCP, and talaromycosis.\n\n【24】### Results\n\n【25】##### Population Profiles\n\n【26】According to the UN data, the population of China was »1.4 billion in 2019, of whom 18% were children; 688 million adults were >40 years of age, of whom 403 million were women 15–49 years of age . The current total number of reported HIV infections in China is 810,000, and there were 26,000 AIDS-related deaths in 2017. Thus, 784,000 persons were living with HIV in China in 2019, of whom 60% were not receiving ART and 165,018 had AIDS (CD4 <200) . The detailed population characteristics and high-risk populations are described in Table 1 .\n\n【27】##### Candidiasis Burden\n\n【28】The overall fungal burden in China, according to major risk factors, is summarized in Table 3 . In 2012, there were an estimated 86,027 intensive care beds, in China and the rate of candidemia in ICU was documented at 3.2/1000 ICU admissions. The median length of stay in the ICU in China is 6.1 days. Thus, there are 16,402 candidemia episodes in the ICU, and we estimated a total of 82,011 episodes of candidemia per year in all units. Although _C. albicans_ remains the most common species associated with candidiasis in ICU patients, other non– _albicans Candida_ (NAC) is becoming increasingly common, and patients with NAC usually have longer antifungal therapy, longer ICU or hospital stay, and slightly higher death rates .\n\n【29】We also estimated 8,201 cases of postsurgical _Candida_ peritonitis (intraabdominal candidiasis) by making the assumption that the rate of _Candida_ peritonitis is 50% of cases of candidemia in the ICU. Given that there were 73,871 patients on CAPD in China in 2017, we estimated 738 peritonitis cases by using the _Candida_ peritonitis episode rate of 0.01/patient-year.\n\n【30】Except for cutaneous disease, recurrent _Candida_ vaginitis is the most common fungal disease, with aspergillosis, including IA, CPA, ABPA, and SAFS, next . We used the base case of RVVC prevalence in adult premenopausal women (15–49 years of age) previously published: 29,082,000 (range 21,812,000–36,353,000) affected women . In addition, 74,258 cases of oral candidiasis and 49,240 cases of esophageal candidiasis were expected annually in patients with HIV.\n\n【31】##### Aspergillosis Burden\n\n【32】We obtained an estimate of 1,178,747 cases of IA (82.1/100,000 population). We estimated 32,840 cases in immunocompromised patients and those with cancer; of these, 20,000 were in patients with lung cancer and 1,040 in patients with AIDS. The remainder of IA cases in this immunocompromised group were in patients with hematologic malignancies, lymphoma, and transplants. We also calculated 1,145,908 IA cases derived from COPD.\n\n【33】In China, there were 844,500 survivors of tuberculosis in 2017 . We expect an annual incidence of 51,683 cases of tuberculosis-related CPA, and we estimated a 5-year period prevalence of 162,905 cases. Because tuberculosis probably comprises only one third of underlying CPA, the total period prevalence estimate is 488,716 cases (34/100,000 population).\n\n【34】Nearly 50 million adults with asthma live in China; of these, 10% have severe asthma. Regarding ABPA, the assumption is that 2.5% of adult asthmatics are affected, leading to a prevalence of 1,237,797 cases. Among patients with severe asthma, we estimate that 1.6 million have SAFS.\n\n【35】##### Burdens of HIV-Related Infections\n\n【36】CM occurs mainly in immunocompromised populations other than HIV patients in China, as well as in immunocompetent individuals. We thus obtained an estimate of adult CM: 13,086 in patients with AIDS and 26,172 each in immunocompromised and in immunocompetent persons. For CM in children, we estimated 77 cases annually, with an annual incidence of 0.43/100,000 population. We calculated a total of 65,507 CM cases.\n\n【37】We estimated the number of patients with PCP in China as 27,723 (18,482 with HIV and 9,241 with other immunocompromised conditions). This estimate implies an annual incidence of 1.93/100,000 population.\n\n【38】We estimated 4,951 talaromycosis cases in patients with AIDS in southern China. From the literature review, we identified 3,163 cases from 12 different provinces. The provinces with the highest prevalence are Guangxi and Guangdong, which each reported >1,000 cases . Both provinces are located in Pearl River Basin, possibly indicating an endemic trend.\n\n【39】We identified 380 histoplasmosis cases in China from the literature review . Most of the cases were reported in the region where the Yangtze River flows, also suggesting a new endemic pattern in China. Disseminated histoplasmosis in patients with AIDS was assumed to affect 5,528 persons annually, but we were unable to estimate the burden in non–HIV-infected persons or the burden of chronic pulmonary histoplasmosis. For the prediction of HIV-related invasive fungal burden by 2050, using annual data from 2012–2017 and extrapolating with our estimates, we expect 86,303 cases of PCP, 61,105 cases of CM, and 23,117 cases of _T. marneffei_ infection .\n\n【40】##### Mucormycosis, Fungal Keratitis, and Onychomycosis Burden\n\n【41】We estimated mucormycosis using the global prevalence rate and calculated 2,868 cases. We estimated fungal keratitis based on the total population and estimated 100,365 cases of fungal keratitis annually in China. We estimated 37,278,384 cases of onychomycosis using the global data.\n\n【42】### Discussion\n\n【43】In this study, we estimated that 71 million persons suffer from a fungal disease in China. A total of 2.4% of the population is affected (excluding onychomycosis because it is superficial), similar to results in other reports from Senegal, Brazil, France, Korea and Germany; the prevalence range is 1.6%–3.6% . Multiple new host risk factors other than HIV/AIDS or hematologic malignancy, especially COPD, asthma, and lung cancer, are associated with fungal disease. We found that even immunocompetent children and women may develop fungal diseases. Chronic respiratory diseases, notably PTB and COPD, are risk factors for all manifestations of aspergillosis. Old pathogens, including _T. marneffei_ and _Histoplasma capsulatum_ , exhibit new endemic trends; the Pearl River Basin bears the greatest burden for _T. marneffei_ and the Yangtze River for _H. capsulatum_ . Our study contributes to the currently limited data on the burden of fungal disease in China and provides a basis for public health and research priorities.\n\n【44】The incidence of candidiasis has increased in recent years . Candidemia is probably underestimated, as we have used only ICU data to explore the total burden of all high-risk populations. Considering the wide use of broad-spectrum antimicrobial drugs and the demographic shift with a largely increasing elderly population, we expect infections to be on the rise.\n\n【45】We have estimated the oral and esophageal candidiasis burdens in HIV-positive patients; these are certainly underestimates of these infections, because many of the populations at risk could not be assessed, such as patients with cancer, those taking oral or inhaled corticosteroids, and newborns. Although the proportions of oral or esophageal candidiasis in this kind of populations might be small, given the relatively large size compared to the rather small HIV population, these cases could multiply our estimates. In addition, oral and esophageal candidiasis and colonization are associated with mucosal malignancy and particularly associated with high alcohol consumption . If the high number of unsuspected cases of esophageal candidiasis based on data from South Korea is also true in China, this association could contribute to the high number of esophageal cancers seen annually in China (307,359 cases) , which could further increase the social and economic burden.\n\n【46】IA is usually severe and fatal, unless diagnosed early. Although profoundly immunocompromised patients are at higher risk, the enormous estimate for China is mostly driven by COPD (97%). Our estimate of IA prevalence in liver transplant recipients of 4% is higher than the report from China at 1.7% , but that study was based on histology or culture alone, which is much less sensitive than _Aspergillus_ antigen detection. The same applies to renal transplant recipients . Other underlying conditions were newly recognized risk factors for IA, such as diabetes mellitus, systemic lupus erythematosus, and postoperative and burn infections related to contaminated air in hospitals; these were not included in our estimation because of the unavailable incidence rate. Nevertheless, the number of IA cases could also be overestimated because _Aspergilli_ are common fungi in the environment, and a positive result from non–aseptic fluid culture does not always represent disease. On the other hand, we have not estimated IA in most medical ICU patients, and not included the potential for IA complicating annual influenza cases or an epidemic.\n\n【47】In contrast ot other countries, where CM is often diagnosed in patients with HIV or immunocompromised patients, in China, a high proportion of cryptococcosis was reported in immunocompetent persons . Jiang et al. reported on 159 HIV-negative patients with CM, of whom 85 were normal hosts; however, whether these persons were immunocompetent is unknown, because several genetic predisposition factors for CM have been found in the ethnic Chinese population . Most large-scale studies have been conducted in adults; few were dedicated to pediatric populations. Although children account for only 0.9%–2.0% of all cryptococcal cases, the death rate is high, up to 43% . In a 12-year retrospective study in Beijing, 53 pediatric case-patients were encountered, of whom 41 had no underlying conditions . However, the denominator was unavailable. Only an annual incidence of 0.43/100,000 HIV-negative children was reported from the Acute Meningitis-Encephalitis Syndrome Surveillance project . Because of the lack of surveillance networks in China, additional studies are required, especially for immunocompetent patients.\n\n【48】Although histoplasmosis is a common endemic mycosis in North America, several sporadic cases were reported in China, especially in the Yangtze River region, which was traditionally thought to be nonendemic for _Histoplasma capsulatum_ . However, >300 histoplasmosis cases have been reported since 1990, and only 17 were identified as imported cases, indicating many authochthonous cases in China . _T. marneffei_ infection, the other fatal endemic opportunistic fungal infection disease in Asia, was reported mostly in the southern part of China, possibly linked to an altered microeukaryotic community in subtropical rivers caused by global warming . Because this disease was associated mainly with HIV/AIDS, we estimated the incidence only in HIV-positive patients. In addition, given the low national reported statistics of HIV infection , the estimation of talaromycosis burden may be far underestimated. More large population-based studies are needed to better clarify the frequency of these fungal infections in these at-risk patients.\n\n【49】The government of China has worked to improve healthcare over the past 2 decades. However, HIV remains a major public health issue, showing the fastest growth among 45 infectious diseases during 2004–2013 with an annual percentage change of 16.3% . Therefore, we made a prediction of HIV-related opportunistic fungal infections. According to our data, it is likely that CM, PCP, and _T. marneffei_ infection are major health burdens, which call for much more clinical training, financial support, and public policies.\n\n【50】Even though the incidence rate was low compared with those for bacterial and viral infections, our study represents a heavy fungal burden considering the immense population base and high mortality of nonsuperficial mycoses. The drawbacks of our study are the few studies conducted in the country for some infections; the prevalence or incidence are not available for those diseases, including sporotrichosis and some dermatophytosis. Epidemiologic studies are required and population-based surveillance data remain to be estimated, both nationally and regionally. Improved epidemiologic data are necessary for better awareness, better diagnostics, and better therapies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aafd5928-1a7e-4947-a80f-c7a1cae45987", "title": "Viral Metagenomic Analysis of Cerebrospinal Fluid from Patients with Acute Central Nervous System Infections of Unknown Origin, Vietnam", "text": "【0】Viral Metagenomic Analysis of Cerebrospinal Fluid from Patients with Acute Central Nervous System Infections of Unknown Origin, Vietnam\nWorldwide, the annual incidence of acute encephalitis in nonoutbreak settings during 1983–2000 ranged from 0.07 to 12.6 cases/100,000 population . According to the World Health Organization, meningitis caused 379,000 deaths and encephalitis caused 150,000 deaths globally in 2015 . As a consequence, central nervous system (CNS) infection is a leading cause of years lived with disability in low-income countries .\n\n【1】More than 100 known pathogens can cause CNS infections . However, the distribution of CNS infection pathogens is geographically dependent and has been shaped by the emergence of novel viruses. In Asia, Nipah virus and enterovirus A71 have been recognized as emerging neurotropic pathogens over the past few decades. In 1999, West Nile virus arrived in the United States and since then has established endemic circulation .\n\n【2】Despite recent advances in molecular diagnostics, especially sensitive virus-specific PCR, encephalitis cases of unknown origin remain a substantial problem. Worldwide, ≈50% of patients with CNS infections have no etiology identified .\n\n【3】Over the past decade, metagenomic next-generation sequencing (mNGS) has emerged as a sensitive hypothesis-free approach for detection of pathogens (especially viruses) in clinical samples . However, in resource-limited settings like Southeast Asia and Vietnam, a limited number of mNGS studies examining known and unknown viruses in cerebrospinal fluid (CSF) samples from patients with CNS infections have been conducted, even though in this tropical region of the world, novel viruses are likely to emerge , and diverse CNS infection pathogens have been documented. Collectively, improving our knowledge about viral causes of CNS infections is essential for clinical management and development of intervention strategies. In this study, by using a mNGS approach, we set out to search for known and unknown viruses in CSF samples collected from patients in Vietnam with CNS infections of unknown causes who were enrolled in a hospital-based surveillance study conducted during 2012–2016.\n\n【4】### Materials and Methods\n\n【5】##### Clinical Study and Selection of CSF Samples for mNGS Analysis\n\n【6】The study used CSF samples collected from patients with suspected CNS infection enrolled in a hospital-based surveillance program conducted in Vietnam during December 2012–October 2016 . The study was conducted as part of the Vietnam Initiative on Zoonotic Infections (VIZIONS) project , and patient recruitment was carried out at 7 provincial hospitals across Vietnam. After collection, as per the study protocol, all CSF samples were tested for a range of pathogens by using the diagnostic work-up of the clinical study . The remaining volume of the CSF samples were stored at −80°C for further testing.\n\n【7】We focused our metagenomic analysis on patients of unknown origin from 4 provincial hospitals in central (Hue and KhanhHoa), highland (DakLak), and southern (DongThap) Vietnam , representing 3 distinct geographic areas in Vietnam. To increase the chance of detecting a virus in the CSF samples, we only selected patients with CSF leukocyte counts \\> 5 cells/mm 3  and an illness duration < 5 days.\n\n【8】##### mNGS Assay\n\n【9】mNGS assay was carried out as previously described . Before viral nucleic acid (NA) isolation, 100 μL of each CSF sample was treated with Turbo DNase  and RNase I enzyme (Ambion). Then viral NA was isolated using a QIAamp viral RNA kit , and recovered in 50 μL of elution buffer provided with the extraction kit. Double-stranded DNA was synthesized from the isolated viral NA by using a set of 96 nonribosomal primers (FR26RV–Endoh primers) and then was randomly amplified by using the FR20RV primer (5′-GCCGGAGCTCTGCAGATATC-3′). Finally, the amplified product was subjected to a library preparation step by using Nextera XT sample preparation kit , following the manufacturer’s instructions, and sequenced by using a MiSeq reagent kit, version 3 (600 cycles) (Illumina) in a MiSeq platform (Illumina).\n\n【10】##### mNGS Data Analysis\n\n【11】Potential viral reads were identified by using an in-house viral metagenomic pipeline running on a 36-node Linux cluster as described previously . In brief, after duplicate reads and reads belonging to human or bacterial genomes were filtered out, the remaining reads were assembled de novo. The resulting contigs and singlet reads were then aligned against a customized viral proteome database by using an approach based on BLAST . Next, the candidate viral reads were aligned against a nonredundant nonvirus protein database to remove any false-positive reads (i.e. reads with expected values higher than those against viral protein databases). Any virus-like sequence with an expected value < 0.00001 was considered a significant hit. Finally, a reference-based mapping approach  was used to assess the levels of identity and genome coverage of the corresponding viruses.\n\n【12】##### PCR Confirmatory Testing of mNGS Results\n\n【13】PCR assays were conducted to confirm mNGS hits for each specific virus identified from the viral metagenomic pipeline. Depending on availability of CSF, the PCR confirmations were performed either on leftover NA or newly extracted NA. A viral mNGS result was considered positive only if it was subsequently confirmed by PCR analysis of the original NA samples. The nucleotide sequences of primers and probes used for PCR confirmatory testing are shown in Appendix Table 2 .\n\n【14】##### Serotype Identification and Phylogenetic Analysis\n\n【15】For enterovirus serotype determination based on the obtained sequences generated by viral mNGS, we used a publicly available genotyping tool . To determine the relationship between enterovirus strains we sequenced and global strains, we first performed pairwise alignment by using the ClustalW tool in Geneious 8.1.5, and then reconstructed a maximum-likelihood phylogenetic tree by using IQ Tree 1.4.3 . A similar phylogenetic approach was used for other viruses. The generated sequences of this study were submitted to GenBank .\n\n【16】##### Ethics\n\n【17】The study was approved by the corresponding institutional review broad of local hospitals in Vietnam, where the patients were enrolled, and the Oxford Tropical Ethics Committee. Informed consent was obtained from each study participant or a legal guardian.\n\n【18】### Results\n\n【19】##### CSF Samples Available for mNGS Analysis\n\n【20】From the clinical study described previously, a total of 841 patients were enrolled from Hue, Khanh Hoa, Dak Lak, or Dong Thap provincial hospitals. Of these, 609/841 (72%) patients had no etiology identified. The etiologic profiles of the patients in whom a pathogen was detected will be reported separately. Of the patients in whom a pathogen was not identified, 204 met our selection criteria, and their CSF samples were included for viral mNGS analysis .\n\n【21】##### Baseline Characteristics of the Included Patients\n\n【22】The baseline characteristics and outcome of the 204 study patients are described in Table . Male patients were predominant. A substantial proportion of the patients were seriously ill; fatal outcome was recorded in 22 (11%), whereas incomplete recovery was recorded in 17% (n = 35) and deterioration (reflected by being transferred to other hospitals) in 16.5% (n = 34).\n\n【23】##### General Description of mNGS Results\n\n【24】A total of 204 CSF samples were subjected to 3 NGS runs, and 108 million reads were obtained (median number of reads per sample 445,412 \\[range 430–908,890\\]). Of these, viral reads accounted for 0.64% (n = 692,731; median number of reads per sample 2,001 \\[range 4–268,933\\]). Excluding common contaminants and commensal viruses such as torque teno virus, which are not reported in this article, sequences related to a total of 8 distinct viral species were identified in 107/204 (52.4%) patients. These viruses are either known to be infectious to humans (e.g. enteroviruses, rotavirus, molluscum contagiosum virus \\[MCV\\], human papillomavirus, HIV, and hepatitis B virus \\[HBV\\]) or are without evidence of human infections besides previous detection in sterile human samples (e.g. cyclovirus-VN and gemycircularvirus) .\n\n【25】##### mNGS Result Assessment by Specific PCR Analysis\n\n【26】After virus-specific PCR confirmatory testing, the proportion of patients in whom a virus was found by mNGS was reduced from 53% (108/204) to 14.7% (30/204). Accordingly, the number of virus species was reduced from 8 to 5 ; enteroviruses were the most common virus detected, accounting for 11.3% (23/204) of the included patients, followed by HBV (n = 3), HIV (n = 2), gemycircularvirus, and MCV (1 each) . Because of the focus of our study and the unavailability of the PCR assays, confirmatory testing for human papillomavirus was not performed.\n\n【27】##### Characteristics of the 23 Enterovirus-Infected Patients\n\n【28】All 23 enterovirus-infected patients were admitted to hospitals from the central or highland areas , and none were from Dong Thap Province. Male patients were slightly predominant, accounting for 56%. Notably, the enterovirus-infected patients were younger than those who were mNGS-negative . At discharge, incomplete recovery or transfer to other hospitals because of disease deterioration were recorded in 21.7% .\n\n【29】Enterovirus cases were not detected during January 2015–December 2016. During 2013 and 2014, two main peaks were observed during March–July and September–December ; cases from Dak Lak and Khanh Hoa contributed to the first peak , and cases from Khanh Hoa and Hue contributed to the second . The general baseline characteristics of patients with HBV, gemycircularvirus, and MCV are shown in Appendix Table 3.\n\n【30】##### Genetic Characterization of Enteroviruses and Gemycircularvirus\n\n【31】mNGS generated sufficient sequence information for an enterovirus serotyping assessment in 11/23 cases. Subsequently, results of serotyping analysis based on the NGS sequences showed that echovirus 30 (E30) was the most common serotype detected (n = 9, 39% of enteroviruses), followed by enterovirus A71 and enterovirus B80 (1 each, 4.3%). Phylogenetically, the 9 E30 strains sequenced in our study belonged to 2 distinct genogroups, V and VIIb, and showed close relationship with E30 strains circulating in Russia and elsewhere in Asia, including China .\n\n【32】In additional to enterovirus sequences, a gemycircularvirus genome was obtained from a 12-year-old boy. Phylogenetic analysis revealed that this gemycircularvirus strain was closely related to a gemycircularvirus species previously found in CSF sample from a patient with a CNS infection of unknown origin in Sri Lanka ; the level of amino acid identities between the 2 strains were 98.79% for replication-coding sequences and 99.3% for capsid protein–coding sequences.\n\n【33】### Discussion\n\n【34】We describe a viral mNGS investigation characterizing the human virome in CSF of 204 patients in Vietnam with suspected CNS infection of unknown origin. We successfully detected 4 human viral pathogens (enteroviruses, HIV, HBV, and MCV) and 1 virus species (gemycircularvirus) of unknown tropism and pathogenicity in a total of 30 (14.7%) patients. Most patients therefore remained without a known etiology, underscoring the ongoing challenge in identifying a plausible viral pathogen in CSF of patients with CNS infections.\n\n【35】Enteroviruses were the most common viruses, found in 11.3% (23/204) of all analyzed patients , most of whom were children and young adults. This age distribution of enterovirus-infected patients is consistent with observational data from a previous report from Vietnam , although the median age was slightly higher compared with data from other countries . Geographically, all the enterovirus-infected patients were admitted to hospitals from central and highland Vietnam, and none was from southern Vietnam. The underlying mechanism determining this observed spatial pattern of enterovirus-positive cases in this study remains unknown. Our sampling timescale perhaps was not long enough to capture the circulation of enteroviruses in Dong Thap Province. Enteroviruses were previously reported as a leading cause of CNS infection across central and southern Vietnam . Collectively, our findings suggest that reverse transcription PCR (RT-PCR) testing for enteroviruses should be considered in children and young adults with CNS infections.\n\n【36】Of the detected enteroviruses, E30 was the most common serotype. E30 is a well-known pathogen of pediatric aseptic meningitis worldwide . Phylogenetically, at global scale, E30 belongs to 2 different lineages with distinct patterns of circulation and spread, 1 with a global distribution and the other with geographic restriction within Asia . The cocirculation of 2 E30 lineages in Vietnam suggests that E30 was imported into Vietnam on at least 2 occasions. Our analyses thus also contribute to the body of knowledge about the genetic diversity of E30 strains circulating in Vietnam.\n\n【37】The detection of bloodborne viruses such as HBV and HIV is unlikely to have a direct link with patients’ neurologic symptoms, although HBV has previously been reported in CSF of patients with CNS infections of unknown origin . The detection of HIV in CSF might have been a consequence of traumatic tap occurring during the lumbar puncture, as reflected by the high number of red blood cells in 1 of 2 HIV-positive CSF samples (data not shown). However, neuroinvasion of HIV has also been reported . Likewise, the pathogenic potential of a gemycircularvirus genome requires further investigation, although the detection of the gemycircularvirus genome in CSF has been reported in several papers . The detection of MCV and papillomavirus in CSF might result from contamination of viral skin flora during lumbar puncture.\n\n【38】Similar to previous reports about discrepancy between mNGS and conventional diagnostic testing , our observations found that most mNGS-positive results were not confirmed by subsequent viral RT-PCR assays, especially the sensitive enterovirus-specific RT-PCR with a limit of detection of ≈9 copies/reaction . Such results could be attributable to bleedover (also called index hopping) of indices from reads of 1 sample into reads of another sample co-sequenced on the same Illumina run . Applying double indexes, which was not used in our study, has been shown to substantially reduce, but not eliminate, the cross-contamination phenomenon between samples in the same run.\n\n【39】Our study has some limitations. First, as outlined previously, we did not employ a double unique index combination strategy per sample as part of the sequencing procedure. The well-known index hopping phenomenon possibly explains the high discrepancy between confirmatory PCR and mNGS results  and emphasizes the usefulness of dual indexing and including no template controls. As such, we pragmatically chose to verify our mNGS by performing specific PCR on original materials. Second, the DNase treatment step in our assay meant to reduce cellular DNA concentration in CSF might reduce the sensitivity of mNGS for the detection of DNA viruses such as herpes simplex virus . Third, some of the non–PCR-confirmed viral sequences likely originated from contamination of reagents, which is a lingering problem for mNGS .\n\n【40】In summary, our results emphasize that mNGS could detect a broad range of viral nucleic acids in CSF. In spite of extensive investigation, establishing the etiology in many patients with CNS infections remains a challenge. However, our findings indicate that enteroviruses are important causes of viral CNS infections in Vietnam and thus should be considered in the differential diagnosis among young patients with CNS infections.\n\n【41】The study was funded by the Wellcome Trust of Great Britain (awards nos. WT/093724 and 106680/B/14/Z \\[to the Oxford University Clinical Research Unit in Vietnam\\], 100087/Z/12/Z \\[to S.B.\\] and 204904/Z/16/Z \\[to L.V.T.\\]), and the Royal Society . X.D. and E.D. were supported by the Blood Systems Research Institute and the National Heart, Lung, and Blood Institute .\n\n【42】VIZIONS Consortium members: from the Oxford University Clinical Research Unit, Bach Tuan Kiet, Stephen Baker, Alessandra Berto, Maciej F. Boni, Juliet E. Bryant, Bui Duc Phu, James I. Campbell, Juan Carrique-Mas, Dang Manh Hung, Dang Thao Huong, Dang Tram Oanh, Jeremy N. Day, Dinh Van Tan, H. Rogier van Doorn, Duong An Han, Jeremy J. Farrar, Hau Thi Thu Trang, Ho Dang Trung Nghia, Hoang Bao Long, Hoang Van Duong, Huynh Thi Kim Thu, Lam Chi Cuong, Le Manh Hung, Le Thanh Phuong, Le Thi Phuc, Le Thi Phuong, Le Xuan Luat, Luu Thi Thu Ha, Ly Van Chuong, Mai Thi Phuoc Loan, Behzad Nadjm, Ngo Thanh Bao, Ngo Thi Hoa, Ngo Tri Tue, Nguyen Canh Tu, Nguyen Dac Thuan, Nguyen Dong, Nguyen Khac Chuyen, Nguyen Ngoc An, Nguyen Ngoc Vinh, Nguyen Quoc Hung, Nguyen Thanh Dung, Nguyen Thanh Minh, Nguyen Thi Binh, Nguyen Thi Hong Tham, Nguyen Thi Hong Tien, Nguyen Thi Kim Chuc, Nguyen Thi Le Ngoc, Nguyen Thi Lien Ha, Nguyen Thi Nam Lien, Nguyen Thi Ngoc Diep, Nguyen Thi Nhung, Nguyen Thi Song Chau, Nguyen Thi Yen Chi, Nguyen Thieu Trinh, Nguyen Thu Van, Nguyen Van Cuong, Nguyen Van Hung, Nguyen Van Kinh, Nguyen Van Minh Hoang, Nguyen Van My, Nguyen Van Thang, Nguyen Van Thanh, Nguyen Van Vinh Chau, Nguyen Van Xang, Pham Ha My, Pham Hong Anh, Pham Thi Minh Khoa, Pham Thi Thanh Tam, Pham Van Lao, Pham Van Minh, Phan Van Be Bay, Maia A. Rabaa, Motiur Rahman, Corinne Thompson, Guy Thwaites, Ta Thi Dieu Ngan, Tran Do Hoang Nhu, Tran Hoang Minh Chau, Tran Khanh Toan, Tran My Phuc, Tran Thi Kim Hong, Tran Thi Ngoc Dung, Tran Thi Thanh Thanh, Tran Thi Thuy Minh, Tran Thua Nguyen, Tran Tinh Hien, Trinh Quang Tri, Vo Be Hien, Vo Nhut Tai, Vo Quoc Cuong, Voong Vinh Phat, Vu Thi Lan Huong, Vu Thi Ty Hang, and Heiman Wertheim; from the Centre for Immunity, Infection, and Evolution, University Of Edinburgh, Edinburgh, Scotland, UK, Carlijn Bogaardt, Margo Chase-Topping, Al Ivens, Lu Lu, Dung Nyugen, Andrew Rambaut, Peter Simmonds, and Mark Woolhouse; from The Wellcome Trust Sanger Institute, Hinxton, UK, Matthew Cotten, Bas B. Oude Munnink, Paul Kellam, and My Vu Tra Phan; from the Laboratory of Experimental Virology, Department of Medical Microbiology, Center for Infection and Immunity Amsterdam (CINIMA), Academic Medical Center of the University of Amsterdam, Amsterdam, the Netherlands, Martin Deijs, Lia van der Hoek, Maarten F. Jebbink, and Seyed Mohammad Jazaeri Farsani; and from Metabiota, California, USA, Karen Saylors and Nathan Wolfe.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "80bdce45-d25f-48a1-ae6e-b143ae96cdb0", "title": "Legionnaires’ Disease Outbreak Caused by Endemic Strain of Legionella pneumophila, New York, New York, USA, 2015", "text": "【0】Legionnaires’ Disease Outbreak Caused by Endemic Strain of Legionella pneumophila, New York, New York, USA, 2015\n_Legionella_ spp. are ubiquitous in nature, live in soil and water, and frequently inhabit human-made water distribution systems, hot water tanks, decorative fountains, and cooling towers . Persons with underlying health conditions, such as chronic lung disease, or those with compromised immunity are at increased risk for contracting Legionnaires’ disease (LD) (also referred to as legionellosis). Signs and symptoms typically include fever, cough, and chest pain; LD is fatal in ≈5%–10% of cases . Transmission of _Legionella pneumophila_ is believed to occur mainly through exposure to contaminated aerosols and not from other infected persons; to date, only 1 case of human-to-human transmission has been documented .\n\n【1】LD was initially detected in 1976, when an outbreak of illness occurred during a meeting of the American Legion in Philadelphia, Pennsylvania, USA; 221 cases were identified, and 34 infected persons died . The outbreak, which remains the largest community-associated outbreak of LD in United States, was later linked to the cooling system of the hosting hotel, and a bacterium classified as _L. pneumophila_ serogroup 1 was subsequently isolated from 4 persons .\n\n【2】In the summer of 2015, a large community-associated LD outbreak affected persons who resided or traveled through a large area in the South Bronx region of New York, New York, USA. During July 2–August 3, a total of 138 adults with LD were linked to the outbreak; 128 patients required hospitalization, and 16 deaths occurred . A joint laboratory investigation to find the source of this outbreak was performed by the New York City Department of Health and Mental Hygiene and the Public Health Laboratory (NYC PHL), the Wadsworth Center (WC) of the New York State Department of Health (Albany, NY, USA), and the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA).\n\n【3】Pulsed-field gel electrophoresis (PFGE), real-time PCR, sequence-based typing (SBT), and whole-genome sequencing (WGS) were used to characterize human and environmental _L. pneumophila_ isolates from the investigation. Epidemiologic data and water testing by PCR quickly led to identification of a cooling tower located on the roof of a South Bronx hotel as a potential source of this outbreak . However, _L. pneumophila_ isolates recovered from a sample taken later during the outbreak from a homeless shelter located in the vicinity of the South Bronx hotel and others facilities within the outbreak zone were found to have PFGE and SBT patterns identical to that of the outbreak strain, raising the possibility that the South Bronx hotel might not have been the only source of an aerosolized _Legionella_ species associated with cases of legionellosis. Our finding of highly related _L. pneumophila_ isolates in multiple environmental samples and from past LD outbreaks suggests the presence of a potentially pathogenic endemic strain in the Bronx community.\n\n【4】### Methods\n\n【5】##### Water Samples and Clinical Isolates\n\n【6】Initially, water and swab samples were collected by the New York City Department of Health and Mental Hygiene and split between the WC and the NYC PHL. Later in the outbreak, water and swab samples were also collected by the New York State Department of Health and submitted to WC. Samples were processed as described , except for a subset of samples, including swab samples and visibly complex samples, that were not concentrated by centrifugation but tested directly. Clinical isolates were received by the NYC PHL and forwarded to WC. A subset of water and clinical isolates was sent to CDC for SBT analysis or sequencing by using the RSII Platform (Pacific Biosciences, Menlo Park, CA, USA). SBT was performed according to the European Society of Clinical Microbiology and Infectious Diseases (Basel, Switzerland) Study Group for _Legionella_ Infections Scheme .\n\n【7】##### Extraction of DNA\n\n【8】Nucleic acid extraction was performed for water and swab samples by using a modified Masterpure DNA Isolation Kit procedure (Epicentre, Madison, WI, USA) . In brief, for each extraction, a 1.2–1.5 McFarland suspension of the isolate in sterile water was centrifuged for 10 min at 7,500 rpm. A volume of 950 μL of supernatant was removed, leaving 50 μL. A total of 300 μL of 2× tissue and cell lysis buffer containing 1.5 μL of proteinase K was then added to each sample. Each extraction incorporated a negative extraction control that consisted of 50 μL of sterile water. The DNA was resuspended in 100 μL of 10 mmol/L Tris. Concentrations of the DNA were quantified by using the Qubit ds DNA HS Assay Kit (Thermo Fisher Scientific, Waltham, MA, USA) according to the manufacturer’s instrctions, with the Qubit 2.0 fluorometer before WGS. Updates to the protocol include the addition of an internal inhibition control.\n\n【9】##### PCR Screening\n\n【10】We tested processed samples for _Legionella_ DNA using real-time PCR with a newly validated and more comprehensive procedure than that previously published  to rapidly screen samples for prioritizing culture efforts. This assay detects and differentiates _Legionella_ spp. _L. pneumophila_ , and _L. pneumophila_ serogroup 1 and uses an internal control to assess for inhibitory substances in the sample.\n\n【11】##### Culture of Water Samples\n\n【12】Samples in which _L. pneumophila_ serogroup 1 was detected were processed and cultured at WC and NYC PHL by using standard methods. Isolates were identified as _L. pneumophila_ serogroup 1 by using direct fluorescent antibody testing or real-time PCR. All _L. pneumophila_ serogroup 1 isolates were initially typed by using digestion with _Sfi_ 1 and pulsed-field gel electrophoresis (PFGE) as described .\n\n【13】##### WGS\n\n【14】DNA sequencing was performed by using the MiSeq platform (Illumina, San Diego, CA, USA) at the WC Applied Genomic Technologies Core and the RSII platform (Pacific Biosciences) at CDC. Individual sample libraries were prepared by using a Nextera XT protocol (Illumina) for sequencing. PacBio-compatible libraries were constructed by using 8 μg of sheared genomic DNA (≈15 kb) prepared by using the SMRTbell Template Prep Kit 1.0 (product number \\[PN\\] 100–259–100; Pacific Biosciences) according to the manufacturer’s protocol (PN 100-092-800-06), the PacBio Binding Calculator version 2.3.11, the DNA Polymerase Binding Kit P6 version 2 (PN 100-372-700), and the MagBead Kit (PN 100-133600).\n\n【15】Sequencing runs were performed with a 2-kb DNA internal control (PN 100-356-500), 240-min movie time, and stage start with a DNA Sequencing Reagent Kit 4.0 (PN 100-356-400). Final library size was confirmed by using the Agilent Tapestation 2200 and the Genomic DNA ScreenTape (5067–5365 and 5067–5366). Hierarchical Genome Assembly Process version 3 was used to construct the complete _L. pneumophila_ genome sequences . The expected genome size was set to 3.4 Mb and target genome coverage parameter was set to 15×. The minimum subread length value was adjusted to decrease genome coverage to the recommended 100×—150× for microbial genomes . Genome closure was performed by identifying and trimming nucleotide overlap at the ends of the single assembled contig sequences with Gepard version 1.3 , and the reformatted genome sequence was used as input for the RS-ReSequencing protocol in the SMRT analysis portal to construct the polished genome sequence.\n\n【16】To confirm nucleotide accuracy, we aligned paired-end Illumina data for each sequenced isolate to its respective PacBio polished sequence by using Bowtie version 2.1.0 . We used Samtools version 0.1.18  and FreeBayes version 0.9.21  to identify nucleotide discrepancies between the 2 types of data. We resolved any discrepancies with the Illumina dataset and used VCFtools version 0.1.11  to construct the final consensus sequence by using both data types . We deposited the closed genome sequence of the South Bronx outbreak strain F4469 in GenBank . All raw Illumina reads used in this study are available in BioProject  .\n\n【17】##### Bioinformatics Analysis\n\n【18】We mapped raw reads to the South Bronx outbreak strain F4469 by using BWA MEM version 0.7.5a-r405 . Single-nucleotide polymorphisms (SNPs) were called by using Samtools/BCFtools version 0.1.19–44428cd , a minimum of Q20 for mapping quality and basecall quality, 10× minimum depth, and 95% of allele read agreements. Positions where \\> 1 of the samples were found to have a mutation was manually verified and used to build a SNP alignment. Positions with ambiguous calls in any of the samples were discarded. We imported the resulting alignment into PHYLOVIZ  and built a minimum spanning tree by using the GoeBURST full minimum spanning tree algorithm . Presence of plasmids was verified by performing de novo assemblies of different isolates in SPAdes version 3.7.0  and compared by using Mauve . Recombination events were determined by finding regions of enriched SNP density by using a probability density function calculation and Fastgear . Blast ring genome comparisons were made by using BRIG .\n\n【19】### Results\n\n【20】WC tested 289 cooling tower water samples from 183 cooling towers by real-time PCR during this outbreak investigation. A total of 162 (88.5%) cooling towers were positive for _Legionella_ species DNA. _L. pneumophila_ DNA was detected in 87 (47.5%) cooling towers; 52 (28.4%) cooling towers were positive _L. pneumophila_ serogroup 1, and 21 (11.5%) showed negative or inconclusive results.\n\n【21】On the basis of the amount of DNA present, which was determined by the initial PCR screening, 26 cooling tower water samples were cultured during July 28–August 14, 2015. We identified 10 culture-positive cooling towers from which 15 _L. pneumophila_ serogroup 1 isolates were subjected to PFGE. In addition, culture at NYC PHL identified isolates from a homeless shelter cooling tower that were identical by PFGE and included in this analysis. These _L. pneumophila_ serogroup 1 PFGE results showed 7 PFGE patterns . Of these isolates, PFGE showed that those from the South Bronx hotel and the homeless shelter were identical to clinical isolates. SBT analysis showed that these isolates also had the same sequence type (i.e. 731).\n\n【22】WGS was used in real time during the course of the South Bronx LD outbreak as a confirmatory method and to provide additional insight on the source. The investigation also subsequently used WGS to help clarify whether the outbreak strain could have been present at other locations during the outbreak or at other times in the past. A total of 156 isolates of _L. pneumophila_ serogroup 1 were available from culture performed at the WC, NYC PHL, and hospital laboratories (115 environmental and 41 clinical isolates from 26 patients, of which 35 were respiratory and 6 were postmortem specimens from 3 patients). These isolates were sequenced and analyzed by using an in-house bioinformatics pipeline developed at the WC.\n\n【23】Most (106/115) of the environmental _L. pneumophila_ serogroup 1 isolates sequenced did not closely match any of the clinical _L. pneumophila_ serogroup 1 isolates suspected to be part of this outbreak, and differed by several thousand SNPs over the 3.4-Mb genome of the South Bronx hotel strain F4469 used as a reference. Five _L. pneumophila_ serogroup 1 isolates recovered from the South Bronx hotel and 41 clinical _L. pneumophila_ serogroup 1 isolates from 26 patients linked to this outbreak were identical (no SNP differences among them) . Eight other _L. pneumophila_ serogroup 1 clinical isolates (15–144, 15–157, 15–158, 15–202, 15–209, 15–215, 15–273, and 15–288) obtained during the same outbreak period had the same PFGE and SBT types as the outbreak isolates. However, these 8 isolates did not meet the epidemiologic case definition , and WGS showed that they contained 1–5 SNP differences compared with the South Bronx hotel isolate.\n\n【24】Four environmental isolates (3 isolates from the same homeless shelter and 1 from an East Bronx College) obtained during the investigation of the South Bronx outbreak  were nearly identical to the South Bronx hotel isolate, each differed by only 1 or 2 SNPs from the South Bronx hotel isolate and from one another. All 3 isolates from the homeless shelter had the same unique SNP that was absent from all clinical and environmental isolates linked to the South Bronx hotel. Moreover, the East Bronx College isolate, which was obtained from a site several kilometers from the South Bronx hotel, was identical by WGS to 1 South Bronx clinical isolate . Five other clinical isolates (15–288, 15–215, 15–157, 15–158, and 15–202) had SNP profiles that were closer to the isolate obtained from the East Bronx College than to the isolate obtained from the South Bronx hotel. Together, these observations suggest that 1) the South Bronx hotel cooling tower, and no other cooling towers, was most likely the source of the South Bronx outbreak; and 2) cases not epidemiologically linked with the outbreak might have originated from other environmental sources.\n\n【25】We also completed WGS for 10 historical clinical isolates of _L. pneumophila_ serogroup 1 DNA from New York, New York, and included 3 genome sequences from a previously published study  that reported identical or similar PFGE patterns and sequence types with those of the South Bronx hotel outbreak strain. These genomes differed by < 5 SNPs from those of the South Bronx hotel isolates. The oldest _L. pneumophila_ serogroup 1 isolate, dating back to 2007, had only 3 SNP differences, indicating that the isolate that caused the current outbreak had been present in the Bronx for \\> 8 years. Two clinical isolates and 1 environmental isolate (NH1, NH2, and NH3) obtained during an outbreak in a Bronx nursing home in 2011–2012 were also found to be closely related to the South Bronx hotel isolate ( < 3 SNP differences), which indicated that this isolate caused \\> 1 previous outbreaks of LD. WGS comparison of 4 other clinical isolates (09–214, 10–351, 10–423, and 10–458) from 2009 and 2010 showed an SNP profile that was identical to that of 3 of the clinical isolates from 2015 (15–157, 15–158, and 15–202) not epidemiologically linked to the South Bronx hotel–associated outbreak. These 2015 clinical isolates differed by 3 SNPs from the South Bronx hotel isolate, which suggested that patients might have been infected by an independent source that was not identified. In addition to SNPs, other genomic differences, such as the presence of plasmids or large indels, were also detected in some of the genomes analyzed.\n\n【26】WGS analysis of 6 _L. pneumophila_ serogroup 1 isolates from a second, late summer outbreak in 2015 in the East Bronx neighborhood (15 cases, 4 clinical isolates, and 2 environmental isolate sequences) that was not suspected to be linked with the July outbreak, was confirmed to be unrelated (1,038 SNP differences) when compared with the South Bronx hotel outbreak strain. However, closer examination of locations of the SNPs showed that most differences were highly clustered in a few genomic locations, rather than being randomly dispersed throughout the genome, and might have been the result of recombination events. Only 8 SNP differences remained when these recombination locations were omitted. Clustering of SNPs in an otherwise isogenic background suggests that the East Bronx and South Bronx strains only recently diverged after horizontal gene transfer events. WGS was the only method powerful enough to discriminate between South Bronx hotel and all the other environmental isolates, including the homeless shelter, and confirmed the South Bronx hotel cooling tower as the source of this outbreak.\n\n【27】### Discussion\n\n【28】This outbreak investigation represents a large-scale testing effort by the NYC PHL, WC, and CDC public health laboratories. As reported by Weiss et al. the environmental and epidemiologic investigation provided a comprehensive set of samples and specimens for laboratory testing.\n\n【29】Large outbreaks of LD can occur in areas of high population density that are near human-made reservoirs and mechanisms of aerosolization, such as cooling towers . Preventing or controlling such outbreaks in urban areas is further complicated by the presence of multiple potential reservoirs, which present substantial challenges when attempting to determine the exact point source. In a metagenomics survey of air samples obtained in New York, New York, _Legionella_ was the predominant genus identified in samples collected from the rooftop of an office building overlooking midtown Manhattan . Further complicating epidemiology studies, it has been shown that aerosols containing _L. pneumophila_ are capable of infecting persons residing at a distance of \\> 6 km from the contaminated source .\n\n【30】Our findings of similar _L. pneumophila_ strains at multiple locations and over extended periods is consistent with results of these studies and further suggest that _L. pneumophila_ is capable of long-term survival in multiple reservoirs over large areas in an urban environment. Our findings also suggest that cooling towers colonized with _L. pneumophila_ might contaminate other sites located nearby, leading to the possibility for an endemic strain to reestablish colonization after elimination of the organism at any single presumed source. This analysis warns us that because of the particular biologic and ecologic nature of _L. pneumophila_ , reliance solely on 1 source of evidence (epidemiologic approaches or molecular data) might be insufficient to identify exact sources of legionellosis outbreaks.\n\n【31】Our extensive sampling and WGS of cooling tower isolates has shown that many cooling towers were colonized with a diverse and heterogeneous _Legionella_ population, most of which have not caused detectable human disease. In the specific case of the South Bronx hotel cooling tower, 2 different _L. pneumophila_ serogroup 1 strains were obtained (among 10 isolates recovered), including the strain responsible for the 2015 outbreak. This finding showed that populations of virulent clones can coexist among a wide variety of nonoutbreak strains not associated with known disease, and for which virulence has not been assessed.\n\n【32】It is still uncertain what triggered the LD outbreak in New York, New York, in 2015, but several factors might have contributed. Improper maintenance of cooling towers or excessive mist generated during operation could have created ideal conditions for _Legionella_ spp. to multiply and aerosolize . In addition, a new _Legionella_ subpopulation could have acquired, through mutation or recombination, new beneficial phenotypic capabilities (such as increased resistance to cleaning agents), better survival to desiccation, or enhanced aerosolization capability . The low level of heterogeneity seen between the historical and 2015 isolates is consistent with results from a similar study of a persistent _L. pneumophila_ serogroup 1 outbreak–associated strain in Alcoy, Spain, where it was estimated that mutation rates for _L. pneumophila_ in cooling towers can be as low as ≈0.15 SNPs/genome/year (or 1 mutation across the entire genome every 6.7 years) . This estimation raises the possibility that _L. pneumophila_ can persist unchanged for extended periods in a dormant state until it is reactivated by favorable environmental conditions.\n\n【33】Genome analysis of the South Bronx outbreak strain identified several variable regions, many of which are associated with virulence factors, when compared with 5 previous outbreak-associated _L. pneumophila_ strains . Two regions, 1 containing genes encoding an F-type IVA secretion system and 1 encoding _Legionella_ U-box type E3 ligase/effector proteins, are also present in the 1976 Philadelphia 1 and Paris strains but absent from the other strains analyzed. The South Bronx outbreak strain F4469 also harbors an expanded isoform of the repeats in structural toxin gene ( _rtxA_ ), similar to that found in the Corby and Alcoy strains. Finally, 2 genomic islands, 1 containing the hippurate hydrolysis A and B gene toxin–antitoxin system, as well as other hypothetical genes, and 1 containing mostly uncharacterized genes, were found to be unique to the South Bronx strain. BLAST  searches on these 2 regions found matches with partial homology with other _L. pneumophila_ strains in the National Center for Biotechnology (Bethesda, MD, USA) nonredundant database. Further laboratory investigations will be required to determine the role of these islands, if any, to pathogenicity of this strain.\n\n【34】Our analysis showed the presence of _L. pneumophila_ strain F4469 in the Bronx since 2007 at multiple locations associated with different outbreaks and sporadic LD. Although it is unclear what caused the identified cooling tower to contribute to so many cases, our findings suggest that a persistent and pathogenic endemic strain exists and might pose a risk for future outbreaks. Conventionally, cooling towers are believed to be seeded by municipal water distribution networks, and although this factor might be true, in a densely populated area such as New York, New York, cross-contamination between towers is a real possibility. This contamination can potentially lead to reestablishment of _L. pneumophila_ in cooling towers after decontamination and cause long-term persistence of endemic strains in communities. Therefore, strict protocols regarding tower operation, maintenance, and cleanup, such as those mandated by recent New York State and New York City legislation, might help to minimize risks associated with locally circulating _L. pneumophila_ strains .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "83fe4cf9-f486-42eb-8a1e-04e1790bd161", "title": "Fly Transmission of Campylobacter", "text": "【0】Fly Transmission of Campylobacter\nAn annual increase in _Campylobacter_ infection in England and Wales begins in May and reaches a maximum in early June. This increase occurs in all age groups and is seen in all geographic areas. Examination of risk factors that might explain this seasonal increase identifies flies as a potential source of infection. The observed pattern of infection is hypothesized to reflect an annual epidemic caused by direct or indirect contamination of people by small quantities of infected material carried by flies that have been in contact with feces. The local pattern of human illness appears random, while having a defined geographic and temporal distribution that is a function of the growth kinetics of one or more fly species. The hypothesis provides an explanation for the seasonal distribution of _Campylobacter_ infections seen around the world.\n\n【1】_Campylobacter_ spp. are the most common bacterial causes of diarrhea in England and Wales . The epidemiologic features of _Campylobacter_ infection have proved difficult to discover, and extensive strain typing has failed to clarify the main transmission routes. Testable hypotheses must be established to explain available evidence, particularly the reason for the observed seasonality. Relatively few outbreaks of _Campylobacter_ gastroenteritis occur , and most cases are sporadic. In case-control and case-case studies of sporadic _Campylobacter_ infections, most cases remain unexplained by recognized risk factors .\n\n【2】The annual increase in _Campylobacter_ infections in England and Wales begins at approximately day 130 (May 9) and reaches a maximum at approximately day 160 (June 8) . Although this seasonal rise is seen in all ages, it is more marked in children . Cases in towns and cities across England and Wales show broadly similar seasonal changes in distribution . The relative geographic uniformity of the increase seen in May of most years has the temporal appearance of an annual national epidemic. Because person-to-person infection within the community is uncommon, it is likely that the epidemic is caused by a single main driver for human _Campylobacter_ infection. The possible seasonal drivers were examined, and only vector transmission by flies appears to provide a convincing explanation for the observed seasonal trends .\n\n【3】The seasonal increase in _Campylobacter_ infections in May and June in England and Wales is hypothesized to reflect an annual epidemic caused by direct or indirect exposure of humans to contaminated material carried by several fly species that have been in contact with human, bird, or animal feces or contaminated raw foods. Flies have been shown to carry _Campylobacter_ and can infect both humans and animals . Intervention studies have demonstrated diarrheal disease reduction linked to control of flies , and deaths from diarrheal diseases have been linked to measurements of fly abundance . The local pattern of human _Campylobacter_ infection appears random, while having a defined geographic and temporal distribution. This distribution is predicted to be linked to the growth kinetics of 1 or more fly species and their access to environmental sources of _Campylobacter_ in feces or food. The seasonal increase in fly populations results from rainy weather and an increase in temperature that causes the development from egg to fly to occur in days rather than months. Individual flies can lay hundreds of eggs, which can result in a large increase in fly numbers in a short period. Fly numbers fluctuate through the summer and decline in October, but the decline is less dramatic and defined than the spring increase.\n\n【4】Disease transmission is hypothesized to occur through small quantities of contaminated material carried on the feet, proboscis, legs, and body hairs or from material regurgitated or defecated by flies. The variety, numbers, virulence and viability of organisms in the contaminated material will differ, and some contamination will include _Campylobacter_ while others will not. Contamination will be distributed over a variety of food types. Contamination of food by flies could occur at any stage of the food supply chain, but _Campylobacter_ counts within the contaminated material on foods will decrease over time; consequently, most infection will result from contamination close to consumption (e.g. in the domestic or catering environment). Because whether a fly has visited contaminated feces is unknown and how a person becomes infected is uncertain, epidemiologic investigation is difficult.\n\n【5】A number of synanthropic fly species could be involved, including houseflies (e.g. _Musca_ spp. _Fannia_ spp.), blowflies (e.g. _Calliphora_ spp. _Lucilia_ spp.), and other dung-related flies (e.g. _Sarcophaga_ spp. _Drosophila_ spp.) . These flies have individual behavioral patterns, ecology, physiology, and temporal and geographic distributions that will influence the likelihood of their being in kitchens, on human or animal feces, and on food. Although _Musca domestica_ is the species most likely to be involved because it is commonly found in houses and food-processing establishments, larger flies (e.g. _Calliphora_ spp.) may be able to transmit larger numbers of _Campylobacter_ .\n\n【6】Flies contaminated through fecal contact will carry heterogeneous mixtures of organisms, including any pathogens that are present within the feces, and may be able to cause a variety of human infections, including infection by different _Campylobacter_ species and types. This fact partially explains the lack of a clear epidemiologic picture arising from _Campylobacter_ typing work. Gastrointestinal disease caused by flies is more likely to involve pathogens with a low infectious dose (e.g. _Shigella_ , _Campylobacter_ , _Cryptosporidium_ , _Giardia_ , _Cyclospora_ , _Escherichia coli_ O157), and some of these could have a seasonal component related to flies. Where high fly populations and poor hygiene conditions prevail, as in disasters or famines, or where pathogens can grow within fly-contaminated food, the potential exists for transmitting pathogens with a high infectious dose (e.g. _Vibrio cholerae_ , _Salmonella_ spp.). The access that flies have to human and animal feces will influence the degree to which they are contaminated with different enteric pathogens.\n\n【7】Contamination of a range of foods by flies will result in a pattern of infection that will not be amenable to identifying specific vehicles through standard case-control, case-case, or cohort studies, unless specific objective or subjective assessments of fly numbers can be obtained. Fly monitoring will need to be undertaken. An alternative approach could use estimates of fly population numbers based on climatic conditions to compare with data on human _Campylobacter_ infections. This approach has the advantage of being able to use historical climatic and disease surveillance data. The broad relationship between _Campylobacter_ cases and ambient temperature has not been explained in terms of disease causation. The time taken for the larvae of _M. domestica_ to develop  was applied to temperature data for England and Wales and has been used to show a strong relationship between _Campylobacter_ cases per week and _M. domestica_ larval development time for 1989 to 1999 . Periods when _Campylobacter_ cases exceed a 7-day average of 170 cases per day occurred when _M. domestica_ larval development time was <3 weeks.\n\n【8】The hypothesis predicts that the _Campylobacter_ infection rates will be higher in persons living close to animal production and lower in urban settings because fly numbers will be lower. Some evidence from the United Kingdom  and Norway  supports this hypothesis. Seasonal changes in _Campylobacter_ incidence that are seen around the world may result from changes in fly populations and flies' access to human and animal feces. Much emphasis on foodborne disease reduction has rightly been on kitchen hygiene, since the low infectious dose of _Campylobacter_ makes cross-transmission from raw meats to ready-to-eat foods a substantial risk in domestic and catering environments. Fly transmission may be the most important source of infection in kitchen transmission routes, and establishments that sell ready-to-eat foods may be sources of _Campylobacter_ , if effective fly control is not in operation. Flies may also be important in transmitting _Campylobacter_ in poultry flocks  and between other agricultural animals.\n\n【9】While flies are regarded as important mechanical vectors of diarrheal disease in developing countries, control has largely concentrated on improving drinking water and sewage disposal. In the industrialized world, flies are thought to play a minor role in the transmission of human diarrheal diseases. Immediately intervening in the transmission of _Campylobacter_ gastroenteritis should be possible through increased public awareness and more effective fly control.\n\n【10】##### Supplementary Information\n\n【11】##### Temperature Data\n\n【12】Temperature data were acquired from the British Atmospheric Data Centre (BADC), the Natural Environment Research Council's (NERC) Designated Data Centre for the Atmospheric Sciences based at the Rutherford Appleton Laboratory in Oxfordshire, part of the Central Laboratory of the Research Councils. Data are available on-line through a World Wide Web interface  by prearranged agreement. Data were collated for the period 1989–1999, with 5 locations selected for each region to provide overall coverage of the region (except London, which had only 2 centers with data available for the given time period). Location of temperature stations is shown in the Figure A1 .\n\n【13】There were a total of 47 sites. Some of the data series were missing data points. The maximum, minimum, and average temperatures were determined for all days between January 1, 1989, and December 31, 1999. Maximum temperatures across all sites were used to calculate the presumptive minimum _Musca domestica_ larval development times.\n\n【14】##### Methods\n\n【15】The data represent patients who had fecal specimens examined by a microbiology laboratory in England and Wales between 1989 and 2003 where _Campylobacter_ was isolated from the sample. Data were acquired through well-described surveillance processes, and analysis was conducted in Microsoft Access and Excel (Microsoft Corp. Redmond, WA, USA). Daily cases were based on the patient specimen date, and a 7-day rolling mean was used to eliminate the weekly cycles that reflect reduced patient sampling on weekends.\n\n【16】Hypothesis generation was performed through a systematic review of known and suggested causes of _Campylobacter_ infection, particularly reflecting on changes in these risks over the period of May and June and assessing their credibility as biological drivers for the observed seasonality.\n\n【17】##### Cities and Towns Included in Figure A1\n\n【18】S1, London; S2, Birmingham; S3, Bristol; S4, Nottingham; S5, Sheffield; S6, Manchester; S7, Leeds; S8, Leicester; S9, Reading; S10, Plymouth; S11, Portsmouth; S12, Colchester; S13, Bradford; S14, Southampton; S15, Poole; S16, Preston; S17, Cardiff; S18, Chelmsford; S19, Norwich; S20, Ipswich; S21, Truro; S22, Oxford; S23, Shrewsbury; S24, Dudley; S25, Taunton; S26, Newport; S27, Cambridge; S28, Newcastle; S29, Chester; S30, Gloucester; S31, Swindon; S32, Chertsey; S33, Coventry; S34, Welwyn; S35, Frimley Park; S36, High Wycombe; S37, Slough; S38, Exeter; S39, Swansea; S40, Luton; S41, Torquay; S42, Derby; S43, York; S44, Worcester; S45, Northampton; S46, Bishops Stortford; S47, Hull; S48, Basildon; S49, Stoke-on-Trent; S50, Worthing; S51, Stafford; S52, Harrogate; S53, Hereford; S54, Halifax; S55, Sunderland; S56, Chesterfield and N Derbyshire; S57, Lincoln; S58, Ashford Kent; S59, Stockport; S60, Blackpool; S61, Maidstone; S62, Liverpool; S63, Bangor; S64, Llandough; S65, Lancaster; S66, Sutton Coldfield; S67, Aylesbury; S68, Grimsby; S69, Doncaster; S70, Peterborough; S71, Brighton; S72, Gateshead; S73, Kettering; S74, Southend; S75, Rhyl; S76, Cheltenham; S77, Epsom; S78, Chichester; S79, Carlisle; S80, Milton Keynes; S81, Dorchester; S82, Durham; S83, Bury; S84, Great Yarmouth; S85, Bury St Edmunds; S86, Warwick; S87, Salisbury; S88, Wolverhampton; S89, Scarborough; S90, Pontefract; S91, Bath; S92, Winchester; S93, Bishop Auckland; S94, Watford; S95, Bolton; S96, Eastbourne; S97, Oldham; S98, North Shields; S99, Burnley; S100, Ashford Middlesex; S101, Kings Lynn; S102, Warrington; S103, Wakefield; S104, Keighley; S105, Crawley; S106, Barnstaple; S107, Abergavenney; S108, Boston; S109, Nuneaton; S110, Northallerton; S111, Wrexham; S112, Macclesfield; S113, Darlington; S114, Bedford; S115, Basingstoke; S116, Weston Supermare; S117, Middlesborough; S118, Dewsbury; S119, Sutton-in-Ashfield; S120, Rochdale; S121, Guildford; S122, Worksop; S123, Wigan; S124, Stevenage; S125, Bridgend; S126, Rotherham; S127, West Bromwich; S128, Solihull; S129, Burton-upon-Trent; S130, Haverford West; S131, Carmarthen; S132, Hemel Hempstead; S133, Stockton-on-Tees; S134, Huddersfield; S135, South Shields; S136, Barnsley; S137, Whitehaven; S138, Chatham; S139, Blackburn; S140, Redditch; S141, St Leonards-on-Sea; S142, Grantham and Kesteven; S143, Ormskirk; S144, Scunthorpe; S145, Canterbury; S146, Kidderminster; S147, Dartford; S148, Aberystwyth; S149, Hexham; S150, Barrow-in Furness; S151, Redhill; S152, Margate; S153, Walsall; S154, Ashington; S155, Salford; S156, Merthyr Tydfil; S157, Stourbridge; S158, Haywards Heath; S159, Banbury; S160, Hartlepool; S161, Prescot; S162, Otley; S163, Southport; S164, Yeovil; S165, Llanelli. The number of reported _Campylobacter_ cases per city and town were based on reports from all laboratories serving the area and are ordered from highest (S1) to lowest (S165) case numbers. Results from towns reporting smaller numbers of cases were excluded from the analysis.\n\n【19】##### Factors Linked to _Campylobacter_ Infection\n\n【20】The Table A1 provides evidence for seasonal associations between factors linked to human _Campylobacter_ infections or outbreaks.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f8e0c543-173d-4c41-bc82-04c00d9c503c", "title": "Leishmania infantum Infection in Blood Donors, Northeastern Brazil", "text": "【0】Leishmania infantum Infection in Blood Donors, Northeastern Brazil\n**To the Editor:** _Leishmania infantum_ is endemic to northeastern Brazil. It is responsible for visceral leishmaniasis (VL), a major emerging health problem in urban areas. Transmission occurs predominantly by the _Lutzomyia longipalpis_ sand fly, but transfusion-associated VL, caused by _L. infantum_ , has been reported from southern Europe and, by _L. donovani_ , on the Indian subcontinent . Most _L. infantum_ infections are asymptomatic , raising concern that the parasite could be present in donated blood from otherwise healthy residents in areas to which it is endemic .\n\n【1】VL caused by _L. infantum_ is endemic to 20 of Brazil’s 27 states; an annual average of 3,553 cases occur nationwide, with 54% of all cases reported from Brazil’s northeastern region. The state of Ceará historically ranks first or second in number of cases; an annual average of 467 cases were reported during the last decade . Thirty-eight percent of cases were reported from Fortaleza, the capital, where 28.4% of the state’s population resides. Over a 10-year-period, 277 (7.8%) persons with VL have died, and 109 (39%) of VL-related deaths have occurred in Fortaleza.\n\n【2】Sixty-nine percent of blood donors for Ceará reside in Fortaleza. To determine the prevalence of _Leishmania_ infection among healthy blood donors, we tested blood donated to the State of Ceará Public Blood Bank. Compulsory serologic testing was also done for _Trypanosoma cruzi_ (Chagas disease), hepatitis B and C, _Treponema pallidum_ (syphilis), human T-cell lymphotropic virus types 1 and 2, and HIV-1 and -2. In the blood bank, 60% of units are centrifuged to separate the buffy coat in preparation for platelet separation. During May–November 2011, we randomly selected 431 buffy coats and tested them for _Leishmania_ spp. by ELISA and PCR. To separate plasma from cells, 10 mL of buffy coat was centrifuged in Ficoll-Hypaque (Histopaque −1077, Sigma-Aldrich, São Paulo, Brazil). We tested plasma for leishmanial IgG by ELISA using a modified protocol of Evans et al. IOC/L2906 _L. infantum_ strain (MHOM/BR/2002/LPC-RPV) was used as the source of promastigote antigens. In addition, DNA was extracted from the mononuclear cell preparation, and PCR was performed with primers 150 (5′-GGG\\[G/T\\]AGGGGCGTTCT\\[G/C\\]CGAA-3′) and 152 (5′-\\[G/C\\]\\[G/C\\]\\[G/C\\]\\[A/T\\]CTAT\\[A/T\\]TTACACCAACCCC-3′) that target the 120-bp conserved region of the _Leishmania_ kDNA minicircle present in all _Leishmania_ spp . As a positive control, kDNA was extracted from _L. amazonensis_ promastigotes, strain BA-125 (MHOM/BR/87), characterized by PCR and isoenzymes . All PCR-positive samples were purified and sent to Ludwig Biotec (Alvorada, Brazil) for sequencing by ACTGENE-Molecular Analysis. The Federal University of Ceará Ethics Committee approved this study.\n\n【3】Buffy coats from 57 (13.2%) serum samples from 431 donors were positive for leishmanial IgG, and 20 (4.6%) were positive for _Leishmania_ spp. DNA. Sequencing of all PCR-positive samples confirmed the _Leishmania_ genus. Three donors tested positive by both ELISA and PCR. Overall, the prevalence of leishmanial infection was 17.1% of blood donors. Eighty of the 431 units tested positive for \\> 1 of compulsorily screened infections and were rejected. Of the remaining 351 that were negative for co-infection, 43 (12.2%) were positive for leishmanial IgG and 15 (4.3%) for _Leishmania_ spp. DNA. Two donors were positive for both by ELISA and PCR. The prevalence of _Leishmania_ infection among blood units accepted for transfusion was 16%.\n\n【4】The results demonstrate a surprisingly high prevalence of _Leishmania_ infection in blood donors in Fortaleza, several times higher than that other diseases for which blood is screened . In a recent study in Salvador, Brazil , 5.4% of blood donors had leishmanial antibodies, of which 68% were positive by its PCR targeting kDNA amplification.\n\n【5】The percentages of antibody- or PCR-positive units capable of transmitting _Leishmania_ and the outcomes are unknown. Viable _Leishmania_ might not be in the blood of all PCR-positive donors, and even when present, the inoculum might be reduced by removal of infected circulating mononuclear phagocytes in the buffy coat, or parasites might be affected by steps involved in preparation or storage. However, if we consider units that test positive by PCR as being potentially infectious, the number of recipients at risk is of substantial concern. For example, in 2011, there were 99,933 blood donations to the State of Ceará Public Blood Bank. After compulsory screening for the other bloodborne pathogens, 93,238 units were accepted for transfusion. Extrapolating from the PCR-positive rate of 4.3%, a total of 4,009 recipients possibly were exposed to infection. Further studies are needed to determine whether recipients of blood from donors who are PCR positive and/or leishmanial antibody positive become infected with _L. infantum_ . Persons with advanced AIDS or other immunosuppressive conditions seemingly would be at greatest risk for VL.\n\n【6】In Brazil, legislation requires that all blood for transfusion be tested for _T. cruzi_ . hepatitis B and C, _T. pallidum_ , human T-cell lymphotropic virus types 1 and 2, and HIV-1 and -2. As additional information becomes available, screening for _L. infantum_ also might be advisable to reduce the possibility of the recipient becoming infected, developing VL, and possibly being a reservoir of infection in the community , particularly in Ceará and other regions where the prevalence of _L. infantum_ infection is high.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d38f83ef-99c4-4f2a-aa91-15bba2ee548c", "title": "Multidrug-Resistant Acinetobacter Extremity Infections in Soldiers", "text": "【0】Multidrug-Resistant Acinetobacter Extremity Infections in Soldiers\nCasualty statistics from the 2003–2005 military operations in Iraq show an increase in the ratio of wounded to fatal casualties compared to previous operations in the Persian Gulf, Vietnam, and Korea . This relative increase of wounded casualties has led to an increased incidence of war wound infection and osteomyelitis, especially caused by multidrug-resistant (MDR) _Acinetobacter_ species. The incidence of bacteremia at military medical facilities caused by _Acinetobacter baumannii_ has also increased . The current incidence of infection with _Acinetobacter_ should not be surprising. These organisms were the most frequently recovered gram-negative isolate from war wounds and the second most frequent bacterium causing bloodstream infection in US Marines with extremity wounds during the Vietnam War . In nonconflict environments, _Acinetobacter_ species are rarely responsible for community-acquired infections. In the hospital setting, _Acinetobacter_ species are an important cause of nosocomial infection, yet these infections were rarely encountered in our facility until we began observing them in soldiers with infected wounds. Nosocomial infections caused by _Acinetobacter_ species include pneumonia, meningitis, bloodstream, urinary tract, surgical wound, and soft tissue infections . Such infections are challenging to treat because of extensive antimicrobial drug resistance. Osteomyelitis caused by _Acinetobacter_ occurs, but it is less frequently reported and had not been identified in our facility during the 14 months before March 2003. Optimal therapy for osteomyelitis caused by these organisms is not well defined because of limited available data. This case series reviews 1 military medical center's experience with these infections, including species identified, antimicrobial drug–susceptibility patterns, antimicrobial drug therapy, and clinical outcomes.\n\n【1】### Methods\n\n【2】Case reports were compiled from active-duty soldiers admitted to Brooke Army Medical Center (BAMC) in San Antonio, Texas. This tertiary military medical center serves a population of active-duty and retired soldiers and their dependents along with a limited number of civilian trauma patients admitted from the local area. The hospital was operating at an average capacity of 175 beds during the study period. This facility also houses the US Army's Institute of Surgical Research, which treats both active-duty and civilian trauma patients with burn injuries. Data collection for this case series was completed under a study protocol approved by BAMC's Department of Clinical Investigation Institutional Review Board.\n\n【3】##### Identification of Patients\n\n【4】All wound, sputum, urine, and blood culture results completed at our hospital from March 1, 2003, to May 31, 2004, were reviewed. Those patients who had _Acinetobacter_ \\-positive cultures were then compared to all active-duty soldiers admitted to our facility. A soldier was considered for inclusion if he had an _Acinetobacter_ \\-positive culture and had been deployed to Iraq or Afghanistan and had an admission diagnosis of injury (ICD codes 800.0–900.0). Similarly, hospital admission and laboratory data were reviewed for the 14 months before the study period to define the incidence of _Acinetobacter_ infection in hospitalized, active-duty soldiers before the onset of military action in Iraq.\n\n【5】##### Case Definitions\n\n【6】Patients with either _Acinetobacter_ contiguous focus osteomyelitis or wound infection are included in this series. Cases were defined as osteomyelitis if bone tissue collected during surgical procedures (primarily open debridements but also including placement of external or internal fixators or bone grafting) was positive for _Acinetobacter_ spp. on routine culture . In addition, patients with open fractures or exposed bone with gross findings of infection (purulence, necrotic tissue, or environmental contamination with exposed bone), clinical evidence of infection (temperature >38°C, leukocyte count >12,000/μL), and _Acinetobacter_ spp. identified from culture of deep wound tissue obtained intraoperatively, excluding bone, were also defined as having osteomyelitis . Cases were defined as wound infection if similar deep wound cultures were positive for _Acinetobacter_ spp. with gross findings and clinical evidence of infection but no exposed bone and no fracture. Colonization with _Acinetobacter_ was defined as a positive culture for _Acinetobacter_ without gross findings or clinical evidence for infection.\n\n【7】The _Acinetobacter_ isolate was defined as MDR if it was resistant to ≥3 classes of antimicrobial agents as tested by automated antimicrobial drug–susceptibility testing (Vitek, bioMérieux, Hazelwood, MO, USA) . On occasion, isolates were further evaluated with disk diffusion antimicrobial testing for susceptibilities to alternate antimicrobial drugs, such as colistin, or to confirm automated susceptibility results. Confirmatory disk diffusion susceptibility testing was completed only for those isolates that were resistant to all antimicrobial agents by automated testing or if only 1 antimicrobial drug was listed as susceptible. Disk diffusion testing was performed in accordance with Clinical and Laboratory Standards Institute (formerly NCCLS) guidelines . Colistin susceptibility was assumed if the zone of inhibition was ≥14 mm .\n\n【8】Patients were evaluated for recurrence of infection. Many patients underwent subsequent reconstructive surgeries, and the bone tissue was sent for culture. Definitions of recurrent infection followed the previously described criteria for the case definitions with the following additions: recurrent infection was defined as having _Acinetobacter_ spp. isolated at the original site of infection after completing an antimicrobial drug treatment course for the initial infection; secondary infection was defined as infection with a different organism at the same site as the initial _Acinetobacter_ infection.\n\n【9】##### Data Collection\n\n【10】Both electronic and paper charts of all patients who met case definition criteria were retrospectively reviewed for demographic, diagnostic, and treatment data. Laboratory results were reviewed for _Acinetobacter_ species isolated and antimicrobial drug susceptibilities. Patients were also interviewed either in person or by telephone to confirm mechanism of injury, length of antimicrobial drug treatment course, recurrence of infection, subsequent hospital admissions, and clinical outcome of the sustained injury and infection (resolved, continuing convalescence, or amputation). Follow-up was defined as the time from completing the initial antimicrobial treatment course to the date of the study interview.\n\n【11】### Results\n\n【12】##### Case Inclusion Criteria\n\n【13】From March 1, 2003, to May 31, 2004, a total of 24,114 cultures (blood, urine, wound, sputum) were completed in our hospital. Of these, 145 (0.6%) were positive for _Acinetobacter_ spp. During the same period, 237 active-duty patients were admitted to our facility with the admission diagnosis of injury . Of these admitted soldiers, 151 (64%) had been deployed to OIF/OEF. Cultures of blood, wound, sputum, urine, or skin were obtained for 84 of these patients; 48 (32% of admitted deployed soldiers) were positive for _Acinetobacter_ spp. Of these, 30 (63%) represented clinical infection; the remaining 18 represented colonization with _Acinetobacter._ Of those patients with cultures that represented clinical infection, 23 met the case definition for _Acinetobacter_ osteomyelitis  or _Acinetobacter_ wound infection . During the 14 months before the study period, only 2 active-duty soldiers, of 326 admitted to our facility, had any _Acinetobacter_ infection. The incidence of _Acinetobacter_ infection during the study period represents a significant increase when compared to the control period (p<0.01 by 2-tailed Fisher exact test).\n\n【14】##### Demographics\n\n【15】All patients included in this series had been transferred to BAMC through the military airmobile medical evacuation system. All, excluding one, were evacuated through, and admitted for at least 1 day to, Landstuhl Army Medical Center in Landstuhl, Germany; 3 patients were admitted to a second US Army medical center before admission to BAMC. The median time from injury to admission at BAMC was 6 days (range 2–36 days). The median time from injury to identification of infection was also 6 days (range 3–12 days). _Acinetobacter_ infection was initially identified at BAMC in 15 of the 23 patients; the remainder were identified at a previous medical center. None were initially diagnosed prior to evacuation from Iraq or Afghanistan. The median age of the patients was 26 years (range 20–48), and all but 2 were men. Patients were generally stable on admission to BAMC and did not require admission to an intensive care unit.\n\n【16】##### Microbiologic Data\n\n【17】Patients with _Acinetobacter_ osteomyelitis primarily had bone tissue collected during surgical procedures that was culture-positive for _A. calcoaceticus-baumannii_ complex. This was the only species and organism identified in all initial tissue cultures. Ten patients had deep wound cultures, excluding bone tissue, that were positive for _A. calcoaceticus-baumannii_ complex. Five (patient numbers 4, 9, 10, 11, and 15) had open fractures with environmental contamination and signs of infection that met the case definition of osteomyelitis. The remaining 5  did not meet criteria for diagnosis of osteomyelitis and were diagnosed with wound infection. Two of these patients had burn injuries. Cultures of debrided soft tissue in these 2 patients were positive for _Acinetobacter_ within the first 8 days of hospitalization, and pathologic evaluation of tissue demonstrated invasive infection. Patient no. 22 had a soft tissue wound culture positive on hospital day 5 (postinjury day 9); patient no. 23 had a soft tissue wound culture positive on hospital day 8 (postinjury day 10).\n\n【18】##### Antimicrobial Drug–susceptibility Data\n\n【19】Thirty-eight cultures from the 23 patients reported in this study were positive for _Acinetobacter_ spp. Twenty-nine isolates were MDR, as tested by automated susceptibility testing. All but 4 of the MDR isolates were susceptible to imipenem, and no imipenem resistance developed in the 15 patients who received this drug during therapy. Three of these 4 isolates were susceptible only to amikacin. Of the 25 imipenem-susceptible MDR _Acinetobacter_ isolates, 10 demonstrated resistance to all other tested antimicrobial agents. Other isolates were susceptible to only 1 other antimicrobial agent: 7 were also susceptible to amikacin, 3 to ampicillin/sulbactam, 2 to tobramycin, and 1 to trimethoprim/sulfamethoxazole. Nine isolates were not MDR. These isolates were susceptible to ≥3 classes of the tested antimicrobial agents. Three MDR isolates were tested for susceptibility to colistin; all 3 were susceptible by disk diffusion testing. One was susceptible only to imipenem, 1 to amikacin alone, and 1 to both amikacin and ceftazidime.\n\n【20】##### Therapy\n\n【21】Antimicrobial drug treatment of these infections was based on susceptibility testing, and all patients with osteomyelitis underwent multiple surgical debridements of necrotic bone. Ten of the patients with osteomyelitis were treated with dual antimicrobial agents, 7 with monotherapy, and 1 with surgical debridement alone. Only patients with osteomyelitis received dual antimicrobial drug therapy. Of the 10 treated with dual therapy, 5 had MDR _Acinetobacter_ spp. and 5 had non-MDR _Acinetobacter_ spp. isolated. The primary combination of antimicrobial agents was imipenem (500 mg every 6 h) in combination with high-dose amikacin (15–20 mg/kg daily). In a few instances, when imipenem was not active against the isolated organism, ampicillin/sulbactam or ceftazidime was used if either was active against the particular isolate . Of the 7 treated with monotherapy, 5 had MDR _Acinetobacter_ isolated. All patients with wound infection received monotherapy based on antimicrobial drug–susceptibility testing results.\n\n【22】##### Follow-up\n\n【23】The follow-up period was 1–23 months (mean 9 months). During this time, no _Acinetobacter_ infections recurred at any site, including the bloodstream. Seven secondary infections occurred, 6 in those with an initial diagnosis of osteomyelitis and 1 with wound infection. Four occurred in patients with MDR _Acinetobacter_ (3 with osteomyelitis and 1 with wound infection). These secondary infections primarily involved other resistant nosocomial pathogens .\n\n【24】##### Control Period\n\n【25】During the 14 months before March 2003, only 2 active-duty soldiers had _Acinetobacter_ infection. A soft tissue infection with _Acinetobacter_ developed in 1 soldier with a history of bullous pemphigoid. Bacteremia with _Acinetobacter_ developed in the other soldier, who had a history of Ewing sarcoma. The latter _Acinetobacter_ isolate was not a MDR organism and was treated with imipenem (500 mg parenterally) for 14 days.\n\n【26】### Discussion\n\n【27】The 23 cases observed during the study period represent a significant increase in the incidence of clinical infection with _Acinetobacter_ in our facility. Similarly, the rate of blood, wound, or urine cultures positive for _Acinetobacter_ species increased 3-fold during the study period as compared to the control time period (data not shown). This increase and the influx of severe extremity infection due to MDR _Acinetobacter_ species posed considerable challenges. The foremost was determining appropriate therapy for osteomyelitis caused by MDR _Acinetobacter_ species without institutional or historical experience to guide us. In addition, increasing prevalence of this MDR gram-negative organism in our facility mandated new infection control procedures to limit nosocomial spread. Finally, the occurrence of _Acinetobacter_ wound infection was somewhat unexpected, and initially the reservoir for infection was unclear and generated much debate. Recent investigation by the military medical and research community suggests that these are nosocomial infections; however, their exact source remains unclear.\n\n【28】Most _Acinetobacter_ infections reported in the literature reflect nosocomial _Acinetobacter_ , as hospitalized patients are at increased risk because of severe illness or disability, extremes of age, and relative states of immunocompromise . _Acinetobacter_ species can cause infection in any organ system, including bacteremia, pneumonia, endocarditis, meningitis, urinary tract infection, intraabdominal abscess, osteomyelitis, soft tissue infection, and surgical site infections . Data collected from a review of sentinel hospitals in the United States demonstrated that 1.5% of all nosocomial bloodstream infections were due to _Acinetobacter_ species . Crude death rates associated with nosocomial _Acinetobacter_ infection are 19%–54% . The difficulty in treating these infections is not due to any excessive virulence of the organism per se but rather to its antimicrobial drug resistance. Many nosocomial isolates are resistant to ≥3 classes of antimicrobial agents, which classifies them as MDR organisms . A common susceptibility pattern in this case series was resistance to all antimicrobial agents except imipenem and amikacin.\n\n【29】When these patients were first evaluated, data to guide therapeutic decisions were limited. Previous reported experience with osteomyelitis caused by _Acinetobacter_ species is scant. It has been described after a hamster bite in an 8-year-old boy  and in a patient who previously had an artillery fragment injury that caused an open fracture of the right femur . Other reviews have described osteomyelitis as a sequela of infection with _Acinetobacter_ species but did not report details of therapy or followup . Patients in our case series primarily received extended dual antimicrobial–drug therapy based on susceptibility patterns of the recovered organisms. Combination therapy has been shown to decrease the risk for development of more highly resistant organisms, which has been reported when single agents are used alone . While on this antimicrobial regimen, patients demonstrated clinical improvement with marked reduction of inflammatory markers. Many of these patients had internal stabilizing hardware placed into the infected area at the time of diagnosis of infection. This hardware remained in place at the completion of parenteral therapy. In these situations, when the causative organism was susceptible to oral antimicrobial agents, oral suppressive therapy was continued as long as the stabilizing hardware remained in place. In most cases, however, because of extended antimicrobial drug resistance, no oral agents maintained activity against the _Acinetobacter_ isolate. Once those infected with MDR isolates demonstrated clinical improvement and normalization of inflammatory markers, antimicrobial drug therapy was discontinued without continuing long-term suppressive therapy .\n\n【30】During the follow-up period, no recurrent episodes of _Acinetobacter_ osteomyelitis have occurred. The relative brevity of follow-up is a limitation of this study. The ultimate outcome for these patients will not be known for many years, as they have increased risk for recurrent infection throughout their lifetime. In addition, _Acinetobacter_ organisms do not possess substantial inherent virulence. None of the patients in this series failed therapy, and none died because of _Acinetobacter_ infection. Such is not the case in outbreaks among immunocompromised or intensive care patients, in whom _Acinetobacter_ infection leads to increased mortality . The successful outcomes in this case series may be a reflection of the youth and general good health of the soldiers infected.\n\n【31】MDR _Acinetobacter_ is an important nosocomial pathogen with multiple recent outbreaks reported . It has the capacity to survive in dry environments , which increases the risk for nosocomial transmission. The increasing prevalence of MDR _Acinetobacter_ in our facility led to new infection control procedures. Currently, all injured soldiers admitted to our facility returning from OIF/OEF are placed in contact isolation. Screening cultures of the axilla, groin, and any open wound are completed to assess for colonization with MDR _Acinetobacter_ , which was identified in 18 of 151 admitted soldiers during the study period . If all cultures taken on admission are negative, the soldier is then removed from contact isolation. Soldiers with wound infection or osteomyelitis caused by MDR _Acinetobacter_ are kept in contact isolation for the duration of hospitalization. Implementation of these types of infection control procedures has limited nosocomial spread in previously reported outbreaks , which is the goal of our current policy, in addition to controlling the continuing reservoir of this organism.\n\n【32】As previously noted, we initially suspected that colonized soldiers themselves were the reservoir for MDR _Acinetobacter_ , and that this colonization was obtained from the environment. This hypothesis was based on 2 facts. First, these organisms are ubiquitous in the environment , and inoculation of these organisms into war wounds during traumatic blast, shrapnel, or projectile injuries seemed to be plausible. Second, _Acinetobacter_ spp. had previously been described as common pathogens in war wounds , supporting the initial hypothesis. However, these infections are apparently similar to recently reported nosocomial MDR _Acinetobacter_ infections. Investigation into the cause of these infections is ongoing, but the source is unlikely to be environmental. Multiple follow-up soil samples have not yielded _Acinetobacter_ , yet it has been recovered from environmental cultures within field medical facilities. The final outcome of this investigation is pending further analysis.\n\n【33】Data from this case series demonstrate that highly resistant _Acinetobacter_ infection, including osteomyelitis, can be successfully treated with appropriate surgical debridement, directed antimicrobial drug therapy, and careful follow-up. Our patients responded to this multifaceted approach, although their final outcome will not be determined for several years. These patients continue to be followed for recurrence of MDR _Acinetobacter_ infection. Clearly, guided therapy based on antimicrobial drug susceptibility leads to suppression of recurrent infection up to 23 months. Most of the patients in this series did not receive extended continuation therapy with oral antimicrobial agents; whether such therapy would provide added benefit is unclear. However, few antimicrobial drug options are currently available, with none soon to be released, to treat infections caused by resistant gram-negative organisms. Increasing prevalence of these types of infections highlights the necessity for newer antimicrobial agents with activity against these organisms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "158d8901-4ab7-464a-9ce3-2d8e85f62b34", "title": "Rickettsia japonica Infections in Humans, Xinyang, China, 2014–2017", "text": "【0】Rickettsia japonica Infections in Humans, Xinyang, China, 2014–2017\nEmerging and reemerging spotted fever group (SFG) rickettsioses are increasingly spreading and being recognized worldwide. Japanese spotted fever (JSF), caused by _Rickettsia japonica_ and first reported in Japan in 1984 , is an SFG rickettsiosis characterized by fever, malaise, chills, headache, rash, and eschars . JSF cases have been increasingly reported, but mostly in Japan and the nearby countries of South Korea, the Philippines, and Thailand . _R. japonica_ has been detected in 3 genera ( _Haemaphysalis_ , _Dermacentor_ , _Ixodes_ ) and 8 species of ticks . Because of the wide spectrum of ticks in which this bacterium has been found, _R. japonica_ is postulated to be distributed more widely throughout China. The identification of _R. japonica_ in _H_ . _flava_ and _H_ . _hystrici_ s ticks in Wuhan, China, further corroborated this hypothesis . Also, serologic responses to _R. japonica_ were detected in patients in Hainan Province in southern China ≈3 decades ago . However, laboratory-confirmed JSF cases have been identified in only a few patients in China . In this study, we aimed to determine if more JSF cases were occurring in China by using hospital-based surveillance.\n\n【1】### The Study\n\n【2】During March 2014–June 2017, we screened 2,236 febrile patients seeking treatment at The People’s Liberation Army 990 Hospital in Xinyang, Henan Province, eastern central China  with a history of tick bite or field activity within the previous month for infection with _R. japonica_ . We extracted DNA from peripheral blood samples collected during the acute phase of illness and screened them by a nested PCR concurrently targeting the citrate synthase gene ( _glt_ A), 17-kDa antigen–encoding gene ( _htr_ A), and outer membrane protein A–encoding gene ( _omp_ A), as previously described . To minimize the risk for contamination, we performed template isolation and PCR setup in separate rooms with specified pipette sets, used certified DNAase- and RNase-free filter barrier tips to prevent aerosol contamination, and performed all PCR assays with appropriate controls. After amplification and sequencing, we found that 20 patients were infected with _R. japonica_ .\n\n【3】The _glt_ A gene sequence (456-bp) obtained from all 20 samples  showed 100% similarity to the corresponding gene of _R. japonica_ strain YH . The _htr_ A gene sequence (394-bp) obtained from 18 samples  was identical to that of _R. japonica_ YH, and the sequence in the remaining 2 samples  differed by only 3 bp. The _omp_ A gene sequence (311-bp) found in 16 samples  was identical to that of _R. japonica_ YH, and the remaining 4 samples each differed by just 1 bp (GenBank accession nos. MF693149–52) .\n\n【4】We also assessed other tickborne agents, including _Anaplasma phygocytophilum_ , _Anaplasma capra_ , _Ehrlichia chaffeensis_ , _Borrelia burgdorferi_ sensu lato, _Babesia microti_ , and severe fever with thrombocytopenia syndrome virus, by PCR or reverse transcription PCR. Six of 20 patients were co-infected with severe fever with thrombocytopenia syndrome virus, so we excluded them from clinical analysis.\n\n【5】We tested the acute serum samples collected < 7 days after illness onset and the convalescence serum samples collected \\> 14 days after illness onset from the 14 patients with _R. japonica_ monoinfection in parallel using an indirect immunofluorescence assay for _Rickettsia heilongjiangensis_ IgG. We considered an IgG titer \\> 1:64 a positive reaction. Using this definition, we found that 6 patients had a seroconversion, and the other 8 had a 4-fold increased IgG titer; thus, all patients had an acute infection with SFG rickettsiae.\n\n【6】All 14 patients had a history of field activity within the previous month, and 5 had a history of tick bite. Median patient age was 61.5 (range 44–77) years; 9 were men. The median time from tick bite to onset of illness was 4 (range 3–8) days and from onset of illness to physician visit 5 (range 2–7) days. The median duration of hospitalization was 6 (range 4–10) days.\n\n【7】Fever and asthenia were reported by all 14 patients. Other nonspecific symptoms included myalgia (10/14), lymphadenopathy (4/14), headache (3/14), dizziness (3/14), and chills (2/14). Gastrointestinal manifestations included anorexia (9/14), nausea (7/14), vomiting (3/14), and diarrhea (1/14). Three patients had cough and pneumonia, and we observed rash in 10 patients . The median time from onset of illness to rash was 4 (range 3–5) days, and the median duration of rash was 5.5 (range 4–8) days. Only 3 patients had an eschar. Other signs included splenomegaly (2/14) and facial edema (1/14).\n\n【8】Urinalysis on admission revealed microscopic hematuria in 2 patients and a slight or moderate proteinuria in 8 patients. The most common findings on laboratory tests were thrombocytopenia, elevated hepatic aminotransferase concentrations, elevated serum lactate dehydrogenase, and hypoalbuminemia, followed by hyponatremia, anemia, hyperbilirubinemia, hypopotassemia, leukopenia, and elevated serum creatine kinase . Mild multiple organ dysfunction developed in 3 patients.\n\n【9】Because of the appearance of typical rash, we suspected SFG rickettsioses in 10 patients and treated them with doxycycline (100 mg 2×/d) until their fevers dissipated and clinical disease improved (median 4 \\[range 3–5\\] days). The other 4 patients were treated with the antimicrobial drugs cefminox or cefoperazone for a median of 6 (range 5–7) days. By the time patients were discharged from the hospital, their leukocyte and platelet counts had returned to reference range levels, but half of the patients still had laboratory test values outside of their respective reference ranges . No patients reported clinically significant sequelae at their 2-week follow-up appointment.\n\n【10】### Conclusions\n\n【11】The tick vectors of _R. japonica_ (i.e. _H. flava_ , _H. hystrici_ s, _H. cornigera_ , _H. longicornis_ , _I. ovatus_ ) are widely distributed throughout China , providing _R. japonica_ ample opportunity to infect humans. By applying molecular screening techniques to simultaneously amplify 3 genes, we identified _R. japonica_ infection in 14 patients.\n\n【12】The clinical signs and symptoms of _R. japonica_ infection in our patient cohort differed from those reported in patients in Japan . We frequently observed fever, asthenia, and rash but not chills and headache. We also saw fewer eschars in our patient cohort, potentially because they were underreported during clinical examination; eschars are not always easy to identify. In addition, the patients we identified usually had myalgia and gastrointestinal symptoms.\n\n【13】These clinical findings expand the available knowledge of the disease spectrum of _R. japonica_ infection. Compared with endemic rickettsiosis caused by _Candidatus_ Rickettsia tarasevichiae infection (reference _16_ in Appendix ), rash is commonly seen, and hemorrhagic and neurologic signs and symptoms are rarely seen in patients with _R. japonica_ infection. These distinctive features could be used to make a differential diagnosis.\n\n【14】We frequently observed in our patient cohort thrombocytopenia, hypoalbuminemia, elevated hepatic enzyme activity, and elevated lactate dehydrogenase levels, findings resembling those of patients with common SFG rickettsioses (reference _17_ in Appendix ). In general, disease is mild or moderate, and no deaths have been recorded, although mild multiple organ dysfunction developed in several patients.\n\n【15】In conclusion, we identified _R. japonica_ as an emerging tickborne pathogen in China. Physicians in areas where _H. longicornis_ and other competent vectors for _R. japonica_ are endemic should be aware of the risk for infection in humans and prescribe doxycycline to patients in cases of ineffective therapy with other antimicrobial drugs. Surveillance needs to be extended to improve our understanding of the health burden of JSF.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7df37836-bc69-4634-b56e-d4b51db8dd53", "title": "Integrating Culture and History to Promote Health and Help Prevent Type 2 Diabetes in American Indian/Alaska Native Communities: Traditional Foods Have Become a Way to Talk About Health", "text": "【0】Integrating Culture and History to Promote Health and Help Prevent Type 2 Diabetes in American Indian/Alaska Native Communities: Traditional Foods Have Become a Way to Talk About Health\nAbstract\n\n【1】**Purpose and Objectives**\n\n【2】The purpose of the Traditional Foods Project (TFP) was to implement and evaluate a community-defined set of strategies to address type 2 diabetes by focusing on traditional foods, physical activity, and social support. The TFP sought to answer 2 questions: first, how do we increase and sustain community access to traditional foods and related activities to promote health and help prevent type 2 diabetes? Second, how do we evaluate interventions across culturally and geographically diverse communities to demonstrate success?\n\n【3】**Intervention Approach**\n\n【4】Public health interventions are most effective when communities integrate their own cultures and history into local programs. The food sovereignty movement among American Indians/Alaska Natives and indigenous populations globally offers ways to address public health issues such as chronic diseases like type 2 diabetes. Historical, economic, social, and environmental determinants of health are critical to understanding the disease.\n\n【5】**Evaluation Methods**\n\n【6】During 2008–2014, seventeen tribal TFP partners implemented locally designed interventions and collected quantitative and qualitative data in 3 domains: traditional foods, physical activity, and social support. Partners entered data into a jointly developed evaluation tool and presented additional program data at TFP meetings. Partner observations about the effect of the TFP were gathered in planned discussions.\n\n【7】**Results**\n\n【8】Quantitative results indicate collaborative community engagement and sustained interventions such as gardening, availability of healthy foods across venues, new health practices, health education, and storytelling. Qualitative results demonstrate the importance of tribally driven programs, underscoring the significance of traditional foods in relation to land, identity, food sovereignty, and food security.\n\n【9】**Implications for Public Health**\n\n【10】Traditional foods and food sovereignty are important areas for American Indian/Alaska Native communities to address the public health issues of chronic disease, specifically type 2 diabetes, locally and nationwide.\n\n【11】Introduction\n\n【12】Diabetes is highly prevalent in the United States and is associated with increased risk of health problems such as vascular diseases (eg, heart disease, stroke), chronic kidney disease, and blindness . It is the seventh leading cause of death in the United States and affects more than 30 million Americans; an additional 84 million adults have prediabetes, and thus are at risk for diabetes . Although not all diabetes is preventable, type 2 diabetes, which accounts for 90% to 95% of all diabetes cases, can sometimes be prevented by maintaining a healthy diet, a healthy weight, and a healthy level of physical activity . The prevalence of diabetes among US adults has been stable for approximately 10 years, but this is not true of children, adolescents, and young adults . A study comparing the prevalence of diabetes among children, adolescents, and young adults (aged <20 y) from 2001 and 2009 found large increases in the prevalence of both type 1 (30.0%, from 1.48 to 1.93 per 1,000 persons) and type 2 (35.0%, from 0.34 to 0.46 per 1,000 persons) diabetes . Health complications in this age group are common. In a study of children and adolescents who had a diagnosis of diabetes for at least 5 years, data collected during 2011–2015 showed that 32% with type 1 and 72% with type 2 had at least 1 health complication related to the function of kidneys, eyes, the heart, and the nerve and circulatory systems .\n\n【13】Diabetes is not equally distributed among US racial and ethnic groups. The rate of diagnosed diabetes among American Indian/Alaska Native (AI/AN) adults in 2013–2015 (15.1%) was twice the rate among non-Hispanic white adults (7.4%) . Disparities among young people are greater. The incidence of type 2 diabetes among AI/AN children, adolescents, and young adults aged 10 to 19 years in 2011–2012 (46.5 per 100,000 population) was more than 10 times the incidence among their non-Hispanic white counterparts (3.9 per 100,000 population) . This disparity may be related in part to differences in obesity rates, a known diabetes risk factor. In 2015–2016, the prevalence of obesity among non-Hispanic white children, adolescents, and young adults aged 2 to 19 years in the United States was 14.1% . Among AI/ANs of the same age in 2015, the prevalence was 29.7% .\n\n【14】The history of type 2 diabetes in the United States illuminates complex issues. The transition from local, harvested foods to foods dense in calories and fat fueled rates of type 2 diabetes and related chronic conditions . After World War II, with the shift to a wage economy , Americans began to consume readily available processed foods high in sugar and fat and low in fiber and were typically less physically active than before. Rates of diabetes in the United States rose from less than 1% in 1958 (~500,000 people) to 9.4% in 2015 (30.3 million people) . Diabetes was also rare among AI/ANs before 1940. Among AI/ANs, as among other Americans, the dramatic changes in diet and declining physical activity preceded rising rates of the disease .\n\n【15】Focusing on biologic factors alone overlooks factors that propel development of chronic diseases . Recognizing historical, economic, and environmental contributions, or social determinants of health, is critical to understanding the trajectory of type 2 diabetes .\n\n【16】Current social determinants of health associated with development of type 2 diabetes include poverty , attaining less than a high school education , physiologic stress responses associated with historical trauma , and adverse childhood experiences . Food insecurity, defined as uncertain or limited access to enough food for a healthy life, is also correlated with increased risk of developing type 2 diabetes . Rates of food insecurity among AI/AN children are approximately 2 times national rates . In 2016 nearly 30% of AI/AN households were food insecure, compared with 16% of non-AI/AN households .\n\n【17】For tribal nations, gathering, planting, or hunting food was integral to physically active and spiritual lives . Decades of federal mandates affected the land and water resources of tribal nations, which in turn profoundly disrupted indigenous food systems and reduced access to traditional foods . Native peoples in the United States were forced to move and had to adjust to different lands, climates, and the foods they could raise and gather. These foods were often supplemented by government provisions to stave off starvation and malnutrition resulting from disrupted food systems .\n\n【18】Since the 1970s, federal food distribution programs have provided commodity foods to AI/AN communities . These processed foods, high in salt and fat, and demanding very little physical activity to access, often result in what Indian people call a “commod bod” (a “commodity body,” or a body type resulting from consuming commodity foods) . Furthermore, food assistance programs alone do not substantially improve food insecurity . Some traditional foods (bison, blue corn meal, wild rice) were added recently to food assistance programs, but these foods are not regularly available . Access to healthy food across Indian Country is further thwarted by distance (food deserts), limited transportation, inadequate supermarkets, environmental contamination, and little money to purchase healthy foods .\n\n【19】Tribally driven approaches to understanding these issues in Indian Country include indigenous science, sometimes called traditional ecological knowledge, a natural science grounded in lifetimes of observation, experimentation, and adaptation . A blueprint for a way of life that has survived , traditional ecological knowledge is inextricably linked to traditional foods and food sovereignty. It informs cultivating, harvesting, and sharing foods; storytelling; games; and traditional wisdom (eg, “water is life”) . Mihesuah and Hoover recently underscored the connection of food sovereignty to cultural knowledge, environments, and health .\n\n【20】The objective of this study was to describe our evaluation of a program designed to promote access to and integrate traditional foods, physical activities, and social support in semistructured ways into culturally and geographically diverse AI/AN communities. The Traditional Foods Project (TFP) provided modest funding and support to 17 AI/AN communities who designed their own interventions to meet the needs of their communities. Stories describing the innovative approaches based on traditional foods, culture, and history to prevent type 2 diabetes among TFP communities have been published elsewhere .\n\n【21】Purpose and Objectives\n\n【22】The purpose of the TFP was to promote access to traditional foods, physical activity, and social support to address community health in AI/AN communities, particularly type 2 diabetes prevention. We sought to answer 2 questions. First, how do communities increase and sustain access to traditional healthy foods, physical activity, and social support to promote health and help prevent type 2 diabetes? Second, how do culturally and geographically diverse AI/AN communities, locally and in partnership with one another and a federal program, successfully evaluate interventions?\n\n【23】TFP objectives were to 1) support sustainable, evaluable ecological approaches to reclaim traditional foods, 2) encourage local practices to increase access to healthy traditional foods and physical activity, 3) revive and create stories of healthy traditional ways, and 4) integrate culture and history to promote community health and help prevent type 2 diabetes.\n\n【24】Intervention Approach\n\n【25】The TFP evolved from the findings of earlier projects where traditional foods emerged as a way to promote health and help prevent type 2 diabetes. The Indian Health Service Tribal Leaders Diabetes Committee had suggested looking to tribal cultures to promote health and prevent type 2 diabetes among AI/ANs . These projects demonstrated that public health interventions are most effective when communities integrate their own cultures and history into local programs .\n\n【26】Community-based participatory research is the foundation of the TFP. In community-based participatory research, culture and context are legitimate foci for interventions , and partnering with communities in program design, evaluation, and reporting criteria is fundamental . Community-based participatory research methods were shaped by tribally driven participatory research  and framed by food sovereignty — the right of people to define their own policies and strategies for sustainable production, distribution, and consumption of food .\n\n【27】Evaluation Methods\n\n【28】The TFP used both quantitative and qualitative evaluation methods. Mixed methods were critical to demonstrate which elements of each intervention worked (quantitative measures) and why and how communities became engaged across programs (qualitative measures). Honoring local knowledge and traditions, TFP partners catalyzed their communities such that farmers, health care providers, tribal leaders, subsistence gatherers, administrators, evaluators, and community members came together for the shared purpose of improving community health. Each TFP partner had a local coordinator and evaluator who developed community-supported programs and collected data in 3 domains: traditional foods, physical activity, and social support. All domain interventions were designed to improve health with the long-term goal of helping prevent type 2 diabetes.\n\n【29】### Setting and participants\n\n【30】The project began in 2008 with 11 tribes and tribal organizations and was expanded to 17 in 2009. The 17 TFP partners were culturally and geographically diverse. Each partner received $100,000 per year. In 2012, the Centers for Disease Control and Prevention’s Tribal Advisory Committee recommended a sixth year to increase capacity and sustain local efforts. Sixteen of 17 partners applied for and participated in the sixth year at the same level of funding .\n\n【31】The initial 11 TFP partners participated in all 5 years of data collection. Five of the 6 partners added later participated in 4 years of data collection. The remaining partner collected data for 3 years but did not participate in the final year.\n\n【32】### Procedures\n\n【33】The first year of the TFP focused on program and evaluation planning. Partners who launched the TFP in 2008 began gathering and reporting data in 2009, and partners who joined in 2009 began gathering and reporting data in 2010. Two 6-month data collection periods took place each year, resulting in 10 data collection periods and a sample size of 156 data points for each variable. Odd-numbered periods (T1, T3, T5, T7, T9) corresponded to data collected from October to March, including winter, when gardening was not possible in some communities. Even-numbered periods (T2, T4, T6, T8, T10) corresponded to data from April through September, including summer.\n\n【34】TFP partners reported on local activities and evaluation outcomes in 2 ways. First, partner evaluators entered local data addressing the 3 domains every 6 months into a shared data elements (SDE) tool developed jointly by the Native Diabetes Wellness Program and TFP grantee partners. The SDE was approved by the Office of Management and Budget . Having 2 collection periods during each intervention year allowed for seasonal analyses of partner activities. SDE data included quantitative information about overall numbers of activities across domains as well as numbers of participants, numbers of persons affected (through social media, local radio, and television), and brief qualitative descriptions of grantee partner activities. We gathered no individual health data for aggregate analysis because of cooperative agreement restrictions and the TFP focus on community health. Second, partners presented information on local program interventions, evaluation methods, and findings at TFP meetings either once or twice a year. Quantitative measures included number and size of gardens, weight and types of produce yields, and number of participants in organized physical activities. Qualitative data included stories used for teaching, descriptions of community responses, and examples of how culture and history were integral to TFP activities.\n\n【35】The Native Diabetes Wellness Program evaluation team aggregated and analyzed quantitative SDE data, with TFP partner program and period as the unit of analysis. We returned grantee-specific data to each partner along with aggregated SDE results after each data collection period. We also presented SDE data updates at every meeting.\n\n【36】We collected qualitative data by using the SDE and in other ways. The SDE used open-ended text fields of 50 to 250 characters to describe activities under general categories such as “gardening,” “health education,” and “measures of participant change.” Additionally, we encouraged TFP partners to collect other local data, such as stories. We also gathered partners’ written and oral comments during semi-annual or annual meetings and monthly conference calls.\n\n【37】### Intervention framework\n\n【38】The Native Diabetes Wellness Program did not prescribe methods of community intervention for the partners. Each TFP partner used various strategies aimed at behavior changes related to TFP goals. Overall intervention components were unique to each group. Components also differed over time, so the interventions implemented by a partner during, for example, the fourth period were likely different than its activities during the seventh period. Most partners engaged in 1 or more activities in each domain during each period.\n\n【39】Some program components affected more than 1 domain. For example, activities included in gardening or subsistence categories often involved physical activity and/or social support as well as traditional foods. For each activity, partners recorded which domain(s) they considered relevant to each project component.\n\n【40】### Measures\n\n【41】Each partner had flexibility to create and implement interventions consistent with local ways, based on local definitions of traditional healthy foods, physical activity, and social support. Traditional foods activities could include gardening, subsistence gathering, hunting, and fishing. Physical activity interventions focused on organized physical activities and places to conduct physical activity programs. We defined social support as any time local participants gathered to support each other, regardless of focus. General categories such as health education, health practices and policies, and storytelling were interventions across all domains.\n\n【42】### Quantitative data analysis\n\n【43】We stratified descriptive analyses by period and tabulated frequencies for categorical variables. Denominators used in percentages varied according to period, because the number of partners varied from 11 to 17. We tabulated numeric variables as counts, sums, measures of central tendency, and maximum values. The denominator used to calculated mean and median values for all 10 periods was 156. For many activities, TFP partners recorded the number of participants, the number of new participants, and the number of people affected in each period. We calculated total number of participants for individual activities across partner groups by period. We did not calculate sums of participants across periods, because we had no way to determine the amount of participant overlap between activities or across time.\n\n【44】We examined changes in the prevalence of all activities over time and used simple regression or the χ 2  test for trend. The 156 data points collected were not independent, because groups were represented up to 10 times. We considered a repeated measures analysis but did not use it because the assumptions were not satisfied. For this reason, our analytic statistics examined trends but not relationships between outcomes. We used Stata version 13 (StataCorp LLC) for all analyses.\n\n【45】### Qualitative data analysis\n\n【46】The Native Diabetes Wellness Program reviewed SDE qualitative data after each data collection period. We also reviewed local qualitative data such as digital stories and other cultural applications of the 3 domains. We analyzed TFP partner evaluation forms and meeting and conference call notes for major themes and illustrative quotes. TFP partners reviewed the themes and quotes to ensure accuracy and intended voice.\n\n【47】Results\n\n【48】### Quantitative findings\n\n【49】From 81.8% (9 of 11) to 94.1% (16 of 17) of TFP partners reported gardening activities during summer periods, and 58.8% (10 of 17) to 82.4% (14 of 17) during winter periods . Garden types included school, community, family, and program gardens. Community gardens were reported by 37.5% (6 of 16) to 64.7% (11 of 17) of TFP partners for all periods (except T1, when the question about community gardens was not asked). The total number of all gardens increased over time, and ranged from a low of 13 gardens in T1 to a maximum of 510 gardens in T6 (n = 10 periods; controlling for season, _r_ 2  \\= 0.85; coefficient = 25.9; _P_ \\= .001). In T10, TFP partners reported 415 gardens, which covered an area of 28.4 acres, up from 206 gardens and 11.1 acres in T2 (the question about garden acreage was not asked in T1). Numbers of participants were highest for school gardens. For example, in T10, the 6 communities that had school gardens involved 3,017 people.\n\n【50】Reporting on traditional healthy food outlets such as health fairs increased from 18.2% (2 of 11) in T1 to 68.8% (11 of 16) in T10. Access to healthy food at other venues also improved over time. By T10, nearly two-thirds of partners (62.5%; 10 of 16) reported that healthy food selections were available at 1 or more of the following venues: worksites, agencies, supermarkets, vending machines, and restaurants. More partners reported healthy food choices at worksites (37.5%; 6 of 16 in T10) and supermarkets (31.2%; 5 of 16) than at vending machines (6.3%; 1 of 16) and agencies (6.3%; 1 of 16) . Increases in the proportion of partners reporting access to healthy foods over time were significant for healthy foods available at restaurants (χ 2  test for trend = 6.9; _P_ \\= .008) and supermarkets (χ 2  test for trend = 6.0; _P_ \\= .01).  \n\n【51】**  \nPercentage of partners reporting healthy food selections at worksites and other venues over time, Traditional Foods Project, October 2009–September 2014. Percentages are based on the following denominators: 11 partners participated during T1–T2; 17 partners during T3–T8; and 16 partners during T9–T10. Abbreviations: S, summer; W, winter. \n\n【52】Storytelling was an important teaching activity for most TFP partners in every period; for example, 14 of 16 (87.5%) in T10 reported 1 or more storytelling activities. Most incorporated 1 or more types of storytelling (eg, narrative, digital, music) into program activities . The highest proportions of storytelling activities were in the traditional foods domain, ranging from 52.9% (9 of 17) in T7 to 82.4% (14 of 17) in T3. Narrative storytelling activities were the most prevalent (mean, 58.3%, or 91 of 156 samples, over all periods), followed by digital (mean, 37.2%, or 58 of 156).\n\n【53】Most TFP partners reported health education activities for each period (range, 76.5% \\[13 of 17\\] in T7 to 100% \\[11 of 11\\] in T2). Individual TFP partners reported implementing up to 180 health education activities in a 6-month period (T10) and involving a maximum of 10,900 participants (T5).\n\n【54】TFP partners reported implementing new health practices (including behaviors, resolutions, policies, and other practices not done before) during each 6-month period at an overall rate of 43.6% (68 of 156 total data points), with a maximum of 58.8% (10 of 17 partners) in T5. The total number of new health practices over all groups for any 1 period ranged from 12 (T1) to 78 (T5). As an example, 1 partner reported that their after-school/summer camp implemented a policy that included not having sugar-sweetened beverages and candy available for purchase. In another, Head Start organizations added physical activities, gardening, and a health education curriculum to their programs.\n\n【55】Most partners reported including organized physical activities in their programs (overall average for all periods, 60.9%, or 95 of 156). As many as 7,500 participants were involved in organized physical activities for an individual TFP partner during 1 period (T3). Examples of organized activities included traditional games such as stickball, fun runs, restoration work, canoeing, and dancing.\n\n【56】Partners measured participant changes such as weight loss, improved levels of physical activity, and healthy food choices in 69 of 156 data points recorded during 10 periods. In most periods, almost half (median, 47.1; 8 of 17) of partners were measuring participant change in 1 or more domains . Numbers of participants who made changes increased from T1 to T10 for each of the 3 domains, with a maximum at T6 in physical activity (n = 1,388 participants) and social support (n = 1,950 participants) and a maximum at T8 in traditional foods (n = 2,152 participants).\n\n【57】Almost all TFP partners reported collaboration with other agencies in all 10 periods. The proportion of partners reporting at least 1 type of collaboration ranged from 87.5% (14 of 16 in T10) to 100% (11 of 11 in T1; 17 of 17 in T5 and T8; and 16 of 16 in T9). Collaboration was reported most often in the traditional foods domain (89.7% of partners; 140 of 156 overall for the 10 periods), and by most grantees in the physical activity (59.6%; 93 of 156) and social support (56.4%; 88 of 156) domains. Resources shared included staff (71.8%; 112 of 156), space (60.3%; 94 of 156), educational materials (59.6%; 93 of 156), traditional foods (55.1%; 86 of 156), marketing materials (44.9%; 70 of 156), and financial support, such as vouchers (40.4%; 63 of 156).\n\n【58】Media outreach events and materials were described in 103 of 156 (66.0%) reports. In T10, 16 partners conducted 308 media outreach events, developed and distributed 9,264 materials, and affected 31,400 people with media materials. In T1, when programs were just getting started, media was even more commonly reported. The 11 partners reported 1,614 media outreach events at which 77,523 media materials were distributed and 278,235 people affected.\n\n【59】In T10, 56.2% (9 of 16) of TFP programs reported implementing environmental changes in 1 or more domain areas that were designed to be sustainable. Sustainability was also evidenced in activities reported in every period, eg, planting and gardening, particularly community gardening (ranging from 37.5% \\[6 of 16\\] to 64.7% \\[11 of 17\\] partners). Other examples of sustainable environmental changes included using heirloom seeds, composting, developing health education activities and materials, implementing media outreach activities, implementing health policies and health practices, and collaboration with other agencies  .\n\n【60】### Qualitative findings\n\n【61】Qualitative data portrayed the role of traditional foods in ways quantitative data could not. The data describe partner perspectives about traditional foods, how well the TFP worked, and why. Results indicate that grantee partners embraced the TFP’s community-based, tribally driven approach. Themes and quotes underscore quantitative findings, such as participation, collaboration, and number of gardens .\n\n【62】The following examples of tribal partner experiences further illustrate the 7 main themes. The examples usually include more than 1 theme, demonstrating not only their interconnectedness but also how difficult it was for us to separate them.\n\n【63】**Traditional knowledge and grassroots.** Local elders remarked that corn did not grow very high in their community’s desert soil. The TFP coordinator took a course to become a Master Composter, balancing traditional ecological knowledge and western science. He created a compost pile to be used in the community garden to increase produce production. In addition to other compost materials, tribal leaders provided an endless supply of discarded paper and coffee grounds . Community members, particularly the elders, were impressed with how tall the corn grew and marveled at the large yields of harvested produce from the garden.  \n\n【64】**  \nThe compost pile was created to increase produce yield in the community garden, Traditional Foods Project, October 2009–September 2014. Compost materials included paper and coffee grounds provided by tribal leaders, Ramah Navajo, 2011. Photo courtesy of Randy Chatto. \n\n【65】**Connections to health.** TFP partners inspired the title of this article. “Traditional foods have become a way to talk about health” was a thread in every discussion. Partners could not underscore enough that chronic disease is deeply connected to social determinants of health, such as historical trauma, adverse childhood experiences, and loss of traditional foodways. The way to reclaim health, they said, is to reconnect with the land, water, traditional foodways, and all that they mean.\n\n【66】**The power of stories and storytelling.** Narrative stories — oral tradition — were most prevalent compared with other types of stories reported . However, TFP partners enthusiastically produced digital stories after learning from another partner how to create them. In turn, they shared the skill with their own people. One story was by a young rapper who had struggled with identity and substance abuse. He “found himself through connection with the earth” in the community garden. He created his digital story to welcome all partners, skillfully rapping their names, at a TFP meeting.\n\n【67】**Community engagement.** Meetings hosted by TFP partners provided settings for sharing traditional foods, cultural ways, and physical activity. One of the most anticipated activities was the traditional game of stickball. The community was invited to participate or observe (and cheer). Stickball literally created a level playing field, where TFP partners, Native Diabetes Wellness Program team members, and community members, women against men, enjoyed a physically strenuous, humor-filled game.\n\n【68】**Knowledge sharing and gratitude** . Dynamic exchange of knowledge demonstrated partners’ engagement with each other. They shared skills (how to create digital stories), traditional foods (meeting hosts always prepared a feast), and gifts (heirloom seeds, wild rice). Partners were grateful for being able to openly express the meaning of traditional foods and spend time together.\n\n【69】**Flexibility to do what works.** At the request of grantee partners, we held a discussion on health policy in the second year of the TFP. Partners stated that measuring health policy only was unacceptable: “written policies tell people what to do.” Health practices, however, “are chosen by the people because they are good ideas and reflect traditional knowledge.” Subsequently, we measured both health policies and health practices.\n\n【70】**Program sustainability.** TFP partners regularly addressed sustainability, particularly toward the end of the TFP. Most partners (11 of 17) sustained some or all activities after the TFP ended in September 2014. Partners secured funding from tribal councils, university partnerships, state and county health departments, federal agencies, or nonprofit organizations .\n\n【71】Implications for Public Health\n\n【72】The TFP’s challenge was to answer 2 questions: How do communities increase and sustain access to traditional healthy foods, physical activity, and social support to promote health and help prevent type 2 diabetes? And, how do we, in partnership with one another, successfully evaluate community-based interventions?\n\n【73】Increasing and sustaining access to traditional foods depends on strong local support, collaboration, and traditional knowledge . Grantee partners believed that traditional foods programs can be sustained if the following conditions are met:\n\n【74】•First, human and financial resources are necessary. A local natural leader, knowledgeable about traditional foods and supported financially, is vital.\n\n【75】•Second, tribal leadership support is needed. Where tribal leadership was not supportive, TFP programs were less productive. In contrast, strong backing by tribal leadership contributed to project endurance.\n\n【76】•Third, sustainability is likely when programs are relevant and meaningful. Local decisions about program content, including what constitutes traditional foods, are critical.\n\n【77】•Fourth, collaborating with programs that have related goals strengthens community infrastructure. Partners noted that, over time, other programs sustained activities originated by the TFP.\n\n【78】•Fifth, communities with few resources need time to grow infrastructure. Among TFP partners, small communities demonstrated change quickly but, without strengthened infrastructure, changes were temporary.\n\n【79】Tribally based health promotion efforts to address access to traditional foods in Indian Country are described in the 2015 report, _Feeding Ourselves_ . Our conclusions are consistent with those described in the report in the section “Case studies: lessons learned and challenges faced by grassroots, nonprofit and tribal food access and health innovators.” As an example, the Communities Creating Healthy Environments program addressed childhood obesity by changing communities rather than focusing on individual behaviors, incorporating aspects such as food inequity, safe places for play, and the social environment. The program noted not only the need for local partners but also the need for ongoing support to “implement victories, consolidate gains, and plan next steps” .\n\n【80】For our project to be successful, forging trust among TFP partners and the Native Diabetes Wellness Program was paramount . Further, equal funding, regardless of community size, gave every program equal voice. In the end, relationships were everything .\n\n【81】TFP data did not include aggregated health measures for individual participants (eg, weight change over time) because of funding restrictions and the focus on environment and community. Future TFPs would benefit from tracking changes in individual health outcomes across communities. Collecting local health data may be challenging, however, because of the sensitivity of personal health information. Tribal nations are particularly cautious about sharing personal health data because of their experiences with data misuse . This history underscores the critical importance of a tribally driven participatory approach , where tribes steer the agenda in partnership with the funding entity to develop the program, choose local and aggregate evaluation measures, and select outcomes.\n\n【82】Population sizes and geography varied widely among participating communities. TFP partners used intervention combinations designed for local conditions that could not be directly compared across sites. Environmental factors also made it difficult to compare certain interventions, such as gardening, because some communities had longer growing seasons than others.\n\n【83】We did not conduct bivariate analyses of the relationships between interventions and outcomes (eg, gardening activities and health policy changes). The project was not designed to imply such causal relationships.\n\n【84】It is methodologically challenging to distinguish effects of a particular program when multiple agencies work together. However, working collaboratively makes any single program, and subsequent community infrastructure, stronger.\n\n【85】The TFP addressed physical activity, social support, and healthy diet, factors associated with individual and community health. Partners developed local programs, framed in local cultural, historical, and environmental contexts, which included social determinants of health. Activities incorporated traditional ecological knowledge and western science, illustrating the integral relationship of traditional foods with community history, culture, and health. The TFP demonstrated that tribally driven programs, guided by traditional knowledge, can facilitate access to traditional foods as part of community health interventions to address chronic disease.\n\n【86】“Traditional ways of knowing” have, for generations, linked physical and spiritual health to traditional foods . The concept is far from new. What is new is the burgeoning food sovereignty movement that reclaims traditional foods in relation to tribal sovereignty, food security and, in this instance, public health. Traditional foods have become, once again, a way to talk about health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a856225b-66f5-4df3-832e-9b7edea8788a", "title": "Rapid Genotyping of Swine Influenza Viruses", "text": "【0】Rapid Genotyping of Swine Influenza Viruses\nCo-infection of influenza A viruses enables viral gene reassortments, thereby generating progeny viruses with novel genotypes. Such reassortants may pose a serious public health threat, as exemplified by the emergence of pandemic influenza (H1N1) in 2009 . Transmission of pandemic (H1N1) 2009 virus from humans to pigs has been reported . We recently identified a reassortment between pandemic (H1N1) 2009 virus and swine influenza viruses in pigs . These results emphasize the potential role of pigs as a mixing vessel for influenza viruses and the need for screening tests that can identify major reassortment events in pigs.\n\n【1】We previously developed 8 monoplex SYBR green–based quantitative reverse transcription–PCRs to detect all 8 gene segments derived from the pandemic (H1N1) 2009 virus or virus segments that are closely related to this lineage (i.e. neuraminidase \\[NA\\] and matrix protein from the Eurasian avian-like swine linage and polymerase basic protein \\[PB\\] 2, PB1, polymerase acidic protein \\[PA\\], hemagglutinin \\[HA\\], nucleocapsid protein \\[NP\\], and nonstructural protein \\[NS\\]) from triple reassortant swine linage . Using these PCRs, we identified swine viruses of atypical genotypes. However, with the exception of the HA-specific assay, the melting-curve signals of pandemic (H1N1) 2009 virus may be indistinguishable from the positive signals generated from its sister clade as indicated above. To differentiate between these closely related groups of viruses, we further optimized these assays by adding sequence-specific hydrolysis probes in the SYBR green assays.\n\n【2】### The Study\n\n【3】For this study, all SYBR green assays were modified from the previously described assays , with the exception of the reverse primers for the newly designed PB1 and NS segments . The subtype H1N1 swine influenza viruses isolated in Hong Kong during the past few years were mainly derived from the Eurasian avian-like swine lineage . To generate more precise genotyping data for our ongoing surveillance, the NA segment–specific assay was specifically designed to react with the pandemic (H1N1) 2009 virus and a portion of Eurasian avian-like swine viruses that are circulating in southeastern China . To avoid overlapping the emission spectrum of SYBR green, we labeled all pandemic (H1N1) 2009 virus–specific hydrolysis probes (Integrated DNA Technologies, Inc. Coralville, IA, USA) with cyanine 5 (Cy5) and Black Hole Quencher-2 dyes at their 5′ and 3′ ends, respectively . To enable use of short oligonucleotide sequences without compromising the annealing temperature of these probes, we modified the probes with locked nucleic acids . RNA extraction and complimentary DNA synthesis were identical to the protocols described . One microliter of 10-fold diluted complimentary DNA sample was amplified in a 20-µL reaction containing 10 µL of Fast SYBR Green Master Mix (Applied Biosystems, Foster City, CA, USA) and the corresponding primer probe set (0.5 µmol/L each). All reactions were optimized and performed simultaneously in a 7500 Sequence Detection System (Applied Biosystems) with the following conditions: 20 s at 95°C, followed by 30 cycles of 95°C for 3 s and 62°C for 30 s. SYBR green and Cy5 signals from the same reaction were captured simultaneously at the end of each amplification cycle. The expected PCR results of virus segment derived from different swine viral lineages are shown in Table 2 .\n\n【4】The dissociation kinetics of PCR amplicons were studied by a melting curve analysis at the end of the PCR (60°C–95°C; temperature increment 0.1°C/s). We also tested various probe and SYBR green concentrations under different PCR conditions. The condition described above gave the most robust and consistent DNA amplification (data not shown). We tested 31 human pandemic (H1N1) 2009 and 63 human seasonal influenza viruses (33 subtype H1N1, 30 subtype H3N2) as controls. As expected, all human pandemic influenza viruses were double positive (i.e. positive with SYBR green and Cy5) and all seasonal influenza samples were double negative in all 8 assays.\n\n【5】To evaluate the sensitivity of the assays, we tested serial diluted plasmid DNA of the corresponding segments of influenza A/California/4/2009 virus as a standard. The fluorescent signals generated from the SYBR green reporter dye in all assays were highly similar to those previously reported , and the modified assays had a linear dynamic detection range from 10 2  to 10 8  copies/reaction . As expected, the threshold cycle values deduced from the Cy5 reporter signal were generally higher than those from the SYBR green reporter  . This finding can be partly explained by the nature of these 2 kinds of real-time PCR chemistries: a single Cy5 fluorophore of the hydrolysis probe was released from quenching for each amplicon synthesized while multiple SYBR green dyes bound to a single amplicon . After 35 PCR amplification cycles, the linear dynamic detection range of Cy5 signals generated from these reactions was 10 2  to 10 8  copies/reaction (data not shown). However, to avoid nonspecific SYBR green signals, we purposely limited the number of amplification cycles to 30.\n\n【6】Using these assays, we tested 41 swine virus isolates collected during January 2009–January 2010. In all 8 reactions, 10 pandemic (H1N1) 2009 virus samples transmitted from humans to pigs  were double positive . In these assays, gene segments of another 31 swine isolates were either SYBR green positive/Cy5 negative  or double negative , indicating that these virus segments were derived from the sister clade of pandemic (H1N1) 2009 virus or other swine lineages (except NA), respectively. For example, the reassortant of pandemic (H1N1) 2009 virus (A/swine/Hong Kong/201/2010 \\[H1N1\\]) was double positive for NA, double negative for HA and matrix protein, and SYBR green positive/Cy5 negative for PB2, PB1, PA, NP, and NS . All genotyping results of the studied viruses were consistent with results of previous phylogenetic analyses , indicating that our modified probes and SYBR green assays can provide more accurate genotyping results. With these genotyping data, viruses with atypical positive signal patterns might suggest a novel viral reassortment event and can be highlighted for investigation with sequencing-based methods.\n\n【7】To demonstrate the potential use of these assays in studying swine viruses circulating in other geographic locations, we tested 7 recent swine isolates (1 pandemic influenza subtype H1N1, 4 subtype H1N2, and 2 subtype H3N2) collected in the United States. Genotyping results agreed 100% with data deduced from sequence analyses . We also analyzed all 436 contemporary  US swine virus segments available from the National Center for Biotechnology Information influenza virus sequence database. On the basis of the in silico analysis of sequences targeted by our primers and probes, 95% of the sequences (n = 413) are predicted to yield the expected results .\n\n【8】### Conclusions\n\n【9】The emergence of pandemic (H1N1) 2009 has highlighted the need for global systematic influenza surveillance in swine. Our results demonstrated that the addition of locked nucleic acid hydrolysis probes specific for pandemic (H1N1) 2009 virus into previously established SYBR green assays can help differentiate segments of pandemic (H1N1) 2009, Eurasian avian-like, and triple reassortant virus lineages. These assays might provide a rapid and simple genotyping method for identifying viruses that need to be fully genetically sequenced and characterized. They may also help provide better understanding of the viral reassortment events and viral dynamics in pigs. Although at present, genes derived from human seasonal viruses cannot be characterized with our modified assays, the performance of our assays warrants similar investigations for genotyping human influenza viruses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35da4e05-5173-4eda-a744-d3102149d252", "title": "Global Incidence of Carbapenemase-Producing Escherichia coli ST131", "text": "【0】Global Incidence of Carbapenemase-Producing Escherichia coli ST131\n_Escherichia coli_ sequence type 131 (ST131) was identified as pathogenic to humans in 2008; retrospective research suggests that its isolates have been present since at least 2003. The group has spread extensively and has been linked to the rapid global increase in the prevalence of antimicrobial resistance among _E. coli_ strains . The intercontinental dissemination of this sequence type has contributed immensely to the worldwide emergence of fluoroquinolone-resistant and CTX-M–producing _E. coli_ . Recent surveillance studies have shown that its overall prevalence ranges from 12.5% to 30% of all _E. coli_ clinical isolates, from 70% to 80% of fluoroquinolone-resistant isolates, and from 50% to 60% of extended spectrum beta-lactamase-producing isolates .\n\n【1】The development of resistance to carbapenems among _E. coli_ is of particular concern because these agents are often the last line of effective therapy available for the treatment of persons with serious infections . New Delhi metallo-β-lactamase (NDM) and carbapenem-hydrolyzing oxacillinase-48 (OXA-48) are the most common carbapenemases among _E. coli_ worldwide .\n\n【2】### The Study\n\n【3】This study describes the characteristics of ST131 isolates among carbapenemase-producing _E. coli_ strains collected globally by 2 research groups during 2008–2013. The Merck Study for Monitoring Antimicrobial Resistance Trends **(** SMART)  started in 2002 and AstraZeneca's global surveillance study of antimicrobial resistance (unpublished data) began in 2012, to monitor global antimicrobial resistance trends among gram-negative bacteria . Antimicrobial susceptibilities of different antimicrobial agents  were determined by using frozen broth microdilution panels according to 2013 Clinical and Laboratory Standards Institute and European Committee on Antimicrobial Susceptibility Testing guidelines . Established PCR and sequencing methods were used to identify β-lactamase genes  and define O25b:H4, O16:H5 ST131, _fimH_ 30 lineage, _H_ 30-Rx sublineage , and virotypes .\n\n【4】Overall, 47,843 _E. coli_ isolates were collected and tested for susceptibility; 407 were found to be nonsusceptible to ertapenem or imipenem and were molecularly characterized for β-lactamase genes. A total of 116 of the 407 isolates were positive for NDM, KPC, OXA-48-like, VIM, and IMP types of carbapenemases. Various gene types were identified: 44 (38%) were positive for _bla_ NDM  , 38 (33%) for _bla_ KPC  , 30 (26%) for _bla_ OXA-48-like  , 2 (2%) for _bla_ VIM-1  and 2 (2%) were positive for _bla_ IMP  .\n\n【5】The countries from which the _E. coli_ isolates were obtained are shown in Table 2 . The isolates were cultured from intraabdominal specimens (37%), peritoneal fluid (16%), biliary fluid (10%), urine (30%), and from miscellaneous sources such as sputum and tissue (9%).\n\n【6】PCR testing for O25b:H4, O16:H5, and MLST showed that 41/116 (35%) belonged to the sequence type ST131. Antimicrobial susceptibilities, types of β-lactamases, the presence of the _fimH_ 30 lineage, and virotypes are shown in Table 1 . ST131strains were more likely than non-ST131 strains to be nonsusceptible to ciprofloxacin and to be positive for _bla_ KPC  , the _H_ 30 lineage, and virotype C; non-ST131 isolates were more likely to be positive for _bla_ NDM  .\n\n【7】The majority, i.e. 24 (58%), of ST131strains were positive for _bla_ KPC  , 13 (32%) for _bla_ OXA-48-like  , 3 (7%) for _bla_ NDM-1  , and 1 (2%) for _bla_ IMP-14  . ST131 was present in 16 countries spanning 5 continents . The distribution of ST131 during 2008–2013 is shown in Table 3 .\n\n【8】Various _fimH_ alleles were identified among ST131 isolates: 24 _H_ 30 (58%), 3 H41 (7%), 3 _H_ 54 (7%), 2 _H_ 22 (5%), 2 _H_ 27 (5%), and 2 _H_ 191 (5%); and 1 each (2%) belonging to _H_ 24, _H_ 32, _H_ 65, and the new fim _H_ alleles _H_ 434 and _H_ 435\\. Of the 24 _H_ 30 ST131 strains, 19 (79%) belonged to the _H_ 30-R sublineage and 5(21%) to the _H_ 30-Rx sublineage.\n\n【9】### Conclusions\n\n【10】NDM variants were the most common carbapenemase identified and were especially prevalent in _E. coli_ strains from India and Vietnam . KPCs, which were the second most common carbapenemase identified, were distributed globally, i.e. in South America, Central America, North America, Europe, the Middle East, and Asia . This was unexpected because KPCs have been relatively rarely reported among _E. coli_ .\n\n【11】Because of the unprecedented global success of ST131, the presence of carbapenemases had been carefully monitored by molecular epidemiologists but has been limited to case reports from several countries . The largest collections of ST131 with carbapenemases were reported from Hangzhou, Zhejiang Province, China  and Pittsburgh, Pennsylvania, USA . Of note, 24/38 (63%) of _E. coli_ strains with _bla_ KPC  belonged to ST131, as opposed to 3/44 (7%) for NDMs and 13/30 (43%) for OXA-48-like strains. Our results suggest that ST131 is most likely responsible for the global distribution of _E. coli_ with _bla_ KPC.\n\n【12】The expansion of the _H_ 30 lineage and its _H_ 30-R and _H_ 30-Rx sublineages have contributed substantially to the spread of ST131 _E. coli_ . In our study, _H_ 30-R, which belongs to virotype C, was the most common lineage among ST131 strains (i.e. 58%); it was associated with _bla_ KPC  and was especially prominent during 2012–2013. The increase of the ST131 _H_ 30 lineage with _bla_ KPC  during 2012–13 is cause for concern.\n\n【13】_E. coli_ ST131 has received comparatively less attention than other antimicrobial-resistant pathogens. Retrospective molecular surveillance studies have shown that ST131 with _bla_ CTX-M-15  was rare during the early 2000s, but that an explosive global increase followed during the mid-to-late 2000s . The results of this study show a similar scenario with _E. coli_ ST131 and _bla_ KPC  ; a low prevalence combined with a global distribution. This study is of special concern because we documented resistance to “last resort” antimicrobial drugs (i.e. carbapenems) in most regions of the world, in a common community and hospital pathogen (i.e. _E. coli_ ) among a very successful sequence type (i.e. ST131). We urgently need well-designed epidemiologic and molecular studies to clarify the dynamics of transmission, risk factors, and reservoirs for ST131.\n\n【14】The medical community can ill afford to ignore _E. coli_ ST131strains with carbapenemases. This sequence type poses a major threat to public health because of its worldwide distribution and association with the dominant _H_ 30 lineage. This sequence type among _E. coli_ has the potential to cause widespread resistance to carbapenems.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d3b0f45b-f38e-48ab-bf8f-2d7e744b9274", "title": "Orogenital Transmission of Neisseria meningitidis Causing Acute Urethritis in Men Who Have Sex with Men", "text": "【0】Orogenital Transmission of Neisseria meningitidis Causing Acute Urethritis in Men Who Have Sex with Men\n_Neisseria meningitidis_ is a gram-negative diplococcus that can cause severe invasive infections. It usually colonizes the oropharynx and spreads through close or prolonged contact with respiratory or throat secretions. Cases of _N. meningitidis_ –associated urethritis have been reported since the 1930s, and orogenital sex has been demonstrated as a potential mode of transmission by genotyping methods .\n\n【1】A 22-year-old man visited a sexual health clinic for symptomatic urethritis with purulent urethral discharge and pain while urinating. Clinical examination showed no other signs or symptoms, including fever. His last sexual intercourse (including orogenital contacts) occurred 3 weeks before with his stable, unique, asymptomatic male partner. Both men reported that they had not had any other sexual partner before and since their first intercourse together <1 year before. Meningococcal vaccination statuses were unknown.\n\n【2】Results of nucleic acid amplification testing for _N. gonorrhoeae_ and _Chlamydia trachomatis_ in urine, pharyngeal, and anal samples, and serologic testing for HIV, hepatitis B and C viruses, and syphilis were negative for the patient and his partner. Urethral culture yielded growth with oxidase and Gram stain results consistent with gonococcus, but matrix-assisted laser desorption/ionization time-of-flight mass spectrometry identified _N. meningitidis_ (isolate TUR1). A pharyngeal swab specimen was collected from the partner of the patient to identify a potential source of contamination. This specimen also grew _N. meningitidis_ (isolate CHA1). Isolates were nongroupable by slide agglutination serogrouping.\n\n【3】We purified genomic DNAs by using a NucleoSpin Tissue Kit , prepared DNA libraries by using the Nextera Library Construction Kit , and sequenced these DNAs by using a MiSeq sequencer (Illumina), which yielded 62× coverage for isolate TUR1 and 66× coverage for isolate CHA1. We assembled the genomes of CHA1 and TUR1 isolates by using SPAdes version 3.10.1 , which resulted in assemblies consisting of 395 contigs for TUR1 and 636 contigs for CHA1 and yielded draft genomes of ≈2.19 Mb. We performed genome annotation by using the MicroScope Platform , which yielded 2,597 coding sequences, including protein-encoding genes and RNAs.\n\n【4】We analyzed high-throughput sequencing data by using the PubMLST website  for typing the CHA1 and TUR1 isolates. These 2 isolates belonged to the clonal complex sequence type (ST) 11 and had fine-type PorA 1.5,2; FetA F1-1. DNA sequence analysis of the _csw_ gene of these isolates showed an open reading frame of 3,114 bp that had 99.9% nucleotide identity with that of the _N. meningitidis_ W:2a:P1.7–2,4 ST11 strain , which was consistent with genogroup W .\n\n【5】We used CSI Phylogeny version 1.4  for phylogenomic analysis on the basis of single-nucleotide polymorphisms (SNPs) and called among core genomes of CHA1 and TUR1 isolates and 3 other ST11 _N. meningitidis_ strains (NM3688 from Brazil \\[Bioproject PRJNA264543\\], COL-201505-31 from the United States \\[BioProject PRJNA324131\\]), and LNP27256 from France \\[Bioproject PRJNA215157\\]). Analysis showed that the CHA1 and TUR1 isolates differed by only 3 SNPs, thus demonstrating their clonal relationship. Conversely, these isolates differed by >890 SNPs from the other strains.\n\n【6】Nucleotide sequence accession numbers for whole-genome shotgun projects have been deposited at DNA Data Bank of Japan/European Molecular Biology Laboratory/GenBank under accession nos. MULP000000001 (Bioproject PRJNA369721) and MULO000000001 (Bioproject PRJNA369166).\n\n【7】The patient recovered quickly after receiving single doses of intramuscular ceftriaxone (500 mg) and azithromycin (1 g). His partner received a single dose of intramuscular ceftriaxone (500 mg) as a decontamination strategy.\n\n【8】Albeit rare, _N. meningitidis_ –associated urethritis is emerging worldwide among men . These infections can be highly symptomatic and mimic gonococcal urethritis. Orogenital transmission is often suspected but rarely proven . In our study, use of whole-genome sequencing strongly supported the hypothesis that the oropharyngeal carriage of _N. meningitidis_ by the man who performed oral sex was the source of the urethritis. Moreover, whole-genome sequencing showed that the strains belonged to the ST11 clonal complex. Several studies reported nongonococcal urethritis caused by _N. meningitidis_ ST11 in Japan and the United States . These worldwide descriptions suggest that _N. meningitidis_ S11 might represent an emerging urethrotropic clade.\n\n【9】This case is notable because the patient had highly symptomatic urethritis without risky sexual behavior. In fact, he reported that he and his only sexual partner had not had sex with others before they met.\n\n【10】Since 2010, several clusters of serogroup C invasive meningococcal disease have been reported among men who have sex with men, and sexual transmission is suspected to be involved . These infections led to an extension of the meningococcal vaccine recommendations to men who have sex with men who are engaged in risky behavior in some outbreaks areas. Our case highlights that sexual transmission of _N. meningitidis_ should be considered for all men.\n\n【11】Finally, although systematically collected information is limited, meningococcal urogenital infections are potentially increasing and raising public health concerns. These infections need to be monitored, and bacteriological culture of purulent exudate should always be considered when available. Also, because fidelity might be contested between partners when a sexually transmitted disease is being diagnosed, identification of meningococcal urethritis and its transmission might have a strong psychological effect on the couple.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "eed4b5fb-33ce-4d57-981b-993b571a15e3", "title": "Correction Vol. 8, No. 7", "text": "【0】Correction Vol. 8, No. 7\nIn Time-Space Clustering of Human Brucellosis, California, 1973-1992, by G. Fosgate et al. an error appears in the results section.\n\n【1】The _Brucella_ species was identified in 229 (55%) of the 416 cases analyzed. _B. abortus_ was isolated from 39 cases, _B. melitensis_ from 181, and _B. suis_ from 9.\n\n【2】We regret any confusion this error may have caused.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cf4862d5-100c-4ac5-8d51-6a4313f0c696", "title": "Proportion of Deaths and Clinical Features in Bundibugyo Ebola Virus Infection, Uganda", "text": "【0】Proportion of Deaths and Clinical Features in Bundibugyo Ebola Virus Infection, Uganda\nEbola hemorrhagic fever (EHF) is a severe disease caused by several species of _Ebolavirus_ (EBOV), in the family _Filoviridae._ Before 2007, four species of EBOV had been identified; 2 of these, _Zaire ebolavirus_ and _Sudan ebolavirus_ , have caused large human outbreaks in Africa, with proportion of deaths ≈80%–90% and 50%, respectively . Large outbreaks are associated with person-to-person transmission after the virus is introduced into humans from a zoonotic reservoir. Data suggest that this reservoir may be fruit bats . During outbreaks of EHF, the virus is commonly transmitted through direct contact with infected persons or their bodily fluids . The onset of EHF is associated with nonspecific signs and symptoms, including fever, myalgias, headache, abdominal pain, nausea, vomiting, and diarrhea; at later stages of disease, overt hemorrhage has been reported in ≈45% of cases .\n\n【1】Bundibugyo District is located in western Uganda, which borders the Democratic Republic of Congo. After reports of a mysterious illness in Bundibugyo District, the presence of a novel, fifth EBOV virus species, _Bundibugyo ebolavirus_ (BEBOV), was identified in diagnostic samples submitted to the Centers for Disease Control and Prevention (CDC), Atlanta, Georgia, USA, in November 2007 . In response to detection of EBOV, an international outbreak response was initiated. In this report, we summarize findings of laboratory-confirmed cases of BEBOV infection.\n\n【2】### The Study\n\n【3】Anecdotal reports suggested that human illness consistent with a viral hemorrhagic fever arose in Bundibugyo District as early as August 2007. After EHF was confirmed , isolation wards were established at 2 medical facilities in the district. Diagnostic samples from hospitalized patients with acute illness and community residents who had febrile illnesses and multiple additional signs, symptoms, or epidemiologic exposures suggestive of EHF, were routinely collected for EBOV testing. These signs and symptoms included headache, vomiting, diarrhea, abdominal pain, conjunctivitis, skin rash, muscle pain, fatigue, difficulty swallowing, difficulty breathing, hiccups, bleeding, unexplained death, or contact with a patient with suspected EHF. A laboratory-confirmed case of EHF was defined as illness in a person whose diagnostic samples were found positive for EBOV infection by any of the following tests: PCR , virus isolation, antigen detection, or immunoglobulin (Ig) M ELISA .\n\n【4】In addition, in a subset of surviving persons who had a history of illness consistent with EHF but no acute-phase blood samples available for testing, a convalescent-phase blood sample was collected for laboratory confirmation by IgG ELISA . Laboratory testing was performed at the Uganda Viral Research Institute in Entebbe, and subsequent testing was performed on some samples at CDC, Atlanta. This analysis is limited to laboratory-confirmed EHF cases, although additional suspected cases were identified during the outbreak.\n\n【5】Fifty-six confirmed cases of EHF were identified; 43 of these were diagnosed on the basis of positive test results from acute-phase specimens . Twenty-six patients had a positive EBOV IgG titer in convalescent-phase serum, including 13 persons who had evidence of EBOV infection in acute-phase and convalescent-phase samples.\n\n【6】The proportion of deaths during this outbreak was calculated for case-patients confirmed on the basis of an acute-phase diagnostic sample (those who only had a convalescent-phase sample are by definition surviving case-patients, and represent a biased sample). Of the 43 cases confirmed from acute-phase samples, 17 deaths occurred, for a proportion of 40%. The mean age of those who died (42 years, range 20–70 years) was significantly higher than that of survivors (33 years, range 12–50 years; p = 0.0390). No gender bias was observed between survivors and those who died .\n\n【7】Signs and symptoms were reported by patients on standardized surveillance case-report forms at the time of case identification, and in some instances, were examined further by chart review or follow-up interview. Common symptoms among patients with laboratory-confirmed cases included fever, fatigue, headache, nausea/vomiting, abdominal pain, muscle/joint pain, diarrhea, and anorexia/weight loss . No difference in the proportion of those reporting signs and symptoms, or between those who survived and those who died was noted, except for difficulty swallowing. This symptom was more common among case-patients who died (though marginally significant, p = 0.0851). Bleeding of any type (bleeding from injection site, gums, eyes, nose, vagina; black or bloody stool; bloody vomitus; or hematuria) was reported among 54% of all patients with laboratory-confirmed cases.\n\n【8】As part of the standardized surveillance case-report form, patients were also asked whether they had had contact with a sick person during the 3 weeks before development of illness. A large portion of the laboratory-confirmed case-patients in this outbreak reported direct contact with a specific person, (case X), who died of a severe hemorrhagic febrile illness consistent with EHF (no diagnostic specimens were collected from this person) in November 2007. Using the date of last contact for those reporting contact with case X or reporting contact with another laboratory-confirmed case-patient to the date of symptom onset, we calculated an average incubation period of 6.3 days among laboratory-confirmed EHF case-patients (n = 24). No significant difference was noted in the incubation period between survivors (5.7 days) and those who died (7.4 days).\n\n【9】### Conclusions\n\n【10】The 2007 outbreak of EHF in Bundibugyo District, Uganda, was caused by a new EBOV species, _Bundibugyo ebolavirus_ . Previous outbreaks of EHF have resulted in high proportion of deaths, ranging from 50% to 90% . The case fatality proportion of deaths among case-patients with EHF confirmed by acute diagnostic specimens in this outbreak was 40%, a lower percentage than for other species of EBOV that have caused human outbreaks in Africa. However, we cannot exclude the possibility that the lower proportion of deaths in this outbreak is an artifact of differences in the severity of laboratory-confirmed cases detected through outbreak surveillance or the quality of care received by hospitalized laboratory-confirmed EHF case-patients in Bundibugyo District. Nonetheless, sustained person-to-person transmission was sufficient to result in a sizeable outbreak, and death was clearly not uncommon. Thus, BEBOV should be considered a pathogen of serious public health concern.\n\n【11】As with previously documented EHF outbreaks, older age appeared to be a risk factor for death . The incubation period of EHF was ≈1 week, and signs and symptoms were largely nonspecific; infections frequently involved fever, fatigue, headache, gastrointestinal involvement, and muscle and joint pain. The nonspecific nature of these signs and symptoms, which may mimic other tropical diseases, made diagnosis of EHF based on clinical characteristics alone particularly challenging and underscores the importance of laboratory-based diagnostics to confirm and monitor control and response efforts for EHF outbreaks. Notably, signs and symptoms described in this report are primarily based on information from patient surveillance case-report forms filled out at the time of triage, and they may not represent the full spectrum of illness experienced by all persons with EHF during this outbreak.\n\n【12】It is apparent that novel emerging infections continue to occur. The outbreak of EHF described in this report involved a previously unidentified EBOV species, with a proportion of deaths of 40%. BEBOV represents the fourth EBOV species–associated disease in humans, and the third species to cause large human outbreaks of EHF. Although proportion of deaths was lower than that documented in previous EHF outbreaks, BEBOV is a severe human pathogen with epidemic potential. These findings demonstrate the need for increased surveillance and diagnostic capabilities, as well as the capacity to respond quickly to emerging human infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3168d78f-9125-42d6-baec-d6b0ceef8338", "title": "Preventing Zoonotic Influenza Virus Infection", "text": "【0】Preventing Zoonotic Influenza Virus Infection\nIn the United States, influenza viruses are estimated to cause 36,000 human deaths and 200,000 hospitalizations annually . The current outbreaks of avian influenza in Asia and Eastern Europe remind us of the zoonotic potential of these viruses. Swine cells express sialic acids that can be receptors for swine, human, and avian influenza strains and facilitate cross-species influenza transmission and the genesis of novel influenza strains. Reported cases of human-to-swine and swine-to-human influenza transmission illustrate this potential .\n\n【1】Persons who work in enclosed livestock buildings (confinement workers) have among the highest risk of becoming infected with swine influenza virus. Their work involves close contact with many swine, including sick ones. The purpose of this cross-sectional study was to learn if these workers had evidence of previous swine influenza virus infection and, if so, to determine factors that cause them to be at increased risk.\n\n【2】### The Study\n\n【3】Iowa is the top swine-producing state in the United States and markets ≈25 million swine a year. From November 2004 to March 2005, we recruited confinement workers. Site selection was based on the authors' community contacts and opportunities to invite workers to participate. Local veterinary clinics advertised the study and permitted enrollment at their facilities. This study was approved by the University of Iowa's institutional review board.\n\n【4】Persons were eligible to participate in the study if they had worked in a swine confinement facility in the past 12 months. Participants completed a questionnaire and permitted blood sample collection on enrollment. The questionnaire captured demographic, medical, and occupational data including influenza immunization history, swine occupational exposures, and use of protective equipment (gloves and masks). Nonexposed controls were enrolled during a concurrent study of University of Iowa faculty, staff, and students .\n\n【5】Serum samples were studied by using a hemagglutination inhibition (HI) assay against 2 recently circulating swine strains, A/Swine/WI/238/97 (H1N1) and A/Swine/WI/R33F/01 (H1N2), and 1 human influenza virus strain, A/New Caledonia/20/99 (H1N1). The swine H1N1 strain represents a lineage of virus that has been circulating among US swine for 70 years. The swine H1N2 strain first appeared in US swine in 1999. HI titer results are reported as the reciprocal of the highest dilution of serum that inhibited virus-induced hemagglutination of a 0.65% solution of guinea pig erythrocytes for human influenza and 0.5% solution of turkey erythrocytes for swine influenza.\n\n【6】Specimen laboratory results were examined for their statistical association with demographic, immunization, occupational, and other behavioral risk factors. Confinement workers were queried about the nature of their work and whether they had used protective equipment. Because incidence of high titers was low or nonexistent in most groups, H1N1 titers >10 were grouped. The resulting categories were <10, 10, and >10. Wilcoxon rank sum and χ 2  statistic or 2-sided Fisher exact test were used to access bivariate risk factor associations. Depending on the nature of the data and modeling assumptions, proportional odds modeling or logistic regression was used to adjust for multiple risk factors. Final multivariate models were designed by using a saturated model and manual backwards elimination. Analyses were performed by using SAS version 9.1 (SAS Institute, Inc. Cary, NC, USA). Questionnaires were made available in both English and Spanish. Site selections were based on personal contacts in 3 completely different areas.\n\n【7】Forty-nine confinement workers and 79 nonexposed controls were enrolled in the study. The distribution of ages was similar for the 2 groups, but the confinement workers were more likely to be male and Hispanic and less likely to have received influenza vaccination .\n\n【8】Swine confinement workers were categorized by type of work, frequency of contact with swine, use of gloves, and use of masks. The question \"When working with sick or diseased swine, how often do you wear gloves?\" explained the most variation in swine H1N1 antibody titers and was included in the best fit model. Workers who sometimes or never used gloves were significantly more likely (odds ratio \\[OR\\] 30.3, 95% confidence interval \\[CI\\] 3.8–243.5) to have elevated titers than the nonexposed controls . These workers also were significantly more likely (OR 12.7, 95% CI 1.1–151.1) (data not shown) to have elevated titers than the other confinement workers who used gloves most of the time or always. Workers who reported smoking also had high OR (data not shown) for elevated titers.\n\n【9】Multivariate analysis also showed that persons who had received the 2003–04 influenza vaccine were significantly more likely to have elevated titers (>10) against swine H1N1 virus  as well as swine H1N2 (data not shown). Although cross-reaction with 1 of the viruses in the 2003–04 vaccine or a circulating influenza virus may explain this occurrence, higher titers would have been expected for all vaccinated persons (including controls), but such higher titers were not observed . We suggest that this result represents other behavior or health-related confounders not identified in the questionnaire for this study.\n\n【10】### Conclusions\n\n【11】These data suggest, like previous studies , that swine confinement workers are at increased risk for zoonotic influenza infection. However, our data are among the first to evaluate swine confinement workers, our sample size was small (not likely representative of all swine workers), and exposure data were self-reported. Confinement workers, in contrast to other swine occupations, are difficult to reach because of language barriers, on-farm policies regarding visitors (biosecurity protocols), and lack of trust in the public health sector.\n\n【12】Several studies have documented smoking as a risk factor for human influenza virus infection . However, we believe our data are the first evidence that smoking also increases the risk for swine influenza virus infections. We believe that this increased risk may be because the workers' oral mucosa are exposed to swine influenza virus after handling pigs.\n\n【13】This study's chief unique contribution is the evidence that use of gloves during swine confinement work noticeably decreases the risk for swine influenza virus infection. Thus, a simple personal protective measure might do much to reduce swine-to-human virus transmission. Future larger studies of swine confinement workers are needed to validate our findings and to better quantify risk factors for this population.\n\n【14】Individual behavior strongly influences influenza virus transmission . The national strategy for pandemic influenza highlights worker education and emphasizes individual responsibilities in preventing the spread of infection . Should a virulent, novel zoonotic influenza virus enter swine confinement facilities and spread among concentrated swine populations, the impact would be grave. Surveillance for zoonotic influenza virus therefore must be routinely conducted among agricultural workers. Also, use of personal protective equipment, frequent hand washing, and restrictions on smoking in or around swine facilities should be encouraged. Further, such workers should be included in state and federal pandemic plans as a high-risk group designated to receive annual influenza vaccines and antiviral drugs during pandemics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2dd2926f-d1cb-4240-8c5d-1c5a6f80d119", "title": "Ebola Virus Stability on Surfaces and in Fluids in Simulated Outbreak Environments", "text": "【0】Ebola Virus Stability on Surfaces and in Fluids in Simulated Outbreak Environments\nSince March 2014, >22,000 cases of Ebola virus disease (EVD) and ≈ 9,000 deaths have been reported in West Africa . Thousands of health care professionals have been mobilized to West Africa to assist with the ongoing outbreak of EVD . More than 800 Ebola virus (EBOV) infections have been reported in health care professionals .\n\n【1】Determining the persistence of EBOV on surfaces and under environmental conditions specific to outbreak settings and disease-endemic areas is critical to improving safety practices for these health care workers , as well as answering questions about EBOV transmission among the public . Researchers have experimentally assessed the stability of other EBOV strains on plastic, glass, and steel within dried media or guinea pig serum ; in the dark on glass ; and during exposure to UV light . However, the environmental conditions of these studies do not reflect the higher temperatures and relative humidities (RHs) in outbreak regions, or the current outbreak strain. No infectious EBOV could be found during environmental sampling in a ward with EVD patients; however, this result could be more indicative of cleaning measures than actual virus stability .\n\n【2】We report stability of EBOV with a current outbreak strain from Guinea (Makona-WPGC07)  on 3 clinically relevant surfaces: stainless steel, plastic, and Tyvek (Dupont, Wilmington, DE, USA). We also determined the stability of EBOV in water, spiked human blood, and blood from infected nonhuman primates (NHPs). These experiments were conducted in 2 environmental conditions, 21°C, 40% RH, and 27°C, 80% RH, to simulate a climate-controlled hospital and the environment in West Africa, respectively.\n\n【3】### The Study\n\n【4】We tested the stability of EBOV on 3 materials commonly found in an Ebola treatment unit (ETU) in West Africa: 1) utility-grade  stainless steel washers (McMaster-Carr, Atlanta, GA, USA); 2) plastic (Teflon \\[polytetrafluoroethylene\\]; McMaster-Carr); and 3) Tyvek (from the front of a coverall). For each time point, 3 disks (4-cm diameter) of each material were placed individually into wells of a 6-well plate. Five samples (10 μL/sample) containing a total dose of 10 6  50% tissue culture infectious doses (TCID 50  s) of EBOV in cell-free medium were evenly distributed on the disks. The plates were divided into groups, and each group was placed into a plastic HEPA-filtered box and placed at 21°C, 40% RH, or 27°C, 80% RH. The samples were dried naturally, and virus titers were determined over a 14-day period.\n\n【5】In the surface and fluid stability experiments, all samples were stored at −80°C until titration (1 freeze–thaw cycle of EBOV samples that did not change virus titer). Titrations were performed on Vero E6 cells as described . The TCID 50  per milliliter for each sample at each time point was calculated by using the Spearman-Karber method .\n\n【6】Because viral decay rates often exhibit first-order kinetics , we log 10  transformed our TCID 50  calculations to represent virus titer and used a linear regression analysis (Prism version 6.05; GraphPad, San Diego, CA, USA) to determine the log 10  reduction rate of EBOV on each surface at both environmental conditions . We also determined whether linear regression models were significantly different from each other at the p<0.05 level by using an analysis of covariance equivalent test in Prism. Overall, virus remained viable longest in hospital conditions and on Tyvek. Viable EBOV was detectable for 3 days on Tyvek at tropical conditions.\n\n【7】The stability of EBOV in water was assessed by diluting 150 µL virus stock in 2.85 mL of Ambion diethylpyrocarbonate–treated water (Thermo Fisher Scientific, Pittsburgh, PA, USA) and removing residual protein and medium with 1 initial and 2 rinse spins on Amicon Ultra Centrifugal Filters 100K MWCO (Merck, Darmstadt, Germany). EBOV was more stable in water at 21°C and had an ≈1 log 10  reduction/day in water at 27°C .\n\n【8】The stability of EBOV in human blood was assessed by spiking blood samples from a healthy human volunteer to achieve a 10 5  TCID 50  /mL virus titer. The spiked blood was distributed in 1-mL aliquots into closed screw-top vials to maintain a liquid state, spread in 50-µL aliquots onto the bottom of a 24-well plate, and dried. One group of samples was stored at 21°C, 40% RH, and the other group was stored at 27°C, 80% RH. EBOV stability in drying blood exhibited first-order kinetics and was viable for up to 6 days at tropical conditions .\n\n【9】To approximate the stability of EBOV in naturally infected human blood, we used blood from cynomolgus macaques ( _Macaca fascicularis_ ) as a proxy. Blood was collected during necropsy from 3 macaques that were previously enrolled in an Animal Care and Use Committee–approved EBOV pathogenesis study and were euthanized because they exhibited signs of EVD and viremia. Blood samples were divided into 2 groups with 2 sets of 150-μL aliquots for each time point; each group was stored at the conditions described above with each set in the liquid or drying state, and virus viability was assessed over a 14-day period. Because of variation in calculated virus titer from each of the individual NHPs, the log 10  reduction rate could not be approximated and only the initial titer and the duration of viability are shown . In general, EBOV maintained viability for a longer duration in liquid than in drying blood regardless of initial titer or environmental condition.\n\n【10】### Conclusions\n\n【11】We found that EBOV can persist on surfaces common in an ETU, highlighting the need for adherence to thorough disinfection and doffing protocols when exiting the ETUs and careful handling of medical waste. In addition, EBOV maintains viability for a longer duration in liquid than in dried blood. EBOV in blood of experimentally infected NHPs persists for a similar duration as EBOV in spiked human blood. A recent study showed that blood in the body cavity of an NHP contained viable EBOV for up to 7 days after death . We detected viable EBOV in drying blood for up to 5 days at both environmental conditions in human and NHP blood. Therefore, dried and liquid blood from an infected person in their home or ETU should be treated as potentially infectious. The finding that EBOV remains viable in water for as long as 3 (27°C) or 6 (21°C) days at the experimental concentration warrants further investigation into the persistence of the virus in aqueous environments, such as in wastewater or sewage canals. Viable EBOV has been isolated from urine  but not from human stool . Therefore, the potential for dissemination of EBOV through wastewater remains unknown.\n\n【12】This study is subject to several limitations. First, because standard volumes for samples were used, different volumes or matrices could influence the stability of EBOV under the tested conditions. Second, blood samples from the NHPs might have different immunologic or biochemical conditions, which can potentially influence virus stability. Third, the experimental conditions in the laboratory are sterile, but in disease-endemic areas and ETUs, bacteria or chemicals could influence EBOV viability.\n\n【13】Overall, we found that different environmental conditions, fluids, and surfaces influence the persistence of EBOV. These findings demonstrate that such factors are crucial in understanding transmission and improving safety practices.\n\n【14】Dr. Fischer and Mr. Judson are researchers in the Virus Ecology Unit at Rocky Mountain Laboratories in Hamilton, Montana. Their research interests are the ecology and evolution of emerging infectious diseases and relationships between human and environmental health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c6e7ee68-8ef7-42dc-9ab8-081b370036bd", "title": "Genetic Homogeneity of Measles Viruses Associated with a Measles Outbreak, São Paulo, Brazil, 1997", "text": "【0】Genetic Homogeneity of Measles Viruses Associated with a Measles Outbreak, São Paulo, Brazil, 1997\nDuring a resurgence of measles in São Paulo, Brazil, in 1997, >40,000 cases (peak incidence rate of 246/100,000 inhabitants) and 42 measles-related deaths were reported. Reverse transcriptase-polymerase chain reaction and nucleotide sequencing were used to analyze specimens from patients who had typical clinical measles infection during this outbreak and from six patients who had had measles in 1995 and 1996. Although wild-type measles viruses (genotypes D5 and D6) were present in São Paulo before this resurgence, we detected only D6 viruses. The genotype D6 viruses isolated during this outbreak had identical sequences to genotype D6 viruses isolated in other parts of Brazil and South America in 1997 and 1998, suggesting that a single chain of transmission was responsible. We also identified genotype A viruses in two vaccine-associated cases from 1995 and 1996. Our findings extend the knowledge of the circulation patterns of measles virus in South America, contributing to measles control efforts in the Americas.\n\n【1】The Pan American Health Organization (PAHO) reported record low numbers of measles cases for 2000–2001, reflecting the success of measles control programs in the Western Hemisphere  . In the most populous country in the region, the United States, indigenous transmission of measles virus has been eliminated since 1993, and only 3 of 41 countries in the region reported indigenous measles transmission during 2001 . Despite this success, measles remains an endemic disease in many areas of the world. The World Health Organization (WHO) estimates that approximately 45 million cases of measles and >800,000 measles-related deaths continue to occur annually . Sporadic measles outbreaks still occur in both developed and undeveloped countries that have failed to maintain adequate immunization levels. These outbreaks, such as the one that occurred in São Paulo, Brazil, in 1997, provide an opportunity to study the virologic and epidemiologic aspects of measles transmission in vaccinated populations.\n\n【2】Before the introduction of vaccine, measles was a substantial public health problem in Brazil, representing a major cause of death among young children. Even after the introduction of vaccine, measles epidemics continued to occur in Brazil. For example, the epidemics of 1984 and 1986 had incidence rates of 63/100,000 and 97/100,000, respectively. PAHO launched an aggressive measles control program for the Americas in 1991 that included one-time national catch-up campaigns for all children, routine “keep-up” vaccination of infants at 1 year of age, and follow-up campaigns at 3- to 5-year intervals. In Brazil, this strategy resulted in a 93% reduction in the incidence of measles in Brazil from 1991 through 1996. For the 4-year period 1993–1996, <1,000 cases were reported annually. However, during the resurgence of measles in 1997 in São Paulo, 42,055 confirmed cases of measles (23,907 laboratory confirmed), were reported, with a peak incidence rate of 246/100,000 inhabitants. Most of the cases were in adults living in metropolitan São Paulo. Forty-two deaths caused by measles were reported during this epidemic .\n\n【3】Virologic surveillance can help to identify the transmission pathways of measles virus and is an important component of measles surveillance activities . Sequence analysis of measles viruses has shown that distinct lineages of wild-type viruses exist and co-circulate, and WHO currently recognizes 20 measles genotypes . The epidemiologic aspects of the measles outbreak in São Paulo have been described . The purpose of this report is to present the genetic characteristics of the measles viruses responsible for the resurgence of measles in São Paulo, Brazil, during 1997. This outbreak provided an opportunity to study the transmission of viruses in an area that had previously had good measles control but then experienced a rapid resurgence in cases. Our study showed that a single genotype of virus, D6, was responsible for the outbreak and that viruses from this chain of transmission subsequently spread to other countries in South America.\n\n【4】### Materials and Methods\n\n【5】##### Specimens\n\n【6】Samples from patients with laboratory-confirmed measles were analyzed . Six of the samples were serum specimens; the rest were heparinized, whole-blood specimens. Peripheral blood mononuclear cells (PBMC) were purified from whole blood by using Ficoll-Hypaque gradient centrifugation. The PBMC were washed three times with phosphate-buffered saline before being resuspended in Dulbecco’s minimal essential medium (DMEM) containing 10% fetal calf serum (FCS). Aliquots of PBMC were cryopreserved by using standard procedures  .\n\n【7】##### Virus Isolation\n\n【8】B95a cells  were injected with 0.2 mL of the clinical samples and maintained in DMEM, 0.2 mM L-glutamine/100 mL and gentamicin 40 mg/mL, in 2% FCS at 37°C. Samples with no apparent cytopathic effect (CPE) after 3 passages were discarded as negative for virus isolation. If CPE was observed, the cells were harvested by centrifugation when CPE was maximal, and RNA was extracted from the cell pellet. Five measles viruses were isolated from PBMC collected during the epidemic.\n\n【9】##### RNA Extraction, Polymerase Chain Reaction, and Sequencing\n\n【10】RNA was extracted from serum, PBMC, or infected cell pellets by the guanidinium acid-phenol method  . cDNA corresponding to the nucleotides coding for the carboxyl terminus of the nucleoprotein (N) or the full-length open reading frame for the hemagglutinin (H) gene were synthesized by using avian myeloblastosis virus reverse transcriptase (RT) and amplified by polymerase chain reaction (PCR), and the sequences of the PCR products were obtained as described previously . Sequence data were analyzed by using version 10.1 of the Genetics Computer Group Sequence Analysis Software Package (Accelrys Inc. Madison, WI)  and phylogenetic analyses were performed with PAUP version 4.0  . A reference N gene sequence was deposited in GenBank under accession number AF495863.\n\n【11】### Results\n\n【12】We obtained genetic information from specimens that were collected in São Paulo from March 1995 to October 1997 . During 1995 and 1996, relatively few measles cases were reported in São Paulo; six acute-phase serum samples were obtained from confirmed, sporadic cases that occurred as early as 2 years before the outbreak. In 1997, a resurgence of measles occurred in São Paulo, beginning in the fall and peaking in late winter . Twenty-three samples were obtained from cases that occurred during the first 42 weeks of 1997; 11 of these samples were obtained in August 1997 during the peak of the outbreak . We obtained most of the outbreak specimens from young adults, which is consistent with the observation that most of the measles cases occurred in unvaccinated, young adults  . Measles-specific immunoglobulin (Ig) M responses were detected in all the patients listed in the Table, except one. The serum sample from case number 802 was negative, probably because it was obtained <3 days after rash onset. However, this case was confirmed by a positive virus isolation .\n\n【13】Virus isolation was not attempted for the serum samples, but five virus isolates were obtained from the PBMC specimens  obtained during 1997. The sequences coding for the carboxyl terminus of the nucleoprotein (456 nucleotides) of these isolates were compared with the sequences of the WHO reference strains  . The results indicated that these Brazilian viruses all had identical N gene sequences and were members of genotype D6 . The N gene sequence of MVi/SãoPaulo.BRA/42.97/35175 has been deposited in GenBank  as a reference sequence for this outbreak. The complete sequence of the H gene was also obtained for MVi/SãoPaulo.BRA/42.97/35175. Comparison of this H gene sequence with the sequences of the H genes from the WHO reference strains confirmed the placement of these viruses in genotype D6 (data not shown).\n\n【14】For the rest of the specimens from 1997, nucleotide sequences were obtained from PCR products that were amplified directly from RNA extracted from the PBMC specimens. In most cases, we had to perform a nested PCR reaction to get sufficient material for sequencing. All specimens obtained in 1997, either just before or during the peak of the outbreak, were placed in genotype D6. Most of the N gene sequences obtained from these specimens were identical to each other and to the N gene sequences obtained from the five isolates from 1997. Only three of the specimens had sequences that differed from the majority sequence by a single nucleotide.\n\n【15】We were also able to amplify measles-specific PCR products from the serum samples obtained from sporadic measles cases that occurred in 1995 and 1996. Two of these specimens had nucleotide sequences in genotype A, which contains all vaccine viruses. Both specimens were obtained from 9-month-old infants who had received their measles vaccination 12–13 days before collection of the serum specimen. One of the samples obtained in 1995 had an N gene sequence in genotype D5, while the other three serum samples had sequences in genotype D6 identical to the D6 sequences obtained during the outbreak. These data showed that D6 viruses and wild-type viruses from genotypes other than D6 were present in São Paulo as early as 2 years before the peak of the outbreak in 1997.\n\n【16】We compared the N gene sequences from the genotype D6 viruses in the São Paulo cases with N gene sequences from genotype D6 viruses isolated in other parts of South America  and the world . For simplicity, three representative sequences from the South American viruses are shown in the phylogenetic tree. The sequences from the São Paulo outbreak were identical to those obtained from viruses isolated in Brazil (i.e.Valenca, Rio de Janeiro, Belo Horizonte, Angra do Reis, and Curitiba) , Argentina , Uruguay  , and Bolivia in 1997 and 1998. This homogenous group also included two viruses that were imported into the United States from Brazil in 1997  . The sequences from genotype D6 viruses imported into the United States  from European sources and genotype D6 viruses isolated in Europe  were slightly different from the South American D6 group. We found that the sequences from the South American viruses were most closely related to a genotype D6 virus isolated in Luxembourg in 1997, although no foreign source was identified for the outbreak in Brazil. Overall, the N genes sequences of D6 viruses within the South American group differed by no more than 0.4% but differed from other genotype D6 sequences by as much as 1.4%.\n\n【17】### Discussion\n\n【18】This report presents the virologic surveillance data from the measles outbreak that occurred in São Paulo, Brazil, during 1997. Sequence data from the specimens obtained during the outbreak indicated the presence of measles viruses belonging to genotype D6, which is one of the prevalent genotypes of measles virus in Europe. Frequently associated with imported measles cases from European countries , D6 viruses have been isolated in the United Kingdom, Spain, Germany, Russia, Poland, Denmark, and Luxembourg . During 1997–2000, these genotype D6 viruses were imported into the United States from Italy, Greece, Ukraine, Croatia, Cyprus, and the United Kingdom, and from the large measles outbreak that occurred in São Paulo . However, because neither conventional case nor virologic surveillance was conducted in Brazil before the start of the measles control program in 1991, it is not possible to determine if the D6 viruses were recently introduced into Brazil by importation or if they represent continued circulation of viruses indigenous to Brazil. Our data show that D6 viruses indistinguishable from the D6 viruses isolated during the São Paulo outbreak were responsible for sporadic measles cases in São Paulo for at least 2 years before the outbreak. In a large urban area such as São Paulo, the virus will likely be continually present either from low-level circulation or frequent importation. In late 1997, a change occurred in the epidemiologic circumstances, resulting in a large outbreak of the measles virus. Factors that contributed to the outbreak included the failure to conduct a timely follow-up vaccination campaign in 1995 and the low level of vaccination coverage among infants achieved by routine health services. Additionally, a large number of unvaccinated young adults who had escaped measles infection had moved to São Paulo from rural areas. The high population density in São Paulo also facilitated rapid spread of the virus  .\n\n【19】Laboratory-based surveillance for measles was established in Brazil in the early 1990s. Although virologic surveillance was certainly incomplete, some information is available about measles genotypes present in Brazil before the 1997 outbreak in São Paulo. Genotype D5 was detected in a sporadic case in São Paulo in 1995 and in sporadic cases that occurred in the state of Bahia in 1996  . Genotype D5 viruses that circulate in Japan have been associated with importation of viruses from Japan into other countries . The source of the genotype D5 viruses detected in São Paulo and Bahia was not identified. Genotype C2 viruses were detected in a small outbreak in the state of Santa Catarina in 1996  . C2, another indigenous genotype in Europe, is often associated with imported cases in other parts of the world .\n\n【20】Fever and rash occur in approximately 5% of measles vaccine recipients  . In areas where wild-type virus is circulating, vaccine reactions may be classified as measles cases. Distinguishing a vaccine reaction from a measles case is not possible with the currently available serologic methods. Only the genetic analysis of virus isolates can confirm the presence of either a vaccine or wild-type virus. In this study, two vaccine viruses were detected in persons who had received vaccine 12 and 13 days before collection of specimens. In vaccine reactions, rash and fever typically occur 7 to 12 days after vaccination; the virus can be detected in circulating lymphocytes even after the rash and fever have resolved.\n\n【21】The situation in Brazil is now very similar to that in the United States in the early 1990s. From 1989 to 1991, the United States experienced a resurgence of measles, with >50,000 reported cases. Virologic surveillance suggested that virus from a single genotype, D3, had seeded the entire country. As was the case with the D6 viruses from South America, the D3 viruses from many different areas in the United States had nearly identical sequences  . Therefore, we conclude that when measles outbreaks occur in areas that have good measles control, genetic analysis of viruses will confirm the presence of a single chain of transmission. In contrast, in areas where measles control programs are less advanced, genetic studies have identified multiple chains of transmission within a genotype  .\n\n【22】Initiation of a two-dose vaccination schedule, along with the success of the PAHO strategy in the Americas, resulted in historically low numbers of cases in the United States in the years following the resurgence there; both standard epidemiologic surveillance and molecular epidemiologic surveillance have documented the sustained interruption of transmission of the genotype D3 viruses . Similarly, the pattern of viral genotypes found in Brazil and other parts of South and Central America will help to assess the success of the PAHO measles control programs. Genotype D6 viruses were circulating in Haiti and the Dominican Republic during 2001 but have not been isolated from ongoing chains of transmission in any other countries in the region  . During 2002, virologic surveillance showed that measles cases imported into El Salvador from Europe were genotype D7  . As the number of measles cases continues to decrease in the Americas, increasing virologic surveillance activities will be important. This virologic surveillance will document the interruption of indigenous transmission of the genotype D6 viruses in Brazil and in other parts of South America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "318f8e6e-3a77-4ad0-86c8-b38360e4dbce", "title": "Comparison of Wrist- and Hip-Worn Activity Monitors When Meeting Step Guidelines", "text": "【0】Comparison of Wrist- and Hip-Worn Activity Monitors When Meeting Step Guidelines\nAbstract\n\n【1】**Introduction**\n\n【2】Physical activity (PA) guidelines aimed at accumulating 10,000 steps per day have become increasingly common with the advent of wristband PA monitors. However, accumulated steps measured with wristband PA monitors may not be equal to steps measured with validated, hip-worn pedometers. Consequently, evaluating and developing guidelines for step counts using wristband PA monitors for the general population is needed. We compared step counts accumulated with hip-worn pedometers with those accumulated with wrist-worn activity monitors during 1) treadmill exercise, 2) treadmill walking, and 3) activities of daily living (ADL) to determine their accuracy in meeting step count guidelines (ie, 10,000 steps/d).\n\n【3】**Methods**\n\n【4】Eighty-six adults (aged 18–65 y; body mass index, 19–45 kg/m 2  ) completed 30 minutes of treadmill exercise while simultaneously using a hip-worn pedometer and wrist-worn PA monitor. Remaining steps needed to reach 10,000 steps (ie, 10,000 steps minus the number of pedometer steps recorded from treadmill exercise = remainder) were completed via treadmill walking or ADL. Steps were recorded for both devices after treadmill exercise, treadmill walking, and ADL for both devices.\n\n【5】**Results**\n\n【6】Fewer steps were accumulated via wrist-worn PA monitors than via hip-worn pedometers during treadmill exercise (3,552 \\[SD, 63\\] steps vs 3,790 \\[SD, 55\\] steps, _P < ._ 01) and treadmill walking (5,877 \\[SD, 83\\] steps vs 6,243 \\[SD, 49\\] steps, _P < ._ 01). More steps were accumulated via wrist-worn PA monitors than hip-worn pedometers during ADL (7,695 \\[SD, 207\\] steps vs 6,309 \\[SD, 57\\] steps, _P < ._ 01). Consequently, total steps were significantly higher for wristband PA monitors than hip-worn pedometers (11,247 \\[SD, 210\\] steps vs 10,099 \\[SD, 39\\] steps; _P < ._ 01).\n\n【7】**Conclusion**\n\n【8】The widely used 10,000-step recommendation may not be accurate for all users of all activity monitors, given the discrepancy in daily step count among wrist-worn and hip-worn devices. Having a more accurate indication of number of steps taken per day based on the device used could have positive effects on health.\n\n【9】Introduction\n\n【10】Engaging in a physically active lifestyle is essential in the prevention and treatment of chronic diseases. While time spent in moderate- or high-intensity activity is the focus of many health organizations , completing a certain number of daily steps is also a popular way to encourage physical activity (PA) participation. Step count PA recommendations typically promote the accumulation of 10,000 or more steps per day . The goal of accumulating 10,000 steps per day was first introduced in 1965 and thought to be the amount of PA needed to reduce the risk of developing coronary artery disease . This recommendation has evolved based on exercise recommendations related to accumulating 30 minutes per day of at least moderate-intensity PA , PA assessments of free-living individuals , and recommendations in the newest guidelines aimed at reducing time spent engaged in sedentary behaviors . Estimates suggest that most adults engaging in moderate intensity PA accumulate 100 or more steps per minute . Therefore, evidence supports the notion that 30 minutes of moderate-intensity PA is equivalent to approximately 3,000 steps per day . Additionally, free-living individuals who meet the goal of achieving 30 minutes per day of at least moderate-intensity PA accumulate on average 8,000 to 11,000 steps per day . Achieving 10,000 steps per day is also associated with meaningful health outcomes (eg, favorable changes to body composition and body mass index \\[BMI\\]), even if most steps taken are not at a moderate or higher intensity level .\n\n【11】Pedometers and, more recently, other wearable devices continue to grow in popularity and are potentially useful for tracking daily step counts and promoting an active lifestyle . Wrist-worn monitors, which tend to use triaxial accelerometry, are a more convenient option to track steps and activity than traditional hip- or waist-worn monitors . There has been a proliferation of validation studies examining step counts measured by various wrist-worn devices. These studies generally show that wrist-worn activity monitors overestimate steps during free-living conditions by 10% to 35%, underestimate steps during ambulatory activities without wrist movement (eg, pushing a shopping cart, wheelchair, or stroller) by 35% to 95%, and vary in accuracy at different walking speeds (typically underestimating step counts at slower speeds) . Existing research comparing wrist- and hip-worn devices for step counting in free-living settings generally show that wrist-worn devices overestimate steps by 10% to 25% , sometimes as high as 40% to 50% , and sometimes demonstrate no differences between placements (ie, wrist vs hip) . This variation in comparability between placements might be due to different monitors used or, more likely, studying populations with different patterns of activity or total activity volume. We are unaware of any previous research comparing device placements for populations meeting step count recommendations (ie, 10,000 steps/d). Given that most of the discrepancy in step counting between wrist and hip devices occurs during nonambulatory activity, it may be that adults who achieve step count recommendations will have less difference in step counts between placement sites due to higher volumes of ambulation.\n\n【12】It remains unclear whether current recommendations of 10,000 steps per day (based on data collected from hip-worn pedometers) is an appropriate target to meet activity guidelines for individuals using wrist-worn monitors or how much discrepancy to expect between wrist- and hip-worn devices to track steps. Therefore, the purpose of this study was to examine steps accumulated on a wrist-worn activity monitor, through treadmill exercise, treadmill walking, and activities of daily living (ADL), compared with a hip-worn pedometer monitor when participants accumulate approximately 10,000 steps per day.\n\n【13】Methods\n\n【14】A total of 86 men (n = 36) and women (n = 50) (aged 18–65 y; BMI, 19–45 kg/m 2  ) completed 2 experimental conditions from September 2016 through June 2019. Three methods were used to recruit participants for this study: 1) a university campus-wide announcement sent out via email, 2) flyers posted around campus and the surrounding area, and 3) word of mouth (eg, participants asking their partners if they would be interested in participating). Participants included normal weight-to-obese adults, including regular exercisers (ie, reported accumulating at least 30 minutes of planned PA, ≥3 d/wk, for the last ≥6 months) and nonexercisers (ie, no planned exercise). All participants were apparently healthy adults (ie, no known cardiovascular or metabolic disease) as assessed via the Physical Activity Readiness Questionnaire (PAR-Q), were nonsmokers, and had no known limiting musculoskeletal or joint issues that would prevent them from engaging in exercise, walking with normal gait, or completing ADL. Written informed consented was obtained from all participants before participation, and all procedures were approved by the Central Michigan University Institutional Review Board.\n\n【15】### Study design\n\n【16】Participants completed an experimental trial where steps were accumulated and recorded from a hip-worn Omron pedometer and wrist-worn Fitbit monitor during 3 modalities: 1) treadmill exercise, 2) treadmill walking, and 3) ADL. Three modalities were used to create 2 experimental conditions. The first experimental condition was a controlled setting in our laboratory where steps were accumulated completely on a treadmill during 30 minutes of treadmill exercise (designed to meet current exercise guidelines) followed by treadmill walking to accumulate the remaining steps needed to reach a total of 10,000 steps. This condition was referred to as _laboratory setting_ and allowed for careful control of participants to ensure they took 10,000 steps. The second experimental condition was designed to represent how most US adults might meet exercise and physical activities guidelines. Data from the National Health and Nutrition Examination Survey indicate that walking, bicycling, and ADL (eg, yard work) are the most commonly reported leisure-time physical activities among a representative group of US adults . Therefore, in the second experimental condition, steps accumulated through treadmill exercise were combined with steps accumulated through ADL. This experimental condition was referred to as _real-world setting._\n\n【17】Age of participants was determined by their date of birth reported on the PAR-Q form. After participants completed the PAR-Q form, they were asked if they were regular exercisers or nonexercisers. “Exerciser” was defined as a participant who accumulated at least 30 minutes of planned PA on 3 or more days per week for the last 3 or more months; “nonexerciser” was defined as one who reported no planned exercise. Next, while wearing light clothing and no shoes, participants’ height was determined using a stadiometer (Seca 213, Seca GmbH), and weight was measured using a standard physician scale (Seca 700, Seca GmbH). Participants’ stride length was determined via manufacturer guidelines prior to being fit with the hip-worn pedometer.\n\n【18】### Devices used\n\n【19】Participants were fitted with an Omron HJ-720 ITC pedometer (Omron Health Care, Inc), which was programmed with user weight and stride length. The Omron was worn as the reference device by participants to provide a step count measure during treadmill exercise, treadmill walking, and ADL. According to manufacturer instructions, participants were fitted with the device at the anterior axillary line of the right hip, clipped to the pant waist. Study staff demonstrated and checked for proper placement on the hip. The Omron HJ-720 ITC provides accurate measures of steps taken in laboratory and free-living settings, rendering it a suitable criterion measure for comparison in this study .\n\n【20】Next, participants were fitted with a Fitbit Charge HR (Fitbit, Inc), worn on the dorsal side of the nondominant wrist (dominant hand was defined as the hand they preferred to write with), per manufacturer recommendations, before data collection. Before study start, the user height, weight, and sex were input into the Fitbit mobile application and synced to the device.\n\n【21】### Treadmill exercise\n\n【22】Treadmill exercise consisted of 30 minutes of exercise on a treadmill at a speed of 1.5–5.8 mph and grade of 1.4%–3.0% to reach 64%–74% of participants’ age-predicted heart rate (HR) maximum (pHRmax = 220 minus age) and equivalent to moderate-intensity exercise . During this exercise session, participants wore a Polar chest strap (Polar H7 Heart Rate Sensor, Polar Electro) to monitor HR during exercise. The HR monitor was worn along the rib cage, just below the xiphoid process of the sternum, secured in place with an elastic strap, and transmitted to a Polar watch (Polar A300, Polar Electro) held by the research team. Speed and/or grade were adjusted within the first 5 minutes of exercise only to meet participants’ target HR range (ie, 64%–74% pHRmax). HR was recorded every 5 minutes during the exercise session to determine the average percent HRmax (%HRmax = \\[average HR/pHRmax\\] × 100) at which participants exercised during the entire exercise bout. Additionally, oxygen consumption (VO 2  ) was estimated based on speed and grade using American College of Sports Medicine equations for treadmill activity  and metabolic equivalents (METs), a second indicator of exercise intensity, were calculated (METs = estimated VO 2  /3.5). After exercise was complete, participants immediately stepped onto the sides of the treadmill, and steps from the Omron and Fitbit were recorded. Next, using the Omron as the criterion device, the remaining steps needed to accumulate a total of 10,000 steps were performed via treadmill walking (shortly after the exercise session) and ADL (the following day). Because steps accumulated during treadmill exercise ranged from approximately 3,000 to 5,000 steps (using the Omron), remaining steps to be completed during treadmill walking (to complete the laboratory setting condition) and ADL (to complete the real-world setting condition) ranged from approximately 5,000 to 7,000 steps.\n\n【23】### Treadmill walking\n\n【24】Treadmill walking was performed at 3 mph and 0.5% grade. Again, the Omron was used as a criterion reference; steps were monitored until the Omron reached a total of 10,000 steps from the treadmill exercise and walking conditions. Once these steps were achieved, participants stepped onto the sides of the treadmill; step counts on both devices, total walking time, and total distance covered (reported on the treadmill) were recorded. Step counts accumulated during treadmill exercise and treadmill walking were combined to determine total steps in the laboratory setting for both devices.\n\n【25】### Activities of daily living\n\n【26】ADL steps were accumulated outside of our laboratory. Participants were sent home with the Omron and Fitbit and instructed to wear both the day after laboratory testing. Participants were shown how to secure each device in the correct orientation and appropriate anatomical location. They were instructed to accumulate their steps through their normal daily activities but not through exercise. Again, the Omron was used as a criterion reference; steps were monitored by participants until they accumulated an amount equal to the number accumulated during the previous day’s treadmill walking. Therefore, participants accumulated 5,000 to 7,000 steps through their ADL. After Omron steps reached the required step count, steps on the Omron and Fitbit were immediately recorded by the participant. Then devices and ADL step count data were returned to the research staff. Steps counts accumulated during treadmill exercise and ADL were combined to determine total steps in the real-world setting scenario for both devices. In total, the 3 PA modalities (ie, treadmill exercise, treadmill walking, and ADL) were completed to create 2 experimental conditions (ie, laboratory setting and real-world setting) to evaluate step count differences between devices. We summed the number of steps accumulated from treadmill exercise and those accumulated from treadmill walking for both devices and subtracted the number of steps accumulated from the Fitbit from those accumulated from the Omron to determine the difference in total steps in the laboratory setting. We summed the number of steps accumulated from treadmill exercise and those accumulated from ADL for both devices and subtracted the number of steps accumulated from the Fitbit from those accumulated from the Omron to determine the difference in total steps in the real-world setting, using the following formulas:\n\n【27】Difference in total steps, laboratory setting = (treadmill exercise + treadmill walking Omron steps) – (treadmill exercise + treadmill walking Fitbit steps)\n\n【28】Difference in total steps, real-world setting = (treadmill exercise + ADL Omron steps) – (treadmill exercise + ADL Fitbit steps)\n\n【29】### Statistical analyses\n\n【30】A paired-samples _t_ test was used to assess potential differences between step counts of devices (ie, Omron vs Fitbit) for each condition exercise, walking, ADL, total step counts (laboratory and real-world setting). Bland-Altman statistics were performed to determine the limits of agreement for the Fitbit device compared with the Omron criterion. A subanalysis was performed to determine potential differences between sexes (men vs women) and exercise status (exercisers vs nonexercisers) in step count differences between devices within each condition (ie, treadmill exercise, treadmill walking, and ADL) for each device using an unpaired _t_ test or Mann–Whitney rank sum test for nonparametric data. A Pearson correlation was used to assess age and BMI vs difference in total steps between devices. All statistical analyses were performed using SigmaPlot 12.5 (Systat Software, Inc). Significance was set at _P_ < _._ 05.\n\n【31】Results\n\n【32】### Participant treadmill exercise and walking characteristics\n\n【33】Participants characteristics are detailed in the Table . During treadmill exercise, participants exercised for 30 minutes at 69.1% (SD, 3.3%) pHRmax, with an average speed of 3.8 (SD, 0.7) mph, a grade of 2.9% (SD, 2.0%), and a distance of 1.9 (SD, 0.3) miles. During treadmill walking, participants walked at 3.0 mph and a 0.5% grade for 55.8 (SD, 5.3) minutes.\n\n【34】### Step counts\n\n【35】Compared with the Omron, significantly fewer steps were accumulated by the Fitbit during walking ( _P_ < _._ 01)  and exercise ( _P_ < _._ 01) . Consequently, when combining steps from treadmill exercise and treadmill walking in the laboratory setting, significantly fewer steps were accumulated with the Fitbit than with the Omron ( _P_ < _._ 01) .\n\n【36】**  \nStep counts detected with the Omron pedometer and Fitbit wristband activity monitors during A) treadmill exercise, treadmill walking, and treadmill exercise plus walking combined (total steps: laboratory setting); and B) treadmill exercise, activities of daily living, and treadmill exercise plus activities of daily living (total steps: real-world setting). All measurements significantly different at _P_ < _._ 05\\. \n\n【37】**  \nBland-Altman plots representing differences between Omron pedometer and Fitbit wrist-worn activity monitor steps vs A) Omron pedometer steps during treadmill exercise, B) Omron pedometer steps during treadmill walking, C) total Omron pedometer steps during treadmill exercise plus walking (ie, laboratory setting), D) Omron pedometer steps during activities of daily living (ADL), and E) total Omron pedometer steps during treadmill exercise plus ADL (ie, real-world setting). \n\n【38】Steps accumulated during ADL with the Fitbit were significantly greater than those from the Omron ( _P_ < _._ 01). When steps from ADL were combined with steps from treadmill exercise, participants’ total steps in the real-world setting remained significantly higher with the Fitbit compared with the Omron ( _P_ < _._ 01). Using the Omron as the criterion device, the average difference in total steps between monitors in the laboratory setting was −604 (SD, 838) steps, or a 6.0% difference, and the difference in the real-world setting was 1,148 (SD, 1,898) steps, or 11.4% difference. The larger difference observed for total steps in the real-world setting was driven by a 22% difference between monitors during ADL, compared with a 6% difference during treadmill exercise and a 6% difference during treadmill walking between monitors in the laboratory setting.\n\n【39】Bland-Altman plots are presented in Figure 2 for steps during treadmill exercise (A), walking (B), total steps in the laboratory setting (C), ADL (D), and total steps in the real-world setting (E). Limits of agreement were narrower for the treadmill exercise and treadmill walking conditions (SD, 9.3%–10.0% of criterion steps) than for the ADL condition (SD, 29.3% of criterion steps), resulting in narrow limits of agreement for the total steps in the laboratory setting (SD, 8.3% of criterion steps) but wide limits of agreement for total steps in the real-world setting (SD, 18.8% of criterion steps).\n\n【40】Our subanalysis indicated that the difference in steps between devices was greater for women than men during treadmill exercise (341 \\[SD, 461\\] vs 94 \\[SD, 132\\] step difference, _P_ < _._ 001) and walking (567 \\[SD, 583\\] vs 87 \\[SD, 447\\] step difference, _P_ < _._ 001), but not ADL (1330 \\[SD, 1,986\\] vs 1464 \\[SD, 1,667\\] step difference, _P_ \\= .34). No significant differences were found in step discrepancies during treadmill exercise and walking, nor for ADL for exercise and nonexercisers (data not shown). No relationship was observed between either age and difference in total steps ( _r_ \\= 0.003, _P_ \\= .69) or BMI and difference in total steps ( _r_ \\= 0.19, _P_ \\= .12).\n\n【41】Discussion\n\n【42】The overall objective of this analysis was to determine potential step differences accumulated through treadmill exercise, treadmill walking, and ADL on a wrist-worn activity monitor versus a hip-worn pedometer with the goal of accumulating 10,000 steps per day. We found that, compared with a hip-worn pedometer, a wrist-worn activity monitor reported fewer steps during treadmill exercise and walking but overreported steps during ADL. We also found that limits of agreement between devices were narrow when steps were accumulated entirely on a treadmill in a laboratory setting, but greater differences and variability between measures were seen during ADL, similar to how steps are accumulated in a real-world setting. These findings suggest that the types of activities engaged in during free living will affect the magnitude of differences between hip- and wrist-worn monitoring devices. Our subanalysis indicated that sex did influence differences between step counts during treadmill activity (ie, exercise and walking) but not during ADL. Additionally, exercise status did not influence step count differences between devices in any modality (treadmill exercise, treadmill walking, or ADL). Finally, we found that participant demographics, including BMI and age, were not associated with the magnitude of step count difference between devices in our real-world setting.\n\n【43】We found that, compared with a hip-worn pedometer, a wrist-worn activity monitor underestimated steps accumulated in a controlled environment (eg, rhythmic activity) during treadmill walking (lower intensity) or exercise (higher intensity). This finding is consistent with previous reports indicating that wrist-worn activity monitors consistently counted fewer steps than hip-worn devices and hand-tallied step counts in controlled environments (eg, laboratory treadmill exercise) . More specifically, we found that the Fitbit Charge underestimated steps by approximately 6%. This finding was similar to previous a report indicating that other Fitbit models also underestimate steps during treadmill walking by approximately 7% . In an applied sense, this means that to meet daily target step goals (eg, 10,000 steps/d) on a treadmill, individuals would end up walking more than their original goal when using a wrist-worn Fitbit device.\n\n【44】Consistent with previous reports, we found that wrist-worn devices overestimate steps taken during ADL. Available data indicate that wrist-worn devices overestimate steps accumulated in free-living conditions or during ADL, compared with hip-worn devices . For example, a previous study found that steps from an accelerometer were underestimated with the device worn at the hip and overestimated when the device was worn on the wrist, compared with video observation . This finding was attributed to certain activities being considered slower, less repetitive or rhythmic, or less structured, and often involving significant arm movement without corresponding stepping, analogous to ADL in a free-living setting. Conversely, previous work with the hip-worn pedometer used in this study resulted in step underestimations in free-living settings, due to typical household activities usually occurring at slower ambulation speeds . Collectively this suggests that activity monitor device and modality of activity should be considered when determining step goals.\n\n【45】Differences in steps between devices appeared to be largely due to device location. For example, a nearly 19% overestimation of steps was reported with a wrist-worn Fitbit compared with a hip-worn accelerometer averaged over a 2-day period . This discrepancy appears to be more location-specific than device-specific because a similar 17% approximate overestimation of accumulated steps averaged over a 7-day period was reported with an accelerometer worn at the wrist versus the waist . These overestimations of steps accumulated with wrist- versus hip-worn devices were slightly higher than the approximate 11% difference we observed in our real-world setting. Although the smaller magnitude of difference we observed may be due to discrepancies in step counts accumulated with waist-worn accelerometers versus pedometers, the potential difference across different types of hip-worn devices appears to be marginal . Rather, the smaller magnitude of difference between wrist- and hip-worn devices in our study is more likely the result of PA modality and total activity level. For example, our real-world setting included structured treadmill exercise, producing more consistent step counts between devices than ADL. Additionally, differences in accumulated step counts appear to be greater as step counts per day increase . Our study design limited daily steps to 10,000, whereas previous investigations under free-living conditions had a higher range of step counts, especially for more active adults . Regardless of the magnitude of difference between wrist- and hip-worn devices in various studies, a consistent finding is that wrist-worn devices report more steps than waist/hip-worn devices (ie, 10%–25%). Unfortunately, this fact could lead wrist-worn device users to think that they have met their PA goals for the day when they indeed have not.\n\n【46】Whereas PA modality impacted step counts, demographics including age, BMI, and exercise status had little to no impact on step counts, regardless of device. In fact, BMI status and age were not associated with step differences between devices. Additionally, although we observed greater steps accumulated among participants classified as exercisers compared with nonexercisers during treadmill exercise, exercise status did not influence step count differences between devices. Step count differences between exercisers and nonexercisers during treadmill exercise was most likely due to the greater distance covered during treadmill exercise among exercisers versus nonexercisers (approximately 2 vs 1.75 miles). Interestingly, sex appeared to impact the difference in step counts between devices only during treadmill activity. More specifically, the discrepancy between step counts on devices was greater for female compared with male participants, during treadmill exercise and walking. Female participants in this study were shorter in stature than male participants, potentially resulting in a shorter stride length. This may have contributed to a shorter arm swing that could have been missed by the Fitbit, contributing to the greater discrepancy we observed between the hip and wrist devices for female participants. Overall, this finding suggests that regardless of age, exercise, and BMI status, individuals who engage primarily in walking where arms can swing freely, as opposed to walking where arms could remain relatively stationary (eg, pushing a grocery cart), can aim to achieve 10,000 steps per day irrespective of their monitoring device placement (hip or wrist). However, individuals who mostly perform nonambulatory ADL should aim for 11,000 to 12,000 steps or more per day again regardless of age, exercise, or BMI status if using a wrist-worn monitor.\n\n【47】Our study has limitations. First, while the use of the Omron pedometer is a noted strength (as the gold standard of step measurement), the accuracy of pedometers is dependent on speed and has shown to yield inaccurate step counts at slower ambulation speeds observed during household activities (eg, vacuuming, sweeping) . Therefore, the Omron may undercount steps in free-living conditions. Additionally, although the wrist-worn activity monitor used (ie, Fitbit Charge) was the most up-to-date at the time of data collection, it has undergone several updates since the time of the study. However, it remains to be a widely used, popular device. Importantly, updated wrist-worn activity monitors have yielded similar results to what we present here . More specifically, wrist-worn devices demonstrate decreased accuracy at lower speeds and for nonambulatory movements and in free-living settings . Another limitation of our analysis is that it included only 1 day of measurement in a laboratory or real-world setting, which may not represent habitual PA patterns. Future research is needed to better understand the accuracy of various activity monitors regarding habitual PA behavior. However, our study design allowed us to examine conditions of a real-world setting aimed at mimicking free-living patterns in addition to the controlled laboratory setting. Importantly, previous work comparing wrist-worn activity monitors to pedometers has largely included either a free-living or laboratory setting . Furthermore, most existing studies included a small and homogeneous population. A notable strength of this work is the large and diverse sample, increasing the generalizability of findings.\n\n【48】Overall, the often-cited 10,000 steps per day recommendation may not be appropriate for everyone. Several factors should be considered for accurate step recommendations, including type of activity performed, type of activity monitor being used, and location of the activity monitor. Other factors to consider include individuals’ goals. For example, based on a population study, men may require 11,000 to 12,000 steps per day and women 8,000 to 12,000 steps per day to maintain a normal weight . These considerations are essential to accurately provide daily step recommendations.\n\n【49】In summary, we found that wrist-worn devices overestimate step count in a real-world setting compared with hip-worn pedometers. The results also showed that in a controlled laboratory setting the wrist-worn device underestimated total step count compared with the hip-worn device. The approximate mean step count difference between the wrist- and hip-worn devices was 1,500 steps per day. The widely used 10,000 step recommendation may not be an accurate messaging tool for all activity monitor users, given the discrepancy in daily step count among wrist- and hip-worn devices. More accurately, accumulating daily steps based on the device and placement, type of activity, and PA goals could have positive impacts on health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7cc97210-afc5-4322-a76a-27689ee12fa6", "title": "High Pathogenicity of Nipah Virus from Pteropus lylei Fruit Bats, Cambodia", "text": "【0】High Pathogenicity of Nipah Virus from Pteropus lylei Fruit Bats, Cambodia\nNipah virus (NiV) is a zoonotic paramyxovirus that was first identified as the cause of an outbreak of encephalitis in humans in Malaysia and Singapore during 1998–1999 . Although NiV infection remains rare in humans, this virus has captured the attention of the public health community and scientists because of its high case-fatality rate, ranging from 40% in Malaysia to >90% in Bangladesh and India . Its high pathogenicity, potential for interspecies transmission, and lack of validated medical countermeasures led to the classification of NiV as a Biosafety Level 4 (BSL-4) pathogen. In 2015, the World Health Organization (WHO) listed NiV as a priority pathogen because of its probability of causing severe outbreaks and subsequently placed NiV on the WHO Blueprint list of priority diseases . This designation was strengthened by the NiV outbreak in Kerala, India, where the virus had not previously been reported .\n\n【1】NiV is a member of the _Henipavirus_ genus, together with Hendra virus, which first emerged in Brisbane, Queensland, Australia, in 1994 , and the nonpathogenic Cedar virus, which was discovered in Australia in 2009 . In addition, full-length henipa-like viral sequences were found in fruit bats in Africa  and in rats in China (Moijang virus) . As part of the Mononegavirales, NiV has a nonsegmented, negative-sense, single-stranded RNA genome. _Pteropus_ fruit bats, commonly known as flying foxes, are considered the henipavirus natural reservoir; these bats, when infected with NiV, do not seem to display any apparent clinical signs of disease .\n\n【2】Only 2 NiV lineages are known to circulate in Asia and cause disease in humans: NiV-Malaysia and NiV-Bangladesh. In India and Bangladesh, NiV transmission from bats to humans was shown to occur through the consumption of raw date palm juice or fruits contaminated with the saliva or urine from fruit bats . In addition, as observed during the outbreaks in Malaysia in 1998 and 1999, transmission can occur via contact with infected domestic animals, such as pigs, that act as amplifying hosts of the virus . Interhuman transmission has been reported in Bangladesh and India in >50% of NiV outbreaks .\n\n【3】Clinical manifestations of NiV infection in humans can range from asymptomatic to acute respiratory syndrome, generalized vasculitis, and fatal encephalitis. Among the few survivors of NiV outbreaks, long-term neurologic problems have been reported; 20% of patients have residual neurologic sequelae , and NiV-Malaysia–infected patients experienced relapse and late-onset encephalitis .\n\n【4】NiV and henipa-like viruses have been detected molecularly or serologically in _Pteropus_ bats in different countries of Asia  and Africa , Australia , and Brazil , and the worldwide distribution of fruit bats poses a continuous threat to another spillover with possible pandemic potential . However, since 1998, all NiV cases in humans have been identified in Malaysia, India, Bangladesh, and the Philippines . Human cases of NiV have not been reported in Cambodia or neighboring countries since the first serologic detection of NiV in Cambodia and isolation of CSUR381 in _Pteropus lylei_ bats in Cambodia in 2003 . Initial phylogenetic analyses of the nucleoprotein and attachment glycoprotein of CSUR381 suggested the virus was part of the NiV-Malaysia genotype . However, a full-genome characterization and phylogenetic analysis have not been performed. In addition, the growth dynamics and virulence of this virus have not been analyzed, thus limiting more comprehensive evaluation of this virus’s pathogenic potential. In this study, we performed an in-depth characterization of CSUR381, including its pathogenicity both in vitro and in vivo, ultimately to assess the outbreak risk that isolates circulating in Cambodia pose in Southeast Asia.\n\n【5】### Materials and Methods\n\n【6】##### Viruses\n\n【7】In this study, we used 3 different NiV isolates: the NiV isolate CSUR381 from Cambodia , NiV-Malaysia isolate UMMC1 , and NiV-Bangladesh isolate SPB200401066 . CSUR381 was isolated from _P. lylei_ bat urine at the Pasteur Institut in Battambang, Cambodia, in 2003 , and the other 2 isolates were obtained from infected patients. We produced and titrated all viruses on Vero E6 cells.\n\n【8】##### Full-Genome Sequencing\n\n【9】We amplified and titrated the Cambodia NiV isolate on Vero cells and, after the second cell passage, extracted viral RNA from supernatant using the QIAamp Viral RNA Mini Kit  according to the manufacturer’s instructions. We treated samples with DNase, purified and quantified RNA using the QuantiFluor RNA System , and analyzed using the AATI High Sensitivity Genomic DNA Analysis Fragment Analyzer . Then, we amplified viral RNA using the Single Primer Isothermal Amplification Kit . We prepared a library using Ovation Ultra Low (NuGen), which gave us average DNA fragment sizes of 382–426 bp. We then sequenced the whole genome of CSUR381 using MiSeq Nano v2 , which produced read lengths of 2 × 150 nt.\n\n【10】We carried out genome assembly de novo using SPAdes  and predicted open reading frames with Prodigal . We reconstructed the missing 5′ and 3′ extremities using the 5′ RACE System for Rapid Amplification of cDNA Ends  according to the manufacturer’s recommendations. We performed reverse transcription with SuperScript II Reverse Transcriptase (Invitrogen) using the primer GSP1-leader (5′-GACCATTGATCCAACATC-3′) to recover the viral leader sequence and GSP-trailer (5′-AAAGTGATTGTCTACTCACT-3′) to recover the trailer sequence. After column purification, we tailed cDNA sequences with cytidine triphosphate and terminal deoxynucleotidyl transferase. Last, we amplified dC-tailed cDNA using the Abridged Anchor Primer provided in the 5′ RACE System for Rapid Amplification of cDNA Ends Kit and primers nested-GSP2-leader (5′-TACAGCTTCAATGTCTGGGTCATT-3′) to amplify the viral leader sequence, and nested-GSP2-trailer (5′-CAAGTTCAAGGACACCAAAAGT-3′) to amplify the viral trailer sequence. We sequenced PCR products using Sanger technology and submitted the complete genome sequence of CSUR381 to GenBank .\n\n【11】##### Phylogenetic Analyses\n\n【12】Using the ClustalW algorithm , we performed multiple alignments for complete genomes and individual gene sequences. We implemented and manually checked the quality of alignments using BioEdit version 7.2.6  and conducted genomic characterization and evolutionary analyses in MEGA version 7.0.26 . After determining the best DNA model to use for each alignment, we constructed maximum-likelihood phylogenetic trees for complete NiV genomes and all virus coding sequences. For statistical support, we used 500 bootstrap replicates for the analysis of the complete genome and 1,000 replicates for analyses of each gene.\n\n【13】##### Cell Lines and Infection\n\n【14】We cultured NCI-H358 (human bronchioalveolar carcinoma) and Vero E6 (African green monkey kidney) cells in Dulbecco’s modified Eagle medium (DMEM) with GlutaMAX  supplemented with 1% penicillin-streptomycin (10,000 U/mL), 1% L-glutamine, and 10% heat-inactivated (56°C for 30 minutes) fetal calf serum (FCS). We cultured human pulmonary microvascular endothelial cells (HPMECs)  in endothelial cell growth medium . We incubated all these cell lines at 37°C with 5% carbon dioxide; all cell lines, including the _Pteropus_ cell line described in the next paragraph, tested negative for _Mycoplasma_ spp. by the MycoAlert kit .\n\n【15】We generated a _Pteropus_ flying fox cell line using a skin biopsy from the wing membrane of a female _P. giganteus_ (also known as _P. medius_ and flying fox) bat  of the order Yinpterochiroptera. Biopsies were collected from bats by Tiergarten Schönbrunn (Vienna, Austria) staff during regular veterinary checkups following appropriate guidelines to minimize animal stress. The biopsies were washed with sterile phosphate-buffered saline and transferred into Freezing Medium Cryo-SFM (PromoCell), and sample vials were put on dry ice for shipment to Centre International de Recherche en Infectiologie in Lyon, France. To obtain primary cell cultures, we fractionated the biopsies into petri dishes, harvested the homogenates, and incubated them at 37°C with 5% carbon dioxide in DMEM/F-12  supplemented with 10% fetal calf serum FCS, 1% L-glutamine (200 mM), 1,000 U/mL of penicillin, 1,000 U/mL of streptomycin, and 2.50 µg/mL amphotericin B (Gibco). We subsequently immortalized primary cells using the lentiviral vector SV40 large T-antigen produced at Genetic Analysis and Vectorology Platform (AniRA, École Normale Supérieure de Lyon, Lyon). We evaluated different clones on the basis of their morphologic stability and transfectability using jetPRIME kit . We confirmed immortalization of clones by detecting large T-antigen inserts by reverse transcription PCR (RT-PCR). We cultured the final _Pteropus_ cell line, which we designated PATGV1.12, in DMEM GlutaMAX supplemented with 10% heat-inactivated FCS. We additionally confirmed that this cell line was derived from _P. giganteus_ bats by sequencing the mitochondrial region D-loop  and nuclear introns ACOX2, COPS7A, BGN, ROGD1, and STAT5A, which has been suggested to be pertinent for distinguishing among closely related bat species .\n\n【16】We infected cells in 12-well plates at 80% confluence with a multiplicity of infection (MOI) of 0.3. For virus replication kinetics studies, we took 4 time points postinfection into consideration: 0 h, 24 h, 48 h, and 72 h. We performed infections in BLS-4 facility Jean Mérieux (Lyon). For each time point, we collected cell lysates according to validated BSL-4 procedures. We collected supernatants and kept them at −80°C until titration by plaque assay on Vero E6 cells.\n\n【17】##### Pseudotyping of Vesicular Stomatitis Virus and Evaluation of Cell Permissiveness\n\n【18】We used rVSVΔG-RFP (a recombinant vesicular stomatitis virus \\[VSV\\] in which the envelope glycoprotein G gene is replaced with the red fluorescent protein gene)  to generate pseudotyped VSVs harboring different combinations of NiV envelope glycoprotein G (attachment protein) and F (fusion protein) on their surfaces. Complementing rVSVΔG-RFP–infected cells with NiV glycoproteins expressed in trans, we were able to produce stocks of pseudotyped VSVs identical in their genetic background and differing only in the nature of their surface glycoproteins. Because the infectivity of rVSVΔG-RFP pseudotypes is restricted to a single round of replication, this tool is largely used for studying viral entry for a broad range of highly pathogenic viruses .\n\n【19】To create the pseudotypes, we cloned the NiV glycoprotein G and F genes from RNA isolated from CSUR381, UMMC1, and SPB200401066 into 6 separate pCAGGS plasmid vectors. We transfected these 3 plasmid pairs separately into BSR-T7 cells using TransIT-LT1 Transfection Reagent . We infected cells with rVSVΔG-RFP 16 h after transfection to produce a pseudotyped VSV for each NiV isolate. We collected supernatants at 24 h postinfection and concentrated pseudotyped VSVs by ultracentrifugation (28,000 rpm for 2 h at 4°C). We titrated these viruses on Vero cells. To evaluate viral entry into different cell lines, we performed infections in 24-well plates using 80% confluent, adherent cells and a 1-h contact between virus and cells. We determined the percentage of cells infected 6 h postinfection by quantifying cells expressing RFP via flow cytometry on a BD LSRFortessa .\n\n【20】##### RNA Extraction and Real-Time RT-PCR\n\n【21】At the indicated time points, we collected cells and extracted RNA using the NucleoSpin RNA Kit  according to the manufacturer’s instructions. We assessed the yield and purity of extracted RNA using the DS-11-FX spectrophotometer . We reverse transcribed extracted RNA using the iScript Select cDNA Synthesis Kit  and performed real-time PCR using Platinum SYBR Green qPCR SuperMix-UDG (Invitrogen) on a StepOnePlus Real-Time PCR System . As previously described , we amplified the NiV nucleoprotein gene and the _Pteropus_ glyceraldehyde 3-phosphate dehydrogenase housekeeping gene using forward primer 5′-ATCATCCCTGCTTCTACT-3′ and reverse primer 3′AGGTCAGATCCACAACT-5′. We analyzed quantitative RT-PCR results using StepOne version 2.3 (Applied Biosystems).\n\n【22】##### Experimental Infection of Hamsters\n\n【23】We obtained 2-month-old male golden hamsters ( _Mesocricetus auratus_ ) from Janvier Labs . We housed hamsters in a BSL-4 containment facility (INSERM P4, Jean Mérieux, Lyon) and handled them according to the regulations for animal maintenance of France. We treated hamsters with isoflurane anesthesia before manipulations. We subcutaneously infected 2 groups of 6 hamsters with a high dose (13,500 PFU/animal) of either the NiV-Malaysia or Cambodia NiV isolate and followed hamsters daily to record their body temperature and weight. The regional ethics committee for animal experimentation (Lyon) approved these animal experiments.\n\n【24】### Results\n\n【25】##### Full-Genome Characterization and Phylogenetic Analyses\n\n【26】Analysis of the assembled viral sequence of CSUR381 showed a total genome length of 18,246 nt, similar to the lengths of NiV isolates reported in Malaysia. The nucleotide composition was 27.8% T or U, 18.4% C, 33.6% A, and 20.2% G; total GC content was 38.6%. To investigate genetic relationships between CSUR381 and other henipaviruses, we constructed distance matrices for the complete genome and for each gene using the p-distance method. When we compared the sequence of CSUR381 with those of other NiVs available in GenBank, the most similar sequences (with 97.7% nucleotide identity) were from NiV-Malaysia human isolates (GenBank accession nos. NC002728.1 and AY029768.1). We also calculated nucleotide identity and amino acid homology for each of the 6 structural genes . Genetic pairwise comparisons with other NiV isolates showed the lowest nucleotide identity and amino acid homology for phosphoprotein (087.1/82.7%) and highest for matrix protein (98.9/99.4%).\n\n【27】Using the maximum-likelihood method, we constructed phylogenetic trees on the basis of the complete genome  and the nucleocapsid gene . The general time-reversible model for the complete genome and the Kimura 2-parameter model for the nucleocapsid gene were predicted to be the best for performing those particular phylogenetic analyses. CSUR381 clustered with the monophyletic group of the NiV-Malaysia genotype for both the whole genome and nucleocapsid gene; bootstrap support was >98% in all cases, confirming the previous partial genomic characterization of CSUR381 . We then generated phylogenetic trees for each of the coding sequences of the 6 NiV structural proteins, which gave equivalent results .\n\n【28】Multiple alignment of the henipavirus phosphoprotein gene  revealed high conservation of the editing site (5′-AAAAAGGG-3′) in CSUR381, similar to other NiV and Hendra virus isolates and different from Cedar virus, a nonpathogenic virus isolated from a _P. alecto_ bat in Australia . This finding suggests that CSUR381 might produce the nonstructural proteins V and W, capable of interacting with the host innate cellular immune response . Comparisons of the deduced V, W, and C amino acid homologies between CSUR381 and other known NiVs showed a variation of 88%–100% .\n\n【29】##### Evaluation of Virus Entry\n\n【30】We next determined the cellular permissiveness of a human endothelial cell line (HPMEC), a human respiratory epithelial cell line (NCI-H358), the newly generated _Pteropus_ bat cell line (PATGV1.12), and Vero cells to CSUR381 compared with the NiV-Malaysia (UMMC1) and NiV-Bangladesh (SPB200401066) isolate using pseudotyped rVSVΔG-RFP viruses. Cell lines were infected for 1 h at an MOI of 0.3. The percentages of cells infected were analyzed by flow cytometry 6 h after infection , and results from HPMEC, NCI-H358, and PATGV1.12 were normalized to the findings from Vero. All tested cell lines were permissive to infection with all 3 viruses tested. Entry of NiV pseudotypes into the bat cell line PATGV1.12 and human respiratory epithelial cell line was similar. Compared with the NiV-Malaysia and NiV-Bangladesh pseudotypes, the CSUR381 pseudotyped virus showed higher but not significantly increased entry into the 3 tested cell lines (1-way analysis of variance).\n\n【31】We further analyzed the amino acid sequences of the F and G proteins of the 3 viruses by multiple alignment. The glycosylation site (N529/Q530/T531)  and ephrin-B2 and ephrin-B3 binding sites  in the G attachment protein were preserved . In addition, multiple alignments showed that the F cleavage site was preserved among all analyzed NiV isolates . Last, an analysis of the predicted N-terminal and C-terminal heptad-repeat regions within the F protein, which are needed for NiV fusion , showed high conservation, and compared with NiV-Malaysia and NiV-Bangladesh, only 1 aa difference (V159→I) was detected in CSUR381 . Altogether, the high conservation of the NiV glycoproteins and results from pseudotype virus studies suggest that CSUR381 can enter target cells at least as well as NiV-Malaysia and NiV-Bangladesh.\n\n【32】##### Replication of NiV Isolates in Different Cell Types\n\n【33】To further evaluate the virulence of CSUR381, we compared the replication kinetics of this virus with those of the NiV-Malaysia and NiV-Bangladesh isolates. We infected cell types known to be primary targets of NiV in humans, pulmonary endothelial (HPMEC) and bronchioalveolar epithelial (NCI-H358) cells, and the bat cell line PATGV1.12 at an MOI of 0.3 . NiV RNA synthesis was highest in HPMEC, where NiV-Bangladesh replicated the best, although a similar level of RNA and infectious virus particle production was observed for all 3 viruses . In accordance with virus entry studies , virus replication was also observed in PATGV1.12 . Differences among the 3 tested NiV isolates were observed only in NCI-H358, where NiV-Malaysia RNA synthesis was significantly increased (p<0.001 by 2-way analysis of variance) compared with NiV-Bangladesh, provoking remarkable cytopathic effects . The formation of giant multinucleated cells, a hallmark of NiV infection, were already visible at 24 hours postinfection in all cell types and further developed during the course of the infection . Vero cells showed the most visible cytopathic effects, probably because of their interferon incompetence .\n\n【34】##### Experimental Infection of Hamsters\n\n【35】We compared the pathogenicities of CSUR381 and the NiV-Malaysia isolate using the golden hamster animal model . We infected 6 hamsters with either CSUR381 or the NiV-Malaysia isolate and followed them for clinical signs of infection. At 6 days postinfection, the first neurologic signs (which included paralysis and trembling limbs) were observed in both groups; their presentation rapidly evolved toward breathing difficulties and prostration. Weight reductions were evident in several animals in the late stages of infection , and decreases in body temperature were found in a few hamsters . At 7 days postinfection, 100% lethality was observed in the CSUR381 group. In the Malaysia group, 1 animal survived until 10 days postinfection ; however, the difference between the 2 groups was not significant. These results demonstrate similar lethality of the 2 analyzed NiV isolates, supporting our other data and suggesting CSUR381 has a high pathogenic potential.\n\n【36】### Discussion\n\n【37】In this study, we performed a molecular and genetic characterization of CSUR381, a NiV isolated from _P. lylei_ bats in Cambodia. Furthermore, we analyzed its pathogenicity compared with those of 2 other NiV isolates derived from human patients from the Malaysia and Bangladesh outbreaks. Our results highly suggest that CSUR381 is part of the NiV-Malaysia genotype. Further phylogenetic comparisons with other NiV isolates demonstrated 83%–99% amino acid homology for each of the 6 structural proteins. In addition, the editing site of the phosphoprotein gene was preserved, suggesting possible production of the nonstructural V and W proteins known to be involved in counteracting the host innate immune system and thus contributing to pathogenicity of CSUR381 .\n\n【38】Our virus entry studies showed highly similar results among the NiVs tested. All isolates entered _Pteropus_ bat and human cell lines at similar levels; high conservation of the NiV entry receptors (ephrin-B2 and ephrin-B3)  might be responsible for the observed results. Our data also indicate that CSUR381 enters all tested cell types as well as the other 2 NiV isolates tested, suggesting that virus entry is not a limiting factor preventing CSUR381 spillover from bats to humans. In addition, all 3 tested NiV isolates infected cells and replicated in bat and human cell lines at similar levels. Results of infections with CSUR381 in hamsters additionally strengthened the notion that CSUR381 is possibly similar pathogenically to the tested NiV-Malaysia strain, which caused fatal outbreaks in Malaysia .\n\n【39】Although NiV has been shown to circulate in Cambodia , Thailand , and Vietnam , transmission to humans or domestic animals has not been reported in these countries. According to our results, the absence of detected outbreaks in this region cannot be attributed to lower pathogenicity of the circulating NiVs; our results suggest that other factors probably contribute. However, the NiV isolate presented in this report has been the only live NiV isolated in this region, and the existence of other NiVs with different pathogenic potentials cannot be excluded.\n\n【40】In Cambodia, _P. lylei_ bats were found to often forage in residential areas and visit palm trees used in the region as a source of date palm sap; thus, opportunities abound for bats to interact with humans and livestock in this country . Bat colony migration toward urban sites is further enhanced by the presence of hunters in rural areas  and deforestation (causing consequent damage to roosting trees and food sources) . Contamination of palm sap, which is consumed raw by persons in the region, with bat urine, saliva, or feces was found to be a major route of NiV transmission to humans during annual outbreaks in Bangladesh .\n\n【41】Diverse agricultural practices in Southeast Asia could also play a role in NiV regional ecodynamics, potentially favoring easier NiV spillover in some countries over others. High-intensity pig farming was recognized as a major risk factor for outbreaks in Malaysia during 1998–1999; because of the low-scale pig production ongoing in Cambodia , the risk for NiV transmission from _Pteropus_ spp. to domestic animals and humans in this country might be reduced.\n\n【42】Unrecognized NiV outbreaks might have occurred in Cambodia and neighboring countries; hospital-based surveillance in Bangladesh was shown to have missed nearly half of the NiV outbreaks in that country since the first reported virus emergence . Interdisciplinary approaches are certainly required to identify these outbreaks and the drivers of NiV emergence , and regular testing of patients with encephalitis in Cambodia and neighboring countries could provide additional insight. Our study contributes to the assessment of the risk for NiV outbreaks in Asia. Our findings can be used to help target adequate preventive measures, which could ultimately help reduce the risk for NiV emergence.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8a3fd108-2a16-412d-9209-6467ba2c6fef", "title": "Lack of Evidence for Cloramphenicol Resistance in Neisseria meningitidis, Africa", "text": "【0】Lack of Evidence for Cloramphenicol Resistance in Neisseria meningitidis, Africa\n**To the Editor:** High-level chloramphenicol resistance has been reported in 11 epidemiologically unrelated Neisseria meningitidis serogroup B strains in Vietnam and in a single strain in France, all isolated between 1987 and 1996 . Resistance was mediated by a chloramphenicol acetyltransferase (Cat) encoded by a catP gene homologous to Clostridium perfringens transposon Tn4451. While used infrequently in industrialized countries, chloramphenicol is often used to treat patients with meningococcal disease in Africa, especially during epidemics, when it frequently becomes the drug of choice because it can be administrated intramuscularly .\n\n【1】To evaluate the presence of meningococcal chloramphenicol-resistant isolates in Africa, we assessed the frequency of the catP gene in 33 N. meningitidis strains of serogroup A selected from the collection of the Centers for Disease Control and Prevention's Epidemic Investigations Laboratory. The isolates, selected to give the maximum geographic and chronological representation, were collected during 1963 to 1998 from Chad, Egypt, Gambia, Ghana, Niger, Nigeria, South Africa, Tanzania, and Uganda, mostly during outbreaks. Thirteen (39.3%) of the strains were isolated during the 1990s, when chloramphenicol resistance was first described in Vietnam. All isolates were characterized by multilocus enzyme electrophoresis and represented four major electrophoretic subgroups . Chloramphenicol and penicillin MICs were determined for all isolates, according to the recommendations of the National Committee for Clinical Laboratory Standards, by the broth microdilution method using Mueller-Hinton broth with 5% lysed horse blood incubated in 5% CO 2  . All isolates were susceptible to both chloramphenicol (MIC <2 µg/mL) and penicillin (MIC <0.06 µg/mL). In addition, we tested all isolates for the presence of catP by polymerase chain reaction (PCR) using primers A, B, C, and D . Primers A and B, designed from the sequence of catP, amplify a 300-bp fragment only in chloramphenicol-resistant isolates. Primers C and D, designed on the basis of meningococcal sequences flanking the Tn4451-like insertion, amplify ~1200-bp fragment in resistant isolates and ~200-bp fragment in susceptible strains. Strain LNP13947 (kindly provided by Marc Galimand) was used as a positive control.\n\n【2】The catP gene was not detected in 32 of 33 N. meningitidis serogroup A strains. One isolate that was negative with primers C and D tested positive with primers A and B (M2786, Nigeria, 1963), which could suggest that catP was present but in a different location in the meningococcal genome. However, the chloramphenicol MIC of that strain was 2 µg/mL (susceptible). Repeated attempts to sequence the A/B amplicon were not successful with either primers A and B or another set of primers internal to primers A and B, implying that only a portion of the catP gene was present or (even more likely, given the conserved nature of this gene) that the PCR result was a false-positive.\n\n【3】Chloramphenicol resistance was first described in meningococcal serogroup B isolates , but only serogroup A strains were included in this study since A is the most prevalent serogroup in Africa. (It accounts for most epidemics in Sub-Saharan regions.) Although our small sample size limited the chances of detecting a rare event, the data suggest that chloramphenicol resistance in Africa is relatively infrequent and that chloramphenicol is still an appropriate agent to treat meningococcal disease.\n\n【4】The acquisition of plasmids encoding Cat, which enzymatically inactivate choramphenicol, is the most common mechanism of resistance in gram-positive and gram-negative organisms. The catP gene has been found on various bacterial chromosomes and conjugative plasmids as part of the transposable element Tn4451. This transposon is derivative of the Tn6338 that contains six genes, the largest of which, TnpX, is required for the excision of the transposon in both Escherichia coli and C. perfringens . The Tn4451 derivative that lacked the functional TnpX gene was completely stable in both organisms because it had lost mobility as a result of these internal deletions . The finding of catP in N. meningitidis within such a truncated immobile transposon  and the possibility of transfer of this type of resistance among highly transformable organisms such as Neisseria spp. are of great concern. Were the transposon to become a stable part of the meningococcal genome, it could potentially be easily exchanged. Interspecies recombination between antibiotic-resistant genes of N. meningitidis and commensal Neisseria spp. has occurred in penicillin- and sulfonamide-resistant meningococci . A similar occurrence may be possible for N. meningitidis chloramphenicol resistance in Africa or other continents where this antibiotic is routinely used for treatment of patients with meningococcal diseases. Studies have not yet demonstrated the clinical significance of chloramphenicol resistance caused by the catP gene in meningococci. However, it is possible that, in developing countries, patients whose illness does not respond to antimicrobial agents may not be detected, or their isolates may not be obtained. Screening a selection of isolates for catP may allow early detection of chloramphenicol-resistant strains. Since detection of increasing chloramphenicol resistance could change recommendations for antimicrobial-drug therapy, surveillance for antimicrobial-drug resistance should be encouraged.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1d10d8d6-1190-4d7b-8ea1-7e7ddea3711d", "title": "Determinants and Drivers of Infectious Disease Threat Events in Europe", "text": "【0】Determinants and Drivers of Infectious Disease Threat Events in Europe\nThe Middle East respiratory syndrome coronavirus (MERS-CoV) outbreak and the large Ebola outbreak in West Africa are striking examples of how emerging and reemerging infectious diseases can threaten international public health and strain governmental resources . Historically, novel pathogens have emerged and reemerged repeatedly in human populations and affected public health; similarly, pathogens that have been present in a population at low levels have increased rapidly in incidence or geographic range with equally grave consequences . The context of infectious disease emergence has changed over the centuries, but Europe has remained and even intensified as a hot spot for emerging infectious diseases over recent decades . Many of the fundamental and basic determinants of emerging infectious diseases have persisted over time, but dynamic global trends provide more opportunities for emerging infectious diseases to occur and expand swiftly .\n\n【1】A 2008 study, which was conducted by the European Centre for Disease Prevention and Control (ECDC) and based on expert consultation and literature review, projecting how the risk of emerging infectious diseases in Europe will be shaped in the future determined that drivers can be categorized into 3 main groups: globalization and environment, sociodemographic, and public health systems  . Although the 3 groups are somewhat artificial and not entirely mutually exclusive, they can serve as a framework for the interpretation of infectious disease threat events (IDTEs) in Europe.\n\n【2】We conducted this study to identify, differentiate, and rank drivers of observed IDTEs in Europe detected through ECDC epidemic intelligence activities. By doing this, the disparate drivers that act on different dimensions and different scales can be disaggregated. A ranking of the relative importance of these drivers can help prioritize risk-based surveillance to anticipate disease emergence and spread . The effect of IDTEs on public health can be attenuated by strengthening the detection of and early response to the threats. However, more important, the likelihood of IDTEs originating in the first place can be reduced by intervening directly on their underlying drivers. Mitigation strategies to reduce the causes rather than the effects of IDTEs can be more cost effective .\n\n【3】### Event-Based Surveillance\n\n【4】Persons, services, goods, capital, and microbes are free to move across borders of the European Union (EU), which currently has 28 member states and an estimated population of 508.2 million. The ECDC is an EU agency with a mission to identify, assess, and communicate current and emerging threats to human health posed by infectious diseases. This charge is accomplished through epidemic intelligence, a process to detect, verify, analyze, assess, and investigate events that may represent a threat to public health. These activities are conducted by a team of >10 epidemiologists in the Emergency Operation Center at ECDC. The daily activity of epidemic intelligence at ECDC involves active or automated web searches from confidential and official sources (e.g. EWRS \\[Early Warning and Response System\\], ProMED \\[Program for Monitoring Emerging Diseases\\], MediSys \\[Medical Information System\\], and GPHIN \\[Global Public Health Intelligence Network\\]), as well as individual reports from the EU and European Economic Area (i.e. EU countries plus Iceland, Liechtenstein, and Norway) member states. The sources of epidemic intelligence information include several websites and a large number of webpages retrieved through specialized search engines. Additional information is gathered through direct contact with epidemiologists and health authorities in the EU and abroad.\n\n【5】Data collection for epidemic intelligence at ECDC was standardized in 2008; thus, we analyzed the epidemiologic characteristics of each IDTE in Europe from July 1, 2008, through December 31, 2013. For each IDTE the following data are routinely collected by ECDC: type of disease or pathogen, geographic location of source of infection, source of infection (e.g. contaminated bean sprouts), duration of the epidemic or of surveillance activities, number of countries affected by the event, number of cases, and number of deaths. IDTEs included in this study were restricted to outbreaks affecting >5 persons in the EU (excluding Croatia, which was not yet an EU member). Persons infected abroad and returning to the EU were included in our analysis. The IDTEs were sorted into 10 categories .\n\n【6】Information about the underlying drivers of these IDTEs was extracted from several sources: the Communicable Disease Threats Report (a weekly bulletin generated by the epidemic intelligence team at ECDC), epidemiologic reports and communications, rapid risk assessments, threat assessments, mission reports, and associated peer-reviewed publications retrieved from PubMed. The IDTE drivers were organized into 3 categories: globalization and environment, sociodemographic, and public health systems  . Expert assessment, performed by the authors, was used to evaluate the quality and validity of the information regarding the drivers. Discordant assessments were resolved by consensus.\n\n【7】Drivers were subjected to descriptive analyses (individually or in combinations), including frequency rates and ranking of the drivers, in relation to different types of IDTEs. Euclidian distances (based on how the IDTE types occurred with the driver category in the empirical data) were also calculated between each 1 driver pair . The calculation of the distance between 2 drivers was derived from the multidimensional driver space, based on whether the drivers were present  or absent  during the emergence of an IDTE. We then used a set of dissimilarities for the distances between drivers to perform a hierarchical cluster analysis . Dissimilarity distances between clusters were recomputed by the Lance–Williams updated formula, according to the average clustering method, by using the statistical computing program R and the algorithms in the R Stats Package . The driver terrorism was not included here because only 1 threat event was linked to the driver. Dissimilarity between clusters of IDTE drivers were described graphically by using tree diagrams (dendrograms) to visualize the similarity and dissimilarity of drivers in the occurrence of the IDTE. The clustering algorithm applied to binary data has been reported to perform well . The distance between clusters measure how similar, or dissimilar, different drivers are in their co-occurrences in outbreaks.\n\n【8】### Determinants and Drivers of IDTEs\n\n【9】Of 274 IDTEs that occurred within the EU during July 2008–December 2013, a total of 116 met the study inclusion criteria. Foodborne and waterborne IDTEs were the most frequently occurring events (n = 48), followed by vectorborne and rodentborne IDTEs (n = 27), airborne IDTEs (n = 10) vaccine preventable IDTEs (n = 10), other zoonotic IDTEs (n = 7), injection drug use–associated IDTEs (n = 4), influenza IDTEs (n = 4), healthcare-associated IDTEs (n = 3), multidrug resistance–associated IDTEs (n = 2), and sexually transmitted IDTE (n = 1). The driver category that was by far the most frequently involved in single IDTEs was globalization and environment (61%), followed by the public health system failure (21%) and sociodemographic (18%) groups. The individual driver travel and tourism was linked to 9 of the 10 IDTE categories, and the vulnerable groups and lifestyle driver categories were linked to 7 and 6 IDTE categories, respectively. Most IDTEs had a combination of drivers: 51% had 2 drivers, and 25% had 3 drivers . Foodborne and waterborne diseases and vectorborne diseases were the most commonly occurring drivers in combinations of 2 and 3 drivers. The most common driver combinations were travel and tourism in combination with food and water quality and global trade in combination with food quality, both of which caused foodborne and waterborne diseases .\n\n【10】### Foodborne and Waterborne IDTEs\n\n【11】The foodborne and waterborne category included all types of diseases transmitted through food or water  and was responsible for the most IDTEs. The global burden of foodborne diseases is considered to be substantial, although no current estimates exist . The most common cause of the observed foodborne and waterborne outbreaks in Europe was norovirus, followed by hepatitis A and _Escherichia coli_ (hemolytic uremic syndrome and Shiga-like toxin–producing _E. coli_ infections) . The strongest driver in this IDTE group was food and water quality, implicating the food industry and water treatment infrastructure, often in combination with the travel and tourism or global trade drivers . An example was the norovirus epidemic that affected >11,000 schoolchildren in 6 countries; the origin of the epidemic was traced to contaminated frozen strawberries .\n\n【12】### Vectorborne and Rodentborne IDTEs\n\n【13】Nearly half of the 27 IDTEs within the vectorborne and rodentborne IDTE category were caused by West Nile virus (WNV) infections. Four of these IDTEs consisted of the first autochthonous WNV cases in 4 different European countries and 1 large outbreak in southeastern Europe with >260 cases . WNV infection and malaria are notifiable diseases in the EU and, thus, subject to indicator surveillance; however, special threat events are picked up by event-based surveillance as well. The natural environment driver was present in all WNV infection events; in half of those events, it was present with the climate driver, and in 6 events, it was present with the surveillance and reporting failure driver . This finding is consistent with other findings that show environmental and climatic determinants play contributing roles in WNV infection outbreaks . Of 7 malaria threat events, 5 included autochthonous cases (in Spain, Greece, and Belgium). Our data also included the large dengue outbreak in Madeira, Portugal, with >2,000 cases  driven by climate, natural environment, and travel and tourism . A large outbreak of hantavirus infections in Germany in 2010 was attributed to bank vole ( _Clethrionomys glareolus_ ) populations, which had increased substantially due to excessive seed production the previous year ; human behavior (e.g. outdoor activities in summer); dust contaminated with rodent excreta following dry and warm weather; and heightened awareness, with better diagnosis and reporting.\n\n【14】### Other Zoonoses IDTEs\n\n【15】Q fever, psittacosis, and diseases caused by cowpox virus and _E. coli_ (with an unusual transmission pathway, e.g. contaminated farm soil and petting of contaminated animals) were included in the other zoonoses IDTE category. Outbreaks often occurred among farm and animal workers. However, the 2007–2009 Q fever outbreak in the Netherlands affected >3,000 persons in densely populated areas (demographic driver) situated in close proximity to commercial dairy goat farms (climate and animal health drivers)  . Contaminated dust particles from ruminant farms probably caused airborne transmission of _Coxiella burnetii_ , the causative agent of Q fever. Vaccination, hygiene measures, and culling of pregnant animals on affected farms eventually ended the outbreak . Due to persistence of _C. burnetii_ in the environment, continued surveillance for Q fever is warranted.\n\n【16】In autumn 2009, an outbreak of 93 cases of _E. coli_ O157 (verotoxin-producing _E. coli_ ) infection in southern England was related to environmental and animal exposure on a petting farm visited by families and children (tourism, vulnerable groups, animal health, and natural environment drivers)  . Horizontal integration of the human, animal, and environmental health sectors, according to the One Health approach, can tackle some of these public health predicaments .\n\n【17】### Vaccine-Preventable IDTEs\n\n【18】Ten IDTEs, including measles, mumps, rubella, and pertussis outbreaks, were reported for the vaccine-preventable IDTE category. A measles outbreak in Bulgaria in 2009–2010, which affected predominantly migrant and hard-to-reach Roma populations, resulted in >24,000 cases and 24 deaths in 1 year . The drivers responsible for this outbreak were a combination of prevention, lifestyle, migration, social inequality, and healthcare system . Measles is still endemic in many European countries because of low vaccination coverage among migrants and hard-to-reach populations and vaccine hesitancy. Specific vaccination strategies are often necessary for these populations to protect children and adults from infectious diseases, prevent spread of infection due to crowded living conditions, and ensure continuity of childhood immunization schedules. Failure to vaccinate susceptible populations diminishes herd immunity and may trigger outbreaks . There was no evidence of an increased risk of infectious disease transmission in the host population in Europe during the 2015 influx of migrants or asylum seekers; however, ECDC advocated for the implementation of basic public health measures, health assessments, and vaccination to address the health needs of migrants . This strategy is supported by the findings from this analysis, in which migration was a comparatively infrequent driver of IDTEs, relative to travel and tourism .\n\n【19】### Multidrug Resistance– and Healthcare-Associated IDTEs\n\n【20】Multidrug-resistant tuberculosis cases were identified as single case events and therefore did not meet our inclusion criteria in this study. The events in the multidrug resistance–associated IDTE group consisted of a nosocomial outbreak of multidrug-resistant carbapenemase-producing _Klebsiella pneumoniae_ infection in 2 hospitals in Ireland and 69 infections with New Delhi metallo-β-lactamase-1 carbapenemase-producing _Enterobacteriaceae_ in persons in the United Kingdom with a travel history to India or Pakistan .\n\n【21】The relatively few healthcare-associated IDTEs picked up by ECDC’s epidemic intelligence represent only a fraction of the expected number in Europe. Seven deaths occurred among 49 detected cases; 6 deaths were in newborns who had been infected while in hospitals (healthcare system, vulnerable groups, human-made environment drivers). The number of events and deaths for other event categories was far below the actual number expected for Europe. An ECDC point-prevalence survey of healthcare-associated infections and antimicrobial use in long-term care facilities in Europe showed that ≈4.1 million patients contract a healthcare-associated infection in the EU each year, and ≈37,000 deaths occur annually as a direct consequence of these infections . However, these types of infections were not captured by epidemic intelligence due to reporting disincentives (e.g. legal and financial).\n\n【22】### Injecting Drug Use–Associated IDTE\n\n【23】Reported infections caused by injection drug use were due to botulism, HIV, and anthrax and caused 69 illnesses and 8 deaths . As an example, a contaminated batch of heroin (global trade driver) caused 31 anthrax infections among heroin users in Scotland (vulnerable groups, lifestyle, sociodemographic drivers) .\n\n【24】### Sexually Transmitted IDTE\n\n【25】Only 1 sexually transmitted IDTE was identified by epidemic intelligence. The event was reported from 3 countries and consisted of invasive meningococcal disease among men who have sex with men; the men had been infected while traveling or through contacts from abroad. Of note, however, many sexually transmitted infections tend to be silent and reach endemic levels that are not captured by epidemic intelligence.\n\n【26】### Influenza IDTEs\n\n【27】Rather than registering as recurrent IDTEs, influenza precipitated several influenza outbreaks that were recorded as Public Health Events at ECDC; examples are the avian influenza A(H5N1) outbreaks and the influenza A(H1N1)pdm09 pandemic. Thus, the number of IDTEs attributed to influenza is underestimated. The drivers for the influenza IDTE group of both seasonal and pandemic influenza were travel and tourism, vulnerable groups, social and demographic, and surveillance and reporting.\n\n【28】### Airborne IDTEs\n\n【29】Ten IDTEs were reported for the airborne IDTE category. Most events were due to legionellosis, but the IDTEs also encompassed the emergence of MERS-CoV infections in 2012–2013; a total of 9 MERS cases were reported from the EU . Our analysis identified human-made environments, in particular contaminated cooling towers or spa pools, to be the overarching driver of _Legionella_ infection events . Proper maintenance of the physical infrastructure can prevent these IDTEs. Two drivers, travel and tourism and healthcare system, were identified for the emergence and spread of MERS-CoV into Europe .\n\n【30】### Driver Ranking\n\n【31】An overall frequency ranking of all events ranked the individual contribution of the top 5 drivers in the following order: travel and tourism, food and water quality, natural environment, global trade, and climate. The hierarchical cluster analysis revealed travel and tourism to be separate from all the other drivers; thus, this driver can be considered distinct, indicating that the distribution of IDTEs within travel and tourism is significantly different from the distribution in the remaining clusters . The hierarchical cluster analysis revealed several similarly clustered segments, such as climate and natural environment and migration and social inequality, indicating that these drivers are more related to each other than to the other drivers.\n\n【32】### Limitations\n\n【33】Although sociodemographic and public health system drivers were less frequent in our analysis of IDTEs, they are nevertheless key contributors to the disease burden from infectious diseases in Europe . They may also be more directly amenable to interventions. However, epidemic intelligence detects IDTEs, not endemic infectious diseases, to which these drivers contribute substantially. Epidemic intelligence is heavily influenced by media coverage, geographic focus, length of the epidemic intelligence monitoring cycle, diagnostic procedures, and sensitivity of surveillance systems, among many other factors. The captured events are then filtered and verified before they are assessed and investigated. One event during the study period was categorized as bioterrorism, because sabotage was suspected due to coliform contamination of drinking water tanks at a hotel. If need be, a Public Health Event is declared to initiate control measures.\n\n【34】However, the ECDC screening of IDTEs is not designed to capture infectious diseases that do not reach outbreak levels or are not picked up by event monitoring (e.g. healthcare-associated or sexually transmitted infections). Therefore, our analysis pertains only to the drivers of IDTEs rather than endemic infectious diseases that are not recorded by epidemic intelligence. Long-term monitoring of the incidence, prevalence, or both of notifiable diseases is performed on a national level and reported through a different reporting system, the European Surveillance System at ECDC .\n\n【35】### Discussion\n\n【36】We found globalization and environment to be the most noteworthy driver category for IDTEs in Europe. More specifically, travel and tourism, food and water quality, natural environment, global trade, and climate were the top 5 drivers of all IDTEs identified through epidemic intelligence at ECDC. Among these, travel and tourism proved to be significantly distinct in the hierarchical cluster analysis and cluster dendrogram . In this analysis of epidemic intelligence data, travel and tourism was not only the most distinct but also the most recurrent driver implicated in the emergence of IDTEs. The volume of international travelers on commercial flights with a final destination in Europe has increased steadily over the years; >103 million travelers entered Europe in 2010 alone, and this number will probably continue to grow . International travel from areas with epidemic and endemic diseases has resulted in continuous importation of infected persons into Europe who can, for example, trigger outbreaks of airborne diseases. Similarly, pathogen introduction into competent vector populations can result in local transmission and threaten the safety of the blood supply . Restricting international travel in a globalized world to reduce the likelihood of IDTEs is both unrealistic and undesirable; however, monitoring and modeling air traffic patterns for pathogen importation risk can potentially accelerate early case detection and rapid response and effective control of IDTEs .\n\n【37】Food and water quality was the second most frequent driver of IDTEs in Europe. Suboptimal food safety systems, even if they are distant to the outbreak, become an international public health issue in an interconnected world in which food and humans move freely  . The occurrence of an IDTE can potentially be mitigated by addressing this driver. Fostering multisectorial collaboration between the food industry, public health, and environmental agencies can prevent IDTEs. High-density agricultural practices need to be subjected to stringent farm biosecurity and sanitary practices to prevent multinational outbreaks . Upgrading water treatment and distribution systems can prevent communitywide outbreaks .\n\n【38】Changes in the natural environment are increasing on a nonlinear scale with habitat destruction and loss of ecosystem services . Monitoring and modeling environmental precursors of IDTEs can help to anticipate, or even forecast, an upsurge of IDTEs . The utility of such predictive models has been documented on several occasions: environmental drivers of IDTE with prediction tools have been made available by ECDC through the E3 (European Environment and Epidemiology) Geoportal  .\n\n【39】In summary, we have taken a systematic approach to categorize and rank the underlying drivers of observed IDTEs in Europe to help anticipate, respond to, and recover from probable, imminent, or current impacts of these events. Drivers of IDTEs can arise as epidemic precursors of IDTEs. Monitoring and modeling these drivers can serve as early warning systems of IDTEs and accelerate responses . However, it is desirable to proactively prevent possible public health emergencies rather than respond to IDTE after they have occurred. Thus, the most cost-effective strategy would be to directly tackle the underlying drivers of an IDTE rather than deal with the actual IDTE after the fact . Intervening directly on drivers may prevent the occurrence of IDTEs and reduce the human and economic cost associated with IDTEs.\n\n【40】Professor Semenza works on environmental and social determinants of infectious diseases and is the head of the Scientific Assessment Section at the European Centre for Disease Prevention and Control in Stockholm, Sweden.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "53c85937-7fec-4734-b6d6-c7d73756f2ea", "title": "Characteristics of 263K Scrapie Agent in Multiple Hamster Species", "text": "【0】Characteristics of 263K Scrapie Agent in Multiple Hamster Species\nTransmissible spongiform encephalopathy (TSE) diseases are infectious, fatal, neurodegenerative diseases of the central nervous system that affect a wide variety of mammals, including humans. In the past several decades, 3 new TSE diseases have been identified in different species: chronic wasting disease of deer and elk, bovine spongiform encephalopathy of domestic cattle, and variant Creutzfeldt-Jakob disease of humans. Thus, a better understanding of the process of cross-species transmission is needed.\n\n【1】Recognition of natural cross-species transmission is not straightforward. If a disease that crosses species has clinical or pathologic features similar to those of an already well-characterized TSE disease, it may not be recognized as a cross-species infection. Furthermore, some cross-species events involve slow processes in which the TSE agent adapts over several passages before recognizable clinical disease occurs .\n\n【2】The amino acid sequence of the prion protein (PrP) is known to be an influential factor for cross-species transmission of TSE disease to a new host. Multiple single nucleotide polymorphisms resulting in amino acid changes that control susceptibility to TSE disease have been identified in sheep, cervids, humans, and transgenic mice . Similarly, incubation periods have also been correlated with nonsynonomous single nucleotide polymorphisms in the PrP gene. For example, 2 polymorphic residues at amino acids 108 and 189 are associated with either short (Leu108/Thr189) or long (Phe108/Val189) incubation periods in mice . Even a single point mutation in mice at amino acid position 101 has been shown to alter proteinase K–resistant prion protein (PrPres) deposition in brain, incubation periods, and host range .\n\n【3】In this study, we examined molecular and biochemical changes associated with cross-species transmission in 6 hamster species of the rodent subfamily _Cricetinae._ All animals were handled according to the National Institutes of Health guidelines and protocols approved by the Rocky Mountain Laboratories’ (Hamilton, MT, USA) Institutional Animal Care and Use Committee **.**\n\n【4】Phylogenetic classification of these hamster subspecies is based on DNA sequences of mitochondrial cytochrome _b_ gene and a portion of the NADH dehydrogenase 4 gene . These 6 species diverge into 3 genera, mainly _Cricetulus_ (including Armenian and Chinese), _Phodopus_ (including Djungarian and Siberian), and _Mesocricetus_ (including Turkish and Syrian) hamster species . By inoculating each of these hamster species with a well-characterized, stable strain of Syrian hamster scrapie (263K), we were able to compare and analyze molecular and biochemical parameters of cross-species transmission events. To identify the cross-species transmission event, we looked for recognizable features that may have emerged in the new host. We found that each new host species presented a profile unlike that of the original Syrian hamster host infected with 263K. These profile changes correlated with unique PrP amino acid sequences within the 6 hamster species. This finding suggests that host PrP sequences can change the phenotype presentation of the agent in the host and could thereby confound identification of cross-species transmission events.\n\n【5】### Materials and Methods\n\n【6】##### Incubation Periods and Titers for Passaged 263K\n\n【7】The original source of 263K hamster agent came from Kimberlin et al. and was passaged 3 times in Syrian hamsters at Rocky Mountain Laboratories. For cross-species transmissions, also referred to as first passage, weanling hamsters from each of the 6 hamster species were intracranially inoculated with 263K stock at a titer of 2 × 10 9  lethal dose for 50% per 50 µL of a 1% brain homogenate. Clinically ill hamsters from each species were killed, and 1% brain homogenate was passaged intracranially into weanling hamster recipients of the same species (second passage). The process was repeated for a third passage. Brain homogenate from clinically ill third-passage hamsters was inoculated intracranially back into Syrian (back-passaged) hamsters.\n\n【8】Each hamster was killed when it had lost ≈30% of its body weight and was no longer able to remain upright and feed itself. We determined endpoint titrations for 263K, first, second, and third hamster passage inocula used in these experiments for all species (except Chinese hamsters) by preparing sequential 1:10 dilutions of a 1% brain homogenate to 10 –8  or 10 –10  . Dilutions were then injected intracranially into 6–8 hamsters for each dilution, and titers were determined as described  .\n\n【9】##### Immunoblot Analysis of Proteinase K–Sensitive Prion Protein and PrPres by Western blot\n\n【10】Proteinase K (PK)–sensitive prion protein (PrPsen) and PrPres were prepared as previously described . Samples were frozen at –20º C until they were subjected to electrophoresis on a 16% sodium dodecyl sulfate–polyacrylamide gel (Invitrogen, Carlsbad, CA, USA). Immunoblots were probed by using polyclonal antibody R30 to PrP (89–103 in Syrian hamsters) , which recognizes PrP from each of the 6 species. Blots were developed by using either enhanced chemiluminescence or enhanced chemifluorescence according to manufacturer’s instructions (Amersham-Pharmacia, Uppsala, Sweden). Enhanced chemifluorescence blots were scanned by using a STORM fluorescent detection system (Amersham-Pharmacia) as described previously .\n\n【11】##### Sensitivity of Proteinase K\n\n【12】To demonstrate PrPres sensitivity to PK, we adjusted 20 μL of a 20% (wt/vol) third-passage brain homogenate in 0.01 M Tris, pH 7.3, from each species to 100 mmol/L Tris HCl, pH 8.3, 1% Triton X-100, and 1% sodium deoxycholate. Samples were treated with 25 μg/mL, 100 μg/mL, 400 μg/mL, or 1,600 μg/mL PK in a total volume of 35 μL and incubated at 37º for 1 h. The reaction was stopped by adding 2 μL of 0.1 M phenylmethylsulfonyl fluoride and placed on ice for 10 min. Samples were then mixed in equal volumes with 2× sample buffer, boiled 5 min, and subjected to electrophoresis on sodium dodecyl sulfate–polyacrylamide gels. PrP bands were quantitated as described above.\n\n【13】##### PrP Gene Sequencing\n\n【14】To sequence the PrP gene open reading frame (ORF), we based primers on published regions of sequence homology between the Syrian, Armenian, Chinese hamster; rat; and mouse PrP genes. Primers 7F, S/O, C, S/T(+2), C/O(+2), and 1R are located outside the ORF. The following primers were used: 7F-5′-GCCTTGTTCTTCATTTTGCAGA-3′, S/O(–1) 5′-TCATTTTGCAGATCAGCCATC-3′, C(–1) 5′-TCATTTTGCAGATTAGCCAT-3′, S/T(+2) 5′-GTACAAGCAGGGAGGCTTCCTTC-3′, C/O(+2) 5′-GTACAAGCAGGGAGGCTTCCCTC-3′, and 1R 5′-ACCCCTCCCCCAGCCTAG-3′.\n\n【15】##### Immunohistochemistry\n\n【16】Immunohistochemistry procedures were conducted as previously described . PrPres was detected by using R30 anti-PrP antibody (residues 89–103).\n\n【17】### Results\n\n【18】##### Incubation Periods and Titers\n\n【19】Incubation periods and infectivity titers are among the criteria used to define TSE strains . Therefore, we compared incubation periods and infectivity titers for 263K in Syrian hamsters with those observed for the 5 other hamster species. Cross-species transmission of Syrian-derived 263K to Turkish hamsters, both members of the genus _Mesocricetus_ , had similar incubation periods . Transmission of 263K to hamsters of the genus _Phodopus_ (Djungarian and Siberian) had incubation periods similar to each other but different from those of the _Mesocricetus_ hamsters. Although Chinese and Armenian hamsters each belong to the genus _Cricetulus_ , after inoculation with 263K, their incubation periods differed from each other and from those of all the other species. In most instances, we observed prolonged incubation periods when Syrian 263K was inoculated into the new hosts. Because all the hamsters received the same inoculum at passage 1, this finding likely reflects the species barrier between the hosts  rather than inoculum titer.\n\n【20】Two additional passages from a donor to a recipient of the same species showed that incubation periods in most species differed considerably from those of the original Syrian hamsters . Incubation periods for the new species decreased and then became stable between second and third passage and were considerably different from those of the original Syrian hamsters . Incubation periods for Chinese hamsters were still decreasing between second and third passages. Because a fourth passage was not performed for Chinese hamsters, whether third passage reflects the stable incubation period is unknown. Thus, for Djungarian, Siberian, and Armenian hamsters, evidence was strong for unique incubation periods in the new species by second passage after inoculation with Syrian 263K agent.\n\n【21】To determine whether incubation period differences for each species resulted from differing infectivity concentration in the inocula, we used endpoint titration to determine the brain titers after first, second, and third passages . These determinations were not conducted for Chinese hamsters because the long incubation periods would require several additional years. When comparing first-passage titers with titers in Syrian hamsters infected with 263K, the only decrease in infectivity titer was seen in Armenian hamsters . When comparing second-passage titers with titers in Syrian hamsters infected with 263K, we observed a minimal decrease in Turkish, Siberian, and Armenian hamsters. We observed no notable changes in titers among third passage animals of all 6 species, which suggests stable titers in each species by third passage.\n\n【22】Additional evidence for adaptation of 263K agent to the new host species was obtained by infecting Syrian hamsters with brain homogenates from third-passage hamsters. If the agent had adapted to the new species, we would expect to see differences in incubation periods. In contrast, in the absence of adaptation, we would expect reversion back to the characteristic 263K Syrian incubation period. The only hamster for which third-passage brain homogenate resulted in a decrease in incubation period was the Chinese hamster. Even so, at 168 days this incubation period still differed from the characteristic 80-day incubation period for Syrian hamsters. These results indicate that the 263K agent adapted to the new host and that each 263K-infected hamster species has a unique incubation period .\n\n【23】##### Glycoform Profiles\n\n【24】PrPres glycoform profiles are another criteria used to differentiate TSE strains . Therefore, we compared PrPres glycoform profiles from each of the 6 hamster species at each passage.\n\n【25】When Western blotting was used to compare percentages between the 3 PrPres bands , the data clearly showed 2 different PrPres glycoform profiles. The Turkish hamster PrPres glycoform profile shared similarities with that of both Syrian and Chinese hamsters. Siberian, Djungarian, and Armenian hamsters shared a second PrPres glycoform profile. PrPres glycoform patterns from Turkish, Chinese, Siberian, and Djungarian hamsters fluctuated noticeably over the 3 passages, which suggests a lack of stability while adapting to the new species. In contrast, PrPres glycoform patterns from Armenian hamsters did not fluctuate over the 3 passages, which suggests a stable strain in Armenian hamsters .\n\n【26】We also injected brain homogenates derived from the third-passage hamsters back into Syrian hosts. We found that glycoform patterns in the Syrian recipients were the same as those ordinarily associated with Syrian hamsters , which suggests, as with incubation periods, that the host had a predominant influence over glycoform patterns.\n\n【27】##### PrPsen\n\n【28】To investigate the possibility that differences in PrPres glycoform profiles were reflections of different PrPsen characteristics in the various hamster species, we analyzed PrPsen profiles by using Western blot. No differences were found in expression levels or banding patterns among the 6 hamster species . Bands detected at 37 kDa were proven to be PrPsen because they were competed out (protein signal disappeared) when we preincubated the antibody to PrP with a synthetic peptide specific for the PrP epitope. Bands >37 kDa did not compete out . All PrPsen samples were PK sensitive (data not shown).\n\n【29】##### PK Resistance\n\n【30】PrPres resistance to PK digestion has also been used to differentiate TSE strains . When we compared PrPres resistance to PK from 263K Syrian and third-passage Turkish, Armenian, Chinese, Siberian, and Djungarian hamsters, we found no differences in sensitivity to PK. All samples retained equivalent PrPres signals on Western blots after treatment with 25 and up to 1,600 μg/mL PK (data not shown).\n\n【31】##### Immunohistochemical Findings\n\n【32】Differentiation of TSE strains has also been based on regional distribution of PrPres and microscopic lesions in brains of infected individuals . Therefore, we studied lesion profiles in brain from 5–8 hamsters of each species at each passage. We scored 13 areas from 0 to 4 (none to the highest degree of PrPres distribution or lesions) . Averages for each brain region were scored and compared . A score change >1 between species or passages was considered a notable change for that region .\n\n【33】Microscopic lesion profiles  from the 3 passages for each species were compared with those of the 263K Syrian hamsters (stained with hemotoxylin and eosin \\[H&E\\]). We found that Djungarian hamster profiles were most similar to 263K Syrian hamster profiles. Second-passage Armenian hamsters had increased vacuolation in the cortex; Turkish, Siberian, and Chinese hamsters differed in multiple regions. Lesion profiles for the 3 passages in each of the new hamster host species, such as Chinese hamsters  regions 2 and 3, did not necessarily correlate with PrPres distribution  within that host.\n\n【34】When comparing the PrPres deposition profile  at each of the 3 passages to the Syrian PrPres deposition profile, we found that the profiles of the Turkish hamsters were the most similar. Because Syrian and Turkish hamsters are closely related phylogenetically and share similar PrPres glycoform patterns, this finding was not surprising. In the other 4 species (Djungarian, Siberian, Armenian, and Chinese), PrPres distribution in the thalamus was increased (region 6) over that in the same region for Syrian hamsters. Further changes were seen in Armenian hamsters; PrPres distribution was increased in the spinal cord (region 5) and olfactory bulb (region 10). PrPres distribution patterns in Chinese hamsters diverged from those in the Syrian hamsters in almost every region except the spinal cord (region 5). In all 6 species, including our original Syrian hamster species, unique pathologic phenotypes developed. Immunohistochemical and glycoform profiles for 2 of the 3 genera were independently unique, which suggests that similar host factors within those genera influenced these TSE characteristics.\n\n【35】To determine the respective contributions of the agent and host in immunohistochemical patterns, we injected brain homogenate from third-passage hamsters back into Syrian hamsters. When patterns were compared with those of 263K Syrian hamster, only a few differences from 263K Syrian were observed. Specifically, less PrPres was deposited in the posterior colliculus (region 2) in Syrian hamsters inoculated with brain homogenate from Chinese hamsters, and more PrPres was deposited in the thalamus (region 6) of Syrian hamsters inoculated with brain homogenate from Djungarian hamsters . We also observed a decrease in vacuolation in the hippocampus (region 8) and cortex (region 9) of Syrian hamsters inoculated with brain homogenate from Chinese hamsters . For all other hamster species inocula, no remarkable differences from 263K Syrian hamster were noted. Therefore, the host appeared to play an important role in determining immunohistochemical patterns. It is not known whether the differences seen in the Chinese and Djungarian hamsters are due to donor species effect , whereby a temporary change occurs as an agent is passed through a new host species, or whether selection of a new strain has occurred. To answer this question, a second passage through Syrian hamsters is needed. These results suggest that although 263K agent was substantially altered by passage through the new hamster species, in general it was not able to impart molecular characteristics like glycoform or the pathologic patterns established in the new species back to the Syrian hamsters.\n\n【36】##### Sequencing\n\n【37】Different PrP amino acid sequences have been associated with variation in lesion profiles, incubation periods, and susceptibility to TSE disease in humans and in sheep, cervid, and mouse models. We report the PrP gene sequences for Siberian, Djungarian, and Turkish hamsters. To determine how amino acid sequence might associate with the TSE characteristics we observed here, we compared our new sequences with the published PrP gene ORF of Syrian , Chinese, and Armenian hamsters .\n\n【38】When comparing the new _prnp_ sequences to the Syrian _prnp_ sequence, we found considerable variation in nucleotides. Siberian hamsters had 40 nt changes, and Djungarian hamsters had 41. These nucleotide changes resulted in 10 aa substitutions for both species; only 1 aa acid substitution, I215V, distinguished sequences between Siberian from Djungarian hamsters . Only Turkish and Syrian hamsters had 3 nt differences, resulting in 2 aa substitutions. An amino acid Y-to-F substitution was found at codon 6 in the signal sequence, and a heterozygous base was found at codon 103 (encoding both S and N). In addition, Turkish hamsters have a deletion of 1 octapeptide repeat (Δ81–87) on both alleles, a characteristic shared with African Green monkeys . In humans, deletion of 1 octapeptide repeat occurs at a frequency of 0.5% and is not associated with disease . We found 34 nt changes for Armenian and 37 for Chinese hamsters, resulting in 7 aa substitutions for each hamster species, a finding that agrees with published data . The 3 aa differences that distinguish Armenian from Chinese hamsters are located at aa positions 103, 108, and 112 .\n\n【39】### Discussion\n\n【40】Of the 6 hamster species inoculated with a well-defined hamster scrapie source, each species not only was susceptible to the agent but also developed unique PrPres biochemical and pathologic characteristics. These characteristics, which included incubation periods, PrPres glycoform patterns, and PrPres distribution in brain, appeared to segregate into related genera with the exception of the genus _Cricetulus_ (Armenian and Chinese hamsters). Because TSE characteristics were unique for 2 of 3 genera, a role for host factor involvement is implicated. Our data suggest that PrP amino acid sequence is the host factor responsible for the unique characteristics.\n\n【41】Data from our study and another experimental study  suggest that not only is the host responsible for determining TSE characteristics such as incubation periods but so is the agent. Our data show that closely related hamsters infected with the same strain of TSE have different incubation periods. These differences cannot be explained by differential expression levels of PrPsen because by Western blot, PrPsen expression was equivalent . Also, we believe that changes in incubation periods cannot be attributed to differential infectivity titers because only minor variations in titers were found. A second set of experiments showed that the same hamster host inoculated with different strains of TSE had different incubation periods, such as hyper (65 ± 1) and drowsy (168 ± 2) .\n\n【42】Because incubation periods and altered PrPres deposition in the brain have been linked to polymorphisms in the PrP amino acid sequence , we investigated the possibility that PrPres biochemical and pathologic changes correlate with different host PrP amino acid sequences. When comparing the PrP gene ORF for each of the 6 hamster species, we found 13 possible amino acid substitutions localized to 2 regions, either the N or the C terminus. Hamsters from 2 genera in this study ( _Phodopus_ and _Mesocricetus_ ) maintained sequence homogeneity at these 13 residues and had very similar TSE characteristics. Hamsters of the third genus, _Cricetulus,_ differed in PrP gene sequence at 3 amino acids within the N-terminal polymorphic region. These polymorphic changes could explain why Armenian and Chinese hamsters have such different incubation periods, PrPres glycoform patterns, and immunohistochemical profiles from each other as well as from the 2 genera _Cricetulus_ and _Phodopus._ It is also possible that closely related hamsters have unidentified genes that also influence TSE characteristics.\n\n【43】Our results are consistent with the published data indicating that amino acids 102–139 may control scrapie incubation periods in experimental models . In addition, our data suggest that PrPres glycoform profiles and lesion profiles are also associated with this region. Armenian and Chinese hamsters, both in the genus _Cricetulus,_ show significant differences from each other, having only 3 aa substitutions at residues 103, 108, and 112 in this N-terminal polymorphic region. In contrast, amino acid sequences from Djungarian and Siberian hamsters were the same in this region, and those from Turkish hamsters matched those from the Syrian hamsters. These findings suggest a direct role for these residues and the N-terminus in determining TSE characteristics. In addition, all 3 genera differ from each other at residue 139, the same position thought to participate in the hydrophobic core potentially stabilizing PrPsen in mice . The unstructured nature of this region has led some investigators to consider this stretch of residues as a potential nucleation site for conversion of PrPsen to PrPres . Sequence differences in this position may dramatically affect initial conversion events.\n\n【44】Certain amino acid substitutions may play only a minor role in TSE strain profiles. For instance, residue V215T is thought to be responsible for the structural differences between mouse and hamster PrPsen by introducing a bend in helix 3 . From our data, T215V/I substitutions may have had a minor in vivo effect with respect to TSE characteristics. Djungarian and Siberian hamsters differed only at this amino acid and had similar glycoform and IHC patterns as well as incubation periods.\n\n【45】Amino acid substitutions identified in this study segregated into genera with similar incubation periods, glycoform profiles, and brain pathologic changes. Because we used only 1 TSE agent, these changes can be attributed to the host factors and not to the agent. A previous study of 3 hamster species also concluded that host rather than agent had a predominant role in determining biochemical and molecular PrPres attributes . Because host factors seem to play a larger role in determining the TSE profile, identifying the source of infection in cross-species infections may be difficult, which would be especially worrisome when species important to humans (e.g. sheep, cattle, cervids) are involved. These ruminant species are heterogeneous and not nearly as closely related as the hamsters studied here.\n\n【46】Our study links PrP amino acid sequences to biochemical and pathologic profiles of PrPres in multiple hamster species infected with 263K hamster scrapie. To further investigate how specific amino acid substitutions in PrP are associated with specific TSE characteristics, these hamster and similar PrP gene sequences could be expressed in tissue culture assays and protein modeling experiments. The results of these experiments could broaden our understanding of TSE profiles and the role that PrP amino acid residues play, possibly leading to assays that could identify instances of cross-species transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "88576af4-0192-4c1f-b23d-2da57a861cb7", "title": "Schmallenberg Virus in Domestic Cattle, Belgium, 2012", "text": "【0】Schmallenberg Virus in Domestic Cattle, Belgium, 2012\nIn the summer and fall of 2011, a nonspecific febrile syndrome, characterized by hyperthermia, drop in milk production, and watery diarrhea, was reported among adult dairy cows on farms in northwestern Europe . In addition, in November 2011, an enzootic outbreak emerged in several European countries; sequelae included abortion, stillbirth, and birth at term of lambs, kids, and calves with neurologic signs or malformations of the head, spine, or limbs . Both syndromes were associated with the presence in the blood (adult animals) or in the central nervous system (newborn animals) of the RNA of a new Shamonda-like orthobunyavirus, provisionally named Schmallenberg virus after the town in Germany where the first positive samples were identified . Because this new viral disease in cattle emerged recently, information on its epidemiology is limited. The objectives of this study were to determine the prevalence of antibodies against Schmallenberg virus in adult cows living within ≈250 km of the location where the virus emerged, 9 months after the emergence, and to determine the proportion of fetal transmission of the virus.\n\n【1】### The Study\n\n【2】During February 13–April 22, 2012, serum samples were obtained at random from blood drawn by field veterinarians from 519 cow/calf pairs at 209 farms located in southeastern (195 farms, rectangle A) or southwestern (14 farms, square B) Belgium. Samples were obtained from 1–7 cow/calf pairs at each farm. None of the 519 calves exhibited neurologic signs of disease at birth through 10 months of age. Serum specimens were also obtained from a cohort of adult cattle in spring 2010 (n = 71) and the first quarter of 2011 (n = 40). We used the ID Screen Schmallenberg Virus Indirect ELISA kit (ID.vet Innovative Diagnostics, Montpellier, France) to determine if the serum samples contained IgG antibodies against the recombinant nucleoprotein of the emerging Schmallenberg virus. Results are expressed as percentages of the reference signal yielded by the positive control serum, with serologic status defined as negative (<60%), doubtful (>60% and <70%), or positive (>70%) by the manufacturer. Contingency tables were analyzed by using χ2 analysis to determine 1) if there was an association between sampling date and occurrence of seroconversion and 2) if there was an association between farm location and occurrence of seroconversion. Significance level was set at p<0.05.\n\n【3】All serum samples collected during spring 2010 and spring 2011 were negative for antibodies against Schmallenberg virus, which is consistent with the emergence of the new virus during the summer and fall of 2011 . Of the 209 farms sampled, only 13 were categorized as having seronegative cattle, each on the basis of the single paired sample that was available. These farms were not clustered by location. In each of the 196 remaining farms, >75% cows had seroconverted. Overall, apparent seroprevalence among adult cows was 90.8% (95%, CI 88.3–93.2). Association between farm location and seroconversion was not significant (p = 0.607), with results of 92.3%, 88.3%, 90.0%, and 92.0% in eastern, southern, western, and central areas, respectively . Acquired herd immunity against the new virus was thus quite high in the adult cattle population sampled, which suggests that this virus has spread quickly throughout the region since its emergence ≈250 km northeast of these areas in the late summer of 2011. Furthermore, a significant association between week of sampling and occurrence of seroconversion was found (p = 0.039), with a progressive increase of apparent seroprevalence: 87.8% (weeks 7–9, 95% CI 82.6–93.1), 90.4% (weeks 10–11, 95% CI 85.8–95.0), and 93.0% (weeks 12–16, 95% CI 89.6–96.4). This finding suggests that the virus was still circulating in the stables during the period examined. This results is not surprising because biting midges of the genus _Culicoides_ , which are believed to transmit Schmallenberg virus , were recently shown to be able to complete their life cycle in animal enclosures .\n\n【4】Of the calves born to seropositive cows, 116 (24.6%) of 471 (95% CI 20.7–28.5) also tested positive, and no association was found between anti–Schmallenberg virus IgG in the newborn calves’ serum samples before they received colostrum and farm location (p = 0.639), with 23.8%, 23.0%, 31.6%, and 25.3% seroprevalence in eastern, southern, western, and central areas, respectively . Infection of pregnant cows by Schmallenberg virus thus often results in transmission to the fetus across the placenta, and this transmission does not lead automatically to abortion, stillbirth, or congenital deformities. Because none of the calves examined here showed the neurologic or musculoskeletal signs typically associated with unrestricted replication of Schmallenberg virus , maternal infection probably occurred after the fetuses became immunocompetent, that is, after the 150th day of gestation . Therefore, on the basis of the corresponding dates of artificial inseminations, we conclude that the 38 seropositive calves born during March 19–April 22, 2012, were infected after November 1, 2011.\n\n【5】Because the corresponding farms were not clustered spatially, we deduced that the new virus and its vectors actively circulated throughout southern Belgium in November 2011. Again, a significant association was detected between week of sampling and occurrence of seroconversion during gestation (p = 0.023), with a progressive decrease of apparent seroprevalence among calves born to seropositive cows: 28.4% (weeks 7–9, 95% CI 21.1–35.6), 21.0% (weeks 10–11, 95% CI 14.7–27.4), and 19.2% (weeks 12–16, 95% CI 13.9–24.4). Thus, the apparent rate of the virus crossing the placenta seems to decrease over time. Because the biology of the new virus would not have changed in a few weeks, this decreased rate of transmission could be attributed to the gradual increase in the relative proportion of cows that were infected before the development of the placenta enables placental transfer of the virus, i.e. before the 30th day of gestation . Therefore, to estimate the risk for fetal infection among immunologically naive cows, we focused on the group of seropositive cows whose gestation began long before the supposed emergence of the new virus. Of the 519 pregnancies, 148 resulted from artificial inseminations performed during May 2–22, 2011. The probability that the cows had been infected before the 30th day of gestation by a virus that emerged in late summer is close to zero. On the basis of data from this pertinent subset, we found that ≈28% of the calves born to cows primo-infected after the first placentome developed (which permitted the virus to cross the placenta) were indeed infected. This finding closely fits with the ≈30% reported for Akabane virus, a close phylogenetic relative of Schmallenberg virus .\n\n【6】In conclusion, this study confirms that the emerging virus was absent from the area examined in spring 2011 and provides evidence that 1 year later almost all adult cattle had seroconverted. Furthermore, the results suggest that the risk for infection of the fetus in an immunologically naive herd is ≈28% and that in utero infections can occur without sequelae visible at birth if the infection occurs when the fetus’s immune system is mature enough to control virus spread. In the case of Akabane virus, the cow’s natural immunity prevents subsequent infections of the fetus . It seems likely, therefore, that the Schmallenberg virus infection itself, and its resulting economic effects on farms in the regions concerned, might disappear in 2012.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "be4e0de4-d98f-44fa-8957-dffab0f0e2f1", "title": "The Travel and Tropical Medicine Manual, 4th Edition", "text": "【0】The Travel and Tropical Medicine Manual, 4th Edition\nLike the previous editions of The Travel and Tropical Medicine Manual, the 4th edition continues to successfully teach the fundamentals of travel and tropical medicine and serve as a useful reference. The manual is well-organized and literally (measuring 12 × 20 cm) and figuratively handy. It has many appealing tables and figures, a colorful cover, and a reasonable price. It remains appropriately titled, with some chapters written from a travel perspective and others from a tropical medicine perspective.\n\n【1】The manual contains 45 chapters divided into 7 sections (Pre-Travel Advice, Advice for Special Travelers, Fever, Diarrhea, Skin Lesions, Sexually Transmitted Diseases, and Worms). The current edition has a new coeditor (Christopher A. Sanford) and new chapters on urban medicine and health advice for long-term expatriates. Its audience should continue to be primary care providers who routinely counsel patients on travel-related issues, infectious disease physicians with an interest in travel medicine, other travel medicine practitioners, and trainees doing rotations in travel medicine. However, its technical terminology and concepts prevent the manual from achieving its stated goal of being a “perfect source” for travelers.\n\n【2】The greatest strengths of the manual continue to be its practical suggestions for counseling travelers prior to their departure and evaluating patients with post-travel illnesses. Notable chapters discuss general approaches to travel medicine, immunizations, managing jet lag and motion sickness, counseling HIV-infected travelers, malaria prevention, avoiding and self-treating travelers’ diarrhea, evaluating diarrhea in returned travelers, tropical dermatology and sexually transmitted infections. The manual also has outstanding tables in its approach to travel medicine chapter. Other informative tables describe the safe selection of food and water, drugs for preventing and treating traveler’s diarrhea, the use of melatonin to prevent jet lag, potential interactions between antiretroviral and travel-related medications, and the differential diagnoses of travel-related skin lesions.\n\n【3】Although the manual has shortcomings, it has no serious deficiencies. Incorporating contributions from 49 authors results in chapters of variable quality, some redundancies, and occasional omissions. A 20-page chapter on water disinfection appears disproportionately lengthy when compared with the immediately preceding 12-page chapter on the prevention and self-treatment of traveler’s diarrhea. Although the chapter on women travelers discusses male condoms superficially, this important topic is well addressed in the chapter on sexually transmitted infections. In future editions of the manual, the editors should consider including more than 2 photographs in the 74-page section on skin lesions, using a larger font, and enlarging the figures.\n\n【4】Clinicians who routinely evaluate patients before or after traveling should definitely consider purchasing this manual, reading select chapters in the pretravel and special travelers sections, and keeping it in their travel clinic as a useful reference.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5dba4516-49a5-4026-8c09-0bf78b144afc", "title": "Health Care Response to CCHF in US Soldier and Nosocomial Transmission to Health Care Providers, Germany, 2009", "text": "【0】Health Care Response to CCHF in US Soldier and Nosocomial Transmission to Health Care Providers, Germany, 2009\nCrimean–Congo hemorrhagic fever (CCHF) is a life-threatening viral illness endemic to areas of Africa, southeastern Europe, Russia, China, India, and the Middle East. CCHF is caused by infection with a tickborne virus (family _Bunyaviridae_ , genus _Nairovirus_ ), and is generally acquired through the bite of an infected tick or contact with blood or body fluids of infected animals . The disease is characterized by the abrupt onset of a febrile illness usually 2–7 d (range 2–14) after exposure to the virus and by subsequent severe changes in mental status, hemorrhagic manifestations, and hepatorenal failure . Case-fatality rates vary by region but are 30%–50% (range 1%–73%) in most regions; death generally occurs 5–14 d after symptom onset and is most commonly a result of multi-organ failure, shock, severe anemia, cerebral hemorrhage, and/or pulmonary edema .\n\n【1】We report a fatal case of CCHF in a US soldier deployed to Afghanistan, who was aero-evacuated to Germany for treatment, and the documented nosocomial infection of 2 health care providers (HCPs) who were at risk for exposure and had received ribavirin postexposure prophylaxis (PEP). We also review infection control interventions and contact surveillance, both of which were required because of the patient’s severe bleeding and the risk for aerosol production. Research on human subjects was conducted in compliance with US Department of Defense, federal, and state statutes and regulations relating to the protection of human subjects and adheres to the principles identified in the Belmont Report .\n\n【2】### The Case\n\n【3】On September 8, 2009, a 22-year-old male US soldier who worked in field operations outside Kandahar City, Afghanistan, sought care at a military medical clinic for a 4-d history of nonbloody diarrhea, abdominal pain, bloody emesis, and fever (39.2°C). The patient reported frequent outdoor activities, tick bites, and exposure to undercooked goat meat and blood the week before the onset of illness. Ciprofloxacin was prescribed for probable gastroenteritis. The patient did not improve by the next day and returned to the clinic, reporting somnolence and lethargy. He was transferred to a Combat Support Hospital at Kandahar Air Base.\n\n【4】Admission laboratory values demonstrated anemia, thrombocytopenia, acute renal insufficiency, and elevated levels of hepatic transaminases. Within a few hours of admission, the patient had worsening lethargy; bloody diarrhea; gingival bleeding; hypoxia requiring intubation; and a hypotensive episode after intubation, which required vasopressor therapy for 24 h. The patient was treated with intravenous levofloxacin (replaced ciprofloxacin), meropenem, fresh-frozen plasma (6 U), platelets (2 U), packed erythrocytes (2 U), and infusions of furosemide and pantoprazole.\n\n【5】On September 10, the patient was emergently aero-evacuated to Landstuhl Regional Medical Center (LRMC; Landstuhl, Germany). During the flight, his respiratory status deteriorated (requiring 100% FiO 2  ) and he continued to bleed from multiple sites (nares, gingiva, gastrointestinal, and venipuncture sites). Treatment on route included fresh-frozen plasma (6 U), packed erythrocytes (1 U), and cold-water lavage to decrease upper gastrointestinal bleeding.\n\n【6】The patient arrived at LRMC on September 11 (day 6 of illness); he was in multiorgan, failure and large amounts of bright red blood were in the endotracheal tube. During the initial physical examination, he exhibited respiratory compromise, temperature of 37.3°C, blood pressure of 147/74 mm Hg (reference values for clinical/laboratory values are in Table 1 ), pulse rate of 118 beats/min, and 90% saturation on 100% oxygen. Significant findings included edematous conjunctivae with mild hemorrhage; nasopharyngeal bleeding; coarse breath sounds; large ecchymoses at venipuncture sites; scattered petechiae on the trunk, arms, and upper thighs; extensive edema of the extremities and scrotum; and melena on rectal examination. Emergent bronchoscopy revealed diffuse bleeding in the airways. Admission laboratory and radiography results  supported a presumptive diagnosis of CCHF, and infection control measures for possible viral hemorrhagic fever (VHF) were implemented .\n\n【7】On day 7 of illness, reverse transcription PCR (RT-PCR) of a serum sample obtained at admission showed a CCHF viral load of 1.2 × 10 9  copies/mL (CCHF virus was later isolated from blood and urine samples obtained on day 6 of illness; CCHF IgM and IgG serologic results were negative on day 6 of illness) . Oral and then intravenous ribavirin therapy were initiated; the intravenous ribavirin was administered within 24 h of admission and under an Investigational New Drug protocol after the patient’s family gave consent  . Treatment with the broad-spectrum antibiotics was discontinued. Repetitive emergency bronchoscopy was required to control severe pulmonary hemorrhage.\n\n【8】On day 8 of illness, the patient’s pulmonary status continued to deteriorate: development of adult respiratory distress syndrome required placement of bilateral chest tubes to drain bloody pleural effusions, administration of nitric oxide, and use of advanced inverse ratio bilevel mechanical ventilation. The severe adult respiratory distress syndrome was complicated by massive pulmonary hemorrhaging that required multiple bronchoscopies and infusion of blood products (packed erythrocytes \\[32 U\\], fresh frozen plasma \\[80 U\\], platelets \\[34 packs\\], factor VII \\[4 U\\], and cryoprecipitate \\[3 U\\]). Tris (hydroxymethyl) amino methane and continuous hemodialysis were initiated for progressive renal failure and severe acidosis (serum creatinine 8.2 mg/dL, bicarbonate 12 mmol/L), and multiple boluses of glucose were given for recurrent hypoglycemia.\n\n【9】On day 9 of illness, the patient’s oxygenation and blood product transfusion requirements lessened, but hypoglycemia and acidosis persisted; a bicarbonate drip was initiated. Because fulminant hepatic failure occurred (aspartate aminotransferase 9,628 U/L, alanine aminotransferase 2,151 U/L, total bilirubin 8.1 mg/dL), liver dialysis was initiated by using a liver albumin dialysis machine. Early on day 11 of illness, liver dialysis was discontinued because the patient's condition appeared to be stabilizing (serum viral load and hepatic transaminases were decreasing, and less ventilatory support and fewer blood product transfusions were required). However, neurologic examination later that morning showed bilateral fixed and dilated pupils, and before brain imaging was possible, the patient suffered a cardiorespiratory arrest. A postmortem computer tomographic scan image demonstrated diffuse cerebral and cerebellar edema, a small right frontal parenchymal hemorrhage, and bilateral cerebellar tonsil herniation.\n\n【10】### Hospital Infection Control\n\n【11】Because a CCHF diagnosis was not considered likely at the time of the initial 2 emergency bronchoscopies, standard precautions for infection control were used. The 2 bronchoscopists wore a gown, gloves, eye protection, and surgical masks; other persons in the room wore N95 respirators or surgical masks. More stringent infection control measures for VHFs, including airborne precautions, were implemented once a diagnosis of CCHF was considered  . The patient was placed in an airborne-infection isolation room with an anteroom, which had restricted visitation and intensive care unit (ICU) entry. Sign-in sheets tracked who entered the patient’s room; those entering were required to wear a fluid-resistant gown, gloves, N95 respirator, eye protection/face shield, and shoe coverings. Biohazard suits (Tyvek; DuPont, Richmond VA, USA) with powered air-purifying respirators were worn during subsequent bronchoscopies and chest tube placement. Infection prevention and control staff provided refresher training on the proper donning and doffing of personal protective equipment (PPE) and oversaw the decontamination of bronchoscopes and ventilators. The procedures for disposing of medical wastes were in accordance with German regulations for handling infectious biohazardous materials .\n\n【12】Laboratory interventions involved limiting blood draws and analyses to the most critical samples and to a single laboratory technician. Laboratory personnel wore gowns, gloves, and N95 respirators. Centrifugation of specimens was performed within a Class II biosafety cabinet. Laboratory equipment was decontaminated immediately after use and all nonreusable equipment was autoclaved before disposal. Blood and urine samples were pretreated with polyethylene glycol (to reduce viral load) before being shipped to Bernard Nocht Institute (Hamburg, Germany) for CCHF diagnostic testing. The cadaver was placed in 2 sealed body bags; the outside of each bag was decontaminated with a 10% bleach solution. RT-PCR analysis of deep cadaver tissue samples was performed after embalming and confirmed to be negative. Bleach (10%) or standard hospital-grade disinfectants were used for terminal cleaning of the patient’s room and all surfaces and equipment in the airplane used to aero-evacuate the patient to Germany.\n\n【13】### Outbreak Investigation\n\n【14】Contact tracing commenced immediately after diagnosis and included a wide group of persons who may have been at risk for exposure to the patient’s blood/body fluids: personnel in the patient’s deployed unit, persons at the Combat Support Hospital in Kandahar, the medical evacuation team, and persons at LRMC (HCPs, laboratory workers, and transport, housekeeping, and volunteer staff). Among these contacts, 18 HCPs were identified as having been at risk for exposure and were offered oral ribavirin PEP (off-label use); 16 of the 18 accepted treatment . Most of the 18 HCPs were present during bronchoscopies or ventilation procedures that used a bag-valve-mask device and had reported blood splashes on their gowns. Although there were no known percutaneous exposures, 2 HCPs reported blood on intact skin. Also, some HCPs wore only a surgical mask as PPE during the initial bronchoscopies and/or were unsure if they had always maintained a properly fitted N95 respirator during subsequent bronchoscopies. The group of 16 HCPs who accepted ribavirin PEP included a medic in Kandahar who had a blood exposure on his ungloved hand during an emergency intravenous catheter insertion and a physician in Kandahar who emergently intubated the patient without wearing an N95 respirator. The group also included an LRMC ICU nurse and respiratory therapist, both of whom had met the patient on arrival at LRMC and manually ventilated him during transport to the ICU without wearing a mask or eye protection; during the transport, the patient was actively bleeding from intravenous catheter sites and coughing blood into the endotracheal tube. These 2 HCPs (and others) were also present during the initial 2 bronchoscopies, during which they may not have worn surgical masks at all times (and no eye protection) and their gowns had been soaked from blood exposures. In addition, the respiratory therapist’s face shield dislodged immediately after being sprayed with blood while she was manually ventilating the patient using a bag-valve-mask device during a life-threatening hypoxic event. The ICU nurse also had blood contact on her skin (wrist) during resuscitation, when her gown sleeve slipped from the glove. The respiratory therapist and ICU nurse were also among the HCPs, aside from a few physicians, who spent the most time directly caring for the patient. The remaining 72 personnel had unlikely/no identifiable exposure risk and were instructed to have their temperatures taken twice daily for 15 d and to contact the infectious diseases physician for any febrile illness within this same time period .\n\n【15】An oral ribavirin PEP regimen of 600 mg twice daily for 7 d was recommended initially; this dosage was based on drug availability, drug tolerance, and dosage regimens reported in the literature . Seventy-two hours later, more oral ribavirin became available, and the 16 HCPs were offered a 4-times-daily dosing regimen (600 mg/dose) and/or extension of PEP from 7 to 14 d. Because of the drug’s side effects, all HCPs chose to remain on a twice-daily dosing regimen; only 2 HCPs accepted an extension of PEP to 14 d. Side effects (mainly fatigue, dyspepsia, nausea, and headache) were reported by all 16 HCPs. Of the 12 HCPs compliant with blood draws, 10 showed an increase in total bilirubin (range 1.2–5.7 mg/dL) and 2 had mild anemia (nadir hemoglobin 11.9 g) attributed to hemolysis caused by ribavirin. Leukopenia was observed in 1 HCP (leukocyte count 2,800 cells/mm 3  ).\n\n【16】To assess possible seroconversion in the patient’s contacts, initial and follow-up (4–6 wk) blood samples were obtained from personnel in the patient’s deployment unit (n = 62), persons at the Combat Support Hospital in Kandahar (n = 55), and persons at LRMC (n = 74) and sent for serologic testing at the Centers for Disease Control and Prevention (Atlanta, GA, USA) . Although baseline serologic testing was not done, results of serologic testing done at 8 weeks for 2 HCPs who received oral ribavirin PEP (the ICU nurse and the respiratory therapist at LRMC) were consistent with acute CCHF seroconversion: CCHF virus–specific IgM and IgG titers were \\> 6,400, and over the next 2 mo, IgM titers declined . These 2 HCPs were the most symptomatic of the 16 persons who received ribavirin PEP, and the only persons to seek medical attention for their symptoms. Symptoms were initially noted 4–5 d after exposure (day 4 of ribavirin PEP). The ICU nurse had moderate abdominal discomfort and jaundice; a total bilirubin of 5.7 mg/dL was the only abnormal laboratory value (direct bilirubin was 0.2 mg/dL). The respiratory therapist experienced fatigue, myalgias, and chills (no documented fever). She had a reference bilirubin level and mild leukopenia (leukocytes decreased from 4,200 cells/mm 3  to 2,800 cells/mm 3  ), and she missed 3 d of work because of her symptoms. For both HCPs, the symptoms and laboratory abnormalities were initially attributed to side effects of ribavirin therapy (particularly the elevated bilirubin level and gastrointestinal discomfort); in retrospect, the respiratory therapist’s symptoms most likely represented CCHF symptoms ameliorated by ribavirin PEP. Ameliorated CCHF as a cause of symptoms in the ICU nurse could not be excluded.\n\n【17】### Discussion\n\n【18】This fatal case of CCHF in a US soldier illustrates several issues regarding clinical management, infection control measures, epidemiologic investigation, and ribavirin PEP in CCHF infection. Early recognition and diagnosis of CCHF is paramount, so that medical care and appropriate infection control measures can be implemented in the initial phase of illness and, thereby, improve survival and prevent nosocomial transmission. Delayed diagnosis and implementation of infection control measures can result in the need for extensive public health resources to evaluate and follow up on exposed HCPs and contacts .\n\n【19】The greatest risk for nosocomial transmission of the CCHF virus has been from percutaneous exposure with contaminated needles. Blood exposure on intact skin is a much lower risk, but sporadic transmission has been reported after skin or mucosal exposures to infectious blood. Nosocomial infection has also been reported without a clear source of virus transmission, and possible droplet (patient-to-patient) transmission has been reported .\n\n【20】Bronchoscopies and other procedures producing infectious aerosols were potential sources of CCHF virus transmission to the 2 HCPs reported here; however, no other HCPs who were present during the initial 2 bronchoscopies showed seroconversion. Thus, it is probable that virus transmission resulted from exposure to infectious blood during initial transport of the patient to the ICU, when neither HCP wore proper PPE (surgical mask/N95 respirator) while manually ventilating the patient or from a breach in PPE (particularly, for 1 HCP when her face shield dislodged during a resuscitation procedure). Of concern, both HCPs were unaware of their PPE breaches/exposures during resuscitation efforts; they were noted by other HCPs. The PPE breaches and potential blood/body fluid exposure risks led to the use of biohazard suits and powered air-purifying respirators during subsequent bronchoscopies and chest-tube placements. Compared with face shields, powered air-purifying respirators are less likely to become dislodged. The 2005 Centers for Disease Control and Prevention’s modified PPE guidelines for suspected VHFs at US hospitals note that extenuating circumstances (i.e. procedures generating aerosols, severe pulmonary involvement, or copious bleeding) may necessitate an increase in PPE (i.e. plastic aprons, leg/shoe coverings) or airborne precautions .\n\n【21】The processing of serum specimens with high viral loads in analyzers outside the Class II biosafety cabinet in a non–negative-pressure laboratory was a concern. Pretreatment of specimens to reduce viral load (i.e. with heat inactivation or polyethylene glycol) will be recommended in future cases, even though such treatment may affect laboratory results . Also, the CCHF virus has been detected in urine by RT-PCR as late as day 36 after illness onset; however, infectivity has been unclear because the virus had not been previously cultured from urine . The culture of the CCHF virus in patient samples obtained on day 6 of illness indicates that a positive RT-PCR result for urine may represent viable virus and the potential for late transmission of virus by patients who have recovered from CCHF.\n\n【22】Ribavirin, a synthetic purine nucleoside analog, has demonstrated in vitro activity against CCHF virus and decreased death rates in infected suckling mice . Ribavirin efficacy in humans has not been evaluated in placebo-controlled trials against CCHF because of ethical concerns. However, retrospective analyses of ribavirin-treated CHHF virus–infected cohorts (compared with untreated historical controls) often report a decrease in CCHF-associated death if given within 72 h after the onset of symptoms . Anecdotal reports of ribavirin PEP in CCHF cases are limited, but they also suggest a possible benefit in preventing or ameliorating disease in most HCPs . However, the optimal dosage and duration of oral ribavirin for CCHF prophylaxis is unknown. PEP regimens in the literature range from 200 mg twice daily to as high as 4 g daily for 5–14 d; the most common regimen is 500 mg 3–4 times/d for 7-10 d. A 2-g loading dose is recommended in some regimens, particularly when treatment initiation is delayed . Drug side effects (mainly gastrointestinal intolerance and fatigue) often limit the dosage and duration of ribavirin PEP .\n\n【23】CCHF virus IgM and IgG titers in the 2 seropositive HCPs corresponded to acute infection from nosocomial transmission because these persons had no other risk factors for recent exposure to the virus. The mild/absent CCHF symptoms in these 2 HCPS who received a lower dose and duration of ribavirin PEP (600 mg twice daily for 7 d) may provide further insight regarding the potential benefit and dosage regimen for oral ribavirin PEP. The estimation of 88% of CCHF cases in Turkey being subclinical in a recent seroprevalence study would likely necessitate a controlled clinical trial to assess the efficacy and dose for ribavirin PEP .\n\n【24】On arrival at LRMC (day 6 of illness), the patient had a poor prognosis for survival: ribavirin treatment had been delayed \\> 4 d after symptom onset; he was somnolent; and he had severe bleeding and coagulopathy, significantly elevated levels of hepatic transaminases, a platelet count of < 20,000/mm 3  , and a serum viral load of \\> 1 × 10 8  copies/mL . However, with supportive care, the patient showed clinical improvement (i.e. improved respiratory status, decreased bleeding and blood product requirements, and improved end-organ function). Continuous renal replacement therapy was particularly helpful in controlling the patient’s life-threatening metabolic derangements; hepatic replacement therapy was of uncertain benefit and interfered with the optimal use of continuous renal replacement therapy. On day 10 of illness, the patient was able to follow commands, his serum viral load was declining, and CCHF-specific IgG was present.\n\n【25】There were multiple possible reasons for the fatal brain herniation on day 11 of illness. CCHF infection can cause endothelial cell dysfunction (with increased vascular permeability) through the induction of cytokines (tumor necrosis factor-α, interleukin-6, interleukin-10, interferon \\[IFN\\]- **γ)** , which can result in cerebral edema . Increase in these cytokines and markers for increased endothelial cell permeability (i.e. vascular endothelial growth factor \\[VEGF\\]-A and soluble VEGF receptor 1) have been correlated with increased serum viral load and increased risk of death and/or severe CCHF disease  . In a similar VHF case (Marburg virus disease) with elevated levels of cytokine and soluble VEGF receptor 1 in which brain herniation occurred, the patient had cerebral edema that was not controlled with renal and hepatic dialysis, mannitol, and hypotonic saline . Other contributing factors for cerebral edema include possible direct effects of the virus in the brain and the combination of hepatic failure, persistent acidosis, and large osmotic shifts caused by dialysis; frontal lobe hemorrhage was not a significant factor because of its small size and location.\n\n【26】Severe CCHF has been attributed to a delayed IFN-α response in infected persons and to insensitivity of infected cells to the effects of the response (i.e. down-regulation of the host’s innate immune response) . The failure of ribavirin to prevent death in mice lacking type I IFN-α receptors (serum viral load was reduced, only delaying death), its poor ability to cross the blood–brain barrier, and its delayed initiation in this soldier, suggest a minimized effect of ribavirin against CCHF virus in this fatal case . However, a newer antiviral drug, favipiravir (a nucleoside analog also known as T-705), may offer promise as a future treatment option for CCHF. Mice lacking type I IFN-α receptors had no detectable CCHF-specific antibody if given favipiravir within 2 d of CCHF virus challenge, and all mice treated within 3 d of challenge survived with no detectable virus in the blood or organs .\n\n【27】This case of a soldier who died from CCHF illustrates the need to maintain an index of suspicion for CCHF and other VHFs in febrile travelers returning from VHF-endemic areas so that supportive care and appropriate infection control measures can be implemented early in the course of illness. This case highlights the critical care challenges in caring for a patient with severe CCHF and describes nosocomial CCHF virus infection in 2 HCPs who were receiving oral ribavirin PEP (600 mg twice daily). The 2 nosocomial infections stress the need for infection control policies that educate HCPs to use contact and droplet precautions (minimal requirements) when caring for patients presenting with fever and hemorrhage. In tertiary-care medical settings, procedures and emergency resuscitations performed on VHF patients with severe hemorrhage may pose risks for HCPs that are different from those in smaller hospitals in developing countries, where such procedures may not be available. Because the risk for aerosol production, splashing blood, and breaches in PPE (i.e. dislodged face shields, face masks, sleeve separation from gloves) is highest during resuscitative efforts, in this VHF case with severe hemorrhage, Tyvek suits with powered air-purifying respirators were indicated for the HCPs at highest risk for possible exposure to infectious materials (i.e. bronchoscopists). Although in this case, the potential antiviral effect of ribavirin may have been decreased because of late initiation of the drug and poor penetration of the blood–brain barrier, ribavirin may have contributed (along with CCHF IgG) to the patient’s improved clinical condition and decreased serum viral load. In addition, the probable CCHF virus seroconversion of 2 HCPs who had ameliorated or no symptoms after receiving ribavirin PEP may contribute further to the experience with and dosing regimen for ribavirin PEP in HCPs exposed to CCHF virus.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "59e7912b-f053-4a5f-960e-60cd415c5da3", "title": "Melanoma Among Non-Hispanic Black Americans", "text": "【0】Melanoma Among Non-Hispanic Black Americans\nAbstract\n\n【1】**Introduction**\n\n【2】Few studies have examined melanoma incidence and survival rates among non-Hispanic black populations because melanoma risk is lower among this group than among non-Hispanic white populations. However, non-Hispanic black people are often diagnosed with melanoma at later stages, and the predominant histologic types of melanomas that occur in non-Hispanic black people have poorer survival rates than the most common types among non-Hispanic white people.\n\n【3】**Methods**\n\n【4】We used the US Cancer Statistics 2001–2015 Public Use Research Database to examine melanoma incidence and 5-year survival among non-Hispanic black US populations.\n\n【5】**Results**\n\n【6】From 2011 through 2015, the overall incidence of melanoma among non-Hispanic black people was 1.0 per 100,000, and incidence increased with age. Although 63.8% of melanomas in non-Hispanic black people were of unspecified histology, the most commonly diagnosed defined histologic type was acral lentiginous melanoma (16.7%). From 2001 through 2014, the relative 5-year melanoma survival rate among non-Hispanic black people was 66.2%.\n\n【7】**Conclusion**\n\n【8】Although incidence of melanoma is relatively rare among non-Hispanic black populations, survival rates lag behind rates for non-Hispanic white populations. Improved public education is needed about incidence of acral lentiginous melanoma among non-Hispanic black people along with increased awareness among health care providers.\n\n【9】Introduction\n\n【10】Melanoma is one of the most common cancers in the United States, and incidence is increasing . Most melanomas are thought to be caused by a combination of exposure to ultraviolet radiation and characteristics of sun-sensitive skin . People with fair skin are generally at highest risk of melanoma; thus, recent research focused primarily on white and Hispanic populations . However, few studies of melanoma were conducted among non-Hispanic black populations . One of these studies found that ultraviolet radiation is associated with skin cancer risk among black men .\n\n【11】Skin type or complexion is a key risk factor for skin cancer . Melanin, which is produced in the skin, gives skin, hair, and eyes their color and protects the deep layers of skin cells from ultraviolet radiation damage . Skin that produces more melanin is naturally darker and provides more sun protection from ultraviolet radiation than light skin, which produces less melanin . Although exposure to ultraviolet radiation can cause DNA damage to all types of skin, some melanoma histology types are not attributable to exposure . Although black men and women in the United States are at lower risk of melanoma than white men and women, they often have melanoma diagnosed at a later stage and have poorer survival as a result . Furthermore, a range of skin complexions among non-Hispanic black people leads to variable risk from ultraviolet radiation exposure .\n\n【12】Previous analyses found that melanoma histologic type and the body site where it occurs differ by race . Acral lentiginous melanoma (ALM), which typically presents on the palms of hands, soles of feet, or nail beds, is associated with poor survival rates, and a greater proportion of melanomas diagnosed among non-Hispanic black people are ALM than are melanomas diagnosed among non-Hispanic whites . We examined melanoma incidence and survival among non-Hispanic black populations in the United States by age, stage at diagnosis, anatomic site, and histology.\n\n【13】Methods\n\n【14】We examined population-based cancer registry incidence data from the US Cancer Statistics (USCS) 2001–2015 Public Use Research Database . This database combines data from the Centers for Disease Control and Prevention’s National Program of Cancer Registries (NPCR) and from the National Cancer Institute’s Surveillance, Epidemiology, and End Results (SEER) program, covering the entire US population for 2011 through 2015. This data set excludes cancer diagnoses identified only on autopsy or death certificate and cancers in patients of unknown age or sex. Cancers were coded by using the _International Classification of Diseases for Oncology_ current at the time of diagnosis and later converted to codes in the _International Classification of Diseases for Oncology_ , third edition (ICD-O-3) . We defined invasive melanomas of the skin as having an ICD-O-3 site code of C440–C449 and an ICD-O-3 histology code of 8720–8790; analyses of histology data were limited to microscopically confirmed cases, and we excluded lentigo maligna melanoma histology because of its low incidence among non-Hispanic black populations. We limited our research to non-Hispanic black populations, except in some instances when non-Hispanic white populations were used as a comparison group. Non-Hispanic black and non-Hispanic white race/ethnicity was identified by the demographic information in the cancer medical case reports that were submitted to state cancer registries. We also presented data for the US Census region in which patients lived at the time of diagnosis: Northeast, Midwest, South, or West.\n\n【15】We used SEER\\*Stat software version 8.3.5 (National Cancer Institute) to calculate age-adjusted rates per 100,000 population (based on the 2000 US standard population), rate ratios, 95% confidence intervals, average annual counts, and 5-year relative survival. Rates based on a count of fewer than 16 patients were not reported because of concerns for rate stability . We limited our analyses to the most recent 5 years of data available to present the most current incidence rates. Rate ratios were considered to be significant if they differed from 1 at _P_ < .05. Survival data were taken from NPCR registries of the 39 participating states with survival data, which covered 81.1% of the US population for 2001 through 2014 . Survival rates were calculated by the actuarial method . The data used in this study were publicly available and contained no patient identifiable information; therefore, no institutional review board oversight was needed.\n\n【16】Results\n\n【17】From 2011 through 2015, 1,795 non-Hispanic black people were diagnosed with melanomas in the United States, an average of 359 per year (incidence rate, 1.0 per 100,000) . Rates were similar for men and women except in the oldest age group (65 or older), and increased with age, with the highest rates among men aged 65 or older (5.5 per 100,000). Half (55.3%) of all melanomas were diagnosed at a localized stage. Women were slightly more likely to be diagnosed at a localized stage (56.7%) than men (53.6%). Lower extremities, including legs and feet, were the most common anatomic sites where melanoma occurred among non-Hispanic black people (48.2%). Patterns by anatomic site did not vary widely by sex, although a slightly higher percentage of women than men had melanoma diagnosed on lower extremities, and men had slightly higher percentages of melanoma diagnosed on the head and neck and trunk than women. Most melanomas had nonspecific histology (63.8%). Among melanomas with specific histology, ALM were the most common (16.7%). Finally, more than half (58.7%) of melanoma diagnoses were among people in the southern United States compared with the Northeast (17.3%), Midwest (13.7%), and West (10.3%).\n\n【18】Half (55%) of non-Hispanic black people and 78% of non-Hispanic white people were diagnosed with melanoma at a localized stage; 18% of non-Hispanic black people and 9% of non-Hispanic white people were diagnosed with regional-stage melanoma . More non-Hispanic black people were diagnosed with melanoma with distant metastasis (16%) than non-Hispanic white people (5%) or with unstaged melanoma (10%) than non-Hispanic white people (8%). Furthermore, fewer non-Hispanic black people than non-Hispanic white people were diagnosed with superficial spreading melanoma (SSM) (non-Hispanic black, 29%; non-Hispanic white, 79%), whereas the proportions diagnosed with nodular melanoma were similar (non-Hispanic black, 25%; non-Hispanic white, 19%). Although rates for ALM among non-Hispanic black people and non-Hispanic white people were similar (0.2 per 100,000), the proportion of ALM diagnosed among non-Hispanic black people was much higher (non-Hispanic black, 46% vs non-Hispanic white, 2%).  \n\n【19】Percentage of non-Hispanic black and non-Hispanic white population diagnosed with melanoma, by stage at diagnosis, United States, 2011–2015. \n\n【20】We examined data on 2,848 melanomas diagnosed from 2001 through 2014 . Overall, the relative 5-year melanoma survival rate among non-Hispanic black populations was 66.2%, compared with 90.1% for non-Hispanic white populations. Survival decreased with age and was poorer among men . The earlier the stage at diagnosis, the higher the survival rate (85.8% for localized melanoma compared with 52.8% for melanoma with regional spread and 19.0% for melanoma with distant metastasis). By anatomic site, melanomas diagnosed on the upper extremities had the best survival rates (82.2%), whereas those on the lower extremities had the poorest survival rates (67.6%). Melanomas with unknown anatomic site (not otherwise specified and other) had a poor relative survival rate (26.2%). Survival rates were highest among people diagnosed with SSM (91.1%), whereas rates were poorest among those diagnosed with nodular melanoma (56.6%) and ALM (66.1%).\n\n【21】Relative survival rates differed between non-Hispanic black populations and non-Hispanic white populations by sex and by stage at diagnosis . Overall, regardless of stage at diagnosis, survival was lower among non-Hispanic black people than among non-Hispanic white people. Survival for localized stage for non-Hispanic black populations was 85.8% versus 97.5% for non-Hispanic white people; for regional stage, survival was 52.8% for non-Hispanic black people versus 63.8% for non-Hispanic white people and for distant stage, 19.0% for non-Hispanic black people versus 19.8% for non-Hispanic white people. Differences by sex showed similar patterns across all stages at diagnosis, with non-Hispanic black men and women having lower survival rates than non-Hispanic white men and women.  \n\n【22】Five-year relative survival rate of melanoma (percentage of people diagnosed with melanoma alive 5 years following diagnosis) among non-Hispanic black (NHB) and non-Hispanic white (NHW) populations, by stage at diagnosis, United States, 2011–2015. \n\n【23】Discussion\n\n【24】Our analysis of survival data from 39 states (81.1% of the US population) showed lower survival rates among non-Hispanic black people with melanoma than among non-Hispanic white people . Our results are consistent with previously published studies of non-Hispanic black populations that showed melanoma incidence rates increasing with age . A smaller proportion of non-Hispanic black people were diagnosed with melanoma in localized stages than with melanoma that had spread to distant anatomic sites, as shown in a previous study in Georgia . Delays in diagnosis of disease may lead to low survival rates; therefore, these differences highlight the need for increased awareness of melanoma in non-Hispanic black people among both the public and health care providers and more timely diagnosis.\n\n【25】Previous research indicated that most melanomas among non-Hispanic black people occur on the lower limbs and hip or lower extremities, including soles of the feet . One study found that ALM was located on the lower limbs 78.3% of the time . Our results were consistent with these findings, showing that most melanomas among non-Hispanic black women were diagnosed on the lower extremities . Furthermore, lower-extremity melanomas had the worst survival by anatomic site overall among both sexes. Differences by sex in melanoma incidence and anatomic site may indicate differences in use of sun protection, because non-Hispanic black women report more frequent use of sun protection than do non-Hispanic black men .\n\n【26】Dark skin confers more protection against ultraviolet radiation than fair skin, but skin type and skin tone can vary widely among people of the same race/ethnicity . About 13% of non-Hispanic black people reported getting a sunburn in the past 12 months; of these, 19% reported getting 4 or more sunburns in the same period . Many non-Hispanic black people perceive themselves to be at lower risk of skin cancer and report using sun protection less frequently than non-Hispanic white people . However, educational interventions have shown promise in improving knowledge and use of sun protection among people with dark skin .\n\n【27】ALM was the most common melanoma subtype diagnosed among non-Hispanic black populations in our study. Survival is poorer for ALM than for SSM, the most common subtype diagnosed among non-Hispanic white populations. Other studies have found that ALM is associated with lower survival rates than other subtypes and that the high proportion of ALM among non-Hispanic black people may be partially responsible for poor survival outcomes . Unlike other types of melanoma, ALM may not be related to ultraviolet radiation, which may account for its higher incidence among non-Hispanic black populations than among non-Hispanic white populations . One study indicated that risk factors for ALM include previous trauma or nevi (a type of mole) on soles or toes and possible genetic or environmental factors . More research is warranted into the causes of the ALM subtype, which affects a high proportion of non-Hispanic black people with melanoma.\n\n【28】Survival was also poor among non-Hispanic black people for nodular melanoma. As with ALM, nodular melanoma is associated with poor survival and is often diagnosed in advanced stages . Both ALM and nodular melanoma have atypical presentation, not adhering to the “ABCD” (asymmetry, border, color, diameter) guidelines for skin mole examination traditionally used to identify melanoma . ALM often resembles a bruise or lesion on the hand or foot or a bruised fingernail; nodular melanomas are often round and symmetrical with a single color . One study suggested that the atypical location of plantar melanomas is what leads to the delay in diagnosis so that patients present with deep tumors at late stages that are hard to treat successfully .\n\n【29】Our study had limitations. One limitation was the exclusion from the incidence database of people with melanomas reported only on autopsy or death certificates; however, these patients constitute less than 5% of the database and therefore would not affect the incidence rates significantly if they were included . Another limitation is that the survival database does not cover the entire US population; however, the 81.1% of the population included was the best representation of survival rates available.\n\n【30】A strength of our analysis is its comprehensive coverage of USCS data, which contain cancer incidence rates for the entire US population. Furthermore, this study provides an in-depth look at melanoma incidence and survival rates among the non-Hispanic black population that have not been previously reported.\n\n【31】Increased awareness is needed among the public and the medical community of the prevalence of the ALM histology type and the common anatomic sites where it occurs because of ALM’s atypical presentation. Opportunities exist to provide increased education and behavioral counseling among non-Hispanic black populations about minimizing exposure to ultraviolet radiation to reduce skin cancer risk and about the need for regular skin checks by medical professionals, particularly in non-sun–exposed areas such as the feet .\n\n【32】Tables\n------\n\n【33】Table 1. Age-Adjusted Melanoma Incidence Rates Among Non-Hispanic Black People, United States, 2011–2015 a  \n\n| Variable | All | Male | Female |\n| --- | --- | --- | --- |\n| Rate per 100,000 (95% CI) | AAC b | N (%) | Rate per 100,000 (95% CI) | AAC b | N (%) | Rate per 100,000 (95 % CI) | AAC b | N (%) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| **Total** | 1.0 (1.0–1.1) | 359 | 1,795 (100.0) | 1.2 (1.1–1.2) | 169 | 843 (100.0) | 1.0 (0.9–1.0) | 190 | 952 (100.0) |\n| **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** |\n| 0–44 c | 0.2 (0.2–0.3) | 56 | 281 (15.7) | 0.2 (0.2–0.2) | 24 | 118 (14.0) | 0.3 (0.2–0.3) | 33 | 163 (17.1) |\n| 45–64 | 1.3 c (1.2–1.4) | 131 | 653 (36.4) | 1.4 c (1.2–1.5) | 66 | 329 (39.0) | 1.2 c (1.1–1.3) | 65 | 324 (34.0) |\n| ≥65 | 4.6 c (4.3–4.9) | 172 | 861 (48.0) | 5.5 c (4.9–6.1) | 79 | 396 (47.0) | 4.1 c (3.7–4.4) | 93 | 465 (48.8) |\n| **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** |\n| Localized | 0.6 (0.5–0.6) | 198 | 992 (55.3) | 0.6 (0.6–0.7) | 90 | 452 (53.6) | 0.5(0.5–0.6) | 108 | 540 (56.7) |\n| Regional | 0.2 c (0.2–0.2) | 65 | 327 (18.2) | 0.2 c (0.2–0.2) | 33 | 167 (19.8) | 0.2 c (0.1–0.2) | 32 | 160 (16.8) |\n| Distant | 0.2 c (0.1–0.2) | 59 | 296 (16.5) | 0.2 c (0.2–0.2) | 29 | 144 (17.1) | 0.2 c (0.1–0.2) | 30 | 152 (16.0) |\n| Unstaged | 0.1 c (0.1–0.1) | 36 | 180 (10.0) | 0.1 c (0.1–0.1) | 18 | 80 (9.5) | 0.1 c (0.1–0.1) | 22 | 100 (10.5) |\n| **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** |\n| Head and neck c | 0.1 (0.1–0.1) | 34 | 166 (9.2) | 0.1 (0.1–0.2) | 18 | 87 (10.3) | 0.1 (0.1–0.1) | 16 | 79 (8.3) |\n| Trunk | 0.1 c (0.1–0.2) | 53 | 263 (14.7) | 0.2 c (0.2–0.2) | 30 | 149 (17.7) | 0.1 c (0.1–0.1) | 23 | 114 (12.0) |\n| Upper extremity | 0.2 c (0.1–0.2) | 56 | 278 (15.5) | 0.2 c (0.2–0.2) | 28 | 140 (16.6) | 0.1 c (0.1–0.2) | 28 | 138 (14.5) |\n| Lower extremity | 0.5 c (0.5–0.5) | 173 | 865 (48.2) | 0.5 c (0.4–0.5) | 72 | 362 (42.9) | 0.5 c (0.5–0.6) | 101 | 503 (52.8) |\n| NOS and other | 0.1 c (0.1–0.1) | 51 | 223 (12.4) | 0.1 (0.1–0.2) | 24 | 105 (12.5) | 0.1 c (0.1–0.1) | 27 | 118 (12.4) |\n| **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** | **Histology** **d** |\n| Superficial spreading melanoma | 0.1 (0.1–0.1) | 37 | 183 (10.4) | 0.1 (0.1–0.1) | 17 | 79 (9.6) | 0.1 (0.1–0.1) | 21 | 104 (11.1) |\n| Nodular melanoma | 0.1 (0.1–0.1) | 32 | 161 (9.1) | 0.1 (0.1–0.1) | 20 | 75 (9.1) | 0.1 (0.1–0.1) | 20 | 86 (9.2) |\n| Acral lentiginous melanoma | 0.2 c (0.2–0.2) | 59 | 294 (16.7) | 0.2 c (0.2–0.2) | 28 | 139 (16.8) | 0.2 c (0.1–0.2) | 31 | 155 (16.6) |\n| Melanoma NOS and other | 0.6 c (0.6–0.7) | 225 | 1,124 (63.8) | 0.7 c (0.7–0.8) | 107 | 533 (64.5) | 0.6 c (0.5–0.6) | 118 | 591 (63.1) |\n| **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** | **US Census region** |\n| Northeast | 1.0 (0.9–1.1) | 62 | 310 (17.3) | 1.1 (0.9–1.3) | 27 | 136 (16.1) | 1.0 (0.8–1.1) | 35 | 174 (18.3) |\n| Midwest | 0.8 c (0.7–0.9) | 49 | 246 (13.7) | 0.8 c (0.7–1.0) | 23 | 106 (12.6) | 0.8 c (0.6–0.9) | 28 | 140 (14.7) |\n| South | 1.1 (1.0–1.1) | 211 | 1,054 (58.7) | 1.2 (1.1–1.3) | 101 | 504 (59.8) | 1.0 (0.9–1.1) | 110 | 550 (57.8) |\n| West | 1.2 (1.0–1.4) | 37 | 185 (10.3) | 1.3 (1.1–1.7) | 22 | 97 (11.5) | 1.0 (0.8–1.3) | 19 | 88 (9.2) |\n\n【35】Abbreviations: AAC, average annual counts; CI, confidence interval; NOS, not otherwise specified.  \na  Source: National Program of Cancer Registries and Surveillance, Epidemiology, and End Results SEER\\*Stat database . Rates are age-adjusted to the 2000 US standard population (19 age groups – Census P25-1130).  \nb  Average annual counts may not sum to total because of rounding.  \nc  This rate is significantly different ( _P_ < .05) from the rate for the reference group (first listed group) as determined by rate ratios (not shown).  \nd  Histologic analysis was limited to microscopically confirmed cases. Lentigo maligna melanoma histology was excluded because of low numbers.\n\n【36】Table 2. Relative 5-Year Survival, Melanoma Among Non-Hispanic Black People, National Program of Cancer Registries, 2001–2014 a  \n\n| Variable | All | Male | Female |\n| --- | --- | --- | --- |\n| N (%) | Relative Survival (95% CI) | N (%) | Relative Survival (95% CI) | N (%) | Relative Survival (95% CI) |\n| --- | --- | --- | --- | --- | --- |\n| **All ages** | 2,848 (100.0) | 66.2% (63.8–68.5%) | 1,202 (100.0) | 59.2% (55.3–62.9%) | 1,646 (100.0) | 71.1% (68.1–73.9%) |\n| **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** | **Age, y** |\n| 0–44 | 579 (20.3) | 75.9% (71.8–79.5%) | 227 (18.9) | 64.1% (56.5–70.7%) | 352 (21.4) | 83.0% (78.3–86.8%) |\n| 45–64 | 1,069 (37.5) | 68.2% (64.6–71.4%) | 503 (41.8) | 59.0% (53.5–64.0%) | 566 (34.4) | 76.2% (71.5–80.2%) |\n| ≥65 | 1,200 (42.1) | 58.7% (54.3–62.9%) | 472 (39.3) | 56.6% (49.1–63.3%) | 728 (44.2) | 60.1% (54.4–65.2%) |\n| **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** | **Stage at diagnosis** |\n| Localized | 1,484 (52.1) | 85.8% (82.6–88.5%) | 571 (47.5) | 81.5% (75.5–86.2%) | 913 (55.5) | 88.4% (84.3–91.4%) |\n| Regional | 596 (20.9) | 52.8% (47.6–57.8%) | 265 (22.0) | 46.4% (38.6–53.8%) | 331 (20.1) | 57.7% (50.6–64.1%) |\n| Distant | 444 (15.6) | 19.0% (14.8–23.7%) | 219 (18.2) | 15.0% (9.8–21.1%) | 225 (13.7) | 22.3% (16.1–29.2%) |\n| Unstaged | 324 (11.4) | 63.2% (55.9–69.7%) | 147 (12.2) | 57.4% (46.0–67.2%) | 177 (10.8) | 67.2% (58.2–74.7%) |\n| **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** | **Anatomic site** |\n| Head and neck | 296 (10.4) | 69.7% (62.2–76.1%) | 153 (12.7) | 62.2% (51.2–71.5%) | 143 (8.7) | 77.6% (66.7–85.3%) |\n| Trunk | 413 (14.5) | 77.2% (71.2–82.1%) | 200 (16.6) | 76.0% (66.4–83.2%) | 213 (12.9) | 77.8% (69.7–84.0%) |\n| Upper extremity | 380 (13.3) | 82.2% (75.8–87.1%) | 161 (13.4) | 81.5% (70.5–88.7%) | 219 (13.3) | 82.9% (74.3–88.8%) |\n| Lower extremity | 1,413 (49.6) | 67.6% (64.0–70.9%) | 532 (44.3) | 57.2% (51.1–62.9%) | 881 (53.5) | 73.5% (69.1–77.3%) |\n| NOS and other | 346 (12.1) | 26.2% (20.9–31.8%) | 156 (13.0) | 16.3% (10.2–23.7%) | 190 (11.5) | 33.9% (26.1–41.9%) |\n| **Histology** **b** | **Histology** **b** | **Histology** **b** | **Histology** **b** | **Histology** **b** | **Histology** **b** | **Histology** **b** |\n| Superficial spreading melanoma | 321 (11.4) | 91.1% (83.9–95.2%) | 134 (11.3) | 92.7% (75.8–98.0%) | 187 (11.5) | 90.0% (81.1–94.9%) |\n| Nodular melanoma | 236 (8.4) | 56.6% (47.8–64.5%) | 105 (8.9) | 49.9% (37.2–61.4%) | 131 (8.1) | 62.3% (50.0–72.5%) |\n| Acral lentiginous melanoma | 452 (16.1) | 66.1% (59.2–72.1%) | 174 (14.7) | 54.7% (43.0–65.0%) | 278 (17.1) | 72.6% (64.0–79.5%) |\n| Melanoma NOS and other | 1,801 (64.1) | 62.1% (59.1–64.9%) | 770 (65.1) | 54.2% (49.5–58.7%) | 1,031 (63.4) | 67.6% (63.8–71.1%) |\n\n【38】Abbreviations: CI, confidence interval; NOS, not otherwise specified.  \na  Relative 5-year survival refers to the percentage of people with diagnosed melanoma alive 5 years following diagnosis compared with the general population. Source: National Program of Cancer Registries SEER\\*Stat database .  \nb  Histologic analysis was limited to microscopically confirmed cases. Lentigo maligna melanoma histology was excluded because of low case counts.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1cb0d199-4610-4de3-80a4-4de6603a7dee", "title": "Past, Present, and Future of Japanese Encephalitis", "text": "【0】Past, Present, and Future of Japanese Encephalitis\nJapanese encephalitis (JE) is a vector-borne viral disease that occurs in South Asia, Southeast Asia, East Asia, and the Pacific . An estimated 3 billion persons live in countries where the JE virus is endemic , and the annual incidence of the disease is 30,000–50,000 cases . The disease can cause irreversible neurologic damage. The JE virus (JEV) is mainly transmitted by the mosquito _Culex tritaeniorrhynchus_ , which prefers to breed in irrigated rice paddies. This mosquito species and members of the _Cx. gelidus_ complex are zoophilic. Wading ardeid water birds (e.g. herons and egrets) serve as virus reservoirs, but the virus regularly spills over into pigs, members of the family of equidae (e.g. horses and donkeys), and humans. The annual number of human deaths is 10,000–15,000, and the estimated global impact from JE in 2002 was 709,000 disability-adjusted life years (DALYs) . However, these statistics should be interpreted with care because the transmission of JE is highly dynamic; hence, the disease usually occurs in epidemics, and there is considerable fluctuation in estimates of its global impact. In 1999, JE caused 1,046,000 DALYs; in the 2 subsequent years, it caused 426,000, and 767,000 DALYs, respectively . Underlying factors that might explain these fluctuations are contextual determinants (mainly environmental factors) and spillover effects into the human population, which trigger epidemics.\n\n【1】Reporting of JE cases depends on the quality of health information systems and the ability to clinically and serologically diagnose the disease. JE is often confused with other forms of encephalitis. Differential diagnosis should therefore include other encephalitides (e.g. conditions caused by other arboviruses and herpesviruses) and infections that involve the central nervous system (e.g. bacterial meningitis, tuberculosis, and cerebral malaria) .\n\n【2】The main pillar of JE control is the use of a live attenuated vaccine for humans, which was developed some 40 years ago . Currently available JE vaccines are relatively safe and effective, but a drawback is that multiple doses are required . Effective delivery of the vaccines to poor, rural communities therefore remains a formidable challenge, and compliance and delivery costs have to be considered . Two vaccine candidates are in late-stage clinical development. The first one is a second-generation, live inactivated, single-dose vaccine grown in Vero cells. It is the yellow fever virus–based chimeric vaccine and will soon enter the market . The second candidate is an attenuated SA 14–14–2 virus strain, adjuvanted with aluminum hydroxide and also grown in Vero cells .\n\n【3】The vaccination of pigs represents another potential strategy to control JE, but it is not widely used for 2 main reasons. First, the high turnover in pig populations would require annual vaccination of newborn pigs, which would be costly. Second, the effectiveness of live attenuated vaccines is decreased in young pigs because of maternal antibodies .\n\n【4】Environmental management for vector control, such as alternative wetting and drying of rice fields (also known as intermittent irrigation), can substantially reduce vector breeding while saving water, increasing rice yields, and reducing methane emission . However, an effective irrigation requires well-organized educational programs, sufficient water at specific times during the rice-growing cycle, and an adequate infrastructure. In addition, because vectors are largely dispersed, intermittent irrigation should be applied to all rice fields over large areas and during the entire cropping season, which is often not feasible . Environmental management measures are most viable if they are readily integrated into a broader approach of pest management and vector management .\n\n【5】Chemical control of vector populations with insecticides such as pyrethroids, organophosphates, and carbamates plays a marginal role in JE control. In some circumstances (for example, when an outbreak of JE occurs in a densely populated area), space spraying can break the transmission cycle in the short term. However, rising levels of insecticide resistance have compromised the effectiveness of this emergency measure. Indeed, JE vectors that prefer manmade habitats, such as irrigated rice fields, are often heavily exposed to pesticide selection pressure. Although JE vectors are prone to develop insecticide resistance, usually this issue arises with insecticides that are not directly targeted to JE control, but rather are targeted to control of other pests .\n\n【6】We provide a historic account of the origin of JE and disease epidemics, describe the current situation, and discuss several factors that might explain the rise of JE incidence in some countries and its decline in others. Finally, we speculate about possible future trends.\n\n【7】### Historic Account\n\n【8】Genetic studies suggest that JEV originated from an ancestral virus in the area of the Malay Archipelago. The virus evolved, probably several thousand years ago, into different genotypes (I–IV) and spread across Asia .\n\n【9】The history of the clinical recognition and recording of JE dates to the 19th century. JE appeared as recurring encephalitis outbreaks in the summer season. The first clinical case of JE was recorded in 1871 in Japan. Half a century later, also in Japan, a large JE outbreak involving >6,000 cases was documented. Subsequent outbreaks occurred in 1927, 1934, and 1935. In 1924 an agent from human brain tissue was isolated; 10 years later, it was proven to be JEV by transfection into monkey brains. The role of _Cx. tritaeniorhynchus_ as a vector and the involvement of wading ardeids and pigs as reservoir hosts were demonstrated in 1938 .\n\n【10】### Present Situation\n\n【11】Nearly half of the human population currently lives in countries where JEV occurs. As shown in Figure 2 , JE is concentrated in China, India, and the Southeast Asian peninsula.\n\n【12】Current epidemiologic data on JE are summarized in Table 1 . These data were gathered from a diversity of sources, including peer-reviewed literature, specialized text books, and reports from national health departments and international organizations, such as WHO and the Food and Agriculture Organization. We also contacted national health ministries and WHO country offices for up-to-date information regarding country-specific JE statistics. This was accomplished by administering a standardized questionnaire.\n\n【13】There are 2 distinct trends in JE incidence. In countries such as Bangladesh, Cambodia, India, and Laos, where no specific diagnostic centers, vaccination programs, and surveillance systems are in place, the incidence of JE appears to have increased in recent years. On the contrary, in China, Japan, Nepal, South Korea, Sri Lanka, and Thailand, where vaccination programs are being implemented and regular surveillance is pursued, the incidence of JE is stable or declining. Despite the availability of WHO estimates, the situation in North Korea, Myanmar, Pakistan, and Papua New Guinea remains largely unknown.\n\n【14】However, underreporting is substantial in most JE-endemic countries; hence, it is conceivable that annual JE incidence is considerably higher than heretofore reported. For example, an estimate that used a representative incidence of 25/10,000 (not immunized), and a 1994 population estimate of 700 million children <15 years of age who live in JE-endemic areas suggested 175,000 cases annually with 43,750 deaths, and 78,750 cases with lasting sequelae. Adjusted for vaccine coverage, the estimate is 125,000 cases per year .\n\n【15】### Emerging JE\n\n【16】The emergence of JE can probably be explained by 2 factors. First, JE-endemic countries experienced an unprecedented population growth in recent decades. For example, in Eastern Asia, South-Central Asia, and Southeast Asia, the population more than doubled, from 1.7 billion in the mid-1950s to 3.5 billion 50 years later . Second, pig rearing has grown exponentially and rice-production systems, particularly irrigated rice farming, have increased both in cropping area and cropping intensity. In China, for example, pork production doubled from 1990 to 2005. Today, the total rice-harvested area of all JE-endemic countries (excluding the Russian Federation and Australia) is 1,345,000 km 2  , an increase of 22% in the past 40 years. Over the same time span, the total rice production in these countries has risen from 226 million tons to 529 million tons (+134%) .\n\n【17】Despite the fact that irrigated rice production and pig rearing are key factors in the transmission of JE, crude numbers fail to completely explain the complex interplay of various contextual determinants of the disease. Clearly, where rice production and pig rearing overlap, the impact on JE transmission is stronger than in areas where both activities are physically separated. This is the case, for example, in Malaysia, where the Malays mainly grow rice in 1 area and the Chinese rear pigs in another area. Here, the social determinant of religion (most Malays are Muslim) plays a decisive factor .\n\n【18】### Conclusion and Outlook\n\n【19】Discovered 125 years ago, JE has spread widely in the 20th century. Almost half of the human population now lives in countries where the disease is endemic. JE is a vector-borne epidemic with several features that are typical of an emerging infectious disease. The failure to halt the spread of JE in Asia and the Pacific region, despite the availability of an effective and inexpensive vaccine for 40 years, is of considerable public health concern. A similar conclusion has been drawn for yellow fever, a disease that can also be prevented by vaccination yet is rampant . Similar to schistosomiasis, malaria, food-borne trematodiasis, lymphatic filariasis, and dengue, one of the main reasons for the proliferation of JE is the ecologic transformation caused by water resources development and management that create suitable breeding sites for vectors and intermediate hosts, which in turn influence the frequency and transmission dynamics of these diseases .\n\n【20】High-quality data on transmission and incidence of JE are lacking in various countries. Although clinical and serologic methods to diagnose and monitor JE are available, health systems in many developing countries are unable to differentiate encephalitis diagnoses. Information regarding the distribution and public health importance of JE in Bangladesh, Cambodia, Indonesia, North Korea, Laos, Myanmar, Papua New Guinea, and Pakistan is inadequate. Such epidemiologic information, however, is mandatory for advocacy and allocation of resources for the control of JE. Examples of countries with successful JE control programs are Japan and South Korea. Before the 1950s, these countries experienced JE outbreaks, but incidence rates have remained stable for >2 decades. The following key control strategies and developments might explain the successful decline of JE in these countries: 1) large-scale immunization programs for humans, 2) pig immunization and the separation of pig rearing from human settlements, 3) changes in agricultural practices (e.g. enhanced mechanization and decrease of irrigated land), and 4) improved living standards (e.g. better housing and urbanization).\n\n【21】We speculate that JE incidence is increasing mainly in low-income countries. However, because reliable figures about JE emergence are lacking due to the absence of rigorous monitoring systems, more research is needed to support or refute this claim. In any event, lack of political will and financial resources are 2 important reasons why JE is often given low priority. These factors might explain the paucity of JE immunization programs for children in low-income countries where the disease is endemic. Nevertheless, Sri Lanka and Nepal, 2 countries with limited health budgets, and Thailand and Vietnam have managed to successfully control JE.\n\n【22】The national situations with respect to JE in the near future could develop as follows. We hypothesize that in Cambodia, Laos, and Myanmar, severe JE outbreaks could occur in the near future, partially explained by increases in irrigated rice farming and enhanced pig rearing. The JE situation in North Korea is not well understood, but on the basis of the population’s general health status, we predict that JE will likely remain a substantial public health issue in the years to come. Bangladesh and Pakistan are among the worst affected and most populous countries in which JE is endemic, and yet effective surveillance is missing. Outbreaks are likely to occur but will remain largely undetected. Muslim countries such as Bangladesh and Pakistan have traditionally been JE free. JEV transmission ends in Pakistan, even though the JE vector is abundant further to the West. The recent rise in JE in those countries has yet to be fully investigated and shows the complexity of transmission of this disease. In Indonesia, Malaysia, the Philippines, and Singapore, JE incidence has usually been low, and transmission will remain stable at a relatively low level. Given the paucity of data in Indonesia, a monitoring system should be established to document changes over time. Occasional small JE outbreaks might also occur in Papua New Guinea with spillover to Australia. Awareness of the disease and vaccination coverage rates are high in Australia, particularly in the region of the Torres Strait; hence, it seems unlikely that larger epidemics will occur anytime soon.\n\n【23】The overall trend of JE has been declining over the past 3 decades, and we anticipate that this trend will continue in the long term. Indeed, China and India influence JE figures on a global scale because most people living in JE-endemic areas are concentrated in these 2 countries. The incidence of JE in China has declined since 1971, coincident with economic growth and development. Meanwhile, the national JE vaccination program has been integrated into the Expanded Program on Immunization, and, at present, >110 million doses of a live, attenuated vaccine (SA14–14–2 strain) are produced annually. However, social, economic, and health policy changes in the face of privatization and a more market-based economy have led to reduced funding for immunization programs and somewhat reduced salaries for public health workers, particularly in the poorest provinces. As a consequence, these changes have contributed to increasing disparities in immunization coverage rates between the wealthy coastal and the less developed rural provinces and thus to the recently observed differences in levels of JE incidence between those regions .\n\n【24】The incidence of JE in India is still increasing, and the case-fatality rate of reported cases is high, i.e. 10%–30% . India currently has no national vaccination program, but the Ministry of Health has recently drawn up a plan in which children 1–12 years of age will be immunized. In Tamil Nadu and Uttar Pradesh, immunization programs are already running; thus, JE incidence might stabilize in those regions. However, overall trends for India are difficult to predict because JE endemicity is heterogeneous and because socioeconomic conditions for control differ substantially from 1 state to another .\n\n【25】Coverage of immunization programs and changes in agricultural practices will further influence JE transmission. In Taiwan, for example, the average age for the onset of confirmed JE cases shifted from children <10 years toward adulthood, explained by a high coverage of vaccinated children . Interestingly, the peak JE transmission, which occurred in August in the 1960s, shifted to June beginning in the 1980s. Improvements in pig-feeding technologies, which resulted in shorter periods from birth to pregnancy of female pigs, has been proposed as an important reason explaining the shift in transmission .\n\n【26】Climate change has been implicated in the increase of transmission of several vector-borne diseases . For example, a potential effect of climate change has been shown empirically for dengue virus, which is closely related to that of JE . Although JE vector proliferation might be influenced in a similar way than that predicted for dengue vectors, the potential impact of climate change on JE remains to be investigated. Indeed, climate change could not only directly increase JE vector proliferation and longevity but could also indirectly increase disease because of changing patterns of agricultural practices such as irrigation . Areas with irrigated rice-production systems may become more arid in the future, and the impact of flooding will be more dramatic, which in turn might result in JE outbreaks. Generally, extreme rainfall after a period of drought can trigger outbreaks in situations in which vector populations rapidly proliferate and blood feeding is spilling over to humans. Climate change may also influence migration patterns of birds, which may result in JEVs being introduced into new areas. However, little is known about reservoir bird migration patterns; hence, this issue remains to be investigated .\n\n【27】The culicines that transmit JE are usually highly zoophilic, and human outbreaks are therefore the result of a spillover of the virus from the animal reservoir into the human population. Studies in Sri Lanka showed that spillovers happen when there is rapid and dramatic buildup of _Culex_ spp. populations to the extent that the number of human blood meals passes a threshold after which virus transmission begins . Such rapid buildups are a result of extreme weather conditions or of rice fields in semi-arid areas being flooded before rice is transplanted. Information on vector population dynamics would be very useful in early warning systems and could also help improve targeting of control programs.\n\n【28】In conclusion, JE can be controlled, with effective surveillance systems and vaccines playing key roles. Although currently available vaccines are effective, the need for 3–4 injections compromises compliance and increases delivery costs . The advent of second-generation, cell-culture–derived vaccines will continuously replace mouse-brain and hamster kidney cell–derived vaccines. Such developments will hopefully boost current vaccination programs and deliver safer, more efficacious, and cheaper vaccines that comply with regulatory norms. Political will and commitment, financial resources, intersectoral collaboration (between the Ministries of Health and Agriculture and other stakeholders to set up vaccination programs for young children), as well as changing agricultural practices, pig vaccination, rigorous monitoring, and surveillance will go a long way in controlling JE.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ea5833ad-d99b-4939-abbe-84a71838aeb7", "title": "Development of the PEA-PODS (Perceptions of the Environment and Patterns of Diet at School) Survey for Students", "text": "【0】Development of the PEA-PODS (Perceptions of the Environment and Patterns of Diet at School) Survey for Students\nAbstract\n\n【1】**Introduction**\n\n【2】Few instruments assess key outcomes of school-based obesity interventions, including student perceptions of school environments and school-specific dietary intake patterns. This study describes development of PEA-PODS (Perceptions of the Environment and Patterns of Diet at School), a 2-part survey to measure these outcomes.\n\n【3】**Methods**\n\n【4】Part 1 (PEA) assessed student perceptions of policies, physical environment, and practices related to healthy eating and physical activity at school. Part 2 (PODS) assessed usual intake (ie, frequency, location obtained, and foods consumed) of breakfast and lunch. Foods consumed were presented by MyPlate categories (eg, Fruits, Grains). Students in grades 3, 6, and 9 participated in 2 phases: cognitive pre-testing (n = 10) and reliability/validation testing (n = 58). Both surveys were administered 1 week apart to assess test-retest reliability and 5-day food records validated PODS. Analyses included percent agreement (70% = acceptable), Pearson correlations, and Cronbach α’s.\n\n【5】**Results**\n\n【6】Cognitive pre-testing provided feedback on content, length, and age-appropriateness. Percent agreements were acceptable for test-retest reliability of PEA (71%–96%). The final version included 34 items with Likert-type responses in 4 subscales (α ≥0.78). For PODS, agreement for breakfast and lunch location was ≥75% for both reliability and validation. For foods consumed at breakfast, reliability agreement ranged from 74% to 93%, and validation agreement from 68% to 91%. For foods consumed at lunch, agreement ranges were 76% to 95% and 73% to 88%, respectively.\n\n【7】**Conclusion**\n\n【8】Both parts of the instrument demonstrate acceptable reliability, and PODS demonstrates acceptable validity. This demonstrates appropriateness for assessing perceptions of the environment and usual dietary intake patterns for school-based obesity prevention initiatives.\n\n【9】Introduction\n\n【10】Schools are a crucial setting for childhood obesity prevention efforts . Children spend much of their day at school, and schools strongly influence attitudes and behaviors related to healthy eating and physical activity (PA) . Federally mandated, district-level Local Wellness Policies (LWPs) guide nutrition and PA policies and practices in schools to enable these behaviors . The degree of LWP implementation in schools affects their impact; thus, it is important to evaluate both school-level implementation and student-level impact .\n\n【11】LWP implementation and schools’ policies and practices for healthy eating and PA are commonly assessed through single-reporter administrator surveys and interviews . This method introduces a high likelihood of bias and does not capture the students’ perceptions, which recent qualitative studies suggest are as meaningful as the policies and practices themselves for improving behavioral and weight outcomes . Brief, psychometrically sound measures are needed to quantify student perceptions and understand these outcomes.\n\n【12】In addition to understanding student perceptions, instruments are needed to assess the impact of LWP implementation on student behaviors. Objective assessment of school-day PA (eg, via accelerometry) and weight status is possible; however, no such objective measure exists for diet. Although several validated self-report questionnaires assess diet behaviors in schools, they focus on specific food groups (eg, fruits, vegetables, and beverages) or assess total energy intake and macronutrients . The limitations of such measures for children and adolescents are well described in the literature, including a limited ability to recall general patterns within a specific timeframe . To determine how LWPs influence students’ dietary behaviors during school, an easy-to-understand tool is needed to assess usual dietary patterns, rather than intake volume, across common food categories.\n\n【13】This article describes development of PEA-PODS (Perceptions of the Environment and Patterns of Diet at School). PEA assesses students’ perceptions of policies, practices, and environment for healthy eating and PA, and PODS assesses students’ usual diet at school, including frequency, location, and type of food consumed. Test-retest reliability of PEA-PODS and validity testing of PODS are reported.\n\n【14】Methods\n\n【15】Survey development was an iterative, 3-stage process: development, cognitive pretesting, and reliability/validation. PEA-PODS was developed by a team of researchers with expertise in nutrition and school wellness, and evaluated among students in grades 3, 6, and 9. Study procedures were approved by the University of Maryland, Baltimore institutional review board.\n\n【16】### Stage 1: Development\n\n【17】#### Section 1 (PEA)\n\n【18】We generated a question bank based on LWP requirements and instruments used in previous school-based studies . The question bank items asked students about policies and practices in their overall school, classrooms, cafeteria, and at recess. Items consisted of 5-option Likert-type response sets (“never” to “always” or “totally disagree” to “totally agree”).\n\n【19】No gold standards exist for constructs of student perceptions; however, face validity was established by distributing the initial question bank to a nationwide panel of 10 school wellness researchers who provided feedback and contributed additional items. The final question bank consisted of 47 items .\n\n【20】#### Section 2 (PODS)\n\n【21】To evaluate “usual” diet patterns during the school week, PODS asked students the frequency with which they usually ate breakfast and lunch acquired from various locations (eg, home, school cafeteria, school but not cafeteria, restaurant, or before-school program) during a usual school week, and inquired about the types of foods consumed within each MyPlate category (Fruits, Vegetables, Protein, Grains, and Dairy) . As MyPlate is taught widely in schools, it was perceived to be recognizable to and easily understood by most schoolchildren. PODS defined each category by using MyPlate definitions, and included an example of how to break down common foods (eg, slice of pepperoni pizza) into categories. Students were asked 2 questions about each category at each location they selected: how often the category was consumed using a 5-option Likert response set (never to always) and the type of foods consumed from a list of common items in each category (eg, toast, bread, or bagel). Example questions are listed in Table 2 .\n\n【22】### Stage 2: Cognitive Pre-testing\n\n【23】A cognitive pre-testing process with a convenience sample of 4 students each in grades 3 and 6 and 3 students in grade 9 attending 6 schools (n = 11; 73% male) assessed feasibility and age-appropriateness of PEA-PODS. Caregivers provided written informed consent. Students provided written assent and received a $10 gift card. Students took notes as they completed the survey, then gave feedback to the research team, in groups by grade.\n\n【24】For PEA, this feedback clarified Likert scales (the “agree” scale was confusing for some questions) and wording of several questions (clarification when students have multiple teachers).\n\n【25】For PODS, the feedback led to minor changes to food items within MyPlate categories, some altered wording, a second example of breaking down meals, more detailed instructions, and a “read aloud” feature. Additionally, 3 non-MyPlate categories were added based on participant feedback: Sweet/Salty Snacks, Sauces/Spreads, and Beverages. Following pre-testing, PODS consisted of a range of 10–175 questions, depending on branching patterns.\n\n【26】### Stage 3: Validity/Reliability study\n\n【27】A separate cohort of students in grades 3, 6, and 9 was recruited to assess test-retest reliability of PEA-PODS and validity of PODS . To detect a minimum correlation of 0.36 with 80% power, our target sample was 60 students. We chose to enroll a broad age range, selecting 20 students each from grades 3, 6, and 9. Students were recruited through snowball sampling, including emails on university listservs, social media postings in parent groups, and advertisements through partners at various institutions across the state. Students who attended public school and could complete online surveys independently were eligible. Parents and guardians were mailed study packets and signed informed consent. Student participants signed assent and received a $50 gift card.\n\n【28】Validation study recruitment, enrollment, and study timeline .\n\n【29】Abbreviations: PEA, Perceptions of the Environment; PODS, Patterns of Diet at School.\n\n【30】PEA-PODS was administered electronically by using Qualtrics Version 2017.11 (Provo, UT) on 2 occasions, 1 week apart (Time 1 and Time 2). The study took place across a period of 2 usual school weeks (without a designated holiday, teacher workday, or early dismissal). To assess test-retest reliability of PEA-PODS, students were emailed survey links on a weekend day, and asked to think about a “usual” school week in the current school year. To assess validity of PODS, participants completed a daily food record on a smartphone by using MovisensXS App Version 1.1. (Karlsruhe, Germany) after Time 2. The daily record is a commonly used method to validate school-based dietary instruments . Students were given a basic Android phone with the application pre-loaded and all other features disabled, and were instructed to fill out the food record once daily between 3:00 pm and 11:59 pm for 5 consecutive school days. The app used wording similar to the PODS survey to assess how often students ate breakfast and lunch, where they acquired each, and what type of foods they consumed.\n\n【31】### Data Analysis\n\n【32】Descriptive data were collected from participants during the consenting process. All analyses were conducted in SPSS V.22 (IBM Corp).\n\n【33】#### PEA\n\n【34】**Test-Retest Reliability.** Item-by-item percent agreement was used to examine reliability given a tolerance of +/− 1 (eg, if student answered “Always” at Time 1 and “Most days” at Time 2, responses were considered in agreement) to account for potential minor temporal instability . Agreement values between 70% and 79% were considered acceptable, 80% to 89% good, and ≥90% excellent . Items with poor agreement (<60%) or redundancy within subscales were removed or reworded to enhance clarity, and relevant, theoretical subscales were generated from remaining items. Sum scores were calculated for each subscale at Time 1 and Time 2, and Pearson correlations assessed scale test-retest reliability. Internal consistency of the overall scales and subscales was described by using Cronbach α’s.\n\n【35】#### PODS\n\n【36】**Test-Retest reliability.** Percent agreements assessed test-retest reliability of PODS. For location, agreement was examined for where students reported getting breakfast and lunch during the school week. For frequency, agreement was examined for how often students reported eating breakfast and lunch across locations by using a tolerance of +/− 1, as described above.\n\n【37】For consumption, agreement was examined in 2 ways. First, it was examined for consumption of any food within each MyPlate category (a dichotomous variable was created where 1 represented consumption of any items within that category across locations). Second, a continuous “healthy” composite score was created by using the Dietary Guidelines for Americans to assess whether overall eating patterns could be detected reliably . Each food item (eg, toast) was scored separately on a scale of 1–4 (very unhealthy to very healthy) by 3 authors with advanced nutrition degrees, who met to reach consensus . To prevent overinflation based on quantity of items selected, mean scores for each MyPlate category were summed to a maximum score of 32 (8 categories × 4 possible points per category). When participants reported getting breakfast, lunch, or both from 2 or more locations, the composite score from the most frequently reported location was used instead of combining all locations, to better identify patterns within locations. Pearson correlation and intraclass correlation coefficients (ICC) of this score described reliability of reporting healthy and unhealthy eating patterns.\n\n【38】Owing to branching patterns, students did not necessarily answer every question — if they reported never eating breakfast, they were not asked further questions about breakfast. Questions with fewer than 20 responses (~35% of the sample) were excluded from analysis.\n\n【39】**Convergent validity.** Percent agreement was also used to compare participants’ reports of location, frequency (with a tolerance of +/− 1), and consumption at Time 1 to aggregated food record data captured over 5 days. After calculating the composite score described above, Pearson correlation coefficients and ICCs assessed validity of reporting of healthy and unhealthy eating patterns. Only participants with at least 3 days of food records were included in analyses. Days of data were converted into a proportion of a full school week, then condensed into categories to match the survey (eg, 0.8–1.0 = Always).\n\n【40】Results\n\n【41】For the validity and reliability study, 87 students were recruited and 65 were screened for eligibility. Sixty-one (93.8%) consented to participate and completed PEA-PODS at Time 1; 58 (95.1%) completed Time 2 and were included in the reliability analysis, and 56 (91.8%) had 3 or more days of food record data and were included in the validity analysis .\n\n【42】Of 58 participants who completed PEA-PODS at both time points, 20 were in grade 3, 20 were in grade 6, and 18 were in grade 9. Participants were 53% male, 79% white or Caucasian, 2% black or African American, 2% Hispanic or Latino, 7% Asian, and 5% associated with more than 1 race. Participants represented 8 of 24 (33%) state school districts. Students completed the Time 1 and Time 2 surveys in an average of 50.7 (SD = 16.2) and 46.1 (SD = 16.5) minutes, respectively.\n\n【43】### PEA\n\n【44】#### Test-retest reliability\n\n【45】The original survey contained 47 items. After preliminary analysis of the reliability data, 13 items were removed and 6 were added to complete subscales or to balance a diet or PA construct or both (eg, add “ _I see signs or posters with pictures of healthy food_ ” to balance “ _I see signs or posters showing ways to be physically active_ ”). Additionally, some items were reworded slightly to match other items within subscales (eg, “ _I see teachers at my school_ ” changed to “ _Teachers at my school_ ”). Data were re-analyzed to generate the final results, and 40 items within 4 subscales were included in the final survey: 1) Perceptions of School PA and Nutrition Policies and Practices; 2) Perceptions of Teacher and Classroom Policies and Practices; 3) Perceptions of Cafeteria Policies and Practices; and 4) Perceptions of Recess Policies and Practices. Table 1 includes survey items, responses, item-by-item agreement, and Cronbach α’s and test-retest reliability for the first 2 subscales. Because of the addition of new questions and a skip pattern that reduced the sample size for 2 items, scale reliability could not be calculated for the cafeteria or recess subscales.\n\n【46】Overall, percent agreement was acceptable or good for all items . Both the School PA and Nutrition Wellness Policies and Practices and the Perception of Teacher and Classroom Wellness Policies and Practices subscales had a high Cronbach α (0.78 and 0.79, respectively) and good test-retest reliability (Pearson r = 0.84, _P_ < 0.001; r = 0.80, _P_ < 0.001, respectively).\n\n【47】### PODS\n\n【48】#### Test-retest reliability\n\n【49】Table 2 describes agreement findings for PODS. At Time 1, most participants (n = 51; 88%) reported usually getting their breakfast from home, compared with 4% (n = 2) from school and 4% (n = 2) from more than 1 location. Usual lunch locations included home only (n = 33; 57%), school only (n = 15; 26%), alternating between home and school (n = 6; 10%), and alternating between home and a restaurant or store (n = 1; 2%). Percent agreements for breakfast and lunch location were good (86% and 81%). At Time 1, 91% (n = 53) of the sample reported eating breakfast all or most days of a usual school week, and 91% reported usually eating lunch all days. For frequency, percent agreement was good (81%) for breakfast and excellent (91%) for lunch.\n\n【50】Across locations, agreement in reporting of consumption of foods within MyPlate categories ranged from 74% (Fruits) to 93% (Beverages) for breakfast, and from 76% (Vegetables) to 95% (Beverages) for lunch. Average overall healthy composite scores at Time 1 were 14.6/32 (SD = 5.3) for breakfast and 18.5/32 (SD = 3.9) for lunch . Pearson r for breakfast and lunch composite scores were 0.65 and 0.75, respectively. ICCs were 0.66 and 0.73. Pearson correlations and ICCs were significant ( _P_ < 0.01) and acceptable based on previous literature on adolescent diet indices .\n\n【51】#### Convergent validity\n\n【52】When comparing Time 1 with the aggregated food records, percent agreements were acceptable, good, or excellent for most location and frequency variables . For most MyPlate categories, consumption was similar between the Time 1 survey and the food records for both breakfast and lunch, with higher agreement shown for lunch variables . Agreement ranged from 68% to 91% for breakfast, and 73% to 88% for lunch.\n\n【53】Average overall healthy composite scores for the aggregated food record were 13.1/32 (SD = 4.9) for breakfast and 17.5/32 (SD = 4.7) for lunch. Pearson correlation coefficients between Time 1 survey and food records were 0.49 for breakfast ( _P_ < 0.01) and 0.34 ( _P_ < 0.05) for lunch. ICCs were similar. Both were significant ( _P_ < 0.01) and acceptable.\n\n【54】Discussion\n\n【55】As school-level LWP implementation continues to be a recommended strategy to prevent childhood obesity, it is critical to develop robust, comprehensive evaluation instruments to assess implementation and impact . This includes the perception of LWP implementation by students and other key stakeholders in addition to student impact measures (ie, weight status, physical activity, diet). This study fills critical gaps in available evaluation tools by developing PEA-PODS, a novel, reliable survey assessing students’ perceptions of the school environment, as well as a feasible, reliable, and valid method to assess students’ usual dietary patterns at school.\n\n【56】PEA represents a novel method to assess students’ perceptions of schools’ health-promoting environment. Students are influenced by their peers, teachers, and structural environments of their schools ; thus, understanding these perceptions is critical to explain health-promoting behaviors. Further, students are key stakeholders in LWP implementation as they uniquely understand their school’s environment, and sustainability depends on their knowledge, acceptance, and identification of gaps and opportunities . Qualitative studies have reported on student stakeholders’ perceptions of various aspects of the social and structural “health-promoting” environments of their school . PEA, which is easy to administer and reliable, builds on these studies by allowing for assessment on a larger scale. PEA also has broad generalizability, as it inquires about components of the school environment that are tied to federal policy requirements , and was developed with input from school wellness researchers nationwide. Use of this tool as part of a comprehensive evaluation can provide support for LWPs and other efforts to promote environmental changes in schools.\n\n【57】PODS has several unique strengths, in addition to demonstrating similar reliability and validity compared with existing dietary questionnaires administered during the school day . First, it uses MyPlate, a recognizable, focused framework, to help students categorize foods. The use of these categories within meal-specific prompts, which have been shown to improve self-report accuracy, demonstrates the potential of PODS to reduce known limitations of self-report dietary assessment tools for youth . Second, unlike previous tools, PODS can assess both inter- and intra-individual differences in types of food consumed by location of origin (eg, school, home, store on the way to school), which can provide information on healthfulness of foods offered at school compared with those brought from home. Previous studies using observational “lunchbox audits” have shown that schools meals are more nutritious than those brought from home. PODS provides a method to understand this on broader scale . Additionally, it can provide information about what is consumed within consistent food categories. Together, findings would provide support for school meal policies, increase student and parent buy-in for eating meals provided at school, and align with the USDA’s goal to ensure access to safe and balanced meals for all children by increasing school meals participation . PODS was also reliable and valid in detecting dietary patterns, as determined by a simple composite score. Finally, PEA-PODS was validated for a broad age range (grades 3–9) including adolescents, a population that is often excluded from dietary instrument validation studies .\n\n【58】### Limitations\n\n【59】Several limitations should be considered when interpreting study findings. Because we added or reworded 18 items in PEA, further psychometric testing is warranted. PODS, similar to other food frequency questionnaires, is potentially prone to self-report errors; however, the reliability and validity of PODS were acceptable compared with other questionnaires, and both PODS and food records had a low participant burden, which is critical for school-based studies. Although PODS was valid and reliable in detecting dietary patterns using a simple composite score, future studies should consider a more sophisticated system and larger sample to identify dietary patterns (eg, factor analysis). Additionally, as PEA-PODS does not account for the influence of parents or the home environment on student perceptions and diet, it should be administered in conjunction with parent surveys or other methods to understand the home environment.\n\n【60】Findings should also be considered within the small, homogenous sample. As our sample size limited analysis of variables that were not answered by at least 50% of the sample, we were unable to further investigate items within MyPlate categories. Further, we could not compare foods accessed from different locations because of insufficient variability. Finally, although our sample included a broad age range and representation from one-third of state school districts, it lacked racial and ethnic diversity.\n\n【61】### Conclusion\n\n【62】This study developed PEA-PODS, 2 distinct yet complementary surveys for examining student perceptions of the school environment and dietary patterns in school, filling a significant gap in the school wellness literature. Each survey may be administered independently. PEA-PODS was found to be reliable, with the dietary component (PODS) also demonstrating validity. Given the focus on LWPs following a recent final rule , PEA-PODS is timely and may be used to examine implementation and impact of LWPs in addition to other school health promotion initiatives.\n\n【63】Tables\n------\n\n【64】Table 1. Test-Retest Reliability of Items in PEA (Perceptions of the Environment) Question Bank, Including Subscales and Modifications\n\n| Subscale | Item | Most of the Time/Always % (Time 1) | Agreement % a |\n| --- | --- | --- | --- |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | If I get to school before school starts, there are places I can be physically active (like playgrounds or basketball courts). | 31.6 | 78.9 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | If I stay after school, there are places I can be physically active (like playgrounds or basketball courts). | 53.4 | 82.8 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I like Physical Education (gym) class. | 62.1 | 91.2 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | When my class has parties or celebrations during the school day, we get to eat things like candy, chips, cupcakes, and dessert. | 36.2 | 82.5 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I can easily get water at my school when I am thirsty. | 75.9 | 80.7 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I can easily get a sugary drink or non-diet soda at my school when I want one. | 6.9 | 86.2 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | We do physical activity (movement and/or stretching) during our school announcements. b | 24.1 | 71.9 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | My school’s announcements include messages about eating healthy foods and being physically active. b | 8.6 | 82.5 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I like the taste of the water at my school. b | 48.3 | 77.6 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I see signs or posters with pictures of healthy food. b | 47.4 | 77.2 |\n| School physical activity and nutrition wellness policies and practices (n = 11); Cronbach α = 0.78; Test-retest Pearson r = 0.84 ( _P_ < .001) | I see signs or posters showing ways to be physically active. | New question | – |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school give us short breaks in class where we stand up or get out of our seats to move (like brain breaks or energizers). | 14.0 | 91.1 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school have students run laps, do push-ups or another physical activity when someone misbehaves in class. b | 7.1 | 92.7 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school give us extra physical activity time for being well behaved in class. b | 13.8 | 82.5 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school let us drink water in class. | 72.4 | 81.0 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school let us eat snacks in class. | 12.1 | 91.2 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school give us treats (like candy) when we do a good job in class. | 12.1 | 91.4 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school talk about being physically active or playing sports. b | 10.3 | 81.0 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school (besides gym teachers) play sports or do physical activity with us during the school day. | 3.4 | 82.8 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school tell us it is important to move and be active. | 25.9 | 84.5 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school tell us it is important to eat healthy foods. | 27.6 | 74.1 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school eat healthy meals or snacks during the school day. b | 22.4 | 77.6 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school drink water during the school day. b | 53.4 | 77.6 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school drink sugary drinks or non-diet soda during the school day. b | 20.7 | 78.9 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers and principals at my school care about making my school a healthier place. | 60.3 | 86.2 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school are good role models for healthy eating. | 32.8 | 77.6 |\n| Perception of teacher/classroom wellness policies and practices (n = 16); Cronbach α = 0.79; Test-retest Pearson r = 0.80 ( _P_ < .001) | Teachers at my school are good role models for physical activity. | 41.4 | 75.9 |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | The breakfast provided by the school is healthy. | 31.6 | 78.9 |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | The breakfast provided by the school cafeteria tastes good. b,c | 45.5 | – |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | The lunch provided by the school is healthy. | 31.0 | 79.3 |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | The lunch provided by the school cafeteria tastes good. b,c | 41.2 | – |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | My school cafeteria is clean and a nice place to eat. | 50.0 | 86.2 |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | I have enough time to eat my lunch during my lunch period. b | 67.2 | 87.7 |\n| Perceptions of cafeteria wellness policies and practices (n = 7) c | At my school, we get to try new foods (like taste tests in the classroom or cafeteria). | 5.2 | 94.7 |\n| Perceptions of recess wellness policies and practices (n = 6) | My class has recess. | 39.7 | 96.4 |\n| Perceptions of recess wellness policies and practices (n = 6) | When someone misbehaves in class, teachers at my school take away their recess or make them sit out. | 29.8 | 85.7 |\n| Perceptions of recess wellness policies and practices (n = 6) | Kids are moving around and being active during outdoor recess. | New question | – |\n| Perceptions of recess wellness policies and practices (n = 6) | There are lots of things that kids can play with or do during outdoor recess. | New question | – |\n| Perceptions of recess wellness policies and practices (n = 6) | Teachers let kids stand still or sit during outdoor recess. | New question | – |\n| Perceptions of recess wellness policies and practices (n = 6) | When we have indoor recess, we are not allowed to move around and be active. | New question | – |\n\n【66】a  5-item Likert scale response, agreement based on +/−1; sample size range 56–58.  \nb  Item slightly re-worded.  \nc  Skip pattern on several items reduced the sample size to n = 37–39 and precluded scale diagnostics.\n\n【67】Table 2. Test-Retest Reliability and Validity Compared with a 5-day Food Record of PODS (Patterns of Diet at School), a Brief Instrument to Assess Location, Frequency, and Type of Foods Consumed at Breakfast and Lunch During a Usual School Week\n\n| Location, Frequency, Type | Breakfast | Lunch |\n| --- | --- | --- |\n| Test-retest Reliability a (n = 58) | Validity b (n = 56) | Test-retest Reliability a (n = 58) | Validity b (n = 56) |\n| --- | --- | --- | --- |\n| Baseline Survey n (%) | % Agreement | Food Record n (%) | % Agreement | Baseline Survey n (%) | % Agreement | Food Record n (%) | % Agreement |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? | Location: During a normal school week, when you eat breakfast/lunch, where do you usually get the foods you eat? |\n| Home | 51  | 86 | 49  | 85 | 33  | 81 | 31  | 67 |\n| School | 2  | 86 | 4  | 85 | 15  | 81 | 10  | 67 |\n| School (not cafeteria) | 1  | 86 | 0  | 85 | 2  | 81 | 0  | 67 |\n| Restaurant or store (on the way to school) | 1  | 86 | 0  | 85 | 0  | 81 | 0  | 67 |\n| Before- school program | 0  | 86 | 0  | 85 | n/a | 81 | n/a | 67 |\n| ≥2 locations | 2  | 86 | 3  | 85 | 7  | 81 | 13  | 67 |\n| Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c | Frequency: During a normal school week, how often do you usually eat breakfast/lunch? c |\n| Always (5 days) | 47  | 81 | 45  | 75 | 55  | 91 | 56  | 95 |\n| Most days (4 days) | 6  | 81 | 5  | 75 | 0  | 91 | 0  | 95 |\n| Half the time (3 days) | 2  | 81 | 3  | 75 | 3  | 91 | 0  | 95 |\n| Some days (2 days) | 3  | 81 | 2  | 75 | 0  | 91 | 0  | 95 |\n| Once in a while (1 day) | 0  | 81 | 1  | 75 | 0  | 91 | 0  | 95 |\n| MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d | MyPlate Categories: During a normal school week, how often do you usually eat fruits for breakfast/lunch? d |\n| Fruits | 34  | 74 | 30  | 73 | 56  | 90 | 48  | 82 |\n| Vegetables e | 18  | – | 13  | – | 36  | 76 | 36  | 73 |\n| Grains | 54  | 90 | 52  | 89 | 51  | 84 | 50  | 88 |\n| Dairy | 50  | 81 | 28  | 77 | 45  | 86 | 46  | 80 |\n| Protein | 36  | 78 | 45  | 71 | 49  | 84 | 41  | 75 |\n| Sweet/salty snacks e | 15  | – | 14  | – | 49  | 88 | 43  | 82 |\n| Spreads and sauces | 36  | 76 | 27  | 68 | 45  | 90 | 37  | 80 |\n| Beverages | 55  | 93 | 54  | 91 | 56  | 95 | 51  | 88 |\n\n【69】a  n = 57–58.  \nb  Comparison of Time 1 survey results to daily food records (≥ 3 days).  \nc  5-item Likert scale response, agreement based on +/−1.  \nd  Because of smaller sample sizes within categories, responses were dichotomized to yes/no for analysis.  \ne  Agreement is not reported for items answered by fewer than 20 (~35%) participant.\n\n【70】Table 3. Healthy Composite Score Reliability and Validity from PODS and 5 Daily Food Records (Pearson r and ICC)\n\n| Meal | Test-Retest Reliability a | Validation b |\n| --- | --- | --- |\n| Time 1 Mean (SD) | Time 2 Mean (SD) | Pearson r | ICC c | Weekly Mean (SD) | Pearson r | ICC c |\n| --- | --- | --- | --- | --- | --- | --- |\n| Breakfast healthy score | 14.6 (5.3) | 14.7 (5.6) | 0.65 ( _P_ < .001) | 0.66 ( _P_ < .001) | 13.1 (4.9) | 0.49 ( _P_ < .001) | 0.47 ( _P_ < .001) |\n| Lunch healthy score | 18.5 (3.9) | 17.5 (4.6) | 0.75 ( _P_ < .001) | 0.73 ( _P_ < .001) | 17.5 (4.7) | 0.33 ( _P_ \\= .013) | 0.33 ( _P_ \\= .007) |\n\n【72】Abbreviations: ICC, intraclass correlation coefficients; SD, standard deviation.  \na  Some participants had missing data; therefore, reliability for breakfast, n = 55; reliability for lunch, n = 57.  \nb  Some participants had missing data; therefore, validation for breakfast, n = 53; validation for lunch, n = 54.  \nc  Single-measurement, absolute agreement, two-way mixed effects model.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d0836153-7fff-45d7-a557-18af072cb3d6", "title": "Geographic Variation in Obesity at the State Level in the All of Us Research Program", "text": "【0】Geographic Variation in Obesity at the State Level in the All of Us Research Program\nAbstract\n\n【1】**Introduction**\n\n【2】National obesity prevention strategies may benefit from precision health approaches involving diverse participants in population health studies. We used cohort data from the National Institutes of Health All of Us Research Program (All of Us) Researcher Workbench to estimate population-level obesity prevalence.\n\n【3】**Methods**\n\n【4】To estimate state-level obesity prevalence we used data from physical measurements made during All of Us enrollment visits and data from participant electronic health records (EHRs) where available. Prevalence estimates were calculated and mapped by state for 2 categories of body mass index (BMI) (kg/m 2  ): obesity (BMI >30) and severe obesity (BMI >35). We calculated and mapped prevalence by state, excluding states with fewer than 100 All of Us participants.\n\n【5】**Results**\n\n【6】Data on height and weight were available for 244,504 All of Us participants from 33 states, and corresponding EHR data were available for 88,840 of these participants. The median and IQR of BMI taken from physical measurements data was 28.4 (24.4– 33.7) and 28.5 (24.5–33.6) from EHR data, where available. Overall obesity prevalence based on physical measurements data was 41.5% (95% CI, 41.3%–41.7%); prevalence of severe obesity was 20.7% (95% CI, 20.6–20.9), with large geographic variations observed across states. Prevalence estimates from states with greater numbers of All of Us participants were more similar to national population-based estimates than states with fewer participants.\n\n【7】**Conclusion**\n\n【8】All of Us participants had a high prevalence of obesity, with state-level geographic variation mirroring national trends. The diversity among All of Us participants may support future investigations on obesity prevention and treatment in diverse populations.\n\n【9】Introduction\n\n【10】Efforts to address the growing obesity epidemic in the US may benefit from precision health approaches that use integrated data on environments, social determinants of health, health behaviors, clinical conditions, and genomic factors that contribute to risks in individuals and in diverse populations . Few cohorts have sufficient size or diversity of data types and populations needed to investigate the multiple potential contributors to obesity in the US. The National Institutes of Health’s (NIH’s) All of Us Research Program (All of Us) is designed to integrate multiple data types for research, with the goal of including data from 1 million people collected longitudinally over 10 years. Baseline assessments include in-person study visits, during which physical measurements are taken by trained study staff, including height and weight measurements . Clinical data, including height and weight measurements used during clinical encounters, are also collected from electronic health records (EHRs) of All of Us participants who consent to provide these data. Reports of behavioral, environmental, social, and demographic characteristics are collected through surveys administered to a diverse participant population. Biologic samples, including blood samples obtained via venipuncture, are obtained for biomarker and genomic studies.\n\n【11】Population-based data, such as results of the Behavioral Risk Factor Surveillance System (BRFSS), are available to study obesity in the US. BRFSS is a large, nationally representative, telephone-based survey of more than 400,000 participants conducted annually by state health departments to collect information on self-reported risk behaviors, chronic health conditions, and use of prevention services . Additionally, the National Health and Nutrition Examination Survey (NHANES) captures data annually on a nationally representative sample of approximately 5,000 participants and includes data from survey interviews, in-person physical measurements, and laboratory tests . BRFSS and NHANES have relative strengths and limitations for conducting obesity research at the population level. The large size of BRFSS enables monitoring of population-based obesity prevalence at the state level. However, measures of weight and height in BRFSS are obtained by self-report and may be subject to underestimating obesity because of self-reporting bias . The smaller NHANES study data are collected in an examination unit and thus provide objective physical measurements rather than self-reported data. However, NHANES data are designed to provide estimates that are nationally representative, but not representative of smaller geographic areas . To address these concerns, Ward et al generated state-level projections of obesity prevalence in BRFSS data that correct for self-reporting bias by using the distribution of obesity in NHANES as a correction factor . Data from All of Us may contribute additional value to these existing population-based resources because of the large size and nationwide distribution of the All of Us cohort for which objective measurements and biomarker data are available through in-person measurement, along with longitudinal data collected through EHRs that are not available in other cohort studies of this scale. Additionally, participant diversity within All of Us may provide insight into factors relevant to obesity risk in various social and geographic contexts and population strata in the US. More than 80% of All of Us participants belong to population groups that have been historically underrepresented in biomedical research, including people who are aged 65 or older, Black or Hispanic, have low income (annual income below the federal poverty level), less than a high school diploma or equivalent, diverse sexual orientation and gender identities, and rural residents .\n\n【12】To facilitate research, the All of Us Researcher Workbench was developed to provide access to integrated data types in the program. All of Us cohort data types include physical measurements, EHR data, surveys, and biospecimens. Data on height and weight from EHR and physical measurements data sources have not previously been reported. The goal of our study was to demonstrate the utility of the All of Us Researcher Workbench for examining obesity prevalence across the US in the All of Us cohort. First, our study validated the data on height and weight by examining the concordance of the 2 data sources for height and weight (physical measurements and EHR) in All of Us. Second, the study estimated state-level obesity prevalence among All of Us participants by sex in physical measurements data, and compared and contrasted our estimates with BRFSS data previously reported at the state level by Ward et al .\n\n【13】Methods\n\n【14】**All of Us demonstration projects** . We conducted our study from May 2018 through December 2020 (data collected in this range are date stamped March 8, 2021). The goals, recruitment methods, study sites, and scientific rationale for All of Us have been described previously . Demonstration projects were designed to describe the All of Us cohort and reproduce previous studies for validation purposes. Our study was proposed by members of the All of Us investigator consortium and reviewed and overseen by the program’s science committee. Our analysis of deidentified data was classified as research not involving human subjects by the All of Us institutional review board. The initial release of data and tools used in our study was published recently . Results reported are in compliance with the All of Us Data and Statistics Dissemination Policy disallowing disclosure of results in group counts under 20.\n\n【15】**All of Us Researcher Workbench** . Our study used data available through the All of Us Researcher Workbench, a cloud-based platform where approved researchers can access and analyze All of Us data . The details of the surveys and methods of data collection are available in the Survey Explorer found in the All of Us Research Hub , a website designed to support researchers . Three currently available data types (survey, physical measurements, and EHR) are mapped to the common data model of the Observational Medical Outcomes Partnership, version 5.2, maintained by the Observational Health Data Sciences and Informatics collaborative . To protect participant privacy, a series of data transformations were applied. These included data suppression of codes with a high risk of identification, such as military status; generalization of categories, including age, sex at birth, gender identity, sexual orientation, and race or ethnicity; and date shifting by a random (less than 1 year) number of days, implemented consistently across each participant record. Documentation on privacy implementation and creation of a curated data repository (CDR) is available in the All of Us Registered Tier CDR Data Dictionary . The Researcher Workbench currently offers tools with a user interface built for selecting groups of participants, creating data sets for analysis, and workspaces  to analyze data. The Notebooks enable the use of saved data sets and direct query by using R (R Project for Statistical Computing) and Python 3 (Python) programming languages. This demonstration project used the All of Us curated data set (CDR version fc-aou-cdr-prod.R2020Q4R2) on a secure server on March 8, 2021, by using a Researcher Workbench interface, version 4, which includes data released by the program in December 2020.\n\n【16】**Study population** . Enrollment in All of Us began in May 2018, and the program currently enrolls participants aged 18 or older from a network of recruitment sites in more than 41 states. Enrollment will continue until at least 1 million participants are enrolled . All of Us is designed to recruit people who are underrepresented in biomedical research with the goal of enrolling its cohort from populations that are more than 75% underrepresented in terms of demographics, geographic location, and other characteristics, with at least 45% of participants coming from racial and ethnic groups that are underrepresented in research . Information on the sites from which participants are recruited has been described . Briefly, recruitment sites for All of Us were selected via an NIH submission and review process. The number of recruitment sites is evolving, and as of this writing, All of Us participants were enrolled at regional medical centers (93.6%), federally qualified health centers (3.2%), Veterans Health Administration sites (1.6%), and “direct volunteer” sites that can provide access for people who are not patients in a health care organization (a designated health clinic, blood bank, laboratory, or other facility) (1.6%). Participants in All of Us enroll digitally and provide informed consent to participate in the program through the website , via a smartphone application, or through one of the participating recruitment sites. After a person 1) consents to participate, 2) provides authorization to share EHR data, or 3) completes the initial baseline survey of demographic information, the participant becomes eligible for in-person visits to have physical measurements and biospecimens collected at one of the All of Us recruitment sites.\n\n【17】**Data collection from in-person physical measurements and EHRs.** Study protocols at each recruitment site were followed to measure objective height and weight during in-person visits. Height is measured via stadiometer and recorded in centimeters to the nearest millimeter. Weight is recorded in kilograms to the nearest 0.1 kg. Clinical data on height, weight, and calculated body mass index (BMI) (weight in kg/height in m 2  ) that were collected and recorded in participant EHRs during in-person clinical visits for routine patient care were extracted and transformed into the Observational Medical Outcomes Partnership common format at each enrollment site . For our analysis, height and weight values from physical measurements visits and EHR data were used to calculate BMI from both data sources. Our analyses also included survey data on demographics (sex and gender identity, education, race or ethnicity, age, and geographic location as US state of residence).\n\n【18】**Statistical methods** . We used the methods of Ward et al  to examine 2 categories of obesity consistent with Centers for Disease Control and Prevention (CDC) definitions of overall obesity (BMI ≥30) and severe obesity (BMI ≥35). We calculated obesity categories separately from physical measurements data only and EHR data only. Among those with both sources of data, we examined the correlation between the two with Pearson correlation coefficients. We examined baseline characteristics of participants from physical measurements data only and those who also contributed EHR data. We compared the characteristics of those with and without EHR data with χ 2  tests of significance for categorical variables. Sufficient data were available to display state-level obesity estimates by using All of Us BMI measurements calculated from physical measurements data, but at this writing, a sufficient sample of EHR data currently collected by All of Us was unavailable to display state-level estimates. We calculated the prevalence of obesity and severe obesity nationwide and for each state, overall and separately for men and women. We also calculated the prevalence of obesity or severe obesity in a complete-case analysis for All of Us participants with known data on age, binary sex assigned at birth (male or female), race or ethnicity (Black, White, Hispanic, and other groups), height and weight, and education levels (less than high school diploma or equivalent, high school diploma to some college, college graduate). Data from participants with nonbinary gender identities were not reported in these results because of small sample sizes. After deletion of participants with incomplete data, we compared the prevalence of obesity and severe obesity to reported BRFSS projections adjusted for self-report bias by Ward et al . We used ArcGIS version 10.7.1 (Esri) to map physical measurements of BMI data by state.\n\n【19】**Exclusions and missing data** . We excluded 9 states (Idaho, Iowa, Kentucky, Maine, Montana, New Hampshire, Ohio, Oklahoma, and Virginia) and the District of Columbia because they had fewer than 100 participants with height and weight data from physical measurements. We excluded individuals for whom information on the state of residence was not available or was suppressed to protect privacy. We included a total of 33 states (Alabama, Arizona, Arkansas, California, Colorado, Connecticut, Florida, Georgia, Illinois, Indiana, Kansas, Louisiana, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Nevada, Nebraska, New Jersey, New Mexico, New York, North Carolina, North Dakota, Oregon, Pennsylvania, South Carolina, Tennessee, Texas, Utah, Washington, and Wisconsin) in the analysis. Other states and territories either had no All of Us participants or had participant information suppressed because of low participant numbers.\n\n【20】To select height measurements from EHR data, we used Standard Concept Names (body height, body height measured, or body height stated), or the Source Concept Names (height, body height, or body height stated). To select weight measurements, we used Standard Concept Names (body weight, dry body weight measured, body weight measured, or body weight stated) or Source Concept names (weight, body weight, or body weight stated) . As quality control of EHR height and weight measurements, we used the methods of Koebnick et al  describing a large young adult multiethnic cohort to guide our weight, height, and BMI exclusion criteria. We excluded inpatient and emergency department visits, visits during pregnancy, body weight below 30 lb or above 1,000 lb, height below 4 ft or above 7 ft, 2 in, and BMI <5 kg/m 2  or ≥100 kg/m 2  . These exclusion and inclusion criteria were used for both physical measurements and measurements taken from EHR data sources . For people with multiple physical measurements of height or weight, the average measurement was taken (to reduce measurement error) and the most recent physical measurement date was used. Only EHR height and weight measurements taken within 1 year of physical measurements were used. For participants with multiple EHR height and weight measurements, we took the EHR height/weight measurement on the date closest to the physical measurements visit date. We also excluded 166 participants whose weight numbers from physical measurements consistently deviated from their EHR weight numbers, suggesting a probable documentation error in physical measurements weight units. Data were analyzed in the All of Us Research Workbench Jupyter notebook by using R software version 4.0.2.\n\n【21】Results\n\n【22】Physical measurements data were available for 244,504 participants . The mean age of participants with physical measurements data in this study was 51.1 years. EHR data were available from 88,840 (36.3%) study participants with physical measurements data, with a mean age of 53.9 years. The overall median and IQR of BMI using physical measurements data was 28.4 (24.4–33.7). The overall median and IQR of BMI using EHR data was 28.5 (24.5–33.6). Participants who contributed only physical measurements data and not EHR data were more likely to be male and underrepresented in biomedical research, including 26.3% who were non-Hispanic Black, 23.2% who were Hispanic, 13.6% who did not have a high school diploma or equivalent, and 50.4% who had a high school diploma or equivalent . EHR data were less frequently available from study participants from states in the South and West/Pacific than other regions.\n\n【23】**Obesity and severe obesity by geographic location and participant demographic characteristics** . Because of sample size limitations we calculated the prevalence of obesity and severe obesity nationwide for each contributing state, overall and stratified by binary sex assigned at birth. The prevalence estimates for obesity (BMI >30) and severe obesity (BMI >35) using All of Us physical measurements data were 41.5% (95% CI, 41.3%–41.7%) and 20.7% (95% CI, 20.6%–20.9%) with large variations across states   . Five states (Alabama, Connecticut, Mississippi, South Carolina, Tennessee) had overall obesity prevalence estimates greater than 50% . Data from Connecticut were primarily collected from federally qualified health centers that serve as the All of Us hub in that state. Eight states (Alabama, Arkansas, Connecticut, Indiana, Mississippi, South Carolina, Tennessee, Texas) had severe obesity prevalence of 25% or greater  .\n\n【24】**  \nPrevalence of obesity in the US among the All of Us Research Program cohort with a calculated body mass index (kg/m 2  ) of 30 or above, based on physical measurement data. Prevalence estimates were not calculated for states with fewer than 100 participants. \n\n【25】**  \nPrevalence of severe obesity in the All of Us Research Program, calculated BMI of 35 kg/m 2  or above, based on physical measurement data. Prevalence estimates were not calculated for states with fewer than 100 participants. \n\n【26】Women in All of Us in each state had a higher prevalence of obesity than men except for 7 states: Kansas, Minnesota, Missouri, Nebraska, Nevada, New Jersey, and North Dakota . Women had a higher prevalence of severe obesity than men in all states except Nevada, New Jersey, and Oregon. The prevalence of obesity and severe obesity differed by race and ethnicity, education, and age, qualitatively reflecting nationwide patterns seen in BRFSS data  .\n\n【27】**Correlation between physical measurements and EHR data.** BMI data from physical measurements were highly correlated with BMI data from EHRs, with a Pearson correlation coefficient of 0.973 (95% CI, 0.972–0.973). Where data were available, the height and weight measurements from physical measurements and EHR data were similar and highly correlated with Pearson correlation coefficients >0.93 for height and >0.98 for weight in all subgroups and overall.\n\n【28】**Geographic variation and comparison to existing BRFSS data projections** . We compared state-level prevalence of obesity and severe obesity in the All of Us cohort with projections reported by Ward et al for the year 2020 calculated by using BRFSS survey data corrected for self-reporting bias . Seven states exhibited a 10% or greater absolute difference from state-level BRFSS projections for obesity prevalence (Connecticut, Louisiana, Maryland, Missouri, North Carolina, Oregon, Washington) and 4 states for severe obesity (Connecticut, Louisiana, Oregon, South Carolina). Larger variation in state-level prevalence estimates was significantly associated with smaller state sample sizes, with a Pearson correlation coefficient of −0.50 (95% CI, −0.72 to −0.19).\n\n【29】Discussion\n\n【30】Our examination of the All of Us cohort data found a high prevalence of obesity (41.5%) and severe obesity (20.7%), which was consistent with national projections for the year 2020, as calculated from BRFSS data by Ward et al, projecting obesity and severe obesity prevalence at 42.0% and 19.4% respectively . We found geographic differences in obesity and severe obesity in the All of Us cohort, with the highest prevalence of each condition in states in the Southeast. Binary sex, race and ethnicity, and education patterns in obesity and severe obesity were also observed to be similar to national data . Our state-level results are congruent with those of Ward et al, with estimates differing by 10% in places that contributed small sample sizes to the analysis . Data from in-person physical measurements and clinical data from EHRs were tightly correlated, providing a measure of concordance validating these data sources.\n\n【31】Our findings suggest features of the All of Us cohort that may be relevant to promoting health equity and precision health in obesity research. Compared with White groups, non-Hispanic Black and Hispanic groups had a higher prevalence of obesity and severe obesity . Our analysis shows that the All of Us cohort is diverse with respect to race and ethnicity, age, education, and categories of body weight, which may enhance studies examining risks and associations in multiple population subgroups with social exposures that vary by geographic location and other factors. This is important because inclusion of diverse populations and a focus on addressing social inequities are key components of a precision population health framework for obesity prevention to provide tailored population health and prevention strategies .\n\n【32】Our study benefits from standardized measurements of weight and height from in-person physical measurements as opposed to self-reported measurements. Data from EHRs and physical measurements were closely correlated, providing some measure of construct validity in data collection. Future studies using the All of Us cohort may benefit from linkages with clinical EHR data, survey data, and biomeasures to better understand genomic, clinical, environmental, and social contributors to obesity and related conditions.\n\n【33】Our study had several limitations related to this early stage of analysis of All of Us data. Several states had few participants (<100), and we found great variability in the sample recruited by each site, which leads to variability in the precision of the state-level prevalence estimate. Additionally, All of Us is not designed as a representative geographic sample of the US . For example, states such as Connecticut predominantly recruited participants from federally qualified health centers, who may have different participant demographic characteristics than the state at large. Thus, prevalence estimates from All of Us are not expected to track estimates from surveys designed to produce representative population-based statistics. Our analysis did find similar obesity prevalence estimates in this analysis of All of Us data compared with studies designed to produce population-based estimates at the state level, which suggests that the All of Us cohort may provide the diversity in health status and geography needed to support investigations of risk factors that will advance the prevention and treatment of obesity. However, some racial and ethnic groups were not represented in sufficient numbers for large-scale analyses. All of Us continues to build community partnerships to increase accessibility to the program for diverse groups. Related to data availability, as of December 2020, the All of Us Researcher Workbench had EHR data for height and weight from 36% of the cohort included in our analysis. An important limitation is that EHR data differed by demographics and geographic region. Additional efforts will be important to ensure equitable availability of clinical data for subgroups who do not contribute data because of structural inequities, preferences, and other factors.\n\n【34】Our study’s ability to reproduce nationwide statistics was largely due to availability of physical measurements data obtained through All of Us. The ability to reproduce data may not be applicable to EHR data, which are not validated through medical record review, patient interviews, or similar processes that formed the basis of a sensitivity analysis for our study. Validation of EHR-based data elements and patient phenotypes may be important for future studies.\n\n【35】In summary, our demonstration project using existing methods for estimating obesity in the All of Us cohort shows parallels in obesity estimates found in data using national probability samples. Our analysis suggests 3 important points: 1) that All of Us has captured significant diversity within the US along the lines of binary sex, race and ethnicity, and age; 2) that the data show good internal consistency between physical measurements and EHR data and good external validity when compared with a second population-level study; and 3) that the data may have sufficient geographic spread to be useful for population-health studies and individual-level studies of contributors to obesity . As All of Us continues enrollment and adds data types (including genomics and biomarkers), additional studies are warranted to continue to monitor the applicability of the data to diverse populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "30a40144-e1c3-4998-aac9-c0d0764d3e40", "title": "Status of Beverages Served to Young Children in Child Care After Implementation of California Policy, 2012–2016", "text": "【0】Status of Beverages Served to Young Children in Child Care After Implementation of California Policy, 2012–2016\nAbstract\n\n【1】**Introduction**\n\n【2】Since 2012, licensed California child care centers and homes, per state policy, are required to serve only unflavored low-fat or nonfat milk to children aged 2 years or older, no more than one serving of 100% juice daily, and no beverages with added sweeteners, and they are required to ensure that drinking water is readily accessible throughout the day. We evaluated adherence to the policy after 4 years in comparison to the adherence evaluation conducted shortly after the policy went into effect.\n\n【3】**Methods**\n\n【4】Licensed California child care sites were randomly selected in 2012 and 2016 and surveyed about beverage practices and provisions to children aged 1–5 years. We used logistic regression to analyze between-year differences for all sites combined and within-year differences by site type and participation in the federal Child and Adult Care Food Program (CACFP) in self-reported policy adherence and beverage provisions.\n\n【5】**Results**\n\n【6】Respondents in 2016 (n = 680), compared with those in 2012 (n = 435), were more adherent to California’s 2010 Healthy Beverages in Child Care Act overall (45.1% vs 27.2%, _P_ < .001) and with individual provisions for milk (65.0% vs 41.4%, _P_ < .001), 100% juice (91.2% vs 81.5%, _P_ < .001), and sugar-sweetened beverages (97.4% vs 93.4%, _P_ \\= .006). In 2016, centers compared with homes (48.5% vs 28.0%, _P_ \\= .001) and CACFP sites compared with non-CACFP sites (51.6% vs 27.9%, _P_ < .001) were more adherent to AB2084 overall.\n\n【7】**Discussion**\n\n【8】Beverage policy adherence in California child care has improved since 2012 and is higher in CACFP sites and centers. Additional policy promotion and implementation support is encouraged for non-CACFP sites and homes. Other states should consider adopting such policies.\n\n【9】Introduction\n\n【10】Nearly 1 in 7 children aged 2–5 years in the United States is obese , and sugary beverage consumption is a major contributing factor to excessive weight gain in young children . Child care sites are an ideal setting for improving the quality of beverages consumed ; nearly two-thirds of US children younger than 5 years receive much of their daily nutrition in nonparental care settings .\n\n【11】Beginning in late 2016, licensed child care centers and homes participating in the federal Child and Adult Care Food Program (CACFP) were required to make readily available and actively offer drinking water throughout the day  and, beginning in late 2017, serve no more than 1 age-appropriate serving of 100% juice daily and unflavored low-fat or nonfat milk to children 2 years and older . Some states, including California, require licensed centers to follow CACFP standards, regardless of program participation . In addition, several states have adopted their own regulations on beverages served in child care, regardless of CACFP participation . For example, California’s 2010 Healthy Beverages in Child Care Act (AB2084), which went into effect in January 2012, requires the same provisions as CACFP for water, milk, and juice (only unflavored milk, 100% juice, and water served). In addition, AB2084 prohibits serving beverages with added natural or artificial sweeteners, such as soda and fruit drinks .\n\n【12】In 2012, several months after AB2084 went into effect, beverage provisions were evaluated in a statewide sample of licensed child care centers and homes in California . Only 60% of child care providers knew about the policy and about one-quarter of sites were fully adherent to the policy. We aimed to evaluate whether adherence has changed since 2012 by comparing to new data collected from licensed child care sites in California in 2016.\n\n【13】Methods\n\n【14】This study was a cross-sectional assessment comparing independent samples of licensed child care centers (hereinafter referred to as “centers”) and family child care homes (hereinafter referred to as “homes”) surveyed in 2012 and 2016. Methods used in 2016 were similar to those used in 2012, as described previously . The study was deemed exempt by the Committee for the Protection of Human Subjects at the University of California, Davis.\n\n【15】### Sample selection and survey instrument\n\n【16】We used California Department of Social Services and Department of Education databases to identify all licensed centers and homes (approximately 43,000). Sites were stratified into 6 categories: 1) Head Start centers (required to participate in CACFP), 2) state preschools (participate in CACFP or follow federal school meal program guidelines that meet or exceed those of CACFP), 3) other centers participating and 4) not participating in CACFP, and 5) homes participating and 6) not participating in CACFP. A sample of 2,400 sites equivalently distributed by category were randomly selected to participate.\n\n【17】The survey was based on a validated instrument  and was previously pilot-tested with minor modifications specific to study objectives. Questions were included about site characteristics as was a frequency checklist of beverage provisions the day before completing the survey for meals (breakfast, lunch, and dinner) and snacks served to young children (aged 2–5 in 2012, aged 1–5 in 2016). Respondents were instructed to report on beverages provided by the site and brought by parents, including those used for celebrations. Additional questions were asked about type of milk provided, how water was provided at meals/snacks, and facilitators and barriers to limiting fruit juice and providing only unflavored low-fat or fat-free milk.\n\n【18】### Sample recruitment and data collection\n\n【19】In fall 2016, all selected child care sites were mailed a postcard in English and Spanish with a link to the survey online (Qualtrics, version 08–2016, 2016); an email also was sent to sites with email addresses (n = 1,248). Paper surveys (in English for centers and both Spanish and English for homes) were mailed to nonrespondents 2 months later. When the response rate by child care category was less than 10%, nonresponders were contacted by telephone to encourage survey completion. A $5 gift card also was provided as an incentive.\n\n【20】### Data analysis\n\n【21】Data from paper surveys were double-entered to ensure accuracy, and online survey data (n = 155) were merged with the paper survey data (n = 581). Twenty surveys were excluded from analysis due to duplication (ie, completed on paper and online, in which case the first survey received was used) for an overall response rate of 30% (similar to the 31% achieved in 2012 ). Respondents to the 2016 survey were similar to the 2,400 randomly selected child care sites in terms of geography (mean difference in percentage by county of 0.3% \\[standard deviation (SD), 0.6%\\]) and whether they were at a center or home (mean, 6.0% \\[SD, 0.1%\\]). Surveys with more than 60% of incomplete questions on site characteristics were excluded (n = 36). Of the 680 surveys included in the 2016 analytical sample, 564 were from centers and 116 from homes. In 2012 and 2016, 12% and 11% of homes completed the survey in Spanish. From 2%–17% and 2%–9% of data were missing for survey items that assessed beverage practices and policy adherence, respectively, in 2012 and 2016. Imputation of missing data was not performed, and nonresponses were not included in the denominator when calculating percentage adherence to the policy.\n\n【22】Data were analyzed by using SAS version 9.4 (SAS Institute, Inc). Differences between survey respondent characteristics in 2012 and 2016 were assessed by using χ 2  tests or Fisher exact tests (when cell sizes were small) for categorical variables and _t_ tests for continuous variables. Binary variables on beverages (provided or not) and adherence to AB2084 (adherent or not) were created. Differences between beverages provided and adherence to AB2084 from 2012 to 2016 were assessed by using logistic regression models adjusted for CACFP status and whether the site was a center or home. The difference between CACFP participation was analyzed by using logistic regression adjusted for the site being center or home; difference between center and home was analyzed by using logistic regression adjusted for CACFP versus non-CACFP participant. Logistic regression was used to compare beverage practices and adherence to AB2084 in 2016 for the 6 types of child care categories and to conduct follow-up pairwise comparisons between the categories. Multiple comparisons were adjusted using the Bonferroni-Holm approach. _P_ values < .05 were considered significant.\n\n【23】Results\n\n【24】### Site characteristics\n\n【25】In 2012 and 2016, most survey respondents were the child care site director or owner, and most sites had been in operation for 5 or more years, provided full-day care (as opposed to only providing care for part of the day), and provided all foods and beverages served to young children. Between 2012 and 2016, there were small but significant differences in the proportion of sites in each child care category and providing full-day care .\n\n【26】### Adherence to beverage policy\n\n【27】Milk was provided the day before the survey by most sites (>90%) in both study years . In 2016 compared with 2012, of the types of milk most often provided, significantly fewer sites proided 2% milk and significantly more provided 1% and rice or soy milk to children aged 2 years or older. Additionally, of the types of milk ever provided, significantly fewer sites ever provided whole, 2%, rice or soy, or flavored or sweetened milk in 2016 compared with 2012. Only 2.2% of sites ever provided flavored or sweetened milk in 2016, down from 7.7% in 2012.\n\n【28】There were no significant differences between 2012 and 2016 in the proportion of sites making water available for self-serve indoors or outdoors. The proportion of sites providing water at the table with meals and snacks also did not change between 2012 and 2016. Both 100% juice and sugar-sweetened beverages (SSBs) were provided by significantly fewer sites the day before the survey in 2016 compared with 2012.\n\n【29】In 2016, sites overall were near full adherence to the beverage provision that restricts serving SSBs (97.4%) and require serving no more than 1 serving of 100% juice (91.2%) .  \n\n【30】**  \nAdherence to policy by California child care sites several months (in 2012) and several years (in 2016) after beverage policies for young children were enacted. Data on policy adherence were collected from 2012 and 2016 surveys of California licensed child care providers. \n\n【31】Overall adherence to all 4 beverage provisions increased significantly to nearly half of sites (45.1%) in 2016 compared with approximately one-quarter of sites (27.2%) in 2012 , and significant improvements were also seen with individual provisions for milk (65.0% vs 41.4%, _P_ < .001), 100% juice (91.2% vs 81.5%, _P_ < .001), and SSBs (97.4% vs 93.4%, _P_ \\= .006). In 2016, overall adherence was significantly higher for CACFP sites (51.6% of sites were adherent) compared with non-CACFP sites (27.9%), and for centers (48.5%) compared with homes (28.0%) .  \n\n【32】**  \nAdherence to all provisions of the California beverage policy, by type of child care in 2016 (n = 597). Data were collected from 2012 and 2016 surveys of California licensed child care providers. Because responses were missing for 1 or more beverages served to children aged 1–5 years, 597 sites in 2016 were assessed for full adherence to beverage policies. Results determined from pairwise comparisons. Values not sharing the same letter were significantly different from each other at _P_ < .05 after applying the Bonferroni-Holm adjustment for multiple comparisons. Abbreviation: CACFP, Child and Adult Care Food Program. \n\n【33】Significant differences existed by child care category as well, with Head Start, state preschools, and other centers on CACFP having the highest proportion of sites adherent, and non-CACFP homes having the lowest proportion adherent.\n\n【34】### Factors influencing adherence to provisions on juice and milk\n\n【35】Most sites reported that limiting juice was not challenging (82.5%). However, the barriers to limiting juice that were most commonly reported included child preference (9.5%), parent preference or practice (9.3%), it not being a priority for the provider (2.8%), and other reasons (3.3%), such as commenting that juice is reimbursed by CACFP. Facilitators to limiting juice included policy or written guidelines (26.4%), information for families (22.3%), support from parents or families (21.6%), and training for child care providers (13.9%).\n\n【36】Most sites reported that serving only unflavored low-fat or fat-free milk to children aged 2–5 was not hard (88.4%). Reported barriers included child preference (4.9%), parent preference or practice (3.8%), it not being a priority for the provider (0.6%), or other reasons (2.6%), such as high cost or unavailability in places where food is purchased. Facilitators to providing only unflavored low-fat or fat-free milk to children aged 2–5 included policy or written guidelines (27.6%), support from parents or families (19.5%), information for families (19.4%), and training for child care providers (15.3%).\n\n【37】Discussion\n\n【38】Improvements in adherence to providing healthy beverages in licensed child care sites were observed between 2012 and 2016 in California, 4 years after implementation of AB2084. Despite positive changes in beverage provisions since the law was enacted in 2012, additional room for improvement remains. More than half of sites were not adherent to all 4 beverage provisions, one-quarter did not make drinking water easily accessible throughout the day, and one-third were not adherent to the provision for milk. Provision of 2% milk (instead of 1% or nonfat) to children aged 2 years or older was the reason that most sites failed to be adherent to the milk policy. It is noteworthy that only 2% of California child care sites reported ever providing flavored or sweetened milk in 2016 to children aged 1–5 years. In comparison, more than 90% of US elementary schools serve flavored milk in grades K-12 .\n\n【39】New York City has a child care beverage policy similar to California, except that flavored milk and juice are not addressed. Compared with our 2016 California survey, in a 2010 study, adherence to the New York City policy was higher for milk (80%), and lower for SSBs (70%) and water (56%) . However, the New York City study only included centers, 90% of which participated in CACFP. Additionally, adherence in that study was assessed by observing beverage inventories of child care center facilities and classroom observations of beverages provisions during meals and snacks as opposed to self-report of beverages provided in our survey. These may contribute to differences between studies in adherence to beverage policy. In 2014 in Pennsylvania, where no state policy on beverages in child care exists, less than half (46%) of child care centers representing 88% of Pennsylvania counties answered no to the survey question “Are sugar-sweetened beverages provided?” . The lack of a policy in addition to the difference in survey question used to assess providing SSBs in the Pennsylvania study compared with our 2016 survey may have resulted in differences in reported SSBs provided. A more recent representative sample of child care centers and homes was assessed in 2017 in Georgia, where no state policy on child care beveverages exists, using the same questions as our California survey. In this study, most child care sites (96% in CACFP and 90% in non-CACFP) did not serve any SSBs on the day before being surveyed . It is noteworthy that in Georgia, fewer sites made water available for self-serve indoors (53%) and outdoors (41%), than in our California sample (84% and 83%, respectively). We are not aware of trend data on SSBs served in child care sites in other states that are comparable to our California data. However, growing awareness of the importance of healthy beverages for young children may have contributed to improved beverage servings practices among child care providers independent of policy. After increases during the end of the twentieth century , daily SSB intake by US children has declined in recent years. For example, in 2013 and 2014, 46.5% of children aged 2 to 5 years consumed an SSB on a given day, down from 68.9% a decade earlier .\n\n【40】Efforts to increase adherence to beverage policy should address provisions for milk and drinking water (adherence was lowest for serving only low-fat or non-fat milk to children 2 years or older) and ensuring water is easily available for self-serve both indoors and outdoors. Although few sites reported that the milk provision was challenging to implement, the barriers to implementation most often reported were child or parent preference. Policy or written guidelines were most often reported as facilitators of following the beverage policy on milk, which validates the need for state or federal policy. Support from parents or families and training for child care providers were also frequently reported. Although barriers and facilitators to implementation of the water policy were not included in this iteration of the child care survey, more than 75% of respondents in 2012 reported no barriers to making drinking water easily available throughout the day . Among those reporting barriers, sites participating in CACFP reported not getting any reimbursement from CACFP for offering water and a perception that CACFP requires that water not be served at meals and snacks (although no such CACFP regulation exists) . Given that the updated CACFP requirements include policy on water, milk, and 100% juice consistent with the California policy and that these federal requirements went into effect the year after our 2016 survey, follow-up assessments are warranted to examine whether adherence to the water and milk policies in California have since improved.\n\n【41】CACFP sites had higher adherence than non-CACFP sites with beverage policy, similar to other studies showing that CACFP participants provide better nutrition than those not participating in CACFP . CACFP sites receive more technical assistance and monitoring regarding required beverage policy than do child care sites that do not participate in the program. CACFP meal and snack reimbursements also may play a role in incentivizing healthier beverage provisions. In addition, it is possible that CACFP sites were more likely to be adherent to the California law in anticipation of the CACFP changes to requirements for water, milk, and 100% juice. Additionally, centers had higher adherence than homes, which may be due in part to the fact that California requires centers to meet CACFP nutrition standards regardless of program participation. Previous studies suggest that centers are often more adherent to nutrition policy than homes . Therefore, education and resources should target non-CACFP sites and homes to support improved beverage policy implementation for all young children in child care.\n\n【42】California licensed child care providers are informed of relevant state and federal requirements governing their operations in several ways. All licensed sites are required to stay up to date with policy, which are posted on the California Department of Social Services (CDSS) website . As of January 2016, all newly licensed child care sites are additionally required to participate in 1 hour of nutrition training, which includes information on AB2084 , per California’s 2013 Foundations for Healthy Nutrition in Child Care Act . However, less than 1% of survey respondents in the present study received their license after January 2016. Enforcement of policy is governed by the CDSS Community Care Licensing Division, which conducts facility evaluations for all licensed child care centers  and homes  once every 3 years. If a site is noncompliant, a citation may be issued based on a violation of the law; repeat violations warrant a civil penalty or fine. Providing additional guidance on AB2084 during licensing evaluations may be a cost-effective way to improve beverage nutrition in licensed child care.\n\n【43】As adherence to state and federal regulations regarding allowable beverages in California child care sites continues to improve, consumption of unhealthy beverages is likely to decrease, there being a strong and direct relationship between beverages served at child care sites and children’s beverage consumption . This may result both in long-term health care cost savings through preventing childhood obesity and possible cost savings for the child care site. Using simulation modeling, Wright et al found that policies in child care settings for restricting SSBs and only serving lower-fat milk, in addition to policies to reduce screen time and increase children’s physical activity, could result in a net health care cost savings per child of $6.15 per dollar spent administering the policy . That study also suggests that beverage policies can result in cost savings of $9.99 and $44.30 per year per child care site, on milk and SSBs, respectively . Further research is necessary to evaluate child beverage consumption at California child care sites and a nonsimulated cost–benefit analysis should be conducted.\n\n【44】This study has many strengths. Our study is the first that we know of to report data trends in providing beverages in child care in a state with a child care beverage policy. Additionally, compared with surveys used in other studies, our survey included multiple questions on types of beverages provided, clearly defined SSBs, and specified child age . Furthermore, it provided data from a large representative sample of California licensed child care providers of multiple child care site types, including family child care homes and CACFP and non-CACFP participants.\n\n【45】This study has several limitations. Even though consistent with other child care studies , the response rate was low for both survey years. Although sites were randomly selected, differences may exist between respondents and nonrespondents, and the sample may not be representative of California child care. In addition, results are based on self-report and sites may have reported desirable practices instead of actual practices. Analogous to how a 24-hour dietary recall is used for surveillance of the intakes of a population of individuals, for juice and SSB provisions, adherence in child care sites was based on beverages provided on a single day. However, assessing a single day may not fully capture long-term beverage practices. Given the observational nature of the study design, causality cannot be inferred; the improvements in beverage provisions from 2012 to 2016 may have resulted at least in part from secular trends in awareness of healthy beverages. Because all licensed sites in California are subject to the state policy, we were unable to include a control group. Finally, although licensed child care in California represents one-eighth of the total sites in the nation , our findings cannot be used to infer current practice in license-exempt or unlicensed sites and sites outside of the state.\n\n【46】To our knowledge, this is the first study to show continued improvement in adherence several years after the implementation of state policy mandating that only healthy beverages be served in child care settings. The study also builds on existing evidence that non-CACFP sites and homes compared with CACFP sites and centers, respectively, are less likely to be fully adherent to nutrition policy. This suggests that additional resources to support full adherence, including monitoring and enforcement of existing policy, should emphasize providing milk and water at targeted sites. Implementation of a statewide child care beverage policy was feasible and has resulted in improved beverages being served to young children in child care centers and homes in California. Thus, adoption of child care beverage policy by other states is warranted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4101c522-164a-4e55-b878-845688485de3", "title": "Characterization of Full Genome of Rat Hepatitis E Virus Strain from Vietnam", "text": "【0】Characterization of Full Genome of Rat Hepatitis E Virus Strain from Vietnam\nHepatitis E virus (HEV) is a positive-sense single-stranded RNA virus , classified as the sole member of the genus _Hepevirus_ in the family _Hepeviridae_ . Hepatitis E, caused by HEV infection, is a serious public health concern in developing countries and is recognized as sporadic and endemic acute hepatitis . To date, at least 4 genotypes of HEV have been isolated from humans . In addition, HEV has been isolated from other mammals, including pigs, wild boars, wild deer, rabbits, ferrets, bats, chickens, and wild rats . Much direct evidence indicates that HEV is transmitted from pigs or wild boars to humans, and therefore hepatitis E caused by genotypes 3 and 4 is recognized as a zoonotic disease .\n\n【1】Rat HEV was first isolated from Norway rats in Germany . Since then, rat HEV strains have been isolated from wild rats in other areas of Germany and detected in wild rats in the United States and Vietnam . Those results suggest that rat HEV infection is not restricted to Germany but is broadly distributed in wild rats throughout the world. The nucleotide sequences of the rat HEV isolated in Germany and the United States are similar; however, the partial sequences of the Vietnam rat HEV strain (V-105, JN040433) have been found to have 78.18%–79.43% identities with isolates from Germany, R63 and R68 . To confirm whether new genotypes of rat HEV exist, we amplified the entire genome of the rat HEV V-105 strain and analyzed the sequences. We confirmed that the rat HEV strain isolated in Vietnam belongs to a new genotype of rat HEV.\n\n【2】### The Study\n\n【3】The rat HEV used in this study was isolated from a 10% lung homogenate of a wild rat from Vietnam, which was positive for rat HEV RNA by reverse transcription PCR (RT-PCR) . Because of the limited availability of rat specimens that are positive for HEV RNA, we first transmitted the rat HEV to a laboratory rat (Wistar) to produce a large amount of virus for RNA extraction and genome amplification. After intravenous inoculation of the rat, fecal specimens positive for HEV RNA were collected, and a 10% suspension was prepared as described . RT-PCR was performed by using Superscript II RNase H –  (Invitrogen, Carlsbad, CA, USA) and primer TX30SXN . The full-length genome of the V-105 strain was amplified by RT-PCR with primers based on the nucleotide sequences of GU345042 and JN040433 . All PCR products were purified by using the QIAquick PCR Purification Kit (QIAGEN, Valencia, CA, USA) and cloned into TA cloning vector pCR2.1 (Invitrogen). The nucleotide sequencing was carried out by using an ABI 3130 Genetic Analyzer automated sequencer (Applied Biosystems, Foster City, CA, USA).\n\n【4】Because 901 nt of V-105, corresponding to nt 4108–5008 of the R63 genome, were already known , primers F13 and open reading frame (ORF) 1–R12 were designed. An ≈2,100-nt fragment of the C-terminus of the rat HEV V-105, nt 4923-poly (A) tail, was amplified with a pair of primers, F13 and TX30SXN, by the first RT-PCR. The ORF1 region was amplified with primers ORF1-F1 and ORF1-R12. Two fragments, 440 nt (nt 11–450) and 1,182 nt (nt 2990–4171), were amplified by nested PCR with 2 sets of primers, ORF1-F2/ORF1-R1 and ORF1-F7/ORF1-R12, respectively. On the basis of the nucleotide sequences of those amplified fragments, ORF1-F9, ORF1-F16, ORF1-R16, ORF1-F18, and ORF2-R21 were designed, and 3 fragments, 1,830 nt (nt 388–2217), 996 nt (nt 2080–3075), and 1,110 nt (nt 3991–5100), were amplified with 3 sets of primers, ORF1-F9/ORF1-R10, ORF1-F16/ORF1-R16, and ORF1-F18/ORF2-R21, respectively.\n\n【5】To amplify the N-terminus nonstructural region of V-105, we synthesized cDNA with primer ORF1-R14, and a DNA anchor (P-CACGAATTCACTATCGATTCTGGAACCTTCAGAGG-NH 3  ) was linked to the N-terminus of the cDNA by T4 RNA Ligase I (BioLabs, Tokyo, Japan). By using this anchor-cDNA as the template, the first and the nested PCRs were carried out with 2 sets of primers, anchor-1/ORF1-R14 and anchor-2/ORF1-R13, respectively.\n\n【6】The V-105 genome consisted of 6,927 nt plus a poly (A) tail of a still-undetermined length . The genomic structure of V-105 was, from the N-terminus toward the C-terminus, the N 5′–untranslated region (UTR) at nt 1–10, ORF1at nt 11–4900, ORF3 at nt 4917–5225, ORF2 at nt 4928–6862, the 3′-UTR at nt 6863–6927, and the poly (A) tail starting at nt 6928. ORF2 and ORF3 encode 644 aa and 102 aa, respectively, as do R63 and R68. However, ORF1 of V-105 encodes 1,629 aa, which is 7 aa shorter than either R63 or R68. The V-105 genome possessed 2 aa insertions (Ser-Pro) between the aa residues 591 and 592 and 9 aa deletions (Ser-Pro-Pro-Gly-Pro-Pro-Pro-Ala-Gly) between aa residues 852 and 853, corresponding to those of R63. The 3′-UTR was 65 nt as were R63 and R68. Unlike R63 and R68, only 1 additional putative ORF, corresponding to ORF4 (nt residues 27–578), was found in V-105, suggesting that other putative ORFs, ORF5 and ORF6 found in R63 and R68, are not common in rat HEV.\n\n【7】When the V-105 genome was compared with reported HEV genomes, the V-105 genome shared identities of only 50.5% with avian HEV, 53.6% with rabbit HEV, 53.7%–54.0% with wild boar HEV, and 53.1%–53.5% with HEV genotypes 1–4. In contrast, V-105 shared relatively high nucleotide sequence identities (76.8%–76.9%) with rat HEV strains (R63 and R68) . The nucleotide and amino acid sequences of ORF1, ORF2, and ORF3 of V-105 were compared with those of other HEV genotypes, and the identities among them are shown in Table 2 . Together, these results suggest that V-105 is more similar to rat HEV than to other HEV genotypes.\n\n【8】Phylogenetic trees were generated on the basis of the nucleotide sequences derived from the entire genome and ORF3 of the genotypes 1–4, wild boar, rabbit, chicken, and rat HEV isolates. These trees demonstrated that V-105 does not belong to any known genotype and should probably be classified into a new genotype .\n\n【9】### Conclusions\n\n【10】In this study we successfully amplified the entire genome of an HEV strain isolated from a wild rat in Vietnam. Phylogenetic analyses and nucleotide and amino acid sequence comparisons demonstrated that the complete rat HEV genome sequences were consistently well separated from those of mammalian genotypes 1–4, wild boar, rabbit, and chicken HEV and close to those of the rat HEV strains. Although the entire genome of V-105 shared nucleotide sequence identities of only 76.8%–76.9% with the isolates from Germany (R63 and R68), the ORF1 and ORF3 amino acid identities between V-105 and these isolates were 86.4%–87.0% and 66.7%, respectively, which suggests that V-105 can be classified into a new genotype of rat HEV. However, ORF2 has relatively high amino acid identities with R63 and R68 (91.6%–92.1%), indicating that the V-105 and rat HEV isolates from Germany share similar antigenicity. In fact, rat HEV–like particles derived from R63 are cross-reactive to serum from V-105–infected wild rats .\n\n【11】In conclusion, we isolated and identified rat HEV strain V-105 from a wild rat in Vietnam, and this strain was highly divergent from known rat HEV isolates. We propose that the strain from Vietnam, V-105, is a new member of the rat HEV genotype.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6f9b78ef-db86-494a-aad9-ac7351eaa3de", "title": "Myocarditis after Trimethoprim/Sulfamethoxazole Treatment for Ehrlichiosis", "text": "【0】Myocarditis after Trimethoprim/Sulfamethoxazole Treatment for Ehrlichiosis\nHuman monocytic ehrlichiosis (HME) is caused by an obligate intracellular gram-negative bacteria of the family _Anaplasmataceae_ . The first reported case of ehrlichiosis occurred in a man, 51 years of age, in Arkansas, United States. The man experienced a prolonged febrile illness after being bitten by ticks. Four years later, _Ehrlichia chaffeensis_ , the causative agent of this syndrome, was recognized . Ticks acquire _E. chaffeensis_ from a reservoir host, the white-tailed deer, and transmit the organism to humans during blood meals .\n\n【1】The clinical manifestations of HME range from a mild febrile syndrome to severe multisystemic illness. Although ehrlichiosis has sometimes been referred to as Rocky mountain spotted fever without the rash, a rash is not uncommon . Gastrointestinal, pulmonary, and central nervous system symptoms are well described, but cardiovascular disease is rare . Including the patient described herein, 4 persons in whom HME was diagnosed have had myocardial involvement.\n\n【2】**The Study**\n\n【3】A woman, 78 years of age, who had a 4-day history of fevers, chills, fatigue, and myalgias sought medical treatment after a syncopal episode. She denied dyspnea or chest pain. Ten days earlier, while in southern Virginia, she had noticed a tick on her ankle. After tick removal, the area became inflamed, and the patient received trimethoprim/sulfamethoxazole (TMP/SMX) from a local urgent care center.\n\n【4】Her vital signs were as follows: oral temperature 39.3°C, blood pressure 120/60 mm Hg, heart rate 90 bpm, respiratory rate 16 breaths per min, and oxygen saturation 98% on room air. Except for a 2–cm erythematous patch on the medial aspect of her right ankle, results of her physical examination were normal. Her leukocyte count was 1.4 ×10 3  cells/μL (34% non-segmented neutrophils) and platelet count was 66 × 10 3  /μL. Blood cultures were negative. The serum aspartate aminotransferase level was 173 U/L (reference <50 U/L), and the serum alanine aminotransferase level was 109 U/L (reference <50 U/L). Urinary sediment was unremarkable and a chest radiograph was normal.\n\n【5】The following day, the patient became dyspneic. A room air arterial blood gas revealed a pH of 7.43 (reference range 7.38–7.42), PO 2  of 70 mm Hg (reference range 75–100 mm Hg), and PCO 2  of 20 mm Hg (reference range 38-42 mm Hg). Pulmonary congestion without focal infiltrates was evident on a chest radiograph; an electrocardiogram indicated normal voltage, and saddle-shaped S-T elevations in 6 leads did not indicate ischemic distribution. On hospital day 3, her cardiac enzyme levels were markedly elevated: creatine kinase was 2,524 U/L (reference <325 U/L), creatine kinase-MB 27.3 ng/mL (reference <2.3 ng/mL), and troponin 27.3 ng/mL (reference 0–0.03 ng/mLl). A transthoracic echocardiogram revealed left ventricular systolic dysfunction with global hypokinesis and an ejection fraction of 30% (reference range 55–70%). Cardiac magnetic resonance imaging showed mild atherosclerosis.\n\n【6】Review of the patient’s peripheral blood smear from the day of admission showed several monocytes with characteristic morulae. She was empirically treated with doxycycline, and experienced prompt defervescence. Her cardiac decompensation resolved over several days and an ejection fraction of 70% was noted on echocardiogram 10 weeks after her admission.\n\n【7】Serologic results for _Borrelia bergdorferi_ , _Rickettsia rickettsii_ , _Anaplasma phagocytophilum_ , and _Francisella tularensis_ were negative. Initial serologic analysis for _E. chaffeensis_ demonstrated a positive result (IgM titer 320) and a negative IgG result . Convalescent-phase serologic analysis 1 month after discharge showed an IgG titer for _E. chaffeensis_ of 8,192, indicating recent infection; markers of cardiac injury had returned to reference levels.\n\n【8】**Conclusions**\n\n【9】The Dallas criteria for a diagnosis of myocarditis are applied based on histopathologic findings of an inflammatory cellular infiltrate with or without myocyte necrosis. Unfortunately, these criteria have low sensitivity, lack prognostic value, and necessitate an invasive procedure. Autopsy studies have shown that myocardial inflammation is not homogeneous and that sampling issues can contribute to a high rate of false negative endomyocardial biopsies. In clinical practice, the diagnosis is made based on the clinical syndrome, cardiac biomarkers, and electrocardiographic and echocardiographic findings .\n\n【10】Although a wide variety of pathogenic organisms have been associated with myocarditis, the etiology of this disorder in most patients remains idiopathic. In those cases in which an etiologic agent has been identified, viruses, in particular coxsackie B viruses, have been most frequently implicated . Endomyocardial biopsies during the late 1990s revealed other viral agents such as adenovirus, influenza A and B, cytomegalovirus, parvovirus B-19, and Epstein-Barr virus, among others . Myocardial involvement and cardiac dysfunction are frequently recognized in patients with HIV infection.\n\n【11】_Trichinella spiralis_ and _Toxoplasma gondii_ have also been recognized as causes of myocarditis, and, outside the United States, trypanosomes are commonly recognized etiologic agents . Bacteria can cause myocarditis through the effects of toxins, as is the case with diphtheria. Endotoxins have a direct suppressant effect on myocardial contractility, and myocardial dysfunction can occur in patients infected with gram-negative bacteremia. Alternatively, the presence of other organismsa, such as _Mycoplasma pneumoniae_ , _Chlamydophilia_ spp. or _Borrelia bergdorferi_ within the myocardium has been demonstrated, indicating a more direct effect .\n\n【12】Myocardial involvement is a rare complication of HME. Studies in which dog models were used have shown that acute infection with _E. canis_ is a risk factor for myocardial injury . In 2 earlier reported cases of HME with myocardial involvement, previously healthy men had clinical symptoms of HME confirmed by serologic analysis and left ventricular dysfunction and electrocardiographic abnormalities developed, similar to those of the patient in this study. Myocarditis resolved in both patients after doxycycline therapy . A third case of HME-related myocarditis occurred in a patient with Wegener granulomatosis . The patient was immunosuppressed by use of a tumor necrosis factor-α inhibitor, which may have contributed to the more severe manifestation of HME. This patient also appeared to recover after initiation of doxycyline. Jahangir et al. reported the sudden death of an otherwise healthy outdoor worker 19 days after the worker was bitten by a tick. An autopsy revealed transmural myocarditis and pulmonary congestion and serology consistent with human granulocytic anaplasmosis .\n\n【13】There is a demonstrated association between use of trimethoprim/sulfamethoxazole and fulminant manifestation of rickettsial diseases; the mechanism is unknown. Severe cases of HME have been reported in otherwise healthy adolescents taking short courses of trimethoprim/sulfamethoxazole , and in transplant patients receiving long-term regimens of sulfa drugs as prophylaxis . Our patient’s disease severity may have been exacerbated by her recently prescribed regimen of trimethoprim/sulfamethoxazole.\n\n【14】Myocarditis associated with human monocytic ehrlichiosis is distinctly uncommon. The possibility that this condition is caused by simultaneous infection with another microorganism cannot be excluded, although such dual infection is rare . Whether subclinical myocarditis may occur more frequently is unknown, but increased evaluation of cardiac function may reveal that this is a more common phenomenon.\n\n【15】The mechanism whereby this organism produces transient myocardial dysfunction is unknown. Rickettsial infection is characterized by direct endothelial cell infection and inflammation mediated by cytokine and chemokine activity, which leads to increased microvascular permeability . Occasionally, small vessel occlusion and local ischemia may occur. In patients with ehrlichiosis, perivascular lymphohistiocytic infiltrates may be seen on histopathologic examination .\n\n【16】The observed improvement in myocardial function following doxycycline therapy in the patient in this study and in those previously described suggests that myocardial dysfunction may be caused by direct infection of the myocyte or a toxic effect of sulfa drugs, or may be secondary to an innate inflammatory response mediated by _E. chaffeensis_ . It would be less likely to be a result of the host’s adaptive immune response, e.g. rheumatic fever. Considering the spread of this organism throughout the south central and southeastern United States, and the frequent use of trimethoprim/sulfamethoxazole to treat localized soft-tissue infections, it is necessary to recognize this pathogen as a treatable cause of myocarditis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9c172de7-2231-4570-8afe-7ada3ec2b43f", "title": "Secondary Shiga Toxin-Producing Escherichia coli Infection, Japan, 2010-2012", "text": "【0】Secondary Shiga Toxin-Producing Escherichia coli Infection, Japan, 2010-2012\nShiga toxin–producing _Escherichia coli_ (STEC), which is characterized by production of the Shiga toxin (Stx) or possession of the Stx-encoding genes, is a notable human pathogen that causes diarrhea, hemorrhagic colitis, and hemolytic uremic syndrome . In Japan, STEC infection is a notifiable disease, and ≈3,500–4,500 (2.7–3.5/100,000 population) annual cases of STEC infection (including asymptomatic carriers) have been reported since 2006 . For ≈50%–60% of these notifiable cases, serotype and Stx type of laboratory-confirmed STEC isolate and clinical manifestations associated with STEC infection are reported . To date, most STEC isolates from asymptomatic carriers collected in Japan were isolated during laboratory-based investigation of outbreaks or intrafamilial infections, together with those from sporadic patients. Therefore, we believe that the characteristics of such STEC isolates could be similar to those from ill patients. We conducted a large-scale study to investigate the prevalence and characteristics of STEC isolated from healthy adults in Japan to evaluate the potential public health risk of infection due to secondary STEC infection.\n\n【1】### The Study\n\n【2】A total of 2,774,824 fecal samples were collected from 472,734 healthy adults throughout Japan during April 2010–March 2012; these samples were examined at Japan Microbiological Laboratory Co. Ltd. (Sendai, Japan) . These healthy persons included food handlers and those who worked in childcare and eldercare facilities; each person was tested \\> 1 times over 2 years. When >1 STEC isolate from the same patient had the same serotype and virulence factor type, we used the first detected isolate in this study. Of 472,734 healthy adults examined, 398 (0.08%) were positive for STEC. Therefore, the estimated incidence rate of asymptomatic carriers among healthy adults was 84.2/100,000 population, indicating that asymptomatic STEC infections are highly prevalent among healthy adults.\n\n【3】A total of 399 STEC organisms were isolated from 398 healthy adults. O-serogrouping showed that 339 isolates comprised 61 different O serogroups; 60 isolates were O serogroup untypeable . Two isolates obtained from the same person >12 months apart belonged to serogroups O103 (first isolate) and O91 (second isolate). The dominant O serogroup of isolates from healthy adult carriers was O91 (n = 89, 22.3%), followed by O103 (n = 23, 5.8%). Of STEC infection cases associated with patients reported during 2010–2011 in Japan, 97.4% were caused by either STEC O157 (68.8%), O26 (16.9%), O111 (3.9%), O145 (3.1%), O103 (2.8%), or O121 (1.9%) isolates . In this study, STEC O157 (n = 13), O26 (n = 6), O145 (n = 2), O103 (n = 23), and O121 (n = 1) isolates were detected, but STEC O111 isolates were not . Therefore, the STEC isolates belonging to these 6 O serogroups represented only 11.3% of all isolates from healthy adults. These results show that the prevalence of O serogroups among STEC from healthy adults was clearly different from that among symptomatic patients.\n\n【4】We also determined the _stx_ type of STEC isolates and investigated the presence of virulence factors . Of 399 STEC isolates, 201 (50.4%) harbored the _stx1_ gene only ( _stx1_ type), 160 (40.1%) harbored the _stx2_ gene only ( _stx2_ type), and 38 (9.5%) harbored both _stx1_ and _stx2_ genes ( _stx1/stx2_ type) . Adherence factors that contribute to virulence of STEC, _eae_ , _saa_ , and Eib, were detected in 55 (13.8%), 125 (31.3%), and 102 (25.6%) isolates, respectively  . No STEC isolate harbored >2 of these adherence genes/factors; _aggR_ was not detected in any isolates; and 117 (29.3%) isolates did not contain any adherence genes/factors.\n\n【5】The combination of _stx2_ and _eae_ genes in STEC is considered a risk factor for high virulence of STEC because these 2 genes are often found together in STEC isolates associated with severe disease such as hemolytic uremic syndrome . To investigate the prevalence of such risk factors among STEC isolates from healthy adults, we analyzed _eae_ \\-positive isolates (n = 55). In this study, all STEC isolates belonging to 1 of 6 major O serogroups (O157:H7/H-, O26:H11, O111:H-, O103:H2/H11, O145:H-, O121:H19) frequently found in patient-derived isolates were positive for the _eae_ gene, except for an O103:H2 isolate . O156:H-/HUT (n = 4), O76:H7 (n = 1), O177:H- (n = 1), and O-untypeable (n = 5) isolates were also positive for the _eae_ gene. From analysis of _stx1_ , _stx2_ , and their subtypes, 16 of 55 _eae_ \\-positive isolates harbored the _stx2_ gene; their _stx2_ subtypes were _stx2a_ (n = 5), _stx2c_ (n = 10), and _stx2e_ (n = 1) . _stx2a_ and _stx2c_ were the _stx2_ subtypes most commonly found in patient-derived isolates . The STEC isolates harboring both _eae_ and _stx2_ occupied 4% of all STEC isolates from healthy adults. Therefore, we estimated that the incidence rate of healthy asymptomatic carriers infected with STEC isolates harboring both _eae_ and _stx2_ was 3.4/100,000 population (16/472,734 isolates). These results highlight the potential risk of serious STEC infection, which may be transmitted by secondary transmission from asymptomatic carriers.\n\n【6】### Conclusions\n\n【7】We found that the incidence rate of STEC infection in healthy asymptomatic carriers was 84.2/100,000 population. This finding suggests that the risk of secondary transmission to susceptible persons may be higher than originally thought. However, many STEC isolates from healthy adults belong to O serogroups that are rarely found in STEC isolates from symptomatic patients, and >80% of those isolates did not have the _eae_ gene that is frequently detected in STEC isolates from symptomatic patients.\n\n【8】Recently, STEC outbreaks caused by isolates from 6 major O serogroups (O157:H7/H-, O26:H11, O111:H-, O103:H2/H11, O145:H-, O121:H19) have been frequently reported in childcare facilities in Japan . Person-to-person transmission is considered a major route of infection in such outbreaks, and many adult family members identified in such outbreaks had asymptomatic cases . In addition, those who work within child- and elder-care facilities are possibly more likely to be exposed to STEC organisms than are the general population because STEC organisms are more likely to be shed in higher numbers in children and elders from such facilities . We found that STEC O157:H7/H-, O26:H11, O103:H2, O121:H19, and O145:H- were isolated from asymptomatic carriers. These findings suggest that a portion of STEC infection due to these serotypes may be caused by secondary transmission through asymptomatic carriers.\n\n【9】Although the prevalence of STEC O157, O26, and O111 in retail raw foods is monitored by the National Food Surveillance System in Japan, prevalence of STEC belonging to other O serogroups in foods is unknown. In 1 study regarding STEC strains from food-producing animals in Japan during 1999–2001, of the bovine isolates, 30.6% belonged to serotypes frequently implicated in human disease, and 37% harbored the _eae_ gene . Isolates with such serotypes and _eae_ were not found among the isolates from swine . Many STEC isolates from food-producing animals, as with those from healthy adults, displayed characteristics rarely found in patient-derived isolates . Food handlers may be at a greater risk of STEC transmission through food preparation, because they are more likely to be exposed to organisms from food-producing animals than the general population .\n\n【10】Our findings provide scientific evidence that can be useful in the management of STEC infection, in particular, in detecting asymptomatic carriers in Japan. Such identification could result in a decrease in asymptomatic carriers and in secondary transmission of STEC organisms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "42119183-e4b4-4962-9c04-9c0d13d8d5f9", "title": "Imported Chikungunya Virus Infection", "text": "【0】Imported Chikungunya Virus Infection\n**To the Editor** : Chikungunya is a disease caused by an arboviral alphavirus transmitted to humans by _Aedes_ mosquitoes ( _Aedes aegypti_ , _Ae_ . _albopictus_ ). Symptoms include fever, myalgia, rash, and joint pain (which can last for several months) . During the 2005–2006 epidemics on Reunion Island, clinical manifestations such as severe hepatitis, severe maternal and fetal disease, and meningoencephalitis not described previously were observed . Occurring in an immunologically uninfected population, this outbreak spread quickly, infecting approximately one third of the population (266,000 of 775,000 inhabitants) . The case-fatality rate on Reunion Island was estimated to be 1/1,000 cases, with excess deaths observed mainly among persons \\> 75 years of age .\n\n【1】Chikungunya disease is endemic to western, central, eastern, and southern Africa; on Indian Ocean and west Pacific Ocean islands; and in Southeast Asia . Before 2005–2006, no outbreak of this disease had been described on islands in the Indian Ocean (Comoros, Mayotte, Madagascar, Reunion Island, Mauritius, and Seychelles). Since the epidemic on Reunion Island, many imported cases caused by this arbovirus have been reported elsewhere in areas where the disease is not endemic, particularly in Europe and the United States.\n\n【2】The main competent vector of chikungunya virus, a mosquito, _Ae. albopictus_ , is indigenous to Southeast Asia and some islands of the western Pacific and Indian Ocean. The mosquito spread to the eastern Pacific, the Americas, Central Africa (Nigeria, Cameroon, Equatorial Guinea and Gabon), Europe, and the Middle East . Entomologic studies have shown that _Ae. albopictus_ mosquitoes can now be found in the southeastern part of the United States, Mexico, Central and South America, the Caribbean, the Middle East, Japan, and southern Europe (Spain, Italy, Bosnia-Herzegovina, Croatia, France, Greece, the Netherlands, Serbia and Montenegro, Slovenia, Switzerland, and Albania) . This mosquito has also been intercepted in Australia’s seaports and is now established in northern Queensland .\n\n【3】_Ae. aegypti_ mosquitoes are indigenous to Africa and disseminated around the tropical and subtropical regions. The southeastern United States, the Middle East, Southeast Asia, Pacific and Indian islands, and northern Australia are also infested by this mosquito. In continental Europe, it has been documented in southern regions but today seems to no longer to be present there .\n\n【4】Climate change, increasing globalization, and ease of travel could favor the continuing spread of mosquitoes to nonindigenous habitats, expanding the number of regions in the world where local transmission of vector-borne disease could occur. In these countries where competent vectors are present, patients coming from disease-endemic areas at an early stage of infection may import the virus and be responsible for locally acquired mosquito-transmitted cases of chikungunya. The risk for local transmission in these countries is not simply theoretical, as shown by the epidemic of chikungunya in the county of Emilie-Romagna, Italy, in which 205 cases were identified between July 4 and September 27, 2007 . In the United States, such secondary transmission of vector-borne disease has also been observed with malaria .\n\n【5】To determine regions of the world at risk for an epidemic of chikungunya virus, we first listed the imported chikungunya cases (i.e. cases diagnosed in nonendemic areas) reported around the world. A literature review was undertaken on Medline by Pubmed and websites provided by the World Health Organization, Eurosurveillance, European Center for Disease Prevention and Control, Health Protection Agency (United Kingdom), Institut de Veille Sanitaire (France), and the Centers for Disease Control and Prevention (United States) were searched for information on imported chikungunya cases. Data were then mapped and compared with the known and theoretical geographic distributions of _Ae. albopictus_ and _Ae. aegypti_ mosquitoes around the world  . This figure shows that imported cases were reported in many countries where mosquito vectors for chikungunya virus are well established.\n\n【6】These facts underscore the need for clinicians to consider the possibility of chikungunya disease in patients who experience acute unexplained fever with joint pain and live in regions where mosquito vectors are established. The presence of imported cases and well-established vectors also confirms the need for an active surveillance system; early detection of unexpected new diseases by physicians will enable the timely implementation of suitable control measures that can interrupt the transmission chain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "547a3972-89a6-4284-a800-3d421a9e4922", "title": "Role of Rhinovirus C in Apparently Life-Threatening Events in Infants, Spain", "text": "【0】Role of Rhinovirus C in Apparently Life-Threatening Events in Infants, Spain\nHuman rhinovirus (HRV) is 1 of the most common agents associated with upper and lower respiratory tract infections in children and infants  and is a major trigger of asthma exacerbations . Recently, molecular methods have shown substantial phenotypic variation of HRV and identified a novel HRV genogroup provisionally named HRV-C . Severe asthma exacerbations in children have been associated with this new genogroup of rhinoviruses. Genogroup C could be resistant to a new candidate group of antipicornavirus drugs, including pleconaril .\n\n【1】Apparently life-threatening events (ALTEs) in infants are associated with bronchiolitis or infections in up to 6% of patients by diagnosis after hospital admission . We assessed the relation between ALTEs and respiratory virus infection in a secondary hospital in Spain.\n\n【2】### The Study\n\n【3】Our study was part of a systematic prospective study to assess the epidemiology of respiratory virus infections in children admitted to the Severo Ochoa Hospital (Leganés, Madrid Province, Spain).We conducted a specific study to determine the incidence of respiratory virus infections in all infants admitted after ALTEs during November 2004–December 2008. An ALTE in a child <1 year of age was defined as an episode that is frightening to the observer and characterized by some combination of apnea, color change, marked change in muscle tone, choking, or gagging so the observer fears the infant has died .\n\n【4】Nasopharyngeal aspirate (NPA) specimens were acquired from each eligible patient at the time of hospital admission (on Monday–Friday). Samples were sent for virologic study to the Influenza and Respiratory Virus Laboratory (National Centre for Microbiology, Institute of Health Carlos III, Spain). Specimens were processed within 24 hours after collection.\n\n【5】Total nucleic acids were extracted from 200-µL aliquots by using a QIAamp MinElute Virus Spin Kit in a QIAcube automated extractor (QIAGEN, Valencia, CA, USA). Simple or multiplex reverse transcription–nested PCR assays (RT-PCR) previously described  were used to assess the virus diagnosis, including 16 respiratory viruses or groups of viruses. Degenerated primers for HRV and enteroviruses were designed between the 3′ end of the 5′ noncoding region (NCR) and the viral protein (VP) 4/VP2 polyprotein gene (TCIGGIARYTTCCASYACCAICC-3′ and CTGTGTTGAWACYTGAGCICCCA-3′). HRVs from positive samples were identified by sequencing and phylogenetic analysis of these sequences. Amplified products (about 500 bp, depending on HRV serotype) were purified and sequenced in both directions by using an automated ABI PRISM 377 model sequencer. Partial sequences of HRV have been submitted to GenBank (accession nos. FJ841954–FJ841957, FJ841959–FJ841961, EU697826, and EU697832). Appropriate precautions were implemented to avoid false-positive results by carryover contamination. Positive results were confirmed by testing a second aliquot of the sample stored at –70ºC.\n\n【6】Sixteen infants (8 of each sex) were enrolled in the study. All patients were <5 months of age (range 7 days–5 months, mean age 7.6 weeks, median 4 weeks). Twelve infants had rhinorrea, cough, and distress signs . A total of 11 (69%) NPA specimens were positive for at least 1 viral agent. For 9 of these patients, positive results for HRV were confirmed, and for the other 2 patients, respiratory syncytial virus was detected.\n\n【7】Phylogenetic analyses of 9 sequences obtained from patients showed distribution of HRV in 3 clusters. Three sequences were included in previously characterized clades, defined by HRV group A (HRV-A, SO4923–EU697826) and B (HRV-B, SO3970–FJ841954 and SO4998–EU697832). Sequence from patient SO4923 had a low sequence similarity with the other serotypes of HRV-A. In contrast, sequences from patients SO3970 and SO4998 were closely related to HRV-35 and HRV-79, respectively. Six sequences were included in the third group corresponding to the new HRV-C: SO5854, SO6666, SO5797, SO6819, SO5986, SO6813- FJ841955-57 and FJ841959-61)  . Different genotypes (collectively called HRV-Cs) were identified in 6 NPA specimens from children with ALTEs (67% of total HRV). Two received cardiopulmonary resuscitation at home; for these 2 patients, a respiratory syncytial virus and an HRV-C were identified. All 16 children survived.\n\n【8】### Conclusions\n\n【9】The most common discharge diagnoses reported for ALTEs are gastroesophageal reflux disease (GERD), unknown causes, seizures, and lower respiratory tract infections . Our series suggests that ALTEs of previously unknown etiology could be related to HRV infections. Rhinovirus infections are known to be a major cause of illness and hospital admission for young children, particularly infants <2 years of age . Detection of viral genomes by nested RT-PCR in NPA specimens led us to analyze the effect of HRV infections in different clinical situations. Respiratory infections associated with HRV might play a major role in young infants, probably with few clinical signs, and might contribute to apnea as a first manifestation. GERD is the most frequent hospital discharge diagnosis in published series . For our patients, GERD also was the most frequent clinical diagnosis (9 patients), but for 7 of them, a respiratory virus was identified. We cannot conclude whether GERD is a risk factor for apnea or whether signs are so nonspecific that diagnoses could be confused.\n\n【10】Alternatively, the new HRV-C group could account for as many as a quarter or even half of HRV infections . In children, it has been associated with bronchiolitis, wheezing, and asthma exacerbations severe enough to require hospitalization; the percentage of these children with hypoxia was substantial . In a case–control study, Khetsuriani et al. found HRV-C only in case-patients, supporting the pathogenic role of this genogroup. They considered that HRV-C infections could be associated with more severe clinical manifestations than infections with other HRV genogroups A and B. These data could also support the role of HRV-C in infants with ALTEs found in this work.\n\n【11】Although we had no control group for our patients, we recently published a study of a cohort of 316 newborns up to 6 months of age tested weekly for respiratory diseases (mainly upper respiratory tract infections), coincident in age and time with our patients . HRV was present in 5 (3.6%) of 72 infants tested. Two viruses were genetically identified as HRV-C, demonstrating they form distinct genetic clusters, and no genetic similarity was obtained with the ALTE–related HRV-C viruses. In addition, a second group of asymptomatic children of different ages but in coincident epidemic seasons was studied. The group of children with HRV was substantially smaller than the group of children with respiratory disease .\n\n【12】Viral infections could play a major role in ALTEs. Rhinoviruses, especially HRV-C, could cause a respiratory infection with few symptoms in young infants and could trigger ALTEs in this age group. Therefore, HRVs and posterior genotyping should be included in studies of the etiology of ALTEs to help identify the true relevance of HRV-C infection to these episodes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b5651796-9bf5-433d-8ab6-c62f1f5b7a25", "title": "Enhanced Adenovirus Vaccine Safety Surveillance in Military Setting, United States", "text": "【0】Enhanced Adenovirus Vaccine Safety Surveillance in Military Setting, United States\nAdenovirus infection results in considerable illness among congregate military populations . US Food and Drug Administration–approved use of a live, oral, bivalent adenovirus vaccine for military populations began in 2011 and was associated with substantial decreases in adenovirus infection incidence at US military basic training centers . The US Naval Academy introduced adenovirus vaccination in 2018 after a large adenovirus outbreak . An adenovirus outbreak involving ≈300 cadets occurred at the US Coast Guard Academy (CGA) in 2019 . Adenovirus vaccines were introduced for incoming first-year CGA cadets on June 28, 2022. To supplement postlicensure data on adenovirus vaccine safety , we monitored postvaccination signs and symptoms in those cadets.\n\n【1】We developed a monitoring system to account for military training features, such as restricted cellphone access and a time-sensitive, regimented curriculum. For sick call visits occurring < 10 days after vaccination, the CGA clinic used a monitoring tool  consisting of 17 postvaccination signs or symptoms obtained from clinical trial results . We measured vaccine uptake and inability to swallow pills and monitored cadets for 90 days after vaccination for US Food and Drug Administration–defined serious adverse events .\n\n【2】Cadets received an in-person briefing from CGA clinic staff on June 27, 2022. The CGA training cadre, with whom the cadets had daily contact, were briefed by clinic leadership on the paper-based reporting tool, reporting requirements, and referring ill cadets to the CGA clinic. Before vaccination, cadets were given the Centers for Disease Control and Prevention adenovirus vaccine information statement and opportunity to ask questions. Subsequently, if cadets sought care for illness at the CGA clinic, staff used the reporting tool to record whether any of the 17 signs and symptoms were present.\n\n【3】During the initial vaccination period (June 28–30, 2022), 293 (97.3%) of 301 first-year cadets received the adenovirus vaccine; 4 (1.3%) cadets were unable to swallow the vaccine. Of 4 cadets isolated for COVID-19 during the initial vaccination period, only 1 subsequently received the vaccine. Of 294 vaccinated cadets, a total of 159 (54.1%) received 1 other vaccine and 53 (18.0%) received \\> 2 additional vaccines.\n\n【4】The average age of the 294 vaccine recipients was 18.25 years; 57% were male, and 43% female. During June 30–July 8, 2022, ≈100 first-year cadets sought care at the CGA clinic for illness, and 65 (22.1%) cadets reported \\> 1 vaccine surveillance sign or symptom. Commonly reported signs and symptoms were cough (20.1%), sore throat (17.0%), headache (16.0%), fatigue (16.0%), nasal congestion (15.3%), and shortness of breath (11.6%) . Frequencies of gastrointestinal symptoms among cadets seeking care at the clinic during the 10-day period after vaccination were 2.3% for abdominal pain, 3.7% for diarrhea, 4.0% for vomiting, and 8.3% for nausea . During the 90 days after vaccination, no serious adverse events were reported, including hospitalization, Guillain-Barre syndrome, or death.\n\n【5】During the enhanced postimmunization monitoring period, a concomitant COVID-19 outbreak affected ≈20% of first-year cadets. Of the 65 students who sought care at the CGA clinic and had \\> 1 enhanced surveillance sign or symptom, 8 (12.3%) also tested positive for COVID-19.\n\n【6】The live adenovirus vaccine was well-tolerated; only 4 vaccination failures occurred, the cadets who could not swallow the medication. Of 294 vaccinated CGA cadets, <25% reported signs or symptoms during the monitoring period. Phase 1 adenovirus vaccine trial data  showed symptom occurrence reached 33% within 8 weeks postvaccination. The most common signs and symptoms reported among CGA first-year cadets corresponded with those noted in a large phase 3 adenovirus trial , including headache, sore throat, nasal congestion, and cough. In this cohort, rates for gastrointestinal symptoms (2.3%–8.3%) were lower than those reported in the phase 3 trial .\n\n【7】Shortness of breath in 11.6% of cadets did not clearly correspond to signs and symptoms identified in phase 1 or phase 3 trial safety data. Given that >25% of nonhospitalized adults with COVID-19 report shortness of breath , the concurrent COVID-19 outbreak might have contributed to reports of this specific symptom.\n\n【8】In conclusion, enhanced passive monitoring established after introducing adenovirus vaccination for incoming first-year CGA cadets did not identify any serious adverse events. Even with the receipt of multiple vaccines and an intercurrent COVID-19 outbreak, the signs and symptoms profile among cadets who had sick calls during the 10-day postvaccination period appears consistent with profiles reported in previous adenovirus vaccine trials. A positive COVID-19 test was observed for 12.3% of cadets who completed the surveillance questionnaire during their sick call, which might explain the 11.6% of cadets who reported shortness of breath. Our favorable real-world findings support continuing adenovirus vaccination of incoming CGA cadet classes and wider use of the vaccine in congregate military settings.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c026d2c5-abb3-43e2-87da-6d8314ba5e47", "title": "Preliminary Characterization and Natural History of Hantaviruses in Rodents in Northern Greece", "text": "【0】Preliminary Characterization and Natural History of Hantaviruses in Rodents in Northern Greece\n**To the Editor:** Hantaviruses (family Bunyaviridae), cause hemorrhagic fever with renal syndrome (HFRS) in Europe and Asia and hantavirus pulmonary syndrome in the Americas. In Greece, hantaviruses are endemic, and small outbreaks or sporadic cases of HFRS have been observed; Dobrava virus is the predominant hantavirus and causes severe HFRS , while Puumala virus is probably the causative agent of the milder forms of this syndrome . Hantaan virus, associated with _Apodemus agrarius_ , and Seoul virus, associated with _Rattus norvegicus_ , have never been detected in Greece. Dobrava virus is hosted by _A. flavicollis_ in the Balkans  or by _A. agrarius_ in northern Europe  .\n\n【1】We surveyed two sites where Dobrava virus-caused HFRS was reported  . The first was Nevrokopi, a small town in the Rhodope Mountains, 12 km from the Greek-Bulgarian border (41° 20.184' N, 23° 49.479' E, elevation approximately 650 m). The second site included Pramanta, a small village in the Pindos Mountains, approximately 67 km southeast of Ioannina (39° 31.722' N, 21° 5.872' E, elevation 790 m), and Matsuki, a small village near Pramanta (39° 33.909' N, 21° 9.713' E, elevation 1080 m). Animals were trapped and sampled outdoors according to established safety guidelines  . Blood, lung, kidney, and spleen samples were kept in liquid nitrogen until transferred to -70°C freezers for storage. At Nevrokopi, 57 small mammals were captured during 887 trap nights for an overall trap success rate of 6.4%. At Pramanta and Matsuki, 13 small mammals were captured during 400 trap nights (3.3% trap success). The total of 70 captured mammals comprised seven species of rodents and one insectivore. _A. flavicollis_ was the most commonly captured species (87% of captures).\n\n【2】Whole-blood specimens were tested for hantavirus immunoglobulin G by indirect immunofluorescence assay and enzyme-linked immunosorbent assay, using Hantaan 76-118 as antigen. Total RNA was extracted from homogenized tissues, and nested reverse transcriptase-polymerase chain reaction (PCR) was performed with two sets of nested primers  : one set designed to detect the partial G1 coding region of hantaviruses associated with rodents of the subfamily Murinae (Hantaan, Dobrava, and Seoul viruses), and another to detect the N coding region of hantaviruses associated with rodents of the subfamily Arvicolinae (Prospect Hill virus). Eight _A. flavicollis_ , all from Nevrokopi, were positive for hantavirus infection by serology or molecular methods, for a 13% overall prevalence in _A. flavicollis_ . Some rodents positive by serology were negative by PCR and vice versa. One of three _R. rattus_ captured at Pramanta was positive for hantavirus infection by indirect immunofluorescence assay.\n\n【3】PCR yielded products from tissues from seven _A. flavicollis_ . A 270-bp segment of the G1 gene was sequenced and analyzed phylogenetically. A mean sequence similarity of 99.1% (range 98.5%-100%) was observed among the seven rodents. These sequences differed by 9.5% from Dobrava virus sequences of HFRS cases from northwestern Greece and by 8.5% from Dobrava virus sequences of HFRS cases from Dobrava-Slovenia. Similarly, sequences from Nevrokopi human samples were closer to Dobrava virus from Slovenia than to such virus from northwestern Greece. The nucleotide difference was 21% when the rodent sequences were compared to an Estonian sequence from _A. agrarius_ . The deduced amino acid sequences of all seven Dobrava virus G1 fragments were identical. This analysis showed that the evolutionary relationship among Dobrava virus subtypes was closely correlated with that of the rodent reservoir and suggests that this virus is stably maintained in the rodent population.\n\n【4】All seropositive _A. flavicollis_ were sexually mature adults; six (75%) of eight were male, compared to 26 (49%) of 53 seronegative animals (Χ 2  \\=0.97, p=0.32). Six (75%) of 8 had scars, compared to 14 (26%) of 53 seronegative animals (Χ 2  \\=5.4, p=0.02). Finally, 14 (44%) of 32 male _A. flavicollis_ had scars, compared to 5 (17%) of 29 females (Χ 2  \\=3.8, p=0.05). Scarring has been significantly associated with Seoul virus antibody in wild rats and has been suggested as the primary mechanism by which hantaviruses are amplified epizootically  . The higher prevalence of scars among male _A. flavicollis_ and especially among males with evidence of hantavirus infection supports the hypothesis that hantaviruses are transmitted when aggressive male animals fight. Although this pattern has been observed for several host species of New World hantaviruses, this is the first known demonstration for Dobrava virus and _A. flavicollis_ . Of the two female rodents with evidence of hantavirus infection, one had scars, and one did not. The latter was positive by PCR on lung tissue but did not have detectable antibody in blood, which perhaps indicates very recent infection.\n\n【5】The seropositive _R. rattus_ from Pramanta is the first evidence of hantavirus infection in _Rattus_ within Greece. No _Rattus_ captured during previous expeditions had hantavirus antibody  . The low antibody titer (1:32) and failure to amplify viral RNA by PCR from this animal could indicate infection with a heterologous hantavirus with low cross-reactivity. Perhaps more likely, the antibody detected in this 39-g juvenile rat may represent waning maternal antibody. Transfer of protective maternal antibody to _R. norvegicus_ pups by Seoul virus-infected dams has been demonstrated  .\n\n【6】Our data implicate _A. flavicollis_ as the reservoir of Dobrava virus in northern Greece and demonstrate the common occurrence of that species in both sylvatic and peridomestic habitats. These preliminary results underscore the need for continued, more intensive reservoir studies in Greece.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3f928d4b-68fb-40da-9c45-2b3184180549", "title": "Revisiting the Island of Doctor Moreau", "text": "【0】Revisiting the Island of Doctor Moreau\n**Kate Gibb , _The Island of Doctor Moreau_ , 2006.** Silkscreen by hand on paper, 19.7 in × 27.6 in/50 cm × 70 cm. Digital image used with permission of the artist. London, England.\n\n【1】This month’s cover art, created by contemporary English printmaker and illustrator Kate Gibb―known for her colorful, detailed screen-printed artwork developed for musicians and fashion designers―was first featured on another cover, the 2005 reissue of a classic work of early horror and science-fiction, _The Island of Doctor Moreau_ by English writer H.G. Wells. Originally published in 1896, Wells’s novel chronicles the story of the shipwrecked Englishman Edward Prendick. Stranded on a remote, uncharted island, Prendick is nursed back to health after his ordeal at sea only to discover he has washed up on an isle of horrors. Doctor Moreau, a mad scientist, has fled to the island from England after his experiments in vivisection were exposed. Here the doctor works without restraints, accountability, or moral guidance, performing gruesome, cruel experiments designed to transform animals into humans.\n\n【2】In _The Island of Doctor Moreau,_ Wells intertwines several societal themes from the latter part of the 19th century, including the growing antivivisection movement, implications of Charles Darwin’s research on evolution, the nature of humankind as explored in Robert Louis Stevenson’s 1886 novel _The Strange Case of Dr. Jekyll and Mr. Hyde_ , and the notion of humans creating chimeras, an idea articulated earlier in Mary Shelley’s 1818 novel _Frankenstein_ . Moreau’s horrific surgical procedures triggered ethical questions and concerns about cruelty, morals, and human–animal relationships that still resonate with contemporary readers. Literature professor Roger Luckhurst notes, “In the 21st century, with genetic splicing making animal–human hybrids an actual possibility, Wells’s queasy exploration of the limits of the human in this provocative satire keeps the book incredibly relevant today.” Moreau’s efforts to control the behaviors of his menagerie of engineered creations through “The Law,” a list of prohibitions recited to the island’s “Beast Folk” to keep them from reverting to their animalistic selves, are also troubling.\n\n【3】Those who have read the novel will recall that after Moreau and his assistant Montgomery are killed, as are a number of the altered creatures, Prendick eventually escapes from the island and returns to London. He discovers, however, that he can no longer live among humans, fearing they, too, will revert to animalistic beings, and so he seeks solitude in the countryside, devoting himself to scientific studies. Gibb’s collage, which draws from the interconnected themes in the novel, features silhouettes of simians, the maw of a felid, and a lacey cross-section of a brain superimposed over a soothing blue-gray background, plus a pair of flies disturbingly close to a human eye peering at the viewer.\n\n【4】As readers of this journal know, the xenotransplantation and surgeries, close interactions of humans and animals, and coerced association of myriad animal species on Moreau’s island could potentially have led to the spread of zoonotic infections, perhaps even the emergence of novel pathogens. But no person or animal is ever infected with a zoonotic pathogen in the novel, unlike in Wells’s next novel, _The War of the Worlds_ , when invading Martians succumb to earthly pathogens to which they had no immunity, “slain, after all man’s devices had failed.”\n\n【5】Zoonotic diseases, an unavoidable consequence of human–animal interactions, are caused by microorganisms such as viruses, bacteria, parasites, and fungi. CDC’s One Health website notes that more than 6 out of every 10 known infectious diseases in humans can be spread from animals. Perhaps in a contemporary reimagining of _The Island of Doctor Moreau_ , existing or emerging zoonotic infections would factor into the narrative. In such a reworking, the isolated island setting and small number of people and animals might mitigate widespread transmission of such infections, in contrast with factors driving the emergence of new infectious diseases. Human encroachment into remote regions, factory farming, animal markets and trade, climate change, disruptions of natural areas and their ecosystems, and global migration all play a role.\n\n【6】Understanding those interconnections between humans, animals, plants, and their communal environment, a concept now known as One Health, is an important public health priority. In a 2018 article published in the _Annual Review of Animal Biosciences_ , researchers Bird and Mazet warn, “We must be prepared to recognize the signs, identify the threat, and rapidly work together to reduce the spread of infections and health consequences before they harm the health of animals and people throughout the world.” Gibb’s silkscreened image seems applicable both to the events in Wells's 125-year-old novel and to those of the present day.\n\n【7】##### About the Artist\n\n【8】Kate Gibb created 17 covers for the 2006 reissues of H.G. Wells’s works and explained, “There were strong themes and descriptive elements within the text which quickly inspired me to find and collage images quite spontaneously. I remember really enjoying the process and being pleasantly surprised by its outcome!” She has worked with fashion designers, publishers, and musicians, and her work has been featured in a number of contemporary publications. In addition to producing her print-based artworks and commercial illustrations, Gibb is an educator, most recently at England’s University of Brighton. After studying textiles at Middlesex University, London, she shifted her focus to silkscreen printing and describes herself as “a silkscreen obsessive.”", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b284f09a-2a37-4c15-a99c-0b0344cfb389", "title": "MERS Health Advisory", "text": "【0】### MERS Health Advisory\n\n【1】**PDF version: Health Advisory: MERS**\n\n【2】### Infographic Description\n\n【3】#### Health Advisory: MERS\n\n【4】**Middle East Respiratory Syndrome**\n\n【5】Were you in the Middle East recently?\n\n【6】*   Watch for fever with cough or difficulty breathing.\n*   If you get sick within 14 days of leaving, call a doctor.\n*   Tell the doctor you traveled.\n\n【7】### Arabic Version\n\n【8】**PDF version: Health Advisory: MERS (Arabic)**", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ad2d3b92-198d-45e0-bcbd-33dfdcad2ce6", "title": "VAR2CSA Serology to Detect Plasmodium falciparum Transmission Patterns in Pregnancy", "text": "【0】VAR2CSA Serology to Detect Plasmodium falciparum Transmission Patterns in Pregnancy\nAgile malaria surveillance and response systems that can be sustained over time are needed for the optimal design of control programs . Rates of _Plasmodium falciparum_ infection among pregnant women are sensitive to changes in transmission  and correlate well with infection in infants  and children . Thus, passive detection of malaria cases at maternal health care services constitutes a promising approach to providing contemporary data on the levels, and changes in levels, of malaria burden in the population for successful malaria control and elimination .\n\n【1】After exposure to _P. falciparum_ parasites that sequester in the placenta , antibodies against VAR2CSA, a multidomain variant antigen of the _P. falciparum_ erythrocyte membrane protein 1 family, develop in pregnant women . VAR2CSA is expressed on the surface of infected erythrocytes and mediates placental sequestration of parasites through binding to chondroitin sulfate A . Levels of antibodies against VAR2CSA are affected by variables that influence the risk for _P. falciparum_ exposure  and mirror malaria trends during pregnancy . Moreover, levels of VAR2CSA IgG at delivery correlate with the risk for malaria in the offspring , suggesting the value of these antibodies for pinpointing areas of high malaria transmission . Because VAR2CSA antibodies persist after the infection is cleared , they can provide a sensitive adjunct for _P. falciparum_ monitoring, especially in areas of low malaria endemicity, where the chances of detecting antibodies are higher than those of detecting the parasite .\n\n【2】The utility of serosurveillance depends mainly on specific properties of the antigen, including immunogenicity, polymorphism, cross-reactivity, and longevity of the antibodies. Because different VAR2CSA domains elicit IgG responses with varying magnitudes and dynamics , we hypothesized that short-lived antibodies against immunogenic nonpolymorphic VAR2CSA epitopes would enable a fine-scale estimation of recent _P. falciparum_ transmission during pregnancy . We examined plasma from pregnant women living in areas in which _P. falciparum_ transmission varied from high to low and absent (Benin, Gabon, Mozambique, Kenya, Tanzania, and Spain) against a quantitative suspension array containing VAR2CSA and general parasite antigens. We first selected IgG responses that were rapidly acquired after _P. falciparum_ infection, did persist in circulation, and were sensitive to the level of parasite exposure in pregnant women from Mozambique and Spain. We then used the serologic assay to quantify the relationship of VAR2CSA antibody responses with _P. falciparum_ infection as well as with temporal, spatial, and intervention-driven changes in malaria burden among pregnant women.\n\n【3】### Methods\n\n【4】##### Study Sites, Population, and Procedures\n\n【5】We included in our study pregnant women who participated in 3 clinical trials of intermittent preventive treatment during pregnancy (IPTp) during 2003–2005 in Mozambique (NCT00209781)  and during 2010–2012 in Mozambique, Benin, Gabon, Kenya, and Tanzania (NCT00811421) . Participants were recruited at their first antenatal visit, and all received a long-lasting insecticide-treated bed net. During 2003–2005, all received 2 doses of sulfadoxine/pyrimethamine ; during 2010–2012, they received 2 doses of mefloquine or sulfadoxine/pyrimethamine if they were not HIV infected  and 3 doses of mefloquine or placebo plus daily cotrimoxazol prophylaxis if they were HIV infected . At delivery, tissue samples from the maternal side of the placenta, as well as 50 μL peripheral and placental dried blood spots (DBS), were collected. Peripheral and placental blood from pregnant women in Mozambique and Benin were also collected into EDTA Vacutainer tubes  and centrifuged; plasma was stored at −20°C. From a subset of pregnant women in Mozambique who delivered during 2011–2012, peripheral blood samples were also collected at the first antenatal visit and before administration of the second IPTp dose. We geocoded the households of women in Mozambique by using a global information system. Clinical malaria episodes were treated according to national guidelines at the time of the study . DBS and plasma samples were also collected from 49 pregnant women never exposed to _P. falciparum_ who delivered in 2010 at the Hospital Clinic of Barcelona (Barcelona, Spain).\n\n【6】The study was approved by the ethics committees from the Hospital Clínic of Barcelona, the Comité Consultatif de Déontologie et d’Éthique from the Institut de Recherche pour le Développement (Marseille, France), the Centers for Disease Control and Prevention (Atlanta, GA, USA), and national ethics review committees from each malaria-endemic country participating in the study. Written informed consent, which included permission to test for immune markers by using stored biological samples, was obtained from all participants.\n\n【7】##### Laboratory Determinations\n\n【8】At recruitment, we assessed HIV serostatus by using rapid diagnostic tests according to national guidelines and hemoglobin level at delivery by using the following mobile devices on capillary blood samples: HemoCue , Hemocontrol , and KX analyzer . Thick and thin blood films and placental biopsy samples were checked for _Plasmodium_ spp. according to standard, quality-controlled procedures . We tested blood on filter paper for the presence of _P. falciparum_ in duplicate by means of a real-time quantitative PCR (qPCR) targeting 18S ribosomal DNA .\n\n【9】##### Antibody Measurements\n\n【10】We measured IgG in plasma (Benin and Mozambique) or on DBS (Gabon, Kenya, and Tanzania) in appropriate conditions for plasma elution  by using xMAP technology and the Luminex 100/200 System  for 37% of pregnant women participating in the clinical trials with samples available. We constructed 2 multiplex suspension array panels  , 1 including _P. falciparum_ recombinant proteins (VAR2CSA Duffy binding-like recombinant domains DBL3X, DBL5Ɛ; and DBL6Ɛ, apical membrane antigen 1 \\[AMA1\\]; and 19-kDa fragment of the merozoite surface protein-1 \\[MSP1 19  \\], from 3D7 strain) and 1 consisting of synthetic peptides (25 VAR2CSA peptides covering conserved and semiconserved regions of VAR2CSA and a circumsporozoite peptide \\[pCSP\\]) . To assess unspecific IgG recognition, we used bovine serum albumin in both arrays . Procedures for reconstitution of DBS and quality control, bead-based immunoassay, data normalization, and definition of seropositivity cutoffs are described in Appendix 1 .\n\n【11】##### _var2csa_ Sequencing and 3D Protein Modeling\n\n【12】We used DNA extracted from 50 DBS that were _P. falciparum_ positive by qPCR for Sanger sequencing of _var2csa_ PCR amplification products covering peptides of interest . Sequence variability with respect to the peptide included in the array was assessed after amino acid alignment, and a 3D model of the DBL1X-ID1 region was developed by using Chimera version 1.5.3 ; Appendix 1 ).\n\n【13】##### Definitions and Statistical Analyses\n\n【14】We included in the analysis pregnant women for whom all information was available for IPTp, date of delivery, HIV status, age, parity, and antibody responses. We classified women as primigravid (first pregnancy) or multigravid ( \\> 1 previous pregnancy) and categorized age as <20, 20–24, or \\> 25 years . Anemia was defined as hemoglobin level at delivery <11 mg/L. We compared proportions by using the Fisher exact test. We used univariate regression models to evaluate the association of log-transformed IgG levels (linear) and seropositivity (logistic) with study periods (2004–2005 and 2010–2012) and country, _P. falciparum_ infection, parity, anemia, and IPTp intervention, taking into account potential confounding variables (HIV and age) in multivariate models. We assessed the modification of the associations by HIV infection or parity by including interaction terms into the regression models. To control the false discovery rate in the selection of antigens, we computed adjusted p values (q-values) by using the Simes procedure . We used multilevel mixed-effect linear regression analysis to estimate half-life and time to double (T 2×  ) IgG levels in the longitudinal cohort of pregnant women from Mozambique . We identified spatial clusters of _P. falciparum_ infection and seropositivity as well as the most likely hotspots by using the Ward hierarchical cluster analysis and Kulldorff spatial scan method . We performed statistical analyses by using Stata/SE software version 12.0 , R statistics software version 3.2.1 , and Graphpad Prism version 6 .\n\n【15】### Results\n\n【16】##### Study Participants and _P. falciparum_ Prevalence\n\n【17】Study participants consisted of 2,354 pregnant women  recruited during 2004–2005 (n = 146) and 2010–2012 (n = 2,208) in the context of IPTp clinical trials . Among them, 993 were from Mozambique, 854 from Benin, 131 from Gabon, 296 from Kenya, 31 from Tanzania, and 49 from Spain. The baseline characteristics of the women selected for this trial were similar to those of the 6,216 women participating in the randomized clinical trials .\n\n【18】The study areas represented 5 sites in sub-Saharan Africa with different intensities of malaria transmission. Prevalence of _P. falciparum_ infection detected by qPCR at delivery, in either peripheral or placental blood (averaged for 2010–2012), among HIV-uninfected women was 46% (332/725) in Benin, 10% (9/89) in Gabon, and 6% (28/452) in Mozambique and among HIV-infected women was 8% (22/273) in Kenya and 4% (13/327) in Mozambique . The prevalence of _P. falciparum_ infection among pregnant women in Mozambique decreased from 25% (37/146) in 2004–2005 to 2% (3/176) in 2010 and increased to 6% (4/72) in 2012. A subset of 239 pregnant women from Mozambique recruited during 2011–2012 was followed during pregnancy; prevalence of _P. falciparum_ infection detected by qPCR was 16% (38/239) at first antenatal visit (mean gestational age ± SD, 20.7 ± 5.45 weeks), 3% (8/239) at the second IPTp administration (25.9 ± 4.98 weeks), and 5% (13/239) at delivery (38.4 ± 2.26 weeks). _P. falciparum_ infection was detected at unscheduled visits for 2% (5/239) of the women. Overall, _P. falciparum_ infection was detected at any of these time points for 21% (49/239) of the women.\n\n【19】##### _P. falciparum_ –Specific Antibody Profiles and Parasite Exposure during Pregnancy\n\n【20】Mean antiparasite IgG levels in pregnant women from Mozambique delivering from 2010 through 2012 were above levels against bovine serum albumin plus 3 SD and higher than IgG levels in pregnant women from Spain except for DBL6Ɛ and 3 of 25 VAR2CSA peptides . Five VAR2CSA peptides, DBL6Ɛ, and pCSP were recognized by IgG from \\> 5% of the pregnant women from Spain who had never been exposed to _P. falciparum_ , suggesting unspecific recognition; thus, these peptides were excluded from subsequent analysis. To further narrow down the VAR2CSA peptide candidates, we compared IgG levels in pregnant women from Mozambique delivering in 2004–2005 and 2010–2012, a period when _P. falciparum_ prevalence assessed by qPCR at delivery in peripheral or placental blood dropped from 25% to 5%  . This decline in infection rates was mirrored by drops of IgG levels against 10 of the 18 previously selected VAR2CSA peptides (p1, p5, p8, p10, p12, p20, p27, p36, p38, p39) .\n\n【21】##### Acquisition and Decay of IgG Responses against VAR2CSA\n\n【22】We assessed the dynamics of IgG responses in a longitudinal cohort of 239 pregnant women from Mozambique . At delivery, compared with uninfected women, the 49 (21%) women infected with _P. falciparum_ during pregnancy had higher IgG levels against the 10 down-selected peptides . At delivery, seroprevalence rates for p1 (23%), p5 (26%), p8 (26%), and p39 (31%) antibodies were above the cumulative prevalence of _P. falciparum_ infection during pregnancy . No difference in IgG levels was observed between primigravid and multigravid women . T 2×  after _P. falciparum_ infection ranged from 0.45 years (95% CI 0.31–0.80 years) for p5 to 1.07 years (95% CI 0.60–5.23 years) for p27 . IgG half-life among seropositive women at recruitment without evidence of _P. falciparum_ infection during follow-up ranged from 0.55 (95% CI 0.38–1.02) years for p8 to 3.66 (95% CI 0.98–∞) years for p1 . Among recombinant antigens, IgG DBL5Ɛ showed the lowest T 2×  (0.31 \\[95% CI 0.21–0.61\\] years) and half-life (0.66 \\[95% CI 0.42–1.65\\] years), whereas AMA1 IgG showed the highest T 2×  (1.76 \\[95% CI 0.76–∞\\] years) and half-life (4.18 \\[95% CI 1.86–∞\\] years).\n\n【23】Among the down-selected VAR2CSA peptides (p1, p5, p8, and p39), IgG against p5 (51 amino acids) and p8 (48 amino acids) showed the lowest half-lives (0.55 \\[95% CI 0.38–1.02\\] years for p8; 1.33 \\[95% CI 0.65–∞\\] years for p5) and the largest increase in women exposed to _P. falciparum_ during pregnancy compared with uninfected women (adjusted ratio \\[AR\\] p5  2.15 \\[95% CI 1.39–3.31\\] and AR p8  2.17 \\[95% CI 1.46–3.23\\], panel B; Appendix 1 Figure 5; Appendix 2 Table 5). IgG levels and seroprevalence rates at delivery for p5 and p8 were higher among pregnant women with active or past malaria infection than among women with no parasite or pigment in the placenta, as assessed by histologic examination . 3D modeling mapped both sequences on the exposed surface of DBL1X-ID1 region of VAR2CSA . Amino acid variability obtained from 50 _P. falciparum_ isolates collected at study sites was 5% ± 2 SD for p5 sequences and 16% ± 5 SD for p8 sequences, compared with the consensus peptide sequence included in the array .\n\n【24】##### Performance of Selected VAR2CSA Peptides for Assessing Spatial and Temporal Differences in _P. falciparum_ Exposure\n\n【25】In pregnant women from Mozambique at delivery, p5 and p8 seroprevalence rates, as well as the composite of both (p5+8), decreased from 2004–2005 to 2010 (adjusted odds ratio \\[AOR\\] p5+8  0.27 \\[95% CI 0.11–0.68\\]), followed by an increase from 2010 to 2012 (AOR p5+8  2.49 \\[95% CI 1.34–4.61\\], panel A; Appendix 2 Table 8). This decrease and subsequent increase mirrored _P. falciparum_ prevalence by qPCR. HIV infection and parity did not modify the associations observed (p value for interaction >0.05 for all cases). Similar to _P. falciparum_ prevalence determined by qPCR, seroprevalence rates were the highest in HIV-uninfected women from Benin, followed by those from Gabon (AOR p5+8  0.31 \\[95% CI 0.21–0.47\\]) and Mozambique (AOR p5+8  0.21 \\[95% CI 0.16–0.28\\], panel B; Appendix 2 Table 9). At delivery, pregnant women living in an area from Tanzania where no _P. falciparum_ infection was detected by qPCR were seronegative for p5, p8, and p5+8 antibodies; 42% were seropositive against AMA1 and 48% were seropositive against MSP1 19  antibodies . Among HIV-infected women, seroprevalence rates for p8 and p5+8 were lower in Mozambique than in Kenya (AOR p5+8  0.58 \\[95% CI 0.38–0.88\\], panel C; Appendix 2 Table 9). p5 and p5+8 seroprevalence rates were higher among anemic than among nonanemic women (AOR p5+8  1.26 \\[95% CI 1.03–1.55\\], panel D; Appendix 2 Table 10). Seroprevalence rates were lower among HIV-uninfected women who received IPTp with mefloquine than among those who received sulfadoxine/pyrimethamine (AOR p5+8  0.74 \\[95% CI 0.59–0.94\\], panel E; Appendix 2 Table 11). Seroprevalence rates among HIV-infected women were lower among those who received mefloquine than among those who received placebo, although differences were not significant (AOR p5+8  0.76 \\[95% CI 0.50–1.15\\], panel F; Appendix 2 Table 11).\n\n【26】##### Geographic Patterns of _P. falciparum_ Transmission through VAR2CSA Serologic Testing\n\n【27】Spatial geocoordinates were available for 698 pregnant women from Mozambique residing in Manhiça District (southern Mozambique). Geographic areas experiencing significantly higher seroprevalence rates than would be expected by chance were observed for p5 (radius 2.82 km; p = 0.024) and p5+8 (radius 1.06 km; p = 0.049) but not for MSP1 19  and AMA1 . The distribution of HIV infection, parity, age, and IPTp was similar among women inside and outside the serologic hotspot (p>0.05).\n\n【28】### Discussion\n\n【29】Routine _P. falciparum_ testing of easily accessible pregnant women at maternal healthcare services has the potential to offer a rapid, consistent, and cost-effective method for evaluating the malaria burden in different communities and tracking progress of interventions. IgGs against 2 VAR2CSA peptides, selected according to their ability to maximize the information about recent _P. falciparum_ exposure during pregnancy, reflected differences in malaria burden over time and space in multiple settings in Africa and changes in parasite rates associated with the use of different preventive regimens. Overall, our results indicate that in areas with well-attended maternal healthcare services, this pregnancy-specific serologic test may serve as a useful sentinel surveillance tool for flagging changes in malaria burden and progress in the path toward elimination.\n\n【30】p5 (51 amino acids) is localized in the DBL1X domain and p8 (48 amino acids) in the ID1 region of VAR2CSA. Limited diversity (5%) of p5 sequence was observed in _P. falciparum_ isolates from a variety of regions of Africa, in accordance with estimates from previous studies for the DBL1X domain . p8 corresponds to a more diverse (16%) variant of the ID1 region in VAR2CSA . Both peptides are exposed on the DBL1X-ID1 N terminal region of VAR2CSA  and recognized by IgG from malaria-exposed pregnant women at levels higher than those of pregnant women from Spain and men from Mozambique . IgG responses against both VAR2CSA peptides increased with _P. falciparum_ infection during pregnancy. Moreover, higher risk for anemia among p5 and p5+8 seroresponders support these antibodies as markers of recent infection, which adversely affects the women’s health . In contrast to the slow decay of IgG responses against AMA1, the half-life of IgG against p5 and p8 was <2 years, the average time reported in Mozambique for a second pregnancy to occur . The short half-life of p5 and p8 IgG, together with the similar IgG levels in multigravid and primigravid women, suggests that antibodies acquired during one pregnancy are not maintained over multiple pregnancies; thus, antibodies can be used as a reliable indicator of recent exposure for pregnant women, regardless of parity.\n\n【31】Seroprevalence rates for p5, p8, and the composite of both peptides (p5+8) mirrored trends in _P. falciparum_ prevalence among pregnant women from Mozambique delivering during 2004–2012 , a temporal pattern that was also observed for PfPR 2–10  . Trends were similar among HIV-uninfected and infected women, suggesting that impairment of _P. falciparum_ –specific antibody responses driven by viral infection  may not affect short-lived IgG responses against p5 and p8. Seroprevalence also reflected the burden of malaria among pregnant women residing in a variety of settings in Africa, as well as reductions in infection rates resulting from the use of mefloquine as IPTp among HIV-uninfected women . Similar trends, although not statistically significant, were observed among HIV-infected women receiving cotrimoxazol prophylaxis alone or in combination with mefloquine , possibly because of the longer duration of protection provided by 3 IPTp doses in HIV-infected women compared with the 2 doses in HIV-uninfected women. We also found that pregnant women living in an area from Tanzania where no _P. falciparum_ infection was detected by qPCR as well as pregnant women from Spain never exposed to malaria were seronegative against p5 and p8, suggesting that pregnancy-specific serology might be used to confirm the eventual interruption of transmission.\n\n【32】Geographic distribution of pregnant women from Mozambique who were seropositive against p5 and p5+8 revealed a serologic hotspot in an area close to the river and sugar cane plantations, where the density of anopheline mosquitoes can be expected to be higher. In contrast, antibodies against MSP1 19  and AMA1 were not able to identify these malaria transmission patterns because of saturation of antibody responses after lifelong exposure to _P. falciparum_ . These results support the value of using VAR2CSA serologic testing to amplify signals of recent exposure and suggest its potential to trigger targeted interventions to persons living in close proximity to passively detected seropositive pregnant women.\n\n【33】Our study has several limitations. First, the peptide array we used may have missed some conformational nonlinear epitopes. Second, different transmission dynamics and host genetic factors may affect the acquisition and decay of antibodies . Third, steeper decay of antibodies may be observed out of pregnancy when infecting parasites express non-VAR2CSA variants. Fourth, the reduction of data from median fluorescence intensity to seroprevalence to simplify the serologic information of the assay may reduce the depth of serologic information. Developing alternative mathematical models that use antibody levels  may increase the sensitivity to detect temporal and spatial changes in malaria transmission. Fifth, small numbers of pregnant women from malaria-free areas in Tanzania and Spain limit the generalizability of our data to support pregnancy-specific serologic testing as a tool to confirm interruption of transmission. Last, antibody assessments in this study were conducted mainly at delivery; further studies should assess the performance of this testing at antenatal visits or soon after delivery (i.e. during infant immunization). Future research is needed to describe the relationship between pregnancy-specific serologic testing and malaria transmission in the general population and its value for confirming interruption of malaria transmission and providing early signals of _P. falciparum_ resurgence after local elimination.\n\n【34】In summary, this study shows that IgG against 2 VAR2CSA peptides from the DBL1X-ID1 domain reveal temporal and spatial differences in malaria burden among pregnant women and reductions in exposure associated with the use of preventive measures during pregnancy. These antibodies enable the identification of local clusters of transmission that are missed by detection of _P. falciparum_ infections. Our results suggest that inferring recent exposure through VAR2CSA serologic testing would amplify signals of ongoing malaria transmission and increase the power to detect changes, either natural or driven by deliberate efforts, as well as malaria hotspots, among pregnant women . Moreover, peptides such as p1 targeted by long-lasting IgG responses may be useful for capturing past changes in transmission by sampling women of child-bearing age and relating seroprevalence with the number and timing of previous pregnancies. Operationally suitable serologic tests  capable of detecting antibodies against VAR2CSA synthetic peptides may be used in programmatic environments to stratify areas based on malaria burden, measure the effects of interventions, and document year-to-year changes in transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "af218130-d72a-4472-b956-391ead006e36", "title": "Non-Leishmania Parasite in Fatal Visceral Leishmaniasis–like Disease, Brazil", "text": "【0】Non-Leishmania Parasite in Fatal Visceral Leishmaniasis–like Disease, Brazil\n**To the Editor:** We read with interest the recent article describing involvement of _Crithidia_ \\-related parasites in visceral leishmaniasis (VL) in Brazil . In 2010, we published a similar study about the identification of _Leptomonas_ sp. another monoxenous trypanosomatid, among clinical isolates from VL patients in India: of 120 cultured isolates, 111 were typed as _L. donovani_ and 9 as _Leptomonas_ sp. As in the Brazil study, we infected BALB/c mice with 1 _Leptomonas_ isolate; at 45 days postinfection, we found _Leptomonas_ and _Leishmania_ DNA by PCR in the animals’ spleens. Assuming that sterility was preserved during the experiment, we interpreted that original infection in patients resulted from a mixture of the 2 species, _Leptomonas_ overgrew _L. donovani_ in culture because of substantial growth advantage of the former, and a few remaining _L. donovani_ cells in the culture spread in the animals after inoculation because of their higher in vivo fitness.\n\n【1】We suspect a similar phenomenon could have occurred in Brazil, and additional analyses are required to support the authors’ conclusions. Given that mice were inoculated with clinical isolates, postanimal typing should have been done. Furthermore, linking genotyping information of cultivated (cloned) strains with a patient phenotype is risky because of the selection biases of in vitro isolation and maintenance. Using a recently developed method for direct sequencing of _L. donovani_ complex parasites in host tissues, we demonstrated that genotypes of parasites in bone marrow samples differed from derived and cultivated isolates. This result most likely was due to polyclonal _L. donovani_ infections and differences in fitness of different genotypes in vitro and in vivo. On the basis of this evidence, we recommend direct parasite sequencing in clinical samples in future work. If impossible, results based on cultured isolates should be interpreted with caution. We recommend a follow-up study to verify the possibility of _Crithidia_ / _Leishmania_ co-infection and the capacity of _Crithidia_ to cause leishmaniasis-like disease as a single infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c81054a8-50fd-4384-94d0-74f87355890f", "title": "Lyssavirus Surveillance in Bats, Bangladesh", "text": "【0】Lyssavirus Surveillance in Bats, Bangladesh\nBats are known reservoirs of viruses in the _Lyssavirus_ genus, family _Rhabdoviridae_ . In the Americas, bats maintain the circulation of different lineages of rabies virus (RABV, genotype 1) . Lagos bat virus (genotype 2) and Duvenhage virus (DUVV, genotype 4) were isolated from bats in Africa . European bat lyssaviruses, types 1 and 2 (EBLV-1 and EBLV-2, genotypes 5 and 6, respectively) circulate in European bat populations . West Caucasian bat virus, a new putative lyssavirus genotype, was recently isolated from a bat in southern Europe . Australian bat lyssavirus (ABLV, genotype 7) was isolated from different genera of Australian bats .\n\n【1】Data on rabies in Asian bats are limited because of a lack of a suitable surveillance system. Only a few investigators reported presumable RABV isolates of bat origin in India and Thailand, but these were not corroborated, nor were reports about bat rabies in Siberia and Uzbekistan . No confirmed genotype 1 lyssavirus isolates are available from bats outside the Americas, to date. Recently, 3 lyssaviruses (Aravan, Khujand , and Irkut  viruses) were isolated from bats in different locations of Asia. These representatives were suggested as 3 new putative genotypes of the _Lyssavirus_ genus, according to their genetic properties. Moreover, antibodies to lyssaviruses have been demonstrated in serum specimens of bats from the Philippines, Cambodia, and Thailand . In this article, we extend information on the geographic distribution of rabies among Asian bats and describe a limited survey in Bangladesh for evidence of lyssavirus activity.\n\n【2】### The Study\n\n【3】This project began as part of a larger study concerned with a suspected Nipah virus outbreak in the region. Active surveillance of bats was performed in 3 districts of Bangladesh: Meherpur and Naogaon during March 2003 and Rajbari during February and March 2004 . The animals were collected randomly from different roosts, trees, and fruit plantations.\n\n【4】Bats were anesthetized by a 0.05- to 0.1-mg intramuscular injection of ketamine hydrochloride. Injured bats and those that had clinical signs and symptoms were euthanized under sedation by exsanguination. Blood was transferred to serum separator tubes and was refrigerated until centrifugation. Serum was decanted into individual screw-topped vials. When possible, bats were identified to species. Brains of all euthanized bats were removed at necropsy and placed into individual sterile containers. Additional organs and oral swabs were also obtained from each bat. All specimens were held in temporary storage at –18°C (after collection) and later placed at –80°C. Carcasses of representative specimens were placed in formalin for archival purposes.\n\n【5】Bat brains collected in 2003 (N = 212) were tested by the direct fluorescent-antibody test (DFAT) , by using both monoclonal (Centocor Inc. Malvern, PA, USA) and polyclonal (Chemicon International, Temecula, CA, USA) fluorescein isothiocyanate–labeled anti–rabies virus antibodies, and were subsequently processed for isolation by the mouse inoculation test (MIT) . If death occurred during the MIT, mouse brains were subjected to the DFAT, and a second intracerebral passage was conducted with 5% brain suspensions, using 0.22-μm filters (Millipore Corp. Bedford, MA, USA). Bat brains collected in 2004 (N = 151) were subjected to the DFAT only.\n\n【6】The presence of virus-neutralizing antibodies was determined by an adaptation of the rapid fluorescent focus inhibition test (RFFIT), as described . Because of volume limitations and the cytotoxicity of some specimens, 207 serum samples were available for testing at a starting dilution of 1:20, and 81 samples at a starting dilution of 1:50 . Four different lyssaviruses were used in an initial in vitro screening: Aravan, Khujand, Irkut, and ABLV. When a positive result was obtained, testing on the sample was repeated, and comparative assays undertaken for additional lyssaviruses: EBLV-1, EBLV-2, DUVV, and RABV (e.g. routine rabies challenge virus standard, CVS-11). The dose of each virus used for the RFFIT was ≈50 infectious units per 100 μL. The duration of the test was 20 hours for ABLV and CVS-11 and 40 hours for other challenge viruses.\n\n【7】No evidence of lyssavirus antigen was detected in any bat brain by DFAT, and no neurotropic viruses were isolated by mouse inoculation. If a limited number of deaths occurred during the initial MIT (1 or 2 mice of 5 infected), those effects were not reproduced during the subpassage by filtration, which suggests that bacterial contamination of the field samples caused the death of mice during the initial MIT.\n\n【8】One serum sample repeatedly demonstrated neutralizing activity against Khujand virus, at a titer of 54. This sample was obtained from a young female giant Indian flying fox ( _Pteropus giganteus_ )  captured in the Meherpur District in 2003. Two other serum samples obtained from the same species (1 male and 1 female) in Rajbari District in 2004, neutralized Aravan and Khujand viruses at titers of 14–16. For these latter samples, neutralization was detected at dilutions of 1:20 and 1:25 but not detected at a dilution of 1:50. The titers of all serum specimens were <10 (i.e. no neutralization occurred in the dilution of 1:20) against other lyssaviruses.\n\n【9】### Conclusions\n\n【10】We have presented serologic evidence of lyssavirus infection in bats from Bangladesh. Antigenic cross-reactivity has been reported among Aravan, Khujand, and other members of the _Lyssavirus_ genus . Therefore, detectable antibody may cross-react with other related lyssaviruses, as well as with viruses yet to be discovered.\n\n【11】Based upon this preliminary survey, a low prevalence of lyssavirus infection appears in _P_ . _giganteus_ in Bangladesh. Thus, it is not surprising that all brain samples collected were negative for detection of lyssavirus antigen. Because different lyssaviruses and bat species are found in Asia, and therefore different virus-host interactions would be expected in the region, extrapolating antibody-positive/virus-positive ratios, as have been estimated from American  or European  bat populations, would be difficult.\n\n【12】Further surveillance for Asian bat lyssaviruses should be conducted. Public health authorities need to be aware of the potential for bats to transmit lyssaviruses, and public education of this potential should be enhanced. Frugivorous bats forage in fruit plantations in many regions of the Old World tropics, including Bangladesh. Indigenous people capture and may consume these animals. Direct contact between humans and bats frequently occurs during these interactions. Absence of current information on human rabies after bat exposure may be a result of inadequate education, incomplete surveillance, and lack of characterization of viruses from rabies cases .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b45f940c-a492-4403-a22b-cf77bc19a967", "title": "Use of US Public Health Travel Restrictions during COVID-19 Outbreak on Diamond Princess Ship, Japan, February–April 2020", "text": "【0】Use of US Public Health Travel Restrictions during COVID-19 Outbreak on Diamond Princess Ship, Japan, February–April 2020\nPublic health travel restrictions (PHTR) have been used by the United States to reduce the likelihood of transmission of selected communicable diseases aboard aircraft . US federal mechanisms used to implement PHTR include the public health Do Not Board (DNB) list and the Public Health Border Lookout (PHLO) record . The DNB, established in 2007, prevents travelers who are contagious or potentially contagious with a communicable disease of public health concern from obtaining a boarding pass for any commercial flight within, to, or from the United States . The PHLO alerts Customs and Border Protection officials to notify the Centers for Disease Control and Prevention (CDC) when a person on PHTR attempts to enter the United States by any port of entry so public health action can be taken, if needed . The public health aspects are managed by CDC and implemented under the legal authority of the Department of Homeland Security (DHS) . State or local health departments, other federal agencies, or international partners may initiate PHTR requests by contacting CDC .\n\n【1】Certain criteria must be met before implementing PHTR . Primarily, the person must be known or believed to be infectious with, or at risk for becoming infectious with, a serious communicable disease that poses a public health threat to others during travel. If not, then \\> 1 of the additional criteria must be met: the person is unaware of his or her diagnosis and cannot be notified by public health authorities, is not following public health recommendations, cannot be located, or is likely to travel on a commercial flight or travel internationally by any means; or PHTR are needed to respond to a public health outbreak or to help enforce a public health order. PHTR are removed when the person is no longer considered infectious or at risk for becoming infectious .\n\n【2】The outbreak of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection on the Diamond Princess cruise ship in Japan in February 2020 was the earliest large-scale use of US PHTR applied to a cohort on the basis of a common exposure. A total of 111 individual PHTR were placed in 1 day, compared with 556 during the 10-year period 2007–2016 . Furthermore, placement of the largest single cohort on US federal PHTR previously comprised 14 persons identified as having had a high-risk exposure to Ebola virus during December 2014–April 2015 .\n\n【3】PHTR generally apply to both US citizens and foreign nationals and can be applied to persons located within the United States or abroad . CDC decided to limit use of PHTR to US citizens and residents on the Diamond Princess on the assumption that these persons had reason to return to the United States. In addition, the DHS implemented and managed travel restrictions for non–US citizens on the Diamond Princess, which are outside of the scope of this article. The use of PHTR for US citizens and residents on the Diamond Princess should also be differentiated from travel restrictions imposed by the US government by presidential proclamation under section 212(f) of the Immigration and Nationality Act that apply to certain immigrants or nonimmigrants .\n\n【4】##### Coronavirus Disease Outbreak on the Diamond Princess\n\n【5】On February 3, 2020, the Diamond Princess cruise ship arrived in Yokohama, Japan, carrying 2,666 passengers and 1,045 crew . Two days earlier, 1 symptomatic passenger who departed the ship in Hong Kong had tested positive for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the causative agent of coronavirus disease (COVID-19). By February 5, additional passengers on the cruise ship tested positive for the virus; Japanese authorities instituted a 14-day onboard quarantine for all passengers. Effective quarantine of the crew was challenged by communal living quarters, few single-occupancy rooms for isolation, and the need for crew to continue performing essential duties . Passengers and crew testing positive for SARS-CoV-2 were transferred to hospital isolation wards, along with some of their family members who had not been tested or had tested negative. During Japan’s 14-day quarantine, public health authorities relocated passengers \\> 80 years of age or with underlying conditions, along with passengers residing in windowless cabins, to land-based quarantine facilities .\n\n【6】Preliminary data suggested that although most transmission occurred before quarantine implementation, there was residual risk for transmission among crew members and among passengers sharing cabins . By February 18, Japan reported 531 confirmed cases (65 crew, 466 passengers) on the Diamond Princess, representing 14% of those on board; additional test results were pending. Concurrently, positive tests among passengers were declining, but positive cases were increasing among the crew . The overall infection rate on the Diamond Princess (19.2% of passengers and crew) exceeded the reported infection rate (110/100,000 population) in Hubei Province, and viral exposure risk was considered high for Diamond Princess passengers and crew . At that time, the United States had reported 15 confirmed cases of COVID-19 in 7 states, all imported or travel related, and travelers from Hubei Province were subject to mandatory quarantine . CDC decided that US citizens and residents on the Diamond Princess should not travel to the United States by commercial carrier until those testing positive for SARS-CoV-2 were no longer infectious and those never testing positive were no longer at risk of becoming infectious .\n\n【7】In light of the apparent continuing spread of COVID-19 aboard the ship during the quarantine, and to prioritize US citizen welfare and safety, the US government offered a large-scale voluntary repatriation involving controlled movement of US citizens, permanent residents, and their partners from the Diamond Princess to the United States and encouraged all eligible to participate. A total of 329 persons disembarked the ship on February 16, evacuating by 2 chartered aircraft configured to prevent and control transmission on board. Except for 1 individual who recovered from COVID-19 in Japan, persons were placed in federally supervised mandatory quarantine by the US government for 14 days after arrival in the United States .\n\n【8】##### PHTR for US Citizens from Diamond Princess in Japan\n\n【9】To prevent commercial travel to the United States by infectious persons, CDC placed all US citizens and residents remaining in Japan on PHTR. Before the repatriation, the US Embassy Tokyo, Mission Japan (USEMB Japan), informed US citizens and residents that those electing to remain in Japan would be placed on PHTR . CDC rapidly established an active monitoring process to confirm that persons not testing positive for SARS-CoV-2 remained asymptomatic and to enable PHTR removal for each person as soon as CDC’s criteria were met. More than 100 US citizens and residents remained in Japan. More than half were hospitalized for medical care due to SARS-CoV-2 infection or underlying health problems; others were spouses or travel companions of hospitalized patients, crew members, or persons who declined repatriation. Operational challenges of PHTR implementation for large cohorts during an epidemic have not been described. To evaluate the use of PHTR in this context and inform future use, we describe PHTR implementation during the Diamond Princess COVID-19 outbreak, including successes, challenges, and lessons learned.\n\n【10】### Methods\n\n【11】##### Implementation and Removal of PHTR\n\n【12】We reconciled manifests from the US repatriation flights and the cruise ship and worked with USEMB Japan to identify and locate US citizens remaining in Japan, whether in hospitals, government quarantine facilities, or hotels; aboard the ship; or as residents of Japan. On February 19, the ship quarantine imposed by Japan’s authorities ended. All identified US citizens and legal permanent residents who had been on the Diamond Princess but declined repatriation were placed on PHTR; they were notified of the PHTR through the USEMB Japan and cruise ship by both email and letter.\n\n【13】For passengers and crew who had not tested positive for SARS-CoV-2, CDC determined that commercial travel would be allowed after they had been off the ship for 14 days, provided they remained asymptomatic and had not tested positive for SARS-CoV-2 in the interim. To ensure these criteria were met, CDC implemented an active monitoring system until 14 days after the last potential exposure (i.e. disembarkation or close contact) . For those who experienced symptoms during the monitoring period, CDC and USEMB Japan facilitated medical evaluation in coordination with authorities in Japan.\n\n【14】For persons who tested positive for SARS-CoV-2, criteria to remove PHTR and discontinue isolation were based on CDC criteria in effect at the time: documentation of negative results in 2 consecutive sets of both oropharyngeal (OP) and nasopharyngeal (NP) specimens collected \\> 24 hours apart; absence of fever without the use of antipyretic medications; and improvement in other symptoms . Hospital discharge criteria in use in Japan at the time required either 2 NP or 2 OP specimens collected \\> 12 hours apart. For some patients, CDC was able to request the additional testing and sampling timeframe needed to meet CDC’s criteria. However, testing for SARS-CoV-2 in Japan at the time was covered under public funding and required approval of local public health centers . Obtaining additional tests to meet CDC criteria (criteria 1) was challenging for many hospitals, and CDC adopted modified criteria (criteria 2) on February 27 that accepted Japan’s testing strategy but added as criteria the times since first positive test ( \\> 15 days) and second negative test result ( \\> 5 days) and absence of a productive cough . Time-based criteria were determined from available CDC data that suggested viable virus was rarely recovered from patients later in their clinical course .\n\n【15】CDC coordinated with USEMB Japan and the cruise line to provide passengers, crew, and hospitals with the criteria and procedures for removal of PHTR . USEMB Japan consular staff monitored clinical status of all hospitalized US citizens, coordinating directly with hospitals and patients, and provided daily updates to CDC response team, which verified when patients met the criteria for discontinuation of PHTR and the anticipated date of removal. Japanese health workers, in close communication with USEMB Japan, assisted CDC active monitoring efforts for 9 passengers hospitalized for non–COVID-19 health concerns.\n\n【16】##### Evaluation of PHTR\n\n【17】Using a database consolidating data from the cruise ship, the government of Japan, USEMB Japan, and CDC that was originally used for PHTR implementation, we described characteristics of persons subject to PHTR: US state of residence; sex; age; crew or passenger; history of close contact with a case (e.g. infected cabin mate); number of cabin mates; initial SARS-CoV-2 test result; presence of symptoms at the time of testing (symptomatic or asymptomatic), and disposition (entered monitoring or hospitalized). For those with COVID-19, we described their clinical severity and types of samples collected to meet the discharge criteria (OP, NP, or both), comparing proportions of those meeting original CDC criteria for PHTR removal to the modified criteria. We described outcomes, successes, and challenges of the monitoring process to identify lessons learned, based on our professional judgment and expertise on containing infectious diseases. We analyzed data using R .\n\n【18】### Results\n\n【19】##### Demographics and Dispositions of Persons with PHTR\n\n【20】CDC initially placed 108 US citizen passengers and crew on PHTR on February 19, 2020. One additional passenger self-declared as a legal permanent resident of the United States, and USEMB Japan identified 2 additional persons with dual citizenship, for a total of 111 PHTR. Thirteen persons did not live in the United States but reported secondary residences in or frequent travel to the United States. Of the 98 persons residing in the United States, 30 (31%) resided in California.\n\n【21】From the 111 US citizens and residents remaining in Japan, 44 entered active monitoring after disembarking the ship. Three of these persons were tested for SARS-CoV-2 during the active monitoring period, 1 because of a fever, 1 because of hospitalization for other reasons, and 1 for close contact to a confirmed case; 2 tested positive for SARS-CoV-2. Overall, 69 US citizens had positive test results for SARS-CoV-2 and were hospitalized in 35 hospitals in Japan . Of these patients, 39 (57%) were symptomatic at the time of testing; some asymptomatic but ultimately testing positive US citizens disembarked the ship with test results still pending. Fourteen percent of all persons with COVID-19 were critically ill at any timepoint, including 1 patient with COVID-19 who became critically ill unrelated to COVID-19. The median age of COVID-19 patients was 71 years (range 25–92 years); median age of those testing negative for SARS-CoV-2 was 63 years (range 3–85 years; p = 0.005).\n\n【22】##### Removal of the PHTR\n\n【23】US citizen and resident passengers disembarked February 19–22 and crew on February 27 . Of the 42 persons (37 passengers, 5 crew) never testing positive for SARS-CoV-2 , 40 (95.2%) completed active monitoring until 14 days after their last potential exposure, spending a median of 16 days with PHTR in place (range 6–34) depending on their disembarkation date . Two persons were unreachable, and CDC removed their PHTR 28 days (2 incubation periods) after the date of disembarkation.\n\n【24】Of the 69 patients with COVID-19, 40 recovered patients were able to meet the original criteria for PHTR removal, and 29 met the modified criteria. The median time from PHTR placement to removal was 17 days (range 7–57 days) . The median time from notification of a positive test to PHTR removal was 25 days (range 12–62 days) . Multiple persons reported persistent positive test results after symptom resolution; complete information was not available for the full cohort. CDC removed all PHTR by April 15.\n\n【25】### Discussion\n\n【26】The Diamond Princess COVID-19 outbreak represents a unique event in public health history: the quarantine of thousands of persons aboard a cruise ship for a newly identified disease at a time when most countries had few imported cases but the epidemic rapidly became a pandemic. Lessons learned from the experience of implementing PHTR on an unprecedented scale can inform the future use of PHTR in outbreaks with pandemic potential.\n\n【27】The first critical lesson is that during a novel disease epidemic, the lack of information about the disease challenges the general practice that ethical use of PHTR requires evidence-based decisions regarding when persons may be contagious or at risk of becoming contagious . CDC routinely uses PHTR for diseases such as infectious tuberculosis or measles, in which case definitions and criteria for determining noninfectiousness are well established . During this incident, little was known about SARS-CoV-2 transmission; transmission dynamics in Wuhan, China, and during the severe acute respiratory syndrome coronavirus  and Middle Eastern respiratory syndrome coronavirus (since 2012) epidemics were used in establishing criteria for PHTR removal.\n\n【28】As the COVID-19 epidemic unfolded, we observed notable challenges in identifying persons who might transmit SARS-CoV-2 during travel or contribute to cross-border spread of disease . Limited information was available to determine whether potentially exposed persons who remained asymptomatic were no longer at risk of becoming infected or when infected persons were no longer infectious because the optimal monitoring period had not yet been determined and it was possible to persistently test positive . Particular challenges included differences in strategies for discontinuing isolation between the United States and Japan, differences in SARS-CoV-2 testing procedures between hospitals and laboratories in Japan and obtaining medical records and English translations . These challenges highlight the importance of multinational coordination at all levels and sectors. CDC used the best available data to modify criteria as needed through the event, ensuring that PHTR removal criteria were both evidence based and feasible in the international setting. The modified criteria simplified the PHTR removal process for many infected persons, and the time to remove PHTR was only 5 days longer for those meeting modified criteria . In this experience, clear and consistently applied criteria for removal of PHTR were essential, as was the flexibility to overcome the systemic challenges to meeting those criteria.\n\n【29】Second, there are logistical challenges in applying and monitoring travel restrictions on this scale. US government personnel had to identify each person for whom travel restrictions were warranted, verify their identities against federal databases, add them individually to the DNB and PHLO, document that criteria for removal of PHTR were met, and ensure timely removal of PHTR once the criteria were met. Because this situation took place on a cruise ship that kept detailed passenger and crew manifests, it was relatively straightforward to identify, notify, and obtain contact information for the persons at risk. If exposure had occurred in a less well-defined situation, identification would have been particularly challenging. However, the complex process to add >100 persons to the DNB and PHLO in the compressed timeframe of 24–48 hours required substantial federal resources.\n\n【30】Typically, documenting criteria for PHTR removal is completed by US jurisdictions or foreign public health authorities . For this event, CDC independently created a mechanism to document that exposed persons remained asymptomatic at the end of the 14-day period through an active monitoring process that continued past the end of Japan’s mandated quarantine period. Despite rapid implementation, CDC’s monitoring process worked well: most travelers had access to email either directly through their personal smartphones or phones provided by the cruise line or through family members in the United States who could communicate with them, and they responded to daily CDC information requests. The USEMB Japan could track the status of hospitalized patients, although this process required substantial resources for translation. Monitored persons and the involved agencies coordinated a large volume of communication both in managing emails with active monitoring information from Diamond Princess travelers and responding to individual travelers’ queries about their situations. However, in situations in which this level of coordination is unlikely or persons are difficult to contact, active monitoring for the purposes of PHTR may be challenging to accomplish . Before considering use of PHTR, especially on a large scale, implementors should evaluate what resources or capacity are available for identifying persons to be placed on PHTR, personnel required across all relevant agencies and countries to monitor travelers’ status and document when they are eligible for removal of PHTR, diagnostic testing capacity, and communications channels.\n\n【31】Third, it is critical that travel restrictions do not create undue risk to affected persons. Persons with positive tests or who became ill could access high-quality medical care in Japan; ethics considerations could be different for persons seeking medical care in areas with inadequate medical infrastructure. In such situations, charter travel or medical evacuation may be options for US citizens and residents to return safely to the United States , but this option is challenging on a large scale and may have been especially difficult in this situation, in which some of the patients remaining in Japan had complex medical needs. In this situation, the cruise line covered most travel expenses incurred by restricted travelers; CDC can assist travelers whose travel is restricted or delayed for public health reasons by requesting airlines to waive rebooking fees.\n\n【32】Limitations of the analysis were incomplete information, including hospital locations and clinical severity, and inability to obtain the specimen collection date. We approximated date of positive test result by the date of notification.\n\n【33】As with all decisions, overall costs and benefits should be considered when determining whether to use PHTR. Protection of the public, individual civil liberties, and increasing risk for disease transmission are all considerations that can inform use of PHTR . Costs and benefits may vary as outbreaks or pandemics progress; the benefit was evident in this event when the United States had only 15 cases. By mid-March 2020, the United States had reported over 6,000 confirmed cases of COVID-19. Most states implemented community mitigation measures such as social distancing requirements and cancellation of mass gatherings, especially once the World Health Organization declared COVID-19 a pandemic on March 11 .\n\n【34】PHTR implementation may have prevented transmission via air travel among Diamond Princess travelers under active monitoring and never tested. Transmission on the cruise ship was possible for many days before the vessel quarantine began; it is likely there were many asymptomatic but SARS-CoV-2–positive persons aboard . As the domestic outbreak grew, CDC shifted to different containment strategies for cruise ships on which COVID-19 cases occurred, including requirements for noncommercial travel after disembarkation and a No Sail Order for all cruise ships operating in US waters .\n\n【35】CDC continues to use individual-level PHTR for persons with known or suspected COVID-19 or high-risk exposures and advises persons who are symptomatic, test positive for SARS-CoV-2, or have been exposed to someone with COVID-19 to delay travel until no longer infectious or at risk of becoming infectious. CDC shares information and considerations for use of PHTR with health authorities of other countries but recognizes that many countries may not have the resources to manage similar systems. Lessons learned from the Diamond Princess outbreak are essential in planning future PHTR use during this pandemic or future outbreaks of novel diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "253c9de4-749f-4bba-bc3a-5930e326ff81", "title": "Animal Viruses: Molecular Biology", "text": "【0】Animal Viruses: Molecular Biology\nIn this multi-author work, Mettenleiter and Sobrino have compiled 10 chapters that describe what is currently known about the molecular biology of some of the most interesting viruses of veterinary importance, from the tiny circovirus of pigs (1,800 nt of single-stranded DNA) to the highly complex African swine fever virus (≈200,000 nt pairs of double-stranded DNA). It is fitting that the first chapter describes foot-and-mouth disease virus, which was the first animal virus to be described by Loeffler and Frosch, who worked in Griefswald-Insel Riems, where Mettenleiter is currently the president of the Friedrich-Loeffler Institut. All 10 chapters are written by experts in their respective fields. Mettenleiter is a coauthor for a chapter about herpesviruses, whereas Sobrino is a coauthor for one on foot-and-mouth disease virus. Polly Roy wrote a chapter about bluetongue virus, one of the major threats to the livestock industry worldwide, which recently emerged in Europe, perhaps because global warming has allowed the _Culicoides_ vector to survive and overwinter. Another chapter is about Hendra and Nipah viruses, which are newly emerging in Southeast Asia and Australia. There are also informative chapters on arteriviruses, coronaviruses, and pestiviruses. Finally, in 1 chapter, Hans-Dieter Klenk and colleagues write about viruses of birds, including avian influenza. They discuss the molecular mechanism of pathogenesis and host range for the virus everyone fears may give rise to the next influenza pandemic.\n\n【1】The book would have been improved by including a chapter on paramyxoviruses, of which rinderpest virus of cattle and Newcastle disease virus of birds are 2 important examples. But, overall, this compilation is excellent and is rounded off by a scholarly and provocative epilogue about animal virology by Esteban Domingo and Marian C. Horzinek.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0aba42b9-a279-46af-9e5a-8d569dad875a", "title": "Mycobacterium triplex Pulmonary Disease in Immunocompetent Host", "text": "【0】Mycobacterium triplex Pulmonary Disease in Immunocompetent Host\nNontuberculous mycobacteria (NTM) are ubiquitous organisms, commonly isolated from environmental and animal sources , whose pathogenicity may vary according to the host’s immune status. Although exposure to NTM frequently causes no symptoms, clinical manifestations may range from hypersensitivity reactions  to destructive, even fatal, lung disease. In the case of lung disease caused by NTM, clinical and radiologic features are sometimes indistinguishable from those seen in lung disease caused by _Mycobacterium tuberculosis_ complex (MTB). The clinical importance of NTM is often difficult to determine, especially in patients with chronic, preexisting lung disease; criteria for diagnosing disease caused by NTM, most recently updated by the American Thoracic Society in 1997 , need to be properly fulfilled.\n\n【1】_M. triplex_ was first described in 1996 . Investigators reported a group of slowly growing, nonpigmented mycobacteria, resembling _M. simiae_ or _M. avium_ complex (MAC) in biochemical tests, which did not react with the commercial probe designed for MAC. The primary characterization of this new organism relied on conventional biochemical tests and analysis of mycolic acids with high-performance liquid chromatography (HPLC), but conclusive evidence was based on sequencing the 16S rRNA hypervariable region. In HPLC analysis, _M. triplex_ produces a triple-clustered mycolic acid profile closely related to those of _M. simiae_ , _M. genavense_ , and the recently described _M. sherrisii_ , but practically indistinguishable from that of _M. lentiflavum_ , a novel mycobacterium characterized by Springer et al. Phylogenetic studies showed that _M. triplex_ and _M. lentiflavum_ are closely related to _M. simiae_ and _M. genavense_ .\n\n【2】_M. triplex_ has been reported to cause episodic infection in AIDS patients or in those with other immunocompromising diseases . We present the case of an apparently immunocompetent patient with pulmonary disease caused by this mycobacterium and review the characteristics of two similar cases.\n\n【3】### The Case\n\n【4】On January 2002, a 54-year-old, HIV-negative, white woman was referred to the outpatient pulmonary service because of persistent cough lasting >2 years and fatigue. She lived in an urban area, had smoked cigarettes (40 packs/year) for several years, but did not report any history of alcoholism or use of immunosuppressive drugs. Results of physical examination and routine laboratory tests, including a standard tuberculin skin test, were unremarkable. A chest x-ray showed considerable fibrotic interstitial changes associated with patchy parenchymal shadowing in both lower fields and multiple thin-walled cavitary lesions (0.7–3.5 cm) in both upper lobes. A computed tomographic (CT) scan of the chest confirmed the above lesions, including multifocal bronchiectases in the right middle lobe and multiple small nodules in the lower lobes. Smears for acid-fast bacilli (AFB) were positive on two bronchial washing samples. These specimens produced negative results when tested by a commercial strand displacement amplification assay (Becton Dickinson Biosciences, Sparks, MD) specific for MTB. Chemotherapy with isoniazid, rifampin, and ethambutol was started. Cultures produced a slow-growing, nonpigmented mycobacterium that was identified in June 2002 as _M. lentiflavum_ , according to the HPLC pattern of mycolic acids. One more bronchial specimen collected in mid-June was AFB smear-positive and yielded the same organism as was detected in January. Isolates were considered clinically relevant, and chemotherapy was changed to ethambutol and clarithromycin.\n\n【5】In December 2002, after 6 months of treatment, a CT scan did not show reduction of pulmonary lesions, although the general condition of the patient had slightly improved. Smears from one out of four additional bronchial washing samples were positive for AFB, and all specimens yielded the same mycobacterium previously detected. A more accurate evaluation of clinical isolates showed that their phenotypic pattern was different from that of _M. lentiflavum_ by results of some biochemical tests and the absence of pigmentation. Further gene sequencing study of the 16S rDNA showed that our isolates showed 100% homology with the reference strain of _M. triplex_ (ATCC 70071). In March 2003, although chemotherapy was well tolerated, microbiologic tests of two bronchoalveolar lavage (BAL) specimens continued to be AFB smear- and culture-positive. Consequently, chemotherapy was changed to include levofloxacin, ethambutol, and clarithromycin. In October 2003, chest x-ray examination and a CT scan showed a slight reduction in the size of lung cavities. The patient continues to receive medication, and the radiologic picture has not improved.\n\n【6】### Microbiologic Aspects\n\n【7】After standard N-acetyl-L-cysteine-sodium hydroxide decontamination , bronchoaspirate washing and BAL specimens were stained with routine Ziehl-Neelsen and cultured by a combination of the radiometric Bactec system (Becton Dickinson Biosciences) and Löwenstein-Jensen medium. Recovered strains were tested with a commercial multiplex line probe assay (Inno-LiPA Mycobacteria v2, Innogenetics, Ghent, Belgium) specific for MAC and 16 other different mycobacterial species . Further identification studies were performed by mycolic acid HPLC analysis  and 16S rDNA sequencing . Drug susceptibility pattern was determined in liquid medium by using the radiometric macrodilution method developed for MAC .\n\n【8】_M. triplex_ strains were repeatedly isolated on both liquid and solid media . An extended panel of biochemical and cultural tests was used for conventional identification  . The hybridization test performed with the multiplex line probe assay was negative apart from the genus-specific line probe. Final identification was achieved by 16S rDNA gene sequencing. In fact, while HPLC profile was poorly discriminative between _M. lentiflavum_ and _M. triplex_ , 16S rDNA sequencing showed 100% homology with the _M. triplex_ reference strain, which allowed the attribution of all our strains to this species . MICs (μg/mL) performed on the first isolate were the following: streptomycin 6.0, isoniazid >0.5, rifampin >2.0, ethambutol 7.5, ciprofloxacin 4.0, clarithromycin 8.0, amikacin 8.0, ethionamide 2.5, and rifabutin 0.5. Drug susceptibility testing was completed after 6 days. Tentative interpretations of MIC results, according to NCCLS M24-A  and data from Heifets , are reported in Table 1 .\n\n【9】### Conclusions\n\n【10】A search of the literature from 1996 (when _M. triplex_ was first described) to January 2004 yielded two reports of pulmonary infections in immunocompetent patients . In those reports, clinical and radiographic features of _M. triplex_ pulmonary infection did not differ substantially from those of TB and other NTM. Cough, hemoptysis, and fatigue were the primary symptoms, while radiographic studies found pulmonary nodules most commonly, followed by lung infiltrates, multifocal bronchiectasis, and cavitations . No patient had underlying diseases when pulmonary infection with _M. triplex_ was diagnosed, and tuberculin skin test results were negative or not reported. Although a history of preexisting pulmonary lesions could not be documented for these patients, repeated isolation of mycobacteria from different respiratory samples and the absence of other possible causes of pulmonary disease suggest that _M. triplex_ was likely to cause symptomatic infection rather than colonization .\n\n【11】_M. triplex_ strains were isolated on both liquid and solid media in our case, while culture media were not reported in the other two previously published cases. One case reported an extended panel of biochemical and cultural tests for conventional identification that showed the absence of pigment and positive reactions to nitrate reductase, urease, and semiquantitative catalase as the most useful characteristics for tentative identification . All strains failed to hybridize with the commercially available genetic probe for MAC (Accuprobe, Gen-Probe Inc. San Diego, CA), so all reported cases could be definitively identified by 16S rRNA gene sequencing and similarity search with the BLAST alignment software . One strain showed 100% homology with the reference strain, while another showed homology of 99.5% and, therefore, despite its close relationship, was regarded as a variant of _M. triplex_ .\n\n【12】Patients (including our case-patient) were given different treatment regimens with two to four antimicrobial agents. Ethambutol, rifampin, clarithromycin, and ciprofloxacin were mainly used . Clinical improvement, as defined by resolution of symptoms and radiographic findings (infiltrates and cavitary lesions), was obtained within 10 months after therapy was initiated in one of three patients. One patient improved when clarithromycin and ciprofloxacin were added to the regimen, while another was reported to have improved after drug therapy was initiated . Our patient was still sputum culture-positive 2 years after therapy was initiated, despite exhibiting minor evidence of clinical and radiologic improvement.\n\n【13】Our findings show that _M. triplex_ infrequently causes pulmonary disease in immunocompetent persons. Treatment with a three- or four-drug combination, including clarithromycin, ciprofloxacin, and ethambutol, was shown to be associated with reduced symptoms and good clinical outcome. Although at present only 16S rDNA sequencing can identify _M. triplex_ , presumptive identification can be made when a slow-growing, nonpigmented NTM reduces nitrates, produces urease and semiquantitative catalase, and exhibits a three-clustered HPLC profile.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "de335136-f77b-4127-8fd3-5b4e2229a03f", "title": "CDC Safety Training Course for Ebola Virus Disease Healthcare Workers", "text": "【0】CDC Safety Training Course for Ebola Virus Disease Healthcare Workers\nIn 2014, epidemic Ebola virus disease (EVD) rapidly spread throughout West Africa; by August of that year, ≈2,600 EVD cases and 1,400 deaths had been reported . Widespread EVD transmission occurred in Guinea, Sierra Leone, and Liberia for several reasons. First, these countries had undergone years of civil war and unrest, which damaged an already fragile healthcare infrastructure and reduced the healthcare workforce , gravely limiting the countries’ ability to rapidly respond to a growing epidemic . Second, EVD is a hemorrhagic fever readily transmissible in the absence of rigorous infection prevention and control (IPC) . Ebola virus is spread by direct contact with body fluids of patients or contaminated fomites . For outbreak control, isolation of patients from the community is essential . EVD patients can arrive at healthcare facilities with severe symptoms such as substantial dehydration from vomiting, diarrhea, or hemorrhage, requiring aggressive intravenous resuscitation . Third, the EVD epidemic placed medical workers themselves at risk. Few healthcare workers have cared for patients with such a severe and highly transmissible disease requiring this degree of stringent IPC. The close patient interactions that were needed put healthcare workers at risk for infection .\n\n【1】Personal protective equipment (PPE) serves as a physical barrier and can protect healthcare workers when used properly. However, PPE is only one IPC measure used to protect healthcare workers from EVD . Moreover, availability of PPE alone is not adequate for preventing infection. Without strict adherence to the complex processes of donning and doffing PPE and proper conduct while wearing PPE, transmission can still occur. Improper donning and doffing of PPE can result in self-contamination if unprotected mucous membranes or broken skin are exposed to infected body fluids . PPE doffing, in particular, carries high risk for self-contamination because of its complexity combined with healthcare worker fatigue after tiring shifts in an ETU . Fourth, the setting of this epidemic was unusual. Unlike previous Ebola outbreaks, which occurred predominantly in rural areas, the 2014 EVD epidemic occurred primarily in densely populated urban areas. Previous rural outbreaks had been controlled by isolating EVD patients from the community through early admission to healthcare facilities capable of managing the disease. In 2014, the rapid increase in the number of EVD patients early in the epidemic quickly overwhelmed the number of trained clinicians and healthcare facilities that could care for them .\n\n【2】By late August 2014, a total of 240 registered healthcare workers had acquired EVD and 120 had died . As the number of infected healthcare workers rose, medical staff became increasingly fearful of contracting EVD from patients. The World Health Organization (WHO) found risk of contracting EVD during this epidemic to be 21–32 times higher among healthcare workers than among non–healthcare workers . Some clinics and hospitals closed because of staff shortages or healthcare workers’ unwillingness to work, exacerbating the lack of facilities .\n\n【3】Transmission models developed by the US Centers for Disease Control and Prevention (CDC) indicated that to halt the epidemic, ≈70% of EVD patients should be isolated in appropriate treatment facilities . The models projected that if transmission were not rapidly reduced, EVD cases in Liberia and Sierra Leone could reach 550,000 by January 2015 . A key component of the international response to the epidemic entailed deploying trained volunteer healthcare workers to EVD-affected areas to reduce community transmission by isolating EVD patients and providing care in a safe healthcare setting. To support this urgent need, CDC developed and implemented an introductory EVD safety training course to prepare volunteer US healthcare workers to work in West Africa Ebola treatment units (ETUs).\n\n【4】Few deploying clinicians had been trained in the infection control practices needed to provide EVD care safely in limited-resource settings, which are distinctly different from US hospitals. In August 2014, the only structured EVD training for healthcare workers was a 2-day course held in Brussels, Belgium, by Médecins Sans Frontières (MSF) . MSF acquired extensive EVD care experience in Africa and developed this course to share knowledge with staff deploying to respond to the epidemic . Given the urgency and need for international healthcare volunteers, demand quickly exceeded course availability. On August 26–27, 2014, three CDC members attended the MSF course in Brussels in anticipation of developing a US-based version of the training. After rapid course planning and development, CDC launched its first EVD Safety Training Course in Anniston, Alabama, USA, on September 22, 2014. We summarize the development and operation of the course.\n\n【5】### Course Concept\n\n【6】The course objective was to introduce deploying healthcare workers to principles and practices of safely providing patient care in a West Africa ETU. Key learning objectives included understanding of the following: EVD modes of transmission, ETU structure and operation, ETU IPC procedures (proper PPE donning and doffing techniques, disinfection, sharps and waste management), and personal safety within ETUs (psychologic preparation, stress management, overheating while wearing PPE). The various organizations with which trainees would deploy stocked different types of PPE. Thus, our training strategy centered on teaching sound principles and methods to prevent disease transmission, rather than focusing on a particular type of PPE or protocol. We wanted to prepare volunteers for the complex and changing clinical and social environment in the center of a transmissible disease epidemic of unprecedented scope and severity. The course included classroom instruction and practical hands-on training in a realistically constructed mock ETU. At the time, West Africa ETUs were simple healthcare isolation units that combined a specific layout with rigorous IPC practices and offered patient isolation, diagnosis, and oral and intravenous rehydration therapy and medications. Therefore, we focused clinical management instruction on these topics.\n\n【7】The course provided introductory training as the first stage of a more comprehensive process, which involved further in-country mentoring under direct supervision of local or international staff with previous EVD experience. We designed a sustainable, repeatable course model that enabled efficient course implementation by sequential cohorts of instructors. Beginning in September 2014, the US-based 3-day course was offered weekly at the same location.\n\n【8】### Staff and Setting\n\n【9】The initial course design team was a multidisciplinary 15-person unit comprising members who had attended the MSF Brussels course, infectious disease physicians, medical epidemiologists, instructional designers, and healthcare workers recently deployed to West Africa who had worked in ETUs or EVD-affected communities (returning responders). Course development incorporated input from experts in public health and EVD from CDC, MSF, and WHO and from US-based infection control experts. When the pilot course was launched, the team had grown to a 40-person unit including data managers, communication specialists, and logisticians.\n\n【10】The course was held at the US Federal Emergency Management Agency (FEMA) Center for Domestic Preparedness (CDP) in Anniston . CDP is an all-hazards training center equipped with classrooms, audiovisual equipment, dormitory-style lodging, and food and transportation services. The 124-acre campus has buildings and outdoor spaces well suited for the construction of austere mock West Africa ETUs for simulated exercises. CDP trains ≈45,000 emergency responders yearly and efficiently supported the rapidly expanding course. The location, 90 miles from CDC headquarters in Atlanta, Georgia, USA, enabled relatively convenient transportation of staff, supplies, and trainees.\n\n【11】### Trainees\n\n【12】We assigned high priority to US healthcare workers scheduled to deploy to West Africa. We required that trainees have a license to provide clinical care, recent experience providing direct patient care, and affiliation with a governmental or nongovernmental organization responsible for travel to and from West Africa. Healthcare workers included nurses, physicians, paramedics, physician assistants, and others who would work directly with EVD patients in ETUs . Additional participants included representatives of organizations who were interested in designing similar courses or assessing the course’s suitability for their deploying staff.\n\n【13】##### Operations and Logistics\n\n【14】The 3-day course consisted of lectures, small-group discussions, and practical exercises requiring trainees to perform simulated patient care activities in a mock ETU . Course days lasted ≈9 hours. During September 2014 **–** March 2015, a total of 16 courses were held. Trainees traveled to Atlanta independently; CDC provided bus transport from Atlanta to Anniston, private dormitory rooms, on-site transport, and 3 meals per day. The environment promoted easy monitoring of trainees and emotional bonding and support among course participants.\n\n【15】The most valuable supplies for the course, and the most challenging to obtain, were PPE. Other materials were supplied by CDP or purchased locally.\n\n【16】PPE for trainees consisted of coverall (protective suit), eye protection (goggles or full face shield), N95 respirator or surgical mask, 2 pairs of latex gloves, hood covering the head and neck, apron, gum boots, and surgical scrubs . PPE procurement was challenging for 2 reasons. First, protocols dictating which PPE supplies were needed had to be established. However, in 2014, consensus on optimal PPE for use in West Africa ETUs was lacking . Consequently, experts from CDC, MSF, and WHO used preexisting MSF and WHO PPE guidelines to develop protocols for the course . Protocols balanced the anticipated availability of specific PPE in West Africa with safe IPC practices. The goal was to impart a fundamental understanding of infection control measures necessary to avoid self-contamination and assess the safety of PPE that trainees might encounter in West Africa ETUs. Within 2 weeks, we procured a combination of MSF- and WHO-style PPE and supplies from local manufacturers, international distributors, and medical supply companies. Second, worldwide shortages of fluid-resistant coveralls and specially made hoods required rapid substitutions to best emulate what participants might encounter in West Africa ETUs . To conserve PPE in short supply, over the 3-day course, trainees reused fluid-resistant suits and aprons.\n\n【17】### Course Content\n\n【18】As the course development team, we drew course content from materials from MSF, WHO, and CDC. We referenced technical manuals , online resources, videos, and other materials from the MSF Brussels EVD course, as well as input from returning responders and Ebola experts. Course materials included lectures, EVD case scenarios, step-by-step PPE protocols, and practical exercise instruction. Course materials underwent CDC institutional clearance, which entailed detailed review of each topic by CDC-designated experts, and were made available to trainees in paper and electronic formats.\n\n【19】Because healthcare workers in West Africa needed to strictly adhere to infection control principles to minimize the risk of contracting EVD, we focused most course content on IPC. Crucial IPC components for preventing EVD transmission are methodical PPE donning and doffing, proper patient flow and triage, injection and sharps safety, environmental cleaning and waste disposal, safe handling of laboratory samples, and safe management of the dead . We taught these principles through lectures, small-group breakout sessions, and practical exercises.\n\n【20】##### Lectures and Classroom Exercises\n\n【21】Morning sessions were devoted to lectures and small-group activities. Lecture topics included EVD epidemiology, transmission, and pathophysiology; elementary clinical management of patients; IPC; proper ETU design; disinfection and waste management in ETUs; mental health resilience; occupational health; community health promotion; and experimental treatments and vaccines for EVD. Small-group activities consisted of discussions with recently returned EVD responders and a series of tabletop exercises: 1) interactive case studies on EVD recognition and triage; 2) designing safe ETUs, including patient care areas, placement of handwashing stations, and healthcare worker flow; and 3) cultural sensitivity exercises, including techniques for interacting with community members while recognizing and respecting local customs.\n\n【22】##### Exercises in a Mock ETU\n\n【23】Afternoon sessions consisted of practical exercises that involved real-life scenarios, which comprised 50% of the course. Practical exercises requiring trainees to be in full PPE were a foundation of this course. We focused on repetitive practical exercises involving donning PPE with a partner, performing simulated high-risk patient-care activities, and doffing PPE under close supervision. Anecdotal observations indicated that trainees entering the mock ETU experienced increased concentration and anxiety, suggesting a level of realism in the simulated training setting.\n\n【24】In West Africa, healthcare workers faced additional challenges of harsh conditions, such as high temperatures, inconsistent electricity, poor lighting and visibility, and overcrowded ETUs . Returning responders described overworked staff in West Africa, covered in layers of PPE in sweltering heat, who experienced excessive sweating, dehydration, fogged eye protection, and decreased dexterity while caring for and transporting critically ill and dying patients. As core body temperatures rise while wearing PPE, overheating can lead to motor and cognitive impairment, further increasing healthcare worker vulnerability to breaches of safety practices . Thus, we constructed 2 mock ETUs to simulate the challenging conditions trainees might face in West Africa. Our mock ETUs had clearly designated low- and high-risk zones, stocks of PPE with changing areas, simulated chlorine footbaths and handwashing stations, weighted patient dummies, a triage area, and a unidirectional flow pattern from low- to high-risk zones .\n\n【25】Teams of 4–6 trainees entered mock ETUs, where they received a focused orientation and then donned PPE under direct supervision of a course instructor. According to MSF protocol, we taught a buddy system during practical exercises, whereby partners observed each other during PPE donning and regularly checked for breaches in PPE or infection control protocol. Trainees then entered the patient-care area, where they conducted instructor-guided simulated patient-care activities, including collecting and preparing blood specimens for transport, transporting a patient into the ETU, performing environmental decontamination and waste management, and transporting a deceased patient from a patient care area to a morgue. After these activities, trainees learned a regimented doffing process in a designated area of the mock ETU, performing the structured PPE removal sequence under direct supervision of course instructors and the observing partner.\n\n【26】### Course Evaluation\n\n【27】##### Trainee Demographics\n\n【28】By March 25, 2015, 570 trainees attended a total of 16 course sessions. Trainees came from US governmental agencies (n = 352, 62%); 43 nongovernmental organizations, (n = 164, 29%); and other organizations, including foreign governments, private healthcare organizations, and academic institutions (n = 54, 9%) . Trainees traveled from 36 states and 20 countries to attend the course. To our knowledge, although most deployed to ETUs in West Africa, some for months at a time, none of the trainees acquired EVD during deployment.\n\n【29】##### Costs and Staff Resources\n\n【30】A course of this scale required substantial resources. As the course evolved, the number of trainees increased each week. To ensure close supervision during practical exercises, we added course graduates to the staff to maintain an instructor:trainee ratio of 1:4. Over the life of the course, a total of 193 staff (89 CDC, 104 non-CDC) provided the training: 26 experts in infectious diseases and 117 practical exercise course instructors.\n\n【31】We estimate that 30,000 staff person-hours were required for course development (12,000 hours), 16 sessions of course instruction (10,000 hours), and course material revision (8,000 hours). The average total cost for a 3-day course was approximately US$27,000, or $750 per trainee for meals, lodging, transport, administrative coordination, and PPE and other supplies . Course development and implementation relied on a multidisciplinary team; outside experts with ETU experience were essential. Because no mechanism existed to rapidly establish a training course of this scope, assembling and maintaining this large, diverse team was time-consuming and challenging. Institutional support was critical for creating interagency collaborations. Modifying an existing interagency agreement with the Oak Ridge Institute for Scientific Education  was instrumental in finding and supporting the travel of many trainers, and a new interagency agreement between CDC and FEMA provided access to the CDP campus and infrastructure.\n\n【32】### Feedback and Observations\n\n【33】Feedback from course graduates and returning responders during and immediately after each course session confirmed that the most crucial aspects of the course were hands-on, practical exercises, especially donning and doffing PPE. We therefore constructed a second mock ETU where students were able to don and doff PPE, practice dexterity exercises in double-gloved hands, and develop and discuss other potential ETU layouts while waiting to perform the practical exercises in the main ETU. This second mock ETU enabled trainees to practice, ask additional questions, and further discuss infection control procedures.\n\n【34】To improve trainees’ understanding of the systematic process, our teaching model also incorporated trainees as instructors during the practical exercises. We asked trainees to identify breaches in their partner’s PPE and instruct fellow trainees during the doffing process. To better understand PPE doffing, trainees replaced course instructors during the doffing process and gave explicit step-by-step instructions to fellow trainees as they removed PPE piece by piece. To ensure that proper techniques and procedures were followed, course instructors supervised all activities.\n\n【35】We encouraged flexibility in course instructors and trainees in various scenarios but still stressed the value of recognizing a safe work environment. International support for control of the 2014 West Africa EVD epidemic entailed aid from hundreds of international organizations. Healthcare workers who deployed to West Africa therefore encountered a wide variety of PPE supplies, ETU layouts, and safety protocols. Hence, rather than focusing our training on mastering a specific protocol, we attempted to instill a general culture of safety by providing trainees with the knowledge and skills to work safely in ETUs, identify and correct safety deficiencies, and feel empowered to withdraw from unsafe situations.\n\n【36】### Course Sustainability\n\n【37】Given the relative rarity of EVD, limited formal training courses exist worldwide. Several organizations and institutions, including foreign ministries of health, have expressed interest in establishing their own EVD training courses and requested our training materials. In response to these requests and to make course content easily accessible and reproducible, we created a Web-based toolkit that included all lectures, facilitator guides for small-group exercises, comprehensive trainer guides with video tutorials of practical exercises, supply checklists, and administrative templates required to implement the course. The toolkit went through extensive review and clearance by representatives of CDC, MSF, and WHO; on April 2, 2015, the complete toolkit was posted on the CDC Ebola website . The step-by-step instructions and detailed materials in the toolkit might enable other organizations and countries to reproduce this training, given appropriate resources. The kit could help other countries, particularly those with a history of EVD outbreaks, better prepare for and respond to future outbreaks.\n\n【38】### Conclusions\n\n【39】Establishment of the CDC EVD Safety Training Course was a relatively low-cost but high-impact activity that required an exceptional time commitment and flexibility from an evolving multidisciplinary team and dedicated trainees. Effective course execution required staff with diverse specialties, specialized supplies, transportation and housing for trainees, specific facilities for training, rapid access to funding, and complex interagency agreements. The implementation challenges included rapid hiring, contracting, and management of nearly 200 staff; recruitment and selection of course trainees and instructors; and development and review of course materials, including PPE protocols for ETUs in West Africa, when no international consensus existed.\n\n【40】Sudden public health emergencies can demand intensive and specialized training. The CDC EVD Safety Training Course was an innovative and extensive US training effort designed specifically to fill the previously unmet need to prepare clinicians to deploy to West Africa in response to the 2014 EVD epidemic. As was the case for the Haiti cholera epidemic of 2010, rapid development of a specialized clinical training course was a fundamental component of the public health response to epidemic disease . The CDC EVD Safety Training Course quickly increased the number of clinicians who could provide care in West Africa ETUs, showing the feasibility of rapidly developing and implementing focused training in response to a public health emergency. Course graduates could use these specialized skills for future outbreaks of hemorrhagic fevers, although setting-specific components of the course addressing epidemiologic, cultural, and other issues would need to be adapted. Moreover, several key components of the EVD Safety Training Course (i.e. IPC procedures such as proper PPE donning and doffing, personal safety measures such as stress management in ETUs) might have considerable applicability to outbreaks of other pathogens that affect resource-limited settings.\n\n【41】In the following ways, advance preparation could greatly help rapidly mobilize a multifaceted training course as part of the response to future complex epidemic infectious disease emergencies. First, having a standing cadre of dedicated staff and a plan for developing training courses would increase the efficiency and speed of course development and implementation. The plan would include maintaining lists of staff specialized in instructional design, infectious diseases, public health, healthcare infection control, and logistics, who could fill needs depending on the course and contact lists of supplemental staff and contractors. Second, having contact and ordering information for various local and international manufacturers with detailed resource estimates could expedite supply procurement. Third, having established training sites with active partnership agreements in place would bypass the time-consuming and burdensome process of site identification and contract negotiation. Fourth, reliable funding sources and high-level institutional support is critical for quickly overcoming barriers.\n\n【42】The 2014 Ebola epidemic provides a reminder that the threat of global outbreaks of emerging infectious diseases is real and immediate. It is with these threats in mind that CDC and public health partners developed the Global Health Security Agenda , which supports capacity-building in ≈40 countries to prevent, detect, and respond to infectious disease threats. The CDC EVD Safety Training Course is a relevant and timely example of how international partners can work collaboratively to meet the Global Health Security Agenda objectives of more rapidly detecting, responding to, and controlling public health emergencies at their source and thereby enhancing global health security. Maintaining institutional memory of this effort, by establishing a core team of educators who could serve as a dedicated rapid training team, would help preserve expertise gained by development of this course, which in turn would enable a more nimble response to future urgent training needs with regard to new or emerging pathogens.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7bc31b0a-1f94-4e26-9c9b-0df0eb351d41", "title": "Blastomycosis Surveillance in 5 States, United States, 1987–2018", "text": "【0】Blastomycosis Surveillance in 5 States, United States, 1987–2018\nBlastomycosis is a fungal infection caused primarily by inhalation of the environmental fungi _Blastomyces dermatitidis_ and _B. gilchristii_ . The incubation period varies from 2 to 15 weeks, and the clinical spectrum ranges from asymptomatic to life-threatening infections involving acute respiratory distress syndrome or extrapulmonary dissemination . Most identified cases involve pulmonary infection that manifests similarly to other causes of pneumonia . The clinical similarities between blastomycosis and other pulmonary infections often result in diagnostic delays and unnecessary empiric antimicrobial drug treatment for suspected bacterial pneumonia . Because acute illnesses can self-resolve before diagnosis, and because physician awareness of this generally uncommon disease probably is low in most parts of the United States, many blastomycosis cases likely go undetected.\n\n【1】In the United States, most blastomycosis cases are thought to occur in the midwestern, south-central, and southeastern states, in areas surrounding the Ohio and Mississippi River valleys, the Great Lakes, and the Saint Lawrence River. Cases also occur outside these regions, indicating that the infection’s true range is broader than generally appreciated . _Blastomyces_ spp. appear to have an affinity for moist soil and decomposing plant matter, but much remains unknown about its precise environmental niche . The fungus is difficult to isolate from the environment, making investigation of potential sources challenging.\n\n【2】Public health surveillance for blastomycosis in the United States is limited because it is currently reportable in only 5 US states: Arkansas, Louisiana, Michigan, Minnesota, and Wisconsin. Blastomycosis is not nationally notifiable, so the Centers for Disease Control and Prevention does not routinely receive case reports from states where it is reportable. Nevertheless, surveillance data represent some of the most comprehensive information about blastomycosis. Before the Council of State and Territorial Epidemiologists (CSTE) approved a standardized surveillance case definition in 2019 , state health departments used different case definitions . However, state surveillance generally collected similar demographic, clinical, and laboratory data elements, enabling comparisons across states. We summarized available blastomycosis surveillance data to assess the overall burden of disease, geographic patterns and temporal trends, and factors associated with poor clinical outcomes.\n\n【3】### Methods\n\n【4】We combined deidentified data on blastomycosis cases reported in Arkansas during January 1995–May 2018, Louisiana during January 1987–October 2018, Michigan during January 2007–December 2017, Minnesota during January 1999–December 2018, and Wisconsin during January 1990–December 2017. We also used the Louisiana Hospital Inpatient Discharge Database to identify additional cases among hospitalized patients in Louisiana during 1999–2014.\n\n【5】We included data elements that were collected by \\> 3 states. We considered event date as the earliest date associated with the case; for example, symptom onset, or first healthcare visit, laboratory test order, or public health report. We considered all laboratory tests recorded as positive for blastomycosis to be positive, even without an explicitly stated qualitative or quantitative result. Negative blastomycosis test results were not routinely available; therefore, we did not include these in the analysis.\n\n【6】We used patients’ state and county of residence to calculate annual state-specific incidence and county-level mean annual incidence per 100,000 persons by using yearly population estimates from the US Census Bureau, Population Division, Vintage 2015 Special Tabulation . We used χ 2  , Fisher exact, and _t_ \\-tests to identify factors independently associated with hospitalization or death, the Cochran-Armitage test for trends in the proportion of patients who were hospitalized or died, and negative binomial regression to assess incidence trends, and we considered p<0.05 statistically significant. We also compared demographic features and outcomes among cases associated with outbreaks (outbreak cases) and those not associated with outbreaks (nonoutbreak cases) for Minnesota and Wisconsin, the 2 states that reported outbreaks during the surveillance periods we examined. Human subjects review by the Centers for Disease Control and Prevention determined this project to be consistent with nonresearch public health surveillance.\n\n【7】### Results\n\n【8】##### Descriptive Analysis\n\n【9】Data were available for 4,441 cases: 348 from Arkansas, 296 from Louisiana, 186 from Michigan, 671 from Minnesota, and 2,904 from Wisconsin. Most (2,892 \\[65%\\]) patients were male, and the median age was 46 years (range 0–97, interquartile range \\[IQR\\] 31–59) . Most (64%, n = 2,778) cases were among persons of White race, 17%  were among persons of unknown race, 9%  were among persons of Black or African American races, and 5%  were among Asian, Native Hawaiian, or other Pacific Islander races. Most (2,828 \\[71%\\]) patients were not Hispanic or Latino; ethnicity was unknown for 1,015 (26%) patients.\n\n【10】Symptom data were available for 2,005 patients from Michigan, Minnesota, and Wisconsin beginning in 2005. The most common symptoms were cough in 79% (range by state 51%–83%) of patients, fever in 61% (range by state 38%–69%), shortness of breath in 55% (range by state 44%–85%), and weight loss in 54% (range by state 29%–62%).\n\n【11】Among 2,912 patients with hospitalization data, 57%  were hospitalized. The median length of hospitalization was 7 days (range 1–379 days, IQR 4–15 days; n = 1,231). Among 3,385 patients with mortality data, 278 (8%) died. The proportion of hospitalized patients did not change significantly during 2007–2017 (p = 0.252), but the proportion of patients who died increased from 9.9% to 12.4% (p = 0.017).\n\n【12】Data on positive blastomycosis laboratory tests were consistently available from Arkansas, Michigan, and Minnesota . Among 1,241 reported cases from the 3 states, the most common test types were culture among 835 (67%) cases and microscopy among 333 (27%) cases. Less commonly reported tests included positive antigen tests for 206 (17%) cases and antibody tests for 59 (5%) cases.\n\n【13】Among 777 patients with available data, the median time from symptom onset to diagnosis was 33 days (range 1–2,996 days; IQR 16–75 days). We did not observe clear seasonal patterns by event month. Minnesota had 32 (5%) outbreak cases and Wisconsin had 181 (6%) outbreak cases. Outbreak cases were more frequent among younger persons (median age 25 years) than nonoutbreak cases (median age 45 years; p = 0.0092). Outbreak cases also more often occurred among female persons (41% vs. 34% of nonoutbreak cases; p = 0.0365) and non-white persons (28% vs. 19% of nonoutbreak cases; p = 0.002). In addition, persons with outbreak cases were less likely to be hospitalized (45% vs. 58% of nonoutbreak cases; p = 0.003) or to have died (2% vs. 9% of nonoutbreak cases; p = 0.001).\n\n【14】##### Bivariable Analysis\n\n【15】Age; female sex; non-White race; and positive antigen, culture, and microscopy tests had statistically significant associations with hospitalization . The median age among hospitalized patients was 46 years compared with 44 years for nonhospitalized patients (p = 0.015). Female patients were more likely to be hospitalized (relative risk \\[RR\\] 1.13; 95% CI 1.06–1.21) than male patients. Persons of non-White races were more likely to be hospitalized (RR 1.13; 95% CI 1.05–1.21) than persons of White race. Patients with positive antigen tests (RR 1.25; 95% CI 1.13–1.37), positive culture (RR 1.28; 95% CI 1.20–1.36), and positive microscopy (RR 1.32; 95% CI 1.23–1.43) were more likely to be hospitalized than patients without positive results for those laboratory tests. Factors significantly associated with death were older age (median 61 years vs. 44 years; p<0.001) and positive microscopy test (RR 1.76; 95% CI 1.34–2.38).\n\n【16】##### Incidence\n\n【17】During years for which data were available from all 5 states, 2007–2017, surveillance detected 2,111 cases, a mean of 192 cases per year. In Arkansas, incidence declined from 1.3 cases/100,000 population in 1995 to 0.4 cases/100,000 population in 2017 (p<0.001) . Incidence was stable during each state’s surveillance period in Louisiana, Michigan, and Minnesota. Mean annual incidence was 0.2 cases/100,000 population in Louisiana, 0.2 cases/100,000 population in Michigan, and 0.6 cases/100,000 population in Minnesota. In Wisconsin, incidence peaked at >3 cases/100,000 population during 2006, 2010, and 2015. Mean annual county-level incidence in Wisconsin was highest in Menominee (42.1 cases/100,000 population), Lincoln (28.4 cases/100,000 population), and Vilas (26.5/100,000 population) counties .\n\n【18】### Discussion\n\n【19】We summarize blastomycosis surveillance data from 5 states and provide a broad update on the basic epidemiology of this enigmatic and underrecognized disease. Many patients experienced severe outcomes and diagnostic delays. Our results show that blastomycosis is underdetected, even in states where it is reportable, and that more standardized and in-depth surveillance, ideally in additional states, would help public health professionals better identify highest-risk groups and emerging areas for targeted prevention messaging.\n\n【20】Blastomycosis often results in severe illness, even in previously healthy persons , but this observation might be influenced by underdetection of asymptomatic or milder, self-resolving disease. The high hospitalization rate of 57% noted in this analysis demonstrates that blastomycosis surveillance detects severe cases, which is typical for passive disease surveillance. We found an annual mean of <200 cases/year; a hospitalization rate of 57% suggests that ≈110 patients are hospitalized each year from states where blastomycosis is reportable. In contrast, ≈1,000 blastomycosis-associated hospitalizations occur nationwide , showing that the limited surveillance likely underdetects cases nationally.\n\n【21】The average time of >1 month from symptom onset to diagnosis indicates delays in seeking healthcare, delays in diagnosis, or both. This time interval is consistent with a previous report describing a median of 23 days between examination at a healthcare facility and a median of 2.5 courses of antibacterial medications for presumed bacterial infection before pulmonary blastomycosis was correctly diagnosed . Earlier diagnosis might reduce unnecessary antibacterial drug use, time, and resources invested in searching for alternative diagnoses and could potentially improve patient outcomes. Therefore, greater public and provider education about blastomycosis is needed, especially in areas where blastomycosis is less commonly recognized.\n\n【22】The high proportion of patients with positive confirmatory laboratory tests, such as culture and microscopy, likely reflects detection of more severe cases because serologic tests for blastomycosis offer only presumptive evidence of infection , and serologic tests were not included in most states’ case definitions . The associations between older age and confirmatory test types with hospitalization point to severe illness, and are unsurprising; however, why women were more likely to be hospitalized is unclear but could be related to delayed diagnosis or underdiagnosis of less severe disease in women. More blastomycosis hospitalizations typically occur among men , although a recent study found female sex was independently associated with death in blastomycosis patients with acute respiratory distress syndrome . The increased risk for hospitalization among persons of non-white races lends further evidence to the existence of blastomycosis-related health disparities, as previously suspected . Further studies could help determine whether these differences are related to genetic predisposition , involvement in outdoor activities resulting in exposures to _Blastomyces_ , or access to medical care .\n\n【23】Reliance on often invasive and time-consuming tests such as culture and microscopy for diagnosis likely is a key factor in underdiagnosis of blastomycosis because these tests might not be ordered until tests for other diseases have been negative. Accordingly, most specimen types in our analysis were from bronchoalveolar lavage and lung and other tissue, which likely required biopsy. Given the prolonged time to diagnosis we identified, improved noninvasive diagnostic methods with high sensitivity and specificity for blastomycosis are needed for earlier and more frequent testing, which could prevent hospitalizations and deaths.\n\n【24】Consistent with previous reports, Wisconsin had the highest number of cases and incidence of the 5 states where blastomycosis is reportable, with mean annual incidence in several northern counties >20 cases/100,000 population. Peaks in incidence in Wisconsin corresponded to a known outbreak at a yard waste site in 2006 , an outbreak likely associated with multiple sources in 2010 , and an outbreak linked to recreational tubing on the Little Wolf River in 2015 . For case-patients in these outbreaks, younger age and higher likelihood of being non-White was consistent with our findings . In addition, the finding that patients with outbreak-associated cases had less severe outcomes could reflect detection of milder cases through enhanced case detection efforts during outbreak investigations. However, outbreaks comprised <6% of cases overall, suggesting that most cases occur sporadically, which also is true for histoplasmosis and coccidioidomycosis. Of note, most of northern Wisconsin is rich in soils classified as spodosols, which are characterized by high concentrations of organic matter in coarse, often sandy, particles . _Blastomyces_ spp. are thought to dwell primarily in organic-rich soils. However, spodosols also occur widely in northern Michigan, where disease incidence was not elevated, and are less common in northern Minnesota, where incidence was higher. Further study, including the role of soil types, could elucidate the natural habitat of these fungi.\n\n【25】For most states in this analysis, the relatively stable incidence and hospitalization rates over time were consistent with a previous analysis of blastomycosis-related hospitalizations during 2000–2011 . Another study found a decline in blastomycosis-associated deaths nationwide during 1990–2010 ; the reasons for the increase in deaths we observed during 2007–2017 are unclear but could reflect improvements in case follow-up, a decline in reporting of less severe cases, or other surveillance changes over time.\n\n【26】The limitations of our study include that pooling surveillance data based on different blastomycosis case definitions is fundamentally problematic; however, few other data sources would enable analyses of thousands of cases, which is helpful for studying this uncommon disease. Furthermore, some states’ case definitions changed over time. Although blastomycosis was reportable in each state during the years included in this analysis, Arkansas did not have a formal case definition, and Michigan did not have one until 2012. Wisconsin classified all cases as confirmed until September 2015, when their case definition changed to include confirmed and probable case classifications; for outbreaks in Wisconsin, a positive serologic blastomycosis test plus an epidemiologic link was sufficient to be considered a case. Moving forward, the standardized blastomycosis case definition from the Council of State and Territorial Epidemiologists will enable more robust comparisons between states and stratification of confirmed and probable cases.\n\n【27】Combining data from different times in each state is an additional potential limitation. Some states’ surveillance systems underwent changes during the analysis period; for example, data elements were added or removed, resulting in inconsistent denominators in the pooled analysis. For certain variables, such as race and ethnicity, missing data or values of “unknown” were common and demonstrate that information can be challenging to obtain because substantial time and resources often are needed to conduct case investigations . Data about environmental exposures, immunocompromised status, body site of infection, occupation, illness duration, and treatment were not available consistently from every state. Wisconsin and Minnesota conducted extensive follow-up on cases , providing deeper insight into state-specific features of blastomycosis. Collecting these types of data in a standardized way in additional states could help identify high-risk populations and activities and help inform prevention efforts.\n\n【28】In summary, blastomycosis remains a rarely reported but severe disease in most areas where it is under public health surveillance. Our findings indicate that blastomycosis likely is underdetected. Blastomycosis also can occur in areas outside those where it is commonly recognized  and might be emerging in new areas, such as east-central New York . Surveillance for blastomycosis in more areas and collection of more standardized, detailed data could help identify emerging geographic hotspots or clusters, new risk factors, and other epidemiologic patterns. Increased awareness among healthcare providers and the public could lead to faster diagnosis and treatment for blastomycosis patients.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "090771f1-ca8d-4cad-9f13-8eb8c823e496", "title": "Superspreading Event of SARS-CoV-2 Infection at a Bar, Ho Chi Minh City, Vietnam", "text": "【0】Superspreading Event of SARS-CoV-2 Infection at a Bar, Ho Chi Minh City, Vietnam\nSuperspreading events occur when a few persons infect a larger number of secondary persons with whom they have contact . For severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), an R 0  of 2–3 with 6–8 secondary cases has been suggested to constitute a superspreading event .\n\n【1】Although SARS-CoV-2 is known to be transmitted through droplets and fomites, there has been growing evidence of airborne transmission . Better understanding of specific settings in which superspreading events are facilitated remains critical to inform the development and implementation of control measures to avoid future waves of the pandemic .\n\n【2】On March 18, 2020, a 43-year old man, patient 1, sought treatment at the Hospital for Tropical Diseases in Ho Chi Minh City, Vietnam, for fever, cough, muscle aches, fatigue, and headache. A sample from a nasopharyngeal throat swab specimen taken at admission tested positive for SARS-CoV-2 by reverse transcription PCR.\n\n【3】During the 14 days before the onset of his symptoms on March 17, he had traveled to Thailand and within Vietnam, between Hanoi and Ho Chi Minh City. From 10:00 PM on March 14 until 2:30 AM of the next day, he participated in a St. Patrick’s Day celebration at bar X in Ho Chi Minh City. The bar had 2 indoor areas for clients, an »300-m 2  area downstairs and an »50-m 2  area upstairs, with no mechanical ventilation. During open hours, the left and right entrances were typically kept closed to facilitate cooling with air conditioners that recycle indoor air; the middle entrance was kept open. The bar also has naturally ventilated outdoor spaces . Patient 1 was inside the bar during the party.\n\n【4】After the confirmed diagnosis of COVID-19 in patient 1, we used contact tracing and testing to detect 18 additional PCR-confirmed cases. Of these, 12 (patients 2–13) were at bar X during the evening of March 14; the other 6 (patients 14–19) were contacts . Of the patients with confirmed cases attending the celebration, 4 were in close contact with patient 1: patients 2–4 went to the celebration with patient 1 and patient 6 worked as a waiter in the bar. Patients 2 and 3, who were roommates, had traveled to Malaysia and returned to Vietnam, patient 2 on March 13 and patient 3 on March 6. The other patients, except for patient 1, had no recent history of travel outside of HCMC .\n\n【5】By exploring the epidemiologic links discovered from in-depth interviews, we identified 3 possible transmission chains involving patients who attended the March 14 celebration . Of these, 2 or 3 patients (patients 5, 10, and possibly 14) were asymptomatic but transmitted SARS-CoV-2 to their contacts . None of the 19 patients with confirmed cases reported that they had respiratory signs or symptoms on March 14–15. However, in addition to patient 1, a total of 5 others developed mild respiratory symptoms (patient 4 on March 16, patient 6 on March 21, patient 9 on March 25, patient 13 on March 26, and patient 17 on March 27), suggesting an incubation period of 2–12 days. Follow-up data were available for 12 patients who participated in our clinical study . Six remained asymptomatic during follow-up .\n\n【6】A total of 11 whole-genome sequences of SARS-CoV-2 were obtained from the patients in the cluster. The obtained sequences were either 100% identical or different from each other by only 1–2 nt . Phylogenetically, they clustered together tightly but were different from sequences obtained from other cases in Ho Chi Minh City during the same period.\n\n【7】As of September 15, 2020, only 30 cases of locally acquired infection had been reported in Ho Chi Minh City , but this cluster represents the only documented superspreading event . Together with data from previous reports , these data suggest that closed settings are facilitators of community transmission of SARS-CoV-2. The mechanism by which infected people without symptoms spread SARS-CoV-2 to others, especially in closed settings, warrants further research, including on transmission through aerosols, which has been suggested .\n\n【8】The high level of genome sequence similarity between the SARS-CoV-2 genomes obtained from the patients and the tight clustering on the phylogenetic tree strengthen the epidemiologic link between the PCR-confirmed cases from this cluster. Together with contact history, these data also support transmission chains involving asymptomatic carriers (patients 5 and 14) as the sources of the ongoing infection. However, the identity of the patient in the index case from the bar could not be confirmed, in part because in-depth interview data were available from only 8 of 13 patients with confirmed cases who consented to participate in the study. In conclusion, our results emphasize that persons in crowded indoor settings with poor ventilation may be considered to be at high risk for SARS-CoV-2 transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5aaac90d-481c-4781-b6a9-19868e8cb4b8", "title": "Exploring Perceptions of Colorectal Cancer and Fecal Immunochemical Testing Among African Americans in a North Carolina Community", "text": "【0】Exploring Perceptions of Colorectal Cancer and Fecal Immunochemical Testing Among African Americans in a North Carolina Community\nAbstract\n\n【1】**Introduction**African Americans have a lower colorectal cancer screening rate than whites and higher disease incidence and mortality. Despite wide acceptance of colonoscopy for accurate screening, increasing promotion of high-sensitivity stool test screening, such as the fecal immunochemical test (FIT), may narrow racial, ethnic, and socioeconomic disparities in screening. This study provides formative research data to develop an intervention to increase colorectal cancer screening among underinsured and uninsured African Americans in central North Carolina. **Methods**We held 4 focus groups to explore knowledge, beliefs, and attitudes about colorectal cancer screening, particularly FIT. Participants (n = 28) were African American adults recruited from neighborhoods with high levels of poverty and unemployment. Constructs from the diffusion of innovation theory were used to develop the discussion guide. **Results**In all groups, participants noted that lack of knowledge about colorectal cancer contributes to low screening use. Attitudes about FIT sorted into 4 categories of “innovation characteristics”: _relative advantage_ of FIT compared with no screening and with other screening tests; _compatibility_ with personal beliefs and values; test _complexity_ ; and test _trialability._ A perceived barrier to FIT and other stool tests was risk of incurring costs for diagnostic follow-up. **Conclusion**Community-based FIT screening interventions should include provider recommendation, patient education to correctly perform FIT, modified FIT design to address negative attitudes about stool tests, and assurance of affordable follow-up for positive FIT results.  \n\n【2】Introduction\n\n【3】Screening for colorectal cancer (CRC), the second leading cause of cancer death in the United States, leads to increased early detection and treatment of this disease . Although 62% of the US population reports following recommended CRC screening guidelines, screening rates are lower for those with lower incomes and those without health insurance . Rates are also thought to be lower among African Americans, who are more likely than other ethnic groups to be diagnosed with late-stage CRC  and 40% more likely than whites to die of CRC . Adults at average risk for CRC may choose from several screening options. Until recently, organizations that develop and issue guidelines were generally in consensus about which tests to endorse. The US Preventive Services Task Force (USPSTF) recommends that patients choose from among yearly high-sensitivity fecal occult blood test (FOBT) or high-sensitivity fecal immunochemical test (FIT), flexible sigmoidoscopy every 5 years, or colonoscopy every 10 years. Each screening regimen is clinically effective for reducing mortality . In 2008, the American Cancer Society (ACS), in collaboration with the US Multi-Society Task Force on Colorectal Cancer and the American College of Radiology, issued a slightly diverging set of recommendations, adding stool DNA and computed tomographic colonoscopy tests. The ACS guidelines also categorize tests according to their potential to detect versus prevent CRC. The first category includes FOBT and FIT. The second category includes screening tests that produce visual images of the colon and, therefore, can detect and guide removal of adenomatous polyps, a benign precursor of most colorectal cancers, thereby preventing them from developing into cancer . With either set of guidelines, patients must choose a test that most closely aligns with their needs and values. Each test conveys distinct benefits and limitations related to test frequency, cost, invasiveness, sensitivity, specificity, convenience, and regional availability. For lay and professional audiences alike, determining the relative advantage of the various screening tests is a complex issue. FOBT and FIT are the least expensive options and do not require access to endoscopy facilities. Some types of FIT are increasingly preferred over FOBT because the tests are specific to human hemoglobin and have a similarly high or higher sensitivity . Vitamins, foods, and drugs do not alter FIT accuracy, and patients may find it easier to use. Improving access to FIT has potential to increase screening by reducing costs and removing some structural barriers, such as geographically distant endoscopy facilities . The objective of this study was to provide formative research data for an intervention to increase CRC screening in a target population of underinsured and uninsured African Americans living in a metropolitan area in central North Carolina. We conducted focus groups with members of the target population to explore knowledge, beliefs, and attitudes about CRC screening, particularly FIT. Focus group data are being used to inform the design of a FIT screening intervention.  \n\n【4】Methods\n\n【5】### Theoretical framework: diffusion of innovations\n\n【6】The diffusion of innovations theory describes the adoption of new practices or products (innovations) and the factors that accelerate or impede their spread throughout a community. Application of this theory during intervention planning can help cancer prevention and control practitioners develop dissemination strategies specific to different CRC screening tests and populations. The theory posits that perceptions of an innovation’s characteristics affect how quickly and widely the innovation is adopted. Five attributes that explain 49% to 87% of variance in adoption rates are _relative advantage_ , _compatibility_ , _complexity_ , _trialability_ , and _observability_ . Relative advantage is the degree to which a potential user perceives the innovation as superior to the practice that it supersedes. Compatibility refers to the beliefs about whether the innovation is consistent with personal values. Complexity is the extent to which the user perceives the innovation as difficult to use. Trialability is the degree to which someone can experiment with the innovation before adopting it. Potential users can also conduct a vicarious trial by observing and learning from someone else’s experimentation . Observability is the extent to which results of adopting an innovation are visible to others. Modifying FIT in ways that affect perceptions of these 5 attributes in the target population can enhance or diminish its diffusion potential. Intervention planning also requires audience and community assessment research to understand how an innovation is likely to interact with individual and environmental characteristics. Elements in the diffusion of innovation theory guided the organization and presentation of the focus group results. \n\n【7】### Procedures\n\n【8】Starting in January 2007, a local community research advisory board, a standing group that advises about research projects in several North Carolina counties, reviewed the research protocol, recommended appropriate honoraria for participants, and guided the research team in disseminating results locally. The institutional review board of the University of North Carolina at Chapel Hill also approved this study. Community organizations assisted in early spring 2007 by posting flyers and allowing study staff to attend their outreach events to recruit and enroll participants. Potential participants contacted research staff in person at these events or by telephone after seeing recruitment materials. Two study staff administered brief eligibility surveys and enrolled participants. Eligible participants were African Americans aged 50 years or older who were not at elevated risk for CRC because of a family history of CRC in a first-degree relative or a personal history of the disease. Those screened by FOBT or FIT within the past year or by any endoscopy method or contrast barium enema within the past 5 years were excluded. Of 51 people who completed eligibility screening, 16 were ineligible because of age or recent screening. Thirty-five eligible people received assignment to a male or female focus group session (there were 2 of each), a confirmation letter, and a reminder letter and telephone call. Of those, 28 attended 1 of 4 two-hour focus groups and completed a brief demographic questionnaire in March 2007. Focus groups of 5 to 9 people were conducted at 2 African American churches of different denominations and at a community resource center. The churches and the resource center were recommended as neutral sites that were likely to be familiar and geographically accessible to participants. Eligible participants chose to attend either a weekend morning or weekday evening focus group. The study covered taxi costs for participants without transportation. All participants received a $30 gift certificate. Focus group members gave written consent for their participation. A trained African American facilitator of the same sex as participants moderated the focus groups. All groups were tape recorded. Facilitators followed a semistructured guide with preset probes  to ensure conversation depth. The moderator began with questions about participants’ knowledge and attitudes about CRC and screening. After distributing a 3-sample Hemoccult ICT packet, an FIT manufactured and donated by Beckman Coulter, Inc (Brea, CA), the moderator asked participants to examine the packet and share opinions about its design, packaging, instructions, and usability. The moderator also asked participants questions about community characteristics and local health services, and participants were able to ask questions or raise topics that they thought were important but had not been addressed. \n\n【9】### Analysis\n\n【10】Verbatim transcription of audio recordings produced 191 pages of text. The first author used Atlas.ti software (Atlas.ti Scientific Software Development GmbH, Berlin, Germany) to conduct a content analysis followed by thematic analysis. Content analysis examined the degree of consensus in responses to questions and generated a list of codes that defined overarching themes. The analyst coded and ranked each comment from most to least frequently mentioned. A second analyst also reviewed transcripts, and differences in the analyses were resolved. Thematic analysis entailed grouping codes by themes, which were defined by elements of diffusion of innovation theory. Qualifying findings had to emerge in at least 2 groups. A finding’s strength increased if it occurred in 3 or all 4 groups. To count as a finding for women, a code had to occur in both women’s groups, and likewise for the men. During thematic analysis, the analyst abstracted quotes that illustrated findings.  \n\n【11】Results\n\n【12】Half of study participants reported an annual income of less than $10,000 . Twenty participants reported having a regular health care provider, yet only 13 had ever spoken with a provider about CRC. More women than men reported talking with a health care provider about CRC. Themes are presented in terms of individual, innovation, and environmental characteristics that, following diffusion of innovation theory, are likely to influence FIT adoption. Comments about FIT aligned with 4 of the 5 innovation characteristics believed to predict adoption: _relative advantage_ of FIT compared to no screening or other CRC screening, _compatibility_ of FIT with personal beliefs and preferences, _complexity_ of the FIT procedures, and strategies to enhance FIT’s _trialability_ . Participant comments did not directly address the innovation’s _observability_ ; however, the low profile of CRC screening emerged as a related theme. \n\n【13】### Individual characteristics: awareness and knowledge of CRC and CRC screening\n\n【14】Across all groups, awareness and knowledge of CRC and screening were low. Female participants said that CRC screening is discussed less frequently than breast or cervical cancer screening. \n\n【15】> You know, you do tell people “I went for a mammogram” because one of the things that women do discuss when they go for a mammogram is . how the test felt, you know, what was done. We talk about Pap\\[anicolaou\\] smears. But I just never, ever heard anybody say anything when they go for their physical about \\[CRC screening\\]. (Woman, group C) \n\n【16】### Innovation characteristics: perceptions about FIT screening\n\n【17】**Relative advantage** Most participants noted that finding cancer early is beneficial relative to late diagnoses, and most indicated that FIT screening was preferable to other CRC screening tests. Several men said that they liked the idea of a home test for CRC.\n\n【18】> There was a time I was, uh, the doctors wanted to take that \\[colonoscopy\\] for me and I wouldn’t allow them because that’s a part of my body. I just, just can’t see nobody doing what they do. So, I told them “Ain’t there no other way you can test it?” (Man, group A) \n\n【19】When differences between FOBT and FIT were discussed, FIT was preferred because it requires no food restrictions. **Compatibility** Negative attitudes about FIT were due mostly to perceptions of the test as “gross.” Women expressed more reluctance about collecting and storing stool samples than men and noted that people may be deterred by smelly odors or embarrassment when returning the samples. All groups discussed problems associated with storing stool samples. Some recommended adding a device to hold used sample cards until they go to the lab.\n\n【20】> No one thinks to let it dry completely overnight. So, I guess you have to put this someplace where — I don’t know — You know, because you got to let it dry completely, and you don't want to just leave it on the sink. (Man, group D) \n\n【21】Participants also said stool tests are a good screening option for those preferring home remedies to medical services. Participants generally approved of the appearance of FIT packaging; however, they thought that the thin, lightweight materials could easily be overlooked or discarded. \n\n【22】> If I don’t really have an understanding of how important it is, I’ll just discard it, you know, because it looks like another piece of mail. So, it just goes in the garbage can with all my other mail. (Man, group D) \n\n【23】**Complexity**Although most said FIT is a simple procedure, in every group participants said FIT’s multistep instructions would challenge some. Instructions in small type were acknowledged as a potential problem for low literacy patients. Not understanding the rationale for each step of the FIT procedure, which is performed over several days, bothered some participants: “Okay, you’re gonna put 2 \\[pieces of stool\\] on it, 1 for the bottom, 1 for the top. Then you’re going to smear \\[them\\] together. For what?” (woman, group B). Another participant said, “All the processes . Lift the toilet seat. Attach it to fit you. Measuring tissue. You know, it’s a lot to do. A lot to do for me” (man, group A).Others shared similar concerns. \n\n【24】> I think before they give a person a kit like that, they just educate them on why theyre doing it and what they should do, what theyre going to be looking for. Cause if they just give it and tell them to take it home, bring their stool sample back, I mean they havent told them nothing. (Woman, group B) \n\n【25】**Trialability** A few had previously tried and completed a 3-sample FOBT. One man explained that “there’s nothing to it” (group D). Another participant described how a female health care provider helped him practice a test procedure before he attempted it independently.\n\n【26】> They went through the whole thing with me. Cause I didn’t know exactly how to do it. Cause I can read and I can guess. She said, “I’m gonna sit here and we’ll go through the whole process.” Then you know exactly what to do. (Man, group D) \n\n【27】In addition to hands-on instruction, video or illustrations were suggested by the participants to improve adherence to test procedures. **Observability** The silence surrounding CRC was discussed in all groups. One woman (group C) compared the invisibility of CRC screening with other screenings: “We hear a lot about mammograms . we hear a lot about cervical cancer . and the importance of Pap\\[anicolaou\\] smears. But we don’t really hear a lot about colon cancer.” Although benefits of early detection or prevention of cancer are not easily observed, negative consequences of late-stage cancer diagnosis are, prompting such remarks as “a lot of people \\[are\\] afraid of taking the necessary steps as far as being examined. I guess they’re scared of what they will find out” (woman, group C).\n\n【28】### Environmental characteristics with potential to affect FIT screening\n\n【29】**Provider recommendation** Lack of provider recommendation appeared as a screening impediment: “If the doctor tells me this \\[FIT\\] is something I must do, or I have to do it, I just have to humble myself and get it done. But until that time, man, I’ll just take my personal beliefs and use them” (man, group A). Participants described physicians as important health information sources, yet rarely discussed CRC with a doctor.**Health care cost and access** Even if free CRC screening were available, participants said they might opt out unless affordable follow-up is assured. In the absence of diagnostic care and treatment, screening may be pointless: “The cost factor. If I do \\[have cancer\\], I can’t afford to continue with the treatment or whatever is needed to be done. Therefore, if I don’t start and I don't know, I won’t have to follow through” (man, group A). \n\n【30】Discussion\n\n【31】The National Commission on Prevention Priorities ranked CRC screening as the fourth most valuable clinical preventive service that medical practices can offer . Although some tests, particularly colonoscopy, have ardent supporters, most people agree that USPSTF- or ACS-recommended tests increase early detection of CRC  and that patients should be able to choose a test they prefer. Results from published studies indicate that patients do not unanimously favor one test over another; some studies indicate FOBT and FIT are least preferred . Others show some patients preferring stool tests over colonoscopy . Findings from our study suggest that FIT is perceived as an acceptable or preferred CRC screening test relative to other screening tests, including colonoscopy. Fisher et al  report that using annual stool tests for primary screening would allow 100% of the age-appropriate US population to be screened at a savings of nearly $10 billion per decade from what is currently spent to screen only half the targeted population. For cancer prevention and control planners working to extend CRC screening to underinsured and uninsured patients, our findings suggest FIT is viable for community screening. Participant comments exposed factors that could impede widespread adoption and should be taken into consideration when planning screening programs that include FIT. Complexity of test procedures emerged as a concern. For innovations requiring acquisition of new skills, diffusion tends to be slow . FIT screening entails following multistep instructions over a span of several days. Other studies have found that people often lack skills and confidence to successfully complete stool tests . Another focus group study reported that clearer instructions about test procedures would improve participation rates . Similarly, participants in our study recommended adding instructions in large type and illustrations of test procedures. Participants also recommended hands-on practice sessions using sample materials or video demonstrations. In addition to reducing complexity, these activities address the concept of trialability by allowing patients to try FIT before committing to using it at home. Three-sample FIT and FOBT are already designed with a certain degree of trialability: patients can gain confidence in doing the test as they attempt to collect a sample each day. Although test accuracy decreases with an incomplete sampling, 1 or 2 samples can still be analyzed. The next year, the patient will have another opportunity to perform the test. Our focus group participants indicated that handling or mailing stool samples is embarrassing and mildly offensive, hence incompatible activities. Similar attitudes have been reported about FOBT . In an Australian study of FOBT use, the 2 main reasons, together accounting for almost 50% of reasons for nonadherence, were perceived unpleasantness and inconvenience . Participants in our study noted that merely adding a storage device to securely contain fecal samples until they are returned to the doctor may make FIT more acceptable. In addition to the innovation characteristics of FIT screening, the public’s low awareness of stool testing may impede adoption . The effectiveness of mass media interventions for increasing CRC screening deserves further research ; however, the most important source of information about CRC screening is health care professionals . Although physician recommendation for screening is a leading predictor of screening adherence , only 13 of 28 participants in this study had talked with a health care provider about CRC. Physicians have reported not recommending CRC screening to uninsured patients if access to diagnostic care is lacking . Participants noted that FIT screening has little value unless diagnostic follow-up services are available and affordable. In regions where CRC screening programs do not fund screening and diagnostic colonoscopy for the underinsured and uninsured, adhering to CRC screening guidelines is a challenge. Focus group data, while offering great depth and detail in response to research questions, cannot be generalized beyond the sample. In our study, convenience sampling and the small sample size further decreased generalizability of the results. Also, only 1 researcher coded the content analysis, potentially decreasing the validity of our findings. CRC disproportionately affects the lives of African Americans, and screening rates must be increased to reduce the number of African American lives lost to the disease. Findings from our study and others indicate that FIT is a viable option for more widespread population-based CRC screening.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9b384aa4-ffe8-432a-a728-581b5267c657", "title": "Risk Factors for Nipah Virus Infection among Pteropid Bats, Peninsular Malaysia", "text": "【0】Risk Factors for Nipah Virus Infection among Pteropid Bats, Peninsular Malaysia\nNipah virus (NiV) disease emerged in Peninsular Malaysia during September 1998–April 1999 and resulted in 105 human deaths (≈40%) and compulsory culling of 1.1 million pigs . NiV disease has been successfully controlled in Malaysia. However, elsewhere in Bangladesh and India, 12 NiV outbreaks have since occurred . Pteropid bats (family Pteropodidae, genus _Pteropus_ ) were the most likely reservoir host of the virus, and evidence of NiV in these bats had been consistently found in populations across southern Asia  and Africa  despite an absence of outbreaks in some areas.\n\n【1】Characteristics of bats that promote their competency as a natural host and reservoir for many emerging pathogens from evolutionary, ecologic, sociobehavioral, and immunologic perspectives are progressively being reported . In Malaysia, previous surveillance work suggest 2 pteropid species, _Pteropus hypomelanus_ (variable flying fox) and _P_ . _vampyrus_ (large flying fox) bats, as reservoir hosts for NiV . _P. hypomelanus_ bats reside on offshore islands along the eastern (n = 14) and western (n = 4) coasts of the peninsula . NiV was first isolated from pooled urine samples from these bats on the island of Pulau Tioman off the eastern coast of the state of Pahang . _P. vampyrus_ bats, the pteropid species identified during the first NiV disease outbreak location in 1998 , are anthropogenic-susceptible bats residing in remote and inaccessible areas such as mangroves and dense forests . _P. vampyrus_ bats roost mainly on the mainland but may have focal transitory points on surrounding islands as they travel .\n\n【2】Our recent follow-up work on a cohort of _P. vampyrus_ bats  showed a possible NiV recrudescent event leading to horizontal viral transmission to other bats in the colony. The findings further elucidated maintenance and transmission dynamics of the virus within and among roosts, colonies, and the bat metapopulation. In this report, we present results of 2 studies conducted concurrently to determine the geographic extent and prevalence of NiV-neutralizing antibody for _P. vampyrus_ and _P. hypomelanus_ bats, and to identify the sexual and reproductive maturity determinants for NiV seropositivity in the wild.\n\n【3】### Materials and Methods\n\n【4】##### Study Design and Study Populations\n\n【5】##### Cross-sectional Study of NiV (Distribution Study)\n\n【6】During March 2004–May 2007, we sampled _P. hypomelanus_ bat colonies from several roost sites in Pulau Kapas and Pulau Perhentian on the northeast coast of Terengganu, Pulau Tioman on the southeast coast of Pahang, and Pulau Pangkor on the northwest coast of Perak. We also sampled _P. vampyrus_ bats in the states of Perlis; Perak (Teluk, Memali, and Lenggong); Terengganu (Kampung Alor Lek, Setiu, and Kuala Berang); Pahang (Tanjung Agas, Pasir Panjang, and Ganchong); and Johor (Benut and Kesang) . We determined these sites using published information on pteropid roost locations , reports from the Malaysia Department of Parks and Natural Resources authority (PERHILITAN), information from local hunters and residents, and field observations. A complete list of observed roost sites was published by Epstein et. al. Colonies were selected for this study on the basis of observed presence of bats, accessibility for bat capture, and roost size (e.g. number of bats). A sample size of 35 bats was targeted at each location or for each sampling effort to be able to detect a NiV-seropositive bat given a minimum prevalence of 10% with 10% precision at a confidence level of 95% .\n\n【7】Permission for the study was granted by PERHILITAN, and Institutional Animal Care and Use Committee approval was obtained from the Wildlife Trust Institutional Animal Care and Use Committee (New York, NY, USA). A special permit to trap and humanely kill the bats was obtained from PERHILITAN. Bats were captured nonrandomly by using 2 methods: opportunistic sampling of hunted bats (hunters were not solicited or incentivized to hunt bats for this study) and mist nets.\n\n【8】Hunted bats were sampled at the site of hunting, and bats were attributed to the location of the nearest known roost. Blood samples were collected by cardiac puncture, and the kidneys were harvested. The blood, urine, and kidney samples were processed as described . Bats captured in mist nets were extracted from the nets immediately after capture and anesthetized before sampling by using medetomidine, a combination of medetomidine/ketamine , or isoflurane gas. Blood (3 mL) was collected from the cephalic vein or brachial vein and placed into serum-separator tubes (Vacutainer; Becton Dickinson, Franklin Lakes, NJ, USA). Sterile cotton swabs were used to collect oropharyngeal and urogenital samples. All samples were stored in liquid nitrogen at −190°C) and transported to the Veterinary Research Institute in Ipoh, Perak, Malaysia.\n\n【9】For each bat hunted or captured, information on the date of sampling, species, location, sex, reproductive status, and estimated age was recorded. We assigned each bat an age category of adult (secondary sexual characteristics visible/reproductive), juvenile (no observable secondary sexual characteristics, not pregnant, dental wear characteristics), or pup (dependent and attached to its dam) . We categorized the reproductive status as pregnant, carrying a pup (adult females with attached pup), nursing (adult females without attached pup but with evidence of lactation), or dry (not in any of the previous categories) using published criteria . The total number of bats sampled for each pteropid species is shown in Tables 1 and 2 . Bats captured in mist nets were released at the site of capture after recovery from anesthesia.\n\n【10】We performed descriptive analysis to describe the seroprevalence on the basis of species of bats and locations. Differences among positive antibody titers (range <8–1,025) between species were tested by using the Mann-Whitney U test. Comparison of the seroprevalence rates between the 2 pteropid species was performed by using the χ 2  test. When there were sufficient data to perform the analysis, several factors were examined for its association to NiV seropositivity by using the χ 2  test.\n\n【11】##### Longitudinal Study on Risk Factors for NiV (Risk Factor Study)\n\n【12】We performed a longitudinal study on a population of _P. hypomelanus_ bats in Pulau Tioman in which 50 bats from the same colony were captured by using mist nets and sampled approximately every 6–8 weeks during January 2004–October 2006. Monsoonal rains prevented access to the island of Pulau Tioman during December–February, which extended the interval between some sampling points. Captured bats were tagged by using a thumb band or implantable microchip (Avid Identification Systems, Inc. Norco, CA, USA) and unique identification numbers to ensure that sampling was not repeated on the same bat. The bats were anesthetized by using ketamine and xylazine . Data, blood, and swab samples were collected as described in the previous section.\n\n【13】We investigated associations between serostatus (positive or negative) and each of the hypothesized risk factors (sex, age, reproductive categories, time) using the χ 2  test or Fisher exact test when the χ 2  test was not appropriate. Our previous study  suggested a correlation between the serologic status of pups and their dams. Therefore, serologic data for _P. hypomelanus_ pups were excluded from further analysis. Logistic regression with generalized estimating equations was used to analyze longitudinal data to control for the effect of clustering, assuming that bats sampled between various times were from the same population. To compare the categories across sexual and reproductive maturity, we reclassified the data on the basis of sex (male and female) and reproductive maturity (juvenile, adult, dry, pregnant, carrying a pup, and nursing) and renamed the variable the sexual and reproductive maturity factor. The logistic regression analysis includes the sexual and reproductive maturity categories and sampling time for the bats. All hypothesis testing was 2-sided, with α = 0.05, and was performed by using SPSS version 19 (SPSS Inc. Chicago, IL, USA).\n\n【14】##### Laboratory Analysis\n\n【15】Plasma-neutralizing antibodies against NiV were measured by using the serum neutralization test (SNT)  with plasma diluted 1:2–1:1,024. A titer ≥8 was considered a positive titer for specific antibody against NiV because serum samples were usually toxic to Vero cells at higher concentrations (i.e. 1:2 or 1:4). Virus isolation was attempted by using rabbit kidney and Vero cells in a biosafety level 3 facility until the third passage before a sample was considered negative . According to the Malaysian Government Act on Control and Prevention of Infectious Diseases 1988 (revised May 25, 2006), NiV is categorized in risk group 3, which enables the virus to be handled in a biosafety level 3 facility. Any tissue culture with an NiV-like cytopathic effect was confirmed by using a PCR as described . All laboratory diagnostics (SNT, virus isolation, and PCR) were conducted at the Veterinary Research Institute in Ipoh.\n\n【16】### Results\n\n【17】##### Distribution of NiV\n\n【18】##### _P. vampyrus_ Bats\n\n【19】NiV-neutralizing antibodies were detected in bats at all locations . Overall, 82 (32.8%) of 253 bats were seropositive. We found no significant difference in seroprevalence rates in bats from the 5 states (p = 0.213, by Fisher exact test). NiV-neutralizing antibody titers ranged from <8 to 1,024 (median <8), and among seropositive bats, the median titer was 64. All culture and PCR results were negative. Among pups, juvenile, and adult bats, 0% (0/1), 25% (14/56), and 35.2% (68/193), respectively, were seropositive . Age, sex, and female reproductive status were examined for their effects on serostatus of _P. vampyrus_ bats. Univariate analysis showed that age and sex were not associated with seroprevalence of NiV. However, among nursing bats, a higher risk for NiV seropositivity was observed than in other adult females .\n\n【20】##### _P. hypomelanus_ Bats\n\n【21】Of 119 plasma samples collected from island sites, 2 were not used because of inadequate amounts of plasma. Of the remaining 117, 13 (11.1%) were seropositive. NiV antibodies were detected in 13 (13.8%) of 94 bats from islands off the east coast (Pulau Perhentian, Pulau Kapas, Pulau Tioman) of Peninsular Malaysia. Titers ranged from <8 to 256 (median <8); among seropositive bats, the median titer was 32. Samples from Pulau Pangkor (n = 24) were negative for NiV-neutralizing antibodies. All culture and PCR results were negative. Pups were not captured, but among juvenile and adult bats, 0% (0/31) and 11.1% (13/73), respectively, were seropositive . Univariate analysis showed that sex was not associated with NiV seropositivity. Stratified data for various reproductive categories were sparse and lacked power for meaningful analysis .\n\n【22】##### Comparison of _P. vampyrus_ and _P. hypomelanus_ Bats\n\n【23】The antibody titer difference between _P._ _vampyrus_ and _P. hypomelanus_ bats was significant (p<0.001, by Mann-Whitney U test. The difference in seroprevalence for NiV between _P. hypomelanus_ (13/117, 11.1%) and _P. vampyrus_ (82/253, 32.8%) bats was significant (χ 2  19.54, p<0.001), and risk for a seropositive reaction to NiV was 3.9× higher for _P. vampyrus_ bats than for _P. hypomelanus_ bats.\n\n【24】##### Longitudinal Study of NiV _P. hypomelanus_ Bats\n\n【25】Characteristics of bats sampled are shown in Table 4 . All P. _hypomelanus_ bat samples had negative culture and PCR results. The overall NiV seroprevalence for _P. hypomelanus_ bats from Pulau Tioman was 9.8%. Differences in seroprevalence between sampling times were significant (p<0.001). The highest seroprevalence rate was in 2004, which then waned in 2005 and 2006 .\n\n【26】The seroprevalence of NiV in adult bats (12.8%) was significantly higher than in juveniles (3.7%; p<0.001), and adults were ≈4× more likely to be seropositive (odds ratio 3.9). Seroprevalence was not significantly different between male (11.1%) and female bats (8%; p = 0.258). However, when data for female bats were examined by their reproductive categories, a significant difference (p<0.001) was observed. Bats that were pregnant, carrying a pup, and lactating were 3.9×, 4.8×, and 3.0× more likely to be seropositive than adult female bats that did not show these features .\n\n【27】Although our data were not sufficiently powered to detect significant differences among various sex and reproductive maturity categories (sample size was not large enough within each category level), when we controlled for the effect of time, the likelihood of seropositivity to NiV among pregnant, carrying, and nursing females remained higher than other female bats, and the highest risk was observed among those females carrying pups. Juvenile female bats were the least likely to have detectable NiV antibodies compared with other bats. Most seropositive cases appeared to cluster in 2004 and decreased toward the end of the study .\n\n【28】Anthropic disturbance to roosting sites during our study resulted in dispersal of _P. hypomelanus_ bats to smaller ( < 20), more disparate cohorts that roosted higher in the trees and finally relocation to another site on Pulau Tioman, ≈6 km from the previous site. We believe roost size affected seroprevalence rates and thus reduced risks for seropositivity, as observed toward the end of the study.\n\n【29】### Discussion\n\n【30】All bats captured during the study, including NiV-seropositive bats, appeared healthy, which was consistent with observations from experimental infections of pteropid bats with NiV . Our cross-sectional survey found that seroprevalences of NiV in _P. vampyrus_ and _P. hypomelanus_ bats were 32.8% and 11.1%, respectively, which differed from results of a study Yob et al. after the first NiV outbreak (17% vs. 31%, respectively). Although both studies support the hypothesis that _Pteropus_ spp. are the natural reservoir for NiV in Malaysia, we believe that this difference may be attributed to our study having a larger sample size for each species and a wider geographic sampling scale, as well as potential differences among the diagnostic assays used.\n\n【31】The presence of NiV-seropositive _P. vampyrus_ bats across Peninsular Malaysia was expected because satellite telemetry studies by Epstein et al._ and Breed et al. showed that _P. vampyrus_ bats are highly mobile, moving beyond state and national borders and making contact with other conspecifics across the region probable. These findings are consistent with the findings of studies of Hendra virus in Australia and NiV in Bangladesh, in which evidence for circulation has been observed across a broad expanse of the home range of each pteropid species . The higher seroprevalence of NiV among _P. vampyrus_ bats than among _P. hypomelanus_ bats suggests that exposure to NiV is more common among the _P. vampyrus_ bats, which is consistent with higher rates of NiV seroprevalence among juvenile _P. vampyrus_ bats than among juvenile _P. hypomelanus_ bats. These findings suggest that viral circulation and exposure are probably more common among _P._ _vampyrus_ bats.\n\n【32】We believe that the greater connectivity of _P. vampyus_ bats among colonies in the region create a metapopulation structure in which there is more opportunity for virus to circulate by migration, resulting in a higher rate of exposure. Island bats have limited connectivity . Thus, herd immunity tends to wane over time, creating a relatively lower seroprevalence . This phenomenon may be illustrated by our survey of _P. hypomelanus_ bats on Pulau Pangkor, in which we did not detect any seropositive bats. We sampled 24 bats, which would have enabled us to detect a seropositive bat, given a prevalence >10% with 95% confidence. The fact that we did not detect any seropositive bats suggests a lower seroprevalence, which could also be attributed to the relatively high degree of urbanization on Pulau Pangkor and consequently smaller colonies and a decreased seroprevalence.\n\n【33】The longitudinal study of _P. hypomelanus_ bats showed that prevalence of neutralizing antibodies to NiV fluctuated during the study (range 1%–20%) and, although unpredictable, seroprevalence generally waned in the final 2 years . We did not observe any consistent and discernible seasonal seroprevalence pattern. However, other studies have suggested periodic patterns of antibody prevalence that are connected to the reproductive cycle of bats for other viruses such as Hendra virus , filovirus, and lyssavirus . With the exception of the first NiV outbreak in Malaysia, other studies have linked NiV outbreaks to bat reproductive seasons . We believe that the waning seroprevalence during our study is partly explained by anthropic disturbances that occurred at the original study site in Pulau Tioman, which resulted in relocation and dispersal of the original colony into smaller roost sizes. In addition to waning immunity in bats, smaller roost size decreases viral transmission to other susceptible bats, resulting in decreasing seroprevalence over time. The lack of viral isolation from any sample collected during our studies may have been caused by a low incidence of viral shedding or low viral excretion doses within the colony, which is supported by studies of henipaviruses .\n\n【34】Sex was not a risk factor for NiV exposure, which is consistent with results of other studies . However, when we stratified the analysis on the basis of sexual and reproductive maturity of the bats, we found that female bats that were pregnant, had an attached pup, and were lactating had a consistently higher likelihood of exposure to NiV than adult males or dry adult females. Analysis of data from a cross-sectional survey of bats of both species suggests an increasing risk for exposure to NiV when female bats were pregnant or lactating. This finding was strengthened in the longitudinal study because female bats that were pregnant, had an attached pup, or were nursing had a higher risk for NiV when we controlled for the potential confounding effect of sampling time (or seasonality).\n\n【35】Among mammals, _Pteropus_ spp. bats are known to carry their pups for < 3 months  after birth and continue to nurse for < 4 months after the pup is independent. Pregnancy and lactation are the most metabolically demanding periods of mammalian life . Therefore, the cumulative stress from reproductive activities, followed by physical exertion from carrying an attached pup to lactation, may have led to the increased risk for NiV infection among this group of bats, consistent with the findings of Plowright et al. We speculate that NiV spillover events are most likely to occur during these periods.\n\n【36】Our study has several limitations, including sampling bias, which results from the nonrandom sampling technique used. Sample numbers were often suboptimal because of extreme difficulty in catching pteropid bats. In addition, there are major challenges associated with interpretation of serologic data in wildlife populations. Although we detected differences in the prevalence of neutralizing antibodies between species and increased risk among bats in different reproductive categories, there is still little known about the timing of actual infection or the duration of NiV antibodies in bats. One technique used to overcome this difficulty was to examine age-stratified serologic data, which has been used in similar epidemiologic studies . _P. hypomelanus_ bats reach sexual maturity at ≈12 months of age  and _P. vampyrus_ at 24 months of age , which enabled us to infer that NiV antibodies in weaned juveniles (≈6–24 months of age) may indicate recent viral circulation. Our previous study among captured _P. vampyrus_ bats demonstrated horizontal transmission of NiV from an adult to juvenile bats within the same colony after a brief shedding episode . Our finding of juvenile _P. hypomelanus_ bats with antibodies to NiV during the longitudinal study support the possibility that virus had been circulating in this population within the lifetime of the juvenile, assuming that the juvenile bats were old enough to have lost maternal antibodies.\n\n【37】Our finding of neutralizing antibodies to NiV in both species of _Pteropus_ bats at almost all locations we studied in Malaysia, coupled with isolation of NiV from these bats by our group and another group, strengthened the theory that NiV is enzootic in both _Pteropus_ bat species, and that these species serve as the natural reservoir for NiV in Malaysia. Longitudinal surveys of _P. hypomelanus_ bats suggest size and colony density may cause lower seroprevalence, and female bats that were pregnant, carrying a pup, and lactating generally had higher rates of NiV exposure than males or nonpregnant adult females, which lend further support to the hypothesis that infection rates may be higher during periods of pregnancy and lactation. Further study of NiV infection and shedding rates in pteropid bats will help elucidate seasonal and intracolonial viral dynamics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b57890c7-2886-421b-ad69-def9983b838e", "title": "Genetic Evidence and Host Immune Response in Persons Reinfected with SARS-CoV-2, Brazil", "text": "【0】Genetic Evidence and Host Immune Response in Persons Reinfected with SARS-CoV-2, Brazil\nConfirmed cases of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) have surpassed 110 million, along with 2.5 million deaths by 2019 coronavirus disease (COVID-19) . New waves of the pandemics in different Northern and Southern Hemisphere countries provide evidence that herd immunity might not have been fully achieved and that new variants could escape the response to natural infection .\n\n【1】Although there is evidence of the generation of B and T memory cells to SARS-CoV-2 proteins after infection , it has also been documented that neutralizing seroconversion is heterogeneous among the population . Even for those who seroconvert, the sustainability of the immune response, as judged by IgG level, might decay after the primary exposure to coronaviruses . Cases of reinfection by SARS-CoV-2 can be associated with the absence of neutralizing serologic titers, diminishment of immunoglobulin titers after primo-infection, or viral polymorphisms to escape the host SARS-CoV-2 immune response .\n\n【2】To better understand the dynamics of the immune and virological responses in mild cases of COVID-19 that might predispose patients to reinfection, we continuously followed up with patients for potential exposure to SARS-CoV-2. For 2 patients, reinfection was documented. The National Review Board of Brazil approved the study protocol (Comissão Nacional de Ética em Pesquisa \\[CONEP\\] 30650420.4.1001.0008), and informed consent was obtained from all participants or patients’ representatives.\n\n【3】### Materials and Methods\n\n【4】##### Ethics and Study Population\n\n【5】During March–December 2020, the COVID-19 research task force screened a group of 30 participants weekly, independent of any symptoms, for SARS-CoV-2 detection by RT-PCR in nasopharyngeal swab specimens. If any of these participants exhibited positive results, or members of their households experienced signs or symptoms of COVID-19, they were invited to participate in the study and follow-up. At baseline and follow-up, we collected plasma, serum, and nasopharyngeal swab samples biweekly or at longer intervals if the patient was unavailable . Households were included upon their request to be tested for SARS-CoV-2. Among the participants, 4 exhibited >1 episode of mild self-limiting COVID-19 with positive RT-PCR. For comparison, we included age-matched controls from the same group of participants and city in which the patients lived, Rio de Janeiro, Brazil. Controls were composed of 5 persons negative for SARS-CoV-2 throughout the investigated period.\n\n【6】##### Measurement of Serum SARS-CoV-2 Antibodies and Plasma Cytokine Levels\n\n【7】For quantitative analysis of SARS-CoV-2 spike protein IgM, IgA, and IgG antibodies, we performed the S-UFRJ test developed at Universidade Federal do Rio de Janeiro  .\n\n【8】We collected plasma samples in tubes containing EDTA. We used commercial ELISA kits from R&D Systems  to measure cytokines and chemokine .\n\n【9】##### Molecular Diagnosis\n\n【10】To determine serum titers to block SARS-CoV-2 infection, we performed miniaturized plaque-reduction neutralization test (PRNT) . SARS-CoV-2 RNA has been detected in accordance with the US Centers for Disease Control and Prevention (CDC) recommendation . We used the standard curve method for virus quantification, using synthetic RNA for gene N . We compared cycle thresholds (C t  ) for the target gene to those obtained with different cell amounts , for reaction calibration .\n\n【11】##### Genomic Analysis\n\n【12】We extracted total viral RNA from nasopharyngeal swabs using QIAamp Viral RNA , with minor modifications  . We performed an amplicon-based enrichment strategy using the ATOPlex SARS-CoV-2 Full-Length Genome Panel version 1.0 . Single-stranded circular DNA library pools were converted to DNA nanoballs by rolling circle amplification and submitted to pair-end sequencing (100 nt) on the MGISEQ-2000 platform (recently named DNBSEQ-G400; MGI Tech Co. Ltd.).\n\n【13】We quality-scored, filtered, trimmed, and assembled genomic sequences in contigs through a validated workflow for SARS-CoV-2 . Genomes were aligned with MAFFT  or ClustalW , and phylogenies were constructed with MEGA version 7.0 , using the Jukes-Cantor model for maximum-likelihood estimates by applying neighbor-joining and BioNJ algorithms , or by MrBayes version 3.2.7   with a relaxed clock model with a priori model testing using the gamma rates and invariant sites nucleotide substitution model, selected by jModelTest version 1. We visualized and edited the tree with FigTree version 1.4.2 . We determined SARS-CoV-2 clades using the Nextclade software, beta version 0.14 . To categorize mutations and polymorphisms, we aligned the SARS-CoV-2 reference genome Wuhan-Hu-1  to our sequences. 636737, 636834–636838. The dataset included in the analysis contained representative sequences of the emerging clades associated with our sequences, 19A and 20B, as well as sequences from the genome 20A as a negative control .\n\n【14】### Results\n\n【15】Among the households of the COVID-19 research task force, a 54-year-old man (patient A) requested an RT-PCR test for SARS-CoV-2 on March 23 because of a recurrent headache on the prior 2 days. He also had previous contact with a symptomatic co-worker returning from travel who refused to be tested. Patient A had a detectable viral load (C t  27.41) of ≈10 5  copies/mL in nasopharyngeal swab samples . Although patient B, a 57-year-old woman with a previous history of discoid lupus erythematosus, was in self-isolation, she was tested because of close contact with patient A. She tested positive for COVID-19 on March 24; her nasopharyngeal swab sample C t  was ≈36.31 (≈10 3  copies/mL) . Two days afterward, she experienced diarrhea .\n\n【16】Patient B shares a household with patients C and D, a married couple, both 34 years old. Patients C and D were not in social isolation because of their work duties. Although patient C was asymptomatic, he displayed a C t  of 35.71 (10 3  copies/mL) on March 25 . Patient D was negative by molecular testing on March 26, but 1 week later, she had a detectable viral load (C t  36.01, 10 3  copies/mL) and reported diarrhea in the following days . On March 27, all 4 patients experienced an increase of inflammatory mediators (interleukin \\[IL\\] 6, IL-8, and tumor necrosis factor α) and regulatory (IL-10) and chemotactic (C-X-C motif chemokine ligand 10) and antiviral (interferon γ) signals, relative to healthy SARS-CoV-2–negative controls . Although cytokine response was consistent with the resolution of the infection, the anti–SARS-CoV-2 neutralizing humoral response was not detected in late March 2020 .\n\n【17】For patients B and C, we were able to obtain a full-length SARS-CoV-2 genome . Complete genome sequencing, with Phred quality score >30, composed of 140,000–20,000,000 reads and 100-fold to 10,000-fold coverage, argues against a false-positive RT-PCR result . For patients A and D, the samples were insufficient for sequencing. In March 2020, patients B was infected with emerging SARS-CoV-2 clade 19A and patient C with SARS-CoV-2 clade 20B, The detection of the 2 distinct lineages indicates that patients B and C were infected independently and did not transmit the virus to each other . These distinct lineages were co-circulating in Brazil in March 2020 when multiple introductions of the SARS-CoV-2 occurred . Emerging clade 19A is associated with imported cases in Brazil, because of its proximity to the Wuhan-01 sequence . Indeed, detection of clade 19A in the sample from Patient B is consistent with household transmission from patient A, and his contact with the symptomatic traveler. Patient C, a police officer, was frequently exposed to various probable sources of contamination; he was infected with an emerging clade 20B virus, the most prevalent variant in Brazil, during December 2020 . All patients recovered from a mild COVID-19 episode and were retested in the first half of April, when they had negative RT-PCR results.\n\n【18】In the last week of May 2020, when COVID-19 cases in Rio de Janeiro were at the peak of the first wave of the pandemic , these 4 patients reported more signs and symptoms of SARS-CoV-2 infection than in March . During the second episode, they experienced fever and cough, along with fatigue, headache, body ache, anosmia, and ageusia. Real-time RT-PCR revealed higher viral loads in the nasopharyngeal swab samples than at the time of the first infection: C t  of 21.76 (≈10 7  copies/mL) for patient A, 21.84 (≈10 7  copies/mL) for patient B, 26.38 (≈10 5  copies/mL) for patient C, and 16.87 (≈10 9  copies/mL) for patient D .\n\n【19】On June 3, a week after the second episode, we detected SARS-CoV-2 immunoglobulins in patients A and B, but they had low to no neutralizing activity . These serologic samples from June indicate that the first episode of COVID-19 was not followed by a sustained neutralizing humoral response, as judged by 90% PRNT (PRNT 90  ) titers . Because signals of a humoral effector memory were inconsistent after the first episode of COVID-19 , we could speculate that the enhanced production of interferons and proinflammatory mediators led to resolution of the primo-infection . During the second episode of COVID-19, most of the cytokine levels were still higher than in healthy volunteers .\n\n【20】On July 9, forty days after the episode of reinfection, all patients had detectable immunoglobulin levels and their lowest PRNT 90  results , declining thereafter by August 10 . In July, patients’ tests continuously showed upregulated pro-inflammatory markers , which are consistent with an enhanced response to a second SARS-CoV-2 exposure. In August, the markers of inflammation and regulatory responses, tumor necrosis factor α and IL-10, decreased compared with levels from previous months .\n\n【21】In the second episode, we fully sequenced the SARS-CoV-2 genome from all patients . SARS-CoV-2 sequences from the reinfection clustered together, suggesting a household transmission for patients A–D . The emerging genotype 20B, which was the main variant circulating in Brazil since May 2020, was detected in all samples from the second episode . For patient B, the first episode was associated with the emerging clade 19A and the second with 20B . Two episodes provoked by genetically distinct lineages support the possibility of reinfection.\n\n【22】Although both episodes in patient C were associated with clade 20B, they clustered apart on the phylogeny with significant statistical support: by 86% of bootstrap using maximum likelihood  and by Bayesian inference . Genetic markers in the SARS-CoV-2 genome were different in the patient’s 2 episodes of COVID-19 . The genomes diverge at the genes encoding the nonstructural protein (NSP) 3, 3C-like proteinase, and exonuclease . In addition to the genetic variations, poor development of anti–SARS-CoV-2 serology between the 2 episodes of infection points suggests a reinfection scenario.\n\n【23】### Discussion\n\n【24】Seasonal human coronaviruses may cause reinfection, as documented for the past 35 years . Of note, in veterinary medicine, domestic mammals also have coronavirus reinfection . Adaptive, memory-generating immunity to coronaviruses is heterogeneously sustainable in mammals, and some events of infection are controlled at the level of the innate immunity . We fully documented reinfection in 2 genetically unrelated persons in Rio de Janeiro, Brazil, describing patients who sought care twice in a 2-month interval who received clinical and laboratory diagnosis of COVID-19. Virus polymorphisms from the primary and second episodes and negative RT-PCR between the events strengthen the argument toward reinfection. Neutralizing anti–SARS-CoV-2 titers were not detected during the first episode, nor at the baseline of the second episode, suggesting that patients were still vulnerable after the primary episode.\n\n【25】SARS-CoV-2 reinfection has been associated with new variants that overcome the immune response to natural infection, short-lasting humoral response, and a limited or absent neutralizing immunity after the primo-infection . The patients in Brazil described in this study are similar to cases in the United States and Ecuador , in which reinfection was associated with more symptoms. Antibody-dependent enhancement or exposure to higher amounts of the virus could be the reason for the change from asymptomatic or oligosymptomatic to syndromic. In our study, primary and second infections were caused by a strain carrying the D614G mutation in the spike protein, which has been associated with higher replication efficiency . We did not detect other contemporaneous changes in the spike protein, such as 69/70 deletion, K417N, E484K, N501Y, P681H, or the 17 unique mutations of the P1 variant, which precluded association with more virulent strains . Beyond the spike protein, we detected the V125F change in the NSP14 protein; V125F is a nonconservative mutation that might increase the volume in the loop between β-sheets number 5 and 6, which could affect its methyltransferase activity . The V125F mutation is unlikely to increase virulence in a second episode. On the other hand, changes in NSP6 protein  and open reading frame 6 mRNA  might result in viral evasion from innate immunity.\n\n【26】The primary infections of patients B and C were associated with emerging clades 19A and 20B, indicating that the 2 cohabitants were infected independently. Indeed, while 1 patient was in social isolation, the others were working outside in the community. The cocirculation of these clades of SARS-CoV-2 is consistent with the COVID-19 databases in GISAID and the multiple introductions of the new coronavirus in Brazil . In the following months, emerging clade 20B was identified as the most prevalent genotype, representing 60% of the deposited genomes on GISAID. The detection of clade 20B on the second episode of COVID-19, by the end of May, is associated with the peak of the pandemic in Rio de Janeiro, Brazil .\n\n【27】Distinct clades of SARS-CoV-2 were found in the primary and secondary respiratory samples from patient B, supporting the notion of reinfection. For patient C, both the first and second detections of SARS-CoV-2 were associated with clade 20B. Although viral persistence could be imagined in this scenario, SARS-CoV-2 genomic sequences from the first and second episodes do not cluster together in the same branch, as they did for an immunocompromised patient that shed SARS-CoV-2 for 150 days . Thus, phylogeny does not support the interpretation of persistence, by different methods. By branching apart, SARS-CoV-2 genomes associated with patient C strengthen the chances of a relevant degree of variation , indicating the direction of reinfection. In the documented case of SARS-CoV-2 and human coronavirus NL63 reinfection, different episodes were genetically associated with similar viral clades or strains . Whereas the detection of 2 episodes of SARS-CoV-2 infection from patient C was separated by >60 days, prolonged virus shedding in the nasopharyngeal swab specimens from mild cases lasted for 22–46 days , which is further evidence against persistence.\n\n【28】Results of SARS-CoV-2 reinfection affirm that immune rechallenge may be necessary to achieve humoral protection and underscore that sustainability of the immune response may be heterogeneous. We documented that these patients with mild COVID-19 displayed an innate immune response composed of pro-inflammatory and regulatory signals. Although cytokine storm has been associated with severe COVID-19 , we interpret that in the case of our patients, the innate immune response might have led to infection resolution . Another possibility, not explored in detail here, is that cellular-mediated immunity could have contributed to the mild clinical outcome . The natural history of mild COVID-19 described for these patients might also be representative of many persons exposed to the first wave of the pandemic, leading to the hypothesis that they would also be susceptible to other episodes of SARS-CoV-2 infections, even without the challenge being imposed by new variants.\n\n【29】We determined, on the basis of 6 years of surveillance and follow-up of human coronavirus reinfections, that initial exposure was insufficient to elicit a protective immune response, imposing limited pressure on selection on new seasonal coronavirus variants . Similarly, our data on a small cluster of patients recapitulate this natural history of reinfection, which may also occur for SARS-CoV-2.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "817781e0-776a-4d9c-895e-9d5461468c10", "title": "Accuracy of Principal and Teacher Knowledge of School District Policies on Sun Protection in California Elementary Schools", "text": "【0】Accuracy of Principal and Teacher Knowledge of School District Policies on Sun Protection in California Elementary Schools\nAbstract\n\n【1】**Introduction**\n\n【2】Policy is a key aspect of school-based efforts to prevent skin cancer. We explored the extent and accuracy of knowledge among principals and teachers in California public school districts about the elements specified in their district’s written sun safety policy.\n\n【3】**Methods**\n\n【4】The sample consisted of California public school districts that subscribed to the California School Boards Association, had an elementary school, adopted Board Policy 5141.7 for sun safety, and posted it online. The content of each policy was coded. Principals (n = 118) and teachers (n = 113) in elementary schools were recruited from September 2013 through December 2015 and completed a survey on sun protection policies and practices from January 2014 through April 2016.\n\n【5】**Results**\n\n【6】Only 38 of 117 principals (32.5%) were aware that their school district had a sun protection policy. A smaller percentage of teachers (13 of 109; 11.9%) than principals were aware of the policy ( _F_ 108  \\= 12.76, _P_ < .001). We found greater awareness of the policy among principals and teachers who had more years of experience working in public education (odds ratio \\[OR\\] = 1.05, _F_ 106  \\= 4.71, _P_ \\= .03) and worked in schools with more non-Hispanic white students (OR = 7.65, _F_ 109  \\= 8.61, _P_ \\= .004) and fewer Hispanic students (OR = 0.28, _F_ 109  \\= 4.27, _P_ \\= .04).\n\n【7】**Conclusion**\n\n【8】Policy adoption is an important step in implementing sun safety practices in schools, but districts may need more effective means of informing school principals and teachers of sun safety policies. Implementation will lag without clear understanding of the policy’s content by school personnel.\n\n【9】Introduction\n\n【10】In 2014, the US Surgeon General issued a call to action to prevent skin cancer , citing the high and increasing prevalence  and cost of the disease. Unprotected, excessive exposure to solar ultraviolet radiation is the primary risk factor for skin cancer . The Centers for Disease Control and Prevention (CDC)  and the Surgeon General identified sun safety in schools as a priority. Children receive substantial ultraviolet exposure , including during outdoor school activities (eg, recess and lunch periods, physical education classes, extracurricular sports). Many children are sunburned annually , and blistering sunburns in adolescence may elevate lifetime risk for melanoma . Schools can design outdoor environments and schedules to reduce ultraviolet exposure and teach lifelong sun protection habits.\n\n【11】Written policy is a key aspect of school-based efforts to prevent skin cancer . Policies can prescribe actions at all levels of school organization  to improve health outcomes , including for children who do not choose or are unable to protect themselves . However, a policy is successful only if implemented. An essential step in policy implementation is for school personnel, including principals and teachers, to be made aware of the policy.\n\n【12】California had an estimated 7,750 cases of melanoma (23 per 100,000) in 2014 . It was one of the first states to enact legislation, in 2002, governing sun protection for students (California Education Code Section 35183.5). In 2006, the California School Boards Association (CSBA) recommended a sun safety Sample Board Policy (designated BP 5141.7) consistent with CDC guidelines  on rescheduling outdoor activities; increasing use of sun-protective clothing, hats, sunglasses, and sunscreen; supporting education for students, and outreach to families on skin cancer prevention; and allocating resources and increasing accountability for sun safety efforts.\n\n【13】The objective of this study was to explore the extent and accuracy of California elementary school principals’ and teachers’ knowledge about the elements specified in their district’s written school board–approved sun safety policy. The study was restricted to California public school districts that already had a sun safety policy.\n\n【14】Methods\n\n【15】We conducted an online baseline survey of principals and teachers in schools that were recruited from September 2013 through December 2015 to a parent study, a randomized trial that tested a technical assistance intervention that promoted implementation of sun safety policy by elementary school personnel. Eligible to be included in the sample were public school districts in California that 1) subscribed to the CSBA, 2) had an elementary school, 3) adopted BP 5141.7, and 4) posted their version of BP 5141.7 online.\n\n【16】CSBA staff members provided lists of member districts that had adopted BP 5141.7 in 2012 and 2013. Project staff members contacted by email and telephone the principals at 489 elementary schools in 59 school districts that had adopted BP 5141.7, inviting them to participate in the parent project. The order in which principals at elementary schools were contacted was stratified by location of the elementary school (whether coastal or inland, because these regions differ in temperature and cloud cover); distance from the study’s offices at Claremont Graduate University, Claremont, California (to control project costs, schools closer to Claremont were contacted first); and the number of elementary schools in the district. Elementary schools in larger school districts were contacted first. Recruitment ended when the quota for 118 elementary schools was met for the parent study, which was set based on a priori power estimates for the policy implementation intervention. Of 130 principals who agreed to have their elementary school participate, 12 (9%) did not complete the baseline survey before the recruitment period ended, so the school was not randomized; 359 principals did not respond to the study invitation or refused to participate. One hundred-eighteen principals provided consent for their school to participate and nominated one teacher or staff member (hereinafter referred to simply as “teachers”) who would be involved in implementing sun safety practices at the school. Teachers were contacted individually and invited to complete the baseline survey. If the teacher identified by the principal did not consent, the principal was asked to designate an alternate person.\n\n【17】**Policy collection and coding procedures.** The BP 5141.7 policy documents for districts with participating elementary schools were downloaded by research staff from district websites or from Gamut Online, CSBA’s database of district policies, from September 2013 through December 2015 as schools were recruited to the trial. Research staff members were trained to code the content of each policy document by using a validated coding system . Coders recorded the presence of policy content (ie, 0 = not present; 1 = present) in 11 categories covering sun safety components recommended by CDC : sunscreen use, ultraviolet-protective clothing, hats, education of students, education of teachers, outdoor shade, scheduling, parent outreach, resource allocation, accountability, and staff modeling. For each category, the strength and intent of the policy components for sun protection of students were recorded but not used in this analysis. The date of adoption and revisions to the policy were recorded. All policies (n = 190) were coded by a single coder. To check inter-rater reliability, 15% (n = 29) of policies were double coded by a second coder; reliabilities for the content and strength score were acceptable (κ > 0.70 \\[mean = 0.94\\] ).\n\n【18】**Online survey procedures.** A principal and a teacher at each elementary school completed a survey on school district sun protection policies and school sun safety practices from January 2014 through April 2016. All surveys were completed online by using a survey software package through Inquisit (Millisecond Software). Respondents received an invitation and up to 3 automated email reminders and follow-up telephone calls, if needed, by project staff members. The survey procedures and measures were classified as exempt by the Claremont Graduate University and Western institutional review boards.\n\n【19】Principals and teachers reported whether or not the school district had a policy on sun protection for students. Those who answered yes were presented with a list of policy components (eg, modifying the school schedule to avoid daily periods with high levels of ultraviolet radiation, increasing shade on school grounds, teaching about skin cancer prevention in the classroom) and asked to indicate which components were included in the policy. Respondents also answered items on job characteristics (years in public education and in their current job), skin type (on a scale of 1 to 5, with 1 being the darkest and lowest risk of skin cancer and 5 being the lightest and the highest risk of skin cancer), personal or family history of skin cancer, and demographic characteristics (age, Hispanic ethnicity, race/ethnicity, sex). Respondents also answered questions on attitudes toward sun protection of students, personal sun protection practices, and sunburn history, but these data were not included in this analysis. Skin types categorized as 4 and 5 were designated high-risk skin types. Teachers were also asked which grades they taught and whether they taught health or science or both. Principals and teachers each received $10 for completing the survey.\n\n【20】**School characteristics.** Research staff members obtained information from the state department of education records on location of each school (and then calculated distance from the study office), school size (number of students), and proportion of racial/ethnic minority students, English-learner students, and students in free or reduced-price meal programs.\n\n【21】### Statistical analysis\n\n【22】PROC MEANS and PROC FREQ in SAS version 9.3 (SAS Institute Inc) were used to generate means and percentages to describe the samples of schools, principals, and teachers. Policy knowledge among principals and teachers was compared with the content of the written school district sun safety policy, and principals’ and teachers’ knowledge of each component was classified as accurate or inaccurate. The effects of role (principal vs teacher), demographics characteristics of the principal or teacher, and characteristics of the school on awareness and knowledge of school district sun safety policy were assessed by using multilevel analysis, with individuals (principal or teacher) nested within schools and schools nested within districts. PROC GLIMMIX was used to fit mixed models for logistic regression on awareness of policy and PROC MIXED was used to fit mixed models for linear regression on number of policy elements correctly known. We set α criterion levels at .05 (2-tailed).\n\n【23】Results\n\n【24】The elementary schools were widely distributed around Claremont, California . Schools enrolled on average 564.6 students; 54.5% were Hispanic and 24.0% were non-Hispanic white. Approximately one-quarter (27.8%) of students were English learners, and 64.1% received free or reduced-price meals. We found no significant differences between participating and nonparticipating schools (n = 371) in distance from study office, number of students, race/ethnicity, and or other student characteristics. Principals had worked on average 21.1 years in public education but had served as principals in their current school district for 4.6 years on average (and worked in the current school district for 11.4 years on average). Principals were aged on average 47.8 years; 21.2% were Hispanic, 69.5% were non-Hispanic white; and 72.0% were women. Approximately one-third (34.8%) of principals had a high-risk skin type, and 39.0% reported they or a family member had been diagnosed with skin cancer.\n\n【25】In the 118 schools, 113 teachers (96%) completed the survey. They had less experience than principals in public education, averaging 14.3 years, but they had worked a similar amount of time in their school districts at 11.6 years on average. Nearly half (45.1%) of teachers did not teach a specific grade; those who did were widely distributed over grades kindergarten through grade 5. Also, 42.0% of teachers taught health or science or both curricula. Teachers were younger (43.4 years on average) than principals, and the percentage of teachers who were women (86.7%) was greater than the percentage of principals who were women. Approximately one-fifth (22.1%) of teachers were Hispanic, and 66.4% were non-Hispanic white. One-quarter (23.6%) of teachers had high-risk skin types, and 42.5% had a personal or family history of skin cancer.\n\n【26】### Awareness of school district sun safety policy\n\n【27】Of 117 principals who answered the policy awareness item, 38 (32.5%) were correctly aware that their school district had a sun protection policy; 79 principals did not know the district had a policy. Even fewer teachers (13 of 109 \\[11.9%\\] who answered the question) were aware of the policy; 96 teachers did not know the district had a policy. Mixed model logistic regression analysis confirmed that principals were more aware of the policy than teachers were ( _F_ 108  \\= 12.76, _P_ < .001). In addition, we found greater awareness of the sun protection policy among principals and teachers who had more years of experience working in public education (odds ratio \\[OR\\] = 1.05, _F_ 106  \\= 4.71, _P_ \\= .03) and were in schools with a larger proportion of non-Hispanic white students (OR = 7.65, _F_ 109  \\= 8.61, _P_ \\= .004) and a smaller proportion of Hispanic students (OR = 0.28, _F_ 109  \\= 4.27, _P_ \\= .04).\n\n【28】### Awareness of policy content\n\n【29】Among the principals and teachers who were aware of their district’s policy on sun protection, knowledge of its content elements varied . When principals or teachers had inaccurate knowledge, they tended to report that the policy did not address a particular component of sun safety when it actually did. Approximately three-quarters of principals were aware that the policy addressed personal protection of students (ie, use of sunscreen, 68.4%; use of protective clothing, 71.1%, and use of hats, 76.3%). Most principals were correctly aware that the policy did not cover training teachers (84.2%) or allocating district resources for sun protection (91.9%). Approximately half of principals had accurate knowledge of whether the policy addressed educating students (44.8%) and communicating with parents on sun safety (54.0%) and adjusting schedules for students to be outdoors outside of midday hours when levels of ultraviolet radiation is highest (48.6%). Principals had the greatest percentage of inaccurate knowledge on whether the policy addressed providing shade on school grounds (60.5%) and encouraged school staff members to model sun safety to students (59.5%).\n\n【30】Compared with principals, teachers were much less aware of the content of the district sun safety policy, except for knowing that no provision existed for educating teachers (92.3%) or allocating resources for sun safety (92.3%) . Approximately half of the teachers accurately reported whether the policy addressed personal sun protection of students while at school (ie, use of sunscreen, 46.2%; use of protective clothing, 53.8%, and use of hats, 53.8%), educating students on sun safety (46.2%), providing outdoor shade at the school (53.9%), adjusting outdoor activity schedules to avoid midday hours (53.9%), and communicating with parents about sun safety (46.2%). Only 23.1% of teachers knew the policy encouraged school staff to model sun safety to students.\n\n【31】Mixed model linear regression confirmed that principals had accurate knowledge of more content components in the district sun safety policy than teachers ( _F_ 108  \\= 15.98, _P_ < .001). In addition, we found greater awareness of the school policy content among principals and teachers who were older (β = 0.037; standard error \\[SE\\], 0.015; _F_ 98  \\= 6.30; _P_ \\= .01), who had more years of experience working in public education (β = 0.040; SE, 0.016; _F_ 106  \\= 6.08; _P_ \\= .015), who were in schools with a larger proportion of non-Hispanic white students (β = 1.950; SE, 0.580; _F_ 109  \\= 11.31; _P_ \\= .001), and who were in schools with a smaller proportion of Hispanic students (β = −1.382; SE, 0.498; _F_ 109  \\= 7.70; _P_ \\= .007), English-learner students (β = −1.802; SE, 0.779, _F_ 107  \\= 5.35; _P_ \\= .02), and students receiving free or reduced-price meals (β = −1.024; SE, 0.468; _F_ 109  \\= 4.80; _P_ \\= .03).\n\n【32】Discussion\n\n【33】In California school districts that had adopted a sun safety policy, a survey of elementary school principals and teachers indicated a low level of awareness of the sun protection elements in their district policy. Most principals and teachers did not know whether or not their district had a sun safety policy. Among those who were aware of the policy, many had incorrect knowledge of its contents. The low level of understanding of the school district’s sun protection policy may reflect a feeling by school personnel that sun safety is a low-priority health and safety topic relative to other issues such as tobacco and substance use and nutrition. Principals and teachers may also consider health and safety issues to be a lower priority than curricular and testing issues.\n\n【34】The greater awareness of the policy among principals than among teachers also may indicate shortcomings in the way many school districts communicate new policies to school personnel. Principals may be more aware of district policies than teachers because they are in a leadership position and thus more likely to be informed about policy changes by district administrators. Districts may rely on principals in turn to communicate policies to teachers, and some principals may do this more effectively than others. In addition, some principals may have more responsibility than teachers for implementing policies that affect the health and safety of students, especially when implementation requires school-wide actions (eg, adjusting outdoor schedules and constructing shade structures) rather than class-wide or grade-wide actions.\n\n【35】Principals and teachers were more knowledgeable about how the elements of the policy pertained to personal protection practices for students (ie, use of sunscreen, protective clothing, and hats) than to the education of students or modifications to the school environment and procedures (ie, provision of shade, adjustment of school schedules, and modeling of sun safety by school staff members). School personnel may consider sun protection to be a personal choice and the responsibility of students and their parents and thus would expect sun protection policy to address students’ personal protection practices. Also, these personal protection practices are covered by state legislation, so awareness of the legislation may have elevated their awareness of their school district’s policy.\n\n【36】In addition to parents’ and students’ personal responsibility for sun protection, more efforts appear to be needed to raise awareness among school personnel about their role and responsibility in implementing modifications to the school environment and procedures when these elements are expressed in the district policy. Environmental and procedural changes are enduring, apply to all students, and offer a certain level of protection for students who do not practice personal sun protection for whatever reason.\n\n【37】That the teachers surveyed in this study were largely unaware of the sun protection policy was surprising, even though principals identified these individuals as someone at the school who would assist in implementing sun safety practices. Teachers may be more likely to learn about policies that affect the curriculum, their classroom procedures, or their own jobs. Teachers did appear to be more aware than principals about whether the policies required them to receive training in sun safety.\n\n【38】The sociodemographic characteristics of the student population at the schools appeared to be associated with principals’ and teachers’ knowledge of their district’s sun protection policy. Principals and teachers at schools with a greater percentage of non-Hispanic white students and affluent students (ie, students who were not participating in a free or reduced-price lunch program) and a smaller percentage of Hispanic and English-learner students were more aware of the sun protection policy and its contents. This finding may reflect a general knowledge that skin cancer is more prevalent among non-Hispanic white people than among Hispanic people. However, the prevalence of skin cancer is increasing in Hispanic populations , and school personnel should be made aware of this fact and the relevance of skin cancer prevention to Hispanic students. The finding that school personnel who had more experience working in public education were more informed about the sun protection policies may be attributed to principals being older than teachers. School personnel with more experience may have more extensive professional communication networks that provided them with more information about the new policies than less experienced and younger school personnel. This finding calls for senior administrators to initiate strategies that actively communicate new policies throughout the school districts to make sure the new policies are disseminated across all personnel regardless of the depth of their professional contacts and position in the district’s leadership.\n\n【39】The study had several strengths and limitations. Strengths are that the sample of schools was large and the presence and content of district policy was obtained from coding of written policy documents. However, the study was limited to a single state’s elementary schools, limiting generalizability. The survey items did not assess details of sun protection (eg, sunscreen sun protection factor, topics in student instruction) and relied on self-report. Efforts and resources devoted by school districts to publicize the policies to principals and teachers were not assessed.\n\n【40】The US Surgeon General’s call to action to prevent skin cancer states that “\\[w\\]e must act with urgency to stop the ever-increasing incidence of skin cancers in the United States.” The call to action identifies schools as essential partners in this effort. Schools are urged to increase the availability of sun protection in educational settings, support skin cancer prevention education, and support inclusion of sun protection in school policies, construction of school facilities, and school curricula. Policy adoption by an elected school district board may be a necessary first step toward implementing other sun safety practices and procedures in schools. However, the knowledge gap among elementary school principals and teachers in their awareness of district sun safety policies suggests a need for improved communication of the policy to principals and teachers as well as encouragement and support for policy implementation so that sun protection can be improved in elementary schools.\n\n【41】Tables\n------\n\n【42】 Table 1. Profile of the Samples of Elementary Schools (n = 118), Principals (n = 118), and Teachers (n = 113) Who Participated in Study of Knowledge of the School District Written Policy on Sun Protection, California Puboplic School Districts, January 2014–April 2016 a\n\n| Characteristic | Mean (SD) or % |\n| --- | --- |\n| **Schools** | **Schools** |\n| Distance from study office in Claremont, California, miles b | 133.6 (180.1) |\n| Total no. of students enrolled in school | 564.6 (215.6) |\n| English-learner students, % | 27.8 (17.0) |\n| Fluent English-proficient students, % | 8.7 (6.6) |\n| Students in free or reduced-price meal program, % | 64.1 (28.8) |\n| African American students, % | 6.4 (7.6) |\n| American Indian or Alaska Native students, % | 1.7 (7.3) |\n| Asian students, % | 6.5 (12.8) |\n| Filipino students, % | 1.6 (2.7) |\n| Hispanic or Latino students, % | 54.5 (26.4) |\n| Pacific Islander students, % | 0.5 (1.1) |\n| Non-Hispanic white students, % | 24.0 (22.3) |\n| Students identifying as ≥2 races/ethnicities | 3.6 (3.3) |\n| **Principals** | **Principals** |\n| **No. of years working .** | **No. of years working .** |\n| In public education in any position | 21.1 (7.1) |\n| As a principal in current school district | 4.6 (4.0) |\n| In current school district | 11.4 (9.2) |\n| **Skin type** | **Skin type** |\n| 1 (Darkest skin/lowest risk) | 28.7 |\n| 2 | 28.7 |\n| 3 | 25.2 |\n| 4 | 17.4 |\n| 5 (Lightest skin/highest risk) | 17.4 |\n| **Age, y** | 47.8 (7.7) |\n| **Personal or family history of skin cancer** | 39.0 |\n| **Hispanic ethnicity** | 21.2 |\n| **Race** | **Race** |\n| American Indian/Alaska Native | 3.4 |\n| Asian | 0.8 |\n| Black/African American | 8.5 |\n| Native Hawaiian/Other Pacific Islander | 0.8 |\n| Non-Hispanic white | 69.5 |\n| More than one race/ethnicity | 5.1 |\n| Prefer not to answer | 3.4 |\n| None of these | 8.5 |\n| **Sex** | **Sex** |\n| Female | 72.0 |\n| Male | 27.1 |\n| Prefer not to answer | 0.9 |\n| **Teachers** | **Teachers** |\n| **No. of years working .** | **No. of years working .** |\n| In public education in any position | 14.3 (7.3) |\n| As a teacher in current school district | 11.6 (7.5) |\n| **Grade(s) currently taught c** | **Grade(s) currently taught c** |\n| Kindergarten | 17.0 |\n| Grade 1 | 17.9 |\n| Grade 2 | 18.8 |\n| Grade 3 | 21.4 |\n| Grade 4 | 21.4 |\n| Grade 5 | 18.8 |\n| Grade 6 | 9.8 |\n| Grade 7 | 2.7 |\n| Grade 8 | 1.8 |\n| No specific grade | 45.1 |\n| **Teach health or science curriculum** | **Teach health or science curriculum** |\n| No | 58.0 |\n| Yes, health | 7.1 |\n| Yes, science | 17.9 |\n| Yes, health and science | 17.0 |\n| **Skin type** | **Skin type** |\n| 1 (Darkest skin/lowest risk) | 26.4 |\n| 2 | 27.3 |\n| 3 | 22.7 |\n| 4 | 20.9 |\n| 5 (Lightest skin/highest risk) | 2.7 |\n| **Age, y** | 43.4 (10.0) |\n| **Personal or family history of skin cancer** | 42.5 |\n| **Hispanic ethnicity** | 22.1 |\n| **Race** | **Race** |\n| American Indian/Alaska Native | 4.4 |\n| Asian | 7.1 |\n| Black/African American | 4.4 |\n| Native Hawaiian/Other Pacific Islander | 2.7 |\n| Non-Hispanic white | 66.4 |\n| More than one race/ethnicity | 3.5 |\n| Prefer not to answer | 2.6 |\n| None of these | 8.9 |\n| **Sex** | **Sex** |\n| Female | 86.7 |\n| Male | 13.3 |\n| Prefer not to answer | 0 |\n\n【44】a  Sample consisted of California public school districts that subscribed to the California School Boards Association, had an elementary school, adopted Board Policy 5141.7 for sun safety, and posted it online from September 2013 through December 2015. Participants completed surveys from January 2014 through April 2016.  \nb  Median, 42 miles; range, 2–809 miles; interquartile range, 30–96 miles.  \nc  Teachers could select more than one grade level.\n\n【45】 Table 2. Percentage of Correct and Incorrect Knowledge of the School District Written Policy on Sun Protection Among Elementary School Principals (N = 38) and Teachers (N = 13) Who Were Aware of the Policy, California Public School Districts, January 2014–April 2016 a  \n\n| Policy Component | Accurate Policy Knowledge b , % | Inaccurate Policy Knowledge c , % |\n| --- | --- | --- |\n| Addressed in Policy | Not Addressed in Policy | Total | Addressed in Policy | Not Addressed in Policy | Total |\n| --- | --- | --- | --- | --- | --- |\n| **Principals** | **Principals** | **Principals** | **Principals** | **Principals** | **Principals** | **Principals** |\n| Sunscreen use | 68.4 | 5.3 | 73.7 | 2.6 | 23.7 | 26.3 |\n| Ultraviolet-protective clothing use | 71.1 | 0 | 71.1 | 2.6 | 26.3 | 28.9 |\n| Hat use | 76.3 | 0 | 76.3 | 2.6 | 21.1 | 23.7 |\n| Educate students on sun safety | 23.7 | 21.1 | 44.8 | 10.5 | 44.7 | 55.2 |\n| Educate teachers on sun safety | 0 | 84.2 | 84.2 | 15.8 | 0 | 15.8 |\n| Provide outdoor shade | 23.7 | 15.8 | 39.5 | 2.6 | 57.9 | 60.5 |\n| Adjust schedules so outdoor activities avoid midday | 35.1 | 13.5 | 48.6 | 2.7 | 48.7 | 51.4 |\n| Communication with parents about sun safety | 32.4 | 21.6 | 54.0 | 8.1 | 37.9 | 46.0 |\n| Allocate resources for sun safety | 0 | 91.9 | 91.9 | 8.1 | 0 | 8.1 |\n| Encourage staff to model sun safety | 32.4 | 8.1 | 40.5 | 2.7 | 56.8 | 59.5 |\n| **Teachers** | **Teachers** | **Teachers** | **Teachers** | **Teachers** | **Teachers** | **Teachers** |\n| Sunscreen use | 38.5 | 7.7 | 46.2 | 7.7 | 46.1 | 53.8 |\n| Ultraviolet-protective clothing use | 53.8 | 0 | 53.8 | 0 | 46.2 | 46.2 |\n| Hat use | 53.8 | 0 | 53.8 | 0 | 46.2 | 46.2 |\n| Educate students on sun safety | 15.4 | 30.8 | 46.2 | 7.7 | 46.2 | 53.9 |\n| Educate teachers on sun safety | 0 | 92.3 | 92.3 | 7.7 | 0 | 7.7 |\n| Provide outdoor shade | 30.8 | 23.1 | 53.9 | 7.7 | 38.4 | 46.2 |\n| Adjust schedules so outdoor activities avoid midday | 38.5 | 15.4 | 53.9 | 0 | 46.1 | 46.1 |\n| Communication with parents about sun safety | 23.1 | 23.1 | 46.2 | 15.4 | 38.4 | 53.8 |\n| Allocate resources for sun safety | 0 | 92.3 | 92.3 | 7.7 | 0 | 7.7 |\n| Encourage staff to model sun safety | 23.1 | 0 | 23.1 | 7.7 | 69.2 | 76.9 |\n\n【47】a  Sample consisted of California public school districts that subscribed to the California School Boards Association, had an elementary school, adopted Board Policy 5141.7 for sun safety, and posted it online from September 2013 through December 2015. Participants completed surveys from January 2014 through April 2016.  \nb  Accurate policy knowledge means that school district policy addressed policy component and principal said the component was addressed in the policy, or school district policy did not address policy component and principal said the component was not addressed in the policy.  \nc  Inaccurate policy knowledge means that school district policy addressed policy component and principal said the component was not addressed in the policy, or school district policy did not address policy component and principal said the component was addressed in the policy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4196288f-1182-4d81-85bb-33b5973347b4", "title": "Wild Felids as Hosts for Human Plague, Western United States", "text": "【0】Wild Felids as Hosts for Human Plague, Western United States\nZoonotic pathogens account for ≈60% of emerging diseases . _Yersinia pestis_ , a vector-borne bacterium and the causative agent of plague in mammals, is 1 such emergent pathogen . Plague is maintained among rodent hosts and their fleas; however, spillover into accidental hosts can result in severe illness and death, as well as geographic spread of the disease .\n\n【1】Domestic cats are a major source of human plague infections in the United States , putting veterinary workers and pet owners at risk for _Y. pestis_ infections. During 1924–2006, a total of 13 human cases of primary pneumonic plague were documented in the United States, and \\> 5 were associated with felids . Twelve cases of plague transmission from nondomestic carnivores to humans have been documented , including a fatal case of human pneumonic plague in 2007 that resulted from direct contact with an infected puma ( _Puma concolor_ ) . Despite the known association of felids with human plague, the prevalence of _Y. pestis_ infection in nondomestic cats remains relatively unknown.\n\n【2】Pumas and bobcats ( _Lynx rufus_ ) are 2 of the most widespread felids in North American, with pumas having the greatest range of any wild terrestrial mammal in the Western Hemisphere . Both species inhabit large territories and travel great distances during dispersal . These highly mobile animals may periodically reintroduce _Y. pestis_ –positive fleas to distant regions, especially during epizootics . Consequently, carnivore-aided flea dispersal could play an important role in the spread and persistence of plague during interepizootic periods.\n\n【3】We examined plague exposure in populations of bobcats and pumas in California and Colorado. This gave us an opportunity to evaluate _Y. pestis_ seroprevalence in multiple difficult-to-sample, plague-susceptible felid species across a wide geographic area.\n\n【4】### The Study\n\n【5】We collected samples from 119 pumas and 212 bobcats  in 3 locations in southern California and 2 locations in western and north-central Colorado  from autumn 2002 through summer 2008. Seventy-seven of these bobcat samples consisted of thoracic fluid collected postmortem from hunter-killed animals. Eight puma samples collected in the 1980s served as historical reference for puma samples from the Colorado Western Slope (i.e. area west of the Continental Divide). Animals were captured, sampled, and released with permission of cooperating agencies after approval by animal care and use committees. Samples were processed according to protocol .\n\n【6】Thoracic fluid samples were immunoblotted onto nitrocellulose membranes (immuno-blot polyvinylidene fluoride membranes; Bio-Rad, Hercules, CA, USA) and probed with goat-anti-cat-phosphatase labeled antibody to verify the presence of immunoglobulin. Reacted membranes were rinsed 3 times with phosphate-buffered saline, once in Milli-Q (Millipore, Billerica, MA, USA) and were then exposed to a 5-bromo-4-chloro-3-indolyl-phosphate/nitroblue tetrazolium (alkaline-phosphatase chromogen) substrate (Kirkegaard and Perry Laboratories, Gaithersburg, MD, USA). Samples were classified by comparing staining intensity to positive (bobcat/domestic cat serum) and negative controls (water and goat serum).\n\n【7】Serum and thoracic fluid samples were analyzed for _Y. pestis_ antibody using a hemagglutination assay according to a standard protocol . Positive samples were evaluated according to Chu . If a limited amount of sample was available, serum was diluted 1:4 and considered positive if titers were >32. Larger serum samples were not diluted, and a reading \\> 16 was considered positive .\n\n【8】Data were analyzed by using a logistic link function and binary error, with antibody presence (positive vs. negative) as the outcome variable (SAS version 9.1; SAS, Cary, NC, USA). Estimates used maximum likelihood. Degrees of freedom were calculated by using a Kenward-Roger adjustment. Categorical factors included location, species, age, sex, and capture season. Animals captured in the fall (September–November) and in Ventura County were not plague positive and were omitted. All factors were treated as fixed variables, including location, because of previously reported differences in regional seroprevalence rates.\n\n【9】A total of 76 of 77 thoracic fluid samples had immunoglobulin present, as assessed by visual comparison of immunoblot staining, and were included in _Y. pestis_ antibody analysis. Interactions were not significant and were omitted. Mean _Y. pestis_ seroprevalence for pumas and bobcats across all locations was 17.7% (95% confidence interval \\[CI\\] 13.6%–21.8%). However, considerable variability existed across locations (Front Range, Colorado, mean 21.1 \\[95% CI 8.23–44.75\\]; Orange County, California, mean 1.23 \\[95% CI 0.13–10.01\\]; San Diego/Riverside counties, California, mean 6.58 \\[95% CI 1.52–24.33\\]; Ventura County, California, mean 0 \\[NA\\]; Western Slope, Colorado, mean 46.03 \\[95% CI 24.37–69.29\\]). Species and sex were not significant predictors of plague exposure; however, animal age, geographic location, and capture season were significant . Adult animals ( \\> 2 years of age) and animals from the Colorado Western Slope were more likely to be seropositive . Sixty-three percent (5/8) of historical puma samples from the Western Slope had detectable plague antibodies, similar to the seroprevalence rate of contemporary puma samples from this region (46.03%). Season also played a role, and spring-captured animals were more likely to be seropositive .\n\n【10】Colorado sample sites showed 51 (38%) positive of 135 animals tested. Seroprevalence rates in the Colorado sample areas were 21% (Front Range) and 46% (Western Slope) respectively, a higher proportion than expected given the severe disease seen in plague infections in some domestic cats . California sample sites had limited plague seroreactivity, with only 4 (2.2%) of 181 animals positive for plague exposure.\n\n【11】The Colorado Western Slope is near the Four Corners region (i.e. contiguous boundaries of southwestern Colorado, northwestern New Mexico, northeastern Arizona, and southeastern Utah). During 1957–2004, a total of 419 human plague cases were documented in the United States, of which 83% were from this region . The complex dynamics governing high plague incidence in this region are not fully understood despite extensive research but most likely involve climate, mammalian reservoirs, vector species, and habitat ecotypes .\n\n【12】### Conclusions\n\n【13】Plague dynamics often are characterized by epizootics, resulting in interannual variation in infection rates among plague hosts; however, seroprevalence of 8 puma samples collected in the 1980s mirrored contemporary samples collected since 2002 and may indicate high levels of sustained plague activity in the area in this species. Seroprevalence rates were similar across multiple sample years. Vector-borne disease often is highly seasonal because of annual shifts in vector activities and abundance ; however, seasonal patterns based on serologic data must be interpreted with caution because of long-term antibody persistence in some recaptured animals .\n\n【14】Puma and bobcat data from this study suggest exposure followed by recovery. All animals were outwardly healthy. Deaths caused by plague have been documented in wild felids , and the potential for plague exposure remains a concern for field biologists, veterinarians, hunters, and skinners. Field biosafety guidelines have been developed in conjunction with Colorado State University’s Biosafety Office as a result of these findings. Recommendations include wearing disposable gloves, long pants, and long-sleeved shirts when handling anesthetized animals and using an N95-rated mask when conducting necropsies or handling deceased animals. Outside of human infections, plague could constitute a problem for felid conservation in areas of high plague activity .\n\n【15】Results suggest large numbers of _Y. pestis_ –exposed pumas and bobcats. Regular serosurveys that document seroreactivity increases above an original baseline could indicate epizootic activity in felids and other plague hosts. High regional seroprevalence indicate these animals may be involved in the persistence and transmission of _Y. pestis_ . This and the documented transmission of plague from nondomestic carnivores to humans  emphasize the need to better understand the role of wild felids in plague dynamics.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5844b386-aa59-4159-b606-f2646503d5cf", "title": "Surveillance for Ebola Virus in Wildlife, Thailand", "text": "【0】Surveillance for Ebola Virus in Wildlife, Thailand\n**To the Editor:** Active surveillance for zoonotic pathogens in wildlife is particularly critical when the pathogen has the potential to cause a large-scale outbreak. The recent outbreak of Ebola virus (EBOV) disease in West Africa in 2014 was initiated by a single spillover event, followed by human-to-human transmission . Projection of filovirus ecologic niches suggests possible areas of distribution in Southeast Asia . Reston virus was discovered in macaques exported from the Philippines to the United States in 1989 and in sick domestic pigs in the Philippines in 2008 (with asymptomatic infection in humans) . Dead insectivorous bats in Europe were found to be infected by a filovirus, similar to other members of the genus _Ebolavirus_ .\n\n【1】Although EBOV has historically been viewed as a virus from Africa, recent studies found that bat populations in Bangladesh and China contain antibodies against EBOV and Reston virus recombinant proteins, which suggests that EBOVs are widely distributed throughout Asia . Thus, an outbreak in Asian countries free of EBOV diseases may not only be caused by importation of infected humans and/or wildlife from Africa but may arise from in-country filovirus–infected wildlife. Serologic and molecular evidence for filoviruses suggests that members of the order Chiroptera (bats) may be their natural reservoir .\n\n【2】As part of a proactive biosurveillance program, we conducted a cross-sectional study for EBOV infection in bats and macaques in Thailand. We screened 500 _Pteropus lylei_ bats collected from 10 roosting sites during March–June 2014  for antibodies against EBOV antigen by using an ELISA validated by the Centers for Disease Control and Prevention (Atlanta, GA, USA) .\n\n【3】Bats and macaques were captured with permission from the Department of National Parks, Wildlife and Plant Conservation. The Institutional Animal Care and Use Committee at the University of California, Davis (protocol #16048) approved the capture and sample collection protocols.\n\n【4】To further screen a wide range of wildlife species in Thailand for active EBOV infection, we sampled and tested 699 healthy bats, representing 26 species, and 50 long-tailed macaques ( _Macaca fascicularis_ ). Additional bat species were randomly captured (≥50/site) in 6 provinces in Thailand during 2011–2013 and identified by morphologic traits. Macaques were captured and sampled in March 2013 from 1 site at Khao Chakan, Srakaeo Province, and released at the same site. Blood, saliva, urine, and feces were collected from anesthetized macaques or nonanesthetized bats. All animals were released after sample collection. Details on species screened, sample sizes, and trapping localities are provided in the Table .\n\n【5】All nonblood specimens were collected in nucleic acid extraction buffer (lysis buffer) and transported on ice to the World Health Organization Collaborating Centre for Research and Training on Viral Zoonoses laboratory (Bangkok, Thailand) for storage and testing. Three types of specimen (saliva, urine, and serum) were collected from individual animals and pooled.\n\n【6】Nucleic acid was then extracted with NucliSENS easyMAG (bioMérieux, Boxtel, the Netherlands) and analyzed by reverse transcription PCR (RT-PCR). A consensus RT-PCR was used to screen for all known species of Ebola virus and Marburg virus, including EBOV . In total, 5 RT-PCRs were performed on each specimen, a regimen that included 4 sets of primers specific to known filoviruses and 1 degenerate primer set to detect novel viruses in this family. The sensitivity of RT-PCR on synthetic standard was 50–500 copies/reaction . We ran 3,745 PCRs, covering a range of assays, to increase detection sensitivity. All specimens examined were negative for filoviruses by EBOV ELISA and PCR . For _P. lylei_ ELISA screening, optical density values for all 500 bats ranged from 0.000 to 0.095, well below the potential positive cutoff value of 0.2.\n\n【7】Assuming a population size of ≈5,000 bats/roost and a sample size of 50 bats/site, we have 95% confidence that if >6% of the population had antibodies against EBOV antigen, we would have detected it. If we assume that all 500 animals are part of 1 large panmictic population, and we have 95% confidence that if EBOV were circulating in >0.5% of the population, we would have detected it. Therefore, although we cannot rule out infection of this species with 100% confidence, _P. lylei_ bats, the most abundant species of large pteropid bats in Thailand, are highly unlikely to be reservoirs for EBOV.\n\n【8】Our sample sizes for PCR screening of other bat species in this study were much smaller, and we had no supported serologic data, but these negative results could add to the knowledge of filovirus infection in nontissue specimens from healthy bats. Previous studies have detected Ebola virus–like filovirus RNA in lung tissue of healthy _Rousettus leschenaultia_ bats in China  and from organs and throat and rectal swab specimens from a die-off of _Miniopterus schreibersii_ bats in Spain . In our study, which included 22 _M. schreibersii_ and 132 _M. magnate_ bats, none of the bats tested positive for filoviruses.One limitation of the cross-sectional sampling strategy used here, however, is that PCR-negative findings do not necessarily mean that the bats were not infected in the past. Although we found no evidence of filovirus infection in wildlife species tested in Thailand, we believe that continuing targeted surveillance in wildlife should enable early detection and preparedness to preempt emerging zoonoses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2d6d308f-1589-4e0b-84b2-4c3fe7f38e41", "title": ": Why Guidelines Differ", "text": "【0】: Why Guidelines Differ\nNumerous international, national, and professional organizations publish guidelines and recommendations for travelers; CDC’s Yellow Book is but one example. Travel health providers should be aware of these recommendations, even though they might not follow them in every instance. Through awareness, travel health providers can explain to their patients how their recommendations, and their patients’ choices, might be discrepant with what others recommend. It can be unsettling for patients to receive travel medicine advice, vaccines, or an antimalarial drug prescription from a provider, only to find that the advice and prescriptions are contradicted by what other professionals, friends, or destination-country nationals have to say. The skillful travel health provider will be able to help the traveler reconcile seemingly conflicting advice, and travelers will be reassured when providers explain why these differences exist.\n\n【1】How Guidelines are Created\n\n【2】In the United States, the Food and Drug Administration (FDA) approves standards for how to use a vaccine or medication, including dosages, ages for which the product is approved, and booster recommendations. Guidance about when to use a product can come from a separate body (e.g. the Advisory Committee on Immunization Practices \\[ACIP\\]). To give ACIP the best possible information on which to base their recommendations, working groups of experts hold meetings to review the literature and new studies.\n\n【3】International bodies (e.g. the World Health Organization \\[WHO\\]), national committees of other countries, and medical organizations (e.g. the International Society of Travel Medicine and the Infectious Diseases Society of America), also promote their own guidance. Other professional organizations might create consensus clinical practice guidelines based on published medical literature and expert opinion. Travel medicine–specific paid subscription services employ travel medicine experts to organize and present guidelines for health care providers who see international travelers in their practice but who might lack expertise in the subject. Guidance about vaccinations and malaria prophylaxis developed by these organizations and subscription services can differ from CDC advice. Reasons for this are varied and include differences in product availability, licensure standards, cultural perceptions of risk, and opinions among experts, as well as lack of definitive evidence.\n\n【4】### Availability of Products\n\n【5】Travel health providers can only use the products available to them. Availability is determined by the regulatory approval status of the product and, to a lesser extent, the marketing and distribution plan of the manufacturer. Regulatory approval processes vary greatly by country. For example, registering a new vaccine or antimalarial drug in the United States is a costly and rigorous process. If the market in a particular country is insufficient to justify the expense of registration, a commercial company might not seek it.\n\n【6】### Licensure Standards\n\n【7】Licensure standards also vary. What might be sufficient for one regulatory authority might not suffice for another. For example, primaquine, an option for malaria prophylaxis in the United States, is not registered or commercially available in Switzerland. Atovaquone-proguanil also was available for malaria prophylaxis in the United States before many other countries. In another example, 4 Japanese encephalitis vaccines are available in the world, but only 1 is licensed in the United States.\n\n【8】### Differences in Data Interpretation\n\n【9】Even when the same products are available, recommendations for use might not be the same in all countries. The injectable Ty21a typhoid vaccine and the oral typhoid vaccine are examples. In the United States, a booster of the injectable Ty21a vaccine is recommended after 2 years, but in most European countries, a booster is recommended after 3 years. In the United States, health providers dispense a packet of 4 oral typhoid vaccine capsules, whereas in Europe, 3 doses are considered adequate. The regulatory agencies might have reviewed the same data and drawn different conclusions, or they might have reviewed different data at separate times. Regulatory submissions to various agencies rarely occur at the same time. Therefore, for legitimate reasons, the data available for review by each agency might not be the same.\n\n【10】### Perception of Risk\n\n【11】People from varying backgrounds can view the same risk data and come to very different conclusions regarding the costs and benefits of minimizing risk to what they consider to be an acceptable level. For example, recommendations to prevent malaria during travel to India vary widely. Germany does not recommend using malaria prophylaxis for any travel to an Indian destination; standby emergency treatment or self-treatment are the only recommendations for identified risk destinations. Guidelines from the United Kingdom recommend only awareness and mosquito bite prevention for more than half the Indian subcontinent, including large cities and popular tourist destinations in the north and south, but suggest prophylaxis consideration for some travelers or for those visiting higher-risk areas. By contrast, CDC recommends malaria prophylaxis for all travelers to any Indian destination, except for some mountainous areas of northern states.\n\n【12】The Impact of Advice\n\n【13】The real question is not just which recommendations each country should adopt, but the possible impact of that advice. Because we do not usually have detailed data on the exact risk to travelers for different vaccine-preventable diseases at a given destination, immunization guidance and recommendations often are based on serologic studies or on the original studies, most often performed in local people, that led to licensing. For example, as noted above, most European countries recommend a booster for the injectable Ty21a typhoid vaccine after 3 years, whereas the United States recommends a booster at 2 years. This difference is based on the perception of falling antibody levels over time, and a decision about where the line of protection against disease falls. Both standards have been in effect for many years, but no current available evidence would lead someone to conclude that one regimen has had a different impact than the other.\n\n【14】An extensive literature review, conducted by the Canadian Committee to Advise on Travel Medicine and Travel, led the committee to limit the recommendation of typhoid vaccine for travelers to South Asia only. This recommendation has not been adopted by CDC or many other international advisory boards. Despite this, we have little evidence that Canadian travelers are experiencing more typhoid fever than their counterparts from other countries.\n\n【15】Similar conclusions can be drawn for malaria prophylaxis recommendations. If the guidance provided by one group or organization consistently resulted in more cases of malaria than another, the guidance likely would change. In the absence of that data, however, health professionals continue to use their best judgment, without much knowledge of the true impact of their advice.\n\n【16】Can We Harmonize Guidelines?\n\n【17】The complex nature of how health organizations obtain, evaluate, and verify data, combined with fundamental differences in risk perception, makes it likely that multiple, overlapping, and at times conflicting guidelines will continue to exist. However, conflicting guidelines have decreased in the past decade due to the efforts of several organizations. A recent example has been the collaboration among the WHO, US CDC, and the European Centre for Disease Prevention and Control to develop consistency and clarity in defining travel-associated risk for Zika infection so that providers can more clearly relay that information, particularly to those who are pregnant or planning pregnancy. In addition, more rapid and frequent communication via the internet and regularly held international conferences have narrowed the gaps between conflicting advice.\n\n【18】In summary, the role of the travel health provider is to understand the differences in guidelines, interpret this information, and convey tailored and informed advice in an assured and comforting manner to travelers. There are no absolute right or wrong answers for many existing travel health guidelines. Even with all the data available, recommendations often are based on expert opinion, which can vary.\n\n【19】The following authors contributed to the previous version of this chapter: David R. Shlim", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f21ba0ac-54b7-4b14-80c0-7204ec520144", "title": "Obesity and Associated Factors — Kingdom of Saudi Arabia, 2013", "text": "【0】Obesity and Associated Factors — Kingdom of Saudi Arabia, 2013\nAbstract\n\n【1】**Introduction**  \nData on obesity from the Kingdom of Saudi Arabia (KSA) are nonexistent, making it impossible to determine whether the efforts of the Saudi Ministry of Health are having an effect on obesity trends. To determine obesity prevalence and associated factors in the KSA, we conducted a national survey on chronic diseases and their risk factors.\n\n【2】**Methods**  \nWe interviewed 10,735 Saudis aged 15 years or older (51.1% women) through a multistage survey. Data on sociodemographic characteristics, health-related habits and behaviors, diet, physical activity, chronic diseases, access to and use of health care, and anthropometric measurements were collected through computer-assisted personal interviews. We first compared sociodemographic factors and body mass index between men and women. Next, we conducted a sex-specific analysis for obesity and its associated factors using backward elimination multivariate logistic regression models. We used SAS 9.3 for the statistical analyses and to account for the complex sampling design.\n\n【3】**Results**  \nOf the 10,735 participants evaluated, 28.7% were obese (body mass index ≥30 kg/m 2  ). Prevalence of obesity was higher among women (33.5% vs 24.1%). Among men, obesity was associated with marital status, diet, physical activity, diagnoses of diabetes and hypercholesterolemia, and hypertension. Among women, obesity was associated with marital status, education, history of chronic conditions, and hypertension.\n\n【4】**Conclusion**  \nObesity remains strongly associated with diabetes, hypercholesterolemia, and hypertension in the KSA, although the epidemic’s characteristics differ between men and women.\n\n【5】Introduction\n\n【6】Obesity is a major risk factor for illness and death. It is associated with diabetes, hypertension, hyperlipidemia, obstructive sleep apnea, and osteoarthritis . With the increase in life expectancy, obesity is causing more years of disability . Hence, the increased cost of obesity and its sequelae will put a strain on the resources of governments and individuals .\n\n【7】In many developing societies, the adoption of a Western lifestyle, characterized by decreased physical activity and high caloric intake, is contributing to an alarming epidemiological transition marked by the shift in the leading causes of death from communicable to noncommunicable diseases . The recognition of the role of elevated body mass index (BMI) in these changes has made obesity a high priority for health authorities around the world .\n\n【8】Socially, the perception of obesity has changed over time. Whereas it once was associated with wealth and prosperity for men and with health and reproduction abilities for women, it is now perceived as a health problem and a risk factor for many diseases . Women, the poor, and older people are at higher risk of obesity worldwide .\n\n【9】The _Global Burden of Disease 2010_ study found that elevated BMI was the leading risk factor for disability-adjusted life years in the Kingdom of Saudi Arabia (KSA) . Previous studies in KSA indicate an increasing trend in the prevalence of obesity. Data from the late 1980s through mid-1990s show a prevalence of obesity averaging about 20%, ranging from as low as 13.1% among men to as high as 26.6% among women. However, all prevalence estimates from 1995 and beyond are above 35% . The last national survey on obesity in the KSA and its associated risk factors was conducted in 2005 in collaboration with the World Health Organization. Since then, no such surveys have been conducted, making it impossible to determine whether the efforts of the Saudi Ministry of Health (SMOH) are affecting obesity trends. To determine current rates of obesity and associated risk factors and chronic conditions, the SMOH, in collaboration with the Institute for Health Metrics and Evaluation, conducted a large household survey in 2013. In this article, we report the findings from that survey.\n\n【10】Methods\n\n【11】The Saudi Health Information Survey (SHIS) was a national multistage survey of men and women aged 15 years or older. Households of Saudi citizens were randomly selected from a national sampling frame maintained and updated by the KSA Census Bureau. The Ministry of Health divides KSA into 13 health regions, each with its own health department . For the survey, we divided each region into subregions and blocks used by the KSA department of statistics. All regions were included in the survey, and a probability proportional to size was used to randomly select subregions and blocks. Households were randomly selected from each block. A roster of household members was created, and an adult aged 15 or older was randomly selected from that household to be surveyed. If the randomly selected adult was not present, our surveyors made an appointment to return, and a total of 3 visits were made before the household was considered as a nonresponse. Weight, height, and blood pressure of the randomly selected adult were measured at the household by a trained professional.\n\n【12】The survey included questions on sociodemographic characteristics, tobacco consumption, diet, physical activity, health care use, various health-related behaviors, and self-reported chronic conditions. These conditions were hypertension, diabetes, hypercholesterolemia, stroke, cardiac arrest, myocardial infarction, congestive heart failure, chronic obstructive pulmonary disease, atrial fibrillation, renal failure, asthma, and cancer.\n\n【13】We used measured weight and height to calculate BMI (kg/m 2  ). Participants were classified into 4 groups: underweight (BMI <18.5), normal weight (BMI 18.5 –24.9), overweight (BMI 25.0–29.9), or obese (BMI ≥30.0) . Respondents were considered to be smokers if they reported currently smoking. We computed the number of servings of fruits, vegetables, red meat, and chicken consumed per day from a detailed dietary questionnaire as the sum of the average daily consumption of fruits, fruit juices, vegetables, red meat, and chicken. We used the International Physical Activity questionnaire  to classify respondents into 4 groups of physical activity: 1) vigorous, 2) moderate, 3) insufficient, and 4) none.\n\n【14】To assess diagnosed blood pressure, diabetes, and hypercholesterolemia status, respondents were asked 3 separate questions: “Have you ever been told by a doctor, nurse, or other health professional that you had: 1) hypertension, otherwise known as high blood pressure; 2) diabetes mellitus, otherwise known as diabetes, sugar diabetes, high blood glucose, or high blood sugar; 3) hypercholesterolemia, otherwise known as high or abnormal blood cholesterol?” Women diagnosed with diabetes or hypertension during pregnancy were not counted as having these conditions. Those who reported diagnosis of any of these conditions were asked if they were currently receiving any treatment for their condition. Similarly, the same question format was used to determine previous diagnosis of stroke, myocardial infarction, atrial fibrillation, cardiac arrest, congestive heart failure, chronic obstructive pulmonary disease, asthma, renal failure, and cancer.\n\n【15】Three blood measurements were taken with the participant resting and at 5-minute intervals. We followed the National Health and Nutrition Examination Survey procedures for editing and determining blood pressure levels . Briefly, if only 1 reading was available for systolic blood pressure, this reading was used. If 2 readings were available, the second reading was taken into account. If 3 readings were available, the average of the last 2 readings was considered the systolic blood pressure. The same method was used for calculating diastolic blood pressure. Respondents were considered to have elevated blood pressure if they met any of the following criteria: 1) measured diastolic or systolic blood pressure exceeding 90 or 140 mm Hg, respectively, or 2) measured diastolic or systolic blood pressure not exceeding the appropriate threshold but the respondent reporting taking medications for blood pressure.\n\n【16】We compared sociodemographic characteristics and BMI categories between men and women by using a multivariate logistic regression model. We used a backward elimination multivariate logistic regression model to measure association between obesity and related factors separately for men and women. All factors were included in the model. Variables were eliminated based on a Wald χ 2  test for analysis of effect. Variables were removed one by one based on the significance level of their effect on the model, starting with the variable with the highest _P_ value greater than 0.5, until all variables remaining had a value of _P_ ≤ .50 in the analysis of effect. The logistic regression excluded cases with missing data. Of the 10,735 completed interviews, we excluded 398 observations that were missing obesity values, 141 missing blood pressure, 266 missing self-reported hypercholesterolemia status, 115 missing self-reported diabetes status, 32 missing self-reported chronic conditions, 212 missing fruits and vegetables consumption, 173 missing meat and chicken consumption, 29 missing smoking status, 33 missing marital status, and 20 missing educational level. In total, 10,293 observations were used in our regression analyses. Data were weighted to account for the probability of selection and age and sex poststratification based on census data for age and sex distribution of the Saudi population. We used SAS 9.3 (SAS Corp) for the analyses and to account for the complex sampling design.\n\n【17】Results\n\n【18】From April through June 2013, we contacted a total of 12,000 households, and 10,735 participants completed the survey (response rate 89.4%).\n\n【19】Compared with male respondents, female respondents were younger, more likely to be married or previously married, less educated, and overweight or obese. Overall, 28.7%, or 3.6 million, Saudis aged 15 years or older were obese. This prevalence ranged from 24.1% among men to 33.5% among women . Both men and women consumed low amounts of fruits and vegetables (more than 81.0% of men and women consumed fewer than 3 servings of fruits and vegetables per day) and most were physically inactive (46.0% of men, and 75.1% of women practiced low to no physical activity at all).\n\n【20】Risk of obesity was lower among men who reported high levels of physical activity than among inactive men . Men who were previously married, consumed 3 or more servings of meat per day, those who were previously diagnosed with diabetes or hypercholesterolemia, and those who had elevated blood pressure were more likely to be obese.\n\n【21】Among women, risk of obesity increased with age, being married or previously married compared with those never married, having been diagnosed with a chronic condition, and being prehypertensive or hypertensive . Women who had more education than high school were less likely to be obese than those who had a primary school educational level or less. Diet indicators and physical activity were not significantly associated with obesity among women.\n\n【22】Discussion\n\n【23】Our study indicated high rates of obesity across all segments of the population in KSA. Our findings showed that most Saudis are physically inactive and consume low levels of fruits and vegetables. Furthermore, we found strong associations between obesity and diabetes, hypercholesterolemia, and hypertension.\n\n【24】Obesity levels in our study are lower than those reported in 2005  and those estimated for 2010 . Our survey had similar sampling frame and methods to the 2005 survey. When comparing our survey with the 2005 survey, we restricted the analyses to the same age groups. Obesity decreased by 4.4% for men and 10.7% for women between 2005 and 2013 . An increase of 7.8% for men and 9.0% for women in normal weight was observed during the same time period. A shift may have occurred from obese to overweight and from overweight to normal weight or from obese to normal weight. Conversely, the change in obesity prevalence differed significantly between age groups for both sexes . Whereas fewer men younger than 35 years were obese in 2013, older men had a higher prevalence of obesity. Obesity prevalence decreased for women of all age groups, except for those aged 55 to 64 years, among whom the prevalence of obesity increased by almost 10%. This overall decline in obesity prevalence is welcome news for the Saudis’ health, especially because there have been few success stories in decreasing obesity levels in other countries resulting from health interventions and environmental and policy changes . Over the last decade, the SMOH has implemented several public health programs to reduce obesity. Most of these programs have focused on awareness and behavioral changes . However, it is too early to determine whether the decline seen in this study is due to chance and thus cannot be sustained.\n\n【25】Associations found in our study are similar to findings from previous studies, with varying results between men and women. The association between marital status and obesity has been previously cited . Despite being generally in better health than their counterparts, married men and women were found to be at higher risks of obesity than those who were unmarried. Among Saudis, risk of obesity was higher in married than in never-married men. Never-married women, as well as those who were separated, divorced, or widowed, were less likely to be obese than married women.\n\n【26】Obesity is higher among less-educated men and women . In the United States, as education levels increased, obesity levels declined among women but remained consistent among men. We observed the same pattern in KSA. With higher education, Saudi women may be making healthier choices that reflect on their body composition. Indeed, more educated women tend to improve their health profile and those of their children .\n\n【27】Smoking is associated with lower weight . We did not see this pattern in KSA. However, smoking rates among women in KSA were low. Similarly, fruit and vegetable consumption were not associated with obesity in our study. These findings differ from those reported elsewhere, because an increased daily consumption of fruits and vegetables is often associated with a decreased risk of obesity . Increased meat consumption, on the other hand, was associated with an increased risk of obesity among men. One explanation is that our participants might have misreported their fruit and vegetable intakes. Another explanation is that as Saudis increased their consumption of fruits and vegetables, they did not account for their total caloric intake.\n\n【28】Physical activity reduces the likelihood of obesity . A strong association between obesity and lack of physical activity has been shown among Saudi adolescents . This association was true among men, but not women, in our study. Whereas men were almost equally distributed between all levels of physical activity, most women reported being inactive or having a low level of physical activity.\n\n【29】Our findings about the lack of association between obesity and fruit and vegetable consumption and physical activity differ from those of previous studies. In KSA, the consumption of fruits and vegetables was similar among those who were obese and those who were not. This finding indicates that the patterns of fruit and vegetable consumption and physical activity are similar across women in KSA, making the effect of these factors undetectable in our analysis. A caloric imbalance, which we did not capture in our survey, may have been the driving factor of the epidemic.\n\n【30】The association we found between obesity and chronic noncommunicable diseases among Saudis is informative on the impact of obesity on chronic diseases in KSA. Diabetes, hypercholesterolemia, and elevated blood pressure were strongly associated with obesity in our study. With the declining rates of obesity over the last 8 years in KSA, we would expect an effect on these conditions in the Kingdom in the coming years. Although social disparities associated with obesity are less evident in KSA, and although the burden of obesity has decreased, its health consequences have not diminished. Saudis who had elevated blood pressure, hypercholesterolemia, or diabetes were more likely to be obese than healthier subjects. The global obesity epidemic has been strongly linked with these diseases . In 2010, hypertension was the second leading risk for death worldwide , and diabetes caused more than 1,200,000 deaths, compared with fewer than 600,000 in 1990 . Today, noncommunicable diseases are the major cause of death, illness, and disability. The success of controlling infectious diseases increased lifespans; however, this longer life expectancy comes at a cost. Specifically, in the Arab world, people live longer but not healthier lives . Hypertension and diabetes are contributing extensively to these unhealthy years lived with disability, increasing health expenditures and reducing quality of life.\n\n【31】Our study has some limitations. First, our data are cross-sectional, so we cannot assess causality in associations. Second, many of our behavioral data, such as diet and physical activity, are self-reported, and subject to recall and social desirability biases. Third, our dietary instrument was not designed to provide total caloric intake, so we were not able to control for the effect of specific food items while controlling for caloric intake. It also captures little information on bread and does not allow the assessment of grain consumption, which would have been an important nutrient to account for. Also, chronic conditions were self-reported, which leaves out respondents who might have these conditions but have not been previously diagnosed. Conversely, our study is based on a large sample size and used standardized methods for all its measures.\n\n【32】Although we focused our study on obesity as an extreme measure of excess weight, overweight should also be a target for prevention. Intervening with overweight Saudis before reaching the obesity level would be easier and more beneficial, especially if they have not yet developed adverse health events.\n\n【33】Our findings indicate that the epidemic of obesity in KSA may be leveling or decreasing. Obesity is a major risk factor for illness and death in KSA and has high associated health costs. Our findings point to the need for continued efforts to ensure a steady decline in obesity prevalence in KSA. For example, identified high risk groups, such as older, uneducated, and unmarried women, and unmarried and inactive men, should be targeted for obesity prevention programs. Saudis should be encouraged to follow a healthier diet by increasing consumption of fruits and vegetables and exercising.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "efb8d326-2186-4aef-8de4-12bdb2526132", "title": "West Nile Virus–associated Flaccid Paralysis", "text": "【0】West Nile Virus–associated Flaccid Paralysis\nAcute paralysis associated with West Nile virus (WNV) infection  has been attributed to Guillain-Barré syndrome , a poliomyelitislike syndrome , and a generalized myeloradiculitis . Several reports have described acute respiratory failure occurring with WNV-associated paralysis . However, the frequency of acute paralysis in WNV neuroinvasive disease remains unknown, and the clinical features of WNV-associated respiratory weakness have not been characterized.\n\n【1】During 2003, Colorado experienced an epidemic of human WNV disease; 2,947 cases were reported to the US Centers for Disease Control and Prevention (CDC) that included 621 neuroinvasive cases and 63 deaths. Acute paralysis was seen in many patients, and in several, acute respiratory failure developed that required emergent intubation. We conducted a population-based assessment of WNV-infected persons in whom acute paralysis developed to describe the clinical features, mechanisms, and short-term outcomes.\n\n【2】### Methods\n\n【3】During the summer and fall of 2003, we identified patients with acute weakness and WNV infection from among the populations of Boulder, Larimer, and Weld counties (combined population ≈724,000) in northern Colorado by active case-finding. We were notified of suspected cases by infection control practitioners and health professionals at 8 hospitals in and around the catchment area and through ongoing state-based surveillance. A suspected case of WNV-associated weakness was defined as muscle weakness in a person of at least 1-point decrement on manual muscle testing by using the Medical Research Council (London, UK) 1–5 scale , respiratory failure requiring intubation that developed <48 hours after presentation, or both. All patients had IgM antibodies detected in acute-phase serum samples by IgM antibody-capture enzyme-linked immunosorbent assay at the Colorado Department of Health Services and Environment . Remaining acute-phase serum samples from 26 patients were tested by plaque-reduction neutralization assays for antibodies to WNV and St. Louis encephalitis virus at CDC . All had WNV-specific neutralizing antibody titers ≥1:10, which were at least 4-fold greater than those for St. Louis encephalitis virus.\n\n【4】Patients were approached under the auspices of a public health event, and oral consent was obtained. Results of an initial neurologic examination were recorded, and standardized demographic, clinical, and medical history data were obtained from patient interviews and medical records. Results of serial neurologic examinations were documented on a standardized form.\n\n【5】Four months after initial assessment, we repeated the neurologic examinations, and patients or family members completed a self-administered questionnaire that gathered information on functioning in daily activities. Strength scores at 14 locations  were evaluated at nadir and followup by using manual muscle testing scores. A proportional odds model for the strength scores was used to evaluate improvement; anticipated correlation was incorporated by estimating model parameters with generalized estimating equations. Within-patient correlations of the scores were estimated, and all pairwise differences were evaluated for significance by using bootstrap methods ; adjustment for multiple comparisons was made by using the Bonferroni adjustment. Statistical analyses were performed with SAS version 8.2 (SAS Institute, Cary, NC, USA), S-Plus version 6.2 (Insightful Corp. Seattle, WA, USA), Sudaan version 8.0.2 (Research Triangle Institute, Research Triangle Park, NC, USA), and EpiInfo version 6.04d (CDC, Atlanta, GA, USA).\n\n【6】### Results\n\n【7】Two hundred nineteen cases of WNV neuroinvasive disease were identified by state-based surveillance in our catchment area; among these, we identified 32 patients with acute paralysis and WNV infection. Eighteen (56%) were male; the median age was 56 years (range 15–84). All but 1 were Caucasian, and 3 (9%) were Hispanic. Sixteen patients (50%) had concomitant encephalitis, 10 (31%) had meningitis, and 6 (19%) had paralysis alone. Those 26 patients with concomitant meningitis or encephalitis represented 12% of the patients identified as having neuroinvasive disease.\n\n【8】Twenty-nine patients were examined by study neurologists within 7 days of weakness onset; 2 patients were evaluated on days 11 and 18 after weakness onset, respectively, and initial neurologic findings for 1 patient were obtained by personal communication (T. Clark, Colorado Pulmonary Associates, Denver, CO, USA). All but 1 of the patients were hospitalized. The median length of stay in the hospital was 17 days (range 2–87 days). Five patients (16%) with encephalitis were immunocompromised: 2 had prior liver transplants, 2 had hematologic malignancies, and 1 was receiving immunosuppressive medication for rheumatoid arthritis. One patient had insulin-dependent diabetes mellitus. Twenty-six (81%) had no prior medical problems.\n\n【9】Twenty-seven patients (84%) had asymmetric weakness consistent with a poliomyelitislike syndrome, 4 (13%) had symmetric ascending weakness with sensory abnormalities consistent with the acute inflammatory demyelinating polyradiculoneuropathy form of Guillain-Barré syndrome, and 1 had scapular winging and shoulder abduction weakness consistent with a long thoracic nerve paralysis.\n\n【10】##### Poliomyelitislike Syndrome\n\n【11】The incidence of poliomyelitislike syndrome was 3.7/100,000. Associated signs and symptoms are shown in Table 1 . Two patients had weakness in the absence of other systemic features of infectious illness. Among 25 patients with systemic signs or symptoms listed in Table 1 , including 4 in which weakness was concurrent with illness onset, the median interval between illness onset and weakness onset was 3 days (range 0–18). All but 1 patient had other neurologic features suggestive of acute WNV infection . Patterns of weakness  included acute monoplegia (weakness or paralysis of 1 limb, n = 5); asymmetric upper (n = 1) or lower (n = 5) extremity weakness; and generalized asymmetric tetraplegia or quadriplegia (asymmetric weakness in ≥3 limbs, n = 16). Deep tendon reflexes in affected limbs were diminished or absent. Nineteen patients (70%) with poliomyelitislike syndrome had cranial nerve involvement, which included unilateral (n = 2) or bilateral (n = 8) facial paralysis, extraocular muscle weakness (n = 4), dysphagia (n = 13), dysarthria (n = 6), and vocal cord paralysis (n = 2). Limb weakness by strength testing or subjective patient interpretation progressed to its lowest point (nadir) within 24 hours in 24 patients (88%), with a range of <6 hours to 3 days. Two patients reported sensory deficits (subjective numbness or paresthesias), and 16 (60%) reported pain in affected limbs preceding onset of weakness. Paralysis due to WNV infection in 2 patients occurred exclusively in limbs with prior lower motor neuron dysfunction due to lumbar disc herniation (previously resolved with discectomy).\n\n【12】Electromyography/nerve conduction studies performed on 14 patients with poliomyelitislike syndrome suggested a motor axonopathy and/or an anterior horn cell process. Preservation of voluntary motor unit potentials was observed in all tested myotomes in 10 patients, and voluntary motor unit potentials were absent in some myotomes in 4 patients. Fourteen patients with poliomyelitislike syndrome underwent magnetic resonance imaging (MRI) of the brain (n = 6), spine (n = 2), or both (n = 6). Two displayed focal lesions of the basal ganglia, thalami, and brainstem bilaterally on T2- and diffusion-weighted sequences, and 4 (including 3 who did not undergo electromyography/nerve conduction studies) had signal abnormalities in the anterior cord and ventral roots on T2- and diffusion-weighted sequences, suggesting anterior horn cell involvement .\n\n【13】##### Guillain-Barré–like Syndrome\n\n【14】Four patients had symmetric, ascending weakness with sensory symptoms suggestive of the acute inflammatory demyelinating polyradiculoneuropathy form of Guillain-Barré syndrome . All 4 had symptoms listed in Table 1 before weakness onset (range 2–12 days), 2 had meningitis before onset of weakness, and 2 had tremors. Initial deep tendon flexes were diminished or absent in all patients. Pain in affected limbs preceded weakness in 2 patients. The median interval between weakness onset and nadir was longer in patients with Guillain-Barré–like syndrome (4 days) than those with poliomyelitislike syndrome (1 day) (p<0.005, Wilcoxon rank sum test). One patient with Guillain-Barré–like syndrome had facial diplegia.\n\n【15】Electromyography/nerve conduction studies were carried out in 3 patients with Guillain-Barré–like syndrome, and all showed findings consistent with a predominantly demyelinating sensorimotor neuropathy. In patients with Guillain-Barré–like syndrome, voluntary motor unit potentials were preserved in tested myotomes.\n\n【16】##### Brachial Plexus Neuropathy\n\n【17】One patient with meningitis exhibited apparent, isolated, long thoracic nerve weakness with scapular winging and shoulder abduction weakness but without additional limb involvement. This patient also had pain before onset of weakness.\n\n【18】##### Respiratory Involvement\n\n【19】Twelve patients (39%), including 1 patient with Guillain-Barré–like syndrome, developed acute respiratory weakness that required endotracheal intubation and eventual tracheostomy . In 10 patients, diaphragmatic paralysis, shown by hemidiaphragmatic elevation on chest radiograph, or pulmonary function test results with a restrictive pattern and retention of carbon dioxide, suggested neuromuscular respiratory failure. These studies were not available for the remaining 2 patients. Five additional patients with poliomyelitislike syndrome had dyspnea and showed evidence of diaphragmatic weakness by chest radiograph and pulmonary function test results, but were not intubated. Three patients died of respiratory failure 7, 27, and 35 days after intubation, after ventilatory support was withdrawn. All patients except the one with Guillain-Barré–like syndrome had encephalitis. Patients in whom dysarthria, dysphagia, or both developed were more likely to experience subsequent respiratory failure (odds ratio \\[OR\\] 62, p<0.0001, Fisher exact test). The interval between onset of these bulbar symptoms and intubation was ≤1 day in 9 of 12 patients (range <1–6 days). Facial nerve paralysis without other cranial nerve abnormalities was not associated with subsequent respiratory failure .\n\n【20】Immunocompromised patients and patients with encephalitis were more likely to experience respiratory failure . Pattern of limb weakness was not predictive of respiratory failure; 3 patients had initial upper extremity involvement, 4 had initial lower extremity involvement, and 5 had more diffuse generalized involvement before respiratory failure.\n\n【21】##### Cerebrospinal Fluid (CSF) Parameters\n\n【22】Lumbar puncture was performed on 30 patients. They had a median CSF leukocyte count of 108 cells/mm 3  (range 0–740 cells/mm 3  ), a median CSF protein level of 98 mg/dL (range 27–138 mg/dL), and a median CSF glucose level of 55 mg/dL (range 36–135 mg/dL). Pleocytosis was lymphocytic in 14 patients, neutrophilic in 13, and unknown in 3. Among recorded cell differentials (n = 27), neutrophils were predominant in CSF collected on or before the day of weakness onset (10/14, 71%). CSF collected after the time of weakness onset was more often lymphocytic (10/13, 77%, OR 8.3, p = 0.03). Three of 4 patients with Guillain-Barré–like syndrome had lumbar punctures; 2 had cytoalbuminologic dissociation (elevated protein levels without pleocytosis) commonly seen in Guillain-Barré syndrome, and 1 had pleocytosis with an elevated protein level. The median CSF leukocyte count and protein level (64 cells/mm 3  and 100 mg/dL, respectively) in patients with respiratory failure were not significantly different from those without respiratory failure (117 cells/mm 3  and 96 mg/dL, each p = 0.5, Wilcoxon rank sum test).\n\n【23】##### Short-term Outcomes\n\n【24】Three patients with poliomyelitislike syndrome died in the hospital after withdrawal of ventilatory support. Of the surviving patients, 13 were discharged and 15 (including 1 with Guillain-Barré–like syndrome) were admitted to long-term care facilities. The patient with long thoracic nerve palsy was not hospitalized. At 4 months, 2 patients with poliomyelitislike syndrome were lost to followup. Of 15 patients sent to long-term care facilities, 5 with poliomyelitislike syndrome and 1 with Guillain-Barré–like syndrome remained in these facilities, and the remaining 9 had been discharged (median length of stay 40 days, range 17–106). No persons initially discharged were readmitted. Of 21 patients employed or in school before illness, 9 were working full-time or part-time at the 4-month followup. Seventeen required continuing physical therapy.\n\n【25】Four of the 27 reassessed patients (patients 4, 18, 20, and 43), all with poliomyelitislike syndrome, showed almost no improvement in strength of involved muscles. Two patients with Guillain-Barré–like syndrome regained baseline strength. Although the remainder showed general improvement by the proportional odds model (p<0.001), none had returned to baseline status . In patients with poliomyelitislike syndrome undergoing electromyography studies, the absence of voluntary motor unit potentials was associated with lack of strength improvement in involved myotomes. Neurologic signs at 4 months are shown in Table 1 . Three patients (all with respiratory paralysis) were nonambulatory; 12 of 24 ambulatory patients, including 2 with Guillain-Barré–like syndrome, required a wheelchair (n = 4), walker (n = 3), or canes or crutches (n = 5).\n\n【26】Pairwise comparisons of the correlations among patients with poliomyelitislike syndrome showed that at both times, weakness was more likely to be of greater severity in comparable limbs across the body (e.g. both arms, both legs) than in limbs on the same side (e.g. right arm and leg); facial weakness was strongly correlated with ipsilateral arm weakness . Limbs improved distally to a greater extent than proximally.\n\n【27】Of 9 surviving patients who had respiratory failure, 2 remained intubated at 4 months, and 7 displayed various degrees of improvement ; however, none had returned to baseline strength or level of function. The median duration of intubation for the 7 extubated patients was 49 days (mean 66 days, range 21–135).\n\n【28】### Discussion\n\n【29】The overall incidence of WNV-associated paralysis in this population was 4.3/100,000, with an incidence of poliomyelitislike syndrome of 3.7/100,000, which is comparable to that of paralysis seen during epidemics of poliovirus disease . During 2003, 219 cases of neuroinvasive disease were reported in this 3-county area of Colorado , of whom we estimate that flaccid paralysis developed in 12%. A total of 2,773 cases of WNV neuroinvasive disease were reported in the United States during the 2003 epidemic ; ≈330 cases of WNV-associated neuromuscular weakness may have occurred in the United States during this period.\n\n【30】In contrast to persons typically considered at risk for WNV encephalitis alone, our patients were relatively young, with a third-quartile age of 61 years , and healthy, with >80% having no prior medical conditions. Many patients required prolonged hospitalization and time in rehabilitation facilities and had severe disabilities, which suggests that paralysis caused by WNV infection is associated with considerable lost productivity and incurred healthcare costs. Thus, the long-term economic impact of WNV paralysis needs assessment.\n\n【31】Twenty-seven patients with WNV-associated paralysis had electrophysiologic or neuroimaging evidence of a poliomyelitislike syndrome or a clinical syndrome compatible with this diagnosis. These patients had asymmetric, acute weakness, pleocytosis, and in 14 patients with studies performed, results of electromyography/nerve conduction studies were consistent with motor axonopathy, anterior horn cell involvement, or both. However, 4 patients had clinical, electrophysiologic, and laboratory findings more suggestive of a Guillain-Barré–like syndrome. These patients had characteristic bilateral, symmetric, ascending weakness with associated sensory symptoms, and 2 of 3 patients with available CSF data showed characteristic cytoalbuminologic dissociation. The 3 patients who underwent electrodiagnostic studies had findings consistent with a demyelinating sensorimotor neuropathy typical of Guillain-Barré–like syndrome . Further electrophysiologic and pathologic characterization of this syndrome is needed because its management may differ from that of poliomyelitis.\n\n【32】Although WNV-associated weakness may occur without other findings suggesting acute WNV disease , all but 6 of our patients had meningitis or encephalitis and displayed other WNV-associated neurologic signs, including tremors, myoclonus, and parkinsonism . A neutrophilic, rather than the more typical lymphocytic, pleocytosis may be seen soon after onset of WNV disease  or other viral infections of the central nervous system . Our patients who had CSF obtained on the day of or shortly after weakness onset were more likely to have a neutrophilic pleocytosis than those with CSF obtained later in their illness. Clinicians should recognize the potential for a neutrophilic predominance in CSF obtained early in the course of WNV neuroinvasive disease.\n\n【33】A generalized asymmetric tetraplegia or quadriplegia was the most common weakness pattern, followed by monoplegia. Consistent with previous reports , all patients except 2 with Guillain-Barré–like syndrome demonstrated continued weakness at 4 months, although nearly all had some improvement in strength as indicated by manual muscle testing scores. All 10 patients with a poliomyelitislike illness who had even minimal preservation of motor unit potentials on initial electromyogram improved in strength in associated myotomes at 4 months; 4 patients with no motor unit potentials on initial electromyogram did not improve in strength in these myotomes. Complete destruction of large spinal motor neurons correlating with a completely paralyzed muscle has been observed in poliovirus-associated paralysis . Absence of motor unit potentials on electromyography may reflect this loss and may have prognostic value for future strength recovery. However, independent predictors of strength outcome remain unknown.\n\n【34】The facial weakness observed in 40% of patients with poliomyelitislike syndrome and in 1 patient with Guillain-Barré–like syndrome nearly or completely resolved, which is consistent with observations of patients with poliovirus disease and suggests a favorable outcome of this manifestation . Weakness was more severe in congruent limbs across the body than in ipsilateral limbs, and facial weakness was associated more strongly with arm than with leg weakness, which is consistent with the patchy focal cell destruction demonstrated pathologically in WNV poliomyelitis . Improvement in limb strength tended to occur distally to proximally.\n\n【35】Thirty-eight percent of the patients, including all 3 who died, had respiratory failure requiring intubation, and 16% of the patients had dyspnea and diagnostic evidence of neuromuscular respiratory failure but were not intubated. Respiratory failure has been described with WNV-associated paralysis  but has not been characterized in detail. In 1 patient with Guillain-Barré–like syndrome, respiratory failure developed, a common complication of this syndrome . However, the association with limb weakness of a motor neuron or anterior horn cell type, presence of an elevated hemidiaphragm on chest radiograph, and a restrictive pattern of respiratory failure by pulmonary function testing suggest a central, poliomyelitislike etiology for respiratory failure in all other patients. This etiology is supported by the electrodiagnostic findings of lower motor neuron involvement in affected limbs in 5 patients with respiratory failure. Additionally, histopathologic findings in patients with respiratory weakness and WNV infection have demonstrated neuronophagia and leukocytic inflammation of the dorsal motor nuclei of the vagus and glossopharyngeal nerves, which is similar to that seen in the spinal anterior horns . Disease due to poliovirus infection has been associated with diaphragmatic, intercostal muscle, and bulbar weakness with respiratory failure , and poliomyelitislike respiratory insufficiency has been described in infections with other flaviviruses .\n\n【36】Lower bulbar dysfunction, specifically dysarthria and dysphagia, was more frequent in patients with respiratory failure; in most, lower bulbar dysfunction followed or was concurrent with limb weakness and preceded respiratory failure by less than a day. Patients with lower bulbar signs and acute limb paralysis require monitoring for respiratory failure. Facial paralysis was not associated with increased risk of respiratory failure, which possibly reflects the neuroanatomic separation of the involved cranial nerve nuclei. Although results of MRI are frequently reported as normal in patients with WNV neuroinvasive disease, detailed images of brainstem and cervical spine are frequently not obtained. Although 4 patients displayed spinal cord and ventral root lesions at involved levels, and 1 patient with respiratory failure had lower brainstem signal abnormalities, detailed brainstem and spinal cord images in our patients were generally not obtained. In patients with acute weakness and bulbar signs, MRI should include the lower brainstem and spinal cord. Although 2 patients remained intubated at 4 months, all other surviving patients with respiratory involvement were extubated, although duration of ventilatory support was often prolonged.\n\n【37】In summary, our findings suggest that involvement of WNV with anterior horn cells, which resulted in a poliomyelitislike syndrome, represents the most common underlying cause of paralysis with WNV infection. In the population assessed by our study, the incidence of paralysis was comparable to that seen during large epidemics of poliovirus infection. Respiratory involvement was a frequent and severe manifestation of this syndrome, with a high degree of illness and death. Thus, early and prominent dysarthria and dysphagia may be predictors of subsequent respiratory failure in this group.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9c27ea39-c127-4ff5-911e-8aa1040028d6", "title": "Spillover of Peste des Petits Ruminants Virus from Domestic to Wild Ruminants in the Serengeti Ecosystem, Tanzania", "text": "【0】Spillover of Peste des Petits Ruminants Virus from Domestic to Wild Ruminants in the Serengeti Ecosystem, Tanzania\nPeste des petits ruminants (PPR) is caused by peste des petits ruminants virus (PPRV), a member of the genus _Morbillivirus_ in the family _Paramyxoviridae_ , and primarily affects sheep and goats. Although PPRV can infect a wide range of domestic and nondomestic species, the disease has not been confirmed in free-ranging wildlife species in sub-Saharan Africa. Increased understanding of the epidemiology of PPRV infection in mixed species environments is urgently needed, especially because the virus range has apparently expanded in recent years, with associated social and economic effects of epidemics in areas where the disease had not been circulating, including programs for ongoing control .\n\n【1】PPR was first reported in northern Tanzania in 2008 and resulted from a southward spread from Kenya and Uganda by migrating livestock . No serologic and clinical reports of PPRV infection in wildlife occurred in sub-Saharan Africa during 2005–2013, although seropositivity was recorded in Uganda, Ethiopia, and other countries in West and Central Africa before this period . A study of serum samples collected from 331 wildlife 1–12 years of age in Tanzania, including in the Serengeti, Arusha, Katavi, and Tarangire National Parks and in the Ngorongoro Crater in the Ngorongoro Conservation Area (NCA), showed seronegative results for PPRV ; however, that study did not include animals from our study area. Another study of >500 serum samples from wildlife in northeastern Kenya, tested during 2008–2010 for the Somali Ecosystem Rinderpest Eradication Coordination Unit program, also showed seronegative results for PPRV . In both studies, samples taken in Kenya and Tanzania were banked samples collected opportunistically during other research activities in the study areas. These results indicated that wildlife were not being infected by PPRV in this region; however, because of the opportunistic sampling, more targeted surveillance was considered necessary to confirm this PPRV-seronegative status . Given the proposed global eradication of PPRV by 2030 , we sought to determine the role of wildlife as possible hosts and sentinels of PPRV infection in northern Tanzania.\n\n【2】### The Study\n\n【3】In June 2014, we investigated whether evidence could be found for PPRV infection in resident wildlife as a result of possible spillover from domestic animals (i.e. nomadic pastoral and agropastoral livestock), with whom they may share pasture and water resources in the Ngorongoro district within NCA . After obtaining ethical approval through the Animal Health and Welfare European Research Area Network; the Biotechnology and Biological Sciences Research Council; and Tanzania government departments responsible for research on wildlife and livestock, we collected serum samples from wildlife near resident livestock or from pastures and water sources shared with resident and nomadic livestock. Sampling was performed during the dry season, so only resident wildlife were sampled; migratory populations had already moved northwest from NCA. Both netting and chemical immobilization were used in 11 sampling sites, and samples were collected from 46 wild animals . After the animals were restrained, they were clinically examined, and age was determined on the basis of incisor tooth eruption. Whole blood samples were collected with and without anticoagulant, and eye and nasal swabs were taken.\n\n【4】Similar sets of blood, eye, and nasal samples were collected from 5 domestic sheep and 5 goats that were reportedly ill with suspected signs of PPR (2 herds) or opportunistically sampled (1 herd) in locations across the wildlife range, which included 1 resident livestock herd in a high-altitude location and 2 nomadic livestock herds in low-altitude locations. No positive results were obtained from small livestock sampled in the highlands of Ngorongoro Crater. Three sheep (1 young, 1 semi-adult, and 1 adult) from the Esieki plains in the northern part of NCA were fresh cases with PPRV-specific clinical signs; their eye and nasal swab samples tested positive for PPRV antigen by a lateral flow device  . Although the adult animals had been vaccinated in April 2013, lambs <4 months of age and those born after that month were unvaccinated, creating a window for PPRV infection in the vaccinated herd and indicating a vital need to vaccinate all kids and lambs immediately after weaning, when they lose protection by maternal antibodies.\n\n【5】Real-time reverse transcription PCR  confirmed PPRV infection in all 3 PPRV-positive sheep from the Esieki plains. In addition, a sample from 1 Grant’s gazelle in the Esieki plains was also positive (cycle threshold 34). Samples from 2 domestic young goats from Ngoile, NCA, were also positive (cycle threshold 32 and 37). Amplification of the PPR genome in gel-based PCR was possible only from swabs from the 3 clinically positive animals by using N-gene primers . The partial N-gene sequences available in GenBank for Africa through December 2014 were aligned and used for constructing a neighborhood-joining phylogenetic tree  that confirmed co-circulation of lineage II PPRV along with lineage III and IV in Tanzania . Recently, lineage II has been circulating in Central Africa . Possible incursion of lineage II from Central to East Africa, particularly to Tanzania, may have been overlooked because not all outbreaks are reported or investigated by viral genome sequencing.\n\n【6】In addition to infection among domestic livestock, detection of antibodies in blood samples by H c-ELISA (Biological Diagnostic Supplies Ltd. Ayrshire, UK)  showed PPRV seropositivity in all wildlife species and herds sampled across NCA except for Thomson’s gazelle, but only 1 animal of this species was sampled . Age-specific data for the buffalo and Grant’s gazelle showed seropositivity increasing with age  and included animals <6 months of age.\n\n【7】### Conclusions\n\n【8】Our findings provide evidence for PPRV infection in wildlife in East Africa. Recurring outbreaks in NCA in Tanzania confirm that PPRV, having recently emerged in this region, is likely now endemic in this area and is circulating among sheep, goats, and wildlife despite several rounds of mass vaccination. Most wild ruminant species and sampled subpopulations or herds sharing range with small livestock in NCA have been infected with PPRV, with the youngest wild animal confirmed antibody positive at ≈6 months of age, suggesting recent exposure.\n\n【9】Our sample represents resident wildlife in NCA and not migrating populations in the ecosystem. The positive result from a small resident herd of wildebeest near Olbalbal warrants closer examination of the PPRV status of migrating populations of wildebeest, Thomson’s gazelle, and topi (a type of antelope, _Damaliscus lunatus_ ), which moved out of the area during April–May 2014. The single Thomson’s gazelle sample is inconclusive. Age-specific data show that antibody prevalence rises with age, suggesting intermittent but regular exposure in the wildlife populations; however, circulation of the virus within and between the populations of each wildlife species is also possible.\n\n【10】The transmission and spread of PPRV appears to be considerable; high seroprevalence is observed at individual and herd levels, without all animals being infected, suggesting lower infective loads in the wildlife and a possibility that most infections could result from direct spillover of virus from infected livestock. The possibility of spillover infections is supported by the apparent absence of antibodies in the wildlife populations that have no contact with livestock .\n\n【11】Absence of clinical evidence in wildlife does not constitute evidence of absence of the disease. Antibodies were present in many wildlife we sampled, and the genome was present in 1 Grant’s gazelle in the Esieki plains, where ongoing outbreaks were confirmed in domestic sheep. Clinical infections caused by PPRV have been recorded often in captive gazelle ( _Gazella_ species)  in United Arab Emirates. Currently, no evidence of wildlife disease exists, but cases or carcasses might go unnoticed because of deaths from other causes and rapid removal of dead animals by scavengers. These findings confirm endemic PPRV in the Greater Serengeti Ecosystem and suggest that free-ranging wildlife are susceptible to infection and can act as sentinels of livestock disease but do not appear to be maintaining infection across their populations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "310a64c6-dff9-43d0-ae90-42750a7d1249", "title": "Widespread Bat White-Nose Syndrome Fungus, Northeastern China", "text": "【0】Widespread Bat White-Nose Syndrome Fungus, Northeastern China\n**To the Editor:** Emerging infectious diseases have caused catastrophic declines in wildlife populations, and the introductions of many pathogen have been linked to increases in global trade and travel . Mapping the distribution of pathogens is necessary to identify species and populations at risk and identify sources of pathogen spillover and introduction. Once pathogen distributions are known, management actions can be taken to reduce the risk for future global spread .\n\n【1】Bats with symptoms of white-nose syndrome (WNS) were first detected in the United States in 2006, and the disease has subsequently caused precipitous declines in temperate bat populations across eastern North America . _Pseudogymnoascus destructans_ , the causative agent of WNS, is a cold-growing fungus that infects bats’ skin during hibernation, leading to more frequent arousals from torpor and death . _P. destructans_ is widespread throughout Europe , but, to our knowledge, its presence in Asia has not been documented.\n\n【2】We sampled bats and hibernacula surfaces (cave walls and ceilings) across northeastern China during 2 visits (June–July 2014 and March 2015) using a previously described swab-sampling technique . Bats were captured inside caves and at their entrances. DNA was extracted from samples by using a modified QIAGEN DNeasy blood and tissue kit (QIAGEN, Valencia, CA, USA) and tested in duplicate for the presence of _P. destructans_ with a quantitative real-time PCR (qPCR) .\n\n【3】In the summer of 2014 and winter of 2015, we collected 385 samples from hibernacula surfaces at 12 sites in 3 provinces and 1 municipality  and 215 samples from 9 species of bats at 10 sites (summer: _Rhinolophus ferrumequinum, Rhinolophus pusillus, Myotis adversus, Myotis macrodactylus, Myotis pilosus, Myotis chinensis, Murina usseriensis_ ; winter: _R. ferrumequinum, Murina leucogaster, Myotis petax_ ). During the summer, _P. destructans_ was widely distributed across the study region with positive samples (determined on the basis of qPCR results) obtained from cave surfaces at 9 of 12 sites and from bats at 2 of the 9 sites where bats were sampled .\n\n【4】Prevalence of _P. destructans_ was low during summer in the environment (mean prevalence across sites 0.06 ± 0.03) and in bats. Bats of 3 species tested positive for _P. destructans_ in the summer: _M. macrodactylus_ (1/10), _M. chinensis_ (1/1), and _M. ussuriensis_ (1/1). _P. destructans_ was not detected in bats of 4 other species, of which >20 individual animals of each species were sampled ( _R. ferrumequinum_ , _R. pusillus_ , _M. pilosus_ , and _M. adversus_ ). The low prevalence of _P. destructans_ in bats and on hibernacula surfaces in China during the summer was similar to comparable results from studies in North America .\n\n【5】In winter, prevalence at the 2 sites we revisited was much higher; 75% of 85 samples from 3 species tested positive, including samples from 16/17 _M. petax_ bats. We also detected _P. destructans_ in bats from 2 additional species ( _R. ferrumequinum_ \\[11/19 bats\\] and _M. leucogaster_ \\[11/16 bats\\]).\n\n【6】In addition, during March 2015, we observed visual evidence of _P. destructans_ in bats ( _M. petax_ , panel C) and obtained 2 fungal cultures from swab specimens taken from these bats. To isolate _P. destructans_ from these samples, we plated swab specimens from visibly infected bats on Sabouraud dextrose agar at 10 **°** C. We identified potential _P. destructans_ isolates on the basis of morphologic characteristics. DNA was then extracted from 2 suspected fungal cultures and tested for _P. destructans_ by qPCR, as previously described.\n\n【7】To further confirm the presence of _P. destructans_ , we prepared the fungal isolates for Sanger sequencing . The 600-nt amplification products from these 2 isolates were sequenced and found to be 100% identical to the _P. destructans_ rRNA gene region targeted for amplification. In addition, using BLAST , we found that sequences were a 100% match with isolates from Europe  and North America . This result confirms that the same species of fungus occurs on all 3 continents. We also obtained wing biopsy punches from these bats and found lesions characteristic of WNS by histopathologic examination .\n\n【8】The occurrence of _P. destructans_ at most sites sampled indicates that this pathogen is widespread in eastern Asia . The presence of _P. destructans_ in bats from 6 species in China and on bats in 13 species in Europe  confirms the generalist nature of this fungus and suggests that it may occur throughout Eurasia .\n\n【9】Decontamination and restrictions on the use of equipment that has been used in caves in Asia would help reduce the probability of introducing _P. destructans_ to uninfected bat populations (e.g. western North America, New Zealand, southern Australia, and temperate areas of South America). These measures would also reduce the risk of introducing new strains of _P. destructans_ to regions where bats are already infected (e.g. eastern North America and Europe). These measures are necessary to prevent the devastating effects this pathogen has had on bats in North America and would help maintain the ecosystem services that bats provide .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3175a5fb-d750-4bf4-8aaf-8ef8a67a8028", "title": "Depression, Diabetes, and Chronic Disease Risk Factors Among US Women of Reproductive Age", "text": "【0】Depression, Diabetes, and Chronic Disease Risk Factors Among US Women of Reproductive Age\nAbstract\n\n【1】**Introduction**Depression and chronic disease have implications for women’s overall health and future pregnancies. The objective of this study was to estimate the prevalence and predictors of diabetes and chronic disease risk factors among reproductive-age women with depression. **Methods**We used population-based data from the 2006, 2008, and 2010 Behavioral Risk Factor Surveillance System to examine prevalence of diabetes and prediabetes, binge and heavy drinking, smoking, overweight and obesity, and physical inactivity among 69,043 women aged 18 to 44 years with current major or minor depression, a past depression diagnosis, or no depression. In a multivariable logistic regression model, we calculated adjusted odds ratios (AORs) and 95% confidence intervals (CIs) of 1, 2, and 3 or more chronic disease risk factors by depression status. **Results**We found that 12.8% of reproductive-aged women experienced both current depression and 1 or more chronic disease risk factors. Compared to women with no depression, currently depressed women and those with a past diagnosis had higher prevalence of diabetes, smoking, binge or heavy drinking, obesity, and physical inactivity ( _P_ < .001 for all). Odds of 3 or more chronic conditions and risk factors were elevated among women with major (AOR, 5.7; 95% CI, 4.3-7.7), minor (AOR, 4.7; 95% CI, 3.7-6.1), and past diagnosis of depression (AOR, 2.8; 95% CI, 2.4-3.4). **Conclusion**Depressed women of reproductive age have high rates of chronic disease risk factors, which may affect their overall health and future pregnancies.  \n\n【2】Introduction\n\n【3】Population-based data show that more than 14% of US women of reproductive age screen positive for major or minor depression . In the general population, depression is correlated with risk factors for chronic disease, such as smoking, excessive alcohol use, obesity, and physical inactivity . Independent associations also exist between depression and cardiovascular disease  and diabetes . Chronic disease comorbidities compromise the health of many people with depression; however, depression and chronic disease comorbidities among women of reproductive age have unique implications for the health of the woman and any future pregnancies. Numerous studies confirm that behavioral and lifestyle risk factors, such as smoking, excessive alcohol consumption, and overweight and obesity, may adversely affect pregnancy outcomes. Depression may also negatively affect pregnancy outcomes . Poor mental health, including depression, may be a barrier to lifestyle changes in other areas, and incorporating mental health counseling into behavioral change interventions may be necessary for success . Thus, understanding the comorbidity of depression and chronic disease risk factors among women of reproductive age may benefit chronic disease prevention and treatment strategies. In a previous analysis of 2006 Behavioral Risk Factor Surveillance System (BRFSS) data, we examined prevalence of depression, associated demographic characteristics, and access to care among women of reproductive age . The objective of this analysis was to examine population-based estimates of the burden of diabetes and chronic disease risk factors among reproductive-age women with depression.  \n\n【4】Methods\n\n【5】We analyzed data from the 2006, 2008, and 2010 BRFSS, a state-based random-digit–dialed survey of the noninstitutionalized US population aged 18 years or older. In 2006, 38 states and the District of Columbia, in 2008, 16 states, and in 2010, 20 states used an optional module on anxiety and depression, which included the 8-question Patient Health Questionnaire (PHQ-8) screener for depression, adapted from the 9-item PHQ-9 , which has a sensitivity of 73% and specificity of 94% for major depression . The PHQ-8 omits 1 question on suicidal ideation because interviewers are unable to intervene. We limited all analyses for this report to nonpregnant women of reproductive age (18-44 y) because associations may differ by pregnancy status and BRFSS surveyed too few pregnant women to provide reliable estimates for this group. For 2006, 2008, and 2010, the median state cooperation rates (percentage contacted who completed an interview) were 75% to 76% and the median response rates (percentage eligible for whom an interview was completed) were 51% to 55%. BRFSS has Centers for Disease Control and Prevention institutional review board approval. We categorized women into 1 of 4 levels of depression status (current major depression, current minor depression, past diagnosis of depression, and no depression) on the basis of their responses to the PHQ-8 screener and 1 additional question assessing whether the woman had ever received a clinical diagnosis of depression. We defined major and minor depression on the basis of responses to the PHQ-8 assessing number of days experiencing anxiety or depression over the last 2 weeks: 1) had little interest or pleasure in doing things; 2) felt down, depressed, or hopeless; 3) had trouble falling asleep; 4) felt tired or had little energy; 5) had a poor appetite or ate too much; 6) felt bad about yourself or that you were a failure; 7) had trouble concentrating on things; and 8) moved or spoke so slowly that other people could have noticed, or the opposite — were so fidgety or restless that you were moving around a lot more than usual. Similar to PHQ-9 methodology , we coded major depression as reporting at least 7 days of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting at least 7 days to at least 5 questions total. We coded current minor depression as reporting at least 7 days of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting at least 7 days to 2 to 4 questions total. We further classified the remaining women who did not screen positive for current depression into 2 groups: those with a self-reported previous clinical diagnosis of depression, as an indicator of history of depression, and those with no depression. Chronic disease conditions and modifiable risk factors inquired of in 2006, 2008, and 2010 included self-reported clinician diagnosis of diabetes or prediabetes, current smoking, binge drinking (>4 drinks at any 1 time) or heavy drinking (>1 drink/d in the past 30 days), overweight (body mass index \\[BMI\\] 25.0-29.9 kg/m 2 ) or obesity (BMI ≥30.0 kg/m 2 ), and physical inactivity (no physical activity in past 30 days aside from regular job). Initially, we examined differential distribution of demographic characteristics by depression status using χ 2 tests. Next, we assessed unadjusted associations between depression and 1) diabetes and prediabetes and 2) risk factors for chronic diseases. We also examined the distribution of number of chronic disease conditions and risk factors (0, 1, 2, or ≥3) by depression status using a χ 2 test. To assess overall odds of 1 or more chronic conditions, in a multivariable multinomial logistic regression model, we calculated the odds ratios (ORs) for 1, 2, and 3 or more chronic conditions and risk factors, compared with 0, among depressed women and those with a past diagnosis of depression compared to women with no depression, adjusted for demographic characteristics. We included demographic characteristics in the multivariable model that, in univariate analyses among all women, were associated with depression status and, in univariate analyses among nondepressed women, were associated with number of chronic conditions and risk factors. BRFSS surveyed a total of 75,450 women in the states that included the Depression and Anxiety optional module in 2006 (n = 42,444), 2008 (n = 16,261), and 2010 (n = 16,745 women). A total of 69,043 nonpregnant women aged 18 to 44 (91.5%) had information on depression and were included in this report. Of those, 4,895 (7.1%) women did not have information on 1 or more chronic conditions or risk factors, and 5,545 (8.0%) women did not have information on 1 or more demographic characteristics, 93% of whom were missing data on income. Therefore, the regression model included 58,603 (84.9%) women with information on depression status and who were not missing chronic disease or demographic information. We conducted all analyses in SUDAAN (RTI International, Research Triangle Park, North Carolina) to account for the complex sampling design and used weights to produce unbiased prevalence and adjusted odds ratio (AOR) estimates and 95% confidence intervals (CIs).  \n\n【6】Results\n\n【7】Among women of reproductive age, 5.6% met PHQ-8 criteria for major depression, 9.2% met criteria for minor depression, and 13.2% were not currently depressed but had a past diagnosis of depression . Additionally, among women with major depression, 67.6% had ever received a clinical diagnosis of depression, and, among women with minor depression, 40.1% had ever received a diagnosis of depression (data not shown). Age, race/ethnicity, education, marital and employment status, and annual household income were associated with depression status. Compared to women with no depression, prevalence estimates for diabetes and chronic disease risk factors were up to 3.6 times higher among women with major depression, 1.1 to 2.9 times higher for women with minor depression, and 1.1 to 1.9 times higher for women with a past diagnosis of depression . The prevalence of 2 and 3 or more chronic conditions and risk factors was significantly higher among women with current depression or a past diagnosis compared to those with no depression ( _P_ < .001) . Eighty-nine percent of women who met PHQ-8 criteria for major depression, 85% who met PHQ-8 criteria for minor depression, and 77% with a past diagnosis compared to 66% of nondepressed women had 1 or more chronic conditions or risk factors. Among all women, 12.8% met PHQ-8 criteria for depression and had at least 1 other chronic condition or risk factor (data not shown). The mean number of chronic conditions and risk factors increased with severity of depression (no depression: 2.0; past diagnosis: 2.3; minor: 2.5; major: 2.7).  **Figure.** Prevalence and distribution of number of chronic conditions and risk factors by depression status. Conditions and risk factors included diabetes or prediabetes, current smoking, binge drinking or heavy drinking, overweight or obesity, and physical inactivity. χ 2 _P_ < .001. Source: Behavioral Risk Factor Surveillance System, 2006, 2008, and 2010.  In the multivariable, multinomial logistic regression model examining the association between severity and history of depression and number of chronic conditions and risk factors, odds of having at least 3 chronic conditions and risk factors were elevated among women with major (AOR, 5.7; 95% CI, 4.3-7.7), minor (AOR, 4.7; 95% CI, 3.7-6.1), and past diagnosis of depression (AOR, 2.8; 95% CI, 2.4-3.4) . Women with current depression or past diagnosis had 1.5 to 3.7 times the odds of having 1 or 2 chronic conditions and risk factors compared to women with no depression. In this sample, 18% of women with 1 or more chronic conditions were currently depressed, compared to 7% of women without a chronic condition or risk factor (χ 2 _P_ < .001) (data not shown).  \n\n【8】Discussion\n\n【9】Almost 13% of US women of reproductive age concurrently experienced both depression and at least 1 additional chronic disease condition or risk factor. Our results suggest a positive dose-response relationship between severity and history of depression and cumulative number of chronic conditions and risk factors, although the temporality of this association is unknown. Both depression and the chronic conditions examined independently place the woman at higher risk of current disease, adverse pregnancy outcomes, and future disease. However, the synergistic effect of poor mental and physical health may have even greater implications for current and future disease risk and future pregnancies. Associations exist between depression and chronic diseases, such as coronary heart disease , cardiovascular disease , and diabetes , and their risk factors, such as smoking , alcohol abuse , obesity , and physical inactivity  in the general adult population and the elderly. This report shows that associations also exist between depression and diabetes and chronic disease risk factors among women of reproductive age. Of note, associations were not limited to major depression or extremes of chronic conditions, such as diabetes and obesity, but also existed for minor depression and past diagnosis of depression, and precursors to chronic conditions, such as prediabetes and overweight. Longitudinal research has shown that bidirectional associations exist between chronic conditions and depression through both lifestyle and biologic factors . Because of these common disease pathways, certain interventions, such as exercise , dietary advice and blood glucose monitoring , and use of antidepressants , may improve a person’s physical and mental health. To prevent or reduce complications from chronic conditions and improve a woman’s preconception health, primary care providers and obstetrician/gynecologists often encourage behavior change, such as increasing physical activity, eating a healthier diet, stopping smoking, and reducing alcohol consumption . Depressed people may be as motivated to change their behavior as nondepressed people . However, behavior change programs may need adaptation to concurrently focus on improving mental health if they are to be successful for depressed people . Clinicians should consider assessing the mental health of a patient when encouraging lifestyle behavior change, since 18% of women with a chronic condition were currently depressed and these women may have better success with behavior change by simultaneously focusing on mental health improvement. Past diagnosis of depression was also associated with a higher risk of chronic conditions and risk factors, and research indicates that more than 45% of people with a past depressive episode will relapse . Assessing the history of depression may be one way to more fully understand a woman’s future risk of chronic disease. Additionally, the increased risk illustrates that treating depression alone may not be sufficient to prevent long-term adverse health outcomes, given that women may have acquired unhealthy behaviors, such as smoking, heavy drinking, or overeating, during a past depressive episode, leaving them at risk of future chronic disease. By assessing lifestyle factors among women with current and past depression, clinicians can refer them to appropriate care when needed. This report has some limitations. The PHQ-8 is a screener for depression with less than 100% sensitivity and specificity; therefore, some misclassification of current depression status may have occurred. We did not have information on current treatment for depression, and some depressed women currently receiving treatment may not have screened positive with the PHQ-8. A past clinical diagnosis of depression does not capture all women who may have experienced depression in the past but only a limited percentage who may have experienced more severe depression or those with more access to care. Therefore, some women in the “no depression” group may have experienced depression in the past, and any misclassification may have attenuated the association between depression and chronic conditions. Data were self-reported and cross-sectional; we do not know the temporal association between depression and the chronic conditions and risk behaviors. Additionally, other chronic disease risk factors, such as hypertension and high cholesterol, were not assessed in the years examined, when depression was assessed; therefore, the cumulative number of chronic conditions and risk factors are likely conservative estimates. Estimates for past diagnosis of depression and chronic conditions may be underestimated if women were unwilling to report or did not know their status. Additionally, we were unable to examine associations between number, timing, or severity of previous depressive episodes and chronic disease conditions or risk factors. This study found that more than 1 in 10 reproductive-aged women currently experience depression and have at least 1 additional chronic disease risk factor or condition. Current or previous depression may be an indicator of other chronic conditions and risk factors and, reciprocally, the presence of chronic conditions and risk factors may indicate the woman is at risk for depression. In addition to assessing a woman’s physical health, her provider should also consider assessing her mental health when services are available for accurate diagnosis and follow-up . This can be done through a brief, 2-item depression screener  and asking about history of depression. Detecting and treating poor mental health may prevent or mitigate many chronic conditions and risk factors and may improve a woman’s overall and preconception health. In addition, interventions addressing both lifestyle behaviors and depression among reproductive-aged women are needed.  \n\n【10】Tables\n------\n\n【11】#####  Table 1. Demographic Characteristics of Nonpregnant US Women Aged 18 to 44 Years (n = 69,043), by Current Depression Status a\n\n【12】CharacteristicDepression Status, % (95% CI)P Value bMajor DepressionMinor DepressionPast Diagnosis of DepressionNo DepressionOverall5.6 (5.1-6.0)9.2 (8.7-9.8)13.2 (12.5-13.9)72.0 (71.1-73.0)NAAge, y18-2419.5 (17.0-22.2)26.7 (24.0-29.6)18.6 (16.9-20.4)21.1 (20.0-22.4).00325-2915.6 (13.4, 18.1)13.9 (12.7, 15.3)15.9 (14.5, 17.3)14.9 (14.1, 15.7)30-3419.0 (16.4, 21.9)19.0 (16.9, 21.3)19.7 (18.4, 21.1)20.4 (19.5, 21.2)35-3919.4 (17.3, 21.8)18.5 (17.0, 20.1)20.8 (19.4, 22.2)19.8 (19.0, 20.6)40-4426.5 (24.2, 29.1)21.9 (19.9, 24.0)25.2 (23.3, 27.1)23.9 (22.8, 25.0)Race/ethnicityBlack14.0 (11.4-17.2)13.5 (11.3-16.0)5.2 (4.4-6.2)11.4 (9.8-13.3)&lt;001Hispanic15.9 (12.4-20.1)20.3 (15.7-25.8)13.6 (10.2-18.0)17.9 (14.0-22.6)Other7.5 (5.9-9.5)7.9 (6.5-9.4)4.7 (3.9-5.6)7.1 (6.3-8.1)White62.6 (58.2-66.9)58.4 (54.0-62.7)76.5 (72.5-80.1)63.6 (59.2-67.7)Education&lt;High school19.2 (16.4-22.2)15.1 (12.6-17.9)8.6 (6.9-10.6)8.4 (7.0-10.0)&lt;001High school31.5 (28.3-34.9)30.2 (27.7-32.8)24.8 (23.1-26.6)23.2 (22.1-24.3)Some college33.1 (29.9-36.4)31.7 (28.8-34.7)30.2 (28.2-32.2)29.2 (28.1-30.4)College degree16.3 (13.9-19.0)23.1 (20.7-25.7)36.5 (34.5-38.5)39.2 (37.6-40.9)Marital statusNever married31.4 (28.5-34.5)30.0 (27.9-32.2)23.7 (21.9-25.7)26.4 (25.2-27.6)&lt;001Divorced, separated, or widowed23.2 (20.9-25.6)13.1 (11.9-14.5)13.1 (12.0-14.4)8.1 (7.6-8.6)Unmarried couple7.4 (6.0-9.1)8.6 (6.9-10.7)7.1 (6.1-8.4)5.6 (4.9-6.4)Married38.0 (35.7-40.5)48.3 (46.2-50.4)56.0 (53.7-58.4)59.9 (58.5-61.3)EmploymentEmployed for wages42.5 (39.5-45.6)56.5 (53.6-59.3)62.8 (60.8-64.7)64.9 (63.1-66.8)&lt;001Homemaker13.0 (11.0-15.3)17.3 (14.7-20.3)16.8 (15.5-18.2)17.9 (16.6-19.2)Unemployed17.2 (15.0-19.8)11.1 (9.6-12.7)7.2 (6.0-8.7)5.9 (5.1-6.7)Unable to work17.8 (15.7-20.2)6.6 (5.3-8.1)3.9 (3.3-4.6)1.1 (0.9-1.4)Student or retired9.4 (7.3-12.1)8.7 (7.4-10.1)9.4 (8.3-10.6)10.2 (9.5-10.9)Annual household income, $&lt;15,00017.5 (15.4-19.7)8.3 (6.9-9.9)4.9 (4.1-5.9)5.2 (4.2-6.4)&lt;00115,000–24,99937.8 (35.1-40.6)27.3 (25.4-29.4)19.4 (17.7-21.2)17.3 (16.1-18.6)25,000–49,99924.3 (21.5-27.4)29.8 (27.3-32.4)26.4 (24.6-28.2)25.1 (24.0-26.3)&gt;50,00020.4 (17.9-23.2)34.6 (31.5-37.8)49.4 (47.0-51.8)52.4 (50.2-54.6)Abbreviations: CI, confidence interval; NA, not applicable.a Current depression defined based on PHQ-8, adapted from PHQ-9 ; major depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to ≥5 questions total. Minor depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to 2-4 questions total; past diagnosis based on self-report. Source: Behavioral Risk Factor Surveillance System, 2006, 2008, and 2010.b Calculated by using χ 2 test. \n\n【13】#####  Table 2. Prevalence of Diabetes and Chronic Disease Risk Factors by Current Depression Status a Among Nonpregnant US Women Aged 18 to 44 Years (n = 69,043)\n\n【14】CharacteristicDepression Status, % (95% CI)P Value bMajor DepressionMinor DepressionPast Diagnosis of DepressionNo DepressionDiabetes statusPrediabetes1.8 (1.3-2.5)0.9 (0.6-1.3)0.9 (0.6-1.4)0.5 (0.4-0.6)&lt;001Diabetes6.4 (5.3-7.6)5.3 (4.3-6.4)3.5 (2.9-4.3)1.8 (1.6-2.1)Current smokerYes44.1 (40.5-47.7)29.6 (27.6-31.8)26.8 (24.7-29.0)14.2 (13.2-15.4)&lt;001Alcohol consumption cBinge or heavy drinker17.6 (15.2-20.4)17.8 (15.9-19.9)19.7 (18.5-20.9)14.5 (13.8-15.2)&lt;001BMI dOverweight24.7 (22.5-27.1)28.7 (26.1-31.5)26.9 (25.1-28.8)25.5 (24.5-26.5)&lt;001Obese35.8 (32.3-39.4)34.9 (32.8-37.1)28.2 (26.2-30.2)19.9 (19.1-20.9)Physical activityInactive e41.6 (38.4-44.8)31.5 (29.3-33.8)21.2 (19.3-23.2)18.9 (18.0-19.9)&lt;001Abbreviations: CI, confidence interval; BMI, body mass index.a Current depression defined based on PHQ-8, adapted from PHQ-9 ; major depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to ≥5 questions total. Minor depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to 2-4 questions total; past diagnosis based on self-report. Source: Behavioral Risk Factor Surveillance System, 2006, 2008, and 2010.b Calculated by using χ 2 test.c Heavy drinker defined as >1 drink/d and binge drinker defined as >4 drinks on 1 occasion in the past 30 days.d Overweight defined as BMI 25.0-29.9 kg/m 2 ; obese defined as BMI ≥30.0 kg/m 2 .e No physical activity other than normal job in past 30 days. \n\n【15】#####  Table 3. Association Between Depression and Number of Chronic Conditions and Risk Factors a Among Nonpregnant US Women Aged 18 to 44 Years (n = 58,603)\n\nCharacteristic bNo. of Chronic Conditions and Risk Factors, AOR (95% CI)12≥3Major depression1.7 (1.3-2.2)3.7 (2.8-4.8)5.7 (4.3-7.7)Minor depression1.9 (1.5-2.3)3.5 (2.8-4.3)4.7 (3.7-6.1)Past diagnosis of depression1.5 (1.3-1.7)2.2 (1.9-2.5)2.8 (2.4-3.4)No depression1 [Reference]Abbreviations: AOR, adjusted odds ratio; CI, confidence interval.a Defined as diabetes or prediabetes, smoking, overweight or obesity, heavy drinking or binge drinking, and physical inactivity.b Current depression defined based on PHQ-8, adapted from PHQ-9 ; major depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to ≥5 questions total. Minor depression defined as reporting ≥7 d of having little interest or pleasure in doing things or feeling down, depressed, or hopeless and reporting ≥7 d to 2-4 questions total; past diagnosis based on self-report. Source: Behavioral Risk Factor Surveillance System, 2006, 2008, and 2010.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "39a92fc8-f373-404d-9c14-d35bd64d984c", "title": "Development of Framework for Assessing Influenza Virus Pandemic Risk", "text": "【0】Development of Framework for Assessing Influenza Virus Pandemic Risk\nPandemic influenza remains a formidable threat to human health. Advances in national pandemic preparedness have been made during recent decades; however, the frequent infection of humans with novel influenza viruses complicates implementation of effective control measures such as vaccines . The emergence of the influenza A(H1N1)pdm09 virus , the ongoing outbreaks of highly pathogenic avian influenza A(H5N1) viruses , and, more recently, the identification of severe human infections caused by avian influenza A(H7N9) virus in China  indicate the need for a more objective, systematic, and transparent approach for evaluating newly emerging influenza viruses with pandemic potential. The Influenza Risk Assessment Tool (IRAT) was developed in response to this need and creates a framework for systematically combining input from influenza experts to support risk management decisions that have important cost implications.\n\n【1】### Development of a Framework\n\n【2】The approach described here uses opinions from subject matter experts to populate a simple, multiattribute additive model  that combines information from well-established decision analysis methods  to assist in decision making and prioritization processes. Traditional risk assessment approaches  are not directly applicable to the IRAT; however, the general guiding principles still apply . The problem definition addressed by IRAT is captured in the formulation of 2 important questions. One question addresses the risk that the virus will achieve sustained human-to-human transmission and emerge as a pandemic virus (the emergence question) and specifically asks, “What is the risk that a virus not currently circulating in the human population has potential for sustained human-to-human transmission?” The second question, called the impact question, asks, “If the virus were to achieve sustained human-to-human transmission, what is the risk that a virus not currently circulating in the human population has the potential for significant impact on public health?” Risk analysis and interpretation involve the assessment of the potential pandemic risk of specific influenza viruses by subject matter experts, who use multiple factors or risk elements identified and defined by a panel of influenza experts as critical for a risk-based assessment. These defined risk elements provide the basis for evaluation and comparison of emerging influenza viruses and contribute to an aggregate risk score.\n\n【3】##### Identification and Definition of Risk Elements\n\n【4】Leading influenza subject matter experts in human and animal health, along with laboratory workers, field epidemiologists, and risk modelers, were invited to a meeting in 2011  and asked to provide input into the development of the IRAT. Participants were asked to provide and refine definitions of elements used by the IRAT to evaluate the potential risk of novel influenza viruses. To distinguish among novel influenza viruses, each element must be amenable to qualitative or quantitative evaluation, independent of other elements, and must not be redundant in what is being measured. Subject matter experts agreed on and defined 10 elements to be scored that would provide the necessary critical information. Each of the 10 elements falls into 1 of 3 categories: virus properties, attributes and immune response of the human population, and the ecology and epidemiology associated with the novel influenza virus . These 10 elements are meant to capture the features of the virus, the population, and the ecology and epidemiology likely to affect whether or not a specific zoonotic virus can spread widely in humans. These elements are also meant to capture factors likely to affect public health during a pandemic caused by a specific virus.\n\n【5】Definitions required for each element included scoring criteria that characterize viruses as low, moderate, and high risk to assist the experts in estimating a score on a scale of 1–10. For example, the definition of the element “Antivirals and Treatment Options” states, “For the purposes of the risk assessment tool, antiviral susceptibility refers to the predicted or demonstrated efficacy of available antiviral agents against animal influenza viruses.” A low-risk score  for this element is defined as “no evidence of clinically relevant resistance to any of the antiviral drugs approved for human use (neuraminidase inhibitors and M2 blockers).” A moderate-risk score  is defined as “sensitive to all neuraminidase inhibitors but resistant to M2 blockers.” A high-risk score  for this element is defined as “resistant to one or more neuraminidase inhibitor antiviral drugs.” All 10 elements have definitions for low-, moderate-, and high-risk scores.\n\n【6】##### Weighting of Risk Elements\n\n【7】Because not all risk elements are equally useful in answering each risk question, a weighting system had to be determined before the final risk score for each virus was generated. A key requirement for creating a weighting system was minimizing bias regarding the weighting of input from each expert’s field of study relative to that of others and reducing the difficulty of eliciting relative weights from experts when multiple attributes are compared. A surrogate weighting method was selected for this purpose . First, the expert panel was surveyed to determine a consensus rank ordering of the 10 risk elements if each risk question was asked separately. When asked to rank the elements when the “emergence question was considered,” 27 of 29 subject matter experts agreed that evidence of human infections is the “most important” of the 10 elements regarding risk that a novel influenza virus can achieve sustained human-to-human transmission. The other 2 subject matter experts believed this element was “very important.” Second in importance was information regarding laboratory animal transmission studies; 26 subject matter experts thought this element was “most important” or “very important.” Third in importance was receptor binding information and population immunity. Of least importance for the experts was information regarding antiviral and treatment options and disease severity; 24 of the 29 subject matter experts rated this element as “not relevant” for the question about the risk that a novel influenza virus can achieve sustained human-to-human transmission.\n\n【8】The second risk question addresses the potential effect of a novel influenza virus on public health in the event that the virus achieved sustained human-to-human transmission (i.e. the impact question). Subject matter experts agreed that knowledge regarding disease severity ranked first (26 ranked this element as “most important” and 3 ranked it as “very important”), followed by information pertaining to population immunity (21 ranked this element as “most important” and 8 ranked it as “very important”), human infections, and antiviral and treatment options in descending order. Of least importance were the elements related to global distribution in animal species (16 ranked this element as “not relevant”) and infections in animals (11 ranked this element as “not relevant”). Although risk element definitions were the same for the emergence and impact questions, the rank order of the risk elements by experts differed for the 2 questions.\n\n【9】This rank ordering of risk elements for the 2 questions facilitated the calculation of surrogate weights by using the formula\n\n【10】where _K_ is the number of elements and _Wk_ is the calculated weight to be applied to each risk element in rank order . Thus, the weight for the top-ranked element of the 10 total elements was calculated as 0.2929, and the lowest-ranked element was assigned a weighting score of 0.001. By convention, the sum of all 10 weights in this example is 1.\n\n【11】##### Scoring\n\n【12】Using the framework of risk element definitions, scoring criteria, and rank-ordered surrogate weights, individual experts were asked to evaluate viruses by using a particular risk element within their field of expertise and to provide a point estimate score. The returned scores for each risk element were averaged and then multiplied by the corresponding surrogate weight, depending on the risk question being asked. A summation of the product of all 10 risk elements provided the final aggregate score for each virus. Tables 2 and 3 show results of subject matter experts’ scoring of 3 avian influenza viruses. The H1N1 subtype virus found in North American mallards was used as an example of an influenza virus that has not infected people and appears to be wholly avian in characteristics.\n\n【13】##### Addressing Uncertainty\n\n【14】Inherent in any risk assessment is the need to address uncertainty. The IRAT attempts to capture this factor by asking subject matter experts to characterize uncertainty in 2 ways. First, uncertainty is captured by each expert’s point estimate score on a scale of 1–10 (i.e. 1 representing the lowest level of risk and 10 representing the highest), along with an upper and lower boundary for their scores, thereby providing a range for their point scores. A second measure of uncertainty uses a confidence score from each expert; this score expresses the expert’s confidence in the available data used to make the point estimate. Finally, the subject matter experts were asked to provide a basis or justification for their risk scores. This justification could include key references and data along with professional observations and experiences. This approach was based on the assumption that subject matter experts using the same knowledge base will produce similar scores for each virus and narrow uncertainty ranges unless data are insufficient or the experts have a fundamental disagreement in the interpretation of available data. The upper and lower bounds of the score for each risk element can be used to determine the highest or lowest (respectively) risk score for a given virus. In addition, a sensitivity analysis wherein the rank order of 2 risk elements can be swapped and the risk score recalculated on the basis of the new weights can be used to show the effect of these changes on a virus’s aggregate score. The level of uncertainty is communicated in the final report of virus scores so that decision makers using the IRAT can use this information in their considerations. Even with these efforts to measure and communicate uncertainty, the numerical score produced by the IRAT may create a sense of precision that is unfounded. Anyone making decisions by using the IRAT scores needs to be reminded that the scores are semiquantitative.\n\n【15】### Case Illustrations\n\n【16】##### H3N2v Virus\n\n【17】In July 2011, the Centers for Disease Control and Prevention (CDC; Atlanta, GA, USA) confirmed the first human case of an influenza A(H3N2) variant \\[A(H3N2)v\\] virus that had acquired the matrix gene from the A(H1N1)pdm09 virus. Eleven additional cases were reported during the rest of 2011, for a total of 12 confirmed cases in 5 states . A preliminary risk assessment using the IRAT was conducted on the basis of information gathered from these 12 cases and available laboratory data . The H3N2v risk score for both the emergence and impact questions in December 2011 placed this virus in the moderate-risk category; the summary scores for both questions were >5.0.\n\n【18】In 2012, a total of 309 cases of A(H3N2)v virus were reported from 12 states. In most cases, the putative source was direct or indirect contact with swine , with no evidence of community transmission. Studies conducted to assess population immunity  provided additional information regarding age groups considered at risk for infection. In 2013, subject matter experts rescored the H3N2v virus  in light of these and other available data. Although 321 confirmed cases were reported over 2 years, the summary scores for the virus were lower than those for the previous scoring. The risk score for the emergence question dropped from 6.5 to 6.1, and the risk score for the impact question dropped from 5.4 to 4.5. This reduction in the scores was caused primarily by information about population immunity. This information indicated that persons at greatest risk for infection were children, especially those <10 years of age; those 20–50 years of age exhibited cross-reactive antibodies against this virus. In addition, the epidemiologic investigations of the 309 cases in 2012 showed that >90% of the patients were <18 years of age; of these cases, 16 hospitalizations and 1 death associated with A(H3N2)v virus were documented . These epidemiologic observations reinforced the population immunity data obtained in the laboratory and supported the virus’s decreased cumulative risk score from its risk score in 2011.\n\n【19】##### H7N9 Virus\n\n【20】On March 31, 2013, the China Health and Family Planning Commission notified the World Health Organization of 3 cases of human infection with influenza A(H7N9) virus . As of April 8, 2013, the China World Health Organization Collaborating Centre for Reference and Research on Influenza, the China OIE (World Organisation for Animal Health) Reference Laboratory for avian influenza, and a provincial public health laboratory in China had identified and analyzed 7 virus isolates. The complete viral genome sequences of 6 isolates were deposited in a publicly accessible influenza database, and additional human cases continued to be reported.\n\n【21】In April 2013, subject matter experts used the IRAT and its 10 individual risk elements to assess the potential pandemic risk for humans posed by the novel A(H7N9) virus. To address the risk of the virus to achieve human-to-human transmission (emergence question), information about the virus’s ability to transmit in laboratory animals was critical. This information was not available in mid-April 2013. The absence of information for this and other elements affected the scoring of the risk elements. The scoring for the virus’s potential for sustained human-to-human transmission was in the moderate-risk category, approaching 5.0 on a scale of 1–10 . The moderate-risk score perhaps emphasized missing information in \\> 1 high-ranking element (Transmission in Laboratory Animals). For the impact question, the risk score was in the higher end of the moderate-risk range , almost 6.0. Cautionary notes regarding interpretation of the scoring in the face of data gaps were included in the subsequent report.\n\n【22】A month later, in mid-May, a second scoring was conducted among CDC subject matter experts . At this time, preliminary results of studies assessing the virus’s ability to transmit between animals in a laboratory setting were available. This information and updated information pertaining to the other elements were incorporated into the evaluation process. The risk score for the virus to achieve sustained human-to-human transmission was 6.2. This score was higher than the previous score based on the preliminary evaluation in April, although both scores were still in the moderate-risk range . The risk score for the virus’s potential to impact public health if it were to achieve sustained human-to-human transmission is 7.0, which falls in the upper limit of the moderate-risk range and which is higher than the preliminary score of ≈6.0 in mid-April .\n\n【23】During both rounds of scoring, the subject matter experts were asked to grade their level of confidence in the available data as it applied to their scoring. They generally expressed a greater confidence in their risk scores in May. For most elements, the risk scores differed little from those in the first round. However, in May, preliminary data were available for the risk element Transmission in Laboratory Animals and informed the risk score for this element. Although the 2 scorings took place only 1 month apart, this change illustrates how a risk score can increase after availability of new data and emphasizes the necessity of reevaluating risk scores when new data become available.\n\n【24】### Discussion\n\n【25】The IRAT is not a tool to assess widespread human seasonal influenza viruses that are already able to transmit from person to person; those viruses are more correctly assessed in terms of disease severity on a season-by-season basis . For seasonal influenza viruses, decisions have already been made regarding mass manufacturing and distribution of seasonal influenza vaccines. The IRAT assesses influenza viruses that are not yet readily transmissible person to person for their potential risk of emergence and potential effect on the public’s health. Furthermore, the IRAT was not developed to predict the next pandemic influenza virus but rather to focus limited pandemic preparedness resources on those viruses that are believed, on the basis of current knowledge, to have the greatest potential to cause a serious pandemic.\n\n【26】Developing an accepted, systematic method for evaluating potentially pandemic influenza viruses to inform risk management decisions requires a framework capable of combining multiple data inputs and information. Conceptually, the IRAT follows the basic principles of microbial risk assessment frameworks but diverges from traditional methods. Microbial risk assessments typically identify and establish a working model that facilitates the analysis of dose responses and exposure estimates so that risks can be calculated and expressed in terms of likelihood, type, or magnitude. The IRAT focuses on a virologic assessment and prioritization. The development of the IRAT followed a process by which the components of a multiattribute decision analysis method were adapted to the application of evaluating influenza viruses with pandemic potential. For example, the IRAT can provide subject matter expert consensus input into decisions regarding whether to stockpile a particular influenza vaccine antigen in case it is needed. Although the IRAT can offer input into such a decision, it is not the only input.\n\n【27】Subject matter experts were called on to characterize, define, and capture the elements to consider when the IRAT was created. These subject matter experts, who generously brought years of experience, expertise, and familiarity with the published literature to the development of the IRAT, continue to play an integral part in its use and development. Beyond what is published, such experts in the global influenza community have insights regarding ongoing studies and research that remain unpublished. The use of subject matter experts to provide input into the IRAT is critical for addressing the inherent data gaps and uncertainty in the available information regarding emerging influenza viruses. The 2 case illustrations show how new information is readily incorporated into the analysis and reflected in the risk assessments.\n\n【28】This tool can also be used to guide global investments in capacity building and to highlight information gaps that can help focus research initiatives to fill these gaps. Such use was seen in the consideration of the H3N2v subtype virus, wherein studies exploring population immunity provided insight into age groups at risk and lowered the risk score of the virus once that information gap was filled. Although prediction of the next pandemic influenza virus is not yet possible, a systematic evaluation of emerging influenza A viruses with the IRAT has provided a more orderly prioritization of work on novel influenza viruses and a more judicious use of resources.\n\n【29】Missing information is clearly a vulnerability of the IRAT, as with any other risk assessment tool and as illustrated in the initial assessment of H7N9. Although some information may remain missing, the tool as designed incorporates weighting factors assigned to each element. To maximize the usefulness of the IRAT, information relating to the top-ranked elements, which carry the most weight, is the most helpful because these elements provide a greater proportion of the total risk score for a virus. However, even with missing or limited information, the IRAT can still provide useful insights about the potential risk posed by a novel influenza virus, particularly if the missing information pertains to lower-weighted elements.\n\n【30】As it is envisioned, the IRAT will continue to evolve. Current definitions of the elements are considered to be working definitions to enable users to familiarize themselves with the tool and its applications. As technology advances, the elements likely will change to reflect increasing understanding of influenza viruses. We hope that this tool will prove useful to the international influenza community yet remain flexible and responsive to increases in knowledge, understanding, and interpretation of potential risks posed by novel influenza viruses to human health.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e28a13ad-9354-4065-a5ef-5a8d86e44e20", "title": "Comparative Antibiotic Resistance of Diarrheal Pathogens from Vietnam and Thailand, 1996-1999", "text": "【0】Comparative Antibiotic Resistance of Diarrheal Pathogens from Vietnam and Thailand, 1996-1999\nAntimicrobial resistance among enteric pathogens in developing countries is a critical area of public health concern. The usual causative agents of dysentery--shigella, campylobacter, and nontyphoidal salmonella--are becoming increasingly resistant to most agents commonly in use . The current recommendation for management of travelers’ diarrhea is self-therapy with antibiotics if illness develops ; fluoroquinolones are standard treatment in much of the world. Although travelers’ diarrhea has traditionally been associated with enterotoxigenic _Escherichia coli_ (ETEC), some studies from Asia have shown that campylobacter is of equal or greater importance . In the early 1990s, fluoroquinolone-resistant strains of campylobacter rapidly became prevalent in Thailand, increasing from 0% to 84% of isolates from 1990 to 1995 .\n\n【1】Azithromycin has been suggested as a replacement for quinolones for empiric treatment of travelers’ diarrhea in Thailand . Although this drug may be a reasonable choice for travelers, its high cost precludes its use as routine treatment for dysentery in disease-endemic areas. Furthermore, the emergence in Thailand of campylobacter resistant to both quinolones and azithromycin  threatens the continued usefulness of both.\n\n【2】The seriousness of the problem has prompted calls for improved surveillance for antimicrobial resistance in developing countries, which could provide early warnings of the emergence of resistant bacterial strains . The prevalence of quinolone- and macrolide-resistant campylobacter in areas of Southeast Asia other than Thailand is unknown, and resistance patterns in Thailand may not apply to other countries in the region. Poorer countries such as Vietnam could benefit by avoiding a rush to newer, more expensive agents if sensitivity to older drugs could be demonstrated. Data from Vietnam relative to this issue are scarce or outdated. Surveillance data from the World Health Organization’s Western Pacific Region collaboration for 1992-93 reported 0% and 2% prevalence of fluoroquinolone resistance among nontyphoidal salmonella and shigella, respectively, in Vietnam, although the numbers of isolates and shigella groups were not specified . Sullivan et al. reported resistance rates in >3,000 isolates of shigella, salmonella, and _Escherichia coli_ collected before 1971 from U.S. soldiers and Vietnamese residents with diarrhea; levels of resistance to tetracycline, chloramphenicol, streptomycin, and novobiocem were high, particularly among shigellae. Other than these two studies, no data have been published in English on resistance patterns in enteric diarrheal pathogens in Vietnam, and no campylobacter resistance data from this country have ever been published.\n\n【3】The Armed Forces Research Institute of Medical Sciences in Bangkok has been coordinating studies of the causes, epidemiology, and treatment of diarrheal diseases in Thailand and Vietnam. Antibiotic susceptibility of bacterial isolates, including shigella, ETEC, and nontyphoidal salmonella from patients with diarrhea, have been tested against the most common antibiotics, including ampicillin, tetracycline, chloramphenicol, trimethoprim-sulfamethoxazole (SXT), nalidixic acid, and fluoroquinolones. In recent years, azithromycin resistance among these pathogens, as well as the emergence of fluoroquinolone and azithromycin resistance in campylobacter species, has been monitored. A review of 15 years of data from 1981 to 1995 has been published . Our study further documents increasing antibiotic resistance in Thailand from 1996 to 1999 and presents resistance data from Vietnam, offering insight into regional differences between two countries that are proximal geographically, but politically and economically distant.\n\n【4】### Methods\n\n【5】##### Source of Isolates\n\n【6】Bacterial isolates from studies of community-acquired diarrhea conducted in Thailand and Vietnam from 1996 to 1999 were routinely assayed for sensitivity to various antimicrobial agents. Populations under study were predominantly children <5 years of age from both urban and rural environments but also included small numbers of adult U.S. soldiers on military maneuvers in Thailand. Some isolates were obtained from asymptomatic persons. These data archives were searched initially for all isolates of shigella (other than _Shigella dysenteriae_ 1), nontyphoidal salmonella, campylobacter, and ETEC. The lists were reviewed for the presence of duplicate isolates from the same person, and duplicates were excluded.\n\n【7】##### Microbiology\n\n【8】Enteric pathogens were cultured and identified by standard methods . ETEC isolates were identified with DNA probes. Antibiotic susceptibilities of shigella, salmonella, and ETEC were determined by the disk-diffusion method of Bauer and co-workers , with commercially prepared antibiotic disks containing ampicillin, tetracycline, chloramphenicol, sulfisoxazole, SXT, streptomycin, azithromycin, nalidixic acid, and ciprofloxacin. MIC agar-plate dilution methods  were used to test for nalidixic acid, ciprofloxacin, and azithromycin resistance among campylobacter species. Because of periodic shortages of reagents, not all isolates were tested to all antimicrobial agents.\n\n【9】### Results\n\n【10】Overall, 2,218 isolates were studied, 76% from Thailand and 24% from Vietnam. Pathogenic bacteria were obtained from subjects without gastrointestinal symptoms in 4.2%. Adult travelers (U.S. soldiers on maneuvers) accounted for 6.4% of Thai isolates, but none of the Vietnamese isolates. All other isolates were obtained from children <5 years of age. Sixty-one percent of Thai isolates were from an urban setting (Bangkok). All Vietnamese isolates were rural. All isolates were obtained either from cohorts enrolled in epidemiologic studies evaluating the prevalence of bacterial diarrhea pathogens or the incidence of diarrhea or from case-control studies evaluating diagnostic techniques.\n\n【11】##### Routine Susceptibility Testing for Shigella Species Other than _S. dysenteriae_ 1, ETEC, and Salmonella\n\n【12】From Thailand, 175 shigella (65% _S. sonnei_ , 32% _S. flexneri_ ), 696 nontyphoidal salmonella, and 203 ETEC isolates were studied during the 3-year period. From Vietnam, 305 shigella (18% _S. sonnei_ , 71% _S. flexneri_ ), 30 salmonella, and 113 ETEC isolates were studied .\n\n【13】Among shigella, high levels of resistance to tetracycline and SXT were noted in both Thailand and Vietnam. Significantly less resistance was noted among Thai isolates to ampicillin (29% vs. 77%, p<0.05) and chloramphenicol (21% vs. 64%, p<0.05). Sorting by shigella group showed that nearly all the difference between Thai and Vietnamese isolates was accounted for by differences in the prevalence and susceptibility of _S. sonnei_ compared with the more resistant _S. flexneri_ strains. In Thailand, _S. sonnei_ were almost all susceptible to ampicillin and chloramphenicol; in Vietnam, _S. sonnei_ was more commonly resistant to these agents.\n\n【14】ETEC isolates demonstrated high-level resistance in both countries to ampicillin, SXT, tetracycline, and sulfisoxazole, but chloramphenicol resistance was relatively low in both countries. In contrast, nontyphoidal salmonella from Thailand had consistently and substantially higher levels of resistance to all agents, compared with those from Vietnam. Serogroup B salmonellae in Thailand were generally more resistant than other serogroups, except for serogroup C isolates, which were relatively more resistant to nalidixic acid and azithromycin.\n\n【15】##### Quinolone and Macrolide Resistance among Shigella, Salmonella, and ETEC\n\n【16】Resistance to nalidixic acid was rare among shigella (<1%) and ETEC (3%) in both Thailand and Vietnam but was more common among Thai salmonella (21%). Resistance to ciprofloxacin was documented in <1% of isolates, including two salmonellae (from Thailand) and four ETEC (three from Thailand, one from Vietnam). No shigellae were resistant to ciprofloxacin in either country .\n\n【17】With the exception of _S. sonnei_ , azithromycin resistance was found in ≤8% of shigella, salmonella, and ETEC, with no significant difference between Vietnam and Thailand. Azithromycin resistance was present in 28% of _S. sonnei_ isolates from Vietnam but only 2% of Thai isolates (p<0.05).\n\n【18】##### Resistance of Campylobacter to Quinolones and Macrolides\n\n【19】We studied 608 campylobacter isolates from Thailand and 88 from Vietnam. Resistance to nalidixic acid (73% vs. 7% respectively, p<0.05) and ciprofloxacin (77% vs. 7% respectively, p<0.05) differed markedly between Thai and Vietnamese isolates. Among Vietnamese isolates, quinolone resistance was predominantly among _Campylobacter coli_ rather than _C. jejuni,_ but rates among Thai isolates were similar between species. Resistance to azithromycin was low in Thailand (6%), where it predominated among _C. coli_ . No azithromycin resistance was present in Vietnamese isolates .\n\n【20】##### Co-Resistance to Quinolones and Macrolides among Campylobacters and Salmonellae\n\n【21】Vietnamese isolates of campylobacter were uniformly sensitive to azithromycin, so we were unable to examine correlations with quinolone resistance. Among Thai isolates, however, azithromycin resistance correlated with resistance to ciprofloxacin (p=0.007) . Among campylobacter tested to both these agents, all isolates (n=29) that were resistant to azithromycin were also resistant to ciprofloxacin. Based on the 23% prevalence of ciprofloxacin sensitivity among azithromycin -susceptible strains, a similar rate of discordance among azithromycin-resistant strains would be expected if these resistance patterns were independent. Co-resistant isolates included both _C. coli_ (n=22) and _C. jejuni_ (n=7), were composed of multiple serogroups and resistance profiles, were present in both travelers and indigenous Thais, and were dispersed geographically around Thailand and temporally over the 4 years of data; therefore, they are not likely to be clonally related. These isolates were significantly more likely to be _C. coli_ (76%) than were azithromycin -sensitive isolates ( _C. coli_ 15%; p<0.001).\n\n【22】Too few salmonellae were resistant to ciprofloxacin for analysis of cross-resistance; however, among isolates tested for resistance to both the first-generation quinolone nalidixic acid and azithromycin , a highly significant correlation was found (p<0.001). Based on the 31% prevalence of nalidixic acid resistance among azithromycin -sensitive strains, a similar proportion would be expected among azithromycin-resistant strains if the two were independent, rather than the 80% documented.\n\n【23】##### Multiple-Antibiotic Resistance among Shigellae, Salmonellae, and ETEC\n\n【24】Multiple resistance to ampicillin, chloramphenicol, tetracycline, SXT, and streptomycin was quite common among shigella isolates in both Thailand and Vietnam, but was significantly more so in Vietnam (46% vs. 21%, p<0.001). Lower levels of multiple resistance to these drugs were noted among salmonella and ETEC isolates from both countries (3% to 7%).\n\n【25】##### Association between Symptoms and Prevalence of Antimicrobial Resistance\n\n【26】No association was found with presence or absence of gastrointestinal symptoms among isolates from Vietnam or among campylobacter or shigella isolates from Thailand. Thai ETEC and salmonellae that were isolated from participants with symptoms were significantly more likely to be resistant across the spectrum of antimicrobial agents studied (Mantel-Haensel summary chi square p=0.01 and p<0.01, respectively).\n\n【27】### Discussion\n\n【28】This study points out several new findings relative to antimicrobial resistance among enteric pathogens in Southeast Asia. We have documented the existence of quinolone resistance among campylobacter, shigella, and ETEC in Vietnam. If the pattern of campylobacter in Thailand holds true in Vietnam, one can expect rapidly increasing resistance rates to quinolones over the next few years in this genus. The reasons for the marked difference in resistance rates to this class of drugs between the two countries are unknown but likely reflect differences in human and veterinary use of fluoroquinolones.\n\n【29】Quinolone resistance among nontyphoidal salmonella was not seen in isolates from Vietnam, and ciprofloxacin resistance remains rare among this species in Thailand; however, progressive resistance to nalidixic acid among Thai isolates is cause for concern. Hoge  documented steadily increasing nalidixic acid resistance among nontyphoidal salmonellae in Thailand, from <1% in 1991-92 to 4% in 1993-94 and 9% in 1995. We documented 21% resistance in this genus during 1996 to 1999, more than double 1995 rates (chi square for trend, p<0.001). As resistance to nalidixic acid due to first-step resistance mutations is generally thought to precede resistance to fluoroquinolones , close continued monitoring of resistance rates to both these drugs among nontyphoidal salmonella is warranted.\n\n【30】Although cases of infections due to macrolide- and fluoroquinolone-resistant campylobacter have been reported, these have usually been among immunocompromised persons who have received multiple antibiotics . Our study is the first to document a statistical correlation between these two resistance patterns among a large number of community-acquired enteric isolates. Although the prevalence of azithromycin resistance remains relatively low in Thailand (6%) while ciprofloxacin resistance is high (77%), our study had sufficient power to confirm that the 100% correlation between resistance to these two agents was not likely due to chance. Previous studies of smaller numbers of azithromycin-resistant isolates from Thailand also support this 100% correlation . Resistance to quinolones is generally mediated by mutations in the chromosomal genes for DNA gyrase or topoisomerase IV, which are targets of action by the quinolone class, or less commonly in genes responsible for membrane flux of the drug , including plasmid-mediated enhanced efflux mechanisms. Resistance to macrolides most commonly results from mutations in the genes encoding ribosomal proteins but has also been associated with decreased permeability of the cell envelope in enterobacteriaceae, including plasmid-mediated mechanisms . Cross-resistance due to decreased membrane permeability has been documented between quinolones and chloramphenicol, trimethoprim, tetracycline, and cefoxitin, but not macrolides . Possible novel cross-resistance mechanisms in these isolates deserve to be more closely evaluated, as a common plasmid-mediated mechanism would increase the likelihood of horizontal spread.\n\n【31】Although resistance to quinolones among campylobacters from Vietnam was markedly lower than that from Thailand, multidrug resistance to other agents among Vietnamese shigella isolates was significantly higher than among Thai isolates, perhaps reflecting differences in human use patterns. In Vietnam in 1997, ampicillin, SXT, tetracycline, and chloramphenicol were the antimicrobial agents most commonly dispensed over the counter . Thai data from 10 years earlier  document common dispensing of the same four drugs by Bangkok pharmacists , although fluoroquinolones were more frequently used for children with diarrhea admitted to Thai hospitals during 1990-1992 , and quinolones have likely become increasingly used in the decade since. Our data suggest that ampicillin, SXT, tetracycline, and chloramphenicol have no reasonable role in the empiric treatment of dysentery in Vietnam and should be replaced with quinolones or macrolides for this indication. Shigellae in Thailand are uniformly sensitive to quinolones, but dysentery and travelers’ diarrhea in Thailand are predominantly caused by quinolone-resistant campylobacter rather than shigella and are thus better treated empirically with macrolides. Although ampicillin resistance among Thai shigellae was stable during the 4 years of this study, data from the preceding 15 years  demonstrate that resistance has decreased linearly (chi square for trend, p<0.001), perhaps reflecting overall reduced human use of this drug in Thailand as other agents have become more popular.\n\n【32】Concern in Thailand over the possibility of increasing rates of resistance to azithromycin developing in campylobacter species is not yet borne out by our study. Hoge  demonstrated azithromycin resistance rates of 15% in 1994 and 7% in 1995 among campylobacters. Our data show 7% resistance among isolates from 1996 to 1999, failing to confirm an upward trend.\n\n【33】Limitations of our study include the fact that isolates were obtained from multiple populations, both indigenous and U.S. military, in Thailand. However, the rates documented likely reflect overall resistance among human isolates. Additionally, the populations under study in Vietnam were all from areas around Hanoi and therefore do not necessarily reflect resistance rates seen in other parts of Vietnam.\n\n【34】This study highlights the need to be aware of regionally specific resistance rates to avoid inappropriate antibiotic use. Additionally, given the evidence for progressive resistance to the quinolone class and co-resistance between quinolones and azithromycin among salmonellae and campylobacters, the need to develop newer classes of antibiotics to treat dysentery and travelers’ diarrhea is apparent. More efficient use of antimicrobial agents, for example, avoidance of antimicrobial therapy for acute noncholera watery diarrhea and use of effective data-based antimicrobial choices for dysentery, would likely help limit the development of resistance, although the impact of these measures would be offset by the widespread availability of antimicrobial agents over the counter in both these countries. Effective vaccines against the major causes of bacterial diarrhea are being developed but are still years away from commercial production. Parallel drug development efforts are required.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f0260dea-d2f1-4601-abe1-c3874976cc72", "title": "Household Transmission of Pandemic (H1N1) 2009 Virus, Taiwan", "text": "【0】Household Transmission of Pandemic (H1N1) 2009 Virus, Taiwan\nPandemic (H1N1) 2009 was first identified in 2 southern California counties in April 2009 , and the World Health Organization declared a global pandemic on June 11, 2009 . In Taiwan, the government suggested that persons with pandemic (H1N1) 2009 remain home until 24 hours after they were symptom free . In some influenza epidemics, ≈50% of households have \\> 1 members who become infected . Further investigation into the transmission of pandemic (H1N1) 2009 virus among household members is needed to help control and prevent additional infections. We investigated the transmission of pandemic (H1N1) 2009 virus and clinical outcomes of infection within households of persons with laboratory-confirmed infection.\n\n【1】### The Study\n\n【2】During August–November 2009, we enrolled patients at the National Taiwan University Hospital who were infected with pandemic (H1N1) 2009 virus and their household members. The following samples were obtained from patients with clinical signs and symptoms suggestive of pandemic (H1N1) 2009 infection who visited the emergency department, outpatient clinics, or inpatient wards: nasopharyngeal swab specimen for rapid influenza antigen testing (QuickVue A+B test; Quidel, San Diego, CA, USA), throat swab specimen for virus isolation and novel subtype H1N1 reverse transcription PCR (RT-PCR), and blood specimen for serum hemagglutination inhibition (HI) assays. Laboratory-confirmed pandemic (H1N1) 2009 infection was defined in 3 ways: 1) isolation of influenza A virus, followed by positive RT-PCR result for pandemic (H1N1) 2009 virus; 2) positive rapid influenza A test result, followed by positive RT-PCR result for pandemic (H1N1) 2009 virus; or 3) pandemic (H1N1) 2009 virus HI titer \\> 40\\. None of the participants had received an influenza subtype H1N1 vaccine before this study.\n\n【3】Persons with laboratory-confirmed pandemic (H1N1) 2009 and their household members were sent a letter and/or received a telephone call inviting them to participate . After they accepted the invitation, we collected their case report forms, which contained data regarding the source of infection, final diagnosis, clinical manifestations, and course of the disease. The index patient was defined as the first person in a household to have laboratory-confirmed pandemic (H1N1) 2009 (body temperature >38.0°C and/or cough and/or sore throat). All enrolled index patients and their household members provided blood samples for further HI assays. A mean of 45 days (SD 26, median 36, range 12–107 days) elapsed between the first day of illness in the index patient and household investigations, including blood sampling. The household transmission rate (secondary attack rate) was defined as the percentage of household members who had laboratory-confirmed pandemic (H1N1) 2009 infection 1–7 days after the onset of symptoms in the index patient.\n\n【4】During August–November 2009, pandemic (H1N1) 2009 was confirmed for 399 patients at National Taiwan University Hospital. Of those 399 patients, 87 patients and their households were enrolled in the study; households included the 87 index patients and their 223 household contacts (172 adults and 138 children) . Of the 87 index patients, 72 (83%) had visited the hospital for illness and had laboratory-confirmed pandemic (H1N1) 2009 infection (PCR-positive test results, HI titer \\> 40, or both); the remaining 15 (17%) index patients attended community clinics and were identified by having titers \\> 40 for pandemic (H1N1) 2009 virus on HI test when household investigations were done. The possible source of infection was traced for 52 (60%) of the 87 households: 46 (53%) infections were traced to schools, 5 (6%) infections were traced to daycare centers or other child care situation (when 1 babysitter cared for a few children), and 1 (1%) infection was traced to a summer camp.\n\n【5】As shown in Table 1 , the mean ages of the index patients and their household contacts were 10.6 and 33.8 years, respectively; only 6 (7%) of the 87 index patients were adults. Households contained a mean of 1.9 children (SD 0.8, median 2, range 0–4 children).\n\n【6】Pandemic (H1N1) 2009 virus was transmitted to 60 (27%) of the 223 household contacts. The virus was transmitted to 35 (63%) of 56 child-aged siblings (but not to 1 cousin), to none of 5 adult-aged siblings, to 20 (14%) of 138 parents, to 4 (22%) of 18 grandparents, and to 1 (20%) of 5 aunts and uncles. Percentage of transmission among the different groups of household contacts differed significantly: the virus was transmitted to 35 (61%) of the 57 children and to 25 (15%) of the 166 adults (p<0.01 by χ 2  test). However, percentage of transmission among different adult groups did not differ significantly (p = 0.86 by χ 2  test). Mean interval between the onset of illness in the index patient and household members was 3.3 days (SD 2.6, median 3, range 1–6 days).\n\n【7】Of the 147 patients with pandemic (H1N1) 2009, 119 (81%) received a diagnosis of influenza-like illness; 10% received a diagnosis of upper respiratory tract infection; 3% each received a diagnosis of bronchitis, bronchopneumonia, asthma, or acute gastroenteritis; and 2% received a diagnosis of pneumonia. Of the 147 patients (all children), 10 (7%) were hospitalized and discharged without sequelae. Seventy-seven (89%) of the 87 index patients and 29 (48%) of the 60 household members received oseltamivir.\n\n【8】### Conclusions\n\n【9】We found children to be >4× more susceptible than adults to the secondary transmission of pandemic (H1N1) 2009 virus within households (61% vs. 15%). Furthermore, 93% of our index patients were children, and for ≈60% of them, the source of exposure to the virus was a school or daycare center. Thus, children play major roles in the introduction and spread of influenza within families. Vaccination and other measures will prevent susceptible children from becoming infected and reduce influenza virus transmission among families and communities.\n\n【10】This study has limitations, however, for example, the potential for nonresponse bias and possible preferential recruitment of families with sick children as index patients. Thus, adults may be relatively underrepresented as index patients in this study. Also, some adults may be less likely to go to the hospital with influenza-like symptoms.\n\n【11】In our study, the secondary attack rate in households was 27%, which is similar to rates in studies by Komiya et al. (26%), Sikora et al. (30.2%), and Looker et al. (33%) but higher than rates in studies by Cauchemez et al. (13%) and Carcione et al. (14.5%) . The secondary attack rate found in this study may have been relatively high because, without a vaccine against pandemic (H1N1) 2009, there were more susceptible children in the households and because most index patients were children who may shed virus for a longer period . Our findings show the key role that children play in introducing and spreading pandemic (H1N1) 2009 virus within households. Public health measures, such as vaccination and community health education, can prevent infections among children and help reduce virus transmission among families and the larger community.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "703882c9-8dc8-4e05-93e9-b1340a449d5d", "title": "Spotted Fever Group and Typhus Group Rickettsioses in Humans, South Korea", "text": "【0】Spotted Fever Group and Typhus Group Rickettsioses in Humans, South Korea\nHuman rickettsioses, known to occur in Korea, include mainly scrub typhus, murine typhus, and epidemic typhus. Scrub typhus, caused by _Orientia tsutsugamushi_ , a major rickettsial disease in Korea, is transmitted through the bites of mite larvae. An earlier study by Choi and colleagues reported that 34.3% of febrile hospital patients in autumn were seropositive for the disease . _Rickettsia typhi_ , transmitted by the fleas of various rodents, causes murine typhus, which is a milder form of typhus than human typhus . The first patient with murine typhus in Korea was reported in 1959. Two cases of murine typhus confirmed by culture were reported since 1988 , and now \\> 200 cases of murine typhus are presumed to occur annually in South Korea. Epidemic typhus is caused by _R_ . _prowazekii_ and is transmitted by the body louse . The disease is fatal in 10% to 30% of patients, depending on underlying diseases and the nutritional state of the host . The disease appeared after the end of the Korean War. Since 1951, however, no other cases have been reported in Korea .\n\n【1】Spotted fever group (SFG) rickettsioses are associated with arthropods, such as ticks, mites, and fleas . SFG comprises several divergent lineages: the _R_ . _rickettsii_ group, _R_ . _japonica_ , _R_ . _montana_ , the _R_ . _massiliae_ group, _R_ . _helvetica_ , _R_ . _felis_ , and the _R_ . _akari_ group . Recently, the nucleic acids of _R_ . _japonica_ and _R_ . _rickettsii_ were found in _Haemaphysalis longicornis_ in Korea . A previous seroepidemiologic study demonstrated that SFG rickettsioses were highly likely in Korea . No clinical human case of SFG rickettsioses, however, has been reported in Korea until now.\n\n【2】In this study, to check whether SFG rickettsioses were present in humans, serum specimens from patients with acute febrile disease were studied by using molecular sequence–based identification techniques. We report the presence of the _rompB_ gene of SFG rickettsiae, similar to _R_ . _akari_ , _R_ . _conorii_ , _R_ . _japonica_ , and _R_ . _felis_ , in serum specimens from Korean patients with acute febrile disease. The nucleic acids of both _R_ . _conorii_ and _R_ . _typhi_ were found to coexist in 7 serum specimens. This study presents the first molecular evidence of SFG rickettsioses in humans.\n\n【3】### Materials and Methods\n\n【4】##### Rickettsial Strains\n\n【5】The following strains were obtained from the American Type Culture Collection (ATCC; Manassas, VA, USA): _R_ . _typhi_ Wilmington (VR-144), _R_ . _prowazekii_ Breinl (VR-142), _R_ . _akari_ MK (VR-148), _R_ . _japonica_ YH (VR-1363), _R_ . _conorii_ Indian Tick Typhus (VR-597), and _R_ . _sibirica_ 246 (VR-151). These rickettsial agents were propagated in Vero (CRL-1586) or L929 (CCL-1) cell monolayers.\n\n【6】##### Serum Samples and Serologic Testing\n\n【7】The serum specimens analyzed in this study were obtained from South Korean patients with acute febrile illness from 1993 to 1999. The specimens were submitted to the Institute of Endemic Disease at Seoul National University’s Medical Research Center for laboratory diagnosis for scrub typhus, leptospirosis, and hemorrhagic fever with renal syndrome caused by hantavirus. Some of the serum specimens were used for the nucleic acid detection study of SFG rickettsial agents. The rationale for selecting the samples for polymerase chain reaction (PCR) analysis included the presence of immunoglobulin (Ig) M antibodies with titers from 1:40 to 1:160 against any of the tested antigens in the samples. Serologic testing was performed by indirect immunofluorescence assay (IFA) with a panel of 4 SFG rickettsial antigens, _R_ . _japonica, R_ . _akari_ , _R_ . _conorii_ , and _R_ . _sibirica_ , as previously described .\n\n【8】##### Oligonucleotide Primers\n\n【9】The oligonucleotide primers used for priming the PCRs are shown in Table 1 . The primers were developed on the basis of the _rompB_ gene sequences of _R_ . _conorii_ strain Seven , and the citrate synthase ( _gltA_ ) gene sequence of _R_ . _prowazekii_  was synthesized. The selection of the primers was based on the “primer 3” program , to obtain the optimal Tm and GC content and to avoid hairpin loop structures. The selected sequences were analyzed through the BLAST program .\n\n【10】##### Detection of _rompB_ Gene in Human Sera\n\n【11】DNA for PCR analysis was extracted from 200 μL of serum samples by using QIAamp Blood Mini Kit (Qiagen GmbH, Hilden, Germany) according to the manufacturer’s instructions. SFG and typhus group (TG) rickettsia _rompB_ gene in human sera were detected with multiplex nested PCR. The primary amplification of the specimen was performed in a final reaction volume of 50 μL. The reaction mixture contained 5 μL of prepared DNA sample, 20 pmol of _rompB_ outer forward primer (OF) and outer reverse primer (OR), 200 μM of deoxynucleoside triphosphate mixture (dNTP, Takara, Otsu, Japan), 1 x PCR buffer, 1.25 U Taq polymerase (Takara EX Taq, Takara), and distilled water. First, PCR reactions were incubated at 95°C for 5 min, subjected to 35 cycles of 95°C for 15 s, 54°C for 15 s, and 72°C for 30 s, and final extension at 72°C for 3 min in a GeneAmp PCR system 9600 (Perkin-Elmer Applied Biosystems, Foster City, CA, USA). After this, 2 μL of the amplified product was again amplified in a nested fashion with inner primer sets (rompB SFG IF, rompB SFG/TG IR, and rompB TG IF). The nested PCR reaction mixture contained 10 pmol of each primer in a PCR premixture tube (AccuPower PCR PreMix, Bioneer Corp. Daejon, Korea) that contained 1 U of Taq DNA polymerase, 250 μmol/L each of dNTP, 50 mmol/L of Tris-HCl (pH 8.3), 40 mmol/L of KCl, 1.5 mmol/L of MgCl 2  , and gel loading dye. The volume was then adjusted to 20 μL with distilled water. Nested PCR reactions were incubated at 95°C for 5 min, subjected to 35 cycles of 95°C for 15 s, 56°C for 15 s, and 72°C for 30 s, and final extension at 72°C for 3 min. PCR amplification of the _gltA_ gene of SFG and TG rickettsiae was performed by using the oligonucleotide pairs RpCS.877p and RpCS.1,258n for the primary PCR amplification and RpCS.896p and RpCS.1,233n for the secondary amplification. The primary PCR cycling condition consisted of incubation at 95°C for 5 min, then 35 cycles each of 15 s at 95°C, 15 s at 54°C, and 30 s at 72°C, followed by a final extension cycle of 3 min at 72°C. The nested PCR cycling condition consisted of incubation at 95°C for 5 min, then 35 cycles each of 15 s at 95°C, 15 s at 54°C, and 30 s at 72°C, followed by a final extension cycle of 3 min at 72°C. To avoid cross-contamination, 3 separate rooms with entirely separate equipment and solutions were used. Thus, the handling and treatment of samples and the addition of a template, the handling of DNA-free PCR reagents, and the post-PCR work were strictly separated. Aerosol-resistant tips (Axigen Scientific, Inc. Union City, CA, USA) were used for the handling of all reagents in the PCR study. The amplification products were visualized by electrophoresis on a 1.5% agarose gel stained with ethidium bromide (0.5 μg/mL) and using 1 x TAE migration buffer (pH 8.0; 40 mM Tris-acetate, 1 mmol/L EDTA).\n\n【12】##### Restriction Fragment Length Polymorphism (RFLP) Analysis\n\n【13】The PCR products were purified by using an AccuPrep PCR purification kit (Bioneer Corp.), according to the manufacturer’s instructions. Restriction endonuclease digestions were performed with 10 μL of amplified products by using _Alu_ I (New England Biolabs, Beverly, MA, USA). The digested DNA was resolved by electrophoresis through a 10% polyacrylamide gel at 100 V for 4 h in a 1 x TBE buffer (pH 8.0; 90 mmol/L Tris-borate, 2 mmol/L EDTA), and was visualized after staining with ethidium bromide.\n\n【14】##### Cloning, Sequencing, and Analysis of Nucleotide\n\n【15】All positive PCR products were cloned by using pGEM-T Easy Vector System I (Promega). Verifying whether the clones contained inserts was accomplished by digestion of plasmid DNA with EcoRI (New England Biolabs) and separation in 1.5% agarose gels. Plasmids containing DNA inserts were sequenced for both strands by using Big Dye Terminator Sequence Kit and ABI Prism 377 Automated DNA Sequencer (Perkin-Elmer Applied Biosystems), according to the manufacturer’s protocol. The obtained sequences, except for the primer regions, were aligned with the corresponding sequences of other rickettsiae deposited in the GenBank database to identify known sequences with a high degree of similarity using multisequence alignment programs, the Phydit software , and the MegAlign software package (Windows version 3.12e; DNASTAR, DYNASTAR Inc. Madison, WI, USA). Phylogenetic trees were generated by using the neighbor-joining algorithms and the Jukes and Cantor matrix. Bootstrap analysis was performed to investigate the stability of the trees obtained through the neighbor-joining method. The percentages of similarity were determined using the FASTA network service .\n\n【16】##### Nucleotide Sequence Accession Numbers Used\n\n【17】GenBank accession numbers of the _rompB_ gene sequences used for sequence comparisons are AB003681 for _R_ . _japonica_ , AF123705 for _R_ . _aeschlimannii_ , AF123706 for _R_ . _africae_ , AF123707 for _R_ . _akari_ , AF123708 for Astrakhan rickettsia strain A-167, AF123709 for _R_ . _australis_ , AF123711 for _R_ . _honei_ strain RB, AF123712 for Israeli tick typhus rickettsia, AF123714 for _R_ . _massiliae_ , AF123715 _R_ . _mongolotimonae_ , AF123716 for _R_ . _montanensis_ , AF123717 for _R_ . _parkeri_ , AF123719 for _R_ . _rhipicephali_ , AF123721 for _R_ . _conorii_ strain Seven, AF123722 for _R_ . _sibirica_ , AF123723 for _R_ . _slovaca_ , AF123725 for _R_ . _helvetica_ , AF182279 for _R_ . _felis_ , AF211820 for _R_ . _prowazekii_ strain Florida, AF211821 for _R_ . _prowazekii_ strain Virginia, AF123718 for _R_ . _prowazekii_ , AF161079 for _R_ . _prowazekii_ , AF479763 for _R_ . _amblyommii_ strain WB-8-2 rompB pseudogene, AY260451 for _R_ . _heilongjiangensis_ , AY260452 for _R_ . _hulinensis_ , L04661 for _R_ . _typhi_ crystalline surface layer protein (slpT) gene, and X16353 for _R_ . _rickettsii_ . The GenBank accession number of the _gltA_ gene sequence used for developing primers is M17149 for _R_ . _prowazekii_ .\n\n【18】### Results\n\n【19】##### Multiplex Nested PCR Amplification of _rompB_ Gene\n\n【20】Nested PCR assay, with primer pairs rompB OF and rompB OR in primary reactions and rompB SFG IF, rompB SFG/TG IR, and rompB TG IF in multiplex-nested reactions, was performed to identify the unknown rickettsial agents in the seropositive serum specimens and to differentiate between SFG and TG rickettsiae in terms of size. When the primers previously mentioned were used, the nested PCR assay generated ≈420 base pairs (bp) for SFG rickettsiae and about 230 bp for TG rickettsiae. The negative controls consistently failed to yield detectable PCR products, whereas the positive controls always gave the expected PCR products. Overall, 200 serum specimens from febrile patients from all areas of South Korea were tested. After the nested PCR was performed, the expected _rompB_ gene products were obtained from 24 seropositive serum samples. Figure 1 shows the result of electrophoresis of 24 PCR-amplified samples. Of the 24 amplified products, 16 showed the electrophoretic pattern of 1 DNA band of ≈420 bp, which corresponded to SFG. The amplified size of only 1 sample was ≈230 bp for TG. The 7 other amplified products showed an electrophoretic pattern of 2 bands of ≈420 bp for SFG and 230 bp for TG. Therefore, the 23 amplified products corresponding to SFG rickettsial agents were named H1 product, while the 8 products corresponding to TG were named H2 product. The H1 products included H1 to H24 (except H19), while the H2 products were H3-2, H7-2, H8-2, H13-2, H14-2, H15-2, H18-2, and H19 .\n\n【21】##### RFLP Analysis and Sequencing Analysis\n\n【22】RFLP analysis of the 23 H1 products corresponding to SFG rickettsial agents using _Alu_ I demonstrated that the restriction patterns of 17 H1 products were identical with that of _R_ . _conorii_ , 2 with that of _R_ . _akari_ , 1 with that of _R_ . _japonica_ , and 3 with that of _R_ . _felis_ . RFLP analysis of the 8 H2 products corresponding to TG rickettsial agents by using _Alu_ I showed that the restriction patterns of all the H2 products were identical with that of _R_ . _typhi_ .\n\n【23】##### Sequencing Analysis\n\n【24】To identify the SFG and TG rickettsiae detected in human serum specimens, nucleotide sequences of the PCR-amplified products were determined and compared with partial _rompB_ gene sequences of various rickettsial agents obtained from the GenBank database. Table 2 shows the similarity between the partial _rompB_ gene sequences of various rickettsial agents and 6 of the sequenced H1 products (clones H1, H3, H5, H10, H20, and H22). Clones H1, H3, and H20 showed 100%, 99.72%, and 98.87% degrees of similarity to _R_ . _conorii_ , respectively. Clone H10 showed 100% similarity to _R_ . _japonica_ , and clone H5 showed 100% similarity to _R_ . _akari_ . In particular, clone H22 showed 99.44% similarity to _R_ . _felis_ . All the compared H1 products showed low levels of similarity (70.90%–74.01%) to the TG species. The clones that clustered partially with the _rompB_ gene of _R_ . _conorii_ were differentiated in 3 groups by their levels of similarity: group 1 (12 H1 products with 100% similarity), group 2 (4 H1 products with 99.72% similarity), and group 3 (1 H1 product with 98.87% similarity). Clones H22, H23, and H24 clustered as the _R_ . _felis_ group. Table 3 shows the similarity between the partial _rompB_ gene sequences of various rickettsial species and H2 product sequences. All H2 products showed low levels of similarity (67.05%–69.94%) to SFG rickettsial species, such as _R_ . _sibirica_ , _R_ . _akari_ , _R_ . _conorii_ , _R_ . _felis_ , and _R_ . _japonica_ . They also showed high levels of similarity (93.64%–100%) to TG rickettsial species, such as _R_ . _prowazekii_ and _R_ . _typhi_ . The H2 products’ levels of similarity to _R_ . _typhi_ ranged from 99.42% to 100%. A neighbor-joining analysis based on partial _rompB_ gene sequences demonstrated that 17 H1 products formed a cluster with _R_ . _conorii_ , 2 with _R_ . _akari_ , 1 with _R_ . _japonica_ , and 3 with _R_ . _felis_ (data not shown). The analysis of the 8 H2 product sequences showed that the sequences of all H2 products formed a cluster with _R_ . _typhi_ and were separated from the SFG rickettsial strains (data not shown).\n\n【25】##### Nested PCR Amplification of _gltA_ Gene\n\n【26】The results of the multiplex nested PCR of the _rompB_ gene were confirmed by a second PCR assay with specific primer pairs RpCS.877p and RpCS.1,258 in primary reactions and RpCS.896p and RpCS.1,233 in nested reactions. The primer sets generated ≈338 bp for SFG and TG rickettsiae. The expected size of the _gltA_ gene fragment was generated in 22 of 24 samples that were positive for the PCR detection of the _rompB_ gene . All positive PCR products were cloned, and their sequences were determined. Since the PCR assay using primer sets for the amplification of the _gltA_ gene could not discriminate between the SFG rickettsia and TG rickettsia by size difference, the sequences of 3 clones for each PCR product were determined. The results of the sequencing analysis for _gltA_ \\-PCR amplifications were identical to those of the analysis of the rompB-PCR product (data not shown). Seven samples that were positive for both the _rompB_ genes of _R_ . _conorii_ and _R_ . _typhi_ were also positive for both of their _gltA_ genes (data not shown).\n\n【27】### Discussion\n\n【28】SFG and TG rickettsial infections occur worldwide and may cause serious diseases in humans. These pathogenic bacteria are transmitted to people by arthropod vectors, such as ticks, fleas, and lice. In this study, multiplex-nested PCR was conducted to detect and identify SFG and TG rickettsial antigens in patient sera with positive results from the serosurvey. The _rompB_ gene domain II region, which is a highly conserved region of _rompB_ , was targeted for PCR amplification for the specific detection of SFG and TG rickettsiae. Amplified DNA sequences were analyzed by using nucleotide-sequencing methods, and RFLP analysis was used to confirm the PCR results. The results indicated the presence of several SFG rickettsiae, _R_ . _conorii_ , _R_ . _akari_ , _R_ . _japonica_ , and _R_ . _felis_ , in the serum specimens. The results were also confirmed by a second PCR with specific primer pairs for the _gltA_ gene and by sequence analysis of its DNA amplicons.\n\n【29】For the first time, SFG rickettsiae in human serum specimens in South Korea have been reported. _R_ . _akari_ is a member of the spotted fever group rickettsiae and is a causative agent of rickettsial pox, a disease transmitted by the bite of _Allodermanyssus sanguineus_ , a mite ectoparasite of the domestic mouse ( _Mus muscularis_ ) . The disease was first described in New York City in 1946. _R_ . _akari_ was isolated from the Korean vole in 1957. The previous seroepidemiologic study conducted by the authors on 3,401 patients with febrile disease indicated that the seropositive rate was 16.24% for the rickettsial antigen through IFA. _R_ . _conorii_ is an etiologic agent of the Mediterranean spotted fever or boutonneuse fever . Our previous study indicated that the seropositive rate was 14.34% for the antigen. _R_ . _japonica_ , the causative agent of Oriental spotted fever, was first isolated from a patient with febrile, exanthematous illness in Japan in 1985 . The disease is now endemic in the southwestern part of Japan, where >100 cases have been described . Previous studies showed the presence of nucleic acids of _R_ . _japonica_ and _R_ . _rickettsii_ in _H. longicornis_ by PCR. Our seroepidemiologic study demonstrated that the seropositive rate was 19.9%. Although no clinical human case of SFG rickettsioses has been reported in Korea until now, this study’s findings strongly suggest the prevalence of SFG rickettsiosis in Korea.\n\n【30】_R_ . _felis_ is an emerging pathogen responsible for fleaborne spotted fever and had been considered a member of the TG rickettsiae based on its reactivity with anti– _R_ . _typhi_ antibodies. A genetic analysis of the 16S rRNA, citrate synthase, _rompA_ , and _rompB_ genes, however, placed _R_ . _felis_ as a member of SFG. _R_ . _felis_ has been reported in various countries, including the United States, Mexico, Brazil, Germany, and France . In Asia, the first case of _R_ . _felis_ infection was reported in 2003 . _R_ . _typhi_ was also among those detected in the SFG rickettsiae in the febrile disease patients’ sera. Fleas are also found to be vectors for _R_ . _typhi_ . Of major importance to the epidemiology of the rickettsioses caused by _R_ . _typhi_ and _R_ . _felis_ is the maintenance of both rickettsial agents in their hosts by transovarial transmission, and the fact that neither organism is lethal for fleas .\n\n【31】Finally, we report the presence of both _R_ . _conorii_ and _R_ . _typhi_ in serum from Korean patients. Sera from patients with SFG rickettsiosis have been reported to react with TG rickettsiae by using serologic analysis methods . The serum specimens from patients with TG rickettsiosis were also demonstrated to contain cross-reactive antibodies against SFG rickettsiae . In a previous study, approximately one third of specimens seropositive for antibodies against SFG rickettsiae had antibodies against TG rickettsiae . Therefore, the multiplex-nested PCR was designed to detect and differentiate SFG rickettsial agents from TG rickettsial agents in the patient serum specimens with positive results from the serosurvey with SFG rickettsial antigens. SFG rickettsiae and TG rickettsiae were differentiated in terms of the size of amplified products. PCR results also confirmed the RFLP and sequencing analysis. In sera taken from 7 patients, both SFG and TG rickettsial antigens were detected, which indicated dual infection. Previously, a case of dual infection with _Ehrlichia chaffeensis_ and an SPG rickettsia was reported in a human patient. Cases of dual infection with _Bartonella clarridgeiae_ and _B_ . _henselae_ in cats have also been reported, as well as infection with the 2 different genotypes of _B_ . _henselae_ . A recent report suggested that coinfection of _R_ . _felis_ with either _B_ . _clarridgeiae_ or _B_ . _quintana_ in fleas may cause dual infection in a human that comes in contact with flea feces . These reports support this study’s findings regarding the dual infection of SFG and TG rickettsiae in 7 patients. The differences between _R_ . _conorii_ and _R_ . _typhi_ vectors _,_ however, still cannot be explained, and further studies are needed.\n\n【32】In conclusion, this study confirmed, by using PCR-based amplification methods, that several SFG rickettsiae, _R_ . _conorii_ , _R_ . _akari_ , _R_ . _japonica_ , and _R_ . _felis_ , existed in the sera of Korean patients with febrile episodes. Our findings indicate that SFG rickettsiae, including _R_ . _felis_ , should be used in serologic tests on Korean patients suspected of having rickettsiosis. TG rickettsiae existed in 8 patients, and 7 of them were also infected with _R_ . _conorii_ . The evidence of double infection is expected to help describe the cross-reactivity between the patient sera of SFG rickettsioses and TG rickettsioses.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1709f3c8-9946-400a-950a-15be1e549f20", "title": "Stakeholders’ Interest in and Challenges to Implementing Farm-to-School Programs, Douglas County, Nebraska, 2010–2011", "text": "【0】Stakeholders’ Interest in and Challenges to Implementing Farm-to-School Programs, Douglas County, Nebraska, 2010–2011\nAbstract\n\n【1】**Introduction**  \nSchools are uniquely positioned to influence the dietary habits of children, and farm-to-school programs can increase fruit and vegetable consumption among school-aged children. We assessed the feasibility of, interest in, and barriers to implementing farm-to-school activities in 7 school districts in Douglas County, Nebraska.\n\n【2】**Methods**  \nWe used a preassessment and postassessment survey to obtain data from 3 stakeholder groups: school food service directors, local food producers, and food distributors. We had a full-time farm-to-school coordinator who was able to engage multiple stakeholders and oversee the development and dissemination of a toolkit. We used descriptive statistics to make comparisons.\n\n【3】**Results**  \nSeven food service directors, 5 distributors identified by the food service directors, and 57 local producers (9 completed only the preassessment survey, 16 completed only the postassessment survey, and 32 completed both) completed various components of the assessment. Interest in pursuing farm-to-school activities to incorporate more local foods in the school lunch program increased during the 2-year project; mean interest in purchasing local foods by food service directors for their districts increased from 4.4 to 4.7 (on a scale of 1 to 5).\n\n【4】**Conclusion**  \nImplementing farm-to-school programming in Douglas County, Nebraska, is feasible, although food safety and distribution is a main concern among food service directors. Additional research on feasibility, infrastructure, and education is recommended.\n\n【5】Introduction\n\n【6】Despite recommendations of the 2010 Dietary Guidelines for Americans to consume 5 to 13 servings of fruits and vegetables daily to reduce risk of cardiovascular disease and obesity, many school-aged children in the United States fail to meet recommendations . Few children (17.6% male, 19.8% female) and adolescents (37.0% male, 28.3% female) report eating the recommended 5 servings of fruits and vegetables daily in a national sample, and in Douglas County, Nebraska, even fewer (<10%) children meet recommendations . Schools are uniquely positioned to influence the dietary habits of children , because they typically provide 2 daily meals . Farm-to-school (F2S) programs can increase fruit and vegetable consumption among school-aged children .\n\n【7】The number of F2S programs is growing across the United States, increasing from 400 participating schools in 2003  to almost 10,000 in 2012 . The primary focus of F2S programs is to connect schools with local farms to incorporate healthful foods into school cafeterias, improve student nutrition, provide health and agricultural education, and support local producers . Many F2S programs include curricula such as experiential education, taste testing, and farm tours . Case studies of school programs suggest that incorporating local foods into school meals increases fruit and vegetable consumption and school meal participation . In addition, food service directors (FSDs) report that they are motivated to procure from local farmers to support their community . However, few community-based evaluations of F2S programs, particularly those that address the logistics and feasibility of implementation and procurement, have been conducted.\n\n【8】We sought to build the capacity for F2S programs in Douglas County schools by developing relationships necessary for procurement of local foods and providing a toolkit aimed at 3 stakeholder groups: FSDs, local food producers, and food distributors. We developed and conducted surveys with these stakeholders to evaluate the process of implementing a new F2S program.\n\n【9】Methods\n\n【10】The Douglas County F2S program focused on procurement, encouraging FSDs to purchase foods from local producers. This project was funded as part of the Centers for Disease Control and Prevention Communities Putting Prevention to Work, a 2-year grant awarded to the Douglas County Health Department . Preassessment occurred from June through September 2010, and postassessment occurred from September through December 2011. All evaluation procedures were approved by the University of Nebraska Medical Center Institutional Review Board.\n\n【11】All FSDs from the Douglas County school districts (n = 7), distributors identified by FSDs (n = 5), and a convenience sample of local producers (41 preassessment, 48 postassessment) were recruited to complete preassessment and postassessment surveys tailored for each group. Douglas County is located in eastern Nebraska, includes a mix of urban and suburban communities (population = 524,861), and is the most racially/ethnically diverse county in Nebraska  .\n\n【12】Engaging community members and stakeholders is a critical step to promote F2S, which was a new idea for this community and required capacity building. In early 2010, we identified FSDs and conducted in-person meetings with them to gain a better understanding of the operations of each district. We also met with local producers to gauge interest in and capacity for selling to schools. We worked directly with the stakeholders to minimize perceived barriers and provide opportunities for developing necessary relationships and developed an online toolkit  for use by the 3 stakeholder groups. This toolkit is comprehensive, discusses general F2S concepts, and provides information and resources for the stakeholders. Throughout the project, we conducted activities and met several milestones (eg, creating procurement policy, holding stakeholder meetings, and developing and launching a toolkit) to increase procurement of local foods in the 7 school districts.\n\n【13】We administered preassessment and slightly modified postassessment surveys (ie, some items anticipated to remain static were not readministered) to each stakeholder group. Questions were derived from the literature  and supplemented as needed to assess key factors related to local food procurement. The postassessment survey determined changes in practices and attitudes that may have occurred among the stakeholders since the initial preassessment survey. Surveys used a 5-point Likert scale (1= strongly disagree to 5 = strongly agree). The FSD survey (49 items) assessed basic information on meal programs, facility capacity, food purchasing, local food practices, attitudes toward local food, perceived barriers, and interest in participating in F2S programming. The producer survey (35 items) assessed the type of production, selling practices, willingness to participate in F2S, and insurance and safety practices. The distributor survey (20 items) assessed service area and distribution practices, sales to schools, local food practices, and willingness to participate in F2S programs.\n\n【14】Surveys were distributed by multiple methods (ie, conducted online via a survey collection program, mailed, hand delivered, and conducted over the telephone) to reach all stakeholders and took approximately 30 to 45 minutes to complete. For both the FSDs and the distributors, survey responses were matched between preassessment and postassessment. For the producers, responses were matched (with a reduced sample, n = 32); otherwise, preassessment data are indicated.\n\n【15】Data were analyzed using SPSS version 19.0 (SPSS, Chicago, Illinois). Descriptive statistics were obtained to examine the distribution of the responses and to calculate frequencies, means, and standard deviations (SDs).\n\n【16】Results\n\n【17】### Food service directors\n\n【18】All FSDs completed the surveys. School districts varied in size, ranging from 108 schools (including elementary, middle, and high schools) and 50,461 students to 3 schools and 631 students . The average cost for lunch was $2.20. Some districts had many salad bars (n = 35), and others had only a few. Kitchen environments of districts varied; FSDs indicated that most kitchens operated on “semi-prepared” ability, meaning that they had satellite kitchen systems in which meals are shipped within districts daily.\n\n【19】At preassessment, FSDs generally agreed that their facilities had capacity to handle serving local foods in schools and had flexibility in producing delivery schedules (mean score, 3.9; SD = 1.0) . At postassessment, they indicated lesser agreement on capacities (mean score, 3.4; SD = 1.1), although they maintained agreement in facilities’ capacity. FSDs reported greater agreement that they had adequate dry storage at postassessment than they did at preassessment.\n\n【20】In general, FSDs’ willingness to participate in F2S improved from preassessment to postassessment; small increases were seen in interest in purchasing local foods for their district (from 4.1 \\[SD = 0.8\\] to 4.2 \\[SD = 0.9\\]) and willingness to pay higher prices for locally produced foods (from 3.4 \\[SD = 0.5\\] to 3.5 \\[SD = 0.8\\]). All FSDs added a signed memorandum, stating a preference for local whenever possible, to their operating procedures. In terms of attitudes toward the benefits of local foods, though agreement decreased slightly at postassessment, respondents generally indicated agreement that local foods supported the economy, increased students’ fruit and vegetable consumption, responded to public demand, and provided higher-quality products. FSDs indicated that overall barriers to purchasing local foods increased slightly, although the ability to find farmers to purchase from and liability and farmer compliance with food safety and handling standards decreased. Across school districts, the response to barriers varied; smaller school districts reported more increases in barriers than did larger school districts . FSDs reported an overall increase in interest in conducting some F2S activities, including asking current vendors to sell local products, highlighting local foods on menus, and hosting local food events (eg, fall harvest festivals). Attitudes toward F2S activities decreased slightly for the following: planning menus around seasonal products each month and buying and highlighting local products each month.\n\n【21】### Local producers\n\n【22】Forty-one local producers responded to the survey in 2010 and 48 in 2011. At preassessment, producers reported producing a range of foods, including vegetables (53%), meat (33%), and dairy (27%). Respondents were from 34 cities in 22 counties in Nebraska (86% in 2010; 89% in 2011), Iowa (12% in 2010; 11% in 2011), and South Dakota (2% in 2010). Of the 57 producers who responded to the survey, 15.8% (n = 9) completed only the preassessment survey, 28.1% (n = 16) completed only the postassessment, and 56.1% (n = 32) completed both and were included in the main pre–post analysis. At preassessment, 24% of producers reported having hoop houses (ie, greenhouse alternatives made of a plastic roof and flexible piping that provide season extension to farmers in colder climates), and at postassessment 55% of producers reported having hoop houses.\n\n【23】Some assessment was completed by the 32 producers because they completed both the preassessment and the postassessment. Among the 32 producers who completed both the preassessment and postassessment, most reported willingness at both preassessment (mean score = 4.7; SD, 0.7) and postassessment (mean score = 4.2; SD, 1.1) to sell to local schools. At postassessment, the number of producers who reported selling to other institutions (eg, hospitals) increased (from 77% to 79%). Most producers reported having a vehicle available for delivery (96% in 2010, 84% in 2011). However, fewer had cold storage (79% in 2010, 58% in 2011) or a staff driver (39% in 2010, 32% in 2011). High levels of willingness to participate in F2S activities were reported at both preassessment and postassessment (mean score, range = 3.7–4.8), with slight decreases overall. Producers expressed willingness to offer taste testing to staff and children, lead guided tours of their farm for staff and children, visit schools or classrooms and speak about their farm products and how they are grown, and join a consortium of producers . At postassessment, 78% (n = 32) had liability insurance, averaging $1,432,143 (range = $200,000–$5,000,000).\n\n【24】Some assessment was completed among 41 producers who reported at preassessment (32 of these also completed the postassessment). At preassessment (n = 41), only 7% of the producers responded affirmatively to having Hazard Analysis and Critical Control Points and only 7% reported being certified for Good Agricultural Practices — both of which are food safety components that some schools require. In addition, 78% of the respondents reported having insurance for their general operations and insurance that would cover children and staff on field trips; however, 15% of all surveyed responded that they would not be willing to host field trips.\n\n【25】Several barriers to selling to schools were identified. At postassessment, the barriers with the highest agreement across all producers included inability to provide product during the entire school year, inability to produce sufficient volume of produce, and inability to provide the most competitive price.\n\n【26】### Distributors\n\n【27】All 5 of the distribution company representatives identified by the FSDs completed the preassessment and postassessment surveys. At preassessment, distributors reported that, on average, 13% of their business went to schools; at postassessment, this was reduced to 10%. At preassessment and postassessment, all 5 distributors reported some local purchasing within the past year from a variety of local producers. At preassessment, 4 of the 5 distributors required producers to have a certain level of liability insurance and food safety standards and meet packing specifications.\n\n【28】At preassessment, distributors reported overall willingness to change some of their practices to focus more on local foods (mean score = 4.4; SD, 1.1), which decreased at postassessment (mean score = 3.8; SD, 0.8). Increases occurred from preassessment to postassessment, indicating distributors would buy more local food if it were available year-round, producers had sufficient quantity of products, producers met food safety standards, a local food hub or packing house were available for central pick-ups, local pricing were competitive, local producers met customer specifications, and there were greater demand for local products.\n\n【29】Discussion\n\n【30】This study aimed to determine the feasibility of F2S in schools in Douglas County, Nebraska, by assessing the needs of, attitudes of, barriers to, and capacity among school FSDs, producers, and distributors regarding procurement of local foods before and after the implementation of a F2S program. Overall, the piloted F2S program demonstrated success and sustainability across all districts. Before the program, no districts were purchasing locally produced food. At postassessment, most districts responded that they planned on purchasing local food during the next school year. Despite the evaluation indicating that small increases in many of the perceived barriers, small decreases in perceptions of facility capacities, and some shifts in attitudes away from willingness to participate in a F2S program among key stakeholders, constructs assessed remained favorable toward F2S activities.\n\n【31】Overall, FSDs’ perceptions of their facilities’ capacities were positive, even with slight reported decreases. They reported having sufficient cold storage, dry storage, preparation space, and equipment for handling whole foods, training for staff to prepare whole fruits and vegetables, and time necessary to use whole fruits and vegetables in lunches. These findings are important, because most districts operate using a satellite kitchen system. After gaining experience with this F2S program, FSDs agreed that they could implement the necessary preparation with current resources. Perceptions of barriers remained low at preassessment and at postassessment. Larger school districts reported more reduction in perceived barriers, whereas smaller school districts reported more increases in perceived barriers. FSDs reported willingness to pay higher prices for local foods, which suggests that cost may not be a major barrier . Similarly, a study of FSDs from 7 F2S programs found that FSDs reported that prices associated with local food procurement were acceptable and competitive . Findings from another study indicated that top barriers to procuring local foods in schools included seasonal availability and lack of partially processed product . FSDs’ willingness to participate in F2S and recognition of the benefits of purchasing local foods increased during the project. Interestingly, the larger school districts were more engaged than the smaller school districts throughout the project with more reported activities such as salad bars, labeling local foods on school menus, and farmers markets at school. The FSDs signed a memorandum that stated a preference for local products whenever possible, establishing an informal policy that was easier to accomplish than establishing larger formal policies.\n\n【32】Producer’s levels of willingness to offer F2S education remained high from preassessment through postassessment. Many producers reported an inability to sell produce to schools year-round as a major barrier. However, incorporating dairy and meat may support year-round participation, as other F2S programs located in cold-weather climates made this a practice . In addition, the use of seasonal extension methods (eg, hoop houses are viable options for producers to extend the growing season ) increased during the project, further supporting year-round production feasibility. Season extension has been noted as a strategy for producers, given increased demand for local production . Producers identified other issues such as liability insurance, which would be important for students engaging in educational activities at local farms. It became apparent throughout this project that there was a lack of infrastructure to support sourcing from local producers. Producer willingness to join a consortium, such as one to sell produce to schools or a food hub, could be a viable way of providing local food to larger institutions, such as schools . To expand F2S programs and solidify the links between producers and school programs, it is important to consider the needs and motivations of the producers . One community has found that both growers and buyers were supportive of the development of a food hub as a way to increase availability of local foods .\n\n【33】Distributors are the link between producers and schools, and few previous F2S studies assessed how this key stakeholder can influence success in local food procurement. Distributors were the most difficult stakeholder to engage throughout this project, which could be explained by findings of a similar study that found that distributors considered budget constraints and the need for large orders to cover overhead costs major barriers to participating in F2S . This difficulty in engaging distributers may also be related to an increase in perceived barriers. Specifically, the barriers that increased the most were availability of products year-round and ability to provide competitive pricing, which may be related to the fact that key stakeholders were not previously exposed to F2S. To overcome these barriers, we provided education on season extension, made products available year-round, and specified local products in the bidding procedure. Other communities have shown that engaging distributors to track and promote local purchases is a successful strategy . The negative shift in distributor attitudes and perceptions could also be due to first-time exposure to F2S.\n\n【34】Our study has limitations, most of which are related to the self-reported nature of the data collected from stakeholders, which could potentially introduce social desirability and recall bias. Because studying F2S is new, we used surveys that were modified versions of existing surveys, which have not undergone validity and reliability testing. Additionally, convenience sampling used for producers at both preassessment and postassessment resulted in only one-third completing both surveys, potentially introducing bias.\n\n【35】Despite these limitations, this study adds to the literature, as few F2S programs have evaluated the feasibility of procurement. Additionally, it helped identify barriers and sustainable solutions in a geographic area where F2S has not previously been evaluated. The findings of this study augment evidence of programs designed to increase procurement of healthful foods in school settings through partnership with local stakeholders. The data were collected from a diverse sample of stakeholders from the community, representing all aspects of the school food system continuum.\n\n【36】These findings and the online toolkit can inform future interventions for communities interested in implementing procurement-based F2S programs. Future studies should explore how enhanced infrastructure, like food hubs, can help FSDs procure more local foods. Enhancing the link between production, distribution, and procurement can strengthen the local food system and ultimately increase consumption of local foods.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "463e786a-e57a-4a94-a1de-1130c0aead1a", "title": "The Worriers' Guild", "text": "【0】The Worriers' Guild\nToday there is a meeting\n\n【1】of the Worriers' Guild,\n\n【2】and I'll be there.\n\n【3】The problems of Earth are\n\n【4】to be discussed\n\n【5】at length\n\n【6】end to end\n\n【7】for five days\n\n【8】end to end\n\n【9】with 1100 countries represented\n\n【10】all with an equal voice\n\n【11】some wearing turbans and smocks\n\n【12】and all the men will speak\n\n【13】and the women\n\n【14】with or without notes\n\n【15】in 38 languages\n\n【16】and nine different species of logic.\n\n【17】Outside in the autumn\n\n【18】the squirrels will be\n\n【19】chattering and scampering\n\n【20】directionless throughout the town\n\n【21】because\n\n【22】they aren't organized yet.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b600ad33-470d-47c7-bbc4-2f334af96f6d", "title": "Shigella spp. Antimicrobial Drug Resistance, Papua New Guinea, 2000–2009", "text": "【0】Shigella spp. Antimicrobial Drug Resistance, Papua New Guinea, 2000–2009\n**To the Editor:** Approximately half the _Shigella_ spp. infections in developing countries are caused by endemic shigellae  _,_ which in these countries are responsible for ≈10% of all episodes of diarrhea among children <5 years of age and up to 75% of deaths from diarrhea . Deaths from epidemic _Shigella_ spp. in the community are estimated to outnumber deaths within the healthcare setting. In Papua New Guinea, diarrhea is a major cause of hospital admission and death ; _Shigella_ spp. are among the most common causes of enteric bacterial infection  _,_ and _S. flexneri_ is the most common serotype  _._ Outbreaks of bloody diarrhea are frequently reported; however, diagnosis in remote settings is challenging, partly because the storage requirements for the organism are difficult to meet.\n\n【1】Multidrug resistance of shigellae is not new ; many countries have reported resistance to amoxicillin, co-trimoxazole, and chloramphenicol. For this reason, the World Health Organization recommends that all patients with bloody diarrhea be treated with either ciprofloxacin or 1 of the 3 second-line drugs: pivmecillinam, azithromycin, and ceftriaxone  _._ The antimicrobial drug currently recommended for patients with bloody diarrhea in primary healthcare settings in Papua New Guinea is co-trimoxazole ; ciprofloxacin is available only in hospitals.\n\n【2】In August 2009, an epidemic of multidrug-resistant _S. flexneri_ infection associated with widespread illness and death across 4 provinces of Papua New Guinea was reported to health authorities. To understand the trends and to inform antimicrobial drug policy makers, we reviewed retrospective microbiological data for 2000–2009. With the exception of 3 isolates collected during an outbreak in the border regions of the 4 provinces during 2009 (excluded from analysis), all isolates in our study were obtained as part of routine surveillance. Fecal samples were collected by clinicians from any patient seeking care for severe diarrhea at Port Moresby General Hospital.\n\n【3】Before serologic testing was conducted, samples were spread directly on desoxycholate citrate agar and MacConkey agar plates for culture. Antimicrobial drug resistance testing was performed by using the Kirby-Bauer method.\n\n【4】From a total of 3,419 fecal samples cultured, 136 (4.0%) were positive for _Shigella_ spp. The most commonly isolated species was _S. flexneri_ (90.4%); less frequently isolated were _S. boydii_ (3.7%), _S. dysenteriae_ (2.9%), and _S. sonnei_ (1.5%). Of the 123 _S. flexneri_ isolates, 20 (16%) were further characterized; the most frequent serovars were serovar 2 (40%) and serovar 3 (30%). Many (48%) _Shigella_ spp.–positive isolates were from children <5 years of age. The highest rates of antimicrobial drug resistance of all _Shigella_ spp. were to amoxicillin (96%), co-trimoxazole (86%), and chloramphenicol (60%); no resistance to ciprofloxacin and cephalexin was found .\n\n【5】Current evidence supports the use of ciprofloxacin, ceftriaxone, and pivmecillinam for treatment of bloody diarrhea  _._ It also suggests that dysentery rarely relapses if an infected child has received a full course of treatment with 1 of these drugs and the causative pathogen is sensitive to the drug. Reducing the risk for relapse of bacterial infections among children is beneficial because it reduces the likelihood of subsequent episodes of dysentery occurring in that child and of transmission to others  _._ In our study, most isolates were resistant to co-trimoxazole and the other available antimicrobial drugs, indicating that their use would not have reduced illness and subsequent transmission in this setting. The lack of resistance to ciprofloxacin and cephalexin indicates that these drugs may be more effective; however, they are neither available at the primary healthcare level nor recommended in Papua New Guinea, which is cause for concern.\n\n【6】Surveillance for antimicrobial drug resistance is essential for the containment of antimicrobial drug resistance globally. However, international surveillance depends on strong national surveillance systems. Despite the existence of a network of subnational laboratories where fecal sample cultures had been performed, these laboratories no longer perform these cultures. In 1964, the laboratory in 1 provincial hospital analyzed and subtyped 1,000 stool samples over a 15-month period  _._ In our study, conducted at the national referral hospital (which limits the representativeness), we analyzed 3,419 fecal samples over a 10-year period.\n\n【7】Outbreaks of bloody diarrhea are common in remote settings in Papua New Guinea, yet with the exception of the 3 isolates from 2009 that were excluded from analysis, no _Shigella_ spp.–positive samples have been identified during outbreaks. Molecular methods may serve as an adjunct to traditional laboratory methods by improving sensitivity and also enabling diagnosis of _Shigella_ spp. outbreaks among remote populations where specimen storage and transport requirements may be challenging  _._\n\n【8】We describe extremely high rates of resistance of _Shigella_ spp. to co-trimoxazole, the recommended treatment for bloody diarrhea in Papua New Guinea. Strengthening national surveillance for antimicrobial drug resistance would provide the evidence to better inform policy decision makers. A review of the national antimicrobial drug policy for management of bloody diarrhea is urgently needed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "418b28ca-952a-47e5-b5e3-a9c644352ab2", "title": "Bartonella henselae and B. koehlerae DNA in Birds", "text": "【0】Bartonella henselae and B. koehlerae DNA in Birds\n**To the Editor** : Bartonellosis, a globally emerging vector-borne zoonotic bacterial disease, is caused by hemotropic, gram-negative, aerobic, facultative intracellular _Bartonella_ spp. Of the 30 _Bartonella_ species/subspecies _,_ 17 have been associated with human infections . Each species has a reservoir host(s), within which the bacteria can cause intraerythrocytic bacteremia with few or no clinical signs of illness ; the bacteria are transmitted by hematophagous arthropod vectors . Various _Bartonella_ spp. have been identified in domestic and wild animals, including canids, deer, cattle, rodents, and marine mammals . _Bartonella_ DNA from the blood of loggerhead sea turtles ( _Caretta caretta_ ) has been PCR amplified and sequenced ; the fact that _Bartonella_ DNA was found suggests the possibility that persistent blood-borne infection can occur in nonmammals and that the host range for _Bartonella_ spp. may be larger than anticipated.\n\n【1】Growing evidence suggests that wild birds play key roles in the maintenance and movement of zoonotic pathogens such as tick-borne encephalitis virus and _Borrelia_ and _Rickettsia_ spp. _Bartonella grahamii_ DNA was amplified from a bird tick in Korea . The substantial mobility, broad distribution, and migrations of birds make them ideal reservoir hosts for dispersal of infectious agents. To investigate whether birds might be a reservoir for _Bartonella_ spp. we screened 86 birds for the presence of _Bartonella_ spp. DNA.\n\n【2】The primary study site was a residential backyard in Morehead City, North Carolina, USA (34°43.722′N, 76°43.915′W). Of the 86 birds screened, 78 (16 species) were captured by mist net during March 2010–June 2012 and 8 (3 species) were injured birds that were to be euthanized . Each bird was examined for external abnormalities and ectoparasites, weighed, measured, and tagged with a US Geological Survey–numbered band. A blood sample (0.10–0.25 mL) was collected from each bird by using a 1-mL insulin syringe with a 28-gauge × 1.27-cm needle. Blood remaining after preparation of blood smears was added to an EDTA tube and frozen (−80°C) until processed. Blood smears were examined for hemoparasites. Research was conducted under required state and federal bird banding permits and with the approval of the North Carolina State University Institutional Animal Care and Use Committee.\n\n【3】Before DNA was extracted from the samples, 10 μL of blood was diluted in 190 µL of phosphate-buffered saline. DNA was automatically extracted by using a BioRobot Symphony Workstation and MagAttract DNA Blood M96 Kit (QIAGEN, Valencia, CA, USA). _Bartonella_ DNA was amplified by using conventional _Bartonella_ genus PCR primers targeting the 16S–23S intergenic spacer region: oligonucleotides, 425s (5′-CCG GGG AAG GTT TTC CGG TTT ATCC-3′) and 1,000as (5′-CTG AGC TAC GGC CCC TAA ATC AGG-3′). Amplification was performed in a 25-μL reaction, as described . All PCR reactions were analyzed by 2% agarose gel electrophoresis. Amplicons were sequenced to identify the _Bartonella_ sp. and intergenic spacer region genotype. To compare sequences with those in GenBank, we identified bacterial species and genotypes by using Blast version 2.0 . DNA extraction and PCR-negative controls remained negative throughout the study.\n\n【4】Results are summarized in the Table . None of the screened birds were anemic, but 5 were PCR positive for _Bartonella_ spp. (3 for _B. henselae_ and 2 for _B. koehlerae_ ). _B. henselae_ was amplified from 2 Northern Mockingbirds ( _Mimus polyglottos_ ) and 1 Red-winged Blackbird ( _Agelaius phoeniceus_ ) . The DNA sequences were identical to each other and had 99.6% (456/457 bp) sequence similarity with _B. henselae_ San Antonio 2 intergenic spacer region genotype . _B. koehlerae_ was amplified from a Red-bellied Woodpecker ( _Melanerpes carolinus_ ) and a Common Loon ( _Gavia immer_ ) . The DNA sequences were identical to each other (404/404 bp) and to GenBank sequence AF312490. Lice (Mallophaga order) were found on 5 Boat-tailed Grackles ( _Quiscalus major_ ), but no ectoparasites were observed on _Bartonella_ spp.–positive birds. Hemoparasites ( _Haemoproteus_ and _Plasmodium_ spp.) were detected in 7 of 86 birds, indicating exposure to hematophagous ectoparasites, but hemoparasites were not detected in the _Bartonella_ spp.–positive birds. No bacteria were visualized in _Bartonella_ PCR–positive blood smears.\n\n【5】_Bartonella_ spp. are increasingly associated with animal and human illnesses; thus, the identification of reservoirs and increased understanding of _Bartonella_ spp. disease ecology are of public health importance. Our finding of 2 pathogenic species not previously reported in birds has expanded the potential sources for zoonotic infection.\n\n【6】There is growing evidence that migratory birds serve as reservoirs and/or mechanical vectors for pathogens such as tick-borne encephalitis virus and _Rickettsia_ spp. Birds have been implicated as reservoirs for several _Borrelia_ spp. and for possible dispersion of other tick-borne pathogens (e.g. _Anaplasma_ and _Bartonella_ spp.) . Tick transmission of _Bartonella_ spp. to birds should be investigated, and additional studies that investigate the reservoir host range of _Bartonella_ spp. and the transmission of these bacteria to non–host species will improve epidemiologic understanding of bartonellosis and will identify additional risk factors for _Bartonella_ spp. transmission to new hosts, including humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "130aea4e-fc30-4809-b3ce-1a6c622a3e71", "title": "In-Flight Transmission of SARS-CoV-2", "text": "【0】In-Flight Transmission of SARS-CoV-2\nIn 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emerged in China, ultimately causing the coronavirus disease (COVID-19) pandemic. Many persons with SARS-CoV-2 infection have since flown into and out of COVID-19–affected areas . Some countries quarantine arriving passengers. Airports are also screening passenger body temperatures before boarding and after arrival. Recent investigations have shown that SARS-CoV-2 can be transmitted before symptom onset, posing a challenge to outbreak control . Although risks for SARS-CoV-2 transmission have been extensively investigated, in-flight transmission of the virus has not been formally confirmed. Airline staff members have voiced concerns over acquisition of SARS-CoV-2 infection . Given that flights are still departing to and from COVID-19–affected countries, determining whether in-flight transmission of SARS-CoV-2 occurs is essential.\n\n【1】### The Study\n\n【2】We examined public records for 1,110 persons with laboratory-confirmed COVID-19 in Hong Kong, China, recorded from January 23 through June 13, 2020; we used Centre for Health Protection (CHP) public records and the Vote4HK COVID-19 in HK database for case-patients who had traveled before diagnosis . At the time, the Hong Kong government had yet to introduce mandatory quarantine and airport screening . We identified a cluster of 4 persons with COVID-19 (henceforth referred to as patients A–D) associated with a commercial flight that departed from Boston, Massachusetts, USA, on March 9 and arrived in Hong Kong on March 10, 2020. The airplane, a Boeing 777-300ER, flew for »15 hours and carried a maximum of 294 passengers. The cluster comprised 2 passengers and 2 cabin crew members. Although these persons did not fulfill the criteria for SARS-CoV-2 testing at the time of arrival, results of reverse transcription PCR conducted in local healthcare settings within 5–11 days of arrival were positive. All 4 case-patients subsequently recovered .\n\n【3】Patients A and B were a married couple. Patient A was a 58-year-old man with underlying disease who sat in a window seat in business class on the airplane . On March 10, fever and productive cough developed; on March 13, he had mild abdominal discomfort, followed by diarrhea 2 days later. His 61-year-old wife, patient B, also had underlying illness. She sat directly in front of him in a business class window seat. On March 10, she had a sore throat. One day later, fever and cough developed. As their symptoms evolved, they sought healthcare and were hospitalized on March 14. On March 15, respiratory samples (collected March 14 for patient A and March 15 for patient B) were positive for SARS-CoV-2. No public record indicates what their underlying diseases were or whether these 2 passengers were symptomatic during the flight. Before the flight and within the 14-day incubation period, they visited Toronto, Ontario, Canada (February 15–March 2); New York, New York, USA (March 2–5); and Boston (March 5–9). CHP classified the couple as imported cases into Hong Kong.\n\n【4】Patient C was an asymptomatic 25-year-old man identified through contact tracing by the Hong Kong government and the airline as a close contact of patients A and B. He was a Hong Kong–based business class flight attendant who served patients A and B during the flight. After patients A and B received their diagnoses, the airline informed patient C, and he attended an outpatient clinic on March 16. He was positive for SARS-CoV-2 on March 17 and was subsequently quarantined and hospitalized. Patient C stayed in Boston during March 5–9. Patient D was a 51-year-old female Hong Kong–based flight attendant on the same flight. Fever and cough developed on March 18, SARS-CoV-2 test result was positive on March 21, and patient D was hospitalized. There is no publicly available information of her travel history before the flight or her contacts with the other patients on or after the flight. Descriptions of the disease experienced by patients C and D were unavailable. CHP categorized patients C and D as close contacts of a person with an imported case.\n\n【5】To generate genetic evidence for transmission between the 4 patients, we sequenced their viruses. Samples were collected under public health authority, and individual patient identities are known to CHP. Retrospective analysis of leftover samples without individual consent was permitted under local regulations and approved by the institutional review board of the University of Hong Kong/Hospital Authority West Cluster (reference UW 20-168) and the London School of Hygiene & Tropical Medicine Ethics Committee (reference 22384). Stored upper respiratory samples were sent to a World Health Organization reference laboratory at the University of Hong Kong. We deduced near full-length genomes (sequence length \\> 29,760 nt) by using the Illumina sequencing method and previously described primers and protocol . All deduced sequences had a minimum coverage of 100. While sequencing and analyzing the specimens, we were blinded to patient status as passenger or crew.\n\n【6】The near full-length viral genomes from all 4 patients were 100% identical and phylogenetically grouped to clade G . Other than these 4, none of the 189 viral sequences deduced from samples collected in Hong Kong , belong to this clade (data not shown) . Conversely, in March 2020, virus sequences related to those of patients A–D with only 2 nt differences were isolated in Toronto, New York City, and Massachusetts , making it plausible that patients A and B acquired a similar virus during their visit. Worldwide during January 10–June 13, »30,000 complete SARS-CoV-2 genomes with high coverage were deposited into the GISAID database. None shares 100% identity with the sequences of the viruses in the cluster reported here.\n\n【7】### Conclusions\n\n【8】Given the case histories and sequencing results, the most likely sequence of events is that one or both of passengers A and B contracted SARS-CoV-2 in North America and transmitted the virus to flight attendants C and D during the flight. The only location where all 4 persons were in close proximity for an extended period was inside the airplane. Passengers and cabin crew do not generally go through the same check-in process at airports before boarding. Although we cannot completely rule out the possibility that patients C and D were infected before boarding, the unique virus sequence and 100% identity across the whole virus genome from the 4 patients makes this scenario highly unlikely. Patient D may have acquired infection from patient C, but because their test results were positive within 1 incubation period, it is more likely that patient D was infected by patient A or B. We therefore conclude that these 4 patients belong to the same in-flight transmission chain.\n\n【9】Our results strongly suggest in-flight transmission of SARS-CoV-2. No other COVID-19 cases associated with this flight have been identified. We were unable to quantify the virus attack rate on this flight because not all passengers were tested.\n\n【10】Previous reports of probable in-flight transmissions of SARS-CoV-2 lack genetic evidence . During January–March 2020, the International Air Transport Association received 3 reports of suspected in-flight transmission . Contact tracing of 2 passengers who flew from China to Canada has yielded no indication of secondary infections from the flight . Nonetheless, SARS-CoV-2 test results have been positive for hundreds of flight attendants and pilots; at least 2 have died . Our results demonstrate that SARS-CoV-2 can be transmitted on airplanes. To prevent transmission of the virus during travel, infection control measures must continue.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fda23a86-1c66-44a8-8171-bbc11cdc8de3", "title": "Dolphin Morbillivirus in Eurasian Otters, Italy", "text": "【0】Dolphin Morbillivirus in Eurasian Otters, Italy\nThe genus _Morbillivirus_ comprises several lympho-epithelio-neurotropic, highly pathogenic RNA viruses of domestic and wild vertebrates, including aquatic mammals. Among them, cetacean morbillivirus (CeMV) has been responsible since the 1980s for dramatic epidemics in free-ranging cetaceans worldwide . Specifically, the CeMV strain termed dolphin morbillivirus (DMV) has caused at least 4 unusual mortality events (UMEs) among striped dolphins ( _Stenella coeruleoalba_ ) in the western Mediterranean Sea and, to a lesser extent, long-finned pilot whales ( _Globicephala melas_ ) and other wild cetaceans from the same region .\n\n【1】We report evidence of DMV infection in 4 wild Eurasian otters ( _Lutra lutra_ ) from southern Italy (Apulia and Basilicata regions). The animals, all adult females, belonged to a group of 7 individuals found dead at Parco Nazionale del Cilento, a large national park that extends to the coastline of southwestern Italy. The animals underwent necropsy at Istituto Zooprofilattico Sperimentale della Puglia e della Basilicata (Foggia, Italy) during 2016 and 2017, according to an official agreement between the institute and the park aimed at assessing the health and conservation status of the otter population.\n\n【2】Within a multidisciplinary approach framework, we conducted in-depth histopathologic, microbiologic, parasitologic, and ecotoxicologic analyses on the 7 otters, along with biomolecular (reverse transcription PCR \\[RT-PCR\\]) and immunohistochemical (IHC) investigations for _Morbillivirus_ spp. After using a technique amplifying a highly conserved fragment of the _Morbillivirus_ nucleoprotein (NP) gene , we applied 2 additional methods aimed at detecting DMV-specific hemagglutinin (HA)  and NP gene sequences  for more detailed analysis. To increase the biomolecular results’ reliability, we performed all the extraction, amplification, and sequencing steps in 3 different laboratories. We also conducted the histopathological and IHC analyses in 3 different laboratories. For IHC analysis, we used a commercial monoclonal antibody  against the NP antigen of canine distemper virus (CDV), including adequate morbillivirus-positive and -negative control tissues in each run.\n\n【3】At necropsy, we found multiple traumatic injuries, probably caused by motor vehicles and deemed the putative cause of death, in all 7 otters. Microscopically, we observed a bilateral, subacute-to-chronic, broncho- or bronchiolo-interstitial pneumonia, showing endobronchial, endobronchiolar, or endoalveolar macrophage infiltration, thickening of alveolar septa, and lympho-histiocytic septal infiltration, in 2 otters; 1 also showed a multifocal, portal and lobular, nonsuppurative hepatitis. Lungs and kidneys from these 2 animals were molecularly positive for DMV . Their pulmonary tissue showed positive morbilliviral antigen immunostaining in vascular walls and endothelial cells, as well as in alveolar epithelial cells (consistent, or not, with hyperplastic type II pneumocytes); we also found positive immunostaining in thickened alveolar septa and endoalveolar macrophages . Although the advanced postmortem autolysis did not enable us to obtain reliable histopathological information from the 5 remaining otters, biomolecular analyses yielded positive results from several tissues (submandibular lymph node, parotidal gland, lung, brain, heart, kidney, urinary bladder, and liver) in 2 of them .\n\n【4】Sequencing of all the 200-bp NP gene viral amplicons obtained (GenBank accession nos. MG836265–8) showed remarkable differences from the available CDV sequences but high homology (99%) with 2 DMV sequences (GenBank accession nos. EF469546.1, KU720625.1). We compared an additional 456-bp HA gene fragment , obtained from the kidney of 1 of the otters we tested, with 2 DMV isolates recovered from Mediterranean striped dolphins in 2007; this fragment displayed 100% homology with 1 DMV isolate  and 99.56% homology with the other .\n\n【5】Susceptibility to morbilliviruses is not new to _L. lutra_ otters; CDV infection has been documented, with subsequent risk for viral spillover to coastal pinnipeds . In this respect, the endemic behavior and the progressively expanding host range of DMV in the western Mediterranean Sea are a matter of concern , as exemplified by a peculiar case of infection in a captive harbor seal ( _Phoca vitulina_ ), a species with a mixed marine–terrestrial ecology .\n\n【6】How the otters in this study may have acquired DMV infection is unknown. Of note, 8 cases of DMV infection were detected during 2016 and 2017 in dolphins stranded along the Ionian Sea coast, in an area geographically consistent with that in which the 4 DMV-infected otters were found. It is possible that \\> 1 dolphins with DMV-associated brain lesions could have entered the rivers or lagoons inhabited by the otters and transmitted the virus to them. Alternatively, the otters’ movement toward and placement close to the sea, clearly documented for 1 of them , could underlie a subsequent encounter with DMV-infected dolphins. As an additional or complementary option, susceptible invertebrate hosts might have acted as DMV reservoirs, similarly to what has been reported for _Baicalia carinata_ and _Limnaea auricularia_ mollusks, which probably were a source of infection during the CDV epidemic among Baikal seals ( _Pusa sibirica_ ) in 1987 .\n\n【7】In conclusion, we identified DMV infection among Eurasian otters in southwestern Italy, along the coast of the western Mediterranean Sea. The effect of DMV infection on the health and conservation of the threatened Eurasian otter populations warrants further investigation.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7a43efc2-d830-4512-be76-eb4453d94359", "title": "Screening at a Federally Qualified Health Center in the Midwest for Hepatitis C Among People Who Inject Drugs, 2019–2020", "text": "【0】Screening at a Federally Qualified Health Center in the Midwest for Hepatitis C Among People Who Inject Drugs, 2019–2020\nAbstract\n\n【1】**Introduction**\n\n【2】Hepatitis C virus (HCV) infection is a public health epidemic. People who inject drugs (PWID) are at high risk for transmitting and contracting HCV. The objective of this study was to assess the effectiveness of a multifaceted intervention at a federally qualified health center in the US Midwest to improve HCV screening rates among PWID.\n\n【3】**Methods**\n\n【4】A prospective quality improvement initiative was conducted to increase the proportion of PWID screened for HCV. Inclusion criteria consisted of being seen by a primary care provider from April 16, 2019, through February 28, 2020, being aged 18 years or older, and confirmation of intravenous drug use. PWID status was confirmed by reviewing electronic health records. The multifaceted intervention consisted of educational sessions for the health care team and workflow changes. We analyzed the proportion of patients screened preintervention and postintervention by using χ 2  tests.\n\n【5】**Results**\n\n【6】Of 742 patients who met the inclusion criteria, the proportion of PWID screened preintervention was 59.6% (n = 329) and the proportion of PWID screened postintervention was 65.1% (n = 283), increasing the screening rate by 5.5 percentage points. A χ 2  test of homogeneity indicated a significant relationship between the preintervention and postintervention periods, and screening outcomes ( _P_ < .001).\n\n【7】**Conclusion**\n\n【8】This multifaceted intervention to increase HCV screening resulted in a modest increase in the proportion of PWID screened. Consistent and health care system–wide screening approaches are needed to optimize the potential of HCV treatment and cure options now available.\n\n【9】Introduction\n\n【10】Hepatitis C virus (HCV) is a blood-borne pathogen spread commonly through sharing syringes. People who inject drugs (PWID) are at high risk of contracting HCV; the global seroprevalence of HCV is 52.9% among PWID . Although the World Health Organization suggests that HCV could be eliminated as a public health threat by 2030, work remains to meet this aim, particularly among PWID . One study estimated that only 7.7% of commercially insured PWID in the US were screened for HCV from 2010 to 2017 . According to Healthy People 2030, increasing the knowledge of an HCV infection from 55.6 percent to 74.2 percent will meaningfully improve population-wide HCV outcomes .\n\n【11】Essential to the eradication of HCV is optimizing steps in the HCV care cascade, which includes diagnosis, curative therapy initiation, and confirmation of sustained virologic response (ie, cure). The number of patients sharply declines at each step of the care cascade in the US; the sharpest decline occurs between infection and diagnosis  . In 2015, 71 million people had HCV worldwide. Approximately 14 million were diagnosed with HCV (20%), and 5 million were treated (7%). In 2019, the number of HCV infections worldwide remained at 71 million ; this number is stagnant globally despite the availability of curative therapy  .\n\n【12】**  \nEstimated percentage of people with hepatitis C virus (HCV) at each step of the care cascade, US, 2014. Approximately 3.5 million people in the US have chronic HCV infection. Figure adapted with permission from Yehia et al . \n\n【13】The optimization of the care cascade among PWID requires integration of PWID identification and documentation into standard medical care, yet best practices for such integration are elusive . The primary objective of this study was to determine the proportion of PWID screened for HCV at a federally qualified health center (FQHC) in the Midwest and measure the effectiveness of an intervention to increase screening among PWID. The study hypothesis was that an evidence-based, multifaceted intervention aimed at the health care team responsible for conducting HCV screening would increase the proportion of PWID screened for HCV.\n\n【14】Methods\n\n【15】A prospective quality improvement initiative was conducted at an FQHC primary care clinic to identify and improve HCV screening among PWID. The intervention consisted of educational sessions for the health care team (nurses, physicians, advanced practice providers, and the clinic-based pharmacy staff) and clinic workflow changes. Education covered the prevalence and incidence of HCV infection, screening guidelines, and new clinic-specific tools to support screening. Workflow changes were implemented to alert medical providers of people who needed HCV testing via electronic health records (EHRs) and patient visit forms.\n\n【16】The North Dakota State University Institutional Review Board approved this study. We defined the primary outcome as an increase in the proportion of PWID screened for HCV after implementation of a multifaceted intervention in October 2019. We aimed the intervention at the FQHC health care team rather than patients. Not all patients were seen in both preintervention and postintervention periods; the opportunity to complete screening occurred when patients presented for medical care. The study focused on the success of the intervention and the increase in the percentage of patients screened, whether seen preintervention or not.\n\n【17】Inclusion criteria for participation were 1) being seen by a primary care provider from April 16, 2019, through February 28, 2020, 2) being aged 18 years or older, and 3) confirmation of current or former intravenous drug use (IVDU). Preintervention data collection took place from April 16, 2019, through October 29, 2019; postintervention data collection took place October 30, 2019, through February 28, 2020. We confirmed PWID status by reviewing the EHR for 2 criteria: 1) an _International Classification of Diseases, 10th Revision, Clinical Modification_ (ICD-10-CM) code for illicit drug use or 2) the nursing form completed when a patient attended a medical visit indicated that the patient reported current or former IVDU. We then systematically verified IVDU history for each patient by examining the EHR for the presence of free text notes in either the patients’ problem list or patient encounter documentation indicating a medical provider had confirmed current or former IVDU or the nursing form documented IVDU. The nursing form contained the question, “Does the patient currently or have they ever used IV drugs?” When IVDU was not documented during the medical visit, we reviewed patient medical records from outside facilities available within the patient’s EHR for any indication of IVDU.\n\n【18】### Data collection\n\n【19】In October 2018 the clinic implemented a new EHR-based questionnaire for nurses to increase identification and documentation of IVDU at each clinic visit, given the lack of ICD-10-CM code specific to injection of illicit drugs. This initiative to improve the identification and documentation of PWID was implemented before the study period to ensure PWID were properly identified before assessing screening proportions. The questionnaire prompted the nurse to ask what kind of drug the patient used and the route of administration. Data collected from October 2018 to April 2019 showed little improvement in PWID documentation; many EHRs indicated illicit drug use but did not identify a route of administration. An explicit question was then added in April 2019 to the EHR-based questionnaire; nurses were required to ask patients if they had ever used intravenous drugs. Anecdotally, after this question was added, PWID identification and information route of administration increased.\n\n【20】We initiated preintervention data collection on April 16, 2019, to establish a baseline proportion of PWID screened for HCV. We examined EHRs for completion of HCV screening on at least 1 occasion. We considered screening complete if an HCV antibody or HCV RNA quantitative polymerase chain reaction test had been completed. If we found no results of a test received in the clinic, we searched the EHR for records of screening conducted outside the clinic. We used the same process to collect data during postintervention.\n\n【21】### Intervention\n\n【22】During the last week of October 2019, a multifaceted intervention was conducted to improve screening, expand medical provider recommendation, and increase HCV screening tests ordered. The intervention consisted of an educational component as well as clinic-specific workflow changes to support HCV screening. An infectious disease physician, a pharmacist specializing in viral hepatitis, and a Doctor of Nursing practice student conducted 2 on-site educational presentations for the health care team members. The presentations focused on the prevalence of HCV, risk-based HCV screening guidelines in place at the time, new clinical tools to support HCV screening, components of a hepatitis C toolkit created by the Centers for Disease Control and Prevention (CDC), and distribution of educational materials. Thirty-five staff members attended the 2 presentations. Changes made to the clinical workflow involved items for nurses, physicians, advanced practice providers and the clinic-based pharmacy team. We updated the preventive care visit form completed by nursing staff to identify PWID and document receipt of HCV screening according to the risk-based screening guidelines in effect at that time. The nurses completed the paper-based, preventive care form before all patient visits during postintervention. Pharmacy students prospectively reviewed EHRs and placed EHR-based alerts to medical providers in the EHRs of patients for whom screening was indicated and had not yet been completed. Medical providers were responsible for discussing HCV screening with patients and ordering tests. Because the intervention was intended to increase screening recommended at medical visits, some patients were seen preintervention only, some were seen postintervention only, and some patients were seen in both periods.\n\n【23】### Analysis\n\n【24】We adopted a general null hypothesis of no relationship between the study period (preintervention vs postintervention) and PWID screening outcomes (ie, the intervention is not effective at identifying and documenting PWID). We also adopted a null hypothesis of no relationship between patients’ demographic characteristics and either the study period or PWID screening outcomes.\n\n【25】We used χ 2  tests of homogeneity to evaluate the general null hypothesis. We created cross-tabulations that disaggregated screening outcomes and patient demographic variables (sex \\[male or female\\], age, and Hispanic ethnicity \\[yes or no\\]) across preintervention and postintervention periods. Data were based on office visits scheduled by the patients, meaning that patients could have been seen in both periods or only 1 period. We used Microsoft Excel and the SPSS Statistics Package version 27 (IBM Corporation) for all statistical analyses; _P_ < .05 was considered significant.\n\n【26】Results\n\n【27】We identified 742 patients as PWID during the study period; 562 patients met the inclusion criteria during preintervention, and 435 patients met criteria during postintervention . Of the patients seen during preintervention, 245 were also seen during postintervention .\n\n【28】**  \nIdentification of people who inject drugs and receipt of screening for hepatitis C virus at a primary care clinic in a federally qualified health center in the US Midwest. Preintervention data collection took place from April 16, 2019, through October 29, 2019; postintervention data collection took place October 30, 2019, through February 28, 2020. Abbreviations: EHR, electronic health record; HCV, hepatitis C virus; ICD-10-CM, _International Classification of Diseases, Tenth Revision, Clinical Modification_ . \\[A text version of this article is also available.\\]\n\n【29】Of the 742 patients, 400 (53.9%) were men and 46 (6.2%) were Hispanic; 258 (34.8%) were aged 30 to 39 . We found no significant differences across study periods in patient sex ( _P_ \\= .62) or ethnicity ( _P_ \\= .79). However, we found significant differences across study periods in patient age. More people in their 50s and 60s than in younger age groups were seen both preintervention and postintervention. Younger patients were more likely to be seen preintervention than postintervention or both periods. We found no significant relationship among 742 patients between HCV screening status (ie, screened or not screened) and patient sex, age, or ethnicity .\n\n【30】When we disaggregated the number of patients seen and the number of patients screened by study phase (preintervention period, postintervention period, or both), we found a significant relationship between the preintervention and postintervention periods and screening outcomes. Of the 552 patients seen during preintervention, 329 (59.6%) were screened for HCV; of the 435 patients seen during postintervention, 283 (65.1%) were screened, an increase of 5.5 percentage points . Of 190 patients not seen preintervention but seen postintervention, 103 (54.2%) were screened for HCV. Of 65 patients seen but not screened preintervention, 16 patients were screened postintervention, indicating potential barriers specific to these 65 patients. We found a high level of consistency in outcomes across patients of differing sex, age, and ethnicity . The significance demonstrated by the χ 2  tests appear to be driven primarily by the relationship between patient visits and screening, rather by differences in patient demographics.\n\n【31】Discussion\n\n【32】This multifaceted intervention increased the proportion of PWID screened for HCV by 5.5 percentage points. Although this increase is modest, the proportion of PWID screened before the intervention was substantially higher than the proportion found in other studies. According to the US Preventive Services Task Force (USPSTF), the screening rate for HCV was 8.3% nationwide in community health centers during the risk-based screening era . A retrospective cohort study was conducted from 2012 to 2017 to estimate the screening rate of HCV among PWID at a single FHQC ; 14% of patients diagnosed with opioid use disorder were screened for HCV.\n\n【33】We found that only 7.1% of patients not screened preintervention were screened when they had a medical provider visit postintervention, indicating persistent barriers to screening. We do not know if these patients declined screening or if the provider did not recommend screening, but this finding warrants further study. One may expect that follow-up medical visits allow more time for preventive care and HCV screening would be conducted as appropriate, but we did not observe this trend. According to a meta-analysis published in 2018, a lack of patient knowledge in areas such as HCV risk factors, HCV transmission, and outcomes of untreated HCV infection was associated with being less likely to get tested . Self-perception of low risk for HCV by patients was associated with more cases of undiagnosed infection. Another patient barrier is fear and stigma associated with a positive HCV test result . Comorbid psychiatric disease and social determinants of health, including but not limited to homelessness, inadequate transportation, distrust of medical professionals, and less than a high school education, have been found to be prevalent among PWID with HCV and affect the HCV care cascade . Although we can anecdotally report that many of these factors are present in our FQHC patient population, we did not collect data on these factors.\n\n【34】Medical provider barriers can also affect the HCV care cascade among PWID. Such barriers could be caused by perceptions about PWID among providers: low rates of adherence to the disease evaluation process and medications, ongoing substance abuse, and risk for reinfection via syringe sharing, leading to a delay in progressing through the HCV care cascade ultimately because of concerns about treating HCV among PWID. Only 20% of a sample of HCV specialists in Canada indicated they would consider treating active PWID for HCV . Similarly, among US physicians who treat substance abuse, 61% reported screening PWID for HCV, but only 9% offered treatment; after provision of training and resources, the proportion willing to treat HCV increased to 30% . Hesitation among physicians to treat patients with HCV affects earlier steps in the care cascade, including diagnosis.\n\n【35】Various interventions have been implemented to improve the ability of health care systems to screen for HCV. One health care system implemented an intervention that prompted physicians with a reminder sticker to assess the patient for 12 HCV risk factors; 27.8% of their patient population had at least 1 risk factor, and of these patients, 55.4% were screened. Other interventions for risk-based HCV screening struggle to screen people at risk of the disease, including PWID .\n\n【36】Other barriers specific to the FQHC studied may exist. EHR-based communication between nurses and other medical team members may be inconsistent, especially compared with in-person communication. The nursing forms used in our intervention identified most confirmed PWID in need of HCV screening, but the intervention may have failed to sufficiently motivate medical providers to increase their attention to improving screening particularly within a FQHC operating with limited resources and staff. Additionally, burnout caused by EHR-related alert fatigue is well documented in the literature and is particularly prevalent among family medicine practitioners such as those providing care at an FQHC . In addition, patient populations served by FQHCs have socioeconomic and psychosocial barriers to adherence to medical treatment and management, including completion of laboratory tests. Additional research and resources are needed to address various obstacles to HCV screening among both patients and medical providers, including health care team communication, patient education, financial barriers, and PWID-related stigma.\n\n【37】New HCV screening recommendations may help to overcome some barriers related to risk-based screening . Shortly after this study, in March 2020, the USPSTF and CDC updated their guidelines to recommend one-time HCV screening for all adults . Additionally, these new universal screening recommendations suggest that HCV screening should take place among women during each pregnancy, and regular screening is indicated for people at high risk of HCV, such as PWID. These updates are based on the surge of HCV cases, particularly among young adults; the availability of curative therapies with greater than 90% efficacy developed in the past 6 years; and concern about the reluctance of patients to disclose stigmatizing risk factors such as IVDU . The universal screening guidelines may support more widespread implementation of HCV screening for all adults, simplify clinic processes by minimizing the need to identify risk factors before screening for HCV, reduce stigma associated with HCV screening, and perhaps even increase financial resources for screening, all of which would benefit PWID, particularly those who choose not to disclose IVDU practices.\n\n【38】The universal screening recommendation for regular HCV screening among PWID requires attention to IVDU as a risk factor and, thus, a one-time screening does not sufficiently meet the health care needs of PWID. Additionally, barriers such as patient and provider education and buy-in, adherence to laboratory tests ordered at a visit, and financial costs are not overcome by the universal screening recommendations.\n\n【39】Our study has limitations. Before implementing the intervention, education on PWID identification and documentation was provided to medical staff members. The nurses received this education before the preintervention period began. The proportion of PWID screened for HCV during preintervention may have increased because of enhanced PWID awareness among the health care team. Furthermore, it was not possible to establish a link between participation in the educational facet of the intervention and the extent to which medical providers recommended screenings to their patients. We could not control for individual variability in provider performance because of changes in staff, patient-declared primary care provider, and patients seeing different medical providers based on schedule availability. Another limitation was the use of an observational design that included all patients seen at the clinic who met the study’s inclusion criteria. The inability to randomly draw study participants from a potentially large population prevented us from ensuring that the study’s statistical tests were adequately powered. Perhaps more importantly, we drew data from a single FQHC primary care clinic. The study also could not evaluate detailed data on race and ethnicity data; the EHR reported only Hispanic or non-Hispanic ethnicity. Because we aimed to increase HCV screening among PWID presenting for medical care, some patients were seen only in 1 phase of the study (ie, preintervention or postintervention). As a result of these factors, the external validity of the study’s results cannot be established. Lastly, we counted patients who were screened for HCV before the intervention as screened after the intervention if they had a medical provider visit during postintervention. Medical providers may have identified patients who had been screened previously and decided they did not need to be screened again because of resolution of IVDU as a risk factor (ie, the patient was no longer injecting drugs) or a lack of clarity about regularly screening PWID for HCV . However, we cannot verify whether medical providers recommended and verified HCV screening completion.\n\n【40】Given the availability of curative therapies, HCV should no longer pose individual risk for chronic infection or population risk as an epidemic. However, HCV remains a major contributor to the global prevalence of disease because of low adherence to all stages of the care cascade. Although universal screening overcomes some barriers for people with a history of IVDU, PWID will require tailored interventions because of the need for regular screening in this population versus the one-time screening recommended for most adults. Furthermore, population health improvements will require reducing the volume of HCV carriers in the PWID population so that sharing syringes poses a lower risk of HCV transmission. Such a change requires HCV screening that targets populations with a high prevalence of HCV disease. Enhanced attention to PWIDs and implementing guideline-based screening is essential to provide comprehensive care to this population. Our FQHC-based HCV screening intervention increased screening modestly while elucidating variables and issues intertwined with HCV screening. Several patient- and provider-related barriers will remain despite universal screening and will require expanded dedication of time and resources. Additionally, successful progression through the HCV care cascade to ensure PWID are treated and cured is required to reduce further transmission; a public health strategy termed “treatment as prevention” .\n\n【41】Education and workflow changes may modestly increase the proportion of PWID screened for HCV, yet further work is needed to achieve goals established by WHO to increase screening rates by 90% by 2030 to reach the goal of HCV eradication . Further research must identify barriers to screening completion and address those barriers to reach this benchmark. Given the prevalence and transmission of HCV among PWID, future studies should explore both patient- and provider-related barriers to recommending and completing HCV screening in this population. Beyond screening, adherence to recommended health care follow-up and HCV treatment will require assessment. The effect of the new universal screening guidelines on screening rates remains to be seen, particularly among PWID, to reach the ultimate goal of reducing the prevalence of HCV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2cecf63d-18d1-4091-8775-9b886acb8e0c", "title": "Obesity and Overweight Prevalence Among Adolescents With Disabilities", "text": "【0】Obesity and Overweight Prevalence Among Adolescents With Disabilities\nAbstract\n\n【1】**Introduction**We examined overweight and obesity prevalence among adolescents with disabilities by disability type (physical vs cognitive) and demographic factors (sex, age, race/ethnicity). **Methods**Parents (N = 662) of adolescents aged 12 to 18 years with disabilities from 49 states responded to an online survey from September 2008 through March 2009. **Results**Prevalence of obesity among adolescents with physical and cognitive disabilities (17.5%) was significantly higher compared with that among adolescents without disabilities (13.0%). Obesity prevalence was higher among males, 18-year-olds, and youths with cognitive disabilities than among females, younger adolescents, and youths with physical disabilities. **Conclusion**The higher prevalence of obesity among youths with disabilities compared with nondisabled youths, particularly in certain subgroups, requires further examination in future surveillance research.  \n\n【2】Introduction\n\n【3】Youths with disabilities account for 9.2%, or 6 million, school-aged children in the United States . The epidemic of childhood obesity observed in youths without disabilities  may be an even more serious health issue for youths with disabilities. Compared with youths without disabilities, youths with disabilities are generally less physically active  and their lifestyle health behaviors (eg, social participation, parent supervision, diet) are often markedly different . Several secondary health conditions reported by youths and adults with disabilities, including chronic pain, social isolation, depression, falls or other injuries, and extreme fatigue, are likely to worsen with excess weight . Being or becoming obese in addition to having a physical or cognitive disability could impose greater demands on the person and the caregiver in performing activities of daily living and instrumental activities of daily living, and could increase health care costs associated with the multiple health effects of having a disability and being obese . Previous studies in select groups of youths with disabilities  and youths who report having a chronic condition  have confirmed that obesity rates are higher in this population. However, there is little, if any, research that compares obesity prevalence by age group, race/ethnicity, and disability type (physical vs cognitive). The main objective of this study was to determine the prevalence of obesity and overweight among youths with disabilities by sex, race/ethnicity, age, and disability type. Results are compared with available data for youths without disabilities.  \n\n【4】Methods\n\n【5】From September 2008 through March 2009, parents who had an adolescent aged 12 to 18 years with a disability were recruited for this cross-sectional study from national and local disability and health advocacy organizations, the subscriber list of a magazine for parents of children with disabilities, and a disability-related independent website. Recruitment methods included blast e-mails, web-banner advertisements, and flyers. Eligibility criteria were having a child aged 12 to 18 years with a disability, living in the United States, and being able to speak, read, or understand English. Exclusion criteria were living outside the United States and having family members who did not read or speak English. Recruitment materials directed eligible family members (eg, parents) to visit the survey website and respond to questions regarding the health status of their child. The research protocol was reviewed and approved by the University of Illinois at Chicago institutional review board. One of the main outcomes of interest was obesity and overweight prevalence between adolescents with physical disabilities and those with cognitive disabilities. \n\n【6】### Instrument\n\n【7】Height, weight, and disability data were obtained from an online survey related to the health and lifestyles of youths with disabilities. The survey had 32 items, including height, weight, and disability classification. Only the height and weight data are reported in this study. \n\n【8】### Disability categories\n\n【9】Disability information was collected using categories of disabilities noted in the federal definition of youths who are eligible for special education services . Parents were asked to select up to 3 conditions associated with their child’s disability from this list of conditions: autism spectrum disorder; attention deficit hyperactivity disorder; blindness or vision problems; deaf-blind; Down syndrome; epilepsy or seizures; head injury; impairment or deformity of foot or leg; impairment or deformity of hand, arm, or finger; learning disability; mental retardation (referred to in this article as intellectual disability, in light of recent initiatives to abandon the use of this term); mental or emotional problem or disorder; missing legs, feet, hands, arms, or fingers; paralysis of any kind; spina bifida; spinal cord injury; and “other.” \n\n【10】### Body weight status\n\n【11】Height, weight, and birth date of adolescents, reported by a parent, were used to obtain body mass index (BMI). A percentile ranking of each adolescent’s raw BMI score relative to age- and sex-specific national norms was obtained by using the criteria established by the 2000 Centers for Disease Control and Prevention Growth Charts for the United States . On the basis of the obtained percentile ranking, BMI status was classified into the following 4 categories: obese (BMI ≥95th percentile), overweight (BMI ≥85th percentile and <95th percentile), healthy weight (BMI <85th percentile and ≥5th percentile), and underweight (BMI <5th percentile) . \n\n【12】### Data analysis\n\n【13】The difference in body weight status among the study subjects by demographics and mobility status (ie, use of an aid such as a wheelchair or other assistive device) was examined by using χ 2 tests and SPSS version 16 (SPSS Inc, Chicago, Illinois). From a descriptive framework only, we compared our data with the population-based data obtained from the 2007 Youth Risk Behavior Survey (YRBS) . Level of significance was set at .05.  \n\n【14】Results\n\n【15】Almost all of the family members who responded to the survey were mothers (91.2%), followed by a small number of fathers (6.5%), grandparents (1.1%), and other family or nonfamily members (1.3%). The largest reported subgroup was autism spectrum disorder, followed by cerebral palsy, Down syndrome, and intellectual disability . Males and whites were overrepresented in the study group compared with the YRBS sample. Youths with disabilities in our survey had a higher prevalence of obesity compared with youths in the YRBS (17.5% vs 13.0%) . In our survey, the rate for males was significantly higher than for females (19.8% vs 13.3%, χ 2  = 4.3, _P_ \\= .04). Youths with disabilities also had a higher rate of obesity than youths in the YRBS when analyzed by sex and race/ethnicity. These differences between the 2 groups across age groups were observed at ages 15 (18.9% vs 13.8%) and 18 years (25.3% vs 12.0%). Within the group of youths with disabilities, blacks and Hispanics also had a higher prevalence of obesity than other ethnic groups but the differences were not significant. The prevalence of obesity for 18-year-old youths with disabilities (25.3%) was significantly higher than that for the younger age groups with disabilities (χ 2  = 4.0, _P_ \\= .04). Youths with cognitive disabilities had a significantly higher rate of obesity compared with youths with physical disabilities (21.1% vs 10.1%, χ 2  = 11.7, _P_ < .001). When compared with youths in the YRBS, youths with disabilities had a higher prevalence of overweight among females, whites, and 17-year-olds.  \n\n【16】Discussion\n\n【17】The higher rate of obesity among adolescents with disabilities compared with that among adolescents without disabilities supports the existing literature  and extends this research by identifying substantial differences between age groups, males and females, and youths with physical versus cognitive disabilities. In particular, our findings showed that the 18-year-old group had the highest obesity rates compared with the younger age groups. This is a potential indicator that obesity may be tracking into adulthood. The significantly higher rate of obesity among youths with cognitive disabilities compared with that among youths with physical disabilities requires further research. One reason for this difference may be related to the potential error associated with BMI in youths with physical disabilities who have some form of paralysis (eg, cerebral palsy, spina bifida). Several studies on adults with spinal cord injury have confirmed that BMI is not an accurate indicator of obesity for adults with some form of paralysis . McDonald et al  also reported that BMI cutoff criteria in adolescents with spinal cord injury significantly underestimate obesity in this population, and they recommended that more research be done on youths with physical disabilities to determine the most appropriate cutoffs for obesity and overweight to compensate for the loss in lean body mass associated with paralysis. Our findings are limited by the use of a convenience sample and the overrepresentation of families who were able to complete an online survey; who were from a more educated, higher socioeconomic status; and who were predominantly white. This overrepresentation limits the generalizability of our findings to the broader population of youths with disabilities. Also, the extent to which our data are directly comparable to the population-based YRBS data is not known. Future research must identify ways to target a higher percentage of minority youths with disabilities. As with most self-reported data on height and weight in adolescents and adults, there is a tendency to underestimate overweight prevalence compared with measured data . The higher prevalence of obesity in youths with disabilities underscores the need to enhance data surveillance systems (eg, YRBS) using consistent disability definitions so that interventions can be targeted to youths with the greatest need. Most of the recent attention on obesity has focused on the epidemic of childhood obesity among youths without disabilities ; less attention has been given to youths with disabilities. Decreasing the incidence of obesity among youths with disabilities must become as important a national priority as it is for youths without disabilities. Federal and private funding agencies must ensure that future obesity-related policy and environmental initiatives recognize the needs of families who have a child with a disability and who may need certain adaptations or accommodations to access existing health promotion programs offered in their schools and communities. Engaging families, and whenever possible youths with disabilities, in developing new strategies to reduce obesity must occur on the front end rather than back end (ie, after the program is installed) to provide greater assurance that new initiatives will be accessible and appropriate for youths with disabilities.  \n\n【18】Tables\n------\n\n【19】#####  Table 1. Demographic Characteristics of Adolescents With Disabilities (N = 662), 2008-2009, and Adolescents in the Youth Risk Behavior Survey,\n\n【20】Demographic VariableAdolescents With Disabilities, n (%)YRBS Sample, % aSexMale426 (64.4)49.5Female236 (35.6)50.5Age, y1290 (13.6)NA13118 (17.8)NA14103 (15.6)NA1594 (14.2)29.0 b1684 (12.7)26.2 c1787 (13.1)23.4 d1886 (13.0)21.3 eRace/ethnicityWhite540 (83.3)60.3Black28 (4.3)15.1Hispanic31 (4.8)16.9Other49 (7.6)7.7Major disability groups fAutism spectrum disorder161 (24.4)NACerebral palsy117 (17.8)NADown syndrome87 (13.2)NAIntellectual disability g86 (13.1)NAAbbreviations: YRBS, Youth Risk Behavior Survey; NA, not applicable.a Weighted percentage from the 2007 YRBS .b Reported percentage for students in grade 9.c Reported percentage for students in grade 10.d Reported percentage for students in grade 11.e Reported percentage for students in grade 12.f Four highest reported disability groups. Remaining reported groups were spina bifida (3.3%); mental or emotional problem or disorder (3.0%); head injury (3.0%); learning disability (2.6%); epilepsy or seizures (2.4%); attention deficit hyperactivity disorder (1.8%); blindness or vision problems (1.4%); deafness and hearing problems (0.8%); deaf-blind (0.8%); missing legs, feet, hands, arms, or fingers (0.3%); spinal cord injury (0.3%); paralysis of any kind (0.2%); impairment or deformity of hand, arm, or finger (0.2%); and “other” (11.3%).g Intellectual disability is the new term for mental retardation, which is no longer used. \n\n【21】#####  Table 2. Percentage of Obese a and Overweight b Adolescents With Disabilities (N = 662), 2008-2009, and Adolescents in the Youth Risk Behavior Survey, 2007, by Selected Characteristics\n\nDemographic VariableObeseOverweightAdolescents With DisabilitiesYRBS c (95% CI)Adolescents With DisabilitiesYRBS c (95% CI)Total17.513.0 (11.9- 14.1)16.015.8 (14.8-16.8)SexMale19.8 d16.3 (15.1-17.5)15.516.4 (15.4-17.5)Female13.39.6 (8.3-11.0)16.815.1 (13.8-16.5)Race/ethnicityWhite16.710.8 (9.3-12.4)16.114.3 (12.9-15.7)Black25.018.3 (16.2-20.7)14.319.0 (17.2-20.9)Hispanic23.316.6 (14.7-18.7)20.018.1 (16.1-20.3)Other17.0NA14.9NAAge, y1213.5NA18.0NA1321.9NA18.4NA1414.9NA15.8NA1518.913.8 e (12.5-15.2)15.617.6 e (15.5-20.1)1613.413.2 f (11.5-15.0)13.416.0 f (14.4-17.6)1714.012.7 g (11.3-14.4)17.415.1 g (13.3-17.1)1825.3 d12.0 h (10.5-13.7)12.014.0 h (12.5-15.6)Disability subgroupPhysical disabilities10.1NA12.0NACognitive disabilities21.1 dNA17.8NAAbbreviations: YRBS, Youth Risk Behavior Survey; CI, confidence interval; NA, not applicable.a Body mass index ≥95th percentile.b Body mass index ≥85th percentile and <95th percentile.c Source: 2007 YRBS .d Significantly higher compared with other disability subgroup(s) on the same demographic variable.e Reported percentage for students in grade 9.f Reported percentage for students in grade 10.g Reported percentage for students in grade 11.h Reported percentage for students in grade 12.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a439ddc9-ac0a-4958-b143-1bb308fd1cb0", "title": "Controversies about Extended-Spectrum and AmpC Beta-Lactamases", "text": "【0】Controversies about Extended-Spectrum and AmpC Beta-Lactamases\nExtended-spectrum beta-lactamases (ESBLs) were first reported in 1983 , and plasmid-mediated AmpC beta-lactamases were reported in 1988 . Typically, ESBLs are mutant, plasmid-mediated beta-lactamases derived from older, broad-spectrum beta-lactamases (e.g. TEM-1, TEM-2, SHV-1), which have an extended substrate profile that permits hydrolysis of all cephalosporins, penicillins, and aztreonam. These enzymes are most commonly produced by Klebsiella spp. and Escherichia coli but may also occur in other gram-negative bacteria, including Enterobacter, Salmonella, Proteus, and Citrobacter spp. Morganella morganii, Serratia marcescens, Shigella dysenteriae, Pseudomonas aeruginosa, Burkholderia cepacia, and Capnocytophaga ochracea . Plasmid-mediated AmpC beta-lactamases have arisen through the transfer of chromosomal genes for the inducible AmpC beta-lactamase onto plasmids. This transfer has resulted in plasmid-mediated AmpC beta-lactamases in isolates of E. coli, Klebsiella pneumoniae, Salmonella spp. Citrobacter freundii, Enterobacter aerogenes, and Proteus mirabilis . To date, all plasmid-mediated AmpC beta-lactamases have similar substrate profiles to the parental enzymes from which they appear to be derived. With one exception , plasmid-mediated AmpCs differ from chromosomal AmpCs in being uninducible. Both ESBLs and plasmid-mediated AmpC beta-lactamases are typically associated with broad multidrug resistance (usually a consequence of genes for other antibiotic resistance mechanisms residing on the same plasmids as the ESBL and AmpC genes). A serious challenge facing clinical laboratories is that clinically relevant ESBL-mediated resistance is not always detectable in routine susceptibility tests.\n\n【1】Many clinical laboratories (as well as the wider medical community) are not fully aware of the importance of ESBLs and plasmid-mediated AmpCs and how to detect them; laboratories may also lack the resources to curb the spread of these resistance mechanisms . This lack of understanding or resources is responsible for a continuing failure to respond appropriately to prevent the rapid worldwide dissemination of pathogens possessing these beta-lactamases. The consequence has been avoidable therapeutic failures (sometimes fatal) in patients who received inappropriate antibiotics  and outbreaks of multidrug-resistant, gram-negative pathogens that required expensive control efforts .\n\n【2】I describe gaps in the capabilities of clinical laboratories to accurately detect and report ESBLs and plasmid-mediated AmpC beta-lactamases; discuss some of the technical difficulties involved in designing tests to detect ESBLs in organisms other than E. coli and Klebsiella spp. correlate laboratory problems with the recent emphasis on medical cost-cutting at a time when bacterial pathogens are increasing in complexity; and propose a way to improve laboratory performance to meet the challenge of antibiotic resistance.\n\n【3】### Laboratory Testing for ESBLs and Plasmid-Mediated AmpC beta-Lactamases\n\n【4】The National Committee for Clinical Laboratory Standards (NCCLS) has issued recommendations for ESBL screening and confirmation for isolates of E. coli and Klebsiella spp. and reporting confirmed organisms . Compliance varies widely. Many laboratories have difficulty detecting ESBL- or AmpC-mediated resistance and may be unaware of the relevant NCCLS reporting guidelines . No NCCLS recommendations exist for ESBL detection and reporting for other organisms or for detecting plasmid-mediated AmpC beta-lactamases.\n\n【5】In the United States, many laboratories await NCCLS recommendations before attempting to detect new resistance mechanisms. Thus, many clinical laboratories attempt to detect ESBLs only in E. coli and Klebsiella spp. Some researchers suggest that this is the correct approach and that even discussion of such issues is unwarranted because it causes confusion. However, organisms possessing these resistance mechanisms do cause infections, making this stance unacceptable. Moreover, the laboratory is an early warning system, alerting us to new resistance mechanisms in patients. An early warning system that allows time lags of 12 or more years before new types of resistant organisms are detected is untenable. Twelve years is not an early warning, and laboratories that operate in this manner cannot meet their responsibility.\n\n【6】### NCCLS and the Emergence of New Pathogens\n\n【7】Can the current deficiencies be rectified? Two issues affect laboratories: the role of NCCLS and the speed with which new types of pathogens are emerging. NCCLS's task of creating laboratory test recommendations is difficult and often underappreciated. The committee has responsibilities in the areas of regulation, standardization, and safety. It is not NCCLS's role to be at the cutting edge of research, nor would it be appropriate for it, or other similar bodies, to be overly hasty and make decisions based on inadequate data. It can take years to gather data about a new, relatively uncommon, resistance mechanism. Time is also needed for analysis and debate. Properly done, the process cannot be rushed. The problem is that bacteria are evolving or adapting faster than this process.\n\n【8】Today, many bacterial pathogens are more complex than a decade or two ago. Thus, previously reliable susceptibility tests may no longer be dependable. For example, there are not only new resistance mechanisms, such as ESBLs, but also isolates that produce multiple beta-lactamases. Such organisms were not encountered often, if at all, when the current NCCLS susceptibility test criteria were prepared. For example, before the 1990s, K. pneumoniae isolates typically produced a single beta-lactamase, SHV-1, or occasionally two beta-lactamases . Today, K. pneumoniae isolates that produce three to six beta-lactamases are commonplace in some centers . Such changes necessitate new or modified tests to provide accurate and clinically relevant susceptibility reports. But instead of laboratory testing methods being upgraded during the last decade, the emphasis has been on cost-cutting and downsizing. Laboratories are under pressure to use cheaper, abbreviated tests or merely to maintain the technical status quo of a decade or more ago. In centers where the newer, more complex pathogens occur, reliance on the older tests leaves patients and institutions at risk.\n\n【9】### A More Responsive Approach\n\n【10】One approach to overcoming such problems would be to ensure that each laboratory has a staff member with the time, interest, and expertise to provide leadership in antibiotic testing and resistance. This person would read relevant publications, network with other laboratories, and evaluate potentially useful tests to detect new forms of resistance in the vulnerable interim period before new NCCLS-recommended tests become available. The person with this responsibility should work closely with reference laboratories, such as those of the Centers for Disease Control and Prevention or other sites with expertise. This would help to ensure that, whenever a new resistance mechanism is suspected, it would be properly checked, and the reference laboratory could provide feedback about whether the finding was \"real.\"\n\n【11】### Unresolved Issues\n\n【12】The gaps in current laboratory knowledge and testing have generated several unresolved issues. One is whether positive, but unconfirmed, ESBL screens should be routinely reported. This is a consequence of the NCCLS two-step approach to ESBL detection. The first step is a screening for reduced susceptibility to any of the recommended screening agents (cefotaxime, ceftriaxone, ceftazidime, cefpodoxime, or aztreonam). Confirmatory testing, initiated only after a positive screening result, is based on tests with combinations of screening agents and the beta-lactamase inhibitor clavulanate. This testing indirectly detects hydrolysis of a screening agent by an ESBL by demonstrating potentiation of the activity of a screening agent in the presence of the beta-lactamase inhibitor. Confirmatory testing may require up to one extra day to detect ESBLs. If the laboratory reports a positive ESBL screening result to the physician and the isolate subsequently proves to be ESBL negative, the report could lead to unnecessary use of a carbapenem. Alternatively, if the laboratory withholds the positive screening result and the isolate is subsequently confirmed as ESBL positive, appropriate therapy may have been delayed for a day. Clearly, a reporting rule cannot cover all situations. Rather, the need to report a positive screening result should be determined on a case-by-case basis using common sense and experience as guides, taking into account the patient's status, infection control considerations, and the likelihood of a positive confirmatory test (based on prior experience with isolates from the same patient population). Using a reliable, rapid confirmatory test could minimize the time required for the second-step test and lessen this reporting dilemma. Another solution would be including ESBL confirmation testing in the routine susceptibility test.\n\n【13】Another issue is which NCCLS screening agent should be tested. Generally, the most reliable screening agent is the most sensitive. Cefpodoxime is the most sensitive ESBL screening agent for K. pneumoniae and E. coli, but a poor screening agent for K. oxytoca . The superior sensitivity of this agent can be accompanied by poor specificity in tests with some ESBL-negative E. coli isolates. This is another problem arising from the two-step approach to detecting ESBLs, which could be avoided by including a confirmatory test (ideally cefpodoxime plus clavulanate for K. pneumoniae and E. coli isolates) in the routine susceptibility test .\n\n【14】How best to detect ESBLs in organisms other than Klebsiella spp. or E. coli has not received much attention. The inhibitor-based confirmatory test approach is the most promising detection method . However, with isolates of some species, clavulanate is an unreliable agent for this test. The inhibitor-based approach is most reliable for isolates that do not coproduce an inhibitor-resistant beta-lactamase, such as AmpC. High-level expression of AmpC may prevent recognition of an ESBL. This problem is more common in tests with species or strains that produce a chromosomally encoded inducible AmpC beta-lactamase (e.g. Enterobacter, Serratia, Providencia, Aeromonas spp. M. morganii, C. freundii, Hafnia alvei, and P. aeruginosa). With these organisms, clavulanate may act as an inducer of high-level AmpC production and increase the resistance of the isolate to other screening drugs, producing a false-negative result in the ESBL detection test . Tazobactam and sulbactam are much less likely to induce AmpC beta-lactamases and are therefore preferable inhibitors for ESBL detection tests with these organisms . Another possible solution is to include cefepime as an ESBL screening agent . High-level AmpC expression has minimal effect on the activity of cefepime, making this drug a more reliable detection agent for ESBLs in the presence of an AmpC beta-lactamase.\n\n【15】A further concern with ESBL-producing organisms other than Klebsiella and E. coli is reporting their antibiotic susceptibilities. In Table 2 , the beta-lactam MICs of an SHV-3-producing C. freundii isolate are within the NCCLS susceptible range of <8 µg/mL. If the isolate were Klebsiella or E. coli, the NCCLS reporting rule would apply, and the isolate would be reported as resistant to all penicillins, cephalosporins, and aztreonam. However, there is no ESBL reporting rule for other organisms; therefore, this organism would be reported as susceptible to cefotaxime, ceftazidime, aztreonam, and cefepime. This is inconsistent. Not only does this C. freundii isolate produce an ESBL, it also produces a chromosomal AmpC beta-lactamase that can hydrolyze the cephalosporins and aztreonam. It therefore seems wrong to report this organism as susceptible to these agents. Moreover, when the organism was tested at a 100-fold higher-than-standard inoculum, a dramatic inoculum effect occurred, with large increases in the MICs of these agents, analogous to the inoculum effect that occurs with ESBL-producing Klebsiella spp. and E. coli . This finding adds support for reporting all ESBL-producing isolates, not just Klebsiella spp. and E. coli, as resistant to all penicillins, cephalosporins, and aztreonam.\n\n【16】Detecting and reporting isolates producing plasmid-mediated AmpC beta-lactamases are more difficult issues than those associated with ESBLs. Detection is technically difficult in organisms that also produce a chromosomal AmpC, since proving that an AmpC is plasmid mediated, and not the usual chromosomal enzyme, is necessary. This determination is beyond the capabilities of most clinical laboratories. However, Klebsiella spp. do not possess a chromosomal AmpC. This makes them convenient indicator organisms to screen when attempting to detect plasmid-mediated AmpCs. Phenotypic tests for AmpC detection are not well defined. Screening tests could be based on decreased susceptibility to cephamycins. AmpC beta-lactamases are resistant to all marketed beta-lactamase inhibitors. Therefore, negative ESBL confirmatory tests based on these inhibitors may provide indirect evidence of AmpC production, or reduced outer membrane permeability. A positive three-dimensional test result with cefoxitin demonstrates hydrolysis of cefoxitin and differentiates between AmpC production and reduced outer membrane permeability . If an investigational AmpC beta-lactamase inhibitor were made available for diagnostic testing, it could be used in combination with a suitable cephem to confirm AmpC production.\n\n【17】Susceptibility reporting may prove controversial for isolates producing plasmid-mediated AmpC beta-lactamases. Isolates that produce these enzymes can be susceptible in vitro to cephalosporins and aztreonam . If these agents are used therapeutically for infections with such organisms, determining if they pose a treatment failure risk for patients is a priority.\n\n【18】### Conclusions\n\n【19】Since clinical laboratories are first to encounter bacteria with new forms of antibiotic resistance, they need appropriate tools to recognize these bacteria, including trained staff with sufficient time and equipment to follow up important observations. Because bacterial pathogens are constantly changing, training must be an ongoing process. As we have learned from ESBLs, the methods and training that were previously adequate may no longer suffice against the newer types of pathogens. If laboratories continue to lag years behind new bacterial developments, new pathogens will spread, resulting in increasing problems and costs for patients and institutions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2195b1e0-a9ec-4e6a-93ca-0fc53569cc77", "title": "VEB-1-like Extended-Spectrum ß-Lactamases in Pseudomonas aeruginosa, Kuwait", "text": "【0】VEB-1-like Extended-Spectrum ß-Lactamases in Pseudomonas aeruginosa, Kuwait\n_Pseudomonas_ _aeruginosa_ has an inducible, naturally occurring cephalosporinase that confers low-level resistance to aminopenicillins and narrow-spectrum cephalosporins such as cephalothin and cefoxitin . Resistance to extended-spectrum cephalosporins may arise from overexpression of this cephalosporinase, acquired beta-lactamases, or both . The acquired beta-lactamases may be either clavulanic-acid inhibited (mostly Ambler class A enzymes) or clavulanic-acid resistant (class B and class D enzymes) . The class A extended-spectrum beta-lactamases (ESBLs) may derive from narrow-spectrum beta-lactamases of TEM and SHV types, as extensively reported for _Enterobacteriaceae_ and rarely for _P. aeruginosa_ . Other class A enzymes reported in _P. aeruginosa_ include PER-1, which we first identified as chromosomally located and which is widespread in _P. aeruginosa_ isolates in Turkey (11% of the hospital isolates) . Lately, another class A ESBL integron-located gene, _bla_ VEB-1  , has been identified from _P. aeruginosa_ and enterobacterial isolates from Southeast Asia .\n\n【1】We report on two novel VEB-1-like beta-lactamases from _P. aeruginosa_ clinical isolates from Kuwait. This is the first report of extended-spectrum enzymes from nosocomial isolates from this part of the world.\n\n【2】### The Study\n\n【3】_P. aeruginosa_ KU-1 was isolated in January 1999 at Ibn Sina Hospital in Kuwait from endotracheal secretions of a 1-day-old infant with respiratory tract infection, hospitalized in the intensive care unit (ICU) unit because of severe enterocolitis. He was first treated with cefotaxime, amikacin, and metronidazole. These antibiotics were discontinued, and he was given imipenem. He improved and was discharged 7 days later. As it was a single isolate of a multidrug-resistant strain, isolation precautions were carried out only in the neonatal ICU. The infant's mother could not remember whether she had received antibiotic therapy during her pregnancy (which was uneventful). She had not traveled outside Kuwait.\n\n【4】_P. aeruginosa_ KU-2 was also isolated in June 1999 from the urine of a 73-year-old man admitted to the ICU of another Kuwaiti hospital, Mubarak Al-Kabeer, with ischemic chest pain and bronchectiasis. On day 2 of his hospitalization, fever developed, but blood and urine cultures were negative. He was treated with ceftazidime for 12 days beginning on day 2. During his hospital stay, hematuria developed, followed by urine retention. On day 28, pus from a urinary catheter infection after transurethral prostatitic surgery grew _P. aeruginosa_ KU-2 that was susceptible to norfloxacin _. P. aeruginosa_ KU-2 was the only _P. aeruginosa_ strain isolated from this patient's clinical specimens. He was treated with norfloxacin. Repeated urine cultures did not yield any organism, and he was discharged 15 days later. He had no history of travel outside Kuwait.\n\n【5】Strains from both patients were identified by using an API-20 NE system (Biomerieux, Marcy-l'Etoile, France). Preliminary antibiotic susceptibility testing by disc diffusion  revealed a slight synergy between ceftazidime- and clavulanic acid-containing discs for two clinical isolates, _P. aeruginosa_ KU-1 and KU-2. Susceptibility testing of beta-lactams for _P. aeruginosa_ KU-1 and KU-2 was then performed by a Mueller-Hinton agar dilution method . Both strains showed decreased susceptibility to all beta-lactams except imipenem and piperacillin/tazobactam . MICs of ceftazidime were decreased by addition of clavulanic acid and tazobactam, indicating the presence of a clavulanic-acid inhibited ESBL . According to antibiotic susceptibility testing by Mueller-Hinton agar disc diffusion, _P. aeruginosa_ KU-1 and KU-2 were also resistant to aminoglycosides (amikacin, gentamicin, netilmicin, tobramycin), chloramphenicol, tetracyclines, sulfonamides, and fluoroquinolones (except for KU-2).\n\n【6】Beta-lactamase extracts from cultures of P. aeruginosa KU-1 and KU-2 were obtained . Isoelectric focusing analysis  revealed beta-lactamases with isoelectric points of 7.4 and 8-8.4, the latter likely corresponding to P. aeruginosa AmpC cephalosporinases. Whole-cell DNAs of P. aeruginosa KU-1 and KU-2 were then obtained . Preliminary polymerase chain reaction (PCR) experiments were performed with DNAs of P. aeruginosa KU-1 and KU-2 as templates and primers specific for the following class A beta-lactamases: TEM, SHV, CARB (PSE-1), GES-1, PER-1, and VEB-1 . Only PCR using internal primers for _bla_ VEB-1  gave a positive result with an identical 642-bp fragment. External _bla_ VEB-1  specific primers gave 1,070-bp PCR fragments with DNAs of both _P. aeruginosa_ strains as templates that were sequenced on both strands . The deduced amino acid sequences, obtained over the internet , identified VEB-1-like sequences that shared 99% amino acid identity with VEB-1 . Compared with VEB-1, the amino acid changes in VEB-1a and VEB-1b from _P. aeruginosa_ KU-1 and KU-2, respectively, occurred in the putative leader peptide sequence . Thus, the hydrolytic activity of VEB-1a and VEB-1b should be identical to that reported for VEB-1 beta-lactamase .\n\n【7】The genetic background of _bla_ VEB-1a  and _bla_ VEB-1b  was further characterized. Plasmid extraction, conjugation, and electroporation experiments were performed . A plasmid (pROT-1) of ca. 70 kb carrying bla VEB-1a  gene was identified according to hybridization results by using an internal PCR-obtained probe for _bla_ VEB-1  . Plasmid pROT-1 was self-conjugative from _P. aeruginosa_ KU-1 to in vitro obtained rifampin-resistant _P. aeruginosa_ PU21 reference strain after selection of transconjugants onto Mueller-Hinton agar plates each containing 150 µg/mL rifampin and 200 µg/mL ticarcillin. As assessed by antibiotic susceptibility testing by disc diffusion, plasmid pROT-1 conferred additional resistance to gentamicin, netilmicin, sulfonamides, and tobramycin. MICs of beta-lactams for _P. aeruginosa_ PU21 (pROT-1) mirrored those obtained for _P. aeruginosa_ KU-1 . While the plasmid location of _bla_ VEB-1  gene is known only in Enterobacteriaceae , its report in _P. aeruginosa_ may signal the evolution of its spread. The _bla_ VEB-1b  gene was not plasmid located, but a PCR-obtained 642-bp internal probe for _bla_ VEB-1  hybridized at chromosomal position of whole-cell DNA of _P. aeruginosa_ KU-2.\n\n【8】The _bla_ VEB-1  gene is located on different structures of class 1 integrons . Integrons comprise two conserved regions (5'-CS and 3'-CS) flanking an internal variable region usually containing several gene cassettes . Integrons are in fact expression vectors for antibiotic resistance genes that are included as gene cassettes and are neighbored . By using primers located either in the 5'-CS sequence and the 5' end of _bla_ VEB-1  or in the 3' end of _bla_ VEB-1  and the 3'-CS sequence , PCR amplification experiments were performed with whole-cell DNAs of _P. aeruginosa_ KU-1 and KU-2 as templates. In one case (strain KU-1), a PCR fragment was obtained by using _bla_ VEB-1  and 5'-CS primers, indicating that _bla_ VEB-1a  was located downstream of a class 1 integrase gene. In this case, the 4-kb PCR fragment differed from those of known _bla_ VEB-1  containing integrons identified in _Escherichia coli_ and _P. aeruginosa_ isolates . Amplimers of 1 kb were obtained for both strains using _bla_ VEB-1  and 3'-CS primers, showing that the 3'-CS end was present in both cases and that the _bla_ VEB-1  \\-like sequences were located next to the 3'-CS end within class 1 integrons. The _attC (59-be)_ recombination sites  located downstream of gene cassettes were identical for _bla_ VEB-1a  and _bla_ VEB-1b  to those described for _bla_ VEB-1 in _P. aeruginosa_ and enterobacterial isolates identified so far from Southeast Asia . Therefore, an identical bla VEB-1  \\-like gene cassette may be located on different class 1 integrons. Using 5'-CS and 3'-CS primers, two additional PCR fragments were obtained for each _P. aeruginosa_ strain, showing that both strains contained another _bla_ VEB  \\-negative class 1 integron. For _P. aeruginosa_ KU-1, a 950-bp PCR fragment for an _aadA1a_ gene coding for an aminoglycoside modifying enzyme was found to be plasmid- and integron-located in _Salmonella enterica_ serotype Typhimurium . For _P. aeruginosa_ KU-2, a 500-bp PCR fragment encoding a putative 95 amino acid protein of unknown function was PCR amplified. It shared 71% amino acid identity with an amino acid sequence from a gene that was Tn _1696_ transposon-located and In4 integron-located in _P. aeruginosa_ .\n\n【9】Finally, _P. aeruginosa_ KU-1 and KU-2 isolates containing VEB-1-like beta-lactamases were compared with VEB-1 positive _P. aerugionosa_ strain JES from Thailand by using random amplified polymorphic DNA technique . The isolates were not clonally related (data not shown). Although the patients had not traveled outside Kuwait, introduction of _P. aeruginosa_ into Kuwaiti hospitals by travelers or patients from Southeast Asia cannot be ruled out.\n\n【10】### Conclusions\n\n【11】The presence of clavulanic-acid inhibited ESBLs in _P. aeruginosa_ isolates may account for part of the 50% resistance to ceftazidime of _P. aeruginosa_ strains isolates from ICUs in Kuwait . ESBLs in _P. aeruginosa_ in Kuwait and other Middle Eastern hospitals may be underestimated because routine detection with a double disc synergy test may be difficult. Identification of ESBLs is of interest since they confer resistance to all extended-spectrum cephalosporins and aztreonam, whatever their MICs. This has been confirmed by experimental data using a model of pneumonia in rats with the Ambler class A ESBL, PER-1 .\n\n【12】This work underscores that very similar ESBLs may be identified in different parts of the world. It is the first report of ESBL genes characterized from _P. aeruginosa_ isolates from the Middle East.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1c81aebc-c75a-4780-a9aa-5eaa8881c280", "title": "Attributing Illness to Food", "text": "【0】Attributing Illness to Food\nFoodborne microbiologic hazards may be responsible for as many as 76 million cases of illness in the United States each year  and are thus an important food safety challenge. To lower the incidence of foodborne disease, many experts and stakeholders urge the development of a science- and risk-based food safety system, in which decision makers prioritize hazards and interventions using the best available data on the distribution and reduction of risks . Such a system requires an understanding of the many risk factors between the point of production and the point of consumption and the ability to systematically target intervention efforts along this \"farm-to-fork\" continuum.\n\n【1】Although the Foodborne Diseases Active Surveillance Network (FoodNet), administered by the Centers for Disease Control and Prevention (CDC), is producing increasingly robust data on the incidence of illness due to specific enteric pathogens, no method exists to categorize these illnesses by mode of transmission, whether drinking water, environmental exposure, or consumption of a specific food. Interventions are almost always food (or process) specific. To design and prioritize effective food safety interventions, we must be able to perform food attribution—that is, identify which foods are vehicles for specific cases of illness. Such data are of particular importance for US government agencies that regulate food and food animals, including the Food Safety Inspection Service (FSIS) of the US Department of Agriculture (USDA), and the Center for Food Safety and Applied Nutrition (CFSAN) and the Center for Veterinary Medicine (CVM) of the Food and Drug Administration (FDA).\n\n【2】Foodborne illnesses can be attributed to foods by using a variety of data sources and analytic approaches; each has its virtues and limitations. In Atlanta on October 31, 2003, the Food Safety Research Consortium (FSRC) sponsored the Food Attribution Data Workshop to explore these approaches in detail. Attendees included representatives from CDC, FSIS, CFSAN, CVM, the Environmental Protection Agency (EPA), consumer advocacy organizations, and member institutions of FSRC, including the University of Maryland at Baltimore, University of Georgia, Iowa State University, University of California at Davis, and Resources for the Future. This article summarizes material discussed at the workshop and identifies challenges that affect progress in this critical component of a risk-based approach to improving food safety.\n\n【3】### Food Categorization\n\n【4】For the purposes of attributing illnesses to foods, food vehicles must be grouped into suitable categories. Although the idea may seem simple, the need for a single food categorization scheme has emerged as a critical issue. At a general level, a list of major food commodities might include 11 categories: poultry, eggs, pork, beef, dairy, fish, mollusks, crustaceans, wild game, row crops (e.g. lettuce and corn), and tree crops (e.g. apples and oranges). Each of these commodities could be divided further, leading to such subcategories as broiler chickens and raw oysters. Additionally, classification could include level of processing (raw, fresh-cut, canned, frozen), origin (domestic, imported), and location of preparation (home, food processor, food service). When illnesses are linked to foods with multiple ingredients, such as soups or casseroles, the choices include whether to omit these cases from analysis, to categorize multiple-ingredient foods by \"essential\" ingredient, or to attribute illnesses by the proportion of individual ingredients.\n\n【5】A common food categorization scheme is essential if different sources of data are to be combined or compared. Because of lack of agreement in categorization, data from CDC, state health departments, and FDA and USDA and their constituent agencies are often not directly comparable. As a first step in any approach to food attribution, food categories need to be standardized across government agencies, with a scheme acceptable to industry, academia, and consumer groups.\n\n【6】### Current Approaches to Food Attribution\n\n【7】Approaches to food attribution can be grouped into 2 broad categories, loosely designated as epidemiologic and microbiologic. Epidemiologic approaches are based on public health surveillance and include foodborne outbreak data and case-control studies. Microbiologic approaches rely on data on pathogen samples drawn from human, animal, and food sources and include pathogen subtyping, as used in Denmark's Salmonella Accounts  and microbial source tracking (MST) methods, as well as risk assessments of specific pathogens in specific foods.\n\n【8】### Danish Experiences\n\n【9】Denmark has an integrated system in which data from public health surveillance and pathogen monitoring of foods and animals are routinely collected, collated, and analyzed by a single coordinating body. Cultures collected from infected persons, animals, and retail food sources are subtyped, allowing for the direct comparison of surveillance and monitoring data and the identification of public health outcomes by food source.\n\n【10】The regular monitoring of food sources is performed on farms, at slaughter, and in retail foods, although the emphasis is on primary production facilities . Every flock of egg-laying chickens is regularly tested for salmonellae, as are all flocks of broiler chickens, turkeys, and ducks. Finishing pigs are continually tested, dairy herds are routinely monitored, and poultry, pork, and beef are examined during slaughter processes. Imported meat and poultry products are monitored, as are wild animals, birds, and pets, and retail surveys are performed on raw meat, pork, poultry, shell eggs, fruits, and vegetables.\n\n【11】_Salmonella_ isolates obtained from animal and food sources are subtyped (with serotyping, phage typing, and pulsed-field gel electrophoresis \\[PFGE\\]) and compared in a quantitative manner with isolates obtained from human infections . A prerequisite of the model is predominance of at least 1 \"distinctive\" _Salmonella_ subtype in each main animal reservoir; human infections of distinctive subtypes are assumed to have originated from that reservoir. Human infections caused by _Salmonella_ subtypes found in multiple animal reservoirs are attributed proportionally to the occurrence of the distinctive subtypes. Model results have been corroborated by case-control studies, outbreak reports, time-series analysis, and risk assessments . In the past 10 years, the Danish model has proven invaluable for identifying pathogen reservoirs in animal populations, tracking trends of human salmonellosis, and guiding interventions .\n\n【12】One weakness of the Danish method is that causation cannot be discerned for cases without distinctive _Salmonella_ subtypes; thus, the proportional attribution of such cases across animal reservoirs may not necessarily be accurate. Also, vegetables, fruits, fish, pets, water, and other sources of infection are not directly included in the analysis, under the assumption that the original sources of bacterial infection are animal reservoirs. Furthermore, the model is currently focused on salmonellae and may not be applicable to other pathogens that do not meet certain prerequisites. For example, although extensive subtyping has also been performed on _Campylobacter_ isolates, the homogeneous distribution of subtypes across reservoirs makes attribution difficult. Since the Danish model is focused on the major food-animal reservoirs, it cannot identify responsible foods at the point of consumption or at other points along the farm-to-fork continuum.\n\n【13】### British Experiences\n\n【14】The United Kingdom uses an integrated systems approach to food safety that includes both epidemiologic and microbiologic methods, with responsibility for foodborne illness consolidated into a single government office. Annual reports on zoonoses, which combine surveillance data with data on food and animal monitoring , are produced. In addition, etiologic analyses of foodborne outbreaks, detailing illnesses by pathogen, food source, and additional risk factors, are performed. UK agencies also perform regular pathogen monitoring and subtyping of animals and retail food .\n\n【15】The United Kingdom has developed a method for estimating the relative risks associated with specific foods, dividing the number of cases due to a specific food (as derived from their outbreak database) by the estimated total servings of that food consumed in a year. The weaknesses of this system include the assumption that all hospitalizations and deaths are routed through general practitioners and reliance on outbreak data, which may not be representative of sporadic disease. However, the UK outbreak dataset is large, and the food vehicles implicated correlate with findings of local epidemiologic studies. Increasingly, data indicate that interventions guided by the system have been successful in reducing cases and risk for foodborne illness.\n\n【16】### US Outbreak Data\n\n【17】Reports of outbreak investigations provide the most comprehensive US data for determining the foods responsible for illnesses. The Foodborne Disease Outbreak Surveillance System contains data on >20,000 US foodborne disease outbreaks reported to CDC since 1973; these reports link specific foods to cases of human illness . CDC, the Center for Science in the Public Interest (CSPI), and the FSRC have estimated food attribution using these data .\n\n【18】Responsibility for investigating foodborne disease outbreaks resides with local and state health departments, which then report these data to CDC. Reported outbreaks represent only a small proportion of those that occur, and the degree of underreporting may vary geographically and temporally. For example, revision of the reporting process and provision of increased resources to CDC and state health departments from the National Food Safety Initiative were associated with a doubling of the number of outbreaks reported annually from 1996 to 1998.\n\n【19】Outbreak data have additional important limitations. Outbreaks that are large, associated with restaurants, have short incubation periods, or cause serious illness are more likely to be investigated and reported. Likewise, illnesses due to pathogens that are difficult to identify or rarely cause outbreaks are underrepresented. For example, the foods most frequently identified as the source of _Campylobacter_ outbreaks differ markedly from those identified as sources in community studies of sporadic cases .\n\n【20】An approach used by CDC to estimate illness due to a particular food-pathogen combination is to count the number of outbreak-related illnesses due to a particular pathogen and to determine the proportion of these due to each food grouping. These proportions are then applied to estimates of incidence of that pathogen, as reported by CDC . This approach is also employed in the Foodborne Illness Risk Ranking Model (FIRRM), an analytic tool developed by FSRC to compare the public health impact of various pathogen-food combinations . As recent CDC outbreak data were not available when FIRRM was developed, FSRC relied on outbreak data compiled by CSPI. The CSPI database consists primarily (88%) of foodborne outbreak data compiled by CDC and now available on the Internet, but it also includes outbreaks not included in CDC data. CDC may not have received reports on these illnesses from state health departments or may have excluded reports from the database because they did not meet CDC criteria for foodborne outbreaks due to specific pathogens.\n\n【21】Improving food attribution from outbreaks will require improving both the quality and quantity of data. In particular, increased efforts are needed to obtain stool specimens from ill persons early in outbreaks to increase the fraction of outbreaks for which a pathogen is identified to >40% and to trace back foods implicated in outbreaks to their sources. CDC has launched an effort to improve the categorization of food items and ingredients, so outbreaks can be grouped in useful ways for regulatory agencies, industry, and consumers. CDC is also creating new analysis capabilities for the foodborne outbreak surveillance system that will provide data summaries for a variety of purposes, including food attribution.\n\n【22】### FoodNet Sporadic Case-Control Studies\n\n【23】FoodNet is an active surveillance program centered at CDC that tracks foodborne illnesses from 9 pathogens in 10 well-defined target populations . In FoodNet case-control studies, patients reporting through FoodNet are contacted for followup interviews and to complete questionnaires to estimate the proportion of illnesses associated with specific foods, food preparation, handling practices, and such behavior as pet ownership, farm visits, or international travel. FoodNet has performed case-control studies on a variety of pathogens, including _Salmonella_ spp. _Escherichia coli_ O157:H7, _Campylobacter_ , _Cryptosporidium_ , and _Listeria monocytogenes_ (among others 17 , 21 – 24 ).\n\n【24】FoodNet case-control studies are of particular value for assessing food attribution of sporadic illness because they are population based. Because the diseases under investigation are rare in all population subgroups, rate ratios in the data closely approximate risk ratios in the population. Along with case exposure percentages, these risk ratio estimates may be used to calculate the \"population attributable fraction,\" the proportion of new cases occurring during a given period in a particular at-risk population that was attributable to the effect of ≥1 exposures.\n\n【25】FoodNet case-control studies have limitations, primarily due to recall bias, long exposure windows, and immunity . First, patients and controls are limited in what they remember and can report in an interview, and the interview format itself has limitations. Second, the periods during which exposures are ascertained for FoodNet case-control studies tend to be long (5–7 days), so the likelihood of detecting a difference in exposure between cases and controls is limited by high exposure frequencies among both cases and controls. Further studies are needed to assess the consequence of using shorter exposure windows. Lastly, if a relatively common infection conveys durable immunity, an important segment of the population may be immune and therefore not susceptible to infection, making the demonstration of an association between exposures and risk for infection more difficult.\n\n【26】### Microbial Subtyping and Microbial Source Tracking\n\n【27】CDC's National Molecular Subtyping Network for Foodborne Disease Surveillance (PulseNet) links public health laboratories that use PFGE to routinely fingerprint suspected foodborne bacteria isolates . Results of PFGE subtyping of 5 bacteria ( _E. coli_ O157:H7, nontyphoidal _Salmonella_ spp. _Shigella_ spp. _L. monocytogenes_ , and _Campylobacter_ spp.) are stored in the electronic PulseNet database; bacterial strains in the database can be compared quickly and provide an early warning system for emerging outbreaks when related strains appear. PulseNet cannot currently be used for food attribution because it does not include isolates from sporadic cases of human illness or from food or animal sources.\n\n【28】MST refers to a specific application of microbial subtyping in which markers from an isolate are used to trace that isolate to an animal source, similar to what is done for salmonellae in Denmark. If different animal species carry unique, host-specific populations of microorganisms, a subtyped isolate drawn from an infected person could indicate that the isolate originated in 1 species as opposed to another. A large number of MST methods are being researched, most of which were originally developed to trace fecal bacteria in natural waters. Most approaches use genetic or phenotyping fingerprinting methods, although chemical markers, biomarkers, viruses, and bacteriophages are also used as indicators of animal source . Although MST techniques have potential, no single approach seems ideal for all pathogens and situations. US agencies have begun to research MST specifically for food attribution purposes; CVM, in particular, has investigated methods for _Salmonella_ and _Campylobacter_ spp . Although results to date are promising, they are only initial steps toward using MST methods to attribute illnesses to food animals. Similarly, data collected on the antimicrobial resistance of bacteria by researchers at CVM, CDC, the Agricultural Research Service (an agency of the USDA), and elsewhere may ultimately prove useful for food attribution purposes.\n\n【29】### Risk Assessments\n\n【30】Risk assessments include food contamination data, food storage and consumption patterns, risk behavior, and dose-response functions to predict risks for illness from specific pathogens found in specific foods. If exposure estimates and dose-response functions are sufficiently accurate (a key consideration), risk assessments may produce excellent estimates of the true impact of illness.\n\n【31】Because risk assessments are so resource-intensive, they have been undertaken for only a limited number of pathogen-food combinations. The most comprehensive risk assessments for a single pathogen are those performed for _L. monocytogenes_ by CFSAN and FSIS . This set of 23 individual risk assessments focused on ready-to-eat foods, including deli meats, dairy, produce, and seafood. Risk assessments have also been conducted on _E. coli_ O157:H7 in ground beef , on _Salmonella enterica_ serovar Enteritidis in shell eggs and egg products , and on _Vibrio parahaemolyticus_ in molluscan shellfish .\n\n【32】For risk assessments to be used for food attribution, however, they need to be performed on most food items associated with a particular pathogen. Considering the 3-year duration of the _L. monocytogenes_ risk assessments, performing comprehensive risk assessments on a sufficiently large number of pathogen-food combinations for systematic food attribution would be a colossal task.\n\n【33】The other major limitation of using risk assessments for food attribution is that they are inherently predictive. Unlike surveillance data, they do not measure observable public health effects, but rather estimate the impact on the basis of assumptions that are difficult to validate in a dynamic system, in particular dose-response functions, food storage and consumption patterns, and consumer behavior. Furthermore, risk assessments are ill suited for temporal analyses, since they are not routinely updated as new data become available. Risk assessments are most useful for food attribution purposes when compared with other estimates, such as those based on outbreak data or case-control studies.\n\n【34】### US Food Monitoring\n\n【35】Various US food safety agencies test for pathogen prevalence in foods through routine monitoring and case studies. FSIS monitoring, focused on the slaughter process, includes regular testing of raw meat and poultry for salmonellae , ground beef for _E. coli_ O157:H7 , and ready-to-eat deli products for multiple pathogens . FSIS and the Agricultural Research Service have examined pathogen prevalence in commercial food products, such as _L. monocytogenes_ in frankfurters . The United States does not have a comprehensive program for monitoring live food animals. Food and animal monitoring data, when not associated with surveillance data, are not applicable for food attribution purposes.\n\n【36】### Expert Elicitation\n\n【37】When scientific or epidemiologic data are lacking, sparse, or highly uncertain, expert judgment may be used to fill gaps or combine conflicting estimates into a meaningful solution. Expert judgments derived through formal methods are increasingly used and recommended for assessing risk and the economic impact of regulations .\n\n【38】FSRC researchers administered an expert elicitation of experienced food safety researchers, public health scientists, and food safety authorities for use in the Foodborne Illness Risk Ranking Model . Produced with a standardized, vetted method, the survey asked respondents to estimate the percentage of 11 pathogens caused by each listed food category and included measures of respondent uncertainty and possible biases. Although data need to be analyzed further, initial results are promising and corroborate food attribution percentages derived from other means.\n\n【39】Expert elicitations are limited because they are based on perception, not on observable data. Results may be circular if experts rely on the same sources, or deceptive if experts are similarly misinformed or biased. Expert judgments are thus not an ideal source of food attribution data but may have utility if data are sparse or inconsistent and uncertainty is substantial.\n\n【40】### Conclusions\n\n【41】A recent National Academies of Science report, Scientific Criteria to Ensure Safe Food, argues for \"the development of a comprehensive national plan to harmonize the foodborne disease surveillance that is conducted by public health agencies with the monitoring of pathogens across the food production, processing, and distribution continuum that is conducted by food safety regulatory agencies\" . The motivation driving this suggestion is the same as that which motivated FSRC to convene the Food Attribution Data Workshop; to make informed science- and risk-based decisions about food safety interventions, we need to be able to associate foodborne illnesses to specific food vehicles. The goal of the workshop was to review the approaches currently used for food attribution, in the United States and abroad, and to identify future options for the collection of food attribution data in the United States.\n\n【42】Although all workshop attendees or institutions did not reach consensus about the ideal data for food attribution, there was nearly universal agreement that none of the current data sources are sufficient on their own because of methodologic limitations or gaps in available data . Furthermore, in the United States, data are spread over a wide range of agencies and researchers, resulting in myriad studies covering different aspects of the food attribution problem. These issues make it difficult to accurately and dependably attribute illnesses to the foods responsible as pathogen vehicles—and, in turn, to target appropriate intervention strategies.\n\n【43】Several characteristics should be considered in evaluating and comparing current and future food attribution methods; their relative importance depends on the purpose for which the attribution data are sought. These include scientific accuracy and uncertainty, quality and breadth of data, computational consistency, practical feasibility, cost of implementation, flexibility and scalability, utility for targeting interventions, and congruency with other relevant data sources. Among the critical unresolved issues is how to balance such factors as scientific accuracy and practical feasibility to produce attribution data that will be both useful and affordable.\n\n【44】With so many institutions responsible for various aspects of the food safety system, collaboration is paramount, as is the explicit delineation of responsibilities and powers. Access to these data is a critical issue. Building a system in which data and conclusions are shared in a timely manner among agencies and with industry and academia, and privacy issues with persons and industry participants have to be addressed. Creation of an open searchable database of outbreaks would greatly expand the opportunities for research and collaboration.\n\n【45】As described here, a variety of approaches have been used to better define the source of foods responsible for human infections. However, none of these approaches is likely to be sufficient on its own. The implicit conclusion, therefore, is that the scientific and accurate attribution of foodborne illnesses to specific foods means developing a comprehensive program that combines many of the discussed methods and data. Such a system can be achieved with increased resources and cooperation among food safety institutions.\n\n【46】The Food Attribution Data Workshop was sponsored by the Food Safety Research Consortium, a multidisciplinary collaboration to improve public health; members include the University of Maryland, Baltimore; University of Georgia; Iowa State University; University of Massachusetts; University of California, Davis; Michigan State University; and Resources for the Future. The workshop grew out of an FSRC project funded by the Robert Wood Johnson Foundation and received generous support from the Joint Institute for Food Safety and Applied Nutrition, the Center for Food Safety and Applied Nutrition and Center for Veterinary Medicine within the FDA, and the Office of Risk Assessment and Cost-Benefit Analysis and the Economic Research Service within USDA.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "90e50208-9857-45f9-a459-129f9b8435d7", "title": "Community-Acquired Invasive GAS Disease among Native Americans, Arizona, USA, Winter 2013", "text": "【0】Community-Acquired Invasive GAS Disease among Native Americans, Arizona, USA, Winter 2013\n**To the Editor:** Group A streptococci (GAS) can cause severe invasive diseases, such as necrotizing fasciitis, streptococcal toxic shock syndrome, and sepsis. In 2012, ≈11,000 cases of invasive GAS (iGAS) disease and 1,100 associated deaths occurred in the United States . The risk for iGAS infection is 10 times higher among Native Americans than among the general population . Other predisposing factors for iGAS infection include skin wounds and underlying diseases, such as diabetes . Household risk factors include exposure to children with pharyngitis and crowding . Most iGAS infections occur sporadically within the community. Postpartum and postsurgical clusters arising from a common nosocomial source occur but are rare .\n\n【1】During the winter of 2012–13, a 3-fold increase in necrotizing fasciitis was observed at an Arizona hospital (hospital X) that predominantly treats Native Americans. Tribal leadership initiated a collaborative investigation with state and federal officials to characterize the outbreak and implement appropriate control measures.\n\n【2】A confirmed case of iGAS was defined as isolation of GAS from normally sterile sites (i.e. blood) or isolation of GAS from nonsterile sites (i.e. wound) in the presence of necrotizing fasciitis or streptococcal toxic shock syndrome among patients who sought care at hospital X during August 2012–March 2013. Hospital X serves ≈45,000 persons in a rural community. Eleven confirmed iGAS cases were identified , of which 8 (73%) occurred in women and 3 (27%) occurred in men. The case-patients had a mean age of 63 years (range 32–92 years). All cases were community-onset illnesses; none of the case-patients had recent exposures to health care settings, and all were of Native American ancestry. Of the 11 case-patients, 8 required critical care treatment and 3 died. Nine (82%) case-patients had open wounds or skin breakdown (e.g. skin abrasion, burns), and 9 had underlying medical conditions that are known risk factors for iGAS (e.g. obesity, diabetes, chronic kidney or heart disease, alcoholism).\n\n【3】Five GAS isolates were available. Two of the isolates were _emm_ type 11; antimicrobial drug–susceptibility profiles for the 2 were identical (i.e. tetracycline resistant). The 2 patients reported no close contact with each other, but they had the same home health aide. The other 3 isolates had different _emm_ types (1, 12, and 82) and were antimicrobial drug pansensitive.\n\n【4】We interviewed 58 household contacts of the case-patients (35 adults, 23 children) regarding symptoms and risks for secondary GAS infection. Among these contacts, 2 adults reported a sore throat and 6 children reported fever (without sore throat), but no confirmed secondary GAS infections were identified. Because of the known increased risk for iGAS among Native Americans and the level of crowding (average of 2–3 persons/bedroom) and the high proportion of adult household contacts with predisposing underlying conditions (29%) in this population, azithromycin prophylaxis was offered to household contacts who spent \\> 24 hours with a case-patient during the 7 days preceding the onset of illness.\n\n【5】With the exception of the 2 case-patients with a common health aide, we found no common epidemiologic links or common behaviors among patients that suggested a single-source outbreak. This was further supported by the finding of multiple _emm_ types among the isolates. These are not unusual findings in community outbreaks of iGAS; clusters of iGAS cases have often been observed without a common source . Localized and transient increases in sporadic GAS infections may occur because of an influx of a new _emm_ type into a population with low levels of community immunity to that specific _emm_ type; an increase in the detection and reporting of iGAS without a true increase in infection; or an increase in conditions that predispose persons to iGAS, such as GAS pharyngitis among children or concurrent influenza or other virus outbreaks in the community.\n\n【6】Past studies have shown that the risk of secondary iGAS infection among household contacts of patients with iGAS disease is higher than that among the general population but still low . Although Centers for Disease Control and Prevention guidelines do not recommend routine chemoprophylaxis for household contacts of patients with iGAS infection, the guidelines state that providers may choose to offer antimicrobial drug prophylaxis to those household contacts at increased risk for iGAS infection . Because Native Americans have increased rates of iGAS disease, compared with those of the general population, and because households in this investigation were crowded and many contacts had predisposing underlying conditions, we recommended that household contacts receive prophylaxis if given within 30 days of the index case-patient’s illness . No additional cases were reported at least 3 months after the investigation and intervention.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ca97fc31-1ddd-45b5-b1ce-832ac4ce98d2", "title": "Enterovirus D68–Associated Severe Pneumonia, China, 2014", "text": "【0】Enterovirus D68–Associated Severe Pneumonia, China, 2014\n**To the Editor:** Over the past 4 years, outbreaks caused by enterovirus type D68 (EV-D68) infection have occurred in many parts of the world ; this virus can cause severe respiratory tract infections (RTIs) in children. This public health concern has been boosted by the recent outbreaks of EV-D68 infection in the United States . Outbreaks associated with novel EV-D68 have also been reported during 2006–2012 in China . However, since 2012, no EV-D68 infections in China have been reported. Whether the EV-D68 outbreaks in the United States affected those in China is unclear. Continuous characterization of EV-D68 epidemics is therefore necessary for purposes of early alert and for facilitating control measure decisions.\n\n【1】To determine EV-D68 prevalence in China, we screened for EV-D68 infections in 2014 in Beijing, China. We tested patients with RTI during August–November 2014, reported by the Respiratory Virus Surveillance System, established by Beijing Center for Disease Prevention and Control. The System covers 30 sentinel hospitals in all 16 districts of Beijing. We obtained 1,478 clinical specimens (1,034 nasopharyngeal swab and 444 sputum). Patient ages ranged from 8 months to 93 years (median 33.5 years, mean 37.9 years). Enteroviruses and other known respiratory viruses were detected by real-time PCR . A total of 70 enterovirus-positive samples were identified. Other respiratory viruses detected were 89 rhinoviruses, 87 influenza viruses, 70 human parainfluenza viruses (types 1–4), 43 human coronaviruses, 26 respiratory syncytial viruses, 29 adenoviruses, 9 bocaviruses, and 2 metapneumoviruses. Among the EV-positive samples, 1 was positive for EV-D68 according to PCR amplification of the viral protein 1 (VP1) gene ; no other respiratory viruses were detected in this patient.\n\n【2】The EV-D68–positive patient was a 5-year-old girl with no underlying disease. On August 5, 2014, she had fever (highest temperature 40°C), cough, breathing difficulty, abdominal pain, diarrhea, and vomiting. Her condition deteriorated, and on August 11, she was hospitalized in Beijing Children’s Hospital with a diagnosis of severe pneumonia. A nasopharyngeal swab sample was collected at the time of admission. She was released on August 16, after receiving symptomatic supportive treatment in the general ward; she had not required mechanical ventilation. She and her family had no history of travel in the months before she became ill.\n\n【3】To further characterize this virus strain, Beijing-R0132, we amplified the genome sequence directly from a nasopharyngeal swab sample by using overlapping primers designed according to the reference sequence . The termini of the genome were independently determined by using the RACE System (Invitrogen, Carlsbad, CA, USA) according to the manufacturer’s protocol. Each amplicon was sequenced 4 times by using the Sanger method, and the forward and reverse sequences agreed well. Sequences were assembled by using DNAStar software (Lasergene, Madison, WI, USA) and deposited in GenBank . The genome of Beijing-R0132 was 7,334 nt long, including 699 nt in 5′-untranslated regions (5′-UTRs), 6,567 nt in open reading frame (ORF), and 68 nt in 3′-UTR. Beijing-R0132 shared 96% nt sequence identity with the virus circulating in the United States in 2014, US/CO/14-60. In contrast to the prototype EV-D68 (Fermon strain, AY426531), deletions of the CTCAAAACCTCCAGTACATAACA sequence in the 5′-UTR and TTATTTATAACA sequence in the front of the ORF of Beijing-R0132 were observed, corresponding with nt 682–704, and nt 721–732 of the Fermon strain, which were similar to those identified in the United States in 2014.\n\n【4】We then used MEGA software version 6.06  to analyze the phylogeny of the whole genome and the VP1 gene with EV-D68 sequences available in GenBank . Beijing-R0132 was clustered with most of the EV-D68 strains that circulated throughout the United States during 2014. The strains identified from Beijing in 2008, represented by BCH895A/2008, belong to another distinct lineage according to genome phylogeny . Similar relationships were observed in the phylogenic tree of the VP1 gene . Beijing-R0132 and some EV-D68 strains from China identified in 2011 grouped with most of the strains obtained from the United States in 2014. These findings demonstrate that the EV-D68 strain circulating in Beijing was closely related to strains circulating in the United States.\n\n【5】The origin of the 2014 EV-D68 outbreaks in the United States is unclear. This Beijing-R0132 genome sequence provides information for tracking EV-D68 as it spreads throughout the world and for evaluating the sequence diversity in circulating EV-D68 strains. In contrast to EV-D68 detection during the outbreaks in the United States, positive detection of EV-D68 in this study was limited, although the circulating virus strains were closely related. The reason for this disparity warrants further investigation. However, the severe pneumonia caused by EV-D68 reported here underlies the need for intensive attention to surveillance and control of EV-D68 in vulnerable populations, such as young children. In addition, for a megacity with very high population density and mobility, such as Beijing, the citywide implementation of the Respiratory Virus Surveillance System is critical for monitoring the epidemic or potential outbreaks of EV-D68 and other respiratory viruses and for enabling early warning.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b5de1546-70b4-4c51-88bd-318d6642ef17", "title": "West Nile Virus Lineage 2 Strain in Greece, 2012", "text": "【0】West Nile Virus Lineage 2 Strain in Greece, 2012\n**To the Editor:** West Nile virus (WNV) has been in Europe at least since the 1960s . Before 2010, WNV epidemics in Europe were caused mainly by lineage 1 strains. However, in 2010, a major WNV epidemic in Central Macedonia, Greece, was caused by a lineage 2 strain (Nea Santa-Greece-2010) .\n\n【1】This strain also circulated during 2011 , causing a second epidemic among humans throughout the country . Although the virus was closely related to the goshawk-Hungary-2004 strain circulating in Hungary, Austria, and Italy , severe epidemics occurred only in Greece; 273 cases of West Nile neuroinvasive disease (WNND) in humans were reported during the 2 seasons .\n\n【2】A third epidemic occurred in 2012, and 109 WNND cases were reported . Until mid-August, most cases were in central (Attica; 29 cases) and northeastern Greece (East Macedonia and Thrace; 10 cases). In contrast, during the same period, only 3 cases were confirmed at the location of the 2010 epidemic epicenter, in Central Macedonia . This situation led to the question of whether the Nea Santa-Greece-2010 strain was responsible for the third epidemic in Greece.\n\n【3】In May 2012, for the second consecutive year, 12 sentinel chicken flocks (72 chickens) and 62 dry ice–baited (source of carbon dioxide) CDC mosquito traps (John W. Hock, Gainesville, FL, USA) were set in areas of Central Macedonia where WNV transmission had been high. The 3 objectives were to 1) monitor WNV activity by testing weekly for antibodies against WNV in sentinel chickens, 2) molecularly characterize the virus, and 3) assess population dynamics of the major vector species.\n\n【4】Serum from the sentinel chickens was tested for WNV-specific antibodies by using the ID-Screen West Nile Competition ELISA kit (IDvet, Montpellier, France). Seropositive chickens were removed from the sentinel flocks and replaced with seronegative chickens.\n\n【5】No enzootic activity was detected until mid-August. Antibodies were first found on August 21, in 2 chickens from the rice-growing region of western Thessaloniki (Delta and Chalkidona municipalities). The seroconversion rate in these municipalities peaked on August 27, when 4 chickens were seropositive, and remained high until September 4. Seroconversion was detected for 10 more birds in the same and other bordering municipalities, all near the rice-growing region and flood plain of the Axios, Loudias, and Aliakmonas Rivers. WNV-specific antibodies were detected in 5 more chickens sampled on September 11, in 2 chickens sampled on September 18, and in 3 chickens sampled on September 25, bringing the total number of seroconverted chickens to 26 (26.5%) of 98 (original 72 + 26 replacements). The highest rates of seroconversion among chickens were detected near rice-growing areas, where _Culex_ mosquito activity (mostly _Cx. pipiens_ , followed by _Cx_ . _modestus_ ) was also highest. _Culex_ mosquito populations peaked in mid-July, ≈30 days before the first case in a human was detected, and maintained high activity until late September.\n\n【6】For the 26 chickens that seroconverted, RNA was retrospectively extracted from serum by using the QIAamp Viral RNA Mini Kit (QIAGEN, Hilden, Germany). Extracts were examined by using a 1-tube, real-time, reverse transcription PCR protocol and primers (WNPolUp, WNPolDo2)  and TaqMan probe (WNPolProb2: 5′-FAM-TCTCTCTCTTTCCCATCATGTTGT-BHQ1–3′) specific to the nonstructural (NS) 5 gene. WNV RNA was detected in 2 seropositive chickens from 2 locations, in samples taken about 1 week before seroconversion.\n\n【7】The positive RNA samples were reverse transcribed, and 3 overlapping fragments of the NS3 gene were amplified by PCR and then sequenced. Both complete NS3 sequences of 1,857 bp were identical . Highest nucleotide sequence identity (99.8%) was to the strain Nea Santa-Greece-2010. Only 3 synonymous nucleotide substitutions consisting of transitions were identified, indicating minimum evolution of the virus during 2010–2012 . Molecular characterization of the 2 isolates actively circulating where cases in humans had been confirmed suggests that the strain responsible for the 2012 epidemic in Greece was again Nea Santa-Greece-2010.\n\n【8】The surveillance system successfully identified areas with increased levels of vector and WNV activity. This information was quickly disseminated to public health authorities so they could intensify control measures in the affected areas. After the first seroconversions in chickens were detected, 9 new WNND cases and 2 cases without central nervous system manifestations in humans were reported from residences near (6–15 km) the chicken coops that housed the seropositive birds . Of the 9 cases of WNND, 7 were reported after the rate of chicken seroconversion peaked.\n\n【9】The continuous occurrence of WNV epidemics in Greece indicates that the virus will probably remain a serious threat. This probability is further supported by the epidemic pattern in the United States, where ≈10 years after its introduction WNV is still causing large epidemics . Surveillance programs that can accurately determine public health risk and lead to timely vector control interventions are needed to prevent human infection.\n\n【10】Particularly in areas such as Europe, where numerous strains of different virulence coexist, molecular identification of the circulating viruses is necessary for risk assessment. Captive sentinel chicken surveillance with repetitive sampling might be an informative tool.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "14b448ee-38f3-4bfa-abfb-4eb2fff931b5", "title": "Snacking on Television: A Content Analysis of Adolescents’ Favorite Shows", "text": "【0】Snacking on Television: A Content Analysis of Adolescents’ Favorite Shows\nAbstract\n\n【1】**Introduction**\n\n【2】Snacking is a complex behavior that may be influenced by entertainment media. Research suggests that snacking and unhealthy foods are commonly shown in programming that targets young audiences, but shows selected for study have been limited. We conducted a content analysis on shows that were named as favorites by adolescents to characterize portrayals of snacking on popular television.\n\n【3】**Methods**\n\n【4】A diverse sample of 2,130 adolescents (mean age, 14.3 y) listed 3 favorite television shows in a 2010 school-based survey. Three episodes each of the 25 most popular shows were coded for food-related content, including healthfulness, portion size, screen time use, setting, and social context. We also analyzed the characteristics of characters involved in eating incidents, the show type, and the show rating. We used χ 2  tests, binomial tests, and multilevel regression models to compare incidence of snacks versus meals, the characteristics of those involved, and snacking across show characteristics.\n\n【5】**Results**\n\n【6】Almost half of food incidents on television shows were snacks. Snacks were significantly more likely than meals to be “mostly unhealthy” (69.3% vs 22.6%, _P_ < .001) and were more likely to include screen time use (25.0% of snacking incidents vs 4.0% of meals, _P_ < .001). Young characters and those coded as being of low socioeconomic status or overweight were overrepresented in snacking incidents. Sitcoms and shows rated for a youth audience were significantly more likely to portray snacking than were shows for adult audiences.\n\n【7】**Conclusion**\n\n【8】Media awareness and literacy programs should include foods and snacking behaviors among the issues they address. More healthful portrayals of food and dietary intake in entertainment shows’ content would create a healthier media environment for youth.\n\n【9】Introduction\n\n【10】Snacking is a complex behavior that has many definitions in research, including eating between main meals and eating typical snack foods, such as chips, candy, and fruit-flavored drinks . National survey data indicate that most adolescents consume 2 or more snacks between meals per day, and energy-dense, nutrient-poor food and beverages are major sources of the energy consumed as snacks . Research has established that dietary intake is influenced by multiple physical and social cues in young people’s home and school environments  and by advertising . However, little research has examined influences on snacking behaviors in particular, especially in the domain of media .\n\n【11】According to a 2015 report, adolescents spend an average of 17 hours per week watching television , and media messages are a powerful social influence . Research on health behaviors such as smoking, early sexual activity, and violence has identified links between viewing entertainment media content and enacting these behaviors . Children and adolescents may form “pseudo-friendships” with television characters and look to them as behavioral role models . Mass media also powerfully affects perceptions of peer norms, which are strong influences on adolescent behaviors . The strength of influence may further depend on the salience of the role model, such as whether the television character matches the viewer in terms of sex or race or has desired attributes (eg, slim body type).\n\n【12】Existing research on the portrayal of food on television has focused almost exclusively on advertising and has suggested associations between food marketing and children’s dietary intake and weight . However, programming itself accounts for a far greater proportion of viewing time than do advertisements. New technology increasingly permits viewing without advertising (which has led to an increase in product placement within shows), giving programming content greater relevance. Little information is available on the portrayal of foods on popular television shows, and most studies are outdated . However, a small body of recent literature found that snacking and unhealthy snack foods are commonly shown in programming that targets young audiences .\n\n【13】A limitation of recent studies is the sample of media content selected for analysis. Researchers selected programming on the basis of its intended audience (ie, children), which may differ from what young people actually watch . Therefore, viewers’ actual exposure to unhealthy content may be inaccurately estimated by studies restricted to youth programming. Similarly, existing studies restricted their parameters to single television networks (eg, Disney channel ) that do not represent the breadth of content available to youth.\n\n【14】This study builds on the limited extant literature on the relationship between media programming and snacking behaviors among youth, using an innovative design. Write-in survey data were collected from a diverse population-based sample of adolescents to identify favorite programs, and popular programs were content-analyzed. The main research objective was to characterize portrayals of snacking, including frequency, healthfulness, portion size, and characters’ use of screens while consuming food. Snacking was compared with eating meals as a reference point for the portrayal of food on television. Because on-screen behaviors may differ for population subgroups and because some characters may be relatable or desirable for young viewers, we also examined the demographic characteristics and weight status of the characters involved. Finally, we examined snacking portrayals by show type (eg, sitcom) and by intended audience. Understanding ways in which snacking behavior is portrayed in popular television will inform recommendations for families and future study of associations with eating patterns among youth.\n\n【15】Methods\n\n【16】### Population, setting, and design\n\n【17】In 2010, 2,793 adolescents attending public middle schools and high schools in Minneapolis/St. Paul, Minnesota, completed surveys as part of a cross-sectional, population-based study of weight and related behaviors (EAT 2010) . Participants were split among middle school (grades 6–8; 46.1%) and high school (grades 9–12; 53.9%). Almost half were male (46.8%), and the sample was racially and ethnically diverse (18.9% white, 29.0% African American, 19.9% Asian American, 16.9% Hispanic, 3.7% Native American, and 11.6% mixed or other race), reflecting the demographic profile of participating schools .\n\n【18】Participants were asked to write in the titles of their 3 favorite television shows. One or more favorite shows were named by 2,130 participants (mean age, 14.3 y \\[standard deviation, 2.0 y\\]), yielding 653 unique shows. Favorite shows were ranked by weighting each participant’s first listed show more highly than the second show, which counted more than the third show. We excluded entries that were broad topic areas (eg, sports, music videos), networks (eg, MTV), or sports or music events (eg, _106 & Park_ ) and included only shows with characters, scenes, dialogue, and plot. Closely related shows such as _CSI_ , _CSI: NY_ , and _CSI: Miami_ were combined and considered as the original version . The 25 most popular shows were content analyzed ; 54.8% of the EAT 2010 sample listed 1 or more of these top 25 shows.\n\n【19】Three episodes of each show were randomly selected from the 2010 season and were accessed via online services (eg, network website, Netflix). Coding was done in 2 batches of 3 coders for the first 10 shows and 2 coders for the remaining 15 shows, with 1 original coder training the 2 new coders. Interrater reliability was ascertained for each batch. The University of Minnesota’s Institutional Review Board’s Human Subjects Committee approved all protocols used in EAT 2010 and determined that this analysis was exempt from review.\n\n【20】### Coding instrument and variables\n\n【21】A coding instrument was created on the basis of previous research for EAT 2010 to assess on-screen food-related incidents, characters traits, and show characteristics . The team developed and revised the coding instrument in multiple iterations and pilot tested it with episodes of the same shows from other seasons before it was finalized. An accompanying codebook was used to detail assessment of each item to enhance consistency across coders.\n\n【22】Coders recorded any time a food was shown or referenced on screen (ie, a _food incident_ ), and 6 measures were the focus of this analysis. We employed a definition of snacking as a food incident that was not part of a meal. _Incident type_ indicated whether a food incident was breakfast, lunch, dinner, or a snack. Several on-screen cues were used to identify meals, including time of day (eg, food eaten in a cafeteria during the school day was coded as lunch), number of foods (eg, multiple food items served at a table), dialogue spoken (eg, “Another dinner got away from you?”), and other context (eg). Food incidents outside of meals were coded as snacks. When typical snack foods were eaten during a meal (eg, chips with a burger), the incident was coded as a meal. Of the 559 food incidents coded across 75 episodes, type could not be determined (eg, foods shown in the background of a scene) for 163 (29.2%), and these were excluded from analysis.\n\n【23】The overall _healthfulness_ of a food incident was coded as mostly healthy (eg, well balanced meals, fruit, vegetables, lean proteins, cheese, yogurt), mostly unhealthy (eg, baked desserts, candy, potato chips, snack foods, sugared cereal), or unclear (typically when an incident was referenced rather than shown). When multiple foods were shown, the incident was coded for the overall balance (eg, “chicken, corn, mashed potatoes, milk, crumble, undeterminable item” was coded as mostly healthy; “pretzel snack bags, cake, ketchup, mustard, other undeterminable food” was coded as mostly unhealthy). If foods were shown being eaten by a character, coders noted whether the portion was _excessive_ (ie, much more than is appropriate for the given meal type, for example, with food heaping over the plate or a character taking multiple servings). Examples include “bucket of cheese,” “plate full of bacon smothered in pancake syrup, sausage links, milk.”\n\n【24】Non–food-related aspects of each incident were also coded. _Screen time_ was coded as a television, computer, or similar electronic device in use during the incident. Two additional items were used to describe food incidents, including the physical _setting_ (eg, eating at school, eating at a table at home) and the _social context_ (eg, eating alone or with others).\n\n【25】Two show characteristics were used in this analysis. _Show type_ was coded as sitcom, cartoon, or drama. Parental Guide _ratings_ were ascertained on the basis of information taken from the show’s website or the Internet Movie Database website  and were combined into 3 categories for analysis: youth audience (TV-Y and TV-G), general audience (TV-PG), and older audience (TV-14 and TV-MA).\n\n【26】Characteristics of the main and supporting characters in each show were recorded . Briefly, characters’ socioeconomic status (SES) was coded as “poor/lower class,” indicated by material goods (eg, cars, clothing, housing) or references to Medicaid, food shortage, or other needs of low-SES people; or “wealthy/upper class,” indicated by material goods (eg, a known celebrity, expensive car, large home) or references to extreme wealth. All other characters were coded as “average/middle class.” A character’s weight status was coded as “thin/underweight” if the character appeared thinner than normal with obvious clavicle bones, facial bones, rib cage, or other bones protruding; if the character’s body mass index (BMI) were calculated, it would be less than 18.5 kg/m 2  . A character’s weight status was coded as “overweight” if the character had excess body fat (eg, obvious pot belly); if the character’s BMI were calculated, it would likely be from 25.0 to 30.0 kg/m 2  . A character’s weight status was coded as “obese” if the character carried an excessive amount of weight; if the character’s BMI were calculated, it would likely be more than 30 kg/m 2  . All other characters were coded as “average weight.”\n\n【27】### Data collection\n\n【28】The data collection protocol was identical in both batches of coding (shows 1–10 and 11–25). Each episode was coded using a 3-step viewing process. First, coders identified all incidents to be coded and noted characters involved in each relevant scene. Second, coders entered all relevant information into the coding instrument. A third and final viewing ensured that all information was captured and appropriately coded.\n\n【29】Intercoder reliability was calculated at the end of the coder training periods, using episodes of select shows in a previous season, and was completed for the 2 batches separately. Cohen’s κ statistic is appropriate for categorical variables and adjusts for chance agreement . In the first batch, individual items that had a κ of less than 0.70 were reviewed by the team and revised to finalize the coding instrument. Discrepancies during the training of the second set of coders were resolved by the lead coder from the first group. Food and beverage items from shows 1 through 10 had a mean κ of 0.73, and food and beverage items from shows 11 through 25 had a mean κ of 0.98. Character demographics from shows 1 through 10 had a mean κ of 0.81, and character demographics from shows 11 through 25 had a mean κ of 1.00, indicating excellent reliability .\n\n【30】### Data analysis\n\n【31】Four data sets were created to address this study’s research questions: favorite television shows (n = 25), food incidents (n = 396), main and supporting characters (n = 366), and a combined data set of characters involved in food incidents (n = 971). For example, if 3 characters were shown eating a meal together, this single incident would generate 3 separate cases, 1 for each character.\n\n【32】We used these data sets separately and in combination for analysis; we used χ 2  tests to compare snacks with meals in favorite television shows and tested differences in the characters involved in snacking versus meal incidents. In addition, 2-sided binomial tests were used to test differences in the characteristics of those involved in snacking incidents compared with the overall sample of characters across all shows. Finally, multilevel regression models were used to test associations between snacking characteristics and show characteristics, clustering food incidents within the same show to permit accurate inference. Predicted probabilities of each snacking characteristic were generated from these models. Analyses were conducted using SAS version 9.3 (SAS Institute, Inc). Significance was set at an α level of .05.\n\n【33】Results\n\n【34】### Snack versus meal portrayals\n\n【35】Food incidents were common in this sample of shows; 92.0% of coded episodes included at least 1 incident (mean = 5.3 incidents/episode; range, 0–17). Almost half of food incidents (48.5%) were snacks and the rest were breakfasts (11.1%), lunches (14.4%) or dinners (26.0%). Food incidents coded as snacks were significantly more likely to be “mostly unhealthy” than foods coded as meals (69.3% vs 22.6%, _P_ < .001) .\n\n【36】Among the 396 food incidents, 63.6% (n = 252) were shown, and 36.4% (n = 144) were referenced or mentioned by characters but not shown. Of the incidents of food shown on screen, 135 involved food shown being consumed by a character, and significant differences were noted between snacks and meals . For example, 25% of snacking incidents included screen time (ie, television or computer use while eating), compared with only 4.0% of meal incidents ( _P_ < .001). There were no differences in the proportion of incidents involving excessive portion sizes in snacks versus meals.\n\n【37】### Characters involved in food incidents\n\n【38】Table 2 shows the descriptive demographic and weight status variables of all main and supporting characters in the top 25 favorite shows (n = 366). More than half of characters were male (58.9%) and most were adults. Most characters were coded as white (76.8%) and as being of average SES (81.1%). Most were coded as thin or of average weight (86.0%).\n\n【39】Two comparisons were made using the 971 distinct character-food incidents: 1) the demographic and weight characteristics of those involved in snack versus meal incidents, and 2) the demographic and weight characteristics of those involved in snack incidents compared with the total sample of 366 characters . Several significant differences emerged: 32.2% of characters in meal incidents were children or adolescents, but 41.8% of those in snack incidents were in these age groups ( _P_ \\= .01). The proportion of snack incidents that included children or adolescents (41.8%) was also significantly higher than the overall proportion of child and adolescent characters (32.5%, _P_ < .001). Involvement in snacks versus meals also differed by characters’ apparent SES; 9.4% of snacking incidents including characters coded as lower SES, compared with 3.5% of meal incidents ( _P_ < .001) and 4.0% of characters overall ( _P_ < .001). Overweight characters were shown in snacking incidents at almost twice the rate of meal incidents (10.3% vs 5.6%, _P_ \\= .03) but were not overrepresented in snacking scenes compared with the total sample of characters (9.6%).\n\n【40】### Differences across shows\n\n【41】Sitcoms were significantly more likely than other types of shows to portray snacking, and shows rated for a general or a youth audience were more likely to show snacking than shows intended for adult audiences . Cartoons were significantly more likely to portray excessive consumption of snacks (37.5%) than sitcoms (2.8%) or dramas (0%; _P_ \\= .05). There were no significant differences in excessive consumption of snacks or screen time with snacks across show types or ratings.\n\n【42】Discussion\n\n【43】In this sample of popular shows, snacks were shown often and typically included unhealthy foods or behaviors, such as watching television while eating. Snacking incidents disproportionately included characters in certain demographic groups (eg, youth, low-SES) and overweight characters. Snacking was also more common in sitcoms than in other types of shows and less common in shows that target adult audiences. Our findings generally align with the limited number of recent studies showing a high prevalence of snacking, unhealthy foods, and poor dietary patterns in entertainment programming , but they are the first in a sample of shows named as popular by youth. Young people experience a media environment that normalizes unhealthy foods and snacking behaviors, which is expected to contribute to unhealthy eating behaviors among viewers.\n\n【44】Although snacking behaviors on television shows may seem innocuous, extensive research has demonstrated that on-screen behaviors create social norms of behavior that are then seen as typical or expected by viewers . Characters may be seen as pseudo-friends and role models . Peer modeling raises concerns that frequent viewing of characters snacking, particularly on unhealthy food items, may contribute to these same behaviors in viewers when they are seen as normative for those with similar demographic characteristics or desirable body types. Furthermore, frequent unhealthy snacking by thin or average-weight characters may set up unrealistic expectations about dietary behaviors and weight gain over time. More frequent snacking among children and adolescents on television (and higher rates of snacking in shows that target youth audiences) may create a norm of snacking, especially among young characters who may be the most salient on-screen “peers” to young viewers. Similarly, characters coded as lower income were more often involved in snacking incidents. Such images contribute to expectations of poor dietary intake in populations that already have a high prevalence of obesity  and may promote negative stereotypes .\n\n【45】This study has several strengths. The shows analyzed were listed as favorites by a diverse sample of adolescents. This sampling method allowed us to analyze television shows that were actually popular among adolescents rather than those that simply target adolescent audiences. In addition, coding 3 episodes of 25 shows identified a large number of food incidents, which permitted statistically valid comparisons across incident, character, and show characteristics.\n\n【46】This study also has limitations. First, certain types of entertainment programming could not be meaningfully coded, such as sports shows or music video shows. Scripted shows (the focus of this study) make up a large portion of the television schedule, but other formats likely include portrayals of food and food-related behaviors . Similarly, less popular shows — nominated as favorites by almost half the student sample — were not included in this analysis. Findings may therefore not apply to all shows that youth favor. Second, snacking is difficult to define; the type of food, the quantity of food consumed, the time of day it was consumed, and other factors contribute to the interpretation of whether the food eaten was a snack or a meal . Although interrater reliability was high for this item (κ = 0.98 in both batches) and television content creators rely on easily interpretable cues to establish a scene (eg, a set table to indicate a meal), viewers may understand eating instances differently, on the basis of their experiences. Finally, favorite shows were nominated in 2010 and portrayals of snacking may have changed in recent years.\n\n【47】Findings provide the basis for continued research that examines on-screen unhealthy snacking behaviors in relation to eating among youth. Detailed exploration is needed of ways in which youth identify with particular characters, perceive on-screen behaviors as normative, and adapt their behavior to understand underlying mechanisms. Both the content of entertainment media and the advertising between segments should be routinely included in studies of media influence on health behaviors.\n\n【48】Findings on the portrayal of unhealthy snacking add to research on media content , which has led to recommendations for reduced screen time, parental coviewing with youth, and media regulation. Media awareness and literacy programs should include foods and snacking behaviors among the issues they cover, to break the links between on-screen behavior, perceived norms, and health behaviors. Within the entertainment industry, adoption of voluntary guidelines for more healthful portrayals of dietary intake in show content, as has been done for smoking depictions, would parallel industry changes in advertising and create a healthier media environment for youth.\n\n【49】Tables\n------\n\n【50】#####  Table 1. Characteristics of Food Incidents (N = 252) Portrayed in Adolescents’ 25 Favorite Television Shows, Minnesota,\n\n| Characteristic | Snacks, % | Meals, % | ? 2 | _P_ Value |\n| --- | --- | --- | --- | --- |\n| **Healthfulness a** | **Healthfulness a** | **Healthfulness a** | **Healthfulness a** | **Healthfulness a** |\n| Mostly healthy | 23.4 | 52.9 | 88.2 | <.001 |\n| Undeterminable | 7.3 | 24.5 | 88.2 | <.001 |\n| Mostly unhealthy | 69.3 | 22.6 | 88.2 | <.001 |\n| **Excessive portion size b** | **Excessive portion size b** | **Excessive portion size b** | **Excessive portion size b** | **Excessive portion size b** |\n| No | 90.0 | 92.0 | 0.16 | .69 |\n| Yes | 10.0 | 8.0 | 0.16 | .69 |\n| **Included screen time use b** | **Included screen time use b** | **Included screen time use b** | **Included screen time use b** | **Included screen time use b** |\n| No | 75.0 | 96.0 | 12.7 | <.001 |\n| Yes | 25.0 | 4.0 | 12.7 | <.001 |\n| **Physical setting b** | **Physical setting b** | **Physical setting b** | **Physical setting b** | **Physical setting b** |\n| At school | 3.3 | 6.7 | 45.1 | <.001 |\n| At a table (at home) | 3.3 | 40.0 | 45.1 | <.001 |\n| At home (but not at table) | 33.3 | 10.7 | 45.1 | <.001 |\n| At a sit-down restaurant | 13.3 | 26.7 | 45.1 | <.001 |\n| At a fast-food restaurant | 3.3 | 1.3 | 45.1 | <.001 |\n| At work | 11.7 | 9.3 | 45.1 | <.001 |\n| “On the run” | 0.0 | 0.0 | 45.1 | <.001 |\n| Other | 31.7 | 5.3 | 45.1 | <.001 |\n| **Social context b** | **Social context b** | **Social context b** | **Social context b** | **Social context b** |\n| Alone | 16.7 | 13.3 | 8.0 | .05 |\n| With peers | 71.7 | 54.7 | 8.0 | .05 |\n| With family member(s) | 11.7 | 30.7 | 8.0 | .05 |\n| Other | 0 | 1.3 | 8.0 | .05 |\n\n【52】a  Overall healthfulness of a food incident was coded as mostly healthy (eg, well balanced meals, fruit, vegetables, lean proteins, cheese, yogurt), mostly unhealthy (eg, baked desserts, candy, potato chips, snack foods, sugared cereal), or unclear (typically when an incident was referenced rather than shown). When multiple foods were shown, the incident was coded for overall balance.  \nb  Of 135 incidents of food consumed on screen.\n\n【53】#####  Table 2. Characteristics of Characters (N = 366) Involved in Food Incidents (N = 971) in Adolescents’ 25 Favorite Television Shows, Minnesota,\n\n| Characteristic | Total % | Snack Incidents, % | Meal Incidents, % |\n| --- | --- | --- | --- |\n| **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 58.9 | 62.8 | 60.6 |\n| Female | 41.1 | 37.2 | 39.4 |\n| Snacks vs meals: 2 , _P_ value | 0.42,52 | 0.42,52 | 0.42,52 |\n| Snacks vs total: _z_ score, _P_ value | 1.59,11 | 1.59,11 | 1.59,11 |\n| **Age group** | **Age group** | **Age group** | **Age group** |\n| Child/adolescent (<20 y) | 32.5 | 41.8 | 32.2 |\n| Young adult (20–29 y) | 17.2 | 13.4 | 17.9 |\n| Adult (=30 y) | 50.3 | 44.8 | 49.9 |\n| Snacks vs meals: 2 , _P_ value | 9.14,01 | 9.14,01 | 9.14,01 |\n| Snacks vs total: _z_ score, _P_ value | 3.98, <.001 | 3.98, <.001 | 3.98, <.001 |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| White | 76.8 | 80.2 | 78.9 |\n| Other | 23.2 | 19.8 | 21.1 |\n| Snacks vs meals: 2 , _P_ value | 0.22,64 | 0.22,64 | 0.22,64 |\n| Snacks vs total: _z_ score, _P_ value | 1.60,11 | 1.60,11 | 1.60,11 |\n| **Socioeconomic status a** | **Socioeconomic status a** | **Socioeconomic status a** | **Socioeconomic status a** |\n| Poor/lower class | 4.0 | 9.4 | 3.5 |\n| Average/middle class | 81.1 | 72.7 | 71.2 |\n| Wealthy/upper class | 14.9 | 17.9 | 25.3 |\n| Snacks vs meals: 2 , _P_ value | 17.3, <.001 | 17.3, <.001 | 17.3, <.001 |\n| Snacks vs total: _z_ score, _P_ value | 5.56, <.001 | 5.56, <.001 | 5.56, <.001 |\n| **Weight b** | **Weight b** | **Weight b** | **Weight b** |\n| Thin/underweight and average weight | 86.0 | 87.0 | 90.9 |\n| Overweight | 9.6 | 10.3 | 5.6 |\n| Obese | 4.4 | 2.7 | 3.5 |\n| Snacks vs meals: 2 , _P_ value | 6.78,03 | 6.78,03 | 6.78,03 |\n| Snacks vs total: _z_ score, _P_ value | 0.59,28 | 0.59,28 | 0.59,28 |\n\n【55】Abbreviation: BMI, body mass index.  \na  Characters’ socioeconomic status (SES) coded as “poor/lower class,” indicated by material goods (eg, cars, clothing, housing) or references to Medicaid, food shortage, or other needs of low-SES people. Characters coded as “wealthy/upper class,” indicated by material goods (eg, a known celebrity, expensive car, large home) or references to extreme wealth. All other characters coded as “average/middle class.”  \nb  A character’s weight status was coded as “thin/underweight” if the character appeared thinner than normal with obvious clavicle bones, facial bones, rib cage, or other bones protruding; if the character’s BMI were calculated, it would be <18.5 kg/m 2  . A character’s weight status was coded as “overweight” if the character had excess body fat (eg, obvious pot belly); if the character’s BMI were calculated, be 25.0–30.0 kg/m 2  . A character’s weight status was coded as “obese” if the character carried an excessive amount of weight; if the character’s BMI were calculated, it would likely be >30.0 kg/m 2  . All other characters were coded as “average weight.”\n\n【56】#####  Table 3. Show Characteristics and Predicted Probabilities of Each Snack Characteristic in Adolescents’ 25 Favorite Television Shows, Minnesota, 2010 a  \n\n| Characteristic | Snacks vs Meals | Mostly Unhealthy Snacks | Excessive Consumption of Snacks b | Screen Time Use With Snacks b |\n| --- | --- | --- | --- | --- |\n| % |\n| --- |\n| **Type** | **Type** | **Type** | **Type** | **Type** |\n| Sitcom (n = 12) | 57.5 c | 65.1 | 2.8 c | 28.5 |\n| Cartoon (n = 4) | 29.9 d | 87.2 | 37.5 d | 27.3 |\n| Drama (n = 9) | 40.2 d | 71.3 | 0.0 c | 9.4 |\n| F statistic, _P_ value | 4.81,009 | 1.05,35 | 3.28,05 | 0.67,52 |\n| **Rating e** | **Rating e** | **Rating e** | **Rating e** | **Rating e** |\n| Youth (Y or G) (n = 5) | 52.4 c,d | 56.3 | 22.8 | 15.6 |\n| General (PG) (n = 8) | 57.3 c | 75.5 | 4.1 | 31.6 |\n| Mature (14 or MA) (n = 12) | 36.7 d | 71.9 | 0 | 19.6 |\n| F statistic, _P_ value | 3.29,04 | 1.48,23 | 1.42,26 | 0.57,57 |\n\n【58】a  From multilevel regression models, accounting for clustering of incidents within shows.  \nb  Of 135 incidents of food consumed on screen.  \nc,d  Predicted probabilities that share a superscript are not significantly different ( _P_ \\> .05).  \ne  Ratings were ascertained on the basis of information taken from each show’s website or the Internet Movie Database website  and combined into 3 categories for analysis: “Y or G,” youth audience; “PG,” general audience; and “14 or MA,” older audience.\n\n【59】1\\. _Family Guy_\n\n【60】2\\. _The Simpsons_\n\n【61】3\\. _SpongeBob Square Pants_\n\n【62】4\\. _CSI_\n\n【63】5\\. _iCarly_\n\n【64】6\\. _South Park_\n\n【65】7\\. _Two and a Half Men_\n\n【66】8\\. _That ’70s Show_\n\n【67】9\\. _The Game_\n\n【68】10\\. _George Lopez_\n\n【69】11\\. _Everybody Hates Chris_\n\n【70】12\\. _House_\n\n【71】13\\. _The Vampire Diaries_\n\n【72】14\\. _My Name is Earl_\n\n【73】15\\. _Gossip_ _Girl_\n\n【74】16\\. _The Office_\n\n【75】17\\. _Degrassi_\n\n【76】18\\. _Hannah Montana_\n\n【77】19\\. _Wizards of Waverly Place_\n\n【78】20\\. _The Suite Life on Deck_\n\n【79】21\\. _Secret Life of the American Teenager_\n\n【80】22\\. _Supernatural_\n\n【81】23\\. _NCIS_\n\n【82】24\\. _Bones_\n\n【83】25\\. _Scrubs_", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "05ec5f3e-1e26-46d6-8cf0-37756d669a57", "title": "Disparities in Health-Related Internet Use Among African American Men, 2010", "text": "【0】Disparities in Health-Related Internet Use Among African American Men, 2010\nAbstract\n\n【1】Given the benefits of health-related Internet use, we examined whether sociodemographic, medical, and access-related factors predicted this outcome among African American men, a population burdened with health disparities. African American men (n = 329) completed an anonymous survey at a community health fair in 2010; logistic regression was used to identify predictors. Only education (having attended some college or more) predicted health-related Internet use ( _P_ < .001). African American men may vary in how they prefer to receive health information; those with less education may need support to engage effectively with health-related Internet use.\n\n【2】Objective\n\n【3】Patients who search for health information on the Internet and discuss their findings with physicians are more engaged during clinical encounters, better prepared to ask detailed questions , and more satisfied with their care .Those who engage in health-related Internet use tend to be white  and female . African American men are disproportionately burdened with health disparities , yet they are less likely to engage in health-related Internet use. Because patients who are more activated (ie, better equipped with the skills and confidence that enable active engagement in their own health care) during encounters with medical professionals have better health outcomes , a lack of health-related Internet use may contribute to health disparities. On the basis of previous studies , we hypothesized that socioeconomic factors and health care access may explain differences in health-related Internet use among African American men.\n\n【4】Methods\n\n【5】We conducted a survey of African American men attending an annual community health fair hosted by the Cleveland Clinic Foundation in 2010. The fair provided free health screenings such as blood pressure and diabetes testing. The 329 participants were 18 years or older, could read and write in English, and verbally agreed to complete an anonymous 47-item survey that took approximately 10 minutes to complete. Study participants constituted 27% of all health fair attendees. Student volunteers administered the surveys; the institutional review board at the Cleveland Clinic approved this study. Behavioral measures were questions from previously validated instruments used in nationally representative health studies.\n\n【6】The dichotomous outcome measure, health-related Internet use, was assessed by asking participants to identify their most recent source of health information. The Internet was coded as 1; all other sources were coded as 0. All predictors were also coded as dichotomous, including age (≥65 vs <65); education (some college or more vs high school diploma or less); income (annual income <$25,000 vs ≥$25,000); chronic illness (diagnosis of heart disease, hypertension, diabetes, kidney disease, or high cholesterol vs none); sick leave (sick leave available from most recent or previous employment vs not available); identification of usual source of care (≥1 health care provider identified vs none); and emergency department (ED) use (ED as most frequent care source vs not most frequent).\n\n【7】Descriptive analyses examined sociodemographic and health characteristics. A χ 2  test identified factors associated with health-related Internet use, and hierarchical logistic regression determined significant predictors of health-related Internet use. Analyses were conducted using SPSS 19 (IBM Corporation, Armonk, New York).\n\n【8】Results\n\n【9】The mean age of participants was 49 (standard deviation, 12.2 y); 53% had attended at least some college; 37.4% were married; 57.4% were employed; 45% reported an annual household income of under $25,000; 47% reported having at least 1 chronic illness; and 26% used the ED as their most frequent source of health care. Only 31% had sick leave available from their previous or most recent employer. The Internet was the most recent source of health information for 39% of participants.\n\n【10】Cross-tabulations of the outcome variable with potential predictors showed that education, income, chronic illness, sick leave, and ED use were significantly associated with health-related Internet use . Age and usual source of care were not significant. Participants with some college education or more, those with an annual income of over $25,000, those with a diagnosis of at least 1 chronic illness, those with sick leave available, and those who did not report the ED as their most frequent care source were more likely to have used the Internet as their most recent source of health information.\n\n【11】Only education (having attended some college or more) significantly predicted health-related Internet use in the final regression model . When other factors were controlled, African American men with some college education or more were nearly 4 times as likely to report recent health-related Internet use than African American men with a high school diploma or less (OR = 3.9; 95% CI, 2.3–6.5). The predicted probability of health-related Internet use was .55 for participants with some college education or more and .20 for those with a high school diploma or less.\n\n【12】Discussion\n\n【13】This study addresses a gap in knowledge on disparities in health-related Internet use among African American men. Health-related Internet use is common among this group; almost 40% of respondents reported that the Internet was their most recent source of health information. Contrary to evidence that suggests that patients lacking health insurance or convenient health care providers may be more likely to engage in health-related Internet use , we found no relationship between health-related Internet use and health care access. Education level was the only significant predictor; as others have found , men with higher education levels were more likely to engage in health-related Internet use.\n\n【14】These findings have several implications for health promotion and practice. First, health providers should note that African American men may vary in how they prefer to receive health information. Others have found that this population is less likely to talk to providers about health-related Internet use but more likely to use the health-related information on Internet to change their health behavior . Therefore, health providers may need to actively engage African American men in discussions about health-related Internet use. Second, African American men with lower education levels may need support for increasing their engagement in health-related Internet use — a goal consistent with _Healthy People 2020_ . Such support may include promoting reliable health-related websites and assisting African American male patients — regardless of education level — in assessing the quality of information they have found.\n\n【15】Limitations of this study include a pilot sample size and the use of convenience sampling. However, our sample is demographically similar to nationally representative samples of African American men  in terms of marital status, education level, income, and unemployment. Future studies should assess ways to increase health-related Internet use among African American men with lower education levels as well as determine how health outcomes are affected by health-related Internet use among this population.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e43bf737-50de-402f-bd7f-5deb3a7f85ab", "title": "Serologic Status for Pandemic (H1N1) 2009 Virus, Taiwan", "text": "【0】Serologic Status for Pandemic (H1N1) 2009 Virus, Taiwan\nAs experts were predicting and warning of a new influenza pandemic , an influenza epidemic occurred in April 2009 in the United States and Mexico and resulted in a pandemic 2 months later. The etiologic agent was identified as pandemic (H1N1) 2009 virus. Worldwide, most patients infected with this virus were <25 years of age, and one third of serious cases were in persons <50 years of age .\n\n【1】The hemagglutinin gene of pandemic (H1N1) 2009 virus was shown to be derived from the 1918 swine influenza virus and contained other genes from human, avian, and swine influenza viruses from Eurasia . In this study, we evaluated levels of preexisting cross-reactive antibodies against pandemic (H1N1) 2009 virus produced after previous infection in children and adults in Taiwan. We also examined serologic changes after vaccination with seasonal nonadjuvanted influenza vaccine.\n\n【2】### The Study\n\n【3】Serum samples were obtained during a nationwide influenza vaccine serologic study in Taiwan that started in 2006. Children (<5 years of age), adults (20–49 years of age), older adults (50–74 years of age), and elderly adults ( \\> 75 years of age) were recruited. Serum samples were obtained immediately before and 3 weeks after intramuscular injection with 1 dose of nonadjuvanted, trivalent, inactivated influenza vaccine formulated for the 2008–09 Northern Hemisphere winter season (samples were obtained from some participants \\> 75 years of age before and after receiving 1 dose of the vaccine formulated for the 2007–08 winter season).\n\n【4】Microneutralization (MN) and hemagglutination inhibition (HI) assays were performed according to the World Health Organization Manual on Animal Influenza Diagnosis and Surveillance . Using these assays with 0.75% guinea pig erythrocytes, we assayed samples for antibodies against A/California/07/2009 (H1N1) virus. Only prevaccination HI assays were conducted for children.\n\n【5】The seroprotection rate was defined as the percentage of serum titers \\> 40 by HI or titers \\> 160 by MN. The seroconversion rate was defined as the percentage of vaccine recipients whose serum HI titers or MN titers increased by at least 4-fold after vaccination. A p value <0.05 was considered significant. Stata software version 8.2 (StataCorp LP, College Station, TX, USA) was used for analysis.\n\n【6】A total of 176 participants (40 children, 36 adults, 50 older adults, and 50 elderly adults) were enrolled . Few or no preexisting cross-reactive antibodies against pandemic (H1N1) 2009 virus were detected by HI assay in samples from children (prevaccination seroprotection rate 0%). As age increased, prevaccination seroprotection rates became higher for HI and MN assays. After vaccination, seroprotection rates and geometric mean titers measured by HI assay were essentially unchanged but increased significantly in the 3 adult groups when measured by MN assay (p<0.05). Seroconversion rates among all participants were low. Analyses of relationships between age and antibody titers are shown in the Figure .\n\n【7】We log-transformed MN and HI titers, and used multiple regression, controlling for age groups to analyze the correlation between age and titer. Doubling of HI titers corresponded to an estimated 75% (p<0.01) increment in MN titers adjusted by age. When adjusted for HI titers, MN titers in older adults and elderly adults were 1.74× (p<0.01) and 2× (p<0.01), respectively, those in adults. Older adults and elderly adults with the same HI titers were more likely to have higher MN titers than adults (p<0.05, by ordinal logistic regression analysis).\n\n【8】### Conclusions\n\n【9】We found that children in Taiwan had few or no cross-reactive antibodies against pandemic (H1N1) 2009 virus. However, adults had some preexisting immunity to this virus. A major finding was that 18 (36%) of 50 elderly adults in Taiwan born before 1935 had protective antibodies against currently circulating pandemic (H1N1) 2009 virus. The seroprotection rate may be 50% in persons >80 years of age.\n\n【10】The MN assay showed that seasonal influenza vaccines generated large increases in geometric mean titers in vaccinees in all age groups. We suggest that seasonal influenza vaccines are likely to elicit a certain degree of cross-reactive antibodies against pandemic (H1N1) 2009 virus and may provide some level of protection. In persons who had no preexisting seroprotective titers against pandemic (H1N1) 2009 virus, the cross-reactivity produced was not sufficient to prevent disease; however, it may protect against the severe forms of the disease.\n\n【11】Hancock et al. reported that only 4% of persons in the United State born after 1980 had preexisting cross-reactive antibodies against pandemic (H1N1) 2009 virus, and that 34% of persons born before 1950 had neutralizing titers \\> 80\\. However, Itoh et al. reported that blood donors from Japan who were born after 1920 had almost no appreciable neutralizing antibodies against this virus. Because the hemagglutinin gene of pandemic (H1N1) 2009 virus is similar to that of viruses that circulated in humans during 1918–1957 , Itoh et al. suggested that pandemic (H1N1) 2009 virus is antigenically divergent from human influenza viruses (H1N1) that circulated during the 1920s–1950s.\n\n【12】Our results are consistent with those of Hancock et al. who suggested that human influenza virus (H1N1) circulating in Taiwan after 1920 resembled the 1918 pandemic virus (H1N1) and pandemic (H1N1) 2009 virus and could lead to cross-protection against the current virus. Furthermore, unlike the situation in the United States, there was no program for vaccination against the 1976 swine influenza virus (A/NJ/76) in Taiwan. However, a similar virus was present in Taiwan before 1957. The results of our study also explain why only 7% of patients hospitalized for pandemic (H1N1) 2009 virus in Taiwan were \\> 65 years of age . Similar epidemiologic observations have been reported in the United States  and New Zealand .\n\n【13】Whether elderly persons still have cross-reactivity several decades after exposure to 1918 (H1N1) virus is unknown. The concept of original antigenic sin is a probable explanation. Original antigenic sin has been described in relation to influenza virus, dengue virus, HIV, and several other viruses . For persons >65 years of age, the 1918 (H1N1) virus is likely the first influenza virus to which they were exposed, and their antibody response should have increased in subsequent years.\n\n【14】Regression analysis showed that older persons with high HI titers were more likely than younger adults to have higher MN titers. Because the HI assay detects only antibodies against hemagglutinin, the MN assay provides more information about the level of protective antibodies against influenza viruses. As a person ages, production of antibodies against hemagglutinin or other components of influenza virus should protect against potential infections. Although a titer of 40 by HI is accepted as a cutoff value for seroprotection, a consensus for protective titers by MN is lacking . We suggest that MN is probably more sensitive than HI for evaluating neutralizing antibodies against pandemic (H1N1) 2009 virus.\n\n【15】Dr D.T.-N. Huang is a pediatric infectious diseases specialist at Mackay Memorial Hospital and National Taiwan University Hospital, Taipei. His research interests are influenza viruses, viral vaccines, and pediatric infectious diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e24f4aac-7230-4219-93d7-ea7953064452", "title": "Extrapulmonary Tuberculosis among Somalis in Minnesota", "text": "【0】Extrapulmonary Tuberculosis among Somalis in Minnesota\nAlthough the incidence of tuberculosis in the United States has declined each year since 1993, tuberculosis remains an important infectious disease in the United States and worldwide. In Minnesota, the incidence of tuberculosis increased during the 1990s and peaked at 4.9 cases per 100,000 population in 2001. From 2001 through 2005, 81% of tuberculosis cases in Minnesota occurred in foreign-born persons; this finding can largely be attributed to dynamic immigration patterns that have included an influx of persons from areas of the world where tuberculosis is endemic .\n\n【1】Somalia ranks in the top 15 countries of origin for foreign-born persons with cases of tuberculosis in reported in the United States . Minnesota has the largest Somali population in the United States . Although Somali persons constitute <1% of Minnesota's population, they accounted for 30% of tuberculosis cases reported statewide from 1999 through 2003. The unique epidemiologic characteristics of foreign-born tuberculosis patients in Minnesota, and Somali tuberculosis patients in particular, have been described .\n\n【2】The emergence of extrapulmonary disease as an important form of active tuberculosis has been noted in many studies . Our purpose was to describe the characteristics of extrapulmonary tuberculosis in ethnic Somalis in Minnesota and to assess factors that may contribute to its disproportionately high prevalence in this population.\n\n【3】### The Study\n\n【4】Data were obtained from the Minnesota Department of Health's tuberculosis database, specifically, surveillance and public health case management data on all cases of tuberculosis reported among ethnic Somalis in Minnesota from January 1, 1993, through December 31, 2003. Cases were defined in accordance with the Centers for Disease Control and Prevention's surveillance case definition for tuberculosis .\n\n【5】Of the 407 cases of tuberculosis in ethnic Somalis reported to the Minnesota Department of Health during this 10-year period, 239 (59%) had extrapulmonary involvement, including 198 (49%) with exclusively extrapulmonary disease and 41 (10%) with pulmonary and extrapulmonary tuberculosis. The remaining 168 (41%) patients had pulmonary disease only.\n\n【6】In 2003, 214 cases of tuberculosis (4.4 cases per 100,000 population) were reported in Minnesota; 173 (81%) of these patients were foreign-born, and 58 (27%) were from Somalia. Of the 58 Somali patients, 45 (78%) had extrapulmonary disease. According to US Census data, an estimated 11,164 ethnic Somalis were residing in Minnesota in 2000 ; by June 2004, this population had increased to ≈25,000 . Based on these numbers, the approximate annual incidence rate of tuberculosis for Somalis in Minnesota in 2003 was 269 cases per 100,000 population, and the approximate rate of extrapulmonary tuberculosis was 209 cases per 100,000 population.\n\n【7】Demographic and clinical characteristics of the 239 Somali patients who had extrapulmonary tuberculosis are summarized in the Table. A total of 179 (75%) patients were tested for HIV within 1 year of the diagnosis of tuberculosis; HIV test results were positive for only 2 (1%).\n\n【8】The only characteristics that differed significantly between patients with extrapulmonary tuberculosis and pulmonary tuberculosis were age and length of time in the United States before diagnosis. Patients who had extrapulmonary tuberculosis were generally older (mean 26.8 years) than those with pulmonary tuberculosis (mean 23.7 years) (p = 0.01). Similarly, the length of time between arrival in the United States and diagnosis of tuberculosis was generally longer for patients with extrapulmonary tuberculosis (mean 2.7 years) than for those with pulmonary disease (mean 1.3 years) (p<0.00001). In a logistic regression model that controlled for the confounding effects of length of time in the United States on the association between a patient's age and risk for extrapulmonary disease, only the patient's duration of residence in the United States was significantly associated with extrapulmonary tuberculosis (p<0.00001). Controlling for age, each additional year of residence in the United States was associated with a 30% increase in the risk for extrapulmonary tuberculosis (odds ratio 1.3, 95% confidence interval 1.2–1.5).\n\n【9】A total of 250 sites of extrapulmonary disease were identified in the 239 patients, representing 26 distinct anatomic locations of disease; several patients had disease in multiple sites: lymph nodes (50%), pleura (9%), peritoneum (8%), skin or soft tissue (8%), central nervous system (6%), bones or joints (4%), various organs (miliary) (4%), urogenital tract (2%), and other sites (9%).\n\n【10】Among the 197 (82%) cases of culture-confirmed extrapulmonary tuberculosis, 18% were resistant to >1 first-line antituberculosis drug (i.e. isoniazid, rifampin, pyrazinamide, or ethambutol). The prevalence of resistance to specific first-line drugs ranged from 16% for isoniazid to 3% for rifampin and 2% each for pyrazinamide and ethambutol. The prevalence of multidrug-resistant tuberculosis (MDRTB) (i.e. resistant to at least isoniazid and rifampin) was 3%. Resistance to rifampin occurred exclusively with MDRTB .\n\n【11】Of the 186 patients who had extrapulmonary tuberculosis and were treated from 1993 through 2002, 91% successfully completed an adequate course of therapy, 76% within 12 months. Of the 186 treated patients, 55% received strict directly observed therapy, and 25% others received a less intensive form of supervised therapy. Response to treatment did not differ significantly between patients who received directly observed therapy or some other form of supervision and those who administered therapy themselves. One patient died, a 9-year-old girl with MDRTB in multiple sites.\n\n【12】### Conclusions\n\n【13】Extrapulmonary tuberculosis is more common than pulmonary tuberculosis in Somalis in Minnesota. Among Somali patients who have extrapulmonary tuberculosis, 50% have lymphatic disease, most are <45 years of age, and slightly more are female; prevalence of HIV infection is low, prevalence of reactive tuberculin skin tests is high, and prevalence of drug-resistant strains is substantial. These findings are similar to those reported for Somali immigrants in other countries . The prevalence of extrapulmonary tuberculosis among all foreign-born tuberculosis patients in the United States is considerably lower than that reported among Somalis in Minnesota and elsewhere , which suggests that the unique characteristics of tuberculosis in this population may reflect host factors or differences in geographically endemic strains of _M. tuberculosis_ .\n\n【14】In Minnesota, Somali patients who had extrapulmonary tuberculosis were older and had resided in the United States longer than Somali patients who had pulmonary tuberculosis. These differences likely reflect the relative difficulty in diagnosing extrapulmonary disease compared with pulmonary tuberculosis. To minimize the interval between clinical manifestation of disease and diagnosis, clinicians should maintain an increased level of suspicion for extrapulmonary tuberculosis in Somali patients.\n\n【15】Immigration is a major factor in sustaining tuberculosis disease in the United States. This study demonstrates how immigration can affect the local epidemiology of tuberculosis through importation of disease patterns from another part of the world.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6e02c2e7-9124-4d36-8bf0-de308b48e817", "title": "Etymologia: Quinine", "text": "【0】Etymologia: Quinine\n### Quinine \\[kwinʹin\\]\n\n【1】From the Quechua _kina_ , “bark,” quinine is an alkaloid of cinchona that has antimalarial properties. In the 1620s, Jesuit missionaries living in Peru learned of the healing powers of the bark of “fever trees” that grew in the high forests of Peru and Bolivia.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ebcf575f-c623-4468-9238-37db775fecec", "title": "Pandemic Influenza and Pregnant Women", "text": "【0】Pandemic Influenza and Pregnant Women\nInfluenza pandemics occur when a new influenza type A virus to which the population has no immunity emerges, spreads efficiently between humans, and results in worldwide outbreaks of severe disease. Pandemics occur infrequently but can be devastating in terms of the effects on illness and mortality. Most influenza experts consider influenza pandemics inevitable . The emergence of avian influenza A virus (H5N1) as a cause of severe human infections has increased concerns about an impending pandemic. Although human disease caused by influenza (H5N1) is a rare event , the virus has become endemic among bird populations in some areas of Asia and has continued to spread geographically and to broaden its host range. Concerns that the virus might acquire the ability to efficiently spread between humans have led public health authorities to accelerate preparations for pandemic influenza.\n\n【1】A key component of pandemic preparedness  involves addressing the specific needs of vulnerable populations, including pregnant women. Pregnant women are at high risk for severe complications of influenza during interpandemic periods  and previous pandemics . In addition, some studies suggest an increased risk for adverse outcomes among infants born to mothers infected with influenza during pregnancy . Special considerations for pregnant women should be addressed in all 3 categories of public health response to pandemic influenza—nonpharmaceutical interventions, antiviral medications, and vaccines. Many articles have discussed issues regarding pandemic influenza in the general population, but limited attention has been given to the effects on the pregnant woman and her fetus. This article focuses on issues regarding pregnant women that should be considered by public health and medical professionals as they prepare for a future influenza pandemic.\n\n【2】### Pandemic versus Seasonal Influenza\n\n【3】Influenza viruses that infect humans are classified into 3 principal types (A, B, and C), of which types A and B are important causes of human disease. Types A and B are associated with seasonal epidemics; only type A viruses have caused pandemics. Influenza A viruses are further classified on the basis of 2 surface proteins, hemagglutinin (H) and neuraminidase (N). Minor mutations that result in subtle changes in these proteins (antigenic drift) occur continuously. Because these mutations produce viruses that can be sufficiently different antigenically from previous influenza viruses, influenza vaccines must be updated annually. More dramatic changes in the surface proteins of influenza viruses, through mutation of nonhuman (e.g. avian or swine) viruses or reassortment of human and nonhuman viruses, result in the creation of novel human subtypes (termed antigenic shift). When novel subtypes that can be efficiently transmitted among humans emerge within a population that lacks immunity, an influenza pandemic can occur .\n\n【4】Avian species are an important reservoir for influenza virus, but avian influenza viruses do not typically infect humans. However, in 1997, human exposure to ill birds infected with the avian influenza A (H5N1) led to a severe outbreak in Hong Kong Special Administrative Region, People’s Republic of China.The H5N1 virus subtype reemerged in early 2003, and since that time, the virus has caused poultry and wild bird illnesses in >50 countries. In addition, 321 confirmed human cases of influenza (H5N1) in 12 countries have been reported to the World Health Organization, 194 of which have resulted in death . Thus far, influenza (H5N1) transmission has been predominantly bird-to-human, but the ongoing avian disease and occasional human disease have raised concerns about the possibility that this virus could gain the capacity for efficient human-to-human transmission and possibly lead to an influenza pandemic . Although influenza (H5N1) represents the greatest current threat for a pandemic virus, global public health authorities recommend increased vigilance for any novel influenza virus infections in humans as a cornerstone of pandemic preparedness.\n\n【5】### Effects of Influenza on Pregnant Women\n\n【6】Pregnancy has been a risk factor for increased illness and death for both pandemic and seasonal influenza. The increased risk is believed to be related to several physiologic changes that occur during pregnancy. Because of mechanical and hormonal alterations that occur during pregnancy, several changes also occur to the cardiovascular and respiratory systems, including increased heart rate, stroke volume, oxygen consumption, and decreased lung capacity . Relevant immunologic alterations also occur during pregnancy, with a shift away from cell-mediated immunity toward humoral immunity. This shift can render pregnant women more susceptible to, or more severely affected by, certain viral pathogens, including influenza .\n\n【7】Although appropriate nonpregnant control groups were generally not available, mortality rates among pregnant women in the pandemics of 1918 and 1957 appeared to be abnormally high . Among 1,350 reported cases of influenza among pregnant women during the pandemic of 1918, the proportion of deaths was reported to be 27% . Similarly, among a small case series of 86 pregnant women hospitalized in Chicago for influenza in 1918, 45% died . Among pregnancy-associated deaths in Minnesota during the 1957 pandemic, influenza was the leading cause of death, accounting for nearly 20% of deaths associated with pregnancy during the pandemic period; half of women of reproductive age who died were pregnant .\n\n【8】Pregnant women have also been shown to be at increased risk for influenza complications during interpandemic periods . In a large study of >4,300 women of reproductive age during 19 interpandemic influenza seasons, pregnant women were compared with postpartum women (a group considered to be most similar to pregnant women demographically and with regard to their health) and were found to be significantly more likely to be hospitalized for a cardiopulmonary event during the influenza season . The risk for hospitalization increased as pregnancy progressed, with women at term nearly 5 times more likely to be hospitalized than postpartum women . Similarly, during 3 influenza seasons in the late 1970s, rates of medical visits for acute respiratory disease were more than twice as high among pregnant women than nonpregnant women . At particularly high risk during the influenza season are pregnant women with underlying medical conditions for which influenza vaccination is recommended, such as asthma . On the basis of these data, pregnant women should be considered a population for which special considerations for prevention and treatment for influenza need to be made.\n\n【9】### Effects of Influenza on the Fetus\n\n【10】Although certain infections are well recognized to increase the risk for adverse pregnancy outcomes, the effects of maternal influenza infection on the fetus are not well understood. Viremia is believed to occur infrequently in influenza , and placental transmission of the virus also appears to be rare . However, even in the absence of fetal viral infection, animal studies suggest that adverse effects can still occur. Prenatal influenza infection in the mouse has been associated with histopathologic changes in the brain  and behavioral alterations  in offspring. Although influenza virus RNA has not been detected in the fetal brain, these changes suggest that fetal effects could be secondary to the maternal inflammatory response, rather than the result of a direct viral effect .\n\n【11】Adverse pregnancy outcomes have been reported following previous influenza pandemics. During the influenza pandemic of 1918, remarkably high rates of spontaneous abortion and preterm birth were reported , especially among women with pneumonia (for example, in 1 study, >50% of pregnancies in which the pregnant woman had influenza and accompanying pneumonia were not carried successfully to term) . During the Asian influenza pandemic of 1957, studies suggested a possible increase in defects of the central nervous system  and several other adverse outcomes, including birth defects, spontaneous pregnancy loss, fetal death, and preterm delivery . Studies of the effects of seasonal influenza infection on the fetus have been contradictory. A small increased risk for birth defects in general and for specific birth defects have been observed in some but not all studies . Using data from a recent case-control study, investigators showed that mothers of infants with any type of birth defect were slightly more likely to report influenza during early pregnancy than mothers of control infants (adjusted odds ratio 1.4; 95% confidence intervals 1.3–1.6), with statistically significant associations for cleft lip with or without cleft palate, and neural tube and congenital heart defects. Verification of maternal report of influenza illness with prospectively collected clinical data was possible for similar numbers of case and control infants , which suggests that recall bias was unlikely to explain the association. The risk associated with influenza was reduced for women who received treatment with antifever medications and for those who had taken folic acid before and during early pregnancy .\n\n【12】Associations between maternal influenza infection after both pandemic and seasonal influenza and outcomes observed long after birth have been reported. Associations between maternal influenza infection and childhood leukemia , schizophrenia , and Parkinson disease  have been suggested by some studies. Even if the influenza virus does not have a direct effect on the fetus, fever that often accompanies influenza infection could have adverse effects. Both animal and human epidemiologic studies suggest that hyperthermia is associated with an increased risk for adverse outcomes , especially neural tube defects . Factors that might attenuate this risk include shorter fever duration , use of fever-reducing medications , and use of folic acid–containing supplements .\n\n【13】More study is needed to better understand the fetal risks of maternal influenza infection. However, data from previous pandemics, although limited, suggest that pregnancy loss and preterm delivery could be important issues during a future influenza pandemic. Information on seasonal influenza indicates that influenza infection or its accompanying hyperthermia might also increase the risk for certain birth defects. Data on these potential risks to the fetus, combined with available information on risks for influenza infection on maternal health, provide ample support for considering pregnant women a high-risk population in an influenza pandemic.\n\n【14】### Pandemic Influenza Response for Pregnant Women\n\n【15】##### Nonpharmaceutical Interventions\n\n【16】A main component of the public health response to pandemic influenza will be nonpharmaceutical interventions to mitigate disease rates and severity and the societal impact of the pandemic beyond health outcomes. The Centers for Disease Control and Prevention recently released guidance for nonpharmaceutical interventions during a pandemic that focuses on isolation of ill persons, voluntary quarantine of households with ill persons, and social distancing techniques (e.g. avoiding crowded settings, closing schools and child care centers) to limit exposure to ill persons . However, these recommendations present special challenges for pregnant women. For example, pregnant women will need guidance on how to protect themselves from becoming infected (e.g. use of protective devices) if they are quarantined with or directly providing care for ill persons. Responsibilities of pregnant women as members of the workforce and as caregivers of their children and other family members may further complicate their adherence to public health recommendations. In addition, because healthy pregnant women will continue to require both outpatient prenatal care and inpatient delivery services during a pandemic, they might be more likely to be exposed to clinical settings where ill persons are receiving care. Given the potential risk to women in clinical settings, guidance will need to be developed regarding whether some routine prenatal care visits could be omitted. Healthcare facilities need to develop plans to ensure that pregnant women receive necessary care, but with minimal exposure to ill persons or their contacts. In addition, plans for care and delivery of pregnant women with confirmed influenza or recent exposure must ensure that these women receive appropriate care without unduly exposing other healthy pregnant women and their infants to illness. An appropriate strategy to address these issues might include designating a location and staff for care of pregnant women and their newborns, separate from those used by patients with influenza. Another strategy could include developing an algorithm that triages pregnant patients on the basis of pregnancy stage and symptoms to ensure that pregnant women most in need of attention receive care, but avoid the risk of influenza exposure when that risk might be greater than the benefit of care. Consideration of these strategies as part of overall community pandemic planning activities will be essential.\n\n【17】Experience from the international outbreak of severe acute respiratory syndrome (SARS)  can shed light on how to approach these complicated issues. In Toronto, obstetric services were moved into a newly designed facility that had entrances, elevators, and air-handling systems that were separate from the rest of the hospital. Hospital staff, patients, and visitors were screened at the hospital entrance for SARS symptoms and to ensure that they had not visited a SARS-affected area. Staff members wore N95 respirator masks, face shields or eye protection, gowns, and non-latex gloves, and employed frequent hand washing with ethanol-based gels. Patients were limited to 1 visitor during labor and delivery, and no visitors were allowed on postpartum wards. All patients and visitors wore N95 respiratory masks. The length of postpartum stay was decreased and, after discharge, women were instructed to stay at home under quarantine for 10 days; a nurse visited them on their third day postpartum. Healthcare workers were asked to observe work quarantine; they were encouraged to go directly to work and home minimizing contact with the public . In Hong Kong, obstetric services were transferred to a hospital separate from hospitals in which SARS cases were managed. Women were discharged sooner after delivery, and all obstetric services considered nonessential (e.g. routine ultrasonography and prenatal diagnosis) were temporarily suspended .\n\n【18】##### Antiviral Medication Use by Pregnant Women\n\n【19】During a pandemic, 2 pharmaceutical options—antiviral medications and vaccination—will be available to reduce the expected illness and death. Given that a vaccine is unlikely to be available for a substantial portion of the population at the beginning of a pandemic, antiviral medications are expected to play an important role in the response to pandemic influenza, both for postexposure prophylaxis and for influenza treatment. Two antiviral medications are currently recommended for treatment and prophylaxis of influenza in humans . These medications, both neuraminidase inhibitors, are available in oral (oseltamivir \\[Tamiflu\\]) and inhaled (zanamivir \\[Relenza\\]) forms, and make up the bulk of stockpiled anti-influenza medications. Two additional anti-influenza medications, the M2 ion channel blockers rimantidine and amantadine, are currently not recommended for use because of high rates of resistance among circulating human influenza A viruses and some avian influenza viruses.\n\n【20】As is the case with >90% of medications introduced in recent years , insufficient information on oseltamivir and zanamivir is available to assess potential risks to the fetus. This is reflected by their category C use-in-pregnancy rating from the US Food and Drug Administration (i.e. insufficient information available to assess their potential risk and benefit during pregnancy) . Animal studies have shown no evidence of increased risk for adverse effects for either medication , but animal data do not always predict the effects on human pregnancies . Human data are very limited. Among 61 pregnant women exposed to oseltamivir in the post-marketing period, most pregnancies had a normal outcome . Single cases of trisomy 21 and anencephaly were reported among these exposed pregnancies, but these cases were not believed to be causally related to oseltamivir exposure. Three pregnancies were inadvertently exposed to zanamivir during the clinical trials, with one ending in spontaneous abortion, one in elective termination, and one in an outcome with no apparent adverse effects . The bioavailability (the proportion of active drug that reaches the systemic circulation) of zanamivir is lower (12%–17%) than that of oseltamivir (≈80%) , leading some to suggest that it might be preferred during pregnancy .\n\n【21】Many pregnant women will require treatment with other medications, such as antibiotics for secondary bacterial pneumonia and antipyretic medications for fever control. Healthcare providers need access to information on these medications and their safe use during pregnancy so that effects on the fetus can be taken into account.\n\n【22】Another issue to consider is that even in the case of serious exposure or illness, pregnant women might fail to comply with recommendations for use of antiviral medications because of concern for the health of the fetus. This emphasizes the importance of communication of the risks and benefits of medications and the serious nature of untreated influenza. As with all communications related to pandemic influenza, messages must be culturally and linguistically appropriate and conducted at an appropriate level of literacy to ensure such efforts are inclusive, given the diversity of the population of pregnant women.\n\n【23】Further research to understand the effects of anti-influenza medications on the pregnant woman and her fetus is essential to guide treatment recommendations during a future pandemic. Although exposures to these medications are likely to be rare in the pre-pandemic period, collection of data on these exposures in a pregnancy registry, as has been used to collect data on other rare exposures , could provide important data to guide pandemic recommendations. In the absence of additional information, healthcare providers will need to consider the type of exposure, risk for serious illness or death, and trimester of pregnancy when weighing the risks and benefits of these medications to the woman and her fetus. Guidelines for the pandemic scenario, similar to those developed for management of sporadic avian influenza (H5N1) infection , could assist healthcare providers in weighing these risks and benefits.\n\n【24】##### Use of Pandemic Influenza Vaccine among Pregnant Women\n\n【25】Once available, a vaccine will be a vital component of the public health response to pandemic influenza. Given their increased risk for illness and death during pandemic influenza, pregnant women should be considered a high priority for receipt of influenza vaccine. Several studies have demonstrated no adverse fetal effects when women received inactivated vaccine during pregnancy . Both the Advisory Committee on Immunization Practices and the American College of Obstetricians and Gynecologists recommend annual vaccination with trivalent inactivated influenza vaccine for women who will be pregnant during the influenza season (October–mid May) to prevent seasonal influenza . (Live, attenuated, influenza virus vaccine, available as an intranasal spray, is not approved for use during pregnancy, given the theoretical risk associated with use of live vaccine during pregnancy.) Inactivated influenza vaccine is recommended in all 3 trimesters of pregnancy . Despite these recommendations, compliance has been low , probably because of concerns among women and their healthcare providers regarding the safety of vaccination during pregnancy. Development of culturally and linguistically appropriate messages will be necessary to ensure that pregnant women receive information regarding care required for their health and that of their fetus in the event of a future pandemic. Professionals who develop these messages need to be aware that some women will have limited access to healthcare services. Innovative strategies will be needed to ensure that these messages reach them.\n\n【26】##### Incorporating Issues of Pregnant Women into Preparedness Exercises\n\n【27】Pandemic influenza planning that specifically addresses the concerns of pregnant women is critical because special issues need to be considered for this high-risk group. Pandemic influenza preparedness exercises should include scenarios in which issues specific to pregnant women require attention. In the event of an influenza pandemic, identification and close monitoring of pregnant women will be important, given their increased risk for influenza-associated illness and death. Intake procedures for women seeking prophylaxis and treatment for pandemic influenza need to incorporate questions about the possibility of pregnancy. By including scenarios involving pregnant women in pandemic influenza preparedness exercises, public health professionals will have the opportunity to weigh the risks and benefits of anti-influenza medications in the context of a specific pandemic scenario. Their inclusion in preparedness exercises will help to identify gaps in our current capacity to provide optimal care for this high-risk population.\n\n【28】### Conclusions\n\n【29】Because of their risk for severe disease and death and the potential for risk for the fetus, pregnant women should be considered to be high-risk in the event of an influenza pandemic. Research into the effects of maternal influenza and its treatment on the pregnant woman and her fetus is sorely needed. Based on the limited information available, pregnant women who become ill with influenza should be treated aggressively with antifever therapy, and should adhere to standard recommendations for folic acid consumption . Given the limited data currently available, plans for prophylaxis and treatment for pandemic influenza will need to include reassessment of risks of influenza and risks and benefits of treatment strategies as a pandemic evolves. In addition to incorporating considerations specific to pregnant women into pandemic influenza planning efforts, strategies to communicate this guidance to pregnant women and their healthcare providers must be planned, developed, and tested. Only through consideration of all these issues, from research and planning to communications and intervention, will the health and well-being of pregnant women be ensured in a future influenza pandemic.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bf236492-0df0-4f01-85ff-2d1234995eee", "title": "Rio Mamore Virus and Hantavirus Pulmonary Syndrome, Brazil", "text": "【0】Rio Mamore Virus and Hantavirus Pulmonary Syndrome, Brazil\n**To the Editor** : Hantavirus pulmonary syndrome (HPS) is an acute, severe, frequently fatal disease associated with cardiopulmonary failure; it is caused by hantaviruses naturally hosted by wild rodents. Rio Mamore virus (RIOMV) was first described in 1996 in Bolivia; it was associated with the small-eared pygmy rice rat, _Oligoryzomys microtis_ . Subsequently, 1 strain of RIOMV was isolated from _O. microtis_ rats in Peru, designated HTN-007 ; and 2 strains were recovered in the Brazilian Amazon from _O. microtis_ rats (RIOMV-3) and uncharacterized species of rodents of the genus _Oligoryzomys_ (RIOMV-4) . Recently, HPS cases associated with RIOMV have been reported: 2 cases in Peru  and 1 case in French Guiana (caused by a variant named Maripa virus) . We report isolation of a strain of RIOMV from a patient with fatal HPS in Brazil.\n\n【1】In June 2011, a 28-year-old man was admitted to the Tropical Medicine Foundation Dr. Heitor Vieira Dourado, Amazonas State, with a 4-day febrile illness that included nonproductive cough, myalgia, and headache. Laboratory testing revealed hematocrit within reference range (43.9%), thrombocytopenia (27,000 cells/mm 3  ), elevated levels of liver enzymes (alanine transaminase 347 IU/L, aspartate transaminase 139 IU/L), creatinine (1.2 mg/dL), and urea (40 mg/dL). Laboratory testing ruled out malaria, leptospirosis, and dengue. About 24 hours after hospitalization, the patient experienced hypotension, progressive dyspnea, and acute respiratory distress. Thoracic radiographs revealed bilateral diffuse alveolar pulmonary infiltrates. Despite empirical treatment with antimicrobial drugs, mechanical ventilation, and inotropic therapy, the patient’s clinical condition deteriorated and he died on day 6 after illness onset.\n\n【2】The patient, who had no history of travel, resided on a submerged region in the western floodplain of the Solimões-Amazon River, Amazonas, a state with low population density (6.2 persons/square mile), in a rural area of Careiro da Várzea Municipality (3°11′53′′S, 59°52′18′′W), where access is possible only by boat. He had a history of contact with rodents not only at home but also in the boat he used. A serum sample collected on day 6 after illness onset was evaluated for hantavirus by serologic and PCR testing. ELISA result was positive for IgM and IgG against recombinant nucleocapsid protein (N) of the Juquitiba virus . Viral genome was detected by reverse transcription PCR, and the complete genomic small segment sequence, designated LH60\\_11/Hu , was determined . This sequence was compared with a reference panel of sequences that covered the diversity of most hantaviruses in South America and was subjected to phylogenetic analysis by MrBayes software version 3.1.2 . Nucleotide and amino acid sequence similarities between all taxa for the partial N gene were calculated by using MegAlign version 5.05 (DNASTAR, Inc. Madison, WI, USA). The best-fit evolutionary model general time reversible + Γ + proportion invariant was determined by using MEGA version 5.2.2  of the dataset compiled only 905 nt of the N gene to include sequences of Anajatuba and Rio Mearim viruses from Brazil for comparison.\n\n【3】Bayesian analysis indicated that strain LH60\\_11/Hu is closely associated with rodent-derived RIOMV-3/Olm strain (Itacoatiara, Amazonas State) and in a sister relationship with RIOMV-4/Olsp strain (Alto Paraíso, Rondônia State) from Brazil . Analysis of the partial sequence revealed 86.6%–95.4% of genetic identity with the strains recovered from rodents and 83.4% with the Maripa virus strain from humans. The sequence from the human patient in Peru was not available for comparison.\n\n【4】In July, the patient’s house and environment were investigated; accumulation of garbage and other waste in homes that were still flooded was observed. We obtained and tested serum samples from 15 healthy residents (10 female, 5 male) with a recent history of acute fever; IgG against hantavirus was detected in samples from 3 women (17, 25, and 57 years of age).\n\n【5】This case report describes RIOMV as a highly pathogenic agent of HPS in Brazil. The location of the patient with this fulminant case of HPS, Careiro da Várzea, borders the Municipality of Itacoatiara, where RIOM-3–infected _O. microtis_ rats and the first HPS case in Amazonas State, with no etiologic identification so far, have been reported . Careiro da Várzea is part of an area in which grain production is expanding, an activity that attracts rodents to human dwellings, especially those in lowland regions that are constantly flooded.\n\n【6】The close association between the sequences from the human and the _O. microtis_ rat (>98% aa identity) suggests that the patient might have been infected as a consequence of close physical contact with an RIOMV-infected _O. microtis_ rat. The geographic distribution of these rats and, thus, the potential area at risk for transmission of RIOMV is vast, including 5 Brazilian states in the Amazon Basin and contiguous lowlands of Peru, Bolivia, and Paraguay .\n\n【7】This study confirms the notion that ROIMV is a highly pathogenic hantavirus. Recent recognition of RIOMV as a causative agent of HPS might be attributed to either increased awareness by local physicians or improved diagnosis of hantavirus infections. This finding emphasizes the need for extensive molecular investigation of undiagnosed infections because of the shared clinical features with other diseases endemic to this region (e.g. malaria and dengue).", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ce1cbf21-4daf-4328-b421-763e55716e52", "title": "The Impact of New York City’s Health Bucks Program on Electronic Benefit Transfer Spending at Farmers Markets, 2006–2009", "text": "【0】The Impact of New York City’s Health Bucks Program on Electronic Benefit Transfer Spending at Farmers Markets, 2006–2009\nAbstract\n\n【1】**Introduction**  \nIncreasing the accessibility and affordability of fresh produce is an important strategy for municipalities combatting obesity and related health conditions. Farmers markets offer a promising venue for intervention in urban settings, and in recent years, an increasing number of programs have provided financial incentives to Supplemental Nutrition Assistance Program (SNAP) recipients. However, few studies have explored the impact of these programs on use of SNAP benefits at farmers markets.\n\n【2】**Methods**  \nNew York City’s Health Bucks Program provides SNAP recipients with a $2 coupon for every $5 spent using SNAP benefits at participating farmers markets. We analyzed approximately 4 years of electronic benefit transfer (EBT) sales data, from July 2006 through November 2009, to develop a preliminary assessment of the effect of the Health Bucks Program on EBT spending at participating markets.\n\n【3】**Results**  \nFarmers markets that offered Health Bucks coupons to SNAP recipients averaged higher daily EBT sales than markets without the incentive ($383.07, 95% confidence interval \\[CI\\], 333.1–433.1, vs $273.97, 95% CI, 243.4–304.5, _P_ < 0.001) following the introduction of a direct point-of-purchase incentive. Multivariate analysis indicated this difference remained after adjusting for the year the market was held and the neighborhood poverty level.\n\n【4】**Conclusion**  \nWhen a $2 financial incentive was distributed with EBT, use of SNAP benefits increased at participating New York City farmers markets. We encourage other urban jurisdictions to consider adapting the Health Bucks Program to encourage low-income shoppers to purchase fresh produce as one potential strategy in a comprehensive approach to increasing healthful food access and affordability in low-income neighborhoods.\n\n【5】Introduction\n\n【6】Increasing access to fresh fruits and vegetables in low-income neighborhoods and promoting consumption of these foods are important strategies for reducing the risk of heart disease, stroke, type 2 diabetes, and cancer . However, in low-income communities, limited availability and high prices present obstacles to the purchase and consumption of fresh produce . In New York City, 2009 data showed that 17.2% of residents in low-income neighborhoods reported eating no fruits and vegetables on the preceding day, compared with 8.0% of residents in high-income neighborhoods ( _P_ < .001) . Farmers markets, which are mobile and can be located throughout urban neighborhoods, offer a promising venue for intervention to decrease this disparity .\n\n【7】In 2005, the New York City Department of Health and Mental Hygiene (DOHMH) introduced Health Bucks, a coupon-distribution program providing financial incentives for low-income New Yorkers to shop at farmers markets in the city’s highest poverty areas. Two-dollar Health Bucks coupons were given to community-based organizations for distribution to residents for use at 11 participating markets during the annual growing season (July 1–November 15). In 2006, the DOHMH expanded Health Bucks to encourage recipients of Supplemental Nutrition Assistance Program (SNAP) benefits to use electronic benefit transfer (EBT) wireless terminals at farmers markets to purchase fruits and vegetables. EBT is the mechanism through which New York State delivers cash and SNAP benefits (formerly known as food stamps) to the state’s recipients. Funds are deposited into the accounts of individual recipients and made accessible to them via state-issued SNAP benefits cards . In 2006 when DOHMH expanded Health Bucks, it gave SNAP recipients at some markets a $2 coupon for every $5 in EBT credits spent. The objective of this study was to examine the program’s effect on mean EBT sales and to determine via a preliminary assessment whether Health Bucks increased EBT spending in a sample of NYC farmers markets.\n\n【8】Methods\n\n【9】Greenmarket , the largest outdoor urban farmers market network in the United States, manages the subset of the markets in New York City that participate in the Health Bucks program. Greenmarket has been on the forefront of national efforts to encourage SNAP spending at farmers markets and has installed EBT wireless terminals at many of the markets that are members of its network. As standard practice, Greenmarket records EBT sales at markets that accept SNAP benefits.\n\n【10】Greenmarket provided DOHMH with records of daily EBT sales for each individual market on each day of operation from July 2006 through November 2009. Data fields included the name and address of the market, the market date, and the total value of EBT sales in the market on that date. Altogether, Greenmarket provided EBT sales data for 1,289 market days (ie, each day of operation for each individual market) over the study period. Thirteen market days were excluded because their EBT sales were not attributed to a specific market, and an additional 15 days were excluded because records reflected EBT sales that took place at special events rather than ongoing farmers markets. Because Health Bucks are offered only during the growing season, defined as July 1 through November 15, days outside these dates were excluded (193 days). After these exclusions, our sample included EBT sales from 24 markets, operating for 45 days on average, for a total of 1,068 market days. Markets included in the final sample spanned all 5 boroughs of New York City, and 18 of the markets were located in high-poverty neighborhoods (≥20% of residents earned less than 200% of the federal poverty level \\[FPL\\]). Eleven of the 24 markets in the sample did not accept Health Bucks during the study period. Four markets had EBT sales data before and after they began accepting Health Bucks, and the remaining 9 markets had sales data available after the market began accepting Health Bucks coupons. The number of markets represented each year varied; our data included 8 markets in years 2006 and 2007, 15 in 2008, and 23 in 2009.\n\n【11】Independent _t_ tests were used to assess differences in mean daily EBT sales among farmers markets. First, for each year of the study period, we compared differences in mean daily EBT revenue among markets that participated in the Health Bucks program and those that did not. Next, we examined a subset of markets for which EBT data were available both before and after Health Bucks coupons were offered. These markets each had a different number of market days before and after the introduction of Health Bucks because of variation in market days from year to year. Also, for one market, there was only one year of data before Health Bucks coupons were introduced compared with 3 years of data after introduction. Because sales before and after Health Bucks coupons introduction at individual markets would be correlated with each other, we used a linear mixed model to account for this dependency while assessing whether adding Health Bucks coupons as an incentive increased average daily EBT sales at these markets. Finally, we used a linear regression model to examine the impact of Health Bucks coupons on mean EBT market revenue. We adjusted for market year, because annual increases in funding of Health Bucks would be expected to cause shifts in related EBT sales each year. We also adjusted for neighborhood poverty level, because EBT use would be expected to vary with neighborhood poverty levels. FPL thresholds were used to define the neighborhood poverty level; a high-poverty neighborhood was defined as one in which 20% of residents or more earned less than 200% of FPL, a determination consistent with DOHMH standard practice. For all significance testing, the α level was set at _P_ < .05. We used SPSS for Windows version 18.0 (SPSS Inc, Chicago, Illinois) to conduct all analyses.\n\n【12】Results\n\n【13】The number of farmers markets accepting SNAP benefits via EBT terminals increased from 8 in 2006 to 23 in 2009. Average daily per-market EBT sales among all markets accepting SNAP benefits rose from $114.55 in 2006 to $465.87 in 2009. Among these markets, participation in the Health Bucks Program also expanded over time. In 2006, 5 Greenmarket markets participated in Health Bucks; this total grew to 12 participating Greenmarket markets in 2009 .\n\n【14】During the first 2 years of the study period, markets that participated in the Health Bucks Program had lower average daily EBT sales than nonparticipating markets . In 2006, average daily EBT sales in participating markets were $94.64 (95% confidence interval \\[CI\\], 71.0–118.3), compared with $161.00 (95% CI, 119.2–202.8) among nonparticipating markets ( _P_ \\= .004); in 2007, participating markets averaged $109.15 (95% CI, 81.8–136.5) in daily EBT sales, compared with $381.28 (95% CI 316.4–446.2) among nonparticipating markets ( _P_ < .001). However, this balance shifted in the later years of analysis. In 2008, when the SNAP incentive expanded and the majority of coupon distribution shifted to EBT, participating markets documented $397.17 (95% CI, 310.5–483.8) in average daily EBT revenues, compared with $211.85 (95% CI, 161.0–262.7) at nonparticipating markets ( _P_ < .001); in 2009, participating markets showed nearly double the average daily EBT revenues of nonparticipating markets ($595.73, 95% CI, 495.4–696.1, vs $301.19, 95% CI, 253.9–348.4, _P_ < 0.001).\n\n【15】To examine potential confounders, our linear regression model adjusted for the income level of the neighborhood in which the markets were located and the year in which data were collected. We found that after controlling for these factors, markets participating in Health Bucks averaged $170.79 more in daily EBT sales than nonparticipating markets (95% CI, 102.4–239.1, _P_ < .001).\n\n【16】To account for variation in market size, types of vendors, and operational hours that may have affected our comparison of EBT sales data across markets, we examined a subset of farmers markets for which EBT data were available both before and after Health Bucks were offered. We compared sales data within each individual market before and after the Health Bucks EBT incentive was introduced and found significant increases in EBT sales after introduction of the Health Bucks incentive in Market A ($340.67, 95% CI, 270.1–411.3 vs $1,607.88, 95% CI, 1341.30–1874.50, _P_ < .001) and Market B ($18.00, 95% CI, 1.1–34.9 vs $66.53, 95% CI, 50.2–82.9, _P_ \\= .041). Figure 2 displays average daily EBT sales for each market before and after introduction of Health Bucks.\n\n【17】Discussion\n\n【18】In this preliminary study of the impact of Health Bucks on EBT spending at NYC farmer’s markets, we found that offering a direct financial incentive to SNAP recipients using EBT at urban farmers markets in 3 low-income New York City neighborhoods was associated with significant increases in EBT sales at those markets. Among markets accepting EBT in 2008 and 2009, the average daily EBT sales at participating markets were nearly double the sales of markets that did not offer the incentive. After controlling for neighborhood income level and the year in which data were collected, increases in sales figures remained significant. Additionally, we compared sales data from 4 markets for which EBT data were available before and after the Health Bucks EBT incentive was introduced and found that the introduction of Health Bucks resulted in significant increases in EBT revenue in 2 of the 4 markets.\n\n【19】These preliminary results suggest Health Bucks could be a useful program model for the delivery of financial incentives that encourage low-income shoppers to visit farmers markets in neighborhoods where the availability of affordable, high quality produce in the retail environment may be limited . Our findings merit further exploration, especially considering research that suggests the consumption of fruits and vegetables may be higher among low-income people and SNAP recipients who shop in farmers markets . Studies have also documented high levels of coupon use in farmers markets among low-income older adults  and among participants in the Special Supplemental Nutrition Program for Women, Infants, and Children (WIC) Farmers Market Nutrition Program . However, if more jurisdictions begin to provide these types of financial incentives, then developing successful strategies for distributing them will be essential. Health Bucks coupons offer a distribution model that should be further investigated for potential application in other settings seeking to encourage SNAP spending at farmers markets.\n\n【20】In 2008, several years after the launch of Health Bucks and midway through our study period, the New York City Human Resources Administration provided substantial financial support for the EBT incentive. This additional funding increased the number of coupons distributed from almost 20,000 in 2007 to more than 200,000 in 2008. Health Bucks was further expanded as part of the Centers for Disease Control and Prevention (CDC’s) Communities Putting Prevention to Work program , which funded grants to markets to hire market managers to operate the required EBT terminals. Health Bucks is now one of the largest farmers market financial programs in the United States, and our study’s findings provide evidence suggesting the effectiveness of encouraging EBT spending at farmers markets via coupons for fresh produce.\n\n【21】This preliminary study has limitations. First, although our findings suggest that SNAP beneficiaries spent more at markets after the Health Bucks incentive was implemented, we cannot be certain that those individuals were buying more fruits and vegetables because farmers markets sell foods and goods other than fresh produce. Second, we acknowledge that the provision of the Health Bucks incentives may have shifted SNAP spending on fruits and vegetables from other food outlets to farmers markets, rather than increasing SNAP spending on produce overall. Alternatively, SNAP recipients may have shifted their spending from farmers markets that did not offer the Health Bucks incentive to markets that did. However, a recent study examining the effects of a subsidy for fresh fruits and vegetables for postpartum women who are beneficiaries of WIC programs indicated that incentive programs did increase purchasing and consumption of fresh produce, particularly for WIC recipients who received coupons for produce at farmers markets . This finding leads us to suspect that Health Bucks may have had the same effect, indicating the importance of further research to explore this possibility. Third, Greenmarket’s data do not allow us to control for variations in size, vendor type, and operational hours in our analyses, and markets included in our analyses varied across years. These differences may have affected EBT sales; however, our internal comparison analysis of mean EBT sales in the same markets before and after the incentive aimed to address this limitation by comparing sales within the same market, where these factors would be relatively constant. Fourth, although the total number of days of sales data for this study was substantial , only 24 markets are represented in this study, and the sample sizes were limited, particularly for the analysis within the same markets. Further studies are needed to determine whether these are reliable effects. Finally, our data are limited to a nonrandom sample of participating Health Bucks markets that are members of the Greenmarket network and for which daily mean EBT sales data are available.\n\n【22】Future studies should address these limitations by exploring in greater detail how Health Bucks affects EBT spending in farmers markets citywide. One avenue for exploration would be to determine how Greenmarket’s data capture can be improved to allow for finer analyses of market characteristics and locations that may affect EBT sales. Second, studies should be designed to assess whether increased EBT spending at farmers markets is directly linked to the purchase of fresh produce at farmers markets in low-income neighborhoods. One way to accomplish this goal would be to track how SNAP recipients spend the financial incentives they receive through Health Bucks. Third, if purchases of fresh fruits and vegetables do increase, then we must also investigate whether increased spending on fresh produce leads to greater consumption of fruits and vegetables among this low-income population; we note that a link between the purchase of fresh produce and its consumption has not yet been reliably established. Finally, by providing SNAP recipients with $2 coupons for every $5 spent via EBT, Health Bucks increased SNAP recipients’ purchasing power by 40% (calculated as 40% of $5 = $2). However, programs across the country provide varying financial incentives, including many with a dollar-per-dollar match . Given the increasingly limited funding available for these types of programs, further research should examine the incentive level required to change SNAP purchasing patterns.\n\n【23】Because US health care costs for obesity and its associated health consequences reach $147 billion per year , municipalities are seeking to identify strategies that encourage healthier behaviors, particularly among populations at greatest risk. Improving the local food environment in low-income neighborhoods is a key avenue for exploration . We urge other urban jurisdictions to consider adapting the Health Bucks program where feasible to encourage low-income shoppers to purchase fresh produce as one potential strategy in a comprehensive approach to increasing access to and affordability of healthful food in low-income neighborhoods.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e68a682e-f246-45d4-acdb-b0ba02e0448e", "title": "Biology of Disease Vectors, 2nd ed.", "text": "【0】Biology of Disease Vectors, 2nd ed.\nThis edition is a massive, 7-section, 57-chapter medical entomology reference text. The chapters are written by 72 experts from around the world and provide an understanding of disease vectors on a broad front, including biologic requirements of vectors, epidemiology, molecular biology, genetics, principles of control, and insecticide resistance. The text consistently emphasizes molecular biologic approaches to these topics.\n\n【1】This book begins by discussing the vectors themselves, with chapters on mites, ticks, true bugs, lice, fleas, mosquitoes, and various dipterans such as tsetse flies and sand flies. Line drawings and black-and-white pictures abound. The number of color photos is limited; those in the kissing bug/bed bug chapter and the flea chapter are especially beautiful. Subsequent sections delve into the physiologic and genetic basis of vector biology. The final 2 sections concern controlling insects and acarines and special (laboratory) methods associated with vectors. The last section, which deals with laboratory methods, is like a giant appendix in which updates are given for the care, maintenance, and experimental infection of various disease vectors, including notes on handling, housing, rearing facilities, containment, and safety issues.\n\n【2】One of the most helpful chapters for this reviewer was the one entitled, \"Systematic Relationships among Disease Vectors,\" which defines molecular systematics terminology and explains how phylogenetic relationships among species are inferred from molecular data. I wish every traditional taxonomist and systematist would read this chapter.\n\n【3】This book contains a few misspellings (e.g. the chapter title in the Table of Contents, Chapter 46), but no major errors. Its only weakness seems to be one of disunity. The title doesn't match the book's content, and the text is so comprehensive that it seems unfocused. There are chapters on chemical and genetic control of vectors, cell culture, and even research safeguards for transgenic mosquitoes. How these fit under the title biology of vectors was difficult to discern. Perhaps in future editions, the chief editor could split the book into several separate volumes, each with a more appropriate title.\n\n【4】Nevertheless, this book is an indispensable reference and a wonderful treasure trove of information about medical entomology. Its only flaws are organizational, not factual. The chief editor, section editors, and authors are to be congratulated on this scholarly work.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a18685b-eafd-436c-b1f7-3ab7c456d83f", "title": "Hepatitis E Virus Infections in Blood Donors, France", "text": "【0】Hepatitis E Virus Infections in Blood Donors, France\nHepatitis E virus (HEV), family _Hepeviridae_ , genus _Hepevirus_ , is a small nonenveloped RNA virus that has an icosahedral capsid and is transmitted by the fecal–oral route . Transfusion-transmitted HEV infections have been documented in several countries in Europe and Asia . HEV has also been detected in human blood products . Despite considerable geographic variation in HEV seroprevalence in Europe, southern France seems to be an area to which this virus is hyperendemic .\n\n【1】We conducted a prospective study in France of minipools of plasma from blood donations that were processed with solvent–detergent and used standardized molecular assays for detection of HEV RNA. Data obtained were used to estimate the detection rate of HEV infections in blood donors in France and the risk for HEV transmission by blood transfusion.\n\n【2】### The Study\n\n【3】Since November 27, 2012, systematic nucleic acid amplification screening for HEV has been used on blood donations (pools of 96 samples) in France for a 70-L plasma pool that was processed with solvent–detergent. A mixture of 96 plasma samples (50 µL from each sample) was prepared by using a distributor (Tecan, Männedorf, Switzerland) to give a final volume of 4.8 mL. Simultaneously, a 96-well archive microplate was used for serologic testing and to identify individual HEV RNA–positive samples. Plasma samples were collected at 13/14 regional blood transfusion establishments of the French Blood Service in continental France .\n\n【4】HEV RNA was extracted by using the Nuclisens Easy MAG Instrument (bioMérieux, Marcy-l’Etoile, France). Amplification was performed by using the single-round RealStar Reverse Transcription PCR (Altona Diagnostics, Courtaboeuf, France), which is specific for the open reading frame 2 (ORF2)–ORF3 overlapping region ; the 95% detection limit was 23 IU/mL. HEV RNA quantification was performed as described ; the limit of quantification was 60 IU/mL. HEV genotyping was performed by sequencing a 305-nt fragment within the ORF2 region and performing phylogenetic analysis.\n\n【5】IgG and IgM against HEV were detected by using an enzyme immunoassay (Wantai Biologic Pharmacy Enterprise, Beijing, China) according to the manufacturer’s instructions . World Health Organization reference material for IgG against HEV was used to determine concentrations of IgG against HEV . The limit of detection was 0.25 World Health Organization U/mL.\n\n【6】We screened 558 pools (53,234 samples) for HEV RNA. Of these pools, 22 (3.94%) showed positive results . We detected 1 HEV RNA–positive sample in each of 20 pools and 2 HEV RNA–positive samples in the remaining 2 pools. The estimated detection rate of HEV RNA in plasma donations was 0.045% (95% CI 0.043%–0.047%).\n\n【7】The geographic distribution of HEV-positive samples included all but 2 blood transfusion establishments (in Centre Atlantique and Ile de France) . The frequency of HEV RNA–positive samples was 0.072% (95% CI 0.067%–0.077%) in 3 blood transfusion establishments in southern France and 0.037% (95% CI 0.035%–0.037%,) in establishments in northern France (odds ratio \\[OR\\] 1.98, 95% CI 0.86–4.53, p = 0.14) . The frequency of HEV RNA–positive samples was 0.049% (95% CI 0.047%–0.051%) in men and 0.018% (95% CI 0.014%–0.022%) in women (OR 2.7, 95% CI 0.4–111.1, p = 0.51). The frequency of HEV RNA–positive samples was 0.048% (95% CI 0.046%–0.052%) for persons ≥45 years of age and 0.041% (95% CI 0.040%–0.042%) for persons <45 years of age (OR 1.2, 95% CI 0.5–2.9, p = 0.83).\n\n【8】HEV RNA concentrations in 11 pools were below the limit of quantification, and the plasma pool with the highest virus load contained 29,796 IU/mL . The range of virus load in individual plasma samples was 468 IU/mL–5,155 800 IU/mL. All strains that were available for sequence analysis were assigned to HEV genotype 3 . A total of 59% of the strains were subgenotype 3f and 36% were subgenotype 3c .\n\n【9】Of the 183 pools tested for IgG against HEV, 175 had positive results and 8 eight had indeterminate results. Of these 175 IgG-positive pools, 173 were negative for IgM against HEV; the remaining 2 pools had indeterminate results. The range of IgG titers against HEV was 0.3 U/mL–10.6 U/mL.\n\n【10】All HEV RNA–positive pools were negative for IgM against HEV and positive for IgG against HEV, except for 1 pool that was negative for IgG against HEV . IgG titers against HEV in viremic pools ranged from 0.3 U/mL to 2.7 U/mL. The prevalence of IgG against HEV for 861 blood donors (whose samples were included in the first 9 HEV RNA–positive pools) was 23.6%. IgG-positive pools included 13.5%–30.2% of plasma collected from blood donors who were positive for IgG against HEV. Pools negative for IgG included 10% of IgG-positive donations. Most (22/24, 91.7%) viremic blood donors were negative for IgM and IgG against HEV at the time of their donation, which indicated recent infection. Two donors (D6 and D21) were positive for IgG and IgM against HEV .\n\n【11】### Conclusions\n\n【12】We conducted a prospective study with a sensitive method to screen plasma pools for HEV RNA and found that the frequency of HEV infections in donated blood in France was relatively high (1/2,218). This is 1 of the highest frequencies reported for Europe. Frequencies in Europe were 1/14,520 in Scotland , 1/7,040 in England , 1/4,525 and 1/1,240 in Germany  and 1/2,700 in the Netherlands .\n\n【13】Our study used plasma pools that contained 96 samples. Therefore, the true frequency of viremic samples could have been underestimated because of a dilution effect. The limit of detection of the PCR was low (<23 IU/mL). However, some pools could have been missed, although 5 positive individual donations having HEV RNA concentrations of 468 IU/mL–2,293 IU/mL were detected .\n\n【14】The detection rate for viremic blood donations in southern France was 2-fold greater than that for the rest of France. This finding is consistent with a previously reported high seroprevalence (52%) of IgG against HEV in blood donors  and the high incidence (3.2%) of infection documented by molecular techniques in local transplantation patients . However, HEV-positive blood donations were detected in all but 2 of the regions of continental France tested. All strains were genotype 3 and the proportions of subgenotypes 3f and 3c strains was similar to those observed in HEV-infected pigs in France, which suggested a zoonotic origin of the 24 HEV infections as reported by Kamar et al.\n\n【15】Infections were detected at an early stage because serologic markers were absent for >90% of the cases. Since November 2012, testing for HEV RNA has reduced the risk for HEV transmission in patients in France who are given plasma processed with solvent–detergent. This testing helps overcome the situation that this treatment does not inactivate HEV, a nonenveloped virus. In January 2015, nucleic acid amplification testing for HEV will begin in Europe for plasma processed with solvent–detergent.\n\n【16】We detected IgG against HEV in nearly all plasma pools, and IgG concentrations ranged from 0.3 U/mL to 10.6 U/mL. The potential of antibodies against HEV to neutralize virus infectivity if the unit is transfused is unknown, but a recent study indicated that IgG against HEV at a concentration of <10 U/mL cannot protect immunocompromised patients against reinfection .\n\n【17】Our findings should be useful for determining the best safety measures to prevent HEV transmission by blood transfusion. The implementation of nucleic acid amplification testing relies on full assessment of transfusion risk in exposed patients and the cost-effectiveness of different strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7d60db90-cea3-42c1-8b15-38c0783430db", "title": "Novel Bluetongue Virus in Goats, Corsica, France, 2014", "text": "【0】Novel Bluetongue Virus in Goats, Corsica, France, 2014\nBluetongue is an infectious, noncontagious, arthropodborne viral disease of domestic and wild ruminants . Twenty-six distinct bluetongue virus (BTV) serotypes have been identified .\n\n【1】The first detection of bluetongue virus (BTV) on the island of Corsica, France, was in 2000, when sheep were found to be infected with BTV genotype 2 (BTV-2); the virus most likely originated from Sardinia . In 2003 and 2004, BTV-4 and BTV-16 were detected on Corsica . Since 2004, no other outbreaks of BTV had been reported on the island until September 2, 2013, when BTV-1 was isolated from 3 flocks of sheep in southern Corsica . In the following weeks, the virus spread across the island, and by March 2014, 169 outbreaks had been reported.\n\n【2】### The Study\n\n【3】During September–December 2013, a total of 1,097 blood and spleen samples were collected from diseased or dead animals on Corsica; the samples were sent to the French National Reference Laboratory at Agence Nationale de Sécurité Sanitaire (Maisons-Alfort, France) to be tested for the presence of BTV. RNA was extracted from the samples and then amplified by using a real-time reverse transcription PCR (RT-qPCR). The ADIAVET BTV Real-time PCR Kit (bioMérieux, Saint Brieuc, France) was used for BTV group (i.e. _Orbivirus Bluetongue virus_ ) and serotype 1 detection. The LSI VetMAX European BTV Typing  Real-time PCR Kit (Life Technologies, Lissieu, France) was used for BTV-2, -4, -9, and -16 serotyping. A total of 531 samples from sheep, cattle, and goats had RT-qPCR results positive for BTV. All genotyped samples were positive only for BTV-1.\n\n【4】At the end of 2013, a compulsory vaccination program of domestic sheep, cattle, and goats was initiated by veterinary authorities in France. During the campaign, testing of ruminants with clinical signs of BTV was continued as part of a corresponding monitoring program.\n\n【5】During January–April 2014, we analyzed 436 samples from goats; 86 (19.7%) had positive BTV group RT-qPCR results. This method detects the BTV RNA genome segment 10. Of the 86 BTV-positive samples, 73 with a cycle threshold (C t  ) value of <35 were genotyped: 57 (78.0%) of the samples were classified as BTV-1, and 10 (13.7%) were classified as non–BTV-1, -2, -4, -9, -16, and -25. By using various primers  with a conventional in-house RT-PCR that amplifies a region of the BTV RNA genome segment 2, we obtained an amplicon for each of the 10 samples that were negative for BTV-1, -2, -4, -9, -16, and -25 (segment 2 encodes viral protein 2 and determines the serotype). The animals from which these samples came were in 5 herds that were sampled in January and February 2014. One of the 10 amplified products (806 bp) was sequenced directly and compared with homologous sequences available in the GenBank database; Blast 2.2.28  was used for the comparison. Because the genome sequence was similar to that of the BTV-25 segment 2, we selected different primers, and by gene walking, we obtained the full coding sequence of the gene.\n\n【6】BLASTN 2.2.29  was used to align sequences: the virus showed highest identity with the BTV-25 and BTV-26 serotypes circulating in Switzerland and Kuwait, respectively  . The complete segment 2 sequence of this new virus from Corsica  shared 73% nt and 75% aa identify with BTV-25 sequences and 65% nt and 60% aa identity with BTV-26 sequences.\n\n【7】We developed a conventional in-house RT-PCR and an RT-qPCR that enabled specific detection of BTV-n by selection of BTV-n–specific primers and probe . We could not detect BTV-n virus genome in any cattle or sheep samples tested in 2014, and we did not detect any co-infections with BTV-1 and BTV-n.\n\n【8】We attempted to isolate BTV-n from blood samples of the 10 BTV-n–positive goats; BTV-group C t  values for the samples ranged from 28.6 to 34.2. After many isolation attempts using embryonated chicken eggs as well as KC cells (a _Culicoides sonorensis_ –derived cell line), Vero cells, and BHK (baby hamster kidney) cells, we isolated 1 virus strain by using BSR (a clone of BHK-21) cells. We used supernatants to conduct a BTV-n–specific RT-qPCR (primers shown in Table ); the resulting C t  value of <17, indicated efficient virus replication. The virus was not neutralized by the reference antisera raised against BTV genotypes 1–24 and 26 or by an anti–BTV-25 serum (a titrated anti–BTV-25 serum is not available).\n\n【9】During May 2014, blood samples were collected from 56 goat bucks on a breeding farm located in the same area as the farms with the 4 previously sampled BTV-n–positive herds. Of the 56 sampled bucks, 51 were BTV-n positive by the BTV group–specific RT-qPCR (C t  value range 27.7–37.8). No buck was BTV-1 positive by the RT-qPCR, but 43 were positive by the BTV-n RT-qPCR; for the other 8 bucks, C t  values were beyond the positive range. Many of the bucks had not been vaccinated against BTV and showed no clinical signs of infection.\n\n【10】### Conclusions\n\n【11】We report the detection and identification of a new BTV, BTV-n, that is circulating among goats on Corsica. BLAST analysis of the BTV-n RNA segment 2 suggests that this virus most likely belongs to the BTV serogroup. However, full-genome segment 2 sequence identity between BTV-n and its closest BTV relative, BTV-25, was low (73.0% homologous); thus, BTV-n may be a novel serotype.\n\n【12】Some criteria for defining serotype/nucleotype have been defined by Maan et al. and Hofmann et al. Comparison of the segment 2 nucleotide sequences of the 26 BTV serotypes enabled definition of 12 nucleotypes; members of the same segment 2 nucleotype are characterized by at least 66.9% identity in their segment 2 nucleotide sequences . Our findings show that BTV-n shares 73.0% identity with the full genome segment 2 sequence of BTV-25. According to criteria defined by Maan et al. BTV-n cannot be considered a new nucleotype and seems to belong to the BTV-25 nucleotype. Maan et al. also showed that the differences in segment 2 sequences correspond to differences in serotypes. They reported overall interserotype variations in segment 2 of 29.0% (BTV-8 and BTV-18) to 59.0% (BTV-16 and BTV-22); the deduced amino acid sequence of VP2 varied from 22.4% (BTV-4 and BTV-20) to 73.0% (BTV-6 and BTV-22) . However, unlike BTV-25, BTV-n can grow in BSR cells .\n\n【13】Like BTV-25  and BTV-26 strains, BTV-n has been detected in goats without clinical signs and symptoms of infection. All of the samples positive for BTV-n by RT-qPCR were from healthy bucks, and most BTV samples from clinically affected goats were negative for BTV group by RT-qPCR. Thus, it is also likely that BTV-n is not pathogenic for goats.\n\n【14】We suggest, on the basis of the following findings, that the newly detected BTV-n strain could be a new serotype: the differences between BTV-n sequences and other BTV sequences were as high as those described by Maan et al. BTV-n is genetically diverse from other BTVs; BTV-n could not be amplified by the BTV-25–specific RT-PCR; anti–BTV-25 serum could not neutralize BTV-n; and, unlike BTV-25, BTV-n can be cultivated on BSR cells . No information is available about the origin of this new BTV strain; more in-depth investigation is needed to determine when and how the virus arrived in Corsica. Additional genome sequencing and experiments in various ruminant species are planned. These studies will help characterize this new BTV strain and evaluate the duration of viremia, humoral immune response, and pathogenicity of this serotype.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "334bc8a2-c6fe-43dd-aa7a-6f68102c5c23", "title": "Mathematical Modeling of Programmatic Requirements for Yaws Eradication", "text": "【0】Mathematical Modeling of Programmatic Requirements for Yaws Eradication\nYaws is a bacterial infection caused by _Treponema pallidum_ subsp. _pertenue_ . The disease predominantly affects children living in poor, remote communities and results in lesions of the skin, bone, and cartilage. Previously, yaws was widespread throughout the tropics , but in the 20th century a series of control efforts based on mass treatment and case finding led by the World Health Organization (WHO) is estimated to have reduced the burden of cases worldwide by up to 95% . Despite these efforts, the disease has resurged in several countries in West and Central Africa, the Pacific, and Southeast Asia.\n\n【1】In 2012, a single dose of azithromycin was shown to be effective treatment for yaws . The availability of a well-tolerated oral agent has prompted WHO to develop a new eradication strategy, known as the Morges strategy, based on community mass azithromycin treatment . The strategy is supported by World Health Assembly resolution 66.12, which calls for eradication of yaws by 2020 . The strategy combines an initial round of total community treatment (TCT) followed by subsequent active case finding and total targeted treatment (TTT) of newly identified patients and their contacts. Pilot studies have shown that community mass treatment with azithromycin is a highly effective strategy for reducing the community prevalence of yaws .\n\n【2】Data are limited to inform the optimum coverage and number of TCT or TTT rounds that are required to achieve elimination (i.e. interruption of transmission) of yaws at a local level to facilitate country-level elimination and ultimately global eradication. In India, a national yaws elimination campaign conducted during 1996–2004 resulted in substantial reduction in the prevalence of yaws, sustained interruption of transmission, and nationwide elimination . This program consisted of case-finding surveys and treatment with parenteral penicillin conducted every 6 months. Although this approach did not include the initial mass treatment round now recommended as part of the Morges strategy, its success indicates that serial rounds of high-coverage treatment might achieve local elimination.\n\n【3】A recent review of important research questions facing the global yaws eradication program has highlighted the need for more accurate data to inform the optimum number and coverage of rounds of TCT and TTT that will be required to achieve yaws eradication . Mathematical modeling has been used to inform control efforts for several other neglected tropical diseases  that are also managed by using community mass treatment strategies, and such approaches could be of value for yaws eradication efforts. In particular, this approach might allow a comparison of the differential effects of alternative mass treatment strategies, which would be difficult to assess by empirical randomized controlled trials because of the size and cost of implementing large-scale cluster randomized studies.\n\n【4】Previous mathematical models for yaws  have assessed the cost-effectiveness of yaws eradication but have not directly addressed the feasibility of achieving this goal or the number of rounds of treatment that would be required. In this study, we aimed to determine whether the eradication of yaws is feasible based on the Morges strategy and, if it is, the number and coverage of mass treatment rounds needed to achieve the goal.\n\n【5】### Methods\n\n【6】We developed a stochastic Markov model of community-level transmission of yaws . This model treats each stage of the disease as a discrete compartment, with persons moving through each compartment as the disease progresses or is treated. Upon infection, susceptible persons acquire primary disease at a rate that is proportional to the transmission rate and the total number of infectious persons. Persons with primary disease can further transition to secondary disease, at which stage they remain infectious, and both those with primary and secondary disease can transition to latent disease, which is not infectious. Last, those with latent disease can relapse back to secondary infectious disease. The model includes a rate of routine treatment for persons with primary or secondary disease, after which they become susceptible to infection again. The model also includes a lower rate of routine treatment for latent disease, after which the patients also become susceptible to infection again. Unlike previous mathematical models of yaws , tertiary yaws was not included in the model because such cases are believed not to contribute to transmission . Because persons might be reinfected many times, we did not consider them to obtain protective immunity after infection or treatment .\n\n【7】Although some evidence suggests the existence of a nonhuman primate reservoir for yaws in Africa , we did not include such infections in our model because there is currently no definitive evidence that the organism responsible for these infections is the same one that causes human yaws or that zoonotic transmission occurs in the real world. We therefore considered the epidemiologic importance of this possible reservoir as minimal when constructing our model.\n\n【8】##### Population Size\n\n【9】Estimates of the starting population for each compartment were derived from published population-based yaws prevalence studies . We modeled a discrete closed population without addition or reduction through births or deaths.\n\n【10】##### Disease Characteristic Variable Estimates\n\n【11】We estimated values for the rates of disease progression between different stages of yaws, including development of and relapse from latent yaws, by using expert opinion, published data, and estimates used in other models  . We defined 3 transmission scenarios (low, medium, and high) by using published age-specific treponemal seroprevalence data , expert opinion, and values used in other yaws models . Based on these data, we calculated initial estimates of the number of new infections arising from a single index case in a fully susceptible population (the basic reproduction number \\[R 0  \\]). Based on the seroprevalence data, we generated R 0  estimates of 1.25 (low), 1.83 (medium), and 2.4 (high). These estimates were converted to estimates of the probability of transmission after contact between an infectious person and a susceptible persons (β). The mean R 0  taking account of the full structure of our model, including duration of infection and the size of each starting population, is 1.96, resulting in a mean R 0  of 1.45 (95% CI 1.01–2.14) for the low transmission settings, 1.95 (95% CI 1.38–2.91) for medium, and 2.47 (95% CI 1.7–3.68) for high transmission. We included a variable to represent the likelihood of a person receiving treatment for yaws in the absence of public health interventions based on published data .\n\n【12】##### Mass Drug Administration Variables Estimates\n\n【13】We performed simulation experiments to estimate the impact of a yaws eradication intervention on disease transmission. In line with the Morges strategy , we considered 2 program components. In the first component, TCT, all persons were considered to have an equal chance of receiving treatment regardless of their infection status. In the second component, TTT, we considered that the coverage achieved among persons with active infection and those with latent infection might differ. Intervention coverage was modeled independently for TCT, with TTT pertaining to persons with active infection and persons with latent infection over a range of plausible estimates (65%–95% population coverage). Mass treatment compliance was simulated as a random, nonsystematic process (i.e. each person had the same chance of receiving treatment, with the likelihood of any 1 person receiving treatment being independent).\n\n【14】We varied the number of treatment rounds of TCT (1–3 rounds) and TTT (0–5 rounds). Where \\> 1 rounds of TTT were implemented, these rounds followed the final round of TCT. In line with the Morges strategy and real-world pilot implementations , rounds of mass treatment were spaced at 6-month intervals. We also conducted an analysis with rounds of treatment spaced at 12-month intervals to assess whether annual treatment might also be effective.\n\n【15】We derived estimates of the efficacy of single-dose treatment with azithromycin from randomized controlled trials of azithromycin for the treatment of yaws  . After successful treatment, yaws lesions become noninfectious within 24 hours ; therefore, we considered treatment to be immediately efficacious at the time of mass drug administration, with persons reverting to a susceptible state after treatment.\n\n【16】##### Implementing the Model\n\n【17】The model was implemented in R software . We performed repeated simulations across a range of assumptions about the rate of transmission (equivalent to low, medium, and high estimates of R 0  ) and assumptions about mass treatment, varying the coverage and number of mass treatment rounds undertaken .\n\n【18】For each combination of disease and intervention parameters, we performed 1,000 simulation experiments. Within each combination of transmission and treatment assumptions, we varied other disease-specific variables (e.g. rate of progression and relapse and treatment in the absence of intervention) across the range of parameter estimates. The model was run for an initial period of 50 months to achieve a steady state with yaws eradication interventions modeled to commence after this initial period. The model then ran for a further 100 months .\n\n【19】##### Assessing Outcomes\n\n【20】For each run of the model, we recorded whether eradication was achieved. Eradication was defined as no cases of infectious or latent yaws at the end of the model run. The eradication probability was defined as the percentage of runs within each permutation of model characteristics where eradication was achieved. All analyses were performed by using R version 3.2.2.\n\n【21】### Results\n\n【22】The model generated a total of 6,174 simulations of variable mass drug administration strategies. Because each strategy was implemented across a range (n = 3) of assumptions about the force of infection, a total of 18,522 simulations were created. The probability of achieving local interruption of transmission varied substantially across estimates of the force of infection and mass drug administration characteristics.\n\n【23】At the lowest estimates of the force of infection (R 0  \\= 1.45) and with treatment rounds at 6-month intervals, the minimum treatment thresholds required to have a transmission interruption probability of \\> 80% were a coverage of >75% of all populations and \\> 8 rounds of treatment (3 rounds of TCT followed by 5 rounds of TTT). Increasing the coverage to 85% reduced the total number of rounds required to 5 (1–3 rounds of TCT followed by 2–4 rounds of TTT) . For comparison, when the gap between treatment rounds was extended from 6 to 12 months, a total of 7 rounds of 85% coverage were required (2–3 rounds of TCT and 4–5 rounds of TTT).\n\n【24】At medium estimates of the force of infection (R 0  \\= 1.95) and with treatment rounds at 6-month intervals , the equivalent thresholds were 90% coverage and a total of 7 rounds of treatment (2–3 rounds of TCT and 4–5 rounds of TTT) . When the gap between treatment rounds was increased to 12 months, no combination of treatment variables was predicted to have a transmission interruption probability of \\> 80%.\n\n【25】At the highest estimates of the force of infection (R 0  \\= 3.32) and with treatment rounds at 6-month intervals, a total of 8 rounds (3 rounds of TCT and 5 rounds of TTT) with 95% coverage were required for a \\> 80% likelihood of interrupting transmission . When the gap between treatment rounds was increased to 12 months, no combination of treatment variables was predicted to have a probability of interrupting transmission of \\> 80%.\n\n【26】We considered it plausible that, under field conditions, the coverage of persons with latent infection would not exceed 70% in any given round of TTT, because such cases are not clinically apparent, and adequate coverage might not be achieved by treating the immediate contacts of persons with clinical infection. At lower estimates of the force of infection, a total of 3 rounds of TCT with 85% coverage and 3 rounds of TTT (each with a coverage of persons with active infection of 85% and coverage of persons with latent infection of 65%) was associated with a \\> 80% probability of interrupting transmission. If only 1 round of TCT was conducted, then coverage during TCT needed to be 90% and a total of 5 rounds of TTT (each with 90% coverage of persons with active infection and 65% coverage of persons with latent infection) were required. For medium estimates of the force of infection, a total of 8 rounds of treatment (3 rounds of TCT and 5 rounds of TTT) with a coverage of 90% were required. If only 1 round of TCT was undertaken, then 95% coverage was required, followed by 5 rounds of TTT with a 95% coverage of persons with active infection and 70% coverage of persons with latent infection. Under the highest estimate of the force of infection, no combination of treatment variables was associated with a high probability of interrupting transmission.\n\n【27】### Discussion\n\n【28】Our study demonstrates that, with implementation of the Morges strategy, interruption of transmission is possible in the setting considered. This finding suggests that eradication of yaws could be achieved, although caution must be applied because variability in the parameter estimates elsewhere could affect the effectiveness of these strategies. The parameter that has the strongest influence on whether elimination can be achieved is the transmission rate; that is, the rate at which infection occurs given contact between a susceptible and an infectious person . We considered 3 different scenarios of the transmission rate of yaws based on serologic data, and our estimate of the feasibility of elimination varied considerably depending on these estimates. Further studies to obtain better estimates of the R 0  in a range of countries where yaws is endemic would be of value to inform improved models and programmatic planning.\n\n【29】A minimum of 8 rounds with coverage of \\> 75% seems to be required for a high likelihood of achieving eradication but would prove inadequate at our highest estimates of possible values for R 0.  The predictions of our model are broadly in keeping with the real-world findings of the successful yaws elimination program in India , where 7 years of consecutive case finding and treatment (analogous to 14 rounds of TTT with 75% coverage) were conducted. In our model, the number of rounds of TTT also had a marked effect on the likelihood of achieving eradication, especially when coverage of persons with latent cases was limited to < 70%. In these settings, the required number of rounds of treatment to interrupt transmission increased considerably.\n\n【30】Relatively few data are available on the transmission rate of yaws. Even within yaws-endemic countries, the prevalence of yaws varies markedly. Studies in the Pacific have found a seroprevalence of antitreponemal antibodies of >30% in several communities  and a prevalence of clinical yaws of ranging from 2.5% to 5% in communities before mass treatment. The prevalence of yaws is markedly lower in many yaws-endemic countries in West Africa , but limited community-based seroprevalence data are available to inform our understanding of disease transmission there.\n\n【31】We modeled a range of estimates of R 0  from 1.08 to 3.32 based on seroprevalence data and expert opinion. Given the substantial influence of these estimates on the likely outcome of community mass treatment, further studies to better understand disease transmission and how this varies within and between endemic communities would be of value. Ideally, these studies would obtain community-level, age-specific seroprevalence data that could be used to calculate the force of infection. No perfect serologic marker can be used for this task. Traditional treponemal serology combines a treponemal test, which reflects lifetime exposure but remains positive for life, with a nontreponemal test, the titer of which rises and falls after treatment. It is therefore not possible to use seroprevalence data to distinguish persons who have been infected many times from those who have been infected once, and seroprevalence estimates are likely to underestimate the actual force of infection. For this study, we calculated the force of infection while relying on treponemal serology alone, which should provide a more accurate estimate of the force of infection than if we used dual-positive serology. It remains, however, an imperfect measure.\n\n【32】Our model predicts that high coverage is required in all rounds of treatment to make yaws eradication feasible. Data from the previous WHO and United Nations Children’s Fund mass treatment campaign have highlighted the importance of achieving high coverage of persons with latent cases of yaws  and that treatment of persons with active cases alone is insufficient to interrupt transmission. These factors were important considerations in the adoption as part of the Morges strategy of an initial round of TCT regardless of the prevalence of active disease in a community. Given the high coverage requirement, particularly of persons with latent cases, and the relatively high fixed-costs of reaching yaws-endemic communities  compared with the relatively low costs of generic azithromycin, it might be preferable to conduct multiple rounds of TCT before the switch to TTT. Such a recommendation would be in line with the original Morges strategy , which recommended that additional rounds of TCT could be considered if the coverage achieved in the initial round of treatment was <90% or if access to yaws-endemic communities was difficult. A switch to multiple rounds of community mass treatment might also facilitate integration with other neglected tropical diseases mass drug administration programs in countries that are also frequently based on whole community mass treatment , although our model predicted a higher probability of achieving eradication with biannual treatment. Further studies to help determine the optimum strategy for achieving high coverage of persons with latent cases during the TTT phase of eradication efforts should be considered (e.g. studies of the spatial epidemiology of latent yaws cases in relation to persons with active cases in both pre– and post–mass drug administration settings or studies of whether additional mass treatment rounds specifically targeting children might be beneficial).\n\n【33】Our study has several limitations. Most notably, we lack accurate estimates for several disease parameters. The parameters used are derived from expert opinion and data from the Pacific region, and the transmission dynamics of yaws might be different in other regions of the world. However, the disease parameters used in this study are broadly in line with those used by other models of yaws transmission . We tested a range of coverage estimates for community mass treatment, but we did not factor in the possibility that some persons might be systematically missed during mass treatment campaigns, a phenomenon that has been observed in control efforts for other neglected tropical diseases . The current Morges strategy does not include adjunctive elements, such as water, sanitation, and hygiene interventions, in addition to mass treatment, although some studies suggest that improved access to water and sanitation is associated with a decreased risk for yaws . We did not include a secular trend in our model, and such a trend could be anticipated to further increase the likelihood of yaws eradication being achieved. Our model was designed to assess the feasibility of achieving yaws eradication in the near future, driven by the current WHO strategy, and in those conditions any effect of a secular trend could be expected to be minimal compared with the substantial impact of community mass treatment. Previous models have shown that secular trends are unlikely to substantially affect the cost-effectiveness of mass treatment ; however, those models were based on an assumption of 90%–99% coverage in a single TCT round and 100% coverage of index patients and their contacts in the TTT round. More generally, we used a single-model structure that is simplified by modeling persons as being in 1 of a small number of disease-related compartments at any time and considering contact to occur at random. Uncertainty in model structure relating to disease progression and the probability of contact means that our findings should be interpreted carefully and potentially reassessed as elimination strategies are being applied.\n\n【34】In conclusion, our study assessed the theoretical achievability of worldwide yaws eradication and represents an important milestone in reaching the WHO’s eradication target. We have defined programmatic thresholds that might need to be met to achieve yaws eradication and identified key research questions to be addressed to inform refinements of the model and the worldwide roll-out of treatment strategies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d90ebd3b-46aa-4cdd-8128-23e4cca6b6c2", "title": "Risk Factors for Acquiring Scrub Typhus among Children in Deoria and Gorakhpur Districts, Uttar Pradesh, India, 2017", "text": "【0】Risk Factors for Acquiring Scrub Typhus among Children in Deoria and Gorakhpur Districts, Uttar Pradesh, India, 2017\nOutbreaks of acute encephalitis syndrome (AES) with high case-fatality rates have been reported from Gorakhpur district, Uttar Pradesh, India, for >2 decades. These outbreaks occur during monsoon and postmonsoon seasons and predominantly affect children. Scrub typhus (ST) accounted for about two thirds of AES cases  and is also an important etiology of acute febrile illness (AFI) in the region . Untreated cases of ST-attributable AFI can progress to AES.\n\n【1】ST, caused by the bacterium _Orientia tsutsugamushi_ , is transmitted by the bite of trombiculid mites, which live in moist soil covered with vegetation . Several risk factors, including certain household characteristics, work-related practices, and behaviors, have been identified among adult ST patients . Household characteristics include location of the house near a grassland, vegetable field, or ditch; presence of mud floors; piled weeds inside the house; and scrub vegetation in the vicinity . Work-related practices include working in vegetable fields or hilly areas and working in short sleeves or with bare hands . Certain behaviors such as lying on grass and squatting to defecate or urinate also are associated with ST . Because these risk factors are region specific, we conducted an exploratory case-control study among children in Gorakhpur and Deoria districts of Uttar Pradesh to identify factors associated with ST infection.\n\n【2】### The Study\n\n【3】We conducted AFI surveillance in public health facilities in Deoria (n = 5) and Gorakhpur (n = 3) districts during October 3–November 11, 2017, a period coinciding with AES outbreaks in Gorakhpur district. We enrolled children 2–15 years of age with a \\> 3-day history of fever, from whom we collected 2 mL of blood after obtaining written informed consent from parents and assent from children 7–15 years of age. We screened serum samples for IgM and IgG against _O. tsutsugamushi_ by using ELISA kits (Scrub Typhus Detect; InBios International Inc. Seattle, WA, USA). For our study, an optical density value \\> 0.5 indicated IgM positivity. This cutoff has 93% sensitivity and 91% specificity for ST diagnosis . An optical density value <0.5 indicated IgM and IgG negativity.\n\n【4】Febrile children who were positive for _O. tsutsugamushi_ IgM were considered case-patients, whereas patients who were seronegative for IgM and IgG were considered controls. Case-patients and controls and their parents or guardians were interviewed in their houses by using a pretested structured questionnaire to collect information on sociodemographics, household characteristics, behaviors, and environmental exposures during the 2 weeks before fever onset. Interviewers were blinded to the case-patient or control status of children except during the first week of study.\n\n【5】We calculated crude odds ratios (ORs) and 95% CIs associated with different exposures. We included variables with p value < 0.2 in univariate analysis in a stepwise backward elimination model to identify variables for inclusion in the final unconditional multiple logistic regression model by using Stata 13 (StataCorp LLC, College Station, TX, USA).\n\n【6】We recruited 819 AFI patients, of whom 155 (18.9%) had _O. tsutsugamushi_ IgM (case-patients) and 409 (49.9%) were seronegative (controls) . We excluded 255 (31.1%) patients who had only _O. tsutsugamushi_ IgG. We interviewed all case-patients and 406 controls. All case-patients and controls were from rural areas. The mean age of case-patients was higher than controls (7.0 vs. 6.3 years; p = 0.018); 39% of case-patients and 51% of controls were < 5 years of age. A higher proportion of case-patients than controls were from households that owned agricultural land (83% vs. 73%; p = 0.017) .\n\n【7】Aside from fever, clinical signs and symptoms among patients varied . Five ST patients, who did not receive doxycycline/azithromycin, had onset of neurologic manifestations and required hospitalization; they recovered after administration of azithromycin.\n\n【8】Univariate analysis showed that ST patients were more likely than controls to live in houses located within or adjoining fields, houses with unpaved surroundings with mud or unkempt grass, and mud-floored houses . In addition, a higher proportion of case-patients lived in households that used wood or cow dung for cooking and stored this fuel indoors. Compared with controls, children with ST were more likely to have played in agricultural fields, defecated in agricultural fields, bathed in a river or stream, carried grass bundles on their heads, handled fodder for cattle, or visited or accompanied parents to an agricultural field during the 2 weeks before fever onset .\n\n【9】Multivariable analysis showed that children residing in houses within or adjoining fields (adjusted OR \\[aOR\\] 1.56, 95% CI 1.02–2.38) and that stored firewood indoors (aOR 1.61, 95% CI 1.06–2.45) had higher odds of acquiring ST. Open defecation (aOR 2.0, 95% CI 1.18–3.39), playing in (aOR 5.2, 95% CI 1.92–14.2) or visiting (aOR 1.65, 95% CI 1.07–2.52) agricultural fields, and handling cattle fodder (aOR 2.05, 95% CI 1.13–3.70) also were associated with ST .\n\n【10】### Conclusions\n\n【11】In Gorakhpur and Deoria districts, recent exposure to the outdoor environment, either while defecating or playing in agricultural fields, as well as visiting agricultural fields, storing firewood indoors, and handling fodder for cattle were associated with higher risk for acquiring ST among children. Of these risk factors, defecation in the agricultural field was the most common exposure at the population level. The observed association of ST and defecation in fields is consistent with the findings from an ST outbreak in Manipur, India, where persons who defecated or urinated in the jungle or bushy areas were found to be at higher risk for ST . In Gorakhpur and Deoria districts, about one third of the study population had toilets, a finding consistent with the 2015 national level survey, which indicated that 29.5% of the households in rural Uttar Pradesh had sanitary toilets and 27.3% of households that had access to toilets were using them . Efforts to prevent ST in the region therefore also need to focus on constructing household and community toilets, as well as behavior change communication about avoiding open defecation in the fields. In addition, storing firewood and fodder indoors attracts rodents that can harbor mites. Mites present on the firewood and fodder collected from fields might expose children to mite bites during storage or handling of the cattle fodder.\n\n【12】Our study had limitations. First, although OT IgM is detectable as early as 4 days after fever onset , some ST patients in our study might have been negative for IgM and hence misclassified as controls. However, such nondifferential misclassification is likely to underestimate the actual association. Further, the median duration of fever was 7 days (interquartile range 4.5–10.0 days) among case-patients and 5 days (interquartile range 4–8 days) among controls , indicating minimal possibility of such misclassification. Some controls might have been misclassified as case-patients because of persistence of IgM from previous infection . However, this possibility is less likely considering that IgM peaks by 4 weeks of infection and declines rapidly thereafter . Second, our control-patients were AFI patients from healthcare facilities and not healthy children from the general population. Nevertheless, the behaviors of children visiting healthcare facilities before their febrile illness and children from the general population are less likely to be different.\n\n【13】For prevention of ST among children in Gorakhpur and Deoria districts, communication messages should focus on changing behaviors such as defecating or playing in agricultural fields and unnecessary visits to agricultural fields. Onset of central nervous system manifestations among untreated ST patients underscores the importance of early administration of doxycycline/azithromycin to ST patients in Gorakhpur and Deoria districts to prevent progression to AES.\n\n【14】Members of the Scrub Typhus Risk Factor Study Group: Vishal Shete, M. Prakash, Surya Prakash (Chennai, India); Reeta Mani, R. Vijayalakshmi, Narendra Kumar, Premanjali (Bangaluru, India); Ujjawal Sinha, Gagan Sharma, Hemant Kharnare, Abhishek Jain, Rachana Kathuria, A.K. Puttaraju, Manish K. Gawande, Ashish Tigga, Vishesh Kumar, Rajesh Badgal, Hamid Sayeed, Madhup Bajpai (New Delhi, India); Prasanth Pareet, Deepchand Agre, Nishikant Kumar (Manipal, India); Mohan Papanna, Ekta Saroha, Rajesh Yadav, Aslesh Prabhakaran (New Delhi, India); and Hirawati Deval and Gajanan Patil (Gorakhpur, India).\n\n【15】This study was funded by the Indian Council of Medical Research and a US Centers for Disease Control and Prevention (CDC) Cooperative Agreement for Global Health Security to the National Institute of Mental Health and Neurosciences.\n\n【16】The Institutional Ethics Committee of the Indian Council of Medical Research National Institute of Epidemiology approved the study protocol. The CDC Center for Global Health Associate Director for Science reviewed the protocol and determined that CDC was not engaged.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "400c83b8-b2c9-4501-b4b2-a9f6245cd3ec", "title": "Tuberculosis Outbreak in Marijuana Users, Seattle, Washington, 2004", "text": "【0】Tuberculosis Outbreak in Marijuana Users, Seattle, Washington, 2004\nAlthough overall US tuberculosis (TB) rates are declining, certain populations such as the foreign-born , homeless persons , and those who use illicit drugs  continue to challenge TB control efforts. A cluster of TB cases was recognized in Seattle from February to April 2004 among 4 young East-African immigrants with histories of incarceration and illicit drug use. Because patients resisted revealing names of contacts, traditional TB control efforts were hampered. We describe an outbreak fueled by illicit drug use and characterized by accelerated progression of disease.\n\n【1】### The Study\n\n【2】_Mycobacterium tuberculosis_ isolates from all culture-positive TB patients in Seattle and King County, Washington, during 2003–2004 were genotyped by spacer oligonucleotide typing and mycobacterial interspersed repetitive unit methods. We included patients who had an isolate that matched the outbreak strain or who had a social link to an already included patient.\n\n【3】Patient medical records were reviewed, and infectious periods were calculated. For sputum smear–positive patients, the infectious period extended from 3 months before symptom onset or the first positive smear (whichever was earlier) until 2 weeks after the start of appropriate TB treatment or until the patient was placed into isolation or produced consecutively negative smears. For sputum smear–negative patients, the infectious period extended from 1 month before symptom onset, the start of appropriate TB treatment, or the date that the patient was isolated (whichever was earlier), until 2 weeks after the start of appropriate TB treatment or until patient isolation .\n\n【4】We interviewed patients to learn their contacts, activities, and locations frequented while they were contagious. Additional contacts were found by outreach workers and a disease intervention specialist from the East-African community who was hired to work in the neighborhoods frequented by the patients. While in these neighborhoods, outreach workers and the disease intervention specialist recruited persons seen with patients or their contacts to be evaluated for TB and latent TB infection. Contact activities, specifically those related to illicit drugs, were observed or self-reported.\n\n【5】We categorized contacts as friends or others. Friends were defined as contacts of patients who spent time within a close-knit network of young men who exhibited similar marijuana-using behavior. Other contacts were defined as the families and relatives of patients and those who were named but were not closely associated with this network. Contacts received a TB evaluation including a tuberculin skin test (TST) to detect infection. Infection rates for friends and others were compared to guide contact prioritization for screening.\n\n【6】Patient 1 was first evaluated in December 2003, when a chest radiograph suggested pulmonary TB (i.e. upper lobe cavitary infiltrate). However, only clarithromycin was prescribed, and the patient was lost to follow-up. He was again seen in an emergency room in April 2004 after the infection evolved into bilateral extensive pulmonary TB. His sputum tested smear-positive for acid-fast bacilli. He was reluctant to name contacts.\n\n【7】Ten additional patients were found from February to October 2004 . Isolates from all patients had matching TB genotypes. In Washington State, this genotype has only been identified among the patients in this outbreak. Patients' median age was 22 years (range 18–41). Eight patients were born in East Africa; a median of 13 years (range 6–22) had passed since their arrival in the United States. All but 1 patient were of East-African origin. Patient 5 was a white woman who received illicit drugs from patient 1.\n\n【8】Patients were symptomatic and had findings indicating infectiousness: all had pulmonary TB, 7 had cavitary disease, and 8 had sputum that tested smear-positive for acid-fast bacilli. One patient was HIV infected. Consecutive chest radiographs indicated progression to cavitary disease in <75 days weeks in 3 patients and <121 days in another patient. Table 2 shows the dates of clear chest radiographs interpreted as normal and the first chest radiographs showing disease.\n\n【9】While contagious, patients stayed in various locations, including cars, for most of the day. A single-bedroom apartment occupied by at least 1 patient while he was contagious was regularly visited by 2 other patients. Numerous members of the friend network slept there on any given night, and many others would regularly visit during a 10-week period beginning in April 2004 . The occupants nailed boards over the apartment windows to conceal activities, primarily marijuana use, from outsiders.\n\n【10】All patients were unemployed and had histories of incarceration and illicit drug use. No patients spent time together while incarcerated. All reported frequent \"hotboxing,\" the practice of smoking marijuana with others in a vehicle with the windows closed so that exhaled smoke is repeatedly inhaled.\n\n【11】The Figure illustrates patients' infectious periods. Considerable overlap in infectious periods was noted, which highlights the potential for simultaneous contact with multiple contagious patients. We found 121 potentially exposed contacts. Fifty-four were friends, and the remaining were other contacts. At least 31 (57%) friend contacts spent time at the 1-bedroom apartment. After those with a past positive TST result were removed, 14 (64%) of 22 screened friends and 6 (23%) of 26 other contacts had a positive TST result. The risk for a positive TST result was 2.8× greater among friends than among other contacts (95% confidence interval = 1.3–6.0). Twenty-nine (54%) friend contacts self-reported or were observed hotboxing. Among the friends who reported or were observed hotboxing, 11 (79%) of 14 who received a TST had a positive result. Twelve friend contacts began treatment for latent TB infection, and 8 completed treatment.\n\n【12】### Conclusions\n\n【13】Risk factors for TB include birth in a country with high TB prevalence  and incarceration . Although most patients in this outbreak were foreign-born and had histories of incarceration, genotyping results and epidemiologic findings suggest that TB was transmitted recently in the community rather than before immigration or during incarceration.\n\n【14】Frequent marijuana use has been reported among TB outbreak patients  and was the behavior linking these patients together. Creative sharing of marijuana has been described recently as a factor for _M. tuberculosis_ transmission. In Australia, sharing a water pipe (i.e. \"bong\") was linked to transmission . \"Shotgunning\" refers to inhaling smoke from illicit drugs then exhaling it directly into another's mouth  and was associated with _M. tuberculosis_ transmission among a group of exotic dancers and their contacts .\n\n【15】This investigation noted that a similar activity, hotboxing, might have contributed to transmission. As with shotgunning, hotboxing promotes the sharing of exhaled smoke and air. One patient with smear-positive cavitary disease reported daily hotboxing with friends, often for most of the day. In addition, marijuana smoking might induce cough, creating an ideal environment for transmission. Many friends stayed and used marijuana at the single-bedroom apartment during the height of the outbreak. Furthermore, by nailing boards over the windows, ventilation was limited, creating an environment similar to that of hotboxing.\n\n【16】Disease rapidly progressed in HIV-negative patients in this outbreak. Seven patients had cavitary pulmonary TB. Three had chest radiographs interpreted as normal <75 days before TB diagnosis. Although progressive primary TB by nature is thought to be due to recent transmission, progressive primary TB with cavitation is uncommon . The pathogenesis of progressive primary TB with cavititation is not clear. However, frequent marijuana use and the setting of intense exposure may have played a role. In addition, poor nutrition and unhealthy lifestyles might have predisposed these young men to more rapid progression of disease. While no laboratory investigation to assess genetic susceptibility or strain virulence was conducted, these factors might have also contributed to the development of cases.\n\n【17】This outbreak resembles an outbreak reported among regular patrons of a neighborhood bar . Both were fueled by a highly infectious source patient who spent extended amounts of time indoors with 1 group of persons who regularly used substances (i.e. alcohol or marijuana). The result in both situations was a higher than expected incidence of TB disease and latent TB infection. In the outbreak reported in this article, however, the substance of choice was illicit and further complicated the control of this outbreak.\n\n【18】Patients' illicit drug activities promoted a reluctance to name contacts at risk and locations frequented. Traditional name- or location-based contact investigations did not work. Efforts had to revolve around meeting these young patients at times and locations convenient to the group. Then after gaining the groups' trust, outreach workers successfully found and screened contacts. Many successful screenings took place on street corners and in parking spaces throughout the community. Often outreach workers were successful only after spending hours driving throughout the community searching for patients and contacts. Four patients were originally screened as unnamed contacts located in the field. Alternative strategies to name-based contact investigations may become increasingly critical to TB control as TB recedes further from the general population, yet persists within smaller guarded groups .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4f8bdb82-93bd-4b35-a8e2-d76f91acb33c", "title": "Epidemiologic Evidence for Airborne Transmission of SARS-CoV-2 during Church Singing, Australia, 2020", "text": "【0】Epidemiologic Evidence for Airborne Transmission of SARS-CoV-2 during Church Singing, Australia, 2020\nThe circumstances under which airborne transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) might occur are uncertain . Previous cluster reports have suggested involvement of airborne transmission , but clear epidemiologic evidence is lacking. We investigated a SARS-CoV-2 outbreak in a church in Sydney, New South Wales, Australia, and reviewed the epidemiologic and environmental findings to assess the possibility of airborne transmission of SARS-CoV-2.\n\n【1】### The Study\n\n【2】On July 18, 2020, the Western Sydney Public Health Unit was notified of a positive SARS-COV-2 test result for an 18-year-old man (PCR cycle threshold \\[C t  \\] values: envelope gene 14.5, nucleocapsid gene 16.8). He had sought testing the day before, after learning of a SARS-COV-2 exposure at a venue he attended on July 11. He reported symptom onset of malaise and headache on July 16 and cough and fever on July 17. He was a church chorist and, during his infectious period (from 48 hours before onset), had sung at four 1-hour services, 1 each on July 15 and 16 and 2 on July 17.\n\n【3】The case-patient had sung from a choir loft, elevated 3.5 m above the congregation, which he entered before and left after the service. He denied touching objects in the church or mixing with the general congregation. Video recordings of the services corroborated this history. We identified close contacts according to the national coronavirus disease (COVID-19) control guidelines at the time : anyone who had spent >15 min face-to-face or shared a closed space for 2 hours with a case-patient during the infectious period of the case-patient. Initially, 10 other chorists and staff were classified as close contacts and required to quarantine .\n\n【4】On July 18, the church informed the community about the case-patient, prompting testing among members. On July 20, the Western Sydney Public Health Unit was notified of 2 additional case-patients who reported attendance on July 15 and 16. Neither was known by the primary case-patient.\n\n【5】Because transmission was deemed likely to have occurred at these services, we classified all attendees of the 4 services as close contacts, required to quarantine, and requested to seek baseline SARS-CoV-2 testing regardless of symptoms (in addition to if symptoms developed). Public health staff telephoned attendees (identified by mandatory service sign-in records), released alerts through the church and media, and established a testing clinic on-site. Close contacts were contacted every 2–3 days to inquire about symptoms and advised to retest if symptoms developed.\n\n【6】We identified 508 close contacts across the 4 services , of which 434 (85%) were recorded as having a test within 17 days after exposure. Most contacts were tested 2–7 days after exposure .\n\n【7】We detected 12 secondary case-patients among 508 service attendees, yielding an overall secondary attack rate (SAR) of 2.4% across the 4 services . Five case-patients attended only the service on July 15 (SAR 5/215, 2.3%), and 7 attended only on July 16 (SAR 7/120, 5.8%). One case-patient who attended on July 16 also attended on July 17; however, no case-patients were identified who attended only a service on July 17. Secondary case-patients showed development of symptoms 2–12 days after exposure . Five of the secondary case-patients were from the same households as earlier cluster case-patients. Thus, these case-patients might have been infected within the household rather than the church. No secondary case-patients reported other SARS-COV-2 exposures outside these services. There were no deaths, although 3 case-patients were hospitalized, including 2 who required intensive care.\n\n【8】SARS-CoV-2 genome sequencing was performed for the primary case-patient and 10 secondary case-patients . These case-patients formed a single genomic cluster with a maximum of 2 nt changes from the SARS-CoV-2 genome of the primary case-patient . High C t  values for the remaining 2 case-patients prohibited sequencing.\n\n【9】To further characterize exposures, we determined the seating positions of secondary case-patients within the church. We asked case-patients to describe where they sat, and the video recordings of the services were reviewed, jointly with the case-patients where possible, to confirm locations.\n\n【10】The church was round, and pews were located circumferentially. We were able to locate the exact location of 10 of the 12 secondary case-patients by using the recordings. The remaining 2 case-patients (case-patients 3 and 4) were unable to review the recordings but described the section and row in which they sat. All secondary case-patients sat within a 70° section, below and 1–15 m from the primary case-patient . The primary case-patient faced away from this area, and used a microphone. Cases were not detected in attendees seated in other sections, and the spatial clustering remains if the 5 potentially household-acquired case-patients are excluded (case-patients 7, 8, 10, 12, and 13). None of the other choristers showed symptoms or tested positive for SARS-CoV-2. Use of masks was not in place.\n\n【11】To understand the ventilation, we conducted 2 site visits with the building manager. The church had a high conical roof, and the ventilation system at the apex was not operating during the services. The doors and windows were largely closed, except as persons entered and exited, and the wall fans were off, meaning there was minimal ventilation.\n\n【12】### Conclusions\n\n【13】We detected 12 secondary case-patients linked to an infectious case-patient at church services on 2 days. Secondary case-patients were seated in the same area of the church, up to 15 m from the primary case-patient, with whom there was no evidence of close physical contact. We believe that transmission during this outbreak is best explained by airborne spread, potentially the result of 3 factors. First, singing has been demonstrated to generate more respiratory aerosol particles and droplets than talking . Second, minimal ventilation might have enabled respiratory particles to accumulate in the air, and convection currents might have carried particles toward the pews where secondary case-patients were seated. Third, the primary case-patient was likely near the peak of infectiousness on the basis of low C t  values  and symptom onset occurring around the exposure dates . Although we cannot completely exclude fomite transmission, this transmission would not explain the spatial clustering of case-patients within the church over 2 days.\n\n【14】Strengths of our investigation include detailed case and contact follow-up, availability of video recordings of the services to confirm movements and locations of case-patients, high uptake of testing by contacts, and that SARS-CoV-2 genome sequencing provided supportive evidence that case-patients were closely related genomically. In addition, the New South Wales context of low community transmission  and high estimated case ascertainment  makes it unlikely that case-patients acquired infection outside this cluster.\n\n【15】A limitation was that most contacts were tested within a week of exposure, which could have been too early to detect some asymptomatic infections. Second, this investigation only provides circumstantial evidence of airborne transmission, and does not help elucidate the exact mechanism of spread. Finally, we are unsure why transmission did not occur at the services on July 17 (except in 1 possible instance); reasons might be related to altered air flow, the primary case-patient being past peak infectiousness, or that cases that did occur went undetected.\n\n【16】This cluster occurred despite adherence to guidelines requiring microphone use and a 3-m cordon around singers. Guidelines for places of worship were tightened after this cluster was detected, including increasing the distance required around a singer to 5 m. However additional mitigation measures might be necessary to prevent airborne infection during church services and singing, including increased natural or artificial ventilation  or moving activities outdoors.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e9fbf988-aee7-41d3-8f3d-c8ed3bb6b310", "title": "Valley Fever (Coccidioidomycosis)", "text": "【0】Valley Fever (Coccidioidomycosis)\nValley fever, also called coccidioidomycosis, is an infection caused by the fungus _Coccidioides_ . The fungus is known to live in the soil in the southwestern United States and parts of Mexico and Central and South America. The fungus was also recently found in south-central Washington. People can get Valley fever by breathing in the microscopic fungal spores from the air, although most people who breathe in the spores don’t get sick. Usually, people who get sick with Valley fever will get better on their own within weeks to months, but some people will need antifungal medication. Certain groups of people are at higher risk for becoming severely ill. It’s difficult to prevent exposure to _Coccidioides_ in areas where it’s common in the environment, but people who are at higher risk for severe Valley fever should try to avoid breathing in large amounts of dust if they’re in these areas.\n\n【1】About\n\n【2】Where it Comes From\n\n【3】Health Professionals\n\n【4】Symptoms\n\n【5】Diagnosis & Testing\n\n【6】Statistics\n\n【7】Risk & Prevention\n\n【8】Treatment & Outcomes\n\n【9】Maps", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ecd7f8b4-b6fb-406f-a9ff-6435a60d380d", "title": "Falls and Fall-Related Injuries Among US Adults Aged 65 or Older With Chronic Kidney Disease", "text": "【0】Abstract\n\n【1】**Introduction**\n\n【2】Falls are among the leading causes of injury and death among adults aged 65 or older. People with chronic kidney disease (CKD) are at increased risk of falling and of having a serious injury from falls. However, information is limited about risk factors for falls and fall-related injuries among people with CKD.\n\n【3】**Methods**\n\n【4】We performed a secondary analysis of 157,753 adults (6.1% with CKD) aged 65 or older surveyed in the 2014 Behavioral Risk Factor Surveillance System.\n\n【5】**Results**\n\n【6】People with CKD were at increased risk of falls (odds ratio \\[OR\\] = 1.81; 95% confidence interval \\[CI\\], 1.63–2.01) and fall-related injuries (OR = 1.50; 95% CI, 1.27–1.78) even after adjusting for differences in demographic characteristics, health conditions, and lifestyle factors ( _P_ < .05 for all). Among people with CKD, women, people diagnosed with diabetes, diabetes duration, and arthritis were all significant predictors of falls and fall-related injuries ( _P_ < .05 for all). Lifestyle factors, such as engaging in recent exercise (adjusted odds ratio \\[AOR\\] = 0.68; 95% CI, 0.56–0.81) and limited physical function (assessed as difficulty in climbing stairs) (AOR = 2.84; 95% CI, 2.30–3.44), were most closely associated with falls and fall-related injuries.\n\n【7】**Conclusion**\n\n【8】Adults aged 65 or older with CKD were at increased risk of falling and of suffering an injury as a result of a fall compared with adults in the same age range without CKD. Potentially modifiable factors such as physical function and recent exercise were most closely related to reduced risk for falls and fall-related injuries and may be an appropriate target for fall prevention and rehabilitation programs in people with CKD.\n\n【9】Introduction\n\n【10】Falls are among the leading causes of injury and death among US adults aged 65 or older . Nearly one-third of adults in this age group report a fall every year , and the annual cost of falls in the United States is approximately $31 billion . Numerous risk factors for falls have been identified, including frailty and chronic diseases .\n\n【11】Chronic kidney disease (CKD) is common among adults aged 65 and older with an estimated prevalence of 14.8% in the US population . People with CKD in this age group have a greater risk of falling than those without CKD . Furthermore, poor kidney function is a risk factor for falling  and for poor outcomes from falls, such as fractures . People with CKD may be more likely to fall and to experience a serious injury from falls; however, information is limited on the prevalence and predictors of falls in adults aged 65 or older with CKD .\n\n【12】Previous studies have identified age, sex, frailty, and diabetes as risk factors for unintentional falls . However, published data have limitations. For example, previous studies primarily used convenience samples from small clinics or health care facilities, assessed falls without exploring fall-related injuries, and often did not focus on older adults. Thus, these studies provided limited assessments of health, lifestyle, and demographic factors that may influence prevalence of falls or CKD. The limited samples in these previous studies, although providing valuable information, have led to conflicting results in relation to many risk factors. For example, some studies of people with CKD found that men were at increased risk of falling , whereas others found that women were more likely to fall . Thus, the purpose of this study was to assess the prevalence of CKD and falls in a large national sample of US adults aged 65 or older and to explore the association between falls, CKD, health risk factors, and demographic characteristics.\n\n【13】Methods\n\n【14】### Study design and participants\n\n【15】We conducted a secondary analysis of data from the 2014 Behavioral Risk Factor Surveillance System (BRFSS). BRFSS is conducted annually by the Centers for Disease Control and Prevention (CDC) to measure behavioral and health risk factors and diseases in US adults. BRFSS is a telephone survey that uses random-digit dialing to randomly select civilian noninstitutionalized adults aged 18 or older. In 2014, BRFSS data were collected from adults across all 50 states and the District of Columbia. BRFSS uses a complex multistage sampling procedure and design weights to adjust for the unequal probability of being selected, for noncoverage, and for nonresponses. This is to ensure the creation of equal population estimates for each geographic region. The combined landline and cellular telephone median weighted response rate was 47.0% (landline telephones, 48.7%; cellular telephones, 40.5%). The BRFSS questionnaire consists of 3 parts: 1) core questions, which are a standard set of questions that all participating states and territories must administer; 2) optional modules; and 3) state-added questions _,_ that is, questions on specific topics that states can choose to include in response to state-specific health concerns. Additional details about BRFSS survey methods, sampling, and response rates are available . Because BRFSS is approved by CDC’s institutional review board (IRB) and because our study used de-identified publicly available data, did not recruit human subjects, and had no direct contact with study participants, no additional approval was required from the authors’ IRBs.\n\n【16】### Measures\n\n【17】**Demographic characteristics and chronic kidney disease.** We included all adults aged 65 or older who participated in the 2014 BRFSS (N = 157,753). BRFSS also collected data on participants’ sex, race, marital status, employment, and education through structured and closed-format questions; we used these demographic variables for our analysis. BRFSS 2014 asked respondents about history of diagnosis of various chronic conditions including CKD with response options of yes, no, or don’t know. The CKD diagnosis question was, “Has a doctor, nurse, or other health professional ever told you that you have kidney disease (excluding kidney stone, bladder infection, or incontinence)?” . Respondents were categorized into 2 groups based on a history of CKD to assess differences in demographic characteristics and lifestyle factors (ie, CKD group vs non-CKD group).\n\n【18】**Health, lifestyle, and disease conditions.** Study participants were asked if they had ever been diagnosed with arthritis, diabetes, or cancer (with response options of yes, no, or don’t know). Response options to questions on current smoking, heavy drinking, difficulty walking, and health coverage were yes or no. Body mass index (BMI) (weight in kg/height in m 2  ) was computed on the basis of self-reported height and weight. A single item assessed participants’ current general health (with response options of excellent, very good, good, fair, and poor) . For prevalence of falls and fall-related injuries, the 2014 BRFSS asked 2 questions: 1) In the past 12 months, how many times have you fallen and 2) how many of these falls caused an injury. The responses were categorized as no for 0 events and yes for 1 or more falls or fall-related injury events . CKD and non-CKD groups were compared for differences in these health and disease variables.\n\n【19】### Data analysis\n\n【20】We first computed descriptive statistics (eg, percentages, frequencies) for all study variables and measures (ie, demographics, lifestyle behaviors, and chronic conditions). Using χ 2  tests we explored the differences in these variables between respondents with a history of CKD versus those without. Second, we used the binary variable history of CKD (yes vs no) as an independent variable to predict the odds of falls and fall-related injuries. In multivariate logistic regression analysis, falls and fall-related injuries were used as an outcome, with CKD as a predictor, and we computed adjusted odds for falls and fall-related injuries after adjusting for demographic characteristics of study participants, their lifestyle and health behaviors, and history of comorbid conditions that may be associated with falls or CKD. All analyses were performed by using the complex sample survey data analysis procedures in SPSS version 24 (IBM Corp). Statistical significance was set a priori at _P_ < .05.\n\n【21】Results\n\n【22】Most study participants were white (79%), female (56%), retired (72%), and married or living with a partner (56%). Slightly more than a quarter of the participants were obese (28%), reported difficulty in walking (27%), and had poor or fair health (26%). In relation to falls, almost a third of participants (29%) had a fall in the past 12 months, and 10% had a serious injury resulting from the fall. Less than a tenth of participants reported a history of CKD (6.1%). A comparison of adults aged 65 or older with and without CKD revealed differences based on demographic characteristics, lifestyle behaviors, and comorbid conditions between adults with and without CKD .\n\n【23】In a logistic regression analysis , we found that people with CKD were more likely to report having falls (OR = 1.81; 95% CI, 1.63–2.01), even after adjusting for demographics, lifestyle behaviors, and comorbid conditions (adjusted odds ratio \\[AOR\\] = 1.26; 95% CI, 1.13–1.47). Moreover, 37.4% of those who fell had a fall-related injury, with injuries occurring more frequently among people with CKD (OR = 1.50; 95% CI, 1.27–1.78), even after adjusting for demographic characteristics, lifestyle behaviors, and comorbid conditions (AOR = 1.23; 95% CI, 1.04–1.40).\n\n【24】Among patients with CKD, men were significantly less likely than women to fall (AOR = 0.79; 95% CI, 0.65–0.93) and have fall-related injuries (AOR = 0.59; 95% CI, 0.44–0.80), after adjusting for race and age . Having a diagnosis of diabetes was associated with an increased likelihood of falling (AOR = 1.25; 95% CI, 1.02–1.53), and the length of time since diabetes diagnosis was associated with both falls and fall-related injuries. People with CKD and arthritis were more likely to fall (AOR = 1.79; 95% CI, 1.46–2.20) and have fall-related injuries (AOR = 1.54; 95% CI, 1.06–2.24), whereas people with cancer were more likely to have a fall-related injury only (AOR = 1.50; 95% CI, 1.04–2.15). Lifestyle factors such as engaging in recent exercise (AOR = 0.68; 95% CI, 0.56–0.81) and limited physical function (assessed as difficulty in climbing stairs) (AOR = 2.84; 95% CI, 2.30–3.44) were most strongly associated with both falls and fall-related injuries.\n\n【25】Discussion\n\n【26】We used secondary data from the 2014 BRFSS to study the relationship between falls in noninstitutionalized adults with and without CKD. After accounting for multiple demographic characteristics, lifestyle factors, and chronic disorders, people with CKD aged 65 and older had a higher prevalence of falls and fall-related injuries than those without CKD. Among people with CKD, multiple lifestyle factors (eg, not currently engaging in physical activity, difficulty climbing stairs) and comorbid conditions (diabetes, diabetes duration, cancer, and arthritis) were found to significantly influence the probability of falls and fall-related injuries. Factors related to exercise and physical function were most closely related to falls and fall-related injuries, suggesting that these may be potential targets of fall-prevention strategies in older adults with CKD.\n\n【27】Our findings are consistent with previous reports that found that patients with CKD  and those who have advanced to end-stage kidney disease (ESKD)  are at increased risk of falling. Numerous physiological changes associated with CKD, such as uremic neuropathy and muscle wasting and weakness may explain the increased risk of falling. Furthermore, changes in bone and mineral metabolism leading to weak, brittle bones may lead to an increased propensity for fall-related injuries in people with CKD, especially those with ESKD . The increased risk of falls and fall-related injuries is a significant finding because of its strong relationship with poor clinical outcomes .\n\n【28】Previous studies of people with CKD identified numerous risk factors for falls, including age, sex, body weight, and education . However, these studies were primarily of small prospective cohorts and produced conflicting results in relation to some risk factors, such as sex. In our study, we used a large sample of US adults aged 65 and older and found that women with CKD were more likely than men to fall or have a fall-related injury. This is similar to the overall population of adults in the age group from BRFSS in which women were also found to be at greater risk of both falls and fall-related injuries than men . Furthermore, we did not find some previously identified demographic factors, such as BMI and education, to be significant predictors of falls in our study sample.\n\n【29】Previous studies also identified diabetes, a leading cause of CKD, as a risk factor for falls among people with CKD . Peripheral diabetic neuropathy is a common complication associated with poor glycemic control and can lead to balance and gait impairment, especially in activities such as walking, climbing, and descending stairs. We previously reported greater gait impairments in CKD patients undergoing hemodialysis who also had diabetes compared with those who did not have diabetes . Despite these observations, previous studies of fall risk have not accounted for the duration of diabetes. In our study, we found that people with CKD aged 65 or older who were diagnosed with diabetes before age 65, and therefore would have likely lived with diabetes for longer than those diagnosed after 65, were more likely to suffer a fall or fall-related injury. This finding suggests that, similar to the non-CKD populations , the physiological changes associated with diabetes that lead to increased injury risk may take time to manifest and that early prevention and management of diabetes may reduce the risk of falls and fall-related injuries in older adults.\n\n【30】Although not as common as diabetes, cancer and arthritis increase fall risk in the general population and are common chronic disorders among people with CKD . In our study, we found that among people aged 65 and older, having cancer and CKD did not increase the risk of falling but did increase the likelihood of suffering a fall-related injury. This increase in injury risk may be due in part to the effect of cancer treatments on bone strength .\n\n【31】Factors related to poor health, reduced physical functioning, and chronic diseases had the largest influence on the probability of both falls and fall-related injuries. For example, a diagnosis of arthritis was associated with an increased likelihood of having both a fall and a fall-related injury. Frailty, a condition consisting of fatigue, weakness, and reduced physical activity, is a strong predictor of falls in both the elderly population and in people with CKD who have advanced to ESKD . A common strategy to prevent falls is improving strength and balance. We found that difficulty climbing stairs, a task that requires strength and balance , was most closely related to falls and fall-related injuries among people with CKD. Furthermore, the absence of exercise in the past month was also a strong predictor of falls and fall-related injuries. Physical function and exercise are potentially modifiable, cost effective, and evidence-based strategies available to enhance mobility. Our study suggests that as in other populations, exercise programs that target strength and balance may be an effective strategy for preventing falls and fall-related injuries among people with CKD, but prospective trials are needed .\n\n【32】Our study has several limitations. First, the BRFSS questionnaire relies on self-reported health and lifestyle factors. Self-reporting may have limitations such as recall bias, social desirability, and over- or underestimation of health-related variables. Second, this study was cross-sectional, and we cannot establish cause and effect relationships between variables. Third, BRFSS is a closed-format survey, limiting internal validity. Also, few BRFSS items measured CKD and falls, resulting in improper estimation of the nature and extent of CKD and falls. Finally, CKD is a complex phenomenon with a multitude of influences on disease causation and prognosis. Variables that may influence CKD outcome or progression were not captured (eg, diet and nutritional status). People with CKD may not always be available to answer questionnaires such as BRFSS (eg, because of cognitive disabilities or hospitalization), limiting the external validity of our results and the ability to generalize our findings to all elderly adults with CKD. Despite these limitations, our study has several strengths. To our knowledge, this is the largest survey to examine risk factors for falls and fall-related injuries among people aged 65 and older with CKD. Furthermore, this study analyzed several demographic factors and lifestyle behaviors, which adds to the body of knowledge pertaining to CKD and falls in older adults.\n\n【33】The results of our study show that people with CKD have a higher likelihood of falling and having fall-related injuries. However, potentially modifiable factors such as recent exercise and difficulty climbing stairs were most closely related to falls and fall-related injuries. These findings suggest that among elderly people with CKD, as among other elderly populations at risk for falls, poor physical function and balance may be appropriate targets of multifactorial fall-prevention strategies.\n\n【34】Tables\n------\n\n【35】 Table 1. Participant (N = 157,753) Characteristics, Study of Falls and Fall-Related Injuries Among US Adults Aged 65 or Older With Chronic Kidney Disease (CKD), Behavioral Risk Factor Surveillance System, 2014 a  \n\n| Variable | Total, N (%) | CKD, N (%), 9,116 | No CKD, N (%), 147,893 |\n| --- | --- | --- | --- |\n| **Demographic Characteristics** | **Demographic Characteristics** | **Demographic Characteristics** | **Demographic Characteristics** |\n| **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 59,746  | 3,547  | 55,924  |\n| Female | 98,007  | 5,569  | 91,969  |\n| **Race b** | **Race b** | **Race b** | **Race b** |\n| White | 132,276  | 7,431  | 124,845  |\n| African-American | 9,323  | 699  | 8,624  |\n| Other | 3,957  | 254  | 3,703  |\n| Multiracial | 2,129  | 167  | 1,962  |\n| Hispanic | 6,583  | 406  | 6,177  |\n| **Marital status b** | **Marital status b** | **Marital status b** | **Marital status b** |\n| Married/ living with a partner | 76,769  | 4,065  | 72,433  |\n| Separated/divorced | 23,031  | 1,430  | 21,475  |\n| Widowed | 49,687  | 3,154  | 46,236  |\n| Never married | 7,212  | 419  | 6,750  |\n| **Employment b** | **Employment b** | **Employment b** | **Employment b** |\n| Employed for wages | 15,544  | 501  | 15,043  |\n| Self-employed | 8,707  | 316  | 8,391  |\n| Retired | 112,707  | 6,695  | 106,012  |\n| Other (unable to work, out of work,student, homemaker) | 18,567  | 1,535  | 17,032  |\n| **Education b** | **Education b** | **Education b** | **Education b** |\n| ≤High school graduate | 65,267  | 4,005  | 61,262  |\n| \\>High school but <college graduate | 40,050  | 2,445  | 37,605  |\n| ≥College graduate | 50,533  | 2,601  | 47,932  |\n| **Physical Function, Health, and Lifestyle Factors** | **Physical Function, Health, and Lifestyle Factors** | **Physical Function, Health, and Lifestyle Factors** | **Physical Function, Health, and Lifestyle Factors** |\n| **General health b** | **General health b** | **General health b** | **General health b** |\n| Excellent | 20,506  | 335  | 20,143  |\n| Very good | 47,064  | 1,353  | 45,587  |\n| Good | 51,967  | 2,835  | 48,881  |\n| Fair | 26,449  | 2,654  | 23,609  |\n| Poor | 10,972  | 1,895  | 8,940  |\n| **Access to health care** | **Access to health care** | **Access to health care** | **Access to health care** |\n| Yes | 154,846  | 8,976  | 145,870  |\n| No | 1,789  | 124  | 1,665  |\n| **Current smoker** | 12,736  | 650  | 12,086  |\n| **Heavy drinker (men >2 drinks/day; women >1 drink/day)** | 5,900  | 219  | 5,681  |\n| **Difficulty walking/climbing stairs b** | 40,615  | 4,471  | 36,198  |\n| **Engaged in any exercise** **in past month b** | 108,953  | 5,189  | 103,764  |\n| **Obese (BMI ≥30) b** | 39,566  | 3,162  | 36,404  |\n| **Chronic conditions in addition to CKD (ever diagnosed)** | **Chronic conditions in addition to CKD (ever diagnosed)** | **Chronic conditions in addition to CKD (ever diagnosed)** | **Chronic conditions in addition to CKD (ever diagnosed)** |\n| Diabetes b | 32,429  | 3,591  | 28,838  |\n| Cancer | 27,133  | 2,441  | 24,692  |\n| Arthritis b | 84,017  | 6,344  | 77,673  |\n| **Had ≥1 falls in past year b** | 43,885  | 3,529  | 40,356  |\n| **Had fall-related injury in past year b** | 16,062  | 1,566  | 14,496  |\n\n【37】a  Not all BRFSS respondents answered the question about CKD. Percentages may not total 100% because of missing values. Percentages are rounded to the nearest whole number.  \nb  Significant differences between groups ( _P_ < .05).\n\n【38】 Table 2. Probability of Falls and Fall-Related Injuries, Among US Adults (N = 157,753) Aged 65 or Older With Chronic Kidney Disease (CKD) (N = 9,116) and Without CKD (N = 147,893), Behavioral Risk Factor Surveillance System, 2014 a  \n\n| Predictors | OR (95% CI) Falls | OR (95% CI) Fall-Related Injury |\n| --- | --- | --- |\n| **Model 1.** Compares CKD group vs non-CKD group | 1.81 (1.63–2.01) b | 1.50 (1.27–1.78) b |\n| **Model 2.** Comparison in Model 1 adjusted for demographic characteristics from Table 1 | 1.75 (1.58–1.94) b | 1.46 (1.24–1.72) b |\n| **Model 3.** Comparison in Model 1 adjusted for physical function, health, and lifestyle factors from Table 1 | 1.36 (1.21–1.53) c | 1.26 (1.08–1.44) c |\n| **Model 4.** Comparison in Model 1 adjusted for chronic conditions from Table 1 | 1.53 (1.38–1.70) b | 1.42 (1.20–1.69) b |\n| **Model 5.** Comparison in Model 1 adjusted for demographic characteristics and physical function, health, and lifestyle characteristics from Table 1 | 1.32 (1.18–1.48) c | 1.25 (1.06–1.49) c |\n| **Model 6.** Comparison in Model 1 adjusted for demographic, physical function, health/lifestyle characteristics and chronic conditions | 1.26 (1.13–1.47) c | 1.23 (1.04–1.40) c |\n\n【40】Abbreviations: CI, confidence interval; OR, odds ratio.  \na  Not all participants responded to the question about CKD.  \nb  Indicates _P_ < .001.  \nc  Indicates _P_ < .01.\n\n【41】 Table 3. Predictors of Falls and Fall-Related Injuries in People With Chronic Kidney Disease, Behavioral Risk Factor Surveillance System,\n\n| Predictors | AOR (95% CI) a Falls | AOR (95%CI) a Fall-Related Injury |\n| --- | --- | --- |\n| **Men versus women** | 0.79 (0.65–0.93) b | 0.59 (0.44–0.80) |\n| **Married/ living with a partner versus other c** | 0.96 (0.78–1.17) | 0.90 (0.69–1.08) |\n| **Education** | **Education** | **Education** |\n| **≤** High school graduate | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| \\>High school but <college graduate | 1.37 (0.98–1.76) | 1.17 (0.81–1.72) |\n| ≥College graduate | 1.01 (0.83–1.22) | 0.72 (0.54–0.90) b |\n| **Any exercise last month, yes versus no** | 0.68 (0.56–0.81) b | 0.70 (0.60–0.93) b |\n| **Difficulty climbing/walking stairs, yes versus no** | 2.84 (2.30–3.44) b | 1.70 (1.27–2.30) b |\n| **Obese or overweight versus normal weight d** | 0.87 (0.67–1.12) | 0.86 (0.60–1.27) |\n| **Heavy drinker** **versus others e** | 1.54 (0.89–2.98) | 1.19 (0.93–2.07) |\n| **Current smoker versus others f** | 1.21 (0.85–1.71) | 1.08 (0.90–1.44) |\n| **Has diabetes versus does not have diabetes** | 1.25 (1.02–1.53) b | 1.07 (0.81–1.44) |\n| **Diabetes diagnosed** **≤64 y versus diagnosed ≥65 y** | 1.45 (1.04–2.02) b | 1.62 (1.08–2.53) b |\n| **Arthritis history, yes versus no** | 1.79 (1.46–2.20) b | 1.54 (1.06–2.24) b |\n| **Cancer history, yes versus no** | 1.03 (0.81–1.30) | 1.50 (1.04–2.15) b |\n\n【43】Abbreviations: AOR, adjusted odds ratio; CI, confidence interval.  \na  Indicates adjustments made for race and age. The outcome is falls and fall-related injuries in the past 12 months (yes vs no).  \nb  Indicates _P_ < .01.  \nc  Includes widowed, divorced, separated, and never married.  \nd  People with BMI ≥25 were categorized as overweight or obese, and people with BMI from 18.5 to <25 were categorized as normal weight.  \ne  Men who regularly consume more than 2 drinks per day and women who regularly consume more than 1 drink per day. Others were those who consumed fewer drinks or did not drink alcohol at all.  \nf  Includes nonsmokers and former smokers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f1571c75-a478-4750-90d2-547d845a735b", "title": "Rise in Babesiosis Cases, Pennsylvania, USA, 2005–2018", "text": "【0】Rise in Babesiosis Cases, Pennsylvania, USA, 2005–2018\nBabesiosis, a tickborne infection caused by protozoan parasites that infect erythrocytes, has been identified as an emerging infection of concern in the state of Pennsylvania, USA . Infection with _Babesia_ parasites can cause a range of symptoms, including fever, myalgias, and fatigue. Although many patients are asymptomatic, the infection can be severe in some persons, especially those who are >50 years of age, immunosuppressed, or asplenic .\n\n【1】The most common species known to cause human infection in the United States is _Babesia microti_ , which is transmitted by the _Ixodes scapularis_ tick (blacklegged tick). Transmission can also occur through blood transfusion; _B. microti_ is currently the most common pathogen transmitted through the blood supply in the United States . The highest incidence of human infection has been reported in the Northeast and upper Midwest states . Until fairly recently, few cases of _B. microti_ infection have occurred in Pennsylvania. However, new data suggest not only increased prevalence of ticks harboring _B. microti_ within the state  but also a rise in the number of cases seen in clinical practice .\n\n【2】Babesiosis is not a mandatory reportable infection in Pennsylvania; however, the Pennsylvania Department of Health does receive reports of cases of babesiosis from healthcare providers that elect to do so. Among these recounted cases, the department has seen a 20-fold increase in the past 12 years . A similar trend was documented in a recent study from February 2019 involving a 4-hospital system in southeastern Pennsylvania, where clinicians saw an increase in cases from < 7 cases annually during 2008–2014 to 26 cases in 2015 .\n\n【3】We have suspected a similar increase in the number of cases at our institution, Penn State Milton S. Hershey Medical Center (Hershey, PA, USA), which is a tertiary academic center located in central Pennsylvania. We performed a retrospective review of all of the confirmed cases of babesiosis at our institution for the period 2005–2018 to determine whether we were truly seeing an increased number of cases and to highlight the demographic and clinical characteristics of these patients.\n\n【4】### Methods\n\n【5】We obtained a list of all patients who had International Classification of Diseases, 9th Revision (ICD-9) (088.82), and International Classification of Diseases, 10th Revision (ICD-10) (B60.0), diagnostic codes for babesiosis as well as patients that had _Babesia_ serologic tests ordered at Hershey Medical Center during 2005–2018. The list consisted of 352 patient encounters, some of which were duplicates. We retrospectively chart reviewed each patient encounter to identify confirmed cases of babesiosis. Only patients who met Centers for Disease Control and Prevention criteria for confirmed cases of babesiosis were included in our study : patients who had confirmatory laboratory results (i.e. parasite seen on peripheral smear, positive PCR from blood, or both) and met \\> 1 of the objective or subjective clinical evidence criteria. Although we did not use positive serologic test results as a part of our diagnostic criteria, identifying patients who had serologic tests ordered was used to increase the number of patients in our initial cohort. We noted inconsistencies in the way blood smears and PCR testing were ordered in our electronic medical records, and we wanted to ensure we did not miss any cases.\n\n【6】The _Babesia_ serologic testing used at Hershey Medical Center is the indirect fluorescent antibody (IFA) test. This test is specific for _B. microti_ species. We considered a titer \\> 1:256 to be positive on the basis of the titer value determined to be supportive of the diagnosis of babesiosis according to Centers for Disease Control and Prevention laboratory criteria . From our initial list of 352 patient encounters, we identified 8 cases of confirmed babesiosis. We maintained demographic, clinical, and laboratory data in a REDCap Electronic Database . Research protocols were reviewed and approved by the Penn State College of Medicine Institutional Review Board.\n\n【7】### The Patients\n\n【8】##### Demographic Characteristics\n\n【9】Of the 8 confirmed cases of babesiosis seen at our institution, 7 of the cases were acquired in the state of Pennsylvania . One case was thought to have been acquired in Massachusetts. Of the 7 cases in Pennsylvania, more than half (4/7) were acquired in south-central Pennsylvania counties; 2 cases were from northeast counties, and 1 case was from a southeast county . No cases were reported during 2005–2010, 1 case was reported in 2011, 1 case in 2015, 2 cases in 2017, and 4 cases in 2018. All cases were diagnosed during the summer months. All but 1 of the patients were \\> 60 years of age at the time of diagnosis. The median age at time of diagnosis was 70 years (range 20–77 years), and 75% of the patients were male. Only 1 patient reported a history of tick bite preceding infection. Six of 8 patients reported history of outdoor activity before seeking care. None of the patients had history of recent blood transfusion.\n\n【10】##### Clinical Manifestations\n\n【11】The cohort included 2 patients with a history of splenectomy and 1 patient with a history of diabetes. Most of the patients were immunocompetent. None of the patients were on immunosuppressive therapy. No HIV patients or posttransplant patients were in the cohort. Most patients reported fever (6/8) and malaise (5/8). Other symptoms included myalgias or arthralgias (2/8), anorexia (2/8), rash (1/8), headache (1/8), nausea or vomiting (1/8), diarrhea (1/8), and respiratory failure (1/8). The average time from symptom onset to diagnosis was 9.7 days. The most common laboratory abnormalities seen were anemia, thrombocytopenia, transaminitis, and hyperbilirubinemia. Anemia was seen in all of the patients; average hemoglobin level was 9.8 g/dL (reference range 13–16 g/dL) . Thrombocytopenia was seen in 7/8 patients; average platelet count was 90.8 × 10 9  /L (reference range 150–350 × 10 9  /L). Most of the patients (6/8) had platelet counts of < 75\\. Elevated alanine aminotransferase and aspartate aminotransferase were seen in 7/8 patients; average alanine aminotransferase was 72 U/L (reference range 0–40 U/L) and average aspartate aminotransferase 176.6 U/L (reference range 0–40 U/L). Hyperbilirubinemia was also seen in 7/8 patients; average bilirubin level was 6.4 mg/dL (reference range 0–1.2 mg/dL).\n\n【12】##### Diagnosis and Treatment\n\n【13】All 8 patients had blood smears that were positive for identification of intraerythrocytic _Babesia_ organisms . Most (6/8) patients had an initial parasitemia of >10% (average parasitemia 18%). Three patients had PCR results obtained, all of which were positive. Six of the 8 patients had serologic test results obtained, and serum samples from all 6 patients were reactive for IgG (titers \\> 1:256).\n\n【14】Concurrent Lyme disease was noted in half (4/8) of patients. Patients were screened for Lyme disease by using ELISA; if the result was positive, then a Western blot was performed. Patients had Lyme disease diagnosed if they had positive ELISA results and positive IgM or IgG results on Western blot.\n\n【15】Most (7/8) patients received a combination of azithromycin and atovaquone for treatment. Three patients received clindamycin and quinine as part of their treatment; of these 3 patients, 1 patient received clindamycin and quinine alone for the duration of their therapy, and 2 patients were switched to azithromycin and atovaquone because of persistent parasitemia. Two of the patients who received clindamycin and quinine (1 of whom was switched to azithromycin and atovaquone) also required blood or platelet transfusions. Five patients underwent red cell exchange transfusions. The average duration of treatment was 18.1 days. The average duration of parasitemia was 9 days, but we only had exact date of clearance for 3 of the 8 patients.\n\n【16】##### Hospital Course and Complications\n\n【17】Patients were identified as having severe babesiosis if they had parasitemia >10% or if they had intensive care unit (ICU) care, exchange transfusion requirement, intubation, acute respiratory distress syndrome, shock, or dialysis . Six of the 8 patients were classified as having severe infection. These 6 patients all had parasitemia >10%. Five of the 8 patients required ICU care and underwent exchange transfusions. One patient required dialysis, and 2 patients required blood or platelet transfusion. No patients required intubation or pressor support, and all patients survived.\n\n【18】Four of the 6 patients with severe infection had co-infection with _Borrelia burgdorferi_ (Lyme disease). The 2 nonsevere patients did not have co-infection. All 6 of the severely ill patients had infectious disease consults during their hospitalization. The 2 patients that were considered not severely ill did not receive an infectious disease consultation. The average length of stay for all patients was 11 days. The average length of stay for patients with severe infection was 12.3 days and for patients with nonsevere infection was 7 days.\n\n【19】### Discussion\n\n【20】Our findings further suggest that babesiosis is an emerging infection in the state of Pennsylvania. Data from our institution as well as the Pennsylvania Department of Health show a clear trend toward increasing cases throughout the state . In our study, 7 of the 8 reported cases were thought to be acquired in Pennsylvania. There were no reported cases during 2005–2010 and 1 case in 2011, followed by a steady rise in cases until 2018, when our institution saw 4 cases. The distribution of the areas of suspected infection acquisition makes us question whether we might be seeing a further expansion into central Pennsylvania over time. Over half (4/7) of the cases seen at Hershey Medical Center were acquired in southcentral Pennsylvania counties, and all 4 of those cases were seen most recently in the year 2018 (contrary to previous years, when cases occurred in northeast and southeast counties). These southcentral counties were determined by the home address ZIP codes of infected patients who had no history of travel before seeking care. Previous studies highlighting cases of babesiosis in Pennsylvania included patients from northeast and southeast counties only ( _2_ – _4_ ). Whether a westward expansion of babesiosis is truly occurring within the state is difficult to conclude on the basis of our small sample size; nonetheless, these findings warrant further inquiry, and we would be interested to see if other institutions in the south-central region have noted a similar trend. Because most persons infected with _B. microti_ are either asymptomatic or have mild symptoms and diagnostic testing is often not obtained, the actual number of babesiosis cases at our institution was most likely underrepresented. In addition, given our methodology of focusing specifically on babesiosis-related codes in ICD-9 and ICD-10, we might have missed patients that were presumed to be co-infected (i.e. having both Lyme and babesiosis) and treated empirically.\n\n【21】Babesiosis is currently considered endemic in the US states of Connecticut, Massachusetts, Minnesota, New Jersey, New York, Rhode Island, and Wisconsin . Surveillance data shows a steady rise in babesiosis cases throughout the United States and further geographic expansion . When examining neighboring states that are currently endemic for babesiosis, specifically New York and New Jersey, we observed a historical pattern of expansion that might be indicative of what is to come in the state of Pennsylvania. Both states were endemic for Lyme disease, which shares the same vector as babesiosis (blacklegged ticks), before seeing the rise in babesiosis cases .\n\n【22】One study suggests that geographic spread of _B. microti_ is favored by prior establishment of _B. burgdorferi_ (the agent responsible for Lyme disease) and that co-infection (in mice reservoirs) with _B. burgdorferi_ increases the likelihood of _B. microti_ transmission . A paper from 1998 documented the presence of _I. scapularis_ ticks in 49 of the 67 counties in Pennsylvania ; however, a more recent study from 2015 identified the presence of the tick in all 67 counties . This same study reported tick infection rates of 47.4% for _B. burgdorferi_ , 3.5% for _B. microti_ , and 3.3% for _Anaplasma phagocytophilum_ . This 3.5% infection rate by _Babesia_ was higher than a previously reported rate of 0.7% in a 2010 report . The reasons for this suspected expansion are thought to be multifactorial, including the results of climatic effects on tick populations, growth of deer populations, and incursion into tick and deer habitats by humans .\n\n【23】When evaluating the patients treated at Hershey Medical Center, we found that our patients were demographically similar to patients described in prior studies; most patients in our investigation were male (75%) and elderly (median age 70 years). Only 1 patient reported a history of tick bite preceding infection, but most (75%) reported a history of outdoor activity. This finding further stresses the importance of having a high clinical suspicion in elderly patient populations despite absence of tick bite history.\n\n【24】The average number of days of symptoms before diagnosis was ≈10 days, which is a slightly shorter period compared with reports from other studies  and might have been because all patients had blood smears obtained early on during admission. The reason for obtaining blood smears was predominately for work-up of new anemia or thrombocytopenia. Most (75%) patients were identified as having severe infection; 6 of the 8 had parasitemia >10%. Five of these 6 patients required ICU care and underwent exchange transfusion. The purpose of ICU care in these patients was for close monitoring and exchange transfusion. No patients required pressor support or mechanical ventilation. The exchange transfusion process consists of removing a patient’s red blood cells and replacing them with donor red blood cells and is recommended for babesiosis patients who have parasitemia ≥10%; severe hemolysis (hemoglobin < 10 g/dL); or pulmonary, hepatic, or renal impairment . No randomized trials have evaluated the efficacy of red cell exchange therapy; recommendations for this treatment are based on case series that show that parasitemia can be reduced by >50%–90% with red cell exchange therapy . Of the patients who required ICU care and exchange transfusion, only 1 of the patients was considered immunocompromised because of history of splenectomy; 1 other patient with severe infection had a history of splenectomy but did not require exchange transfusion. The remaining patients were considered immunocompetent, including 1 patient with diabetes. The unifying risk factor for most patients was older age. The high percentage of patients with severe infection was attributed to our hospital being a large tertiary academic center that received referrals from other hospitals. Some of the cases were referred specifically for evaluation for exchange transfusion. Co-infection with _B. burgdorferi_ might have also contributed to severity of infection, given that 4 of the 6 patients with severe infection had serologic test results indicative of Lyme disease. Prior studies have shown increased disease severity and duration of illness in patients co-infected with _B. burgdorferi_ and _Babesia_ , which is consistent with the findings in our study.\n\n【25】No deaths occurred in our study cohort. Average length of hospital stay was 11 days. Most patients (7/8) received treatment with azithromycin and atovaquone. Three patients received clindamycin and quinine; 1 patient received clindamycin and quinine alone for the duration of their therapy, and 2 patients were switched to azithromycin and atovaquone because of persistent parasitemia. These 2 patients improved with the azithromycin and atovaquone regimen; however, this outcome might have been attributable to their having undergone exchange transfusion. Historically, clindamycin and quinine was the regimen of choice for the treatment of _B. microti_ infection . Later, azithromycin and atovaquone became recommended for mild to moderate disease because the regimen was shown to be as effective as the combination of clindamycin and quinine and had fewer adverse effects . Most recently, a study from 2017 suggested that azithromycin plus atovaquone was equally effective for patients with severe infection .\n\n【26】### Conclusions\n\n【27】Ours is yet another article highlighting the emergence of babesiosis in the state of Pennsylvania. Given the nonspecific signs and symptoms associated with the illness and the potential severity of infection, especially in our elderly population, we believe that increased awareness and reporting of this infection is necessary. Clinicians must maintain a high index of suspicion in patients with a nonspecific febrile syndrome despite absence of tick bite history or lack of an immunocompromising condition. Evaluation for co-infections, particularly co-infection with _B. burgdorferi_ , should be considered given patients with co-infection appear to have more severe disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1afd89a0-3876-42ed-8125-b694d487a69b", "title": "A Mismatch Between Patient Education Materials About Sickle Cell Disease and the Literacy Level of Their Intended Audience", "text": "【0】A Mismatch Between Patient Education Materials About Sickle Cell Disease and the Literacy Level of Their Intended Audience\nAbstract\n\n【1】**Introduction**\n\n【2】Despite the first goal of the 2010 National Action Plan to Improve Health Literacy, the literacy demands of much health information exceeds the reading skills of most US adults. The objective of this study was to assess the health literacy level of publicly available patient education materials for people with sickle cell disease (SCD).\n\n【3】**Methods**\n\n【4】We used 5 validated tools to evaluate 9 print and 4 online patient education materials: the simple measure of gobbledygook (SMOG) to assess reading grade level, the Peter Mosenthal and Irwin Kirsch readability formula (PMOSE/IKIRSCH) to assess structure and density, the Patient Education Materials Assessment Tool (PEMAT) to assess actionability (how well readers will know what to do after reading the material) and understandability, the Centers for Disease Control and Prevention’s (CDC’s) Clear Communication Index (Index) to obtain a comprehensive literacy demand score, and the Printed Cancer Education Materials for African Americans Cultural Sensitivity Assessment Tool.\n\n【5】**Results**\n\n【6】Materials’ scores reflected high reading levels ranging from 8th grade to 12th grade, appropriate (low) structural demand, and low actionability relative to understandability. CDC suggests that an appropriate Index score should fall in or above the 90th percentile. The scores yielded by materials evaluated in this assessment ranged from the 44th to the 76th percentiles. Eight of the 13 materials scored within the acceptable range for cultural sensitivity.\n\n【7】**Conclusion**\n\n【8】Reading levels of available patient education materials exceed the documented average literacy level of the US adult population. Health literacy demands should be a key consideration in the revision and development of patient education materials for people with SCD.\n\n【9】Introduction\n\n【10】Public health’s population-based strategies for improving community health include outreach to and communication with vulnerable populations. Patients, their families, and their communities need clear, understandable information; therefore, clear communication is a component of public health’s mission at the national, regional, state, and local levels . As indicated by the 2010 National Action Plan to Improve Health Literacy, accessible health information is key to promoting population health .\n\n【11】The first goal of the 2010 National Action Plan to Improve Health Literacy calls for the development and dissemination of health and safety information that is accurate, accessible, and actionable . However, surveys conducted by the US Department of Education and by the Organisation for Economic Co-operation and Development (OECD) indicate that large proportions of adults in the United States and in most industrialized nations have difficulty understanding commonly available written information . The most recent assessment of adult literacy skills indicates that more than half of US adults have difficulty using print materials and basic arithmetic in everyday activities and tasks . Each wave of literacy assessments of US adults indicated that minority population groups were more likely than majority population groups to have limited literacy skills . Approximately 80% of people with sickle cell disease (SCD) in the United States identify as black . The 2003 National Assessment of Adult Literacy (NAAL) found that only 2% of US black adults were proficient in general literacy skills compared with 13% of the general population . The 2006 NAAL subreport, which addresses health literacy, found that 12% of US adults were proficient in prose, document, and numeric health compared with only 2% of black adults . Although easily understandable health information should be accessible to everyone, special consideration should be given to making health information accessible to population groups with documented low literacy skills — those living in poverty and in under-resourced areas, members of minority population groups, and members of immigrant populations.\n\n【12】Unfortunately, as more than 2,000 peer reviewed studies showed, health information is often inaccessible because materials are written at reading levels that exceed the literacy skills of most US adults . Furthermore, one study in health literacy indicated that people with limited literacy were more likely to experience diminished health outcomes . The mismatch between the literacy skills of patients and the literacy demands of health education materials and instructions may play a significant role in enabling or inhibiting people to make healthful choices .\n\n【13】Insights from health literacy studies are directly applicable to public health’s mission to improve the health of communities and the prevention and management of chronic diseases. SCD is a significant concern among the many issues addressed in public health practice. This disease is disproportionately experienced by people of African, Mediterranean, or Latin descent  and affects an estimated 90,000 to 100,000 people in the United States . Several acute and chronic complications are associated with SCD, requiring complex disease management in both home and clinical settings. However, SCD patients in the United States have notably diminished comprehensive care services available to them, relative to other genetic disorders . Thus, patients with SCD and their family members could benefit from having appropriate educational materials about treatment options and procedures to help them in planning and making decisions . However, studies and investigations related to health literacy and SCD are absent from the literature. This study examines the literacy level required for use of available SCD educational materials (literacy demand) and the cultural appropriateness of such information for the intended audience.\n\n【14】Methods\n\n【15】The lead author (E.M.) conducted an initial search of the literature to find widely available educational materials related to SCD. Because state health departments, public hospitals, and other public institutions frequently rely on free print and electronic information provided by national organizations when choosing educational materials for families and communities, we evaluated only free materials. We identified materials through the PubMed database by using the MeSH term “sickle cell anemia” or one of the key terms, “sickle cell” or “sickling,” in the title and abstract and the publication type, “patient education.” The educational materials were reviewed and were excluded if they were not written in English, if they were not free, if they were not intended for use in the United States, or if they did not meet the topic criterion of patient education. These criteria yielded 9 print and 4 online materials, which we then reviewed for literacy and numeracy demand (ie, the level of literacy and facility with arithmetic required for readers to understand the material and take appropriate action based on what they read).\n\n【16】We analyzed the educational materials by using several tools and guidelines to assess the characteristics of the materials . The simple measure of gobbledygook (SMOG) readability test was used to ascertain school grade reading level. SMOG determines grade level with attention to both word and sentence length . The PMOSE/IKIRSCH measure (named for the individuals who developed the measure, Peter B Mosenthal and Irwin S. Kirsch) was used to assess structure and density in displays (lists, charts, graphs) contained in the materials . PMOSE/IKIRSCH provides a grade-level score based on the complexity of documents and the presence of all needed information within the confines of the document. A series of bullet points was considered a list if the content of each bullet point was less than a complete sentence.\n\n【17】We used the Patient Education Materials Assessment Tool (PEMAT) to evaluate the understandability and actionability of materials . _Understandability_ is the degree to which people with low health literacy can interpret key messages, and _actionability_ is the degree to which people can know the proper next steps (eg, when and where to seek care, which healthful behaviors to adopt) on the basis of the information provided . In addition, the Centers for Disease Control and Prevention’s (CDC’s) Clear Communication Index (Index) was used to provide one overall outcome measure of readability and audience appropriateness, which is an amalgamation of measures related to the relevance of 1 to 4 factors — core items, behavioral recommendations, numbers, and risk . Because minority racial/ethnic groups make up most of the SCD patient population, the Index stresses that analyses and discourse must examine issues of culture in addition to, and in the context of, health literacy. The Index assesses appropriateness for the audience and, therefore, cultural appropriateness. However, to more specifically assess the materials’ cultural appropriateness, we also applied the Printed Cancer Education Materials for African Americans Cultural Sensitivity Assessment Tool (AACSAT). This tool measures acceptability in the cultural domain that the other measurement tools we used do not address, in the format, visual message, and written message .\n\n【18】Three authors (E.M. J.N. K.V.) independently scored all materials with each tool. As part of a training in health literacy theory and reading level measurement in a graduate course at the Harvard TH Chan School of Public Health, each of these 3 authors reviewed a set of materials unrelated to the subject matter and presented their findings to a panel of 8 experts trained in health literacy theory to analyze any differences, resolve errors, and establish consensus. Thereafter, each of the study materials was read and assessed by each of the 3 authors independently, and inconsistencies were resolved in group meetings of all authors. The PEMAT, Index, and AACSAT components each contain value judgments bounded by standardized parameters. For these assessments, the average of the reviewer scores was reported. When the 2 initial scores differed by more than 10%, the scores were discussed and recalculated by all 3 authors. The lead author (E.M.) was solely responsible for the AACSAT.\n\n【19】Results\n\n【20】The PubMed search returned 6 sources for patient education information of which 3 sources were eliminated during initial review because they were not written in English, were not available free of charge, were not intended for use in the United States, or were not related to patient education. The 3 remaining sources of patient education materials were CDC , the National Institutes of Health (NIH) , and the American Academy of Family Physicians (AAFP) . NIH and AAFP materials were Web-based, and the CDC materials were formatted for print distribution but were available online. The NIH publication was a multipage site with comprehensive outlines of risks, diagnoses, symptoms, and treatments . It included lists, charts, statistical graphs, and other graphics. AAFP, however, provided a 1-page synopsis of SCD for parents of children with a new diagnosis of SCD. Its website contained no charts, statistical graphs, or other graphics . Five of the 9 materials from CDC addressed self-management and complication prevention. Those 5 CDC materials ranged in length from 1 to 40 pages of text, and 2 of the 5 included photo images. Four of the 9 materials focused on disease overview and disease inheritance. They ranged in length from 1 to 2 pages, and they all contained photos. Two of them contained an image illustrating the inheritance pattern of SCD . Table 2 presents the scores from each of the 5 tools for each of the educational materials.\n\n【21】The SMOG scores of the materials we evaluated ranged from 8th grade to 12th grade reading level; most materials fell in the 10th grade to 12th grade range. A SMOG score of 7 or below is recommended for average readers. A score of 8 is generally assumed to represent the reading skills of average high school students . SMOG focuses on word and sentence length. Several sections of the assessed materials contained long complex sentences exceeding 3 lines of text.\n\n【22】Only 3 of the materials (printed or Web-based) contained lists, and none contained charts or statistical graphs. PMOSE/IKIRSCH assesses charts, statistical graphs, lists, and layout and is scored on a 17-point scale. All materials with lists received a PMOSE/IKIRSCH score indicating a low complexity level . Scores in this range are estimated to require an 8th-grade literacy level. As indicated by the PMOSE/IKIRSCH tool, use of simple lists with 1 heading and a limited number of items minimizes literacy demand .\n\n【23】Next, all 13 materials were scored with PEMAT. All materials scored above the 50th percentile in understandability, except the “How Is Sickle Cell Anemia Treated” NIH Web page . As noted, PEMAT evaluates understandability of materials (ie, patients from different backgrounds can understand the messages conveyed) and actionability (ie, patients from different backgrounds know what to do with the information), and scoring is based on clarity of purpose, organization, and difficulty of content (including numerical demand). PEMAT has no recommended cutoff for acceptable scores. Rather, the scores are meant to be used as relative indicators of quality when choosing between materials . Low PEMAT scores indicate that the materials assessed would be improved by simplifying content. For example, all numerical concepts should be presented in a way that does not require the reader to perform any calculations . Finally, all 13 materials scored low on PEMAT in actionability, relative to usability. Many of the materials discussed healthful behaviors but did not provide explicit instruction or diagrams demonstrating actions to be taken.\n\n【24】The Index yields a percentile score, and CDC states that documents with appropriate health literacy demand will score at the 90th percentile or higher . None of the materials received a score in the 90th percentile or higher, and some were below the 50th percentile . The Index has 7 main areas of interest, including 2 behavior- or action-oriented foci and 2 numeric concept-related foci. Similar to the recommendations drawn from PEMAT, low scores from the Index assessment indicate that the included materials could be improved with more behavioral and instructive language.\n\n【25】Finally, the AACSAT printed cancer education materials yielded scores in the acceptable range for 8 of the 13 materials evaluated . Scores were acceptable if materials achieved cultural relevance and appropriateness for the intended audience in format, content, and graphics . Acceptable scores from the AACSAT were obtained by meeting the audience-specific literacy demand requirements addressed in SMOG, PMOSE/IKIRSCH, PEMAT, and Index and inclusion of culturally relevant and modern imagery .\n\n【26】Discussion\n\n【27】The free patient education materials assessed are those derived from a PubMed search meant to capture documents that clinicians would be likely to recommend to patients, that came from government agencies or professional societies, and that public health departments are likely to disseminate through community organizations and clinics. The reading level of the materials assessed fell between 8th-grade and 12th-grade levels, and these scores are considered too high for the general US adult population . Problematic content associated with high scores can be mitigated with attention to vocabulary (ie, substitute short and common terms for long, unusual, or technical terms. For example, use _doctor_ instead of _physician_ ) and with attention to sentence length (ie, long sentences increase reading difficulty). The various charts and lists included in the assessed materials yielded literacy demand scores at appropriate levels. Scores related to the graphics explaining heredity of SCD, however, indicated high literacy and numeracy demand. Results also indicated a need for action-oriented language (eg, “Do X. Then do Y.” rather than something like “It is recommended that patients do X followed by Y.”) and instructions about what to do as a result of reading the materials. At the same time, findings indicated that some basic concepts were clearly described and that many of the materials contained culturally appropriate content.\n\n【28】Given the proportion of Americans proficient in quantitative literacy reported in the 2003 NAAL report and in numeracy reported in the 2013 OECD report, use of numeric content must be treated with caution . Broad assessments of the 13 materials evaluated suggest that the presentation of numeracy content was overly complex and lacked adequate explanation. The Index tool points out that an explanation with graphics is particularly important for mathematically complex concepts such as those displayed in the “inheritance tree” in patient materials related to risk and sickle cell trait . Insights at this level of detail demonstrate the strength of the Index, a more complex tool. The inheritance graphics and explanations of illustrations in educational materials for people with SCD require careful attention to ensure they are understandable by their intended audience.\n\n【29】Our assessment of the 13 patient education materials included in our analysis suggests that literacy and numeracy demands associated with use of these materials are likely to exacerbate difficulties people with SCD have in understanding patient education information designed to assist them in managing their disease. This may contribute to health literacy disparities and related health outcomes outlined by the National Action Plan to Improve Health Literacy . The educational materials addressing SCD receive scores that exceed recommended reading levels to ensure accessibility for the average American adult who has completed high school .\n\n【30】The content of the materials evaluated addressed some of the major issues related to SCD. Many issues that affect lifelong health of people with SCD, however, were not covered. Several patients lose primary and regular health care during the transition from pediatric to adult care . These ideas were covered in the publication “9 Steps to Living Well with Sickle Cell Disease in College,” but transition of care is not directly addressed in that publication . Additionally, management of SCD differs from childhood to adulthood, and no specific guidelines were offered for each stage of life in the materials assessed. Finally, perhaps the most complex issues in living with SCD relate to accessing information on pain management and routine care . The materials examined in this study did not address these issues directly.\n\n【31】Our study had 2 limitations. First, no patient group or review team apart from the authors was involved in the analysis of materials. Therefore, assessments were inadequate for drawing full conclusions regarding cultural relevance and appropriateness . However, AACSAT provided one indication of appropriateness of the patient education materials for their intended audience . Second, the sample was limited to free publications. Many educational materials for patients with SCD were developed by nongovernment organizations and by private institutions and were not necessarily free. Only free materials were included in this evaluation. Despite these limitations, this study provides an assessment of available materials through use of several different tools for assessing health literacy demand. Use of 5 different tools highlighted strengths and weaknesses of patient education materials in several domains of health literacy — organization, content, word choice, numeracy, usability, actionability, and cultural sensitivity.\n\n【32】Resulting scores for the 13 publications evaluated fell short of the standards articulated in the commonly referenced Doak, Doak, and Root text, _Teaching Patients with Low Literacy Skills_ , which covers educational theories and applies them to improving health communication , and in the 2010 National Action Plan to Improve Health Literacy . A better match between the literacy requirements of the materials and the known literacy levels of their audience would better address information access needs of patients with SCD. Matching materials to their audience would improve the ability of people with SCD to make decisions and take healthful action.\n\n【33】The results of both the preliminary literature search and materials assessments suggest that there is a shortage of available and appropriate published information for people with SCD. Study findings indicate that SCD patient education materials should be revised or developed to use language tailored to their intended audience . In addition, input from patient focus groups to address appropriateness and usefulness  is critical.\n\n【34】This study supports the importance of health literacy as a key consideration in the development and revision of patient education materials for people with SCD. Health departments should assess the suitability of materials they distribute in their communities. In so doing, health departments can use existing tools such as the Index and related documentation for analyses. An understanding of the literacy skills of US adults must help shape the development of important health information . An awareness of the existing mismatch between commonly available health materials and the literacy levels of the intended audience can help inform strategic decisions of public health professionals for dissemination of information.\n\n【35】Tables\n------\n\n【36】#####  Table 1. Comparison of Characteristics Captured by Four Tools for Assessing Health Literacy Demand\n\n| Literacy Assessment Tool |\n| --- |\n| SMOG | PMOSE/IKIRSCH | PEMAT | Index |\n| --- | --- | --- | --- |\n| **Organization** | **Organization** | **Organization** | **Organization** |\n| Tool does not assess this component | Simple-list structure | Material breaks or “chunks” information into short sections | Material uses bulleted or numbered lists |\n| Tool does not assess this component | Simple-list structure | Sections have informative headers | Material is organized in chunks with headings |\n| Tool does not assess this component | Combined-list structure (includes pie-charts and time-lines) | Presents information in logical sequence | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Intersected-list structure (includes bar charts, line graphs, and maps) | Provides a summary | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Nested-list structure (includes bar charts and line graphs with tested tables | Material uses visual cues | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Material contains a reasonable number of labels | Material uses visual aids | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Material contains a reasonable number of items | Visual aids reinforce content rather than distract | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Dependence (material) does not make reference to information in an outside document) | Visual aids have clear titles or captions | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Dependence (material) does not make reference to information in an outside document) | Material uses illustrations and photographs that are clear and uncluttered | Most important information is summarized in first paragraph or section |\n| Tool does not assess this component | Dependence (material) does not make reference to information in an outside document) | Material uses simple tables with short and clear row/column headings | Most important information is summarized in first paragraph or section |\n| **Content of Main Message** | **Content of Main Message** | **Content of Main Message** | **Content of Main Message** |\n| Tool does not assess this component | Tool does not assess this component | Purpose is evident | Material contains one main message |\n| Tool does not assess this component | Tool does not assess this component | No distractors | Main message is at the top, beginning, or front |\n| Tool does not assess this component | Tool does not assess this component | No distractors | Main message is emphasized with a visual cue |\n| Tool does not assess this component | Tool does not assess this component | No distractors | Material contains visual(s) that convey or support the main message |\n| Tool does not assess this component | Tool does not assess this component | No distractors | Material explains what is known or not known about topic |\n| **Word Choice and Style** | **Word Choice and Style** | **Word Choice and Style** | **Word Choice and Style** |\n| Material contains minimal necessary word length | Tool does not assess this component | Material uses common, everyday language | Material always uses language the primary audience would use |\n| Material contains minimal necessary sentence length | Tool does not assess this component | Medical terms are defined | Main message and calls to action use active voice |\n| Material contains minimal necessary sentence length | Tool does not assess this component | Active voice is used | Main message and calls to action use active voice |\n| **Use of Numbers** | **Use of Numbers** | **Use of Numbers** | **Use of Numbers** |\n| Tool does not assess this component | Tool does not assess this component | Numbers are clear and easy to understand | Material always explains what the numbers mean |\n| Tool does not assess this component | Tool does not assess this component | Material does not expect user to perform calculations | Audience does not have to conduct mathematical calculations |\n| **Risk** | **Risk** | **Risk** | **Risk** |\n| Tool does not assess this component | Tool does not assess this component | Tool does not assess this component | Material explains the nature of risk |\n| Tool does not assess this component | Tool does not assess this component | Tool does not assess this component | Material explains the risks and benefits of recommended behaviors |\n| Tool does not assess this component | Tool does not assess this component | Tool does not assess this component | If numeric probability is included, it is explained with text or a visual |\n| **Actionability** | **Actionability** | **Actionability** | **Actionability** |\n| Tool does not assess this component | Tool does not assess this component | Material states at least one action reader can take | Material includes one or more calls to action for primary audience |\n| Tool does not assess this component | Tool does not assess this component | Material addresses user directly when describing action | Material includes one or more behavioral recommendations for primary audience |\n| Tool does not assess this component | Tool does not assess this component | Action is broken down into manageable, explicit steps | Material explains why recommendation is important |\n| Tool does not assess this component | Tool does not assess this component | Material provides a tool that can help user take action | Material includes specific directions about how to perform the behavior |\n| Tool does not assess this component | Tool does not assess this component | Material explains how to use the charts, graphs, tables, or diagrams to take actions | Material includes specific directions about how to perform the behavior |\n\n【38】Abbreviations: Index, Centers for Disease Control and Prevention Clear Communication Index; PEMAT, Patient Education Materials Assessment Tool; PMOSE/IKIRSCH, \\[measure developed by Peter B. Mosenthal and Irwin S. Kirsch\\]; SMOG, simplified measure of gobbledygook.  \nSource: Christine E. Prue, PhD, MSPH, Associate Director for Behavioral Science, Centers for Disease Control and Prevention, National Center for Emerging and Zoonotic Infectious Diseases.\n\n【39】#####  Table 2. Health Literacy Demand Scores of 13 Patient Education Materials on Sickle Cell Disease, Evaluated by Measurement Tool\n\n| Measurement Tool |\n| --- |\n| Educational Material | SMOG a | PMOSE/ IKIRSCH Measure b | PEMAT Usability c | PEMATActionability c | PEMAT Overall c | Index d | AACSAT e |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Toolkit for Living Well With Sickle Cell Disease  | 10 | 5 | 68 | 58 | 64 | 57 | 3.0 |\n| Tips Sheet: Supporting Students with Sickle Cell Disease  | 12 | 6 | 65 | 76 | 70 | 58 | 2.2 |\n| Fact Sheet: Sickle Cell Disease  | 11 | — f | 74 | 0 | 38 | 44 | 3.2 |\n| Fact Sheet: Sickle Cell Disease and College  | 11 | — f | 73 | 40 | 61 | 75 | 2.3 |\n| Fact Sheet: Sickle Cell and Pregnancy  | 11 | — f | 68 | 36 | 52 | 55 | 3.0 |\n| Fact Sheet: Sickle Cell Trait | 10 | — f | 75 | 27 | 51 | 63 | 3.1 |\n| Living Well With Sickle Cell Disease  | 10 | — f | 70 | 28 | 49 | 59 | 3.0 |\n| Five Tips to Prevent Infection | 10 | — f | 85 | 63 | 74 | 76 | 3.0 |\n| Emergency Guide: When to See the Doctor  | 9 | — f | 83 | 64 | 72 | 64 | 3.2 |\n| NIH Web pages: What Is Sickle Cell Anemia ? | 10 | — f | 78 | 0 | 42 | 46 | 2.5 |\n| NIH Web pages: Causes  | 10 | 8 | 66 | 0 | 35 | 53 | 2.5 |\n| NIH Web pages: How Is Sickle Cell Anemia Treated ? | 12 | — f | 44 | 18 | 32 | 52 | 2.0 |\n| AAFP Web page: When Your Child Has Sickle Cell Disease  | 8 | — f | 75 | 62 | 68 | 71 | 2.9 |\n\n【41】Abbreviations: AACSAT, African Americans Cultural Sensitivity Assessment Tool; AAFP, American Academy of Family Physicians; Index, Centers for Disease Control and Prevention Clear Communication Index; NIH, National Institutes of Health; PEMAT, patient education materials assessment tool; PMOSE/IKIRSCH, \\[measure developed by\\] Peter B. Mosenthal and Irwin S. Kirsch; SMOG, simplified measure of gobbledygook.  \na  Numeric score indicating school grade reading level.  \nb  Numeric score and ranking (very low = 3–5, low = 6-8, moderate = 9–11, high = 12–14, very high = 15–17); tool applies only to materials without lists or charts.  \nc  Numeric component score and overall score (out of 100 possible points). PEMAT assesses usability (the degree to which people from different backgrounds can understand the messages conveyed) and actionability (the degree to which people from different backgrounds know what to do with the information \\[eg, when and where to seek care, which healthful behaviors to adopt\\] on the basis of the information provided).  \nd  Percentile score out of 100 possible points (minimum score of 90 is considered passing).  \ne  Numeric score out of 5 possible points (minimum score of 2.5 is considered acceptable).  \nf  Contains no lists, charts, or graphs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "926c3b32-b810-429f-8354-72e9a1b4d416", "title": "Coordinated Implementation of Chikungunya Virus Reverse Transcription–PCR", "text": "【0】Coordinated Implementation of Chikungunya Virus Reverse Transcription–PCR\nChikungunya fever, caused by chikungunya virus (CHIKV), is an acute febrile illness that causes severe and long-lasting arthralgia . A recent and ongoing epidemic in the Indian Ocean area extended far beyond this region and caused hundreds of imported cases worldwide . Chikungunya fever is difficult to clinically distinguish from co-endemic diseases such as malaria or dengue fever. Laboratory testing is required for appropriate case management and public health response . Pilot studies have shown that reverse transcription–PCR (RT-PCR) reliably detects acute infections in humans , but many laboratories were not ready to conduct such tests when this epidemic occurred.\n\n【1】During 2006 and 2007, the European Network for the Diagnosis of Imported Viral Diseases (ENIVD) received requests by many laboratories for assistance with CHIKV diagnostics. On the basis of experiences during the outbreak of severe acute respiratory syndrome (SARS) in 2003 , an ENIVD member laboratory distributed a then-unpublished real-time RT-PCR protocol that had been evaluated with a large number of clinical samples from imported cases to laboratories asking for assistance . To determine efficacy of RT-PCR testing for CHIKV, we distributed testing materials to 31 participating laboratories in an external quality assurance study. Laboratories sent their results to ENIVD for analysis of efficacy.\n\n【2】### The Study\n\n【3】Information distributed to laboratories asking for assistance with CHIKV RT-PCR included reaction chemistry setup, cycling profile, and primer and probe sequences. A quantified CHIKV in vitro RNA transcript containing 9 × 10 10  subgenomic RNA copies/μL was used as a noninfectious positive control. Additional measures were taken to provide proper primers and probes because these components are most vulnerable to variation when assays are adapted from protocols, e.g. because of synthesis errors or poor purification. Primers and probes were synthesized in large reference lots and stored centrally at an oligonucleotide factory. Samples of these lots were validated by the reference laboratory and confirmed to provide full sensitivity as achieved with the original primers used in developing the prototype assay . Recipients of protocols were invited to order and use aliquots of primers directly from the validated reference lot.\n\n【4】To receive feedback on performance of this method and other methods of CHIKV detection, a proficiency study was organized among ENIVD members. All participants were informed about the option of obtaining the preformulated assay. Laboratories in Europe , Asia , South America , and Africa  participated.\n\n【5】Inactivated and stable testing material was generated from cell culture supernatants of 4 CHIKV strains from the epidemic in the Indian Ocean area (1 each from Seychelles, Mauritius, Réunion Island, and India) and 1 East/Central Africa strain (S27). Virus solutions were inactivated by heating at 56°C for 1 h and gamma irradiation with 30 kGy. Residual infectivity was excluded by 3 blind passages of a sample of each solution on Vero cells. Solutions were diluted in human fresh-frozen plasma, aliquoted (100 μL), and lyophilized. Test aliquots were reconstituted in 100 μL of water, and CHIKV RNA was quantified by RT-PCR . Lyophilized samples were shipped at ambient temperature to participating laboratories. Each shipment contained a coded panel of 9 CHIKV RNA positive– and 3 CHIKV RNA–negative lyophilized samples with virus concentrations shown in Table 1 . Participants were asked to test the material with any molecular assay routinely used for detecting CHIKV in human plasma or with the preformulated test. We requested test results and assay details (PCR formulations and extraction methods). A total of 36 sets of results were received by the study coordinator, including 3 double sets from 3 laboratories that used 2 methods each. One laboratory provided triple sets of results from 3 tests.\n\n【6】We used 2 criteria to define successful participation in the external quality assessment study. First, those samples containing \\> 7,040 RNA copies/mL should be correctly identified. Analogous to previous external quality assessments , we chose this threshold because it is ≈5–10× above the limit of detection of current CHIKV RT-PCR protocols . Second, no false-positive results were allowed in virus-free samples.\n\n【7】Samples containing 10,487,171 RNA copies/mL were correctly detected by all participating laboratories . Fifteen (48%) of the laboratories were able to detect samples containing \\> 7,040 RNA copies/mL. Only 22.6% correctly detected the sample with 1,076 copies/mL. Of 31 laboratories, 14 (45.2%) met all proficiency criteria. Seventeen laboratories missed the proficiency criteria because of a lack of sensitivity. Two of these laboratories reported \\> 1 false-positive result. Both laboratories had used a nested RT-PCR, which likely indicated cross-contamination during RT-PCR procedures. No other laboratories reported false-positive results.\n\n【8】To project performance of a hypothetical average laboratory, cumulative fractions of positive results reported for each test sample were correlated against RNA concentrations in samples and subjected to probit analysis. This procedure used a dose-response model, which predicted for the average laboratory that a 50% certainty of detection was achieved for CHIKV plasma concentrations \\> 10,000 RNA copies/mL (95% confidence interval \\[CI\\] 3,162–19,952 copies/mL) . A 95% certainty of detection was achieved for CHIKV plasma concentrations \\> 7,943,282 copies/mL (95% CI 2,511,886–39,810,717 copies/mL).\n\n【9】To evaluate critical criteria in laboratory practice, we determined whether particular components of laboratory procedures had any systematic influence on laboratory performance. Selection of criteria was based on experiences from earlier external quality assessment studies . We evaluated automated versus manual RNA extraction methods, 1 widely distributed procedure for RNA extraction (viral RNA mini kit; QIAGEN, Hilden, Germany), any real-time RT-PCR, any nested RT-PCR, or the preformulated RT-PCR distributed with this study. Cumulative fractional positive results of all low- and medium-concentration samples ( < 86,197 copies/mL) were subjected to multifactor analysis of variance, which eliminated influence of other defined factors in each analysis. The only technical factor that increased sensitivity was the preformulated RT-PCR . Thirteen (42%) of 31 participants used this assay. Another factor with nonsignificant benefit (p = 0.08) was use of automated RNA extraction.\n\n【10】### Conclusions\n\n【11】Because of little disease activity before the epidemic, laboratories inside and outside epidemic regions were not prepared to detect CHIKV when the epidemic occurred. In a similar situation during the SARS epidemic in 2003, we demonstrated that rapid provision of a commercial test kit could greatly assist laboratories worldwide, enabling them to perform state-of-the art molecular diagnostics during the epidemic . However, for chikungunya fever, commercial firms did not rapidly prioritize development of CHIKV test kits. ENIVD attempted to assist implementation of molecular diagnostics on an ad hoc basis by distributing a validated CHIKV RT-PCR and all required reagents.\n\n【12】Our proficiency study showed surprisingly good overall performance of participating laboratories than most of our previous external quality assessments . Analysis of factors identified that this success was primarily due to the preformulated assay. In our earlier external quality assessments on detection of emerging viruses, many participants used diagnostic methods reported in the literature, which did not provide technical features such as real-time PCR . This in-house assay was readily implemented by a large number of laboratories. It improved diagnostic proficiency similar to the commercial assay distributed during the SARS epidemic . We showed that novel PCR diagnostics for emerging diseases can be implemented on an international scale. However, enhanced support by reference laboratories through efficient collaborative networks of laboratories is indispensable. Public health organizations should be encouraged by these data to strengthen and extend networking between diagnostic laboratory facilities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b70b5afc-2cc3-43d1-b577-0d13d5158fcd", "title": "Geographic and Social Factors Associated With Chronic Disease Self-Management Program Participation: Going the “Extra-Mile” for Disease Prevention", "text": "【0】Geographic and Social Factors Associated With Chronic Disease Self-Management Program Participation: Going the “Extra-Mile” for Disease Prevention\nAbstract\n\n【1】**Introduction**\n\n【2】We examined geographic and social factors associated with participation in the Chronic Disease Self-Management Program (CDSMP) and the Diabetes Self-Management Program (DSMP) implemented at 144 sites in Illinois.\n\n【3】**Methods**\n\n【4】Programs were delivered by trained facilitators, once per week, during 6 weeks to 1,638 participants aged 50 or older. Of the 1,638 participants, we included in our analysis 1,295 participants with complete geographic information and baseline data on demographic characteristics, health history, and health behaviors. We assessed the following program data: program type (CDSMP or DSMP), workshop location, class size, and number of sessions attended by participants. We geocoded each participant’s home address, classified the home address as rural or urban, and calculated the distance traveled from the home address to a workshop. We used linear and logistic regression analyses to examine the associations between participant and program factors with number of sessions attended and odds of program completion by whether participants lived in an urban or rural county.\n\n【5】**Results**\n\n【6】Average program attendance was 4.2 sessions; 71.1% (1,106 of 1,556) completed 4 or more sessions. Most participants enrolled in CDSMP (59.6% \\[954 of 1,600\\]), but DSMP had greater completion rates. Less than 7% (85 of 1,295) of our sample lived in a rural county; these participants had better completion rates than those living in urban counties (89.4% \\[76 of 85\\] vs 75.6% \\[890 of 1,178\\]). Traveling shorter distances to attend a workshop was significantly associated with better attendance and program completion rates among urban but not rural participants. The number of sessions attended was significantly higher when class size exceeded 16 participants. Not having a high school diploma was significantly associated with lower levels of attendance and program completion.\n\n【7】**Conclusion**\n\n【8】Participation in CDSMP and DSMP was associated with distance traveled, program type, class size, and education. Increasing participation in self-management programs is critical to ensure participants’ goals are met.\n\n【9】Introduction\n\n【10】Management of chronic health conditions such as diabetes, hypertension, and arthritis is a public health concern among the growing older population . Currently, 68% of older adults have at least 2 chronic diseases , and people who are a racial/ethnic minority, live in a rural area, or have lower socioeconomic status are disproportionally affected . People with chronic diseases have higher risks of disability, loss of independence, and reduced quality of life, and these higher risks can lead to decreases in productivity and increases in health care costs and the burden of caregivers . Furthermore, chronic diseases account for 95% of health care costs in the United States . However, chronic diseases can be prevented and managed through healthy lifestyles and self-management education .\n\n【11】Two widely accepted chronic disease self-management education programs, the Chronic Disease Self-Management Program (CDSMP) and the Diabetes Self-Management Program (DSMP), give participants knowledge and skills to manage chronic diseases . These programs are endorsed by the National Council on Aging, and they have strong evidence to support their implementation . However, program attendance and completion are a challenge to many program providers, and little is known about their barriers and facilitators. Factors that influence participation are particularly important in rural areas, which often have limited access to health care services and chronic disease management programs . The limited evidence available about program participation in chronic disease management programs suggests that participation is greater among healthier people and urban dwellers . The objective of our study was to describe geographic and social factors associated with participation of adults aged 50 or older in chronic disease self-management programs in rural and urban areas.\n\n【12】Methods\n\n【13】As part of a state-wide effort to disseminate and implement chronic disease self-management programs, many organizations (senior centers, Cooperative Extension offices, assisted living facilities, Area Agencies on Aging, and local hospitals) implemented the CDSMP and DSMP at 144 sites in Illinois during 2016–2017. Workshop sessions were offered once per week for 6 weeks by trained facilitators in the community’s language of preference. Participants aged 50 or older completed questionnaires at the beginning of the first workshop. The questionnaires collected information about demographic characteristics (age, sex, race, and education), health history, physical activity level, whether the respondent cared for someone with a long-term health problem or disability, and health care practices (eg, health confidence, self-efficacy in communicating with health providers). We also collected data on each participant’s home address and each workshop location.\n\n【14】### Measures\n\n【15】**Program attendance and completion.** We obtained information on attendance (number of workshops attended by participants), class size, and type of program (CDSMP or DSMP) from the workshop facilitators. The number of workshop sessions attended ranged from 1 to 6. According to program developers, attendance in 4 or more sessions is considered program completion . We measured program attendance as a continuous variable and completion as a dichotomous variable. We classified participants who completed 4 or more sessions as completers.\n\n【16】The Stanford guideline for class size requires their self-management workshops (including CDSMP and DSMP) to have a minimum of 10 and a maximum of 16 participants . However, adherence was low in evaluations of Stanford’s CDSMP . Thus, we examined adherence to class size criteria by comparing attendance and completion rates among participants in workshops that satisfied the class-size criteria and workshops that did not. We further compared attendance and completion rates for 3 workshop sizes: small (<10 participants), medium (10–16 participants), and large (>16 participants). In all regression analyses, we evaluated the effects of class size as a continuous measure.\n\n【17】**Geographic factors** . Geographic factors associated with program attendance and completion were whether a participant lived in a rural or an urban county and the distance traveled from a participant’s home address to the workshop site. We geocoded the address for each workshop site and participant residence into latitude and longitude by using Google Earth Pro. We used a participant’s home address to identify county of residence; we then classified these counties as rural or urban according to classifications of the US Census Bureau’s Office of Management and Budget , which defines a metropolitan (ie, urban) area as having a population of 50,000 or more and a micropolitan (ie, rural) area as having a population of 10,000 to 49,999 . We used rural as the reference category. After mapping workshop sites and participant residences by using ArcMap 10.5.1 (Esri), we calculated the distance in miles traveled by participants from home to a workshop site by using the Network Analysis tool Origin-Destination Cost Matrix. We used the dot-density function to indicate the correct number of participants per county while protecting information on participants’ exact residential locations.\n\n【18】To examine the effect of proximity (including on-site delivery) on attendance and completion, we dichotomized data on distance traveled by participants from home to a workshop into 2 categories: participants who traveled less than 0.1 miles (considered living in proximity) and participants who traveled 0.1 miles or more (considered not living in proximity). Participants who traveled less than 0.1 miles included participants who resided at a workshop location (residents of community housing programs, nursing homes, assisted living facilities, retirement communities, or other organizations that hosted workshops). We adopted 0.1 miles as a cut point on the basis of previous research. One study on mathematical modeling of proximity relations suggested 0.1 miles as the minimum distance for linguistic proximity analyses ; another study used distances shorter than 200 m (approximately 0.12 miles) to set accessibility benchmarks for walking and public transportation journeys among older adults ; and a US survey on walking for transportation that found that older adults and those with chronic diseases were more likely to favor walking short distances .\n\n【19】**Social factors.** We collected self-reported data on age, sex, race/ethnicity, level of education, and care for someone with a long-term health problem or disability. We coded race/ethnicity as a dummy variable, with non-Hispanic white as the reference category. Participants reported their education level, and we dichotomized responses into participants who did not receive a high school diploma and those who received a high school diploma or more. Participants were asked if they cared for someone with a long-term health problem or disability, and responses were dichotomized into yes or no. We assessed weekly physical activity by asking about time spent in physical activities such as walking, bicycling, and gardening. We classified answers into 2 categories to examine differences in attendance between respondents who met the national physical activity guidelines (≥150 min per week ) and those who did not. Finally, we collected data on class size for each workshop.\n\n【20】### Statistical analyses\n\n【21】Overall, 1,638 adults aged 50 or older participated in CDSMP or DSMP; for 38 of these participants, we did not have information on which program they attended. We excluded 343 participants from analysis because we did not have their geographic information; our analytic sample consisted of 1,295 participants. We calculated descriptive statistics for the overall sample (those with geographic information and those without) and for our analytic sample, stratified by rural and urban residence. Not all 1,295 participants answered all questions on the questionnaire; we calculated percentages according to the number of participants who answered each question. For all available data, we conducted _t_ tests to compare participants’ outcomes and identify potential cofounders. Linear regression examined factors associated with program attendance. Logistic regression examined how these factors influenced the odds of program completion. Logistic regression tested the odds of completion by using the dependent dichotomous variable to examine completion of 4 or more sessions. Independent variables were added by using the enter method, which enters variables into the model simultaneously. Both linear and logistic regression tested the influence of meeting the physical activity recommendations, living in proximity to programs (<0.1 miles), class size, type of program attended (CDSMP or DSMP), and the main effects and interaction of the miles traveled from participant’s home to a workshop, by residence in an urban or rural county. We found that attendance and completion were not influenced by caregiving, and thus we did not include this variable in our analyses. Statistical models controlled for the effect of sex, race/ethnicity, education, and program type. All analyses were computed in SPSS Statistics 24 (IBM Corporation).\n\n【22】Results\n\n【23】Overall, 59.6% (954 of 1,600) participants attended CDSMP, and 40.4% (646 of 1,600) attended DSMP. Of the 1,295 participants in our analytic sample, 93.4% (n = 1,210) lived in urban counties, and 6.6% (n = 85) participants lived in rural counties .  \n\n【24】Location of workshop sites for the Chronic Disease Self-Management Program and the Diabetes Self-Management Program and distribution of participants’ home addresses by rural and urban counties in Illinois during 2016–2017. We used the dot-density function to indicate the correct number of participants per county while protecting information on participants’ exact residential locations. Data sources: Illinois Pathways to Health , US Census Bureau . \n\n【25】Of the 1,556 participants for whom we had attendance data, 1,106 (71.1%) completed 4 or more sessions , and mean (standard deviation \\[SD\\]) attendance was 4.2 (1.9) sessions. Participants living in rural counties had a higher completion rate (76 of 85 \\[89.4%\\]) than participants living in urban counties (890 of 1,178 \\[75.6%\\]). Program completion and mean number of sessions attended in CDSMP and DSMP varied by workshop site. Overall, mean (SD) attendance in DSMP (4.4 \\[1.8\\] sessions) was significantly greater than attendance in CDSMP (4.1 \\[1.9\\] sessions; _t_ 1,550  \\= −2.9, _P_ \\= .004). Overall, estimated median (interquartile range \\[IQR\\]) distance traveled to a workshop site was 2.0 (0.4–5.2) miles. Participants in rural counties traveled a median (IQR) distance of 1.3 (0.5–9.2) miles to a workshop, and participants in urban counties traveled a median (IQR) of 2.1 (0.4–5.0) miles. Most (88.7%; 71 of 80) participants in rural counties were non-Hispanic white; the 1,064 participants in urban counties were more racially and ethnically diverse, with 52.4% (n = 558) non-Hispanic white, 30.9% (n = 329) non-Hispanic black or African American, and 8.6% (n = 91) Hispanic. Of the 1,295 participants in our analytic sample, 28.6% (n = 370) reported caring for someone with a long-term health problem or disability.\n\n【26】Workshop sizes ranged from 4 to 39 participants (mean \\[SD\\] = 16.0 \\[7.3\\] participants). Overall, 41.6% (653 of 1,569) of participants attended workshops that satisfied the Stanford guideline of 10 to 16 participants. Of the 58.4% (916 of 1,569) who attended workshops that did not satisfy the Stanford guideline, 19.9% (313 of 1,569) of participants attended workshops with fewer than 10 participants, and 38.4% (603 of 1,569) attended programs with more than 16 participants. Mean \\[SD\\] attendance in small workshops (4.3 \\[1.8\\] sessions) was greater than attendance in workshops that satisfied the Stanford guideline (4.1 \\[1.9\\] sessions) ( _t_ 1,519  \\= 2.1; _P_ \\= .04). We found no significant differences in mean attendance between large (>16 participants) and small (<10 participants) workshops. However, mean (SD) attendance per class was higher for large workshops (4.4 \\[1.8\\] sessions) than for workshops that satisfied class size requirements (4.1 \\[1.9\\] sessions) ( _t_ 1,215  \\= −2.7; _P_ \\= .008).\n\n【27】The overall model explained 4.8% of the variance in the number of sessions attended ( _R_ \\= 0.24, adjusted _R_ 2  \\= 0.048) . Not having a high school diploma (β = −0.09, _P_ \\= .01) and class size (β = 0.11, _P_ \\= .01) were significantly associated with fewer sessions attended. Fewer miles traveled from home to a workshop (β = −0.12, _P_ \\= .001) was significantly associated with a greater number of sessions completed.\n\n【28】### Moderation between distance and attendance among participants who resided in urban counties\n\n【29】We found a significant interaction between miles traveled from home to a workshop and whether participants lived in an urban or a rural county (β = 0.09, _P_ \\= .049). The simple slopes showed that distance traveled from home to a workshop significantly influenced the number of sessions completed by participants living in an urban county ( _b_ \\= −0.29, _P_ < .001) . In contrast, distance traveled had no significant effect on the number of sessions completed by participants living in a rural county ( _b_ \\= 0.04, _P_ \\= .17).  \n\n【30】Simple slopes describing the association between the distance in miles between a participant’s home address to a workshop site and the number of program sessions completed among from adults aged 50 or older in the Chronic Disease Self-Management Program or the Diabetes Self-Management Program in Illinois during 2016–2017. We used a participant’s home address and criteria from the US Census Bureau’s Office of Management and Budget  to determine whether the participant lived in a rural county or an urban county. Abbreviation: SE, standard error. \n\n【31】Although we did not find an interaction effect, the main effects of several geographic and social factors were significantly associated with the odds of program completion . The overall logistic regression model explained 9% of the variance in odds of completing the program ( _R_ \\= 0.06, Nagelkerke-adjusted _R_ 2  \\= .090). Participants with a high school diploma or more were nearly 2 times as likely as participants who did not have a high school diploma to complete at least 4 workshop sessions (odds ratio \\[OR\\] = 0.54, _P_ \\= .02). Participants who traveled less than 0.1 miles to attend a workshop session were 1.69 times as likely as participants who traveled 0.1 miles or more to complete the program (OR = 1.69, _P_ \\= .04). Odds of program completion decreased as distance between a participant’s home and workshop site increased (OR = 0.96, _P_ \\= .008). The interaction between distance traveled by whether participants lived in an urban or a rural county was not significant (OR = 1.20, _P_ \\= .07).\n\n【32】Discussion\n\n【33】Our findings indicate that travel distance was a barrier for attendance among participants who lived in urban counties but not rural counties. A study in 2014 also found that distance was a barrier to accessing health care resources more often among urban dwellers than rural dwellers . That study asserted that rural residents are used to navigating distances and therefore may negotiate them better, whereas urban dwellers may have difficulties finding transportation for even a short distance if they have no vehicle or have mobility issues . Therefore, when working with older adults, especially those in urban communities, program planners should pay attention the distance people must travel to get to program sites. Sites should be situated in neighborhoods where the target population lives. On-site delivery (delivery in senior housing programs or community housing sites where adults aged 50 or older reside) may be an option.\n\n【34】Rural dwellers in our study had higher rates of completion and attendance than urban dwellers. This finding is consistent with findings of a nationwide study of chronic disease management dissemination that examined data on more than 300,000 participants in rural and urban areas . One explanation is that health education programs may compete with other activities to which urban dwellers have access locally. In contrast, rural communities tend to offer fewer “distractions” and residents may make such health education programs their priority.\n\n【35】Although program coordinators reached out to rural areas through senior centers, Extension offices, AAAs, and local hospitals, rural areas were underserved. Rural areas are home to 18.6% of adults aged 65 or older in Illinois , yet they represented only 6.6% of our sample. Several factors, such as access to appropriate meeting facilities, affect rural service delivery . The availability of program facilitators and partnerships in the southern, less populated areas of Illinois was a challenge for program delivery. The workshops in rural areas were attended mostly by non-Hispanic white people. However, rural areas are more homogenous than urban areas in Illinois: only 9.5% of the population is black and 11.7% Hispanic . Rural areas can benefit from more culturally tailored recruitment strategies.\n\n【36】Attendance rates in DSMP were significantly greater than in CDSMP. The better attendance in DSMP could have been due to a more focused workshop content. Erdem and Korda reported higher completion rates for people with diabetes who participated in DSMP than in CDSMP, also attributing that outcome to the focus on diabetes in DSMP . The study suggested that higher completion rates could be due to factors such as type of recruiting methods, program site (ie, senior center vs health care facility), and type of program offered near participants’ home. Future research should consider administering both the CDSMP with DSMP in the same location at the same time to determine differences in attendance or completion outcomes and whether any differences can be explained by geographic factors.\n\n【37】Our results indicated that class size was associated with attendance. This finding suggests that the experience of participating in a chronic disease program goes beyond its content. Participation in such programs likely promotes social interactions essential to motivating and encouraging attendance. Gallant reported on the valuable role of friends in chronic disease self-management and emphasized the importance of self-management educational programs to incorporate skills and strategies that enhance social interactions . Meek and colleagues found that older adults with chronic disease restrict their social engagement with family, friends, and community . They suggested that behavioral interventions could help older adults better manage their chronic conditions and maintain active social lives . These findings underscore the importance of recruitment that leads to larger classes, and ultimately, increases attendance. Although some studies found that programs with fewer participants had higher attendance rates, the settings for these programs varied, and therefore the results were inconclusive . Interestingly, these studies included sites that delivered programs to classes that were larger than the class size specified in the Stanford guideline. These findings warrant further study because adapting chronic disease self-management programs to fit a particular facility could affect program fidelity, which may also affect program efficacy . However, Smith and colleagues suggested that some flexibility can be allowed in program implementation to fit the needs of a particular facility as long as the core elements of the program are maintained and program outcomes are not compromised . Both Smith and colleagues  and Carvahlo and colleagues  suggested further research to determine whether significant outcomes can be achieved when interventions are adapted to a particular environment.\n\n【38】Low educational attainment was associated with lower attendance in rural and urban areas. Much literature exists on the association between education and health literacy, defined as “the degree to which individuals have the capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions” . Older adults have a double burden, with a disproportionally high prevalence of chronic diseases and a greater risk of poor health literacy . In the context of chronic disease self-management, education affects a person’s ability to read health information, process oral communication, and conceptualize activities . Mackey and colleagues found that health literacy-sensitive interventions resulted in significant improvements in self-care practices . Educational barriers may be associated with a limited sense of purpose and belonging in health programs, which can create feelings of frustration and affect attendance and completion. Zoellner and colleagues underscored the importance of integrating recruitment strategies that attend to the needs of audiences with a low level of education . Although CDSMP and DSMP materials address low literacy levels, our findings reiterate the importance of focusing on how chronic disease self-management programs are designed and marketed. Best practices include the creation of easy-to-read marketing and communication materials with appropriate language, font style, and font size . Graphic illustrations and experiential activities are useful strategies in mitigating some education limitations and could result in increased attendance .\n\n【39】Our study had several limitations. First, we had a small sample of rural residents. Although we assessed distance objectively through geographic information, our assessment did not account for the time burden associated with various modes (eg, car, bus, walking) of transportation. Additional limitation was the use of only 2 measures of rurality/urbanicity (ie, rural and urban), which may not have accounted for racial/ethnic diversity among the rural population. Also, information about physical activity collected in the questionnaire did not include information on the intensity of physical activity. Therefore, we were unable to account for the difference between light and moderate or vigorous activity. Further examination is needed to better understand the role of travel time, modes of transportation, diversity among the rural population, and physical activity levels. Future studies should also consider using survey instruments to assess health literacy levels.\n\n【40】Our findings underscore the need to develop strategies to improve attendance in CDSMP and DSMP among adults aged 50 or older. Ideally, workshop sites should be located near to participants’ homes to promote completion and increase attendance among urban dwellers. Classes with a larger number of participants should be a goal, keeping in mind program fidelity. Recruitment and program materials should be developed to appeal to people who may not have a high school diploma.\n\n【41】Tables\n------\n\n【42】Table 1. Characteristics of Adults Aged ≥50 Who Participated in the CDSMP or DSMP, Overall and in the Analytic Sample, Categorized as Living in a Rural County or an Urban County, a  in Illinois, 2016–2017\n\n| Characteristic | Analytic Sample, by County of Residence | Overall b (N = 1,638) c |\n| --- | --- | --- |\n| Urban (n = 1,210) c | Rural (n = 85) c |\n| --- | --- |\n| **No. in analytic sample** | 1,210 (93.4) | 85 (6.6) | 1,295 (79.1) |\n| **Sex, no. (%)** | **Sex, no. (%)** | **Sex, no. (%)** | **Sex, no. (%)** |\n| Male | 244 (21.4) | 12 (14.5) | 282 (21.1) |\n| Female | 894 (78.6) | 71 (85.5) | 1,056 (78.9) |\n| **Age, mean (SD), y** | 70.7 (10.6) | 74.7 (7.4) | 71.0 (10.5) |\n| **Race, no. (%)** | **Race, no. (%)** | **Race, no. (%)** | **Race, no. (%)** |\n| Non-Hispanic white | 558 (52.4) | 71 (88.7) | 679 (54.1) |\n| Non-Hispanic black or African American | 329 (30.9) | 7 (8.8) | 377 (30.0) |\n| Hispanic | 91 (8.6) | 1 (1.2) | 107 (8.5) |\n| Other | 86 (8.1) | 1 (1.2) | 92 (7.3) |\n| **Education** | **Education** | **Education** | **Education** |\n| <High school diploma | 122 (11.0) | 5 (6.0) | 148 (11.3) |\n| High school diploma or GED | 279 (25.0) | 35 (42.2) | 332 (25.4) |\n| Some college or technical school | 392 (35.2) | 28 (33.7) | 459 (35.1) |\n| ≥College graduate | 321 (28.8) | 15 (18.1) | 370 (28.3) |\n| **Time spent in physical activity per week, no. (%)** | **Time spent in physical activity per week, no. (%)** | **Time spent in physical activity per week, no. (%)** | **Time spent in physical activity per week, no. (%)** |\n| <30 min | 260 (26.4) | 14 (20.6) | 348 (26.6) |\n| 30 min to 2.5 h | 460 (46.7) | 32 (47.1) | 619 (47.3) |\n| \\>2.5 h d | 266 (27.0) | 22 (32.4) | 340 (26.0) |\n| **Program participation** | **Program participation** | **Program participation** | **Program participation** |\n| Enrolled in CDSMP, no. (%) | 702 (58.6) | 58 (68.2) | 954 (59.6) |\n| Enrolled in DSMP, no. (%) | 496 (41.4) | 27 (31.8) | 646 (40.4) |\n| Mean no. (SD) of sessions attended | 4.4 (1.7) | 4.9 (1.1) | 4.2 (1.9) |\n| Attended ≥4 sessions, no. (%) | 890 (75.6) | 76 (89.4) | 1,106 (71.1) |\n| Distance traveled from participant’s residence to workshop site, median (IQR), mile | 2.1 (0.4–5.0) | 1.3 (0.5–9.2) | 2.0 (0.4–5.2) |\n| Distance traveled from participant’s residence to workshop site, mean (SD), mile | 4.1 (8.1) | 4.9 (5.7) | 4.9 (6.1) |\n| Class size, mean (SD) | 16.5 (7.8) | 11.5 (3.6) | 16.0 (7.3) |\n| **Provides care to someone with a long-term health problem or disability, no. (%)** | 319 (28.9) | 16 (19.5) | 370 (28.6) |\n\n【44】Abbreviations: CDSMP, Chronic Disease Self-Management Program; DSMP, Diabetes Self-Management Program; GED, general educational development certificate; IQR, interquartile range; SD, standard deviation.  \na  Geographic information was available for 1,295 of 1,638 participants; only these 1,295 participants were classified as living in a rural or an urban county and comprised our analytic sample. Urban and rural classifications were determined by using participants’ home address and criteria from the US Census Bureau’s Office of Management and Budget .  \nb  “All” participants refers to all participants in CDSMP or DSMP: the 1,295 for whom geographic information was available (the analytic sample), plus the 343 participants for whom geographic information was not available.  \nc  Not all numbers in categories add to number in column head because not all participants answered all questions. Percentages in each category sum to 100% (unless because of rounding they do not) and are based on number of participants who answered the question.  \nd  Satisfies current physical activity recommendations per US guidelines (≥150 min/wk ).\n\n【45】Table 2. Linear Regression Coefficients of Variables Associated With Number of Sessions Attended Among Adults Aged ≥50 in the CDSMP and DSMP, Illinois, 2016–2017 a  \n\n| Variable | B (Standard Error) | β (95% Confidence Interval) \\[ _P_ Value\\] |\n| --- | --- | --- |\n| **Sex** | **Sex** | **Sex** |\n| Male | −0.13 (0.14) | −0.03 (−0.40 to 0.13) \\[.32\\] |\n| Female | Reference | Reference |\n| **Race** | **Race** | **Race** |\n| Non-Hispanic white | −0.27 (0.14) | −0.09 (−0.55 to 0.01) \\[.055\\] |\n| Non-Hispanic black, Hispanic, or other | Reference | Reference |\n| **Education** | **Education** | **Education** |\n| <High school diploma | −0.49 (0.19) | −0.09 (−0.86 to −0.11) \\[.01\\] |\n| ≥High school diploma | Reference | Reference |\n| **Physical activity recommendations** **b** | **Physical activity recommendations** **b** | **Physical activity recommendations** **b** |\n| Satisfies | 0.07 (0.12) | 0.02 (−0.17 to 0.31) \\[.57\\] |\n| Does not satisfy | Reference | Reference |\n| **Distance traveled from participant’s residence to workshop site** | **Distance traveled from participant’s residence to workshop site** | **Distance traveled from participant’s residence to workshop site** |\n| <0.1 mile | 0.32 (0.16) | 0.07 (0 to 0.64) \\[.05\\] |\n| ≥0.1 mile | Reference | Reference |\n| **Class size** | 0.02 (0.01) | 0.11 (0.01 to 0.04) \\[.01\\] |\n| **Type of program** | **Type of program** | **Type of program** |\n| DSMP | 0.02 (0.13) | 0.01 (−0.24 to 0.29) \\[.87\\] |\n| CDSMP | Reference | Reference |\n| **Classification of participant’s county of residence** | **Classification of participant’s county of residence** | **Classification of participant’s county of residence** |\n| Urban | 0.16 (0.30) | 0.03 (−0.43 to 0.74) \\[.60\\] |\n| Rural | Reference | Reference |\n| **No. of miles traveled from participant’s home to workshop site** | −0.03 (0.01) | −0.12 (−0.05 to −0.01) \\[.001\\] |\n| **Distance traveled × urban or rural county of residence** **c** | 0.07 (0.04) | 0.09 (0 to 0.14) \\[.049\\] |\n\n【47】Abbreviations: CDSMP, Chronic Disease Self-Management Program; DSMP, Diabetes Self-Management Program.  \na  Attendance information for each participant in the program was provided by facilitators of the CDSMP and DSMP programs. The number of attended sessions ranged from 1 to 6. The overall model explained 4.8% of the variance in the number of sessions attended ( _R_ \\= 0.24; Adjusted _R_ 2  \\= 0.048; Δ _R_ 2  \\= 0.004; _P_ \\= .049).  \nb  Per US guidelines (≥150 min per week ).  \nc  Urban and rural classifications were determined by using participants’ home address and criteria from the US Census Bureau’s Office of Management and Budget .\n\n【48】Table 3. Logistic Regression Coefficients of the Odds of Program Completion of the CDSMP and DSMP Among Rural and Urban Adults Aged ≥50 in Illinois, 2016–2017 a  \n\n| Variable | Odds Ratio (95% Confidence Interval) \\[ _P_ Value\\] |\n| --- | --- |\n| **Sex** | **Sex** |\n| Male | 0.81 (0.54–1.19) \\[.28\\] |\n| Female | 1 \\[Reference\\] |\n| **Race** | **Race** |\n| Non-Hispanic white | 0.75 (0.49–1.16) \\[.20\\] |\n| Non-Hispanic black, Hispanic, or other | 1 \\[Reference\\] |\n| **Education** | **Education** |\n| <High school diploma | 0.54 (0.31–0.92) \\[.02\\] |\n| ≥High school diploma | 1 \\[Reference\\] |\n| **Physical activity recommendations** **b** | **Physical activity recommendations** **b** |\n| Satisfies | 1.26 (0.86–1.85) \\[.24\\] |\n| Does not satisfy | 1 \\[Reference\\] |\n| **Distance traveled from participant’s residence to workshop site** | **Distance traveled from participant’s residence to workshop site** |\n| <0.1 mile | 1.69 (1.02–2.81) \\[.04\\] |\n| ≥0.1 mile | 1 \\[Reference\\] |\n| **Class size** | 1.04 (1.01–1.08) \\[.007\\] |\n| **Type of program** | **Type of program** |\n| DSMP | 1.27 (0.84–1.94) \\[.26\\] |\n| CDSMP | 1 \\[Reference\\] |\n| **Classification of participant’s county of residence** **c** | **Classification of participant’s county of residence** **c** |\n| Urban | 0.86 (0.31–2.40) \\[.78\\] |\n| Rural | 1 \\[Reference\\] |\n| **No. of miles traveled from participant’s home to workshop site** | 0.96 (0.92–0.99) \\[.008\\] |\n| **Distance traveled × urban or rural county of residence** **c** | 1.20 (0.99–1.47) \\[.07\\] |\n\n【50】Abbreviations: CDSMP, Chronic Disease Self-Management Program; DSMP, Diabetes Self-Management Program.  \na  The number of attended sessions ranged from 1 to 6. Attendance at ≥4 of 6 sessions is considered program completion by the program developers .The overall logistic regression model explained 9% of the variance in odds of completing the program ( _R_ \\= 0.06, Nagelkerke-adjusted _R_ 2  \\= .090).  \nb  Per US guidelines (≥150 min per week ).  \nc  Urban and rural classifications were determined by using participants’ home address and criteria from the US Census Bureau’s Office of Management and Budget .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "10bd6d64-ed96-43ed-98ae-93f143b701b1", "title": "Human Salmonella and Concurrent Decreased Susceptibility to Quinolones and Extended-Spectrum Cephalosporins", "text": "【0】Human Salmonella and Concurrent Decreased Susceptibility to Quinolones and Extended-Spectrum Cephalosporins\nAlthough antimicrobial agents are not indicated for uncomplicated _Salmonella_ infections, fluoroquinolones and extended-spectrum cephalosporins are potentially life-saving treatments for extraintestinal infections . The National Antimicrobial Resistance Monitoring System (NARMS) has monitored antimicrobial drug resistance among enteric pathogens since 1996. NARMS has documented decreased susceptibility to each of these drug classes, in most instances among separate serotypes . Historically, decreased susceptibility to fluoroquinolones, which can be monitored by tracking resistance to nalidixic acid, has been noted among _Salmonella_ serotypes (ser.) Typhi, Senftenberg, and Virchow . More recently, decreased susceptibility to fluoroquinolones has been noted among _Salmonella_ ser. Enteritidis . Decreased fluoroquinolone susceptibility has also been seen among nalidixic acid–susceptible isolates . Extended-spectrum cephalosporin resistance was noted among 15 non-Typhi _Salmonella_ NARMS isolates (including 12 ser. Typhimurium) during 1996–1998 . In all instances, extended-spectrum cephalosporin resistance was the result of _bla_ CMY-2  , a class C plasmid-encoded _ampC_ gene . In addition to conferring resistance or decreased susceptibility to extended-spectrum cephalosporins such as ceftiofur and ceftriaxone, this gene also confers resistance to ampicillin (AMP), amoxillin-clavulanate, cephalothin, and cefoxitin. This AmpC resistance phenotype has been seen in strains of _Salmonella_ ser. Newport along with resistance to other drugs including chloramphenicol, streptomycin, sulfamethoxazole, and tetracycline. This MDRAmpC strain rose from 1% (1/77) of _Salmonella_ ser. Newport submissions in 1998 to 25% (31/124) in 2001 . CMY β-lactamases are largely responsible for extended-spectrum cephalosporin resistance among _Salmonella_ ser. Newport, Typhimurium, and others isolated in North America .\n\n【1】Coresistance to fluoroquinolones and extended-spectrum cephalosporins would limit therapeutic options for _Salmonella_ infections. Decreased susceptibility to both drug classes was identified in Thailand in 1993 (ser. Anatum, Derby, Enteritidis, Typhimurium, Weltevreden, and I 4,5,12:i:-) , the United Kingdom in 1998 (ser. Senftenberg, Typhimurium, and Virchow) , Belgium as early as 2001 (ser. Virchow) , India in 2002 (ser. Typhi) , the United States in 2002 (ser. Mbandaka) , France in 2003 , and Taiwan in 2004 (ser. Choleraesuis, Cairo, and Kaduna) . In the United States, 27 (4.6%) of 588 _Salmonella_ ser _._ Typhimurium isolates (clinical and slaughter) obtained from food animals in 1999 were resistant to ceftiofur and nalidixic acid: 22 (81%) from turkeys, 4 (15%) from horses, and 1 (4%) from cattle .\n\n【2】To understand coresistance to both antimicrobial classes among _Salmonella_ isolates obtained from humans in the United States, we studied the NARMS human collection from 1996 through 2004, looking for decreased susceptibility to quinolones and extended-spectrum cephalosporins. Information for some of the isolates has been presented elsewhere . We present the molecular epidemiology of this phenotype and mechanisms responsible for its decreased susceptibility.\n\n【3】### Materials and Methods\n\n【4】##### Isolates and Antimicrobial Drug Susceptibility Testing\n\n【5】NARMS-participating state and local public health laboratories submitted non-Typhi _Salmonella_ isolates to the Centers for Disease Control and Prevention (CDC) for antimicrobial susceptibility testing: every 10th isolate from 1996 through 2002 and every 20th isolate from 2003 to present. Serotypes were determined by the submitting laboratory and, for this study, were confirmed by the CDC National _Salmonella_ Reference Laboratory according to the Kaufmann-White scheme as described . MICs were determined by using broth microdilution (Sensititre, Westlake, OH, USA). Isolates exhibiting an amikacin MIC >4 μg/mL were confirmed by Etest (ABBiodisk, Piscataway, NJ, USA). Criteria for decreased susceptibility to quinolones and extended-spectrum cephalosporins were as follows: MIC \\> 32 μg/mL for nalidixic acid or \\> 0.12 μg/mL for ciprofloxacin and \\> 2 μg/mL for ceftiofur or ceftriaxone. Susceptibility testing was performed according to manufacturer’s instructions by using control strains _Escherichia coli_ ATCC25922 and ATCC35218, and _Klebsiella pneumoniae_ ATCC700603 (for extended-spectrum β-lactamase \\[ESBL\\] confirmation only). When available, Clinical Laboratory Standards Institute (CLSI) guidelines were used for interpretation .\n\n【6】##### Isoelectric Focusing for β-Lactamases\n\n【7】The methods of Rasheed et al. were used with modification . Three-hour trypticase soy broth cultures (grown at 37°C with shaking at 300 rpm) were pelleted, resuspended in 0.2% sodium acetate to 5% of original culture volume, and freeze-thawed 4 times (dry ice/ethanol bath and 37°C water bath). Preparations were diluted 2-fold with distilled water and swirled occasionally on ice for 30 min. Supernatants were collected after centrifugation (30 min at 20,200× _g_ ), and 3–5-µL aliquots were resolved for 1.5 h on Ampholine PAGplate polyacrylamide gels, pH 3.5–9.5 (APBiotech **,** Piscataway, NJ, USA). Gels were stained with nitrocefin (500 μg/mL; Becton Dickinson, Franklin Lakes, NJ, USA). Isoelectric points (pIs) were estimated by comparison with the following standard β-lactamases: TEM-12 (pI 5.25), KPC-2 (pI 6.7), SHV-3 (pI 7.0), SHV-18 (pI 7.8), and MIR-1 (pI 8.4).\n\n【8】##### PCR Detection of Antimicrobial Drug Resistance Genes\n\n【9】Presence of _qnr_ genes was determined by using PCR with primers QP1 and QP2 for _qnrA_ , FQ1 and FQ2 for _qnrB_ , and 5′-ATGGAAACCTACAATCATAC-3′ and 5′-AAAAACACCTCGACTTAAGT-3′ for _qnrS_ . The _qnrB_ allele was determined by amplification and sequencing with primers FQ1 and FQ2. Screening for _aac(6′)-Ib-cr_ was performed as described . Primer pairs used for amplification of β-lactamase genes were: _bla_ CMY  (5′-ATGATGAAAAAATCGTTATGC-3′) and (5′-TTGCAGCTTTTCAAGAATGCGC-3′) ; _bla_ OXA-1  (5′-AATGGCACCAGATTCAACTT-3′) and 5′-CTTGGCTTTTATGCTTGATG-3′) ; _bla_ TEM  (5′-TTCTTGAAGACGAAAGGGC-3′) and (5′-ACGCTCAGTGGAACGAAAAC-3′) ; and _bla_ SHV  (5′-GGTTATGCGTTATATTCGCC-3′) and (5′-TTAGCGTTGCCAGTGCTC-3′)  or at Lahey (5′-GCCGGGTTATTCTTATTTGTC-3′) and (5′-TCTTTCCGATGCCGCCGCCAG-3′) . _bla_ CTX-M  genes were screened by using a multiplex PCR assay .\n\n【10】##### DNA Sequencing\n\n【11】Full-length sequences were obtained for β-lactamase genes. A 255-bp region covering the quinolone-resistant determining region (QRDR) of _gyrA_ (Met52 to Leu137) was amplified by using primers gyrA1: 5′-CATGAACGTATTGGGCAATG-3′ and gyrA2: 5′-AGATCGGCCATCAGTTCGTG-3′. QRDRs of _gyrB_ , _parC_ , and _parE_ were amplified and sequenced by using previously described primers , except primers parCF (5′-ATCGTGCGTTGCCGTTTAT-3′) and parCR (5′-GCCGCTTTCGCCACTTC-3′) were used to enhance coverage of _parC_ . Amplicons were sequenced by using ABI Big-Dye 3.1 chemistry and ABI 3730XL automated DNA sequencers (PE Biosystems, Foster City, CA, USA). Analysis was performed by using BioEdit  or SeqMan software (DNAStar, Madison, WI, USA). QRDR sequences of _gyrA_ , _gyrB_ , _parC_ , and _parE_ were compared with those of _Salmonella_ ser _._ Typhimurium LT2 (GenBank accession nos. AE008801, AE008878, AE008846, and AE008846, respectively).\n\n【12】##### Pulsed-Field Gel Electrophoresis (PFGE)\n\n【13】PFGE was performed as previously described . Isolates that produced indistinguishable patterns with _Xba_ I (Roche Molecular Biochemicals, Indianapolis, IN, USA) were restricted with _Bln_ I. Patterns were analyzed by using the BioNumerics version 4.0 software (Applied Maths, Sint-Martens-Latem, Belgium) and compared by unweighted pair group method with averages by using the Dice coefficient with a 1.5% band position tolerance window. The DNA sequence and deduced amino acid sequence for the _Salmonella_ ser. Senftenberg _bla_ CMY-23  gene were assigned GenBank accession no. DQ463751.\n\n【14】### Results\n\n【15】Decreased susceptibility to quinolones and extended-spectrum cephalosporins was first noted in NARMS data in 1997 and represented 0.19% (27/14,043) of non-Typhi _Salmonella_ from 1996 through 2004 . _Salmonella_ ser. Senftenberg was the most frequent serotype (n = 11), followed by Typhimurium (n = 6), Newport (n = 3), and Enteritidis (n = 2). The phenotype was found in 9 different serotypes .\n\n【16】PFGE comparison by _Xba_ I and, if applicable, _Bln_ I restriction showed that 15/27 _Salmonella_ isolates differed by \\> 1 band. No indistinguishable patterns among different _Salmonella_ serotypes were identified. Of the 3 ser. Newport isolates tested, 2 (AM15201 and AM21465) had indistinguishable _Xba_ I patterns but different _Bln_ I patterns (87.51% similarity). The 2 Enteritidis isolates (AM09124 and AM15266) were indistinguishable by both enzymes. Of the 11 ser. Senftenberg isolates, 5 exhibited unique _Xba_ I PFGE patterns, while the remaining 6 were separated into 2 groups with indistinguishable _Xba_ I PFGE patterns (group 1: AM06960, AM08081, AM16094, and AM19422; group 2: AM20227 and AM20256). _Bln_ I restriction demonstrated that AM19422 differed from the other group 1 isolates by a single band difference (97.44% similarity). PFGE results for some of the Senftenberg isolates are described elsewhere . All Typhimurium isolates exhibited unique _Xba_ I PFGE patterns (77%–93% similarity).\n\n【17】Antimicrobial drug susceptibility results are presented in Table 3 . For nalidixic acid, 25 isolates exhibited an MIC >32 μg/mL, and 2 (Mbandaka and Newport) had an MIC of 16 μg/mL. For ciprofloxacin, MICs of 0.12–0.5 μg/mL were found for all isolates except the 11 Senftenberg, for which the MIC was >4 μg/mL. For ceftiofur, 14 isolates exhibited resistance (MIC \\> 8 μg/mL). For ceftriaxone, 2 isolates exhibited resistance according to the current CLSI breakpoint (64 μg/mL), and 7 exhibited intermediate resistance (MIC 16 or 32 μg/mL). Three isolates (Mbandaka, Senftenberg, and Typhimurium) exhibited an ESBL phenotype according to ceftazidime and cefotaxime MIC alone and with clavulanate. Seven isolates exhibited the MDRAmpC phenotype, including 1 Agona, 2 Newport, 3 Typhimurium, and 1 Uganda. According to current CLSI guidelines, 1 isolate (ser. Senftenberg) was fully resistant to ciprofloxacin, ceftriaxone, ceftazidime, and cefotaxime.\n\n【18】The mechanisms responsible for resistance and decreased susceptibility are shown in Table 4 . Some mechanisms for some of the isolates are presented elsewhere . At least 1 _gyrA_ mutation was found in 26 of 27 isolates. A _gyrA_ mutation at codon 83 only was found for 11 isolates; a mutation at codon 87 only was found for 3; mutations at both codons were found for 12. No functional mutations were detected in _gyrB_ or _parE_ genes. All Senftenberg isolates had _parC_ mutations (S80I and T57S), and 6 other isolates had the T57S mutation. In addition to the T57S mutation in _parC_ , the Mbandaka isolate contained a plasmid-mediated _qnrB2_ gene and has been described . Four isolates contained _aac(6′)-Ib_ , but none contained the ciprofloxacin-modifying _aac(6′)-Ib-cr_ variant.\n\n【19】Nine AmpC phenotype isolates produced β-lactamase with a pI \\> 8.4 ; 8 contained _bla_ CMY-2  , but the Senftenberg strain contained a _bla_ CMY-23  gene  identical to that found in an _E. coli_ isolate . This gene differs from _bla_ CMY-2  by 1 amino acid. Three of the _bla_ CMY  \\-positive isolates, including the strain positive for _bla_ CMY-23  , also contained _bla_ TEM-1b  . The Mbandaka isolate was positive for _bla_ SHV-30  with pI 7.0  and also produced an enzyme with a pI 7.6, the nature of which is still under study. Two isolates (1 Senftenberg and 1 Typhimurium) contained _bla_ SHV-12  , and both also contained _bla_ OXA  and _bla_ TEM-1  genes. Of the 11 Senftenberg isolates, 10 contained _bla_ OXA-1  (n = 9) or _bla_ OXA-9  (n = 1). No isolates contained _bla_ CTX-M  genes.\n\n【20】### Discussion\n\n【21】Fluoroquinolone and extended-spectrum cephalosporin coresistance is rare; however, the appearance of this phenotype in 2 commonly isolated serotypes from humans (Typhimurium and Newport) is concerning. Sporadic infections are alarming, but if clonal expansion of an isolate with this phenotype were to take place, as occurred with _Salmonella_ ser. Typhimurium DT104 and Newport-MDRAmpC, the clinical consequences could be dramatic. Statistically significant increases in resistance to nalidixic acid (odds ratio \\[OR\\] 6.7, 95% confidence interval \\[CI\\] CI 2.6–17.7) and ceftiofur (OR 43.2, 95% CI 10.5–177.4) have been documented among non-Typhi _Salmonella_ of human origin submitted to NARMS during 1996–2003 . Of 202 nalidixic acid–resistant non-Typhi _Salmonella_ collected by NARMS during 1996–2003, most were ser. Enteritidis (31%) or Typhimurium (10%). Most of the 324 ceftiofur-resistant non-Typhi _Salmonella_ collected by NARMS during the same time period were ser. Newport (56%) or Typhimurium (23%). A slightly broader geographic representation can be found in the SENTRY surveillance project, which analyzed 786 _Salmonella_ isolates (blood and stool) from medical facilities in Latin America and North America (including Canada) during 2001–2003 . Of these, 11% were resistant to nalidixic acid, and 2% exhibited decreased susceptibility to ceftazidime, ceftriaxone, or aztreonam.\n\n【22】Extended-spectrum cephalosporin-resistant Newport and Typhimurium isolates are typically obtained from community-acquired infections. Newport-MDRAmpC infections have been associated with consumption of contaminated beef and unpasteurized dairy products . _Salmonella_ containing _bla_ CMY  genes have been isolated from ground chicken (Typhimurium DT208), turkey (Agona), and beef (Agona) purchased from retail outlets in the Washington DC area . In addition, cattle, chickens, turkeys, pigs, horses, and dogs have all been sources of _bla_ CMY  \\-containing _Salmonella,_ including common serotypes such as Typhimurium, Newport, and Heidelberg . Decreased susceptibility to fluoroquinolones among _Salmonella_ serotypes that typically carry _bla_ CMY  genes warrants exploration of factors that could select for decreased susceptibility to fluoroquinolones in animal reservoirs and in the human host.\n\n【23】PFGE showed diversity within some serotypes and indistinguishable strains within others. PFGE diversity among 2 serotypes commonly associated with extended-spectrum cephalosporin resistance (Newport and Typhimurium) is not surprising, given that CMY-producing strains have been seen at least since the late 1990s. Isolates of ser. Enteritidis are highly clonal; therefore, PFGE-indistinguishable patterns among isolates with no apparent epidemiologic link are not unusual. All PFGE-indistinguishable Senftenberg isolates from group 1 were isolated in the same state. Results for the Florida Senftenberg isolates are described elsewhere .\n\n【24】_Salmonella_ ser. Senftenberg exhibiting decreased susceptibility to fluoroquinolones has been associated with nosocomial infections in healthcare facilities in the United States . All 11 isolates contained identical _gyrA_ mutations (S83Y and D87G) and _parC_ mutations (T57S and S80I). These _parC_ mutations have been identified in several _Salmonella_ serotypes including Senftenberg . Ten Senftenberg isolates included in this study contained _bla_ OXA  genes; the _bla_ OXA  \\-negative Senftenberg strain contained a _bla_ CMY-23  mechanism of extended-spectrum cephalosporin resistance. Acquisition of a _bla_ CMY  gene by a traditionally nalidixic acid–resistant serotype warrants further epidemiologic and laboratory investigation. The _bla_ OXA-1  gene has been identified in _Salmonella_ ser. Typhimurium and is reported to be carried by an integron ; _bla_ OXA-9  has been associated with Tn1331 .\n\n【25】The epidemiology of _Salmonella_ with decreased susceptibility to fluoroquinolones is relatively well characterized, as is that of _Salmonella_ with _bla_ CMY  \\-mediated extended-spectrum cephalosporin resistance. Conversely, little is known about the events leading to quinolone and extended-spectrum cephalosporin coresistance and the epidemiology of these infections in humans. Patients with _Salmonella_ infections who exhibit decreased susceptibility to both antimicrobial drug classes should be interviewed to determine risk factors and the effects of antimicrobial drugs and other potential selective factors on this phenomenon.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2d991045-1224-4f9a-970d-3101c8789bee", "title": "Health Care Use and Costs for Participants in a Diabetes Disease Management Program, United States, 2007-2008", "text": "【0】Health Care Use and Costs for Participants in a Diabetes Disease Management Program, United States, 2007-2008\nAbstract\n\n【1】**Introduction**The Disease Management Association of America identifies diabetes as one of the chronic conditions with the greatest potential for management. TRICARE Management Activity, which administers health care benefits for US military service personnel, retirees, and their dependents, created a disease management program for beneficiaries with diabetes. The objective of this study was to determine whether participation intensity and prior indication of uncontrolled diabetes were associated with health care use and costs for participants enrolled in TRICARE’s diabetes management program. **Methods**This ongoing, opt-out study used a quasi-experimental approach to assess program impact for beneficiaries (n = 37,370) aged 18 to 64 living in the United States. Inclusion criteria were any diabetes-related emergency department visits or hospitalizations, more than 10 diabetes-related ambulatory visits, or more than twenty 30-day prescriptions for diabetes drugs in the previous year. Beginning in June 2007, all participants received educational mailings. Participants who agreed to receive a baseline telephone assessment and telephone counseling once per month in addition to educational mailings were considered active, and those who did not complete at least the baseline telephone assessment were considered passive. We categorized the diabetes status of each participant as “uncontrolled” or “controlled” on the basis of medical claims containing diagnosis codes for uncontrolled diabetes in the year preceding program eligibility. We compared observed outcomes to outcomes predicted in the absence of diabetes management. Prediction equations were based on regression analysis of medical claims for a historical control group (n = 23,818) that in October 2004 met the eligibility criteria for TRICARE’s program implemented June 2007. We conducted regression analysis comparing historical control group patient outcomes after October 2004 with these baseline characteristics. **Results**Per-person total annual medical savings for program participants, calculated as the difference between observed and predicted outcomes, averaged $783. Active participants had larger reductions in inpatient days and emergency department visits, larger increases in ambulatory visits, and larger increases in receiving retinal examinations, hemoglobin A1c tests, and urine microalbumin tests compared with passive participants. Participants with prior indication of uncontrolled diabetes had higher per-person total annual medical savings, larger reduction in inpatient days, and larger increases in ambulatory visits than did participants with controlled diabetes. **Conclusion**Greater intensity of participation in TRICARE’s diabetes management program was associated with lower medical costs and improved receipt of recommended testing. That patients who were categorized as having uncontrolled diabetes realized greater program benefits suggests diabetes management programs should consider indication of uncontrolled diabetes in their program candidate identification criteria.  \n\n【2】Introduction\n\n【3】Disease management programs educate patients and teach them self-management skills that lead to more healthful lifestyle choices and appropriate use of health care services. These improvements can produce better health outcomes and reduced medical costs. The Disease Management Association of America describes type 2 diabetes as one of the chronic diseases best suited for disease management; most studies have found improved patient outcomes after participation in diabetes management programs . TRICARE Management Activity, the health care benefits administrator for US military service personnel, covers 9.4 million beneficiaries (active-duty service members, family members, and retirees and their family members); approximately 225,000 beneficiaries who are aged 18 to 64 years and ineligible for Medicare have been diagnosed with diabetes . In June 2007, TRICARE Management Activity contracted with its 3 regional managed care support contractors to implement a diabetes management program for patients whose medical records indicated high use of health care services. To improve patient selection criteria for program eligibility, inform program structure and content, and evaluate diabetes management programs, we assessed TRICARE’s disease management program to determine whether and how patient outcomes differed by need for diabetes management (defined by indication of uncontrolled diabetes preceding program participation) and by intensity of program participation (defined by active or passive participation).  \n\n【4】Methods\n\n【5】### Overview\n\n【6】Patients were eligible to participate in this voluntary, opt-out program if during the previous 12-month period they had any diabetes-related emergency department (ED) visits or hospitalizations, more than 10 diabetes-related ambulatory visits, or more than twenty 30-day prescriptions for diabetes drugs. From June 2007 through September 2008, 37,370 patients were automatically selected for enrollment in this program; 11% opted out. Among program participants, approximately 25% chose to receive personalized telephone counseling. Services included a baseline assessment by telephone with a care manager (lasting 40-50 min); monthly telephone calls to educate patients and assess progress toward goals; educational materials (eg, pamphlets, videos, cookbooks); and newsletters and online materials. These patients were categorized as “active” participants. The remaining 75% of patients, categorized as “passive” participants, received newsletters but chose not to receive personalized counseling. All patients could contact care managers by telephone if they had questions about their diabetes, and they could access Internet-based educational resources. Additional information about TRICARE’s disease management program is available elsewhere . The voluntary, opt-out nature of this program, which based eligibility on high use of health care services, introduced 3 phenomena that we considered in the evaluation design. First, selection bias occurs when highly motivated patients are more likely to participate, participate at higher intensity, or more actively manage their disease. Second, regression to the mean occurs when program eligibility is determined by high use of health care services. That is, patient medical costs and health care use tend to decline in the period following program eligibility (even in the absence of a disease management program) when participants are identified on the basis of their high health care use. Third, the opt-out design does not allow for a concurrent natural comparison group to quantify program effect. We identified a historical control group (HCG) and used this group’s information to develop equations predicting outcomes for patients in the absence of a diabetes management program. To estimate program effect, we calculated the mean difference between patients’ observed and predicted outcomes. To create the analytical database, we used a unique patient identifier that linked medical and pharmacy claims from the Defense Health Information Management System Clinical Data Repository, patient characteristics from the Defense Enrollment Eligibility Reporting System, and disease management services received as indicated in the patient tracking information systems of the 3 disease management service providers. An institutional review board (Independent Review Consulting, Inc, Corte Madera, California) approved the study protocols. \n\n【7】### Outcome metrics\n\n【8】On the basis of a literature review and data availability, we chose indicators considered best practices for evaluating program performance . Indicators of health care use were number of ED visits, hospital inpatient days, ambulatory visits, and number of prescriptions. We used the medical component of the Consumer Price Index to calculate TRICARE’s medical costs in 2008 dollars . Because most diabetes-attributed costs stem from its complications and higher use of general medical services, we report both 1) diabetes-related costs and health care use and 2) total costs and health care use (excluding claims for injury, pregnancy, congenital abnormalities, and cancer) . Clinical indicators were the percentage of patients who annually received at least 1 hemoglobin A1c (HbA1c) test, dilated retinal examination, or urinary microalbumin test. Laboratory test results were unavailable. \n\n【9】### Evaluation sample\n\n【10】From the 37,370 patients who were eligible for the program, we excluded 1) 3,021 patients with fewer than 6 months’ program enrollment (for insufficient time to complete the program), 2) 449 patients not continuously eligible for TRICARE during the evaluation period, 3) 218 patients who became eligible for Medicare (for whom complete medical records were unavailable), 4) 154 patients who died or had HIV/AIDS or end-stage renal disease (ESRD), and 5) 3,924 opt-out patients. The final sample contained 29,604 patients; the median program tenure was 15 months .  **Figure.** Evaluation model for the TRICARE diabetes disease management program, United States, 2007-2008. Abbreviations: ESRD, end-stage renal disease; HbA1c, hemoglobin A1c.  After TRICARE Management Activity identified candidates for the program, the disease management provider for each region contacted patients by letter and by telephone. Patients could choose to receive personalized counseling by telephone and mailing, to receive only the newsletters and other mailings, or to have no further contact with the program (ie, opt out). The disease management providers did not record patients’ reasons for opting out. However, care managers said that opt-out patients’ typical reasons for choosing not to participate is that they would be unable to complete the program because of impending military transfers and enlistment terminations, did not have diabetes, or previously participated in other diabetes management programs and did not believe they would benefit from participating in this program. Patients who opted out had similar demographic characteristics to those of participants but had slightly higher medical costs in the year preceding program eligibility. The patient-level analytic file linked medical claims to program participation records. For each patient, we summarized health care use during 2 periods. The identification period was 1 year preceding the eligibility date; the postidentification period varied from 7 months to 15 months after the eligibility date. The outcome variables for each patient, standardized to a 12-month period, take into account a patient’s program tenure. Although medical claims for participants were available through March 2009, to control for the time lag in claims processing and adjudication and to ensure we had complete claims data for participants, we allowed a minimum of 6 months from date of service to the final record extraction date and ended the observation period at September 30, 2008. We categorized patients as having uncontrolled diabetes if they had at least 1 episode of care with the  diagnosis code for uncontrolled diabetes ( _International Classification of Diseases, Ninth Revision_ , 250.02, 250.03) during the year preceding program eligibility. Although lack of a medical claim with a diagnosis of uncontrolled diabetes does not necessarily mean a patient’s glucose level was controlled, for discussion purposes we refer to these patients as having “controlled” diabetes. \n\n【11】### Historical control group\n\n【12】The HCG contained 23,818 patients who in October 2004 met the criteria used later to determine eligibility for the program established in June 2007 . After excluding patients who died or had HIV/AIDS or ESRD, the HCG analytic sample consisted of 23,778 patients. We used multivariable regression to assess the difference between patient health care use and costs, by patient characteristics, postidentification (November 2004 through October 2005) and from the eligibility determination period (November 2003 through October 2004). We used Poisson regressions to analyze count variables such as inpatient days and ED visits, generalized linear models with gamma distribution to analyze health care costs, and logistic regressions to analyze dichotomous clinical variables such as having an HbA1c test during the year. Covariates included patient age group (18-34 y, 35-54 y, 55-64 y) and sex; TRICARE region; military service branch; number of ED visits, inpatient days, ambulatory visits, and 30-day prescriptions; indication of uncontrolled diabetes; Charlson comorbidity index score ; and medical costs. Regressions to estimate diabetes-specific health care use and costs incorporate diabetes-specific health care use and cost covariates. We estimated separate regressions for 1) patients covered under the managed care plan and 2) patients in the preferred provider organization (PPO) plans. Regression results, available from the authors on request, are consistent with expectations. For example, sicker patients (ie, those with a higher Charlson comorbidity score) in the November 2003 through October 2004 period had higher average health care use and costs in the November 2004 through October 2005 period. The goodness-of-fit measures for the regressions — based on root mean square error for continuous outcomes and receiver operating curve for dichotomous outcomes — suggest that the regressions are robust. We used the regression coefficients based on these HCG analyses to predict program participants’ annual health care use and costs.  \n\n【13】Results\n\n【14】### Sample characteristics\n\n【15】Of the 29,604 participants in the analytic sample, 12,776 (43%) had indications of uncontrolled diabetes in the year preceding program start . The rates of active participation were similar for patients with indications of uncontrolled (26%) and controlled (25%) diabetes. A higher proportion of active participants than passive participants were in the managed care plan and had higher preprogram rates of receiving an HbA1c test, retinal examination, and microalbumin urine test. They also had a higher Charlson comorbidity score and higher preprogram health care use and medical costs. The HCG was similar to the program participant group in demographic characteristics, medical costs, and health care use. However, patients in the HCG had lower rates of receiving appropriate tests, particularly HbA1c and microalbumin urine tests. \n\n【16】### Health care use and costs\n\n【17】Because high health care use determined program eligibility, patient health care use and cost outcomes were higher in the year preceding program eligibility than in the year following eligibility (as extreme scores moved toward being less extreme). For the HCG, diabetes-related costs dropped by 28% and total costs dropped by 17% from the preidentification to postidentification period . However, the decline was even larger for the program participants, who experienced 35% and 20% postprogram declines in annual diabetes-related and total costs, respectively, compared with the year preceding program eligibility. Before adjusting for case mix, these estimates suggest that program participants generated $353 per year less in diabetes-related costs and $408 per year less in total costs than did the HCG group. The decline in diabetes-related costs was largest for patients with prior indications of uncontrolled diabetes, and the decline in total costs was largest for patients with no prior indication of uncontrolled diabetes. \n\n【18】### Observed versus predicted outcomes\n\n【19】Program participants with prior indication of uncontrolled diabetes had higher observed diabetes-related costs per year  and higher total costs per year  than did patients with controlled diabetes. However, this outcome was expected, based on prediction equations generated from the HCG group, because patients with uncontrolled diabetes had higher Charlson comorbidity scores than did patients with no indication of uncontrolled diabetes in the year preceding eligibility. The difference in observed and predicted outcomes (which controls for differences in case mix between subsets of program participants) suggests an average annual reduction in diabetes-related costs of $249 per person overall . The reduction was largest among patients with a previous indication of uncontrolled diabetes, whether active ($388) or passive ($392), driven primarily by larger-than-average reductions in use of inpatient services; the smallest decline in average annual diabetes-related costs was for active participants with controlled diabetes ($67) (data not shown). Costs associated with the rise in diabetes-related ambulatory visits and receipt of tests partially offset medical savings associated with the modest decline in use of hospital services. Program participation was associated with receiving an annual HbA1c test, retinal examination, and microalbumin urine test compared with the year preceding enrollment; active participants experienced the greatest improvement. The average per-person reduction in total costs was $783 per year . The reduction was largest for active participants with prior indications of uncontrolled diabetes ($1,007) and smallest for passive participants with controlled diabetes ($577) (data not shown). Compared with passive participants, active participants experienced larger declines in inpatient days and ED visits and larger increases in ambulatory visits. For passive participants, medical savings appear to be primarily realized through reduced use of prescription drugs.  \n\n【20】Discussion\n\n【21】This study suggests that TRICARE’s diabetes management program is associated with modest annual reductions in medical costs and improved receipt of recommended screening. Improvements were largest for active participants with prior indication of uncontrolled diabetes and smallest for patients with no prior indication of uncontrolled diabetes (whether active or passive participants). Active participants had greater improvements than passive participants in the proportion receiving appropriate screening. Increased costs for ambulatory care offset a portion of medical savings from reduced inpatient days and ED visits. Our estimates of average annual medical savings among all program participants ($249 for diabetes-related costs and $783 for total costs) are within the range of estimates from other diabetes management programs. Published estimates for other programs, which may differ from this program in terms of population served and intensity of services, suggest an average annual per-patient savings of $626 in diabetes-related costs (with estimates ranging from a $2,787 loss to a $4,329 gain) . This average comes from randomized clinical trials, quasi-experimental, and pre-post studies (although the sample sizes in these studies are much smaller than for our study). Studies that used a quasi-experimental design similar to our use of the HCG estimated average per-patient savings to be approximately $1,292 per year . A major contribution of our study is that patients with prior indications of uncontrolled diabetes appear to experience greater program benefits than do patients with no indication of uncontrolled diabetes in the 12 months preceding program eligibility. Patients with uncontrolled diabetes who want to improve their self-management skills (as indicated by active program participation) likely have the most to gain from an education-based diabetes management program. Compared with passive program participation, active participation is associated with larger reduction in inpatient days and ED visits and higher rates of recommended screening. Active participants also experienced larger increases in ambulatory visits and smaller decreases in use of medications. These differences in outcomes by participation intensity were not as pronounced as expected. The voluntary nature of this program, in which patients could choose their participation intensity, warrants caution when making comparisons between active and passive participants. We controlled for differences between active and passive participants, such as active participants’ slightly higher preprogram expenditures for diabetes-related health care services, by using prediction equations. In addition, we controlled for differences that likely correlated with patients’ motivation to improve their health: active participants had higher rates of receiving at least 1 HbA1c test, retinal examination, and microalbumin urine test annually. This study had several limitations. Patients could opt out of the disease management program, whereas the HCG had no opt-out component. Anecdotal evidence suggests that many who opted out would have been excluded from the HCG, for example, patients who would soon be leaving the Military Health System. Observed characteristics for the opt-out group were similar to those of program participants. The opt-out nature of this program also meant that there was no similar concurrent control group. The HCG was part of our study design because comparing program outcomes for TRICARE’s disease management program with outcomes from published literature would have posed multiple problems. These include the datedness of published findings and the differences in populations, diseases, program designs, and insurance benefits. Identifying an HCG that met the eligibility criteria for program participation within the TRICARE health plan limited these issues. By using prediction equations, we could control for differences between the HCG and program participants . A limitation of this approach is that secular trends continue to change health care delivery patterns. To mitigate the potential bias from secular trends we 1) chose an HCG that predated the disease manage program only slightly, 2) used patient preprogram characteristics to predict outcomes after program start, and 3) adjusted past health care costs to 2008 dollars. Unobservable differences between active and passive participants may explain the smaller-than-expected differences in outcomes. Our study, for example, does not control for the possibility that patients received adequate diabetes counseling from other sources (eg, primary care provider or endocrinologist). Such information is unavailable for both the HCG and the program participants. If patients who already receive good diabetes counseling from other sources are more likely to be passive participants then our prediction equations could overestimate the program’s effect on passive participants and underestimate the effect on active participants when comparing observed with predicted outcomes. Another limitation is the inability to precisely identify patients whose diabetes is controlled. The unavailability of laboratory results (eg, HbA1c results) prohibited us from testing whether patients with extremely high glucose levels benefited from the program more than patients with moderately high glucose levels. Some patients with no diagnosis code for uncontrolled diabetes undoubtedly had uncontrolled diabetes, and some patients with a prior diagnosis of uncontrolled diabetes likely had their diabetes under control at the start of disease management. This inexactness in identifying patients with uncontrolled diabetes may reduce the estimates of program differences by uncontrolled status. TRICARE’s education-based disease management program was associated with modest reductions in annual medical costs and more appropriate use of health care services. Patients with prior indications of uncontrolled diabetes actively participated in the program at the same rate as patients with no indication of uncontrolled diabetes, and they appeared to benefit more from the program, leading to higher cost savings. This finding suggests that patients with indications of uncontrolled diabetes are strong candidates for participation in diabetes management programs.  \n\n【22】Tables\n------\n\n【23】#####  Table 1. Characteristics and Outcomes of Participants in the TRICARE Diabetes Disease Management Program, United States, 2007-2008\n\n【24】Characteristic/OutcomeDiabetes Status/Program Participation Status aAll, n = 29,604Historical Control Group, b n = 23,778ControlledUncontrolledActive, n = 4,204Passive, n = 12,624Active, n = 3,332Passive, n = 9,444Age, mean (SD), y55.1 (8.0) c53.0 (9.7)53.9 (9.1) d50.6 (11.5)52.6 (10.1)53.0 (9.9)Men, %40 c4837 d444444Region, %North37 c2231 d202431South47 c5251525141West16 c2618 d282428Military branch, %Army413938393939Air Force272829292825Navy252525252529All other778777Managed care plan, %62 c5772 d676363Preventive care received in previous 12 months, %HbA1c test5 c4371 d665528Retinal exam23 c1935 d302523Microalbumin urine test32 c2847473716Charlson comorbidity score, e mean (SD)0.9 (1.2) c0.8 (1.2)1.3 (1.5) d1.1 (1.4)0.9 (1.3)0.7 (1.2)Health care use, mean (SD)No. of ED visits1.6 (2.4)1.6 (3.1)2.1 (3.8)2.1 (3.7)1.8 (3.3)1.8 (3.4)No. of inpatient days1.5 (5.0)1.4 (5.8)3.2 (9.2)3.0 (8.9)2.1 (7.3)2.5 (8.1)No. of ambulatory visits22  c18 33  d27 23 22 No. of 30-day prescriptions93  c77 104  d86 85 81 Diabetes-related costs, f mean (SD), $4,148  c3,607 7,398  d6,925 5,169 5,215 Total costs, f mean (SD), $13,452  c11,104 20,387  d16,689 14,264 14,376 Abbreviations: SD, standard deviation; HbA1c, hemogloblin A1c; ED, emergency department.a Controlled defined as not having an episode of care with the _International Classification of Diseases, Ninth Revision_ , diagnosis code for uncontrolled diabetes (250.02, 250.03) during the year preceding program eligibility. Uncontrolled defined as having at least 1 such episode. Active defined as receiving personalized telephone counseling; passive defined as declining personalized counseling but receiving educational mailings.b Refers to patients who in October 2004 met the criteria used later to determine eligibility for the diabetes disease management program established in June 2007.c Significantly different from participants in the controlled, passive category at _P_ < .05. Calculated by using a 2-tailed _t_ test.d Significantly different from participants in the uncontrolled, passive category at _P_ < .05. Calculated by using a 2-tailed _t_ test.e An index of comorbid conditions; a higher score indicates a sicker patient .f Adjusted to the medical component of the Consumer Price Index in 2008 . \n\n【25】#####  Table 2. Health Care Costs, by Diabetes Status, Before and After Participation in the TRICARE Diabetes Disease Management Program, United States, 2007-2008 a\n\n【26】Cost Category/ Diabetes Status bPeriodHistorical Control Group (HCG), c n = 23,778Program Participants, n = 29,604Difference Between HCG and ParticipantsMean Per-Person Annual Cost, d $Pre-Post Change, %Mean Per-Person Annual Cost, d $Pre-Post Change, %$%Diabetes costsControlledPre3,720−27.83,568−34.6−200−6.8Post2,6872,335UncontrolledPre6,822−27.46,627−34.7−427−7.3Post4,9504,328AllPre5,215−27.65,169−34.6−353−7.1Post3,7773,378Total costsControlledPre11,239−14.411,307−19.3−564−4.9Post9,6209,124UncontrolledPre17,751−19.416,957−21.0−120−1.6Post14,30813,394AllPre14,376−17.414,264−20.4−408−3.0Post11,87911,359a Unadjusted for case mix.b Controlled defined as not having an episode of care with the _International Classification of Diseases, Ninth Revision_ , diagnosis code for uncontrolled diabetes (250.02, 250.03) during the year preceding program eligibility. Uncontrolled defined as having at least 1 such episode.c Refers to patients who in October 2004 met the criteria used later to determine eligibility for the diabetes disease management program established in June 2007.d Adjusted to the medical component of the Consumer Price Index in 2008 . \n\n【27】#####  Table 3. Predicted and Observed Diabetes-Related Health Care Use and Costs for Participants in the TRICARE Diabetes Disease Management Program, United States, 2007-2008 a\n\n【28】OutcomeDiabetes Status/Program Participation Status bAll, n = 29,604Controlled, n = 16,828Uncontrolled, n = 12,776Active, n = 7,536Passive, n = 22,068Observed outcomeTotal costs, c mean (SD), $2,516 4,514 3,692 3,271 3,378 No. of inpatient days, mean (SD)0.1 (1.2)0.4 (2.8)0.2 (1.4)0.3 (2.2)0.3 No. of ED visits, mean (SD)0.2 (0.6)0.3 (1.1)0.2 (0.8)0.2 (0.8)0.2 (0.8)No. of ambulatory visits, mean (SD)3.8 (4.6)6.6 (6.7)5.7 (6.1)4.7 (5.6)5.0 (5.8)No. of 30-day prescriptions, mean (SD)19 23 23 20 21 Received HbA1c test, %5371695861Received retinal exam, %2231322426Received microalbumin urine test, %3347443739Predicted outcomeTotal costs, c mean (SD), $2,657 4,905 3,900 3,534 3,627 No. of inpatient days, mean (SD)0.2 (0.2)0.5 (1.2)0.3 (0.7)0.3 (0.9)0.3 (0.8)No. of ED visits, mean (SD)0.1 (0.1)0.3 (0.5)0.2 (0.3)0.2 (0.3)0.2 (0.3)No. of ambulatory visits, mean (SD)3.6 (2.1)6.5 (2.8)5.3 (2.9)4.7 (2.7)4.9 (2.8)No. of 30-day prescriptions, mean (SD)21 24 24 21 22 Received HbA1c test, %5069635758Received retinal exam, %2027262323Received microalbumin urine test, %2841353333Difference in observed and predicted outcomes dTotal costs, c $−141−391−208−263−249No. of inpatient days−0.03−0.07−0.09−0.03−0.05No. of ED visits0.010.030.0020.030.02No. of ambulatory visits0.140.110.420.020.13No. of 30-day prescriptions−1.3−0.9−0.4−1.3−1.1Received HbA1c test, %32612Received retinal exam, %23712Received microalbumin urine test, %67956Abbreviations: SD, standard deviation; ED, emergency department; HbA1c, hemoglobin A1c.a Adjusted for case mix.b Controlled defined as not having an episode of care with the _International Classification of Diseases, Ninth Revision_ , diagnosis code for uncontrolled diabetes (250.02, 250.03) during the year preceding program eligibility. Uncontrolled defined as having at least 1 such episode. Active defined as receiving personalized telephone counseling; passive defined as declining personalized counseling but receiving educational mailings.c Adjusted to the medical component of the Consumer Price Index in 2008 .d All differences between observed and predicted outcomes were significant at _P_ < .05, calculated by using a paired _t_ test. Differences may not be exact because of rounding. \n\n【29】#####  Table 4. Predicted and Observed Total Health Care Use and Costs for Participants in the TRICARE Diabetes Disease Management Program, United States, 2007-2008 a\n\nOutcomeDiabetes Status/Program Participation Status bAll, n = 29,604Controlled, n = 16,828Uncontrolled, n = 12,776Active, n = 7,536Passive, n = 22,068Observed outcomeTotal costs, c mean (SD), $9,619 13,650 12,922 10,825 11,359 No. of inpatient days, mean (SD)1.1 (6.3)1.9 (8.6)1.4 (7.1)1.4 (7.5)1.4 (7.4)No. of ED visits, mean (SD)1.0 (2.8)1.3 (3.9)1.1 (3.4)1.1 (3.3)1.1 (3.3)No. of ambulatory visits, mean (SD)17 24 23 19 20 No. of 30-day prescriptions, mean (SD)78 87 94 77 82 Predicted outcomeTotal costs, c mean (SD), $10,255 14,627 13,820 11,568 12,142 No. of inpatient days, mean (SD)1.1 (1.5)2.0 (2.8)1.6 (2.2)1.5 (2.1)1.5 (2.2)No. of ED visits, mean (SD)0.9 1.3 (1.4)1.1 (1.2)1.0 (1.2)1.1 (1.2)No. of ambulatory visits, mean (SD)16 22 21 18 19 No. of 30-day prescriptions, mean (SD)84 92 99 84 87 Difference in observed and predicted outcomes dTotal costs, c $−636 e−977 e−898 e−743 e−783 eNo. of inpatient days−0.03−0.15 e−0.2 e−0.03−0.1No. of ED visits0.1 e0.1−0.040.1 e0.1 eNo. of ambulatory visits0.8 e1.2 e1.8 e0.7 e1 eNo. of 30-day prescriptions−6 e−5 e−4 e−6 e−6 eAbbreviations: SD, standard deviation; ED, emergency department; HbA1c, hemoglobin A1c.a Adjusted for case mix.b Controlled defined as not having an episode of care with the _International Classification of Diseases, Ninth Revision_ , diagnosis code for uncontrolled diabetes (250.02, 250.03) during the year preceding program eligibility. Uncontrolled defined as having at least 1 such episode. Active defined as receiving personalized telephone counseling; passive defined as declining personalized counseling but receiving educational mailings.c Adjusted to the medical component of the Consumer Price Index in 2008 .d Differences between observed and predicted outcomes may not be exact because of rounding.e Difference between observed and predicted outcomes significant at _P_ < .05, calculated by using a paired _t_ test.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d7380135-97c7-49c1-8959-e4c04d65af40", "title": "Microbiota That Affect Risk for Shigellosis in Children in Low-Income Countries", "text": "【0】Microbiota That Affect Risk for Shigellosis in Children in Low-Income Countries\nDiarrheal disease contributes substantially to illness and death in children in low-income countries . Recent investigations of enteric illness have shown many cases with >1 pathogen identified . The paradigm of 1 pathogen and 1 disease has been questioned with the advent of microbiological and molecular detection methods that have lower limits of detection. Children in developing countries are exposed to an array of pathogenic organisms. Recent studies have shown a complex relationship between gut microbiota and diarrheal illness; children with severe illness tend to have a less diverse microbiota and a predominance of specific genera of organisms . Molecular-based approaches to pathogen detection enhance the ability to quantify the abundance of pathogens shed in the stool.\n\n【1】Two recent studies of children in low-income countries have highlighted the need for pathogen quantitation. Lindsay et al. using the Global Enteric Multicenter Study (GEMS) specimen collection, found that 80% of controls and 89% of case-patients had detectable levels of _Shigella_ spp. To identify which children had shigellosis, Lindsay et al. determined a quantitative threshold and, when applied, it identified twice as many cases compared with standard culture. Platts-Mills et al. in a study of populations with a high prevalence of malnutrition and enteric infections in Tanzania, compared samples taken before and during diarrheal episodes . They did not find an association between the presence of any pathogen and diarrhea for 15 pathogens studied (rotavirus, adenovirus, astrovirus, norovirus, sapovirus, _Cryptosporidium_ spp. _Giardia lamblia_ , _Campylobacter jejuni_ , _Clostridium difficile_ , _Salmonella_ spp. Shiga-toxigenic _Escherichia coli_ , _Shigella_ spp./enteroinvasive _E. coli_ \\[EIEC\\], enterotoxigenic _E. coli_ \\[ETEC\\], typical enteropathogenic _E. coli_ \\[tEPEC\\], and enteroaggregative _E. coli_ \\[EAEC\\]). However, when they considered quantity of pathogen on a continuous scale, 3 organisms (rotavirus, astrovirus, and _Shigella_ spp.) were associated with diarrhea.\n\n【2】In disease-endemic settings, detection of multiple enteropathogens in asymptomatic and symptomatic children is common . Samples from one third of patients with diarrhea in a hospital study in Kolkata, India, contained >1 pathogen. Negative associations were demonstrated between _Shigella_ spp. and rotavirus and _Shigella_ spp. and _Vibrio cholerae_ . However, a limitation of this study was that it was conducted only in patients with diarrhea. Thus, differential comparisons could not be made between pathogen associations in diarrheal and nondiarrheal samples. A recent study by Taniuchi et al. reported the etiology of diarrheal episodes by using molecular methods in Bangladeshi children during their first year of life. They found that multiple enteropathogens were present by the first month of life in stool specimens from healthy children and from children with diarrhea . If multiple pathogens are present, they might interact to increase or decrease the probability of symptomatic infection. Bhavnani et al. reported synergistic effects in rotavirus– _G. lamblia_ and rotavirus– _E. coli_ infections, in which the presence of these co-occurring organisms increased the probability of disease .\n\n【3】The gut microbiota are composed of thousands of species that might play a role in the risk for diarrhea. Some _Lactobacillus_ and _Veillonella_ species are potentially protective against diarrhea or serve as markers of healthy gut microbiota  Probiotic activity has been associated with some _Lactobacillus_ spp. bifidobacteria, _Veillonella_ spp. _Streptococcus_ spp. _Enterococcus_ spp. nonpathogenic _E. coli_ , and _Saccharomyces boulardii_ . Randomized clinical trials have investigated the role of some _Lactobacillus_ spp. in treating infectious diarrhea and identified that these organisms can provide a benefit in the treatment of acute, infectious, watery diarrhea in infants and young children . In this study, we examined relationships between _Shigella_ spp. _/_ EIEC, microbiota, and diarrhea by using 16S rRNA marker gene surveys of stool specimens from a large international study of diarrhea in children <5 years of age .\n\n【4】### Methods\n\n【5】##### Study Design and Participants\n\n【6】We used stool specimens collected from children <5 years of age who participated in a matched case–control study of moderate-to-severe diarrhea sponsored by the GEMS consortium . In brief, the GEMS was a prospective case–control study of infants and young children at 7 sites in sub-Saharan Africa and southern Asia. Case-patients with moderate-to-severe diarrhea were enrolled when they came to a health clinic. Moderate-to-severe diarrhea eligibility criteria included dehydration (sunken eyes, loss of normal skin turgor, or a decision to initiate intravenous hydration), the presence of blood in the stool (dysentery), or a clinical decision to hospitalize the child. In the GEMS, matching controls (for sex, age, and community) were sampled from a demographic surveillance database of the area and included if they reported no diarrhea within the previous 7 days. For this study, 4 of the 7 GEMS sites (The Gambia, Mali, Kenya, and Bangladesh) elected to participate in further molecular characterization of their samples. When samples were sent for analysis, it was not required that their matched specimen be included, which resulted in disruption of matches. Including only samples with the complete matched set would have limited our sample size. Therefore, we included age and location as covariates in our analysis but did not analyze samples as matched sets.\n\n【7】All specimens for this study were collected during December 2007–December 2009. One specimen was collected for each child at the time of enrollment. The Institutional Review Boards at all participating institutions reviewed and approved the protocol.\n\n【8】##### Specimen Collection\n\n【9】Stool specimens were handled according to the GEMS protocol . DNA was isolated from frozen stool specimens by using a bead beater with 3-mm diameter solid glass beads (Sigma-Aldrich, St. Louis, MO, USA) and, subsequently, with 0.1-mm zirconium beads (BioSpec Products, Inc. Bartlesville, OK, USA) to disrupt cells. The cell slurry was then centrifuged at 16,000 × _g_ for 1 min, and the supernatant was processed by using QIAamp DNA Stool Extraction Kit (QIAGEN, Valencia, CA, USA). Extracted DNA was precipitated with ethanol and shipped to the United States.\n\n【10】##### Detection of EAEC, tEPEC, ETEC, Rotavirus, Norovirus, _G. lamblia_ , and _Cryptosporidium_ spp.\n\n【11】Diagnostic microbiological methods for rotavirus, norovirus, _G. lamblia_ , _Cryptosporidium_ spp. and diarrheagenic _E. coli_ (EAEC, tEPEC, and ETEC) were conducted at each site as described by Panchalingham et al. In brief, _E. coli_ isolates were selected from MacConkey agar plates and tested by using motility indole ornithine medium. EAEC, tEPEC, and ETEC pathotypes were identified by using PCRs for known virulence determinants of each pathogen (ETEC: heat-labile and heat-stable enterotoxins; tEPEC: intimin \\[ _eae_ \\] and bundle-forming pilus; and EAEC: _aatA_ and _aaiC_ genes).\n\n【12】Rotavirus was detected by using the ProSpecT ELISA Rotavirus Kit (Oxoid, Basingstoke, UK). Norovirus genogroups GI and GII were detected by using a multiplex PCR specific for synthesized complementary DNA after viral RNA extraction and reverse transcription. _G. lamblia_ and _Cryptosporidium_ spp. were detected by using commercially available immunoassays (TechLab, Blacksburg, VA, USA) following the manufacturer’s protocols. Although the GEMS study tested for other enteric pathogens, our analysis included only pathogens that were present in ≥10 specimens. This lower limit was chosen after examination of the distribution of positive samples for each pathogen and to avoid statistical testing with small sample sizes.\n\n【13】##### Quantitative PCR for Detection of _ipaH_ Gene in _Shigella_ spp. and EIEC\n\n【14】Each stool DNA specimen was tested by using a quantitative PCR (qPCR) for _Shigella_ /EIEC that included the 7500/700 Fast Real-Time PCR System, software V2·0·5, and SYBR green–based fluorescent dye (Applied Biosystems, Foster City, CA, USA). Details on primer design, PCR conditions, and standard curve analysis have been reported elsewhere . _Shigella_ /EIEC was identified by using primers specific for the _ipaH_ gene . We used a cutoff of 14,000 _ipaH_ gene copies to distinguish children shedding low levels of _Shigella_ spp. from those shedding high levels of _Shigella_ spp. in their stools. The threshold was established by constructing receiver-operating characteristic curves to determine sensitivity and specificity of incremental increases in levels of _ipaH_ compared with disease status and set on the basis of the point that maximized sensitivity and specificity . Stool specimens with high and low levels of _ipaH_ indicate the relative amount of _Shigella_ spp. detected.\n\n【15】##### 16S rRNA Gene Sequencing and Analysis and Identification of _C. jejuni_\n\n【16】DNA was amplified by using universal primers specific for the V1–V3 region of the 16S rRNA gene in bacteria (518R \\[5′-CAATTACCGCGGCTGCTGG-3′\\] and 27F \\[5′-AGAGTTTGATCCTGGCTCAG-3′\\]). Individual reads were filtered for quality by using custom in-house scripts that removed low-quality sequences as described . Remaining high-quality sequences were separated into sample-specific sets according to barcodes. Conservative operational taxonomic units (OTUs) were clustered by using DNACLUST with parameters (–r 1 and 99% identity clusters) to ensure that the definition of an OTU was consistent across all samples . For taxonomic identification, a representative sequence from each OTU was aligned to the Ribosomal Database (RDP)  by using BLASTn  with long word length (–W 100) to detect only nearly identical sequences . Sequences without a nearly identical match to the RDP (>100-bp perfect match and >97% identity, as defined by BLAST) were marked as being unassigned and assigned a unique OTU identifier. If a sample contained taxa classified as _C. jejuni,_ we identified this sample as positive for _C. jejuni_ .\n\n【17】##### Statistical Analysis\n\n【18】Associations between high levels of _ipaH_ and each additional pathogen, stratified by diarrheal status, were assessed for children with moderate or severe diarrhea and for controls by using separate logistic regression models with high levels of _ipaH_ as the dependent variable. Logistic regression with moderate-to-severe diarrhea as the outcome of interest was then conducted to test for the interaction between high levels of _ipaH_ and either 1) pathogenic microorganisms tested for by the GEMS or 2) species identified by 16S rRNA gene sequencing. All models were adjusted for potential confounding caused by location and age with categorical location and age terms in the model.\n\n【19】Statistical modeling was used to examine whether microbes interact to effect diarrhea risk. To assess whether the risk caused by having _Shigella_ spp. and an additional microbe differed from the product of the risks caused by having each microbe separately (i.e. multiplicative interaction), we used a logistic regression model with an interaction term containing the level of _ipaH_ and the additional microbe of interest . To assess whether the excess risk of having _ipaH_ and an additional microbe differed from the sum of the excess risk of having each separately (i.e. additive interaction), we estimated the relative excess risk caused by the interaction (RERI, also called the interaction contrast ratio \\[ICC\\]) . An RERI = 0 indicates no additive interaction, an RERI>0 suggests positive additive interaction, and an RERI<0 suggests negative additive interaction. Ninety-five percent CIs were estimated by using the Hosmer-Lemeshow procedure . All statistical analysis was performed by using SAS version 9·2 (SAS Institute, Cary, NC, USA) and R 2.15 , and p values <0.05 were considered significant.\n\n【20】### Results\n\n【21】A total of 3,035 (1,735 nondiarrheal and 1,300 diarrheal) stool specimens from children <5 years of age from The Gambia, Mali, Kenya, and Bangladesh were examined for _Shigella_ spp. by identification of _ipaH_ by qPCR; for _G. lamblia_ , _Cryptosporidium_ spp. and rotavirus by ELISA; for norovirus by PCR; for EAEC, ETEC, or tEPEC by culture and subsequent PCR; and for _C. jejuni_ by 16S rRNA gene sequencing. Characteristics of samples are shown in Table 1 . Diarrheal samples had a significantly higher number of pathogens (diarrheal mean 1.4, nondiarrheal mean = 0.95; p<0.05).\n\n【22】##### Associations and Interaction Effects of Co-occurring Pathogens\n\n【23】In 69% (278/404) of samples with high levels of _ipaH_ (71% of diarrheal samples with high levels of _ipaH_ and 64% nondiarrheal samples with high levels of _ipaH_ ), we identified an additional pathogen. In samples with low levels of _ipaH_ , we identified an additional pathogen in 69% (1,815/2,631; 80% of diarrheal samples with low levels of _ipaH_ and 62% of nondiarrheal samples with low levels of _ipaH_ ). After adjusting for age and location, rotavirus exhibited a negative association with high levels of _ipaH_ in case-patients (odds ratio \\[OR\\] 0.31, 95% CI 0.17–0.55). In case-patients and controls, no other pathogen showed an association .\n\n【24】Of the 8 pathogens tested, none showed any interaction effects. However, rotavirus was negatively associated with high levels of _ipaH_ , and only 15 samples that had high levels of _ipaH_ were positive for rotavirus; 14 of these 15 samples were diarrheal samples. The presence of rotavirus and high levels of _ipaH_ resulted in an OR of 29 (95% CI 3.8–220) for diarrheal risk. Although this point estimate was much higher than the expected additive effect of 11, this result and results of tests for interaction were not statistically significant .\n\n【25】##### Interaction Effects of _Lactobacillus_ and _Veillonella_ Taxa\n\n【26】A total of 61 _Lactobacillus_ and _Veillonella_ taxa were identified by 16S rRNA gene sequencing. Analyses were limited to 31 taxa that co-occurred in samples with high levels of _ipaH_ ≥10 times. We tested for interaction between high levels of _ipaH_ and the identified 16S rRNA gene taxon by using a logistic regression model adjusted for age and location, including an interaction term between level of _ipaH_ and taxon presence . Five taxa showed significant negative multiplicative interactions identified by an interaction term with p ≤ 0·05. Of these taxa, _Lactobacillus ruminis_ (RERI −1.92, 95% CI −3.36 to 0.47), _Lactobacillus_ DJF-RP24 (RERI −2.44, \\[95% CI −3.93 to 0.95), _Lactobacillus_ KLDS 1.0718 (RERI −1.93, 95% CI −3.56 to 0.29), and _Lactobacillus_ TSK G32.2 (RERI −2.69, 95% CI −4.55 to 0.84\\]) showed additive interactions . The combined effect of high levels of _ipaH_ in the presence of _L. ruminis, Lactobacillus_ DJF RP24, _Lactobacillus_ KLDS 1.0718, or _Lactobacillus_ TSK G32.2 was lower than expected , which suggested an antagonistic interaction or a decreased association with diarrhea when specific _Lactobacillus_ taxa were present than versus when they were absent. When we tested the 4 _Lactobacillus_ taxa against 8 additional pathogens; no additive interaction was observed .\n\n【27】##### 16S rRNA Gene–based Bacterial Community Profiles\n\n【28】The proportional abundance of the 9 most common genera identified by 16S rRNA marker gene sequencing differed among stool specimens with high and low levels of _ipaH_ and among specimens from children with and without diarrhea . Overall composition of diarrheal stool specimens had an increased relative proportional abundance of facultative anaerobes when compared with composition of nondiarrheal stool specimens, even when diarrheal stool specimens had low levels of _ipaH_ (mean diarrheal specimens 0.47, mean nondiarrheal specimens 0.22; p<0.0001). Overall, diarrheal stool specimens with high levels of _ipaH_ had the lowest proportion abundance of _Prevotella_ spp. (0.11), diarrheal stool samples with low levels of _ipaH_ had the second lowest proportional abundance (0.15), and nondiarrheal stool specimens with high and low levels of _ipaH_ had similar proportion abundances for _Prevotella_ spp. (0.25 and 0.24, respectively; p = 0.63).\n\n【29】_Shigella_ spp. identified by the _ipaH_ qPCR were a subset of the genera _Escherichia/Shigella_ identified by 16S rRNA gene sequencing, but could not be distinguished from commensal strains. Diarrheal specimens had a larger proportion of members of the genera _Escherichia/Shigella_ (p<0.0001) than nondiarrheal specimens, regardless of levels of _ipaH_ . However, diarrheal stool specimens with high levels of _ipaH_ had a significantly higher proportion of _Escherichia/Shigella_ sequences (31%; p<0.0001) than did diarrheal stool specimens with low levels of _ipaH_ .\n\n【30】Members of the genus _Veillonella_ was found equally in diarrheal and nondiarrheal stool specimens. Streptococci were found more often in diarrheal stool specimens, regardless of levels of _ipaH_ . There was no association between Shannon diversity indices and levels of _ipaH_ (p = 0.95), although diversity was significantly associated with age, location, and moderate-to-severe diarrhea (p<0.0001, p = 0.004, and p<0.0001, respectively) .\n\n【31】### Discussion\n\n【32】In this study, we used qPCR and 16S rRNA gene sequencing to identify interactions between _Shigella_ /EIEC and co-occurring enteric pathogens or microbes within the gut microbiota in children with moderate-to-severe diarrhea. We explored these interactions in stool specimens obtained from children with and without diarrhea who participated in the international GEMS study. We used detection of the _ipaH_ gene as an indicator of _Shigella_ /EIEC infection because this molecular method is highly sensitive and specific . Use of quantitative identification methods, rather than colonization , is advantageous for identifying true disease associations. We found that 69% (1,815/2,631) of stool samples with high levels of _ipaH_ had a co-occurring pathogen.\n\n【33】This study confirms the findings of the negative association between rotavirus and _Shigella_ spp. in polymicrobial infections in diarrheal patients in India and The Gambia ( _Shigella_ spp./rotavirus: OR 0.36, 95% CI 0.14–0.92) . Although we identified correlations between rotavirus and _Shigella_ spp. as determined by levels of _ipaH_ , no pathogen showed an antagonistic or synergistic interaction on the odds of moderate-to-severe diarrheal illness. Although all but 1 of the rotavirus-positive samples that had high levels of _ipaH_ were associated with diarrhea cases, the negative association between these pathogens gave us limited sample size to assess a possible synergistic effect. A synergistic effect between high levels of _ipaH_ and rotavirus (RERI/ICC 9.9, 95% CI 2.6–28.4) was previously observed in Ecuador, and it was concluded that pathogenic potential appears to be enhanced during co-infection . Our RERI/ICC was 18, but the effect was not significant, possibly because of small sample size.\n\n【34】The GEMS was designed as a matched case–control study of thousands of stool samples from persons matched by age, sex, community, and time. A possible limitation of our methods is that we did not use matched pairs but rather adjusted for country-level location and age by using statistical modeling. This method was used because of the large number of broken matched pairs. Thus, inclusion of all samples greatly increased our sample size. Generally, the lack of an appropriate matched analysis biases the effect on estimates toward the null. However, studies have shown that including samples with missing data and adjusting by using confounding terms is valuable, particularly with a larger sample size . We adjusted for location at the country level, and the GEMS matched case-patients at the community level, which could fail to adequately address bias, particularly when one considers that the distribution of case-patients and controls differed at multiple sites and that _Shigella_ spp. are more common in Bangladesh. An additional limitation of our study design is that, given a cross-sectional study sample set, we were unable to identify temporal associations and to attribute cause and effect. Furthermore, associations between pathogens and taxa may be explained by seasonality, and further work should be conducted to investigate this as a possible explanation. Finally, the 16S rRNA gene-sequencing method used to identify taxa of interest is limited because our identification was only as precise as the RDP attributed taxonomy. Thus, further characterization and species-specific identification are warranted in uncultured bacteria such as _Lactobacillus_ KLDS 1.0718.\n\n【35】A previous study showed cross-sectional differences in stool microbiota of children with diarrhea compared with children without diarrhea in low-income countries . Our study showed that the composition of microbiota is more closely associated with diarrheal status than with co-infection by _Shigella_ spp./EIEC as measured by high levels of _ipaH_ . Microbiota in stools of children without diarrhea but who had high levels of _ipaH_ (i.e. were colonized with _Shigella_ spp./EIEC) were more similar to microbiota of healthy children with low levels of _ipaH_ than microbiota in samples from children with diarrhea and high levels of _ipaH_ .\n\n【36】In low-income countries, infection/colonization with pathogens occurs commonly in persons without diarrhea. Our study found little evidence of interaction between _Shigella_ spp. and co-occurring pathogens. Although the cross-sectional study design precludes strong statements of cause and effect, our data are consistent with the possibility that some _Lactobacillus_ taxa naturally occurring in the gut and are protective against _Shigella_ spp.–induced diarrhea. Future studies should continue to consider the effects of co-occurring species.M.L. K.K. and J.P.N. BMGF OPP42917 to J.P.N. and O.C.S. and BMGF OPP1016839 to J.P.N. and O.C.S.); the National Institutes of Health (grant U19 090873 to James Kaper); and the US National Science Foundation (Graduate Research Fellowship DGE0750616 to J.N.P.).\n\n【37】At the time of this study, Dr. Lindsay was a research analyst and doctoral student at the University of Maryland, Baltimore, Maryland. Her research interests are identification and quantitation of _Shigella_ spp. and how this pathogen interacts with human gut microbiota.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "05c117ba-578e-4c63-91c4-f9054dd78652", "title": "Detection and Full-Length Genome Characterization of Novel Canine Vesiviruses", "text": "【0】Detection and Full-Length Genome Characterization of Novel Canine Vesiviruses\nMembers of the family _Caliciviridae_ are small (30–35 nm in diameter), nonenveloped viruses with a single-stranded, positive-polarity RNA genome of 7.4–8.3 kb. The family consists of the genera _Vesivirus_ , _Lagovirus_ , _Norovirus_ , _Sapovirus_ , and _Nebovirus_ as well as unassigned caliciviruses .\n\n【1】Vesiviruses were originally identified in 1932 in California, USA, in domestic swine with vesicular disease. Since then, vesiviruses have been described in several animal species and humans, and they have been associated with a variety of clinical signs and lesions, including abortion, hepatitis, respiratory disease, diarrhea, myocarditis, encephalitis, mucosal ulcerations, vesicular lesions, and hemorrhagic syndromes . Unlike other caliciviruses, vesiviruses appear to readily cross host species barriers, and the marine ecosystem is believed to constitute a large reservoir of vesiviruses for terrestrial animals .\n\n【2】Vesiviruses have occasionally been detected in dogs with diarrhea and, in some instances, in dogs with glossitis, balanitis, or vesicular vaginitis . However, with the exception of a canine calicivirus, strain 48 , the caliciviruses detected in dogs have been feline viruses . The prototype canine calicivirus (CaCV) strain 48 was identified in Japan in 1990; the virus was isolated from a 2-month-old pup with intermittent watery diarrhea . Strain 48, which is antigenically and genetically unrelated to feline caliciviruses, was tentatively proposed as a true CaCV in the _Vesivirus_ genus . Antibodies to strain 48 have been detected in 57.0% of dogs in Japan  and 36.5% of dogs in South Korea , but no information is available regarding the circulation of analogous viruses in dogs elsewhere.\n\n【3】In 2003, a novel vesivirus (strain 2117) genetically similar to CaCV strain 48 was accidentally isolated as a contaminant in Chinese hamster ovary (CHO) cell cultures by a pharmaceutical company in Germany . These cells are mostly used by biotech companies for the production of recombinant drugs; possible sources of contamination included reagents used for cell cultivation, such as porcine-derived trypsin or fetal bovine serum.\n\n【4】The limited information available does not clarify whether vesiviruses play a role as enteric pathogens in dogs. Considering the ability of vesiviruses to cross the host species barriers  and the close social interactions between humans and dogs, it is essential to determine whether dogs harbor viruses with a zoonotic potential. To further investigate the molecular epidemiology of vesiviruses, we screened fecal specimens from asymptomatic dogs and from dogs with diarrhea in Italy.\n\n【5】### The Study\n\n【6】In 2007, we collected 385 samples from dogs in Bari, Italy. A total of 183 samples were fecal specimens from household dogs (1–6 months of age) hospitalized with signs of mild to severe gastroenteritis (collection A); 88 were rectal swab specimens from clinically healthy juvenile and adult dogs housed in 4 separate shelters (collection B); and 114 were fecal swab specimens from asymptomatic household dogs (1–6 months of age) receiving routine care at 2 veterinary clinics (collection C).\n\n【7】By using reverse transcription PCR with the broadly reactive consensus primers for caliciviruses, p289/p290 , and strain 48–specific primers 493F–526R , we detected vesivirus RNA in 1.1% (2/183), 64.8% (57/88), and 3.5% (4/114) of collection A, B, and C samples, respectively. Partial RNA-dependent RNA polymerase sequences, obtained by using the primers p289/p290 , were determined for 10 samples. The sequences shared closest nucleotide identity (90.7%–92.6%) with vesivirus strains 2117, Geel/2008, Allston/2008/USA, and Allston/2009, which were identified as contaminants in CHO cells; the sequences shared 73.6%–74.8% nt identity with CaCV strain 48. We determined the full-length genomic sequence (8,453 nt) of 1 of the canine vesivirus strains, Bari/212/07/ITA, by using consensus primers and 3′ and 5′ RACE protocols (Invitrogen Ltd, Milan, Italy); the sequence was deposited in GenBank . Primers used for virus detection and sequencing are listed in the Table .\n\n【8】The full-length genomic sequence of strain Bari/212/07/ITA shared 89%–90% nt identity with sequences of viruses identified as CHO cell contaminants; it shared only 71.0% nt identity with the prototype CaCV strain 48. Three open reading frames (ORFs) were predicted by sequence analysis of the nucleotide sequence and by comparing results with the genomic organization and ORFs of other vesiviruses . ORF1 was 5,796 nt in length (nt 12–5807) and encoded a 1,921-aa polyprotein. The ORF1 stop codon was followed by 3 nt and then by the ORF2 start codon. ORF2 was 2,079 nt in length (nt 5811–7889) and encoded a 692-aa capsid protein. ORF3 was 405 nt in length (nt 7886–8290) and encoded a 134-aa protein. ORF2 overlapped with ORF3 by 4 nt . The 163-nt 3′ untranslated region of CHO cell–associated strain 2117 was shorter than the 235-nt region in strain 48. The full-length ORF1-encoded polyprotein of Bari/212/07/ITA shared highest amino acid identity (98.5%–98.9%) with CHO cell–associated vesiviruses; it shared 80.5% aa identity with the ORF1-encoded polyprotein of canine strain 48 and <56% aa identity with other vesiviruses. Within the ORF2-encoded protein, the capsid cleavage motive FRAES (aa 155–159) was conserved. As observed, in CHO cell–associated strain 2117 and canine strain 48, a 7-aa insertion (KTIKSQV) was present in the conserved region D. The full-length capsid protein of Bari/212/07/ITA shared 92.6%–92.9% aa identity with the CHO cell–associated vesivirus strains Allston/2009/USA and Allston/2008/USA from the United States, 90.0% aa identity with the CHO cell–associated isolate Geel/2008 from Belgium, 86.3% aa identity with the CHO cell–associated isolate 2117 from Germany , and only 70.3% aa identity with the prototype CaCV strain 48 isolated in Japan in 1990 . Strain Bari/2012/07/ITA shared < 35.8% aa identity with other vesiviruses .\n\n【9】In previous investigations in dogs, strain 48–like vesiviruses were detected in only 2 (1.7%) of 119 samples: a fecal specimen from a dog with signs of enteric disease and a tonsillar swab specimen from a dog with respiratory disease . However, serologic investigations suggested that vesiviruses actively circulate in the canine population . In our virologic investigations, vesivirus RNA was detected in 64.8% (57/88) of dogs housed in 4 shelters but in only 1.1% (2/183) and 3.5% (4/114) of symptomatic and asymptomatic household dogs, respectively. These findings demonstrate that canine vesiviruses can be widespread in some settings or populations (e.g. kennels or shelters) where high population densities create favorable epidemiologic situations for circulation of some microorganisms.\n\n【10】Of interest, partial sequence analysis of the RNA-dependent RNA polymerase fragment of several of the dog-derived strains and full-length genomic sequencing of strain Bari/212/07/ITA showed that these canine vesiviruses were more similar to some vesiviruses found as contaminants of CHO cell cultures than to the prototype CaCV strain 48. Contamination of CHO cells by vesiviruses was documented in 2003 in Germany , 2008 and 2009 in the United States, and 2009 in Belgium. The 2008–2009 contamination in the United States was estimated to cost US $100–300 million in lost revenues to the biotech company because production was interrupted to provide adequate sanitation and maintenance of the bioreactors . In addition, these contaminations have raised concerns for the potential exposure of humans to these novel viruses because vesiviruses can readily cross the host species barrier .\n\n【11】### Conclusion\n\n【12】Our findings show that genetically heterogeneous vesiviruses are common in dogs in Italy. Further studies are necessary to understand the ecology of this group of vesiviruses; that is, it must be determined whether vesiviruses circulate in humans or other animal species and, above all, whether they are associated with disease in humans, animals, or both.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "dd980fd8-5ab8-4da1-a33a-00c46a6a8f7d", "title": "Clearing the Air: Smoke-Free Housing Policies, Smoking, and Secondhand Smoke Exposure Among Affordable Housing Residents in Minnesota, 2014–2015", "text": "【0】Clearing the Air: Smoke-Free Housing Policies, Smoking, and Secondhand Smoke Exposure Among Affordable Housing Residents in Minnesota, 2014–2015\nAbstract\n\n【1】**Introduction**\n\n【2】During the past 30 years, local and state tobacco use control laws in the United States have helped reduce smoking prevalence and exposure to secondhand smoke, but progress among low socioeconomic populations has been slow. Implementing smoke-free housing policies in affordable housing may help address this issue. The purpose of our study was to assess how such policies affect smoking rates and exposure to secondhand smoke among residents of affordable housing.\n\n【3】**Methods**\n\n【4】We conducted a pretest–posttest longitudinal study of 180 residents from 8 affordable housing properties in Minnesota. Participating properties agreed to adopt a smoke-free housing policy covering indoor grounds, and 3 of these properties also prohibited smoking on all outdoor grounds. Policies were implemented with assistance from local public health departments and the Statewide Health Improvement Program. Participants completed surveys one month before policy implementation and 6 months postimplementation. Surveys assessed smoking, quit attempts, and indoor and outdoor secondhand smoke exposure.\n\n【5】**Results**\n\n【6】Results indicated a significant reduction in nonsmokers’ indoor exposure to secondhand smoke ( _F_ 1,144  \\= 22.69, _P_ < .001) and no change in outdoor exposure to secondhand smoke from Time 1 (pretest) to Time 2 (posttest) ( _F_ 1,140  \\= 2.17, _P_ \\= .14). However, when examining sites that only prohibited smoking indoors, we observed an increase in outdoor secondhand smoke exposure that approached significance ( _F_ 1,118  \\= 3.76, _P_ \\= .055). Results showed no change in quit attempts over time, but 77% of residents who smoked at pretest reported reducing the amount that they smoked at posttest, and an additional 5% reported that they had quit.\n\n【7】**Conclusions**\n\n【8】Smoke-free housing policies may be an effective strategy to reduce exposure to indoor secondhand exposure and promote decreased cigarette smoking among residents of affordable housing.\n\n【9】Introduction\n\n【10】Smoking remains the leading cause of preventable death in the United States, contributing to more than 480,000 deaths per year among smokers and 50,000 deaths among nonsmokers . During the past 30 years local and state tobacco use control laws such as those that prohibit smoking in restaurants and bars have helped reduce smoking prevalence by 40% and exposure of nonsmokers to secondhand smoke by 71% . Despite declines in these indicators for the general population, progress has been considerably slower for populations of low socioeconomic status, who experience a disproportionate burden of harm from smoking . This is particularly true in Minnesota where the rate of smoking is 121% higher among those without a high school degree than among those with more than a high school degree .\n\n【11】Because smoking rates are disproportionately high among populations of low socioeconomic status, the risk of secondhand smoke exposure among these groups is high. Results from a nationally representative study showed that nonsmokers with an annual household income of less than $20,000 were 36% more likely to have elevated serum cotinine levels — a marker of high secondhand smoke exposure — than those with an annual household income of $20,000 or more . Researchers have posited that a primary contributor to socioeconomic disparities in secondhand smoke exposure is unequal access to quality housing, and a primary component of quality housing is a smoke-free environment. People of low socioeconomic status have less access to smoke-free housing than do those of high socioeconomic status and consequently are more likely to live in multi-unit buildings where smoking is permitted in individual housing units . Consequently, people of low socioeconomic status are at increased risk of secondhand smoke exposure in the home environment, which is where people spend 69% of their time .\n\n【12】In 2007, Minnesota passed the Clean Indoor Air Act, which prohibits smoking in most indoor public spaces, including common areas in rental apartment buildings. However, this law did not prohibit smoking in individual units, leaving open the possibility that smoke from one unit could transfer to other units. Investigations of secondhand smoke in multi-unit residential buildings found nicotine concentrations in units without smokers to be comparable to those of units with smokers, suggesting that secondhand smoke infiltration from neighboring units may be substantial . Consistent with these findings, studies found that indoor secondhand smoke exposure among nonsmokers in such housing is common, with 53% of residents reporting any exposure  and 41% reporting frequent exposure .\n\n【13】Smoke-free policies for multi-unit housing can address secondhand smoke transfer by prohibiting smoking in all indoor areas, and — with a stronger policy — all outdoor areas too. Few studies have investigated the effects of smoke-free policies in multi-unit housing on secondhand smoke exposure and quit attempts, but evaluations of other smoke-free indoor policies are promising. For example, smoke-free workplace policies are effective at reducing exposure to secondhand smoke . These policies also appear to decrease cigarette smoking and promote smoking cessation. One study found that smokers who worked in environments that implemented a smoke-free workplace policy were nearly twice as likely to quit as workers without a smoke-free workplace policy, and those who continued smoking reported a significant reduction in number of cigarettes smoked daily .\n\n【14】Findings from these studies suggest that policies prohibiting smoking in multi-unit affordable housing could help decrease tobacco use and secondhand smoke exposure among residents and consequently reduce socioeconomic tobacco-related health disparities . However, little is known about the effects of smoke-free housing policies on smoking and secondhand smoke exposure among low-income populations. To our knowledge, only 2 studies have examined this issue with a focus on low-income residents, and these studies were limited by the use of retrospective self-reports of pre-policy smoking  and a policy that included indefinite grandfathering (ie, allowing smokers residing in the apartment at the time the policy was implemented to continue smoking in their units) . A recent review noted the shortage of research on smoking and secondhand smoke exposure in affordable housing and called for more prospective studies on the effect of smoke-free housing policies on residents’ smoking behavior . The purpose of our study was to determine the effects of smoke-free policies in multi-unit housing on secondhand smoke exposure and tobacco use among low-income populations.\n\n【15】Methods\n\n【16】Eight public housing properties participated in a longitudinal pretest–posttest evaluation. The properties were from 4 regions of Minnesota and comprised a mix of urban and rural communities (population range: 1,700–86,000). A large proportion of residents at participating properties were seniors. The smoke-free policies implemented at each property prohibited smoking in all indoor areas; 3 of the properties also prohibited smoking on all outdoor grounds. Residents were notified of the smoke-free policy between 3 and 12 months prior to implementation. Policies were implemented with the assistance of the staff of the local public health department and with the support of the Statewide Health Improvement Program, a grant program that focuses on increasing physical activity and healthy eating, and reducing tobacco use and exposure to secondhand smoke in Minnesota. Sites were recruited with the help of staff members of the local public health department, who helped identify properties that had agreed to implement a smoke-free multi-unit housing policy. Residents were recruited via flyers posted on bulletin boards at each site. The Minnesota Department of Health institutional review board approved the study.\n\n【17】Time 1 (pretest) data were collected at each site one month before implementation of the smoke-free multi-unit housing policy (from February 2014 through March 2015), and Time 2 (posttest) data were collected 6 months postimplementation (from September 2014 through October 2015). Time 1 surveys were distributed door to door to all residents in participating buildings. Time 2 surveys were distributed only to Time 1 respondents who still lived at the site. Sites could choose from 2 compensation options for participants. Five sites chose to have participants receive a $15 gift card for completing the Time 1 survey and a $20 gift card for completing the Time 2 survey. The other 3 sites chose to do random drawings for several $50 gift cards at both Time 1 and Time 2. Smoking cessation resources were offered to all resident smokers before policy implementation.\n\n【18】A total of 578 residents received a Time 1 survey, and 289 residents completed it (50% response rate). Of these 289 residents, 25 moved out before the Time 2 survey. Of the remaining 264 Time 1 respondents, 180 (62.3%) also completed a Time 2 survey, yielding a 68.2% response rate at Time 2 and a 62.3% retention rate.\n\n【19】The Time 1 survey assessed secondhand smoke exposure, cigarette smoking, quit attempts, and participant demographics. The Time 2 survey was identical to the Time 1 survey, except for additional items assessing smoking cessation methods and reasons for reducing smoking or trying to quit. Demographic characteristics collected were age, race/ethnicity, highest level of education (1 = less than high school degree, 2 = high school graduate or general equivalency degree, 3 = some college, associate degree or vocational, technical, or business school, 4 = bachelor degree or higher, 5 = PhD, MD, JD, or other professional degree), and annual household income (1 = ≤$10,000, 2 = $10,001-$20,000, 3 = $20,001-$25,000, 4 = $25,001-$35,000, 5 = $35,001-$50,000, 6 = $50,001-$75,000, 7 = ≥$75,000). Questions related to secondhand smoke assessed both indoor and outdoor exposure. For indoor exposure, participants were asked 1) how often they smelled or breathed secondhand smoke when inside their own apartments and 2) how often they smelled or breathed secondhand smoke in shared areas such as hallways, stairwells, community rooms, or laundry rooms. For outdoor exposure, they were asked how often they smelled or breathed secondhand smoke in parking lots, lawns, or playgrounds. Responses for indoor and outdoor exposure were rated on a scale of 1 to 5 (1 = never, 2 = hardly ever, 3 = a few times a month, 4 = a few times a week, 5 = every day). Responses to the 2 indoor exposure questions were aggregated to form an indoor exposure scale (α = .74). Smoking status was assessed by asking if residents had smoked 100 cigarettes in their lifetime (yes or no) and if they currently smoked cigarettes (responses: every day, some days, not at all). Participants were classified as current smokers if they reported smoking 100 cigarettes during their lifetime and currently smoked cigarettes “some days” or “every day.” Residents’ quit attempts were assessed by asking whether in the past 6 months they had stopped smoking for one day or longer because they were trying to quit (yes or no). In the Time 2 survey, participants were asked if the amount they smoked changed in the past 6 months (responses: smoked more, smoked less, smoked about the same, quit smoking). Smokers who had reduced their smoking or had tried to quit in the past 6 months were asked their primary reasons for doing so (responses were family reasons, health, cost, or inconvenience \\[ie, not allowed to smoke in their apartments\\]; residents could choose more than one response).\n\n【20】Data were analyzed with SPSS version 22.0 (Stata Corp LP); significance was set at _P_ < .05. Independent sample _t_ tests were used to identify differences in demographics between residents lost to follow-up between the Time 1 and Time 2 surveys and those who were retained. Time 1 to Time 2 changes in nonsmokers’ reported indoor and outdoor exposure to secondhand smoke were examined by using repeated-measures ANOVAs. These analyses were followed by another repeated-measures ANOVA to test for differences in secondhand smoke exposure outdoors among sites that did not prohibit smoking outdoors. Differences in quit attempts from Time 1 to Time 2 were tested using McNemar’s test.\n\n【21】Results\n\n【22】Demographic characteristics of participants retained at Time 2 were not significantly different from those lost to follow-up, except for race/ethnicity, education, and income . Among those retained at Time 2, race, education, and income were not significantly related to the outcomes of interest (all _P_ \\> .05).\n\n【23】The final sample of 180 was 68% female, 73% white, 23% black, and had a mean age of 63 years (range: 21–99). The proportion of current smokers at Time 1 was 15%. Twenty-nine percent of the sample had less than a high school education, 82% had less than a bachelor’s degree, and 72.5% earned $20,000 a year or less.\n\n【24】A repeated-measures ANOVA indicated a significant decrease in nonsmokers’ reported exposure to secondhand smoke indoors from Time 1 (44.0%) to Time 2 (23.6%), _F_ 1,144  \\= 22.69, _P_ < .001 . Conversely, there was no significant difference from Time 1 to Time 2 in outdoor secondhand smoke exposure, _F_  = 2.17, _P_ \\= .14. Follow-up analyses tested for a difference in outdoor secondhand smoke exposure among nonsmokers living in properties that did not prohibit smoking outdoors (n = 119). Results revealed a marginally significant increase in outdoor secondhand smoke exposure from Time 1 to Time 2, _F_  = 3.76, _P_ \\= .055 .\n\n【25】Results from McNemar’s test indicated no significant difference in the proportion of smokers who reported making a quit attempt in the past 6 months at Time 1 (77.8%) and Time 2 (63.0%), _P_ \\> .05.\n\n【26】At Time 2, 77% of smokers reported reducing the amount that they smoked in the past 6 months, and an additional 5% reported that they had quit. Those who had quit, tried to quit, or reduced the amount they had smoked in the past 6 months reported their reasons for doing so at Time 2. The most commonly cited reasons for quitting, trying to quit, or reducing smoking were inconvenience attributable to not being able to smoke in the apartment (52%) and health (52%). The next most common reasons were cost (40%) and family (25%).\n\n【27】Discussion\n\n【28】Results of this study suggest that smoke-free policies in affordable multi-unit housing can reduce residents’ exposure to secondhand indoor smoke. This finding is consistent with that of Pizacani and colleagues , who found that frequent secondhand smoke exposure inside apartments dropped from 41% to 17% after implementation of a smoke-free policy. Although indoor exposure to secondhand smoke decreased significantly from Time 1 to Time 2, 23.6% of residents reported secondhand smoke exposure at least a few times per month after the policy, suggesting enforcement and compliance issues. Lack of compliance with the policy can undermine health benefits associated with smoke-free housing. Future studies should use long-term follow-up surveys to examine whether compliance issues diminish over time and also conduct landlord interviews to investigate resident complaints and enforcement practices.\n\n【29】Analyses examining changes in outdoor exposure to secondhand smoke showed no difference from Time 1 to Time 2 for all nonsmokers. However, further analyses indicated a marginal increase in outdoor secondhand smoke exposure among nonsmokers living in properties that did not restrict smoking outdoors. These findings suggest that secondhand smoke exposure may increase in locations not covered by the smoke-free policy after its implementation. The observed increase in outdoor secondhand smoke exposure may be due to smokers obeying the smoke-free policy but smoking in locations that are unavoidable by nonsmoking residents (eg, outside the front entryway, in the parking lot). These findings highlight the importance of implementing comprehensive smoke-free policies that cover all property grounds rather than indoors only.\n\n【30】There was no change in quit attempts in the past 6 months from Time 1 to Time 2. Residents were informed well in advance of the implementation date — 6 months or more, in most cases — that the smoke-free policy would be implemented. Therefore, residents may have attempted to quit smoking before the implementation date in anticipation of the policy going into effect, resulting in an inflated number of baseline quit attempts. The proportion of smokers who made a quit attempt in the 6 months before implementation (77.8%) was substantially greater than the proportion of smokers in the general population who are seniors (mean age was 63 in the current study) and made a quit attempt in the past 12 months (38.8%) . Future studies should assess quit attempts before residents are informed about a forthcoming smoke-free policy to establish baseline quit attempts.\n\n【31】The percentage of Time 1 smokers who successfully quit smoking at Time 2 (4.5%) was comparable to previous estimates of the annual quit rate for smokers in the general population in the absence of an intervention (2.6%) , suggesting that the policy did not substantially increase quitting. These findings are inconsistent with a recent evaluation of smoke-free multi-unit housing policies among low-income residents that found a quit rate of 22.1% post implementation; however, this quit rate was observed 18 months following implementation of the smoke-free policy, a follow-up period 3 times as long as in this study . Although quitting smoking did not increase in our study, most smokers (77%) did report reducing the amount that they smoked at the 6-month follow up, a figure that is substantially greater than that found in the Pizacani et al study (49%) . This finding suggests that, although residents were not more apt to make a quit attempt or successfully quit smoking, they did reduce their smoking. Residents may have planned to reduce the amount they smoked before making a quit attempt with the hope that they would increase their chances of success. Previous research has found that, although the health benefits of reducing smoking are modest to negligible , reducing smoking is predictive of later cessation among smokers who are motivated to quit . Thus, it is conceivable that many smoking residents were in the early stages of quitting at the Time 2 assessment. Long-term follow-ups with multiple assessments are needed to test this hypothesis.\n\n【32】Among Time 1 smokers who had quit, tried to quit, or reduced the amount that they smoked at Time 2, the most commonly cited reason for changing their smoking behavior was the inconvenience of not being allowed to smoke in their apartment. Having to go outside or off property grounds to smoke — a direct result of a smoke-free housing policy — was enough of a deterrent that residents reported smoking less after the policy was implemented. This finding has implications for cessation interventions, because inconvenience was cited as frequently as health, and more so than other frequently targeted cessation motivations, such as familial concerns and the high cost of smoking.\n\n【33】A limitation of this study was that we did not measure the number of cigarettes smoked at Time 1 and Time 2. Results suggested a reduction in smoking from Time 1 to Time 2, but it is possible that residents knew the study was focused on smoking behavior and responded in a socially desirable way — by reporting that they were smoking less than they actually were after the policy. Future studies should include objective measures of smoking (eg, blood cotinine) and smoking reduction measures that assess reduction by 50% or more to facilitate comparison with other smoking interventions . Second, we had no control group in this study, which limits inferences about causality. For example, a substantial proportion of smokers reported reduced smoking at Time 2, but the lack of a control group made it difficult to determine whether this effect was significant. Post hoc 1-sample _t_ tests were conducted to test whether the proportion of smokers who quit or reduced their smoking at Time 2 (82%) was significantly different from the proportion of smokers in the general population who made a quit attempt in the past year (ie, stopped smoking for 1 day or longer when trying to quit \\[38.8%\\]) . Results of this test did indicate a significant difference ( _P_ < .001); however, larger studies that include a valid comparison group (eg, affordable housing residents living in buildings that have not implemented a smoke-free policy) are needed to corroborate these findings. Third, a convenience sample composed primarily of senior residents was used; therefore, results may not generalize to other affordable housing populations. Fourth, this study did not assess whether smokers changed where they smoked post implementation. These data would have provided more insight into the observed effects on indoor and outdoor secondhand smoke exposure and should be collected in future studies.\n\n【34】Populations of low socioeconomic status have disproportionately high rates of smoking and exposure to secondhand smoke . Results from this study suggest that smoke-free multi-unit housing policies in affordable housing properties could reduce secondhand smoke exposure and cigarette smoking among low-income populations. Although more research is needed to investigate strategies to address compliance and enforcement issues, implementing smoke-free multi-unit housing policies in affordable housing may be a promising step toward eliminating tobacco-related disparities.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "894a9485-72f1-44bc-9382-0319425495c1", "title": "Unexplained Deaths and Critical Illnesses of Suspected Infectious Cause, Taiwan, 2000–2005", "text": "【0】Unexplained Deaths and Critical Illnesses of Suspected Infectious Cause, Taiwan, 2000–2005\nIn 2003, the outbreak of severe acute respiratory syndrome (SARS) demonstrated that the world has become a global village in which human risk for exposure to different kinds of biological hazards is increased through frequent travel and commercial activities . Historically, emerging diseases occur abruptly in outbreaks of unknown cause. Although various efforts have been proposed and conducted to analyze secondary data periodically , they generally provide information for the less urgent decision making in health policy and may not be in time for infectious disease control. Thus, a task force is needed to provide timely and accurate diagnosis for early control of any potential epidemic infection, especially in a newly developed country like Taiwan, where the healthcare resources may not be evenly distributed and autopsy for diagnosis is not widely accepted culturally.\n\n【1】In 2000, the Taiwan Centers for Disease Control collaborated with academic institutions, medical examiners, local health authorities, and experts from different fields to establish a nationwide surveillance center for outbreak and unexplained death investigation due to unknown infectious causes (COUNEX) . This effort was to build Taiwan’s capacity for detecting and responding to uncommon and unrecognized pathogens, which was conceptually the same as that of the study of Hajjeh et al. We defined the surveillance case-patient as a previously healthy resident who died or was admitted to a hospital with a life-threatening illness possibly caused by infection of unidentified etiology. Usually the death occurred within 3 days of the patient’s admission. Patients were excluded if the cause of death was noninfectious. A life-threatening illness was defined as any illness requiring admission to an intensive care unit or report as being critical. An infectious disease is generally suspected if the case-patient has \\> 1 of the characteristics such as fever, leukocytosis, histopathologic evidence of an acute infectious process or more specific symptom patterns, or infection precipitating adult respiratory distress syndrome, renal failure, or sepsis.\n\n【2】A total of 130 cases were reported during 2000–2005, for an annual average rate of 0.12 cases per 100,000 persons. The annual incidence rates varied by year and among 4 branches of Taiwan Centers for Disease Control . The highest rate was in the eastern branch, where surveillance was conducted in a well-defined population of ≈596,119 persons. Ninety-five (73%) of the case-patients died. For 47 (49%) of those who died, an autopsy was performed, a rate much higher than the national autopsy rate of <11% . The mean age of case-patients was 33.8 years. The incidence rates varied by age group; it was highest in those 85–89 years of age, followed by those <1–4 years, and then 65–69 years, with 0.48, 0.30, and 0.23 per 100,000 person-years, respectively. Men had a higher incidence rate than women (0.16 vs. 0.10 per 100,000 person-years).\n\n【3】Approximately 10% of 130 case-patients and 16% of 81 patients with cases of infection had a history of animal contact; 9% of 130 case-patients and 10% of infection case-patients had a history of travel outside Taiwan within the previous 3 months. The most common initial syndromes were acute respiratory (59%), acute neurologic (22%), and acute diarrhea-related syndrome (13%). Initially, 8 patients had acute heart-related syndrome, and 11 had acute kidney-related syndrome; both of these syndromes had a 100% case-fatality rate.\n\n【4】The Appendix Table lists all the infectious pathogens and noninfectious causes identified among 95 fatal cases. One third were related to bacterial infection and one fourth to viral infection; 22 remained unclassified. The proportion of explained cases was lower among patients who survived (74%) than that among patients who died (77%). The proportion of explained cases was also higher for patients who underwent autopsy (83%) than for nonautopsied patients (71%) but not statistically significantly so. Explained cases were similar to unexplained cases in terms of patient age and interval between dates of disease onset and report (median 7.2 and 6.8 days, for explained and unexplained cases, respectively). Although the overall case-fatality rate was 73%, patients were more likely to die if they had multiple organ system involvement.\n\n【5】We have established the infrastructure needed to detect critical and fatal cases of unknown causes; such a surveillance system is essential to identify early potential infectious threats in a period of globalization and increasing travel between countries. The contributions of our surveillance system are demonstrated by early detection and control of at least 3 outbreaks of serious viral diseases: hantavirus pulmonary syndrome, rabies, and SARS.\n\n【6】In 2001, a family cluster occurred in Huanlian city; dyspnea, cough, leukopenia, and pulmonary edema developed in both parents, who died. Their 16-year-old daughter was also ill, but she survived. COUNEX quickly intervened, and hantavirus pulmonary syndrome was confirmed by positive serologic test results, which led to an early control of local rodents and spread of the disease.\n\n【7】Taiwan has been free of human and animal rabies since 1961. However in 2002, a 45-year-old woman from mainland People’s Republic of China was admitted to a hospital because of difficulty in swallowing, fear of wind (aerophobia), and numbness of the arms. Her condition was reported to the surveillance system as suspected rabies. Our personnel quickly confirmed the diagnosis by reverse transcription–PCR and DNA sequence analysis of the samples from cerebrospinal fluid, saliva, and trachea while the patient was still alive . The patient had been bitten by a domesticated dog in mainland China 2 months earlier.\n\n【8】During the SARS outbreak in 2003, the surveillance system received reports of 6 cases; autopsies were performed on 3 patients. As a result, the correlation between clinical course and pulmonary pathology at different stages of the disease was possible, and corroborative evidence for control measures was provided .\n\n【9】Had the surveillance system for unexplained death and critical illness not functioned normally during these 3 outbreaks, more people in Taiwan would have been ill and died from the diseases because of the high population density on this island. This system was particularly useful for infection control at remote regions with limited resources. Most physicians in the rural eastern part of the country have less access to consultation and referral to other specialties in medical centers and teaching hospitals. Thus, they rely more on this kind of surveillance system for early detection of potential infectious threats. This was especially important for acute unexpected deaths, as was demonstrated by a higher incidence and autopsy rates in eastern Taiwan.\n\n【10】Throughout this project, we have increased the autopsy rate and established a population-based bank of specimens for future research. This collection could provide a better opportunity for corroboration or refutation of any previous diagnosis of infectious disease. This improved decision making in regard to control of infections was demonstrated in November 2003, when influenza virus (H5N1) was diagnosed in a patient who had a previous misdiagnosis of SARS .\n\n【11】Because emerging and reemerging infectious diseases may quickly travel between different countries, the system is becoming more crucial for early detection and control of potential health hazards. The system depends on close cooperation among different disciplines and staff from different agencies. Thus, education, empowerment, and good feedback incentives should be continually offered to keep this system sustainable.\n\n【12】Dr T.-H. Wang is the head of the public relations office of Taiwan CDC. She was the previous coordinator of COUNEX and conducted investigations for infectious disease outbreaks. Her research interests include governmental strategic planning and formulation and evaluation of response measures for public health disasters.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "10f9e65f-46c3-4c53-88bd-c8d89b27ed00", "title": "SARS-CoV-2 Vaccine Effectiveness against Omicron Variant in Infection-Naive Population, Australia, 2022", "text": "【0】SARS-CoV-2 Vaccine Effectiveness against Omicron Variant in Infection-Naive Population, Australia, 2022\nUntil February 2022, Western Australia (WA), Australia, successfully delayed sustained transmission of SARS-CoV-2 by using rigorous international quarantine and state border entry restrictions, supported by comprehensive local outbreak control measures when breaches occurred. Data corroborating no substantive community transmission of SARS-CoV-2 in WA until February 2022 include the incidence of reported laboratory-confirmed SARS-CoV-2 infections  and serologic testing of blood donors. By the time WA lifted restrictions and experienced the first wave of SARS-CoV-2 infection, >90% of the state’s residents \\> 16 years of age had received \\> 2 doses of a SARS-CoV-2 vaccine . The WA public health laboratory, PathWest Laboratory, determined which SARS-CoV-2 variants were circulating in the state during the first pandemic wave. During the study period (February 1–May 31, 2022), all 2,695 specimens for which lineage could be assigned were the Omicron variant, of which 2,668 (99%) were designated as BA.1 or BA.2 sublineage .\n\n【1】This unique pandemic experience enabled real-world assessment of vaccine effectiveness (VE) without potential bias caused by population-level background immunity resulting from prior infection. With this study, we assessed VE for mRNA vaccines and viral-vectored SARS-CoV-2 vaccines against laboratory-confirmed infection and severe illness caused exclusively by the Omicron variant in a SARS-CoV-2–naive population. This work was approved by the WA Department of Health Human Research Ethics Committee (RGS0000005522).\n\n【2】### Methods\n\n【3】##### Study Design\n\n【4】We used a test-negative case–control design to compare persons \\> 16 years of age who had positive PCR SARS-CoV-2 test results with matched controls who were SARS-CoV-2 negative. We compared the odds of vaccination among those with PCR-confirmed SARS-CoV-2 infection or severe COVID-19 with the odds of vaccination among matched PCR-confirmed negative controls.\n\n【5】##### Data Linkage\n\n【6】In early 2020, WA created a statewide linked data repository to collect information essential for public health management of the pandemic, including SARS-CoV-2 vaccinations that began in February 2021 . In brief, COVID-19 vaccination data from the Australian Immunisation Register (AIR) were linked to existing WA Department of Health data collections, including statewide hospital admission data, mortality data, and SARS-CoV-2 pathology test results.\n\n【7】##### Ascertainment of Infection\n\n【8】All persons with \\> 1 SARS-CoV-2 PCR tests performed within the study period were eligible for inclusion. We included in the analysis only a person’s first positive or negative PCR test result during the study period. Persons who initially tested negative but subsequently tested positive during the study period were removed from the negative test pool and reclassified as a case-patient (i.e. each person appeared once in the dataset, as either a case-patient or a control). Because the reason for SARS-CoV-2 testing was not recorded in the data repository, we excluded persons with >20 test results reported during January 2020–May 2022 because those persons were probably subjected to regular asymptomatic screening for employment purposes. We also excluded persons with only SARS-CoV-2 rapid antigen test (RAT) results because negative RAT results were not required to be reported and compliance with mandatory reporting of positive RAT results is unknown.\n\n【9】##### Vaccination Data\n\n【10】For this analysis, we established vaccination status at the time of the person’s eligible SARS-CoV-2 PCR test result by linking the person to their respective SARS-CoV-2 immunization history in AIR. We included in our analysis persons who received any dose of vaccine \\> 14 days before the eligible PCR result and excluded persons who had received a vaccine dose <14 days before the eligible PCR result. We defined unvaccinated persons as those who had no record of receiving a SARS-CoV-2 vaccine dose before their linked PCR test result.\n\n【11】The vaccines analyzed were the adenovirus-vectored vaccine produced by AstraZeneca  and the mRNA vaccines produced by Pfizer-BioNTech  and Moderna . We defined a homologous 3-dose vaccination schedule as 2 doses of an mRNA vaccine and an mRNA booster dose, regardless of brand/manufacturer, and a heterologous 3-dose vaccination schedule as 2 doses of ChAdOx1 followed by an mRNA booster dose, regardless of brand/manufacturer. Matched pairs in which the case-patient or control had received a protein-based SARS-CoV-2 vaccine, a ChAdOx1 booster dose, or a mixed 2-dose primary schedule (ChAdOx1 and an mRNA vaccine) were few; we excluded those pairs from subanalysis examining heterologous and homologous vaccine combinations.\n\n【12】##### Hospitalization and Mortality Data\n\n【13】During the study period, we extracted data from the WA Hospital Morbidity Data Collection  and linked them to all SARS-CoV-2 test results for specimens collected. We defined a SARS-CoV-2 hospitalization as \\> 1 inpatient admissions 0–7 days after the date of a positive PCR result. To reduce potential bias that might be introduced through routine preadmission SARS-CoV-2 screening, we excluded admissions for patients indicated by the specialty of the admitting clinician to be unlikely to have been hospitalized for treatment of COVID-19 and for patients admitted for boarding purposes . We also performed supplementary sensitivity analyses, which included all hospital admissions regardless of admitting clinician specialty or those defined as a COVID-19 hospitalization on the basis of select codes from the International Statistical Classification of Diseases and Related Health Problems, Tenth Revision, Australian Modification (ICD-10-AM).\n\n【14】The WA Registry of Births, Deaths and Marriages , the mandatory repository for death reports in WA, provided mortality data. We defined SARS-CoV-2–associated deaths as death from any cause 0–30 days after an eligible PCR test and included those cases in the analysis. We defined severe disease as SARS-CoV-2 hospitalization, an associated death, or both.\n\n【15】To identify pre-existing medical conditions that could potentially increase a person’s risk for SARS-CoV-2 infection or severe disease, we reviewed any hospital admissions in the 24 months before their eligible PCR test. We selected 14 diagnostic categories and the ICD-10-AM codes used to define them, based on previous literature  . If a person had any of the ICD-10-AM codes listed as either a principal or secondary diagnosis for an admission in the 24 months before their eligible PCR test, they were assigned a score of 1 for the corresponding diagnostic category. We then summed the total number of unique diagnostic categories assigned a score of 1 for each person, yielding a score of 0–14 (0 if there had been no admissions or only admissions that did not include any of the selected ICD-10-AM codes). We used this aggregate comorbidity score as a proxy index of underlying conditions and controlled for it in the analyses.\n\n【16】##### Statistical Analyses\n\n【17】We assessed VE by using conditional logistic regression of matched case–control pairs. We calculated adjusted odds ratios (aORs) by using the clogistic function in the Epi package in R version 4.1.0 . Vaccine effectiveness was defined as (1 – aOR) × 100 and is presented with 95% CIs. We assessed differences between odds ratios by calculating the absolute difference between the log odds and the SEs of the difference. We calculated p values by using z-scores; to account for multiple testing, we considered p<0.01 significant.\n\n【18】Case-patients and controls were matched on week of eligible PCR test, age group (16–19 years, then 10-year intervals thereafter), sex, Aboriginality (defined as Aboriginal and/or Torres Strait Islander ancestry), Index of Relative Socioeconomic Advantage and Disadvantage decile  (a score of 1 demonstrating relatively greater disadvantage and a score of 10 indicating a relative lack of disadvantage), and comorbidity score using by the MatchIt package  on a 1:1 basis. We randomly sorted all eligible case-patients and controls by using the sample function before matching.\n\n【19】We calculated effectiveness by comparing unvaccinated persons with those who had received either 2 or 3 doses of vaccine. We also performed subanalyses to examine VE for breakthrough infection by time since last vaccine dose and between the homologous and heterologous 3-dose vaccination schedule. Analysis of VE against severe disease included positive case-patients (by PCR) with severe disease who, along with their matched controls, received 2 or 3 vaccine doses.\n\n【20】### Results\n\n【21】A total of 1,306,453 PCR tests were reported for specimens collected during February 1, 2022–May 31, 2022, from which 188,950 positive case-patients and 188,950 negative matched controls were eligible for inclusion in the analysis . Of the 377,900 study participants, 30,420 (8%) were unvaccinated at the time of testing, 84,237 (22%) had received 2 doses, and 263,243 (70%) had received 3 doses.\n\n【22】##### VE against Breakthrough Infection of Any Severity\n\n【23】Overall, in adjusted analyses comparing those who were unvaccinated with those who had received 2 vaccine doses, VE for preventing PCR-confirmed infection of any severity was 24.9% (95% CI 21.2%–28.4%), increasing to 42.0% (95% CI 40.2%–43.6%) for persons who had received 3 doses . Breakthrough infection of any severity waned notably after a booster dose. For those who had received a booster dose 15–29 days before testing, VE was 70.7% (95% CI 67.4%–73.7%) but fell to 13.5% (95% CI 5.6%–20.8%) for those whose booster was administered \\> 120 days before testing . The median time between the most recent vaccine dose and PCR test date was 21 days longer for persons who received 2 doses (101 days) than for those who received 3 doses (80 days); given the waning immunity after 3 doses, it is possible that increased time since vaccination contributed to lower VE estimates for 2 versus 3 doses.\n\n【24】##### VE against Severe Disease\n\n【25】Hospitalizations and deaths in the study cohort were rare. Among the 188,950 matched case-patients, there were 264 deaths within 30 days of testing and 1,456 hospital admissions within 7 days, excluding persons admitted under clinician specialties .\n\n【26】Overall, VE against severe disease for 2 doses of vaccine was 41.9% (95% CI 4.8%–64.5%) and increased to 81.7% (95% CI 73.9%–87.2%) for 3 doses . The number of case-patients with severe disease in individual time strata was insufficient to permit meaningful VE estimates to be generated by time since last vaccination.\n\n【27】##### VE for Homologous versus Heterologous 3-Dose Series\n\n【28】Our analysis of VE by different vaccine series was restricted to 100,142 case-patients who received either a homologous or heterologous series of 3-dose vaccination , as defined previously, and their matched controls. Persons who were vaccinated according to the heterologous versus homologous schedule were older (median age at test 65 vs. 38 years), more likely to be male, and more likely to have \\> 1 comorbidity .\n\n【29】VE was higher, although not significantly, up to 60 days after administration of the booster dose among persons who received the homologous or heterologous 3-dose vaccination series. However, VE was significantly higher among those who received a heterologous 3-dose vaccination series 60 days through <120 days of follow-up .\n\n【30】Restricting the samples to those <60 years of age to account for differences in behavior that may contribute to differential breakthrough infection rates yielded similar results. We observed the same pattern of waning immunity in the homologous and the heterologous groups, and protection was significantly higher from 60 to <120 days of follow-up (data not shown) for those on the heterologous schedule\n\n【31】Consistent with the results for breakthrough infection, VE against severe disease was ≈10% higher for those on the heterologous schedule (85.7%, 95% CI 73.1%–92.4%) than on the homologous mRNA schedule (75.8%, 95% CI 61.5%–84.8%), although that difference was not statistically significant in the context of a relatively small number of severe outcomes (p = 0.19). Low numbers of severe outcomes also precluded further analysis of effectiveness by time since vaccination.\n\n【32】##### Sensitivity Analyses\n\n【33】We performed sensitivity analyses to explore the potential effects of measurement bias, including the effect of using different options for defining disease severity and selecting covariates for matching case-patients to controls. We found no statistically significant effect on VE for severe disease when we restricted hospitalizations to those likely to be for COVID-19 by selecting specific ICD-10-AM codes used for the primary diagnosis instead of the specialty of the admitting clinician (point estimates for VE after 3 doses changed from 79.2% when restricted by primary diagnosis codes to 81.7% when restricted by admitting clinician specialty). In addition, we explored using different time frames to capture hospitalizations after a positive PCR, specifically 0–14, 2–14, and 2–9 days. VE estimates for severe disease did not differ significantly when we varied the window for hospitalization from 0–7 days to these alternate time frames.\n\n【34】Last, we re-ran the analyses while individually and sequentially adding covariates of interest to the matching algorithm to explore potential effects of confounding. Matching by age group and week of PCR test was sufficient to produce stable VE estimates that were statistically similar to those observed when matching on the full set of covariates for infection of any severity and severe disease.\n\n【35】### Discussion\n\n【36】The ability of WA to limit the introduction of SARS-CoV-2 and prevent sustained local transmission 2 years into the pandemic provides a rare opportunity to assess VE in a population without potential confounding from prior asymptomatic or undiagnosed infection, factors that could affect VE assessments performed in almost all other settings. An assessment of VE in Sydney, New South Wales, Australia, also cited low rates of SARS-CoV-2 background infections in its study population, but WA and metropolitan Sydney have had different pandemic experiences . Cohort follow-up for the New South Wales study began in January 2022, by which time 157,880 SARS-CoV-2 infections had been reported in the Sydney metropolitan area, largely the result of a protracted outbreak of the Delta variant in 2021 . Given that an estimated 40% of all SARS-CoV-2 infections are thought to be asymptomatic  and that the degree of underascertainment of those with mild illness who do not seek testing is unknown, it is difficult to quantify with certainty the extent of prior SARS-CoV-2 infection in the Sydney cohort. In contrast, the very low number of locally acquired infections identified before mid-February 2022 in WA (in the context of high rates of testing and robust contact tracing) and the extremely low rate of nucleocapsid antibody positivity among WA blood donor specimens collected during late February–early March 2022  provide convincing evidence that prior SARS-CoV-2 infection was close to negligible among the population used for our analysis.\n\n【37】Most of our key findings are consistent with those from previous studies . First, overall VE against breakthrough infection of any severity across the full study period for persons who received 2 doses of vaccine was low (24.9% vs. 42.0% for those who received 3 doses). Subanalyses demonstrated that for those who received a booster dose, VE against any infection was near 71% at 15–29 days after vaccination but declined to <14% by 120 days **.**\n\n【38】Second, protection against severe disease after 3 vaccine doses was much higher than that after 2 doses; overall VE was estimated to be >80%. This finding is relevant because there has been concern that the level of protection afforded by vaccines based on the spike protein of the ancestral strain would be inadequate against Omicron variants, necessitating development of new bivalent vaccines designed to enhance the immune response to Omicron-specific epitopes .\n\n【39】This study showed considerable protection from monovalent vaccines against clinically severe illness during an exclusively Omicron wave among a population with negligible background immunity from exposure to the ancestral lineage or previous variants of concern. Our study definitively demonstrates that ancestral strain vaccines still provide substantial protection against severe disease caused by newer variants among a population free from potential confounding of previous immunity conferred by natural infection with an earlier variant.\n\n【40】Vaccine-derived cross-protection observed in our setting may have implications for SARS-CoV-2 vaccine science going forward, specifically for assessing the need to continually design new variant-specific vaccines as the virus evolves. Although greater follow-up time is needed, these data support the hypothesis that vaccine effectiveness against severe disease caused by Omicron is likely to be substantially higher than the estimates against symptomatic disease, as has been observed for previous variants of concern .\n\n【41】We observed that a heterologous schedule consisting of 2 primary doses of ChAdOx1 followed by a booster dose of an mRNA vaccine provided significantly greater protection against infection >60 days after the last dose, compared with a homologous 3-dose series of mRNA-based vaccines (mostly 3 doses of BNT162b2). This finding is unlikely to be explained by recipients of the heterologous vaccine schedule being inherently less prone to COVID-19 because after the association between ChAdOx1 and thrombotic events was identified, ChAdOx1 was almost entirely administered to persons >60 years of age across Australia. However, in accordance with guidance in Australia about brand-based differences for recommended first- and second-dose intervals, the median time between 2 doses of an mRNA primary series in our cohort was 27 days; for a ChAdOx1 primary series, it was 84 days. It is therefore possible that the longer interval between doses contributed to the more durable protection we observed with the heterologous schedule.\n\n【42】A study of Omicron infections in England found that VE against symptomatic illness was similar between those who received 2 doses of either ChAdOx1 or BNT162b2 followed by a booster dose of BNT162b2, specifically 62.4% and 67.2% given 2–4 weeks after the booster, falling to 39.6% and 45.7%, respectively, after \\> 10 weeks . One key difference between our setting and that of the study in England is that in late 2020, the United Kingdom began recommending up to 12 weeks between the first and second doses of BNT162b2, a practice uncommon in the WA cohort, and a longer interval between BNT162b2 doses has been shown to enhance immunogenicity .\n\n【43】Alternatively, the superior performance of the heterologous schedule in protecting against Omicron infection in WA may be a real phenomenon, unmasked without interference from substantial levels of background immunity caused by prior infections with earlier variants. The enhanced immune response, and in some instances clinical protection against non-Omicron variants, produced by heterologous vaccination schedules has been documented in a variety of settings .\n\n【44】Among the limitations of our study, our analysis was restricted to using PCR test results from licensed laboratories to determine a person’s status as a case-patient or control and excluded self-administered RATs reported by the general public. This approach was necessary because RAT results were not systematically linked to other datasets used in this analysis. However, even if possible, including RAT results in our setting would have been methodically undesirable because reporting of negative RAT results is not mandatory and the degree of underreporting of positive RAT results by the public is not quantifiable. Furthermore, because there may be relevant differences between those who seek PCR testing and those who choose to self-administer a RAT, including only positive RAT results would have the potential to introduce significant bias when using a VE study design that explicitly relies on matching test-positive case-patients to similar test-negative controls.\n\n【45】Another limitation is that although we attempted to control for relevant confounders, we were limited by the information available in the data repository, and the potential for residual confounding remains. For example, without knowing the reasons for obtaining a PCR test, we could not exclude tests performed for asymptomatic screening, perhaps before an elective hospital admission or for work requirements. Instead, to account for this limitation, we created a surrogate by excluding all tests for persons with >20 tests over the study period and admissions to hospital services not likely to be treating COVID-19. Likewise, because we were not able to directly access information about a person’s comorbidities, we created a comorbidity score based on hospital ICD-10-AM codes accumulated in the previous 2 years. Although we believe those strategies reduced confounding, they are probably imperfect proxies.\n\n【46】Among the strengths of this study, first, the entire cohort of persons \\> 16 years of age who were tested for SARS-CoV-2 by PCR was eligible for inclusion, which resulted in a large sample, probably representative of the population at risk during the study period. Second, we accessed all SARS-CoV-2 PCR results from every private and public clinical laboratory in the state notified to the WA Department of Health as required by law. Third, we used a comprehensive linked data collection, which included hospitalization data from public and private hospitals and mortality data for the entire state. Fourth, vaccination status was obtained from a population-based, whole-of-life, mandatory national immunization register. AIR data are generally considered to be of good quality , and there was a strong incentive for persons to be sure that their COVID-19 vaccination record was accurate and up to date because activities such as employment and attending public venues were dependent on vaccination status . Last, the pandemic experience in WA enabled assessment of VE in a SARS-CoV-2 infection–naive cohort, which eliminated potential interference from non–vaccine-derived prior immunity.\n\n【47】In summary, as a result of a sustained, concerted effort to prevent introduction and spread of SARS-CoV-2 during the first 2 years of the pandemic, WA was in a unique position to evaluate, on the basis of the ancestral spike protein, the effectiveness of vaccines to prevent infection and severe disease caused by the Omicron variant in a population that had not experienced prior local SARS-CoV-2 transmission. We demonstrated that VE against infection of any severity was ≈70% up to 1 month after vaccination but waned to very low levels by 4 months. Compared with a homologous 3-dose mRNA vaccine series, a heterologous series consisting of 2 doses of ChAdOx1 followed by an mRNA booster provided significantly longer protection after 60 days, up to 4 months. Protection against hospitalization and death after 3 doses was high (i.e. ≈80%), reinforcing the value of SARS-CoV-2 vaccines for preventing serious outcomes from SARS-CoV-2 infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "896f704d-446b-478f-8b83-0cb3e5f91ac4", "title": "Soft Tissue Infection with Diaporthe phaseolorum in Heart Transplant Recipient with End-Stage Renal Failure", "text": "【0】Soft Tissue Infection with Diaporthe phaseolorum in Heart Transplant Recipient with End-Stage Renal Failure\n_Diaporthe phaseolorum_ is a fungal plant parasite found in soil, salt and fresh water, and sewage . There are few case reports of human infection with _Diaporthe_ species, and most have been described in highly immunosuppressed persons, especially solid organ transplantation recipients . One case of _Diaporthe_ spp. soft tissue infection was reported in a heart transplant patient in the United States , but the patient did not have end-stage renal failure (ESRF), making the choice of antifungal therapy less complex. We report a case of human infection with _D. phaseolorum_ in a heart transplant patient with end-stage renal failure.\n\n【1】The patient was a 46-year-old man from Samoa, resident in New Zealand, who had had a heart transplant 10 years earlier for dilated cardiomyopathy. We obtained signed consent from the patient for publication of the details of his condition. He had a 1-year history of a slowly enlarging, nontender lump in the pretibial area of his left leg. He noticed the lesion after sustaining a minor abrasion while playing a game of touch rugby. He had not been back to Samoa for 17 years. He was receiving peritoneal dialysis for ESRF secondary to tacrolimus toxicity. At the time of presentation, he was taking mycophenolate (500 mg 2×/d), tacrolimus (5 mg 2×/d; daily trough level 6.6 µg/L), and prednisone (2.5 mg 1×/d).\n\n【2】Magnetic resonance imaging of the leg showed a 37 × 23 × 43 mm complex subcutaneous cystic lesion over the proximal medial tibia with thin septations and no evidence of bony invasion. An aspirate of the lesion was used for microbiologic analysis and culture. Histological examination showed reactive fibroblastic proliferation and numerous fungal hyphae by periodic acid–Schiff staining .\n\n【3】Microscopy showed numerous fungal hyphae with no identifiable distinctive features by direct microscopic examination. After 3 days of culture, there was growth of small, white, woolly colonies on only chocolate agar and no bacterial growth.\n\n【4】We extracted DNA by using the UCP Pathogen Kit  and sequenced the internal transcribed spacer 2 region. A BLAST query  showed 100% identity and 100% query coverage with _D. phaseolorum_ (GenBank accession nos. KX498068.1, KX355829.1, JQ514150.1, and AY577815.1) and _Phomopsis_ sp. (GenBank accession nos. GU066693.1 and GQ352481.1, isolates from India and Malaysia).\n\n【5】These findings raised the question of genus assignment. Previously, _Phomopsis_ was considered to be the asexual morphotype of _Diaporthe_ species. Thus, it is possible that these deposits in GenBank were the same genus and perhaps even the same species. However, without further information about GenBank cultures or morphologic description of our isolate, we can only conclude that our isolate was probably _D. phaseolorum_ .\n\n【6】The patient was given voriconazole, and domperidone was replaced with metoclopramide. Severe tremor then developed, so he was given itraconazole before surgical excision of the lesion. Tissue samples grew _D. phaseolorum_ . However, drug susceptibility testing could not be performed because of inadequate growth. The patient received itraconazole for 7 months and the infection resolved, with no evidence to date of recurrence.\n\n【7】Infection with _D. phaseolorum_ usually occurs after inoculation from direct trauma . In this patient, the history of a minor leg abrasion during touch rugby was only suggestive of direct inoculation. Previous publications have reported different suspected sources of infection, such as a prick from a plant thorn or spine , walking barefoot , or eye surgery . Activities that have been implicated in acquisition of infection include gardening , farming , and hunting . Another possibility suggested in a case series  is that the patient might have had a penetrating injury with a wood fragment many years earlier. Marty et al. reported 3 cases of cutaneous, invasive fungal disease in which patients had received penetrating soft-tissue injuries with wood fragments months to years (10 months–13 years) before their transplant, suggesting the fungus persisted at the site of injury over a long period.\n\n【8】In the past, diagnosis of this type of unusual fungal infection would be reliant on macroscopic and microscopic morphology and growth characteristics. For this patient, the fungus did not grow on subculture. However, advances in molecular microbiology now enable clinical microbiologists to identify unusual fungal pathogens by sequencing of 18S rDNA or internal transcribed spacer 2 region.\n\n【9】This case was challenging because human infection with this pathogen is rare. Therapeutic options are based on experience detailed in a limited number of case reports. Treatment with itraconazole , posaconazole , and voriconazole  has been successful. However, treatment failure has been reported with voriconazole (despite a low MIC) and terbinafine . Most case-patients needed surgical resection for infection resolution. Keratitis has been successfully treated with topical amphotericin B and voriconazole (topical and oral)  and topical natamycin and fluconazole (topical and oral) .\n\n【10】This case highlights the difficulties faced by clinicians trying to use appropriate directed antifungal therapy when a patient is receiving multiple immunosuppressing drugs. Clinicians and clinical microbiologists should be aware of the possibility of invasive fungal infection with unusual pathogens, even if the patient is seen many years after a transplant.\n\n【11】At the time of this study, Dr. Howard was a microbiology registrar at the Canterbury Health Laboratories, Christchurch, New Zealand. She is currently a clinical microbiologist at the Waikato District Health Board, Hamilton, New Zealand. Her research interests include antimicrobial resistance, infections in immunocompromised persons, and public health microbiology.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a7d040c-d25b-4635-b198-750606097272", "title": "Rare Cryptosporidium hominis Subtype Associated with Aquatic Center Use", "text": "【0】Rare Cryptosporidium hominis Subtype Associated with Aquatic Center Use\n**To the Editor:** Cryptosporidiosis is the most frequently reported gastrointestinal illness in outbreaks associated with treated (disinfected) recreational water venues in the United States . In 2003, an increased number of cryptosporidiosis cases occurred in the Tri-Cities area of the Lower Mainland region (near Vancouver), in British Columbia, Canada. Although all cases were associated with the use of a community aquatic center, their onset dates were spread over a 3-month period, and the link between cases was unclear. The aim of this study was to determine if the cases in this disease cluster were related. Although suitable molecular markers had yet to be defined at the time of the outbreak, recent reports on the use of the gp60 gene for subtyping in molecular epidemiologic studies  have enabled us to reanalyze the isolates and report these results.\n\n【1】Fifteen laboratory-confirmed cases were identified from October 15 to December 5, 2003. This number was in excess of the anticipated incidence rate for this community, which averaged 5 reported cryptosporidiosis cases per year. During the period of investigation, an incident of fecal contamination at the aquatic center on October 10, 2003, was documented and remediation involved increasing the free chlorine concentration. Because the regional health authority was concerned about the increased number of cases, the facility closed voluntarily on December 5 for further remediation. However, recorded free chlorine concentrations did not exceed 2.0 ppm at any time during the investigative period (October 5–December 31).\n\n【2】The health authority released a public advisory encouraging those who used the facility to submit fecal specimens for laboratory testing. The health authority also sent letters to family physicians in the area, informing them of the disease cluster and requesting that unpreserved stool specimens be collected, in addition to the formalin-fixed specimens, for routine diagnostic testing. Nine fecal specimens were collected from clinically symptomatic case-patients with histories of exposure to the implicated aquatic center. Five specimens were selected by using the criteria that they were from patients with laboratory-confirmed cryptosporidiosis cases from 5 separate households. The specimens were then coded for anonymity before subsequent molecular analysis. Genomic DNA was extracted from purified _Cryptosporidium_ oocysts by freeze-thawing, and the species was determined by PCR amplification and sequencing of the 18S rRNA gene as described previously . The gp60 gene was also amplified by PCR by using primers described by Ong and Isaac-Renton . DNA sequences of amplicons were determined by cycle sequencing and assembled as described previously . The gp60 allele and subtype were identified by multiple sequence alignment with GenBank reference sequences and phylogenetic analysis that used ClustalX version 1.8  as well as manual quantification of microsatellite repeats.\n\n【3】The 18S rRNA and gp60 genes were amplified successfully from 4 specimens. On the basis of the 18S rRNA gene sequence, all case-patients were infected with _Cryptosporidium hominis_ , a species associated primarily with human-to-human transmission. The gp60 sequences from all 4 case-patients were identical and were subtype IdA19, a rarely reported subtype of _C. hominis_ . Globally, most reports of the gp60 Id allele, such as 9 reported cases from Australia, have identified the IdA15G1 subtype . Another subtype, IdA18, was isolated from 5 case-patients in a 1997 foodborne outbreak in Spokane, Washington . To date, the IdA19 subtype has been identified in only 1 sporadic case, in northern Ontario , and a subset of cases (seven 1dA19 and 2 mixed IdA19 and IbA10G2) in the 2001 waterborne outbreak in North Battleford, Saskatchewan . The IdA19 subtype is identical in sequence to the IdA18 subtype except for 1 extra TCA repeat in the microsatellite region. Neither subtype has been reported anywhere in the world except in Canada and the Pacific Northwest.\n\n【4】Because cases from all previous _C. hominis_ outbreaks of cryptosporidiosis in British Columbia have been caused by the IbA10G2 subtype, the most prevalent subtype in sporadic and outbreak cases around the world , our results indicate the presence of a new subpopulation of _C. hominis_ parasites that could cause future disease outbreaks. The identification of the same subtype in all 4 case-patients with cryptosporidiosis associated with the use of a community aquatic center was consistent with their exposure history and confirmed that all cases were linked epidemiologically. However, the association between the single northern Ontario sporadic case and the larger number of Saskatchewan and British Columbia outbreak cases is uncertain. The association with the IdA18 subtype in the Washington foodborne outbreak is also unknown. Further research is needed to determine the distribution and prevalence of gp60 subtypes in Canada as well as other parts of the world before we can more clearly understand the transmission of the IdA19 subtype.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "07c78a10-c47d-457f-926d-bfb12eaefea7", "title": "Utility of Oral Swab Sampling for Ebola Virus Detection in Guinea Pig Model", "text": "【0】Utility of Oral Swab Sampling for Ebola Virus Detection in Guinea Pig Model\nEbola virus (EBOV) causes Ebola virus disease (EVD), which results in a high number of deaths in humans. EBOV is the etiologic agent of the ongoing EVD outbreak in West Africa. Nonadapted EBOV causes disease in nonhuman primates, but adaptation is required for the virus to cause disease in rodent models . Fatal disease has been observed in 20% of guinea pigs infected with wild-type (WT) nonadapted EBOV, but a uniformly lethal guinea pig–adapted EBOV isolate was found to have developed after a limited number of serial infection passages in guinea pigs .\n\n【1】Real-time quantitative reverse transcription PCR (qRT-PCR) is used to detect EBOV in the current West Africa outbreak. Appropriate sample collection and knowledge of interpreting results on the basis of specimen type are essential for accurate triage of patients thought to have EVD. Oral swab sampling for postmortem EBOV diagnosis has been supported by use of a nonhuman primate model , and oral swab sampling for antemortem EVD diagnosis has been a major consideration in the current outbreak because collection of swab samples is less invasive than collection of serum samples and poses a much lower risk of transmitting EBOV to the person obtaining the sample than traditional phlebotomy. However, the utility of oral swabs for antemortem testing has not been investigated in detail under controlled experimental conditions. In addition, Bausch et al. have suggested that the oral milieu, such as saliva composition and oral cavity tissue structure, may potentially inhibit diagnostic capabilities of oral swab sampling .\n\n【2】Wong et al. have shown that oral swabbing can be used to detect virus and shedding in guinea pigs at isolated intervals after infection . We investigated oral swab sampling as an antemortem means of diagnosing EVD and used qRT-PCR to detect EBOV RNA in daily oral swab samples obtained from guinea pigs infected with guinea pig–adapted EBOV (GP-EBOV) and with WT-EBOV.\n\n【3】### The Study\n\n【4】Procedures and experiments described herein were approved by the Centers for Disease Control and Prevention (CDC) Institutional Animal Care and Use Committee and conducted in strict accordance with the Guide for the Care and Use of Laboratory Animals . CDC is a fully accredited research facility of the Association for Assessment and Accreditation of Laboratory Animal Care International.\n\n【5】Healthy adult male and female strain 13/N guinea pigs, 1.0–2.5 years of age, were housed in a Biosafety Level 4 laboratory in microisolator cage systems filtered with high-efficiency particulate arrestance filters. Groups of 5 animals, distributed proportionally by age and sex, were inoculated intraperitoneally with a 50% tissue culture infectious dose (TCID 50  ) at low (5 TCID 50  ) or high (5,000 TCID 50  ) levels of GP-EBOV-Mayinga, or with 5 × 10 5  TCID 50  of either the WT-EBOV-Mayinga 1976 variant (Ebola virus/ _H. sapiens_ \\-tc/COD/1976/Yambuku-Mayinga) or the WT-EBOV-Makona 2014 variant . To serve as negative controls, 3 animals were inoculated intraperitoneally with Dulbecco’s Modified Eagle’s Medium. Animals were monitored for signs of clinical illness, and body weight and temperature readings were obtained daily. Oral swab samples were collected daily for isolation of RNA and analyzed by qRT-PCR. Postmortem oral swab samples were obtained from 10 animals that were euthanized because of severe clinical illness consistent with EBOV. Carcasses of the dead animals were kept in an incubator at 30°C to simulate conditions in equatorial Africa. Samples were obtained from 9 of the 10 animals up to 5 days after death and from 1 animal at 2 days after death. In addition to oral swab samples, paired blood samples were collected from the cranial vena cava of anesthetized animals at 3 days postinfection (dpi) and by cardiac puncture at the time of death for euthanized animals.\n\n【6】Low and high doses of GP-EBOV-Mayinga were uniformly lethal. Clinical illness was delayed in 1 animal in the high-dose group; the animal was euthanized at 12 dpi, but all other animals were euthanized by 9 dpi. One animal infected with nonadapted WT-EBOV-Mayinga was euthanized at 9 dpi because of clinical illness. No severe clinical illness developed in any of the other animals infected with WT-EBOV-Mayinga or WT-EBOV-Makona . Fever developed in all animals infected with low- and high-dose GP-EBOV-Mayinga, in 20% of animals infected with WT-EBOV-Makona or WT-EBOV-Mayinga, and in none of the negative control animals. Hypothermia, typical during the terminal phases of many disease processes, was observed in animals with end-stage EVD . Substantial weight loss (>15%) was observed in all febrile animals . The 1 animal infected with WT-EBOV-Makona that showed clinical sign experienced transient fever and weight loss but started to regain weight by 9 dpi.\n\n【7】Oral swab samples were analyzed by qRT-PCR targeting the EBOV nucleoprotein gene; 18s ribosomal RNA levels were also analyzed to serve as a sampling control. EBOV RNA abundance was calculated by comparing the cycle threshold values to an in vitro–transcribed small-segment RNA standard of known copy number. All oral swab samples that were collected 0–4 dpi were negative for EBOV nucleoprotein RNA . At 3 dpi, blood samples from 7 (41%) of 17 infected animals from which blood samples could be obtained were positive for EBOV, but no viral RNA was detected in any of the paired oral swab samples. The earliest detection of EBOV RNA by oral swabbing was at 5 dpi in an animal infected with WT-EBOV-Mayinga. At 6 dpi, coinciding with the time of overt clinical signs of disease (i.e. fever, weakness, anorexia, and ruffled fur), qRT-PCR of oral swab samples detected EBOV RNA in 8 (73%) of 11 animals in which fatal illness developed and in 10 (50%) of 20 infected animals. EBOV RNA was detected by qRT-PCR in all postmortem swab samples.\n\n【8】### Conclusions\n\n【9】Our data suggest that oral swab samples obtained early in the course of infection, before death, are not a reliable method for diagnosing infection with EBOV. Paired oral swab and blood samples collected at 3 dpi and at time of euthanasia showed that sensitivity of oral swab samples was low compared with the sensitivity of traditional blood samples. Testing of oral swab samples did not indicate infection until 3 days after EBOV RNA was detectable in blood samples, with the exception of 1 animal in which oral swab samples revealed viral RNA 2 days after the blood sample. At the time of overt clinical disease, the utility of oral swab samples for diagnostics improved but was not completely consistent with infection until postmortem analysis. Our studies also enabled us to investigate whether the virulence of the WT-EBOV-Makona variant in guinea pigs was as low as that of the prototypic WT-EBOV-Mayinga variant. As shown in previous studies , WT-EBOV is less pathogenic than GP-EBOV, regardless of variant, in this animal model.\n\n【10】Investigating the utility of oral swab samples for diagnosing EVD in humans is challenging because paired blood and oral swab samples are rarely available and because the timing of sample collection relative to onset of disease and course of infection is often estimated. Although EVD in the nonhuman primate model mimics many aspects of the disease in humans, sampling from nonhuman primates in an experimental setting is problematic because of the species’ temperament, which requires anesthesia during specimen collection and venipuncture. The guinea pig model of EVD  offers the convenience of daily oral swab sampling without the need for anesthesia.\n\n【11】Although suggestive, as with any animal model system, when extrapolating these data to human diagnostics, the effect of potential differences in oral milieus (e.g. saliva composition and oral cavity tissue structure) must be considered. In the future, additional studies that use paired oral swab and blood samples from humans would provide information for continued discussion of antemortem swab sampling as a useful diagnostic modality of EVD in humans.\n\n【12】Our data support the use of oral swab samples as a sensitive modality for postmortem diagnostics; however, the utility of oral swab samples under field conditions, especially those collected before death, may decrease because of inherent problems with sampling techniques and specimen handling conditions (i.e. delays in transport and storage at typically high ambient temperatures). Despite these considerations, oral swab sample collection could be a useful sampling strategy for humans and animals with unknown causes of death when EVD is suspected and when other types of samples are more prohibitive to obtain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3878ecb8-01c7-4356-a600-c134b694ac3a", "title": "Susceptibility of Human Prion Protein to Conversion by Chronic Wasting Disease Prions", "text": "【0】Susceptibility of Human Prion Protein to Conversion by Chronic Wasting Disease Prions\nChronic wasting disease (CWD) is a fatal contagious prion disease of cervids that is found in the United States, Canada, South Korea, and most recently in Europe . The species affected differ in these geographic areas; mule deer ( _Odocoileus hemionus_ ), white-tailed deer ( _O. virginianus_ ), North American elk ( _Cervus canadensis_ ), and moose ( _Alces alces_ ) are most commonly affected in the United States and Canada, and red deer ( _C. elaphus_ ) and sika deer ( _C. nippon_ ) in South Korea also have been affected . The first identification of CWD in Europe occurred in 2016 in wild moose ( _A. alces_ , also known as Eurasian elk) and in a free-ranging reindeer ( _Rangifer tarandus,_ closely related to the free-ranging caribou of North America), a species not previously known to be affected by CWD in wild or farmed animals . CWD is a pressing animal health issue, but whether it might become a human public health issue should also be considered.\n\n【1】Scrapie and bovine spongiform encephalopathy (BSE) are well-characterized animal prion diseases affecting animals of consumption; scrapie affects sheep and BSE cattle. Human prion diseases occur as sporadic, genetic, and acquired forms; the variant form of Creutzfeldt-Jakob disease is zoonotic BSE, acquired through the oral route and having a long incubation period . In contrast, sheep scrapie is generally considered to be of no or only very low risk to human health, although this possibility has been questioned recently . The molecular basis of prion replication is a change in conformation of the normal cellular prion protein (PrP C  ) into the abnormal and misfolded conformer (PrP Sc  ) that is partially protease-resistant (PrP res  ). However, the molecular criteria for predicting zoonotic potential for prions are unclear. Consequently, approaches to understanding the zoonotic risk for prion transmission to humans have been empirical, involving in vivo and in vitro models . Experimental transmission of mule deer CWD to nonhuman primates showed that squirrel monkeys are susceptible, whereas cynomolgus macaques appear to be resistant . Similarly, attempted transmission of deer or elk CWD to transgenic mouse lines expressing human PrP C  have largely failed, indicative of a species barrier , although this might be overcome in some instances .\n\n【2】In contrast to these in vivo approaches, we have used protein misfolding cyclic amplification (PMCA) to investigate the molecular compatibility of bovine, ovine, and cervid prions with full-length, glycosylated and glycosylphosphatidylinositol-anchored human prion protein (PrP) . We found that scrapie samples failed to convert human PrP C  to PrP res  , whereas cattle BSE converted the human protein efficiently . These observations suggest that PMCA can reproduce aspects of cross-species transmission potential and inform assessment of zoonotic risk. The single CWD-affected specimen (from a North American elk) previously available to us was found to be capable of converting human PrP C  to PrP res  . Here, we expand on the previous report; analyzing more elk specimens of 2 different genotypes (132 MM, homozygous for methionine at _Prnp_ position 132, and 132 ML, methionine–leucine heterozygous at the same position), analyzing white-tailed deer CWD specimens for the first time, and analyzing reindeer that have been experimentally infected with white-tailed deer CWD. The elk ML polymorphism at position 132 of _Prnp_ is of particular interest because it corresponds to the methionine–valine (MV) _PRNP_ codon 129 polymorphism in humans, which is itself a major genetic susceptibility factor associated with human prion disease . Although these examples of CWD all derive from North America, the inclusion of white-tailed deer (a relative of European roe deer, _Capreolus capreolus_ ), North American elk (a near relative of red deer), and, most important, reindeer might help in formulating risk assessments in Europe.\n\n【3】### Materials and Methods\n\n【4】##### Animal Tissue\n\n【5】We obtained from the National and OIE Reference Laboratory for Scrapie and CWD (Ottawa, ON, Canada) elk, white-tailed deer, and reindeer frozen brain tissue from CWD-affected animals that had been confirmed positive by using a statutory diagnostic testing regime . Small pieces of tissue were obtained, and we analyzed all for the presence of PrP res  by using Western blot, as previously described . We used transgenic mouse brains produced by gene replacement and expressing physiologic levels of the human PrP of the 3 _PRNP_ codon 129 polymorphic genotypes as the PMCA substrate .\n\n【6】##### Protein Misfolding Cyclic Amplification\n\n【7】##### Substrate and Seed Preparation\n\n【8】We homogenized transgenic mouse brains in conversion buffer by using a glass on glass manual grinder and a conversion buffer made of 1× phosphate-buffered saline (PBS), 150 mmol/L of NaCl, 1% Triton X-100, and a complete protease inhibitor cocktail (cOmplete; Roche, Mannheim, Germany) to obtain a final 10% weight-to-volume solution. We cleared the homogenized tissue by using centrifugation as described previously , aliquoted the supernatant (i.e. PMCA substrate) into 1.5 mL tubes, and stored at −80°C until used. We homogenized the CWD brain material (i.e. PMCA seed) by using 1.5-mL Eppendorf tubes and disposable polypropylene pestles that used the same buffer.\n\n【9】##### PMCA Procedure\n\n【10】We performed amplification in a programmable Q-700 sonicator attached to a microplate aluminum horn . We mixed brain homogenate CWD PMCA seeds with aliquots of PMCA substrate in a final volume of 120 µL in PCR tubes at a 1:3 ratio. We included low molecular weight heparin at 100 µg/mL  in all PMCA reactions and added EDTA to a final concentration of 6 mmol/L. To perform a comparison between samples before and after the amplification procedure, we took 19 µL of each reaction mixture before the serial cycles of sonication and incubation. Each cycle consisted of 20 s sonication (at an amplitude of 38, wattage 278–300) followed by 29 min and 40 s incubation; we repeated this procedure 96 times (48 h).\n\n【11】##### Proteolytic Treatment and Western Blotting\n\n【12】We evaluated the presence of PrP res  by Western blot after proteinase K treatment. We incubated 19 µL of each sample with proteinase K in a final concentration of 50 µg/mL for 1 h at 37°C in a standard thermoblock. Before loading, we mixed samples with an appropriate volume of 4× NuPAGE buffer (Invitrogen, Carlsbad, CA, USA) and boiled them at 100°C for 10 min. We loaded the samples on NuPAGE Novex (Fisher Scientific; Loughborough, United Kingdom) 10% Bis-Tris gels (1.0 mm, 10 wells) and subjected them to electrophoresis at 200 volts for 55 min. We transferred proteins to a polyvinylidene difluoride membrane by using 800 mA for 60 min  and blocked membranes with 2% milk for 1 h. We determined accumulated human PrP res  on the basis of the specific immunoreactivity of the 3F4 monoclonal antibody (mAb) diluted 1/10,000 (Millipore; Watford, United Kingdom). We detected CWD PrP by stripping the Western blots (Thermo Fisher, Bleiswijk, Netherlands) and reprobing them with a 6H4 antibody diluted 1/40,000 (Prionics; Schlieren, Switzerland). We used ECL antimouse IgG, peroxide-linked species-specific F(ab′)2 fragment from sheep (GE Healthcare Life Sciences; Little Chalfont, United Kingdom) as a secondary antibody diluted 1/25,000. We developed membranes by chemiluminescent detection using ECL Prime (GE Healthcare Life Sciences) and acquired digital images by using an XRS Bio-Rad system (Bio-Rad Laboratories, Hercules, CA, USA) with a CCD camera.\n\n【13】##### Criteria for Positivity\n\n【14】The criterion for conversion was 3F4 antibody detection of a pattern of 3 clear protease-resistant bands of the expected electrophoretic mobility and ratio for PrP res  in the amplified sample after a defined period of Western blot image capture. The triplet pattern had to be absent from the unamplified sample tested under the same conditions analyzed in parallel.\n\n【15】##### Precipitation of Insoluble PrP\n\n【16】We incubated brain homogenate from white-tailed deer and reindeer CWD specimens by using 20% sarkosyl (diluted in PBS) for 10 min at room temperature. We subjected samples to centrifugation for 1 h at 100,000 × _g_ at 4°C as described previously . After centrifugation, we discarded the supernatant and washed the pellet with PBS followed by a second centrifugation (100,000 × _g_ for 1 h at 4°C). We resuspended the washed pellets directly in PMCA substrate before using them in a single round of amplification.\n\n【17】### Results\n\n【18】##### Determination of Total PrP in Cervid CWD Specimens\n\n【19】We first characterized the CWD brain tissues for the presence of total PrP and PrP res  by using mAb 6H4. We detected similar levels of total PrP by using Western blotting among the elk specimens analyzed . However, we did not find readily detectable levels of PrP res  in all the samples; we detected PrP res  in 3 of the 5 elk specimens of the 132 MM genotype and both of the 132 ML samples . White-tailed deer samples showed similar expression levels of total PrP in the 2 specimens analyzed, but PrP res  levels were low . We also confirmed total PrP in the reindeer specimens, with a robust detection of PrP res  in 1 of the available samples (reindeer 1) but low levels in the other specimen (reindeer 2) .\n\n【20】##### In vitro Conversion of Human PrP by Cervid Prions\n\n【21】##### Elk ( _C. Canadensis_ )\n\n【22】We then performed a single round of PMCA, incubating the _PRNP_ codon 132 MM elk CWD seeds in humanized transgenic 129 MM mouse brain substrate. Only those elk CWD samples that had readily detectable PrP res  (as detected by 6H4 mAb) were able to produce human PrP res  (detectable by the 3F4 mAb) after Western blot and proteinase K treatment, consistent with CWD PrP res  playing a direct role in the misfolding process .\n\n【23】We then addressed the role of the human _PRNP_ codon 129 and the cervid _PRNP_ codon 132 polymorphisms in the conversion of human PrP C  . To seed PMCA reactions, we used CWD brain homogenates from 132 MM and 132 ML elk with comparable quantities of CWD PrP res   and then used humanized transgenic mouse substrates of the 3 possible _PRNP_ codon 129 genotypes (129 MM, 129 MV, and 129VV). Each of the CWD methionine homozygous (132 MM) samples resulted in human PrP res  formation when used to seed the matched humanized substrate (129 MM). The heterozygous elk seeds did not result in any detectable conversion of the humanized 129 MM PrP substrate. When we incubated these same CWD brain homogenates with the heterozygous (129 MV) and homozygous (129VV) humanized substrates, we observed low levels of human PrP res  formation but no obvious difference in efficiency between the CWD 132MM and CWD 132ML samples in the 129VV substrate .\n\n【24】##### White-tailed Deer ( _O. virginianus_ )\n\n【25】We performed similar analyses to determine the competence of white-tailed deer CWD to convert the human PrP . To maintain consistency, we homogenized the 2 available CWD white-tailed deer specimens at 10% (weight/volume) and then normalized by volume  to seed the PMCA reactions. Detection of CWD PrP res  by 6H4 antibody revealed that the levels of PrP res  in the unamplified samples were not equivalent to the elk CWD specimens used for PMCA . However, 1 of the analyzed specimens showed some conversion of the humanized 129 MM PrP substrate, although PrP res  formation was undetectable in the heterozygous and valine homozygous substrate with the 3F4 antibody . These results suggest a higher degree of molecular compatibility of the _PRNP_ codon 129 MM human genotype and CWD PrP Sc  , consistent with what we observed in most of the elk CWD 132 MM specimens.\n\n【26】##### Reindeer ( _R. tarandus_ )\n\n【27】Natural cases of CWD in reindeer have been detected in Norway, but North American reindeer were previously shown to be experimentally susceptible to white-tailed deer CWD by the oral route . We tested samples from these 2 experimentally infected reindeer  . Both reindeer specimens were capable of converting the humanized 129 MM PrP substrate, although different amounts of CWD PrP res  were detected by the 6H4 antibody in the frozen samples. Densitometry analysis suggested that 1 specimen (from reindeer 2) had roughly one tenth of the PrP res  of the other sample, showing that the reaction with small amounts of reindeer PrP res  are able to convert the humanized substrate. The _PRNP_ codon 129 MV and VV genotype substrates also were readily susceptible to conversion by the reindeer seeds . Seeding efficiency of reindeer CWD was maintained when the seeding material was normalized by using semipurified PrP res  , arguing against the possibility that the apparent enhanced seeding potential of reindeer CWD simply reflects the increased abundance of PrP res  in reindeer samples or was a result of conversion of endogenous reindeer seed–associated PrP C  .\n\n【28】### Discussion\n\n【29】Characterization of the transmission properties of CWD and evaluation of their zoonotic potential are important for public health purposes. Given that CWD affects several members of the family _Cervidae_ , it seems reasonable to consider whether the zoonotic potential of CWD prions could be affected by factors such as CWD strain, cervid species, geographic location, and _Prnp_ – _PRNP_ polymorphic variation. We have previously used an in vitro conversion assay (PMCA) to investigate the susceptibility of the human PrP to conversion to its disease-associated form by several animal prion diseases, including CWD . The sensitivity of our molecular model for the detection of zoonotic conversion depends on the combination of 1) the action of proteinase K to degrade the abundant human PrP C  that constitutes the substrate while only N terminally truncating any human PrP res  produced and 2) the presence of the 3F4 epitope on human but not cervid PrP. In effect, this degree of sensitivity means that any human PrP res  formed during the PMCA reaction can be detected down to the limit of Western blot sensitivity. In contrast, if other antibodies that detect both cervid and human PrP are used, such as 6H4, then newly formed human PrP res  must be detected as a measurable increase in PrP res  over the amount remaining in the reaction product from the cervid seed. Although best known for the efficient amplification of prions in research and diagnostic contexts, the variation of the PMCA method employed in our study is optimized for the definitive detection of zoonotic reaction products of inherently inefficient conversion reactions conducted across species barriers. By using this system, we previously made and reported the novel observation that elk CWD prions could convert human PrP C  from human brain and could also convert recombinant human PrP C  expressed in transgenic mice and eukaryotic cell cultures .\n\n【30】A previous publication suggested that mule deer PrP Sc  was unable to convert humanized transgenic substrate in PMCA assays  and required a further step of in vitro conditioning in deer substrate PMCA before it was able to cross the deer–human molecular barrier . However, prions from other species, such as elk  and reindeer affected by CWD, appear to be compatible with the human protein in a single round of amplification (as shown in our study). These observations suggest that different deer species affected by CWD could present differing degrees of the olecular compatibility with the normal form of human PrP.\n\n【31】The contribution of the polymorphism at codon 129 of the human PrP gene has been extensively studied and is recognized as a risk factor for Creutzfeldt-Jakob disease . In cervids, the equivalent codon corresponds to the position 132 encoding methionine or leucine. This polymorphism in the elk gene has been shown to play an important role in CWD susceptibility . We have investigated the effect of this cervid _Prnp_ polymorphism on the conversion of the humanized transgenic substrate according to the variation in the equivalent _PRNP_ codon 129 polymorphism. Interestingly, only the homologs methionine homozygous seed–substrate reactions could readily convert the human PrP, whereas the heterozygous elk PrP Sc  was unable to do so, even though comparable amounts of PrP res  were used to seed the reaction. In addition, we observed only low levels of human PrP res  formation in the reactions seeded with the homozygous methionine (132 MM) and the heterozygous (132 ML) seeds incubated with the other 2 human polymorphic substrates (129 MV and 129 VV). The presence of the amino acid leucine at position 132 of the elk _Prnp_ gene has been attributed to a lower degree of prion conversion compared with methionine on the basis of experiments in mice made transgenic for these polymorphic variants . Considering the differences observed for the amplification of the homozygous human methionine substrate by the 2 polymorphic elk seeds (MM and ML), reappraisal of the susceptibility of human PrP C  by the full range of cervid polymorphic variants affected by CWD would be warranted.\n\n【32】In light of the recent identification of the first cases of CWD in Europe in a free-ranging reindeer ( _R. tarandus_ ) in Norway , we also decided to evaluate the in vitro conversion potential of CWD in 2 experimentally infected reindeer . Formation of human PrP res  was readily detectable after a single round of PMCA, and in all 3 humanized polymorphic substrates (MM, MV, and VV). This finding suggests that CWD prions from reindeer could be more compatible with human PrP C  generally and might therefore present a greater risk for zoonosis than, for example, CWD prions from white-tailed deer. A more comprehensive comparison of CWD in the affected species, coupled with the polymorphic variations in the human and deer _PRNP–Prnp_ genes, in vivo and in vitro, will be required before firm conclusions can be drawn. Analysis of the _Prnp_ sequence of the CWD reindeer in Norway was reported to be identical to the specimens used in our study . This finding raises the possibility of a direct comparison of zoonotic potential between CWD acquired in the wild and that produced in a controlled laboratory setting.\n\n【33】The prion hypothesis proposes that direct molecular interaction between PrP Sc  and PrP C  is necessary for conversion and prion replication. Accordingly, polymorphic variants of the PrP of host and agent might play a role in determining compatibility and potential zoonotic risk. In this study, we have examined the capacity of the human PrP C  to support in vitro conversion by elk, white-tailed deer, and reindeer CWD PrP Sc  . Our data confirm that elk CWD prions can convert the human PrP C  , at least in vitro, and show that the homologous _PRNP_ polymorphisms at codon 129 and 132 in humans and cervids affect conversion efficiency. Other species affected by CWD, particularly caribou or reindeer, also seem able to convert the human PrP. It will be important to determine whether other polymorphic variants found in other CWD-affected _Cervidae_ or perhaps other factors  exert similar effects on the ability to convert human PrP and thus affect their zoonotic potential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2abaf3e1-94cb-4e5c-8c38-cc6d0257d961", "title": "Burkholderia pseudomallei Misidentified by Automated System", "text": "【0】Burkholderia pseudomallei Misidentified by Automated System\n_Burkholderia pseudomallei_ , the etiologic agent of melioidosis, can cause pyogenic or granulomatous lesions and is endemic to tropic regions, mainly in Southeast Asia and northern Australia. This organism is a potential category B bioterrorism agent . Melioidosis occurs sporadically in travelers returning from disease-endemic areas, and laboratories in regions where this disease is not endemic are not familiar with identification of _B_ . _pseudomallei_ , thus potentially leading to misidentification . We report the misidentification of this organism by an automated microbiology system.\n\n【1】### The Study\n\n【2】On August 20, 2008, a 35-year-old man from Switzerland was admitted to the Cantonal Hospital in St. Gallen, Switzerland. He had extradural cranial abscess of the right parietal area and a defect in adjacent bone. In May and June 2008, he traveled to Singapore and Malaysia (Kuala Lumpur and the Perhentian Islands), then to southwestern (Ko Samui, Ko Tau) and northern (Chiang Mai) Thailand where he went trekking and river rafting.\n\n【3】The patient did not remember receiving a head injury during his trip. Seventeen days after returning to Switzerland, he had a swelling in the right parietal area of the head. The parietal bulge increased, but puncture by his general practitioner showed no aspirate. During the next 7 weeks, the bulge became painful and secretion of pus was noted at the time of admission.\n\n【4】At admission, his general condition was good and he had no signs of systemic inflammation. He did not have any neurologic deficits or other abnormal findings. Test results for complete blood cell count, C-reactive protein and creatinine levels, and liver functions were normal. Computed tomography and magnetic resonance imaging of the head showed the abscess and a small defect of bone . The abscess and part of the cranial bone were then surgically removed.\n\n【5】Culture material from the abscess and biopsy specimens of the abscess capsule and the cranial bone grew gram-negative, oxidase-positive rods, and smooth creamy colonies on sheep blood agar after incubation for 48 hours at 35°C. For identification, a suspension of the isolate was prepared and tested in a UNMIC/ID-62 panel of the BD Phoenix Automated Microbiology System (Becton Dickinson AG, Allschwil, Switzerland) per the manufacturer’s instructions. This system identified the isolate as _B_ . _cepacia_ (99% confidence). When a species is identified with >90% confidence, the Phoenix System gives an identification result as a measure of likelihood that the identification is the only correct one.\n\n【6】The patient was discharged from hospital after 5 days with a preliminary diagnosis of _B_ . _cepacia_ infection of soft tissue (the cranial bone lesion was attributed to trauma). He was treated with oral cotrimoxazole (160/800 mg 2× a day) for 16 days. The isolate was sensitive to cotrimoxazole (MIC = 1 mg/L for trimethoprim and 19 mg/L for sulfamethoxazole) by the Phoenix System for _B_ . _cepacia_ (Becton Dickinson). The isolate was sensitive to imipenem, ceftazidime, doxycycline, cotrimoxazole, and tetracycline by Etest (AB Biodisk, Solna, Sweden).\n\n【7】The diagnosis was regarded as preliminary for the following reasons. First, identification of _B_ . _cepacia_ by common automated identification instruments such as the Phoenix System or VITEK 2 (bioMérieux, Geneva, Switzerland) requires confirmatory identification by molecular tests . Second, an abscess is an uncommon location for _B_ . _cepacia_ . Third, the bacterial colonies emitted an unexpected, earthy odor. Fourth, the isolate was sensitive to amoxicillin (MIC = 8 mg/L) and clavulanate (MIC = 4 mg/L).\n\n【8】To verify identification of the isolate, a 500-bp fragment of the 16S rRNA gene was amplified and sequenced by using the Fast MicroSeq 500 16S rDNA Bacterial Identification Kit and a PRISM 310 Genetic Analyzer (both from Applied Biosystems, Foster City, CA, USA) according to the manufacturer’s instructions. Sequence analysis was performed by using MicroSeq ID Microbial Identification Software (Applied Biosystems). The MicroSeq ID 2.0 500-bp library identified _B_ . _pseudomallei_ (ATCC 23343, gb DQ108392.1) with a 446-bp consensus length and 1 mismatch with _B_ . _mallei_ (NCTC 10247, gb CP000548.1). These results suggested that our isolate was _B_ . _pseudomallei_ .\n\n【9】Additional investigations with a flagellin C gene–specific real-time PCR  suggested that the isolate was _B_ . _pseudomallei_ or _B_ . _mallei_ . Multilocus sequence typing  showed the allelic profile 1/2/3/1/1/2/1, which identified the isolate as _B_ . _pseudomallei_ sequence type 306. This sequence type was isolated from serum samples of 2 patients in Thailand with invasive melioidosis in 1991 and 2006.\n\n【10】Forty-four days after surgery, new biopsy specimens of soft tissue below the parietal scar and a sample of galea aponeurotica were cultured for _B_ . _pseudomallei_ after the patient received 2 weeks of inadequate therapy with low-dose cotrimoxazole to determine its drug resistance pattern. Cultures yielded only _Staphylococcus epidermidis_ , which was regarded as a contaminant. To ensure eradication of _B_ . _pseudomallei_ , the patient was treated with imipenem (500 mg, 4×/day), cotrimoxazole (400 mg trimethoprim/day), and leucovorine (15 mg, 3× a week) for 2 weeks and later with cotrimoxazole (320 mg trimethoprim/day) and leucovorine (15 mg, 3×/week) for 6 months. The patient recovered after 6 months; he had a small indentation without signs of inflammation at the site of the abscess.\n\n【11】For comparison with the Phoenix System, we retrospectively analyzed the isolate by using the API 20NE biochemical test panel V7.0 (bioMérieux). This panel identified the isolate as _B_ . _pseudomallei_ (profile 1156577; 99.9% ID, 1.0 T).\n\n【12】### Conclusions\n\n【13】Automated methods for identification of bacterial isolates and testing of antimicrobial drug susceptibility, such as the Phoenix System, have become standard in most clinical laboratories because they are easy to use and turnaround time is rapid. The Phoenix System uses fluorogenic and chromogenic substrates for its identification algorithms, a broth-based antimicrobial drug–susceptibility testing method, and a data-processing application (Phoenix EpiCenter; Becton Dickinson AG). Unfortunately, _B_ . _pseudomallei_ was not in the database of this system, which led to misidentification of our isolate as _B_ . _cepacia_ . Failure to correctly identify _B_ . _pseudomallei_ has also been reported with another widely used automated system, the Vitek2 system . To identify isolates from patients traveling to disease-endemic areas, automated systems should be updated for identification of _B_ . _pseudomallei_ and other rare bacteria that cause severe human infections, are hazardous to laboratory personnel, and may be used as bioterrorism agents (e.g. _Brucella_ spp. and _B_ . _mallei_ ).\n\n【14】Three modes of acquisition (inhalation, ingestion, and inoculation) are recognized for _B_ . _pseudomallei_ infection. Skin and soft tissue infections may occur after minor wounds or from hematogenous spread . In our patient, inoculation of the skin with _B_ . _pseudomallei_ after a minor injury in Thailand is probably the mode of infection.\n\n【15】Although laboratory personnel handled cultures of _B_ . _pseudomallei_ for identification and drug resistance testing and smelled culture plates without knowing the isolate’s identity, none became ill or showed signs of melioidosis. According to expert consensus , exposure of our laboratory personnel was classified as a low risk. Serum samples were stored at –30°C to enable serologic testing for any subsequent illness.\n\n【16】Laboratories in regions where _B_ . _pseudomallei_ is not endemic should be aware of misdiagnosis of isolates by automated methods for bacterial identification and antimicrobial drug susceptibility testing. Identification of _Burkholderia_ spp. by the Phoenix EpiCenter should be confirmed by molecular methods and by the API 20NE system in suspected cases of _B_ . _pseudomallei_ infection. Because of its high rate of accuracy and ease of use, the API 20NE system should be used first for any suspected colony when automated systems do not contain the adequate profile .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8d90d410-aaf8-42df-89fc-720f695f4a74", "title": "Effectiveness of BNT162b2 Vaccine Booster against SARS-CoV-2 Infection and Breakthrough Complications, Israel", "text": "【0】Effectiveness of BNT162b2 Vaccine Booster against SARS-CoV-2 Infection and Breakthrough Complications, Israel\nThe mass severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) BNT162b2  vaccination campaign in Israel was associated with a decline in the number of SARS-CoV-2 infections, hospitalizations, and deaths, reaching a nadir by mid-May 2021 . However, beginning the third week of June 2021, a new rise in the number of SARS-CoV-2 cases was observed, including cases among fully vaccinated persons . Waning humoral immune response after the second vaccine dose was then found to be associated with increased incidence of SARS-CoV-2–related infections, hospitalizations, and deaths caused primarily by the B.1.617.2 (Delta) variant . In response to the increasing illness and deaths, the Israel Ministry of Health (MOH) recommended a third (booster) BNT162b2 vaccine dose for persons for whom at least 5 months had passed after receiving the second vaccine dose . The elderly and other high-risk groups were prioritized at first , and other age groups were added rapidly thereafter . We estimated the booster dose vaccine effectiveness (VE) against SARS-CoV-2 infection and the rate reduction of complications in breakthrough coronavirus disease (COVID-19) cases after the BNT162b2 booster dose in persons \\> 16 years of age, by age group, for up to 20 weeks after receipt of the booster dose.\n\n【1】### Methods\n\n【2】##### Study Design\n\n【3】We conducted a retrospective longitudinal cohort study using 2 MOH national repositories: the COVID-19 vaccine repository and the SARS-CoV-2 test repository. The national COVID-19 vaccine repository includes vaccine type, vaccine lot number, and date of dose administration for each person vaccinated in Israel. The national SARS-CoV-2 PCR test database includes the results of each test performed, the date of testing, and the date results were obtained for each person. It also includes the date of hospitalization, severity of illness, and date of death of persons with COVID-19, if applicable. Personal identifiers such as unique personal identity number, age, and sex of each person registered in the repositories (because of PCR testing or vaccination) are included in both databases. We retrieved individual deidentified data from both databases and matched persons by using twice-encrypted unique personal identity numbers.\n\n【4】During the first stage of our study, we determined VE for booster dose vaccine recipients against SARS-CoV-2 infection by using unvaccinated persons as controls. During the second stage, we determined the rate reduction for hospitalizations, severe or critical disease, and deaths among persons who tested positive for SARS-CoV-2 after the booster dose (i.e. breakthrough cases).\n\n【5】We defined as index dates the dates on which third-dose vaccine recipients in our study received the booster dose . Booster dose recipients and unvaccinated controls included in each index date represented a single cohort. We performed analyses for persons 16–59 years of age across 14 consecutive cohorts with the index dates August 29, 2021–September 11, 2021. These dates were selected because, by that period, persons 16–59 years of age had already been approved by the MOH to receive the booster dose . Analyses for persons \\> 60 years of age were performed across 14 consecutive cohorts with index dates occurring during August 1, 2021–August 14, 2021. These dates were chosen for this age group because this group was the first to receive the booster dose  and because most persons \\> 60 years of age received the third dose before August 29, 2021 . We followed each cohort through January 1, 2022.\n\n【6】##### Estimation of VE\n\n【7】We excluded residents of Israel who tested positive for SARS-CoV-2 by PCR before the evaluation periods from the analyses . We estimated VE for the 16–59-year and \\> 60-year age groups, as well as for age groups 16–29 years, 30–39 years, 40–49 years, and 50–59 years . We first estimated VE for each cohort starting week 2 after the index date. We then estimated VE for all 14 cohorts combined. Because of the different index dates for these age groups, we followed persons 16–59 years of age for 16 weeks and persons \\> 60 years of age for 20 weeks .\n\n【8】##### Hospitalizations, Severe Disease, and Death among SARS-CoV-2–Positive Booster Dose Recipients\n\n【9】We determined rates of SARS-CoV-2–related hospitalizations, severe or critical disease, and deaths for booster-dose recipients and for unvaccinated persons who tested positive for SARS-CoV-2 by PCR during the evaluation period described previously (breakthrough cases). The time allotted for the occurrence of hospitalization and severe or critical disease after the first positive PCR test was 14 days . We did not set a time limit for death after the first positive PCR test. We determined disease severity in accordance with US National Institutes of Health guidelines .\n\n【10】##### Statistics\n\n【11】We determined VE and 95% CI using the formula (1 – incidence rate ratio \\[IRR\\]) × 100. The IRR represents the ratio of PCR-confirmed SARS-CoV-2 infection rate in the group of booster-dose recipients to the corresponding rate in the unvaccinated control group. For persons who tested positive for SARS-CoV-2 by PCR several times during the evaluation period, we included only the first positive test result in the analysis.\n\n【12】We excluded persons who had a positive SARS-CoV-2 PCR test before the evaluation periods from the analysis, regardless of their vaccination status. Unvaccinated persons included in the study who were vaccinated during the cohort evaluation period were censored (removed from the study) on their vaccination dates.\n\n【13】We computed the number of unvaccinated controls by age and sex for each cohort by subtracting the number of residents of Israel, by age and sex, who were vaccinated with any number of BNT162b2 vaccine doses before or on the cohort vaccination date (index date) from the number of residents who did not have a recorded positive SARS-CoV-2 PCR test by that date. We calculated the number of person-days each person contributed as unvaccinated during each evaluation period. The number of residents (total, by age and by sex) was based on the 2021 Central Bureau of Statistics statistical abstract . We took into account unvaccinated participants who were included in >1 cohort when calculating VEs and CIs.\n\n【14】VE was first calculated for each age group daily cohort by week starting the second week after the booster dose. VE was estimated separately for each week that passed since the index date. For the combined VE estimate (for all 14 cohorts together), we took several steps. First, we summed the number of booster-vaccinated and unvaccinated SARS-CoV-2–positive cases for the evaluation period. Second, we counted the days at risk for each age-group cohort on the basis of the number of person-days for each booster-vaccinated and unvaccinated person from the start of the study until the person became SARS-CoV-2–positive or until the end of follow-up, whichever date was earlier. Third, we summed the days at risk for each age group cohort during the evaluation period to provide the total number of person-days at risk in the booster-vaccinated or unvaccinated status for all age group cohorts. Finally, we calculated IRR for the age group cohorts combined.\n\n【15】We evaluated the reduction in SARS-CoV-2–related hospitalizations, illness severity during hospitalizations, and death in persons who received 3 BNT162b2 vaccine doses compared with unvaccinated persons using the formula (1 – IRR) × 100. We performed adjustment of IRR and 95% CI for age group (16–29, 30–39, 40–49, and 50–59 years for persons 16–59 years of age; 60–79 and \\> 80 years for persons \\> 60 years of age), sex and epidemiologic week, provided the data sizes were sufficiently large, by using Poisson regression. Statistical analysis was performed using SAS Enterprise Guide 7.1 software . The study was approved by the superior ethical committee of the Israel MOH  with exemption from informed consent.\n\n【16】### Results\n\n【17】##### Booster Dose Vaccination Campaign\n\n【18】By October 31, 2021, persons ≥60 years of age reached a vaccination rate of ≈80% . Vaccination rates by that date were 70.2% for the 50–59-year age group, 62.4% for the 40–49-year age group, 53.1% for the 30–39-year age group, and 44.7% for the 16–29-year age group .\n\n【19】##### Booster Dose VE in Persons 16–59 Years of Age\n\n【20】Adjusted VE point estimates reached 92.8% (95% CI 91.3%–94.0%) in week 2 of the evaluation period and 96.8% (95% CI 96.0%–97.5%) by week 3 . The adjusted VE remained above 95% until week 10 and thereafter started to slowly decline, reaching VE of 89.6% (95% CI 85.4%–92.7%) in week 14. In weeks 15 and 16, VE point estimates declined by 12%, reaching a point estimate of 77.6% (95% CI 68.4%–84.2%) . The evaluation dates of weeks 15 and 16 occurred during December 2021 , when the percentage of the B.1.1.529 (Omicron) variant among reported sequenced samples in Israel rapidly increased  . VE estimation by age groups demonstrated similar patterns .\n\n【21】##### Booster Dose VE in Persons \\> 60 Years of Age\n\n【22】Adjusted VE point estimates reached 76.4% (95% CI 70.9%–80.9%) on week 2 of the evaluation period and 93.1% (95% CI 91.8%–94.2%) by week 3 . The adjusted VE remained above 93% until week 13, and thereafter started to slowly decline, reaching VE of 90.6% (95% CI 87.2%–93.1%) at week 17. In weeks 18 and 19, VE point estimates declined by 7% and in week 20, VE declined by 21.6%, reaching a point estimate of 61.3% (95% CI 52.5%–68.4%) . The evaluation dates of weeks 19 and 20 occurred during December 2021 , when the percentage of the B.1.1.529 (Omicron) variant among reported sequenced samples in Israel rapidly increased  .\n\n【23】##### Hospitalizations among SARS-CoV-2–Positive Booster Dose Vaccine Recipients\n\n【24】We analyzed rate reductions of hospitalizations among persons who became SARS-CoV-2-positive by week and for all evaluation weeks combined . The hospitalization rate reduction by week for persons 16–59 years of age was between 62.8% (95% CI −0.6% to 86.2%) and 100.0%. The combined rate reduction for weeks 2–16 was 89.2% (95% CI 79.1%–94.4%) .\n\n【25】The hospitalization rate reduction by week for persons \\> 60 years of age was between 54.9% (95% CI −35.9 to 85.1%) and 95.0% (95% CI 72.1%–99.1%). The combined rate reduction for weeks 2–20 was 75.1% (95% CI 71.3%–78.5%) .\n\n【26】##### Severe Disease among SARS-CoV-2–Positive Booster Dose Vaccine Recipients\n\n【27】The severe or critical disease rate reduction for persons 16–59 years of age was 92.0% (95% CI 70.0%–97.9%) on week 2 . No cases of severe or critical disease were recorded among booster-dose recipients for weeks 3–16. The combined rate reduction for weeks 2–16 was 97.3% (95% CI 89.7%–99.3%) .\n\n【28】The rate reduction of severe or critical disease by week for persons \\> 60 years of age was between 58.6% (95% CI 11.1%–80.7%) and 100%. The combined rate reduction for weeks 2–20 was 81.6% (95% CI 78.3%–84.3%) .\n\n【29】##### Deaths among SARS-CoV-2-Positive Booster Dose Vaccine Recipients\n\n【30】No deaths were recorded among booster-dose recipients 16–59 years of age during the evaluation weeks, compared with 1–45 deaths per week in the unvaccinated group, a rate reduction of 100% . The death rate reduction by week for persons \\> 60 years of age was between 49.1% (95% CI −44.3% to 82.1%) and 100%. The combined rate reduction for weeks 2–20 was 77.1% (95% CI 71.2%–81.8%) . Analysis of death rate reduction by using only deaths that were highlighted by hospitals as deaths caused by COVID-19 and limiting the time from positive PCR test to death by up to 28 days yielded similar results .\n\n【31】### Discussion\n\n【32】Our results demonstrate that, after the BNT16b2 booster dose, VE against SARS-CoV-2 infection reached levels that were observed shortly after the second vaccine dose . VE point estimates of >90% were observed in week 2 in persons 16–59 years of age and in week 3 in persons \\> 60 years of age. Similar delay in achieving high VE among elderly persons was also shown after the second BNT162b2 vaccine dose . Highest-level VE was maintained for up to 11 weeks, as shown in persons \\> 60 years of age included in our study. The decline in VE that occurred afterward was initially mild, still maintaining VE point estimates \\> 90% for up to week 17 of the evaluation period in persons \\> 60 years of age. The decline in VE became steeper during the last 2 weeks of the evaluation period.\n\n【33】The B.1.617.2 (Delta) variant was the most prevalent variant in Israel through November 2021. However, the last 2 evaluation weeks, which occurred in December 2021 , coincided with the beginning of a new wave of illness and the sharp rise in the B.1.1.529 (Omicron) variant in Israel. Waning immunity was shown several months after the second BNT162b2 vaccine dose  and was temporarily associated with the rise of the B.1.617.2 (Delta) variant in Israel. However, a fresh 2-dose BNT162b2 vaccination regimen was found to be highly effective against the B.1.617.2 (Delta) variant .\n\n【34】Early evaluations suggest that VE of 2 doses of the BNT162b2 against B.1.1.529 (Omicron) variant–related infection, symptomatic disease, and hospitalizations was reduced compared with VE against the B.1.617.2 (Delta) variant . VE of the BNT162b2 booster dose against infection and symptomatic disease caused by the B.1.1.529 (Omicron) variant was also lower than for the B.1.617.2 (Delta) variant . The difference in VE against the B.1.1.529 (Omicron) and B.1.617.2 (Delta) variants increased as time passed from booster dose administration . Therefore, the steeper decrease in the 3-dose VE in the last 2 weeks of our study period could be caused, at least in part, by the rapid spread of the B.1.1.529 (Omicron) variant in Israel.\n\n【35】Several studies have evaluated the shorter-term effect of the BNT162b2 booster dose on SARS-CoV-2 infection and complications  and found that a high degree of protection was achieved. Some of these studies used booster-eligible 2-dose vaccine recipients as controls , but our study evaluated VE by using unvaccinated persons as controls. Booster dose VE analysis using unvaccinated persons as controls is paramount, because the baseline VE against SARS-CoV-2 for booster dose recipient is >0%, and time to eligibility for a booster dose might vary among countries. Furthermore, our analysis shows the magnitude of protection offered by the booster dose in a manner that enables easy comparison with other VE studies.\n\n【36】Analyzing the reduction in complications among SARS-CoV-2 vaccine recipients is crucial for public health policy. Our results demonstrated substantial protection from complications among booster-dose vaccine recipients throughout the evaluation period and, further, suggest that this protection may be higher than the protection found shortly after the receipt of the second dose . Although a study from a health maintenance organization in Israel demonstrated VE estimates of 93% against hospitalizations, 92% against severe disease, and 81% against death , such analysis cannot distinguish between complications averted because of reductions in SARS-CoV-2 infections and reduction of complications among breakthrough cases. Further analysis is necessary to determine whether rate reductions of complications in booster-dose recipients are affected by the spread of the B.1.1.529 (Omicron) variant and whether those rate reductions are waning over time.\n\n【37】Our study’s first limitation is that the size of the unvaccinated control study group was calculated on the basis of Israel Central Bureau of Statistics data. Nevertheless, these data included population size by sex and age, which enables statistical adjustment. Furthermore, data concerning hospitalizations, disease severity, and deaths were available in the SARS-CoV-2 PCR test repository for unvaccinated SARS-CoV-2–positive persons. The lack of information regarding the presence of comorbidities constitutes another limitation. However, the use of multiple cohorts, the size of the population included in our study, the consistent VE estimates among various age groups, and the successful use of similar methodology in previous SARS-CoV-2 VE studies  support the validity of our results.\n\n【38】In this evaluation, we did not estimate VE against symptomatic disease. When the number of PCR-positive persons increases, the ability to conduct epidemiologic investigation and determine whether symptoms were present greatly diminishes. A further limitation was the low number of weekly complications in SARS-CoV-2–positive persons, particularly in weeks of lower SARS-CoV-2 circulation . However, this limitation was less evident among persons \\> 60 years of age, for whom the number of weekly complications is higher than for persons 16–59 years of age.\n\n【39】SARS-CoV-2 PCR testing and vaccination practices could vary among persons. Such differences can stem from behavior, occupation (such as being a healthcare worker), or health factors (such as having symptoms or risk factors or residing in a nursing home) and can potentially affect VE estimates against infection. Because SARS-CoV-2 PCR testing has been commonly performed among hospitalized patients, determination of reductions in hospitalizations, severe or critical disease, and death rate were probably not affected by factors that might affect testing practices of nonhospitalized patients.\n\n【40】No distinction was available in the MOH SARS-CoV-2 data repository between persons who were hospitalized because of COVID-19 and those who were hospitalized because of other reasons and were SARS-CoV-2–positive. However, the severity status that is registered in the repository is given to COVID-19 patients on the basis of National Institutes of Health guidelines .\n\n【41】In conclusion, our results showing high VE of the BNT162b2 booster dose against SARS-CoV-2 cases and the maintenance of positive effects among breakthrough cases demonstrate the duration of the booster-dose effect during a period in which the Delta variant was predominant. However, the reduced VE in an Omicron-variant setting indicates that additional tools are required to combat new variants of concern.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0d15b676-c483-43c6-aa82-5937460787e2", "title": "Prioritizing Zoonoses for Global Health Capacity Building—Themes from One Health Zoonotic Disease Workshops in 7 Countries, 2014–2016", "text": "【0】Prioritizing Zoonoses for Global Health Capacity Building—Themes from One Health Zoonotic Disease Workshops in 7 Countries, 2014–2016\nEmerging and endemic zoonotic diseases pose a threat not only to the health of animals and humans but also to global health security. An estimated 60% of known infectious diseases and up to 75% of new or emerging infectious diseases are zoonotic in origin . Globally, infectious diseases account for 15.8% of all deaths and 43.7% of deaths in low-resource countries . It is estimated that zoonoses are responsible for 2.5 billion cases of human illness and 2.7 million human deaths worldwide each year . Emerging zoonoses are responsible for some of the most high profile and devastating epidemics ; however, endemic zoonoses  may actually pose a more insidious and chronic threat to both human and animal health. As one comparison, the 2014 Ebola epidemic was responsible for 11,316 deaths and $2.2 billion in economic losses , whereas each year rabies accounts for ≈59,000 human deaths and roughly $8.6 billion in economic losses worldwide . The global impacts of emerging and endemic zoonoses on both human and animal populations make fostering collaboration between human and animal health sectors using a multisectoral, One Health approach a critical step toward improving animal and human health.\n\n【1】Early detection of zoonotic pathogens through enhanced laboratory capacity and surveillance at the animal–human interface is a crucial step toward controlling and preventing zoonoses  and a core capacity for implementation of the World Health Organization International Health Regulations 2005 (IHR 2005) and the Global Health Security Agenda  . Rapidly detecting, responding to, and controlling public health emergencies at their source, including those caused by outbreaks of zoonotic diseases, is essential for global health security. However, in low-resource settings, capacity-building efforts should be initially focused on a few key diseases . Disease prioritization enables effective capacity building and resource allocation to increase surveillance, guide research, and improve preparedness and response protocols, further advancing global health security and the international health regulations .\n\n【2】To address this prioritization need, the US Centers for Disease Control and Prevention (CDC) developed the One Health Zoonotic Disease Prioritization (OHZDP) tool  as a multisectoral approach to rank a country’s zoonotic diseases using an objective, semiquantitative method. The OHZDP tool enables a country or region to bring together representatives from human, animal, and environmental health sectors to prioritize the endemic and emerging zoonoses of greatest national concern that should be jointly addressed by human, animal, and environmental health ministries using country- or region-specific criteria. Zoonotic diseases can be prioritized even in the absence of reliable prevalence data by using alternative measures for disease burden so that outcomes are provided in a timely manner, enabling country representatives to give immediate feedback, develop action plans, and capitalize on collaborations built during the prioritization process.\n\n【3】During 2014–2016, CDC implemented 7 OHZDP workshops. We summarize overarching themes identified from these workshops and highlight successes and lessons learned to best support additional countries in prioritizing zoonotic diseases by using this tool.\n\n【4】### Methods\n\n【5】CDC conducted OHZDP workshops using methods previously described . CDC maintains a pool of trained OHZDP workshop facilitators to conduct workshops and to train in-country facilitators to promote country ownership of the prioritization process and to leave the capacity to conduct future prioritization workshops in each country. We interviewed workshop facilitators, reviewed data from workshop materials maintained as part of our routine monitoring and evaluation activities, and reviewed available publications for information on alternate methods and outcomes. Variables collected are number and type of workshop participants by sector (voting members and observers), disease assessment criteria selected during the workshop and the resulting zoonoses rankings, and outcomes or planned next steps for multisectoral capacity building activities. Where appropriate, data for certain variables (e.g. disease ranking criteria) were standardized and combined into larger categories to look for overarching themes. Data were analyzed using Microsoft Excel (Microsoft, Redmond, WA, USA).\n\n【6】### Results\n\n【7】During 2014–2016, at countries’ request, CDC conducted OHZDP workshops in Thailand, Kenya , Ethiopia , Azerbaijan, Cameroon, South Africa, and the Democratic Republic of the Congo. All countries prioritized diseases on a national level; 4 (57.1%) workshops were specifically conducted to advance GHSA implementation in the country, but all countries had a goal to strengthen multisectoral collaboration and focus laboratory, surveillance, and prevention efforts. All workshops took place over a 2-day period, with an additional 1–2 days for training local facilitators, when requested.\n\n【8】All workshops used standard methods as previously described  for conducting the preworkshop activities and the in-country facilitated group work. Two countries (Kenya and Thailand) diverged from the standard methods by including more than the recommended number of voting members. Kenya placed voting members into 5 groups, then used group discussion and consensus to assign weights to the individual criteria . Thailand held 2 separate, concurrent workshops that produced 2 different outcomes; these outcomes were then combined at a separate meeting held 1 month later to develop a final list of criteria by discussion and consensus. Members were then grouped by their agencies and voted on the ranking or weight applied to each criterion before conducting a final ranking of the diseases.\n\n【9】##### Facilitators and Participants\n\n【10】Fourteen CDC-trained OHZDP workshop facilitators were used for the 7 workshops and represented interdisciplinary backgrounds with expertise in zoonoses. A total of 21 in-country facilitators were trained at 5 of the 7 workshops, with an average of 4 (range 2–6) facilitators per workshop. In-country facilitators represented ministries of health (n = 8), agriculture (n = 5), environment (n = 1), and wildlife (n = 1); research institutes (n = 2); CDC in-country staff (n = 2); and other partners (n = 2). Field Epidemiology Training Program graduates were a resource for in-country facilitators in 2 workshops. Postworkshop debrief meetings and CDC facilitator interviews revealed specific lessons. For example, facilitators who held high-level positions were not available for the entire workshop because of competing priorities. In addition, it was deemed important that in-country facilitators be seen as unbiased during the facilitation process.\n\n【11】A total of 107 voting members participated in the 7 workshops (range 5–33), and multiple sectors were represented . The average number of voting members per workshop was 15, but excluding 2 outlier workshops that grouped voting members (Kenya, n = 33; and Thailand, n = 22), the average was 10 (range 5–11).\n\n【12】Six of workshops included observers from partner organizations or ministries. The number of observers averaged 10 (range 1–26) per workshop. Observers typically included in-country representatives from ministry partners, universities and research institutes, the World Health Organization, the Food and Agriculture Organization of the United Nations, Defense Threat Reduction Agency, the US Agency for International Development and its implementing partners, and CDC.\n\n【13】##### Zoonotic Disease Lists\n\n【14】All countries provided an initial list of zoonotic diseases from the relevant ministries to the OHZDP core planning team. Many of these lists were initially created by referencing the countries’ human and animal health sector reportable disease lists. The presence of a reportable disease list did not reflect the surveillance capacity, and this variable, if selected, was assessed on-site by in-country subject matter experts. The core planning team conducted an extensive country and regionally specific literature review on the disease list. Voting members reviewed and approved the disease list on the first day of the workshop for use in the prioritization process.\n\n【15】Each list, on average, included 37 (range 25–43) diseases or syndromes. Zoonoses on these lists were classified as 41.4% (range 27.8%–51.3%) bacterial, 37.7% (range 28.0%–44.4%) viral, 18.3% (range 13.9%–25.0%) parasitic, 2% (range 0%–11.1%) fungal, and 0.8% (range 0%–4%) prion in nature. All lists included endemic and emerging zoonotic diseases relevant to the country or region.\n\n【16】All 7 initial country lists included the following bacterial zoonoses: anthrax, brucellosis, leptospirosis, plague, Q fever, salmonellosis, and zoonotic tuberculosis. All lists also included the following viral zoonoses: Crimean-Congo hemorrhagic fever; coronaviruses, including Middle East respiratory syndrome and severe acute respiratory syndrome; flaviviruses, including yellow fever and West Nile; hemorrhagic fever viruses, including Ebola and Marburg; rabies; and zoonotic influenza viruses. Six of the country lists included the following parasitic diseases: cysticercosis or taeniasis, echinococcosis, and toxoplasmosis.\n\n【17】##### Prioritization Criteria\n\n【18】Six of the 7 countries selected 5 disease-ranking criteria; 1 country selected 6 criteria. All selected criteria were categorized into 7 overarching topic areas; 4 of those topics were further broken down into 2–3 more specific subtopics . All 7 countries ranked diseases on the basis of social, economic, or environmental impact. Six of 7 countries ranked zoonotic diseases on the basis of availability of proven interventions, epidemic or pandemic potential, and severity of disease in humans; 5 ranked zoonoses on the basis of documented presence of disease in the country or region.\n\n【19】When looking at the weighting, or level of importance, voting members assigned severity of disease in humans and epidemic/pandemic potential as the 2 criteria with the highest average weight. Next were documented presence of disease in the country or region, and economic, environmental, or social impact. Last, availability of proven interventions and all other remaining criteria categories were assigned the lowest weight. However, no single criterion stood out across all 7 workshops.\n\n【20】##### Criteria Questions and Responses\n\n【21】Six of the 7 countries created 1 single or compound question for each selected criterion. One country created 2 separate questions for 4 of their 5 criteria, for a total of 9 questions. Voting members chose ordinal variables for all responses assigned to each criteria question. Seven (17.5%) questions had a binary response (yes/no), whereas most (82.5%) had \\> 3 possible responses per criteria question. Regardless of the number of responses per question, all scores were normalized among criteria by using standard OHZDP tool methods .\n\n【22】A higher ordinal value (or score) was assigned to the responses for each question that correlated with a more severe, or negative, outcome. For example, a disease with a 50% case-fatality rate would receive a higher ordinal value than a disease with a 10% case-fatality rate. For questions that evaluated existing preventive measures, diagnostic capacity, and multisectoral collaboration, a higher ordinal score was given to responses indicating existing capacity or resources. For example, a zoonosis that could be diagnosed in the country would receive a higher score than one that could not.\n\n【23】##### Zoonotic Disease Ranking\n\n【24】As a result of the tool’s ranking process in these 7 countries, 19 diseases or syndromes were ranked as prioritized diseases . Of those, zoonotic influenza virus (n = 5), rabies (n = 5), brucellosis (n = 5), and anthrax (n = 4) were ranked by the most countries. Four of the 7 countries ranked a mix of endemic and emerging zoonoses; 2 ranked only endemic zoonoses , and 2 ranked only emerging zoonoses. Of the 4 countries that listed endemic and emerging diseases, on average, 76% (range 60%–83%) of the zoonoses on the final list were known to be endemic in the country. Six countries ranked viral, bacterial, and fungal zoonoses, and 2 countries also ranked parasitic diseases; 1 country ranked only viral diseases.\n\n【25】##### Final Prioritized List of Zoonotic Diseases\n\n【26】Four of the 7 counties used the original zoonoses produced by the OHZDP tool as their final prioritized list. Two countries agreed to adjust their lists to incorporate other zoonoses that the voting members felt should be in the top 5, and 1 country chose to adjust the order of the rankings to better reflect importance but retained the same zoonoses. Five countries chose a final list of 5 prioritized zoonoses, 1 country chose 6, and 1 country chose 3.\n\n【27】The most common zoonoses seen on the final prioritized lists remained the same as the original ranked list with the exception that rabies was selected in an additional country and brucellosis was removed in 1 country . Five of the seven countries included both endemic and emerging zoonoses on their final prioritized lists; 69% (range 33%–83%) of these prioritized zoonoses were considered endemic to the country prioritizing the disease. Two countries prioritized only endemic zoonoses . All of the emerging zoonoses prioritized by each country were viruses. All voting members came to consensus on the final prioritized zoonoses list, modified or not. This final list was then endorsed and adopted by the participating ministries.\n\n【28】### Outcomes\n\n【29】Six of 7 countries planned follow-up activities as part of the workshop. Twenty postworkshop action themes were identified . All 6 countries sought to ensure that the final prioritized list and any after-action items were approved by all participating ministries. Developing or updating and approving some type of national One Health strategy, guiding principles, or workplan was also universally identified as a desired outcome of this prioritization process. Four of the 6 countries indicated plans to use this list to establish recurring meetings, a multisectoral One Health working group or coordinating mechanisms, or both; 1 country that did not list this as an outcome already has a One Health coordination mechanism in place. The remaining action areas focused on various aspects of capacity building .\n\n【30】Kenya, which did not plan postworkshop activities, had previously created a One Health strategic plan in 2012 . Their plan included many of the same capacity-building activities stated by other countries, and prevention and control activities were already under way for 4 of the 5 prioritized zoonoses. Kenya’s prioritized list validated existing activities and enabled the Zoonotic Disease Unit, the One Health coordinating mechanism for Kenya, to garner further support from the Government of Kenya to continue these efforts.\n\n【31】### Discussion\n\n【32】During 2014–2016, CDC successfully carried out 7 OHZDP workshops in Thailand, Kenya , Ethiopia , Azerbaijan, Cameroon, South Africa, and the Democratic Republic of the Congo. Several other tools and methods have been applied to prioritize zoonotic diseases , but the OHZDP process is unique in that it enables country-led decisions using a multisectoral approach to prioritize both emerging and endemic zoonotic diseases while strengthening One Health collaborations and developing action plans to build capacity for the prioritized zoonoses. In addition, the OHZDP tool can meet the needs of those working in areas where quantitative data on zoonoses are lacking. Last, the OHZDP process provides outcomes in a timely manner so that participants may give immediate feedback and capitalize on One Health collaborations built during the prioritization process.\n\n【33】We have found key successes and lessons learned through the review of these workshops. First, successful outcomes are dependent on trust, transparency, equal representation, and consensus from all relevant sectors participating in the prioritization process and approving the final prioritized list of zoonoses. The CDC-trained OHZDP workshop facilitators not only conduct workshops but also train in-country facilitators to promote country ownership of the process and to build in-country capacity to conduct future workshops. Trained facilitators ensure that the prioritization process is standardized and conducted effectively. We found that using an interdisciplinary team of trained facilitators who remained neutral, unbiased, and did not focus on their specific sector, affiliation, or area of expertise enabled voting members’ voices to be heard and recognized. Our review found that most voting members were from the human (35.5%) and animal (30.8%) health sectors, but additional sectors were represented where available, ensuring the multisector nature of this process.\n\n【34】To accommodate a larger number of voting participants, methods were modified in 2 workshops. However, because these methods have not been rigorously tested, it is still advised that future workshops maintain the recommended number of participants (8 to 12) to enable more focused discussion during and timely results from the 2-day workshop.\n\n【35】Funding partner advocacy and support of the process and future activities is a potential benefit of observer participation. However, care is needed to ensure that the number of observers in their role as advisors and participants during discussions do not overwhelm or influence the process. Keeping to the recommended 10–15 total observers  is needed so that voting members can focus on the workshop process. We recommend having an overview summary at the end of the workshop that is open to a larger group of higher level in-country representatives and other partners to share the workshop outcomes in a timely way.\n\n【36】The OHZDP tool was designed to accommodate diversity in location (i.e. globally) and scale (i.e. local, national, regional) into the prioritization process so participants can select criteria relevant to their needs. We found that most countries were interested in selecting criteria that targeted zoonoses known to be present in country with the following attributions: high illness and death rates in humans; pandemic potential; availability of proven interventions; and economy, environment, or societal impact. Most prioritized zoonoses were endemic diseases, illustrating that countries wanted to first focus their limited resources on diseases for which they could successfully implement enhanced diagnostic capacity, surveillance, and proven interventions.\n\n【37】Common priority action items identified in these workshops are highly relevant to advancing global health security, including improving data sharing between ministries, improving communication to the public, strengthening the One Health workforce, developing disease-specific subcommittees, and increasing general surveillance and outbreak response capacity. Such activities will enhance the capacity of countries to rapidly detect, respond to, and contain public health emergencies, including outbreaks of zoonotic diseases, at their source and thereby ensure global health security. Most countries with identified priority action items planned to use this list to solicit or engage funding partners, which highlights countries taking ownership of the prioritization process, and recognizing and advocating for support around their country-specific priorities. Six countries made sure that the prioritized list and any after-action items were approved by all participating ministries and that a national One Health strategy or multisectoral coordination mechanism was established if it had not been already. By forming or hosting these prioritization workshops with a ministerial One Health coordinating committee, these after-action plans are more readily taken up.\n\n【38】Four of the 7 countries conducted this activity to meet Joint External Evaluation and GHSA zoonotic disease prioritization and collaboration goals. The next step is that these countries then build these plans into their existing activities.\n\n【39】As part of the continual improvement process for the OHZDP tool, we are employing postworkshop evaluations, in addition to continuing the postworkshop debriefs and facilitator interviews to ensure that these workshop continue to have successful outcomes. Moving forward, lessons learned from OHZDP workshops conducted during 2014–2016 will be applied to standardize and enhance the prioritization process in the future.\n\n【40】All 7 prioritizations were conducted during or in the wake of the 2014 West Africa Ebola outbreak . This event likely influenced the outcome for 1 country that prioritized Ebola despite the disease not being endemic or a likely risk in the country or region. Periodically repeating this prioritization process could help eliminate bias from current events, as well as aid in reevaluating if currently prioritized diseases still pose a public health threat, if sufficient capacity has been built, and if newly emerging diseases or other zoonoses need to be considered.\n\n【41】In summary, the GHSA uses a One Health multisectoral approach to strengthen the capacity at the global and national levels to prevent, detect, and respond to human and animal infectious disease threats, whether naturally occurring or accidentally or deliberately spread, that threaten global health security. Both endemic and emerging zoonotic diseases are recognized as being critical for global health security and related efforts. The OHZDP tool aids the GHSA mission by helping countries and regions prioritize their zoonotic diseases of greatest national concern and focusing GHSA capacity-building efforts on improving laboratory capacity, surveillance, outbreak response, and prevention activities on a few key zoonoses at first. The OHZDP process also supports progress toward the Joint External Evaluation, specifically for the zoonotic disease indicators, on having national laboratory, surveillance, and joint outbreak response plans and strategies in place for priority endemic/emerging zoonotic diseases with evidence of a multisectoral, coordinated approach. A multisectoral zoonotic disease prioritization with equal engagement from all sectors active in zoonotic disease work is one of the most cost-effective ways a country, especially one with limited resources, can begin using a One Health approach to prevent, detect, and respond to public health threats. By building these capacities and strengthening One Health partnerships for prioritized diseases, a country will not only more effectively address existing diseases but also have the systems in place to be better prepared to detect and respond to new and emerging diseases that may occur and become a threat to global health security.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ac7f0d3b-708a-4798-978d-452365bc74c7", "title": "Effects of Pneumococcal Conjugate Vaccine 2 Years after Its Introduction, the Netherlands", "text": "【0】Effects of Pneumococcal Conjugate Vaccine 2 Years after Its Introduction, the Netherlands\n_Streptococcus pneumoniae_ is a leading cause of invasive infections, such as meningitis, septicemia, and bacteremia, and of more common respiratory tract infections, such as pneumonia and otitis media. Young children and elderly persons are at particularly high risk for pneumococcal infection . In the United States, the introduction in 2000 of the CRM197-conjugated 7-valent pneumococcal vaccine (PCV-7) resulted in a 77% reduction in 2005 of invasive pneumococcal disease (IPD) in children <5 years of age from IPD rates reported in 1998–1999 . IPD rates in children decreased mostly within the first 2 years after introduction of PCV-7; leveled off in 2002; and then stabilized, despite an ongoing decrease of vaccine-serotype IPD, due to a gradual increase of non–vaccine-serotype IPD, particularly serotype 19A . In addition, use of the vaccine in children was associated with reduced IPD rates for unvaccinated age groups, which resulted from reduced nasopharyngeal colonization of vaccine-serotype _S. pneumoniae_ in vaccinated children and concomitant reduced transmission . The cost effectiveness of herd immunity conferred by the conjugate vaccine in the United States prompted implementation of the vaccine in the Netherlands .\n\n【1】Data from the United States concerning both direct and indirect vaccine benefit, however, cannot be translated indiscriminately to European countries because of several major differences. Vaccine-serotype coverage by PCV-7 was lower in European countries (60%–70%) than in the United States (>80%) , which may leave more room for non–vaccine-serotype replacement in European countries. Second, in the Netherlands (as in most European countries), baseline IPD incidence rates are based mainly on culture-confirmed cases in hospitalized children, resulting in markedly lower IPD incidence rates for young children in the Netherlands than for those in the United States, where blood samples are cultured for more patients. Before introduction of PCV-7 in the Netherlands, overall IPD rates were 35 cases/100,000 children <2 years of age, of which 15 cases/100,000 children were meningitis . In contrast, in the United States, IPD incidence before introduction of PCV-7 peaked at 188 cases/100,000 children <2 years of age in 1998–1999 , and 10 cases/100,000 children in that age group were meningitis .\n\n【2】Consequently, introduction of PCV-7 may have affected IPD incidence in European countries differently than in the United States. To assess the effectiveness of PCV-7 on IPD in the Netherlands, we evaluated the incidence and clinical syndromes of IPD in PCV-7–vaccinated and –unvaccinated children and in other age groups during the first 2 years after implementation of PCV-7.\n\n【3】### Materials and Methods\n\n【4】##### Surveillance and Data Collection\n\n【5】PCV-7 was introduced into the Netherlands’ national immunization program (NIP) in June 2006 and was recommended for all infants born after April 1, 2006, at 2, 3, 4, and 11 months of age . Our study comprised all patients with culture-confirmed IPD during June 1, 2004–June 1, 2006 (preimplementation period) and June 1, 2006–June 1, 2008 (postimplementation period). Isolates were serotyped by the Netherlands Reference Laboratory for Bacterial Meningitis (NRLBM), which collects nationwide bacterial isolates from blood, cerebrospinal fluid (CSF), and/or other normally sterile bodily fluids for laboratory-based surveillance. Isolates from all patients with IPD were submitted by 9 sentinel laboratories throughout the country. These laboratories covered ≈4.074.412 and ≈4.090.233 residents in the preimplementation period and postimplementation period, respectively, representing ≈25% of the population of the Netherlands. Laboratories were selected on the basis of their reliability for submitting pneumococcal isolates; they submitted ≈90%–95% of the pneumococcal isolates from CSF and ≈83% of pneumococcal isolates from blood .\n\n【6】Isolates were serotyped as previously described, by using antiserum from the Statens Serum Institute (Copenhagen, Denmark) . Isolates with serotypes 4, 6B, 9V, 14, 18C, 19F, and 23F, the serotypes contained in PCV-7, were considered vaccine serotypes. All other serotypes were considered non–vaccine-serotypes.\n\n【7】##### Clinical Characteristics\n\n【8】Nearly all (97%–98%) IPD cases were in hospitalized patients. We retrospectively abstracted information about their clinical syndromes and underlying conditions from hospital records. Clinical syndromes were categorized as meningitis or nonmeningitis IPD (invasive pneumonia, IPD with other focus, and bacteremia without focus). Meningitis was defined as CSF culture positive for _S. pneumoniae_ (or positive CSF by PCR) and/or clinical diagnosis of meningitis in combination with a blood culture positive for _S. pneumoniae_ . Invasive pneumonia was physician-diagnosed pneumonia and a blood culture positive for _S. pneumoniae_ . IPD with other focus was an _S. pneumoniae_ –positive culture of blood or other normally sterile body fluid in combination with a clinical focus other than meningitis or pneumonia. For bacteremia without focus, no clinical focus was identified. Underlying conditions were classified as immunocompromised conditions or other comorbidities, as described previously . Case fatality was defined as in-hospital death and/or death within 30 days after the first reported blood/CSF culture positive for _S. pneumoniae_ .\n\n【9】##### Statistical Analyses\n\n【10】To study the effectiveness of the vaccination program, we compared age-specific incidences during the preimplementation and postimplementation periods. Incidence rates of IPD were calculated as number of cases per 100,000 persons per year by using 25% of the Dutch population on January 1 for each considered year, accounting for the 25% coverage of surveillance data. Changes in incidence rates from the preimplementation to the postimplementation period were presented as incidence rate ratio with 95% confidence intervals (CI) and as percent changes. We compared the preimplementation and postimplementation periods with regard to distribution of clinical syndromes, comorbidities, and outcomes. Theoretical coverage of IPD was based on data from the preimplementation and postimplementation periods for future 10-valent PCV (PCV-10, covering PCV-7 serotypes plus serotypes 1, 5, and 7F) and 13-valent PCV (PCV-13, covering PCV-10 serotypes plus serotypes 3, 6A, and 19A). Proportions were tested with χ 2  or Fisher exact tests, as appropriate. We considered p <0.05 to be significant. Statistical analyses were performed with SAS version 9.1.3 (SAS Institute, Cary, NC, USA), Excel 2007 (Microsoft, Redmond, WA, USA), and Episheet .\n\n【11】### Results\n\n【12】During the study period, the NRLBM received 2,649 _S. pneumoniae_ isolates: 1,297 during the preimplementation period and 1,352 during the postimplementation period. Medical records were assessed for 1,235 (95%) cases during the preimplementation period and for 1,317 (97%) cases during the postimplementation period. Pneumococcal serotype was available for 1,225 and 1,304 cases (both 99%), respectively.\n\n【13】##### IPD Incidence\n\n【14】Overall incidence of IPD remained stable; 15.9 vs. 15.0 cases/100,000 persons during the postimplementation and preimplementation periods, respectively. Incidence of vaccine-serotype IPD did not change significantly. For non–vaccine-serotype IPD, incidence increased 13% (95% CI 2%–26%; p = 0.02) . In children <2 years of age, including those not vaccinated or incompletely vaccinated, the incidence of IPD decreased 35% (95% CI 4%–56%; p = 0.006), from 34.5 cases/100,000 persons in the preimplementation period to 22.5 cases/100,000 persons in the postimplementation period . Incidence of vaccine-serotype IPD declined by 67% (95% CI 41%–81%; p<0.0001), from 24.3 to 8.0 cases/100,000 persons. In contrast, non–vaccine-serotype IPD incidence increased, but not significantly, from 10.1 to 14.5 cases/100,000 persons (p = 0.40).\n\n【15】Among children born after April 1 2006 (i.e. age-eligible for vaccination according to the NIP), the incidence rate of vaccine-serotype IPD in the postvaccination period (2.4 cases/100,000 persons) decreased 90% (p<0.0001) compared with that for an age-matched group in the preimplementation period (24.2 cases/100,000) . Although not significant because of low numbers, the incidence of non–vaccine-serotype IPD had risen by 71%, from 9.8 to 16.8 cases/100,000 persons (p = 0.12), leading to a total net reduction of 44% (95% CI 7%–66%; p = 0.02) in the birth group age-eligible for vaccination.\n\n【16】Three vaccine-serotype IPD cases occurred among children born after April 1, 2006; 2 cases after 1 vaccine dose (serotypes 9V and 23F) and 1 case within 1 week after the second dose (serotype 9V, isolated from CSF). In infants <2 years of age born before April 1, 2006 (i.e. age-ineligible for PCV-7), no changes occurred in vaccine- or non–vaccine-serotype IPD rates in the postimplementation period compared with those for age-matched children in the preimplementation period.\n\n【17】##### Serotype Distribution\n\n【18】After introduction of PCV-7, for all vaccine serotypes, the number of IPD cases among the total population remained stable, except for serotype 19F (44 vs. 23 cases; p = 0.004). Also, proportions of non–vaccine-serotype 1 and 22F significantly increased . Among children born after April 1, 2006, serotypes 1 and 7F increased in comparison with those for age-matched infants in the preimplementation period . For serotype 6A, 6C and 19A, no significant changes occurred in any age group.\n\n【19】##### Clinical Characteristics\n\n【20】Among children <2 years of age, incidence rates decreased for all clinical syndromes to approximately the same extent . Rates of meningitis declined 34% from 14.7 to 9.6 cases/100,000 children (29 vs. 18 cases) and of nonmeningitis IPD 35% from 19.8 to 12.9 cases/100,000 children in this age group (39 vs. 24 cases). Of these children, 30% (20/66) had comorbidities in the preimplementation period, compared with 9% (4/44) in the postimplementation period (p<0.001). In all other age groups, clinical syndromes did not change from the preimplementation to the postimplementation period, except for a 124% rise in rates of non–vaccine-serotype meningitis for persons 5–49 years of age (95% CI 19%–320%; p = 0.01). The proportions of adult patients with comorbidities and immunocompromising conditions were similar in the preimplementation and postimplementation periods: 71% vs. 74% and 19 vs. 22%, respectively. For the vaccinated group of children, the case-fatality rate remained stable (9.3% vs. 8.3%) in the preimplementation and postimplementation periods, respectively. In other age groups, case-fatality rates did not differ (data not shown).\n\n【21】##### Estimated Coverage by Future Vaccines\n\n【22】Among vaccination-eligible children, the additional coverage rates in the preimplementation and postimplementation periods were 2.2% (1/45 cases) and 54.2% (13/ 24 cases) for PCV-10 (p<0.0001). For PCV-13, they were 19.6% (8/45 cases) and 66.7% (16/24 cases) (p<0.001) .\n\n【23】### Discussion\n\n【24】The total net IPD reduction of 35% among children <2 years of age observed in the Netherlands differs from the favorable results reported in the United States, where a 69% decrease in IPD within the first 2 years after PCV-7 introduction was reported despite lower vaccine uptake (i.e. estimates of national immunization coverage) rates in the United States than in the Netherlands . US estimates for PCV-7 uptake among children born in 2001 who received \\> 1 and \\> 3 doses were 89% and 68%, respectively . In contrast, in the Netherlands, 94.4% of all infants born in 2006 were fully vaccinated at 2 years of age . Also, the decrease in meningitis incidence in the Netherlands was lower than that for US infants <2 years of age (34% vs. 59%), whereas during the preimplementation period, meningitis incidence was comparable in the 2 countries. In the first 2 years after implementation in the United States, IPD requiring hospitalization decreased 63% in children <2 years . In contrast, all IPD in this age group decreased 35%, and almost all reported IPD cases occurred in hospitalized patients.\n\n【25】The difference between the impact of PCV-7 in the Netherlands and the United States may be attributable to the lower proportion of vaccine-serotype cases covered by PCV-7 before implementation in the Netherlands and in Europe. Reports about PCV-7 effectiveness by other European countries support this observation. In Germany and Norway, PCV-7 effectively prevented disease in the youngest age groups during the first years after implementation, without major increase of non–vaccine-serotype IPD . In Spain, vaccine-serotype IPD decreased after PCV-7 implementation; however, non–vaccine-serotype IPD increased . In France, 3 years after PCV-7 introduction, overall IPD cases decreased 21% among children of < 2 years of age, when 44%–56% of children were vaccinated . In addition, similar to findings in the Netherlands, a simultaneous increase in IPD from non–vaccine-serotype pneumococci reduced the net benefit of vaccination. Differences in surveillance systems, temporal fluctuations of circulating serotypes, antimicrobial drug resistance and penicillin susceptibility of circulating pneumococcal strains, vaccination schedules, vaccine uptake, and blood sampling practices also may play a role in the differences between countries . Like Norway, a reduced-dose PCV-7 schedule has been introduced in the United Kingdom. In the first years after implementation, surveillance data from the United Kingdom have tended to show a major decline in vaccine-serotype IPD in infants <2 years of age concomitant with a substantial rise in non–vaccine-serotype IPD, reducing net vaccine benefits . Unlike in the United States, where IPD in children <2 years of age stabilized within 2 years after introduction of the vaccine , in Europe and in our study, incidence has not yet stabilized. Our results emphasize the need for continued surveillance to monitor the long-term public health benefits of the vaccination program.\n\n【26】Despite the high vaccination uptake in the Netherlands, we observed no indication of herd immunity in other age groups during the first 2 years after implementation, except for a decrease in serotype 19F. This observation may be explained by the relatively short evaluation period of 2 years, a relatively small vaccinated group (2.25%) of the total population, and lack of a catch-up program for older children. In Australia and the United Kingdom, which have catch-up programs for children < 2 years of age, decreases in vaccine-serotype IPD in unvaccinated children within 3 years after PCV-7 implementation have been reported .\n\n【27】The small increase we found in non–vaccine-serotypes 1 and 7F among vaccinated children could be attributed to temporal fluctuations . The numbers in our study were too small and the period we studied too short to enable us to draw firm conclusions about changes in serotype-specific incidence. Serotype 1 also has increased in other age groups and may cause local outbreaks, as observed in other countries before the implementation of PCV-7 . In our study, we could not find evidence of outbreaks associated with serotype 1.\n\n【28】The increase in non–vaccine-serotype IPD in the vaccinated age group was not explained by more children with comorbidities or immunocompromised conditions in the years after introduction and not associated with a change in the case-fatality rate. Longer follow-up is needed to assess whether this increase and that in non–vaccine-serotype IPD in the overall population are temporary or are vaccine related. Several countries have suggested that use of PCV-7 might enhance the emergence of serotype 19A pneumococcal clones, often associated with penicillin resistance . In the Netherlands, where use of antimicrobial drugs is restricted, few penicillin-resistant pneumococcal isolates were received during the study period; 98.8% were susceptible to penicillin (MIC < 0.06 mg/L), 0.8% were intermediately susceptible (0.06–1.0 mg/L), and 0.4% were resistant (>1.0 mg/L). We did not see a prominent increase in serotype 19A among patients; only 1 isolate was penicillin resistant (>1.0 mg/L) in the first 2 years after PCV-7 implementation. However, an increase in serotype 19A pneumococci was found in a randomized controlled study of nasopharyngeal carriage among vaccinated children compared with unvaccinated controls before national implementation of PVC-7 . Also, no changes in distribution of serotype 6A or 6C were reported. Theoretical coverage of the future conjugate vaccines PCV-10 and PCV-13 increased in the postimplementation period in vaccination-eligible children. In all other age groups, no changes were observed. Future vaccines need to be considered to improve the net benefit of immunization against pneumococcal diseases.\n\n【29】Some limitations should be acknowledged. Although our study covered ≈25% of the population of the Netherlands, numbers of IPD cases and serotype distribution are small and need cautious interpretation. After 2 years, final vaccine benefits cannot be established. Furthermore, distribution of serotypes among invasive pneumococci may fluctuate over time, and temporal trends in serotype may vary across geographic regions and independent of PCV-7 implementation . Although the 9 sentinel laboratories submitted 25% of all pneumococcal isolates received by the NRLBM (nationwide coverage 95%), the population under surveillance might be overestimated because the laboratories were selected on reliability of stable pneumococcal isolates submission over the years. Second, our surveillance system depended on how well the 9 sentinel laboratories were submitting their isolates, and small shifts in the proportion of submission to the NRLBM cannot be excluded . Blood culture rates may have influenced IPD incidence reported in this study . However, these changes are not likely to be substantial. National IPD incidence rates estimated from the number of isolates submitted by the 9 sentinel laboratories are similar to those of neighboring countries, e.g. Denmark and the United Kingdom, that have comparable health system practices . The rates of submission of blood and CSF isolates for children <5 years of age have been stable in the Netherlands for the past 10 years at ≈90%. Enhanced surveillance with such high submission rates and stable overall IPD rates cannot explain the 71% increase of nonvaccine serotypes. Also, and most important, the changes in IPD serotype distribution occurred only in vaccination-eligible infants. No changes in serotype distribution or signs of herd immunity were observed in unvaccinated infants. A potential bias by enhanced awareness would cause differences also in this group. Long-term surveillance data will elucidate whether the changes in serotype distribution in vaccinated and unvaccinated persons remain. Lastly, in the Netherlands, no changes in diagnostic methods or blood culture practices have been implemented recently.\n\n【30】Strengths of our study include the detailed information about the IPD cases and the established high degree of vaccine uptake for the NIP in the Netherlands (≈95% of infants of < 1 year of age are fully vaccinated) . Vaccination with the 23-valent pneumococcal polysaccharide vaccine has not been routinely recommended for elderly persons, and uptake has been negligible in the Netherlands; thus, any influence of this vaccine can be excluded . The low proportion of penicillin-resistant pneumococcal isolates and the densely living but relatively homogeneous population make the Netherlands particularly suitable for describing vaccine effects. Our study provides accurate data from a representative group of the Dutch population with fairly detailed information about the distribution of clinical syndromes and presence of comorbidities.\n\n【31】Shortly after introduction of PCV-7 vaccination for infants, the direct vaccine effectiveness on IPD caused by vaccine-serotype pneumococci appeared high in the Netherlands. However, the net benefit is partly offset by the increased incidence of nonvaccine serotypes. For this reason, future conjugate vaccines may be valuable in further reducing IPD incidence. These results further emphasize the need for ongoing surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "693fc4ca-7e23-41f3-94bb-00185f9cd3ac", "title": "Strategies Proposed by Healthy Kids, Healthy Communities Partnerships to Prevent Childhood Obesity", "text": "【0】Strategies Proposed by Healthy Kids, Healthy Communities Partnerships to Prevent Childhood Obesity\nAbstract\n\n【1】**Introduction**  \nHealthy Kids, Healthy Communities (HKHC) is an initiative of the Robert Wood Johnson Foundation to prevent obesity among high-risk children by changing local policies, systems, and environments. In 2009, 105 community partnerships applied for funding from HKHC. Later that year, the Centers for Disease Control and Prevention (CDC) released recommended community strategies to prevent obesity by changing environments and policies. The objective of this analysis was to describe the strategies proposed by the 41 HKHC partnerships that received funding and compare them to the CDC recommendations.\n\n【2】**Methods  \n**We analyzed the funded proposals to assess the types and prevalence of the strategies proposed and mapped them onto the CDC recommendations.\n\n【3】**Results**  \nThe most prevalent strategies proposed by HKHC-funded partnerships were providing incentives to retailers to locate and serve healthier foods in underserved areas, improving mechanisms for purchasing food from farms, enhancing infrastructure that supports walking and cycling, and improving access to outdoor recreational facilities.\n\n【4】**Conclusion**  \nThe strategies proposed by HKHC partnerships were well aligned with the CDC recommendations. The popular strategies proposed by HKHC partnerships were those for which there were existing examples of successful implementation. Our analysis provides an example of how information from communities, obtained through grant-writing efforts, can be used to assess the status of the field, guide future research, and provide direction for future investments.\n\n【5】Introduction\n\n| **Box. CDC-Recommended Community Strategies and Measurements for Preventing Obesity in the United States, 2009 a** |\n| --- |\n| **Category 1: Strategies to promote the availability of affordable healthy food and beverages** |\n| 1: Communities should increase availability of healthier food and beverage choices in public service venues. |\n| 2: Communities should improve availability of affordable healthier food and beverage choices in public service venues. |\n| 3: Communities should improve geographic availability of supermarkets in underserved areas. |\n| 4: Communities should provide incentives to food retailers to locate in and/or offer healthier food and beverage choices in underserved areas. |\n| 5: Communities should improve availability of mechanisms for purchasing foods from farms. |\n| 6: Communities should provide incentives for the production, distribution, and procurement of foods from local farms. |\n| **Category 2: Strategies to support healthy food and beverage choices** |\n| 7: Communities should restrict availability of less healthy foods and beverages in public service venues. |\n| 8: Communities should institute smaller portion size options in public service venues. |\n| 9: Communities should limit advertisements of less healthy foods and beverages. |\n| 10: Communities should discourage consumption of sugar-sweetened beverages. |\n| **Category 3: Strategy to encourage breastfeeding** |\n| 11: Communities should increase support for breastfeeding. |\n| **Category 4: Strategies to encourage physical activity or limit sedentary activity among children and youth** |\n| 12: Communities should require physical education (PE) in schools. |\n| 13: Communities should increase the amount of physical activity in PE programs in schools. |\n| 14: Communities should increase opportunities for extracurricular physical activity. |\n| 15: Communities should reduce screen time in public service venues. |\n| **Category 5: Strategies to create safe communities that support physical activity** |\n| 16: Communities should improve access to outdoor recreational facilities. |\n| 17: Communities should enhance infrastructure supporting cycling. |\n| 18: Communities should enhance infrastructure supporting walking. |\n| 19: Communities should support locating schools within easy walking distance of residential areas. |\n| 20: Communities should improve access to public transportation. |\n| 21: Communities should zone for mixed-use development. |\n| 22: Communities should enhance personal safety in areas where people are or could be physically active. |\n| 23: Communities should enhance traffic safety in areas where people are or could be physically active. |\n| **Category 6: Strategy to encourage communities to organize for change** |\n| 24: Communities should participate in community coalitions or partnerships to address obesity. |\n| Abbreviation: CDC, Centers for Disease Control and Prevention.a Source: Khan et al . |\n\n【7】The Centers for Disease Control and Prevention (CDC) and the Institute of Medicine (IOM) promote adding environmental and policy changes to the existing methods of health promotion to prevent obesity , and they have recently recommended evidence-based strategies for preventing obesity in the United States . The Robert Wood Johnson Foundation’s (RWJF) Healthy Kids, Healthy Communities (HKHC) national program aims to support healthier communities by implementing healthy eating and active living policies, systems, and environmental changes . The program focuses on reaching children at highest risk for obesity because of their race/ethnicity, income, or location. Full proposals for the second round of the HKHC initiative were submitted in May 2009 after the launch of a limited number of sites in the first round in 2008. The HKHC program proposals allow a snapshot of the strategies the communities selected to address childhood obesity. We compared these proposed strategies with expert recommendations. The CDC and IOM recommendations for preventing obesity overlap , but we focus only on the strategies released by CDC in July 2009, soon after the submission of the HKHC proposals. Drawing on the best available evidence and expert opinion, the CDC recommendations, developed for local governments and communities, include a set of 24 community strategies grouped into 6 categories (Box) .\n\n【8】The objectives of this research were to undertake a content analysis of 41 funded HKHC proposals and compare them with the CDC recommendations to better understand 1) how the HKHC proposed strategies aligned with CDC recommendations that came later, 2) which strategies were proposed by funded HKHC communities with different characteristics, 3) which strategies were proposed by communities despite limited information about effectiveness, and 4) the relationship between broad national recommendations and concrete local community strategies.\n\n【9】Methods\n\n【10】RWJF received 540 brief proposals for the second round of HKHC funding in February 2009, after launching 9 leading sites in December 2008. From this pool, 110 community partnerships were invited to submit full proposals. Finally, 41 grantees were selected from 105 full proposal submissions based on guidelines laid out in the call for proposals and on reviewers’ predetermined criteria. Reviewers included RWJF staff, HKHC staff, members of the National Advisory Committee for the HKHC initiative, and other external representatives with expertise in childhood obesity prevention or community-based initiatives. Each review committee had at least 1 representative from each group. Communities were considered high-priority if they had a high representation of children from low-income households, had a high percentage of racial/ethnic minority groups at higher risk of obesity, and were in states with a high prevalence of childhood obesity (Alabama, Arkansas, Arizona, Florida, Georgia, Kentucky, Louisiana, Mississippi, New Mexico, North Carolina, Oklahoma, Tennessee, Texas, South Carolina, and West Virginia).\n\n【11】HKHC defined a community as a municipality, county, or region. The HKHC call for proposals focused on approaches that involved policy, systems, and environmental changes for preventing childhood obesity in settings outside the school day. Although the community partnerships were to propose their best thinking on the types of changes they might pursue, the grant program included a planning period during which communities conducted assessments and received technical assistance from the HKHC National Program Office to refine their strategies to meet their community’s needs. The HKHC call for proposals was developed independently from the CDC recommendations, despite RWJF involvement in both. The technical assistance provided to applicants focused on guiding them toward selecting approaches that focused on policy, systems, and environmental changes and not on selecting specific strategies.\n\n【12】Proposals were submitted to RWJF using an online submission system. Funding decisions were based on need, apparent capacity, feasibility of implementation, and geographic and demographic diversity. The detailed content analysis described in this article was limited to full proposals submitted by the 41 HKHC grantees.\n\n【13】We analyzed the content of proposals in a 3-step process that used standardized abstraction and analysis forms. In Step 1, information from the proposal narrative was abstracted into Excel spreadsheets and classified according to the following themes: organizational capacity (description, mission, and unique characteristics); community partnerships (lead agency background, age of partnerships, key partners, previous work); proposed initiative (plan, evidence used, goals, major activities, and outcomes); and readiness (previous assessments, support from decision makers and politicians). Variables describing the communities and partnerships were quantified from the abstracted information. A more detailed analysis conducted on abstracted sections described communities’ proposed plans and strategies to map them onto the CDC recommendations. In Step 2, the proposed HKHC strategies were sorted into the 6 broad categories of CDC recommendations described in the box. In Step 3, each proposed strategy was matched with the corresponding CDC recommendation in the identified category. For some, information was available only for broad initiatives from the community; such cases were classified as “strategy to be determined” under the appropriate category. The research staff conducted the initial proposal abstraction (Step 1) under the supervision of one of the authors (L.B.). The lead author (P.O.V.) mapped and matched proposed strategies (Steps 2 and 3) with help from a trained research assistant; they both mapped the proposed strategies onto the CDC recommendations, and in cases of disagreement a resolution was made in consultation with another author (L.C.L.).\n\n【14】Most of the community strategies proposed were highly consistent with the CDC recommendations and with CDC’s suggested indicators to measure progress over time. If a community strategy matched the general aim of a CDC recommendation but was not consistent with the proposed CDC indicator of progress, it was classified as a match with the CDC recommendation. For example, CDC recommendation 18 is to enhance infrastructure to support walking. This recommendation can include a variety of efforts to promote walking; however, the suggested indicator of progress quantifies only paved sidewalk miles in the jurisdiction. HKHC communities chose various strategies to enhance walking, in addition to paving new sidewalks: signalized intersections, signage, and traffic-calming measures. All these strategies were classified as consistent with the CDC recommendation 18.\n\n【15】Although CDC used 6 general categories of recommendations, we analyzed only 4 in the analysis. Given HKHC’s focus on children aged 3 to 18 years, communities did not propose to encourage breastfeeding (CDC category 3). Also, because the HKHC call for proposals required communities to work through multidisciplinary partnerships (CDC category 6), all communities had established partnerships, and no variability in assessment was anticipated.\n\n【16】Not all of the community strategies fit the CDC recommendations. Nevertheless, all community strategies were abstracted to capture the HKHC partnerships’ understanding of the possible approaches to policy, systems, and environmental change. These strategies were categorized as community gardens, food policy councils, health promotion, and education.\n\n【17】Results\n\n【18】Twenty-three of the funded communities were from the priority states . All but 2 had a median income below $50,000, more than half exceeded 100,000 population, and more than half were in urban areas. Almost half of the partnerships in the funded communities had 3 or more years of experience working together.\n\n【19】Of the 41 proposals selected for funding, 12 community partnerships proposed to work at the county level, 21 at the municipal level, and 5 at the regional level. In most cases, leadership for these proposals was provided by nonprofit organizations (19 sites) and government departments (14 sites) .\n\n【20】### Matching HKHC proposed strategies with CDC recommendations\n\n【21】**CDC category 1: strategies to promote the availability of affordable healthy food and beverages**\n\n【22】Thirty-one partnerships proposed at least 1 strategy to promote the availability of affordable healthy foods and beverages . Providing incentives to food retailers and improving mechanisms for purchasing foods from farms were proposed by almost half of the partnerships . Incentives to food retailers included targeted economic development loans for small stores and technical assistance for creating links with federal food assistance programs. Strategies for improving mechanisms for purchasing foods from farms included establishing new farmers’ markets; improving their capabilities for using the electronic benefits transfer cards and vouchers from the Special Supplemental Nutrition Program for Women, Infants and Children; and expanding transportation options.\n\n【23】Eleven of the partnerships also proposed increasing the availability of healthier foods and beverages in public service venues, including day care and after-school settings and parks.\n\n【24】**CDC category 2: strategies to support healthy food and beverage choices**\n\n【25】Only 4 partnerships proposed strategies for supporting healthy food and beverage choices . Limiting advertising for less healthy food, restricting availability of unhealthy choices, and instituting smaller portion sizes were the proposed strategies in this category .\n\n【26】**CDC category 4: strategies to encourage physical activity or limit sedentary activity among children and youth**\n\n【27】Fifteen of the 41 partnerships proposed strategies to encourage physical activity or limit sedentary activity among children and youth . HKHC partnerships proposed strategies consistent with all the CDC recommendations in this category ; the most common was increasing opportunities for physical activity outside of school. Ten partnerships proposed working collaboratively with schools to implement joint-use agreements that would expand use of school and community facilities for physical activity. HKHC partnerships proposing to establish standards for physical activity in after-school settings were classified under the CDC recommendation to increase the amount of physical activity in physical education programs in schools.\n\n【28】**CDC category 5: safe communities that support physical activity**\n\n【29】Thirty-five partnerships proposed strategies to create safe communities that support physical activity, making these the most frequently adopted set of strategies . Improving access to outdoor recreation facilities and enhancing infrastructure to support walking and cycling were the most commonly proposed, followed by strategies to improve personal and traffic safety in areas where people could be physically active . Examples for improving access to outdoor recreation facilities included efforts to revitalize parks, enhance community awareness about such facilities, and improve their connectivity to residents. Safe Routes to School was mentioned by more than half of the partnerships working in this area. Sidewalks, bike lanes, improved connectivity between pedestrian and bike paths, traffic calming, and crossing aids were some ways partnerships proposed to improve infrastructure for promoting walking and cycling. To enhance personal and traffic safety, most efforts focused on increased police presence, street lighting, traffic calming patterns, and plans to design “complete” streets with all users in mind, including bicyclists, public transport vehicles and riders, and pedestrians.\n\n【30】### Additional common strategies proposed by HKHC communities\n\n【31】HKHC partnerships proposed additional strategies for preventing childhood obesity. Twenty-three partnerships proposed community gardens, making it 1 of the 2 most common community strategies (the other being to enhance infrastructure to support walking). Seven partnerships proposed creating local food policy councils. The focus of the call for proposals was on policy, systems, and environmental change approaches; health education, marketing, and other promotion strategies were acceptable as part of the overall plan, as long as other funding sources supported such efforts. Eleven partnerships proposed health promotion (eg, social marketing campaign) and education efforts (eg, educational videos, curriculum design).\n\n【32】### Which communities choose which strategies?\n\n【33】Of the 4 partnerships that proposed strategies to support healthy food and beverage choices, all were urban communities from priority states, 3 were in the lowest median household income group, 2 were large (population >250,000), and 2 were communities with low proportions of Hispanics and African Americans . Partnerships that identified strategies to encourage physical activity and to limit sedentary behavior in children were also primarily urban, with lower proportions of Hispanics and African Americans. No other patterns based on community characteristics emerged.\n\n【34】Discussion\n\n【35】The content analysis of 41 HKHC-funded proposals suggests that these community partnerships were already moving in several directions that CDC would recommend soon thereafter. Although RWJF was involved in both projects, the HKHC call for proposals was developed independently from the CDC recommendations. The way in which the HKHC proposals reflected the soon-to-be recommended strategies are therefore noteworthy and encouraging because it implies that the HKHC community partnerships perceive the CDC recommendations to be feasible. The communities could visualize and describe how they would implement such abstract concepts as “promote food access” to achieve their aims. In the absence of evidence for effectiveness, implementation decisions are often based on perceptions of impact, taking into consideration community assets and challenges .\n\n【36】Better access to food and recreation facilities and increased personal and traffic safety were common approaches among the 41 grantees. Their community partnerships could envision how to implement these environmental changes. Indeed, many examples of successful food-access efforts in other communities are now available . Moreover, cities, counties, municipalities, and neighborhoods often have the authority to affect the availability and location of healthy food and to support mechanisms promoting local foods. The same is true for improving access to facilities for physical activity, including cycling and walking, and for improving traffic and personal safety .\n\n【37】Some approaches, however, were not as common among proposals: influencing healthy food choices, limiting calorie-dense foods and beverages outside of school, and zoning for mixed-use development to encourage physical activity. Most examples of success in setting these standards and limits are from schools , and communities may perceive them as infeasible outside the school day. Additional technical assistance may be needed to translate these recommendations into practical, implementable activities.\n\n【38】HKHC partnerships proposed some strategies that CDC did not recommend. Foremost in that category are community gardens. Despite scant evidence for their role in improving nutrition  and in preventing childhood obesity, the popularity of community gardens in HKHC proposals likely results from the recognition that these gardens promote community involvement, neighborhood revitalization, and green and sustainable environments .\n\n【39】For the most part, the CDC-recommended strategies were applicable to the communities regardless of their size, demographic makeup, location, type of leadership, or age of their partnership. Community differences did appear, however, for the strategies to encourage physical activity and limit sedentary behavior. Urban communities and communities with the lowest proportions of African American and Hispanic populations were most likely to choose strategies to encourage physical activity and limit sedentary behavior. The greater probability of joint-use agreements between communities and schools in urban areas may be due to easier access to school and community facilities compared with access in communities with less dense populations. The lack of these strategies in proposals from predominantly African American and Hispanic communities is of concern. Environmental and policy change strategies show enormous potential, yet are often underused in populations with health disparities .\n\n【40】Our analysis has several limitations. The proposals were selected on the basis of criteria of community need (location, income levels, predominant race/ethnicity), and grants were awarded based on a selection committee’s joint assessment of merit. Therefore, they are not necessarily representative of community plans for preventing childhood obesity in the United States. Abstracting strategies from the proposal narratives was challenging because applicants presented their plans in multiple places in their proposals. Every effort was made to abstract information, but some proposed strategies may have been overlooked. Lastly, abstracting and matching proposed strategies with CDC recommendations was a subjective process. To improve the reliability of abstraction, 2 coders matched the strategies and, in case of discrepancies, reached consensus through discussion with another author.\n\n【41】A comparison of strategies proposed by the HKHC-funded partnerships and CDC’s recommendations for preventing obesity shows that the 2 overlap considerably. Areas for which CDC recommendations were more commonly proposed included providing incentives to food retailers to offer healthier options in underserved areas, increasing the number of farmers’ markets, improving access to outdoor recreation facilities, enhancing cycling and walking infrastructure, and enhancing personal and traffic safety. The communities likely consider these strategies feasible to implement, especially given examples in the field. On the other hand, few strategies were proposed by HKHC partnerships in areas where previous implementation had been limited to schools. Changing environments and policies relevant during the school day was not the focus of HKHC. With the exception of joint-use agreements being more popular in urban areas and in areas with lower African American and Hispanic populations, use of strategies did not differ by community demographic characteristics.\n\n【42】In addition to providing a snapshot of communities’ thinking on approaches for policy, systems, and environmental change strategies for preventing childhood obesity, our analysis provides an example of how information from communities, obtained through grant-writing efforts, can be used to assess the status of the field, guide future research, and provide direction for future investments.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4c45cb01-c093-47c6-9bd0-5a5df20d9fe5", "title": "Unique Strain of Crimean–Congo Hemorrhagic Fever Virus, Mali", "text": "【0】Unique Strain of Crimean–Congo Hemorrhagic Fever Virus, Mali\n**To the Editor:** Crimean-Congo hemorrhagic fever (CCHF) is an acute viral infection that causes mild to severe hemorrhagic fever characterized by petechiae, ecchymosis, disseminated intravascular coagulation, and multi-organ failure . The etiologic agent, CCHF virus (CCHFV; family _Bunyaviridae,_ genus _Nairovirus_ ), is maintained in enzootic cycles involving agricultural and wild animals and the vector, _Hyalomma_ ticks. CCHF predominantly affects persons who have 1) substantial contact with ticks and/or agricultural animals in areas where CCHF is endemic or 2) close contact with infected persons, predominantly close relatives and health care workers. The case-fatality rate for CCHF is generally accepted as 30% .\n\n【1】CCHF has a wide geographic distribution; cases have been reported in >30 countries across Africa, southeastern Europe, the Middle East, and western Asia. In the western African countries of Nigeria, Mauritania, and Senegal, serologic evidence of CCHFV infections in humans and agricultural animals has been documented frequently ; however, reports of the disease in humans have been limited to Senegal and Mauritania . In neighboring Mali, where the tick vector is known to be present, little information exists regarding the presence of CCHFV. Thus, to determine if the virus is circulating undetected in Mali, we conducted a study to determine if CCHFV is present in _Hyalomma_ ticks in the country.\n\n【2】In November 2011 and March 2012, unfed _Hyalomma_ ticks (adults and nymphs) were collected from 20 cattle at the Daral livestock market (12° 49.855′ N, 08° 05.651′ W) near the town of Kati, Mali, ≈25 km from the capital, Bamako. In the field, ticks were visually identified to genus and pooled accordingly (3–4 ticks per pool, all collected from the same animal). A total of 23 tick pools, representing 80 ticks, were manually homogenized, and RNA was extracted and tested for the presence of CCHFV RNA by using in-house assays that selected for 3 virus genes. Of the 23 tick pools tested, 1 was positive for CCHFV by all 3 assays. Phylogenetic analysis of the complete nucleocapsid protein gene (KF793333) showed that the CCHFV strain from Mali most closely resembled a strain from Mauritania , sharing 98% sequence identity .\n\n【3】Further analysis of fragments of the medium segment (pre-Gn coding region, KF793334) and large segment (polymerase coding region, KF793335) confirmed these findings, showing sequence identities of 91% and 98%, respectively, with ArD39554 . In a Biosafety Level 4 facility at Rocky Mountain Laboratories, Hamilton, Montana, USA, the original homogenates from the positive pool were passaged in multiple cell lines. After 3 passages, no discernible cytopathic effect was observed and, aside from the initial passage, CCHFV RNA was not detected.\n\n【4】Genetic identification of ticks in the CCHFV RNA–positive pool was conducted as described . Amplified sequences most closely resembled those of _H. dromedarii_ , (97.2%–100% sequence identity), although genetically, we cannot exclude the possibility that _H. truncatum_ and _H. rufipes_ were present with individual sequence identities of >97% to published sequences.\n\n【5】The Daral cattle market in Kati is the largest of its kind in Mali, and animals from across the country come into the market every week. Although the market provided a convenient opportunity for collecting ticks, we cannot determine where the infected ticks, and possibly cattle, contracted CCHFV because the animals traversed great distances on foot before arriving at the market. Nevertheless, this study demonstrates the presence of a distinct strain of CCHFV in _Hyalomma_ ticks in Mali, thereby expanding the geographic distribution of this virus in western Africa. Not surprisingly, the highest sequence identity for the CCHFV strain from Mali is to strains known to circulate in neighboring countries . We propose Daral 2012 Mali as the temporary designation for this sequence. Unfortunately, our attempts to isolate the virus were unsuccessful, most likely because of processing and storage conditions for homogenates used in these studies.\n\n【6】Species of _Hyalomma_ ticks are widely distributed across western Africa, and although reports of CCHF are limited to a few countries, CCHFV is most likely circulating undetected in vast areas of this region. No cases of CCHF have been reported in Mali; however, on the basis of our findings, the potential for human infections exists. Thus, CCHF should be considered in the differential diagnosis of febrile illnesses, with or without hemorrhagic symptoms, in residents of Mali and for persons with a recent history of travel to this country.\n\n【7】The ease of CCHFV transmission and the high case-fatality rate associated with infection could have a potentially substantial effect on public health. Future studies in Mali are required to define the geographic distribution of infected ticks and animals and to isolate CCHFV to help focus public health preparedness and countermeasures. In addition, across Mali, operational protocols should be reviewed for persons working at jobs in which the risk for CCHFV transmission is high (e.g. occupations with direct contact with agricultural animals and/or animal blood products), and appropriate countermeasures should be put in place to prevent transmission among such persons.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "42245839-36a4-4ac2-8755-38c12c9b4e96", "title": "Human Case of Lobomycosis", "text": "【0】Human Case of Lobomycosis\nIn February 2001, a healthy, 42-year-old, female geologist from Canada came to her dermatologist with a slowly growing, 1.5-cm diameter, dusky-red, nontender, plaque-like lesion surrounded by keloidal scar tissue on the posterior aspect of her right upper arm . It was located at the site of a scar from a previous excision attempt of a similar lesion 2 years earlier. The original lesion was first noticed while the patient was visiting Southeast Asia in 1996, although she did not seek medical attention until returning to Canada 1 year later. At that time, coccidioidomycosis was diagnosed based on her history of previous travel to a lobomycosis-endemic region and on the presence of oval, yeast-like organisms in histologic sections. However, _Coccidioides immitis_ was never cultured from the lesion, and serologic studies for this fungus were negative. After the excision in 1997, nothing was noted further until October 1999, when a small lesion, similar in color to the original one, reappeared under the scar and gradually increased in size. The patient had no other skin lesions and was otherwise asymptomatic.\n\n【1】The patient had spent time doing geologic work in various tropical regions over a 7-year period. In 1992–1993, she traveled to the Four Corners region of the United States as well as to California, northern Mexico, and Costa Rica. She lived and worked in the jungles of Guyana and Venezuela for 2 years (1993 and 1994), spending most of her time around the Cuyuni, Essequibo, and Rupununi River areas, although she also spent some time in the Bolivar state of Venezuela. Thereafter, she traveled to Kazakhstan, Indonesia (Irian Jaya), and the Philippines (1995 to 1996). During her time in South America, she lived mainly in rural camps and had extensive exposure to freshwater, soil, and underground caves. Health problems encountered during her travels included dengue fever, amebic dysentery, and intestinal helminthiasis. Of note, she had never traveled to the African continent.\n\n【2】Her medical history was otherwise unremarkable. Family history was positive for hypothyroidism. She was a nonsmoker and a social alcohol drinker. She was on hormone replacement therapy but no other medications and had no known allergies. Review of systems was unremarkable. Other than the lesion on her right arm, results of physical examination were normal.\n\n【3】Biopsied tissue specimens of the lesion were obtained and submitted for pathologic and microbiologic examination. The hematoxylin- and eosin-stained tissue sections showed a diffuse, superficial, and deep granulomatous dermatitis with multinucleated giant cells. Intracellular and extracellular unstained fungal cells with thick refractile walls were seen, giving a “sieve-like” pattern to the granulomatous inflammation. The fungal cells stained strongly with periodic acid-Schiff and Grocott methenamine silver stains ; cells were spherical or lemon-shaped, approximately 10 μm in diameter, and uniform in size. They were arranged as single cells or in short budding chains joined by narrow, tubelike bridges. Calcofluor white stain  of fresh tissue indicated fluorescent, spherical fungal organisms similarly arranged in chains. The organisms were not cultivatable. Fungal morphology was consistent with _Loboa loboi_ . The lesion was completely excised with no subsequent recurrence.\n\n【4】### Discussion\n\n【5】Lobomycosis is a chronic granulomatous infection of the skin and subcutaneous tissues caused by the fungus _L._ _loboi_ . It is characterized by the appearance of slowly developing (months, years, or decades), keloid-like, ulcerated, or verrucous nodular or plaque-like cutaneous lesions , usually at a site of local trauma such as from a cut, insect bite, animal bite, or ray sting . Lesions may be single or multiple and tend to occur on exposed, cooler areas of the body, particularly the extremities and ears . Other sites such as the forehead, face, chest, scapula, lumbosacral spine, buttocks, and scrotum have also been reportedly involved . Mucous membranes or internal organs are not involved . Lesions may be nonpigmented, hypopigmented, or hyperpigmented and are usually painless or only slightly pruritic . Little if any local host inflammatory response and no systemic symptoms exist . In some cases, lesions may spread contiguously or through lymphatic channels, causing cosmetic disfigurement .\n\n【6】On histologic examination, the lesions are composed of dermal granulomas with multinucleated giant cells filled with spherical or lemon-shaped fungal cells 6–12 μm in diameter with doubly refractile walls that are commonly arranged in chains of budding cells connected by thin, tube-like bridges . _Loboa loboi_ has never been cultivated in vitro , although it has been successfully transmitted to armadillos, tortoises, and the footpads of hamsters and mice under experimental conditions .\n\n【7】Natural disease has been described only in humans and in marine and freshwater dolphins . Lobomycosis was first described by the dermatologist Jorge Lobo in 1931 . His patient was a 52-year-old man who worked as a rubber collector in the Amazonas state of Brazil, who had numerous slowly developing nodular keloidal lesions in the lumbosacral spine area, from which microorganisms resembling _Paracoccidioides brasiliensis_ were observed on microscopy . Lobo suspected that the patient had a modified form of paracoccidioidomycosis, which he called keloidal blastomycosis . A second human case was reported in 1938, after which the disease was termed Lobo’s disease . More than 500 human cases have been reported to date , although the disease appears to be confined to remote tropical areas of South and Central America, especially in communities along rivers . The natural habitat of the fungus is not known but is believed to be aquatic or associated with soil and vegetation . Rubber workers, farmers, miners, fishermen, and hunters are particularly at risk due to extensive outdoor exposure . The condition has been described in Bolivia, Brazil, Colombia, Costa Rica, Ecuador, French Guiana, Guyana, Mexico, Panama, Peru, Suriname, and Venezuela .\n\n【8】The first report of nonhuman infection occurred in 1971 in an Atlantic bottle-nosed dolphin from Florida . In 1973, a dolphin with Lobo’s disease was described in Europe along the Atlantic coast of France and Spain . The dolphin’s caretaker, a resident of Holland, later acquired the disease, the first human case of lobomycosis reported outside of Latin America . Other cases of lobomycosis in dolphins have been confirmed, including one in a dolphin off the Texas coast .\n\n【9】Cases of imported human lobomycosis have been described in the United States and elsewhere . A case of lobomycosis involving the chest was recently described in an Atlanta, Georgia, man who had previously traveled to Venezuela and was exposed to extremely high water pressure while walking under Angel Falls , although the first published description of a human case in the United States appears to be in an immigrant from Suriname . Recently, cases of imported human lobomycosis were reported in France  and Germany . Other imported cases in industrialized countries are believed to occur but may be misdiagnosed due to physician unfamiliarity with the disease. As far as we are aware, our report is the first of human lobomycosis in Canada. The disease was presumably acquired in Guyana or Venezuela, because her visits to Mexico and Costa Rica were unlikely to put her at risk for infection.\n\n【10】Based on our patient’s history, the physical findings, and the histologic appearance of the skin lesion, the diagnosis of lobomycosis was unequivocally made. Although other fungi may resemble _L. loboi_ microscopically, including _P. brasiliensis_ , _Blastomyces dermatitidis_ , and _Histoplasma capsulatum_ var _dubiosii_ (the cause of African histoplasmosis), none of them form the characteristic chains of fungal cells of uniform size, 6- to 12-μm in diameter, connected by thin tubelike isthmuses, the hallmark of lobomycosis . Furthermore, unlike _L. loboi_ , the other fungi can be grown in vitro on routine mycologic media . In contrast to _L. loboi_ , the mother cell of _P. brasiliensis_ forms multiple buds and remains larger than the daughter cells, giving the characteristic “ship’s wheel” appearance . In addition, paracoccidioidomycosis is a disease of the oronasal mucous membranes and lungs . In contrast to _B. dermatididis_ , the fungal cells of _L. loboi_ do not form broad-based buds . Because the patient had never traveled to Africa, she was not at risk for African histoplasmosis. Other conditions that may clinically resemble lobomycosis include keloids, leprosy, leishmaniasis, chromoblastomycosis, and malignancy .\n\n【11】Lobomycosis does not usually affect the general health of a person. However, unless lesions are removed at an early stage, the disease persists for life . Rarely, squamous cell carcinoma may develop from lobomycotic lesions .\n\n【12】Successful treatment of lobomycosis usually consists of total surgical excision of the lesion, preferably with wide margins , although adjunctive medical therapy with clofazamine or other agents has sometimes been used in patients with extensive disease, with limited success . However, a German patient with lobomycosis had complete clinical and histopathologic resolution of disease after a 1-year treatment course of oral clofazamine and itraconazole . One dolphin with Lobo’s disease was successfully treated with miconzaole . Our patient has remained disease-free for more than 2 years after surgical excision of her lobomycotic lesion.\n\n【13】The nomenclature of the fungus has been subject to ongoing debate, although a new genus, _Lacazia_ , with _Lacazia loboi_ as the type species, was recently proposed by Toborda et al. and appears to be taxonomically valid. The definitive taxonomic status of this fungus awaits in vitro culture and subsequent molecular studies, although phylogenetic analysis of 18S small-subunit rDNA molecules indicates that _L. loboi_ is a distinct and novel species phylogenetically close to but fundamentally different from _P. brasiliensis_ .\n\n【14】In summary, lobomycosis is a slowly progressive, chronic, fungal infection of the dermis that is rarely seen in industrialized countries. This disease should be suspected in patients with single or multiple keloidal skin lesions, particularly if they have traveled to remote areas of Latin America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e81bb82a-0cff-4a55-8fdf-99a9f73636ae", "title": "Rickettsia felis Infections, New Zealand", "text": "【0】Rickettsia felis Infections, New Zealand\n**To the Editor:** Members of the genus _Rickettsia_ have garnered much attention worldwide in recent years with the emergence of newly recognized rickettsioses. In New Zealand, only _Rickettsia typhi_ and _R. felis,_ belonging to the typhus and spotted fever groups, respectively, have so far been found . _R. typhi_ , primarily transmitted by the oriental rat flea ( _Xenopsylla cheopis_ ), has a worldwide distribution and causes murine typhus in humans . At the end of 2009, a total of 47 cases of murine typhus had been recorded in New Zealand. In contrast, although the cat flea ( _Ctenocephalides felis_ ) can carry _R. felis_ in New Zealand , no human infections have been reported. However, because _R. felis_ shares a similar clinical profile to murine typhus, infection can be mistaken for a suspected case of _R. typhi_ .\n\n【1】Clinical suspicion of rickettsial infection is widely confirmed by serologic tests with the indirect immunofluorescence assay (IFA) being the standard test. However, antibodies against _R. felis_ in human sera are known to cross-react with _R. typhi_ in IFA . Western blot (WB) and cross-adsorption assays, in combination with IFA, can differentiate between several rickettsioses . We report on the trial in New Zealand of WB and cross-adsorption assays for differentiating retrospectively between past _R. typhi_ and _R. felis_ infections and evidence of _R. felis_ infection in persons living in the country.\n\n【2】Serum samples were obtained from 24 volunteers from the Institute of Environmental Science and Research Limited, Porirua, New Zealand. Samples were tested using _R. typhi_ IFA slides (Australian Rickettsial Reference Laboratory \\[ARRL\\], Geelong, Victoria, Australia). After incubation (37°C for 30 min), slides were washed 3 times, incubated with fluorescein-conjugated antihuman IgG, IgM, and IgA (ARRL), and washed again before examination. All samples were then tested by using an IgG IFA kit (Focus Diagnostics, Cypress, CA, USA) against typhus group (TG) _R. typhi_ and spotted fever group (SFG) _R. rickettsii_ .\n\n【3】TG-positive and SFG-negative serum samples may represent _R. typhi_ infections, and SFG-positive and TG-negative serum samples may represent _R. felis_ infections. Because _R. typhi_ can cross-react with SFG rickettsiae , and _R. felis_ with _R. typhi_ , results that are TG positive and SFG positive may be caused by either rickettsiae. Positive reactivity may also represent overseas-acquired rickettsioses. Thus, WB and cross-adsorption assays using _R. typhi_ (Wilmington) and _R. felis_ (URRWXCal2) antigens (Unité des Rickettsies, Marseilles, France) were used to confirm any _R. typhi_ or _R. felis_ infections .\n\n【4】Antigens (2 mg/mL) were solubilized (100°C for 10 min) in 2× Laemmli buffer  and subjected to electrophoresis (20 μg/well; 20 mA, 2.5 h) through polyacrylamide gels (12.5% resolving; 4% stacking) (BioRad, Hercules, CA, USA). Resolved antigens were electroblotted (100 V for 1 h) onto 0.45-μm polyvinylidene difluoride membranes, which were blocked by using Tris-buffered saline with 0.1% Tween 20. Each antigen lane was divided into 2 strips before incubation (room temperature for 1 h) with serum (diluted 1:200). After three 10-min washes with 5% milk–Tris-buffered saline with 0.1% Tween 20, strips were incubated (room temperature, 1 h) with horseradish peroxidase–conjugated antihuman IgG (1:150,000; SouthernBiotech, Birmingham, AL, USA) and washed again. Enhanced chemiluminescent detection of bound horseradish peroxidase (ECL Plus; GE Healthcare, Buckinghamshire, UK) enabled identification of reactive band sizes with Precision Plus standards (BioRad). Cross-adsorption was carried out by incubating serum diluted 1:30 in boiled antigen (37°C for 5.5 h, then 4°C overnight) before centrifugation (10,000 × _g_ for 10 min) . Supernatants were applied to WB strips and results compared with _R. typhi_ and _R. felis_ antisera.\n\n【5】Of the 24 serum samples, 3 (12.5%) were positive on the ARRL slides, and 11 (45.8%) showed IgG reactivity on Focus slides . Of these 11 serum samples, 8 (33.3%) were SFG positive and TG negative, and 3 (12.5%) were SFG and TG positive. Of the 12 serum samples that showed some IFA reactivity, after cross-adsorption, none had specific reactivity against _R. typhi_ , and 2 were confirmed as _R. felis_ . Both volunteers recorded risk factors associated with _R. felis_ infection. Six serum samples were indeterminate. Detectable antibodies remained after both cross-adsorptions, which may be caused by infection by _R. felis_ and _R. typhi_ , or other rickettsiae or cross-reactive pathogens .\n\n【6】Although IgG titers decline over time, detectable levels can remain for 4 years and thus exposure to _R. felis_ may have occurred any time during this period . Because both _R. felis_ –infected persons had traveled overseas within the past 4 years and _R. felis_ has a wide distribution , overseas exposure is possible. _R. felis_ is known to be prevalent in _C. felis_ fleas, including in New Zealand . This prevalence and the high rate of cat and dog ownership have public health implications and support the recognition of _R. felis_ as an emerging global health threat . Infection from _R. felis_ in addition to _R. typhi_ should be considered in the differential diagnosis of fever, headache, myalgia, and rash.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e3f8512b-dea4-4247-acf9-b05936b84119", "title": "Cross-Variant Neutralizing Serum Activity after SARS-CoV-2 Breakthrough Infections", "text": "【0】Cross-Variant Neutralizing Serum Activity after SARS-CoV-2 Breakthrough Infections\nThe B.1.1.529 (Omicron) variant of concern of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) carries a high number of nonsynonymous mutations in the spike glycoprotein, relative to that of the ancestral (wild-type) strain (Wu01). Those mutations result in a strong immune evasion phenotype, as demonstrated by severely reduced serum neutralization after vaccination or previous infection with ancestral variants in most persons , lower vaccine effectiveness, and increased rates of reinfection . However, booster vaccinations with 1 dose of mRNA vaccine after priming with an initial 2 doses induce high levels of serum neutralizing activity against Omicron . Substantial efforts have therefore been made to speed up booster vaccination campaigns in light of the rapid spread of Omicron and the recent surge of infections worldwide. Breakthrough infections after 2-dose mRNA vaccination can result in a natural boost to humoral immunity against SARS-CoV-2 , and emerging evidence suggests that breakthrough infections with non-Omicron SARS-CoV-2 variants also elicit cross-neutralizing serum activity against Omicron .\n\n【1】We determined serum neutralizing activity against the spike pseudotypes of SARS-CoV-2 Wu01 strain and 4 variants of concern (Alpha, Beta, Delta, Omicron \\[BA.1\\]) in 20 persons with non-Omicron (Alpha, Delta) SARS-CoV-2 infection after 2-dose mRNA vaccination with BNT162b2  or heterologous vaccination with ChAdOx1  and BNT162b2 . We compared serum neutralization activity for this cohort with that of 2 age-matched cohorts, 1 consisting of 20 persons who received 2 or 3 doses of mRNA vaccine  and did not experience breakthrough infection and another cohort of 10 persons who experienced Omicron breakthrough infection after 2-dose vaccination .\n\n【2】We detected significantly higher serum neutralizing activity against all investigated variants in serum from vaccinated persons with subsequent non-Omicron SARS-CoV-2 infection  than in serum from persons who received the regular 2 doses of vaccine and experienced no subsequent infection. The geometric mean 50% inhibitory serum dilution (ID 50  ) against Wu01 was 6.3-fold higher after breakthrough infection (640 \\[95% CI 409–1,003\\] vs. 4,056 \\[95% CI 2,174–7,568\\]). This difference in serum neutralizing activity was particularly pronounced against the Beta (23.5-fold higher ID 50  , 49 \\[95% CI 28–85\\] vs. 1,148 \\[95% CI 524–2,514\\]) and Omicron (23.8-fold higher ID 50  , 9 \\[95% CI 5–13\\] vs. 202 \\[95% CI 79–515\\]) variants, each of which exhibits substantial immune escape. The boosting effect of non-Omicron breakthrough infections was highly variable  because serum neutralizing titers (ID 50  ) showed a strong correlation with the interval between second vaccination and diagnosis of breakthrough infection (Omicron, Spearman ρ = 0.8299, p<0.0001; Wu01, ρ = 0.7048, p = 0.0005) . Breakthrough infections acquired >3 months after the second vaccination resulted in serum neutralizing capacity against both Wu01 and Omicron, which was comparable to that after 3-dose vaccination. This effect was observed after both non-Omicron and Omicron breakthrough infections . Similarly, neutralizing capacity against the Delta variant was increased after Omicron breakthrough infections . Limitations of this study include limited sample size and application of a pseudovirus-based neutralization assay.\n\n【3】In summary, we found that Omicron and non-Omicron SARS-CoV-2 breakthrough infections elicit cross-variant neutralizing antibodies. Our results suggest that short vaccination-to-infection intervals correlate with lower neutralizing titers, which may be relevant for recommendations concerning additional booster vaccination of persons who experience early breakthrough infections after initial immunization.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6faf472f-a0a2-4ec8-86e9-2773822927db", "title": "Who’s in Charge? Leadership during Epidemics, Bioterror Attacks, and Other Public Health Crises", "text": "【0】Who’s in Charge? Leadership during Epidemics, Bioterror Attacks, and Other Public Health Crises\nDr Laura Kahn has produced a useful book that provides a brief historical background on public health and terrorism, followed by interesting examples of leadership during outbreaks and events that escalated to public health crises. The roles of astute clinicians, public health professionals, appointed public health leaders, and elected officials are described by the players themselves. These insights provide important perspectives and are fascinating reading, but each event includes the voices of only a few of many participants. This omission may leave the reader hungry for a wider variety of viewpoints.\n\n【1】Kahn takes the reader through a thought-provoking overview of the complexity of leadership and some early milestones in public health. Kahn makes it clear that politics, economics, communications, and interpersonal relations are as central to today’s public health crises as they were in the past.\n\n【2】Persuasive examples support Kahn’s main thesis that political leadership is critical during a public health crisis, whether the crisis results from natural causes or from bioterrorism. Kahn says, “Questions about leaders and leadership have intrigued scholars in both Western and Eastern civilizations for centuries. Plato, Confucius, and Machiavelli all speculated about leaders… and the qualities of leadership.” Kahn concludes that 1) informed, engaged, and prepared elected officials are essential to effective response; 2) because crisis response decisions inevitably will be made in the absence of perfect information, leaders require judgment and common sense; 3) elected and appointed leaders must be effective; and 4) dual leadership during a crisis can cause confusion.\n\n【3】The author provides a convincing case for her conclusions with lively examples and first-hand accounts and offers several concrete suggestions to prepare elected officials for leadership roles. The same compelling case is not made for Kahn’s assertion of a “legal conundrum when dealing with the bioterrorism attack.” She suggests that the Centers for Disease Control and Prevention (CDC) should lead the public health response to such episodes but alleges that legal and organizational impediments hinder CDC from fulfilling that lead role.\n\n【4】Unquestionably, CDC must and does play a lead role during large-scale, multistate public health events. The legal and organizational impediments to fulfilling that role are not obvious to this reviewer, especially given CDC’s success in addressing many such crises. Kahn may be referring to impediments within the federal structure and chain of command. However, current law specifies the roles of CDC and the departments of Health and Human Services, and Homeland Security. CDC has ample legal authority to supplement its technical and scientific leadership during an emergency, especially when state and local capacities are outstripped.\n\n【5】Kahn suggests federalizing and centralizing the national response system through changes in the legal framework and organizational structures of the public health system, arguing that if CDC were organized for response as the Environmental Protection Agency or the Federal Bureau of Investigation is, delays, leadership confusion, and communication issues would be resolved. She identifies some leadership problems but fails to acknowledge the strong collegial relationship between state public health authorities and CDC that has produced innumerable successful responses to crises. Restructuring the traditional relationship between states and the federal government seems unnecessary.\n\n【6】This problem does not overshadow an otherwise informative and engrossing book. In an era of emerging infectious diseases, bioterrorism, and large-scale natural disasters, we will continue to have to address the types of events Dr. Kahn describes. Those involved in responding to such events would benefit from studying the lessons of the past to better manage future emergencies.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e88af932-d0be-4851-8b3e-168c68368422", "title": "Eugène Delacroix (1798-1863). \"Arab Horses Fighting in a Stable.\" 1860.", "text": "【0】Eugène Delacroix (1798-1863). \"Arab Horses Fighting in a Stable.\" 1860.\nEugène Delacroix . \"Arab Horses Fighting in a Stable.\" 1860. Oil on canvas. Photo: Gerar Blot. Copyright Réunion des Musées Nationaux/Art Resource, NY Louvre, Paris, France\n\n【1】From his early years, Delacroix, like his contemporary Théodore Gericault, was attracted to the savagery of wild animals. In a note written in Morocco, Delacroix mentions a scene of fighting horses. Among the precedents for this kind of wild-animal imagery was the antique group \"Lion Attacking a Horse\" (Rome, Mus. Conserv.), which was said to have been particularly admired by Michelangelo and which was copied in stone by Peter Scheemakers (1740; Rousham Park, Oxon). George Stubbs used the wild-animal theme within naturalistic settings in several paintings, for example, in \"Horse Attacked by a Lion,\" (1770; London, Tate), of which Gericault made at least one copy (1820/21; Paris, Louvre).\n\n【2】In \"Arab Horses Fighting in a Stable,\" his version of the subject, Delacroix was able to synthesize the classical with the exotic, with his studies of ecorché (French for flayed bodies), with the example of English art, and with the work of Rubens—Delacroix owned Pieter Claesz Soutman's engravings of Rubens' paintings of hunts. In 1847, Delacroix described two of these engravings in detail, indicating how highly he valued the elements of movement, variety, and unity. Of Delacroix' three great lion hunts, three have survived, a fragment (1855, now in Bordeaux, France) and two complete paintings: one from 1858 (now in Boston, Massachusetts) and one from 1861 (now in Chicago, Illinois); the latter is the most spacious and free in its handling of circular, dancelike movements that suggest a perpetual struggle—one of the underlying themes in which form and content are inseparable.\n\n【3】From The Dictionary of Art, Macmillan, NY, NY, 1996.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "713c69a8-4164-4bdf-9d7d-8fb083f5eaf5", "title": "Enzootic Angiostrongyliasis, Guangdong, China, 2008–2009", "text": "【0】Enzootic Angiostrongyliasis, Guangdong, China, 2008–2009\n**To the Editor:** The nematode _Angiostrongylus cantonensis_ was discovered in pulmonary arteries and hearts of domestic rats in Guangzhou (Canton), China, by Chen in 1935 . This parasite has a complex life cycle  and causes cerebral angiostrongyliasis after ingestion of infective larvae found in freshwater and terrestrial snails and slugs, paratenic hosts (such as freshwater fish, shrimp, frogs, and crabs), and contaminated vegetables .\n\n【1】During 2000–2006, a total of 7 outbreaks of angiostrongyliasis were reported in the People’s Republic of China, including an outbreak in Zhaoqing, Guangdong Province . We conducted a survey of _A. cantonensis_ nematodes in mollusks and rodents in Qingyuan, Guangong Province, during August 2008–October 2009.\n\n【2】Qingyuan is located in northern Guangdong Province (23°31′–25°12′N, 111°55′–113°55′E). It is the largest city in the province. Qingyuan borders Zhaoqing on the west and Guangzhou on the south. Its climate is subtropical monsoon, and it has an average annual temperature of 20.7°C. The city has an area of 19,152.89 km 2  and a population of 3.87 million. Nematode hosts were obtained in 3 counties in Qingyuan: Qingxin, Fogang, and Lianzhou.\n\n【3】During August 2008–October 2009, we captured 288 rats of 7 species (257 _Rattus norvegicus_ , 13 _R_ . _flavipectus_ , 7 _R_ . _losea_ ; 6 _R_ . _rattus_ , 3 _Bandicota_ _indica_ , 1 _R_ . _rattus alexandrinus_ , and 1 _Mus musculus_ ). Rats were examined for adult _A. cantonensis_ nematodes in pulmonary arteries and right heart cavities.\n\n【4】Among the 288 rats examined, 27 (9.4%) from 3 species were infected with _A_ . _cantonensis_ adults in their cardiopulmonary systems . Infected rodents were found in all 3 counties. The 27 infected rats were 25 _R_ . _norvegicus_ , 1 _R_ . _losea_ , and 1 _M_ . _musculus_ . _R_ . _norvegicus_ rats were most frequently captured in the 3 counties, and this rodent had the highest prevalence of infection. Infected _B_ . _indica_ rats in Lianzhou and _M_ . _musculus_ rats in Qingxin were also found, but the total numbers of infected animals and the prevalences are lower than that for _R_ . _norvegicus_ rats. On the basis of these findings, we conclude that _R_ . _norvegicus_ rats are the major definitive host for _A_ . _cantonensis_ nematodes in Qingyuan.\n\n【5】Specimens from 510 snails (144 _Pomacea canaliculata_ , 306 _Achatina fulica_ , and 60 _Bradybaena despecta_ ) were digested with pepsin for isolation of _A_ . _cantonensis_ larvae . Metastrongylid larvae were found in 21 (4.1%) of 510 examined snails. Prevalence rates of _A_ . _cantonensis_ in _P_ . _canaliculata_ , _A_ . _fulica_ , _and B_ . _despecta_ were 8.3%, 2.0% and 5.0%, respectively. Differences between the 3 prevalence rates were significant (χ 2  9.604, p<0.05). Prevalence rates in the 3 counties are shown in the Table.\n\n【6】All 3 species of infected snails were found in Qingxin and Fogan Counties. _P_ . _canaliculata_ and _B_ . _despecta_ snails were found infected in Qingxin County. However, only _A_ . _fulica_ snails were found infected in Fogang. These findings are similar to those of studies conducted in Guangdong Province .\n\n【7】Distributions of snail species among the 3 sites differed. Although all 3 species were found in Qingxin and Fogang Counties, only _A_ . _fulica_ snails were found in Lianzhou County. Lower temperatures in this county may contribute to this uneven distribution. Our failure to detect infected snail hosts in Lianzhou County was unexpected, and further surveys are needed to identify parasite hosts in this area. Our findings suggest that the 3 species may play a major role as intermediate hosts for _A_ . _cantonensis_ nematodes in human infections.\n\n【8】Qingyuan is a natural focus for _A_ . _cantonensis_ nematodes. Residents in the study area frequently eat raw or undercooked snails and slugs, unaware that these animals may contain infective larvae of _A_ . _cantonensis_ that can cause eosinophilic meningitis. Therefore, to protect local residents from parasite infections, inhabitants of this region must be given relevant information about _A_ . _cantonensis_ nematodes. Control measures to control spread of this parasite must also be implemented.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "133eeb09-57dc-4b16-a4ae-856bba573d58", "title": "Plasmodium cynomolgi Co-infections among Symptomatic Malaria Patients, Thailand", "text": "【0】Plasmodium cynomolgi Co-infections among Symptomatic Malaria Patients, Thailand\n_Plasmodium cynomolgi_ , a simian malaria parasite, possesses biological and genetic characteristics akin to those of the most widespread human malaria parasite, _P. vivax_ . Although _P. cynomolgi_ circulates among monkey species such as long-tailed macaques ( _Macaca fascicularis_ ) and pig-tailed macaques ( _M. nemestrina_ ), experimental and accidental transmissions have been implicated in symptomatic infections in humans . Several mosquito vectors for human malaria can also transmit _P. cynomolgi_ , posing the risk of cross-species transmission in areas where its natural hosts coexist with people . Among pig-tailed and long-tailed macaques living in various countries in Southeast Asia, including Thailand, _P. cynomolgi_ infections are not uncommon . A case of naturally transmitted _P. cynomolgi_ malaria in a human was reported from eastern Malaysia . Subsequent surveillance in western Cambodia and northern Sabah state in Malaysia revealed asymptomatic human infection, albeit at low prevalence . Symptomatic _P. cynomolgi_ infection was diagnosed in a traveler returning to Denmark from Southeast Asia . During testing of symptomatic malaria patients in Thailand, we identified 9 co-infected with cryptic _P. cynomolgi_ and other _Plasmodium_ species.\n\n【1】### The Study\n\n【2】We examined 1,359 blood samples taken from febrile patients who sought treatment at malaria clinics or local hospitals in 5 Thailand provinces: Tak (n = 192, during 2007–2013), Ubon Ratchathani (n = 239, during 2014–2016), Chanthaburi (n = 144, during 2009), Yala (n = 592, during 2008–2018), and Narathiwat (n = 192, during 2008–2010). Using microscopy, we found 1,152 cases in which malaria was caused by _P. vivax_ (869 patients, 75.43%), _P. falciparum_ (272 patients, 23.61%), or co-infection with both species (11 patients, 0.96%). Using species-specific nested PCR, including for _P. cynomolgi_ , targeting the mitochondrial cytochrome _b_ gene ( _mtCytb_ ) of 5 human malaria species for molecular detection, as described elsewhere , we found malaria in 1,180 patients; _P. vivax_ infections exceeded _P. falciparum_ infections . Submicroscopic parasitemia occurred in 28/1,180 (2.4%) patients: 19 infected with _P. vivax_ , 7 with _P. falciparum_ , 1 with _P. vivax_ and _P. falciparum_ , and 1 with _P. malariae_ .\n\n【3】The mean age of all patients was 26.3 (range 7–85) years; 940/1,180 (79.7%) of patients were men. Febrile symptoms, lasting 1–7 days (mean 3.1, SD ±1.3 days) before blood sample collection, developed in all PCR-positive malaria patients. Monoinfection with _P. knowlesi_ occurred in 4 patients, _P. malariae_ in 3, and _P. ovale_ in 1. We detected co-infections in 77 (0.93%) patients; of these co-infections, 55 were _P. falciparum_ and _P. vivax_ . In total (i.e. including both monoinfections and co-infections), _P. knowlesi_ was detected in 18 patients, of which 10 cases were newly identified from Ubon Ratchathani Province, which borders Cambodia and Laos.\n\n【4】We detected _P. cynomolgi_ in 9 patients, all of whom were co-infected with _P. vivax_ (n = 7), _P. falciparum_ (n = 1), or both _P. vivax_ and _P. knowlesi_ (n = 1). The overall prevalence of _P. cynomolgi_ infections was 0.76%. Patients infected with _P. cynomolgi_ were found in all provinces. Although 5 of these patients were from Yala Province, the proportion of _P. cynomolgi_ infections among malaria cases in each malaria-endemic area (0.52%–0.87%) was comparable.\n\n【5】DNA from 10 _P. knowlesi_ isolates from Ubon Ratchathani Province and the 9 _P. cynomolgi_ isolates were subject to nested PCR amplification spanning a 1,318-bp region of mitochondrially encoded cytochrome c oxidase I ( _mtCOX1_ ). Direct sequencing of the purified PCR-amplified template was successfully performed from all 10 _P. knowlesi_ and from 6 _P. cynomolgi_ isolates. The remaining 3 _P. cynomolgi_ isolates could not be further amplified due to inadequate DNA in the samples. All _mtCOX1_ sequences of _P. knowlesi_ from Ubon Ratchathani Province were different from one another and distinct from those from the previous case of natural human infection in Thailand  . All 6 amplified _P. cynomolgi_ isolates contained different sequences belonging to 2 clades. One was closely related to the Gombak strain  and the remaining 5 isolates were clustered with the RO strain  .\n\n【6】All but 1 _P. cynomolgi_ infection occurred in male patients (age 15–53 years, median 32 years). Most _P. cynomolgi_ malaria patients resided in areas where domesticated or wild macaques were living in proximity to humans. Infections with _P. cynomolgi_ occurred in different annual periods; more cases were detected in rainy seasons than in dry seasons . The parasite density of _P. cynomolgi_ could not be determined from blood smears because of morphologic resemblance to _P. vivax_ ; an isolate co-infected with _P. falciparum_ (YL3634) had very low parasitemia. Of 8 patients with _P. cynomolgi_ co-infection, 6 had parasitemia <10,000 parasites/μL (<0.2% parasitemia). It remains unknown whether _P. cynomolgi_ was co-responsible for symptomatic infections or merely coexisted asymptomatically with other human malaria parasites. However, self-reported defervescence among _P. cynomolgi_ –co-infected patients occurred 1–3 days after antimalarial treatment with chloroquine plus primaquine after onsite microscopic diagnosis of _P. vivax_ malaria or artesunate plus mefloquine for _P. falciparum_ malaria. Unfortunately, data on long-term follow-up were not available.\n\n【7】### Conclusions\n\n【8】This report highlights the presence of _P. cynomolgi_ in the human population of Thailand, where natural hosts, both pig-tailed and long-tailed macaques, are prevalent. All patients with _P. cynomolgi_ infections harbored either _P. falciparum_ or _P. vivax_ in their blood, implying that this simian malaria species could share the same anopheline vectors or have different vectors with similar anthropophilic and zoophilic tendencies. The presence of _P. cynomolgi_ in diverse malaria-endemic areas of Thailand suggests that cross-species transmission has occurred. Human infection with _P. cynomolgi_ seems not to be newly emerging because it was detected among blood samples collected over a range of time periods since 2007. Undoubtedly, morphologic similarity between _P. cynomolgi_ and _P. vivax_ can hamper conventional microscopic diagnosis . Cryptic co-existence of simian and human malaria species could further preclude accurate molecular detection when inadequate diagnostic devices are used.\n\n【9】Previous surveys of _Plasmodium_ infections in pig-tailed and long-tailed macaques have revealed the presence of _P. cynomolgi_ and other simian malaria species in Thailand, mainly in the southern part of the country . Most patients infected with _P. cynomolgi_ resided in areas where macaques were living in proximity to humans; therefore, the risk of acquiring malaria from this parasite could increase as people encroach into the habitats of infected macaques, as happened with malaria caused by _P. knowlesi_ . Of note, co-infection with _P. cynomolgi_ , _P. knowlesi_ , and _P. vivax_ occurred in a patient in Yala Province whose housing area was surrounded by several domesticated pig-tailed and long-tailed macaques.\n\n【10】Analysis of the _mtCOX1_ sequences of _P. cynomolgi_ among 6 patients showed that all isolates possessed different genetic sequences, suggesting that several strains or clones of this simian parasite are capable of cross-transmission from macaques to humans. Meanwhile, _P. cynomolgi_ seems to contain 2 divergent lineages , represented by RO and Gombak strains. The _mtCOX1_ sequences of both _P. cynomolgi_ lineages were found in human-derived isolates in this study, further supporting that diverse strains of this parasite can infect people. Likewise, sequence diversity in the _mtCOX1_ of _P. knowlesi_ from Ubon Ratchathani Province suggests that cross-transmission from macaques to humans may not be restricted to particular parasite strains.\n\n【11】Although human malaria from either parasite may be asymptomatic, infection with _P. knowlesi_ can result in death, but patients infected with _P. cynomolgi_ at worst had only benign symptoms . However, severe and complicated malaria has been observed in rhesus macaques experimentally infected with _P. cynomolgi_ .\n\n【12】Whether severe cynomolgi malaria can occur in humans remains to be elucidated. However, if human infections with _P. cynomolgi_ do become public health problems, diagnostic and control measures might be complicated by the morphological similarity between _P. vivax_ and _P. cynomolgi_ . This possibility makes further surveillance of this simian malaria in humans mandatory.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8fd8aee2-b199-41a7-b0dc-408c6da5b10b", "title": "School Health Index", "text": "【0】#### An online self-assessment and planning tool for schools\n\n【1】The _School Health Index (SHI) Self-Assessment and Planning Guide_ is an online self-evaluation and planning tool for schools. The SHI is built on CDC’s research-based guidelines for school health programs that identify the policies and practices most likely to be effective in reducing youth health risk behaviors. The SHI is easy to use and is completely confidential.\n\n【2】The SHI (and related materials) is available as an interactive, customizable online tool or downloadable, printable version. The SHI aligns with the Whole School, Whole Community, Whole Child (WSCC) model.\n\n【3】Start Here\n\n【4】How Schools Can Use the School Health Index\n\n【5】CDC developed the SHI in partnership with school administrators and staff, school health experts, parents, and national nongovernmental health and education agencies to:\n\n【6】*   Enable schools to identify strengths and weaknesses of health and safety policies and programs.\n*   Enable schools to develop an action plan for improving student health that can be incorporated into the School Improvement Plan.\n*   Engage teachers, parents, students, and the community in promoting health-enhancing behaviors and better health.\n\n【7】Download the School Health Index\n\n【8】*   SHI for Elementary Schools pdf icon \\[PDF – 2.3 MB\\]\n*   SHI for Middle and High Schools pdf icon \\[PDF – 3.2 MB\\]\n\n【9】Your Guide to Using the School Health Index\n\n【10】Your Guide to Using the School Health Index (SHI) pdf icon \\[PDF – 13 MB\\] includes information and resources for district and school staff who are familiar with the SHI and who are charged with completing the assessment. Included in this Guide are information, materials, and resources on how to implement the SHI in schools, as well as a Facilitating Groups section for conducting trainings, workshops, or presentations on the SHI.\n\n【11】School Health Index e-Learning Course\n\n【12】This course introduces you to CDC’s _School Health Index: Self-Assessment and Planning Guide_ . After this course , you will be ready to conduct or participate in a self-assessment and create a plan to improve the health of students in your school or district.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9df7e4cb-048f-49b2-a94c-fa48eeb7eea3", "title": "Early Outcomes of State Public Health Actions’ School Nutrition Strategies", "text": "【0】Early Outcomes of State Public Health Actions’ School Nutrition Strategies\nAbstract\n\n【1】**Introduction**\n\n【2】Since 2013, the State Public Health Actions to Prevent and Control Diabetes, Heart Disease, Obesity and Associated Risk Factors and Promote School Health (State Public Health Actions) program has been implemented to support and reinforce healthy choices and healthy behaviors among the US population. The Centers for Disease Control and Prevention’s Division of Population Health’s School Health Branch has been a critical component, ensuring that state health departments support schools in adopting nutrition standards and creating a supportive nutrition environment. The objective of this article was to describe early outcomes of the school nutrition strategies of State Public Health Actions.\n\n【3】**Methods**\n\n【4】We examined the extent of progress for short-term performance measures and for school nutrition evaluation questions, using data secured from 51 grantees through the performance measures database and state evaluation reports.\n\n【5】**Results**\n\n【6】During the first 4 years of the cooperative agreement, grantees demonstrated significant progress compared with year 2 for school nutrition performance measures. Collectively, grantees provided professional development and technical assistance to staff in 7,672 local education agencies and reached more than 29 million students. Success was also noted for several nutrition practices in schools.\n\n【7】**Conclusion**\n\n【8】These early outcomes suggest that State Public Health Actions has had a positive impact on the nutrition environment of US schools. Systematically addressing areas for improvement could further expand the reach of these efforts during the remainder of the cooperative agreement.\n\n【9】Introduction\n\n【10】State Public Health Actions to Prevent and Control Diabetes, Heart Disease, Obesity and Associated Risk Factors and Promote School Health (State Public Health Actions) is a 5-year cooperative agreement that supports state health departments in the promotion of healthy choices and healthy behaviors among the US population . The Centers for Disease Control and Prevention’s (CDC’s) Division of Population Health, School Health Branch seeks to prevent childhood obesity through healthy eating and physical activity policies and practices and through supporting students with chronic health conditions.\n\n【11】More than one-third of children and adolescents in the United States are overweight or obese . Because many students consume up to half of their daily calories at school , the school nutrition environment can affect children’s diet and overall health. Schools are a priority setting for addressing healthy eating behaviors through access to healthy foods and beverages and obtaining consistent information about healthy eating . The school nutrition environment includes foods and beverages available through multiple venues (eg, school meal programs, vending machines, celebrations, fundraisers), messages about good nutrition that students see and hear, and opportunities for students to learn about and practice healthy eating behaviors . Although recent policy changes at the federal, state, and local levels have improved the school nutrition environment, continued progress on policy implementation is needed .\n\n【12】This article describes the early outcomes of the school nutrition strategies, specifically changes in school-level nutrition practices, of State Public Health Actions.\n\n【13】Methods\n\n【14】In June 2013, all 50 states and the District of Columbia were awarded funding under either the basic or enhanced component of State Public Health Actions . The basic component awarded funding to all states to support health promotion, epidemiology, surveillance activities, and targeted strategies. The competitive enhanced component awarded additional funding to 32 states to build on and extend the activities supported with basic funding to achieve greater reach and impact. In this study, we examined data from grantees that reported use of these funds to adopt nutrition standards and create supportive nutrition environments in schools. The required strategies focused on interventions that included providing professional development and technical assistance; establishing standards (including for sodium) for all competitive foods (ie, foods and beverages sold outside of the school meal programs); prohibiting advertising of unhealthy foods; and promoting healthy foods in schools, including those sold and served in school meal programs and other venues. Grantees are required to report component-specific performance measures annually and provide data on year 5 targets; states that receive enhanced funding are required to report outcomes. In addition, grantees that receive enhanced funding are expected to work with up to 15 local education agencies (LEAs) (ie, school districts). States identified LEAs, which represent a subset of all the LEAs in a state, on the basis of their own criteria; however, they were encouraged to select LEAs in areas disproportionately affected by chronic diseases and the risk factors that cause them, including those with students who have a high prevalence of overweight or obesity, have limited access to healthy foods and beverages, and do not obtain adequate physical activity. The analysis and findings presented in this article are a subset of a national evaluation , and approval was secured from the Office of Management and Budget .\n\n【15】We conducted a document review of annual progress reports and analyzed quantitative performance measure data using a mixed-methods design. These data represented performance from July 1, 2013, through January 31, 2017. Data were obtained from 51 grantees that reported on basic performance measures; 32 of those grantees also reported on enhanced performance measures for enhanced states. School Health Profiles (Profiles) was the primary data source for all school-level performance measures. Profiles is a system of surveys that assess school health policies and practices among secondary schools in states, territories, tribal jurisdictions, and large urban school districts . Profiles uses random, systematic, equal-probability samples to produce data representative of schools with 1 or more of grades 6 through 12 in each site. Data were collected in spring 2014 and 2016. Profiles data were collected in the targeted LEAs identified in each of the 32 enhanced states. The primary data source for the policy-related performance measures (ie, 2.3.03 and 2.3.05) was WellSAT 2.0, an interactive tool used to assess the quality of written local school-wellness policies . Grantees also developed their own data collection systems to track professional development and technical assistance. Only data from years 2 and year 4 were included for all measures.\n\n【16】In addition, we collected and analyzed year 3 state evaluation reports. These evaluation reports represented data collection from August 1, 2015, through July 31, 2016. Grantees submit annual evaluation reports that provide detailed findings derived from their state-level evaluation of interventions and strategies. All grantees submitted evaluation reports, of which 29 of the 32 enhanced grantees evaluated the implementation of the enhanced school nutrition strategy. Evaluation reports addressed core questions corresponding to level of implementation. Grantees self-identified as being in either the adoption or implementation phase for the school nutrition strategy. Grantees at the adoption stage reported findings in response to the following core questions: 1) “What state activities have been effective in promoting nutrition policy development and nutrition practice adoption among districts and schools?” and 2) “What are the major facilitators and barriers in helping districts and schools create a supportive nutrition environment, such as partnerships with the Department of Education? How were the barriers overcome?” Grantees at the implementation stage reported findings related to the following core questions: 1) “What critical factors or activities influence the successful implementation of nutrition policy and nutrition practice?” and 2) “To what extent has implementation of nutrition policies and nutrition practices increased access to healthier foods and beverages at school?” In their evaluation report, grantees provided summaries of the approach and strategies of the intervention, the facilitators and barriers to implementation of the strategies, the evaluation indicators, and the evaluation findings.\n\n【17】For analysis of the annual progress reports, year 2 and year 4 data for eligible states were aggregated. Reach estimates were calculated on the basis of the performance measure type (ie, count, numerator and denominator, and percentage). Measures reported as counts were summed as totals across eligible states. Measures reported as percentages were reported as means across eligible states. In addition, for each performance measure, the percentage change in the actual values reported from year 2 to year 4 were calculated. Next, to determine whether the percentage change for each measure was significant ( _P_ < .05), the average percentage change (across all grantees) was compared with zero by using one-sample _t_ tests. Achievements of basic component short-term measures are also described.\n\n【18】Primarily qualitative data were obtained from evaluation reports. We identified and summarized the key school nutrition evaluation findings for year 3. Thematic analysis was conducted to identify and classify patterns or themes across the evaluation reports in terms of the types of changes in the nutrition environment, facilitators, barriers, lessons learned, and recommendations reported. The analysis involved inductively coding the data, deductively identifying themes, visualizing the relationship between codes and themes, sorting and sifting themes to identify interrelationships, and constructing memoranda that summarized the themes. This method of analysis incorporated both the data-driven inductive approach of Boyatzis  and the deductive a priori template of codes approach outlined by Crabtree and Miller .\n\n【19】Results\n\n【20】Data from the program’s fourth year of implementation demonstrated reach of professional development and technical assistance, progress on the implementation of various evidence-based practices, and identification of factors related to effective nutrition policy and practice. Data were provided by 51 grantees (all 50 states and the District of Columbia).\n\n【21】### Performance measures\n\n【22】Data from the basic component show that 7,672 LEAs received professional development and technical assistance on strategies to promote the adoption of nutrition standards, affecting 29.3 million students in these LEAs  and representing a significant increase of 3,771 LEAs from year 2. Among the 32 enhanced grantees with targeted work in up to 15 LEAs each, 609 LEAs received professional development on creating a supportive nutrition environment in schools, affecting approximately 3.7 million students at year 4. The increase between year 2 and year 4 was significant.\n\n【23】In the areas of nutrition policy and school nutrition practices among the 32 enhanced grantees, 59.5% of LEAs reported adopting and implementing policies to establish standards (including for sodium) for all competitive foods available during the school day, compared with 50.4% in year 2 . Additionally, 58.5% of schools reported they do not sell less-healthy foods and beverages, a significant increase from year 2. In year 4 only 33.5% of LEAs reported adopting and implementing policies that prohibit all forms of advertising and promotion, while 56.9% of schools indicated they prohibited all forms of advertising and promotion for candy, fast food restaurants, or soft drinks. A little more than 13% of schools also reported they priced nutritious foods and beverages at a lower cost, and almost half of schools provided nutrition information to students or families, a significant increase from year 2. Eighty percent of schools increased accessibility of healthy options by placing fruits and vegetables near the cafeteria cashier. The percentage of schools that allow students to have access to drinking water decreased slightly, from 58.6% in year 2 to 57.9% in year 4 . Nearly one-third of schools offered fruits and nonfried vegetables when foods are offered at school celebrations, and 14.4% of schools allowed students to purchase fruits and vegetables from vending machines, or at the school store, canteen, snack bar, or as à la carte items, a decrease from 19.9% in year 2 .\n\n【24】### State evaluation\n\n【25】Fifty-one state evaluation reports containing results of interventions designed to create supportive nutrition environments were analyzed. Fifty-seven percent of grantees evaluated the strategy focused on creating healthy school nutrition environments (12 in the adoption phase and 17 in the implementation phase). More than half of the 29 grantees evaluating this strategy reported the facilitators and barriers to creating supportive nutrition environments. Among them, 67% identified strong leadership, committed staff, formal partnerships, and engaging champions for school nutrition as key facilitators. The availability of professional development and technical assistance to support staff in the development, implementation, and evaluation of school nutrition interventions also was beneficial. Collectively they provided more than 780 professional development and technical assistance opportunities; 11 grantees reached more than 4,200 state, district, and school administrators; food service staff; and teachers. Other facilitators included communicating about school nutrition, offering incentives, and disseminating resources. Forty-one percent of grantees also found these facilitators influential in the successful implementation of nutrition policy and practice. Sixty percent of grantees reported barriers to creating a supportive nutrition environment, including inadequate capacity (eg, lack of appropriate facilities, staff), negative attitudes, perception of changing traditions (ie, snacks for celebrations), and lack of buy-in and support.\n\n【26】Grantees also described many activities that contributed to the perceived increase in access to healthier foods and beverages. For 75% of grantees, the implementation of nutrition practices and policies to increase access to healthier foods and beverages in schools was largely accomplished by changing and adopting district-level nutrition policies and practices for competitive foods, healthy school celebrations, events, or fundraisers; food and beverage procurement; and meal preparation and service. Many of these policy changes were enacted to better align with state and national nutrition standards such as the new Smart Snack regulations, but some also aimed to improve the consistency and quality of how food is presented to students, availability of information about menu nutrition contents for parents and students, and access to healthier food options (ie, in vending machines and during meal service). Some of these policies also prohibited advertising and promotion of less nutritious foods and beverages on school property.\n\n【27】As a result of these policy changes, 66.7% of grantees saw practice changes implemented in schools and districts, including increased use of locally grown food (eg, via school gardens) and scratch cooking methods, thus improving the quality and nutritional value of school meals. Changes in local wellness policies also bolstered the development of school health improvement plans that targeted healthy eating habits; the consumption of fruits, vegetables, and healthy snacks; and use of evidence-based and comprehensive health education curricula that align with national and state standards to promote healthy behaviors among students in districts.\n\n【28】In addition, some districts and schools restricted food celebrations, marketing of unhealthy choices, and use of food and beverages as a reward. For example, one grantee reported that nutrition guidelines changed to require that healthy food options be made available at school celebrations, events, and/or fundraisers. Finally, grantees reported observing some improvements in the availability, quality, and selections of foods and beverages in food service lines and à la carte items among its targeted districts, and one grantee reported a 4% decline in the consumption of soda among high school students.\n\n【29】Discussion\n\n【30】State Public Health Actions’ school health strategies aim to provide a comprehensive approach to adopting nutrition standards and creating a supportive nutrition environment in schools through professional development and training of school staff, adoption and implementation of nutrition policies, and implementation of various evidence-based nutrition practices. The findings from this evaluation provide both a broad perspective and more details of school nutrition work in the United States. For example, significant progress has been made in reaching school nutrition and other school health professionals through professional development and technical assistance. This progress enables new knowledge, skills, and abilities among key stakeholders who are also often the implementers of school nutrition policies and strategies.\n\n【31】Outcome data for most school-level nutrition performance measures were favorable. Grantees implemented practices such as placing fruits and vegetables near the cafeteria cashier, where they are easy to access, and prohibiting all forms of advertising and promotion for candy, fast-food restaurants, or soft drinks. These are just 2 examples of no-cost or low-cost strategies that schools can implement to create a supportive nutrition environment. Grantees also reported achieving significant increases in 2 evidence-based practices: not selling less-healthy foods and beverages (soda pop or fruit drinks, sport drinks, baked goods, salty snacks, candy) and providing information to students or families on the nutrition, caloric, and sodium content of foods available. Slight nonsignificant increases were observed in the percentage of local education agencies that adopted and implemented policies that prohibit all forms of advertising and promotion (eg, contests and coupons) of less-nutritious foods and beverages on school property, the percentage of schools that price nutritious foods and beverages at a lower cost while increasing the price of less nutritious foods and beverages, and the percentage of schools that offer fruits or nonfried vegetables when foods or beverages are offered at school celebrations. Surprisingly, decreases were observed in 2 evidence-based practices: allowing students to have access to drinking water and allowing students to purchase fruits and vegetables from vending machines or at the school store, canteen, snack bar, or as à la carte items. Although these decreases suggest an opportunity for improvement, these achievements collectively demonstrate the capacity of school nutrition nationwide .\n\n【32】These data also show that more than half of all targeted LEAs have adopted nutrition standards. These standards require schools and school cafeterias to offer more fruits, vegetables, whole grains, and fat-free or low-fat dairy products and limit sodium, added sugar, calories and unhealthy fat in competitive foods found in à la carte, vending machines, and other venues . The findings are promising, because the consumption of healthier foods by students may be influenced by implementing these nutrition standards that promote the availability of healthier options offered and purchased at school .\n\n【33】School nutrition policies, when implemented, support consistent and lasting change in schools. Although 56.9% of schools reported prohibiting all forms of advertising and promotion for candy, fast foods, or soft drinks, only 33.5% of LEAs adopted and implemented policies that prohibit all forms of advertising and promotion. This area is one that needs to be better understood. In practice, grantees are making progress in prohibiting advertising; however, ensuring that this practice is sustainable through the adoption of policy still needs to be improved.\n\n【34】Furthermore, federal legislation such as the Healthy Hunger Free Kids Act of 2010 has been a leverage point for State Public Health Actions grantees, including local wellness policy requirements, Smart Snacks in School, and school meal standards . State Public Health Actions grantees have not only supported LEAs and schools to implement these requirements but also helped them go above and beyond the requirements, as evidenced by the performance measures. For example, Smart Snacks in School standards apply only to foods and beverages sold during the school day, whereas this cooperative agreement and its related performance measure addresses all competitive foods available during the day. Through legislation, there will be continued opportunities for grantees to support LEAs and schools in limiting or prohibiting advertising of unhealthy food and beverages, as local school wellness policy requirements now require LEAs to include information about marketing and advertising in their local wellness policies and only market foods that meet Smart Snacks standards.\n\n【35】Finally, state evaluations have allowed for exploration of facilitators, barriers, and outcomes that enable both CDC and grantees to identify future training, technical assistance, and policy development needs among LEAs and schools. For example, from grantees’ evaluation reports, common facilitators have been identified, such as having strong leadership, committed staff, formal partnerships, and engaged champions. Other studies have found similar facilitators to those of grantees, suggesting that these elements support greater implementation of local wellness policies and environmental strategies to promote healthy food and beverages . These facilitators are important to not only identify but continue to support healthy school nutrition environments, policies, and practices. At the same time, grantees reported that barriers persist, such as negative perceptions (eg, of school meals, nutrition), lack of support and buy-in from key stakeholders (eg, administrators, parents), and lack of facilities and resources (eg, lack of free, filtered water; suitable space).\n\n【36】Although results are encouraging, our study has limitations. Many of the findings were derived from performance data from 32 enhanced grantees who were awarded funding on a competitive basis. These data, although representative of the LEAs selected, may not be generalizable to all LEAs in the United States because grantees’ used their own criteria (eg, high need, existing relationships) to select LEAs. In addition, we used self-reported data that were not verified or corroborated by other data or observations. Furthermore, grantees who choose to evaluate the school nutrition strategy may represent more highly motivated grantees, so data from these reports should be interpreted cautiously. Despite these limitations, the data we used were collected by using quality surveillance and data collection systems.\n\n【37】To date, close to 8,000 LEAs and more than 29 million students, representing 57% of US school districts and 59% of students, have been reached by State Public Health Actions . Our findings demonstrate that through professional development and technical assistance grantees increased their capacity to implement evidence-based nutrition policies and practices in US schools. The work of grantees will likely continue to have an effect on school nutrition in the final year, as local school wellness policy requirements will be implemented, equipment and facilities grants from the United State Department of Agriculture (USDA) will be available, and tools, resources, materials, and trainings will also be available through USDA, CDC, and other organizations. There will also be opportunities for grantees to support LEAs and schools in limiting or prohibiting advertising of unhealthy food and beverages. Through professional development, training, and local school wellness policy requirements, grantees will continue to work with LEAs to improve nutrition policy and implement evidence-based strategies that improve student nutrition and the school nutrition environment as a whole.\n\n【38】Finally, as grantees enter the final year of the cooperative agreement, they are required to develop impact statements. These impact statements will highlight the accomplishments of their work during the past 5 years. These impact statements can be used by grantees and CDC to garner support from funders and key decision makers for continued work in the area of school nutrition.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c47c0cc7-16c5-4de5-8807-1d90ddddbb3a", "title": "Erratum, Vol. 9, May 10 Release", "text": "【0】### ERRATUM\n\n【1】Erratum, Vol. 9, May 10 Release\n===============================\n\n【2】_Suggested citation for this article_ : Erratum, vol. 9, May 10 release. Prev Chronic Dis 2012;9:110134e.\n\n【3】Because of an editing error, we inadvertently omitted the name of an author from the Author Affiliations list in the article “Predictors of Risk and Resilience for Posttraumatic Stress Disorder Among Ground Combat Marines: Methods of the Marine Resiliency Study.” Caroline M. Nievergelt, University of California-San Diego, La Jolla, California, should have been included. We regret any confusion or inconvenience this error may have caused.\n\n【4】* * *\n\n【5】The opinions expressed by authors contributing to this journal do not necessarily reflect the opinions of the U.S. Department of Health and Human Services, the Public Health Service, the Centers for Disease Control and Prevention, or the authors' affiliated institutions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "4762da4b-d6b5-4f68-8c47-8e62362131e6", "title": "Duffy Phenotype and Plasmodium vivax infections in Humans and Apes, Africa", "text": "【0】Duffy Phenotype and Plasmodium vivax infections in Humans and Apes, Africa\n**To the Editor:** Benign tertian malaria, caused by _Plasmodium vivax,_ has long been considered absent, or at least extremely rare, in western and central Africa. In these regions, 95%–99% of humans are of the Duffy negative phenotype, a condition that is thought to confer complete protection against the parasite during the blood stages of its life cycle . Sporadic reports throughout the latter half of the 20th century, however, have hinted at the presence of the parasite in these regions, the most convincing of which were the steady and consistent numbers of non-African travelers who returned to their countries of origin infected with malarial parasites that were subsequently identified as _P_ . _vivax_ .\n\n【1】More recently, evidence has emerged regarding the transmission of _P. vivax_ in regions of Africa where the local human population is predominantly Duffy negative . In 4 (3.5%) of 155 patients from western Kenya , 7 (0.8%) of 898 persons from Angola , and 8 (8.2%) of 97 persons from Equatorial Guinea , _P. vivax_ parasites were detected in the blood of apparently Duffy-negative persons, suggesting that the parasite might not be as absolutely dependent on the Duffy receptor for erythrocyte invasion as previously thought.8%) of 476 Duffy-negative persons who had symptoms of malaria were reported to be positive for _P. vivax_ by both microscopy and PCR . The prevalence of _P. vivax_ in Duffy-negative persons was significantly lower than its prevalence in Duffy-positive persons residing in the same area, suggesting that Duffy negativity is a barrier to the parasite to some degree. Given the extremely high rates of malaria transmission in western and central Africa, a _P. vivax_ parasite that could efficiently invade Duffy-negative erythrocytes would, presumably, become highly prevalent very rapidly.\n\n【2】With the exception of the cases reported from Angola and Kenya, which lie outside the area where the proportion of the population with Duffy negativity is highest, the reports of the transmission of _P. vivax_ within predominantly Duffy-negative populations all come from regions inhabited by chimpanzees and gorillas (i.e. Democratic Republic of the Congo , Uganda , and Equatorial Guinea ). During our seroepidemiologic study from the Democratic Republic of the Congo, in which _P. vivax_ sporozoite–specific antibodies were detected in ≈10% of the population, we found that women were significantly more likely than men to have been exposed to _P. vivax_ sporozoites . Women in this region typically spend more time than men near the forest fringe, where they work in crop fields. This forest is within the known habitat range of the chimpanzee _Pan troglodytes_ and the gorilla, _Gorilla gorilla gorilla_ , both of which have been reported to be natural hosts of the malaria parasite _P. schwetzi_ , which is a _P. vivax_ –like or _P. ovale_ –like parasite that might also be unable to invade the erythrocytes of persons who are Duffy negative . These animals have recently been shown to be infected occasionally with parasites that have mitochondrial genomes closely resembling those of _P. vivax_ .\n\n【3】We have argued that, given the high malaria transmission rates in sub-Saharan Africa, it is plausible that the 1%–5% of the human population who are Duffy positive might maintain the transmission of the parasite . The discovery of _P. vivax_ parasites (or _P. vivax_ –like parasites) in the blood of African great apes leads to a question: could nonhuman primates in Africa be acting as Duffy-positive reservoirs of _P. vivax_ in regions where the human population is almost entirely insusceptible? This possibility warrants further investigation. Given the increasing rarity of the great apes, however, their capacity to act as zoonotic reservoirs could be limited. It would be informative, in any case, to determine how the regions that _P. vivax_ –positive travelers visit during their stay in Africa correspond with the ranges of chimpanzees and gorillas.\n\n【4】If African great apes do, indeed, constitute a zoonotic reservoir of _P. vivax_ parasites, what are the repercussions for human health? Given that 95%–99% of humans possibly exposed to such a reservoir are Duffy negative, and therefore resistant to the parasite, these would appear to be slight. However, as humans encroach more frequently into ape habitats, the chances of humans encountering the parasite will increase. In the short term, the risks are probably limited to Duffy-positive persons who enter areas where apes are present, such as tourists and migrant workers.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "928d6982-5d9b-4659-8f95-bf5902c93085", "title": "Mycobacterium leprae on Palatine Tonsils and Adenoids of Asymptomatic Patients, Brazil", "text": "【0】Mycobacterium leprae on Palatine Tonsils and Adenoids of Asymptomatic Patients, Brazil\nLeprosy is a chronic infectious disease caused by _Mycobacterium leprae_ that especially affects skin and peripheral nerves . In 2018, the registered global prevalence in the 6 World Health Organization regions was 184,238 cases (0.24/10,000 population), showing a decrease of 8,475 cases over the previous year . Although its incidence in Brazil has declined during 2009–2018, leprosy continues to be a major public health problem at the national level . Reports of _M. leprae_ resistance against antimicrobial drugs used in multidrug therapy raise concern about the future of leprosy treatment . Therefore, not only does leprosy persist, but the emergence of multidrug-resistant _M. leprae_ is a potential threat to global public health .\n\n【1】Although the exact mode of leprosy transmission is not known, it is thought that the upper respiratory tract, in particular the nasal mucosa, is the usual site of primary infection . Because we have previously identified _M. leprae_ in oral mucosa of leprosy patients , we aimed to investigate other anatomic sites that could host this microorganism to clarify the epidemiology and transmission mechanisms of leprosy.\n\n【2】In this study, we hypothesized that _M. leprae_ , after penetration through the airway mucosa, could infect the palatine tonsils and adenoids, because these organs represent the first immune defense line against inhaled or ingested antigens . We also theorized that if leprosy is a highly contagious disease , a considerable part of the population in endemic regions might be infected with _M. leprae_ .\n\n【3】We conducted a cross-sectional study of 397 paraffin-embedded blocks of palatine tonsils and adenoids extracted from 144 patients due to otorhinolaryngological indication during 2011–2016 at the Regional Hospital, Presidente Prudente, Brazil. The local Research Ethics Committee approved the study (protocol #1.920.994).\n\n【4】Microscopic analysis using hematoxylin-eosin staining did not reveal granulomas. We analyzed 50 fields in the 100× objective (1,000× magnification) per slide stained with Fite-Faraco; of the positive cases (9 \\[2.3%\\] slides from 8 \\[5.6%\\] patients, 6 men and 2 women \\[mean age 11 \\+ 5.5 years\\]), we observed only 1 acid-fast rod per slide. We studied all the blocks of these 8 patients, a total of 20 blocks .\n\n【5】Immunohistochemistry with 1:20,000 anti–phenolic glycolipid-I (anti-PGL-I) antibody , specific for _M. leprae_ , was conducted with the Mach 1 polymer-based biotin-free detection kit  . We used deparaffinized skin sections from multibacillary leprosy as positive control. For the negative control, we omitted the antibody. To confirm specificity, we used deparaffinized skin sections from paucibacillary leprosy and atopic dermatitis (excluding inflammatory cell recognition by the antibody), normal human scalp, and tuberculous lung section.\n\n【6】We extracted DNA from paraffin sections with isopropanol-ammonium acetate . The resulting DNA was used in conventional PCR with sense 5¢-ATTTCTGCCGCTGGTATCGGT-3¢ and antisense 5¢-TGCGCTAGAAGGTTGCCGTAT-3¢ primers  to amplify _M. leprae_ microsatellite sequences, according to a previous report . We assessed amplicons of 148 bp on 2% agarose gel. The assays included negative (DNA omission) and positive (DNA from multibacillary leprosy skin sample) controls. We confirmed specificity with DNA extracted from _M. tuberculosis_ culture. In addition, we conducted PCR with TB1/TB2 primers to detect _Mycobacterium_ spp. and with T4/T5 primers to detect _M. tuberculosis_ .\n\n【7】Immunohistochemistry was positive in 18/20 (90%) blocks. By PCR, 19 (95%) were positive with RLEP and 5 were simultaneously positive with TB1/TB2; all 19 positive by PCR were negative by T4/T5 primers. Samples that tested negative in IHC or PCR were also negative in Fite-Faraco staining .\n\n【8】Little attention has been paid to the role that mucosa-associated lymphoid tissue (MALT) plays in the mechanisms used by mycobacteria during host invasion. In tuberculosis infection, bacilli cross mucous membranes and penetrate into palatine tonsils and adenoids, where they initiate an immune response. However, bacilli may develop immune-evasion strategies and disseminate into the organism or return to the mucosa surface and be eliminated to the environment . This process might also occur in leprosy, but to our knowledge there are no reports on this subject.\n\n【9】Our results corroborate the hypothesis that _M_ . _leprae_ bacilli infect palatine tonsils and adenoids. Prospective studies with a larger population group are necessary to clarify these findings. We could not infer from this retrospective study with paraffinized samples whether patients who had positive results for _M. leprae_ identification had leprosy or were asymptomatic carriers. In both clinical scenarios, however, our findings indicate that palatine tonsils and adenoids may represent reservoirs for _M. leprae_ bacilli in persons inhabiting a leprosy-endemic region.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d600a002-223f-4c49-9980-6ce8a2e39c2a", "title": "The Next Pandemic: On the Front Lines Against Humankind’s Gravest Dangers", "text": "【0】The Next Pandemic: On the Front Lines Against Humankind’s Gravest Dangers\nStalin notoriously remarked that one death was a tragedy, while a million were a statistic. An impressive feature of this book is that, while recounting a great many deaths, it maintains a human and humane perspective. Dr. Ali S. Khan is a former director of the Office of Public Health Preparedness and Response at the Centers for Disease Control and Prevention and has been close to the epicenter of almost all the potential pandemic threats of the last few decades. With William Patrick, an accomplished writer and editor in his own right, Khan has produced an accessible, fascinating book for the general reader that shares his experiences in the service of his country and the world .\n\n【1】The book describes so many outbreaks that its approach is inevitably a little scattershot, but 2 subjects stand out: the investigation of the anthrax mailings to Congress is covered as well as I have seen anywhere, and the sections dealing with the nightmare of Ebola in West Africa should be required reading for anyone looking to improve our response to infectious disease emergencies. As the authors note, maintaining high standards of data hygiene is difficult anywhere, but particularly so if we lack the will to prioritize public health even when resources are scarce.\n\n【2】Throughout, there is a subtle but steady bass line of anguish over the difficulty of translating the science we know into effective action. Khan argues that to keep our priorities straight we need to recognize how the unscrupulous, from black marketeers to politicians, exploit outbreaks as opportunities for advancement; addressing that issue is an essential part of fighting the disease. In the words of Albert Camus, “It is up to us not to join forces with the pestilences.” We might not, but how can we stop others from doing so? Khan suggests the creation of a United Nations Undersecretary for Health Security, which sounds sensible, but it is not clear what policies that person might enact, and even less so how they would be enforced.\n\n【3】It is the personal stories here that are the most affecting, in the understanding that Khan extends to an Ebola-infected nurse who denies that she knows how she acquired the disease, or the prejudice faced by Khan himself as a Muslim American in the years following 9/11. These diseases infect humans, kill humans, and are battled by humans, with all the complicated consequences that entails.\n\n【4】There are a few missteps. The persistent nagging use of clichés like “disease detectives” can sound patronizing. An assertion that viruses are “collectively intelligent” will surely raise eyebrows, and the title is a little misleading, in that there is less focus than expected on genuine catastrophic pandemic threats. However, as another confessed “infectious disease nerd” I found it fascinating. It will be enjoyed by working epidemiologists; it should be a point of reference for policymakers; and I suspect it will inspire many future officers in the Epidemic Intelligence Service.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "70f32128-59dc-41bd-90a3-d217ff98563f", "title": "Antiretroviral Therapy–associated Coccidioidal Meningitis", "text": "【0】Antiretroviral Therapy–associated Coccidioidal Meningitis\n**To the Editor:** Coccidioidomycosis, a reemerging fungal infection in the United States, comprises ≈150,000 cases annually . We report a case of postmortem examination–proven antiretroviral therapy (ART)–associated coccidioidomycosis manifesting as atypical lymphocytic meningitis, which we believe represents a rare presentation of immune reconstitution inflammatory syndrome (IRIS).\n\n【1】In September 2011, a 59-year-old man sought care in Atlanta, Georgia, USA, with new-onset headache, photophobia, and neck stiffness. He also reported fevers, chills, weight loss, dyspnea, and cough with scant hemoptysis. Two months earlier, he had sought care for epididymitis; HIV infection was diagnosed at that time (CD4+ T-cell count of 45 cells/µL \\[7%\\] and plasma HIV RNA level of 420,720 copies/mL \\[reference not detectable\\]). He was started on an ART regimen, and 1 week before his September 2011 illness, HIV RNA level had decreased to 790 copies/mL and CD4+ count had risen to 163 cells/µL (13%).\n\n【2】The patient was a thin African-American man who reported marked discomfort, with nuchal rigidity. Laboratory results were unremarkable except for serum sodium of 128 mEq/L (reference 132–144 mEq/L) and creatinine of 1.7 mg/dL (reference 0.7–1.2 mg/dL). Chest imaging showed a diffuse infiltrate in a miliary pattern. Noncontrast computed tomography scan of the head was normal. Cerebrospinal fluid (CSF) examination revealed an opening pressure of 31 cm H 2  O, 365 leukocytes/µL (reference <11/μL), (93% lymphocytes, 80% described as atypical), glucose 13 mg/dL (reference 40–70 mg/dL), and protein 171 mg/dL (reference 15–45 mg/dL).\n\n【3】Closer examination of the atypical CSF lymphocytes showed mostly CD3+ T cells, markedly variable in size and morphology, containing substantially irregular nuclear membranes, coarsened chromatin with multilobulated flower-shaped nuclei, and increased cytoplasm . India ink stain, acid-fast bacillus stain, and Gram stain were all negative. CSF Venereal Disease Research Laboratory and cryptococcal antigen test results also were negative. Flow cytometry on the CSF was not performed. No abnormal-appearing lymphocytes were noted in the blood.\n\n【4】Empirically, the patient was placed on 4-drug therapy plus dexamethasone for tuberculous meningitis, along with bacterial and viral meningitis coverage. Results of CSF cultures and viral PCR studies were negative. Magnetic resonance imaging of the brain showed 2 areas consistent with cavernous malformations but no abnormal meningeal enhancement. On day 5 of admission, blurred vision and confusion developed and rapidly progressed to obtundation. A repeat head computed tomography scan showed marked edema and transtentorial herniation, and a ventricular drain was placed. Full-strength voriconazole was added empirically for coverage of common fungal organisms. However, the patient’s clinical status worsened, and he died 2 days later.\n\n【5】On postmortem examination, brain pathology showed areas of necrosis, along with massive hemorrhage. Special staining of the necrotic tissue revealed marked inflammation surrounding multiple characteristic _Coccidioides_ spp. spherules, 1 of which had ruptured and was spilling endospores . Examination of the lungs showed multiple granulomas that also contained coccidioidal spherules.\n\n【6】Pathogenic _Coccidioides_ spp. is not indigenous to the Atlanta area; however, the patient was homeless and could have traveled to _Coccidioides_ spp.–endemic areas. The need for increased suspicion for coccidioidomycosis in areas to which it is not endemic was highlighted further by a recent report identifying a case in Rome, Italy .\n\n【7】Coccidioidal meningitis is the most severe complication of coccidioidomycosis and results from lymphohematogenous spread from the lung, manifesting as fever, headache, changes in sensorium, malaise, and meningismus. CSF studies typically show 100–500 leukocytes/µL (predominantly lymphocytes), low glucose, and protein >150 mg/dL. Culture of _Coccidioides_ spp. from the CSF is diagnostic but is much less sensitive than detection of anticoccidioidal antibodies in the CSF .\n\n【8】The most intriguing aspect of this case is the atypical pleocytosis, which initially suggested lymphomatous or tuberculous meningitis and obscured the true diagnosis of coccidioidal disease. Atypical reactive CSF lymphocytes were described in a lymphoma patient in whom coexistent cryptococcal meningitis was diagnosed . Those cells were initially confused for CNS lymphoma and caused a similar diagnostic and therapeutic dilemma.\n\n【9】Haddow et al. recently proposed case definitions for such clinical phenomena involving cryptococcosis, offering that newly defined cryptococcal disease identified after ART initiation be termed ART-associated cryptococcosis, with a more virulent subset of disease attributed to the unmasking of cryptococcal IRIS . We propose that the case described here parallels that described by Haddow et al. and illustrates ART-associated coccidioidomycosis. Furthermore, because of the significant inflammatory process, granulomatous pathology, and exaggerated clinical deterioration in the setting of rapid ART-induced CD4+ T-cell recovery, we suggest that this case meets the proposed criteria for the unmasking form of IRIS .\n\n【10】The high percentage of atypical lymphocytes described here is unusual for an IRIS response and might reflect an unusual variation of the diverse immune mechanisms used during an IRIS phenomenon . Another unusual case of coccidioidal IRIS manifested as superior vena cava syndrome . Additionally, the use of high-dose corticosteroids in the absence of antifungal therapy might have contributed to more aggressive disease progression. We suggest that the discovery of atypical lymphocytic meningitis in a patient shortly after ART-associated immune recovery should alert the clinician to the possibility of coccidioidal meningitis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a781f628-e5d3-41c4-b57f-e93bc2c9693e", "title": "Activity Profile and Energy Expenditure Among Active Older Adults, British Columbia, 2011–2012", "text": "【0】Activity Profile and Energy Expenditure Among Active Older Adults, British Columbia, 2011–2012\nAbstract\n\n【1】**Introduction**\n\n【2】Time spent by young adults in moderate to vigorous activity predicts daily caloric expenditure. In contrast, caloric expenditure among older adults is best predicted by time spent in light activity. We examined highly active older adults to examine the biggest contributors to energy expenditure in this population.\n\n【3】**Methods**\n\n【4】Fifty-four community-dwelling men and women aged 65 years or older (mean, 71.4 y) were enrolled in this cross-sectional observational study. All were members of the Whistler Senior Ski Team, and all met current American guidelines for physical activity. Activity levels (sedentary, light, and moderate to vigorous) were recorded by accelerometers worn continuously for 7 days. Caloric expenditure was measured using accelerometry, galvanic skin response, skin temperature, and heat flux. Significant variables were entered into a stepwise multivariate linear model consisting of activity level, age, and sex.\n\n【5】**Results**\n\n【6】The average (standard deviation \\[SD\\]) daily nonlying sedentary time was 564  minutes (9.4 \\[1.5\\] h) per day. The main predictors of higher caloric expenditure were time spent in moderate to vigorous activity (standardized β = 0.42 \\[SE, 0.08\\]; _P_ < .001) and male sex (standardized β = 1.34 \\[SE, 0.16\\]; _P_ < .001). A model consisting of only moderate to vigorous physical activity and sex explained 68% of the variation in caloric expenditure. An increase in moderate to vigorous physical activity by 1 minute per day was associated with an additional 16 kcal expended in physical activity.\n\n【7】**Conclusion**\n\n【8】The relationship between activity intensity and caloric expenditure in athletic seniors is similar to that observed in young adults. Active older adults still spend a substantial proportion of the day engaged in sedentary behaviors.\n\n【9】Introduction\n\n【10】The amount of time people engage in physical activity tends to decrease with increasing age , leading to numerous functional and cardiometabolic sequelae. Older adults make up both the least active and most sedentary cohort in Western countries . A lack of energy expenditure from physical activity is considered to be one of the contributors to the growing worldwide rates of obesity . Activity profiles of differently aged populations show that the main contributor to calorie expenditure among young adults is time spent in moderate to vigorous physical activity , while light activity is the most important contributor to caloric expenditure among older adults .\n\n【11】Both young adults  and older adults  spend large quantities of their waking hours engaged in sedentary behaviors, which has cardiometabolic consequences independent of the amount of time spent in leisure-time physical activity. What has not been examined, however, is the activity profiles of older adults who meet guidelines  for physical activity (≥150 minutes of physical activity per week). The objective of this study was to measure (by accelerometer) the amount of time active older adults spend in sedentary behavior and to determine which intensity of activity best predicts daily caloric expenditure.\n\n【12】Methods\n\n【13】This was a cross-sectional observational study. This study was approved by the human subjects committee of the University of British Columbia, and all participants gave written informed consent.\n\n【14】### Participants and recruitment\n\n【15】Fifty-five community-dwelling men and women aged 65 years or older were screened through their affiliation with the Whistler Senior Ski Team of British Columbia, Canada, via a study poster and information session. Participants were enrolled from October 2011 through June 2012.\n\n【16】All participants had to be able to independently perform all basic activities of daily living, climb 1 flight of stairs, and walk 2 blocks without assistance. Current smokers, recreational drug users, and those with diabetes mellitus or cardiovascular disease (prior strokes, transient ischemic attacks, angina, myocardial infarction, or coronary revascularization in the previous 2 years) were not eligible. All participants had to meet current guidelines for physical activity (≥150 minutes of physical activity per week) .\n\n【17】### Research procedures\n\n【18】Each participant was required to make at least 1 study visit to allow researchers to collect demographic data and apply an accelerometer. Sensewear Pro armband triaxial accelerometers (BodyMedia, Sword Medical Limited) were fitted snuggly around the left upper triceps and used to monitor levels of physical activity 24 hours per day for 7 full days. Participants were instructed to wear it continuously, including during sleep, except when bathing or swimming. Minute-by-minute epoch data from the SenseWear Pro was analyzed using Body Media InnerView Research Software (version 5.1, BodyMedia, Inc).\n\n【19】To be included in the analysis, participants were required to comply with wearing the accelerometer for at least 5 valid days, including 1 weekend day. A valid day was defined as at least 21 hours of recorded activity on the accelerometer.\n\n【20】### Measures\n\n【21】Accelerometer data was recorded in 1-second epochs. Average time per day spent in sedentary activity, light activity, and moderate to vigorous activity levels was recorded as minutes per day. On the basis of a systematic review of accelerometry practice for older adults , the following cut points were used: 99 counts or less per minute as sedentary time, 100 to 1,951 counts per minute as light physical activity, and 1,952 or more counts per minute as moderate to vigorous level of activity .\n\n【22】The SenseWear Pro armband was also used to measure heat flux (the amount of heat dissipating from the body), galvanic skin response (the amount of evaporative heat loss), and skin temperature (an estimate of the body’s core temperature). These parameters are then entered into proprietary algorithms to estimate caloric expenditure. The use of the SenseWear Pro to measure caloric expenditure due to physical activity has been used in previous investigations in older adults  and has been validated against doubly labeled water techniques .\n\n【23】### Statistical analysis\n\n【24】All measures of physical activity were normalized by the amount of time per day the accelerometer device was worn. Our primary response variable was caloric expenditure (energy expenditure per day, kcal). The 3 levels of physical activity (sedentary, light, and moderate to vigorous) and predictors in the multivariate linear regression model were determined a priori. Previous investigations showed that age and sex are predictors of energy expenditure in older adults; these predictors therefore were also added to our initial model .\n\n【25】Scatterplots were visually inspected for outlier data, and density plots were examined to identify data skewing. Any predictors that demonstrated skewing were logarithmically transformed (base 10) before the univariate and multivariate analyses. A tiered approach was used for the analysis whereby the initial model consisted of all of our predictor variables. The data were fitted with a linear model using the least squares method and the parameters (intercept and β coefficients) were calculated  as well as scaled β coefficients using standard methods . A stepwise method was used to generate each successive regression model; the criterion for removal of variables was the least significant predictor with a _P_ value greater than .05. In each iteration of the stepwise regression model, the least significant predictor was removed . After the removal of each predictor, an analysis of variance (ANOVA) was performed with the previous model to verify that there had been no significant change. To ensure the assumptions of the multivariate regression were met, variance inflation factors were examined for multicolinearity in each iteration of the model. A value of a variance inflation factor greater than 10 was interpreted as an indicator of collinearity problems . Plots of residuals and a QQ plot were examined in our final minimum effective model. The R core software package version 3.0.1 was used for statistical analysis; a significance level of _P_ < .05 was set .\n\n【26】Results\n\n【27】Fifty-five people were screened; 1 person was excluded because of a cardiovascular event in the previous 2 years. Of the 54 originally recruited, 2 participants withdrew; 1 participant did not meet the accelerometer compliance criteria, and 1 participant wore the monitor incorrectly. Data from 50 participants (23 men, 27 women) were used in data analysis. The accelerometers were worn for an average of 98.4% (standard deviation \\[SD\\], 1.4%) of the study time.\n\n【28】Participants spent an average (SD) of 159  minutes per day in moderate to vigorous activity and 245  minutes per day in light activity . Despite this high level of activity, each participant spent a large quantity of time in sedentary behaviors while not lying down (average \\[SD\\] = 564  min/d or 9.4 h/d).\n\n【29】None of the predictor variables demonstrated skewing, and no transformation was required before the analysis. In our final minimum effective model, moderate to vigorous physical activity was the only activity parameter significantly correlated with caloric expenditure. Time spent in sedentary activity and light activity was negatively associated with caloric expenditure, but these associations were not significant .\n\n【30】A multivariate regression model including 5 predictors (time spent in sedentary activity, time spent in light activity, time spent in moderate to vigorous activity, age, and sex) explained 73% of the variance in caloric expenditure (Model 1). The highest variance inflation factor was 2.97 (for time spent in sedentary activity), indicating no issues of multicolinearity.\n\n【31】Model 1  demonstrated positive associations between time spent in light activity, time spent in moderate to vigorous physical activity and male sex with caloric expenditure and a negative association between increasing age and time spent in sedentary activity with caloric expenditure. However, our minimum effective model (Model 4), which included only moderate to vigorous physical activity and sex, explained 68% of the variance in caloric expenditure. Every extra minute spent in moderate to vigorous physical activity per day was associated with an increasing caloric expenditure of 16 kcal. In addition, male sex was associated with a higher caloric expenditure (Model 4).\n\n【32】One participant had very high levels of activity and was therefore an outlier. A sensitivity analysis in which data for this participant were excluded showed that this exclusion had no effect on the results of the analysis.\n\n【33】Discussion\n\n【34】We demonstrated that an athletic older adult population spent a substantial portion of their waking hours in sedentary activity (approximately 9.4 hours per day). We also showed that the main contributor to energy expenditure in active older adults is the amount of time spent in moderate to vigorous activity.\n\n【35】The most surprising finding of our study was that despite exceeding the current guidelines for physical activity of 150 minutes or more of physical activity per week , highly active older adults spent a large quantity of the day completely sedentary. The amount of sedentary time observed in our active population was comparable to that seen in sedentary adults over 60 years old , sedentary adults over 65 years old , middle-aged sedentary adults , adult men over 70 years old , and older adult men over 80 years old . This amount of sedentary time was surprising given that the physical activity level of our participants is observed in less than 5% of the older adult population .\n\n【36】Although the contribution of different levels of activity to energy expenditure has been studied in sedentary young  and sedentary older  adults, this relationship has not been examined in active older adults. Previous work showed that the main contributor to energy expenditure in young adults is moderate-to-vigorous intensity activity . In contrast, light activity has been shown to be the main contributor to energy expenditure in older adults . Our participants demonstrated a relationship between activity intensity and caloric expenditure that was more in keeping with a younger population, with moderate-intensity activity predicting energy expenditure.\n\n【37】Although the reason for this “young” profile for activity and energy expenditure is unclear, some exercise intervention studies suggest an underlying mechanism. Unlike young adults, older adults may increase their physical activity though a shift from sedentary to light intensity activities, because these activities tend to be better tolerated . Our participants clearly did not follow this pattern, perhaps because most of our participants were continuing an established pattern of high levels of physical activity as opposed to starting an exercise program from a previous sedentary state.\n\n【38】Although regular leisure-time physical activity has many benefits , sedentary behavior has recently been identified as an independent risk factor for dyslipidemia, diabetes mellitus, obesity, and hypertension . Of even more concern, these associations persist despite accounting for the level of moderate and vigorous physical exercise . These findings suggest that sedentary behavior may pose a risk for cardiometabolic disease that is distinct from physical exercise, or the lack thereof. Our study population was an extremely active group of individuals; despite this, they spent a large amount of time in sedentary behaviors. In fact, the time spent sedentary was similar to that observed in studies of average normal older adults . Although more work needs to be done, our results suggest that even active older adults could benefit from interventions (such as the use of standing desks) that would reduce sedentary time without interfering with their current high levels of moderate-intensity activity.\n\n【39】Our study has several potential limitations. The cross-sectional nature of the study design limits inference about causality. Prospective or interventional trials are needed to define the physiologic and behavioral factors involved in the associations observed in this study. In addition, our highly active study population and the study’s small sample size make generalizability of our results to less active populations problematic.\n\n【40】Tables\n------\n\n【41】#####  Table 1. Demographic, Metabolic, and Activity Characteristics of Participants, Study on Physical Activity Among Older Adults (N = 50), British Columbia, 2011–2012\n\n| Characteristic | Value |\n| --- | --- |\n| Age, mean (SD) \\[range\\], y | 71.4 (4.2)  |\n| Women, n (%) | 27  |\n| **Average time in activity level, mean (SD), min/d \\[% of day a \\]** | **Average time in activity level, mean (SD), min/d \\[% of day a \\]** |\n| Lying down | 480   |\n| Sedentary b | 564   |\n| Light c | 245   |\n| Moderate to vigorous d | 159   |\n\n【43】Abbreviation: SD, standard deviation.  \na  Percentages do not sum to 100% because of rounding.  \nb  Defined as ≤99 counts per minutes as measured by accelerometry.  \nc  Defined as 100 to 1,951 counts per minute as measured by accelerometry.  \nd  Defined as 1,952 or more counts per minute as measured by accelerometry.\n\n【44】#####  Table 2. Multivariate Regression Analysis Using Caloric Expenditure (kcal) as Primary Response Variable, Study on Physical Activity Among Older Adults (N = 50), British Columbia, 2011–2012\n\n| Predictor | Pearson _R_ (95% CI) | _P_ Value |\n| --- | --- | --- |\n| **Activity level a** | **Activity level a** | **Activity level a** |\n| Sedentary b | −0.21 (−0.46 to 0.08) | .15 |\n| Light c | −0.21 (−0.46 to 0.08) | .15 |\n| Moderate to vigorous d | 0.48 (0.23 to 0.67) | <.001 |\n| **Age** | 0.01 (−0.28 to 0.29) | .97 |\n\n【46】Abbreviation: CI, confidence interval.  \na  All activity levels are in minutes per day.  \nb  Defined as ≤99 counts per minutes as measured by accelerometry.  \nc  Defined as 100 to 1,951 counts per minute as measured by accelerometry.  \nd  Defined as 1,952 or more counts per minute as measured by accelerometry.\n\n【47】#####  Table 3. Stepwise Multivariate Regression Analysis Using Caloric Expenditure (kcal) as Primary Response Variable, Study on Physical Activity Among Older Adults (N = 50), British Columbia, 2011–2012\n\n| Model/Predictors | Unstandardized β (SE) | Standardized β (SE) | _P_ Value |\n| --- | --- | --- | --- |\n| **Model 1** (F 5,44 \\= 23.7; _R_ 2 \\= 0.73; _P_ < .001) | **Model 1** (F 5,44 \\= 23.7; _R_ 2 \\= 0.73; _P_ < .001) | **Model 1** (F 5,44 \\= 23.7; _R_ 2 \\= 0.73; _P_ < .001) | **Model 1** (F 5,44 \\= 23.7; _R_ 2 \\= 0.73; _P_ < .001) |\n| Time spent in sedentary activity | −7.68 (4.08) | −0.25 (0.14) | .07 |\n| Time spent in light activity | 15.37 (5.82) | 0.33 (0.12) | .01 |\n| Time spent in moderate to vigorous activity | 24.97 (5.06) | 0.64 (0.13) | <.001 |\n| Increasing age | −33.92 (61.47) | −0.05 (0.08) | .58 |\n| Male sex | 4,801.5 (550.0) | 1.47 (0.17) | <.001 |\n| **Model 2 (** F 4,45 \\= 30.0; _R_ 2 \\= 0.73; _P_ < .001) | **Model 2 (** F 4,45 \\= 30.0; _R_ 2 \\= 0.73; _P_ < .001) | **Model 2 (** F 4,45 \\= 30.0; _R_ 2 \\= 0.73; _P_ < .001) | **Model 2 (** F 4,45 \\= 30.0; _R_ 2 \\= 0.73; _P_ < .001) |\n| Time spent in sedentary activity | −7.49 (4.04) | −0.25 (0.13) | .07 |\n| Time spent in light activity | 15.27 (5.77) | 0.32 (0.12) | .01 |\n| Time spent in moderate to vigorous activity | 25.26 (5.00) | 0.65 (0.13) | <.001 |\n| Male sex | 4,725.8 (528.5) | 1.45 (0.16) | <.001 |\n| **Model 3** (F 3,46 \\= 36.9; _R_ 2 \\= 0.71; _P_ < .001) | **Model 3** (F 3,46 \\= 36.9; _R_ 2 \\= 0.71; _P_ < .001) | **Model 3** (F 3,46 \\= 36.9; _R_ 2 \\= 0.71; _P_ < .001) | **Model 3** (F 3,46 \\= 36.9; _R_ 2 \\= 0.71; _P_ < .001) |\n| Time spent in light activity | 7.58 (4.12) | 0.16 (0.09) | .07 |\n| Time spent in moderate to vigorous activity | 18.14 (3.27) | 0.47 (0.08) | <.001 |\n| Male sex | 4,652.3 (540.8) | 1.42 (0.17) | <.001 |\n| **Model 4** (F 2,47 \\= 51.1; _R_ 2 \\= 0.68; _P_ < .001) | **Model 4** (F 2,47 \\= 51.1; _R_ 2 \\= 0.68; _P_ < .001) | **Model 4** (F 2,47 \\= 51.1; _R_ 2 \\= 0.68; _P_ < .001) | **Model 4** (F 2,47 \\= 51.1; _R_ 2 \\= 0.68; _P_ < .001) |\n| Time spent in moderate to vigorous activity | 16.28 (3.19) | 0.42 (0.08) | <.001 |\n| Male sex | 4,379.9 (533.1) | 1.34 (0.16) | <.001 |\n\n【49】Abbreviation: SE, standard error.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "aa2121da-7f00-47a2-8f75-d789b3611014", "title": "Weighing Potential Benefits and Harms of Mycoplasma genitalium Testing and Treatment Approaches", "text": "【0】Weighing Potential Benefits and Harms of Mycoplasma genitalium Testing and Treatment Approaches\n_Mycoplasma genitalium_ causes 20%–30% of male urethritis cases  and has been associated with a 2-fold increased risk for female cervicitis, preterm delivery, and pelvic inflammatory disease (PID) . It has also been linked to tubal-factor infertility in sero-epidemiologic studies, even after accounting for previous chlamydial infection . Despite these strong observational data, relatively few prospective studies of reproductive and perinatal complications of _M. genitalium_ exist, and a separate meta-analysis of the association with PID restricted to cohort studies (n = 2) observed a smaller, nonstatistically significant association . Coupled with this epidemiologic conundrum are substantial treatment challenges. Doxycycline is only 30%–40% effective , and the efficacy of azithromycin and moxifloxacin has declined substantially  since the Centers for Disease Control and Prevention (CDC) 2015 Sexually Transmitted Diseases Treatment Guidelines were written. The reasons for the low efficacy of doxycycline are unclear, but treatment failures after azithromycin and moxifloxacin are caused by the emergence and spread of antimicrobial resistance . In a recent meta-analysis, detection of macrolide resistance mutations worldwide rose from 10% before 2010 to 51% in 2016–2017 (p<0.0001). Detection of the quinolone resistance–associated mutation most clearly linked to treatment failure (S83I) was 4.3% , although prevalence as high as 84% has been observed in China . Among urethritis cases in the United States during 2017–2018, macrolide resistance–associated mutations were detected in 64.4% and quinolone resistance–associated mutations in 11.5% .\n\n【1】### Methods\n\n【2】This systematic review of the evidence was guided by 2 types of key questions : informational questions outlining data published since the 2015 update to the CDC treatment guidelines and discussion questions outlining how the evidence should inform guidelines. We identified articles published in English since the 2015 treatment guidelines (January 1, 2015–November 30, 2021) by using the search terms mycoplasma AND genitalium OR _M. genitalium_ to search PubMed. One investigator (L.E.M.) reviewed abstracts and manuscripts for inclusion. All authors verified included papers. In this article, we focus on new evidence on disease syndromes and antibiotic therapy and the implications for testing and treatment strategies.\n\n【3】### Results\n\n【4】##### Identifying Evidence\n\n【5】We identified 743 papers. Of these, we excluded 150 by title and 195 after abstract review; 398 articles underwent full-text review . Articles did not distinguish birth sex from gender identity, so we use the terms male or men and female or women.\n\n【6】##### Associations with Disease Syndromes\n\n【7】##### Male Urethritis\n\n【8】_M. genitalium_ is consistently and strongly associated with acute, persistent, and recurrent urethritis  and is an accepted cause of male urethritis. No new data have emerged to change this conclusion.\n\n【9】##### Epididymitis\n\n【10】Evidence to determine whether _M. genitalium_ causes epididymitis remains insufficient. Only case reports have been published , and no new studies have compared _M. genitalium_ prevalence in men with and without epididymitis.\n\n【11】##### Male Infertility\n\n【12】Limited available evidence does not support a role for _M. genitalium_ as a cause of male infertility. Although an increasing number of studies have investigated this possibility, many evaluated sperm quality rather than documented infertility. Studies enrolling fertile and infertile men have reported no association .\n\n【13】##### Proctitis\n\n【14】Prevalence of rectal _M. genitalium_ infection in a recent meta-analysis was higher among men with rectal symptoms than in men without rectal symptoms (16.1% vs. 7.5%; p = 0.039) . However, few studies included both symptomatic and asymptomatic men , and results from those studies are inconsistent. No association was observed between _M. genitalium_ and proctitis (odds ratio 0.8; 95% CI 0.45–1.35) in ≈1,000 men who have sex with men (MSM) in Australia , yet a later study from Australia found MSM with proctitis were significantly more likely to have _M. genitalium_ than MSM without proctitis (risk difference 4.3%; 95% CI 1.1%–7.5%) .\n\n【15】##### Female Cervicitis\n\n【16】Earlier evidence on the relationship between _M. genitalium_ and cervicitis was somewhat conflicting, partly because of varying definitions of cervicitis, but generally supported an association . This inconsistency remained the case in 4 newer cross-sectional studies. Of those studies, 2 demonstrated increased risk , 1 demonstrated increased risk only among a subset of persons , and 1 reported no increased risk .\n\n【17】##### Pelvic Inflammatory Disease\n\n【18】Data remain conflicting on the association of _M. genitalium_ with PID, as reflected in the 2 published meta-analyses . Studies published since 2014 continued this trend, demonstrating a statistically significant 2-fold increased risk for histologic endometritis , a nonsignificant ≈3-fold increase in risk for incident PID , and no association . No randomized trials testing whether screening and treatment of _M. genitalium_ reduces PID incidence have been conducted. In the absence of such trials, there are no population-based screening recommendations.\n\n【19】##### Female Infertility\n\n【20】In vitro inoculation of _M. genitalium_ into fallopian tube tissue results in damage and destruction of cilia , suggesting that _M. genitalium_ might impair female fertility. Consistent with this finding is the 2-fold increased risk for infertility in meta-analysis, especially among studies accounting for other microbial causes of infertility . More recent studies have reported mixed results. In 2 studies, serologic testing was used to detect antibodies indicative of previous infection; 1 observed a significantly longer time to conception , whereas the other reported no association with tubal factor infertility . Three studies used nucleic acid amplification testing (NAAT), which typically detects active infection, and found that _M. genitalium_ infection was more common among infertile than fertile women .\n\n【21】##### Ectopic Pregnancy\n\n【22】Data on ectopic pregnancy remain limited. No association was seen among women in Sweden using first-generation serologic assays , but a study in Saudi Arabia using NAAT to detect _M. genitalium_ in fallopian tube tissue demonstrated a 2-fold increased risk for ectopic pregnancy among women . Additional studies will be required to determine whether _M. genitalium_ causes ectopic pregnancy.\n\n【23】##### Preterm Delivery\n\n【24】Evidence remains insufficient on the relationship between _M. genitalium_ infection and preterm delivery. An earlier meta-analysis demonstrated a 2-fold increased risk for preterm delivery, which was stronger among studies accounting for other infections . However, 2 more recent studies observed low _M. genitalium_ prevalence (0%–1%) and were unable to evaluate an association . In a third study, _M. genitalium_ infection was more common in women in Australia who experienced a preterm birth than in those who did not (15.4% vs. 2.3%), but this difference was not statistically significant . Adequately powered studies are needed.\n\n【25】##### Other Perinatal Outcomes\n\n【26】Evidence for an association between _M. genitalium_ infection and spontaneous abortion is conflicting. An initial meta-analysis reported significantly increased risk in _M. genitalium_ –infected persons , but subsequent studies did not . Using NAAT, _M. genitalium_ has been detected in the endotrachea of neonates , bronchoalveolar lavage samples from children 0–5 years , and ocular samples from infants born to infected mothers , suggesting that transmission of _M. genitalium_ during vaginal delivery might occur. However, positive NAATs might only reflect residual DNA; larger studies are needed to determine the extent of maternal-to-child transmission.\n\n【27】##### Who Should Be Tested for _M. genitalium_ ?\n\n【28】Screening, diagnostic testing, and tests of cure each have different goals. Screening tests are undertaken in asymptomatic persons to provide treatment, limit sequelae, and prevent transmission. Diagnostic tests are performed in symptomatic persons to direct treatment at a specific pathogen and eliminate the organism. Tests of cure are undertaken to confirm eradication of pathogens. Recommendations for each type of testing are typically grounded in a robust body of evidence. When there is less robust evidence, as with _M. genitalium_ , potential benefits must be weighed against potential harms.\n\n【29】##### Screening\n\n【30】Screening at-risk women for gonorrhea and chlamydia is recommended in the United States because infections are frequently asymptomatic ; robust data exist on risk for sequelae from untreated infections ; effective treatment is readily available ; and, for chlamydia, data from a randomized trial demonstrated decreased risk for PID in persons screened and treated . _M. genitalium_ meets only 1 of these criteria: infections are frequently asymptomatic (≈30%–60% in clinic populations) . Data on the risk for sequelae from untreated _M. genitalium_ infections in women are less robust than data for chlamydia. Prospective studies of sequelae in women are limited, challenging our ability to infer cause . No evidence of adverse sequelae in men exists, although such data are sparse. Given the high rates of macrolide resistance and the advent of fluoroquinolone treatment failures, effective treatment is not readily available for all infected persons  . Finally, randomized controlled trials evaluating whether screening and treatment of _M. genitalium_ infections could prevent PID and perinatal complications are costly; to date, none have been undertaken despite the articulated need . Some population subgroups have higher risk for PID, and screening and treatment in those subgroups might yield greater benefit, but this possibility has not been assessed.\n\n【31】##### Benefits of Screening for _M. genitalium_\n\n【32】Screening and effectively treating asymptomatic persons for _M. genitalium_ could theoretically reduce transmission and population-level prevalence . However, reductions in prevalence after implementing screening programs for sexually transmitted infections do not always occur, as demonstrated by increasing chlamydia prevalence in the United States . This increase has been offset by some declines in PID, although PID rates have slightly increased in recent years . If identified infections could be effectively treated while minimizing the selection of resistance, screening could reduce the likelihood of transmitted resistance, leading to subsequent reductions in the population-level prevalence of antimicrobial resistance.\n\n【33】##### Harms of Screening for _M. genitalium_\n\n【34】If asymptomatic infections do not cause sequelae, screening and treating will result in unnecessary antibiotic exposure. On an individual level, antibiotics might disrupt a person’s microbiota and lead to other health conditions, and adverse effects associated with antibiotics are occasionally serious . On a population level, more widespread antibiotic use speeds the emergence and spread of antimicrobial resistance, and multidrug-resistant _M. genitalium_ infections are often refractory to treatment. Anecdotal reports suggest that treatment-refractory infections can lead to anxiety and depression that would not occur in the absence of screening. Consistent with earlier assessments , screening asymptomatic persons for _M. genitalium_ is not recommended in the 2021 CDC Sexually Transmitted Infections Treatment Guidelines .\n\n【35】##### Diagnostic Testing\n\n【36】Etiologic management is increasingly recommended over syndromic management in clinical care  and requires accurate diagnostic tests. Given the time and difficulty in culturing _M. genitalium_ , NAATs are the preferred method of detection. Two NAATs are approved by the US Food and Drug Administration: the Aptima _Mycoplasma genitalium_ assay  and the Cobas TV/MG assay . Both tests have good sensitivity and specificity when recommended specimen types are used (urine for men and vaginal swab for women) . Several other NAATs are available in other countries, including assays that incorporate the detection of macrolide-resistance mutations. The ResistancePlus MG assay , S-DiaMGRes assay , and RealAccurate TVMGres assay  are approved for use in Australia, the United Kingdom, Europe, and Canada. To date, no assays to detect macrolide resistance have been approved by the US Food and Drug Administration, although 1 company has developed analyte-specific reagents for use in laboratory-developed tests (Hologic).\n\n【37】##### Benefits of Diagnostic Testing\n\n【38】The clinical manifestations of _M. genitalium_ and _Chlamydia trachomatis_ infections are similar , yet antibiotics recommended for syndromic treatment of urethritis and cervicitis have low efficacy against _M. genitalium_ . Diagnostic testing for _M. genitalium_ when patients first seek care would enable providers to rapidly follow nonspecific syndromic therapy with a regimen more effective against _M. genitalium_ . This practice would shorten the time to appropriate therapy, reduce the duration of infection, more rapidly alleviate symptoms, likely reduce risk for sequelae, reduce opportunities for transmission to partners, and lead to fewer interactions with the healthcare system.\n\n【39】##### Harms of Diagnostic Testing\n\n【40】Diagnostic testing to determine etiology and appropriate therapy might increase healthcare costs. Detecting _M. genitalium_ results in additional treatment, potentially with expensive antimicrobial drugs that can have serious adverse effects. Even when symptoms resolve, a positive test result might exacerbate patient distress. Sequelae in men are uncommon, and the debate over sequelae in women has led some to believe asymptomatic _M. genitalium_ infections are not of sufficient concern to warrant treatment. If asymptomatic infections truly do not warrant treatment, the cost might outweigh the benefits; however, this possibly has not been studied. The 2021 CDC treatment guidelines recommend diagnostic testing only for persons with persistent or recurrent symptoms .\n\n【41】##### Tests of Cure\n\n【42】Tests of cure are undertaken to confirm pathogen eradication, preventing transmission and reinfection. Because NAATs detect both viable and nonviable DNA/RNA, explicit time frames for tests of cure have been outlined, ranging from 7–14 days after therapy for pharyngeal gonorrhea to ≈28 days for chlamydia in pregnant persons . Macrolide-sensitive _M. genitalium_ infections are cleared relatively quickly after azithromycin therapy ; when tests of cure are performed, the timeframe is similar to that for gonorrhea and chlamydia. Australian guidelines recommend a test of cure 14–21 days after treatment .\n\n【43】##### Benefits of Tests of Cure\n\n【44】The primary benefit of tests of cure is verifying that the organism has been successfully eradicated, which is key for pathogens that cause serious sequelae, particularly during asymptomatic infection. Because _M. genitalium_ sometimes recrudesces after symptoms resolve , a test of cure would identify the need for additional therapy earlier, in turn reducing the risk of infecting sex partners, potentially with resistant strains not detected initially or selected during treatment. Finally, patients might appreciate confirmation that they are cured and not contagious.\n\n【45】##### Harms of Tests of Cure\n\n【46】A positive test of cure indicates either treatment failure or reinfection. Cases of reinfection are usually retreated with the same antibiotic, whereas cases of treatment failure are typically treated with an alternative antibiotic. When the risk for long-term sequelae is high, the benefit of assuring eradication outweighs the harm of additional antibiotic pressure. When the risk for sequelae is low or uncertain, the potential harm of additional antibiotics might outweigh the benefit of confirming eradication. Despite some evidence that _M. genitalium_ can result in adverse sequelae in women, numerous outstanding questions about natural history remain. These questions include uncertainty over the frequency of upper reproductive tract sequelae, frequency of spontaneous clearance, clinical significance of asymptomatic infections, and transmission risk with low organism load that persists after treatment. Given these outstanding questions, the benefit of tests of cure is currently unknown. The 2021 CDC treatment guidelines only recommend tests of cure when resistance testing is not available and moxifloxacin cannot be used .\n\n【47】##### Which Antimicrobial Therapies Should Be Used against _M. genitalium?_\n\n【48】Azithromycin (1 g, 1 dose) was recommended over doxycycline (100 mg 2×/d for 7 days) for _M. genitalium_ in the 2015 treatment guidelines. Moxifloxacin was recommended for azithromycin treatment failures . The 2021 guidelines removed single-dose azithromycin from recommended therapies , primarily because of increasing antimicrobial resistance .\n\n【49】##### Doxycycline\n\n【50】Doxycycline is now the recommended first-line therapy for nongonococcal urethritis and cervicitis in the United States  and in many other countries globally. However, microbiologic cure rates of _M. genitalium_ infection after doxycycline treatment are low, ranging from 30% to 45% in 3 randomized trials of urethritis treatment . Little data are available for women, and no new trials have been performed. Estimates of doxycycline efficacy remain unchanged.\n\n【51】##### Azithromycin\n\n【52】The efficacy of azithromycin for _M. genitalium_ infection declined over time through 2015 , and more recent studies observed microbiologic cure rates as low as 52% . Selection for macrolide resistance typically occurs in ≈10%–12% of _M. genitalium_ –infected persons receiving single-dose azithromycin therapy , and this regimen is no longer recommended in most contexts . Some studies have evaluated extended-dose azithromycin (1.5 g given over 5 days) and a meta-analysis suggested less frequent selection of macrolide resistance with this regimen . However, a historical comparison reported no difference in efficacy or in selection for macrolide resistance between the 2 regimens . Presence of macrolide-resistance mutations in the 23S rRNA gene is strongly correlated with microbiologic treatment failure after azithromycin, and the global prevalence of these mutations increased from 10% before 2010 to 51% in 2016–2017 . In 6 US sexual health clinics, the prevalence of macrolide resistance was 64% (range 59.6%–75.9%) and higher in MSM (75.7%) .\n\n【53】##### Moxifloxacin\n\n【54】Although the efficacy of moxifloxacin for _M. genitalium_ was initially high (100%), it declined to 89% in studies during 2010–2017 . Sitafloxacin, a more potent fluoroquinolone, has somewhat higher cure rates but is not available in many countries including the United States. Multiple mutations in the _parC_ gene have been reported, but only some have been correlated with moxifloxacin treatment failure. The S83I mutation is most common and most strongly associated with treatment failure; it has been detected in 4.3% of infections globally . Additional mutations in the _gyrA gene_ (M95I and D99N) likely have a synergistic effect , but GyrA mutations are rarely assessed.\n\n【55】##### Resistance-Guided Therapy\n\n【56】Where diagnostic assays that can detect macrolide resistance mutations are available, sequential resistance-guided therapy is feasible and results in higher cure rates. Under sequential resistance-guided therapy, initial empiric treatment with doxycycline is followed by a second antibiotic (either azithromycin or moxifloxacin) on the basis of the pathogen’s susceptibility profile. Tetracycline resistance in _M. genitalium_ develops infrequently, and treatment with doxycycline reduces the organism load, subsequently limiting development of resistance to the second antibiotic . In Australia, where this approach was developed, macrolide-susceptible infections are treated with high-dose azithromycin (1 g, then 500 mg/d for 3 days) after the initial doxycycline regimen, whereas macrolide-resistant infections are treated with moxifloxacin (400 mg/d for 7 days) after the initial doxycycline regimen. In a setting where macrolide resistance was high (>70%) and quinolone resistance moderate (13%–22%), cure rates for doxycycline followed by azithromycin were 95.4% and 92.0% for doxycycline followed by moxifloxacin; selection for macrolide resistance occurred in only 4.6% . Given high macrolide resistance in the United States, the 2021 CDC treatment guidelines recommend resistance-guided therapy where possible and sequential treatment with doxycycline followed by moxifloxacin when resistance testing is not available .\n\n【57】##### PID Treatment\n\n【58】Recommended outpatient PID treatment in the 2015 CDC treatment guidelines consisted of antimicrobial drugs for empiric treatment of gonorrhea and chlamydia (a cephalosporin and doxycycline). This regimen had limited effectiveness against _M. genitalium_ ; cure rates were as low as 56% . More recent data demonstrated cure rates of ≈95% for PID treatment regimens that incorporated metronidazole, regardless of whether the regimen included azithromycin or moxifloxacin . Similarly, persistent cervical _M. genitalium_ infections were significantly less common among patients receiving a PID regimen with metronidazole than in patients not taking metronidazole . Metronidazole targets anaerobes and is thought to lack activity against _M. genitalium_ , suggesting that eradicating anaerobes might enhance _M. genitalium_ clearance. However, this hypothesis has not yet been evaluated.\n\n【59】##### Alternative Antibiotics\n\n【60】Minocycline (100 mg orally 2×/d for 14 days) and pristinamycin (1g 3×/d for 10 days) in combination with doxycycline (100 mg 2×/d for 10 days) have been used to treat _M. genitalium_ –infected patients when other antibiotics have failed. Minocycline cured 71% (25/35) patients in a sexual health clinic in Australia, and pristinamycin cured ≈75% (55/73) patients . Minocycline is widely available, but pristinamycin is not in many areas, including the United States. MICs for lefamulin are low , but no efficacy data for _M. genitalium_ infection in humans have been published. Gepotidacin also has low MICs and in vitro experiments suggest combination therapy with doxycycline might improve efficacy, but this possibility has not been tested in vivo . Longer durations of doxycycline have not been tested in comparative studies, but a recent study found 59% of patients with macrolide resistant infections experienced microbiologic cure after a 14-day regimen . Combination therapy using doxycycline and moxifloxacin yielded microbiologic cure rates equivalent to those observed using sequential therapy , suggesting efficacy but no advantage over resistance-guided therapy. Combination therapy using doxycycline and sitafloxacin has been more effective .\n\n【61】##### Sex Partner Management\n\n【62】Concordance of _M. genitalium_ infection was 40%–50% in heterosexual partnerships and 27% in MSM partnerships in a recent meta-analysis . The lower observed concordance in MSM may reflect inclusion of persons tested at only one anatomic site. When both partners were tested at the urethra and rectum, concordance was 42% . This result suggests that partners of _M. genitalium_ –positive patients should be tested and treated to reduce the risk for reinfection. This strategy is recommended in guidelines from Europe, the United Kingdom, and Canada , although testing of partners is not specified in Canada. Australia guidelines also recommend contact tracing for heterosexuals and ongoing partners of symptomatic MSM . No evidence suggests that partner infections have differential antimicrobial susceptibility. The 2021 CDC treatment guidelines recommend testing and treating infected partners with the same antibiotic provided to the index patient .\n\n【63】### Conclusion\n\n【64】_M. genitalium_ is now an established sexually transmitted infection that poses substantial challenges for developing optimal testing and treatment approaches. Antimicrobial resistance has grown rapidly, and untreatable infections have begun to appear. New antimicrobial drugs and better antimicrobial stewardship of existing antibiotics are urgently needed. Resistance-guided therapy is a vital tool to reduce antibiotic pressure and maintain the efficacy of existing antimicrobials. However, only some diagnostic tests incorporate the detection of antimicrobial resistance, limiting our ability to use this tool. Concerns about the rapid spread of resistance have led to recommendations to limit testing and treatment for _M. genitalium_ infections to patients with symptoms. Rigorous and adequately powered clinical trials of screening and treatment of _M. genitalium_ in women are critically needed. Without a better understanding of the natural history of infection, particularly the risk for sequelae, the benefits and harms of testing and treatment approaches cannot be truly weighed.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f198a213-c3c0-490e-8eba-91e8e837c05d", "title": "Syphilis in Maria Salviati (1499–1543), Wife of Giovanni de’ Medici of the Black Bands", "text": "【0】Syphilis in Maria Salviati (1499–1543), Wife of Giovanni de’ Medici of the Black Bands\nSyphilis today is a reemerging infectious disease that affects not only the developing countries but also the Western world. In recent years, a new increase has occurred in the incidence of sexually transmitted diseases, among which syphilis is one of the most common . Especially in the United States, the rates of primary and secondary syphilis have increased since 2000–2001. A total of 27,814 syphilis cases were reported in 2016 . During 2015–2016, the US syphilis rate increased by 17.6%, reaching 8.7 cases/100,000 population, the highest rate reported since 1993 . Europe experienced a similar trend; in 2016, a total of 29,365 confirmed syphilis cases were reported in 28 countries, a rate of 6.1 cases/100,000 population. The highest rates (cases/100,000 population) in Europe were observed in the United Kingdom (9.9), Malta (9.2), Iceland (9.0), and Germany (8.7) .\n\n【1】Venereal syphilis is a treponematosis caused by the bacterium _Treponema pallidum_ subsp. _pallidum_ , the most widespread disease among the 4 treponematoses. Pinta ( _T. carateum_ infection) is spread only in tropical areas of the Americas, yaws ( _T. pallidum_ subsp. _pertenue_ infection) in humid tropical and subtropical regions, and bejel ( _T. pallidum_ subsp. _endemicum_ infection) in arid-temperate and subtropical rural areas. All these diseases involve the human bone, with the exception of pinta. Three clinical stages are typical of venereal syphilis: the primary stage is a painless lesion (chancre) on the genitals, which heals in 2–6 weeks; some months later, the secondary stage is characterized by a widespread skin rash; and several years later, the tertiary stage involves different organs, including the skeleton .\n\n【2】Venereal syphilis first emerged in Europe at the end of the 15th century, as a result of the sexual and social behavior of the time . Soon after the disease’s arrival, its sexually transmitted nature was recognized, becoming a mark of immoral behavior ; however, the social implication of syphilis was not the same for men and women, especially in the aristocracy . In fact, the sexual conduct of the noblemen and the possible infectious diseases that followed were not subject to moral censorship; instead, there was severe moral judgement for venereal diseases in noblewomen . Some paleopathological cases have indicated the impact of syphilis on the aristocratic classes of the Renaissance . Paleopathology offers a source for increasing the diagnosis of infection in the past and for understanding the social and cultural impact of infectious diseases in previous populations. The models obtained can be compared with what happens today with emerging and reemerging diseases and can serve to refine the systems of prevention and fight against future infection outbreaks . The study of the skeletal remains of Maria Salviati , wife of Giovanni de’ Medici, which revealed lesions typical of third-stage syphilis, has enabled us to understand the dynamics of the infection in one of the most famous families of the Renaissance and to examine the perception of the illness in 16th century Europe.\n\n【3】### Syphilis in the 16th Century\n\n【4】After the 1494 invasion of Italy by the troops of Charles VIII, King of France , venereal syphilis had a pandemic spread in Italy and in Europe . The origin of the disease remains one of the greatest issues in the history of medicine and is still discussed by scholars. One theory suggests the disease originated in the Americas and was introduced by Columbus’ crew returning to Europe from the New World in 1493. According to a second theory, syphilis previously existed in the Old World but went unrecognized until the late 15th century, when there was increased prevalence and virulence of the disease . In the past few years, further paleopathologic evidence has indicated the presence of the disease in Europe before 1492 . However, the transmission at the end of the 15th century is undeniable and can be explained in the light of the wider sociocultural context of the period .\n\n【5】From the late 15th to mid-16th centuries, Italy was a great field of war and an ideal social environment for the spread of the disease. In the Renaissance, before the advent of the Catholic Counter-Reformation, Italy experienced an increase in the volume of trade exchanges, new contacts between populations, migration from other countries, and, above all, a time of greater sexual liberty . The activity of prostitution among the troops and the civilians in the towns, as well as the opportunities for extramarital sex created by the permanence of armies, generated a perfect basis for the spread of syphilis . Because of its sexual connotation, the disease embodied the concept of divine ill-punishment, which, as an archetype, pervaded the popular religious sensibility of European populations of the early modern age . The early pandemic and violent phase of syphilis, before the classical chronicization to three stages, had an impressive impact on European society .\n\n【6】The first physician observing syphilis was Alessandro Benedetti, field doctor of the Italian confederate army fighting against the French during the battle of Fornovo in 1495 . Girolamo Fracastoro  successfully coined the name of the disease in his famous poem “Syphilis sive morbus gallicus \\[Syphilis or the French disease\\]” . Contemporary physicians described the manifestation of the disease, consisting in the appearance of an ulceration on the penis, followed by pustules and sores all over the face and body with joint pain and pruritus. The doctors quickly acknowledged that the infection had been transmitted through sexual intercourse . After the most aggressive first phase of the “new” disease, syphilis quickly changed from an acute and debilitating disease into a less severe chronic infection, probably because the selection of the less virulent strains represented an evolutionary advantage for the pathogen .\n\n【7】Syphilis was rife in all social classes and affected many members of the aristocracy. Many noblemen undertook military careers as captains of mercenary troops, which typically involved extramarital affairs, not only with regular lovers but also, and frequently, with prostitutes . Famous are the cases of Cesare Borgia , son of Pope Alexander VI, who had to wear a leather mask covering half of his face, which had been disfigured by syphilis in his later years , and of Francesco II Gonzaga , Marquis of Mantua, who had a form of tertiary syphilis . Evidence that the disease was widespread in the 16th century aristocratic classes is also demonstrated by paleopathology. The cases of Maria of Aragon, Marquise of Vasto  , Vespasiano Gonzaga, Duke of Sabbioneta  , and Cardinal Giulio Della Rovere  ( _12_ ) are some of the most famous Renaissance figures for whom syphilis was diagnosed.\n\n【8】### Brief Biography and Nosography of Maria Salviati\n\n【9】Maria Salviati, daughter of Lucrezia de’ Medici and Jacopo Salviati and granddaughter of Lorenzo the Magnificent, was born in Florence in 1499. Her marriage to Giovanni de’ Medici  took place in 1516. Giovanni died of gangrene and septicemia  on November 30, 1526, complications resulting from an injury and amputation of his right leg after the Battle of Governolo, near Mantua, leaving his wife a widow at the age of 27. Maria never remarried. Cosimo, the son of Maria and Giovanni, was called to govern Florence after the death of Duke Alessandro de’ Medici , giving rise to the Grand Ducal Medici branch, which ruled Tuscany until 1737.\n\n【10】Archival research by Gaetano Pieraccini was able to reveal important information about Maria Salviati, with particular regard to the last years of her life . Until 1540, she enjoyed good health, except for a brief episode of fever of unknown origin recorded in 1517. In the last 3 years of her life, from May 1541 to her death on December 29, 1543, many symptoms of severe illness were described in letters sent by Andrea Pasquali, the court physician, to Duke Cosimo, including abundant recurring proctorrhagias (bleeding from the rectum from ½ to 3 libras of blood \\[i.e. 180 g to ≈1 L\\]), rectal and perianal ulcers, headaches, and abdominal colic. The letters also report “chronic weakness…, shortness of breath, frequent lipothymic episodes, severe syncopes, cold extremities, vomit and agitation…” and note “the pulse was deeply reduced, the frequency increased” . The symptoms were certainly the expression of a severe anemia caused by chronic leakages of blood.\n\n【11】The portraits of Maria Salviati clearly show the significant changes that occurred in her aspect, marking the progress in terms both of age and of illness. A portrait by Jacopo Pontormo , painted in 1537 and preserved at the Uffizi Museum, shows Maria as a beautiful lady, still young; but only 6 years later, in a portrait by Bronzino , she appears as a very old woman. Rather than to the artistic choices of the painter, this transformation seems to be strongly related to the accelerating physical decay of Maria, which was probably connected to the illness in the last years of her life.\n\n【12】### Historical, Archeological, and Taphonomic Background\n\n【13】Almost all the bodies of the Medici, and also that of Maria Salviati, were embalmed before burial , but it was not until the mid-19th century that they were given a definitive grave location. Until the 19th century, the bodies of the Medici of the Grand Duchy dynasty were preserved in wooden coffins inside the 2 sacristies of the Basilica of San Lorenzo in Florence. During 1857–58, the Gran Duke of Tuscany Leopold II of Lorrain arranged the bodies of the Medici in the crypt of the Medici Chapel to give a proper and dignified burial place to the founders of the Grand Duchy of Tuscany . The body of Maria Salviati, identified in 1857 thanks to the presence of a copper epigraph on the coffin , was deposed, together with that of her husband Giovanni “of the Black Bands,” in a tomb at the center of the crypt floor . We have a description of this event given by Luigi Passerini-Rilli, director of the State Archive of Florence and responsible for the recognition of the bodies of the Grand Dukes : “The body, although reduced to almost a skeleton in the face, was however very well preserved in the other parts. The head lay on two bricks. The clothing that covered it resembled that of a nun, i.e. a black cloth, but eaten by the moths: the leftovers of the wimple were still discernible, though the veil that at first covered the head, was worn.”\n\n【14】In 1946, Gaetano Pieraccini and the anthropologist Giuseppe Genna conducted an exhumation ; they heavily manipulated the skeletal remains of Maria with the removal of residual soft tissues before reburial in a small zinc coffin . On this occasion, they made a plaster cast of the skull, which is now in the Museum of Anthropology of Florence. In November 2012 the last exhumation took place, under the management of the Division of Paleopathology of the University of Pisa, during some architectural checks of the stability of the floor of the Chapel. The paleopathologists found that the remains of Maria were in excellent state of preservation, unconnected in a small zinc coffin bearing an epigraph with her name .\n\n【15】### Anthropologic and Paleopathologic Study\n\n【16】We examined the nearly complete skeleton of Maria Salviati macroscopically  and performed radiographic and computed tomography (CT) scans. We compared the skull bone recovered during exhumation with the skull cast, and they showed the same lesions. The anthropologic study of the skeleton revealed a female individual , 40–45 years of age , with a stature of 1.56 m . Maria had severe periodontal disease, as evidenced by the resorption of alveolar edges and an abscess at the buccal portion of the third right maxillary molar. Nonpenetrating caries affected 10 teeth, of which 4 were mandibular and 6 maxillary. The poor dental health of Maria is consistent with that of the other members of the Medici family . We detected many lytic lesions on the skull. Two circular ectocranial depressions are visible on the frontal bone, on the glabella and above the left supraorbital ridge. They are irregularly elliptical in shape, measuring ≈1 × 0.7 cm and 0.5 × 0.4 cm, respectively, with a central destructive focus and a reactive compact bone formation on the margins . These frontal bone lesions are consistent with 2 destructive osteolytic inflammatory processes, in advanced reparative phase, as clearly revealed by CT examination . Furthermore, the cranial vault on the parietal bones shows several osteolytic lesions in the form of circumvallate depressed areas with fine scar lines radiating inside the shallow depressions . CT examination confirms the lytic and reparative nature of the lesions, which are morphologically similar to internodular stellate depressions. We observed no other lesions in the postcranial bones macroscopically, by radiograph, or by CT scan. The presence of strong bone reaction excludes metastatic osteolytic carcinoma, multiple myeloma, tuberculosis, and fungal bone infections . The presence of superficial circumvallated cavitations with radial scars is pathognomonic of cranial syphilis (caries sicca) .\n\n【17】### Discussion\n\n【18】The combination of crater-like lesions, such as circumvallate cavitations on the frontal bone (phase 4 in the Caries Sicca sequence depicted in Hackett ), and circumvallate cavitations with radial scars on the parietal bones (phase 5 in the Caries Sicca sequence ) is pathognomonic for tertiary syphilis. Pathography attests that Maria had abundant recurring proctorrhagias. This symptom strongly suggests tertiary syphilis with cranial and possible colorectal localization. Apart from proctorrhagias, the other symptoms, such as frequent fever and headache, abdominal colic, rectal and perianal ulcers, reported by the written sources are very compatible with tertiary syphilis  but were not attributed to syphilis by the physicians of that time nor by the nosographic scholars in recent times . Nevertheless, rectal syphilis is a rare disease that usually shows proctitis with perianal ulcers but lacks pathognomonic clinical symptoms, making diagnosis difficult. In clinical medicine, anal syphilis could be easily misdiagnosed as cancer or advanced stage hemorrhoids, upholding the reputation of syphilis as the “great mimicker” .\n\n【19】One hypothesis is that the contemporary physicians might have correctly diagnosed and recognized Maria Salviati’s disease but concealed the sexual component of the infection. Another hypothesis is that Maria Salviati, who never allowed the physicians to inspect her genitals , might have hidden the symptoms of her disease out of modesty. A further explanation, based on political reasons, is that the mother of Duke Cosimo I could not appear to be affected by venereal syphilis, to avoid corrupting the image of the Medici family that Cosimo was laboriously trying to promote among the royal rank. The fact that in her last years of life Maria was always portrayed with a veil might indicate her intention to hide the luetic skin lesions.\n\n【20】Syphilis is likely to have been more widespread among the noblewomen of the Renaissance than is attested by the written sources. Paleopathology has in some cases revealed some hidden illnesses of the Italian noblewomen, as in the case of Isabella of Aragon, Duchess of Milan , and Maria of Aragon , Marquise of Vasto and wife of the governor of Milan Alfonso of Avalos . Isabella was probably affected by syphilis, despite the absence of bone lesions on her skeletal remains. The syphilitic infection was diagnosed indirectly, on the basis of the paleopathologic analyses performed on her teeth . On the buccal surfaces of the teeth, Isabella showed a strong abrasion caused by pumice powder and toothpicks of cuttlebone that she used to remove the blackish patina produced by her mercurial therapy. In fact, energy-dispersive spectroscopic analysis of the dark material detected a massive presence of mercury, largely employed in the treatment of syphilis since the early 16th century in the form of salves, fumigations, and ointments . The care given to Isabella of Aragon clearly demonstrates her willingness to erase the evident traces of chronic mercury intoxication caused by the antisyphilitic therapy. However, no references to syphilis are reported in the written documents about the life of Isabella. In the artificial mummy of Maria of Aragon, the histologic, immunohistochemical, and ultrastructural study of a cutaneous ulcer of the left arm led to the direct identification of _Treponema pallidum_ and the diagnosis of tertiary venereal syphilis . The biographic sources report that Maria of Aragon periodically spent time at the Agnano Baths, near Naples , probably to treat a skin disease with the sulfuric waters. However, in the written sources, there is no mention of any possible syphilitic infection affecting the noblewoman, who was famous at that time for her beauty and cultural refinement.\n\n【21】It is difficult to speculate on how Maria Salviati contracted the disease, but she is likely to have been infected by her husband Giovanni before his death in 1526, and more probably after the birth of her son Cosimo (June 12, 1519); indeed, the historical sources do not reveal any details about a possible infection of the child, nor do the skeletal remains of the first Grand Duke of Tuscany show any signs of congenital syphilis . The lifestyle of Giovanni “of the Black Bands”  was characterized by intense sexual extramarital affairs, as witnessed by many documents of the time preserved in the archives of Florence . On October 20, 1521, Giovanni wrote to his treasurer and lieutenant Francesco Albizi to dispose of war supplies during the military campaign against the French army in Northern Italy: “send me that Greek whore I left in Viterbo” . On September 22, 1522, during some military actions in the Marche region on behalf of the Pope, Giovanni wrote again to Francesco Albizi, ordering him to kidnap “Lucrezia, courtesan of Rome” and to bring her to him by force . A considerable series of names of prostitutes frequented by Giovanni are cited in his correspondence: Flora from Padua, Nicolosa “the painted Jewess,” Camilla Orsini, Giulia, Angelica “the Venetian,” Lorenzina “the Greek slave,” Baccia from Rome, Lucrezia nicknamed “Matrema non vole \\[Mom does not want\\],” and Paolina . The skeleton of Giovanni de’ Medici does not reveal any lesion of syphilis , probably because he died at the age of 28 years, before the development of the tertiary stage of the disease.\n\n【22】Extramarital affairs were common among aristocrats ; therefore, syphilis could be considered a disease characterizing the pathocenosis of high social classes. Noblewomen were at risk for contracting sexually transmitted diseases caused by the lifestyle of their husbands, who led an unregulated sexual life characterized by occasional relationships with prostitutes. It was already a well-known fact at the time that syphilis was transmitted sexually ; therefore, some preventive practices were undertaken by the noblewomen, wives, or lovers of the men affected by the disease. Just like Isabella d’Este, wife of Francesco II Gonzaga, Marquise of Mantua, these women sometimes refused to have sexual intercourse with their partners . Although the disease had no consequences on the reputation of the men, whose sexual conduct was not subject to moral censorship, the situation was different for the women, who tried to hide their sexually transmitted infections .\n\n【23】### Conclusions\n\n【24】A diagnosis of syphilis for Maria Salviati does not emerge from the historical sources, although the symptoms that manifested in the last years of her life are compatible with a colorectal localization and severe hemorrhages caused by syphilitic infection. In her final years, Maria had a very withdrawn life, possibly to conceal the signs of the illness and certainly for the social complications caused by her recurrent anal hemorrhages . However, we have no reports that she was marginalized from the ducal court; instead, she was held in high regard as the mother of the reigning duke. In the famous portrait painted by Bronzino during 1542–1543 , she appears veiled and in widow’s clothes, as if to hide the signs of illness.\n\n【25】The paleopathologic study of Maria Salviati reveals osteolytic and reparative lesions on the skull, which are pathognomonic for syphilis. The diagnosis of syphilis for Maria Salviati enables us to directly observe the gender-based consequences of a pathology to which the Renaissance noblewomen were subjected as a consequence of the sexual conduct of their husbands. The real nature of the disease might have been kept secret at the time for reasons of political opportunity and privacy. A disease that was not a reason of particular shame for sovereigns, princes, and gentlemen, that did not affect their political role and leadership, and was therefore unnecessary to hide, was instead jealously concealed by noblewomen as a “secret illness” that often did not seep outside the private apartments and perhaps did not even reach the attention of the court physicians. This attitude reveals a disparity of perception and of mentality, symptomatic of gender discrimination that was well implanted in the heart of Renaissance society .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "41044920-41db-4839-9134-c71875652e15", "title": "Operationalizing Surveillance of Chronic Disease Self-Management and Self-Management Support", "text": "【0】Operationalizing Surveillance of Chronic Disease Self-Management and Self-Management Support\nAbstract\n\n【1】Sixty percent of US adults have at least one chronic condition, and more than 40% have multiple conditions. Self-management (SM) by the individual, along with self-management support (SMS) by others, are nonpharmacological interventions with few side effects that are critical to optimal chronic disease control. Ruiz and colleagues laid the conceptual groundwork for surveillance of SM/SMS at 5 socio-ecological levels (individual, health system, community, policy, and media). We extend that work by proposing operationalized indicators at each socio-ecologic level and suggest that the indicators be embedded in existing surveillance systems at national, state, and local levels. Without a robust measurement system at the population level, we will not know how far we have to go or how far we have come in making SM and SMS a reality. The data can also be used to facilitate planning and service delivery strategies, monitor temporal changes, and stimulate SM/SMS–related research.\n\n【2】Why Is Surveillance of Chronic Disease Self-Management and Self-Management Support Important?\n\n【3】Sixty percent of US adults have at least one chronic condition, and more than 40% have 2 or more chronic conditions . Individual self-management (SM) of chronic conditions, also called self-care, is central to effective disease management. The Chronic Care Model identifies productive interactions between patients with the knowledge, skill, and motivation to participate in their care (also known as “activated patients”) and their proactive practice team as critical to achieving positive functional and clinical outcomes . Self-management support (SMS), or efforts to support individuals in their SM, is one of the Chronic Care Model’s pillars of quality improvement needed to effectively manage chronic conditions in health care . A robust measurement system for measuring SM/SMS at the population level could be beneficial for assessing whether the nation is making progress in SM/SMS. The data could be used to facilitate planning and service delivery, monitor temporal changes, and stimulate SM/SMS–related research.\n\n【4】Ruiz and colleagues identified concepts that could be developed into SM/SMS surveillance indicators . These concepts spanned 5 socio-ecological levels (individual, health system, community, policy, and media), mirroring Frieden’s pyramid of public health impact  and reflecting the multiple dimensions of SMS . We attempt to operationalize Ruiz et al’s concepts  and, where they were not feasible for surveillance, we propose new indicators that can be used to assess status, trends, and disparities in SM/SMS at the local, state, and national levels.\n\n【5】Moving From the Forest to the Trees — Operationalizing Surveillance of Self-Management and Self-Management Support\n\n【6】We adopted Ruiz et al’s definitions of SM/SMS . SM consists of the tasks people must do to live well with chronic conditions and to pursue the life they desire . SMS is the systematic provision of education and supportive interventions by health care providers and others to strengthen patients’ skills and confidence in managing their health problems (ie, things done by others to support individuals in their SM efforts) .\n\n【7】We sought surveillance indicators that were sensitive to change, were actionable, had easily definable numerators and denominators, had face validity, were low-cost, required minimal effort to collect and analyze data, and — most importantly — had an existing system available to collect the data. If following a recommendation from Ruiz et al  was not feasible per these criteria, we explored other possible indicators to identify ones with the desired characteristics.\n\n【8】To standardize surveillance, we adopted the definitions in the strategic framework for multiple chronic conditions . We debated attempting surveillance of a single chronic disease or all chronic diseases. Should we select a single condition (like diabetes) and ascertain SM/SMS for that “sentinel” as an indicator of what is happening in other conditions? Although single conditions are likely to have specific SM activities (eg, a person with diabetes testing their blood sugar), our notion of SM addresses cross-cutting dimensions rather than disease-specific activities. Thus, we concluded that we needed to measure SM/SMS dimensions relevant across all chronic diseases, because focusing on a single condition might not be representative.\n\n【9】For each indicator proposed by Ruiz et al, we attempted to craft a question and specify the potential data source and numerator and denominator necessary for calculation of the indicator. As we attempted to operationalize the Ruiz et al concepts , we encountered consistent feasibility problems. First, we had difficulty defining a “case” of SM or SMS (the numerator) and the relevant denominator. We also lacked existing data-collection systems. The remainder of the article summarizes our deliberations on the indicators we discarded  and our recommendations for indicators at each socio-ecological level .\n\n【10】What Won’t and What Might Work for Surveillance of Self-Management and Self-Management Support\n\n【11】### Self-management at the individual level\n\n【12】SM at the individual level involves a variety of activities, including adopting healthy behaviors (eg, not smoking, appropriate diet, taking medications), action planning, self-monitoring, coping with emotions, managing disability, and navigating the health care system . SM objectives (disease control, symptom control, prevention of deterioration) vary by condition . In an attempt to identify core cross-cutting elements of SM, attendees at a 2014 Self-Management Alliance meeting were asked to identify observable actions that would indicate that a person was actively and effectively self-managing his or her chronic condition. The divergent written answers indicated that SM is an abstract concept defined in many ways, including many behaviors that are hard to differentiate from general wellness behavior and that are not necessarily reflective of chronic condition SM (eg, resolutions to lose weight or exercise). The most frequently reported observation reflected taking action to manage the condition. We reasoned that other mentioned items like goal-setting and action-planning are skills taught to help transform someone into a self-manager, that is, more a means to the end rather than the end itself. We felt that the essence of SM is being able to make wise decisions and recognize and respond to changing circumstances, adapting to the changes in the disease’s trend and tempo and to the complicated realities of life with chronic disease . Taking action applies to any chronic disease and encapsulates a range of activities to respond to symptoms or situations as they arise. Action reflects the embodiment and culmination of translating education, plans, and counseling in daily life . The general idea of taking action to improve a chronic condition or avoid making it worse offers a comprehensive, action-oriented indicator of SM.\n\n【13】Because many people have multiple chronic conditions, the relative importance of the conditions may shift over time , and because SM may differ across conditions , we suggest first identifying the condition that is of most concern to consistently anchor responses. We also recommend focusing on the past 3 months to lessen recall problems . Such questions could be used in any survey that ascertains whether respondents have a chronic disease.\n\n【14】Several of the recommendations of Ruiz et al  related to self-management education, a common cross-cutting strategy to facilitate transformation into a self-manager . As a secondary individual-level indicator, we suggest including a question about taking a course or class .\n\n【15】### Self-management support at the health system level\n\n【16】The most commonly recognized site for provision of SMS is within health care systems; the Institute of Medicine definition of SMS specifies only health care providers . SMS at this level can take the form of motivational interactions (eg, motivational interviewing), collaborative care planning strategies, tools to enhance the patient–provider interaction, self-management education programs, and referrals to community SMS resources . Goal-setting and action-planning feature prominently in many primary care quality improvement projects  and allow the clinician to help create the self-management plan .\n\n【17】Action planning, or collaborating with patients to develop an SM plan, is a key means of SMS and a key activity in most health care provider–based SMS strategies . Privacy concerns make it difficult to ascertain what happens in clinical encounters. Consequently, we propose an indicator to capture from health care providers how frequently action planning occurred in the past 7 days. A parallel indicator is available from the National Committee for Quality Assurance’s 2017 standards for recognition as a patient-centered medical home, which includes an elective element on developing a self-management plan . Because providing SMS is not a traditional part of health care provider training , we also recommend an indicator on specialized training in helping patients develop an SM plan . These questions could be added to existing surveys of physicians or other health care providers.\n\n【18】### Self-management support at the community level\n\n【19】Community resources are one of the pillars of quality improvement in the original  and in the Expanded Chronic Care Model . SMS at this level could be reflected by the availability, accessibility, and awareness of SMS programs or by community structures (coalitions, parks, greenspaces, access to appropriate foods) that support SM . Clinical–community linkages could be reflected in referrals to community programs.\n\n【20】We encountered difficulties enumerating SMS programs and supportive community structures . We considered identifying a sentinel SM program (eg, diabetes SM programs  or the Stanford Chronic Disease Self Management Program ) and using number of programs listed on a centralized listing of available programs (ie, a program locator) as the indicator. However, as with using SM practices for only a single disease, we were concerned with the representativeness of one program.\n\n【21】To examine geography-based community availability measures, we explored the use of web-based search services (eg, Google Maps) to search for nearby SMS programs. However, searches for “self-management” or “managing X condition” returned little of value. Until there is a roster of SMS programs and robust program locators, developing an indicator based on the geography of SMS programs is not feasible.\n\n【22】We concluded that the most feasible indicator of community-level SMS was the availability of SMS programs in the community. Directly asking people with chronic disease if they were aware of a course appeared to be the most feasible way to measure community-level SMS, because it implicitly integrates awareness and availability. This question, used in combination with a question about taking a course , could capture both access and uptake.\n\n【23】### Self-management support at the policy level\n\n【24】Policy can be an effective facilitator of SM/SMS at all levels of the socio-ecological model and is highlighted in the Expanded Chronic Care Model . Many major changes in public health (eg, decreased smoking, increased use of seat belts) came from policy interventions. Policy can help reduce barriers or enhance facilitators such as SMS program availability or financing. Ruiz et al  suggested a focus on health care systems and health insurance policies; however, we identified multiple issues that rendered them infeasible . Instead, we identified alternative approaches in health care professional training and certification, professional practice guidelines, and federal rules and grants.\n\n【25】#### Professional training and certification\n\n【26】Health professionals’ promotion of SM can influence patients’ use. However, most providers receive no formal training in SM/SMS . A content review of curriculum standards from the American Association of Colleges of Nursing  indicated no SMS-related standards. A survey of medical school curricula did not list SMS specifically, although some curricula included counseling for behavior change, which could be a component of SMS . Continuing medical education (CME) requirements for physician and nurse relicensure, on the other hand, is a potentially useful metric to follow (baseline = zero), given the ease in searching requirements and variation by state. Although some states dictate CME in certain topics (eg, pain management), none mention SMS .\n\n【27】#### Professional practice guidelines\n\n【28】Treatment guidelines can reflect the integration of SMS into routine care delivery. PubMed provides a standardized searchable source for peer-reviewed literature, where physicians likely look for guidelines. Using the strategy shown in Table 2, we found mentions of SMS in practice guidelines for diabetes (n = 28), asthma (n = 6), arthritis (n = 4), chronic obstructive pulmonary disease (n = 1), and HIV (n = 1).\n\n【29】#### US government policy as reflected in the _Federal Register_  \n\n【30】We found no ongoing inventory of federal activities or funding on SMS . Monitoring issuance of federal government rules on SM/SMS over time can reflect policy changes. The _Federal Register_ has a searchable database containing rules, notices, and proposed federal rules. Two searches  of the database identified modest variability in numbers by year, but numbers were very low .\n\n【31】### Self-management support at the media level\n\n【32】Media coverage of SM/SMS could help set social norms and influence people with chronic disease to explore SM/SMS . We identified several impediments to exposure-based indicators proposed by Ruiz et al   but found that measuring volume of coverage was possible. Several services routinely monitor media streams (eg, newspapers, wire feeds). We searched ProQuest (ProQuest LLC) for English-language articles containing “self-management” and “condition” (specifying the list of chronic conditions ) from 2010–2016 in newspapers, wire feeds, or magazines. On the basis those results , we concluded that using such a database with a consistent search strategy  can reflect the volume of SM–related content.\n\n【33】Advancing Surveillance of Self-Management and Self-Management Support\n\n【34】We propose operationalized indicators for surveillance of SM/SMS at 5 levels of the socio-ecological model . Measuring both SM and SMS is important, as is measuring SMS at multiple levels. SMS indicators could be considered process indicators for the eventual outcome of SM. If society effectively promotes SMS at multiple levels, more people with chronic disease may actively self-manage.\n\n【35】Identifying indicators that are a reasonable reflection of SM/SMS at each level was challenging . A recurring constraint was our assumption that resources for such surveillance would be minimal. Thus, we limited ourselves to using existing surveys, systems, and databases rather than creating new surveillance systems as would have been required by the indicators proposed by Ruiz et al . Our proposed indicators provide only small glimpses of the big picture. Nevertheless, we believe they can provide useful information on SM/SMS status. Survey-dependent indicators will require cognitive testing.\n\n【36】Our proposed indicators of individual-level SM and community-level SMS rely on surveys of individuals with chronic diseases, such as the Behavioral Risk Factor Surveillance System. Local communities, health systems, or other organizations could gather data to reflect SM/SMS within their nexus of control. Surveillance of community-based SMS programs would be strengthened by development of consistent, accurate program-locator services, which could also facilitate program participation.\n\n【37】Two of our proposed indicators at the health system level (co-development of an SM plan and specialized training in SMS) require surveys of health care providers. Local health systems could survey their affiliated providers to assess SMS in their systems. One way to both strengthen surveillance and facilitate SMS delivery at the health system level would be the creation of Current Procedural Terminology codes within the Healthcare Common Procedure Coding System specific to chronic disease SMS .\n\n【38】We were unable to identify a strong surveillance indicator of SMS at the policy level. However, the indicators we proposed are promising because they are searchable and likely to detect change if consistent search strategies are used. Finally, we suggest surveillance of media coverage that includes mention of SM, and we identified a potential database for national-level surveillance. Similar media monitoring could be conducted at the state or local level by adding geographic restrictions to the search.\n\n【39】Consistent with the idea that “what gets measured gets done,” surveillance of SM/SMS can serve as both a progress report and a motivator for expansion . Gathering data directly from people with chronic conditions is essential to national and state surveillance to capture SM and SMS provided beyond the health care system. Inclusion of the proposed indicators into surveys like the National Health Interview Survey, Behavioral Risk Factor Surveillance System, and Medicare Current Beneficiary Survey and adding provider-based indicators to provider surveys would advance SM/SMS surveillance. Achieving the improved functional and clinical outcomes predicted by the Chronic Care Model  is unlikely without leadership and investment in promoting SM/SMS. Without surveillance, we will not know how far we have come or how far we have to go in making SM/SMS an integral part of health and health care.\n\n【40】Tables\n------\n\n【41】 Table 1. Surveillance Concepts and Associated Measurement Challenges in Operationalizing Surveillance of Self-Management and Self-Management Support\n\n| Socio-Ecological Level and Concept Indicator | Measurement Challenges |\n| --- | --- |\n| **Individual Level** | **Individual Level** |\n| Broad issues with concepts proposed | All measures involve obtaining data from individuals with chronic disease via survey.Need to determine whether focus should be on 1) intention or action; 2) optimal SM versus some SM versus no SM; 3) whether was done within a recent period or ever done; and 4) SM for all conditions, each condition, or just one condition (SM practices might vary by condition). |\n| Proportion and characteristics of individuals who can articulate setting a health-related SM goal and related action plans  | Requires ascertaining whether the person has a goal and an action plan. What if a person had one but not the other?Articulating a goal is not the same as doing (intention vs action).Uncertain which individual characteristics to select and why. |\n| Proportion of individuals attending a series of SM education sessions in a health care setting that help solve health-related problems  | Uncertain whether a series of sessions is necessary or whether one session is enough.Unclear why sessions are restricted to a health care setting. |\n| Proportion of individuals who report receiving support for or assistance with their SM goals in the past year  | Must first establish whether the person has a goal(s).Unclear what constitutes support or assistance.Unclear whether once is enough or if it must it be ongoing.Long recall period. |\n| Proportion of individuals who report an improvement in their chronic disease  | Unclear how to link the improvement to SM. |\n| Proportion of individuals with chronic disease who feel confident they can manage their health | Unclear how managing health differs from managing a chronic condition. Do general wellness behaviors qualify?Should confidence be measured with a binary or scalar response option? If scalar, what should be the cut point? |\n| Proportion of individuals with chronic disease who report they have solved a problem related to their health in the past year | Unclear whether solving a problem related to health is different from solving a problem related to a chronic disease. Doesn’t necessarily equate to self-managing a chronic condition.Impact of solving just one problem unclear.Long recall period. |\n| Proportion of individuals with chronic disease who regularly self-monitor their chronic disease symptoms | Unclear how to define “regularly.”Meaning of “self-monitoring” is unclear; for hypertension it may mean taking blood pressure, for diabetes it may mean measuring blood sugar, for conditions associated with pain it may mean quantifying pain on a scale.Does some sort of record or log need to be made?Unclear whether monitoring is the same as managing. |\n| **Health System Level** | **Health System Level** |\n| Broad issues with concepts proposed for this level | Confidentiality and access issues associated with accessing medical records.Ongoing changes in electronic health records and absence of standardized systems.Unclear how to define or enumerate a health care system.No existing data collection system for measuring linkage to community resources, individual interaction with clinicians, and health care workforce training and monitoring. |\n| Proportion of systems that incorporate SM support as part of their quality improvement plan  | Likely proprietary data.Unclear how to define SMS in this context. |\n| Proportion of individual practices that track patient SM goal-setting and goal attainment and progress in the medical record  | Enumerating all individual practices is not feasible.Unclear why group practices excluded.Unclear what to look for in a patient health record and whether it needs to be updated at each visit.Unclear what constitutes tracking — recorded at every visit, some visits? At how many? |\n| Proportion of accredited PCMH delivering SM support at least 50% of the time  | High percentage of providers choose not be a part of the PCMH quality improvement process.PCMH standards were established in 2011 and revised in 2014 and 2017; SM elements have changed in each revision. The 2017 standards contain 2 core elements (Team-based Care-9, and Performance Measurement-14) and one elective element (Knowing and Managing-22) that are too inclusive to assure that they reflect SMS. |\n| Proportion of health care systems that link to community resources offering SM support (eg, direct referral to programs, follow-up to see if an individual attended)  | What if resources were offered by the system and there was no need for a community link?Unclear what would qualify as a link or which community resources it should be linked to.Unclear how data would be ascertained. |\n| Proportion of individuals who engaged in a process with a health system that significantly changed their ability to manage their health problem  | Implies a survey of patients in a system.Unclear what individuals would be asked in order to ascertain whether change was significant.Unclear what is considered a process. |\n| Proportion of health care professionals who received training on working with patients to set and monitor self-management goals  | Unclear what type of training should be included or whether it would differ by specialty.One trained person in a practice may be enough.Unclear denominator — which health care professionals? |\n| Proportion of PCP practices that have provision of SM support written into staff job descriptions  | Unclear how to enumerate PCP practices.Unclear denominator — PCPs? Practices?Unclear how job descriptions would be gathered or if such information would be in a job description.Job descriptions may not reflect actual practice.Access and confidentiality issues associated with obtaining job descriptions. |\n| **Community Level** | **Community Level** |\n| Broad issues with concepts proposed for this level | Unclear what defines a community or community-based program.The more broadly SMS programs are defined, the more difficult it is to enumerate them.Unclear how to enumerate programs in a community.Uncertain how to count Internet-based programs.If focused on a “sentinel” delivery system (eg, YMCAs, senior centers), unclear how to determine if these facilities offer SMS programs. |\n| Proportion of SM education/SM support programs by organization types in given counties  | Unclear what defines an SM education program or an SM support program.Unclear which organizations should be included and how they would be found and enumerated. How would a program be defined (eg, does an ongoing exercise class count the same time as a one-time offering of an SMS class)?Unclear denominator.Implies multiple surveys of community organizations. |\n| Proportion of communities actively promoting the construction of supportive environments that encourage people to be active  | Unclear how to define active promotion, construction, a supportive environment, and encouraging being active.Unclear how to define a community and if communities of different sizes counted equally.Unclear data source. |\n| Proportion of communities that actively promote programs that offer affordable healthy foods  | Unclear what specifically defines active promotion and a program that supports healthy affordable food.Unclear what defines affordable or what happens if healthy food is offered but expensive.Unclear data sources. |\n| Proportion of communities that have infrastructure/partnerships for organizations in the community to work together to foster SM among people with chronic diseases  | Unclear how to define “fostering,” “community,” “infrastructure,” “partnership,” or “working together” (once? ever? ongoing?).Unclear data sources. |\n| Proportion of individuals being encouraged to attend community programs  | Unclear how “being encouraged to attend” and “community programs” are defined.Unclear how data could be collected reliably. |\n| Proportion of population with chronic disease with SM support or education programs available within 5 miles of home (20 miles for rural areas) | Difficulty geo-locating people with chronic disease.People with chronic diseases may not be distributed the same as total population.Need geo-location of person and program to measure distance from person to program.Access to programs has multiple possible definitions. If access is an issue, is it immediate proximity (distance between a location and closest program), availability in an area (number of programs within an area), availability in immediate area (number of programs within a given distance of a point), or average distance between a location and all (or individual) SMS programs? |\n| Proportion of community benefit surveys that address the availability and/or quality of SM support programs | Lack of standard data collected by hospital associations.No one place to enumerate all the surveys that have been done. |\n| **Policy Level** | **Policy Level** |\n| Broad issues related to health care systems | Unclear what defines a health care system and how could these systems be enumerated.Unclear data source. |\n| Broad issues related to health insurance | The health insurance marketplace is complex with a number of types of insurers (government, employer, and commercial) and varied policies (individual, family, group); all have different requirements and benefits packages.Most employer-related health insurance benefits packages are inaccessible for review by nonemployees, and the number of employers is not feasible to monitor.Besides DSMT, unclear whether SMS programs for other diseases are covered.CPT or HCPCS codes are used for billing purposes and could be monitored to detect changes in health care delivery. CPT and HCPCS codes other than for DSMT are generic and are used for various activities, not specific to SMS. |\n| Broad issues related to professional practice guidelines | Many chronic conditions (eg, cancer, chronic kidney disease, dementia, arthritis) entail multiple subconditions for which guidelines may vary.Even a discrete condition may have varying treatment guidelines.Unclear whose guidelines should be used as the standard. |\n| Proportion of health plans financing/reimbursing for SM support  | No easy way to get this information from plans.Unclear what constitutes SMS in this context.No specific CPT code for SMS.Unclear how to count plans for the denominator. |\n| Proportion of health care systems/plans including pay-for-performance incentive tied to the delivery of SM support  | Likely proprietary information.Unclear what constitutes a health care system and how these systems are distinct from plans.Unclear how to enumerate all plans and systems. |\n| Proportion of insurance benefit packages that include SM support benefits  | Unclear what constitutes an SMS support benefit.Definition of an insurance benefit package is unclear. |\n| Proportion of public health departments supporting SM support programs | Unclear what constitutes an SMS program and how to define support.Level of analysis is unclear (state? county? city?).Data source unclear.Unknown quality and timeliness of data among state health departments with program locator functions. |\n| Proportion of insurance plans that reduce health insurance costs for improved SM by employees | Likely proprietary data.Reduce costs to whom? Why employees only?Unclear what activities constitute improved SM and how to distinguish from general wellness.Unclear how to define plan for the denominator. |\n| Proportion of nursing schools with SM support included in curricula | No centralized repository of curricula. |\n| Proportion of physician specialty certifications requiring SM support training | Exploration of new certification exam content (because need for exams on recertification is not consistent) in the specialties most likely to manage patients with chronic conditions (ie, internal medicine, family medicine, and preventive medicine) showed that SM is not explicitly mentioned per se in any of these exams. |\n| Proportion of employers who provide SM support as an employee wellness benefit | Examined Kaiser Employee Health Benefits Survey ; questions not specific enough to be sure they represent SMS. |\n| Federally funded research or programs on SM | Number of results in grants.gov  was low, highly influenced by choice of keyword, and produced different results on different days. |\n| **Media Level** | **Media Level** |\n| Broad issues with concepts proposed for this level | Unclear how to determine whether an individual was exposed and whether exposure was relevant to the chronic disease.Recall period unclear (eg, ever? last month? last week?). |\n| Proportion of individuals exposed to media campaigns locally, regionally, or nationally that promote SM, including collaborative goal-setting  | Unclear whether collaborative goal-setting must be included if other aspects of SM were covered.Unclear how to determine whether an individual was exposed.Denominator unclear (adults, adults with chronic condition, adults exposed to any media campaign?). |\n| Proportion of newspaper columns or radio/television stories on SM support  | Unclear what elements must be present to constitute SMS content and how those elements would be detected (ie, what specifically would we search for?) Would it be disease-specific?Unclear denominator (the sum of all TV, radio, and newspaper stories?) and uncertain how those sums would be ascertained. |\n| Proportion of individuals exposed to public health campaigns promoting SM | Unclear what constitutes a public health campaign or how to determine whether an exposure occurred. |\n| Proportion of individuals exposed to social media campaigns promoting SM | Unclear what constitutes a social media campaign or how to determine whether an exposure occurred. |\n| Proportion of product commercials that articulate SM as part of their product’s use | Unclear how SM would be defined in this context.Unclear how to enumerate products and commercials (billboard, print advertisement, television/radio advertisement?) |\n\n【43】Abbreviations: CPT, current procedural terminology; DSMT, diabetes self-management training; HCPCS, Healthcare Common Procedure Coding System; PCMH, patient-centered medical home; PCP, primary care provider; SM, self-management; SMS, self-management support.\n\n【44】 Table 2. Recommendations for Surveillance of Self-Management and Self-Management Support at Each Level of the Socio-Ecological Model\n\n| Indicator, Rationale, Data Source | Operationalization |\n| --- | --- |\n| **Individual level** | **Individual level** |\n| **Indicator:** Proportion of individuals with one or more chronic conditions who report taking action within the past 3 months (or alternate reference period) to improve their most concerning chronic condition.**Rationale:** Taking action to solve problems and adapt to a health condition’s fluctuations is a key and essential feature of effective self-management. A yes answer implies that the person monitors his/her condition and takes action to maintain quality of life.**Potential data source:** Surveys that assess chronic disease status such as National Health Interview Survey , Behavioral Risk Factor Surveillance System , or Medicare Current Beneficiary Survey .**Notes:**Responses to Q1 can also be used to calculate which conditions among those covered in a survey are most concerning to people who have multiple chronic conditions.Responses to Q2 can also be used to calculate patterns of disease trajectory (eg, among people who report arthritis as their most concerning condition, what proportion report better, worse, or stayed the same during the reference period) and show how these proportions change over time for a given condition. Q2 responses can also be combined with Q3 responses to compare proportions taking self-management action given the condition’s getting better or worse or remaining the same. Both of these secondary analyses can be used to further target self-management support interventions. | **Questions:** Only those with 1 or more chronic diseases should be asked this question.Q1. Earlier you told me that you had (fill in list of chronic diseases \\[eg, diabetes, arthritis, heart disease\\]). Which of these conditions concerns you the most?Q2. Do you feel as if your (name of condition of most concern) has gotten better, worse, or stayed the same in the last 3 months?\\*BetterWorseStayed the sameNot sure/don’t knowRefused\\*Three-month reference period can be reduced (eg, to one month) to match other reference periods used on the survey.Q3. Based on what you just told me, have you changed anything about how you manage your (name of condition of most concern)?Yes (optional to go to Q4)NoNot sure/don’t knowRefused**Numerator:** Q3 = A (yes)**Denominator:** Q2 = A + B + C + D +E (better, worse, same, don’t know/not sure, refused)**Optional 4th question if more detailed information is desired**Q4. What did you do? \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_(Record verbatim).Probe: Did you do anything else?Interviewer note: type of actions could be avoiding certain foods or environmental situations, performing specific types of activities, etc. |\n| **Secondary Indicator:** Proportion of individuals with one or more chronic conditions who report they have taken a course or class to help manage their most concerning chronic condition.**Rationale:** Provides estimate of proportion of people attending a self-management program. Can be asked in conjunction with community-level course awareness indicator question to compute proportion who are aware and attended (if they know, will they come?). Attending a class is a self-management behavior that cuts across chronic diseases; attendance is one means to the end of becoming a good self-manager.**Proposed data source** : Surveys that assess chronic disease status such as National Health Interview Survey , Behavioral Risk Factor Surveillance System , or Medicare Current Beneficiary Survey . | **Question:** Only those with 1 or more chronic diseases should be asked this question.Q1. Have you ever attended a course or class on how to manage your (name of condition of most concern)?YesNo (Optional to go to Q2)Not sure/don’t knowRefused**Numerator:** Q1 = A (Yes)**Denominator:** Q1 = A + B + C (Yes, No, Not sure/don’t know)**Optional second question if more detailed information is desired**Q2. There are many reasons people do not participate in a course or class to help them manage their (name of condition of most concern). Why have you not attended a course or class to help you manage that condition? \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_(Record verbatim).Probe: Anything else? |\n| **Health System Level** | **Health System Level** |\n| **Indicator:** Proportion of chronic disease patients seen by clinician in past 7 days who have a jointly developed self-management plan.**Rationale:** Collaborating with their patients to jointly develop a self-management plan is a key self-management support strategy performed by health care providers.**Potential data source** : Survey of health care providers, such as those conducted by the Physicians Foundation  or other state or local organizations. | **Question:** Some clinicians think it advisable for patients with chronic diseases to develop their own plan of action for managing their condition(s). Think about the patients with chronic disease you saw in the past 7 days. With what percentage of them have you or a member of your practice team ever jointly developed such a self-management plan? Fill in percentage. \\_\\_\\_\\_\\_\\_\\_.Don’t knowRefusedNote to interviewer: If a range is given, record the midpoint of that range.**Numerator:** Sum of stated percentages.**Denominator:** Number of respondents who gave percentage (excludes don’t know and refused). |\n| **Indicator:** Proportion of National Committee on Quality Assurance Patient-Centered Medical Homes (PCMH) that report including a self-management plan in individual care plans.**Rationale:** Collaborating with their patients to develop a self-management plan is a key self-management support strategy performed by health care providers; organizations that apply for PCMH recognition report this activity if they report on elective element CM8.**Potential data source:** NCQA PCMH recognition applications database . | **Measurement:** Organizations that report meeting NCQA’s PCMH elective element 8 (Care Management and Support Element) (CM8) (ie, includes a self-management plan in individual care plans) .**Numerator:** Number of organizations that report meeting elective element CM8.**Denominator:** Number of organizations that are NCQA-recognized as PCMHs. |\n| **Indicator:** Proportion of clinicians reporting they or a member of their practice team received specialized training on how to help patients develop a self-management plan.**Rationale:** Health care providers traditionally receive little or no specialized training in SMS techniques such as motivational interviewing.**Potential data source:** Survey of health care providers, such as those conducted by the Physicians Foundation  or other state or local organizations. | **Question:** Have you or a member of your practice team received specialized training in helping patients develop a self-management plan? Such specialized training includes motivational interviewing, stages of change, or other patient activation techniques.YesNoDon’t knowRefused**Numerator:** A (yes)**Denominator:** A + B+ C (yes, no, or don’t know) |\n| **Community Level** | **Community Level** |\n| **Indicator:** Proportion of individuals with 1 or more chronic conditions who report that they are aware of courses or classes in their community to help manage their most concerning chronic condition.**Rationale:** Awareness of classes is a proxy for community availability and also the communication of such availability. If classes exist but people are unaware of them, they are effectively not available.**Proposed data source:** Surveys that assess chronic disease status such as National Health Interview Survey , Behavioral Risk Factor Surveillance System , or Medicare Current Beneficiary Survey . | **Questions:** Only those with 1 or more chronic diseases should be asked this question.Q1. Earlier you told me that you had (fill in chronic diseases \\[eg, diabetes, arthritis, heart disease\\]). Which of these conditions concerns you the most?Note: This question can be eliminated if individual-level self-management questions have already identified the most concerning condition.Q2. Do you know of any courses or classes in your community to help people manage (name of condition of most concern)?YesNoNot sure/don’t knowRefused**Numerator** : Q2 = A (yes)**Denominator:** Q2 = A + B + C (yes, no, or don’t know) |\n| **Secondary Indicator:** Proportion of people with a chronic condition who were aware a class existed and took it.**Rationale:** Awareness of courses or classes is necessary but not always sufficient to motivate a person to attend the course or class. This indicator can help determine if intervention efforts need to focus on raising awareness of classes (if the proportion of people who are both aware of class and took it is high) or if there is a need for further education about the value of self-management (if the proportion of people who are aware and participating is low). | **Measurement:** If individual-level secondary indicator question on class participation, and community-level question on awareness of courses are both asked, then this indicator can be computed.**Numerator:** Individual-level secondary indicator Q1 = A (yes)**Denominator:** Community-level indicator Q2 = A (yes) (was aware of classes) |\n| **Policy Level** | **Policy Level** |\n| **Indicator:** Proportion of medical school curricula that include coverage of self-management/self-management support.**Rationale:** Easily searched. As SMS becomes increasingly viewed as an important tool for quality care, it will be increasingly taught in medical schools.**Proposed data source:** American Association of Medical Colleges Curriculum Inventory and Reports  | **Measurement:** Search medical school curricula for SMS as a specifically taught topic.**Numerator:** Number of medical schools with SMS in curricula**Denominator:** Total number of medical schools |\n| **Indicator** : Proportion of States that include training on self-management support in Physician Continuing Medical Education requirements.**Rationale:** Easily searched; sensitive to change in state physician CME requirements. As SMS becomes increasingly viewed as an important tool for quality care, it may be increasingly required by states.cmeweb.com/gstate\\_requirements.phpFor nurses: Nursing Continuing Education Requirements by State. Brookfield WI: OnCourse Learning Corporation; 2017  | **Measurement:** Search each state’s CME requirements for mention of patient self-management.**Numerator:** Number of states requiring CME on SM**Denominator:** Total number of states |\n| **Indicator:** Number of clinical care guidelines that include recommendations for self-management support.**Rationale:** Easily searched; sensitive to change if consistent search strategy used. As SMS becomes increasingly viewed as part of quality care, clinical guidelines should reflect the importance of doing it.**Proposed data source:** PubMed  | **Measurement:** Search strategy disease \\[all fields\\]) and (self-management \\[all fields\\] and (“guidelines” \\[publication type\\]) where “disease” is replaced with the specific chronic condition. Restrict articles to English to better reflect United States.**Numerator:** Number of guidelines including SM**Denominator:** None |\n| **Indicator:** Number of federal rules referencing self-management.**Rationale:** Easily searched; sensitive to changes in numbers of rules about SM if consistent search strategy is used. Compare results of (“self-management” and “chronic disease”) to “self-management” and “chronic condition”) to identify unique rules. Search by year.**Numerator:** Number of rules by year**Denominator:** None |\n| **Media Level** | **Media Level** |\n| **Indicator:** Number of stories in written media that mention self-management for a particular chronic condition.**Rationale:** Annual number of stories reflects media dialogue about an issue.**Notes:** Restriction to news, wire feeds, and magazines provides reasonable breadth of coverage. Can restrict to certain areas or regions and shorter time periods.**Proposed data source:** ProQuest  or similar media monitoring service. | **Measurement:** Number of stories in English containing the words “self-management” AND “condition” in a given year in newspapers, wire feeds, or magazines. Replace term “condition” with the individual chronic condition. Search by year.**Numerator:** Number of stories by year**Denominator:** None |\n\n【46】Abbreviations: CME, continuing medical education; NCQA, National Committee for Quality Assurance; SM, self-management; SMS, self-management support.\n\n【47】 Table 3. Number of _Federal Register_ Rules Related to Chronic Condition Self-Management, by Relationship to Diabetes, 2009–2016\n\n| Year | “Self-Management” and “Chronic Disease” a | “Self-Management” and “Chronic Condition” b | Unique c | Diabetes-Related d | Not Diabetes-Related e |\n| --- | --- | --- | --- | --- | --- |\n| 2009 | 2 | 1 | 2 | 1 | 1 |\n| 2010 | 5 | 0 | 5 | 2 | 3 |\n| 2011 | 3 | 1 | 3 | 1 | 2 |\n| 2012 | 1 | 1 | 2 | 1 | 1 |\n| 2013 | 1 | 1 | 2 | 2 | 0 |\n| 2014 | 1 | 2 | 2 | 1 | 1 |\n| 2015 | 1 | 0 | 1 | 0 | 1 |\n| 2016 | 5 | 5 | 5 | 2 | 3 |\n| Total | 19 | 11 | 22 | 10 | 12 |\n\n【49】a  Number of rules found in _Federal Register_ search of Rules only using search terms “self-management” and “chronic disease.”  \nb  Number of rules found in _Federal Register_ search of Rules only using search terms “self-management” and “chronic condition.”  \nc  Number of unique rules found after manual comparison of search results “self-management” and “chronic disease” vs “self-management” and “chronic condition.”  \nd  Number of unique rules found that were specific to diabetes.  \ne  Number of unique rules found that were not specific to diabetes.\n\n【50】 Table 4. Number of Newspaper, Wire Service, and Magazine Stories That Mention Self-Management, by Chronic Condition, 2010–2016\n\n| Condition | Number of Self-Management Mentions a |\n| --- | --- |\n| Total | 2010 | 2011 | 2012 | 2013 | 2014 | 2015 | 2016 |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Diabetes | 4,297 | 502 | 548 | 685 | 653 | 728 | 669 | 512 |\n| Cancer | 1,632 | 219 | 197 | 291 | 226 | 280 | 231 | 188 |\n| Chronic disease | 1,464 | 166 | 187 | 221 | 225 | 252 | 217 | 196 |\n| Depression | 956 | 124 | 118 | 151 | 141 | 160 | 133 | 129 |\n| Breast cancer b | 933 | 184 | 89 | 124 | 150 | 165 | 130 | 91 |\n| Stroke | 718 | 93 | 73 | 106 | 113 | 135 | 108 | 91 |\n| Arthritis | 700 | 89 | 89 | 101 | 117 | 122 | 106 | 76 |\n| Asthma | 400 | 56 | 44 | 66 | 56 | 81 | 60 | 37 |\n| Dementia | 370 | 16 | 45 | 63 | 18 | 124 | 45 | 59 |\n| Hypertension | 290 | 30 | 38 | 35 | 51 | 65 | 33 | 38 |\n| Autism | 279 | 44 | 50 | 28 | 42 | 44 | 44 | 27 |\n| COPD | 220 | 11 | 27 | 44 | 43 | 38 | 25 | 32 |\n| HIV | 212 | 28 | 31 | 33 | 32 | 33 | 28 | 27 |\n| Osteoporosis | 187 | 29 | 20 | 24 | 38 | 32 | 22 | 22 |\n| Chronic heart failure | 105 | 10 | 10 | 13 | 25 | 22 | 18 | 7 |\n| Alcoholism b | 84 | 6 | 4 | 16 | 11 | 26 | 16 | 6 |\n| Chronic kidney disease | 75 | 17 | 1 | 3 | 9 | 32 | 9 | 4 |\n| Hepatitis | 63 | 1 | 10 | 11 | 12 | 8 | 13 | 8 |\n| Autism spectrum disorder | 49 | 3 | 4 | 5 | 11 | 9 | 11 | 6 |\n| Schizophrenia | 47 | 7 | 7 | 6 | 8 | 5 | 2 | 12 |\n| Coronary artery disease | 26 | 2 | 9 | 4 | 3 | 1 | 4 | 3 |\n| Hyperlipidemia | 12 | 4 | 1 | 0 | 2 | 3 | 2 | 1 |\n| Cardiac arrhythmia | 8 | 1 | 2 | 0 | 0 | 0 | 5 | 2 |\n| Substance abuse disorder | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |\n| **Total** | 13,128 | 1,642 | 1,604 | 2,030 | 1,986 | 2,366 | 1,931 | 1,574 |\n\n【52】Abbreviations: COPD, chronic obstructive pulmonary disease; HIV, human immunodeficiency virus.  \na  Numbers are from the PROQUEST database .  \nb  Breast cancer added (to compare with cancer) and alcoholism added (to compare with substance abuse disorder) to assess using differing terms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1103199e-1e91-4305-91f5-8ec04372c4b2", "title": "CDC at Work: Harmful Algal Blooms (HABs)", "text": "【0】CDC at Work: Harmful Algal Blooms (HABs)\n*   Airborne Cyanotoxin Exposure Study\n*   Studying the Impact of a Water Advisory in Ohio\n*   Making the Great Lakes Safer and Healthier\n*   Using Lab Tests to Help Alaska Solve a Case of Shellfish Poisoning\n*   Using Poison Center Information to Identify a Harmful Algal Bloom (HAB) Outbreak in Utah\n\n【1】### Assessing Health Effects from Airborne Cyanobacterial Toxin Exposure\n\n【2】CDC is exploring the health effects of cyanobacterial toxins, especially whether they can get into our bodies through the air we breathe. Similar studies have been done in Florida, Michigan, and California. The study will follow people who live or work on the Florida waters of Lake Okeechobee, St. Lucie River, Caloosahatchee River, or Cape Coral Canals over the bloom season. Study staff members will work with participants to record information. The information collected will advance the understanding of how cyanobacterial toxins affect people. Find more information about the study, including how to participate .\n\n【3】### Studying the Impact of a “Do Not Drink” Water Advisory in Ohio\n\n【4】In 2014, routine testing of the water supply in Lucas County, Ohio, showed that levels of a toxin called microcystin were above the limit set by the Ohio Environmental Protection Agency. Microcystin toxin is produced by certain types of harmful algal blooms (HABs) and can cause diarrhea, vomiting, and liver damage in people and animals when consumed.\n\n【5】In response to these tests, Ohio’s Governor declared a state of emergency and issued a “Do Not Drink” water advisory for two days. This advisory alerted the 450,000 water supply users not to drink or use the water for cooking or brushing teeth. In this case, even boiling the water would not make it safe to use. The Ohio National Guard brought in emergency water supplies and set up water distribution sites for communities in the affected areas, while public health authorities and other partners worked to clear the microcystin toxin from the water supply. Authorities lifted the “Do Not Drink” water advisory after two days, once tests showed that the toxin levels in the water were below the set limit.\n\n【6】After the advisory was over, the Ohio Department of Health and the Toledo-Lucas County Health Department, with assistance from CDC, set out to learn about people’s experiences, evaluate their response to the threat, and understand how to better protect the public’s health if a similar event occurred in the future. Public health officials conducted a Community Assessment for Public Health Emergency Response (CASPER) to find out the effectiveness of their communication strategies, how many people and animals were exposed to the contaminated water after the advisory announcement, and how the advisory itself affected people’s lives. A CASPER is a tool developed by CDC that public health professionals can use to gather information quickly during or after a public health emergency using an in-person survey. This instance marked the first time that public health professionals used a CASPER for a HAB event. To develop the survey and prepare for the CASPER, a CDC/ Council of State and Territorial Epidemiologists (CSTE) Applied Epidemiology Fellow with experience on other HAB projects worked with the state and local health departments, as well as federal partners.\n\n【7】Public health officials gathered valuable insight into the community’s experiences through door-to-door interviews with people in 171 households affected by the advisory. While the great majority of the households had quickly learned about the advisory through traditional media (e.g. TV) or social media, over 50% of households still reported using or drinking the contaminated water in some way during the advisory, primarily for hygiene uses like bathing, handwashing, and brushing teeth. Some households experienced health problems like nausea, diarrhea, stomach pain, vomiting, headache, skin irritation, eye irritation or pain, respiratory illness, and/or coughing. A few households also dealt with anxiety, stress, and disruptions in their daily lives caused by the advisory. About 5% of the households with pets said their pets experienced health issues. Many households relied on alternative water sources, like bottled water, during the advisory and for a short while afterward.\n\n【8】Based on these results, public health officials developed a list of recommendations to guide planning and response efforts for future emergency events. The recommendations included promoting water preparedness for community members, finding ways to improve the distribution of alternative water supplies, focusing future public messaging on TV primarily, and making people aware of health and mental health resources. By preparing for the next event, the state hopes to reduce the number of water-related illnesses in people and animals and minimize the stressful impact of water advisories on the community.\n\n【9】### Making the Great Lakes Safer and Healthier\n\n【10】The Great Lakes—made up of Lakes Michigan, Superior, Erie, Huron, and Ontario—cover more area (close to 95,000 square miles) than any other group of freshwater lakes on the planet. These lakes supply drinking water to tens of millions of Americans and have more than 50 million visitors every year. Over the last few years, CDC and other government agencies have worked together to protect this area from growing environmental threats—such as invasive species and nutrient runoff—and restore its habitat through the Great Lakes Restoration Initiative (GLRI) , a federal program that is administered by the Environmental Protection Agency (EPA).\n\n【11】The work done by the GLRI is speeding up efforts to make the Great Lakes safer and healthier for people, plants, and animals. One of the program’s focus areas is reducing nutrients and runoff in Great Lakes water, both of which can lead to harmful algal blooms (HABs) and waterborne illnesses. CDC joined the GLRI in 2013 to help the program better understand when, where, and why waterborne illnesses and outbreaks—including ones caused by HABs—occur in the Great Lakes area. Other GLRI partners—such as the United States Geological Survey, the Bureau of Indian Affairs, and the National Oceanic and Atmospheric Association—provide input on various topics related to their areas of expertise. The long-term goals of the GLRI are to address threats to the health of the Great Lakes ecosystem and improve the overall environmental conditions in the area, which will help to protect the health of people and animals.\n\n【12】CDC asks state and local health departments in the Great Lakes area to report information on waterborne illnesses and outbreaks back to CDC. In order to do that, these health departments need more staff and resources that are dedicated to GLRI-specific health issues, like HABs. To help fill this need, CDC has used GLRI funding to place Epidemiology Fellows from the CDC/Council of State and Territorial Epidemiologists (CSTE)’s Applied Fellowship Program in the health departments of Great Lakes states for two years at a time. The fellowship program has helped to improve methods to increase the reporting of waterborne illnesses and outbreaks to CDC. The GLRI also has provided funding for the development of the One Health Harmful Algal Bloom System (OHHABS) , a tool for states to report HAB-associated illnesses and environmental information to CDC.\n\n【13】From providing drinking water to offering a place for recreational activities, the Great Lakes affect the lives of millions of Americans every year. Through their GLRI-funded work, CDC and other federal agencies help to make the Great Lakes and the area surrounding them safer and healthier for everyone to enjoy now and in the future.\n\n【14】**Reference**\n\n【15】*   Great Lakes Restoration Initiative. Action Plan II. \\[PDF – 30 pages\\] September 2014.\n\n【16】### Using Lab Tests to Help Alaska Solve a Case of Shellfish Poisoning\n\n【17】In May 2016, a woman experiencing symptoms that included generalized weakness and slow breathing entered a hospital in Alaska and spent six days recovering before she was well enough to go home. Doctors working to diagnose her illness focused on the fact that she had eaten shellfish that she and her family had picked up from the shore of a local beach (also known as non-commercial shellfish\\*). Non-commercial shellfish are not tested by laboratories, so they may contain harmful algal bloom (HAB) toxins, which can cause serious illness in humans and animals. Based on the doctors’ knowledge of the patient’s symptoms and the fact she had eaten the non-commercial shellfish, the hospital suspected she had an illness called Paralytic Shellfish Poisoning (PSP) .\n\n【18】PSP is a reportable condition in Alaska, which means that if a hospital thinks a patient has PSP, they must report it to the state health department. When the hospital contacted the Alaska Division of Public Health (DPH), they requested assistance with laboratory testing to determine whether the patient was suffering from PSP or something else. This required testing the shellfish the patient had eaten and testing a urine sample taken from the patient herself. The Alaska Department of Environmental Conservation was able to confirm that the shellfish were contaminated with algal toxins, specifically two groups of toxins called saxitoxins and gonyautoxins. However, they did not have the technology available to test the human samples, so they requested help from CDC’s Division of Laboratory Sciences (DLS) in the National Center for Environmental Health (NCEH).\n\n【19】Based on what the Alaskan lab had learned about the shellfish samples, the DLS’s Emergency Response Branch tested for the same toxins (saxitoxins and gonyautoxins) in the patient’s sample, using its new gonyautoxins test for human samples. This test detected high concentrations of saxitoxins and gonyautoxins in the patient’s sample, confirming that the patient was sick with PSP, as the hospital had originally suspected. This marked the first time CDC confirmed gonyautoxins and saxitoxins in a human sample for a state. Not only did it represent a big step forward for PSP laboratory testing at CDC, but it also improved CDC’s ability to assist partners in diagnosing HAB-associated illnesses in the future.\n\n【20】Collaborative relationships with state and local partners are crucial to the success of CDC’s HABs work. This holds true for the HABs-related technical assistance provided by CDC at the request of its partners, both outside and inside CDC laboratories. As CDC continues to study algal toxins and their effects on human and animal health, it will continue to work alongside public health labs around the country to develop new and improved testing methods.\n\n【21】\\* To protect individuals from HAB toxins, Alaska urges residents who eat shellfish to only buy commercially sold shellfish, which are subject to routine testing. Non-commercial shellfish (ones collected by individuals on the shore) do not undergo this testing and can, therefore, be contaminated.\n\n【22】### Using Poison Center Information to Identify a Harmful Algal Bloom (HAB) Outbreak in Utah\n\n【23】Since 2002, CDC has worked with the American Association of Poison Control Centers (AAPCC) and the National Poison Data System (NPDS) to monitor poison center calls and watch for information that could indicate a potential public health issue. In the summer of 2016, CDC scientists tasked with monitoring these calls noticed an increase in callers from the state of Utah. The callers had one thing in common: they had visited Lake Utah and had symptoms related to toxic algal bloom exposure, including vomiting, diarrhea, and rashes.\n\n【24】Once the CDC scientists realized these calls could be the first sign of a harmful algal bloom in the lake, they quickly worked to report the calls to their poison center contact and to public health officials from Utah. Armed with this information, Utah officials immediately assessed the situation and determined that a large toxic algal bloom was spreading across the lake. They decided to close the lake to the public until the bloom ran its course and disappeared a few months later in mid-October.\n\n【25】This example shows the way in which CDC collaborates with partners to assist them in identifying and responding to HAB events. In this case, Utah was prepared to address the HAB without additional help from CDC; in other cases, health officials may ask CDC for further assistance. Whatever the situation may require, CDC is ready to work with its partners to help to protect the health of the public.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2e9a89f6-7e4e-4d99-adc0-9bc749ab9a4a", "title": "Swine Influenza Virus (H1N2) Characterization and Transmission in Ferrets, Chile", "text": "【0】Swine Influenza Virus (H1N2) Characterization and Transmission in Ferrets, Chile\nIn the past decade, variant swine-origin influenza A viruses (swIAVs) of subtypes H1N2 and H3N2 have been associated with human infections, particularly in persons with close contact to swine , although evidence of human-to-human transmission is minimal. The emergence and subsequent pandemic caused by the unique influenza A(H1N1)pdm09 virus (pH1N1) from swine in 2009 highlights the public health threat posed by swIAVs, especially in populations having little preexisting immunity . Therefore, it is imperative to understand the circulation of swIAVs on a global level.\n\n【1】While our knowledge of the global diversity and evolution of swIAVs has increased, less information has been available regarding swIAVs circulating in South America. Recently, subtypes H1N1, H1N2, and H3N2 were identified in commercial pigs in Chile . Phylogenetic analysis from 18 swIAVs sequenced from a 2012 cross-sectional sampling of 22 production companies in Chile (95% of their commercial swine) demonstrated extensive genetic diversity in the hemagglutinin (HA) segment . The human seasonal H1-like viruses were unique to swine in Chile and not closely related to other H1 swIAVs. These viruses represented 2 independent introductions from humans, with Chile H1 human I clade most closely related to human viruses from the late 1980s and Chile H1 human II clade most closely related to human viruses from the early 1990s . No further phylogenetic or virus characterization information was available.\n\n【2】In addition to commercial pig farming, Chile has an extensive backyard production system, a common form of animal production throughout the world. Backyard production farms (BPFs) are a particular public health concern given poor biosecurity and close interaction between humans and multiple animal species . During 2014, we conducted active backyard production system surveillance in Chile’s main swine production area  and isolated a subtype H1N2 swIAV virus (A/swine/Chile/YA026/2014; sw/Chile). We performed phylogenetic analyses and full genomic sequencing on this virus to determine its relationship with other known influenza isolates. We propagated it in primary human bronchial and swine respiratory epithelial cells, as well as C57Bl/6 mice, to determine its infectiousness. We also characterized its transmission using ferrets. Hemagglutination inhibition (HI) assays with human serum samples were done to assess the risk this virus poses for human infection, and serological analyses were conducted with pig serum samples from Chile to determine the prevalence of the virus and its risk for emergence.\n\n【3】### Materials and Methods\n\n【4】##### Study Area and Nasal Swab Sampling\n\n【5】Active surveillance was conducted as described  in 40 BPFs in Central Chile, representing 90% of the backyard production system. During this time, only 3 farms had swine (n = 3) available for sampling. We collected nasal samples using disposable sterile swabs, which were placed in 1 mL universal transport media. None of the animals were symptomatic.\n\n【6】##### H1N2 Virus Isolation\n\n【7】We screened nasal swab samples by real-time reverse transcription PCR for matrix gene as described . PCR-positive samples were centrifuged at 8,000 rpm for 15 min and the clarified supernatant (200 µL) inoculated on MDCK cells in 24-well plates in duplicate and incubated at 37°C in 5% CO 2  for 1 h. Following incubation, cells were washed then incubated with Eagle’s minimum essential medium (MEM; MediaTech, Manassas, VA, USA) supplemented with 2 mM glutamine, bovine serum albumin, and 1 µg/mL TPCK (L-1-tosylamide-2-phenylethyl chloromethyl ketone)–trypsin until cytopathic effect was observed. Then, cell supernatant was collected, centrifuged, and stored at −80°C as described . Only 1 virus was isolated: A/swine/Chile/YA026/2014 (H1N2) (sw/Chile).\n\n【8】##### Cells\n\n【9】MDCK cells were cultured in MEM supplemented with 2 mM glutamine and 10% fetal bovine serum (Gemini BioProducts, West Sacramento, CA, USA) and grown at 37°C in 5% CO 2  . A549 cells were cultured in Dulbecco’s minimum essential medium (DMEM; Lonza, Walkersville, MD, USA) supplemented with 4.5 g/L glutamine and 10% fetal bovine serum and grown at 37°C in 5% CO 2  . Well-differentiated (transepithelial resistance >300 Ω × cm 2  ), primary normal human bronchial epithelial (NHBE) cells were grown as previously described . In brief, cells were plated on collagen-coated membranes in bronchial epithelial basal media (Lonza) supplemented with SingleQuots additives (Lonza) and media changed every 48 h. When cells became 100% confluent, basal media was replaced with DMEM:F12 (Lonza) supplemented with SingleQuots, and apical media was removed. Every 48–72 h, the apical surface was washed with 0.9% sodium chloride solution (Sigma, St. Louis, MO, USA). Cells were incubated at the air-liquid interface at 37°C in 5% CO 2  . Primary swine nasal epithelial cells (sNECs) were prepared as described . In brief, nasal swab specimens were collected, placed in 50 mL conical tubes, and incubated in phosphate-buffered saline (PBS) at 4°C for 10–15 min before centrifuging for 10 min at 300 × _g_ at 4°C. The pellet was resuspended in supplemented DMEM:F12 and incubated on collagen-coated plates at 37°C in 5% CO 2.  Cells were plated onto transwells and, when 100% confluent, moved to an air–liquid interface to differentiate for 1–3 wk. sNECs were moved to 33°C 1 d before infection.\n\n【10】##### Virus Sequencing\n\n【11】Sanger sequencing of viral stocks was performed by the St. Jude Hartwell Center , and deep amplicon sequencing and assembly were conducted as described previously . Sequences can be obtained in GenBank (accession nos. CY207245–CY207252).\n\n【12】##### Viral Propagation\n\n【13】A/swine/Chile/YA026/2014 (H1N2, sw/Chile), A/Memphis/3/1987 (H1N1, Mem/87), A/swine/Iowa/13-1015/2010 (H1N2, sw/IA), A/swine/Indiana/26-0818/2011 (H1N2, sw/IN), A/swine/North Carolina/18161/2002 (H1N1, sw/NC), A/swine/Italy/1310-2/1995 (H1N1, sw/IT), and A/California/04/2009 (pH1N1, CA/09) viruses were propagated in the allantoic cavity of 10-day-old, pathogen-free, embryonic chicken eggs at 37°C and viral titers determined by 50% tissue culture infectious dose (TCID 50  ) analysis as previously described . These viruses were used as comparators for all studies except the ferret experiments.\n\n【14】##### Phylogenetic Analysis\n\n【15】We performed sequence assembly and editing by using BioEdit version 7.2.5  and sequence alignment with MUSCLE version 3.8.3 . Reference sequences were obtained from the Influenza Virus Resource at NCBI . Phylogenic relationships for each gene was inferred by maximum likelihood, incorporating a general time-reversible model of nucleotide substitution with a gamma-distributed rate variation among sites by using RAxML version 8.0 . A bootstrap resampling process of 200 replicates was implemented to provide statistical robustness to each node.\n\n【16】##### Swine and Human Serum Samples\n\n【17】During 2013–2015, swine serum samples (n = 266, 1–24 animals/BP) were collected from 80 BPFs in Central Chile, including the Valparaiso, Metropolitan, and Libertador General Bernardo O’Higgins regions. During 2009–2015, human serum samples were collected as part of an ongoing prospective observational study at the University of North Carolina Family Medicine Center (Chapel Hill, NC, USA). We chose a subset of these samples, collected 28–32 d after seasonal influenza vaccination; grouped them according to birth decade (1920–1929, 1930–1939, 1940–1949, 1950–1959, 1960–1969, 1970–1979, 1980–1989, and 1990–1999; n = 7–22/age group); and balanced for sex and race whenever possible .\n\n【18】##### Hemagglutination Inhibition Assay\n\n【19】Ferret antiserum was generated against sw/Chile, Mem/87, sw/IA, CA/09, sw/IN, sw/NC, and sw/IT viruses. HI assay was conducted as previously described . In brief, antiserum was treated with receptor-destroying enzyme (Denka Seiken, Tokyo, Japan), inactivated, and diluted 1:10. Serial dilutions of receptor-destroying enzyme–treated serum were incubated in duplicate with each virus for 15 min at room temperature, followed by 30 min incubation at 4°C with 0.5% turkey erythrocytes. HI titer was defined as the reciprocal dilution of the last well that reacted.\n\n【20】##### In Vitro Replication\n\n【21】MDCK cells were infected with a multiplicity of infection (MOI) of 0.01 for 1 h at 37°C. Cells were washed 3 times to remove unbound virus, and infected cells were cultured in MEM containing 0.075% bovine serum albumin and 1 μg/mL TPCK-treated trypsin. Aliquots of culture supernatants were collected at 6 h postinfection (hpi), 24 hpi, 48 hpi, and 72 hpi and immediately stored at −80°C. For infection of NHBE cells and sNECs, the apical surface was washed twice and incubated with serum-free DMEM containing virus for 2 h at 37°C, after which both apical and basal media were removed and fresh growth medium was added to the basal chamber as described . At 6 hpi, 24 hpi, 48 hpi, 72 hpi, and 96 hpi, DMEM was added to the apical surface, and cells were incubated for 30 min at 37°C (for NHBE) or 33°C (for sNEC). Apical media was collected and stored at −80°C. Viral titers were determined by TCID 50  on MDCK cells followed by Reed-Munch analysis .\n\n【22】##### Animal Infections\n\n【23】Six- to 8-week-old female BALB/c mice (Jackson Laboratory, Bar Harbor, ME, USA) (n = 11 mice/group) were lightly anesthetized with isofluorane and intranasally inoculated with PBS or 10 5  TCID 50  units of virus in 25 μL PBS. Mice were monitored daily for signs of infection and weighed every 48 hpi . At 3 d postinfection (dpi) and 6 dpi, 3 control and 3 infected mice were humanely killed; lungs were harvested and homogenized in 1 mL PBS and nasal washes were collected as previously described . In brief, an incision was made in the trachea and 500 µL of PBS was flushed through the upper respiratory tract and collected. Viral titers were determined by TCID 50  . Data are representative of 2 separate experiments. For transmission studies, 9- to 15-week-old male ferrets (n = 2, Triple F Farms, Sayre, PA, USA) were intranasally inoculated with 10 6  TCID 50  units in 1 mL PBS. Twenty-four hours later, naive ferrets (n = 2 mice/group) were either placed in direct contact with infected ferrets or housed in separate cages (respiratory contact) separated by ≈5 inches. Nasal washes were collected every 2 dpi for viral titration and serum samples collected at 21 dpi for HI analysis as described .\n\n【24】##### Ethics Statement\n\n【25】All sampling was approved by the ethics and biosecurity committee of Faculty of Veterinary Science, University of Chile, and collected by trained veterinarians according to Food and Agriculture Organization guidelines . Animal experiments were approved by the St. Jude Children’s Research Hospital institutional biosafety committee and animal care and use committee and were in compliance with the Guide for the Care and Use of Laboratory Animals. These guidelines were established by the Institute of Laboratory Animal Resources and approved by the Governing Board of the US National Research Council. All human procedures were approved by the Biomedical Institutional Review Board at the University of North Carolina.\n\n【26】##### Statistical Analysis\n\n【27】We performed statistical analyses using JMP statistical software (SAS Institute, Cary, NC, USA) and GraphPad Prism software (GraphPad Software Inc. La Jolla, CA, USA). The Kruskal-Wallis test (α = 0.05) was used to analyze nonparametric data, while normally distributed data were analyzed by 2-way analysis of variance with virus strain and time postinfection as main effects. The Student _t_ \\-test was used for posthoc comparisons between the viruses, and Tukey's honest significant difference was used for posthoc comparisons among the times postinfection. Differences were considered significant at p<0.05.\n\n【28】### Results\n\n【29】##### Phylogenetic Similarity of Chilean H1N2 and Human Seasonal Viruses\n\n【30】Phylogenetic analysis demonstrated that sw/Chile H1N2 belongs to the δ-like H1 subgroup  and is phylogenetically similar to the swIAVs from Chile described by Nelson et al. The HAs of these swIAVs are closely related to the HAs of human seasonal H1 viruses from the late 1980s and early 1990s (closest relative A/Suita/1/89, 94% nucleotide identity), although they are separated by long branch lengths and represent over 15 years of evolution . Unfortunately, only the HA segments from the 2012 Chilean swIAVs were available in public databases, so we were unable to compare full genomes.\n\n【31】The sw/Chile neuraminidase (NA) segment arose from a reassortment between human seasonal H1N1 and H3N2 viruses . However, long branch length indicates extensive genetic divergence from human N2 sequences, with the closest sequence being an early 1990s North American H3N2 virus (A/New York/762/1993, 93% nucleotide identity). This difference can be indicative of possible temporal and species-specific adaptations, leading to genetic and antigenic differences with contemporary human N2 viruses. The sw/Chile virus internal genes are pH1N1 in origin, consistent with the high reassortment rate between pH1N1 and swine viruses  . Overall, while sw/Chile does appear to originally be of human origin, detection of these viruses in multiple BPFs suggests these viruses might have become established in the swine population in Chile.\n\n【32】##### Antigenicity and Seroprevalence of Chilean H1N2 Virus\n\n【33】HI assays with a panel of ferret antiserum against classical swIAV (sw/IN and sw/NC from β and γ subgroups, respectively); pandemic CA/09 virus; sw/Iowa (from δ subgroup); and human seasonal H1 virus Mem/87 showed that sw/Chile H1N2 virus was antigenically distinct from other H1 viruses . To determine seroprevalence in swine within the surveillance area, serum samples (n = 266, 1–24 animals/BPF) were collected from swine on 80 BPFs in Central Chile in 2013–2015. Forty-eight (60%) BPFs were positive by nucleoprotein-specific ELISA, and the prevalence of positive pigs (86/266, 32.3%) on each BPF ranged from 14.2% to 100% animals . HI assays performed with a panel of influenza viruses on the ELISA-positive samples demonstrated that 14/48 (29.1%) BPFs and 22/86 pigs were positive for \\> 1 of the test viruses. Antibodies against sw/Chile were identified in BPFs located in 3 regions of Central Chile . Of concern, 11/14 (78.6%) of the positive BPFs, representing 15/22 (68.2%) of the positive samples, were positive for multiple influenza strains, suggesting that different influenza viruses are co-circulating in backyard swine in Central Chile . Not all nucleoprotein ELISA–positive samples could be subtyped.\n\n【34】##### Chile H1N2 Virus in Primary Swine and Human Cells\n\n【35】Swine possess both mammalian (α-2,6) and avian (α-2,3) influenza receptors . Receptor binding assays  revealed that all viruses used in this study had greater binding affinity for mammalian-type (α-2,6–linked sialic acid) receptors . These viruses also replicated effectively in MDCK cells; differentiated, primary NHBE cells; and sNECs cultured at the air-liquid interface. Cells were infected at MOI 0.01 and viral titers determined at the indicated times by TCID 50  analysis on MDCK cells . All viruses replicated to similar titers in MDCK cells  and sNECs , except for an increased replication of CA/09 virus in MDCK cells. However, significant differences in replication were observed in primary NHBE cells. The human seasonal Mem/87 and sw/Chile viruses replicated to lower titers than the other swine viruses, suggesting reduced fitness in human cells . Overall, these studies demonstrate that sw/Chile can productively replicate in human and swine cells.\n\n【36】##### Chile H1N2 Virus in Mice\n\n【37】Recently identified swine H1N2 viruses from China replicated efficiently in mice and pigs ; however, no data were available on viruses from South America. Thus, we intranasally inoculated Balb/c mice with 10 5  TCID 50  units of each virus and monitored weight loss for 14 dpi. Viral titers were measured 3 dpi and 6 dpi . CA/09 virus caused considerable weight loss, as did sw/IA (H1N2, δ1 subclade), which was similar to CA/09, sw/IN (H1N2, β subclade), and sw/NC (H1N1, γ2 subclade) viruses. Increased weight loss was associated with efficient replication in lungs . In contrast, the Mem/87 and sw/Chile viruses caused minimal illness and had lower replication in lungs . The sw/Chile virus replicated the most effectively in the upper respiratory tract compared with the other swine viruses, which reached titers similar to that of CA/09 virus . These data show that while sw/Chile virus can replicate in mice, it causes minimal illness. The genetic basis for differences in pathogenicity and tropism between the swine strains warrants further investigation.\n\n【38】##### Chile H1N2 Virus in Respiratory Droplets\n\n【39】Eurasian swine H1N2 viruses transmit in ferrets by direct contact and, in some cases, by respiratory droplet . To test the transmissibility of sw/Chile virus, we intranasally inoculated ferrets with 10 6  TCID 50  units of sw/Chile, Mem/87, or sw/IA viruses; at 1 dpi, naive ferrets were either placed in direct contact or housed in cages adjacent to the donor ferrets to monitor respiratory droplet transmission. Nasal washes were collected every 2 dpi to assess viral shed, and animals were monitored through 21 dpi. All of the donor animals shed virus, and all viruses transmitted through direct contact and respiratory droplet with some differences . Overall, the swine viruses replicated to a higher titer than the Mem/87 virus did in the donor and direct contact animals; swine viruses also transmitted faster by respiratory droplet. Respiratory contacts of sw/Chile virus–inoculated animals were shedding virus by 6 dpi, whereas Mem/87 virus–inoculated animals only showed detectable nasal washes after 8 dpi and 10 dpi. The sw/IA virus appeared to transmit faster and replicate to higher titer than did the sw/Chile and Mem/87 viruses . All animals seroconverted at 21 dpi. These studies highlight that both swine H1 δ viruses transmitted effectively in ferrets.\n\n【40】##### Decreased HI Antibody Titer in Persons Born after\n\n【41】Because the HA and NA of sw/Chile were derived from human seasonal strains originating around 1990, we hypothesized that persons born after 1990 would have decreased HI antibody titers. To test this, we conducted HI analyses against CA/09, Mem/87, sw/IA, and sw/Chile viruses on serum samples from 137 persons in North America collected in 2010–2015; samples were grouped by decade of birth (n = 7–22 samples/group). Most persons born before 1990 had titers against all 4 viruses . However, few persons born after 1990 had titers against sw/Chile (9%, panel B); sw/IA (31.8%, panel C); or Mem/87 (27.2%, panel D) viruses. Persons with titers ranging from 1:10 to 1:160 were all born in 1990 or 1991. In contrast, most (91.7%) persons born after 1990 had titers against pH1N1 virus, ranging from 1:320 to 1:10,240 .\n\n【42】### Discussion\n\n【43】Our data show that the swine H1N2 virus from Chile is antigenically distinct from other H1 viruses, replicates efficiently in mammalian cells, can transmit by respiratory droplet, and might pose a risk to immunologically naive persons born after 1990. Our study furthers the data recently published on prevalence and HA phylogeny of Chile swIAVs . We show that these viruses are circulating outside of commercial swine herds in BPFs, where animal–human interaction is high.\n\n【44】Since emergence of pH1N1 in 2009, reverse transmission of the virus from humans to pigs has been documented worldwide, resulting in several reassortments with classical swine and seasonal human viruses . As of July 2016, variant (v) influenza viruses have caused over 380 human infections in the United States alone. While most of these infections have been H3N2v viruses, 8 have been H1N2v viruses. These infections are particularly concerning because most infections occurred in children (persons <18 years of age) who had direct or indirect exposure to swine . Census estimates indicate that 50% of BPFs in Chile have \\> 1 person born after 1990, and 35% have \\> 1 person born after 2000 . Our data indicate that younger age groups have reduced HI titers to sw/Chile virus; however, studies with serum samples obtained from throughout the population of Chile are warranted to better determine the risk these viruses pose to swine herds.\n\n【45】This study is subject to several limitations. First, these studies were limited to 1 virus isolate. A panel of swine viruses from Chile isolated from different years and regions warrant study, especially if genetic differences are found among viruses. Second, these results represent a limited number of the backyard production system within Chile. Enhanced swine surveillance throughout Chile and South America is needed. Third, studies at the animal–human interface are required to decipher the risk circulating swIAVs pose to humans. We were limited to serum samples from persons living in North America, and further work should be done on South America populations, especially with those exposed to swine. Antigenic cartography on serum samples from persons involved in backyard farming would provide invaluable new information on transmission of swine viruses to humans. Finally, further work is needed to improve understanding of swIAV genes involved in transmission in the ferret model.\n\n【46】Despite no evidence of sustained human-to-human transmission, increased risk for human infection with H1N2v viruses calls for further study and enhanced monitoring. Risk for possible emergence of zoonotic strains has been demonstrated worldwide. H1N2 viruses isolated from pigs in China replicated efficiently in mice and pigs , while viruses from South Korea and Europe transmitted to and even caused death in ferrets . Future work will focus on how these reassortants arose in Chile BPFs and the role of specific genetic differences in pathogenicity and transmissibility. In addition, further studies for seroprevalence in Chile BPF workers and antigenic cartography with the sw/Chile virus are necessary. Studies at the animal–human interface are needed to determine seroprevalence, identify broadly protective antibodies, and characterize T-cell responses, and active surveillance is needed to uncover infection rates to better determine the risk for zoonotic transmission of swine-origin H1N2 to human health. In summary, our findings highlight the need for continued, vigilant influenza virus surveillance in Chile and throughout South America.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "29155018-c02f-442a-a912-152faeacc16c", "title": "Promoting Evidence-Based Decision Making in a Local Health Department, Pueblo City–County, Colorado", "text": "【0】Promoting Evidence-Based Decision Making in a Local Health Department, Pueblo City–County, Colorado\nBackground\n\n【1】Evidence-based decision making (EBDM) provides a framework to address many critical challenges (eg, setting priorities, making efficient use of resources) facing the public health system . EBDM is a process used to determine the best intervention for a population; it is rooted in community needs, practitioner experience, and existing evidence . EBDM produces high-quality information on what works in populations, resulting in implementation of successful programs and policies, greater workforce productivity, and more efficient use of funding . Public Health Accreditation Board Standard 10 requires the use and dissemination of evidence , which contributes to the momentum for EBDM.\n\n【2】Although the key tenets of EBDM are now well established , information is sparse on how to implement EBDM concepts in day-to-day public health practice. Research shows that EBDM practice does not happen organically ; there are common barriers to engaging local health departments in EBDM. These include philosophical differences between practitioners and researchers; lack of time, money, or incentives; and insufficient organizational support . One survey of 447 state and territorial health care practitioners treating chronic disease showed that the strongest barriers to using EBDM were organizational factors . Other national data show the importance of agency size and make-up (ie, types of staff members and their qualifications) in EBDM .\n\n【3】Organizational structures and activities associated with EBDM performance, called administrative evidence-based practices (AEBPs) , fall into 5 domains: workforce development, leadership, organizational culture and climate, relationships and partners, and financial characteristics of the agency . This article describes an assessment of AEBPs and individual staff-member factors in a local health department involved in adopting EBDM, the activities related to promoting EBDM that took place over 1 year, and the impact of those activities on staff attitudes and perceptions toward EBDM.\n\n【4】Community Context\n\n【5】The Pueblo City–County Health Department (PCCHD), located in southeastern Colorado, was selected for this case study because of the public health director’s interest in establishing EBDM as standard practice throughout the department. Initial contact was made between the director and the research team through an EBDM training. PCCHD is a mid-sized health department governed by a local board of health. It serves a population of 159,000 that is 54% non-Hispanic white and 42% Hispanic. Approximately 18% of residents in PCCHD’s jurisdiction have incomes below the federal poverty limit, and 72% have completed high school . PCCHD has approximately 90 employees in 8 departments: Women, Infants, and Children program; budget and finance; vital statistics; laboratory; environmental health; community health services; administration and disease prevention; and emergency preparedness. The department’s operating budget is $6.7 million, with funding from local, state, and federal government and from private foundations. At the start of this study, the director was a physician with a master’s degree in public health who had been at PCCHD for 22 years. The director had invested in building the EBDM capacity of her staff and wished to create the infrastructure and organizational culture that would make EBDM the norm in the department. The objective of this study was to improve EBDM among PCCHD staff members, which would be evidenced by changes in staff members’ perceptions and knowledge of EBDM from baseline to follow-up 1 year after community engagement began.\n\n【6】Methods\n\n【7】This study used a mixed-methods approach consisting of a quantitative survey, individual staff member interviews, and a focus group. The survey was based in part on the project team’s prior research . We measured a set of AEBPs and EBDM skills by asking respondents to rank them according to importance and availability.\n\n【8】On the basis of results of the initial quantitative survey, gaps between scores for importance and availability and low-ranking AEBPs were identified, and qualitative interview questions were developed to explore those gaps. The individual interviews consisted of 24 questions, which were reviewed for comprehension with 1 staff member and 1 public health professional from a different local health department.\n\n【9】In addition to the individual interviews, 1 focus group was conducted and opened to all staff members in order to allow those who were not interviewed a chance to express their views. A shortened 12-question version of the individual interview was given to focus group participants. The institutional review board of Washington University in St. Louis approved this project.\n\n【10】### Data collection\n\n【11】In February 2013 a baseline survey was sent to 74 PCCHD staff members chosen by administrators on the basis of how pertinent EBDM was to the type of work the staff members performed. Email addresses were acquired from PCCHD, and surveys were distributed via individualized links generated using the Qualtrics online research suite . PCCHD administrators encouraged the staff to complete the surveys through face-to-face interaction or email. No incentives were given. The follow-up survey contained the same questions as the initial one with an additional 4 questions to assess the impact of activities that occurred during the time between surveys. The follow-up survey was administered to all staff members who received the initial survey and were still employed by PCCHD in April 2014 (n = 46).\n\n【12】Drawing from respondents to the quantitative survey, a random sample of 11 staff members was selected to participate in a semistructured face-to-face interview.\n\n【13】### Data analysis\n\n【14】We calculated frequencies and proportions for both participant characteristics and for the various survey items. Several categorical items were recoded as binary (1 = yes, 0 = no or do not know), and χ 2  tests were used to assess differences in proportions between the 2 surveys. To examine mean differences, we conducted paired sample _t_ tests (repeated measure) for items related to organizational and individual skills. To describe the magnitude and direction of any significant differences, we calculated effect sizes. Cohen’s _d_ values were calculated for mean differences observed in _t_ tests. Cohen suggests the following effect ranges for _d_ : 0.2, small; 0.5, medium; and 0.8, large . In addition, we calculated Cramér’s _V_ values for effect sizes for differences in proportions. Cramér’s _V_ may be interpreted as 0.1, small effect size; 0.3, medium effect size; and 0.5, large effect size . All analyses were conducted in SPSS version 20 (IBM Corp) or Microsoft Excel version 2010 (Microsoft Corp).\n\n【15】All responses were recorded and were confidential and anonymous. The interviewer transcribed the qualitative interviews. Open coding was completed for each transcript to ascertain general themes. The interviewer next used focused charting to develop a codebook that was distributed to a team of researchers. The team made changes to more adequately capture codes and subcodes. The final codebook was then used by the interviewer to analyze each script. In addition, 1 research assistant analyzed 3 scripts for interrater reliability. Discrepancies between reviewers were minimal. A nonparticipant transcribed the content of the focus group, which was then analyzed by verbally verifying content for accuracy among the group, and then capturing major themes and concluding remarks by using open coding.\n\n【16】Outcome\n\n【17】### Initial survey, staff interviews, focus group\n\n【18】Of the 74 staff members who received the baseline survey, 59 completed it, an 80% response rate. The majority of respondents were technical experts (ie, planner, grant writer, epidemiologist, health educator, environmental health specialist, emergency preparedness educator \\[49.2%\\]) or other (23.7%), which included nurses, a registered dietician, administrative assistants, accountants, and the vital statistics staff. On average, participants had spent 5.7 years in their current position (standard deviation \\[SD\\], 6.9), and 9.5 years in public health (SD, 8.5). Two AEBP domains ranked lowest in the surveys, leadership and organizational climate and culture . The personal skill for which respondents reported the highest proficiency was the ability to develop evidence-based interventions. Staff members reported that training in EBDM would most encourage them to use EBDM .\n\n【19】Among 11 staff members interviewed, the length of employment at PCCHD ranged from 1.5 to 21 years. Nine interviewees were line staff, and 2 held manager or supervisory roles. Five of 8 departments were represented.\n\n【20】Qualitative findings were grouped into benefits and barriers to EBDM use and action steps to promote EBDM at PCCHD . The benefits to using EBDM cited most often were that it fosters a targeted use of limited resources and that it improves service to the community. Barriers to EBDM use at PCCHD were a lack of capacity and internal and external inflexibility (ie, resistance to change).\n\n【21】Staff recommended several action steps for improving the use of EBDM at PCCHD. First, health department leaders and managers should clearly communicate EBDM messages to various departments and model EBDM as a process, in a manner that crosses categorical programs. A second suggestion was to listen to and follow up on staff suggestions. Third, staff members requested positive feedback from leadership to encourage use of EBDM. Finally, staff members wanted to understand the relevance of EBDM to their work. As for training in EBDM, and consistent with adult education theories , staff respondents indicated that the most useful training experiences are those that are hands-on and have practical application to their professional duties.\n\n【22】Focus group findings were also clustered into 3 general topics related to EBDM: benefits, barriers, and action steps. Respondents said that benefits to using EBDM in a local health department were “smart business”: EBDM can improve programs, help government agencies stay accountable, determine program effectiveness, and improve efficiency. The barriers were internal and external politics, funding and time limitations, personal agendas, differences in recognizing the value of EBDM across departments within local health departments, and limited knowledge and self-efficacy for using the process. When asked, “If you were the administrator and wanted to make EBDM a priority, what would you do to bridge the gap between benefits and barriers to EBDM?” the overwhelming majority agreed that securing sustainable funding, rather than grant-focused funding, would be the most beneficial. Other responses included explaining how EBDM is pertinent to each department, demonstrating and explaining specific steps in EBDM, and improving the public’s understanding of the purpose of public health.\n\n【23】### Activities between surveys\n\n【24】Baseline survey findings were presented to key administrators at PCCHD, the local board of health, and to all PCCHD staff in several meetings. Presentations were a collaborative effort between the research team and PCCHD administration and staff. The results were shared in order to engage the PCCHD staff in developing policies and activities aimed at reducing the barriers to EBDM.\n\n【25】The following is a summary of the recommendations given at the conclusion of the research assistant’s (A.H.’s) time with PCCHD:\n\n【26】*   Clearly define and differentiate workforce improvement initiatives.\n\n【27】*   Pay attention to EBDM messaging, modeling, and follow-up activities, because they are vital for the success of EBDM.\n\n【28】*   Ensure every person and division understands the value of EBDM and how each staff member can use the process to improve service.\n\n【29】*   Ensure that administration and management regularly discusses EBDM ideas and progress in meetings.\n\n【30】*   Create an EBDM promotion team from among staff members trained in EBDM.\n\n【31】Two-thirds of the staff agreed or strongly agreed that these recommendations accurately captured staff behaviors and feelings about EBDM.\n\n【32】Following the assessment period, the research team maintained communication with the PCCHD community via conference calls and email to learn about, and track progress on, their plans for addressing improvement in the uptake of EBDM. Notably, many strategies were operationalized through collaboration between staff members and external partners. For example, PCCHD partnered with a state university to tailor a 3-day EBDM training for PCCHD staff that was based on an established training model . Agency leaders invited all staff members who held positions that allowed them to make evidence-based decisions. An EBDM team in PCCHD made up of staff members with EBDM training and experience was identified as a resource for staff members to contact for consultation as they learned the EBDM process. Administrators clarified messages about EBDM (ie, that it is a process, not a specific program) as the results of the survey were shared with staff members. To minimize confusion, messages and concepts related to EBDM, quality improvement, and the plan-do-check-act model (a management method used for continuous improvement) were merged into 1 decision flowchart and presented to the entire staff . EBDM was then incorporated into the 5-year strategic plan by the PCCHD strategic planning committee, where it now states that PCCHD “will implement a quality improvement plan using EBDM.” The local board of health was asked to promote the process by asking the PCCHD administration to formalize the use of EBDM in PCCHD as they made decisions on policies and programs. For example, EBDM language was incorporated into several PCCHD policies, namely the formal grant process, personnel policies, the strategic plan, and supervisor guidelines (in process).\n\n【33】### Follow-up survey\n\n【34】The follow-up survey was sent to the 46 staff members who were respondents to the initial survey and who were still employed at PCCHD; the survey had a 100% response rate. Like the initial survey, the majority of respondents were technical experts (50.0%) or other (23.9%). Participants had an average of 6.9 years in their current position (SD, 7.0), and 10.3 years in public health (SD, 8.5). Between surveys, 55.9% of the respondents participated in an all-staff meeting where information on EBDM was presented, and 52.5% attended the tailored 2-day training.\n\n【35】Significant changes were made in the AEBPs in the follow-up survey for access to training in EBDM and quality improvement processes, access to current information on EBDM, and knowing PCCHD allocated resources for quality improvement . Significant improvement in individual skills assessed in the follow-up survey were having skills in developing evidence-based interventions, possessing communication skills, and acknowledging the self-sustaining nature of evidence-based interventions.\n\n【36】Changes between pre- and post-survey data on activities that would most encourage staff to use EBDM  are noteworthy. For example, at baseline survey, training in EBDM was ranked as the activity most likely to motivate staff members to use EBDM. At the post-training survey, training in EBDM had shifted to third highest priority, and encouragement in use of EBDM was ranked highest. This change suggests that, after administrators provided training opportunities, the next phase in increasing use of EBDM was for administrators to provide positive feedback or encouragement in employees’ use of EBDM.\n\n【37】The success of PCCHD staff engagement was largely due to administrative support. Administrators were dedicated to delivering messages about EBDM in ways their staff members found helpful. They combined EBDM and quality improvement processes in order to make using both less confusing. The partnership between a state university and the PCCHD was crucial to delivering the 2-day EBDM training, which was tailored to address the needs identified by staff during the assessment phase.\n\n【38】Interpretation\n\n【39】This case study provides evidence that a local health department can improve attitudes toward EBDM among its staff in a relatively short time. First, through qualitative and quantitative evaluation, we were able to understand the barriers a local health department faced in using EBDM. Second, we showed evidence of staff engagement in improving attitudes and perceptions of EBDM principles as shown by two-thirds of the staff reporting that the team accurately captured their attitudes toward EBDM and significant improvement in several quantitative outcomes.\n\n【40】This study had several limitations. The main limitation relates to community engagement in that not all staff members were included in the survey. Because the pool of respondents was chosen by PCCHD administrators on the basis of job descriptions, results may be skewed in favor of EBDM practice. Another limitation is that 17% (10/59) of the original sample was lost to follow-up because they were no longer employed at PCCHD. In addition, organizational activities occurred during the year from baseline to follow-up, and activities occurring close in time to follow-up data collection may have been recalled more clearly, allowing the potential for bias. Finally, the context-specific nature of a case study limits the generalizability of any findings to other local health departments.\n\n【41】The activities conducted by PCCHD show evidence of improvement in attitudes and perceptions toward EBDM over 1 year. Previous studies suggest that strong leadership is a key ingredient in creating a work climate conducive to EBDM in a local health department . At baseline, the PCCHD staff members ranked organizational culture and climate as one of the lowest AEBPs. However, in 1 year, through continuing community engagement, and with an administration committed to making EBDM standard practice, organizational culture and climate were no longer among the lowest ranked AEBPs. The workforce development efforts described here may have influenced this improvement. Enhancing the climate and culture of a local health department is likely a pivotal activity for enhancing the uptake of EBDM .\n\n【42】This case study of a local health department community builds on previous work  by providing insight into steps that may improve EBDM adoption within local health departments. Additional observational studies may examine the dissemination of EBDM among PCCHD’s network with other local health departments to assess dissemination and implementation within the public health system . EBDM training might focus on hands-on case studies and use of a range of potential resources  that pertain to various topics covered by different departments within local health departments and other community-level agencies (eg voluntary health organizations). In practice, leadership may seek to concentrate on fostering a culture and climate conducive to using EBDM by establishing its relevance to all staff members, reiterating that EBDM includes community and practitioner input, and including staff members in key decision making processes.\n\n【43】Tables\n------\n\n【44】#####  Table 1. Responses of Administrative and Individual Staff Members to Baseline (N = 74) and Follow-Up (N = 46) Surveys, Evidence-Based Decision Making Practices (EBDM), Pueblo City–County, Colorado, Health Department, February 2013 and April\n\n| EBDM Effect on Work Practice, by Domain | Baseline Survey, February 2013 | Follow-Up Survey, April 2014 | _P_ Value a | Cramér’s _V_ | Cohen’s _d_ |\n| --- | --- | --- | --- | --- | --- |\n| **Administrative practice** | **Administrative practice** | **Administrative practice** | **Administrative practice** | **Administrative practice** | **Administrative practice** |\n| **Leadership, mean (SD)** | **Leadership, mean (SD)** | **Leadership, mean (SD)** | **Leadership, mean (SD)** | **Leadership, mean (SD)** | **Leadership, mean (SD)** |\n| Enhanced my ability to lead in EBDM b | 5.6 (1.2) | 6.0 (1.1) | .14 | — | 0.3 |\n| Encouraged use of EBDM b | 5.6 (1.5) | 5.9 (1.3) | .27 | — | 0.2 |\n| Fostered staff participation in decision making b | 5.0 (1.6) | 4.9 (1.9) | .77 | — | 0.1 |\n| Hires people with public health degree b | 4.3 (1.4) | 4.7 (1.2) | .14 | — | 0.3 |\n| Hires people with experience in public health b | 4.6 (1.3) | 4.8 (1.2) | .53 | — | 0.1 |\n| Overall domain c (%) | 41.0 | 51.7 | — | — | — |\n| **Organizational climate and culture of agency, mean (SD)** | **Organizational climate and culture of agency, mean (SD)** | **Organizational climate and culture of agency, mean (SD)** | **Organizational climate and culture of agency, mean (SD)** | **Organizational climate and culture of agency, mean (SD)** | **Organizational climate and culture of agency, mean (SD)** |\n| Culture that supports EBDM b | 5.3 (1.5) | 5.5 (1.4) | .48 | — | 0.1 |\n| Access to current research evidence b | 5.3 (1.4) | 5.8 (1.2) | .11 | — | 0.3 |\n| Promotes life-long learning b | 5.7 (1.5) | 5.7 (1.6) | .98 | — | 0.0 |\n| Access to EBDM information relevant to community needs b | 5.4 (1.3) | 5.7 (1.6) | .36 | — | 0.2 |\n| Overall domain c , % | 51.2 | 70.7 | — | — | — |\n| **Financial characteristics of agency, N (%)** | **Financial characteristics of agency, N (%)** | **Financial characteristics of agency, N (%)** | **Financial characteristics of agency, N (%)** | **Financial characteristics of agency, N (%)** | **Financial characteristics of agency, N (%)** |\n| Funded through several sources d | 41 (95.3) | 43 (95.6) | .96 | 0 | — |\n| Allocated resources for quality improvement d | 9 (20.0) | 30 (65.2) | <.001 | 0.5 | — |\n| Overall domain c , % | 55.9 | 79.4 | — | — | — |\n| **Workforce development, N (%)** | **Workforce development, N (%)** | **Workforce development, N (%)** | **Workforce development, N (%)** | **Workforce development, N (%)** | **Workforce development, N (%)** |\n| Access to training in EBDM d | 23 (57.5) | 38 (92.7) | <.001 | 0.4 | — |\n| Access to training in quality improvement processes d | 19 (42.2) | 32 (71.1) | .01 | 0.3 | — |\n| Access to training in management practices d | 26 (57.8) | 24 (52.2) | .59 | 0.1 | — |\n| Access to training in performance assessment d | 29 (64.4) | 28 (60.9) | .73 | 0 | — |\n| Access to current information on improving EBDM processes b , mean (SD) | 4.8 (1.4) | 5.5 (1.4) | .03 | — | 0.5 |\n| Overall domain c , % | 59.2 | 66.1 | — | — | — |\n| **Relationships and partnerships, mean (SD)** | **Relationships and partnerships, mean (SD)** | **Relationships and partnerships, mean (SD)** | **Relationships and partnerships, mean (SD)** | **Relationships and partnerships, mean (SD)** | **Relationships and partnerships, mean (SD)** |\n| Partnerships have missions that align with agency b | 5.5 (1.2) | 5.5 (0.9) | .96 | — | 0.0 |\n| Important to have partners who share resources b | 5.4 (1.1) | 5.8 (1.1) | .12 | — | 0.3 |\n| Important to develop partnerships with both health and other sectors b | 6.2 (0.9) | 6.4 (0.7) | .26 | — | 0.2 |\n| Overall domain c , % | 59.4 | 73.2 | — | — | — |\n| **Individual practice, mean (SD)** | **Individual practice, mean (SD)** | **Individual practice, mean (SD)** | **Individual practice, mean (SD)** | **Individual practice, mean (SD)** | **Individual practice, mean (SD)** |\n| I have skills necessary for developing evidence-based interventions b | 5.3 (1.3) | 5.9 (0.8) | .02 | — | 0.4 |\n| I can effectively communicate information on evidence-based strategies to policy makers b | 4.9 (1.5) | 5.6 (1.1) | .02 | — | 0.4 |\n| I feel I need to be an expert on many issues in order to effectively make evidence-based decisions b | 4.7 (1.5) | 4.2 (1.3) | .09 | — | 0.3 |\n| My fears about job security prevent me from using EBDM b | 2.5 (1.6) | 2.3 (1.4) | .51 | — | 0.1 |\n| I feel evidence-based interventions are packaged in a way I can use them b | 4.7 (1.2) | 5.0 (0.8) | .21 | — | 0.2 |\n| I feel evidence-based interventions are designed in a way to be self-sustaining b | 4.7 (1.0) | 5.2 (1.1) | .04 | — | 0.5 |\n\n【46】Abbreviation: SD, standard deviation; —, not applicable.  \na  _P_ values are reported for χ 2  test, paired-sample _t_ test, or independent _t_ tests.  \nb  Seven-point Likert scale response option (7 = strongly agree; 1 = strongly disagree).  \nc  Expressed as a percentage of yes for yes/no/don’t know and “strongly agree” or “agree” for Likert response options within each domain.  \nd  Yes/no/don’t know response option.\n\n【47】#####  Table 2. Rankings of Activities That Would Most Encourage Staff to Use Evidence-Based Decision Making Practices (EBDM), a  Baseline and Follow-Up Surveys of Administrative and Individual Staff Members (N = 46), Pueblo City–County, Colorado, Health Department, February 2013 and April\n\n| Rank a | Baseline Survey, February 2013 | Follow-Up Survey, April 2014 |\n| --- | --- | --- |\n| 1 | Training on EBDM | Positive feedback or encouragement to use EBDM |\n| 2 | Placing high priority on EBDM by leaders in my agency | Placing high priority on EBDM by leaders in my agency |\n| 3 | Positive feedback or encouragement to use EBDM | Training on EBDM |\n| 4 | Professional recognition for use of EBDM | Professional recognition for use of EBDM |\n| 5 | Performance evaluation that considers use of EBDM | Performance evaluation that considers use of EBDM |\n\n【49】a  The 5 items were ranked on a scale of 1 to 5 (1 = highest ranking; 5 = lowest ranking).\n\n【50】#####  Table 3. Responses of Administrative and Individual Staff Members (N = 46) to Open-Ended Interviews, Evidence-Based Decision (EBDM) Making Practices, Pueblo City–County, Colorado, Health Department, June–July\n\n| Theme | Response |\n| --- | --- |\n| Benefits of using EBDM | Better use of resources: “\\[Using EBDM\\] we would stop doing things that aren’t essential and that would free up resources to do something else or do it right . you look at the numbers and do what’s right.” |\n| Benefits of using EBDM | Better service to community: “ . letting people know the health department is doing this because we . are a credible agency and we want to be taken seriously, we value the information that we’re giving you and we want to make sure that what we are doing is going to work for you.” |\n| Barriers to using EBDM | Lack of capacity (finances, personnel, time): “There’s not enough capacity in public health to do everything that evidence would show us would be a wise approach.”; “It’s really hard; they (funding agencies) are telling us, ‘use evidence-based, but we’re not going to give you . in fact we’re going to give you less money.’”; “\\[W\\]e’ve been short staffed, so it’s been difficult.” “Often times that gets difficult when you’re down at the level of doing a lot of work because you’re trying to get through the day and deal with all the things that come up.” |\n| Barriers to using EBDM | Internal inflexibility: “Being that I guess we’ve never done \\[EBDM\\] in the other programs, it might be a little difficult to get some people on board with that because . people have been doing things a certain way for years.” |\n| Barriers to using EBDM | External inflexibility: “We’ve always done things this way in Pueblo.” |\n| How to promote EBDM to staff | Explain how EBDM fits within each job description/department: “I think every program is so different that you can’t really just throw one instance of . the proof of EBDM for this program. I think each program would have to have its own SOP \\[standard operating procedure\\] written up. Maybe the directors would be able to explain to the people that work in that department . the process to go through.” |\n| How to promote EBDM to staff | EBDM is a process, not program: “I think the one thing that happened . that I found very frustrating, and I think other people did too, is \\[EBDM\\] became like a buzz word; it had to be a program, not just evidence-informed, that it had to be like this evidence-based program in order to be successful. And I think that turned people off.” |\n| How to promote EBDM to staff | Model use of EBDM: “\\[Hearing administration’s\\] use of evidence-based programming and their successes with it would help promote an environment that supports EBDM.” |\n| How to promote EBDM to staff | Listen to and follow-up on staff suggestions: “So actually following through on those ideas and saying ‘Well, we could really fix this’ in a meeting, but then everyone going back to their silo and never talk\\[ing about it\\] is the death of all that \\[feedback\\].” |\n| How to promote EBDM to staff | Positive reinforcement: “I mean we wouldn’t get into public health for the money or anything like that. So I think just hearing a ‘Good job!’” |\n| Training | Hands-on experience that pertains to their individual interests: “So I think the more we can get away from things like Webinars \\[the better\\] . So those types of \\[training\\] of hands-on with practical application are very good.” |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "98eae7bc-da37-4196-89d6-9d1926e26265", "title": "Differences in Knowledge of Breast Cancer Screening Among African American, Arab American, and Latina Women", "text": "【0】Differences in Knowledge of Breast Cancer Screening Among African American, Arab American, and Latina Women\nAbstract\n\n【1】**Introduction**We examined differences in knowledge and socioeconomic factors associated with 3 types of breast cancer screening (breast self-examination, clinical breast examination, and mammogram) among African American, Arab, and Latina women. **Methods**Community health workers used a community-based intervention to recruit 341 women (112 Arab, 113 Latina, and 116 African American) in southeastern Michigan to participate in a breast cancer prevention intervention from August through October 2006. Before and after the intervention, women responded to a previously validated 5-item multiple-choice test on breast cancer screening (possible score range: 0 to 5) in their language of preference (English, Spanish, or Arabic). We used generalized estimating equations to analyze data and to account for family-level and individual correlations. **Results**Although African American women knew more about breast cancer screening at the baseline (pretest median scores were 4 for African American, 3 for Arab and 3 for Latina women), all groups significantly increased their knowledge after participating in the breast cancer prevention intervention (posttest median scores were 5 for African American and 4 for Arab and Latina women). Generalized estimating equations models show that Arab and Latina women made the most significant gains in posttest scores ( _P_ < .001). **Conclusion**Racial/ethnic differences in knowledge of breast cancer screening highlight the need for tailored information on breast cancer screening for African American, Arab, and Latina women to promote adherence to breast cancer screening guidelines.  \n\n【2】Introduction\n\n【3】Despite growing interest in factors associated with cancer-related disparities and interventions to reduce disparities, information is still limited regarding differences in screening practices by geographic location and effective tailored interventions for specific racial/ethnic groups. African American and Latina women have disproportionately higher death rates due to breast cancer yet nationally lower incidence rates than those of their non-Hispanic white counterparts . Although screening rates for breast cancer have increased for African American and Latina women, breast cancer cases among these populations are often diagnosed at later stages, which limits treatment options . Although national data illuminate prevailing disparities, data are not available for women who are categorized as non-Hispanic white but who consider themselves to be of a different racial/ethnic group and are low-income or medically underserved or both. In this study we included Arab women traditionally categorized as non-Hispanic white but representing 10 northern African countries and 12 Persian Gulf countries  who are not ethnically identifiable in national databases such as the Behavioral Risk Factor Surveillance System (BRFSS) . As women’s screening rates for breast cancer increase nationally, barriers to screening remain for African American, Arab, and Latina women from poor backgrounds. Multiple factors are associated with breast cancer screening rates among these groups of women, including socioeconomic status, health insurance coverage, usual source of care, perceptions and fears about breast cancer, race, ethnicity, age, and knowledge of breast cancer screening . A special edition of the Michigan Cancer Behavioral Risk Factor Survey (BRFS) conducted in 2006  disaggregated Arab women from non-Hispanic white women. The survey showed that screening rates for annual mammography and clinical breast examination were 53% (95% confidence interval \\[CI\\], 6%) and 46% (95% CI, 20%) for African American and Latina women aged 40 years or older, respectively, which were lower than rates for the general Michigan population (54% \\[95% CI, 4%\\]) and for Arab women (68% \\[95% CI, 24%\\]). Nationally, the 2005 BRFSS indicated that recent mammography screening rates for African American women aged 40 years or older were approximately 67% and for Latinas aged 40 years or older were approximately 65%, comparable to rates for non-Hispanic white women (63%). Mammogram screening data serve as a proxy for adherence to guidelines that recommend yearly mammograms for women aged 40 or older . However, the combination of mammography screening and clinical breast examination would give a more accurate picture of current breast cancer screening behaviors overall; guidelines also recommend clinical breast examinations every 3 years for women in their 20s and 30s and every year for women aged 40 years or older . We studied differences in knowledge of and socioeconomic factors associated with breast cancer screening and assessed baseline and postintervention test scores on the knowledge of breast cancer screening practices among African American, Arab, and Latina women who participated in the Kin Keeper Cancer Prevention Intervention, a family-focused educational intervention for women . We hypothesized that, whereas all 3 population groups should be targeted for the educational intervention, Arab and Latina women have lower socioeconomic status and lower levels of knowledge about screening for breast cancer than do African American women. These racial/ethnic differences are important for the design of tailored interventions because they are likely to affect the screening behaviors and, ultimately, the breast health outcomes among these groups of women.  \n\n【4】Methods\n\n【5】### Data collection and sample\n\n【6】The Michigan State University institutional review board approved this study. The locations of the community-based study were southeastern Michigan, in the cities of Detroit and Dearborn. Detroit is the largest city in Michigan and the 11th most-populated city in the United States. Approximately 80% of the residents are African American and 5% are Latino/a, according to the 2000 US Census. The 2006-2008 US Department of Treasury American Community Survey data for Michigan indicates that the city’s median annual household income is $29,526 compared with Dearborn’s $44,650. Nearly 33% of the residents in Dearborn are Arab; the city has the largest Arab population outside of the Middle East. Study participants came from 1 of 3 community-based organizations affiliated with the Detroit Department of Health and Wellness Promotion: 1) Village Health Worker Program, 2) Community Health and Social Services (CHASS), and 3) the Arab Community Center for Economic and Social Services (ACCESS). Although the health department serves all racial/ethnic groups, for the purposes of this study, it was asked to recruit only African American and Latina women through its healthy lifestyle program. CHASS, known for its specialized services to Latinos/as in Detroit, recruited Latinas from the REACH Detroit Partnership (a diabetes prevention and complications program). ACCESS, which is located in Dearborn, recruited Arab women from its healthy lifestyle program. The organizations had credibility in the community and employed community health workers (CHWs). We used the Kin Keeper model to recruit women and deliver breast cancer education in the homes of a family member (the _kin keeper_ ) . \n\n【7】### Kin Keeper Cancer Prevention Intervention\n\n【8】The Kin Keeper Cancer Prevention Intervention is community-based; it uses CHWs to educate groups of female family members about breast or cervical cancer or both . In 2 home visits, the CHWs use breast models and other educational visual aids to teach participants about breast cancer, early detection, clinical screenings, and breast self-examination. At each education session, the participants are given pretests and posttests designed to assess their knowledge of breast cancer screening. Respondents have the option to hear and see test questions and receive the educational intervention in their language of preference (English, Arabic, or Spanish) . As part of the Kin Keeper model, CHWs read the questions aloud while participants follow and write their responses. At the end of the second home visit, the CHW works with each participant individually to set up a personal action plan that allows her to set screening goals for a 12-month follow-up visit. For the purposes of this article, we focus only on the pretest and posttest results from the first 3 months (August through October 2006). \n\n【9】### Recruitment\n\n【10】This model has a unique recruitment method: CHWs recruit clients/kin keepers and kin keepers recruit female family members. It begins with cross-training CHWs (from their respective noncancer-related public health programs) in the basics of breast and cervical cancer prevention and control and recruitment of clients into the study . After 20 hours of training, CHWs ask clients from their public health programs to participate in the research project. The recruitment phase of this study was 3 months. Pretests and posttests  were administered by CHWs (7 from each racial/ethnic group) to 104 families, comprising 341 women (116 African Americans, 113 Latinas, and 112 Arabs). Clients had to be aged 18 or older, self-identify as belonging to 1 of the 3 racial/ethnic groups and have blood parents and both sets of blood grandparents in the same race/ethnicity, and be willing to gather 3 to 4 of their adult female family members (in any combination mother, sisters, grandmothers, aunts, daughters) for 2 home-based educational sessions delivered by the CHW. Because the CHW had an established relationship of trust with clients, recruiting clients into the study was not difficult — all eligible clients who were asked to participate in the study consented to become kin keepers. Although other family members were free to hear the education, they were not considered part of the research project. Kin keepers also helped CHWs to locate family members who were part of the study for the follow-up visit when necessary . All participants (kin keepers and family) answered questions about their knowledge of breast cancer screening in a familiar environment — the home. \n\n【11】### Home visits\n\n【12】Each family unit received 2 home visits. At the first home visit, participants signed informed consent forms and completed pre-intervention (baseline) sociodemographic forms and a 16-item assessment of breast cancer literacy. Both the sociodemographic and the cancer literacy assessments were administered orally; participants followed as the CHW read in the preferred language. Latina and Arab CHWs were bilingual and at some visits had to read in both English and the preferred language, which allowed us to measure participants’ actual knowledge about screening for breast cancer regardless of their ability to read and comprehend the assessment items. From the second home visits, 333 women were retained (114 African Americans, 112 Latinas, and 107 Arabs), resulting in an overall sample retention rate of approximately 98%. The 16-item assessment tool for breast cancer literacy has 3 domains: 1) cancer awareness, 2) knowledge and screening, and 3) prevention and control. It uses a multiple-choice and true-or-false format, and has been validated (English: Cronbach α, 0.99; Spanish: Cronbach α, 0.99; and Arabic: Cronbach α, 0.81) . For the purposes of this study, we focused only on the knowledge and screening domain, which consists of 5 items (Cronbach α, 0.78) . The 5 items are specific to knowledge of breast cancer screening . In addition, we analyzed variables from the sociodemographic questionnaire.  **Figure 1.** Distribution of pretest scores by race/ethnicity in the Kin Keeper Cancer Prevention Intervention, Dearborn and Detroit, Michigan, August through October 2006. The pretest evaluated baseline knowledge of breast cancer screening methods. The highest possible score on the pretest was 5.  After completing the intervention pretest to assess breast cancer literacy, participants received the education followed by a posttest. The home visit lasted 1.5 to 2 hours. CHWs who needed to speak in 2 languages took longer. At the end of the first home visit, the second home visit was scheduled for 1 to 3 weeks later. At the second home visit, the second posttest was administered before the second educational session. During the second educational session, CHWs cleared up myths, answered questions, and reviewed basic points. Then they administered a third posttest and worked with participants to complete a personal action plan. \n\n【13】### Outcome measures\n\n【14】We analyzed 5 binary outcome variables in this study, each corresponding to a response on the knowledge and screening domain of the assessment tool for breast cancer literacy (1 if correct and 0 if incorrect). We used baseline and postintervention responses (ie, repeated measures) to compare racial/ethnic differences in knowledge of breast cancer screening, which allowed us also to assess changes in knowledge of breast cancer screening for each race/ethnicity. \n\n【15】### Statistical analysis\n\n【16】We graphed the distributions of the pretest and posttest scores and computed descriptive statistics for the whole sample and by racial/ethnic group. Sociodemographic characteristics were analyzed by race/ethnicity. Two-sided χ 2 and Fisher exact tests were performed to test for categorical association. For these preliminary analyses, familial association was ignored. We used SAS version 9.1 (SAS Institute, Inc, Cary, North Carolina) to perform all statistical tests and modeling. Statistical significance was set at _P_ < .05. To analyze differences in women’s knowledge of breast cancer screening by race/ethnicity and across time, we considered generalized estimating equations (GEE) models  accounting for the familial associations and for participant associations (over time). We anticipated that our data would be correlated because women were recruited from the same families and longitudinal measurements were recorded on each participant. It is well-established that ignoring these associations (for example, by fitting a classical independence logistic regression model) is likely to yield incorrect standard errors of model estimates. The GEE method adjusts these standard errors by using the so-called robust sandwich estimator that corrects for any misspecification of the true underlying correlation . The probability of answering each question correctly was modeled separately, controlling for age, income, highest level of education attained, marital status, employment status, and health insurance status. The basic GEE regression model for the binary outcome w _kfij_ is given by the following equation: Equation  , \\[ Description of this equation \\] Let w _kfij_ be the binary variable defining whether the question under consideration is answered correctly (w _kfij_ \\= 1) or not (w _kfij_ \\= 0), for woman _i_ of family _f_ of race _k_ at time point _j_ ( _j_ \\= 1 for baseline test and _j_ \\= 2 for postintervention test). Parameter _β jk_ represents the log odds of answering correctly a question at time point _j_ for race _k_ , adjusted for covariates in the design vector _C kfij_ . The standard error of the estimate of _β jk_ is typically computed from the sandwich estimator of the variance-covariance matrix of the parameter vector estimate, which takes into account the longitudinal nature of the binary outcomes and the familial clustering.  \n\n【17】Results\n\n【18】Sociodemographic characteristics differed significantly by race/ethnicity . African American women had higher levels of education, employment, access to health insurance, and income. Approximately 53% of Arab women and 41% of Latina women had not completed high school education or obtained a general equivalency diploma, compared with approximately 3% of African American women ( _P_ < .001). Overall, 61% of the women were aged 40 years or older, the age category of women recommended for yearly mammography screening . Scores on baseline knowledge test  and postintervention tests  were compared by race/ethnicity. At baseline, African American women had higher knowledge scores; more than 30% obtained a perfect pretest score compared with 9% for Arab women and 5% for Latina women. Mean pretest scores were 3.91 of 5 (SD, 0.92) for African Americans, 3.02 (SD, 1.12) for Latinas, and 2.92 (SD, 1.18) for Arabs. Posttest scores for Latina and Arab women increased significantly ( _P_ < .001), as depicted by distribution, which is relatively skewed compared with the pretest distribution in Figure 1. Posttest median scores were 5 for African American women and 4 for Latina and Arab women. The percentage of women in each racial/ethnic group that answered each question correctly varied . Question 4, which asked the women to differentiate between types of screening test (self-examination, clinical examination, and mammography) proved to be the most difficult  for all racial/ethnic groups.  **Figure 2.** Distribution of posttest scores by race/ethnicity in the Kin Keeper Breast Cancer Prevention Intervention, Dearborn and Detroit, Michigan, August through October, 2006. The posttest evaluated knowledge of breast cancer screening methods after the intervention. The highest possible score on the pretest was 5.  In the pretest, Arab women had lower odds of correctly answering the first question (who does a breast self-examination?) than did African American women (odds ratio \\[OR\\], 0.27; 95% CI, 0.08-0.89), adjusted for other sociodemographic variables . Similarly, for the second question (who does a clinical breast examination?), and the fourth question, which tested the ability to distinguish screening types, Arab women were less likely to answer correctly in the pretest. However, for the third question (who does a mammogram?) and the last question, which distinguished who performs each screening type, the odds of answering the question correctly in the pretest were lower for Arab women but not significantly. For Latinas, pretest results were somewhat similar to those of Arabs. The GEE-based analysis showed that Latinas had lower odds of correctly answering each question in the pretest than African Americans, except for questions 3, 4, and 5, in which the GEE-based odds were not significant. Latinas had significantly lower odds of knowing the differences between breast self-examination, clinical breast examination, and mammogram than did African Americans. Posttest GEE estimates show that, compared with African American women, Arab women had lower odds of correctly answering questions 1 and 4, whereas Latina women had significantly lower odds of correctly answering question 4.  \n\n【19】Discussion\n\n【20】To our knowledge this is the first study to empirically compare knowledge of breast cancer screening types among African American, Arab, and Latina women. It yielded 3 major findings: 1) knowledge of breast cancer screening practices was highly associated with race/ethnicity; 2) Arabs and Latinas had similar patterns of knowledge and lower levels of education and insurance coverage compared with African American women; and 3) knowledge of breast cancer screening significantly increased for Latinas and Arabs after community-based intervention, as evidenced by posttest scores. Overall, the results show that, controlling for sociodemographic characteristics, Arab and Latina women had lower pretest and posttest scores for breast cancer screening knowledge compared with African American women, despite significantly improving their scores in the posttest. Respondents did not fully understand the differences in breast cancer screening types. African American women’s socioeconomic characteristics were highly associated with their higher knowledge levels. Overall, women with higher education levels had higher baseline knowledge of breast cancer screening and reported higher levels of breast cancer screening regardless of race/ethnicity. This finding is consistent with the results of previous studies that show education and access to health care are major predictors of cancer screening knowledge and practices . The higher scores on knowledge of breast cancer screening among African American women may also be a result of benefits they received from earlier cancer disparities research that focused on African Americans and breast cancer screening awareness campaigns conducted in English. For nonEnglish-speaking Latinas and Arabs, breast cancer awareness campaigns using Spanish and Arabic messages and materials  have been made available more recently. Also, popular breast cancer websites do not offer educational materials in Arabic. This disparity might explain the lower baseline scores among Arab and Latina women. More than half the Arab (54%) and Latina (54%) women reported not having health insurance. Other factors — low educational levels and limited English language proficiency, combined with low health literacy, limited availability of linguistically and culturally appropriate materials on breast cancer, and limited experience with the health care system — are strongly associated with knowledge barriers regarding breast cancer screening . Irrespective of a woman’s education level, posttest scores increased among the 3 groups; Latina and Arab women achieved the largest gains in knowledge of breast cancer screening. These findings demonstrate that information on breast cancer screening can be effectively provided when interventions address barriers. The Kin Keeper intervention mitigated 3 barriers: low education levels, literacy, and limited English language proficiency . If the intervention had not been administered orally and without language preference, 76 (68%) of 112 Arab women would have been missed. Oral administration of the questionnaire permitted the researchers to measure what respondents knew regardless of the respondents’ ability to read Arabic or English. Translated materials that are linguistically and culturally appropriate and at an appropriate literacy level  increase the effectiveness of cancer screening interventions for women of specific ethnicities. For Arab women, community-based interventions are necessary to reduce disparities, given that they are more likely than non-Hispanic white women to have irregular screenings and cancer that is detected at later stages . Language, access to health care, and geographic residence have also been found to adversely affect Latinas’ screening practices . Although CHWs completed the same training  and had the same basic information regarding specifics of breast cancer education, they had flexibility to tailor their home education sessions. The Latina and Arab CHWs found this especially helpful when talking with older participants, who did not speak or understood very little English, or women who did not socialize much and had various cultural or educationally shaped perceptions of cancer . The CHWs’ knowledge of their respondent population, and the CHWs sensitivity to women’s perceptions, cultural and language nuances, and questions informed the educational intervention to ensure that information was transmitted in the context of women’s lives. The posttest scores suggest that the intervention was effective. Data were not collected on whether racial/ethnic concordance of CHW with respondent increased participation or learning; it is an area for future inquiry. Methodologic cautions are warranted in terms of instrument and identification of populations. As observed, future research needs to include definitions of medical screening procedures for women to ensure their understanding of survey questions being administered, particularly for low-income women with limited proficiency in English. A community-based study, by design, represents the unique needs of a particular geographic area and designated population groups. Therefore, the results are not meant to be applicable to the general US population. Other limitations include lack of longitudinal data to report knowledge retention and length of residency status of Latinas and Arabs who selected a language other than English. The inclusion of Arab women presented several challenges. Although the Arab world comprises various countries, including 10 in Africa , Arabs have traditionally been classified as white and combined with other non-Hispanic whites and nonmultiracial groups in survey data. Therefore, their unique health status is masked. Furthermore, cancer is not readily discussed among Arab groups because of its severity and the perception that all cancer is hereditary . The similar socioeconomic characteristics of Arab and Latina women were predictive of lower baseline knowledge levels, suggesting that low education levels and lack of health insurance coverage are the most powerful predictors of knowledge of breast cancer screening regardless of ethnicity. Because of the complexity of cancer disparities, education interventions must be developed that are appropriate to the linguistic, health literacy, and cultural needs of participants. Population-specific materials need to be administered in conjunction with community-based participants such as CHWs. Increasing women’s knowledge about breast cancer screening is an important first step, but moving women in the direction of adherence to breast cancer screening guidelines and assuring their access to health care services would reduce disparities in breast cancer death and illness.  \n\n【21】Tables\n------\n\n【22】#####  Table 1. Sociodemographic Characteristics of Women Participating in the Kin Keeper Breast Cancer Prevention Intervention, Dearborn and Detroit, Michigan, August-October 2006 a\n\n【23】CharacteristicsAfrican American,No. (%) (n = 116)Arab American,No. (%)&nbsp; (n = 112)Latina,No. (%) (n = 113)Total,No. (%) (N = 341)P Value bAge, y18-3945 34 52 131 .0340-4923 38 24 85 ≥5048 40 36 124 Annual income, $I would prefer not to answer9 30 36 75 &lt;001&lt;10,0007 31 19 57 10,000-19,99936 17 36 89 20,000-39,99947 25 13 85 ≥40,00017 9 9 35 EducationSome college or higher61 26 33 120 &lt;001High school graduate or GED52 27 33 112 Less than high school or GED3 59 46 108 Marital statusMarried29 75 73 177 &lt;001Widowed/separated/divorced42 24 26 92 Single/never married43 10 13 66 Employment statusFull-time/self-employed75 16 46 137 &lt;001Part-time10 17 19 46 Unemployed27 73 14 114 Retired/not working because of disability4 4 33 41 Health insuranceHave health insurance110 46 51 207 &lt;001Do not have health insurance6 60 61 127 Language of instrumentEnglish116 36 28 180 NASpanish0085 85 Arabic076 076 Abbreviation: GED, general equivalency diploma; NA, not applicable.a Percentages may not total 100% because of rounding.b The χ 2 statistic was used to calculate _P_ values. \n\n【24】#####  Table 2. Baseline Knowledge of Breast Cancer Screening Methods by Race/Ethnicity, Kin Keeper Breast Cancer Prevention Intervention, Dearborn and Detroit, Michigan, August-October 2006 a,b\n\n【25】Pretest QuestionsAfrican American,No. (%) (n = 116)Arab, No.(%)(n = 112)Latina, No.&nbsp;(%)(n = 113)Total, No. (%)N = 341P Value cQuestion 1: Who does a breast self-examination?Incorrect7 32 38 77 &lt;001Correct109 77 74 260 Question 2: Who does a clinical breast examination?Incorrect3 28 24 55 &lt;001Correct113 81 88 282 Question 3: Who does a mammogram?Incorrect10 27 34 71 &lt;001Correct106 79 77 262 Question 4: Which of these commonly used screening practices are the same?Incorrect72 94 92 258 &lt;001Correct42 15 20 77 Question 5: Which of these statements is true?Incorrect32 46 35 113 .04Correct83 59 76 218 a Percentages may not total 100% because of rounding.b Answer choices are given in the Appendix.c The 2-sided Fisher exact χ 2 statistic was used to calculate _P_ values. \n\n【26】#####  Table 3. Racial/Ethnic Differences in the Probabilities of Answering Each Pretest and Posttest Question Correctly, Kin Keeper Breast Cancer Prevention Intervention, Dearborn and Detroit, Michigan, August-October 2006 a,b\n\n【27】Questions Arab AmericanLatinaPretestAOR (95% CI) aPosttestAOR (95% CI) aPretestAOR (95%CI)PosttestAOR (95% CI)Question 1: Who does a breast self-examination? (n = 324)0.27 (0.08-0.89)0.78 (0.03-3.10)0.25 (0.08-0.74)0.56 (0.19-1.69)Question 2: Who does a clinical breast examination? (n = 324)0.11 (0.02-0.48)0.52 (0.15-1.75)0.14 (0.04-0.54)0.39 (0.13-1.18)Question 3: Who does a mammogram? (n = 320)0.49 (0.17-1.43)0.93 (0.31-2.73)0.40 (0.16-1.01)0.98 (0.37-2.58)Question 4: Which of these commonly used screening practices are the same? (n = 322)0.34 (0.12-0.92)0.22 (0.09-0.56)0.37 (0.13-1.00)0.11 (0.04-0.30)Question 5: Which of these statements is true? (n = 320)0.43 (0.16-1.17)1.95 (0.70-5.45)0.69 (0.29-1.64)1.40 (0.58-3.36)Abbreviations: AOR, adjusted odds ratio, CI, confidence interval.a Adjusted odds ratios were simultaneously adjusted for age, income, highest level of education attained, marital status, employment status, and health insurance status. African American women are the reference group.b Answer choices are given in the Appendix.  \n\n【28】**Question 1: Who does a breast self-examination?** \n\n【29】> _A woman in her home every month._ A health care provider in a clinic or doctor’s office, once a year. An X-ray technician, once a year. \n\n【30】**Question 2: Who does a clinical breast examination?** \n\n【31】> A woman in her home every month. _A health care provider in a clinic or doctor’s office, once a year._ An X-ray technician, once a year. \n\n【32】**Question 3: Who does a mammogram?** \n\n【33】> A woman in her home every month. A health care provider in a clinic or doctor’s office, once a year. _An X-ray technician, once a year._ \n\n【34】**Question 4: Which of these commonly used screening practices are the same?** \n\n【35】> Breast self-examination and clinical breast examination. Clinical breast examination and mammogram. Mammogram and breast self-examination. All are the same. _None are the same._ \n\n【36】**Question 5: Which of these statements is true?** \n\n【37】> Breast self-examinations can be done monthly by all women. Clinical breast examinations can be done yearly by a health care provider. Mammograms can be done yearly beginning at age 40, by an X-ray technician. None of these statements are true. _All of these statements are true._ \n\nNote: The correct responses are italicized.   |  |\n| --- | --- | --- | --- |\n\n|  |  |  |  |\n| --- | --- | --- | --- |\n\n|  |  | \n* * *\n\nThe findings and conclusions in this report are those of the authors and do not necessarily represent the official position of the Centers for Disease Control and Prevention. HomePrivacy Policy | AccessibilityCDC Home | Search | Health Topics A-Z This page last reviewed March 30, 2012 Centers for Disease Control and PreventionNational Center for Chronic Disease Prevention and Health Promotion United States Department ofHealth and Human Services  |  |\n| --- | --- | --- | --- |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "52da940d-5c3d-42d0-ab78-d69bb084c31c", "title": "Neisseria meningitidis ST-11 Clonal Complex, Chile 2012", "text": "【0】Neisseria meningitidis ST-11 Clonal Complex, Chile 2012\n_Neisseria meningitidis_ (meningococcus) is the causative pathogen of invasive meningococcal disease (IMD), which includes a set of infectious syndromes, mainly meningitis or meningococcemia (septicemia) and, less commonly, pneumonia or other infections . Humans are the only reservoir for meningococcus, which usually colonizes the upper respiratory tract of ≈8%–25% of persons .\n\n【1】In Chile, the incidence of IMD decreased steadily during 2000–2012 from 3.6 to 0.7 cases/100,000 inhabitants . However, deaths from this disease have not followed this trend; the case-fatality rate increased from 8.9% in 2009 to 14.1% in 2010, 14.7% in 2011, and 27.0% in 2012 . During this period, the distribution of meningoccal serogroups has changed. There has been a large increase in frequency of serogroup W meningococci, which has replaced serogroup B as the most common serogroup.\n\n【2】A total of 101 cases, 57 culture-confirmed: 42/57 (73.7%) serogroup B and 1/57 (1.8%) serogroup W were reported in 2009; 78 cases: 56 culture-confirmed 36/56 (64.3%) serogroup B and 6/56 (10.7%) serogroup W were reported in 2010; 73 cases: 63 culture-confirmed, 32/63 (50.8%) serogroup B and 22/63 (34.9%) serogroup W were reported in 2011; and 133 cases: 103 culture-confirmed, 38/103 (36.9%) serogroup B and 60/103 (58.3%) serogroup W were reported in 2012 . We conducted this study to determine whether W meningococci belonged to a hypervirulent genetic lineage of the ST-11 clonal complex (CC).\n\n【3】### The Study\n\n【4】A national epidemiologic program for surveillance and control of IMD is conducted by the Department of Epidemiology of the Ministry of Health of Chile. Every national health care center must report suspected cases of IMD and send bacterial isolates to the Institute of Public Health of Chile (Santiago, Chile) or send cerebrospinal fluid samples when cultures have shown negative results. IMD cases are defined by clinical signs and symptoms (neck stiffness, altered state of consciousness, rash, meningeal irritation) and confirmed by isolation of _N. meningitidis_ from cerebrospinal fluid, blood, or another sterile body fluid or tissue. Each case is coded according to the International Classification of diseases, 10th Revision, as meningitis (code A39.0), meningococcemia (A39.2), Waterhouse-Friderichsen syndrome (A39.1), other meningococcal infections (A39.8), and unspecified meningococcal infections (A39.9).\n\n【5】In 2012, a total of 32 health care centers located throughout Chile reported 133 IMD cases. Of these cases, 103 were laboratory confirmed by bacterial isolation and biochemical identification . Serogroup was determined by slide agglutination with polyclonal antibodies. Genosubtyping was conducted by amplifying and sequencing variable regions 1 and 2 of the _porA_ gene as described by Russell et al. Variants were defined by reviewing the _Neisseria PorA_ typing database . Sequence types (STs) were determined as described by Maiden et al. on the basis of housekeeping genes _abcZ_ , _adk_ , _aroE_ , _fumC_ , _gdh_ , _pdh_ , and _pgm_ . Sequences were compared with those in the _Neisseria_ locus/sequence definition database , and STs and CCs were assigned. The nomenclature used in this report, when appropriate, is serogroup: genosubtype: CC.\n\n【6】The _fHbp_ genetic variant was identified as described by Brehony et al. Allele numbers were assigned by querying a public database . Genotyping with higher resolution was conducted by using pulsed-field gel electrophoresis (PFGE) and restriction endonuclease _Spe_ I. Electrophoretic profiles were analyzed by using BioNumerics software (Applied Maths NV, Sint-Martens-Latem, Belgium). A PFGE pattern was considered unique when ≥1 DNA bands in the electrophoretic migration profile differed from each other. A code was assigned to each pattern. To establish the magnitude of associations, we cross-tabulated data and calculated odds ratios (ORs) by using Med Calc software version 12.4.0.0 . The 95% CI was established, and p values <0.05 were considered significant.\n\n【7】Serogroup B meningococci were isolated from 38 (36.9%) case-patients, and serogroup W meningococci were isolated from 60 (58.2%) case-patients . Serogroups C and Y meningococci were rarely isolated (3 and 2 isolates, respectively). Multilocus sequencing typing showed serogroup B isolates belonged mainly to ST-32 and ST-41/44 CCs, including 4 and 6 STs respectively. Of 60 W isolates, 98% belonged to ST-11 CC . This finding is consistent with the serogroup of CCs found in W meningococci isolated in Chile during 2010 (4/4 W isolates analyzed) and 2011 (19/21 W isolates) . Among this CC, 3 STs were identified, of which ST-11 was the most common .\n\n【8】Genosubtyping of W:ST-11 strains obtained during 2012 indicated that 58/60 strains belonged to genosubtype P1.5,2 . In addition, sequence analysis of the _fHbp_ gene identified allele 22 as the most common variant; it was present in 58 (96.7%) of 60 strains that belonged to the W serogroup . Allele 22 of _fHbp_ was not detected in strains belonging to other serogroups, which indicated a strong association with W meningococci. These results indicated that most IMD cases reported during 2012 in Chile were caused by a hypervirulent genetic lineage of _N. meningitidis_ serogroup W.\n\n【9】Serogroup W meningococci W:P1.5,2:ST-11 with allele 22 of _fHbp_ were isolated from samples of 12 of 42 meningitis case-patients reported during 2012 and from 38 of 51 samples from meningococcemia case-patients. This association with meningococcemia cases was significant (OR 5.5, 95% CI 2.4–12.9, p = 0.0001). Overall, of 103 IMD case-patients in this study, 22 died. W:P1.5,2:ST-11 meningococci carrying allele 22 of _fHbp_ were isolated from most patients with lethal cases. However, this association was not significant (16 of 22 deaths; OR 2.5, 95% CI 0.9–6.9, p = 0.08).\n\n【10】Analysis of W:P1.5,2:ST-11 meningococci by PFGE identified 9 electrophoretic patterns . PFGE showed that most hypervirulent W:ST-11 clones were closely related to each other. Meningococci with a Cl-Nm-Spe-031 pattern caused most meningococcemia cases . However, this association was not significant (OR 1.8, 95% CI 0.59–5.38, p = 0.31). Of 16 lethal cases reported that were associated with W meningococci, 9 were caused by strains with the Cl-Nm-Spe-031 pattern, 3 by strains with the Cl-Nm-Spe-030 pattern, 3 by strains with the Cl-Nm-Spe-046 pattern, and 1 by a strain with the Cl-Nm-Spe-085 pattern.\n\n【11】### Conclusions\n\n【12】We showed that most W meningococci belonged to a hypervirulent genetic lineage of the ST-11 CC. Hypervirulent serogroup W meningococci W:P1.5,2:ST-11, which has the _fHbp_ gene allele 22, was the main cause of IMD in Chile during 2012. Its presence was associated with meningococcemia cases and partially accounted for more deaths during 2012 than in previous years. These isolates have a genetic profile similar to that of isolates from the first outbreak of IMD attributed to serogroup W, which affected the Hajj pilgrimage in Saudi Arabia in 2000, and to that of isolates from a larger outbreak in Burkina Faso in 2002 . However, these isolates from Chile have allele ID 22 of _fHbp_ instead of alleles ID 9 or ID 23. This allele has also been found in a hypervirulent W:P1.5,2:ST-11 strain in Mali in 1994 , but we have not found more instances of its presence.\n\n【13】Some genotypes of these isolates have been detected in serogroup W strains obtained in previous years, specifically genosubtype , allele 22 of the _fHbp_ gene (17 strains obtained in 2011), and specific PFGE patterns (9 strains were Cl-Nm-Spe-030, 4 were Cl-Nm-Spe-031, and 3 were Cl-Nm-Spe-046). These results indicate that hyperinvasive clones are circulating in Chile.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d859e0f4-6d29-4eab-883d-60e88caf1edb", "title": "Molecular Typing of Multidrug-Resistant Salmonella Blockley Outbreak Isolates from Greece", "text": "【0】Molecular Typing of Multidrug-Resistant Salmonella Blockley Outbreak Isolates from Greece\n_Salmonella_ Blockley is rarely isolated in the Western Hemisphere. According to Enter-net, the international network for surveillance of _Salmonella_ and verocytotoxin-producing _Escherichia coli_ infections, _S._ Blockley represented 0.6% of all _Salmonella_ serotypes isolated in Europe during the first quarter of 1998, a full 100-fold lower than the dominant serotype, _S._ Enteritidis (67.1%)  . However, _S._ Blockley is among the five most frequently isolated serotypes from both avian and human sources in Japan , Malaysia , and Thailand  . A single foodborne outbreak in the United States  and sporadic human infections in Europe associated with travel to the Far East  , animal infection  or carriage , and environmental isolates have also been reported .\n\n【1】Regardless of the frequency of _S._ Blockley isolation, its rates of resistance to antibiotics have been high. Among Spanish salmonellae isolated from natural water reservoirs, _S._ Blockley and _S._ Typhimurium had the highest rates of multidrug resistance  . Comparing 1980-1989 with 1990-1994, researchers from Tokyo noted an increase in the number of _S._ Blockley isolates resistant to one or more antibiotics, from 92.0% to 98.2% for imported cases and from 57.4% to 88.7% for domestic cases . In Thailand, isolates from human or other sources also had high rates of resistance to streptomycin, tetracycline, kanamycin, and chloramphenicol and lower rates to ampicillin and trimethoprim/sulfamethoxazole  .\n\n【2】Nevertheless, few attempts at typing _S._ Blockley isolates with molecular methods have been described, and these have been limited to the characterization of plasmid content .\n\n【3】During the second and third quarters of 1998, Enter-net reported higher numbers of _S._ Blockley isolates than during the same period of the previous year in several European countries  . The epidemiologic investigations conducted in Germany, England and Wales, and Greece did not confirm a source for this increase .\n\n【4】In this study, we characterized the Greek outbreak isolates further, both with respect to their antibiotic resistance phenotypes and DNA fingerprints obtained by pulsed-field gel electrophoresis of genomic DNA.\n\n【5】### The Study\n\n【6】The study sample consisted of 28 of 35 _S._ Blockley strains isolated from May to December 1998  , one strain from February 1999, and four epidemiologically unrelated control strains: one from 1996 and three from 1997. All isolates were from human cases of enteritis. Identification was performed by the API 20E system (BioMerieux S.A. Marcy l'Etoile, France) and serotyping with commercially obtained antisera (BioMerieux)  .\n\n【7】Susceptibility to kanamycin, streptomycin, ampicillin, amoxicillin/clavulanic acid, cefepime, tetracycline, chloramphenicol, trimethoprim/sulfamethoxazole, gentamicin, nalidixic acid, and ciprofloxacin was tested by a disk diffusion assay according to National Committee for Clinical Laboratory Standards guidelines  . Genomic DNA was prepared and digested with _Xba_ I (New England Biolabs)  . Chi-square tests or Fisher exact tests were used to calculate two-tailed probabilities.\n\n【8】_S._ Blockley accounted for seven of the 13,199 salmonella isolates identified in Greece from 1976 to 1997. However, 35 gastroenteritis cases due to this serotype were reported from May to December 1998  . Twenty-nine _S._ Blockley strains isolated from fecal specimens of patients with gastroenteritis during May 1998 to February 1999, along with four epidemiologically unrelated clinical isolates from 1996 and 1997, were therefore studied for susceptibility to antibiotics. The 1998 outbreak isolates were scattered throughout Greece; _S._ Blockley was isolated later, starting in August 1998, in northern Greece.\n\n【9】All isolates were susceptible to trimethoprim/sulfamethoxazole, ampicillin, amoxicillin/clavulanic acid, gentamicin, and ciprofloxacin . High resistance rates were observed to tetracycline (100%), streptomycin and kanamycin (90%), chloramphenicol (83%), and nalidixic acid (52%). Six resistance phenotypes could be distinguished  with the two major phenotypes of outbreak isolates being resistant to kanamycin, streptomycin, tetracycline, and chloramphenicol (ATC) or kanamycin, streptomycin, tetracycline, chloramphenicol, and nalidixic acid (ATCN). Most (76%) strains isolated after August 24, 1998, were nalidixic acid-resistant (resistance phenotypes ATCN, TCN, ATN), unlike strains isolated up to August 17, 1998 (17%) (1.29 <RR = 3.03 <7.11, p = 0.005).\n\n【10】When pulsed-field gel electrophoresis was used to obtain DNA fingerprints for these isolates , all belonged to the same type, A, although eight subtypes, A1-A8, could be distinguished on the basis of one to three DNA fragment differences . Two of the four isolates from previous years belonged to unique subtypes A6 and A7; the other two belonged to subtypes A2 and A8, shared by outbreak isolates . In contrast, 93% of the 1998 outbreak strains yielded PFGE patterns common to two or more isolates. Indeed, most outbreak isolates were grouped in subtypes A2 and A4, consisting of 11 and 12 isolates, respectively .\n\n【11】Pulsed-field gel electrophoresis subtypes were associated with resistance phenotypes. Most resistance phenotype ATC isolates belonged to subtype A2 (3.03. <RR = 11.86 <46.5, p = 0.0000098), while most resistance phenotype ATCN isolates belonged to A4 (2.76 <RR = 7.11 <18.30, p = 0.0000416). In addition, most isolates in the two major subtypes appearing before August 17, 1998, belonged to the A2 group, while most isolates appearing after August 24, 1998, belonged to the A4 group (1.47 <RR = 9.82 <65.45, p = 0.0006). Finally, unlike A4, the earlier A2 subtype was not isolated in northern Greece.\n\n【12】### Conclusions\n\n【13】Our results indicate that PFGE is useful in distinguishing epidemiologically related _S._ Blockley isolates since two of the four nonoutbreak isolates displayed unique PFGE patterns, A7 and A8, while PFGE patterns A2 and A4 grouped most of the 29 outbreak isolates ( 11 and 12 , respectively).\n\n【14】These two chromosomal fingerprints, differing by two DNA fragments, were associated with two distinct resistance phenotypes. The resistance phenotype of A4 isolates, ATCN, was identical to the earlier resistance phenotype of A2 isolates, ATC, except for the resistance to nalidixic acid. Nevertheless, these two PFGE/antibiotic resistance types, A2/ATC and A4/ATCN, displayed a clear distribution both in time and space.\n\n【15】The data may, therefore, indicate two main sources for the outbreak. Alternatively, and perhaps more likely, these two closely related types may together constitute the outbreak clone, evolved with time to acquire resistance to nalidixic acid. Resistance may well have originated in the food source, since several antibiotic classes are used as feed supplements in animal rearing and aquaculture in Greece: sulfonamides (trimethoprim/sulfathiazine), tetracyclines (oxytetracycline), and quinolones (oxolinic acid). However, as in other European countries , the epidemiologic investigation did not locate a common source to account for the wide geographic spread of cases  . Although travel was not mentioned in the Greek patients' questionnaire responses, the possibility that the source was an imported food cannot be ruled out. The association with smoked eel of Italian origin in the German outbreak has not been microbiologically confirmed  . The only other previous European report of a human outbreak attributed to _S._ Blockley, probably from vegetables contaminated by this organism, which was prevalent in irrigation water in the Spanish region of Granada, is anecdotal  . A documented _S._ Blockley enteritis epidemic in a U.S. hospital in 1966 was attributed to contaminated ice cream; however, this was also not microbiologically confirmed  .\n\n【16】While this serotype may remain important in Europe, its high rates of resistance to kanamycin, streptomycin, tetracycline, and chloramphenicol, which were in agreement with studies from the Far East  and Spain  , are cause for concern. Unlike the Far Eastern strains, no resistance to ß-lactam antibiotics or cotrimoxazole was observed in our study. The two dominant resistant phenotypes of _S._ Blockley from natural polluted waters in Spain were sulfonamides, streptomycin, and tetracycline; and neomycin, streptomycin, kanamycin, tetracycline, and chloramphenicol  , as in the Greek strains, except for the absence of resistance to nalidixic acid.\n\n【17】In agreement with differences in animal reservoirs and transmission routes and therefore the mechanism of resistance acquisition among different _Salmonella_ serotypes, the main patterns of resistance observed in _S._ Blockley were distinct from those predominating in the two major serotypes from isolates of both human and animal food origin in Greece. In _S._ Enteritidis, the most frequent resistance phenotype was resistance to ampicillin  , while in _S._ Typhimurium, the most frequent resistant phenotype was resistance to sulfonamides and streptomycin (A. Markogiannakis, P.T. Tassios, N.J. Legakis, unpub. obs.). Furthermore, the considerably high rate of resistance to nalidixic acid is equally unprecedented in both the Far Eastern and Spanish _S._ Blockley isolates and in other salmonella serotypes from Greece. Since resistance to nalidixic acid can be a precursor of resistance to fluoroquinolones, one of the two drug classes of choice for invasive salmonella disease, this feature of these _S._ Blockley strains is particularly disturbing. _S._ Blockley, previously a prevalent serotype in the Far East but rare elsewhere, nevertheless posed a public health problem in several European countries. The source of the European outbreaks, however, remains unclear. Given the increased international commerce in food, a collaborative study would be useful in identifying potential similarities between the recent European strains and established strains from the Far East.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "351c3419-8953-49a8-a0c7-0bcfab6ee85e", "title": "Asthma-Related School Absenteeism and School Concentration of Low-Income Students in California", "text": "【0】Asthma-Related School Absenteeism and School Concentration of Low-Income Students in California\nAbstract\n\n【1】**Introduction**  \nAsthma is one of the leading causes of school absenteeism. Previous studies have shown that school absenteeism is related to family income of individual students. However, there is little research examining whether school absenteeism is related to school-level concentration of low-income students, independent of family income. The objective of this study was to examine whether the proportion of low-income students at a school was related to school absenteeism due to asthma.\n\n【2】**Methods**  \nUsing data from the 2007 California Health Interview Survey, a population-based survey of California households, we examined the association between attending schools with high concentrations of low-income students and missing school because of asthma, adjusting for demographic characteristics, asthma severity, and health insurance status. Schools with high concentrations of low-income students were identified on the basis of the percentage of students participating in the free and reduced-price meal program, data publicly available from the California Department of Education.\n\n【3】**Results**  \nStudents attending schools with the highest concentrations of low-income students were more likely to miss school because of asthma. Students from low-income families, younger students, those with more frequent asthma symptoms, or those taking prescription asthma medications also were more likely to miss school because of asthma.\n\n【4】**Conclusion**  \nThe use of school-level interventions to decrease school absenteeism due to asthma should be explored, especially in schools with high concentrations of low-income students. Potential interventions could include school-based asthma education and disease management or indoor and outdoor air pollution control.\n\n【5】Introduction\n\n【6】Asthma is one of the most common chronic conditions among children in the United States. In 2009, an estimated 10.2 million US children (9.6%) had diagnosed asthma . In California in 2007, approximately 1.4 million children (15.4%) had diagnosed asthma . Asthma is also one of the leading causes of school absenteeism . In 2008, asthma accounted for an estimated 14.4 million lost days of school among children nationally . In California, school-aged children missed approximately 1.6 million days of school because of asthma in 2007 . School absences have consistently been associated with worse academic performance . Students who miss more school score lower on standardized tests in both reading and math  and are more likely to drop out before graduating from high school . Students with asthma are absent from school more than those without asthma . As a result, students with asthma may be more likely to experience the poor academic outcomes associated with increased absenteeism.\n\n【7】Students from families with lower socioeconomic status (SES) miss more school . However, few studies have examined whether school absenteeism is related to the school-level concentration of low-income students, independent of family income. Rothman found that having a higher percentage of students from low-SES families was associated with a higher school absence rate . If school-level factors are related to absenteeism, school-level interventions may help to reduce absenteeism. School absence due to illness such as asthma not only affects individual students but also can affect other students who attend the same schools. In California, school districts are funded on the basis of daily attendance . When students miss school, funding for their school is reduced. Because low-income school children are more likely to have severe asthma and to miss school because of asthma, schools with a large proportion of low-income students may be disproportionately affected by absences due to asthma . As a result, school-level interventions can affect not only the targeted students but also other students attending the same schools .\n\n【8】Examining whether school absenteeism is related to indicators of school-level disadvantage can inform the development of school-level interventions to reduce asthma-related school absences. We tested the hypothesis that children attending schools with the highest proportion of low-income students were more likely to miss school because of asthma than children attending schools with the lowest proportion of low-income students. We examined the relationship of school-level socioeconomic indicators with absenteeism while accounting for a wide range of individual-level factors including income, asthma severity, and health insurance status.\n\n【9】Methods\n\n【10】### Data source and population\n\n【11】We used data from the 2007 California Health Interview Survey (CHIS), a random-digit–dialed telephone survey of households that uses a 2-stage, geographically stratified design to produce a representative sample of California’s noninstitutionalized population. Residential telephone numbers were selected from within predefined geographic areas, and respondents were then randomly selected from within sampled households. One randomly selected adult (aged ≥18 y) was interviewed in each household. In households with adolescents (aged 12–17 y), 1 adolescent was randomly selected and interviewed directly after obtaining both a parent’s permission and the adolescent’s assent. In households with children aged 0 to 11 years, 1 child was randomly selected and the adult most knowledgeable about that child was interviewed.\n\n【12】CHIS interviewers conducted interviews in English, Spanish, Chinese, Vietnamese, and Korean. Detailed information about the CHIS methodology is available elsewhere . Interview completion rates among screened households were 73.7% for children and 44.1% for adolescents . We analyzed responses from school-aged children (aged 4–17 y) who reported having had asthma diagnosed and who attended public school. The University of California, Los Angeles Office for the Protection of Research Subjects certified this research as exempt from review.\n\n【13】### Measures\n\n【14】For the outcome variable, we examined missing school because of asthma on the basis of responses to the following question: “During the past 12 months, how many days of school did you miss due to asthma?” Adolescents self-reported the number of school days missed, and the most knowledgeable adult reported for children. The distribution of responses to this question was highly skewed; 76% reported not missing any school days because of asthma, and only 7.6% reported missing 5 or more days of school. Thus, responses were analyzed as a dichotomous variable with the following categories: 0 days and 1 or more days. Only respondents who reported having had asthma diagnosed (in response to the question, “Has a doctor ever told you that you \\[or your child\\] have/has asthma?”) were asked about missing school because of asthma. As a result, our analysis is limited to respondents who had diagnosed asthma.\n\n【15】We examined the percentage of enrolled students who participated in the free or reduced-price meal program at the respondent’s school (<25%, 25%–49%, 50%–74%, ≥75%) as the primary predictor of interest. In California, a child’s family income must fall below 130% of the federal poverty guidelines ($27,564 for a family of 4 in 2007) to qualify for free meals or below 185% of the federal poverty guidelines ($39,226 for a family of 4 in 2007) to qualify for reduced-cost meals . This variable served as a proxy for identifying schools with high concentrations of low-income students, consistent with previous research . We obtained this information from publicly available 2007 data from the California Department of Education. We merged these data with CHIS data based on respondents’ school. Because these data are available only for public schools, we limited our analysis to students attending public schools. In addition to the meal program variable, we included covariates to adjust for individual-level family income, demographic characteristics, health insurance status, and asthma severity.\n\n【16】The adult respondent reported family income, and this variable was included in the analysis as a percentage of the federal poverty guidelines (≤185%, 186%–399%, and ≥400%). Analyses included the following demographic characteristics: age (4–10 y, 11–14 y, 15–17 y), sex, and race/ethnicity (white, Latino, Asian, African American, or mixed race). We also included the following indicators of asthma severity: frequency of asthma symptoms (at least monthly, less than monthly) and currently taking daily prescription medication to control asthma (yes, no). Adolescent respondents self-reported demographic characteristics and asthma indicators. The most knowledgeable adult reported this information for children. We used continuity of insurance coverage during the past year (insured all year, uninsured all or part of the year) to indicate health insurance status. The presence of secondhand smoke in the household (no smoking in house vs smoking occurs ≥1 d/wk) was assessed because this is a common asthma trigger. The adult respondent in the household reported both of these.\n\n【17】### Statistical analysis\n\n【18】We conducted logistic regression analyses to examine factors associated with missing school because of asthma. The first model examined the association of missing school because of asthma with age, sex, race/ethnicity, family income, frequency of asthma symptoms, use of daily prescription medications for asthma, insurance status, and household secondhand smoke exposure. The second model included the indicator for school-level disadvantage, proportion of low-income students, to examine the independent association of this variable with missing school because of asthma, adjusting for the covariates in the first model. The CHIS sample had 1,302 children (aged 4–17 y) with asthma who attend public school. The final analysis was limited to 1,276 children for whom data on the proportion of students at the school who participated in the meal program could be linked. Data were analyzed with SAS version 9.2 (SAS Institute, Inc, Cary, North Carolina) and SUDAAN version 10.0.1 (RTI International, Research Triangle Park, North Carolina). Analyses were weighted to be representative of the California population and adjusted for the complex survey design of CHIS.\n\n【19】Results\n\n【20】Seventeen percent of children aged 4 to 17 who attended public school had diagnosed asthma, of whom 23% had missed at least 1 day of school because of their asthma . The first regression model indicated that children in the lowest family income group were more likely to miss school because of asthma than children in the highest income group, while adjusting for all other covariates . In addition, younger children, those who had more frequent asthma symptoms, or those using prescription asthma medications were more likely to miss school because of asthma.\n\n【21】The results of the second model indicate that children attending schools with the highest proportion of low-income students were more likely to miss school because of asthma than children attending schools with the lowest proportion of low-income students (OR, 1.99; _P_ \\= .03), while adjusting for age, sex, race/ethnicity, family income, insurance status, and severity of asthma . Although the effect was somewhat attenuated, the association between family income and missing school because of asthma remained significant in model 2. As with model 1, younger children, children with more frequent asthma symptoms, or children taking prescription asthma medication also were more likely to miss school because of asthma.\n\n【22】Discussion\n\n【23】This study found that students who attended schools with the highest concentrations of low-income students were more likely to miss school because of asthma than those at schools where the concentration of low-income students was lower. In addition to family income, the proportion of low-income students in the school is associated with asthma-related school absences after adjusting for family income, demographic characteristics, health insurance status, asthma severity indicators, and exposure to household smoke. Because California schools receive funding based on daily attendance, these findings suggest that resources allocated to schools with a large proportion of low-income students may be reduced because their children are more likely to miss school because of asthma . However, these findings also suggest that school-level interventions to help children manage their asthma may be explored as a strategy to decrease school absenteeism.\n\n【24】Our findings indicate that children from low-income families, who are younger in age, who have more frequent asthma symptoms, or who take daily prescription asthma medications were also more likely to miss school because of asthma. These findings are consistent with those of previous studies, which showed that children’s absences from school because of asthma were related to the underlying severity of their asthma . For instance, among children with asthma, those with more frequent asthma symptoms and those taking prescription asthma medications were more likely to miss school because of their asthma. Consistent with existing literature, our findings also showed that students from poor families were more likely to miss school than those from more affluent families .\n\n【25】Our analysis does not provide a direct explanation for why children attending schools with the highest concentrations of low-income students were more likely to miss school because of asthma. However, other research offers possible explanations. For example, students at disadvantaged schools may have higher exposure to indoor or outdoor asthma triggers. School absences are related to air pollution exposure . School location can determine exposure to air pollution, particularly traffic-related pollutants, and schools with a high proportion of students in the free and reduced-price meal program are more likely to be near busy roads . Furthermore, Simons et al found that students who attend schools with poor building conditions, such as visible mold, vermin, and poor ventilation, are more likely to miss school . In addition, they found that schools in lower SES districts show some of the strongest associations between poor building conditions and absenteeism. Poor building conditions may also lead to more intrusion of outdoor pollutants, such as motor vehicle exhaust .\n\n【26】School absences due to asthma can be avoided by appropriate asthma management, including appropriate use of medications and reduced exposure to triggers . Our results suggest that school-level interventions, especially in schools with high concentrations of low-income students, could be effective in decreasing school absenteeism due to asthma. Potential interventions could include indoor and outdoor air pollution control, for instance, locating schools, day care centers, and sports fields away from busy roadways. In 2003, California passed legislation (SB 352) that requires school districts to ensure that new schools are not built on old hazardous waste sites, hazardous release sites, or sites that contain pipelines that carry hazardous substances; in addition, school site boundaries cannot be within 500 feet from the edge of the closest traffic lane of a freeway or busy traffic corridor . This legislation could be a model for other states to follow.\n\n【27】School-based asthma education and disease management could be another effective measure to reduce absenteeism. For example, 1 study demonstrated that a comprehensive school-based asthma program led to fewer absences attributable to asthma . Other research suggests that children with asthma enrolled in schools without a school-based health center miss more days of school than those enrolled in schools with such a center . However, supplemental funding from public sources is likely needed for schools serving low-income populations to establish programs such as asthma education or school-based health centers, because these schools do not have the same capacity to raise funds from within the community as schools serving high-income populations. Many of these strategies are included in the Centers for Disease Control and Prevention’s _Strategies for Addressing Asthma Within a Coordinated School Health Program_ .\n\n【28】This study has several limitations. First, the CHIS data, including our outcome indicator, school absence due to asthma, are self-reported. These data may be subject to recall bias or error in the self-reported asthma-related indicators. For instance, respondents might not be able to accurately recall the number of days of school missed due to asthma for more than the proximate past. As a result, we used the measure of 1 or more school days missed instead of actual number of school days missed, which may be less error prone. We also expect this error to be similar for those who attended schools with high or low concentrations of low-income students, that is, nondifferential with regard to SES of their schools. Second, our results are based on data from a single state, and the results may not be generalizable to all states. However, California is the most populous US state, with approximately 36.5 million residents, and its population is diverse. In addition, California often provides an early indication of health and policy trends for the rest of the nation. For example, California has more stringent state air quality standards than the nation and was the first state to regulate the siting of schools. As a result, the findings of this study could be informative for a national audience. Additionally, this study does not explain the underlying causes of school absences. Future research could examine these underlying causes by linking school-based environmental indicators, such as traffic near schools and air pollutant monitoring data, with asthma outcome data. Future research could also explore other indicators of neighborhood SES.\n\n【29】Although further studies are necessary to confirm the findings of this cross-sectional study and explore the underlying causes of school absences, these results suggest that school-level interventions, especially in schools with high concentrations of low-income students, should be explored as a strategy to decrease school absenteeism due to asthma. Potential interventions could include school-based asthma education and disease management and indoor and outdoor air pollution control.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "89736f45-dc03-4f46-9727-626f35dc462d", "title": "Fly Reservoir Associated with Wohlfahrtiimonas Bacteremia in a Human", "text": "【0】Fly Reservoir Associated with Wohlfahrtiimonas Bacteremia in a Human\n_Wohlfahrtiimonas chitiniclastica_ is a rarely reported cause of bacterial infection that has been isolated in humans and other mammals from a variety of organs . In addition, _Wohlfahrtiimonas_ spp. have been isolated from 4 species of nonbiting flies in Asia and Europe  that can cause myiasis, fly larvae infestation of a host’s tissue. Wound myiasis has been reported in patients infected with _W. chitiniclastica_ and with _Ignatzschineria_ spp. an organism closely related to _W. chitiniclastica_ . These findings provide evidence that _W. chitiniclastica_ is transmitted by flies or fly larvae during myiasis. However, no reported attempt has been made to isolate _Wohlfahrtiimonas_ spp. or _Ignatzschineria_ spp. from larvae associated with a patient. We report a case of _Wohlfahrtiimonas_ infection in a man in Washington, USA, and results of environmental investigations.\n\n【1】### The Study\n\n【2】The case-patient was a 57-year-old man who developed wet gangrene of the right ankle and myiasis below the waist. Hematology at hospital admission was notable for leukocytosis and a predominance of neutrophils with a high ratio of band neutrophils . Chronic cirrhosis, localized lung atelectasis, and multiorgan failure secondary to septic shock were diagnosed. The patient underwent amputation below the right knee but died 3 days after admission. Blood, urine, and tracheal aspirates collected <8 hours after admission revealed a mixed bacterial infection, including gram-positive cocci and gram-negative rods . _Propionibacterium acnes_ and _Staphylococcus hominis_ ssp. _hominis_ were isolated from blood cultures, in addition to an unidentifiable gram-negative rod. No medical history was available; proxy interviews excluded recent travel outside Washington.\n\n【3】We performed presumptive identification of the gram-negative rod with phenotypic studies and matrix-assisted laser desorption/ionization time-of-flight mass spectrometry . Amplification and sequencing of the near full-length 16S ribosomal RNA (rRNA) gene was performed, a phylogenetic tree was inferred by using the neighbor-joining method, and the topology was assessed by a bootstrap analysis of 1,000 replicates . We used pulsed-field gel electrophoresis (PFGE) to assess isolate relatedness .\n\n【4】Because larvae found on the patient had been discarded, we collected live and dead insects from the patient’s home and identified them to genus or species level . To remove surface contamination, all live fly larvae and adult specimens were rinsed 5 times with sterile phosphate-buffered saline (PBS), homogenized, and sequentially diluted. We cultured the first rinse, fifth rinse, and diluted homogenates to isolate _Wohlfahrtiimonas_ spp.\n\n【5】We identified 6 species of flies  and collected live larvae (≈20) from the patient’s house . We performed bacterial culture on a pooled sample of half of these larvae  and then individually on adult flies that emerged from the other half . One green bottle fly ( _Lucilia sericata_ )  was caught alive in the house in a sterile container and laid eggs inside the container before dying . We isolated a _Wohlfahrtiimonas_ sp. from 2 of 6 insect samples on blood agar plates  but not from any other samples, including adult flies that emerged from the positive batch of larvae.\n\n【6】The isolates grew on blood agar, yielding colonies with a smooth center and rough edges, and displayed α hemolysis. Matrix-assisted laser desorption/ionization time-of-flight mass spectrometry yielded a presumptive result of _W. chitiniclastica_ . A phylogenetic analysis of the 16S rRNA gene sequence of all isolates (1,462 bp) showed that the most closely related type strains were _W. chitiniclastica_ DSM 18708 T  (98.3% sequence similarity) and _W. larvae_ JCM 18424 T  (97.3% sequence similarity) . The PFGE pattern indicated that all isolates from flies and fly larvae were indistinguishable and 74% similar to that of the patient isolate .\n\n【7】### Conclusions\n\n【8】Our isolates possibly represent a new species in the genus _Wohlfahrtiimonas_ based on the percentage sequence similarity with _W. chitiniclastica_ and _W. larvae_ type strains . We isolated _Wohlfahrtiimonas_ sp. from insects in the Americas and in a previously undescribed host, the green bottle fly ( _L. sericata_ , Diptera: Calliphoridae). Previously, _Wohlfahrtiimonas_ spp. have been identified in only 4 species of flies in Asia and Europe ( _Wohlfahrtia magnifica, Chrysomya megacephala_ , _Hemetia illucens_ , _Musca domestica_ ) , each representing a different fly family (Diptera: Sarcophagidae, Calliphoridae, Stratiomyidae, and Muscidae, respectively). We isolated a _Wohlfahrtiimonas_ from a larva hatched from eggs laid by a fly in a sterile container, providing evidence that _Wohlfahrtiimonas_ sp. can be transmitted vertically.\n\n【9】_L. sericata_ has been associated with _W. chitiniclastica_ infection in a patient with myiasis and bacteremia only once, in the United Kingdom , but a definitive link could not be established in that case because larvae from the patient had been discarded. The scarcity of reports of _Wohlfahrtiimonas_ spp. infections might be attributable to the difficulty in laboratory identification  or because wound myiasis is routinely addressed with broad-spectrum antimicrobial drugs.\n\n【10】Because the pooled larvae  emerged as multiple fly species, we are unable to ascertain in which other species _Wohlfahrtiimonas_ sp. growth occurred. _Wohlfahrtiimonas_ was not isolated from these adult flies, which might be because the competent host was not present in the batch of larvae left to emerge or because of the association between _Wohlfahrtiimonas_ spp. and flies during successive developmental stages. Indeed, previous studies isolated _W. chitiniclastica_ from the gut of larvae and adult flies , and _Ignatzschineria_ spp. are hypothesized to play a role in larval development , indicating that these bacteria might belong to fly microbiota. In one study, the relative abundance of _Ignatzschineria_ spp. fluctuated during life stages of _L. sericata_ and was among the dominant bacterial genera during the larval and pupal life stages . Bacterial flora further decline during pupation, when reorganization of the intestinal tract leads to extrusion of the gut lining . These factors might explain why we did not isolate _Wohlfahrtiimonas_ spp. from the adult flies that emerged, or our protocol might have been of insufficient diagnostic sensitivity to detect _Wohlfahrtiimonas_ spp. among adult flies.\n\n【11】The concurrent isolation of _Wohlfahrtiimonas_ sp. in the blood from a patient with myiasis and in fly larvae found at the patient’s home provides further evidence that fly larvae can act as vectors of _Wohlfahrtiimonas_ spp. Because PFGE patterns of the isolates obtained from the fly larvae and from the patient’s blood did not match, we cannot definitely identify the fly species that led to his infection.\n\n【12】We isolated _Wohlfahrtiimonas_ sp. from a patient’s blood along with other bacteria, precluding us from assessing the pathogenicity of our isolate. However, in 2 previous reports , _W. chitiniclastica_ was the only bacterium isolated from the blood, indicating its pathogenic potential .\n\n【13】Most cases of _W. chitiniclastica_ infection have occurred among persons with a history of poor hygiene and exposed wounds . Green bottle flies are among the most common species associated with myiasis in the United States , and risk for infection is expected during warm environmental conditions favorable to their development. In addition, green bottle fly larvae are the most commonly used larvae for maggot debridement therapy  Infection with _Wohlfahrtiimonas_ spp. should be considered as a potential risk for patients undergoing this therapy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c4309675-f9b0-4465-8ccc-779c4d316c01", "title": "Cholera Outbreak in Southern Tanzania: Risk Factors and Patterns of Transmission", "text": "【0】Cholera Outbreak in Southern Tanzania: Risk Factors and Patterns of Transmission\nThe reemergence of cholera is presenting unprecedented challenges . Since the seventh pandemic caused by Vibrio cholerae biotype El Tor began in Indonesia in 1961, most regions of the world continue to report cholera. 1997 was marked by a cholera epidemic affecting most countries in East Africa, with spread toward central and southern parts of the continent. Africa reported 118,349 cases to the World Health Organization in 1997, for 80% of cases worldwide. Africa also had the highest overall case-fatality rate (4.9%), compared with 1.3% in the Americas and 1.7% in Asia . Tanzania has consistently reported cholera cases; annual reports ranged from 1,671 cases in 1977 to 18,526 in 1992 . During the last 2 decades, three major cholera epidemics have occurred: 1977-78, 1992, and 1997 . In 1997, Tanzania had one of the highest case-fatality rates in East Africa (5.6%), with 2,268 deaths in 40,226 cases . We describe risk factors and pattern of spread of the 1997 cholera epidemic in a rural area in southern Tanzania.\n\n【1】### Methods\n\n【2】Ifakara is 270 m above sea level in the Kilombero District river valley in southeastern Tanzania (08ºS; 32ºE) . It is a rural area (estimated population 57,000), and most inhabitants are subsistence farmers growing rice and maize. Fishing is also common. The town is crossed by the Lumemo River, a tributary of the Kilombero River. There are two rainy seasons: March through May, and December through January (1997 rainfall 1,439 mm). A short, cool dry season follows the long rains in June and July.\n\n【3】In Ifakara, the last cholera epidemic was in 1992. Most outpatient and admissions occur at Ifakara's Saint Francis Designated District Hospital (SFDDH), a 375-bed, district referral hospital. Demand-driven research takes place in an adequately equipped, district-based international research center, the Ifakara Health Research and Development Centre (IHRDC).\n\n【4】##### The Outbreak\n\n【5】An increase in acute severe diarrhea cases in adults was noted in early June 1997 at SFDDH. On June 23, the first cholera case was confirmed by culture. At that time, a cholera control campaign was launched, following standard recommendations . As part of control activities, an outbreak investigation was begun, and two cholera treatment sites were established. One was near the area where the outbreak was first noticed, the Lumemo River; the other site was in a ward at the district hospital.\n\n【6】The outbreak investigation consisted of a prospective hospital-based, case-control study designed to identify cholera risk factors. A case was defined as illness in a patient >5 years of age, visiting either of the cholera sites from June 23 to December 31, 1997, and hospitalized with acute onset of watery diarrhea. For each case, two age- and sex-matched controls were selected and interviewed on the same day as the case interview. Controls were selected from patients admitted to the district hospital within 2 days of the date of case admission. Data on possible risk factors were documented by a standardized questionnaire . After preliminary analysis of the first 60 cases and 120 controls, access to the Lumemo River was restricted. The final analysis was based on 180 cholera cases and 360 controls. We evaluated possible sources of bias 1 year later by interviewing a random sample of 100 community controls matched by age, sex, and neighborhood. This sampling procedure was a two-stage cluster (10 families per cell unit) intended to assess prevalence of cholera risk factors at the community level.\n\n【7】The distribution of cholera cases in relation to the main water source was determined by mapping the distance from ill persons' houses to the Lumemo River. Mapping was facilitated by use of a GeoExplorer II Mapping system and two Trimble Global Positioning System receivers (Trimble, Liverpool, United Kingdom). Differentially corrected positions were then processed with PFINDER software. To assess whether cases were equally distributed outwards from the river, areas were mapped at 250-m intervals. Four major zones were used, two on each side of the river; these zones were used for sampling rather than streets, which are not well defined in Ifakara. Zone-specific attack rates were calculated as cases per 1,000 residents for the duration of the outbreak, after a house-to-house census of all residents living within 500 m of the river was conducted.\n\n【8】Fecal specimens and rectal swabs were obtained from all patients admitted to the treatment sites until 60 positive cultures were obtained. Samples were obtained at the beginning of the epidemic and 1 month later from two water sources (the Lumemo River and the water pump closest to the river) and were sent to the IHRDC laboratory. Each specimen was immediately processed and plated onto thiosulfate-citrate-bile salt-sucrose agar. Sucrose- positive colonies were further tested on the basis of standard biochemical reactions . V. cholerae isolates were agglutinated with polyvalent 01 and monospecific Ogawa, Inaba, and Hikojima antisera (Difco Laboratories, Madrid, Spain). The isolates were tested for susceptibility to seven antibacterial drugs by disk diffusion . Susceptibility testing was done for the following antibiotics: ampicillin, 30 mg per disk; chloramphenicol, 60 mg; ciprofloxacin, 10 mg; gentamicin, 40 mg; nalidixic acid, 130 mg; tetracycline, 10 mg; and trimethoprim-sulfamethoxazole, 5.2 mg and 240 mg, respectively.\n\n【9】Polymerase chain reaction (PCR) was used to detect the ctx gene for all isolates. The primers used were specific for the V. cholerae enterotoxin (Takara Shuzo Co. Ltd. Otsu, Shiga, Japan). The analysis of chromosomal DNA by digestion with low-frequency-of-cleavage restriction enzymes and separation by pulsed-field gel electrophoresis (PFGE) was done with the Not I enzyme on a CHEF-DR IIII system (Bio-Rad Laboratories, Richmond, CA).\n\n【10】Data were double-entered into FoxPro databases (Microsoft Corp.) and checked for range, internal consistency, and referential integrity. Statistical analysis was performed with STATA statistical software (Stata Corp. 1997). McNemar's chi-square test was used for the univariate analysis. Variables significantly associated with cholera at or below the 5% level in the univariate analysis were included in the modeling. Multivariate conditional logistic regression was performed, the final model was fitted, and attributable risk percentages were calculated .\n\n【11】### Results\n\n【12】From June 23 through December 31, a total of 785 patients were admitted to the Ifakara cholera treatment sites. The number of cases peaked between June 30 and July 4 ; 369 (47%) were males; 376 (48%) of the cholera patients were <25 years of age. Seventeen deaths were reported (case-fatality rate 2.1%). Zone-specific attack rates were from 11 (west zone) to 41 cases per 1,000 residents (east zone), showing an increase in the densely populated zones of the town.\n\n【13】In univariate analysis of recent exposure to potential risk factors, low standards of living (indicated by a mud house, having a thatched roof, and having simple pit latrine or none) affected significantly the likelihood of disease . Risk factors for exposure to water, including long distance to water source (>10 minutes walk), use of unboiled water, bathing in the river, and the use of tap water inside the house instead of pumped water, were all significantly associated with cholera, as well as having recently eaten dried fish and prawns. Multivariate analyses showed that distance to the water source (odds ratio \\[OR\\] 2.7; 95% confidence interval \\[CI\\] 1.7-4.4), having eaten dried fish recently (OR = 12.1; 95% CI 7.7-19.1), and bathing in the river (OR = 14.4; 95% CI 8.8-23.5) were independently significantly associated with risk for cholera . Interaction terms neither changed nor improved the fit of the model.\n\n【14】The attributable risk percentages  provide an approximate summary of the importance of each risk factor, taking into consideration both the strength of the association and its prevalence; these estimates assume that all confounding variables were measured and no sources of bias exist. For example, river bathing accounted for 93% of all cases among the population with this exposure, and 49% of all cases could have been averted by preventing this practice.\n\n【15】Cholera patients were more likely to be exposed to bathing in the river than community controls (p<0.01) but hospital controls were less likely to be exposed than 100 community controls (p<0.01) . Cholera cases were more likely to walk >10 minutes to water sources than community controls (p<0.05).\n\n【16】V. cholerae was isolated from stool specimens and water samples from the Lumemo River, but not from any other water source. All isolates were toxigenic V. cholerae O1, biotype El Tor, serotype Ogawa; DNA genotyped by PCR-PFGE in samples isolated from the river and patient stools had identical patterns . All isolates were susceptible to tetracycline, ampicillin, amoxycillin + clavulanic acid, nitrofurantoines, and quinolones and were resistant to cotrimoxazole.\n\n【17】### Conclusion\n\n【18】This hospital-based, case-control study identified three risk factors independently associated with the spread of the 1997 cholera outbreak in Ifakara, rural Tanzania. These factors were bathing in the river, eating dried fish, and living >10 minutes walking distance from the closest water source. Data from zone-specific attack rates also suggest an increase in cholera cases in the densely populated zones of the town.\n\n【19】Bathing in Ifakara's principal water source, the Lumemo River, had an independent, statistically significant OR of 14.4. More than 90% of river bathers were ill, and approximately 50% of these cases could have been prevented if access to the river had been closed. Moreover, identical strains, based on the DNA typing, were isolated both from the patient stools and river water, as previously reported in Bangladesh and the 1992 epidemic in Burundi . After the 1997 outbreak in Ifakara, the largest town in Kilombero District, most areas of the district were affected by the epidemic. Cases in other areas in the district began to be reported on September 2, reaching 134 by December 1997 and >1,000 by September 1998 . In the town of Itete, we were able to follow the beginning of the outbreak, which appeared to have been due to contamination of the Itete River with clothes of a cholera adult patient who came from Ifakara town and died in Itete. The epidemic in Ifakara was preceded by a funeral near the Lumemo River, which travelers from Dar es Salaam had attended (data not shown). Indeed, the last three cholera epidemics in Ifakara have been preceded by epidemics in Dar es Salaam .\n\n【20】Long distance to water source, defined as >10 minutes walk, was also independently associated with risk for cholera. Twenty-six percent of cases and 11% of controls had to walk >10 minutes to collect water. Long distance may be an indirect measure of poor hygienic habits caused by scarcity of water. In addition, this link may be indirectly associated with drinking water from the river: 23% of cholera patients living >10 minutes walking distance from a water source and 9% of controls actually collected water from the river. The other 77% and 91%, respectively, collected water from a pump or tap. Persons living farther away from the river may store water for longer periods, thus facilitating V. cholerae growth; risk for cholera is dependent on inoculum size .\n\n【21】Eating dried fish in Ifakara also seemed to be a cholera risk factor. Although to our knowledge, this is the first time a foodborne exposure has been identified as a cholera risk factor in East Africa, this association should be interpreted with caution. We were unable to isolate V. cholerae in dried fish samples, and hospital controls ate less dry fish than community controls . However, hospital controls may have been more likely to recall food served at the hospital, where fish is rarely offered.\n\n【22】These associations could also be explained by different sources of bias. At the design stage, we asked questions to identify possible sources of bias. First, we asked questions about activities such as recent travel or attendance at a party. We aimed to identify whether hospital controls, as a result of their cause of admission, were as capable of bathing in the river as patients. We then assumed that this increased risk could not have resulted from selecting hospital controls who were physically incapable of bathing in the river, since they were equally capable of having attended a party or traveled recently. Additionally, we recruited only recently admitted patients (within 2 days of admission). Second, we included prawns in the questionnaire. Since prawns are never offered at the hospital, if food availability at the hospital were the underlying reason for the fish-cholera association, prawns should be associated as well. However, multivariate analysis showed only fish to be associated. The extent to which this fish-cholera association may be due to bias remains unclear.\n\n【23】The hospital-based case-control study, which offers the advantage of lower cost compared with a community-based design, seemed to be a reasonable approach for identifying risk factors such as bathing in the river and distance to water source. However, it may lead to over-estimation of the strength of the association, and its role in identifying food as risk factor is debatable.\n\n【24】In conclusion, in Ifakara, as reported previously in Tanzania , cholera transmission seems to have been predominantly waterborne. The 1997 Ifakara cholera outbreak could have been initiated by river contamination and facilitated by poor sanitation and lack of safe water. Control measures such as blocking access to the river and surveillance of travelers to avoid contamination of water sources seem to be impractical. Besides other control measures described elsewhere for African settings , trying to control cholera in densely populated cities such as Dar es Salaam may help control its spread. In the absence of adequate sanitation and water supply in less developed countries, health education campaigns stressing the need for safe water, food handling, and prompt medical care remain feasible options to reduce disease and death from cholera in rural East Africa.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c6904f00-1d43-49dc-b6aa-ae8c565fb17f", "title": "Hepatitis E Virus in Pork Liver Sausage, France", "text": "【0】Hepatitis E Virus in Pork Liver Sausage, France\nFoodborne transmission of hepatitis E virus (HEV) to humans from consumption of undercooked pig liver and deer meat has been reported in Japan . In addition, commercial pig livers purchased from local grocery stores in Japan, the United States, and Europe were contaminated with HEV . Epidemiologic and PCR results linked a cluster of autochthonous acute hepatitis E cases to ingestion of raw figatelli, which is a dried, cold-smoked sausage containing ≈30% pig liver . We investigated the viability of HEV in pork liver sausages produced in France.\n\n【1】### The Study\n\n【2】Four samples of pork liver sausage (designated sausages A–D) collected at the final production stage from 4 independent manufacturers in 3 locations in southern France were found to be HEV positive by using real-time reverse transcription PCR (RT-PCR). Samples were tested in 2 institutes, the Animal Health and Veterinary Laboratories Agency, United Kingdom, and Wageningen University and Research Centre, the Netherlands. Three-dimensional (3D) cell culture propagation was performed to investigate the presence of infectious HEV particles. PLC/PRF/5 hepatocarcinoma cells (American Type Culture Collection 8024) were cultured as a monolayer (2D) at 37°C in GTSF-2 medium  in a 5% CO 2  environment. Cells were trypsinized at 95% confluence and resuspended in fresh medium to a density of 2 × 10 5  cells/mL. Fifty milliliters of cell suspension was then introduced into a rotating wall vessel with 10 mg/mL of porous Cytodex-3 microcarrier beads (collagen type I–coated porous microspheres, average diameter 175 μm (Sigma, Dorset, UK and Zwijndrecht, the Netherlands) and incubated at 37°C in 5% CO 2  . The cells were incubated for \\> 28 d before inoculation to enable complete differentiation.\n\n【3】The inoculum was prepared by homogenizing a 2.5-mg fragment of each sample with mortar and pestle in 5 mL of culture medium. The homogenate was centrifuged at 8,000 × _g_ for 3 min, and the supernatant was filtered sequentially through 1.2-µm, 0.45-µm, and 0.2-µm filters to reduce the risk for bacterial contamination. The medium was removed from the rotating wall vessel, and 2.5 mL of inoculum was incubated with the cells for 2 h at 35.5°C; at that point, 47.5 mL of fresh medium was added. Subsamples of medium (140 µL) were collected in duplicate, added to 560 µL of lysis buffer (QIAamp Viral RNA Mini Kit; QIAGEN, Crawley, UK, and Venlo, the Netherlands), and stored at −20°C until RNA extraction was performed.\n\n【4】Both institutes performed real-time RT-PCR by using primers and probe as described , using the Superscript III Platinum One-Step Quantitative RT-PCR System (Invitrogen, Paisley, UK, and Bleiswijk, the Netherlands). Negative (water) and positive (extract from positive fecal sample, genotype 3) controls were included.\n\n【5】HEV RNA was detected in the 3D cell culture supernatants of all 4 sausage samples up to 8 d postinfection (dpi). Thereafter, HEV RNA was detected only in the cells inoculated with the sample A homogenate ; it was assumed that the signals from the other 3 samples represented residual inoculum or an abortive infection. The sample A culture showed a cycle threshold (C t  ) value of 27 on the day of inoculation (dpi 0) that increased to 35 on dpi 5, continued to increase until dpi 11, when it peaked at 38 and then began to decrease. A low C t  value of 29 was observed on dpi 44. Similar results were obtained in the laboratory in the Netherlands .\n\n【6】To evaluate the infectivity of progeny viruses from the primary inoculation, supernatant positive for HEV by RT-PCR from dpi 16 was used to infect fresh 3D PLC/PRF/5 cultures according to the protocol described for the primary inoculation. Cells infected with the supernatant from the original sample A homogenate cultures had positive HEV RNA test results on most days after inoculation; C t  values remained relatively constant at an average of 37 from immediately after inoculation (dpi 0) to the end of the experiment at dpi 35.\n\n【7】To compare the cultured virus to the inoculum, a partial fragment of the open reading frame 2 of HEV extracted from culture subsamples (dpi 16, dpi 55; progeny subsample at dpi 35) was sequenced as described . HEV RNA sequences detected in the culture subsamples were characterized and demonstrated 100% identity with that of the inoculum (304 bp of open reading frame 2) but differed from the control strain used. These sequences were confirmed as HEV genotype 3.\n\n【8】Further confirmation of the presence of viable virus particles in cell culture was sought by using electron microscopy. Supernatants of the cell cultures were applied to Pioloform/carbon-coated, 400-mesh copper grids (Plano GmbH, Wetzlar, Germany) for 10 min, fixed with 2.5% aqueous glutaraldehyde solution for 1 min, and stained with 2% aqueous uranyl acetate solution for 1 min. The specimens were examined through transmission electron microscopy by using a JEM-1010 microscope (JEOL, Tokyo, Japan) at an 80-kV accelerated voltage. HEV particles were observed in the supernatant of the sample A culture collected at dpi 33 . For additional proof by immunoelectron microscopy, an _Escherichia coli_ –expressed, His-tagged HEV genotype 3 capsid protein derivative harboring amino acid residues 326–608  was used to generate HEV-positive serum in a rabbit by 3 subcutaneous inoculations at 4-week intervals . The immunoelectron microscopy examination of this serum and goat anti-rabbit IgG linked with 5-nm gold particles (BBInternational, Oconomowoc, WI, USA) confirmed the presence of HEV particles .\n\n【9】### Conclusions\n\n【10】We confirmed that 1 sample of pork liver sausage that had positive test results for HEV RNA by real-time RT-PCR contained viable HEV. We cultured 4 sausage samples in 2 different institutes by performing 3D cell-culture propagation of HEV; HEV replication was detected in the same sample independently in both institutes, and the replicated virus was shown to be infectious. We observed entire, cell-free virus particles by transmission electron microscopy at dpi 33, providing further proof of in vitro replication of the virus that contaminated the pork liver sausage.\n\n【11】We conclude that pork liver sausages can contain infectious HEV and that consuming these products should be regarded as a risk factor for HEV infection. Furthermore, we have shown the potential of the 3D culture system to correlate the presence of HEV RNA with the presence of infectious HEV particles.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5d6f053d-3006-47c7-96b5-acc6a03747b8", "title": "Web-based Investigation of Multistate Salmonellosis Outbreak", "text": "【0】Web-based Investigation of Multistate Salmonellosis Outbreak\nThe epidemiology of foodborne illnesses is influenced by a variety of factors, some of which have changed dramatically in recent years. The increased availability of preprocessed foods and the improved survival of persons with immune defects have affected the sources and nature of foodborne illness . Increased mobility of Americans through interstate travel has complicated the identification and investigation of outbreaks. New technologies for outbreak investigation have the potential to greatly assist public health officials in successfully managing these changing factors. We describe an outbreak of _Salmonella enterica_ serotype Javiana infections affecting a large group of geographically dispersed organ transplant recipients. The prompt and successful investigation of this outbreak was facilitated by the use of Web-based surveys.\n\n【1】### The Study\n\n【2】On July 16, 2002, the Minnesota Department of Health identified 2 cases of _S._ Javiana infection among persons who attended the 2002 U.S. Transplant Games, an Olympics-style athletic competition among recipients of solid organ and bone marrow transplants that was held June 25–29 at theme park A in Orlando, Florida. Isolates from the 2 patients were indistinguishable when subtyped by pulsed-field gel electrophoresis (PFGE). Approximately 6,000 people, including 1,500 transplant recipient athletes, attended the 2002 Transplant Games.\n\n【3】To identify additional cases, state health departments were asked to report any _S._ Javiana isolates with a PFGE pattern indistinguishable from the outbreak strain. To develop hypotheses about potential sources of infection, we conducted in-depth telephone interviews with several persons who were identified with culture-confirmed illness.\n\n【4】On the basis of interview results, we conducted a Web-based cohort study among Transplant Games attendees to identify risk factors for infection by using eQuest, a software package developed by the Centers for Disease Control and Prevention (CDC) that allows rapid development of Web-based surveys . Using email addresses provided to us by the Transplant Games organizers, we electronically distributed a message on July 20 to attendees (including athletes and spectators), requesting that they complete an outbreak survey. We included information about salmonellosis and its treatment and provided a link in the email to the secure Web site containing the outbreak survey. In each survey respondent’s household, we collected information for a single person visiting Orlando, regardless of whether he or she had been ill. A case was defined as fever or diarrhea with onset between June 25 and July 7 in a person who visited Orlando. Submitted answers were automatically stored in a secure electronic database and linked only to a random survey number.\n\n【5】To identify the specific food item responsible for illness, we performed a Web-based case-control study. On July 31, we distributed a survey containing detailed questions about specific food items available in theme park A to persons who had responded to the first survey. Case-patients were questioned about food items eaten in the 3 days before illness onset. Controls were defined as well survey respondents, and all were questioned about the middle 3 days of the Transplant Games (June 26–28).\n\n【6】Plant X, the processing plant that supplied tomatoes to theme park A, was inspected on August 13. Molecular subtyping of confirmed _S._ Javiana isolates was performed at state public health laboratories . Diced Roma tomatoes from unopened boxes that had been stored frozen at theme park A were cultured at the Florida State Public Health Laboratory.\n\n【7】Statistical analyses were conducted by using SAS software version 8.2 (SAS Institute Inc. Cary, NC, USA) to calculate odds ratios (OR) and 95% confidence intervals (CI). Multivariable logistic regression analyses were conducted for variables that were significantly associated with illness.\n\n【8】Through laboratory surveillance, 21 additional _S._ Javiana infections with indistinguishable PFGE patterns were identified in 10 states, for a total of 23 identified culture-confirmed cases. Dates of illness onset were from June 24 to July 8. Of 22 patients for whom travel information was collected, 19 reported visiting theme park A in the last week of June; 16 visited theme park A but did not report any contact with the Transplant Games, which suggests a true outbreak of _S._ Javiana infections among visitors to the theme park.\n\n【9】An electronic link to the Web-based cohort study survey was distributed on July 20, 2002, to 1,100 Transplant Games attendees. Among these 1,100, we received survey responses from 369 persons (34%) in 42 states; 80% responded within 48 hours. Of the 369, a total of 82 (22%) reported illness and 41 (53%) were female. The median age of ill respondents was 47 years (range 4–71 years); 48 (59%) were transplant recipients. Dates of symptom onset were June 26–July 7 . Predominant symptoms included diarrhea (93%), abdominal pain (79%), and fever (51%). Three respondents (4%) had been hospitalized. No deaths were reported to the organizers of the Transplant Games or CDC. Among ill respondents, 75 (91%) reported eating food items at specific food courts in theme park A.\n\n【10】The Web-based case-control study was distributed on July 31 to the 369 persons who responded to the first survey. By August 2, a total of 222 persons (60%) responded. Of 217 valid responses, 41(19%) were ill persons who met the case definition; the remaining 176 were healthy controls. Ill persons were significantly more likely to report eating dishes containing diced Roma tomatoes than were well persons (44% of ill vs. 15% of well, OR = 4.3, 95% CI 2.1–9.1). Other food items that were significantly associated with illness on univariate analysis were dishes containing shredded iceberg lettuce (OR = 3.7, 95% CI 1.8–7.4), pre-shredded cheddar cheese (OR = 2.9, 95% CI 1.5–5.9), fresh ground beef (OR = 3.0, 95% CI 1.4–6.4), and pre-sliced beefsteak tomatoes (OR = 4.6, 95% CI 1.1─19.4) . In multivariable logistic regression modeling, only diced Roma tomatoes remained independently associated with illness at the 0.05 significance level.\n\n【11】Diced Roma tomatoes were supplied to theme park A from plant X, where whole Roma tomatoes were mechanically diced and washed in a manually chlorinated recycled water tank. Levels of chlorine in the tank were variable (≈1.5–3.5 ppm free chlorine), providing potential opportunity for the amplification of any existent microbial contamination. Review of invoices showed that diced Roma tomatoes used in food courts patronized by persons with outbreak-related illness were processed at plant X from June 20 through July 3. No diced tomatoes from the implicated lots were available for testing. Microbiologic evaluation of an unopened box of plant X diced Roma tomatoes processed on July 12 indicated the presence of fecal coliforms (150–1,000 CFU/g).\n\n【12】### Conclusions\n\n【13】The nature of this outbreak highlights several changing features of foodborne disease epidemiology, including the enhanced mobility of persons through air travel, an increasing reliance on pre-processed foods, and an expanding immunocompromised population at risk. Through a Web-based investigation, we were able to rapidly identify the source of this outbreak and inform an immunocompromised population of its potential risk for illness. Our approach allowed us to contact and question several hundred geographically dispersed persons in a matter of days. Survey respondents’ answers were automatically stored in a secure electronic database, eliminating the need for data entry. With the development of questionnaire templates, a public health official could select sets of questions and pre-coded answers from pull-down menus and modify them to design an outbreak-specific, Web-based questionnaire that is automatically linked to an electronic database . These methods make it increasingly possible to design and post a Web-based investigative tool for wide distribution within hours.\n\n【14】Our investigation has several limitations. Use of an Internet-based investigation tool limited responses to only those Transplant Games attendees with known email addresses and Internet access. The initial response rate to our survey was only 34%; households with ill persons may have been more likely to respond to our Web-based survey. However, most (>75%) respondents to both surveys were from households in which no one had experienced illness, which provided us with a sufficient number of responses from both well and ill persons to identify the source of the outbreak. Hospitalized and severely ill persons may have been too sick to respond to the survey or may have been unable to access the Internet, which limited our ability to calculate accurate hospitalization or attack rates among persons attending the Transplant Games. Although isolation of _S._ Javiana from plant X diced tomatoes would have strengthened our findings, the tomatoes tested were processed well after the outbreak period, when levels of contamination may have differed considerably.\n\n【15】As use of the Internet becomes more widespread for participation in regional, national, and international conferences, groups, and listservs, electronic mail cohorts are becoming more commonplace. The development of Web-based public health investigative tools can facilitate future investigations of outbreaks affecting geographically dispersed persons who may be part of an electronic mail cohort. A Web-based approach to data collection can also play a critical role in rapidly sharing data in outbreaks involving multiple jurisdictions . The growing use of Web-based technologies in public health investigations will have to be balanced with the need to protect the privacy of personal information in the online environment . The careful application of emerging technologies and conventional epidemiologic techniques can help public health officials effectively cope with the multitude of changing factors that shape public health in the United States.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ab8f4e31-7a86-4ac4-b19d-376c081a806e", "title": "Host-Associated Absence of Human Puumala Virus Infections in Northern and Eastern Germany", "text": "【0】Host-Associated Absence of Human Puumala Virus Infections in Northern and Eastern Germany\nPuumala virus (PUUV) causes most hantavirus disease cases in Central and Western Europe and is the only human pathogenic hantavirus in Fennoscandia . The human infection is characterized by a mild-to-moderate form of hemorrhagic fever with renal syndrome designated nephropathia epidemica (NE), with a case fatality rate of <0.1%. The only virus reservoir in Central and Western Europe is the bank vole, _Myodes glareolus_ .\n\n【1】PUUV causes most human hantavirus infections in Germany, with an incidence of 10.31 cases/100,000 inhabitants . Human disease reports fluctuate temporally with peaks in the years 2007, 2010, and 2012, but reports also show a heterogeneous spatial distribution . Generally and during outbreak years, the highest numbers of cases occurred in the western and southern parts of Germany, whereas in the northern and eastern parts of the country only a few cases were recorded .\n\n【2】Molecular analyses of bank voles from endemic regions detected the presence of PUUV at 30 sites in Germany  and resulted in the definition of several PUUV sublineages of the Central European (CE) clade . In addition, an 8-year monitoring study on the bank vole populations in a PUUV-endemic region of northwestern Germany indicated the long-term presence of particular PUUV strains .\n\n【3】To evaluate potential reasons for the almost total absence of human PUUV infections in northern and eastern Germany, we investigated bank voles from these regions and from PUUV-endemic regions in the western and southern parts of Germany for the presence of PUUV and typed the voles to major evolutionary lineages on the basis of _cytochrome_ _b_ gene sequences.\n\n【4】### The Study\n\n【5】A total of 1,774 bank voles were collected by partners of the network Rodent-borne Pathogens  at sites in PUUV-endemic regions of western and southern Germany and sites in the eastern and northern parts of Germany . Chest cavity lavage samples of voles were investigated by IgG ELISA using a recombinant nucleocapsid protein of PUUV . For molecular PUUV detection, RNA was isolated from lung or heart tissue by using a QIAzol Lysis Reagent (QIAGEN, Hilden, Germany) extraction protocol. The RNA samples were subjected to small (S) segment reverse transcription PCR (RT-PCR) with primer pair Pu342F and Pu1102R , and the resulting cDNAs were sequenced. RNA samples were also subjected to a novel PUUV S segment–specific real-time RT-PCR with primers PUUV S-broad-F (5′-AACCCGCCATGAACAACAAC-3′) and PUUV S-broad-R (5′-TGCTGACACTGTTTGTTGCC-3′) and fluorescence reporter probe PUUV S-broad (5′ 6-FAM-GGAAATGGACCCAGATGACGT-BHQ-1 3′) (for further details see footnote of Technical Appendix Table).\n\n【6】First, serologic investigation of 1,758 chest cavity lavage samples indicated 99 seropositive voles exclusively originating from the endemic regions in southern and western Germany . This analysis failed to detect any antibody-positive animals within the 1,210 bank voles of this panel originating from the eastern and northern parts of Germany.\n\n【7】Subsequent conventional PUUV RT-PCR analysis of RNA samples from 440 voles (comprising 86 seropositive and 334 seronegative voles, 9 with equivocal results, and 11 not investigated because of the lack of chest cavity lavage samples) revealed 79 positive and 361 negative samples . All RT-PCR–positive samples again only originated from the PUUV-endemic regions. A final real-time RT-PCR investigation of 364 RNA samples, 34 being positive and 329 being negative by conventional RT-PCR, confirmed the results of the conventional RT-PCR analysis.\n\n【8】Including results of previously published studies , PUUV seroprevalence in the endemic regions showed an average of 23.9% and varied between 4.6% and 66.7% . According to the serologic and RT-PCR data, a PUUV-endemic region can be identified spanning the western and southern parts of Germany . In this study, the easternmost PUUV-positive sites were located in Saxony-Anhalt (site 97), Lower Saxony (site 60), and Thuringia (site 100) . The northernmost sites were located in Lower Saxony (sites 57 and 60) and Saxony-Anhalt (site 97). Nucleotide sequence determination and subsequent phylogenetic analysis showed that all PUUV sequences belong to the CE PUUV clade, which is divergent from other European PUUV lineages .\n\n【9】To test for a potential association between PUUV distribution in the reservoir and evolutionary bank vole lineages, we isolated mitochondrial DNA from 383 selected voles by using the GeneMATRIX Tissue DNA Purification Kit (Roboklon, Potsdam, Germany) according to manufacturer’s guidelines. The _cytochrome b_ PCR was performed and used for determination of the bank vole evolutionary lineages as described previously .\n\n【10】The _cytochrome_ _b_ –based typing revealed the presence of the bank vole Western, Eastern, and Carpathian evolutionary lineages . Most of the territory of Germany was inhabited by the Western evolutionary lineage, with its northern and eastern borders located close to the Elbe River . The distribution of the Eastern lineage ranged over almost the entire northern part of Germany, with partial sympatric occurrence of the Carpathian lineage in the northeast (sites 34, 68, 77) and the Western lineage in the central and northwest (sites 41, 52, 53, 57, 93, 97, 98). The Carpathian lineage was additionally located in the southeastern part of Germany (sites 28–30).\n\n【11】A comparison of the distribution of PUUV and the bank vole evolutionary lineages indicates an association of PUUV with the Western evolutionary lineage . This finding is in line with the detection of PUUV in Belgium and France and the exclusive occurrence of the Western evolutionary lineage in the PUUV-endemic regions of these countries . In the Bavarian Forest, the district Osnabrück (site 57), and at the easternmost distribution range in Walbeck (site 97), PUUV infections were also detected in sympatric bank voles of the Carpathian (n = 6) and Eastern (n = 7; n = 1) lineages, respectively.\n\n【12】### Conclusions\n\n【13】The occurrence of PUUV in Germany (and Belgium and France) is preferentially associated with the presence of the Western evolutionary lineage of the bank vole, but the virus was also detected in sympatric animals of the Eastern or Carpathian lineage. Future studies will have to determine if the current distribution of PUUV can be explained by the postglacial colonization of Germany by bank voles of the Western evolutionary lineage from western refugia through southern Germany .\n\n【14】The observed limited geographic distribution of PUUV in bank voles has important implications for public health measures and development of early warning modules for hantavirus outbreaks. These public health measures of monitoring local bank voles for PUUV strains  should be expanded to evaluate for further northeastern expansion.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "bc136eab-47ce-40ae-adf5-0a21f6fb7f3d", "title": "Bedside Manners", "text": "【0】Bedside Manners\nHow little the dying seem to need—\n\n【1】A drink perhaps, a little food,\n\n【2】A smile, a hand to hold, medication,\n\n【3】A change of clothes, an unspoken\n\n【4】Understanding about what's happening.\n\n【5】You think it would be more, much more,\n\n【6】Something more difficult for us\n\n【7】To help with in this great disruption,\n\n【8】But perhaps it's because as the huge shape\n\n【9】Rears up higher and darker each hour\n\n【10】They are anxious that we should see it too\n\n【11】And try to show us with a hand-squeeze.\n\n【12】We panic to do more for them,\n\n【13】And especially when it's your father,\n\n【14】And his eyes are far away, and your tears\n\n【15】Are all down your face and clothes,\n\n【16】And he doesn't see them now, but smiles\n\n【17】Perhaps, just perhaps because you're there.\n\n【18】How little he needs. Just love. More Love.\n\n【19】**From In John Updike's Room: New and Selected Poems, by Christopher Wiseman.**\n\n【20】Copyright 2005 The Porcupine's Quill.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c4b4bc9e-1419-47b3-a3c8-1df9587a942f", "title": "Molecular Typing of IberoAmerican Cryptococcus neoformans Isolates", "text": "【0】Molecular Typing of IberoAmerican Cryptococcus neoformans Isolates\nCryptococcosis is among the most prevalent life-threatening mycoses and has a worldwide distribution. The etiologic agent is the basidiomycetous yeast _Cryptococcus neoformans_ ; three varieties are recognized: _C. neoformans_ var. _grubii_ , serotype A , _C. neoformans_ var. _neoformans,_ serotype D, _C. neoformans_ var. _gattii_ , serotypes B and C, and the hybrid serotype AD .\n\n【1】Humans are infected by inhaling infectious propagules from the environment, which primarily colonize the lung and subsequently invade the central nervous system . _C. neoformans_ var. _grubii_ / _neoformans_ has been isolated worldwide from soil enriched with avian excreta . More recently, decaying wood from certain species of trees has been proposed as environmental habitat for this variety . In contrast, the distribution in nature for _C_ . _neoformans_ var. _gattii_ is geographically restricted to mainly tropical and subtropical regions . To date, specific host trees are represented by _Eucalyptus_ species, _Moquilea tomentosa, Cassia grandis, Ficus microcapra_ , and _Terminalia catappa_ .\n\n【2】Worldwide, in immunocompromised hosts, most infections are caused by _C. neoformans_ var. _grubii_ . In contrast, _C. neoformans_ var. _gattii_ virtually always affects immunocompetent hosts .\n\n【3】In the last decade, a number of DNA typing techniques have been used to study the epidemiology of _C. neoformans_ . These techniques include karyotyping, random amplification of polymorphic DNA, restriction fragment length polymorphism (RFLP), DNA hybridization studies, amplified fragment length polymorphism (AFLP), and polymerase chain reaction (PCR) fingerprinting .\n\n【4】PCR fingerprinting has been used as the major typing technique in the ongoing global molecular epidemiologic survey of _C. neoformans_ , dividing >400 clinical and environmental isolates into eight major molecular types: VNI (var. _grubii,_ serotype A), VNII (var. _grubii,_ serotype A), VNIII (serotype AD), VNIV (var. _neoformans_ , serotype D), VGI, VGII, VGIII, and VGIV (var. _gattii_ , serotypes B and C). No correlation between serotype and molecular type has been found for _C. neoformans_ var. _gattii_ . The molecular types were recently confirmed by RFLP analysis of the orotidine monophosphate pyrophosphorylase ( _URA5_ ) gene and the phospholipase ( _PLB1_ ) gene .\n\n【5】Globally, most of the isolates recovered from AIDS patients belong to the genotypes VNI and VNIV, whereas the genotypes VNI and VGI are predominant throughout the world for _C. neoformans_ var. _grubii_ and _C. neoformans_ var. _gattii,_ respectively. The larger number of genotype VNI isolates agrees with the fact that _C. neoformans_ var. _grubii_ causes most human cryptococcal infections worldwide .\n\n【6】The aims of this study were the following: 1) to extend the molecular epidemiologic survey to other parts of the world, 2) to establish a regional network of participating reference laboratories, and 3) to apply PCR fingerprinting and _URA5_ RFLP typing to investigate the genetic structure and possible epidemiologic relationships between clinical and environmental isolates obtained in Latin America and Spain. The results of this study permitted us to determine the major molecular types and their distribution within each participating country.\n\n【7】### Materials and Methods\n\n【8】##### Study Design\n\n【9】During the 12th International Society for Human and Animal Mycoses meeting in Buenos Aires, Argentina, in March 2000, it was decided to establish an IberoAmerican Cryptococcal Study Group under the coordination of E. Castañeda and W. Meyer. Each of the participating laboratories was asked to submit 10–30 isolates. For the clinical isolates, the following data were requested: isolation date, demographic data (age and gender of patient), collection location, risk factors, source and variety, and serotype. For the environmental and veterinary isolates, the data collected included isolation date, source, collection location, variety, and serotype.\n\n【10】##### Fungal Isolates\n\n【11】Cryptococcal isolates studied in this study are listed in Appendix Tables 1 and 2 . The isolates were obtained by the participating laboratories of the IberoAmerican Cryptococcal Study Group and maintained on Sabouraud dextrose agar slants at 4°C and as water cultures at room temperature. Isolates were identified as _C. neoformans_ by using standard methods . The variety was determined by the color reaction test on L \\-canavanine-glycine-bromothymol blue medium , and the serotype was determined, in selected isolates, by the use of the Crypto Check Kit (Iatron Laboratories Inc. Tokyo, Japan).\n\n【12】The isolates were sent for molecular typing to the Molecular Mycology Laboratory at the University of Sydney at Westmead Hospital, Sydney, Australia, either as water cultures or on Sabouraud dextrose agar slants. For long-term storage, the isolates were maintained as glycerol stocks at –70°C.\n\n【13】##### Reference Strains\n\n【14】A set of laboratory standard _C. neoformans_ reference strains representing each molecular type were used in PCR fingerprinting and _URA5_ RFLP as follows: WM 148 (serotype A, VNI), WM 626 (serotype A, VNII), WM 628 (serotype AD, VNIII), WM 629 (serotype D, VNIV), WM 179 (serotype B, VGI), WM 178 (serotype B, VGII), WM 161 (serotype B, VGIII), and WM 779 (serotype C, VGIV) .\n\n【15】##### DNA Extraction\n\n【16】High-molecular-weight DNA was isolated as described previously . Briefly, _C. neoformans_ isolates were grown on Sabouraud’s dextrose agar at 37°C for 48 h, a loopful of cells from the culture was mixed with sterile deionized water and centrifuged. The supernatant was discarded, and the tube containing the yeast cell pellet was frozen in liquid nitrogen. The pellet was ground with a miniature pestle. The cell lysis solution (100 mg triisopropylnapthalene sulfonic acid, 600 mg para-aminosalicylic acid, 10 mL sterile deionized water, 2.5 mL extraction buffer (1 M Tris-HCl, 1.25 M NaCl, 0.25 M EDTA, pH 8.0) and 7.5 mL phenol saturated with Tris-EDTA was preheated to 55°C, and 700 μL of this mixture was added to the frozen, ground cells. The tubes were incubated for 2 min at 55°C, shaken occasionally, and then 500 µL chloroform was added, and the mixture was incubated for a 2 min at 55°C and shaken occasionally. The tubes were centrifuged for 10 min at 14,000 rpm, and the aqueous phase was transferred to a new tube. Then, 500 μL of phenol-chloroform-isoamyl alcohol (25:24:1) was added, shaken for 2 min at room temperature, and centrifuged as above. The aqueous phase was transferred to a new tube, 500 µL of chloroform was added, shaken, and centrifuged as above. To precipitate the genomic DNA, the aqueous phase was again transferred to a new tube, and 0.03 volumes 3.0 M sodium acetate (pH 5.2) and 2.5 volumes cold 96% ethanol were added, and the mixture was gently shaken and incubated at –20°C for at least 1 h or overnight. The solution was centrifuged for 30 min at 14,000 rpm to pellet the DNA. The DNA pellet was washed with 70% ethanol and centrifuged for 10 min at 14,000 rpm and air-dried. The DNA was resuspended in 200 μL sterile deionized water at 4°C overnight and stored at –20°C.\n\n【17】##### PCR Fingerprinting\n\n【18】The minisatellite-specific core sequence of the wild-type phage M13 (5′ GAGGGTGGCGGTTCT 3′)  was used as single primer in the PCR. The amplification reactions were performed in a volume of 50 μL containing 25 ng high-molecular-weight genomic DNA, 10 mM Tris-HCl, pH 8.3, 50 mM KCl, 1.5 mM MgCl, 0.2 mM **\\[Q1: decimal point OK?\\]** each of the dATP, dCTP, dGTP and dTTP (Roche Diagnostics GmbH, Mannheim, Mannheim, Germany), 3 mM magnesium acetate, 30 ng primer, and 2.5 U Amplitaq DNA polymerase (Applied Biosystems, Foster City, CA). PCR was performed for 35 cycles in a Perkin-Elmer thermal cycler (model 480) with 20 s of denaturation at 94°C, 1 min annealing at 50°C, and 20 s extension at 72°C, followed by a final extension cycle for 6 min at 72°C. Amplification products were removed, concentrated to approximately 20 μL and separated by electrophoresis on 1.4% agarose gels (stained with ethidium bromide, 10 mg/mL stock) in 1X Tris-borate-EDTA (TBE) buffer at 60 V for 14 cm, and visualized under UV light . Molecular types (VNI–VNIV and VGI–VGIV) were assigned, according to the major bands in the patterns. All visible bands were included in the analysis, independent of their intensity .\n\n【19】##### _URA5_ Gene RFLP\n\n【20】PCR of the URA5 gene was conducted in a final volume of 50 µL. Each reaction contained 50 ng of DNA, 1X PCR buffer (10 mM Tris-HCl, pH 8.3, 50 mM KCl, 1.5 mM MgCl2; Applied Biosystems, Foster City, CA), 0.2 mM each of dATP, dCTP, dGTP, and dTTP (Roche Diagnostics GmbH), 3 mM magnesium acetate, 1.5 U AmpliTaq DNA polymerase (Applied Biosystems), and 50 ng of each primer URA5 (5'ATGTCCTCCCAAGCCCTCGACTCCG 3') and SJ01 (5'TTAAGACCTCTGAACACCGTACTC 3'). PCR was performed for 35 cycles in a Perkin-Elmer thermal cycler (model 480) at 94°C for 2-min initial denaturation, 45 s of denaturation at 94°C, 1 min annealing at 61°C, and 2-min extension at 72°C, followed by a final extension cycle for 10 min at 72°C. Amplification products were mixed with one fifth volume of loading buffer (15% Ficoll 400, 0.25% orange G, MilliQ water), 15 µL of PCR products were double digested with Sau96I (10 U/µL) and HhaI (20 U/µL) for 3 h or overnight and separated by 3% agarose gel electrophoresis at 100 V for 5 h. RFLP patterns were assigned visually by comparing them with the patterns obtained from the standard strains (VNI-VNIV and VGI-VGIV) .\n\n【21】##### Statistical Analysis\n\n【22】Initially, individual PCR fingerprints were visually compared to those of the standard strains, amplified in parallel, to determine the major molecular type for each isolate. The computer program GelComparII, version 1.01 (Applied Maths, Kortrijk, Belgium) was used to determine the genetic relationship of the strains. DNA bands of each fingerprint pattern were defined manually with a band-position tolerance of 0.9%, being the optimal settings needed to define the molecular size marker bands as 100% identical. Similarity coefficients were calculated by using the Dice algorithm, and cluster analyses were performed by the neighbor-joining algorithms by using the \"Fuzzy Logic\" and \"Area Sensitive\" option of the GelcomparII program.\n\n【23】### Results\n\n【24】During the course of this investigation, a network was established with 15 laboratories from nine countries participating in this study. The participant countries were: Argentina, Brazil, Chile, Colombia, Guatemala, Mexico, Peru, Spain, and Venezuela _._\n\n【25】A total of 340 _C. neoformans_ isolates, comprising 266 clinical, 7 veterinary, and 67 environmental isolates were submitted for molecular typing. Of these, 57 were from Argentina (53 clinical and 4 environmental), 66 from Brazil (56 clinical, 9 environmental, and 1 veterinary), 19 from Chile (15 clinical and 4 environmental), 62 from Colombia (39 clinical and 23 environmental), 15 from Guatemala (all clinical), 69 from Mexico (46 clinical and 23 environmental), 13 from Peru (all clinical), 19 from Spain (9 clinical, 6 veterinary, and 4 environmental), and 20 from Venezuela (all clinical). From the total isolates investigated, 271 (79.6%) were _C. neoformans_ var. _grubii/neoformans;_ 251 (92.6%) of them were _C. neoformans_ var. _grubii_ , 6 (1.8%) were _C. neoformans_ var. _neoformans,_ and 13 (4.8%) were AD hybrid isolates. The remaining 69 (20.4%) isolates were _C. neoformans_ var. _gattii_ .\n\n【26】All 340 isolates were typed by PCR fingerprinting by using the minisatellite-specific oligonucleotide M13 as a single primer and RFLP analysis of the _URA5_ gene with the restriction enzymes _Sau_ 96I and _Hha_ I in a double digest. The molecular types were determined for each isolate by comparing the obtained PCR fingerprint profiles and _URA5_ RFLP patterns with the respective standard patterns for each molecular type.\n\n【27】The serotyping results (Iatron) correlated with the molecular subtyping results in all serotype B (n=31) and C (n=13) isolates. Regarding serotype A, 99 from a total of 102 (97%) isolates correlated; the remaining 3 were serotype A by the Iatron and serotype AD by the molecular typing method. Regarding serotype D, one of four reported was confirmed by molecular typing; the other three were serotype AD. For serotype AD, two isolates were found when typed with the Iatron kit and eight when typed with molecular typing techniques. All the changes were found in the isolates from Spain. This finding is not surprising, taking into account that problems with the serotyping concerning potential serotype AD hybrids are known . Appendix Tables 1 and 2 list the characteristics of the studied isolates by participating country, laboratory, laboratory code, clinical, veterinary or environmental origin, isolate characteristics (isolation year, isolation location, source, gender, age and risk factor), variety, serotype and the molecular type identified during this study.\n\n【28】Both molecular typing techniques grouped the isolates in the eight previously established major genotypes . From the isolates investigated, 232 (68.2%) were molecular type VNI (serotype A, var. _grubii_ ), 19 (5.6%) were molecular type VNII (serotype A, var. _grubii_ ), 14 (4.1%) were molecular type VNIII (serotype A/D, hybrid between the serotypes A and D), with 5 having a RFLP pattern of the _URA5_ gene with seven bands, indicated by VNIII in Appendix Tables 1 and 2 and Figure 3B and 4B , corresponding to a hybrid between VNI, VNII, and VNIV and 9 isolates having an RFLP pattern of the _URA5_ gene with six bands, indicated by VNIII\\* in Appendix Tables 1 and 2 and Figure 4B and 5B, corresponding to a hybrid between VNII and VNIV, 6 (1.8%) were molecular type VNIV (serotype D, var. _neoformans_ ), 12 (3.5%) were molecular type VGI (serotypes B and C, var. _gattii_ ), 21 (6.2%) were molecular type VGII (serotypes B and C, var. _gattii_ ), 31 (9.1%) were molecular type VGIII (serotypes B and C, var. _gattii_ ), and 5 (1.5%) were molecular type VGIV (serotypes B and C, var. _gattii_ ).\n\n【29】From the 340 isolates studied 277, marked with “\\*” in Appendix Tables 1 and 2 , have been included in the GelCompar _II_ analysis. Sixty-three isolates were excluded from the analysis since their PCR fingerprinting patterns were not sharp or had been run under slightly different electrophoresis conditions, making the band positions impossible to compare. Cluster analysis of the PCR-fingerprinting profiles by using the GelCompar _II_ program grouped all isolates into three major clusters according to variety and into eight major groups according to the molecular type. The overall homology observed was 50.4% among isolates of _C. neoformans_ var. _grubii_ , 50.9% for _C. neoformans_ var. _neoformans,_ and 51.2% for _C. neoformans_ var. _gattii_ . The homology within a given molecular type was as follows: 54.8% VNI, 57.3% VNII, 51.9% VNIII, 50.9% VNIV, 56.4% VGI, 56.4% VGII, 54.4% VGIII, and 68.3% VGIV.\n\n【30】Besides grouping all isolates into the eight major molecular patterns, the molecular type VNI could be subdivided into eight main subclusters, with most of these subclusters’ grouping isolates obtained from specific countries. The similarity between isolates obtained from any individual country varied from 65% to 82%. Most of the main subclusters within molecular type VNI also contained isolates from different countries, indicating gene flow and strain dispersal between South American countries, Spain, or both. However, all isolates could be separated by their unique PCR-fingerprinting pattern, with the highest homology being 84% between two unrelated environmental isolates from Mexico City (LA 22, budgerigar \\[parakeet\\] droppings, and LA 25, pigeon droppings).\n\n【31】Of the 266 clinical isolates, 177 (66.5%) were obtained from HIV-positive patients, with 139 (78.5%) being VNI, 14 (7.9%) VNII, 13 (7.4%) VNIII, 6 (3.4%) VNIV, 3 (1.7%) VGII, and 2 (1.1%) VGIII. Most (86.4%) isolates from HIV-positive patients belonged to the molecular types VNI and VNII, representing serotype A, _C. neoformans_ var. _grubii_ . Of these, 266 clinical isolates, 51 (19.2%) were recovered from patients with no reported risk factors. From those, 23 (45.1%) were var. _grubii_ with the molecular type VNI (n=21) or VNII (n=2), and 28 (54.9%) were var. _gattii_ with the molecular types VGI (n=3), VGII (n=10), VGIII (n=14), and VGIV (n=1). For 26 of the clinical isolates, no data concerning risk factors were available. Six veterinary isolates were molecular type VGI, and one was VGII. Most of the environmental isolates belonged to _C. neoformans_ var. _grubii_ with 73.1% being VNI (n=49) and 1.5% VNIII (n=1) AD hybrids. The remaining 17 (25.3%) isolates were _C. neoformans_ var. _gattii_ with the molecular type VGI (n=1), VGII ( n \\=3), VGIII (n=12), and VGIV (n=1).\n\n【32】Cryptococcal isolates included in the IberoAmerican study were more frequently obtained from men than from women. The male-to-female ratio was 2l1 to 41, i.e. cryptococcosis was 5.1 times more common in men than in women. In the HIV-positive population alone, the incidence of cryptococcosis was 5.5 times more frequent in men than in women, based on the data obtained from the isolates investigated in this study. The age of the patients with clinically manifested cryptococcosis ranged from 4 to 73; 175 (65.9%) were between 21 and 40 years old.\n\n【33】The clinical isolates submitted to this study were collected over a period of 41 years, 1961–2001; most (92.5%) of the isolates were collected in the mid-1990s. The veterinary isolates were recovered from goats in Spain in 1995 and from a parrot in Brazil in 2000. The environmental isolates submitted were collected over a period of 7 years, 1993–2000.\n\n【34】### Discussion\n\n【35】This retrospective study of cryptococcosis in IberoAmerica was set up in an effort to establish a network of medical mycology laboratories to study the distribution of cryptococcal isolates, including the varieties and molecular types within the participating countries. The network was aimed at generating PCR-fingerprint and _URA5_ RFLP patterns under standardized conditions in the Molecular Mycology Laboratory of the University of Sydney at Westmead Hospital, for a subset of clinical and environmental _C. neoformans_ isolates from each participating country. These reference profiles are now available to each participating laboratory so they can set up the molecular typing techniques in their own laboratories, and to serve as internal controls in future extended studies of cryptococcal isolates in each country.\n\n【36】The data were obtained from a random selection of cryptococcal isolates from each participating country and laboratory, which do not necessarily reflect the true situation in IberoAmerica. Nonetheless, the data offer a general overview of molecular types and variety distribution of _C. neoformans_ in IberoAmerica. For the first time, two different molecular typing techniques, PCR-fingerprinting with the minisatellite specific primer M13 and _URA5_ gene RFLP analysis, were applied simultaneously to the same set of cryptococcal isolates, demonstrating identical groupings to the eight major molecular types previously described .\n\n【37】Previous pilot studies that used PCR-fingerprinting at the University of Sydney at Westmead Hospital distributed more than 400 clinical and environmental isolates obtained from Argentina, Australia, Belgium, Brazil, Germany, Italy, New Zealand, Papua New Guinea, South Africa, Thailand, Uganda, and the United States in eight major molecular types, VNI and VNII (serotype A), VNIII (serotype AD), VNIV (serotype D), VGI, VGII, VGIII and VGIV (serotypes B and C) . At the time of this original work, the molecular types VNI and VGI were found to be the most common genotypes worldwide. The present study, which includes more isolates from Latin America, showed the same results as regards variety _grubii,_ with VNI being the predominant molecular type, accounting for 68.2% of all isolates. However, the situation changed drastically for variety _gattii_ ; as in this study, the predominant molecular type was VGIII, accounting for 9.1% of all isolates, in contrast to previous studies which showed that the molecular type VGIII was geographically restricted to India and the United States . In the present study, VGIII was also found in Argentina, Colombia, Guatemala, Mexico and Venezuela, suggesting that it is not as limited as previously suggested. The same was true for the molecular type VGIV, previously assigned only to India and South Africa ; its presence in Colombia and Mexico, although in very low numbers, indicates a wider geographic distribution.\n\n【38】In general, the most common variety was _C. neoformans_ var. _grubii,_ 73.8% (n=251), followed by variety _gattii,_ 20.3% (n=69). Much less common were the AD hybrids, 4.1% (n=14) and variety _neoformans,_ 1.8% (n=6), which reflects the global distribution previously established .\n\n【39】The overall grouping of the isolates into eight major molecular types by PCR-fingerprinting with the minisatellite specific primer M13, obtained in this study and the previous pilot study by Meyer et al. 1999  and Ellis et al. 2000 , agrees with the findings by Boekhout et al. 2001  and by Cogliati et al. in 2000 . Boekhout et al. used AFLP analysis to study 206 global isolates of _C. neoformans_ , and grouped them into six major AFLP groups, whereas Cogliati et al. using a slightly modified PCR-fingerprinting technique with the microsatellite specific primer (GACA) 4,  grouped Italian isolates of _C. neoformans_ var. _grubii_ and var. _neoformans_ into four major molecular types. Comparable molecular/genotypes, where identified in the four cited independent studies, are VNI = AFLP1 = Cogliati VN6 (serotype A, var. _grubii_ ); VNII = AFLP1A (serotype A, var. _grubii_ ); VNIII = AFLP3 = Cogliati VN3 and VN4 (serotype AD, hybrid between var. _grubii_ and var. _neoformans_ ); VNIV = AFLP2 = Cogliati VN1 (serotype D, var. _neoformans_ ); VGI = AFLP4, VGII = AFLP6, VGIII = AFLP5 and VGIV (all corresponding to serotypes B and C, var. _gattii_ ) .\n\n【40】The overall results show some clonality between isolates obtained from a certain country or even between different countries, suggesting partial clonal spread of the pathogenic yeast within South America. However, the approximately 50%, overall similarity between _C. grubii_ isolates, with the highest being 82%, suggests that these South American isolates are more varied than those obtained in a previous study by Franzot et al. in which they examined a limited number of isolates from Brazil by using less discriminatory molecular techniques (CNRE-1 RFLP analysis and _URA5_ sequencing). In Franzot’s study, the highest similarity was >94% between the Brazilian isolates, suggesting a higher clonality than observed in the isolates obtained from New York City studied in the same paper .\n\n【41】Interestingly, Chile and Spain share similar molecular types. Both countries have a large number of molecular type VNIII isolates (AD hybrids), 15.8% and 42.1%, respectively, although VNIV serotype D isolates were present only in Chile (26.3%). These groups are usually common in a number of European countries, such as France and Italy . However, only these two countries show two different _URA5_ RFLP patterns, one consisting of seven bands, indicating a hybrid between VNI, VNII, and VNIV, and a second new hybrid _URA5_ RFLP pattern, consisting of six bands, indicating a hybrid between VNII and VNIV. As a result of the IberoAmerican study, these hybrid patterns had recently been reported as part of Jackson’s honor’s thesis work . The seven-band _URA5_ RFLP pattern was exclusively found in Spain (n=3) and Chile (n=2). These strains seem to be triploid, and cloning with subsequent sequencing of the PCR product showed that they contain three different copies of the _URA5_ gene. The six-band _URA5_ RFLP pattern found in Spain (n=5) and Chile (n=1), was also found in Mexico (n=1) and Argentina (n=2), possibly due to the presence of the molecular types VNII and VNIV in these countries . These hybrid isolates are diploid at the _URA5_ locus and contain two different copies of the gene .\n\n【42】Further studies are needed to investigate the special relationship between isolates obtained from these two countries. The similarity in the molecular types obtained from Spanish and Chilean isolates provides further evidence, that the cryptococcal strains present today in South America could be introduced during the European colonization. This idea had been suggested by Franzot et al. when investigating isolates obtained from Brazil. The authors argue that the pigeon ( _Columba livia_ ), thought to provide a major reservoir of _C. neoformans_ in pigeon excreta, is believed to have originated in southern Europe and northern Africa and has been dispersed worldwide by human travel .\n\n【43】Most of the cryptococcal isolates in this study were recovered from patients whose main risk factor was HIV infection. Overwhelming numbers of these isolates corresponded to the molecular type VNI, in accordance with previous findings, showing that isolates of this molecular type are the major source of infection in HIV-positive patients worldwide . This finding highlights the fact that most human cryptococcal infections are caused by _C. neoformans_ var. _grubii,_ serotype A . A distinct picture emerged in the group of isolates obtained from patients with no known risk factors, as most were _C. neoformans_ var. _gattii_ isolates (n=28), with the molecular types VGI (n=3), VGII (n=10), VGIII (n=14), and VGIV (n=1), compared to 23 isolates belonging to the overall most common molecular type, VNI (41.2%) of _C. neoformans_ var. _grubii_ . This finding supports the conclusion that variety _gattii_ primarily infects immunocompetent patients as Chen et al. had found when investigating Australian isolates . These authors have proposed that aboriginal people living in rural areas of Australia’s Northern Territory have a higher risk of cryptococcosis because they live in close proximity to the potential natural host of _C. neoformans_ var. _gattii,_ the eucalyptus trees .\n\n【44】Despite the fact that isolates included in this study constituted a random sampling, the results show again that HIV infection is the most important risk factor for cryptococcosis . This conclusion is supported by the number of isolates recovered from HIV-positive patients (n=177), the age distribution, which peaks between 20 and 40 years of age, and the date of isolation with a peak corresponding to the 1990s.\n\n【45】Overall, the network of mycology laboratories established in IberoAmerica provided, for the first time, a baseline knowledge of _C. neoformans_ variety and molecular type distribution in the participating countries, placing the IberoAmerican isolates in the global picture of cryptococcosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "425ec093-8104-4b22-8787-a1d6b792ea8e", "title": "Opioid-Related Hospitalization and Its Association With Chronic Diseases: Findings From the National Inpatient Sample, 2011–2015", "text": "【0】Opioid-Related Hospitalization and Its Association With Chronic Diseases: Findings From the National Inpatient Sample, 2011–2015\nAbstract\n\n【1】Chronic disease and opioid-related hospitalizations in the United States are increasing. We analyzed nationally representative data on patients aged 18 years or older from the 2011–2015 National Inpatient Sample to assess the association between opioid-related hospitalization and chronic diseases. We found that most patients with opioid-related hospitalization were white, aged 35–54 years, in urban hospitals, and had 2 or more comorbid conditions. Patients with 2 or more chronic conditions accounted for more than 90% of opioid-related hospitalizations in all years. The results suggest a need for targeted interventions to prevent opioid misuse in patients with multiple chronic conditions.\n\n【2】Objective\n\n【3】Patients with chronic conditions such as cancer, stroke, asthma, and obesity often experience chronic pain and have a higher likelihood of receiving one or more opioid prescriptions . In this study, we examined whether the prevalence of opioid-related hospitalization was associated with chronic diseases among inpatients. This study is the first to examine the prevalence of opioid-related hospitalization and chronic disease among patients admitted to community hospitals. Opioid-related inpatient stays  have increased, mainly due to opioid abuse, addiction, poisoning, and dependence, resulting from misuse of opioid pain relievers that are often originally prescribed  to treat chronic pain.\n\n【4】Methods\n\n【5】In this cross-sectional study, we analyzed data on patients aged 18 years or older from the National Inpatient Sample (NIS) from January 1, 2011, to September 30, 2015. Claims occurring from October through December of 2015 were not included because of the transition from the International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) to the 10th revision (ICD-10-CM) coding system on October 1, 2015. We included arthritis, spinal disease, asthma, liver disease, stroke, cancer, and obesity because these diseases are most likely to be prescribed opioids in the internal and family medicine specialties. Family medicine (21.8%) and internal medicine (17.6%) reportedly write the most opioid prescriptions, followed by physical medicine/rehabilitation, anesthesiology/pain, hematology/oncology, and neurology . Relationships between asthma and opioid abuse and dependence  and between chronic liver disease and opioid use  have been established, as has a relationship between stroke and opioid abuse among young stroke patients . Cancer patients rely on opioid medications for relief from cancer pain . Obesity was included because of its significant association with chronic pain . Using the NIS sample, we identified 3,239,136 opioid-related hospitalization cases for from January 1, 2011, to September 30, 2015.\n\n【6】We combined opioid dependence and unspecified use, adverse effects of opioids, opioid abuse, and opioid poisoning to create a single variable, _opioid-related hospitalization_ (specific ICD-9-CM codes are available in Appendices A and B . The data were weighted by using sampling weights provided by the NIS Healthcare Cost and Utilization Project . For creating national estimates for the prevalence of opioid-related hospitalizations and all analyses, we implemented Stata’s trend weight (TRENDWT) for 2011 and discharge weight (DISCWT) from 2012 through 2015 (StataCorp LLC). Because of the redesign of NIS data in 2012, we used TRENDWT for 2011, which allowed the national estimates for trend analysis in 2011 to be consistent with data from 2012–2015. To use trend weight, we merged the trend weight file  into the original NIS 2011 data. We analyzed the trends in opioid-related hospitalizations by type of chronic disease for 2011–2015 and used the Pearson χ 2  test to test for significance. Significance was set at _P_ < .05.\n\n【7】Results\n\n【8】The highest total number of opioid-related hospitalization cases was in 2014 (n = 704,670) . From 2011 to 2015, the highest prevalence of opioid-related hospitalization was among patients aged 35–54 years (5-year average, 37%), followed by those aged 18–34 years. The percentages of men and women with opioid-related hospitalizations were approximately equal. The highest prevalence of opioid-related hospitalization was among white patients (5-year average, 72%), and the second-highest was among black patients (5-year average, 15%). The prevalence was higher in urban hospitals (5-year prevalence range, 58%–64%) than in rural hospitals (5-year prevalence range, 36%–42%). The prevalence was also higher among patients who used Medicaid and Medicare than among those who did not. The prevalence of opioid-related hospitalization was highest (94%) among patients with 2 or more comorbid conditions. In 2014 and 2015, as many as 95% of the patients hospitalized for opioid-related causes had 2 or more comorbid conditions.\n\n【9】The prevalence of opioid-related hospitalizations significantly increased among patients with cancer, stroke, obesity, asthma, liver disease, and arthritis across all years . The most prevalent chronic disease significantly associated with opioid-related hospitalization was asthma, followed by obesity and liver disease. There was a significant association between opioid-related hospitalization and cancer, stroke, obesity, asthma, and liver disease every year of the study period, with the exception of cancer in 2013 and obesity in 2015. The association between arthritis and opioid-related hospitalizations was significant only in 2014 and 2015, and the association between spinal diseases and opioid-related hospitalizations was significant only in 2012 and 2015.\n\n【10】Discussion\n\n【11】We found that more than 90% of opioid-related hospitalizations were among patients with 2 or more chronic diseases and that the trend in opioid-related hospitalization among patients with chronic diseases is increasing. This finding is salient given that 1 in 4 US adults are living with 2 or more chronic diseases . Both the opioid crisis and rising rates of chronic disease have been described as epidemics; however, they are currently being treated separately. The US Department of Health and Human Services’ 5-point strategy to combat the opioid crisis focuses on the treatment of addiction, which is important for addressing the epidemic in the short term . A population-level approach to preventing and treating chronic diseases is also needed. Health services must address the 2 issues — opioid misuse and management of chronic disease — simultaneously.\n\n【12】Using alternative approaches to pain management is one public health strategy for addressing the opioid crisis  and is consistent with the Centers for Disease Control and Prevention’s (CDC’s) Guideline for Prescribing Opioids for Chronic Pain . Based on our findings, this approach may be particularly important in patients with multiple chronic diseases. Consistent with the CDC guideline, nonpharmacologic treatment including exercise therapy and cognitive behavioral therapy, as well as multimodal therapies combining exercise therapy with psychologically based approaches, should be used to decrease pain and improve functional ability . When opioids are used for pain management, patients with multiple chronic diseases may require close monitoring and additional education on risks of misuse.\n\n【13】This study has several limitations. Because we analyzed data from 2011 to 2015, our findings are not generalizable to more recent years. Our analysis was cross-sectional and examined the associations between opioid-related hospitalization and chronic diseases without controlling for covariates, which prevents us from drawing causal conclusions about the direction of the observed relationship. Furthermore, the nature of the data did not allow us to explore why the associations were not significant in some years. Opioid prescriptions for patients with arthritis have increased in recent years, so that could be an explanation for the strong correlation we found . Finally, we studied a small number of chronic diseases and did not assess any diseases related to mental health.\n\n【14】Despite these limitations, our findings show an increasing trend in opioid-related hospitalization among patients with chronic diseases and a high prevalence of opioid-related hospitalization among patients with multiple chronic conditions. The observed association warrants further research to determine causal inference. Results from this study can help clinicians develop strategies to prevent opioid misuse in patients with multiple chronic conditions and inform strategies for addressing both the opioid epidemic and rising rates of chronic disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "fc09adc8-54c1-4dc2-8363-a238348b311f", "title": "Maximizing the Value of Drug Stockpiles for Pandemic Influenza", "text": "【0】Maximizing the Value of Drug Stockpiles for Pandemic Influenza\n**To the Editor:** Tamiflu (oseltamivir; Roche, Indianapolis, IN, USA) is destined to be one of the few branded drugs to develop instant street recognition because of its status as 1 of only 2 licensed drugs shown to be active against the influenza A pandemic (H1N1) 2009 virus. Tamiflu is the major drug stockpiled by governments around the world in preparedness against an influenza pandemic. More than 70 governments have placed orders for Tamiflu, and at least 220 million treatment courses have been stockpiled since 2003 at a cost of $6.9 billion . Roche is producing 110 million courses for the 5 months from May to fall 2009 and will produce up to 36 million courses per month by year’s end if necessary. Given the estimated world population of 6.8 billion, it is clear that, on a global basis, stockpiles are woefully inadequate. For the United Kingdom, official estimates indicate sufficient stocks currently exist for half of the population .\n\n【1】Given the high cost of these stockpiles, every effort should be made to maximize usage of the drug. Most of us are aware of shelf-life assignment to foods, a concept first applied to drugs well before its adoption by food manufacturers. Shelf-life extension could potentially yield significant cost savings in the event stockpiled drugs are not required for use within the typical 5-year shelf life. We use Tamiflu as a case example to suggest how this could be done through careful evidence-based risk assessment.\n\n【2】The chemical integrity of any medicine, including Tamiflu, is important because decomposition may lead to loss of activity or formation of toxic products. For formulated products, decomposition may lead to impaired bioavailability. As it became obvious that there were inadequate drug stockpiles even in affluent countries, one of our authors (A.L.W.P. who had served as a member of the UK Committee on Safety of Medicines ) wrote to his local member of parliament (coauthor N.P.) to suggest that the government institute a program to extend the shelf life of drug stockpiles. A.L.W.P. argued that the relatively minor development work necessary to implement a shelf-life extension program would be highly cost-effective. The UK Department of Health then initiated a collaboration with Roche to extend the shelf life of Tamiflu. On May 8, 2009, the European Medicines Agency independently advised that new batches of Tamiflu would have a shelf life of 7 years instead of only 5 years .\n\n【3】Oseltamivir is a prodrug that needs metabolic activation  . Prodrugs are used typically to reduce toxicity caused by functional groups such as the carboxylate ion, to alter release properties (e.g. prolonging action of antipsychotic agents), or to improve absorption (bioavailability) by making the drug more lipophilic.\n\n【4】Oseltamivir carboxylic acid has poor bioavailability; <5% orally compared to 80% for oseltamivir, the parent drug . The carboxylate is the only major metabolite and the principal degradation product . Therefore, with poor storage the major risk is reduced activity through reduced absorption rather than formation of toxic by-products.\n\n【5】Health agencies also stockpile other drugs , most notably antimicrobial drugs and vaccines. Shelf-life extension would need to be assessed on a product-by-product basis. For example, antimicrobial drugs are often quite unstable and the toxicologic implications are less clear; some evidence suggests that allergenic polymers could be formed while the drugs are in storage. On the positive side, antimicrobial drugs are considerably less expensive than neuramidase inhibitors such as Tamiflu, making antimicrobial stockpiles less costly to replenish.\n\n【6】Products with more complicated delivery systems, such as zanamivir in inhalers, would require more validation. For biological products in complex formulations such as vaccines, stability validation may not be cost-effective.\n\n【7】Given the high costs involved in maintaining adequate drug stockpiles, attempts should be made to optimize the value of drugs; shelf-life extension is one of the easiest and most cost-effective ways of doing this. We suggest that governments undertake a systematic program for iterative shelf-life extension, ideally cooperatively. The considerable financial savings could mitigate drug shortages of expensive antiviral drugs. The chemical profile of oseltamivir and its degradation pathway suggest that extending the shelf life of Tamiflu to >20 years should be feasible. Storage in dry airtight containers should be able to maintain the integrity of the product for >7 years. During a pandemic, when supplies are unavailable, the balance of benefit to harm would favor using the expired product.\n\n【8】The 1918 influenza pandemic is estimated to have killed 50 million persons worldwide , many in developing countries. By better safeguarding available drug stockpiles, more drugs could be made available to poorer countries that have few drugs stockpiled.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "591a7423-fbdc-4a13-89f7-7be07b0413df", "title": "Perceived Stress, Behavior, and Body Mass Index Among Adults Participating in a Worksite Obesity Prevention Program, Seattle, 2005–2007", "text": "【0】Perceived Stress, Behavior, and Body Mass Index Among Adults Participating in a Worksite Obesity Prevention Program, Seattle, 2005–2007\nAbstract\n\n【1】**Introduction**  \nStress in numerous contexts may affect the risk for obesity through biobehavioral processes. Acute stress has been associated with diet and physical activity in some studies; the relationship between everyday stress and such behavior is not clear. The objective of this study was to examine associations between perceived stress, dietary behavior, physical activity, eating awareness, self-efficacy, and body mass index (BMI) among healthy working adults. Secondary objectives were to explore whether eating awareness modified the relationship between perceived stress and dietary behavior and perceived stress and BMI.\n\n【2】**Methods**  \nPromoting Activity and Changes in Eating (PACE) was a group-randomized worksite intervention to prevent weight gain in the Seattle metropolitan area from 2005 through 2007. A subset of 621 participants at 33 worksites provided complete information on perceived stress at baseline. Linear mixed models evaluated cross-sectional associations.\n\n【3】**Results**  \nThe mean (standard deviation \\[SD\\]) Perceived Stress Scale-10 score among all participants was 12.7 (6.4), and the mean (SD) BMI was 29.2 kg/m 2  (6.3 kg/m 2  ). Higher levels of perceived stress were associated with lower levels of eating awareness, physical activity, and walking. Among participants who had low levels of eating awareness, higher levels of perceived stress were associated with fewer servings of fruit and vegetables and greater consumption of fast food meals.\n\n【4】**Conclusion**  \nDietary and physical activity behaviors of workers may be associated with average levels of perceived stress. Longitudinal studies are needed, however, to support inclusion of stress management or mindfulness techniques in workplace obesity prevention efforts.\n\n【5】Introduction\n\n【6】The high prevalence of obesity is a major public health problem because of the association of obesity with chronic health conditions such as coronary heart disease, type 2 diabetes, and some cancers . The increase in prevalence during the last several decades more likely stems from environmental factors at multiple levels than genetic changes . More than one-third of the American adult population is obese ; long-term behavior-change interventions that focus on weight reduction have had limited success in reducing overweight and obesity. From a socioecological perspective, behavioral interventions may have limited success because they inadequately consider environmental factors .\n\n【7】Research interest in the potential role of stress in health is growing. Stress is an individual-level factor associated with environmental phenomena and individual-level disease processes . Stress occurs when environmental demands tax or exceed the adaptive capacity of an organism; the demands result in physiologic or psychological processes that put the organism at risk for disease . Adults experience numerous types of stress (eg, work, finances, family), and each type contributes to overall stress. Although types of stress may vary from person to person and group to group, overall measures of stress allow generalized comparisons. Measures of stress are imperfect, but measures of perceived stress are valuable because they account for differences in the appraisal of what is stressful, exposure to stressors, and coping ability .\n\n【8】Perceived stress is associated with direct changes to both physiologic (eg, hormonal response) and psychological processes. Chronically elevated levels of perceived stress affect cortisol levels, which have been associated with increased risk for central obesity . Evidence for an association between stress and physical activity behaviors is mixed , and more testing of physical activity theory is needed to identify inconsistencies in the literature . Experimental studies have demonstrated that acute stress affects dietary behaviors, especially among people with certain eating behaviors, such as restrained eating (ie, intentional caloric restriction) . The way in which food is consumed has also been associated with overweight and obesity in epidemiologic settings . Such behaviors include emotional eating (ie, eating to manage feelings) and task eating (ie, eating while doing other activities). These behaviors connote a level of distraction — a lack of eating awareness — that has also been linked to obesity . A lack of eating awareness may co-occur with a hectic or stressful lifestyle and facilitate unhealthy dietary choices.\n\n【9】Some types of stress, such as work stress, have been associated with obesity-related behaviors among adults . Evaluating the relationship between perceived stress and obesity-related behaviors in a working population may be especially valuable because this population likely experiences numerous types of stress and a broader range of stress exposure . More than half of adults in the United States and in King County, Washington, our study population, are employed; they typically spend more than half of their waking hours at work . The objective of this study was to examine associations between perceived stress, dietary behavior, physical activity, self-efficacy, and body mass index (BMI) among healthy working adults. Secondary objectives were to explore whether eating awareness modified the relationship between perceived stress and dietary behavior. We hypothesized that behavioral and self-efficacy indices of dietary behavior and physical activity would be associated with overall perceived stress .\n\n【10】Methods\n\n【11】### Study participants and procedures\n\n【12】PACE (Promoting Activity and Changes in Eating) evaluated associations of stress and obesity-related behaviors in a population of sedentary, mostly non-Hispanic white workers in metropolitan Seattle. PACE was a large group-randomized intervention to prevent weight gain in 34 worksites employing 2,900 workers. Baseline data collection took place from 2005 through 2007; design and eligibility criteria are described elsewhere . All study participants completed a standard questionnaire on demographic characteristics, dietary behaviors, eating awareness, physical activity, and self-efficacy. Investigators approached a random subsample of 1,263 employees at 34 worksites to complete additional measures; 864 employees at 34 worksites agreed to participate. Of those, 33 worksites comprising 627 participants completed the 10-item Perceived Stress Scale (PSS-10); 621 participants had no missing data on PSS-10 items. We conducted our analyses among this group. The University of Washington and Fred Hutchinson Cancer Center institutional review boards reviewed and approved all study protocols and materials. All study participants provided written informed consent.\n\n【13】### Measures\n\n【14】#### Perceived stress\n\n【15】We assessed the level of stress perceived by each participant in the past 30 days through the PSS-10 . Each item (eg, “In the last month, how often have you been angered because of things that were outside of your control?”) was rated on a 5-point Likert scale that ranged from 0 (never) to 4 (very often). Possible total scores range from 0 to 40; higher scores indicate higher levels of perceived stress. Studies of human social stress and health outcomes use the PSS-10, which has good reliability (α ranges from 0.84 to 0.86) .\n\n【16】#### Body mass index\n\n【17】We calculated BMI (kg/m 2  ) by using heights and weights measured by investigators at worksites during a proctored survey or other assessment or at a height and weight clinic.\n\n【18】#### Dietary behavior\n\n【19】We used indices of dietary behavior instead of actual consumption measurements as our main outcome measures. The average number of fruit and vegetable servings consumed per day by participants was assessed by using the National Cancer Institute 5 A Day 7-question fruit and vegetable assessment tool  and a single-item summary question . Fast food meals and soft-drinks consumed were also measured via single-item .\n\n【20】#### Eating awareness\n\n【21】Eating awareness was assessed by measuring the frequency of task eating. We assessed the frequency of task eating by using a single question: “How often do you eat food (meals or snacks) while doing another activity, for example, watching TV, working at a computer, reading, driving, playing video games?” Respondents chose from a 5-point Likert scale ranging from 1 (never) to 5 (always). We then defined a low level of eating awareness as task eating always or most of the time and a high level of eating awareness as task eating sometimes, seldom, or never. Eating awareness may be a correlate of obesity , but validation and reliability studies are lacking.\n\n【22】#### Physical activity\n\n【23】We used 3 measures of physical activity. Two measures were from the Godin Leisure-Time Exercise Questionnaire . We asked respondents to indicate the number of times in the past 7 days during their free time they engaged for at least 10 minutes in exercise categorized as strenuous (eg, running, jogging), moderate (eg, fast walking, baseball), and mild (eg, yoga, easy walking). We computed a physical activity score according to the recommended protocol, which uses metabolic equivalent task (MET) units; higher scores indicate more activity . Scores ranged from 0 to more than 180, but we capped the maximum at 180 to minimize the effect of outliers. We also asked respondents to indicate the frequency of sweat-inducing activity in the last 7 days as “often,” “sometimes,” or “never/rarely.” We defined regular engagement in sweat-inducing exercise as behavior that occurred often. The third measure of physical activity was frequency of walking. We used a single-item, adapted from the International Physical Activity Questionnaire (IPAQ): “During the last 7 days, on how many days did you walk for at least 10 minutes at a time?” These 3 physical activity measures have been previously validated .\n\n【24】#### Self-efficacy for behavior change\n\n【25】Self-efficacy, a determinant and consequence of behavior change , is integral to fruit and vegetable consumption , the ability to make healthier choices about weight , and the uptake and maintenance of physical activity behaviors  in adults. We assessed self-efficacy to change dietary and physical activity behavior by using 2 items: “How sure are you that you can stick to a plan to monitor your eating choices on a regular basis?” and “How sure are you that you can increase your level of physical activity on a regular basis?” Responses were on a 5-point Likert scale ranging from “not sure” to “extremely sure.”\n\n【26】### Statistical analyses\n\n【27】We conducted linear mixed-model analyses to examine the hypothesized associations. In all models, we calculated predicted means and 95% confidence intervals, adjusting for age, sex, race, and education (as fixed effects) and a worksite random effect. We additionally included an interaction term between perceived stress and eating awareness in each dietary behavior model to explore whether eating awareness modified the relationship between perceived stress and dietary behavior. Analyses were then stratified by low and high levels of eating awareness. When modeling total walking behavior, we also included manual occupation as a fixed effect because this behavior was not evenly distributed among occupational groups. Variables for BMI and the number of fast food meals and soft drinks consumed per week were log-transformed to avoid heteroscedasticity of residual errors. Variables were then back-transformed. Each predicted mean difference was related to a 9-point increase in the PSS-10 score; the 9 points correspond to the interquartile range (approximately 8.0 to 17.0). We used the Wald test to determine whether predicted mean differences or multiplicative interaction terms were significantly different from zero. We used SAS version 9.2 (SAS Institute Inc _,_ Cary, North Carolina _)_ for all analyses.\n\n【28】Results\n\n【29】Among PACE participants, 82.7% were non-Hispanic white; almost half were college graduates and had a household income of $75,000 or more . The mean (SD) PSS-10 score among all participants was 12.7 (6.4), and the mean (SD) BMI was 29.2 (6.3). A higher level of stress was associated with a higher percentage of participants who engaged in task eating always or most of the time, a lower Godin leisure-time exercise score, a lower percentage of participants who regularly engaged in sweat-inducing exercise, and fewer minutes walked per week . We found a significant interaction between perceived stress and eating awareness in relation to the 5 A Day ( _z_ \\= −3.01, _P_ \\= .003), single-item summary of fruit and vegetable consumption ( _z_ \\= −2.77, _P_ \\= .006), and fast food consumption ( _z_ \\= 3.00, _P_ \\= .003). Among participants who had a low level of eating awareness, higher levels of perceived stress were associated with fewer servings of fruit and vegetables per day and greater consumption of fast food meals . We also found a significant interaction between perceived stress and eating awareness in relation to soft drink consumption ( _z_ \\= 2.97, _P_ \\= .003); associations between perceived stress and soft drink intake in stratified analyses, however, were not significant.\n\n【30】Discussion\n\n【31】Our sample of working adults was similar to US adults in terms of BMI  and perceived stress . Compared with the US population, however, our sample had a higher level of income and education and a higher proportion of non-Hispanic whites . BMI in our sample was not associated with perceived stress. The distribution of perceived stress scores, however, indicated average levels of stress (ie, levels experienced in everyday living) and does not reflect a highly stressed population (eg, shift-workers, racial/ethnic minority groups) where we would expect to find an association . In other words, the range of perceived stress scores in our sample setting was likely too low to detect an association. However, this same range was sufficient to detect associations with more proximal obesity-related behavioral indices.\n\n【32】Higher frequency of task eating was associated with higher levels of perceived stress, but overall dietary behavior and perceived stress was not associated. We also did not detect a sex interaction in the relationship between perceived stress and dietary behavior and physical activity. The relationship between perceived stress and dietary behavior may be modified by several factors, including overweight or obesity, sex, and domains of eating behavior (eg, restrained or emotional eating) . Women and overweight or obese people may be more likely to eat in ways in which dietary choice is influenced by stress. In controlled laboratory studies, participants, particularly women and restrained eaters, have responded to high levels of stress by consuming foods high in calories, sugar, and fat (ie, more palatable foods), fewer main meals, and fewer portions of vegetables .\n\n【33】Growing evidence suggests that stress’s influence on _what_ you eat depends on _how_ you eat. People characterized as stress- or emotional-eaters tend to choose calorically dense foods to blunt their stress response or reduce negative emotions . The relationship between restrained eating and stress is more complex; retrained eaters tend to eat less during normal conditions and overeat when stressed . We propose that variables used to operationalize these eating behaviors share the underlying construct of lack of eating awareness. That is, when energy-dense eating serves as reward during times of stress or negative emotions, assessment and moderation (ie, awareness) of caloric intake is absent. The interaction of low eating awareness on the association between stress and dietary behaviors in our data supports this line of thinking.\n\n【34】Higher levels of perceived stress were also associated with lower levels of physical activity. These findings are consistent with those found in most previous studies , although the direction of the relationship has not been determined. Stress may inhibit people from engaging in physical activity, or lack of physical activity may lead to increased stress levels, or both of these scenarios may occur. Numerous cross-sectional studies document both a presence and lack of an association between higher levels of stress and lower levels of physical activity . Of the null studies, however, two  measured perceived stress among older persons, who tend to have lower levels of stress , which decreased the likelihood of detecting an association. A post hoc analysis  found that stress was more strongly associated with physical activity among people aged 30 to 44 (a group hypothesized as having higher levels of stress) than among people aged 18 to 29 or 45 or older. Higher levels of stress also predicted lower levels of physical activity among a prospective cohort of people at risk for diabetes .\n\n【35】Higher levels of stress have been associated with a lack of adherence to physical activity . Among a study of postmenopausal women, stress was modestly associated with failure to return to activity among those who had stopped . Studies on middle-aged women  and university students  found a decrease in exercise duration and an increase in the number of missed exercise opportunities as stress increased. Self-efficacy for dietary behavior and physical activity has also been negatively associated (but not significantly) with perceived stress . These findings support the idea that stress inhibits positive health behaviors, ultimately affecting one’s risk for obesity; they are also consistent with studies on stress and obesogenic behaviors among adults. Lower levels of social support among workers were associated with lower fruit and vegetable intake and lower levels of leisure-time physical activity , whereas greater work demands were associated with lower levels of physical activity , higher levels of task eating (women only) , and higher levels of fast food consumption (men only)  among working adults.\n\n【36】This study has several limitations, including its secondary cross-sectional design and the potential issue of multiple comparisons. Confirmatory analyses are needed, especially within a longitudinal setting. We did not include other measures of stress-related eating behaviors (eg, emotional eating) because evaluating the relationship between stress and eating was not the primary objective of the PACE intervention. Alcohol consumption, a potentially confounding variable in the relationship between stress and BMI, was also not measured . Using single items may also have been a disadvantage and perhaps relevant to our measures of self-efficacy. It may be important, for example, to measure both confidence in and barriers to self-efficacy because each may relate to different aspects of adopting behaviors in this population . Finally, most of our data were based on self-report. By using behavioral indices, we sought to minimize the biases potentially introduced by self-report and decrease the burden on the participant. Strengths of the study included adjustment for the multilevel data, analyses accounting for variance at worksite and individual levels, and use of physical measurements for BMI calculation.\n\n【37】In this study, perceived stress was associated with several obesogenic behaviors among mostly non-Hispanic white working adults with average levels of everyday stress. Our data provide insight into the potential role of even average levels of everyday stress in dietary, eating, and physical activity behaviors. The association of perceived stress and these behaviors is likely greater in people who have higher levels of stress. Further exploration of the role of stress in occupational groups in which excessive obesity-related behaviors have been documented may benefit future intervention strategies. Our findings may also suggest that including stress management and/or mindfulness techniques in worksite behavior-change interventions could improve program effectiveness.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6e6b12be-2028-4001-82d1-87624cf32e05", "title": "Association of GII.P16-GII.2 Recombinant Norovirus Strain with Increased Norovirus Outbreaks, Guangdong, China, 2016", "text": "【0】Association of GII.P16-GII.2 Recombinant Norovirus Strain with Increased Norovirus Outbreaks, Guangdong, China, 2016\nDuring the past 20 years, GII.4 genotypes have been responsible for most norovirus outbreaks globally . New GII.4 variants have emerged every 2 or 3 years, replacing the previous dominant variant. Likewise, in Guangdong, China, as well as in other regions of China, GII.4 noroviruses have caused most outbreaks; several other genotypes, such as GI.3, GI.6, GII.6, and GII.21, were occasionally detected in sporadic cases but rarely caused large outbreaks .\n\n【1】The Guangdong provincial surveillance network for foodborne disease outbreaks has been active since 2008 and is responsible for the surveillance of norovirus outbreaks in Guangdong. During winter 2014–15, a new GII.17 variant (GII.P17-GII.17 Kawasaki) was first identified in Guangdong and caused a substantial increase in the number of acute gastroenteritis outbreaks . Epidemics caused by this lineage were detected almost simultaneously in several other provinces of China, and sporadic cases were reported worldwide . We report an increase in the number of outbreaks associated with a GII.P16-GII.2 recombinant norovirus strain in the last months of 2016 in Guangdong.\n\n【2】### The Study\n\n【3】In Guangdong, norovirus outbreaks are highly seasonal; most (>90%) are reported during November–March . In November 2014, a GII.17 variant was identified in Guangdong and became predominant during the 2014–15 and 2015–16 norovirus seasons . After June 2016, the number of GII.17 outbreaks decreased, replaced by GII.4 outbreaks . During November and December 2016, a sharp increase in the number of norovirus outbreaks was reported in multiple cities of Guangdong through the provincial surveillance network . Seventeen (81%) of the 21 outbreaks in 10 cities were typed as GII.2; these outbreaks resulted in 760 clinical cases . This genotype was first detected in Guangzhou on November 14, 2016, and spread rapidly thereafter. The sharp increase in the number of outbreaks and the unusual GII.2 genotype prompted us to further characterize these viruses.\n\n【4】We randomly selected 5 fecal samples from each GII.2 outbreak for Sanger sequencing. The full length of _RdRp_ and _VP1_ genes were sequenced by using specific oligonucleotide primer sets  Representative sequences were deposited in GenBank (accession nos. KY485107–KY485126). A primer set (JV12RY and JV13I ) commonly used for RdRp typing failed to amplify the corresponding fragments of GII.2/Guangdong/2016 strains because of mismatches with these primers. We compared the Guangdong GII.2 viruses with related _VP1_ (GII.2) and _RdRp_ gene sequences (P16) from GenBank. Molecular clock phylogenetic analysis was performed to analyze the evolution of the _VP1_ gene sequences by using the Bayesian Markov chain Monte Carlo framework with a generalized time-reversible nucleotide substitution model  and an uncorrelated lognormal relaxed clock model  . Most GII.2 sequences before 2004 uploaded to GenBank were from Japan, the United States, and the Netherlands . GII.2 strains detected after 2004 clustered into a major lineage that included the viruses detected in Guangdong in 2016. This lineage, which also included the GII.2 viruses reported in Germany , was estimated to have emerged during 2011–2014 (95% highest posterior density 2011.7–2014.5) and was divergent from GII.2 strains detected in Japan in 2014. The genetically closest _VP1_ gene sequences were GII.2 viruses from the United States in 2011 and from Japan in 2011 and 2012 . Consistent with the tight cluster of the capsid sequences, the GII.P16 _RdRp_ gene sequences of the Guangdong and Germany outbreaks were closely related with GII.P16 sequences from GII.4 Sydney viruses detected in Japan and the United States .\n\n【5】### Conclusions\n\n【6】We report an increased number of norovirus GII.2 outbreaks during fall 2016 in Guangdong Province, China. The provincial surveillance network reported 17 GII.2 norovirus outbreaks that affected 760 persons during November 14–December 28, 2016. On the basis of complete viral polymerase gene ( _RdRp_ ) and capsid gene ( _VP1_ ) nucleotide sequences, all viruses could be typed as GII.P16-GII.2.\n\n【7】The driving force behind the emergence and spread of GII.2 noroviruses in Guangdong Province is not clear, but the fact that all viruses had a GII.P16 _RdRp_ gene might indicate that possession of this polymerase gene makes these viruses more virulent. Similar GII.P16 _RdRp_ genes have been reported as part of GII.4 Sydney viruses recently identified in Japan   and the United States  , which suggests that recent recombination events of GII.2 and GII.4 Sydney viruses with GII.P16 _RdRp_ gene sequences might have been responsible for most of the recent norovirus activity.\n\n【8】The number of GII.P16-GII.2 outbreaks peaked at the end of 2016, similar to the epidemic season caused by the GII.P17-GII.17 variant in 2014. The emergence of GII.2 viruses was almost simultaneously reported in several other regions of China (China National Surveillance Network for Foodborne Disease Outbreaks, February 2017) as well as in Germany , suggesting the fast spread of this genotype on multiple continents. As a result, the predominant norovirus genotype in Guangdong Province has changed from GII.4 viruses, of which a new variant emerged every 2 or 3 years, to non-GII.4 viruses with GII.17 during the 2014–15 winter season and GII.2 viruses during the 2016–17 season. Since the GII.17 variant has spread to several countries in Asia and has been reported on different continents, the effect of this emerging GII.2 recombinant strain warrants further global surveillance.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35cd6f2c-b877-4f23-8b07-20e01b4725d0", "title": "Community-Acquired Staphylococcus argenteus Sequence Type 2250 Bone and Joint Infection, France, 2017", "text": "【0】Community-Acquired Staphylococcus argenteus Sequence Type 2250 Bone and Joint Infection, France, 2017\n_Staphylococcus argenteus_ (formerly _S. aureus_ clonal complex 75) is an emerging species in the _S. aureus_ complex . Several studies reported sporadic cases of _S. argenteus_ infections mainly in Asia, Oceania, and the Pacific Islands  but rarely in Europe . We report the clinical characteristics of a community-acquired bone and joint infection with _S. argenteus_ in a child living in France.\n\n【1】At the end of July 2017, a 9-year-old boy with no unusual medical history or previous local trauma was hospitalized because of acute signs of infection of the third finger on his right hand. He was first seen in a local hospital and given an initial diagnosis of cellulitis (arthritis). Two days later, he was admitted to the emergency pediatric ward of a tertiary care hospital where a surgical joint exploration was performed and confirmed the diagnosis of arthritis associated with an abscess of the extensor tendon sheath .\n\n【2】Surgical microbiological samples cultured on blood agar plates (aerobic conditions at 37°C for 24 h) grew a strain that was identified by matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (Microflex LT, Bruker, France) as having log scores ranging from 1.39 to 1.87 (corresponding to _S. aureus_ , _S. simiae_ , or _S. epidermidis_ ). These scores were lower than those required for reliable identification of species (2.3) or genus (2.1) levels. Antimicrobial drug susceptibility testing (Vitek2; bioMérieux, Marcy l’Etoile, France) identified resistance to penicillin G.\n\n【3】Because there was no initial reliable identification of this strain, we performed molecular tests. The strain was negative for _nuc_ , _lukS_ \\-PV, _mec_ A, _mec_ C, _tst_ \\-1, and _spa_ genes. The V3 region sequence of the gene coding for 16S rRNA showed 100% identity with that for _S. aureus_ . Microarray analysis ( _S. aureus_ Genotyping Kit 2.0; Alere Technologies GmbH, Jena, Germany) assigned this strain to the clonal complex 2250/2277 of one of the main clusters of _S. argenteus_ . The patient was discharged and received an oral antimicrobial regimen for 6 weeks; healing was closely monitored .\n\n【4】Five weeks later, the patient was hospitalized because of recurrent signs of infection . Magnetic resonance imaging of the right hand showed osteitis . A second surgical procedure was performed . Cultures of surgical samples remained sterile after 10 days and a 16S rDNA PCR result was negative. Histologic analysis showed chronic osteitis . An oral drug regimen, including clindamycin and ofloxacin for 6 weeks, was prescribed. Long-term outcome was good , despite persistence of stiffness in the finger.\n\n【5】This rare case of osteomyelitis caused by _S. argenteus_ highlights the difficulties in correctly identifying this species. As with _S. schweitzeri_ , _S. argenteus_ is an emergent species that has been described as part of the _S. aureus_ species complex . _S. argenteus_ was first described in 2002 as a CC75/sequence type T1223 clone of _S. aureus_ in Aboriginal communities in Australia . This species was named _S. argenteus_ in 2011 because of its lack of staphyloxanthin production . Most studies reported prevalence rates for _S. argenteus_ among strain collections of 0.16%–18.6% according to geographic distribution, with a clear predominance in Asia and the West Pacific region  and a rare description in Europe .\n\n【6】In our case, no epidemiologic link to Asia or the West Pacific region was observed. The clinical spectrum of infections with _S. argenteus_ remains poorly described, but varies from asymptomatic nasal carriage  to community-acquired infections, including skin and soft tissue infections , bacteremia , and foodborne illness . Apart from 2 cases of bacteremia reported by Dupieux et al. clinical data for _S. argenteus_ infections has been poorly detailed . To the best of our knowledge, only 1 case of osteomyelitis has been reported .\n\n【7】Previous cases and the case we report indicate that _S. argenteus_ could be responsible for invasive infections that are difficult to manage. _S. argenteus_ was initially considered to be less virulent than _S. aureus_ on the basis of a study in Australia, which reported that this species was associated mainly with skin and soft tissue infections and rarely with bacteremia (3/220 cases) . However, a comparative study of 311 cases of _S. argenteus_ and _S. aureus_ sepsis in Thailand showed a similar outcome after 28 days . Moreover, virulence factors, such as Panton-Valentine leucocidin and enterotoxins, have been described in _S. argenteus_ isolates . In contrast to reports from Aboriginal communities in Australia  and remote populations in the West Pacific region , resistance to methicillin was not detected in strains from the case-patients in this study.\n\n【8】In contrast to _S. aureus_ , the effect of carriage of _S. argenteus_ has not been studied. For our case-patient, screening for _S. argenteus_ nasal carriage was not performed. However, a recent study of foodborne illness outbreaks reported the ability of this bacterium to spread in the environment and colonize food handlers .\n\n【9】_S. argenteus_ is an emerging species for which its clinical spectrum remains poorly described. Further studies are needed to better address the global prevalence and clinical role of this bacterium, including its potential effects in chronic human infections.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "5ae09d80-ae3b-47d9-a25b-85fb8dd2f597", "title": "Stockpiling Supplies for the Next Influenza Pandemic", "text": "【0】Stockpiling Supplies for the Next Influenza Pandemic\n### Introduction and Historical Context\n\n【1】Preparations for the next influenza pandemic have captured a remarkable amount of attention, effort, and fiscal funding since 2004, when the scientific and public health communities became increasingly concerned about the emergence of a novel influenza virus (H5N1) infecting humans in Eurasia . Many feared the occurrence of an outbreak on the scale of the 1918–19 pandemic, during which one third of the world’s population became infected and as many as 100 million persons died \n\n【2】Numerous guidance documents call for stockpiling certain supplies that might be needed to care for influenza patients during a pandemic . Just-in-time supply chains and standard operating procedures may be insufficient to meet demand as the number of cases increase . Healthcare systems have been challenged to determine the medical supplies that should be procured. Despite publication of numerous pandemic planning recommendations, little or no guidance has been available about this topic.\n\n【3】In December 2005, the US Department of Veterans Affairs (VA), which has governance over the largest integrated healthcare system in the United States, directed its medical centers to make detailed pandemic influenza preparations. This directive was ushered in by a guidance document  that broadly defined the goals and expectations of individual VA medical centers and provided a framework for planning and preparedness. Steps taken by medical centers, including decisions about stockpiling items, were determined by leaders at the local level.\n\n【4】To help our healthcare system prepare for a pandemic, a multidisciplinary group of experts drawn from within the VA system were empaneled to help bridge the gap between policy and procedure. Among the most challenging tasks was the development of a prioritized list detailing supplies and the essential quantities that should be stockpiled. This report aims to provide detailed example of 1 healthcare system’s approach to building a cache of supplies for the next influenza pandemic and to help identify critical gaps in knowledge that must be addressed for adequate preparedness.\n\n【5】### Steps toward Preparedeness\n\n【6】The 1,400 medical facilities in the national VA healthcare system are decentralized into 23 Veterans Integrated Service Networks (VISNs), each representing a specific region of the nation. The concepts described in this report are based on actions taken by staff in VISN 8, which includes southern Georgia, most of Florida, and all of Puerto Rico, and provides healthcare for ≈500,000 veterans. Regional network offices help integrate the activities of the medical facilities included in each VISN. Local medical center leaders are primarily responsible for the activities at each hospital and its affiliated outpatient clinics.\n\n【7】##### Step 1: Committee Formation\n\n【8】The VISN 8 leadership appointed a multidisciplinary team to a pandemic influenza planning committee (PIPC)  that was tasked to ensure a coordinated and consistent planning and response effort across the VISN. Although the committee had many responsibilities, the focus of this report is limited to the framework developed to estimate supply needs.\n\n【9】The PIPC agreed on a set of basic philosophical principles that guided our pandemic preparation efforts. During a pandemic, the first priority would be to provide the best possible care to patients while maximizing healthcare worker safety. Essential and relatively affordable patient care supplies and medications meant for basic life support (e.g. intravenous fluids, oxygen, and antimicrobial drugs) would be purchased first, and more expensive, technologically advanced life support (e.g. mechanical ventilation) equipment would be purchased when additional funds become available. This approach to balancing utilitarian and deontologic decision making is discussed elsewhere . Vaccines and antiviral drugs would not be relied upon as primary means of intervention because their availability and effectiveness during a pandemic remain uncertain . Although plans to acquire, store, distribute, and administer these countermeasure supplies would be made when possible and necessary, we acknowledge that these plans would not be relied upon as primary countermeasures in most pandemic scenarios.\n\n【10】##### Step 2: Agreeing on Assumptions to Key Questions\n\n【11】PIPC members recognized that the uncertainty surrounding a pandemic would require a series of assumptions and that any assumption would include some guesswork. To minimize errors, available historical data and guidance from governmental institutions were used to estimate the effect on our healthcare system.\n\n【12】##### How Many Persons Should We Expect Would Seek Healthcare at Our Facilities?\n\n【13】Most tools estimate effect on healthcare facilities based on population size, but we were dealing with a subpopulation of veterans that may seek care at the VA facilities or at any other community resource. In addition, VA facilities may open their doors to nonveterans during a pandemic. We decided, arbitrarily, to define our universe of patients as the number of individually enrolled persons who sought care at VA facilities during the previous fiscal year. This figure enabled us to calculate system and facility needs in a standardized fashion.\n\n【14】Once the number of patients was established, we used the US Department of Health and Human Services (DHHS) 1918-scale pandemic model   and FluSurge version 2.0 software   to estimate the number of persons who would be expected to seek care, be hospitalized, admitted to an intensive care unit (ICU), or be treated with mechanical ventilation . The only modification to the DHHS model was in the proportion of the population likely to contract influenza. The model calls for 40% disease incidence for children, 20% for healthy adults, and a somewhat higher incidence for elderly persons. Therefore, 25% seemed like a reasonable number for the VA, an institution that does not provide healthcare to children. We based calculations on the population likely to request care, not on the physical or personnel capacity of our facilities. It was our assertion that physical capacity would be increased and standards of healthcare would be lowered, as necessary, during a pandemic to permit serving as many people as possible. We acknowledge that alternate sites of care might become available during a pandemic. However, we viewed this possibility as too unpredictable to include in our assumption model.\n\n【15】##### What Length of Hospital Stay Would Be Required by Our Patients?\n\n【16】Length-of-stay figures were needed to calculate supply needs because resource use is more accurately calculated by patient-days of care instead of number of admissions. We used some of the assumptions made by FluSurge version 2.0 as follows: average length of stay (not in ICU) of 5 days per patient, an additional 10 days for those requiring an ICU stay, and an average time receiving mechanical ventilation of 10 days.\n\n【17】##### What Personal Protective Equipment Would Be Needed To Care for Patients with Pandemic Influenza?\n\n【18】Among the gaps in knowledge regarding pandemic influenza is the mechanism of human-to-human transmission of influenza . The Institute of Medicine recommendation  to consider all transmission routes probable and consequential was accepted. Precautions against standard, contact, droplet, and airborne transmission  were incorporated into the plan. We assumed that sole use of disposable N95 respirators would be prohibitively expensive or otherwise not possible because of global shortages . Instead, we decided that staff with prolonged periods of exposure (e.g. physicians, nurses, respiratory technicians, selected housekeepers) would be issued and that just-in-time fit testing, a reusable elastomeric half-face mask with 3 sets of filters, would be used. We estimated that we would need ≈1,000 of these masks and reusable goggles for each 50,000 patients served (on the basis of the size and catchment population of one of our medium-size facilities). Disposable masks would be limited to the beginning of the pandemic and to personnel with infrequent exposure. Using these principles, we calculated the workload, supplies, and medication required to care for typical influenza patients. Accordingly, estimates were produced for the average needs of influenza patients requiring \\> 1 types of services, including outpatient, inpatient medical ward, or ICU settings with or without mechanical ventilation.\n\n【19】##### Step 3: Calculating Supply and Medication Needs\n\n【20】We estimated the per patient–encounter needs by staff category  and the number of healthcare worker contacts per patient, per day, for each type of healthcare setting . In a similar fashion, supply needs were estimated per patient encounter (for outpatients) or per patient-day of care (for inpatients) . The ascertained supply and medication needs were combined in a spreadsheet to estimate the needs of each facility and for our network. Spreadsheet formulas enabled the needs of each facility or healthcare system to be easily modified by using the number of individually enrolled patients .\n\n【21】##### Step 4: Prioritizing Supply Needs\n\n【22】Because limited financial resources were available, the PIPC was asked to establish a prioritization scheme. Although every item on the list was considered important, each was subcategorized into purchase priority A, B, and C; A was the most important . To arrive at the category level, the following scheme was used. Category A was personal protective equipment, basic life-support items (intravenous fluids, oxygen), and first-line antimicrobial drugs. Category B was second-line antimicrobial drugs, ventilator supplies, sedatives, nebulizers and β-agonists, home care packs, and morgue packs. Category C was disposable ventilators, proton pump inhibitors, and vasopressors. Antiviral medications and vaccines were not included in this list because it was expected that the VA would acquire and maintain a centralized cache of oseltamivir, and vaccine availability and effectiveness were unknown.\n\n【23】##### Step 5: Compromising\n\n【24】The calculated cost of purchasing all essential items for a population of 500,000 amounted to ≈$11 million. Despite efforts to prioritize the items into 3 categories, the calculated cost of category A items far exceeded the amount of funds available. The PIPC debated the best approach and recommended that the available funds be used to purchase a percentage of category A items and that future funds would be used to purchase additional category A items and decreasing percentages of category B and category C items. For example, the funds available at that time were sufficient to purchase 12.5% of category A items. Upon the availability of future funding, perhaps an additional 7.5% of category A items would be purchased along with 5% of category B items and 2.5% of category C items.\n\n【25】##### Step 6: Ordering Items\n\n【26】Purchasing items in large quantity through a prenegotiated agreement enabled a discount off retail prices. However, despite this contract, back-order delays occurred (and would be expected to occur during a pandemic) for several key items. One supplier of personal protective equipment indicated that shipment would be delayed by 6–9 months, affirming predictions of shortages of personal protective equipment. This experience underscored making purchases well in advance of the date when the items were expected to be used for patient care.\n\n【27】##### Step 7: Storing Items\n\n【28】Storage of supplies proved to be among the most resource-intensive components of cache-building. Although initial wishes were to store a cache on the campus of each medical center, the space necessary was too large for most VA institutions to accommodate. After extensive discussion and careful analysis of options, a decision was made to store pandemic supplies in a 10,000–square foot, temperature-controlled, leased warehouse. Quoted costs for space ranged from $10 to $14 per square foot per year ($100,000–$140,000 per year for 10,000 square feet). The recommended location was near an airport to ensure efficient transport of supplies either by tractor trailer or by air cargo. A back-up emergency generator was included to maintain air-conditioning in the event of a power failure.\n\n【29】Many items purchased for the cache had expiration dates. Although most items had multiple-year shelf lives, some shelf lives were as short as 1 year. The variability of manufacturer-ascribed expiration dates and other reasons for supply rotation led to the recognition that the cache would become a dynamic component of medical system supplies. Items would need to be inspected regularly and rotated through the storage facility on a regular basis. To meet this need, a human resource commitment of 1 full-time employee equivalent would be necessary for logistics management of the inventory. Duties of this person would include inspecting the inventory, assisting with incoming and outgoing deliveries, rotating items into the routinely used supplies of the medical system to ensure use before expiration, and prodding physical security for the inventory. This person would also be charged with developing and maintaining a plan for transportation and deployment of the inventory in the event of a pandemic. In addition, each medical facility would also be required to provide an employee to help manage the inventory and who would report to the cache in the event of a pandemic.\n\n【30】### Discussion and Recommendations\n\n【31】Despite the numerous uncertainties posed by pandemic influenza, the types and quantities of essential items that should be stockpiled can be estimated by using a reasoned approach. What is offered in this report is a method to calculate the components of a stockpile by using assumptions that are drawn from previous pandemics. This method enables modification of figures, making them scalable and adaptable to any size population. By following the logic of the proposed calculations, it should be possible to modify the assumptions and other figures as needed for almost any community or healthcare system.\n\n【32】Perhaps the most important limitation with this method is a reliance on assumptions. No one knows what the next pandemic will bring. We believe that it is better to plan for a more severe event that will leave the system overprepared than to risk being underprepared. However, this approach may be viewed by some as unnecessary or too expensive. A stratified purchase plan, in which a fraction of essential items is purchased periodically, is recommended on the basis of availability of funds. Some may favor purchase of all items in 1 category before moving to the next category.\n\n【33】The formation and management of a pandemic supply cache would require considerable human and financial resources. The level of commitment may be viewed by some healthcare systems as too costly, especially in an era of economic instability and healthcare system instability likely requiring major reform. Some of the more resource-intensive components of the proposed approach, such as the storage of items in a staffed central facility, were facilitated by the large size of our healthcare system and available resources. Achieving a similar product in the private sector, where healthcare systems are typically much smaller than those in the federally managed VA, might require a partnership among multiple healthcare systems in a region.\n\n【34】One of the most common and widely held misconceptions we encountered was the notion that a healthcare system could be stressed to the breaking point, such that a large surge of patients could eventually render a hospital unable to function. It is our view that in reality, healthcare systems are designed to operate in a graded fashion. Although it is theoretically conceivable that a catastrophe could cause hospitals to cease functioning, it is much more likely that they will continue functioning, even under the most ominous circumstances . What will change is the standard of care that is delivered; many patients may have access to less resources than would normally be available . Stockpiling supplies should help prepare for a downgraded level of care that becomes inevitable as resources are increasingly stretched. A key message taken from our experiences was that supplies need to be ordered far in advance of a pandemic to avoid major problems with back orders and supply shortfalls.\n\n【35】The estimated cost of purchasing all supplies and medications needed to provide healthcare to a population of 500,000 during a wave of an influenza pandemic, including negotiated and contracted prices, was ≈$11 million, or approximately half that amount if one only considers purchasing priority A items. This amount is considerably higher than the amount estimated elsewhere . This difference may stem from the way we calculated our needs: we did not assume that we could reach a full capacity. Instead, we attempted to estimate the population that is likely to seek care and assumed that under the dire circumstances of a severe pandemic facilities would decrease standards of care, open alternate sites of care, and creatively care for those patients who came for treatment. We also did not assume that we would have a shortage of personnel to care for these patients. Through altered standards of care; emergency privileging and cross-training of healthcare workers, volunteers, and other persons willing and able to care for our patients, a temporary and substantially changed workforce would be expected to emerge.\n\n【36】Numerous gaps in knowledge were encountered. The mechanisms of human-to-human transmission of seasonal and pandemic influenza are poorly understood. Numerous articles have discussed the types of respiratory protection that should be considered and stockpiled  for healthcare workers. However, there has never been a definitive, prospective clinical trial that shows whether respirators are superior to surgical masks. Translational research funding to answer these questions should become a priority.\n\n【37】This report is an attempt to describe the challenges our healthcare system faced when preparing for an influenza pandemic. By no means are all the answers, or even the questions, reported here. Additional work is needed to further identify important questions and appropriate solutions. We hope these concepts will help guide the decisions of other healthcare systems as they work through this challenging task.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d0e4115a-6b1d-4de4-b02f-c5227abbaa91", "title": "Tularemia in Pregnant Woman, Serbia, 2018", "text": "【0】Tularemia in Pregnant Woman, Serbia, 2018\nTularemia is a systemic, potentially serious zoonotic disease caused by _Francisella tularensis_ bacteria . Interhuman transmission of tularemia has not been reported. Transmission is primarily by Ixodidae ticks ; the second most frequent vectors are mosquitoes in restricted areas (e.g. Sweden and Finland) . Contact with live infected animals can be a source of human infection. Many tularemia cases also occur through contact with contaminated aquatic environments (e.g. swimming, canyoning, fishing) . Incubation period is 1–10 days, and clinical signs depend on the route of infection. Direct contact with animals or tick bites leads to a skin lesion with satellite lymphadenopathy. Ingesting contaminated food or drinking contaminated water leads to pharyngitis and cervical lymphadenopathy. Clinical signs are ulceroglandular disease with cutaneous ulcerations and marked lymphadenopathy; glandular disease with no ulcers but marked lymphadenopathy; oculoglandular disease with preauricular lymphadenopathy and conjunctivitis; oropharyngeal disease with pharyngitis and cervical lymphadenopathy; gastrointestinal disease with vomiting, abdominal discomfort, and diarrhea; respiratory disease (pneumonia or pleuritis); and typhoid tularemia (fever without early signs/symptoms) . If not treated adequately, tularemia can spread to the lungs, pleura, gastrointestinal tract, or central nervous system .\n\n【1】There are 2 subspecies of _F. tularensis_ : type A, which is almost completely restricted to North America, and type B, which is found in Europe . Type A strains are highly virulent. Before the advent of effective antimicrobial drugs, mortality rate for both types was 5%–10% . Current mortality rates are 2%–3% for type A and <1% for type B tularemia but vary according to type of infection and _F_ . _tularensis_ genotype. Mortality rates among patients with bilateral type A acute pneumonia are \\> 30% .\n\n【2】After 2000, the annual incidence rate of tularemia in Europe was highest in Kosovo (>5 cases/100,000 population), followed by Sweden, Finland, Slovakia, Czech Republic, Norway, Serbia, Hungary, Bulgaria, and Croatia . Tularemia is more common in men than in women and extremely rare in pregnant women . To date, at least 12 cases of tularemia during pregnancy have been described in the literature . We report the case of a pregnant woman with tularemia in Serbia.\n\n【3】### The Case\n\n【4】In 2018, a 33-year-old woman in the second trimester of pregnancy (18 weeks’ gestation) was referred to the Gynecology Department of the Policlinic for Students’ Healthcare, University of Pristina, for a painless lymphadenopathy on the left side of her neck. The woman denied having a fever or other signs/symptoms of infection. She lived in the urban area of Kosovska Mitrovica and reported no contact with known reservoirs of _F. tularensis_ , although she did mention having gone for a walk and spending some time in natural areas, but not swimming, a few days before symptom onset. She denied drinking nonpotable water (e.g. spring water) and confirmed that she drank only bottled water.\n\n【5】Three months before pregnancy, cervical screening (including smear test and microbiology) detected no abnormalities. She had no previous pregnancies or remarkable medical or surgical history. During week 7 of gestation, a viable, eutopic, singleton pregnancy was confirmed, and at week 12, gynecologic and ultrasonography examinations and double marker, blood, urine, and biochemical testing were performed. Fetal growth corresponded with the length of the amenorrhea, and all results were within the normal range for pregnancy. Double marker testing indicated low risk for trisomies. The patient was asymptomatic. The next appointment was at week 16 of gestation, when uneventful pregnancy with constant growth velocity was confirmed.\n\n【6】The painless lymphadenopathy was detected at week 18. The lymph node was mobile, ≈2 cm in diameter, and painless. Blood analysis results, including leukocytes, were within reference limits and showed no evidence of bacteremia. Nose and throat swab sample cultures showed no microbial growth.\n\n【7】For biopsy of the swollen lymph node, we referred the patient to the Clinic for Otorhinolaryngology and Maxillofacial Surgery at the Clinical Centre of Serbia, University of Belgrade. At the time of the transfer, she was 19 weeks pregnant, febrile, but clinically stable. Fetal growth corresponded to gestational age. Immunologic analyses, blood count, and peripheral blood smear performed by the hematologist ruled out blood disorders and systemic diseases (antinuclear antibodies were negative). Serologic analyses excluded infection caused by Epstein-Barr virus, cytomegalovirus, HIV, herpes simplex virus, and hepatitis B and C viruses. Toxoplasmosis and brucellosis were excluded by relevant testing.\n\n【8】The enlarged neck node was excised with local anesthesia at week 19 of gestation. Histopathologic findings confirmed histolytic necrotizing lymphadenitis (also called Kikuchi-Fujimoto disease), which can be induced by an infectious disease. Serum was sent to SYNLAB (Augsburg, Germany) for _F. tularensis_ testing, and after 2 weeks, enzyme immunoassay showed increased IgM against _F. tularensis_ (16.1 U/mL, cutoff<10 U/mL) but not IgG, which correlated with the clinical signs and indicated acute infection.\n\n【9】The patient was hospitalized for 2 weeks; she was febrile for 2 days (temperature not >39°C) and treated symptomatically. Ultrasonography for fetal anomaly performed at the Institute for Gynaecology and Obstetrics “Visegradska,” Belgrade, detected no abnormality, and fetal growth velocity was maintained. Blood analysis remained within reference limits, with no evidence of bacteremia. After receiving gentamicin (120 mg intramuscularly 1×/d) for 7 days, the patient was discharged at week 22 of gestation.\n\n【10】At term, the woman vaginally delivered a healthy boy, weighing 3,860 g, with Apgar scores of 9 at 1 minute and 10 at 5 minutes. Histopathologic examination of the placenta revealed no evidence of pathology. Newborn hearing screening on day 3 was normal bilaterally. At examinations performed 1 and 4 months after delivery, both mother and baby were healthy, with no complications, and all blood test results were within reference limits. The newborn was not tested further for infectious diseases.\n\n【11】### Conclusions\n\n【12】According to the available literature, tularemia in pregnancy is rare; at least 12 cases have been described . Although gentamicin, ciprofloxacin, and doxycycline are not recommended as the first-line treatment for pregnant women, the World Health Organization recommends ciprofloxacin and gentamicin for treatment of tularemia during pregnancy . Use of gentamicin, ciprofloxacin, and doxycycline is not associated with increased risk for birth defects, stillbirth, premature birth, or low birth weight . A previous case indicated that during pregnancy, dacryocystitis and abscess on the neck with pharyngitis did not affect the pregnancy outcome after surgical and antimicrobial treatment . The gradient of symptoms and clinical manifestations of tularemia in pregnant women differ and depend on the mode of transmission of the infection, the amount and virulence of the pathogen, and the host immunity. Mild clinical signs/symptoms can be expected in healthy young pregnant women without other comorbidities who acquire infection in the second or third trimester, as did the patient we report.\n\n【13】Tularemia is extremely rare during pregnancy; mild disease can appear during the second and third trimesters, and adequate treatment can be administered without consequences to the fetus. Therefore, in a pregnant woman with lymphadenopathy, with or without signs/symptoms, infections such as tularemia should be considered in the differential diagnosis, especially in areas where tularemia still circulates at a high incidence rate. To optimize maternal and infant outcomes, tularemia can and should be adequately and effectively treated during pregnancy.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c219ddd7-9907-4c78-b2f6-c3c35283f4fd", "title": "Mycobacterium tuberculosis Complex Drug Resistance in Italy", "text": "【0】Mycobacterium tuberculosis Complex Drug Resistance in Italy\n**To the Editor:** The reemergence of tuberculosis (TB) as a global health problem over the past 2 decades, accompanied by increased drug resistance, which represents a serious problem both in terms of TB control and clinical management , prompted Western European countries to develop comprehensive national surveillance systems to monitor trends in TB drug resistance. Moreover, the World Health Organization (WHO) and the International Union Against Tuberculosis and Lung Disease (IUATLD) launched the Global Project on Anti-Tuberculosis Drug Resistance Surveillance to measure the prevalence of drug resistance by using standardized methods and assess its correlation with indicators of TB control . Since comprehensive data on resistance to firstline drugs were not available in Italy, a network of 20 regional laboratories was established to participate in this project. The Department of Bacteriology and Medical Mycology of Istituto Superiore di Sanità in Rome and the Mycobacteriology Unit of Istituto Villa Marelli in Milan (appointed respectively as Supranational Reference Laboratory and National Reference Laboratory) supervised and controlled the network of regional laboratories. The combination of reference laboratories in the network and associated clinical units, which covered 30% of definite cases reported each year , was known as SMIRA (Italian Multicentre Study on Resistance to Anti-Tuberculosis Drugs). The WHO/IUATLD coordinating center in Ottawa, Canada, provided a batch of 20 _Mycobacterium tuberculosis_ strains to set up proficiency testing to check drug susceptibility procedures in all SMIRA laboratories . We summarize the nature and extent of TB drug resistance in Italy between 1998 and 2001.\n\n【1】Isolates from all consecutive, definite cases diagnosed in TB units during 1998 through 2001 were included. When a patient’s previous treatment status was unknown or dubious, the case was excluded. Resistant cases from patients with and without history of previous treatment were stratified by the following categories: any resistance, monoresistance, resistance to both isoniazid and rifampicin (known as rifampin in the United States), or resistance to three or more drugs. Confidence intervals were also calculated. Participating laboratories were allowed to use the WHO-recommended drug susceptibility method with which they were most familiar: absolute concentration method, resistance ratio method, proportion method and its variants, or BACTEC 460 radiometric method (Becton Dickinson, Towson, MD) . Among the laboratories reporting results by the proportion method, the majority used Löwenstein-Jensen medium while others used liquid nonradiometric media . Each of the 20 _M. tuberculosis_ strains was tested against firstline drugs by the Italian Reference Laboratories in Rome and Milan and classified as resistant or susceptible. Results were compared to the standard criterion, represented by the judicial results of the WHO/IUATLD Global Network of Supranational Laboratories . Each network laboratory was validated for each firstline drug when no more than two results were different from the standard criterion.\n\n【2】The prevalence of drug resistance detected during the period 1998–2001 is summarized in the Table . Among previously untreated cases, the prevalence of resistance to isoniazid, rifampicin, ethambutol, and streptomycin was 3.5%, 0.8%, 0.5%, and 4.3%, respectively, while prevalence of multidrug resistance (resistance to at least isoniazid and rifampicin) and polyresistance (resistance to two or more drugs, but not both isoniazid and rifampicin) was 1.1% and 2.4%, respectively. No difference was found by stratifying prevalence data by age, sex, or HIV status. In isolates from patients with previous treatment, drug resistance was found to be almost four times higher than in those from patients with no history of treatment. However, the prevalence of monoresistant strains was low (5.3%, 4.3%, 0.3%, and 4.3% for isoniazid, rifampicin, ethambutol, and streptomycin, respectively) compared with the prevalence of multidrug-resistant strains whose rate reached a peak of 30.4%.\n\n【3】Drug-resistant TB in countries with good national control programs, such as in Western Europe, is not commonly a major health problem, although increasing immigration prompts public health authorities to maintain vigilant surveillance systems. The results of our study indicate that throughout Italy, prevalence of resistance to firstline drugs and multidrug resistance among isolates from new cases was consistently low over the 4-year survey period. Prevalence of multidrug resistance among isolates from previously treated patients was high, although a downward trend could be demonstrated during the last 2 years. Since almost 2 out of 10 isolates resistant to rifampicin were multidrug resistant, using rapid molecular methods to identify rifampicin resistance in questionable cases appears cost-effective to facilitate early detection and control of multidrug-resistant TB . Resistance to isoniazid is associated with immigration from countries where isoniazid was used extensively in the past. This information is a useful tool for clinicians, as isoniazid resistance may be suspected early in the disease and properly treated. Finally, the finding of substantial multidrug resistance among isolates from previously treated patients, combined with the evidence that immigrants from areas where isoniazid resistance is endemic contribute substantially to the number of new TB cases in Italy every year, strongly suggests that public health action is needed to improve treatment outcomes.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e26403cb-d887-4c5c-b977-d17e85789bce", "title": "State and Local Area Integrated Telephone Survey", "text": "【0】The State and Local Area Integrated Telephone Survey (SLAITS) collects important health care data at State and local levels. This data collection mechanism was developed by the National Center for Health Statistics (NCHS) of the Centers for Disease Control and Prevention (CDC). It supplements current national data collection strategies by providing in-depth State and local area data to meet various program and policy needs in an ever-changing health care system.\n\n【1】What's New\n\n【2】*   Design and Operation of the National Survey of Children’s Health, 2011-2012 Cdc-pdf (7/2017)\n*   Reported Child Health Status, Hispanic Ethnicity, and Language of Interview: United States, 2011–2012 Cdc-pdf \\[PDF – 276 KB\\] (9/2015)\n*   Diagnostic Experiences of Children With Attention-Deficit/Hyperactivity Disorder Cdc-pdf \\[PDF – 231 KB\\] (9/2015)\n*   Design and Operation of the National Survey of Children with Special Health Care Needs, 2009–2010 Cdc-pdf \\[PDF – 9 MB\\] (11/2014)\n*   Adverse Family Experiences Among Children in Nonparental Care, 2011–2012 Cdc-pdf \\[PDF – 158 KB\\] (5/2014)\n*   2011-2012 National Survey of Children’s Health\n*   Changes in Prevalence of Parent-reported Autism Spectrum Disorder in School-aged U.S. Children: 2007 to 2011–2012 Cdc-pdf \\[PDF -163 KB\\] (3/2013)", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f9dcde38-7ab1-4d61-9f2d-8f5f9a7df9a7", "title": "Staphylococcus aureus Infections in New Zealand, 2000–2011", "text": "【0】Staphylococcus aureus Infections in New Zealand, 2000–2011\nDespite advances in diagnostics and therapeutics, the clinical and economic burdens of _Staphylococcus aureus_ infections remain a substantial public health problem . During the past decade in several parts of the world, most notably in North America, the epidemiology of _S. aureus_ infections has changed dramatically, predominantly because of the epidemic spread of a strain of community-associated methicillin-resistant _S. aureus_ (MRSA) . Infections caused by community-associated MRSA are most commonly skin and soft tissue infections (SSTIs) and typically occur in patients with no history of exposure to health care facilities . In addition, specific sociodemographic associations for community-associated MRSA infection have been described and include younger patient age, specific ethnic groups, and economic deprivation . Although the epidemiology of _S. aureus_ infections has been well studied in North America, comparatively little is known about the trends and patient demographics for _S. aureus_ infections in other geographic settings, particularly in the Southern Hemisphere. Knowledge of the overall prevalence and distribution of _S. aureus_ infections, regardless of methicillin resistance, at a population level is crucial for informing prevention and control strategies.\n\n【1】The incidence of invasive and noninvasive _S. aureus_ infections is reportedly higher In New Zealand than in other developed countries; rates are highest among Māori (indigenous New Zealanders) and Pacific Peoples . For example, in 1 study, _S. aureus_ bacteremia was 2 times more likely to develop among Māori patients and 4 times more likely to develop among Pacific Peoples than among European patients . To date, however, studies describing _S. aureus_ infections in New Zealand have generally been confined to 1 geographic region, to children, or to 1 specific aspect of _S. aureus_ disease such as bloodstream or MRSA infection . Accordingly, we sought to describe the longitudinal trends for _S. aureus_ infection and demographic characteristics of patients across the entire New Zealand population for the 12-year period 2000–2011.\n\n【2】### Methods\n\n【3】##### Study Setting\n\n【4】New Zealand is an island nation in the southwestern Pacific Ocean and has ≈4.4 million residents. The population is ethnically diverse, consisting of the following ethnicities: 67% European, 15% Māori, 10% Asian, 7% Pacific Peoples, and 1% other . New Zealand has a public health care system; data on all publicly funded hospital admissions are recorded by the New Zealand Ministry of Health in the National Minimum Dataset (NMDS). In addition to basic patient information such as age, sex, and ethnicity, these data include principal and additional hospital discharge diagnoses, which since July 1999 have been coded according to the International Classification of Diseases, Tenth Revision (ICD-10). Our study population included all patients discharged from New Zealand hospitals from January 2000 through December 2011.\n\n【5】##### Data Collection and Definitions\n\n【6】In New Zealand, a unique identifier (the National Health Index number) is assigned to each person who accesses public health services; this number can be used to extract information from the NMDS about patient hospitalizations. Patients were identified from the NMDS on the basis of _S. aureus_ –associated ICD-10 discharge codes. These ICD-10 codes were A410 (sepsis due to _S. aureus_ ), J152 (pneumonia due to staphylococci), and B956 ( _S. aureus_ as the cause of diseases classified elsewhere). A case of _S. aureus_ SSTI was defined as infection in a patient who had 1) a principal discharge diagnosis of SSTI (according to an epidemiologic case definition validated in a previous study ), 2) an additional discharge diagnosis of B956, and 3) no additional discharge diagnoses containing either A410 or J152. The National Health Index number can also be used to filter out unrelated hospital admissions. We filtered our data to exclude the following groups: overseas visitors, patients on waiting lists, hospital attendees who did not stay overnight, hospital transferees, and patients readmitted to the hospital within 30 days of first admission.\n\n【7】The following information about each patient who was discharged from the hospital for an _S. aureus_ –associated cause was extracted from the NMDS: age, sex, ethnicity, and socioeconomic status (derived from the New Zealand deprivation index \\[NZDep\\] . The NZDep score is an area-based measure of socioeconomic deprivation derived from New Zealand census data; the score is based on various measures of deprivation, including household income, household ownership, household occupancy, employment and education levels, and access to telecommunications. It is expressed as a score between 1 and 10; a score of 10 represents the most deprived neighborhoods. To determine whether any increasing trends in _S. aureus_ infection were associated with a general increase in all hospital admissions, we obtained information from the NMDS on all patients acutely hospitalized overnight in New Zealand over the study period, applying the same exclusion filters described above.\n\n【8】##### Statistical Analyses\n\n【9】Age-adjusted incidence rates were calculated per 100,000 population and standardized to the age distribution of the 2006 New Zealand census . These incidence rates were stratified according to sex, ethnicity, NZDep score, and geographic region. For analysis, we used 4 major ethnic groups: European, Māori, Pacific Peoples, and Asian/other. To determine possible geographic differences in incidence of _S. aureus_ infection across New Zealand, we analyzed 4 broad geographic regions: northern, midland, central, and southern . Population denominator data were obtained from Statistics New Zealand . A Poisson regression model, with log population data as an offset variable, was used to assess trends over time. The Kruskal-Wallis analysis of variance test was used to determine differences in the geographic incidence of _S. aureus_ infections. Relative risks were calculated with 95% CIs, and all statistical analyses were performed by using SAS version 9.3 (SAS Institute Inc. Cary, NC, USA) or STATA version 11.1 (StataCorp, College Station, TX, USA). We considered p<0.05 to be statistically significant.\n\n【10】### Results\n\n【11】For the study period, 61,522 _S. aureus_ –associated hospital discharges were identified. The overall averaged 12-year incidence rate for all _S. aureus_ infections was 127 (95% CI 122–133) per 100,000 population per year. The overall incidence rate for _S. aureus_ SSTIs was 108 (95% CI 105–111) per 100,000 population, _S. aureus_ sepsis 14 (95% CI 13–16) cases per 100,000, and staphylococcal pneumonia 5 (95% CI 4–6) cases per 100,000. The incidence rate for sepsis caused by _S. aureus_ and pneumonia caused by staphylococci did not change significantly over the study period; however, the incidence rate for _S. aureus_ SSTIs increased significantly, from 81 (95% CI 78– 84) cases per 100,000 population in 2000 to 140 (95% CI 137–144) cases per 100,000 in 2011 (p<0.001) , which represents an increase of ≈5% each year. In contrast, the rate of acute all-cause hospital discharges in New Zealand fell significantly, from 9,657 (95% CI 9,625–9,689) per 100,000 population in 2000 to 8,701 (95% CI 8,673–8,729) per 100,000 population in 2011 (p<0.001). Consequently, the relative proportion of _S. aureus_ SSTIs to all hospital discharges doubled, from 0.8% in 2000 to 1.6% in 2011.\n\n【12】Incidence of staphylococcal pneumonia did not vary significantly by geographic location (p = 0.8); however, incidence of staphylococcal sepsis (p = 0.02) and SSTIs (p = 0.01) did . In particular, there was a distinct north–south gradient for staphylococcal SSTIs; rates in the northern and central regions were ≈3 times rates in the southern region.\n\n【13】Incidence of _S. aureus_ infections also varied markedly by sociodemographic characteristics . Staphylococcal infections of all forms were significantly more likely to occur among male than female patients; this difference was most marked for _S. aureus_ sepsis (relative risk \\[RR\\] 1.9; 95% CI 1.8–2.0). The incidence rates for sepsis and pneumonia were significantly higher among patients >70 years of age (62 and 24 cases/100,000 population/year, respectively) than among patients of other age groups . In contrast, the incidence rate for _S. aureus_ SSTIs was highest among those <5 years of age (242 cases/100,000 population/year). The incidence of all disease types was highest among Māori and Pacific Peoples . In particular, Māori were 3 times more likely and Pacific Peoples almost 5 times more likely than Europeans to have an _S. aureus_ SSTI.\n\n【14】The incidence of _S. aureus_ disease also varied significantly according to socioeconomic deprivation; the incidence rates for sepsis, pneumonia, and SSTI were significantly higher among patients residing in areas of high socioeconomic deprivation. This disparity was most marked for SSTIs; patients residing in areas of high deprivation were almost 4 times more likely to have _S. aureus_ SSTIs than were those residing in areas of low deprivation (RR 3.7, 95% CI 3.6–3.8). An independent association seemed to exist between _S. aureus_ disease and ethnicity after socioeconomic status was adjusted for, such that for each tier of socioeconomic deprivation, all 3 types of _S. aureus_ disease were more common among Māori and Pacific Peoples than among those of European or other ethnicity .\n\n【15】### Discussion\n\n【16】In this study, we analyzed the longitudinal incidence and epidemiology of serious _S. aureus_ disease across the entire New Zealand population during 2000–2011. Incidence of _S. aureus_ SSTI increased dramatically while incidence of _S. aureus_ sepsis and pneumonia remained relatively stable. Our finding of a persistent increase in serious _S. aureus_ SSTIs over the past decade is a substantial public health concern, particularly given the overall decrease in acute overnight hospital admissions in New Zealand.\n\n【17】The factors underlying the increase in such infections are unknown, but risk factors for the development of _S. aureus_ SSTI are multifactorial and probably include household crowding, delayed or inadequate access to health care, and issues associated with household hygiene . The reasons for the relatively unchanged rate of _S. aureus_ sepsis and staphylococcal pneumonia in New Zealand are unclear; however, recent studies highlight the decreasing incidence of invasive _S. aureus_ infections in other geographic settings, particularly among those patients recently exposed to health care facilities or receiving health care . Although improvements in infection prevention practices probably contribute to the decrease of invasive infections , other possible unexplored factors include changes in host susceptibility to _S. aureus_ infection (e.g. improved management of concurrent conditions such as cardiovascular disease and diabetes) or temporal changes in the virulence profiles or transmissibility of circulating _S. aureus_ strains.\n\n【18】Consistent with the findings of other studies of infectious diseases in New Zealand , we found notable sociodemographic disparity in the incidence of _S. aureus_ infections; incidence of all _S. aureus_ infections was highest among Māori or Pacific Peoples and among those residing in areas of high socioeconomic deprivation. Even after adjusting for socioeconomic deprivation, we found that the incidence of all _S. aureus_ disease was significantly higher among Māori and Pacific Peoples than among patients of European and other ethnicities; this pattern is seen for infectious diseases generally in New Zealand . The underlying reasons for this apparent ethnic disparity in staphylococcal disease are uncertain. Unexplored possibilities include a higher prevalence of _S. aureus_ colonization among Māori or Pacific Peoples or differences in the circulating _S. aureus_ strain types among distinct ethnic groups, as previously described for our setting . However, an alternative possibility is that the area-based NZDep score used to record socioeconomic deprivation does not fully represent those facets of poverty that contribute to the development and prevention of serious _S. aureus_ disease. These unmeasured risk factors include aspects of health literacy relating to early management of insect bites and skin infections, availability of household amenities such as hot water, and affordable and timely access to health care. Specific individual-level and household-level studies are required for determination of the relative contribution of such potentially modifiable risk factors.\n\n【19】We also observed significant geographic variation in the incidence of _S. aureus_ SSTI, with a distinct north–south gradient. This finding can probably be explained by the distribution of population groups in New Zealand; the groups most affected by _S. aureus_ SSTI reside predominantly in the North Island . However, other possible contributory factors include geographic differences in access to and provision of health care and climate differences; the climate in the upper North Island is relatively warmer and more humid than that in the southern regions.\n\n【20】A limitation of our study was our use of hospital discharge data for case ascertainment. Use of these data meant that we were unable to determine the proportion of cases occurring in the community versus in the hospital setting, although previous studies have demonstrated that most _S. aureus_ infections in New Zealand originate in the community . However, our aim was not to provide detailed information on individual _S. aureus_ infections but rather to provide a broad overview of the trends and demographics of serious _S. aureus_ infections across the entire New Zealand population. In addition, our data represent only those patients whose hospital discharge was associated with _S. aureus_ disease; they do not represent those patients who sought care from a primary care physician or who sought care at a hospital but were not admitted. For example, a recent study of children with SSTIs in 1 New Zealand region found an estimated 14 primary care cases for every 1 hospital admission . Furthermore, these data represent only those instances in which an etiologic agent was described and recorded in the discharge diagnoses. It is therefore highly likely that the overall prevalence of staphylococcal disease in our setting is substantially higher than that estimated here.\n\n【21】In summary, our study provides valuable longitudinal data on the prevalence of serious _S. aureus_ disease in the New Zealand population and represents one of the few studies that systematically assessed the epidemiology and demographics of staphylococcal infections across an entire nation. The steady and significant increase in serious _S. aureus_ SSTI coupled with notable sociodemographic disparity in disease incidence is a disturbing national trend. A concerted multimodal public health intervention is urgently required to tackle this problem.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "970096dd-8ddf-43ab-b7b5-e30d93ff4397", "title": "Reston Ebolavirus Antibodies in Bats, the Philippines", "text": "【0】Reston Ebolavirus Antibodies in Bats, the Philippines\n**To the Editor:** Filoviruses cause highly lethal hemorrhagic fever in humans and nonhuman primates, except for Reston Ebolavirus (REBOV), which causes severe hemorrhagic fever in macaques . REBOV epizootics among cynomolgus macaques occurred in 1989, 1990, 1992, and 1996  and among swine in 2008 . African fruit bats have been suggested to be natural reservoirs for Zaire Ebolavirus and Marburg virus . However, the natural reservoir of REBOV in the Philippines is unknown. Thus, we determined the prevalence of REBOV antibody–positive bats in the Philippines.\n\n【1】Permission for this study was obtained from the Department of Environment and Natural Resources, the Philippines, before collecting bat specimens. Serum specimens from 141 wild-caught bats were collected at several locations during 2008–2009. The bat species tested are summarized in the Table . Captured bats were humanely killed and various tissues were obtained. Carcasses were then provided to the Department of Environment and Natural Resources for issuance of a transport permit.\n\n【2】We used immunoglobulin (Ig) G ELISAs with recombinant nucleoprotein (NP) and glycoprotein (GP) of REBOV  to determine REBOV antibody prevalence. REBOV NP and GP were expressed and purified from Tn5 cells infected with recombinant baculoviruses AcResNP and AcResGPDTM, which express NP and the ectodomain of GP with the histidine tag at its C-terminus. We also used histidine-tagged recombinant Crimean-Congo hemorrhagic fever virus NP as a negative control antigen in the IgG ELISA to confirm specificity of reactivity.\n\n【3】In IgG ELISAs for bat specimens, positive results were detected by using rabbit anti-bat IgG and horseradish peroxidase–conjugated anti-rabbit IgG. Anti-bat ( _Rousettus aegyptiacus_ ) rabbit IgG strongly cross-reacts with IgGs of other bat species, including insectivorous bats . Bat serum samples were 4-fold serially diluted (1:100–1:6,400) and tested by using IgG ELISAs. Results of IgG ELISAs were the sum of optical densities at serum dilutions of 1:100, 1:400, 1:1,600, and 1:6,400. Cutoff values (0.82 for both IgG ELISAs) were determined by using serum specimens from REBOV antibody–negative bats.\n\n【4】Among 16 serum samples from _R. amplexicaudatus_ bats, 5 (31%) captured at either the forest of Diliman (14°38′N, 121°2′E) or the forest of Quezon (14°10′N, 121°50′E) had positive results in the IgG ELISA for REBOV NP, and 5 (31%) captured at the forest of Quezon had positive results in the IgG ELISA for REBOV GP. The REBOV NP antibody–positive bats serum samples were confirmed to be NP antibody positive in the IgG ELISA by using glutathione-S-transferase–tagged partial REBOV NP antigen . Three samples had positive results in both IgG ELISAs . Serum samples from other bat species had negative results in IgG ELISAs.\n\n【5】All bat serum samples were also tested by indirect immunofluorescence assays (IFAs) that used HeLa cells expressing NP and GP . In the IFAs, 2 samples from _R. amplexicaudatus_ bats captured at the forest of Diliman and the forest of Quezon had high titers (1,280 and 640, respectively) of NP-specific antibodies, and 1 sample from an _R. amplexicaudatus_ bat captured at the forest of Quezon had a positive result in the GP-specific IFA (titer 20). All IFA-positive samples were also positive in the IgG ELISA .\n\n【6】The forest of Diliman is ≈30 km from the monkey facility and the Bulacan farm where REBOV infections in monkeys and swine, respectively, were detected. The forest of Quezon is ≈60 km from the monkey facility. Samples from other bat species had negative results in IFAs. We also performed heminested reverse transcription PCR specific for the REBOV NP gene with spleen specimens from all 16 _R. amplexicaudatus_ bats but failed to detect any REBOV-specific amplicons.\n\n【7】REBOV-specific antibodies were detected only in _R. amplexicaudatus_ bats, a common species of fruit bat, in the Philippines. In Africa, _R. aegyptiacus_ bats _,_ which are genetically similar to _R. amplexicaudatus_ bats, have been shown to be naturally infected with Zaire Ebolavirus and Marburg virus. Thus, _R. amplexicaudatus_ bats are a possible natural reservoir of REBOV. However, only 16 specimens of _R. amplexicaudatus_ bats were available in this study, and it will be necessary to investigate more specimens of this species to detect the REBOV genome or antigens to conclude the bat is a natural reservoir for REBOV.\n\n【8】We have shown that _R. amplexicaudatus_ bats are putatively infected with REBOV or closely related viruses in the Philippines. Antibody-positive bats were captured at the sites near the study areas, where REBOV infections in cynomolgus monkeys and swine have been identified. Thus, bats are a possible natural reservoir of REBOV. Further analysis to demonstrate the REBOV genome in bats is necessary to conclude that the bat is a reservoir of REBOV.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d9f59525-839a-4757-a8f6-ec126e2681af", "title": "Analysis of Complete Puumala Virus Genome, Finland", "text": "【0】Analysis of Complete Puumala Virus Genome, Finland\nThe outcome of a viral infection is determined by the agent’s pathogenicity and by host factors, such as genetic predisposition. For RNA viruses, which are notorious for their swift evolution and adaptation, a pathogen’s specific genotype usually is to blame for devastating effects . In many cases, however, the virus genome is not easy to search for particular mutations because recovery of complete viral sequences from clinical specimens remains extremely difficult. This is especially true for hantaviruses (family _Bunyaviridae_ ) that cause hemorrhagic fever with renal syndrome (HFRS) and hantavirus cardiopulmonary syndrome . Thus far, only a few complete hantavirus genomes originating from persons with clinical cases have been reported , and only 1 was recovered without passaging first in cell culture , which by itself can induce adaptive changes in the viral genome . We present the complete genome of PUUV directly recovered from a person with fatal infection.\n\n【1】Usually PUUV causes mild HFRS (also called nephropathia epidemica \\[NE\\]). In Finland, 1,000–3,000 NE cases are diagnosed annually, i.e. ≈60 cases/100,000 persons during years when the vole population peaks . Almost 100% of infected persons recover, and long-lasting complications are rare. The few fatal cases reported  showed no apparent geographic clustering. Thus, whether more severe illness could be connected to certain genetic variants of PUUV remains unknown.\n\n【2】### The Study\n\n【3】The patient was a previously healthy 37-year-old man with a history of smoking. He died in November 2008 of severe NE on day 4 after the onset of symptoms that started with high fever, vomiting and diarrhea, headache, and visual disturbances. His condition deteriorated quickly, and multiorgan failure developed, including respiratory distress, acute kidney failure, liver failure, and severe thrombocytopenia.\n\n【4】A standard autopsy was performed, and tissue samples were stored fresh at −70°C and fixed in formalin. PUUV infection was confirmed initially by IgM test and later by reverse transcription PCR (RT-PCR), followed by sequencing. Genetic analysis was performed from autopsy samples stored fresh at −70°C (the high quality of clinical samples was crucial for the downstream applications). Complete PUUV genomes were recovered in a set of nested and seminested PCR (sequencers of primers are available on request). Amplicons were gel-purified and sequenced directly by using ABI PRISM Dye Terminator sequencing kit (PerkinElmer/ABI, Foster City, CA, USA). Quantitative RT-PCR was used to measure PUUV load with DyNAmo Capillary SYBR Green kit (Finnzymes, Espoo, Finland). Copy numbers were calculated from a standard curve created by using in vitro transcribed PUUV small (S) segment RNA (T7 transcription kit, Fermentas, Vilnius, Lithuania).\n\n【5】Quantitative RT-PCR revealed the highest numbers of virus genome copies in lungs and kidneys: 1,881 and 1,136 per μg of total RNA, respectively. Copy numbers per μg of total RNA in other tissues were lower: 240 in the heart, 160 in the spleen, 50 in the liver, and 42 in the brain. In agreement with these findings, complete PUUV genome sequences (12,059 nt) were recovered from the lung and kidney and partial sequences of different lengths from heart, liver, and brain . Corresponding sequences recovered from different tissues were identical, i.e. no tissue-specific mutations were observed.\n\n【6】To determine whether this fatal NE case was caused by an unusual or rare genetic variant of PUUV, we searched for identical or closely related genetic variants in bank voles trapped near the patient’s house (storage buildings and surroundings within 500 m) in Pieksämäki, central Finland (62°18′N, 27°08′E). Travel history of the case-patient suggested that the infection had been acquired at his residence. In 2008, the vole population peaked in the southern half of Finland, including Pieksämäki, and 3,259 NE cases were diagnosed nationwide , the highest number ever registered in Finland. Sixty-three bank voles were snap-trapped during 3 consequent nights in December 2008.\n\n【7】Lung tissue samples from the bank voles were screened for PUUV N protein antigen by using immunoblotting, and 45 (71%) voles tested positive. Tissues from 25 virus-infected voles were taken for genetic analysis, and partial sequences of PUUV genome S, medium (M), and large (L) segments (≈12% of the total virus genome) were recovered from them.\n\n【8】In agreement with previously published data , the number of PUUV genome copies in bank vole tissues was within the range of 10 5  –10 6  /μg of total RNA, i.e. ≈100-fold higher than in tissues of the case-patient. Partial virus sequences from 4 voles were 100% identical to those from human tissues. Next, complete PUUV genome sequences were recovered from 2 of these voles; the sequences differed at only 4 positions in the L segment (all silent mutations, right column). One of the complete rodent-originated PUUV sequences was 100% identical to the sequence from the case-patient. PUUV sequences have been deposited in GenBank under accession nos. JN831943–JN831952. Phylogenetic analysis confirmed that the hantavirus involved belonged to _Puumala virus_ species and was most closely related to the earlier described genetic variants from Finland, particularly to those circulating at Konnevesi (62°34′N, 26°24′E) and Puumala (61°52′N, 28°17′E) localities .\n\n【9】### Conclusions\n\n【10】Our findings established an unequivocal genetic link between the fatal human NE case and local wild-type PUUV strains. These findings also revealed that no mutations had accumulated in the genome of PUUV during transmission of the virus to the patient and the fatal generalized infection that followed. Finally, we demonstrated that the wild-type PUUV strain that caused the fatal infection was neither a unique nor rare genetic variant; the exact sequence match to the complete human-originated PUUV sequence was found among the first 25 bank voles analyzed. Genetic links of the type have been reported for PUUV infections in Finland  and for Sin Nombre virus infection during the outbreak in the Four Corners area of the United States , but perfect sequence match was not observed.\n\n【11】In PUUV infections, renal insufficiency is a hallmark of the disease, but pulmonary, cardiac, central nervous system, ocular, and hepatic manifestations and, in severe cases, hypophyseal injury also can occur . In the fatal case described here, death resulted from multiorgan failure when kidneys, lungs, heart, and liver were affected. The viral load was higher in the lungs and kidneys and lower in the heart, spleen, liver, and brain. Whether this load distribution is unique for fatal PUUV infections remains to be seen because corresponding data for other hantavirus infections are missing. Moreover, severe histopathologic changes were detected not only in lungs and heart but also in liver and hypophysis, whereas kidneys, in this respect, were almost normal. Thus, viral load does not seem to correlate with tissue pathology. A more detailed pathologic description of this and other lethal cases is under way.\n\n【12】Two more observations might be relevant to the case. First, human leukocyte antigen typing showed that the patient had the risk haplotype for severe NE including a C4A null allele, i.e. a major antivirus defense system complement was impaired . Second, the patient was a smoker and thus more likely to become infected with PUUV . These factors might have substantially affected the fatal outcome. We anticipate that our investigation will prompt further full-length genome analyses of the wild-type strains of bunyaviruses that cause infections in humans.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "643dae2d-8e70-41fa-9764-ed4cbb0d7069", "title": "Determining Mortality Rates Attributable to Clostridium difficile Infection", "text": "【0】Determining Mortality Rates Attributable to Clostridium difficile Infection\n_Clostridium difficile_ infection (CDI) has emerged as a major health care–associated infection; incidence, hospitalizations, and mortality rates are increasing . Reported case-fatality rates are 6%–30% and seem to be rising . The reporting of CDI-associated deaths could be considered a quality indicator; however, the accuracy of death certificate data is questionable . We analyzed CDI deaths in 3 hospitals in Ontario, Canada, and compared 3 measures for attributing death to CDI: death certificate, death within 30 days of CDI, and a panel review process (considered the reference standard).\n\n【1】### The Study\n\n【2】From April 2007 through February 2008, as independent quality initiatives, 3 hospitals in Ontario reviewed deaths among patients with CDI infection. Patients were identified by using existing surveillance data. To calculate the time from CDI diagnosis to death, we compared date of death with date of onset of CDI symptoms or date of the positive _C. difficile_ test result if symptom onset was unclear. For recurrent CDI, date of recurrent symptoms nearest to date of death was considered. Patients with suggestive recurrent symptoms but no laboratory confirmation were included in a further analysis in this study.\n\n【3】For panel review, clinical data and cause of death indicated on death certificate were anonymously summarized for each patient. Each case summary was reviewed by 3 physicians with varying levels of expertise with CDI. Panel members were asked to independently categorize their interpretation as follows: a) death was directly attributable to CDI; b) CDI strongly contributed to death; c) CDI somewhat contributed to death; d) death was unrelated to CDI; or e) information was insufficient to determine the role of CDI in the death. Hospital A was the first facility to undertake a panel review and included only 4 categories (category b was excluded). Feedback from reviewers at hospital A led to development of category b.\n\n【4】After individually classifying each death, reviewers participated in a panel discussion to achieve consensus. For analysis, the 5 categories were subsequently collapsed into 3: CDI directly caused or strongly contributed to death; CDI somewhat contributed to death or was unrelated to death; and information was insufficient to determine the role of CDI in the death.\n\n【5】The κ statistic was used to determine the level of agreement on cause of death between death certificate and panel review. The 3 categories listed above were compared with the following death certificate categories: CDI, enterocolitis, or toxic megacolon as primary or contributory cause of death; death unrelated to CDI; and missing information.\n\n【6】The percentage agreement of death within 30 days of CDI and panel review consensus was calculated by using combined data from hospitals A and B. Hospital C was excluded from this analysis because only patients who died within 30 days of CDI diagnosis were included in that review. At hospital B, because data on individual physician assignment of categories were available, inter-rater reliability was analyzed by using the Fleiss κ statistic.\n\n【7】CDI was diagnosed for 501 patients, and 188 CDI patients died. Of these, 120 (64%) patients died within 30 days of CDI. The 30-day case-fatality ratios for hospitals A, B, and C were 20% (25/124), 35% (62/177), and 16% (33/200), respectively.\n\n【8】Panel reviews were conducted for all 31 in-hospital deaths in hospital A, 90 deaths in Hospital B, but only 30 of the deaths that occurred within 30 days of CDI diagnosis in hospital C. Among the 151 deaths included in the panel review process, CDI directly caused or strongly contributed to the death for 101 (67%) and somewhat contributed or was unrelated to death for 49 (32%). For 1 patient, information was insufficient for determining cause of death. Where data were available (hospital B), inter-rater reliability among panel members was satisfactory (κ = 0.71, 95% CI 0.59–0.83).\n\n【9】According to death certificate data, CDI was the primary cause of death for 7 (5%) patients and a contributory cause of death for 44 (29%). Of the 101 deaths classified by panel review as strongly attributable to CDI, 37 were classified by death certificate as having CDI as primary/secondary cause . When panel review data were compared with death certificate data, κ was 0.07 (95% CI 0.05–0.20), indicative of poor agreement. The exclusion of hospital A, where deaths were originally classified into only 4 categories, did not alter the observed κ scores (κ = 0.04, 95% CI 0.09–0.17).\n\n【10】To compare the proportion of in-hospital deaths within 30 days with results of the panel review, we used only data from hospitals A and B because hospital C data only included patients who had died within 30 days. The panel concluded that CDI directly or strongly contributed to death within 30 days of onset for 80% (63/79) of patients . If cases suggestive of recurrent CDI (not confirmed by testing) were included, this percentage rose to 86% (68/79). When panel review was used as the reference standard, the sensitivity of death within 30 days of CDI onset was 80%, specificity 41%, and positive predictive value 72%.\n\n【11】### Conclusions\n\n【12】Agreement between causes of deaths categorized by review panel and causes listed on death certificates is poor. Ontario’s vital statistics system currently codes only 1 cause of death, limiting the ability to identify deaths for which CDI might have been a contributing cause. These shortcomings suggest that death certificate data may be inaccurate for assigning CDI-attributable death.\n\n【13】Panel review of all deaths is an alternative approach that would enable clinical analysis of the circumstances surrounding the death. A panel review reduces individual reviewer bias; however, while arguably the most accurate method of determining cause of death other than autopsy, it is not feasible for wide-scale public reporting.\n\n【14】Our study supports the use of death within 30 days as a marker for CDI-attributable death because 80% of deaths identified by panel review as being directly or strongly attributable to CDI occurred within 30 days of diagnosis. This percentage increased to 86% if clinical recurrences were included. Compared with panel review, death within 30 days had reasonable sensitivity (80%) and positive predictive value (72%) but, as expected, was not specific (41%). Capturing data on death within 30 days would be more feasible than panel review and more accurate than death certificate data. However, data on death within 30 days could not be used to determine the contribution of CDI to any patient’s death.\n\n【15】Our study has a few limitations. Because data were derived from 3 hospitals that undertook reviews for different purposes, the slight differences in inclusion criteria and death categorizations necessitated subanalyses for the death within 30 days comparison and the collapsing of the categorizations into 3 groups. Data quality also has inherent problems associated with retrospective chart audits because of limitations in the documentation of clinical events. Finally, we acknowledge that categorization of deaths by panel review is a subjective process based on interpretation of the clinical case summaries and expert opinion.\n\n【16】Our findings suggest that cause of death on death certificate is an inaccurate measure of death attributable to CDI. However, death from CDI within 30 days should be considered a feasible measure for the purposes of aggregate public reporting.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "6dc2fe38-055b-4cb8-a82d-41fb478835cc", "title": "Hepatitis E Virus Genotype 1, Cuba", "text": "【0】Hepatitis E Virus Genotype 1, Cuba\n**To the Editor:** Hepatitis E virus (HEV) causes acute viral hepatitis, which in rare cases leads to fulminant hepatitis with high death rates, especially among women in their third trimester of pregnancy . Sporadic infections and epidemics have been reported from all parts of the world, especially Asia, Africa, and Latin America. Although indigenous hepatitis E has rarely been observed in industrialized countries, higher than expected anti-HEV prevalence has been detected in these areas . In the Caribbean region, many countries including Cuba, Haiti, Guatemala, and Honduras have reported hepatitis E , but the viruses have not been characterized.\n\n【1】The transmission of HEV is primarily fecal–oral, through contaminated drinking water; limited zoonotic transmission has also been reported. Despite only 1 serotype, 4 major genotypes of HEV have been reported . Genotype 1 is mainly responsible for sporadic infections and large outbreaks in Asia and Africa. Genotype 2 was first found in Mexico and later on the African continent. Genotypes 3 and 4 have been reported from the United States, Europe, China, Japan, and Taiwan; this group also includes the related swine HEV .\n\n【2】We report the phylogenetic analysis of 11 HEV isolates from 2 outbreaks and sporadic cases in Havana, Cuba. The first outbreak occurred in 1999 in a factory; 20 persons were affected (12 women, 8 men; median age 45 years, range 22–53 years). The second outbreak was in 2005 in a suburb of Havana and involved 26 persons (15 women, 11 men; median age 24 years, range 17–45 years). We also analyzed HEV in 12 sporadic clinical cases obtained from the Cuban national surveillance program for viral hepatitis. Most patients reported asthenia, epigastric pain, nausea, and vomiting. None had any history of international travel, contact with persons traveling from disease-endemic areas, or consumption of exotic foods. Serologic screening showed all patients to be negative for immunoglobulin (Ig) M against hepatitis A and hepatitis C viruses. One patient had positive results for hepatitis B surface antigen but negative results for anti-hepatitis B core antigen IgM and hepatitis B virus DNA. All patients were positive for anti-HEV IgM (Genelab Diagnostics, Singapore) according to the manufacture’s criteria. A total of 22 serum samples (outbreak 1, n = 9; outbreak 2, n = 7; sporadic cases, n = 6) were tested for HEV RNA; only 2 (both from sporadic cases) were positive. A total of 31 serum samples were also tested for anti-HEV IgG (Genelab Diagnostics), of which 22 were positive (outbreak 1, n = 7/10; outbreak 2, n = 10/13; sporadic cases, n = 5/8).\n\n【3】A total of 44 stool samples were collected 2–4 weeks after onset of symptoms and stored at –70°C until use. Fecal samples were screened for HEV open reading frame (ORF) 2 by using reverse transcription (RT)–PCR . For genotyping, nested RT-PCR was then performed for the ORF1 RdRp region  on 18 samples that were positive for ORF2. Of the 12 PCR products obtained, 11 fragments were cloned into pGEMT Easy Vector (Promega, Madison, WI, USA). At least 3 positive clones for each sample were sequenced, and the consensus sequence was used for phylogenetic analysis. The GenBank accession numbers of ORF1 for the HEV outbreak cases from Cuba are CUB10-1999 (EU165504), CUB11-1999 (EU165502), CUB13-1999 (EU1655019), CUB19-1999 (EU165500), CUB24-1999 (EU165499), CUB68-2005 (EU165496), and CUB71-2005 (EU165495). For the sporadic cases they are CUB9-2005 (EU165503), CUB1803-2003 (EU165494), CUB2-2005 (EF493155), and CUB27-2005 (EU165498). Additionally, nested RT-PCR was conducted with ORF2-specific primer pairs  for 2 HEV isolates, 1 each from outbreak and sporadic cases, and sequences were obtained. The accession numbers for ORF2 are outbreak CUB10D-1999 (EU284749) and sporadic CUB2D-2005 (EU284748).\n\n【4】Phylogenetic analysis of ORF1 nucleotide sequences showed that HEV isolates from Cuba clustered in genotype 1 with high bootstrap values . The same genotype was detected in an outbreak of hepatitis E in UN peacekeepers deployed from Bangladesh to Haiti . Although the outbreak was adequately contained, anti-HEV immunoglobulin was subsequently detected in 3% of civilians in Haiti . Nucleotide identity between isolates from Cuba and other HEV strains from genotype 1 ranged from 91.7% to 99%. The strains from Cuba were closely related to the isolates from India and shared 97.8%–99% homology with Yam-67 . Absolute ORF1 nucleotide differences  of isolates from Cuba ranged from 0% to 1.6%, demonstrating a high degree of relatedness. The ORF2 analysis supported our ORF1 findings because the CUB2D-2005 and CUB10D-1999 sequences also clustered with genotype 1 . Both strains from Cuba shared 96.1% nucleotide homology with a prototype strain from Burma (Bur82) and were related to the strains from India (Hyderabad and Yam-67), sharing 97.4%–99% homology. Absolute ORF2 nucleotide differences ranged from 0.8% to 1.9%. This value for ORF1 ranged from 0.05% to 0.08% for the same isolates from Cuba (CUB2-2005 and CUB10-1999).\n\n【5】HEV shows a global presence. The genotype distribution, although dominant in a given geographic area, is not limited to that area. For example, genotype 2, first identified on the American continent in Mexico , was later found in Namibia and Nigeria on the African continent . We report indigenous HEV genotype 1 strains in the Americas.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a9a3962a-195a-42f1-a4b8-f87b42144697", "title": "Fatal Monocytic Ehrlichiosis in Woman, Mexico, 2013", "text": "【0】Fatal Monocytic Ehrlichiosis in Woman, Mexico, 2013\n_Ehrlichia_ are rickettsia-like intracellular bacteria of human medical and veterinary importance. The first cases of human monocytic ehrlichiosis (HME) were described in 1987, and the etiologic agent was subsequently identified in the United States as _Ehrlichia chaffeensis_ , a strictly intracellular bacterium belonging to the family _Anaplasmataceae . _E. chaffeensis_ is transmitted by _Amblyomma americanum_ ticks; reservoirs include domestic and wild animals . In the United States, most cases occur from April through September. Ehrlichiosis is usually self-limiting with nonspecific symptoms similar to those of influenza: fever, malaise, headache, and myalgia. Leukopenia is found in 60%–70% of patients, thrombocytopenia in 60%, and mild to moderate elevation of serum transaminase levels in 80%–90% . Serologic evidence of infection may be absent during the early acute phase of illness. In patients with illness severe enough that they seek medical attention, 50% require hospitalization and 2%–3% die .\n\n【1】In Mexico, only 1 case of _E. chaffeensis_ has been reported . Recently, _E. chaffeensis_ has been identified in _Rhipicephalus sanguineus_ and _Amblyomma cajenennse_ ticks, which are found throughout Mexico . We report another case in Mexico, this one fatal.\n\n【2】### The Study\n\n【3】In late August 2013, a previously healthy 31-year-old woman from Estado de Mexico, in central Mexico, was admitted to an emergency department with a history of fever for 15 days, chills, muscle aches, malaise, loss of appetite, and headache. She had worked in a marketplace selling fruit and vegetables; she had not traveled abroad in the previous 3 months and was not aware of having been bitten by a tick. At the time of physical examination, she was confused and had mild respiratory distress, hepatosplenomegaly, tachycardia, and blood pressure within reference range. Blood collected at the time of admission showed leukopenia (0.90 × 10 9  cells/L), neutropenia (0.31 × 10 9  cells/L), lymphocytopenia (0.59 × 10 9  cells/L), thrombocytopenia (76 × 10 9  platelets/L), anemia (hemoglobin 8.2 g/dL), and elevated serum concentrations of aspartate transaminase (2,748 IU/L) and alanine transaminase (350 IU/L). A full evaluation for sepsis (blood cultures, morphologic evaluation, and culture of bone marrow aspirate) was performed. The bone marrow aspirate contained no significant abnormalities. Computed tomography indicated hepatosplenomegaly and a small pericardial effusion; ultrasonography indicated bilateral nephromegaly; and echocardiography indicated a small pericardial effusion and an ejection fraction of 59%.\n\n【4】After these procedures were completed, the patient was transferred to the intensive care unit (ICU); 1 day later, she was stable and discharged to a regular hospital ward, at which time blood and bone marrow culture results were negative. No morulae were detected in smears of peripheral blood and bone marrow. At that time, the patient’s mental status included confusion, a psychotic episode, and symptoms of anxiety; a psychiatrist prescribed benzodiazepines. Prednisone therapy was added for suspected hemophagocytic syndrome. The patient’s condition deteriorated; she experienced bleeding and hemodynamic instability and persistent fever. Four days after initial ICU discharge, she was transferred back to ICU, where she received antimicrobial drug therapy consisting of levofloxacin, amikacin, and meropenem and a transfusion of erythrocytes, plasma, and platelets. On her third day in the ICU, the patient still had pancytopenia and elevated concentrations of aspartate transaminase (674 IU/L) and alanine transaminase (105 IU/L). Blood, liver, and spleen samples were evaluated by PCR for _Mycobacterium_ spp. _Rickettsia_ spp. _Ehrlichia_ spp. and _Anaplasma phagocytophilum_ . _E. chaffensis_ was found in the blood sample, and a morula-like structure was observed in a liver biopsy sample . Treatment with doxycycline (100 mg/12 h) was initiated; 2 days later her fever abated, but hypovolemic shock resulting from hemorrhage necessitated mechanical ventilatory assistance. On day 4 after initiation of doxycline, acute renal failure developed and hemodialysis was begun. On day 10, the patient experienced multisystemic failure with hemodynamic instability; despite inotropic support, she died.\n\n【5】Laboratory studies of a blood sample taken on the second day after the patient’s original admission to hospital revealed no antibodies against hepatitis A, B, C, or E; parvovirus B-19; or HIV. PCR for _Mycobacterium_ spp. was negative. Histopathologic study of the liver showed centrilobular hepatic necrosis, macrovesicular steatosis, and lymphohistiocytic inflammation .\n\n【6】After death, the diagnosis of HME was confirmed by nested PCR amplification of the 16S rRNA gene from spleen and liver tissues, by use of primers previously described . Sequencing of the PCR products showed 99.8% homology with _E. chaffeensis_ str. Arkansas . PCR results for _Anaplasma phagocytophilum_ and _Rickettsia rickettsii_ were negative.\n\n【7】Blood, serum, liver, and spleen samples were transferred to the Rickettsial and Ehrlichial Research Laboratory, University of Texas Medical Branch (Galveston, TX, USA) where 2 fragments of the _dsb_ gene were amplified from blood, liver, and spleen DNA by real-time PCR as previously described . The amplicons showed 100% homology with _E. chaffeensis_ str. Arkansas. ELISA and immunofluorescence assays (IFAs) were also performed. Serum antibodies were detected by ELISA (anti–tandem repeat proteins 120 and 32 of _E. chaffeensis_ ; titer 1:100) and by IFA (IgG; 1:512 titer) .\n\n【8】### Conclusions\n\n【9】Góngora-Biachi et al. previously reported a probable case of HME in Mexico, diagnosed by IFA only . More recently, _E. chaffeensis_ was detected in 5.5% of _Peromyscus_ spp. rodents collected from 31 sites in Mexico . The presence of this pathogen in a wild host is evidence of a tick–vertebrate cycle and represents a potential risk for humans exposed to these tickborne rickettsiae.\n\n【10】The patient we report was hospitalized within 32 days of nonspecific clinical manifestations (leukopenia, anemia, thrombocytopenia, increased serum transaminase concentrations, and hepatosplenomegaly), which have been reported for persistent infection . Some authors have suggested that the clinical triad of leukopenia, thrombocytopenia, and elevated serum transaminase levels in a febrile patient without a rash is common for patients with HME . For this patient, multiorgan dysfunction and hematologic abnormalities persisted despite treatment with doxycycline. Death generally results from complications such as acute respiratory distress syndrome or sepsis with multiorgan failure .\n\n【11】Multiple neurologic manifestations have been reported for patients with ehrlichiosis, including severe headache, confusion, lethargy, hyperreflexia, clonus, photophobia, cranial nerve palsy, seizures, blurred vision, nuchal rigidity, and ataxia . The patient we report experienced changes in mental status, confusion, and a psychotic episode that was not improved by antimicrobial drug therapy.\n\n【12】To our knowledge, fatal cases of HME have not been reported in Mexico; they may have been ignored by clinicians or they may represent true emergence of the disease in Mexico. In a mouse model, a fatal course of infection has been associated with an _Ehrlichia_ strain that induces a toxic shock–like syndrome with high serum levels of tumor necrosis factor α . Fatal infection is often confounded because the signs and symptoms can mimic findings commonly associated with other infections, such as dengue fever, Rocky Mountain spotted fever, murine typhus, or other misdiagnosed febrile diseases  that are common in Mexico.\n\n【13】Illness caused by _Ehrlichia_ spp. results in nonspecific signs and symptoms, and diagnosis requires a high index of suspicion . Seasonality should be taken into account; most new cases in humans occur during the summer, when tick activity is highest. Serologic evidence of infection may be absent during the early acute phase of illness , but use of PCR may help confirm suspected diagnoses. In Mexico, the possibility of _E. chaffeensis_ infection should be investigated for patients with febrile illness, leukopenia, thrombocytopenia, and elevated liver enzymes; early diagnosis and timely treatment may prevent death.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3e27aa78-24a3-418a-85fa-50c2e28d1035", "title": "Lassa Fever in Travelers from West Africa, 1969–2016", "text": "【0】Lassa Fever in Travelers from West Africa, 1969–2016\nOriginally discovered in 1969, Lassa fever is a rodentborne viral hemorrhagic fever endemic to West Africa and caused by Lassa virus . The clinical course of Lassa fever is either not recognized or mild in 80% of patients; however, ≈20% of patients might experience severe disease, including facial swelling, hepatic and renal abnormalities, pulmonary edema, and hemorrhage. Although overall case-fatality rates for patients with Lassa fever is ≈1%, rates among hospitalized case-patients are \\> 15% . Intravenous administration of the antiviral drug ribavirin has become the standard of care for treatment of Lassa fever, but data on the efficacy of intravenous ribavirin are limited. The original study among Lassa fever patients in Sierra Leone found survival to be significantly higher (p = 0.0002) among those who obtained ribavirin within the first 6 days of illness (55%) compared with those who never received the drug (5%) . In the United States, intravenous ribavirin use is still considered investigational and may only be obtained through Emergency Investigational New Drug application to the US Food and Drug Administration .\n\n【1】Diagnosis of Lassa fever in patients arriving from West Africa might be challenging for healthcare providers unfamiliar with the spectrum of its clinical presentation, a challenge that is also common to the consideration of other viral hemorrhagic fevers in returning travelers . Additionally, although Lassa virus is not transmitted through casual contact, contact-tracing investigations of returning case-patients have often been large in scale . To quantify the frequency of case-patients having distinctive clinical features, time from patient presentation to clinical suspicion of a Lassa fever diagnosis, and the risk for secondary Lassa virus transmission, we performed a retrospective review of all 33 reported cases of Lassa fever imported from West Africa during 1969–2016.\n\n【2】### Methods\n\n【3】We searched PubMed for publications using the terms “Lassa” and “Lassa fever.” We identified additional articles by reviewing references in retrieved reports  and official correspondence by public health officials involved in these cases. We selected 74 publications discussing clinical or epidemiologic aspects of the 33 imported Lassa fever cases for review and collected information pertaining to case demographics, distinctive clinical features suggestive of Lassa fever, time from patient seeking care to clinical suspicion of Lassa fever, and number of contacts traced. We defined distinctive clinical features as fever and \\> 1 of the following: sore throat or pharyngitis, retrosternal chest pain, or proteinuria. We selected these features on the basis of the cumulative positive predictive value for fever, sore throat, retrosternal chest pain, and proteinuria for Lassa fever of 0.81 in a case–control study among 441 hospitalized patients in Sierra Leone . Although precise definitions varied between investigations, high-risk contacts were typically defined as contacts with substantial direct contact with patients or their body fluids.\n\n【4】### Findings\n\n【5】During 1969–2016, a total of 33 patients traveling from 7 West Africa countries to 9 other countries were diagnosed with Lassa fever . The median age of these patients was 45 years (range 18–72 years). Potential sources of Lassa fever exposures varied. Eleven patients were healthcare workers working in West Africa with either known or suspected exposures to Lassa fever patients; 4 patients had known exposure to rodents or history of travel to rural areas in West Africa. The only known risk factor for 18 patients was living in or traveling to West Africa. Twenty patients had illness onset during the West Africa dry season (November–April), and 10 patients had onset during the wet season (May–October); time of year for disease onset was not specified for 3 patients.\n\n【6】Twenty patients traveled to their destination on a commercial airliner; of these, 12 were symptomatic during flight. Ten patients were medically evacuated, 6 of whom had a known or suspected exposure to Lassa fever at the time of evacuation. Information on method of travel was not available for 3 patients. At the time patients sought care, medical providers were aware of travel history to West Africa for 26 (87%) of 30 patients; ascertainment of travel histories by medical providers was not described for 3 cases.\n\n【7】Of the 29 patients for whom clinical information was available , 17 (59%) had fever and \\> 1 distinctive clinical features of Lassa fever. Time from patients seeking medical care to clinical suspicion of Lassa fever by clinical providers in their destination country ranged from 1 to 22 days (median 5 days). The time from when patients sought care to patient isolation ranged from 1 to 25 days (median 7 days). We found no reports of Lassa virus PCR testing performed on any patient before 2000; however, 9 of 16 patients (56%) in 2000 and later years had a positive Lassa virus PCR test within 1–2 days of hospital admission. Of the 32 patients for whom information on isolation procedures were described, 24 patients were isolated at some point during their hospitalizations in their destination countries. Of these, 11 (34%) patients were placed in a form of isolation immediately after they sought medical care; 3 patients were transferred to biocontainment units, and the remaining 8 patients were isolated with techniques ranging from standard precautions to a combination of contact, droplet, and airborne precautions. Of the 13 patients who were isolated later in their hospital stay, 2 patients were isolated with contact and airborne precautions, and 11 were subsequently transferred to specialized hospitals with infection control capacity designed for the care of patients with highly infectious diseases. Time to isolation ranged from 3 to 15 days after hospital admission . The last 2 patients who sought care in the United States were admitted to dedicated Ebola treatment units established during the 2014–2015 West Africa Ebola epidemic. Of the 31 patients for whom outcomes were described, 12 patients died, yielding a case-fatality rate of 39%.\n\n【8】Treatment regimens were described for 23 patients. Twelve (52%) patients initially received antimalarial medications or antimicrobial drugs because of clinical suspicion of malaria or another infectious disease during their treatment course. In total, intravenous ribavirin was ordered for 7 (30%) patients. Four patients received intravenous ribavirin; 2 received a full course, and the other 2 died during treatment. Three patients had intravenous ribavirin ordered but died before receiving the medication.\n\n【9】Contact tracing investigations were either not performed or not described in the literature for 16 (48%) patients. For the remaining 17 (52%) patients, a total of 3,420 contacts were followed; the number of contacts followed per investigation ranged from 3 to 552 (median 173). Eleven contact investigations stratified contacts into high-risk and low-risk contacts, with some further separating high-risk contacts into first-line or second-line contacts . High-risk contacts were defined as having substantial exposure to patients or their body fluids, such as through direct unprotected exposure to blood or other body fluids from a case-patient. By these criteria, 139 total contacts were defined as being high-risk across 11 investigations. In 9 investigations, high-risk contacts accounted for 2%–8% of total contacts; in 2 investigations, they accounted for 40%–60% of total contacts.\n\n【10】Only 2 cases of secondary transmission of Lassa virus occurred, both in Germany. Neither of the source case-patients for these 2 patients was isolated. The first instance of transmission occurred to a physician who performed a physical examination, obtained intravenous access, and obtained blood samples from a Lassa fever patient without wearing any personal protective equipment . Because of the physician’s high-risk exposure, ribavirin prophylaxis was initiated and completed. Serologic testing was performed and yielded IgG titers of 1:320 specific to the strain of Lassa virus from the case-patient, indicating probable seroconversion in the physician. However, the physician remained asymptomatic.\n\n【11】The second instance of secondary transmission, reported in 2016, occurred in a mortician who handled the body of a healthcare worker who was evacuated from Togo to Germany and diagnosed with Lassa fever retrospectively. The mortician reported wearing 2 pairs of gloves when handling the corpse but did not wear an apron or a facial mask. The mortician reported mild upper respiratory tract symptoms before contact with the deceased patient. However, 4 days after handling the corpse, his symptoms worsened. Six days after handling the corpse, the mortician tested positive for Lassa virus by real-time reverse transcription PCR. The mortician’s clinical course was notable for fever, upper respiratory tract symptoms, and pharyngeal erythema with exudates, myalgias, and arthralgias. He received intravenous ribavirin for 10 days and oral favipiravir for 4 days, with gradual resolution of his symptoms and clinical recovery . Contacts of this secondary case-patient were followed but did not indicate any evidence of further transmission.\n\n【12】### Discussion\n\n【13】The 33 cases of imported Lassa fever that occurred during 1969–2016 posed a similar set of challenges: timely diagnosis of a rare infectious disease not endemic to the patient’s destination country, timely treatment, and prevention of Lassa virus transmission to contacts. Among patients who were not medically evacuated, the median number of days from patient presentation to clinical suspicion of Lassa fever by clinicians in the destination country was 5 days. Several factors might have contributed to this delay in diagnosis. First, patients were seen by providers in countries where Lassa fever is not endemic, requiring consideration of a travel-associated illness infrequently encountered outside of West Africa. Second, in many cases, the patients’ travel to West Africa was not known at the time they initially sought care. Third, the clinical findings of Lassa fever are variable, ranging from nonspecific symptoms, such as fever, nausea, and myalgias in the early phase, to more distinctive features later, including pharyngitis, sore throat, tonsillitis, oropharyngeal ulcers, facial and neck swelling, conjunctival injection, and proteinuria. Hemorrhage is usually seen only in a minority of cases. Although fever and \\> 1 distinctive clinical features can be suggestive of the diagnosis, they were only present in 59% of patients. In addition, of patients with a known travel history to West Africa, 12 (48%) did not demonstrate distinctive clinical features of Lassa fever. As such, providers encountering patients who have a nonspecific febrile illness after travel to West Africa should elicit a travel history and consider Lassa fever early in the differential diagnosis. Suspicion should be especially high for those patients with fever and \\> 1 of the distinctive features we have described. Although most returning travelers from West Africa with Lassa fever in 2000 or later had viremia confirmed through a positive Lassa virus test obtained within 1–2 days of admission, some patients did not have their illness diagnosed until weeks into their illness. Samples of patients with suspected Lassa fever should be obtained as early as possible and tested by Lassa virus PCR at a reference laboratory; most reference laboratories in Europe and elsewhere have demonstrated proficiency in performing Lassa virus molecular diagnostics .\n\n【14】Treatment of Lassa fever comprises effective supportive care and use of intravenous ribavirin. Although timely treatment with intravenous ribavirin depends on successful procurement of the drug, it also rests on early consideration of the diagnosis, and might even be administered before laboratory confirmation of Lassa fever diagnosis in patients with severe illness. The relative minority of case-patients who received intravenous ribavirin in our review highlights the importance of early consideration of Lassa fever in the differential diagnosis for appropriate patients.\n\n【15】Infection control was another challenge encountered by medical providers and healthcare systems caring for Lassa fever patients. The lack of appropriate use of isolation or barrier precautions in the 2 instances of secondary transmission speaks to the importance of adhering to standard precautions when caring for all patients, regardless of their diagnosis or presumed infectious status. In addition, the case of secondary transmission to the mortician in Germany illustrates the importance of maintenance of standard precautions during autopsy. Early consideration of Lassa fever as a diagnosis might also enable early institution of isolation and prevention of secondary transmission. Among those case-patients for whom a specific form of isolation was specified, most were admitted to high-security containment facilities or negative-pressure rooms with airborne precautions. Although these forms of isolation can prevent secondary transmission of Lassa virus, simple barrier or contact precautions have also been demonstrated to be safe and are less expensive and labor-intensive .\n\n【16】Contact tracing investigations frequently involved hundreds of contacts and a substantial investment of time and labor on the part of public health teams. One investigation noted that “active surveillance of contacts by public health teams was impracticable and required enormous resources, involving over 3,000 communications” . Most investigations were similarly comprehensive, involving identification and longitudinal follow-up of case-patients’ friends, family, and casual contacts, including airplane passengers, as well as numerous healthcare staff. Contacts were often separated into 2 categories: high-risk (i.e. having substantial exposure to case-patients) and low-risk (i.e. having only casual contact or proximity to case-patients). However, body temperature monitoring, home visits, and serologic testing were frequently coordinated for contacts in both high- and low-risk categories. To minimize the burden on public health systems and maximize the likelihood of successful secondary case identification, future responses should consider focusing on investigating high-risk contacts exclusively.\n\n【17】Our review had several limitations. Information on historic cases, particularly those before 1985, was incomplete and limited. In some cases, reports provided scant or no information on the physical examination or laboratory studies of patients upon admission. Reports on contact tracing provided different degrees of detail, and levels of risk assessment were variable between investigations.\n\n【18】With the ease and frequency of international travel, Lassa fever will continue to be encountered by healthcare providers in countries where Lassa fever is not endemic. Strict maintenance of standard infection control precautions in healthcare is critical for all patients and will help prevent secondary transmission of Lassa virus. Timely recognition of distinctive clinical features, earlier treatment of patients, and targeted public health responses focused on high-risk contacts will also be important components of future responses to imported cases of Lassa fever.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "305dfd6f-471d-4d7f-be2a-6100659b58e8", "title": "Tuberculosis-Associated Hospitalizations and Deaths after COVID-19 Shelter-In-Place, San Francisco, California, USA", "text": "【0】Tuberculosis-Associated Hospitalizations and Deaths after COVID-19 Shelter-In-Place, San Francisco, California, USA\nSince the emergence of a novel coronavirus, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which causes coronavirus disease (COVID-19), unprecedented measures have been recommended to reduce transmission. In San Francisco, California, USA, progressively restrictive health officer orders implemented since early 2020 have included travel quarantines, shelter-in-place (SIP), deferral of routine medical appointments and elective surgeries, closure of public-facing events and businesses, and isolation and quarantine when appropriate . Nationwide, disruptions in medical services have contributed to delaying or avoiding routine care and a decrease in non–COVID-19-related hospital admissions and emergency department visits . Similarly, worldwide tuberculosis (TB) case reports have declined, including in San Francisco, where a ≈60% decrease in newly diagnosed TB cases compared with prior years was observed in the first 4 months of the pandemic .\n\n【1】The San Francisco Department of Public Health (SFDPH) Tuberculosis Prevention and Control Program manages all cases of active TB in San Francisco residents (≈881,549 population). In 2019, San Francisco had a high incidence of TB, with rates >4-fold higher (11.9 cases/100,000 persons) than the national rate. The affected population is predominantly non–US-born (86%) with >80% residing in the United States >5 years . We reviewed overall numbers of active TB case-patients in San Francisco and newly diagnosed cases including those resulting in hospitalization, intensive care unit admission, and death. We divided our analysis into 2 periods: pre-SIP (January 1, 2019–March 15, 2020) and during SIP (March 16, 2020–January 31, 2021). TB was reportable within 1 working day of diagnosis. Cases were diagnosed by microbiologic testing or medical assessment for consistent clinical and radiographic findings. All patients who received a TB diagnosis after SIP began were tested for SARS-CoV-2 co-infection at the time of TB diagnosis, except for 7 patients during March–May 2020, when testing was less available. For all fatalities, we used a standardized algorithm to review medical records and death certificates to determine whether cause of death was TB-related. Because these activities were public health surveillance and not research, review by institutional review board was not requested.\n\n【2】During the 14.5-month pre-SIP period, the monthly average number of patients receiving TB treatment was 73.0 persons, compared with 42.7 persons during the 10.5-month SIP period, resulting in a 42% reduction. The initial SIP period was marked by low numbers of new TB diagnoses during mid-March through June; increasing numbers starting in July, when more case-patients had TB diagnosed while they were hospitalized or dying from TB . Pre-SIP, a total of 114 patients (average 7.9 patients/month) were newly diagnosed with TB. A total of 38 (33.3%) patients were hospitalized, including 5 (4.4%) who required intensive care. A total of 4 (3.5%) patients died with cause of death assessed as TB-related. In comparison, after SIP began, 52 patients (average 5.0 patients/month) were newly diagnosed with TB. A total of 33 (63.5%) patients were hospitalized, including 12 (23.1%) patients who required intensive care; 7 (13.5%) patients died with cause of death assessed as TB-related. No patients diagnosed with TB during SIP reported having previous SARS-CoV-2 infection; all patients screened for SARS-CoV-2 had negative results. One patient experienced new-onset low-grade fever and cough 37 days after starting TB treatment and subsequently tested SARS-CoV-2 positive; this patient had no new radiographic abnormalities or COVID-19–related complications. More patients during SIP than before SIP required hospitalization, received intensive care, or had a TB-related death (p<0.05 by Pearson χ 2  test . We found no difference in duration of TB symptoms pre-SIP (median 1 month, range 0–120 months) than that during SIP (median 1.5 months, range 0–24 months).\n\n【3】Our preliminary findings suggest that delayed TB diagnosis early in the pandemic, coinciding with implementation of SIP and other restrictive measures, might have contributed to an increasing proportion of patients who later experienced severe illness or death. Although we used SIP as a proxy, other factors probably contributed to the trend. The same racial, ethnic, and socioeconomic inequities that contributed to limited healthcare access during the COVID-19 pandemic are prevalent in TB-infected populations . Symptomatic patients might have been reluctant or unable to seek medical evaluation, thereby leading to worsening TB disease. The overlap of signs, symptoms, and abnormal radiographic findings for COVID-19 and TB could have resulted in prioritizing COVID-19 screening over TB diagnosis.\n\n【4】Our observations are a snapshot in time and are not representative of TB activity in other cities or regions where COVID-19 transmission rates and corresponding SIP and public health responses differ. Nevertheless, we collected real-world data consistent with the Stop-TB Partnership modeling studies predicting that prolonged disruption of TB activities could result in an excess of millions of TB cases and deaths through 2025 . As vaccination rates increase and restrictions ease, continued vigilance and public messaging about the importance of early diagnosis of TB in high-risk populations remain critical.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "396625ad-cb42-4cb8-99ce-063d2cffb8d8", "title": "Novel Paramyxoviruses in Bats from Sub-Saharan Africa, 2007–2012", "text": "【0】Novel Paramyxoviruses in Bats from Sub-Saharan Africa, 2007–2012\nMembers of the _Paramyxoviridae_ family are enveloped negative-sense RNA viruses, further classified into either the _Pneumovirinae_ or _Paramyxovirinae_ subfamily  _._ The _Paramyxovirinae_ subfamily has increasingly been associated with bat species across the globe. The _Henipavirus_ genus is 1 of 7 genera in this subfamily and contains the first recorded zoonotic paramyxoviruses, Hendra virus and Nipah virus. These 2 viruses are associated with severe respiratory and neurologic syndromes, and regular spillover from _Pteropus_ spp. bats causes infections in humans and domestic animals .\n\n【1】Enhanced surveillance for bat-associated pathogens has led to the discovery of numerous novel paramyxoviruses . _Henipavirus_ \\-related viruses were identified in another pteropodid species, _Eidolon helvum_ , sampled in Ghana, West Africa. This finding suggests an extension of the geographic and host ranges of the members of this virus genus  _._ Subsequent studies demonstrated a high diversity of paramyxoviruses in _E. helvum_ bat population in Africa, as well as in other bat species from different continents. This finding suggests that bats may have a global role as potential paramyxovirus reservoirs . To contribute toward the knowledge of bat-associated paramyxovirus diversity and distribution, we sampled multiple bat species from several sub-Saharan African countries.\n\n【2】### The Study\n\n【3】During 2007–2012, we sampled 1,220 bats representing at least 48 species from multiple locations in selected countries in Africa . Bats were anesthetized with the use of ketamine (0.05–0.1 mg/g body mass) and exsanguinated by cardiac puncture. Voucher specimens were identified through morphologic characterization  or, alternatively, through genetic barcoding. Approximately 30–100 mg of renal tissue was used for RNA extraction. A heminested primer set targeting the conserved polymerase (large) gene of _Respirovirus, Morbillivirus,_ and _Henipavirus_ was used for sample screening through reverse transcription PCR . A total of 103 samples (8.4%) tested positive, and the obtained amplicons of ≈490 bp were sequenced . For phylogenetic analysis, representative paramyxovirus sequences available from GenBank were included , and Bayesian analysis was performed by using BEAST version 1.7.4 software  .\n\n【4】Several samples from bat species not previously implicated as paramyxovirus reservoirs tested positive in our study. Some of these implicated species are known to roost in peridomestic environments. Sequence analysis of paramyxovirus sequences showed a clear bifurcation of the phylogenetic tree, segregating paramyxoviruses detected in pteropodid bats (Pteropodidae) from paramyxoviruses detected in bats of other families . The former contained henipaviruses and related viruses. Two viral sequences detected in _Rousettus aegyptiacus_ bats grouped within this cluster as part of a sister clade to the henipaviruses. The second cluster contained sequences derived from nonpteropodid bats. Some of these sequences grouped with the sequences from the _Morbillivirus_ and proposed _Jeilongvirus_ genera, whereas others could not be included in any of the other paramyxovirus genera.\n\n【5】We observed a strong association of several viral lineages to particular bat genera for paramyxoviruses identified in _Hipposideros_ , _Miniopterus_ , _Coleura_ , _Myotis,_ and _Pipistrellus_ bats, although the bats were sampled from geographically distant locations. In contrast to the sequences of European and South American origin, for which geographic clustering was observed, no such clustering was found among the sequences from African bats.\n\n【6】The incidence and diversity of viral sequences varied according to bat species. For example, nearly identical sequences were detected in 50% of _Pipistrellus_ spp. sampled from a single colony in the Democratic Republic of the Congo (n = 40). In other cases, several distinct viral sequences were detected in different individual bats of 1 species, such as _Miniopterus minor_ bats sampled from a single colony in Kenya (n = 53), which harbored 6 distinct viral sequences. Some of the sequences were found more frequently than others. In contrast to a previous study which did not identify paramyxoviruses in _Coleura afra_ bats sampled in Ghana (n = 71) , we detected a substantial paramyxovirus incidence (37%, n = 27) in the same bat species sampled in Kenya .\n\n【7】### Conclusions\n\n【8】The henipaviruses were the first bat paramyxoviruses directly linked to human disease; however, most aspects of pathogenicity and the host ranges of the increasingly detected novel bat paramyxoviruses remain to be investigated. Here we report information regarding paramyxovirus distribution through molecular evidence of bat-associated paramyxoviruses in Cameroon, Nigeria, and South Africa, as well as evidence of paramyxoviruses in nonpteropodid bats from the Democratic Republic of the Congo. Our results suggest that 2 separate lineages were established during the evolution of bat-associated paramyxoviruses: the pteropodid bats potentially harbor 1 lineage, and the nonpteropodid bats potentially harbor the other. In contrast to the proposed chiropteran classification, which supports a sister-taxon relationship between Rhinolophoidae and Pteropodidae on the suborder level, paramyxovirus divergence appears to correlate with traditional bat taxonomy. The evolution behind this divergence might be a result of multiple evolutionary origins or a single origin with subsequent divergence. As with the evolution of echolocation, this question remains to be answered . More extensive bat sampling and molecular dating of the paramyxovirus phylogeny may help resolve this question.\n\n【9】Intensified anthropogenic transformations have facilitated closer contact between humans, domestic animal populations, and wildlife. Our study demonstrates that some bat species, adapted to peridomestic roosting, can have a substantial incidence of diverse paramyxoviruses. The variation in incidence and viral diversity observed in several bat species may suggest that some species are the true reservoirs, whereas others are mere incidental hosts. Given the observed virus diversity, implications for public health and veterinary medicine should be taken into account, especially considering the known likelihood of direct bat-to-human and human-to-human transmission of Nipah virus . Enhanced surveillance in bats and other animals will be useful for detecting possible spillover events and host shifts. Clearly, systematic longitudinal studies are needed to elucidate critical factors of paramyxovirus circulation within bat communities , and further research is needed to clarify the pathobiology, tissue tropism, and excretion pathways of these novel paramyxoviruses because these factors can be directly related to their zoonotic potential.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "52c1d363-5bd7-4889-ad86-e25d2676cf9d", "title": "Francisella tularensis Peritonitis in Stomach Cancer Patient", "text": "【0】Francisella tularensis Peritonitis in Stomach Cancer Patient\n### Case Study\n\n【1】A 50-year-old man arrived at the emergency department in September 2003 with a 2-day history of high fever (temperature up to 40.8°C), rigors, dry cough, nausea, vomiting, lower abdominal pain, and melena. The patient had recently been diagnosed with signet-ring–cell carcinoma of the stomach with evidence of metastasis to the lung and peritoneum and multiple thoracic and abdominal lymph nodes. Chemotherapy had been planned to start soon.\n\n【2】Physical examination showed fever (temperature 39.7°C), hypotension (96/51 mm Hg), a systolic heart murmur with regular rhythm, and lower abdominal tenderness and rebound. Laboratory examination showed microcytic anemia (hemoglobin 87 g/L), relative neutrophilia (82% of 7.8 x 10 9  /L total leukocytes), and relative and absolute lymphopenia (7% of leukocytes or 0.55 x 10 9  /L). A chest x-ray was normal, as were liver function tests and pancreatic enzymes. A presumptive diagnosis of sepsis with peritonitis was made, and blood and urine were collected for cultures. Empiric cefepime (2 g every 8 h) and tobramycin (one dose 500 mg) therapy was started before hospital admission.\n\n【3】The following day, an esophagogastroduodenoscopy showed cancer ulceration as the source of melena. An echocardiography excluded endocarditis. An abdominal sonogram showed small pockets of ascites in the abdomen and pelvis, and the fluid showed many neutrophils, lymphocytes, and macrophages, consistent with peritonitis. The ascites (5 mL) was also cultured. Despite cefepime treatment, the patient’s fever persisted for 36 hours, which prompted a change to imipenem (500 mg every 6 h) and vancomycin (1 g every 12 h). The fever subsided in 1 day, as did the abdominal manifestations. The patient was discharged the following day with further oral gatifloxacin (400 mg four times a day) and amoxicillin/clavulanate (875 mg twice a day) for 10 days.\n\n【4】Anticancer therapy that consisted of radiation to the stomach and daily capecitabine and weekly paclitaxel was begun 5 days after discharge. Two weeks later, at completion of these treatments and the oral antimicrobial drugs, the abdominal lymphadenopathy showed improvement on computed tomography. However, the tumor itself, as well as the lung nodules, remained stable. Additional chemotherapy with three cycles of paclitaxel and carboplatin was started soon afterwards.\n\n【5】Meanwhile, the ascites culture (Bactec Aerobic/F bottle with resins) became positive after 8 days of incubation, and a small gram-negative coccobacillus (strain MDA3270) was isolated. Its fastidious growth and unusual Gram stain features prompted sequencing analysis of the 16S rDNA for identification . A 586-base pair DNA fragment, amplified by polymerase chain reaction, demonstrated 100% sequence homology with _Francisella tularensis_ subsp _. holarctica_  . On review, the culture and stain features fit _F. tularensis_ . The subspecies was confirmed by the Centers for Disease Control and Prevention (CDC) (Fort Collins, CO). The blood culture remained negative after 7 days of incubation.\n\n【6】The diagnosis of typhoidal tularemia (24 days after onset) led the patient to be further treated with intravenous gentamicin for 2 weeks (120 mg every 8 h), followed by 2 weeks of oral ciprofloxacin (750 mg twice a day). A query of exposure history was also made. The patient was a farmer from northeastern Mississippi and had cut hay in a field infested with rodents 3 weeks before onset. He had traveled from home to Houston for the cancer care. The patient had no history of camping, hunting, or bites by ticks or deerflies. After 6 weeks of anticancer therapy (7 weeks after tularemia), the patient’s carcinoembryonic antigen decreased substantially. However, a predominant 6-cm mass in the gastrohepatic ligament region persisted, which raised the question of infection versus cancer. Thus, a percutaneous needle biopsy was performed, and cancerous mucin was demonstrated. Further chemotherapy continued.\n\n【7】A convalescent antibody against _F. tularensis_ was detected 4 weeks after onset (titer 1:40, direct agglutination method); however, it disappeared at 3 months after chemotherapy-associated lymphopenia . Before chemotherapy started, the lymphocyte counts had been 1.0 x 10 9  /L before infection, 0.55–0.40 x 10 9  /L in early infection, and 1.94 x 10 9  /L 8 days postonset. After chemotherapy, however, the counts dropped sharply to 0.07 x 10 9  /L (96% reduction) in 2 weeks. During the remaining weeks of therapy, lymphopenia persisted despite improvement. In contrast, the patient’s neutrophil counts were normal to slightly elevated during the entire course.\n\n【8】### Conclusions\n\n【9】The interest in tularemia and its pathogen, _F. tularensis_ , is renewed due to the high bioterrorism potential of the organism, i.e. listed as a category A by CDC . _F. tularensis_ , a gram-negative coccobacillus, survives well in the environment and is facultative in infected host cells (macrophages). It has a high infectivity rate and is zoonotic. Human infection occurs mainly in animal handlers and those who are bitten by ticks, deerflies, or both. Airborne and waterborne outbreaks have also been reported . From 1990 to 2001, a total of 1,497 tularemia cases was reported to CDC (mean 125 cases per year), with 55% occurring in the states of Arkansas, Missouri, South Dakota, and Oklahoma . During those 12 years, however, Mississippi, the home state of our patient, had only one case. In view of the highest disease activity in neighboring Arkansas (324 cases), underreporting of the disease in Mississippi was a possibility in addition to other explanations, such as geographic differences and barriers (Mississippi River).\n\n【10】Tularemia manifests a few clinical forms and, before the antimicrobial era, carried a high fatality rate. The diagnosis of tularemia is often difficult to make, especially for the typhoidal and pneumonic forms. Most cases are diagnosed by serologic tests late in infection or afterwards. In an epidemiologic study of >1,000 cases , only 11% were diagnosed by isolation of _F. tularensis_ from a body source, such as ulcer fluid, blood, lymph node aspirate, and pleural fluid. With improved blood culture methods in the past 2 decades, however, cases of _F. tularensis_ bacteremia have been reported . These blood cultures became positive after an incubation period of 3 days to 3 weeks (median 7 days). Most cases were in patients with underlying conditions or diseases, such as old age, alcohol abuse, diabetes mellitus, transplantation, or AIDS. The immunocompromised patients tend to have prolonged infection or die. In a syngeneic bone marrow transplant patient , the infection presented as a 3-cm solitary pulmonary nodule, and after 6 weeks of antimicrobial treatment, the culture-positive nodule vanished.\n\n【11】Tularemia with associated peritonitis is extraordinary rare. Our patient’s peritonitis was likely related to metastatic stomach cancer that had breached the integrity of peritoneum and regional blood vessels and lymph nodes, leading to peritoneal spill of the organism (free or intramacrophage ones). The ascites did contain many macrophages. To combat the infection, neutrophilia developed. Because the patient was severely anemic, absolute lymphopenia developed from normal baseline . Lymphopenia is generally absent in tularemia, and this patient’s response was likely a compromise for neutrophilia. However, the lymphocyte count rebounded a few days later. The patient’s response to cefepime therapy was suboptimal in view of the persistence of fever and concurrent isolation of the organism. Streptomycin or gentamicin, not a cephalosporin, is recommended to treat tularemia. Successful treatment with a fluoroquinolone has also been reported in at least 10 recent cases . The source of infection could not be determined definitively; however, living and working in a farm and the history of exposure to rodent-infested hay were probably important. Recently, landscaping occupation, such as lawn mowing and weed-whacking, is recognized as a risk for exposure .\n\n【12】The antibody response against tularemia is usually strong, peaking at 2–3 months after onset . In an outbreak caused by _F. tularensis_ subsp. _holarctica_ , the peak titer reached 1:256 to 1:8,192 (median 1:1,024) . In our patient, urgent initiation of the lymphotoxic anticancer chemotherapy blunted the initial response by ablating the antibody-producing lymphocytes. One of the agents, capecitabine, causes lymphopenia in >90% of patients following treatment . During the 11-week chemotherapy, existing antibodies (titer 1:40) were degraded in the circulation (3–4 half-lives) and became undetectable. Therefore, this case illustrates that, after anticancer chemotherapy, lack of antibody does not exclude an infection that usually elicits antibody response.\n\n【13】_F. tularensis_ has four subspecies (biovars): _tularensis_ , _holarctica_ , _novicida_ , and _mediasiatica_ , and the first two subspecies are the main causes of tularemia in the United States. _F. tularensis_ subsp. _holarctica_ , also known as type B or biovar _palaearctica_ , is generally less virulent than _F. tularensis_ subsp _. tularensis_ (type A). Both typhoidal and cutaneous forms have been reported for _F. tularensis_ subsp. _holarctica_ . For the typhoidal cases, including ours, all nine reported patients recovered, and the median incubation of blood cultures was 9 days (4 days–3 weeks) , similar to the 8 days in our ascites culture.\n\n【14】Identifying _F. tularensis_ may be difficult because of its rarity and fastidious growth, especially in areas where disease is nonendemic. Our laboratory has been using the 16S rDNA sequencing method to identify mycobacteria and other fastidious organisms. The method is considered to be the single best method to identify bacteria and will likely impact patient care in addition to microbiologic research.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "f7c513f6-8dc6-4962-a14d-c652cc9bb11f", "title": "Reflections on the 1976 Swine Flu Vaccination Program", "text": "【0】Reflections on the 1976 Swine Flu Vaccination Program\n\"Flu to the Starboard! Man the Harpoons! Fill with Vaccine! Get the Captain! Hurry!\"\n\n【1】**Edwin D. Kilbourne, New York Times, February 13, 1976** \n\n【2】\"Grounding a Pandemic\"\n\n【3】**Barack Obama and Richard Lugar, New York Times, June 6, 2005** \n\n【4】\"It has been 37 years since the last influenza pandemic, or widespread global epidemic, so by historic patterns we may be due for another.\"\n\n【5】**New York Times, July 17, 2005** \n\n【6】Kilbourne in 1976  noted that pandemics of influenza occur every 11 years. Since the latest prediction in the New York Times  suggests that after 39 years we may be overdue for a pandemic, and since 2 US senators have recently headlined the possibility , that observation may become a political fact. Whether it becomes a scientific fact and a policy fact is yet to be seen. Some reflections on 1976 from 2 insiders' viewpoints may identify some of the pitfalls that public health policymakers will face in addressing potential influenza pandemics.\n\n【7】### Swine Flu at Fort Dix\n\n【8】On February 3, 1976, the New Jersey State Health Department sent the Center for Disease Control (CDC) in Atlanta isolates of virus from recruits at Fort Dix, New Jersey, who had influenzalike illnesses. Most of the isolates were identified as A/Victoria/75 (H3N2), the contemporary epidemic strain. Two of the isolates, however, were not typeable in that laboratory. On February 10, additional isolates were sent and identified in CDC laboratories as A/New Jersey/76 (Hsw1N1), similar to the virus of the 1918 pandemic and better known as \"swine flu.\"\n\n【9】A meeting of representatives of the military, the National Institute of Health, the Food and Drug Administration (FDA), and the State of New Jersey Department of Health was quickly convened on Saturday, February 14, 1976. Plans of action included heightened surveillance in and around Fort Dix, investigation of the ill recruits to determine if contact with pigs had occurred, and serologic testing of recruits to determine if spread had occurred at Fort Dix.\n\n【10】Surveillance activities at Fort Dix gave no indication that recruits had contact with pigs. Surveillance in the surrounding communities found influenza caused by the current strain of influenza, A/Victoria, but no additional cases of swine flu. Serologic testing at Fort Dix indicated that person-to-person transmission had occurred in >200 recruits .\n\n【11】In 1974 and 1975, 2 instances of humans infected with swine influenza viruses had been documented in the United States. Both persons involved had close contact with pigs, and no evidence for spread of the virus beyond family members with pig contact could be found .\n\n【12】### The National Influenza Immunization Program\n\n【13】On March 10, 1976, the Advisory Committee on Immunization Practices of the United States Public Health Service (ACIP) reviewed the findings. The committee concluded that with a new strain (the H1N1 New Jersey strain) that could be transmitted from person to person, a pandemic was a possibility. Specifically, the following facts were of concern: 1) persons <50 years of age had no antibodies to this new strain; 2) a current interpandemic strain (A/Victoria) of influenza was widely circulating; 3) this early detection of an outbreak caused by A/New Jersey/76/Hsw1N1 (H1N1) provided an opportunity to produce a vaccine since there was sufficient time between the initial isolates and the advent of an expected influenza season to produce vaccine. In the past when a new pandemic strain had been identified, there had not been enough time to manufacture vaccine on any large scale; 4) influenza vaccines had been used for years with demonstrated safety and efficacy when the currently circulating vaccine strain was incorporated; 5) the military vaccine formulation for years had included H1N1, an indication that production was possible, and no documented adverse effects had been described.\n\n【14】ACIP recommended that an immunization program be launched to prevent the effects of a possible pandemic. One ACIP member summarized the consensus by stating \"If we believe in prevention, we have no alternative but to offer and urge the immunization of the population.\" One ACIP member expressed the view that the vaccine should be stockpiled, not given.\n\n【15】Making this decision carried an unusual urgency. The pharmaceutical industry had just finished manufacture of the vaccine to be used in the 1976–1977 influenza season. At that time, influenza vaccine was produced in fertilized hen's eggs from special flocks of hens. Roosters used for fertilizing the hens were still available; if they were slaughtered, as was customary, the industry could not resume production for several months.\n\n【16】On March 13, an action memo was presented to the Secretary of the Department of Health Education and Welfare (DHEW). It outlined the problem and presented 4 alternative courses of action. First was \"business as usual,\" with the marketplace prevailing and the assumption that a pandemic might not occur. The second was a recommendation that the federal government embark on a major program to immunize a highly susceptible population. As a reason to adopt this plan of action, the memo stated that \"the Administration can tolerate unnecessary health expenditures better than unnecessary death and illness if a pandemic should occur.\" The third proposed course of action was a minimal response, in which the federal government would contract for sufficient vaccine to provide for traditional federal beneficiaries—military personnel, Native Americans, and Medicare-eligible persons. The fourth alternative was a program that would represent an exclusively federal response without involvement of the states.\n\n【17】The proposal recommended by the director of CDC was the second course, namely, for the federal government to contract with private pharmaceutical companies to produce sufficient vaccine to permit the entire population to be immunized against H1N1. The federal government would make grants to state health departments to organize and conduct immunization programs. The federal government would provide vaccine to state health departments and private medical practices. Since influenza caused by A/Victoria was active worldwide, industry was asked to incorporate the swine flu into an A/Victoria product to be used for populations at high risk.\n\n【18】Before the discussions with the secretary of DHEW had been completed, a member of his staff sent a memo to a health policy advisor in the White House, raising the specter of the 1918 pandemic, which had been specifically underemphasized in the CDC presentation. CDC's presentation highlighted the pandemic potential, comparing it with the 1968–69 Hong Kong and 1957–58 Asian pandemics. President Gerald Ford's staff recommended that the president convene a large group of well-known and respected scientists (Albert Sabin and Jonas Salk had to be included) and public representatives to hear the government's proposal and make recommendations to the president about it. After the meeting, the president had a press conference, highlighted by the unique simultaneous appearance of Salk and Sabin. President Ford announced that he accepted the recommendations that CDC had originally made to the secretary of DHEW. The National Influenza Immunization Program (NIIP) was initiated.\n\n【19】The proposal was presented to 4 committees of the Congress, House and Senate authorization committees and House and Senate appropriation committees. All 4 committees reported out favorable legislation, and an appropriation bill was passed and signed.\n\n【20】The estimated budgeted cost of the program was $137 million. When Congress passed the appropriation, newspapers mischaracterized the cost as \"$1.9 billion\" because the $137 million was included as part of a $1.9 billion supplemental appropriation for the Department of Labor. In the minds of the public, this misconception prevailed.\n\n【21】Immediately after the congressional hearing, a meeting of all directors of state health departments and medical societies was held at CDC. The program was presented by CDC, and attendees were asked for comments. A representative from the New Jersey state health department opposed the plan; the Wisconsin state medical society opposed any federal involvement. Otherwise, state and local health departments approved the plan.\n\n【22】Within CDC, a unit charged with implementing the program, which reported to the director, was established. This unit, NIIP, had complete authority to draw upon any resources at CDC needed. NIIP was responsible for relations with state and local health departments (including administration of the grant program for state operations, technical advice to the procurement staff for vaccine, and warehousing and distribution of the vaccine to state health departments) and established a proactive system of surveillance for possible adverse effects of the influenza vaccines, the NIIP Surveillance Assessment Center (NIIP-SAC). (This innovative surveillance system would prove to be NIIP's Trojan horse.) In spite of the obstacles discussed below, NIIP administered a program that immunized 45 million in 10 weeks, which resulted in doubling the level of immunization for persons deemed to be at high risk, rapidly identifying adverse effects, and developing and administering an informed consent form for use in a community-based program.\n\n【23】### Obstacles to the Vaccination Plan\n\n【24】The principal obstacle was the lack of vaccines. As test batches were prepared, the largest ever field trials of influenza vaccines ensued. The vaccines appeared efficacious and safe (although in the initial trials, children did not respond immunologically to a single dose of vaccine, and a second trial with a revised schedule was needed) . Hopes were heightened for a late summer/early fall kickoff of mass immunization operations.\n\n【25】In January 1976, before the New Jersey outbreak, CDC had proposed legislation that would have compensated persons damaged as a result of immunization when it was licensed by FDA and administered in the manner recommended by ACIP. The rationale given was that immunization protects the community as well as the individual (a societal benefit) and that when a person participating in that societal benefit is damaged, society had a responsibility to that person. The proposal was sent back from a staff member in the Surgeon General's office with a handwritten note, \"This is not a problem.\"\n\n【26】Soon, however, NIIP received the first of 2 crippling blows to hopes to immunize \"every man, woman, and child.\" The first was later in 1976, when instead of boxes of bottled vaccine, the vaccine manufacturers delivered an ultimatum—that the federal government indemnify them against claims of adverse reactions as a requirement for release of the vaccines. The government quickly capitulated to industry's demand for indemnification. While the manufacturers' ultimatum reflected the trend of increased litigiousness in American society, its unintended, unmistakable subliminal message blared \"There's something wrong with this vaccine.\" This public misperception, warranted or not, ensured that every coincidental health event that occurred in the wake of the swine flu shot would be scrutinized and attributed to the vaccine.\n\n【27】On August 2, 1976, deaths apparently due to an influenzalike illness were reported from Pennsylvania in older men who had attended the convention of the American Legion in Philadelphia. A combined team of CDC and state and local health workers immediately investigated. By the next day, epidemiologic evidence indicated that the disease was not influenza (no secondary cases occurred in the households of the patients). By August 4, laboratory evidence conclusively ruled out influenza. However, this series of events was interpreted by the media and others as an attempt by the government to \"stimulate\" NIIP.\n\n【28】Shortly after the national campaign began, 3 elderly persons died after receiving the vaccine in the same clinic. Although investigations found no evidence that the vaccine and deaths were causally related, press frenzy was so intense it drew a televised rebuke from Walter Cronkite for sensationalizing coincidental happenings.\n\n【29】##### Guillain-Barré Syndrome\n\n【30】What NIIP did not and could not survive, however, was the second blow, finding cases of Guillain-Barré syndrome (GBS) among persons receiving swine flu immunizations. As of 1976, >50 \"antecedent events\" had been identified in temporal relationship to GBS, events that were considered as possible factors in its cause. The list included viral infections, injections, and \"being struck by lightning.\" Whether or not any of the antecedents had a causal relationship to GBS was, and remains, unclear. When cases of GBS were identified among recipients of the swine flu vaccines, they were, of course, well covered by the press. Because GBS cases are always present in the population, the necessary public health questions concerning the cases among vaccine recipients were \"Is the number of cases of GBS among vaccine recipients higher than would be expected? And if so, are the increased cases the result of increased surveillance or a true increase?\" Leading epidemiologists debated these points, but the consensus, based on the intensified surveillance for GBS (and other conditions) in recipients of the vaccines, was that the number of cases of GBS appeared to be an excess.\n\n【31】Had H1N1 influenza been transmitted at that time, the small apparent risk of GBS from immunization would have been eclipsed by the obvious immediate benefit of vaccine-induced protection against swine flu. However, in December 1976, with >40 million persons immunized and no evidence of H1N1 transmission, federal health officials decided that the possibility of an association of GBS with the vaccine, however small, necessitated stopping immunization, at least until the issue could be explored. A moratorium on the use of the influenza vaccines was announced on December 16; it effectively ended NIIP of 1976. Four days later the New York Times published an op-ed article that began by asserting, \"Misunderstandings and misconceptions. have marked Government . during the last eight years,\" attributing NIIP and its consequences to \"political expediency\" and \"the self interest of government health bureaucracy\" . These simple and sinister innuendos had traction, as did 2 epithets used in the article to describe the program, \"debacle\" in the text and \"Swine Flu Fiasco\" in the title.\n\n【32】On February 7, the new secretary of DHEW, Joseph A. Califano, announced the resumption of immunization of high-risk populations with monovalent A/Victoria vaccine that had been prepared as part of the federal contracts, and he dismissed the director of CDC.\n\n【33】### Lessons Learned\n\n【34】NIIP may offer lessons for today's policymakers, who are faced with a potential pandemic of avian influenza and struggling with decisions about preventing it . Two of these lessons bear further scrutiny here.\n\n【35】##### Media and Presidential Attention\n\n【36】While all decisions related to NIIP had been reached in public sessions (publishing of the initial virus findings in CDC's weekly newsletter, the Morbidity and Mortality Weekly Report (MMWR); New York Times reporter Harold Schmeck's coverage of the ACIP sessions, the president's press conference, and 4 congressional hearings), effective communication from scientifically qualified persons was lacking, and the perception prevailed that the program was motivated by politics rather than science. In retrospect (and to some observers at the time), the president's highly visible convened meeting and subsequent press conference, which included pictures of his being immunized, were mistakes. These instances seemed to underline the suspicion that the program was politically motivated, rather than a public health response to a possible catastrophe.\n\n【37】Annex 11 of the draft DHEW pandemic preparedness plan states, \"For policy decisions and in communication, making clear what is not known is as important as stating what is known. When assumptions are made, the basis for the assumptions and the uncertainties surrounding them should be communicated\" . This goal is much better accomplished if the explanations are communicated by those closest to the problem, who can give authoritative scientific information. Scientific information coming from a nonscientific political figure is likely to encourage skepticism, not enthusiasm.\n\n【38】Neither CDC nor the health agencies of the federal government had been in the habit of holding regular press conferences. CDC considered that its appropriate main line of communication was to states and local health departments, believing that they were best placed to communicate with the public. MMWR served both a professional and public audience and accounted for much of CDC's press coverage. In 1976, no all-news stations existed, only the nightly news. The decision to stop the NIIP on December 16, 1976, was announced by a press release from the office of the assistant secretary for health. The decision to reinstitute the immunization of those at high risk was announced by a press release from the office of the secretary, DHEW. In retrospect, periodic press briefings would have served better than responding to press queries. The public must understand that decisions are based on public health, not politics. To this end, health communication should be by health personnel through a regular schedule of media briefings.\n\n【39】##### Decision To Begin Immunization\n\n【40】This decision is worthy of serious question and debate. As Walter Dowdle  points out in this issue of Emerging Infectious Diseases, the prevailing wisdom was that a pandemic could be expected at any time. Public health officials were concerned that if immunization was delayed until H1N1 was documented to have spread to other groups, the disease would spread faster than any ability to mobilize preventive vaccination efforts. Three cases of swine influenza had recently occurred in persons who had contact with pigs. In 1918, after the initial outbreak of influenza at Fort Riley in April, widespread outbreaks of influenza did not occur until late summer .\n\n【41】The Delphi exercise of Schoenbaum in early fall of 1976  was the most serious scientific undertaking to poll scientists to decide whether or not to continue the program. Its main finding was that the cost benefit would be best if immunization were limited to those >25 years of age (and now young children are believed to be a potent source of spread of influenza virus!). Unfortunately, no biblical Joseph was there to rise from prison and interpret the future.\n\n【42】As Dowdle further states , risk assessment and risk management are separate functions. But they must come together with policymakers, who must understand both. These discussions should not take place in large groups in the president's cabinet room but in an environment that can establish an educated understanding of the situation. Once the policy decisions are made, implementation should be left to a single designated agency. Advisory groups should be small but representative. CDC had the lead responsibility for operation of the program. Implementation by committee does not work. Within CDC, a unit was established for program execution, including surveillance, outbreak investigation, vaccine procurement and distribution, assignment of personnel to states, and awarding and monitoring grants to the states. Communications up the chain of command to the policymakers and laterally to other directly involved federal agencies were the responsibility of the CDC director, not the director of NIIP, who was responsible for communications to the states and local health departments, those ultimately implementing operations of the program. This organizational mode functioned well, a tribute to the lack of interagency jealousies.\n\n【43】##### Decision-making Risks\n\n【44】When lives are at stake, it is better to err on the side of overreaction than underreaction. Because of the unpredictability of influenza, responsible public health leaders must be willing to take risks on behalf of the public. This requires personal courage and a reasonable level of understanding by the politicians to whom these public health leaders are accountable. All policy decisions entail risks and benefits: risks or benefits to the decision maker; risks or benefits to those affected by the decision. In 1976, the federal government wisely opted to put protection of the public first.\n\n【45】Dr Sencer was director of CDC from 1966 to 1977.\n\n【46】Dr Millar was director of NIIP in 1976.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "76459967-b33b-4a0a-a686-4840fa8e7af8", "title": "Engaging Communities to Develop and Sustain Comprehensive Wellness Policies: Louisiana’s Schools Putting Prevention to Work", "text": "【0】Engaging Communities to Develop and Sustain Comprehensive Wellness Policies: Louisiana’s Schools Putting Prevention to Work\nBackground\n\n【1】Almost all long-term tobacco use begins during childhood and adolescence , as each day nearly 3,500 American youths try their first cigarette . In Louisiana, smoking rates and the use of smokeless tobacco among high school and middle school students exceed national averages by more than 50%. According to the 2011 Louisiana Youth Tobacco Survey conducted by the Louisiana Department of Health and Hospitals, more than 42% of Louisiana’s public school students reported that they were never taught about the dangers of smoking, and among Louisiana’s youth who smoke, approximately half (47%) reported that they want to quit. Youth tobacco behavior is a significant concern for the state.\n\n【2】The prevalence of overweight (19.5%) and obesity (16.1%) among Louisiana’s high school students is another serious concern . In addition to the links between body mass index and various chronic diseases/conditions, being overweight is associated with unhealthy weight-control behaviors . Among Louisiana’s high school students, approximately 36% reported that they tried to lose weight by fasting, vomiting, and using laxatives or over-the-counter diet aids . Furthermore, Louisiana’s youth are insufficiently physically active . In 2011, only 24% of Louisiana’s high school students met Louisiana Department of Health and Hospitals guidelines for aerobic physical activity ; 42% had daily physical education classes; and 51% played on a school sports team at some point during the year . Recently, an examination of calorie intake among US youth noted that obese and overweight children consume fewer calories than do their healthy-weight counterparts, suggesting that energy expenditure/physical activity might be a particularly relevant component of weight management during adolescence .\n\n【3】Since the Women, Infants and Children (WIC) Reauthorization Act of 2004, which mandated the establishment of local school wellness policies, school districts have been struggling to develop and implement policies with guidelines for on-campus food and beverages, nutrition education, and physical activity . The importance of this Act lies in the potential of the school environment to affect the health and well-being of students . More recently, as a result of the Healthy Hunger-Free Kids Act of 2010, the US Department of Agriculture’s school meal guidelines have been revised to improve the quality of nutrition provided to our nation’s youth. Changes to the National School Lunch Program went into effect July 1, 2012 . The implementation of food, nutrition, and physical activity guidelines in schools is a recommended response to the increasing prevalence of obesity . School wellness policies are effective intervention points for managing youth tobacco behaviors as well .\n\n【4】The youth of Louisiana would benefit from schools that consistently provide healthy food choices, physical activity opportunities, and tobacco-free campuses . This was the impetus for the Louisiana Tobacco Control Program’s (LTCP’s) Schools Putting Prevention to Work (SPPW) pilot program. The pilot objective was twofold: to assist selected public school districts to develop a comprehensive school wellness policy and to engage the community (students/staff/parents/businesses) to support, implement, and sustain healthy behaviors by generating a school environment conducive to healthy choices and activities. SPPW was funded under the statewide policy and environmental change component of the Communities Putting Prevention to Work as a step toward reducing tobacco use and obesity among Louisiana’s youth.\n\n【5】Community Context\n\n【6】The SPPW pilot was implemented in 27 of Louisiana’s 70 school districts across the state . It is estimated that the SPPW project reached 302,450 students and 22,080 teachers and staff. SPPW activities also reached parents and guardians and other community members. The racial and economic composition of SPPW students reflected that of the state’s public school students . Before the pilot, 23 of the 27 participating sites claimed to have a comprehensive wellness policy. However, a systematic review of policy components indicated that none of these policies met the SPPW criteria for a “comprehensive” policy, which is one that calls for 100% tobacco-free school campuses and activities, a minimum of 30 minutes per day of physical activity, and the availability of healthier food and beverage choices in the cafeteria and throughout the campus .\n\n【7】Methods\n\n【8】The SPPW pilot consisted of a series of activities that began with an application/site selection process and ended with an approved comprehensive wellness policy. Participating school districts were required to 1) coordinate an active school health advisory council (SHAC) to assist with policy development and sustain implementation; 2) organize a local SPPW kick-off event to publicize the target issues; 3) implement a local media campaign to communicate messages about healthy eating, physical activity, and tobacco-free living to the school community; 4) develop and authorize a district-wide comprehensive wellness policy; and 5) plan for implementation and sustainability of the wellness policy. Participating districts were expected to implement their policy and continuously integrate successful wellness activities and modifications into classroom/school-based routines and practices. Guided by MAPPS (Media, Access, Point of decision information, Price, and Social support/services) interventions for CDC’s Communities Putting Prevention to Work, sites had the flexibility to develop plans and activities that were most appropriate for their community. Each participating site was given a $17,000 minigrant to defray the costs of project activities. Half of the grant funds were distributed at project initiation and half upon completion of the required activities. SPPW was carried out from June 2010 through January 2012.\n\n【9】### Administration\n\n【10】The LTCP maintained overall responsibility for this project but collaborated with the school wellness project state team (state team), a stakeholder advisory group organized for this purpose. The state team was made up of representatives from the Department of Health and Hospitals’ Chronic Disease Unit’s Tobacco Control Program, the Governor’s Council on Physical Fitness and Sports, the Department of Education’s Safe and Healthy Schools, the Louisiana Obesity Council, the Louisiana School Boards Association, Pennington Biomedical Research Center, LA Action for Healthy Kids, the Louisiana Public Health Institute, the University of Louisiana’s Picard Center, all of which have a statewide reach, and the Rapids Foundation, which serves 9 parishes in central Louisiana. The state team included experts in tobacco prevention, youth programming, physical activity education, nutrition and school food service, evaluation, school health and wellness, and policy analysis. These experts soon became the state team work group.\n\n【11】Daily project management was contracted with the Louisiana School Boards Association. A paid pilot coordinator functioned as project liaison, instructor, and comptroller, working closely with the LTCP, each project site, and the state team. The LTCP funded project evaluation through a contract with Pennington Biomedical Research Center. The external project evaluator participated in project planning, survey development/implementation, training events, and followed state team activities.\n\n【12】The LTCP used an application process to select project sites. For this purpose, the pilot coordinator and the state team work group drafted a detailed request for applications that was released on July 6 and distributed by mail and e-mail to school superintendents and school board presidents. The opportunity was announced at meetings of the Children’s Cabinet Advisory Board, the Louisiana Obesity Council, LaCapitale of the Links, Inc, and the annual Southern Obesity Summit. To further promote the opportunity, the Louisiana School Boards Association’s executive director, legislative liaisons, and school wellness coordinator telephoned key individuals around the state. A technical assistance conference call was held on July 22 to respond to questions and issues. Applications were due on August 6, 2010.\n\n【13】Applications were scored by the state team work group in accordance with a scoring rubric designed to favor the school districts that could benefit most from SPPW participation. The state team allocated priority points to districts that serve a largely at-risk demographic, are rural, did not have a functional SHAC, and did not have an adequate wellness policy. Points were also given for administrative capacities. The list of grant recipients was released as a statewide event, and contracts with the 27 school districts were formalized.\n\n【14】### Training\n\n【15】Each participating district selected a site coordinator to develop and manage SPPW activities. Each coordinator, generally with 1 or 2 additional project representatives, attended 3 day-long training sessions during the project period. Each training session was developed by the state team work group and evaluated to inform future program development and decision making.\n\n【16】The first training session was held at the beginning of the project period. It covered background information on the burden of tobacco and obesity and the role schools can play in food selection, physical activity, and tobacco-free environments. It also covered the project/contract deliverables, the _how-to_ of developing a SHAC and district wellness policies, MAPPS strategies, action planning, and project evaluation. This training was held in both southern and northern Louisiana to accommodate all projects.\n\n【17】The second training session was held 6 months later. This session revisited the topics of action planning and MAPPS strategies because they were reported as problematic for several project sites. The session further covered the _how-to_ of working with school boards and sustaining program achievements. Time was also devoted to the sharing of successes and lessons learned in a panel discussion led by selected site coordinators. Evaluation findings from the SPPW management perspective were shared.\n\n【18】The third training session was developed and scheduled as a wrap-up. The agenda included an overview of project accomplishments, presentations on district-specific changes resulting from the SPPW project, additional training on sustainability planning, and discussions on future school wellness activities and strategies.\n\n【19】The pilot coordinator monitored SPPW activities for compliance with project timelines and milestones through monthly progress reports submitted from each site. Site coordinators were encouraged to include newspaper articles, radio spots, and photos with their reports. Through a series of 3 open-ended surveys, the evaluator collected data from site coordinators on their experiences with processes such as SPPW message promotion and action planning, interacting with SHACs and school boards, problem solving, systematic reporting, and sustaining and increasing positive school-based practices and changes.\n\n【20】Similarly, the evaluator surveyed state team members on lessons learned during planning and implementation processes. All information was used to verify that project requirements were met, to describe how they were achieved, to assess their impact on students and community, and to inform program modifications.\n\n【21】Outcome\n\n【22】The SPPW application window was a short 1-month period, but the opportunity was well publicized. It was acknowledged by 1 applicant as “the most publicized grant I have ever seen.” However, the timing of the application process was inconvenient. Several site coordinators reported that many district employees take vacation in July so the August deadline presented a challenge.\n\n【23】The SPPW engagement process began in March 2010 with the organization of the school wellness policy state team. This group of 18 stakeholders from around the state facilitated comprehensive project planning and shaping through routine quarterly meetings. By July, however, the state team work group emerged as a core of members actively involved in informing and guiding SPPW implementation. The pilot sites viewed this multidisciplinary management team as an asset to the project.\n\n【24】According to training evaluation reports, the first training session assisted site coordinators by clarifying how to use and convey information relevant for building support for SPPW and school-based change. The second training session addressed the coordinator’s policy presentation concerns, suggested new activities or ideas that could be replicated in local communities, and prompted thinking along new lines. Notably, site coordinators reported that as a result of this training, they felt prepared to approach their school boards with their wellness policies. The third training session provided the opportunity for site coordinators to present their project successes, lessons learned, and next steps. After the third training session, all participants reported that they were able to pinpoint project areas that still need work, use suggested approaches to problem solving, and adapt new ideas/activities to sustain their wellness efforts. An unexpected product of the training sessions was a list of items submitted by the site coordinators that prompted actions by the state team work group, ultimately strengthening the SPPW program .\n\n【25】Early in the project year, site coordinators established a new SHAC (20 sites) or revitalized an existing SHAC (7 sites), enlisting membership from the school district community. Sites reported that these current SHACs incorporated a more diverse representation, became more active and influential with school district policy, and began to rethink school health issues. Site coordinators worked with their SHAC to keep the school board and district administrators informed of SPPW activities as they progressed. To date, each SHAC remains in place and continues to promote a healthier school environment.\n\n【26】The kick-off event and a small-scale media campaign were in place to generate and reinforce interest in healthy food choices, physical activity, and tobacco-free environments among the school community. Together, these activities consisted of various forms of rallies; sporting events; programs and workshops; incentives; healthy food tasting; poster contests; and messaging through radio, television, billboards, newspapers, websites, flyers, and local celebrities. Data from progress reports and surveys indicated that 1 project developed their media campaign around billboards that were so hard to read that the campaign was ignored. But among the other sites, approximately half of the kick-off events and 92% of the media campaigns reached members of the entire intended target (the school community). Furthermore, 89% of these projects engaged the larger community through collaborations with foundations, universities, athletic associations, law enforcement, or other local agencies.\n\n【27】One project leader stated that the kick-off event was “unnecessary — that it just added more work” and noted that several parents resented the anti-smoking/tobacco information. But the other project leaders reported that the kick-off was a positive event that was interesting to attendees and prompted talk of behavioral change among students, parents, and administrators. Project leaders reported that the media campaign motivated some degree of healthier behaviors:\n\n【28】*   Students would stop to show me the snack they brought from home and ask me if it is healthy.\n*   Students are telling \\[their parents\\] how to eat better and to buy healthier food.\n*   Some teachers are now \\[incorporating\\] exercise into their class routine.\n*   Several administrators and teachers are wearing their pedometers at school and closely watching their eating habits.\n*   Parents are volunteering and asking how they can help.\n\n【29】Of the original 27 pilot sites, 25 sites (36% of all school districts in the state) developed the comprehensive policy and received approval by their district school board. One site withdrew because of administrative complications; the other site drafted policy components but would not create a single comprehensive policy as required. In addition, each of the completing sites adopted various environmental changes that over time could improve health behaviors in the school communities . Each of these sites also held sustainability discussions with their SHAC and/or district administrators, and 9 sites produced a sustainability plan. Site coordinators understood the value of maintaining an active SHAC and the benefit of continuously engaging the greater community to institutionalize healthful choices and normalize healthful behaviors.\n\n【30】Each site coordinator dealt with implementation challenges. Several of these challenges and coping strategies were common to more than 1 site. First, site coordinators reported that early on their efforts were frequently interrupted by holidays and weather-related school closings. Though frustrated, they persevered. Next, most of the coordinators indicated that meetings, record keeping, and monthly reporting responsibilities created major scheduling stresses; they would have preferred quarterly reporting. However, they worked through it by prioritizing and applying other time-management strategies. A larger issue was that some school principals and staff assigned to project activities viewed their SHAC and the project as irrelevant. When necessary, site coordinators circumvented these principals by working through assistant superintendents; when possible, coordinators replaced uninterested staff. Several other site coordinators reported that their SHAC was disjointed and struggling. After consulting with successful SHACs on best practices, these groups began to focus on team building and leadership.\n\n【31】Interpretation\n\n【32】The SPPW pilot demonstrated a unique approach to achieving its objectives. It is the first program to partner with a school boards association, to make use of a state team advisory group to provide guidance and training to site coordinators on tobacco and obesity strategies simultaneously, and to create incentives for policy development. By the end of the pilot period, the 25 completing sites had achieved the community buy-in that is essential for implementing new wellness policies, reshaping their school environment, and sustaining the effort. A 1-year follow-up on SPPW-related efforts and activities is in progress. Over the long term, the Louisiana Youth Tobacco Survey and the Youth Risk Behavioral Survey can be used to monitor trends in key food, tobacco use, and physical activity habits of Louisiana students.\n\n【33】Lessons Learned\n\n【34】1.  For SPPW to comfortably coincide with the academic year, project planning and organization should be in place before summer recess.\n2.  Working through a large statewide group of stakeholders was constructive for project initiation. Once the pilot was in place, a smaller workgroup of key stakeholders was more effective and efficient.\n3.  Training for site coordinators is important for project success. Participants reported that the trainings provided guidance and focus, facilitated collaborations, and fostered confidence.\n4.  Most of the participating pilot sites reported that they could not have achieved their successes without the associated minigrant. Though minimal, the funds served as a goodwill gesture and helped administrators justify staff time and effort from among competing activities.\n5.  As a rule, SPPW activities piqued the interest and prompted participation of students, encouraged parental involvement, and initiated teacher-driven classroom and school-based changes. However, some SPPW changes (eg, a switch to whole-grain items in cafeterias; 100% tobacco-free sporting events) required time and repetition before they became acceptable.\n6.  Project support among SHACs, administrators, and school boards was built and maintained through straightforward, ongoing communications. This support facilitated the process of wellness policy passage by each district’s school board.\n\n【35】The Louisiana SPPW model is an effective approach to establishing school wellness policies and healthier school environments.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "08425641-321e-4129-ae07-a87f8191428b", "title": "Genetic Characterization of Toggenburg Orbivirus, a New Bluetongue Virus, from Goats, Switzerland", "text": "【0】Genetic Characterization of Toggenburg Orbivirus, a New Bluetongue Virus, from Goats, Switzerland\nBluetongue is a vectorborne disease affecting all ruminant species but causing clinical disease mostly in sheep; cattle and goats are usually considered asymptomatic reservoir hosts . The disease occurs worldwide and is caused by bluetongue virus (BTV), which belongs to the genus _Orbivirus_ within the family _Reoviridae_ . Other species of the orbiviruses are African horse sickness virus, epizootic hemorrhagic disease virus (EHDV), and some lesser-known viruses such as Peruvian horse sickness virus, Chuzan virus, Saint Croix River virus, and Yunnan orbivirus . The virus has a segmented double-stranded RNA genome consisting of 10 segments that code for 11 viral proteins. BTV is transmitted between its ruminant host mainly by blood-feeding midges of the _Culicoides_ spp. Twenty-four serotypes of BTV can be distinguished on the basis of the antigenic profile of its major outer capsid protein VP2 .\n\n【1】In Europe, several serotypes of BTV were detected sporadically in the 1990s . From 1998 through 2006, a total of 5 serotypes (1, 2, 4, 9, and 16) became established in southern Europe , but they were kept confined to this area, most likely because their main insect vector, _C_ . _imicola_ , has never been detected north of the Alps in Europe . A BTV-monitoring program in cattle herds has been in effect in Switzerland since 2003 in Ticino Canton, south of the Alps . However, no BTV has been detected until now in this region of the country. Furthermore, northern Europe had never been affected by bluetongue before 2006, although BTV can also be transmitted by other _Culicoides_ species, such as _C. obsoletus_ and _C. pulicaris_ , which are native to this area.\n\n【2】In 2006, BTV serotype 8 (BTV-8) of likely African origin was introduced into Belgium by an as yet unknown mechanism . This virus spread throughout many northern European countries. By mid-2008, BTV-8 had caused numerous clinical bluetongue cases in sheep and cattle in the Netherlands, Belgium, Germany, Luxembourg, France, Denmark, Czech Republic, United Kingdom, Italy, Spain, and Switzerland .\n\n【3】The first clinical case of bluetongue caused by BTV-8 in Switzerland was detected in October 2007 on a dairy cattle farm . Because Switzerland was already included in a surveillance zone and, after detection of the first BTV-8 infection, a protection zone, all animals were tested serologically and virologically before trade. Four additional cases of BTV-8 infection were detected from October through November 2007 .\n\n【4】Early in 2008, several BTV-positive, clinically healthy animals were detected in a goat flock in northeastern Switzerland  by BTV-specific antibody ELISAs and viral RNA detection, by using a BTV-specific real-time reverse transcription–PCR (rRT-PCR) . However, levels of antibody and viral RNA were unexpectedly low. Furthermore, presence of BTV RNA could not be confirmed by use of other BTV-specific rRT-PCR protocols , and amplification curves obtained in the screening rRT-PCR suggested that the target sequence on RNA segment 10 of this virus might be different from all known BTV strains (data not shown). For this reason, we determined the nucleotide sequence of the rRT-PCR amplification product.\n\n【5】Experimental infection of goats and sheep by subcutaneous and intravenous injection of blood from virus-positive animals from the herd in which the infection was first discovered demonstrated that the virus is transmissible to and replicates in goats, albeit without causing bluetongue-specific clinical symptoms. In contrast, the virus could only be sporadically detected in inoculated sheep . On the basis of the sequence of the rRT-PCR product, this newly detected virus, termed Toggenburg orbivirus (TOV), could represent an unknown orbivirus of low pathogenicity, or a new serotype of BTV. Thus, we performed a detailed genetic characterization of the TOV genome.\n\n【6】The complete coding sequence for the 7 RNA segments (2, 5, 6, 7, 8, 9, and 10) was determined. Segment 2 was chosen because it encodes the viral structural protein VP2 that contains the serotype-specific determinants, whereas the genome segments 5 through 10 are short enough to enable sequencing of the cloned cDNA without the need for internal sequencing primers. We present evidence that TOV is genetically related to BTV but cannot be assigned to any of the 24 known serotypes; instead, it likely represents a 25th BTV serotype.\n\n【7】### Materials and Methods\n\n【8】##### Source of Viral RNA and Extraction of RNA\n\n【9】BTV-like orbivirus RNA and antibodies were detected in several adult and newborn goats in a herd in the northeastern part of Switzerland (St. Gallen Canton) . We used erythrocytes from blood collected into tubes containing EDTA from 1 adult and 1 newborn goat, which had cycle threshold (C T  ) values of 22 and 32, respectively, in the rRT-PCR . Total RNA was extracted from 250 μL of erythrocyte suspension by using Trizol (Invitrogen, Basel, Switzerland) according to the manufacturer’s instructions. Precipitated purified RNA was dissolved in 20 μL of RNase-free water.\n\n【10】##### RT-PCR for Full-Length cDNA Amplification, Cloning, and Sequencing\n\n【11】Primers corresponding to inter-BTV serotype-conserved 5′ and 3′ terminal sequences were designed for each of the viral RNA segments to be analyzed . Before RT-PCR, 8 μL of extracted viral RNA was mixed with 50 pM of 2 RNA segment-specific forward and reverse primers and heat-denatured for 5 min at 95°C. Reverse transcription of both RNA strands into double-stranded cDNA was performed by using SuperScript III reverse transcriptase (Invitrogen) in a final volume of 25 μL. A total of 8 μL of cDNA was then added to a PCR mixture containing Platinum Taq High Fidelity Polymerase (Invitrogen) and amplified by 40 cycles at 95°C for 30 s, 50°C for 1 min, and 72°C for 1 min (for segments 5–10) or 72°C for 8 min (for segment 2).\n\n【12】PCR-amplified cDNA of the expected length was purified by agarose gel electrophoresis and ligated into plasmid vector pCR4-TOPO (Invitrogen). Clones of transformed _Escherichia coli_ harboring the TOV insert-containing vector were identified by PCR preps using insert-spanning M13 primers. Miniprep DNA of \\> 5 cDNA clones from each viral genome segment was sequenced by cycle sequencing using IRD800 and IRD700 infrared dye–labeled M13 (Eurofins MWG Operon, Ebersberg, Germany) and TOV segment 2–specific internal primers. Sequencing reactions were subjected to electrophoresis in a 4300L DNA sequencer (LI-COR, Lincoln, NE, USA) and analyzed by using e-Seq V3.0 and AlignIR V2.0 software (LI-COR). The coding region of each TOV genome segment analyzed in this study was compared with published orbivirus sequences by using online BLAST analysis . For phylogenetic analysis, TOV-specific sequences were aligned to a selection of available corresponding sequences from GenBank that represented all orbivirus species by using MEGA version 4.0 software  with default parameters. The open reading frame (ORF) sequences of the 7 analyzed genome segments of TOV were submitted to GenBank under accession nos. EU839840 (S2), EU839841 (S5), EU839842 (S6), EU839843 (S7), EU839844 (S8), EU839845 (S9), and EU839846 (S10).\n\n【13】### Results\n\n【14】TOV was initially detected by using an RNA segment 10–specific rRT-PCR . Nucleotide sequencing of this rRT-PCR product indicated that TOV is an orbivirus closely related to, but not identical with, any known BTV serotype. The analyzed sequence showed a 1-base mismatch in the probe region of the rRT-PCR target sequence , which could explain the unusual amplification curves (low delta Rn value, the magnitude of the signal generated by the given set of PCR conditions). Various additional rRT-PCR protocols all yielded negative results (data not shown). When the primer and probe sequences of these published rRT-PCR protocols for segment 1  or segment 5 , respectively, were aligned to the corresponding TOV sequences determined in this study, numerous mismatches were found . These mismatches were the likely reason for the failure of these assays to detect TOV. Because sequence amplified by the rRT-PCR  was most closely related to several BTV-4 and BTV-10 serotypes, RNA segment–specific primers binding to the conserved 3′ and 5′ untranslated regions were designed on the basis of these serotypes whenever no consensus sequence matching with all published sequences (representing all 24 serotypes) could be found, i.e. for segments 2 and 6.\n\n【15】Although the amount of viral RNA in blood from infected but clinically healthy goats was low (C T  values >30 in the rRT-PCR) we could amplify the full-length cDNA sequence for all 7 RNA segments analyzed, including the 2.9-kb fragment of segment 2. PCR products all showed the expected length as predicted from the corresponding BTV genome segment. After cloning into the pCR4-TOPO vector, both DNA strands of 5 clones were sequenced for each genome segment. A consensus sequence could be unambiguously determined because the number of point mutations in the clones was low, and never more than 1 of the 5 clones showed a difference from the consensus sequence (data not shown).\n\n【16】BLAST analysis of the 7 TOV genome segments showed in all cases the highest sequence similarity to BTV, although the search algorithm had to be changed from megablast (highly similar sequences) to blastn (somewhat similar sequences) to detect any sequence homology. However, results varied widely in terms of percentage of identity to their closest BTV sequence and to BTV serotype showing the highest similarity . No BTV strain was distinctly more closely related to TOV than all other known sequences, although for several genome segments BTV serotype 4 isolates were found to be the closest relatives to TOV. When identity levels between analyzed TOV genome segments to their genetically closest BTV strain were compared with respective values of the 2 most distantly related BTV serotypes, TOV sequences were in some cases (segments 5, 8, and 10) clearly more different than the BTV sequences among themselves. However, for the remaining segments, TOV sequences were less different from their closest BTV sequence than the 2 most distantly related BTV serotypes (segments 2, 6, 7, and 9).\n\n【17】Results of phylogenetic analysis using ClustalW alignment  and neighbor-joining tree construction confirmed the results of BLAST analyses by locating some TOV genome segments within the BTV serogroup (segments 2, 6, 7, and 9). In contrast, for segments 5, 8, and 10, TOV sequences were outside the BTV subtree . Furthermore, the S2 gene could not be assigned to any of the 9 proposed nucleotypes  (data not shown). Despite the different placement of the 7 TOV genome segments, they were all more closely related to BTV than to any of the other known orbivirus species, in particular to EHDV, which is the closest relative to BTV .\n\n【18】### Discussion\n\n【19】In the context of trade investigations implemented in Switzerland shortly after the first BTV-8 outbreak was detected , BTV-specific RNA was detected in clinically healthy goats in St. Gallen Canton in northeastern Switzerland by segment 10–specific rRT-PCR. However, none of the additional rRT-PCR protocols for other BTV genome segments, which were performed for confirmatory purposes, yielded positive results. Furthermore, the short (58-nt) segment 10–specific sequence amplified by the initial rRT-PCR was similar but not identical to any known BTV. Therefore, to characterize TOV as a potentially novel BTV, we performed a detailed genome analysis by cloning and sequencing the entire coding region of most viral genome segments.\n\n【20】cDNA of smaller genome segments that encode the structural proteins VP5 (segment 6), VP6 (segment 9), VP7 (segment 7), and nonstructural (NS) proteins NS1 (segment 5), NS2 (segment 8), and NS3/NS3A (segment 10) could be amplified and sequenced without the need for internal, TOV-specific primers. The 2.9-kb region spanning segment 2 was also included; however, a primer-walking approach had to be used for design of TOV-specific sequencing primers (data not shown). VP2, which is encoded by segment 2, is exposed at the virion surface, together with VP5 , and carries the epitopes inducing serotype-specific neutralizing antibodies . Thus, segment 2 was expected to contain the genetic information enabling assignment of TOV to 1 of the 24 BTV serotypes.\n\n【21】Although initial attempts to isolate TOV in several mammalian cell lines failed, there is preliminary evidence that the virus replicates in the highly BTV-susceptible KC insect cell line derived from _Culicoides_ spp. However, to avoid introduction of mutations in the viral genome caused by cell culture adaptation, we used blood as the source for cloning and nucleotide sequencing of the TOV genome.\n\n【22】For accurate identification of a consensus sequence for each cloned TOV genome segment, 5 clones were analyzed for each segment. When sequences of individual clones were compared, mismatches were found in <1% of the nucleotide positions (data not shown), and always in only 1 of the 5 clones. If one takes into account the error rate of the reverse transcriptase and Taq DNA polymerase used to transform the viral RNA into PCR-amplified cDNA, this finding supports reported genetic stability of the double-stranded RNA genome of orbiviruses .\n\n【23】BLAST analysis of the ORF sequence of the 7 TOV genome segments studied suggested that TOV most likely belongs to the BTV serogroup because although the degree of nucleotide sequence identity between TOV and the closest BTV relative was low, each of the TOV genome segments was closely related to BTV and not to another orbivirus. The BLAST results were confirmed by individual phylogenetic analysis of the 7 TOV genome segments. Depending on the segment, TOV was placed in a different neighborhood among BTV and EHDV . Although segments 2, 6, 7, and 9 encoding viral structural proteins were placed within the BTV subtree in the dendrograms, albeit being distinct from all 24 BTV serotypes, the remaining segments 5, 8, and 10 coding for NS proteins were located outside the branching area of the BTV subtree toward the EHDV subtree. These findings suggest that TOV might have diverged from BTV and evolved independently or could represent a reassortant between BTV and an unknown orbivirus closely related to EHDV. This assumption is supported by results of BLAST analysis that showed a greater evolutionary distance between TOV and BTV than among the various BTV serotypes (data not shown), which is evident in the case of the segment 10 encoding NS3/NS3A. It will be useful to determine whether TOV genome segments 1, 3, and 4, which encode viral structural proteins, are also more closely related to BTV than the NS genes.\n\n【24】Definitive characterization of TOV as a new BTV serotype will require a comprehensive serologic typing with BTV serotype–specific antisera and TOV-specific antisera. It is likely that TOV represents a new BTV serotype because agreement of segment 2–based genotypic and serology-based phenotypic serotype differentiation, involving a large collection of BTV strains representing all 24 known serotypes, has been reported .\n\n【25】We propose that TOV more likely represents an unknown 25th serotype of BTV rather than a new orbivirus species, on the basis of grouping of structural protein genes within BTV and because sera from goats from which the virus was isolated showed reactivity in several BTV-specific antibody ELISAs. These tests did not show reactivity with antibodies against EHDV, which represents the closest relative to BTV among the orbiviruses (data not shown).\n\n【26】TOV did not cause bluetongue-specific clinical signs in experimentally infected goats . In addition, virus load was low. These findings are consistent with the fact that goats generally show no clinical disease after BTV infection but can serve as reservoir hosts . However, TOV was only sporadically detected by rRT-PCR in inoculated sheep, and these animals did not show any pronounced clinical signs of bluetongue. If TOV also shows an apathogenic phenotype in other ruminants, this finding would further support the possibility that TOV may have evolved differently than classic BTV strains. Identification of insect vector(s) for TOV would be useful because these vector(s) might influence transmission and phenotypic properties of TOV.\n\n【27】An apathogenic BTV termed the KM strain has been reported in goats from Taiwan . This virus was characterized as a typical BTV and was added to BTV reference sequences for the present phylogenetic analysis . On the basis of its high segment 2 sequence similarity with BTV-2 , it is likely that KM belongs to this BTV serotype. Comparison of TOV with KM shows that TOV is less related to classic BTV than KM, which rules out TOV and the KM strain having a common ancestor, although the 2 viruses share a high sequence homology in their segment 9 ORF . Therefore, TOV might represent a reassortant BTV that acquired its segment 9 from a BTV closely related to KM.\n\n【28】To determine the genetic relationship of TOV with the BTV-8 strain currently circulating in northern Europe, the segment 10 coding region of the Swiss BTV-8 isolates from 2 of the late 2007 outbreaks was sequenced and submitted to GenBank (accession nos. EU450660 and EU450661). This BTV-8 was different from a BTV-8 reference strain obtained from the Office International des Epizooties Reference Laboratory for bluetongue (Institute of Animal Health, Pirbright, UK) . Although TOV was initially detected in Switzerland at the same time as the first BTV-8 infections, these 2 BTV strains are genetically diverse, as shown in the dendrogram of segment 10 . None of the 7 genome segments analyzed or any of the 3 remaining genome segments (1, 3, and 4) (data not shown), showed sequence homologies >79% with any other BTV, and none of the BLAST analyses showed a maximal sequence identity level with the corresponding genome segment of the BTV-8 currently circulating in Europe . These findings indicate that TOV is not a reassortant between BTV-8 and another BTV.\n\n【29】Distinct sequence differences on RNA segments 1 and 5 may also explain why TOV was not detectable by rRT-PCR protocols designed to detect a wide range of BTV serotypes . Only the segment 10–specific rRT-PCR showed a positive result, whereas all segment 5– and segment 1–specific assays did not detect TOV. These findings demonstrate the importance of selecting an appropriate rRT-PCR protocol for detection of TOV and other unusual BTV-like viruses. The fact that most diagnostic laboratories use segment 1– or segment 5–specific rRT-PCRs for routine detection of BTV might also explain why TOV has never been detected in other regions.\n\n【30】TOV and other BTV-25 strains, as potential apathogenic viruses, will have major implications in control of bluetongue. A similar situation is found in the United States, where several BTV serotypes circulate without causing any clinical signs in domestic ruminants; nevertheless, national and international animal trade is heavily affected .\n\n【31】Additional animal experiments in various ruminant species and adaptation of TOV to cell culture for extended phenotypic characterization (e.g. replication kinetics) are currently under way. TOV-specific rRT-PCR protocols will be developed to determine TOV prevalence and to clarify its role as an animal pathogen. Furthermore, TOV-specific serologic tools that use immunologically dominant epitopes of VP2 and VP5 as recombinant ELISA antigens for detection of antibodies to TOV should be developed. These tools would facilitate epidemiologic studies to determine actual and retrospective seroprevalences of TOV in goats and other domestic and wild ruminants in Switzerland and throughout the world, in particular in countries that have long claimed to be free of BTV infections. Furthermore, TOV-specific diagnostic tools will enable identification of natural reservoir(s) and address how and by which insect vectors this orbivirus is transmitted.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "2a791cbc-c79f-4e3c-95c5-b67e8c15cea0", "title": "Self-Reported Sitting Time in New York City Adults, The Physical Activity and Transit Survey, 2010-2011", "text": "【0】Abstract\n\n【1】**Introduction**  \nRecent studies have demonstrated the negative health consequences associated with extended sitting time, including metabolic disturbances and decreased life expectancy. The objectives of this study were to characterize sitting time in an urban adult population and assess the validity of a 2-question method of self-reported sitting time.\n\n【2】**Methods**  \nThe New York City Health Department conducted the 2010–2011 Physical Activity and Transit Survey ( _N_ \\= 3,597); a subset of participants wore accelerometers for 1 week ( _n_ \\= 667). Self-reported sitting time was assessed from 2 questions on time spent sitting (daytime and evening hours). Sedentary time was defined as accelerometer minutes with less than 100 counts on valid days. Descriptive statistics were used to estimate the prevalence of sitting time by demographic characteristics. Validity of sitting time with accelerometer-measured sedentary time was assessed using Spearman’s correlation and Bland-Altman techniques. All data were weighted to be representative of the New York City adult population based on the 2006–2008 American Community Survey.\n\n【3】**Results**  \nMean daily self-reported sitting time was 423 minutes; mean accelerometer-measured sedentary time was 490 minutes per day ( _r_ \\= 0.32, _P_ < .001). The mean difference was 49 minutes per day (limits of agreement: −441 to 343). Sitting time was higher in respondents at lower poverty and higher education levels and lower in Hispanics and people who were foreign-born.\n\n【4】**Conclusion**  \nParticipants of higher socioeconomic status, who are not typically the focus of health disparities–related research, had the highest sitting times; Hispanics had the lowest levels. Sitting time may be accurately assessed by self-report with the 2-question method for population surveillance but may be limited in accurately characterizing individual-level behavior.\n\n【5】Introduction\n\n【6】Sitting is a sedentary behavior and has been linked to all-cause and cardiovascular disease mortality and to decreased life expectancy; 3 hours per day of sitting leads to a life expectancy decrease of 2 years . Sitting is distinct from a lack of recreational or nonrecreational physical activity (PA), and these behaviors have differing demographic determinants . Furthermore, these distinct behaviors may affect health outcomes via alternative biological mechanisms . This research has given rise to the “inactivity physiology paradigm,” or that “sitting too much is not the same as lack of exercise, and as such, has its own unique metabolic consequences” such as decreased lipoprotein lipase activity in skeletal muscles in the legs . People who exercise regularly but are still sedentary for several hours a day may have a risk of adverse health outcomes that is higher than would be expected given their overall PA levels, although recent findings in the literature are mixed .\n\n【7】A common practice is to use a broad definition of _sedentary_ , although specific definitions are needed ; this investigation thus focused on sitting time. Measured estimates in New York City (NYC) have not been previously published and are of interest, given the differences in PA levels between NYC and US adults . In a report released by the NYC Health Department, adult New Yorkers were 3 times as likely as adults nationwide to meet PA recommendations (29% versus 11%) . Previous surveys have also assessed sitting time by using 1 or 2 questions on weekdays versus weekends  or up to 21 questions to capture specific domains of sitting time . However, these prior question sets may not adequately address sitting time in a single day or may be too long to administer at the population level. Recall would likely be improved if the single day were divided into smaller periods.\n\n【8】We used 2 questions derived from 1 previously standardized question, dividing a single day into daytime and evening sitting times to improve recall and estimation. The objectives of this analysis were to describe self-reported sitting time and accelerometer-measured sedentary time in a diverse, urban population and to test the validity of a 2-question method of self-reported sitting time that allowed for discrete periods of recall.\n\n【9】Methods\n\n【10】The NYC Health Department conducted the Physical Activity and Transit (PAT) Survey, a cross-sectional, assessment of PA and transit behaviors in NYC adults. The PAT Survey was a random-digit–dial telephone survey designed to provide estimates of PA at the city, borough (county), and subgroup (eg, race/ethnicity) levels. An overlapping landline and cellular telephone sample frame to contact adults in residential households in NYC — with disproportionate, equal-sized samples from the 5 boroughs — was used, and areas with higher levels of obesity were oversampled. The first wave of interviews was conducted from September through November 2010 (n = 1,323); the second wave was conducted from March through November 2011 (n = 2,488; N = 3,811). The PAT Survey and device portion have been described previously . The institutional review board of the NYC Health Department approved this study as human subjects research.\n\n【11】### Measurement and definitions of covariates\n\n【12】Self-reported sitting time was assessed by combining responses from 2 questions on time spent sitting for daytime and evening: “On an average day during the last 7 days, from the time you woke up to around 5 o’clock in the evening, how many hours or minutes did you spend sitting?” and “From 5 o’clock in the evening to the time you went to bed on an average day during the last 7 days, how many hours or minutes did you spend sitting?” Demographic characteristics (age, sex, race/ethnicity, poverty status, education, nativity) and height and weight were self-reported. Race/ethnicity was assessed using 2 questions on Hispanic ancestry and race group and was categorized as non-Hispanic white, non-Hispanic black, Hispanic, non-Hispanic Asian, and non-Hispanic other. Poverty level was based on annual combined household income and was grouped according to federal poverty guidelines (<200%, 200%–399%, ≥400% of the federal poverty level \\[FPL\\]). Nativity was defined as self-reporting being born in the United States or elsewhere. Puerto Ricans and respondents born in other US territories were considered US-born. Body mass index (BMI) was calculated from self-reported height and weight (kg/m 2  ), and participants were categorized as underweight/normal (BMI <25.0), overweight (BMI of 25.0–29.9), or obese (BMI ≥30.0).\n\n【13】### Accelerometer subset\n\n【14】Mobile individuals (people who could walk >10 feet) in the second wave of the PAT Survey were asked to participate in a device follow-up study using accelerometers. Participants were asked to wear the accelerometer for 1 week during all waking hours and to remove it only when in water. Of those who completed an interview in the second wave of the PAT Survey, 803 (32%) agreed to participate and returned devices with data. Participants wore hip-mounted ActiGraph GT3X accelerometers (ActiGraph, LLC). Because they wore the devices at home and while working, while commuting, and during recreational time, data are comparable to the self-report data. To process the accelerometer data for analysis, activity thresholds from the National Health and Nutrition Examination Survey (NHANES) were used to assign minutes as sedentary time. Sedentary time was defined as accelerometer minutes with fewer than 100 counts on valid days (≥10 hours of wear time).\n\n【15】### Statistical methods\n\n【16】All analyses of self-reported and accelerometer data were restricted to mobile participants, those with plausible PA values, and responses to the questions on sitting time; the final analytic sample size for the self-report data was 3,597 (94.4% of 3,811 participants). The minimum accelerometer wear time for a reliable estimate of weekly activity is 10 or more hours on 4 or more days . With this cutoff as the inclusion criterion, 667 participants (83.1% of 803 accelerometer participants) were included in the final analytic data set for accelerometer-related analyses.\n\n【17】Validity of self-reported sitting time with accelerometer-measured sedentary time was assessed by using Spearman’s correlation and agreement methods outlined by Bland and Altman . Linear regression was used to check whether the mean difference and limits of agreement (LoA, ±2 standard deviations) varied across average values of self-reported sitting time and accelerometer-measured sedentary time (\\[self-reported sitting time + accelerometer sedentary time\\]/2).\n\n【18】Mean self-reported sitting time and accelerometer-measured sedentary time was assessed overall and stratified by demographic characteristics; differences were assessed using _t_ tests. Self-report of day and evening sitting time was also stratified by demographics, and differences were assessed using _t_ tests. Pearson correlations of sitting time to sedentary time were also run separately for day and for evening sitting. Multivariable linear regression models adjusted for age, sex, race/ethnicity, poverty status, education level, nativity, and BMI category were used to assess the factors associated with sitting time. To estimate results representative of the NYC population, analyses incorporated sampling weights to account for complex survey design and nonresponse. Data were analyzed using SUDAAN (version 10.0; Research Triangle Institute) and SAS (version 9.2, Research Triangle Institute).\n\n【19】Results\n\n【20】Seventy percent of respondents were aged 25 to 64 years, more than half were female, nearly half were black or Hispanic, and almost 40% had an income of less than 200% of the FPL and were foreign-born . The subset of participants who wore the accelerometer was similar to the overall sample, although they were slightly more likely to be US-born versus foreign-born.\n\n【21】### Sitting and sedentary time in NYC adults\n\n【22】Self-reported mean daily sitting time was 423 minutes per day (7.1 hours/d), and accelerometer-measured mean daily sedentary time was 490 minutes per day (8.2 hours/d) . High poverty individuals or those with lower education reported less sitting time per day. Self-reported sitting time was lower in Hispanics (324 min/d vs 465 min/d in non-Hispanic whites); it was also lower in women and those who were foreign-born and obese. Mean self-reported sitting time during daytime hours was 243 minutes per day (4.0 hours/d); during evening hours it was 180 minutes per day (3.0 hours/d) . Mean sitting time was 8 or more hours in residents in Manhattan (Upper West Side, Upper East Side–Gramercy, Chelsea–Village, Union Square, Lower Manhattan) and in 2 areas of Brooklyn (Borough Park, Greenpoint) and Jamaica, Queens .\n\n【23】Results using accelerometer data differed slightly from self-reported data; among participants aged 65 or older, measured sedentary time (532 min/d) was higher than self-reported sedentary time (418 min/d). Accelerometer data were similar in men and women (both 490 min/d). Statistical relationships with education level remained consistent across the 2 measures, though the dose–response effect across categories was not as apparent in the accelerometer data. No meaningful differences were observed by poverty level, nativity, or obesity in accelerometer data.\n\n【24】In multivariable models after adjustment for covariates (age, sex, race/ethnicity, poverty status, education level, nativity, and BMI category), self-reported sitting time was significantly higher in men than in women, highest in Asians and lowest in Hispanics, lower in those at lower education levels than in those at higher education levels, lower in those at higher poverty levels than in those at lower poverty levels, and lower in those who were foreign-born than in those who were US-born . Accelerometer-assessed sedentary time was higher in older adults than in younger adults, lower in Hispanics than in other racial/ethnic groups, and lower among those with lower education levels than among college graduates.\n\n【25】### Assessing validity of self-reported sitting\n\n【26】The correlation between self-reported sitting time and accelerometer-assessed sedentary time was modest ( _r_ \\= 0.32, _P_ < .001); the correlation was stronger in daytime ( _r_ \\= 0.37, _P_ < .001) versus evening ( _r_ \\= 0.23, _P_ < .001) sitting. Correlations were highest among younger adults (aged 18–24 y; _r_ \\= 0.46, _P_ < .001) and those with higher incomes (≥400% FPL; _r_ \\= 0.42, _P_ < .001). Correlations were lowest for black ( _r_ \\= 0.24, _P_ < .001), Hispanic ( _r_ \\= 0.27, _P_ < .001), low-income (<200% FPL; _r_ \\= 0.19, _P_ < .001), foreign-born ( _r_ \\= 0.28, _P_ < .001), and obese ( _r_ \\= 0.17, _P_ \\= .02) participants. The Bland-Altman plot shows a scatterplot of the difference between measures by the average of the 2 measures . The mean difference between self-report and measurement was 49 minutes per day. The LoA were −441 to 343 minutes per day, meaning that an estimate of sitting time could be underreported by as much as 7.4 hours per day or overreported by 5.7 hours per day. Linear regression showed a significant positive association for the difference between self-reported sitting time and accelerometer-measured sedentary time. At lower levels of sitting time, self-report was lower than accelerometer-measured sedentary time. At higher levels of sitting time, self-report was higher than accelerometer-measured sedentary time (β = 0.59; standard error = 0.02; _P_ < .001; LoA = mean difference ± 200.34).\n\n【27】The difference between self-report and accelerometer is plotted on the y-axis and the average of the 2 measures on the x-axis. The small dashed line represents the mean difference between the 2 measures; the large dashed lines are the 95% limits of agreement (N = 667).\n\n【28】Discussion\n\n【29】In a representative sample of noninstitutionalized NYC adults, mean self-reported sitting time was 423 minutes per day (7.1 hours/d). Accelerometer-assessed sedentary time was 490 minutes per day (8.2 hours/d). These values are similar to national values of sedentary time assessed using accelerometer data from NHANES 2003–2004, which showed that the average US adult was sedentary for 7.7 hours per day . Mean sitting and sedentary time was highest in adults 65 years or older and in those who were college educated; accelerometer measurements demonstrated that these groups were sedentary for 8.9 hours per day.\n\n【30】Hispanics had the lowest values for self-reported sitting and measured sedentary time of all racial/ethnic groups. These results may reflect more physical occupations among Hispanics  but also point to the value of incorporating questions on both leisure-time and occupational activity when assessing PA levels in Hispanic populations. Similar results on occupation-related PA levels in Mexican Americans have been demonstrated in national data .\n\n【31】Although variations were seen in sitting times (lower values for Hispanics and those with lower educational levels and incomes), the amount of sitting time across all sociodemographic groups greatly exceeded the 3 hours per day associated with decreased life expectancy. Interventions for decreasing sitting time are needed to improve health outcomes across all groups. Worksite and home interventions should target all sociodemographic groups; however, the interventions should consider sex, age, and cultural and socioeconomic differences, as well as the time of day the sitting is occurring.\n\n【32】The main challenge of research on sedentary behavior is in defining it . This analysis focused on sitting time as a distinct sedentary behavior. We found that a 2-question method to assess sitting time demonstrated modest correlation with measurements of sedentary time by accelerometer ( _r_ \\= 0.32, _P_ < .001), which is low compared with correlations of other self-reported and objective measures but similar to those observed in the PA literature . In a sample of Belgian adults aged 65 or older (n = 508), similar results were reported for validation of self-reported sitting time using a 21-question instrument and accelerometer-assessed sedentary time . The authors reported, in comparing self-report to measured values, a correlation of _r_ \\= 0.30, a mean difference of 82 minutes per day, and wide LoA (−364 to 200 min/d) . The modest correlations in our and prior research may be because sitting is a subset of sedentary time, which includes a broader range of behaviors, such as reclining and lying down. This is reflected in the correlation data where the daytime correlation ( _r_ \\= 0.37) was larger than that observed for evening ( _r_ \\= 0.23) between self-report and accelerometer data.\n\n【33】The overall mean difference between self-report and accelerometer was 49 minutes per day, and wide LoA was observed in the Bland-Altman plot (−441 to 343 min/d). For population surveillance, cost and ease of administration are usually prioritized over more intensive and accurate measures, and questionnaires are often the only feasible method for assessing PA . Despite limitations to questionnaires for assessing PA accurately, they are useful in characterizing groups, providing rankings among subgroups, or both . Results from the plot corroborate this evidence and reinforce the notion that sitting time may be accurately assessed by self-report using the 2-question method for population surveillance but may be limited in accurately characterizing individual-level behavior.\n\n【34】NYC is among the most walkable cities in the nation. Recent analysis using an NYC-specific walkability scale that incorporated 5 equally-weighted components (residential density, intersection density, land use, subway stop density, and ratio of retail building floor area to retail land area) quantitatively illustrated the walkability of NYC; Manhattan in particular, was flagged as an area with high walkability . The geographic location of increased sitting time (ie, concentration in Manhattan) observed in this study is noteworthy and points to opportunities for intervention design. For example, given the high sitting times and favorable walking conditions in Manhattan, interventions that introduce breaks in sitting time with short walks are feasible. This finding highlights the potential for intervention through locating worksites in walkable neighborhoods in the United States and considering walkability in the design of new business districts. Similar intervention strategies could be implemented in other urban populations with comparable geographic concentrations of these characteristics.\n\n【35】In addition to promoting walking breaks among workers in walkable neighborhoods, other workplace interventions are being considered in NYC and elsewhere. A Cochrane review concluded that using a sit–stand desk with or without additional interventions such as information or counseling reduced sitting time by 113 minutes per workday; however, the quality of evidence was low, because the available studies had small numbers of participants and low-quality research design . Complementary to interventions in the workplace or during the day, strategies that target sitting time in the evening, such as encouraging movement during television watching, should be considered.\n\n【36】In NYC, promotion of stair use is also being used to provide opportunities for small bouts of physical activity throughout the day. The NYC Health Department has worked with the Department of Citywide Administrative Services to open stairwells in many publicly owned worksite buildings, coupled with posting of stair prompt signage encouraging people to “Burn Calories, Not Electricity. Take the Stairs!” Use of this signage is associated with stair use among occupants of multiple NYC buildings .\n\n【37】One of the main strengths of this analysis was the assessment of behaviors and exploration of validity of self-reported sitting time in an urban and diverse sample of adults. The availability of accelerometer data was an additional strength; however, the accelerometer is not able to distinguish between different domains of sedentary time  and may not be as accurate a method for validating self-report of sitting time as a device that distinguishes between sitting and lying down. However, given the absence of a “gold standard” for measuring sitting time, the accelerometer-assessed sedentary time may be considered as a fair proxy . Accelerometer technology has advanced to produce devices that distinguish between standing and lying down, but such devices may not always function properly. For instance, we set the model we used for this investigation to estimate standing versus lying down; however, we found that the setting did not work. Measurement of sedentary behavior is still a nascent field , and careful consideration should be given to the best required method for the specific sedentary behavior and question under study . Therefore, the focus on sitting time specifically was an additional strength of this study. Also, it may be easier to report and recall one specific sedentary behavior than multiple sedentary behaviors under one inclusive definition.\n\n【38】This study had limitations. Validation of a 2-item versus 1-item question assessing sitting time would have been more appropriately assessed by comparison with each other; however, standards of practice for improving a single question from an established instrument were applied by using a previously validated single-item question and by breaking up the period of recall into 2 periods. Although data were weighted to be representative of the NYC adult population, the small sample sizes of some subgroups (eg, Asian) limited external validity of findings in these groups. Citywide results, however, may be generalizable to other urban areas with similar urban landscapes and opportunities for walking and active transport. Finally, to successfully quantify sitting time, it has been suggested to incorporate both measured data and context-specific information . We did not have this level of information available for this study, given that the PAT Survey was designed for citywide surveillance across multiple domains of PA and active transport.\n\n【39】The 2-question method to quantify sitting time is limited in quantifying behavior accurately at the individual level but may be valuable for population-level surveillance over time. For population surveillance, particularly in settings with limited resources for conducting health surveys, assessing the accuracy of single or dual-item questions for characterizing behaviors is critical. Opportunities to decrease sitting time in urban areas with high walkability are implicated. Our findings are key for health intervention and policy planning focused on improving sitting-related health outcomes, distinct from interventions promoting other recreational and nonrecreational physical activity, with potential generalizability to other urban, adult populations.\n\n【40】Tables\n------\n\n【41】#####  Table 1. Demographic Characteristics of Participants, Physical Activity Transit Survey, 2010–2011\n\n| Characteristic | All Participants, Self-Report | Subset Participating in Accelerometer Component |\n| --- | --- | --- |\n| N (%) | Weighted a n (Weighted %) | N (%) | Weighted n (Weighted %) |\n| --- | --- | --- | --- |\n| **Total sample** | 3,597  | 6,196,000  | 667  | 5,980,000  |\n| **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** |\n| 18–24 | 262 (7.0) | 760,000 (13.4) | 40 (5.9) | 759,000 (12.9) |\n| 25–44 | 1,132 (31.5) | 2,375,000 (41.8) | 217 (32.6) | 2,479,000 (42.0) |\n| 45–64 | 1,383 (38.5) | 1,762,000 (31.0) | 291 (41.9) | 1,838,000 (31.1) |\n| ≥65 | 812 (22.6) | 789,000 (13.9) | 128 (19.6) | 832,000 (14.1) |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 1,470 (40.9) | 2,692,000 (47.3) | 263 (39.3) | 2,782,000 (47.1) |\n| Female | 2,127 (59.1) | 3,000,000 (52.7) | 404 (60.7) | 3,127,000 (52.9) |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 1,578 (43.9) | 2,083,000 (36.6) | 293 (44.2) | 2,120,000 (35.9) |\n| Non-Hispanic black | 851 (23.7) | 1,241,000 (21.8) | 179 (26.8) | 1,304,000 (22.1) |\n| Hispanic | 808 (22.5) | 1,509,000 (26.5) | 150 (22.4) | 1,524,000 (26.1) |\n| Non-Hispanic Asian | 285 (7.9) | 739,000 (13.0) | 32 (4.7) | 753,000 (12.7) |\n| Other | 75 (2.1) | 121,000 (2.1) | 13 (1.9) | 191,000 (3.2 b ) |\n| **Poverty/income c , % of federal poverty level** | **Poverty/income c , % of federal poverty level** | **Poverty/income c , % of federal poverty level** | **Poverty/income c , % of federal poverty level** | **Poverty/income c , % of federal poverty level** |\n| <200 | 1,288 (38.0) | 2,328,000 (43.4) | 220 (34.5) | 2,213,000 (38.7) |\n| 200–399 | 585 (17.3) | 853,000 (15.9) | 122 (18.8) | 1,135,000 (19.8) |\n| ≥400 | 1,315 (38.8) | 1,698,000 (31.6) | 275 (42.3) | 1,841,000 (32.2) |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| Less than high school | 445 (12.4) | 1,057,000 (18.6) | 56 (8.6) | 1,147,000 (19.4) |\n| High school | 893 (24.9) | 1,403,000 (24.7) | 153 (22.9) | 1,473,000 (24.9) |\n| Some college | 772 (21.5) | 1,339,000 (23.6) | 154 (23.2) | 1,346,000 (22.8) |\n| College graduate | 1,479 (41.2) | 1,880,000 (33.1) | 304 (45.4) | 1,943,000 (32.9) |\n| **Nativity d** | **Nativity d** | **Nativity d** | **Nativity d** | **Nativity d** |\n| US born | 2,232 (62.1) | 2,999,000 (52.7) | 463 (68.9) | 3,242,000 (54.9) |\n| Foreign born | 1,362 (37.9) | 2,687,000 (47.3) | 204 (31.1) | 2,667,000 (45.1) |\n| **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** |\n| Underweight/normal (<25.0) | 1,398 (39.1) | 2,470,000 (43.6) | 244 (37.1) | 2,499,000 (42.8) |\n| Overweight (25.0–29.9) | 1,221 (34.1) | 1,731,000 (30.6) | 231 (24.8) | 1,868,000 (32.0) |\n| Obese (≥30.0) | 959 (26.8) | 1,461,000 (25.8) | 189 (28.1) | 1,476,000 (25.3) |\n\n【43】a  Data weighted to be representative of the New York City adult population based on the 2006–2008 American Community Survey.  \nb  Estimate’s relative standard error (a measure of estimate precision) is greater than 30% or the sample size is less than 50, making the estimate potentially unreliable.  \nc  “Don’t know” category is not presented, so percentages do not sum to 100%.  \nd  Nativity was defined as self-reporting being born in the United States or elsewhere. Puerto Ricans and respondents born in other US territories were defined as being US-born.\n\n【44】#####  Table 2. Mean Self-Report of Sitting Times and Accelerometer-Measured Sedentary Times, by Demographic Characteristics, Physical Activity Transit Survey, 2010–2011\n\n| Characteristic | Self-Reported Sitting Time | Accelerometer-Measured Sedentary Time |\n| --- | --- | --- |\n| Mean min/d (95% CI) | _P_ Value a | Mean min/d (95% CI) | _P_ Value a |\n| --- | --- | --- | --- |\n| **Overall** | 423  | NA | 490  | NA |\n| **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** |\n| 18–24 | 405  | .14 | 449 b  | .40 |\n| 25–44 | 434  | 1 \\[Reference\\] | 480  | 1 \\[Reference\\] |\n| 45–64 | 417  | .23 | 501  | .21 |\n| ≥65 | 418  | .33 | 532  | <.001 |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male | 440  | 1 \\[Reference\\] | 490  | 1 \\[Reference\\] |\n| Female | 407  | <.001 | 490  | .96 |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 465  | 1 \\[Reference\\] | 513  | 1 \\[Reference\\] |\n| Non-Hispanic black | 441  | .11 | 494  | .18 |\n| Hispanic | 324  | <.001 | 428  | <.001 |\n| Non-Hispanic Asian | 471  | .75 | 550 b  | .20 |\n| Other | 448  | .66 | 475 b  | .02 |\n| **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** |\n| <200 | 375  | <.001 | 493  | .33 |\n| 200–399 | 421  | <.001 | 504  | .85 |\n| ≥400 | 492  | 1 \\[Reference\\] | 508  | 1 \\[Reference\\] |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| Less than high school | 328  | .01 | 430  | <.001 |\n| High school | 383  | <.001 | 500  | .03 |\n| Some college | 444  | <.001 | 467  | <.001 |\n| College graduate | 491  | 1 \\[Reference\\] | 535  | 1 \\[Reference\\] |\n| **Nativity c** | **Nativity c** | **Nativity c** | **Nativity c** | **Nativity c** |\n| US born | 454  | 1 \\[Reference\\] | 497  | 1 \\[Reference\\] |\n| Foreign born | 388  | <.001 | 481  | .32 |\n| **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** | **Body mass index category (kg/m 2 )** |\n| Underweight/normal (<25.0) | 437  | 1 \\[Reference\\] | 510  | 1 \\[Reference\\] |\n| Overweight (25.0–29.9) | 421  | .25 | 466  | .02 |\n| Obese (≥30.0) | 404  | .02 | 491  | .30 |\n\n【46】Abbreviations: CI, confidence interval; NA, not applicable.  \na  _P_ values determined using _t_ tests for proportions.  \nb  Estimate’s relative standard error (a measure of estimate precision) is greater than 30% or the sample size is less than 50, making the estimate potentially unreliable.  \nc  Nativity was defined self-reporting being born in the United States or elsewhere. Puerto Ricans and respondents in other US territories were defined as being US-born.\n\n【47】#####  Table 3. Beta Coefficients, Differences Associated with Self-Reported Sitting Time and Accelerometer-Measured Sedentary Time, Physical Activity Transit Survey, 2010–2011\n\n| Characteristic | Self-Reported % Sitting Time | Accelerometer-Measured % Sedentary Time |\n| --- | --- | --- |\n| β, min/d (95% CI) | _P_ Value a | β, min/d (95% CI) | _P_ Value a |\n| --- | --- | --- | --- |\n| **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** | **Age group, y** |\n| 18–24 | −3.3 (−42.3 to 35.6) | .87 | −0.21 (−49.3 to 48.9) | .95 |\n| 25–44 | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| 45–64 | −8.0 (−33.3 to 17.4) | .54 | 17.2 (−7.1 to 41.6) | .17 |\n| ≥65 | −3.6 (−36.1 to 28.9) | .83 | 60.4 (35.5 to 85.3) | <.001 |\n| **Sex** | **Sex** | **Sex** | **Sex** | **Sex** |\n| Male |  |  | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Female | −29.51 (−7.0 to −52.1) | .01 | −3.5 (−27 to 19.9) | .77 |\n| **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** | **Race/ethnicity** |\n| Non-Hispanic white | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Non-Hispanic black | 20.6 (−11.5 to 52.7) | .21 | −7.7 (−34.9 to 19.5) | .58 |\n| Hispanic | −66.3 (−96.1 to −36.5) | <.001 | −42.3 (−71.2 to −13.4) | <.001 |\n| Non-Hispanic Asian | 57.7 (14.6 to 100.7) | .01 | 42.7 (−6.2 to 91.5) | .09 |\n| Other | 10.3 (−63.5 to 84.2) | .78 | −13.0 (−47 to 21) | .45 |\n| **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** | **Poverty/income, % federal poverty level** |\n| <200 | −40.0 (−70.7 to −9.2) | .01 | 40.9 (11.7 to 70.1) | .01 |\n| 200–399 | −36.4 (−69.3 to −3.5) | .03 | 15.1 (−17.7 to 48) | .37 |\n| ≥400 | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| **Education** | **Education** | **Education** | **Education** | **Education** |\n| Less than high school | −94.9 (−136.7 to −53.2) | <.001 | −89.1(−129.5 to −48.7) | <.001 |\n| High school | −77.9 (−107.1 to −48.7) | <.001 | −37.8 (−65.8 to −9.7) | .01 |\n| Some college | −22.3 (−52.1 to 7.5) | .14 | −53.1 (−90.6 to −15.7) | .01 |\n| College graduate | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| **Nativity b** | **Nativity b** | **Nativity b** | **Nativity b** | **Nativity b** |\n| US born | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Foreign born | −43.7 (−68.1 to −19.2) | <.001 | −7.0 (−30.1 to 16.1) | .55 |\n| **BMI category (kg/m 2 )** | **BMI category (kg/m 2 )** | **BMI category (kg/m 2 )** | **BMI category (kg/m 2 )** | **BMI category (kg/m 2 )** |\n| Underweight/normal (<25.0) | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] | 1 \\[Reference\\] |\n| Overweight (25.0–29.9) | −5.7 (−32.8 to 21.4) | .68 | −14.2 (−40.9 to 12.5) | .30 |\n| Obese (≥30.0) | 1.4 (−27.5 to 30.3) | .93 | 2.3 (−22.1 to 26.6) | .86 |\n\n【49】a  _P_ values determined using linear regression analyses.  \nb  Nativity was defined as self-reporting being born in the United States or elsewhere. Puerto Ricans and respondents born in other US territories were defined as being US-born.\n\n【50】Mean Self-Reported Day and Evening Sitting Times, by Demographics, Physical Activity Transit Survey, 2010–2011. This file is available for download as a Microsoft Word document word icon \\[DOCX – 21.8 KB\\] .\n\n【51】Post-Test Information\n\n【52】To obtain credit, you should first read the journal article. After reading the article, you should be able to answer the following, related, multiple-choice questions. Credit cannot be obtained for tests completed on paper, although you may use the worksheet below to keep a record of your answers.org, please click on the “Register” link on the right hand side of the website to register. Only one answer is correct for each question. Once you successfully answer all post-test questions you will be able to view and/or print your certificate. For questions regarding the content of this activity, contact the accredited provider, CME@medscape.net . For technical assistance, contact CME@webmd.net . American Medical Association’s Physician’s Recognition Award (AMA PRA) credits are accepted in the US as evidence of participation in CME activities. The AMA has determined that physicians not licensed in the US who participate in this CME activity are eligible for _**AMA PRA Category 1 Credits™**_ . Through agreements that the AMA has made with agencies in some countries, AMA PRA credit may be acceptable as evidence of participation in CME activities. If you are not licensed in the US, please complete the questions online, print the AMA PRA CME credit certificate and present it to your national medical association for review.\n\n【53】Post-Test Questions\n\n【54】### Article Title: Self-Reported Sitting Time in New York City Adults, The Physical Activity and Transit Survey, 2010-2011\n\n【55】**CME Questions**\n\n【56】1\\.  You are seeing a 55-year-old business executive whose main complaint is that “my life is too stressful.” He has been working at least 6 days per week and puts in approximately 11 hours per day.  His workday consists of meetings and working on his computer, and he estimates that he is sitting for all but 1 hour per day. What can you tell this patient regarding sedentary behavior and health outcomes?\n\n【57】A. Sitting has been linked with cardiovascular mortality  \nB. Sitting closely shares demographic determinants with recreational physical activity  \nC. Sitting is associated with increased lipoprotein lipase activity in skeletal muscle  \nD. There is conclusive evidence that higher levels of sedentary activity promote negative health outcomes, even among adults who exercise frequently\n\n【58】2\\. What should you consider regarding data on sitting time in the current study as you evaluate this patient?\n\n【59】A. Mean self-reported sitting time was approximately 2 hours per day  \nB. Mean accelerometer-reported sitting time was approximately 8 hours per day  \nC. The correlation between self-reported and accelerometer-reported sitting time was strongest at night  \nD. Age and race/ethnicity were not significant factors in the correlation between self-reported and accelerometer-reported sitting time\n\n【60】3\\. Which of the following variables was **_most_** significantly associated with a higher mean duration of self-reported sitting time in the current study?\n\n【61】A. Higher educational attainment  \nB. Female gender  \nC. Hispanic ethnicity  \nD. Foreign-born status\n\n【62】4\\. Which of the following variables was **_most_** significantly associated with a higher mean duration of accelerometer-reported sitting time in the current study?\n\n【63】A. Older age  \nB. Male gender  \nC. Non-Hispanic black race  \nD. Normal weight vs overweight or obese\n\n【64】**Evaluation**\n\n| 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. | 1\\. The activity supported the learning objectives. |\n| --- | --- | --- | --- | --- |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. | 2\\. The material was organized clearly for learning to occur. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. | 3\\. The content learned from this activity will impact my practice. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |\n| 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. | 4\\. The activity was presented objectively and free of commercial bias. |\n| Strongly Disagree |  |  |  | Strongly Agree |\n| 1 | 2 | 3 | 4 | 5 |", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "35f66432-9568-4026-ac7d-40a25e1873f6", "title": "Identification of Residual Blood Proteins in Ticks by Mass Spectrometry Proteomics", "text": "【0】Identification of Residual Blood Proteins in Ticks by Mass Spectrometry Proteomics\nWithout transovarial or venereal transmission, a vector-borne pathogen’s persistence in nature depends on successful passage between \\> 1 species of vertebrate reservoirs. For Lyme borreliosis in eastern North America, black-legged tick ( _Ixodes scapularis_ ) larvae acquire _Borrelia burgdorferi_ from a reservoir host during their first blood meal. Infection persists through subsequent molts, and when a tick feeds for the second time as a nymph it may transmit infection to another competent reservoir or to a human. Reservoir hosts for _B_ . _burgdorferi_ are commonly white-footed mice but also include chipmunks, voles, shrews, and ground-foraging birds.\n\n【1】When a tick-borne agent has multiple reservoir hosts, assigning relative contributions of each species to maintenance of the pathogen in the environment may be difficult. One approach is to capture animals, sample blood or tissue for evidence of infection, and examine embedded ticks for the microorganism . However, this approach is labor- and resource-intensive, and sample sizes are limited. Greater statistical power could be attained with fewer resources if questing ticks were examined not only for infection but also for the source of the last blood meal because the tick would likely have acquired the infection from that vertebrate. If the tick were engorged, this would be straightforward with the PCR, as demonstrated in mosquitoes . However, host-seeking nymphal hard ticks are flat because their last blood meals were months earlier. Use of PCR to identify DNA of vertebrate mitochondria in ticks has been reported , but results lacked full sensitivity .\n\n【2】An alternative approach is to detect residual proteins from the blood meal. Uptake and retention of host immunoglobulin into the hemolymph of different species of ticks have been documented , and Venneström and Jensen found vertebrate actin in _I_ . _ricinus_ nymphs weeks after the molt . Given these observations, we hypothesized that sufficient host proteins remained in flat ticks for identification of blood meal by using proteins instead of DNA.\n\n【3】### The Study\n\n【4】We used mass spectrometry (MS)–based proteomics, as described by Breci et al. and Koller et al. Individual ticks or pools were pulverized after freezing in liquid nitrogen. Total proteins were precipitated in 95% ethanol at –20°C and recovered by centrifugation. Proteins of individual ticks were reduced with 100 mmol/L dithiothreitol, alkylated with 50 mmol/L iodoacetamide, digested with trypsin at a final concentration of 0.01 μg/μL, and filtered through a C18 cartridge before being subjected to liquid chromatography (LC) with a 5%–50% acetonitrile gradient in 0.1% formic acid, followed by tandem MS (LC-MS/MS; LTQ ThermoElectron, San Jose, CA, USA). Proteins of pooled ticks were separated by 1-dimensional sodium dodecyl sulfate–polyacrylamide gel electrophoresis, and gel slices were digested in situ with trypsin before using LC-MS/MS. Upwards of ≈7,000 spectra were submitted to a protein identification algorithm for each pooled sample. Sizes of sequenced peptides ranged from 5 aa to 34 aa. The SEQUEST search algorithm  with data-filtering criteria was used to identify sequence matches of output against databases of rabbit, sheep, deer, goat, mouse ( _Mus musculus_ ), and tick proteins.\n\n【5】We studied _I_ . _scapularis_ and the lone-star tick, _Amblyomma americanum_ , which is a vector of human monocytic ehrlichiosis in the United States. Ticks were provided by the tick-rearing facility of the Department of Entomology and Plant Pathology of Oklahoma State University (Stillwater, OK, USA). We examined pools of 15 _A_ . _americanum_ nymphs that had fed on sheep or rabbits as larvae and were 3 months postmolt. Predominant vertebrate peptides in all pools were α- and β-globin chains of hemoglobin and immunoglobulins. Sequences of these proteins corresponded to the source of the blood for the ticks. Other mammalian proteins detected in pools from ticks fed on sheep or rabbits were histone H3, histone H2, mitochondrial malate dehydrogenase, glyceraldehyde-3-phosphate dehydrogenase, mitochondrial ATP synthase, interferon regulatory factor, α tubulin, β tubulin, and transferrin.\n\n【6】We then studied individual ticks that had fed as larvae on mice ( _I_ . _scapularis_ ), rabbits ( _A_ . _americanum_ and _I_ . _scapularis_ ), or sheep ( _A_ . _americanum_ ) and examined flat nymphs of _A_ . _americanum_ at 7 months postmolt and _I_ . _scapularis_ at 3–11 months postmolt. We also examined _I_ . _scapularis_ adults (2 males and 1 female) that had fed as larvae and nymphs on mice and were 3–5 months postmolt. Concentrations of extracted proteins from individual ticks were 50–70 μg/tick. The Figure shows representative LC-MS/MS spectra of an _A_ . _americanum_ nymph that had fed on a sheep as a larva. Panel A shows the tandem mass spectrum for the singly charged peptide AAVTGFWGK, corresponding to residues 8–16 of sheep hemoglobin β-subunit (P02075). Panel B shows the tandem mass spectrum for doubly charged VKVDEVGAEALGR, corresponding to residues 17–29 of the same protein. These 2 peptides cover 15.2% of the protein sequence and differ from the orthologous sequence of rabbit (P02099) at 8 of 22 positions.\n\n【7】The Table summarizes results of all individual tick analyses. There was no correlation between number of proteins detected and postmolt period. Although some proteins, such as immunoglobulin and histone H3, were detected in both species, other proteins distinguished between _A_ . _americanum_ and _I_ . _scapularis_ . Globin chains were more commonly found in _A_ . _americanum_ than in _I_ . _scapularis_ nymphs. Cytochrome c-type heme lyase, which binds heme moieties and is transported from the cytoplasm to mitochondria of eukaryotes, was present in all samples of _I_ . _scapularis_ but not in _A_ . _americanum_ (2-sided p<0.001, by likelihood ratio). Peptides detected included those specific for the host animal for the blood meal.\n\n【8】### Conclusions\n\n【9】Digestion of a blood meal in ticks differs from what generally occurs in hematophagous insects. In ticks, digestion takes place gradually within cells of the intestinal tract after endocytosis, rather than by intraluminal enzymatic breakdown of blood cells and plasma components, as in insects . The combination of slow assimilation and uptake of some host proteins into hemolymph may explain persistence of host blood proteins for months after feeding and molting. Immunoglobulins in hemolymph have been demonstrated, but demonstration of several other proteins, including globin chains, histones, and mitochondrial enzymes, indicates that host protein persistence is not limited to 1 type of molecule.\n\n【10】The success of our study depended on access to a large database of protein sequences for sheep, rabbits, and laboratory mice. For most host species for these ticks in nature, such as the white-footed mouse ( _Peromyscus leucopus_ ), protein databases currently are much less extensive. Thus, a priority before application of this approach is MS-based sequencing of highly prevalent proteins from the blood of _P_ . _leucopus_ and other host species. Although LC-MS/MS of individual ticks is feasible and highly sensitive, its cost confines it to exploratory studies. High-throughput analysis of hundreds or thousands of specimens will likely require species-specific assays that use antibodies or aptamers for detection and identification of selected proteins.\n\n【11】Understanding contributions of different vertebrate hosts to pathogen maintenance is a prerequisite for effective monitoring, modeling, and disease prevention efforts that focus on natural reservoirs. Unfortunately, this level of understanding has not been broadly achieved. A major impediment to success in this area for most tick-borne zoonoses has been the absence of reliable and reproducible methods for identification of the vertebrate source of the infection for the tick vector by characterizing residual blood components. Our study shows a way to achieve this goal. Similar data on uninfected ticks would establish the denominator for prevalence studies and indicate the relative competence of different host species.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "be7125ad-a971-488b-adf3-ebc35bc07841", "title": "From Health Determinant Benchmarks to Health Investment Benchmarks", "text": "【0】From Health Determinant Benchmarks to Health Investment Benchmarks\n*   Essay\n*   Acknowledgments\n*   Author Information\n*   References\n\n【1】It is well established that the health system in the United States is underperforming. We lag most developed countries by a wide margin, despite spending more on health care . In addition, substantial geographic variation is seen in health outcomes in the United States, including unacceptable disparities in morbidity and mortality. Even absolute worsening of mortality rates in many US counties has been noted during the last several years .\n\n【2】Although we have better data systems than other countries have and a plethora of measurement efforts, these systems and efforts focus primarily on indicators or determinants of poor health, such as uninsured rates and smoking rates, or socioeconomic factors, such as income and education. This information on determinants — for health care and nonmedical determinants — is important, but it falls short: it does not tell us the level of financial investment needed to change each or all of the determinants and their resulting health outcomes. Population health improvement during the next decade requires a move from measuring only health determinants to developing investment benchmarks that can guide public and private action in communities and across the nation.\n\n【3】**Currently measured health outcomes and determinants.** For measuring broad population health, metrics such as _Healthy People 2020_  (for the nation), America’s Health Rankings  (for states), and County Health Rankings  (for counties) are useful for tracking progress and motivating change. In County Health Rankings, counties are ranked on an index of health outcomes and an index of the factors or determinants producing those outcomes. For each county in County Health Rankings, a “snapshot” provides national benchmarks for outcomes and determinants such as smoking rates and child poverty. These benchmarks are useful to local community public and private policy makers in choosing which priorities to tackle first.\n\n【4】**Moving to investment benchmarks.** Benchmarks for outcomes and determinants do not provide guidance on which new or additional population health investments are appropriate for a particular community. The poor performance of our health system compared with the health systems of other nations and the huge disparities in health outcomes in the United States must be the result of decades of varying levels of financial investment and health promotion in all policies. What guidance would trustees have for their decisions if a medium-sized community were to make available a pool of investment dollars, say $2 million per year for 10 years, from a public–private health outcome trust  or from reformed Internal Revenue Service community benefit policy aligned with the Community Health Needs Assessment process ? Why have we not added to our metrics reports useful benchmarks for optimal levels of per capita financial investment and policy strength across health-promoting factors to improve health determinants and ultimately outcomes?\n\n【5】It is largely because we have not yet made comparative effectiveness research on population health a high enough priority to enable us to know what the appropriate investment benchmarks would be. The Trust for America’s Health estimated in 2008 that investing $10 per person per year in proven community-based programs to increase physical activity, improve nutrition, and prevent smoking could save the United States more than $16 billion annually within 5 years .\n\n【6】In 2009, Kim and Jennings found that more generous education spending, progressive tax systems, and more lenient welfare program rules at the state level help to improve population health . In a cross-national analysis, Bradley and colleagues  argued that an important reason for the poor performance of the US health system is the relative proportion of non–health care social services spending to health care services spending; in other developed countries it is 2.00 to 1, whereas in the United States it is 0.91 to 1.\n\n【7】**Challenges to population health investment benchmarks.** Why do we know so little? It is true that going beyond simply describing differences to finding causal pathways is complicated. Methods and data sets to explore these relationships are limited. In addition, we lack comparable investment information across small units of population, such as communities and counties. Tim Casper and I examined the availability of such data in a sample of Wisconsin counties for per capita expenditures in selected categories of health care, public health, human services, income support, job development, and education. We found that even this well-resourced state is challenged in locating usable data and having adequate information technology systems, and it lacks enterprise-wide coordination and geographic detail in data collection efforts .\n\n【8】An additional challenge is that communities vary in their outcomes and population health investments, so optimal additional population health investments would be different in different places. Consider, for example, 2 overall healthy states, Utah and North Dakota. In Utah, new population health investments might be directed toward education or health insurance, 2 areas in which Utah has relatively low rankings, whereas North Dakota might focus most effectively on health behavior issues, an area in which this state has fallen behind.\n\n【9】**Moving ahead.** Although it will not be easy, it is time to focus our analytic and policy attention on such population health investment benchmarks. We need much more evidence of the effectiveness of different programs and policies, particularly on cost-effectiveness beyond effectiveness itself. Such investment benchmarks would be helpful to guide work in the many places where community health improvement discussions are under way. Benchmarks need not be prescriptive, but they would be a menu of the investments likely to produce the best health outcome improvements. They would help ensure that local passion and commitment is channeled in an evidence-based direction while preserving autonomy and sensitivity to community preferences.\n\n【10】Some might argue that developing benchmarks is too difficult a task. However, we should not let the perfect be the enemy of the good. Although challenging, developing investment benchmarks is an essential step if we are to reverse the poor performance of our health system, with its negative impact on our quality of life and national productivity. It is time to get serious and use big data from public and private sources, such as electronic medical records and school records, and advanced modeling techniques to provide population health investment benchmarks to improve overall heath and reduce unacceptable health disparities across our wealthy country.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "19c06620-16e1-4fd0-9443-f03da9dd69dd", "title": "Severe Fever with Thrombocytopenia Syndrome Virus among Domesticated Animals, China", "text": "【0】Severe Fever with Thrombocytopenia Syndrome Virus among Domesticated Animals, China\nSevere fever with thrombocytopenia syndrome virus (SFTSV) is a newly identified pathogenic member of the _Phlebovirus_ species in the family _Bunyaviridae_ , which in humans causes fever with thrombocytopenia syndrome (SFTS) . The common signs and symptoms of SFTS include high fever, gastrointestinal symptoms, thrombocytopenia, leukocytopenia, and multiorgan dysfunction with an average case-fatality rate of 10%–16%, according to the information system for disease control and prevention, Chinese Center for Disease Control and Prevention (China CDC). In severe SFTS cases, neural symptoms, hemorrhages, disseminated intravascular coagulation, and multiorgan failure can occur and may result in death .\n\n【1】SFTS has been reported in at least 13 provinces in the central, eastern, and northeastern regions of the People’s Republic of China. Most patients are farmers living in wooded, hilly, or mountainous areas, and the epidemic season is from March through November, with the peak incidence usually in June and July . Although _Haemophysalis longicornis_ ticks have been implicated as vectors of SFTSV , and high seroprevalence to SFTSV has been reported in goats _, the host range of the virus has not been determined. The role of domesticated animals in the circulation and transmission of SFTSV remains unclear.\n\n【2】To assess the prevalence of SFTSV infections in domesticated animals, combined cross-sectional and cohort studies were conducted in Laizhou and Penglai in Shandong Province. This study area was selected on the basis of its high incidence of human SFTS cases in 2010 . We report the detection of viral RNA and antibodies in sheep, cattle, dogs, pigs, and chickens. Our findings may provide valuable insights for understanding SFTSV ecology and transmission among animals and from animals to humans.\n\n【3】### Materials and Methods\n\n【4】##### Study Design and Sample Collection\n\n【5】Animal sampling took place in Laizhou (119°33′–120°18′E, 36°59′–37°28′N) and Penglai (120°35′–121°09′E, 37°25′–37°50′N), Shandong Province, China. The most common domesticated animal species in the region include sheep, cattle, dogs, pigs, and chickens . Animals of these species were sampled monthly in the villages where SFTS human cases were reported in 2010. Fifty to 100 animals of each species were sampled each month from April through November of 2011 and the serum samples were collected for detection of viral RNA, antibodies, and virus isolation. Goats were not surveyed because their populations are small in the sampling area.\n\n【6】Each sampled animal was marked with a unique label except for the pigs, which were sampled at the time of slaughter. Sampled animals were excluded in subsequent sampling events. In addition, to monitor new infections of SFTSV, a cohort of 38 sheep (Laizhou, n = 17; Penglai, n = 21) negative for SFTSV viral RNA and antibodies was established, and the animals were sampled every 10 days from June 20 through November 30. Serum samples were aliquoted and kept in cryovials in liquid nitrogen. Viral RNA and antibodies were first screened at the local CDC laboratories in Laizhou and Penglai, respectively, and the results were confirmed later in the national laboratory of China CDC. During the sampling period, serum samples from new SFTS patients in the study region in 2011 were also collected for virus isolation and genetic analysis.\n\n【7】##### ELISA\n\n【8】Antibodies in sheep, cattle, dogs, pigs, and chickens against SFTSV were detected by using a double-antigen sandwich ELISA described previously . In brief, a His-tagged affinity chromatography purified recombinant nucleocapsid (N) protein of SFTSV (strain HB29) expressed in _Escherichia coli_ was used as the coating antigen for 96-well plates, and horseradish peroxidase (HRP)–conjugated antigen was used for detection. Serum samples obtained from 46 sheep, 55 cattle, 30 dogs, 55 pigs, and 50 chickens originating from either Beijing or Hebei provinces, where no human SFTS cases had been reported, were used as negative controls, and convalescent-phase SFTS patient serum samples were used as positive controls. All serum samples including negative controls were tested at a dilution of 1:10. After 96-well plates were coated with purified N protein (0.2 μg/well), serum samples were added, followed by HRP-conjugated N protein. TMB peroxidase substrate (3,3′,5,5′tetramethylbenzidine and hydrogen peroxide \\[H 2  O 2  \\]) was used for color development, and substrate conversion was measured by using a DTX 880 multimode detector (Beckman Coulter, Brea, CA, USA) with an incidence wavelength of 450 nm and a reference wavelength of 620 nm.\n\n【9】Each sample was tested in triplicate within the same test run and mean optical density (OD) values for each sample were converted to a percentage of the positive control (PP) values by using the following equation: (mean of net OD of test sample/mean of net OD of positive control) × 100. The PP value frequency distribution resulting from analyses of serum samples from control animals was determined to be normal by using the χ 2  goodness-of-fit test. Cutoff values were determined by adding 3 × SD to the mean of the PP values obtained from analyses of negative control serum samples. The resulted cutoff values were 15.6, 15.3, 14.4, 13.7, and 14.2 for sheep, cattle, dog, pig, and chicken serum samples, respectively. A sample was considered positive if the PP value was above the cutoff threshold. For the detection of antibodies in a dog that was found positive with viral RNA in the blood, an indirect IgG ELISA was used. Briefly, the plate was coated with purified N protein as described, 2-fold dilutions of the serum samples were added, followed by HRP-conjugated anti-dog IgG (Sigma, Saint Louis, MO, USA). Cutoff values for the assay were determined by adding 3 × SD to the mean of OD values resulting from analyses of negative control serum samples from non-infected dogs. Endpoint titers were expressed as the reciprocal of the highest dilution of the serum samples.\n\n【10】##### Neutralization Assay\n\n【11】A microneutralization assay was performed to detect neutralizing antibodies against SFTSV as described . Serial dilutions of sheep serum samples collected at various time points were mixed with an equal volume of 100 50% tissue culture infective doses of SFTSV (strain HB29), and the mixture was incubated at 37°C for 1.5 h. The mixture was then added to cultured Vero cells in a 96-well plate in quadruplicate. The plates were incubated at 37°C with 5% CO 2  for 7 days. Viral infection was assessed by an immunofluorescence assay with mouse polyclonal antibodies against SFTSV. End-point titers were expressed as the reciprocal of the highest dilution of the serum that prevented infection.\n\n【12】##### TaqMan Quantitative Real-Time Reverse Transcription–PCR\n\n【13】A TaqMan quantitative real-time reverse transcription PCR (qRT-PCR) was performed on all animal serum samples by using a certified real-time RT-PCR kit for clinical diagnosis . This kit targets the small segment of SFTSV, with 98.6% sensitivity and 99.1% specificity as described . Viral RNA copy numbers were determined from amplification of a standard curve of positive control RNA, and the cutoff cycle threshold (Ct) value for a positive reaction was set at 35 cycles. The detection limit for this kit is 50 RNA copies (5 μL/test) or 10 copies/μL. A result was considered positive if a sample had a Ct value below the cutoff.\n\n【14】##### Virus Isolation and Sequence Analysis\n\n【15】Serum samples collected from animals or SFTS patients in acute phase, newly occurred in the study region in 2011, were inoculated onto monolayers of Vero cells for virus isolation as described . Cells were cultured at 37°C in an incubator supplied with 5% CO 2  with media changes done twice per week. The isolated virus was sequenced and subjected to phylogenetic analysis by using the neighbor-joining method with MEGA version 5 , and compared with previous published viral sequences of SFTSV strains isolated from 11 SFTS patients and _H. longicornis_ ticks in 2010 .\n\n【16】##### Statistical Analyses\n\n【17】The rates of SFTSV RNA and antibody detection in Penglai and Laizhou were calculated monthly by animal species, and the statistical significance was analyzed by using either the Pearson χ 2  or continuity correction χ 2  test. Analyses were performed by using the SPSS software version 16.0 (IBM, Armonk, NY, USA). Viral RNA copies among animal species were analyzed by using the Kruskal–Wallis test, performed by using GraphPad Prism software (ver. 5.0; GraphPad Software, Inc. San Diego, CA, USA). Values of p<0.05 were considered statistically significant.\n\n【18】##### Ethical Considerations\n\n【19】According to the medical research regulation of the ministry of health, all studies involved in human samples were reviewed and approved by the ethics committee of China CDC, which uses international guidelines to ensure confidentiality, anonymity, and informed consent. Written informed consent was provided by the patients. The protocols for animal sampling have been approved by the animal care committee of China CDC.\n\n【20】### Results\n\n【21】##### SFTSV Infection in Domesticated Animals by Month\n\n【22】We sampled 472 sheep, 842 cattle, 359 dogs, 839 pigs, and 527 chickens in Laizhou and Penglai counties to assess the prevalence of SFTSV RNA and antibodies. Our results showed that 3.8% (18/472) of sheep, 4.2% (35/842) of cattle, 5.3% (19/359) of dogs, 2.6% (22/839) of pigs, and 1.7% (9/527) of chickens were viral RNA-positive . The monthly positive rates of viral RNA in sampled animals varied from April to November, ranging from 2% (n = 51) to 12.7% (n = 55) in sheep, 2.1% (n = 51) to 13.2% (n = 53) in cattle, 0 (n = 53) to 12.5% (n = 40) in dogs, 0 (n = 51) to 19.1% (n = 47) in pigs, and 0 (n = 40) to 13.5% (n = 37) in chickens, in which n is referred as to denominator of all animals sampled in that month . Although the overall positive rates for SFTSV varied between the 2 counties, the variance was not statistically significant (p>0.05). The median RNA concentration was 2.0 × 10 4  copies/mL (95% CI: 1.6–2.6 × 10 4  ) in sheep; 2.0 × 10 4  copies/mL (95% CI: 1.5–2.9 × 10 4  ) in cattle; 2.1 × 10 4  copies/mL (95% CI: 0.96–4.7 × 10 4  ) in dogs; 1.7 × 10 4  copies/mL (95% CI: 1.4–2.1 × 10 4  ) in pigs; and 1.7 × 10 4  copies/mL (95% CI: 1.0–2.7 × 10 4  ) in chickens , and the differences among the studied animal species were not statistically significant (p = 0.87) .\n\n【23】Viral antibody prevalence was significantly higher (p<0.005) in sheep (69.5%, 328/472), cattle (60.4%, 509/ 842), dogs (37.9%, 136/359), and chickens (47.4%, 250/527) than in pigs (3.1%, 26/839) . As shown in Figure 2 , monthly positive rates of antibodies in sheep varied from 30% (n = 30) to 80.4% (n = 51) in Laizhou County, and 48% (n = 50) to 100% (n = 82) in Penglai County during the sampling period; monthly positive rates in cattle varied from 37.5.3% (n = 48) to 66.3% (n = 80) in Laizhou, and from 31.3% (n = 48) to 78.7% (n = 87) in Penglai. In dogs, monthly positive rates ranged from 34.2% (n = 41) to 42.6% (n = 47) in the 2 regions, and the variation between sampling months was not statistically significant. Seroprevalence in pigs was low, and SFTSV antibodies were not detected in pig serum samples collected in August and September (n = 181). Monthly seroprevalence ranged from 28% (n = 25) to 72.7% (n = 44) in chickens in the regions.\n\n【24】##### SFTSV Infection in a Cohort of 38 Sheep\n\n【25】In addition to the 472 sheep described above, a cohort of 38 sheep was monitored for SFTSV seroconversion during a 6-month period. As shown in Figure 3 , seroconversion occurred in the cohort during the surveillance period, with an increasing seroprevalence that peaked at 76.5% (n = 17) in Laizhou and 100% (n = 21) in Penglai in November. In this cohort, 17 sheep were viral RNA positive either once or twice (6/17) over the sampling period. Viral RNA was mainly detected in August and September, and serum virus RNA quantities ranged from 10 4  copies/mL to 1.7 × 10 5  copies/mL (Sheep PKG-13). In 5 sheep, viral RNA appeared before seoconversion; but in 2 sheep (PSL-15, LHJ-19), viral RNA and SFTSV antibodies were detected on the same sampling day. In 10 of 17 sheep, viral RNA was detected once or twice after seroconversion .\n\n【26】Neutralizing antibody levels were also measured in 17 sheep serum samples on day 1 (June 21) and on the day when viral RNA was first detected, and again at a later stage (October 20). All tested samples were negative for neutralizing antibodies on day 1 but were positive at later times . No visible clinical signs of infection were detected in the cohort of sheep over the testing period regardless of antigen or antibody status.\n\n【27】##### Course of SFTS Virus Infection in a Dog\n\n【28】During the testing, we found 1 dog with a high viral RNA load (1.7 × 10 7  copies/mL) and no antibodies. The dog was isolated, examined, and sampled daily from day 8 through day 22 and a sampled once more on day 90 . This dog had detectable viral RNA on days 8 and 10; levels declined thereafter, and levels were undetectable levels on day 12. Concurrently, SFTSV IgG titers reached 1:2,560 on day 10 and remained at that level through day 90. No signs of illness were observed during the entire period of isolation.\n\n【29】##### Virus Isolation and Sequence Analysis\n\n【30】Virus isolation was attempted on all viral RNA–positive serum samples (n = 103), but isolates were successfully obtained from only 1 sheep (Laizhou), 1 cattle (Laizhou), and 1 dog (Penglai), named SDLZSheep01/2011, SDLZCattle05/2011, and SDPLDog01/2011, respectively. In addition, 10 viral isolates were obtained from SFTS patient serum samples collected in 2011, 9 were from Laizhou (SDLZP01–09/2011) and 1 was from Penglai (SDPL01/2011). Phylogenetic analysis of the S segment of these SFTSV isolates indicated that the viral isolates from sheep, cattle, and dog are genetically close to the 10 SFTS patient-derived isolates in 2011 from the study areas and also to the previous published sequences of viral isolates from 11 SFTS patients reported in 2010 in 6 provinces and _H. longicornis_ obtained in 2010 in Laizhou . Pairwise distances analysis showed that all sequences of the isolates from domesticated animals, human patients, and _H. longicornis_ shared more than 95% identity, which demonstrated a close evolutionary relationship among those SFTSV isolates from domesticated animals, ticks, and SFTS patients.\n\n【31】### Discussion\n\n【32】In our study of >3,000 serum samples collected from 5 species of domesticated animals in 2 SFTS-endemic counties of Shandong Province, we provided evidence of natural infection and circulation of SFTSV in these animals. We found especially high (>60%) seropositivity of SFTSV-specific antibodies in sampled sheep and cattle, while almost one-half of chickens and one third of dogs tested were seropositive. We detected SFTSV RNA in all 5 species of domesticated animals at a level ranging from 1.7% to 5.3%. A cohort study with 38 sheep and a follow-up study of an infected dog provided insights into the kinetics of natural SFTSV infections in these species. Moreover, the isolation of SFTSV from a sheep, cattle, and a dog provided evidence of SFTSV viremia in domesticated animals.\n\n【33】A previous study in Yiyuan County, Shandong Province, showed a high seroprevalence in goats (83%) , and another serosurvey of domesticated animals conducted in Jiangsu Province found SFTSV antibody– positive rates of 57% in goats, 32% in cattle, 6% in dogs, 5% in pigs, and 1% in chickens but no antibodies in geese and mice . Our investigation, conducted in 2 other SFTS-endemic regions in China , demonstrates a high seroprevalence of SFTSV in sheep (69.5%), cattle (60.4%), dogs (37.9%), and chickens (47.4%), but low rates in pigs (3.1%). Results of seroprevalence of SFTSV infection were consistent and especially high in goats and sheep but low in pigs in these studies. However, our data showed a higher seropositivity of SFTSV-specific antibodies in cattle, dogs, and chickens than the previous report in Jiangsu Province.\n\n【34】In addition, we also found that the SFTSV RNA was detected in all sampled animal species, but the prevalence was low, ranging from 1.7% to 5.3%, which indicates the potential viremia in these animals. The differences in the rates of SFTSV infections among various animal species and regions were statistically significant, perhaps because of varying degrees of exposure to virus-infected ticks or differences in host susceptibility to SFTSV infection. The sheep and cattle in our study regions were grazed on pastures or hills during the day and kept in household backyards at night; the dogs and chickens roamed freely in fields. Pigs were the only animals that did not roam freely. We found that the animals in this study, particularly the sheep and cattle, were heavily infested with ticks. _H. longicornis_ , which has been implicated as a vector of SFTSV, is the dominant tick species in these areas. SFTSV RNA had been detected by using a method of real-time RT-PCR from _H. longicornis_ ticks collected from grass, cattle, and sheep with a detection rate of ≈2% in disease-endemic areas in previous studies , and a viral isolate was obtained from _H. longicornis_ ticks on sheep from the same study area of Laizhou in 2010 . Phylogenetic analysis of the virus isolates obtained from the sheep, cattle, and dog in this study and from _H. longicornis_ ticks in our previous study  showed >95% homology with SFTSV isolates obtained from patients from the same region, which suggests a potential link of SFTSV infections among humans, domesticated animals, and ticks.\n\n【35】Infective virus, a competent insect vector, and susceptible vertebrate hosts must coexist to establish and maintain arbovirus transmission cycles . Threshold viremia levels of 10 2.0–4.7  50% lethal dose/mL have been reported as sufficient for the infection of ticks with Colorado tick fever virus, Russian spring-summer encephalitis virus, and louping ill virus . In our study, we detected viral RNA in all domesticated animal species examined, although the viral RNA copy numbers were low (<10 5  copies/mL). The individual dog in which the clinical course of infection was followed showed a course of acute infection typical of a bunyavirus infection, including the stages of viremia and antibody response. Successful isolation of SFTSV from a sheep, cattle, and a dog confirms the occurrence of SFTSV viremia in these domesticated animals. However, only 3 isolates were obtained from 103 SFTSV RNA–positive serum samples, which may indicate that most infected domesticated animals might have either a low level of viremia, a short period of viremia, or that few infectious virions might be present under the condition of high level of serum antibodies.\n\n【36】During the cohort study of 38 sheep, seroconversion was observed, indicating ongoing circulation of SFTSV in the endemic areas. In 17 of 38 sheep, neutralizing antibodies were observed in the viral RNA in the blood. Although more evidence is needed to illustrate the underlying mechanisms, this case is not the only case of _Bunyaviradae_ virus infection. A persistent infection of hantavirus can be established in the rodent reservoir, lasting several months or years with high titers of neutralizing antibodies in the serum . Persistent infection of SFTSV in animals has not been shown to date, and a more sensitive nucleotide acid detection method may help to define whether SFTSV infection is prolonged in domestic animals. However, the evidence suggests that SFTSV is circulating among animals and between animals and humans in the endemic areas.\n\n【37】The results of this study are subject to several limitations. First, only 3 viral isolates (1 from each animal species) were obtained and sequenced, which limits the ability to generalize the representativeness of SFTS viral diversity in these areas. Second, SFTSV infections in ticks on animals were not systematically studied to evaluate their relation to SFTSV-infected status or with seroconversion of studied animals, which makes it difficult to associate the infection of SFTSV in ticks directly with that in animals and human patients. Third, we cannot determine whether the virus is pathogenic to the domesticated animals in this study because no adequate examinations have been taken place.\n\n【38】In conclusion, we have provided evidence to show that SFTSV is circulating among several species of domesticated animals and between animals and humans in disease-endemic areas of China. The domesticated animals with high infection rates of SFTSV may act as amplifying hosts for the virus during the epidemic season. More studies are needed to elucidate the SFTSV transmission model in nature, which includes determining the duration of viremia levels and the possibility of persistent infection in each animal species, potential reservoirs, and links of SFTSV infections among humans and domesticated animals. These findings will help formulate effective measures for containing the infection of this emerging pathogen.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "879ea96c-81bf-4ddc-8b0d-18a564e1bb44", "title": "Opportunities for Increased Physical Activity in the Workplace: the Walking Meeting (WaM) Pilot Study, Miami, 2015", "text": "【0】Opportunities for Increased Physical Activity in the Workplace: the Walking Meeting (WaM) Pilot Study, Miami, 2015\nAbstract\n\n【1】**Introduction**\n\n【2】Despite the positive impact walking has on human health, few opportunities exist for workers with largely sedentary jobs to increase physical activity while at work. The objective of this pilot study was to examine the implementation, feasibility, and acceptability of using a Walking Meeting (WaM) protocol to increase the level of work-related physical activity among a group of sedentary white-collar workers.\n\n【3】**Methods**\n\n【4】White-collar workers at a large university were invited to participate in a newly developed WaM protocol. Workers who conducted weekly meetings in groups of 2 or 3 individuals were recruited for the pilot study (n = 18) that took place from January 2015 to August 2015. Seventeen participants wore an accelerometer to measure physical activity levels during 3 consecutive weeks (first week baseline, followed by 2 weeks of organized WaMs) and participated in focus groups conducted during week 3 to document experiences with the WaM protocol.\n\n【5】**Results**\n\n【6】The WaM protocol met study criteria on feasibility, implementation, and acceptability among study participants. The average number of minutes (standard deviation) participants engaged in combined work-related moderate/vigorous physical activity per week during the 3 weeks increased from an average of 107  minutes during the baseline week to 114  minutes at week 2 and to 117  minutes at week 3.\n\n【7】**Conclusion**\n\n【8】White- collar workers were supportive of transforming regular seated meetings into walking meetings and increased their work-related physical activity levels.\n\n【9】Introduction\n\n【10】Prolonged periods of time sitting may increase risk of premature death, have associations with health problems, and compromise metabolic health . Walking decreases risk for all-cause mortality because of its positive impact on physical health . Engaging in moderate exercise, which includes brisk walking, for as little as 15 minutes per day can add up to 3 years of life expectancy . The American Heart Association recommends walking for at least 30 minutes per day to reduce the risk for chronic diseases such as osteoporosis, breast and colon cancer, and type 2 diabetes . The Centers for Disease Control and Prevention (CDC) recommends walking at least 10,000 steps per day . A recent study documented that 3 brief sessions of higher-intensity physical activity (PA) of at least 10 minutes throughout the day has benefits for physical health . Despite the positive impact walking has on human health, few opportunities exist for workers with largely sedentary jobs to increase PA while at work.\n\n【11】Professional white-collar work environments typically do not support PA throughout the workday ; blue-collar workers report greater physical demands than do white-collar workers . Finding ways to incorporate PA into white-collar work settings could benefit many workers. One way to incorporate and encourage PA is by walking during a meeting instead of sitting. No studies have yet evaluated the use of walking meetings in the workplace. A recent Cochrane review examining interventions to reduce sitting in the workplace noted that no studies yet have investigated the effect of periodic breaks or standing or walking meetings . The objective of our pilot study was to examine the acceptability among white-collar workers of engaging in walking meetings during the workday and the feasibility of organizing these meetings around their work schedules. We also examined the implementation of the Walking Meeting (WaM) protocol and assessed the impact of the protocol on work-related PA levels in a sample of white-collar workers.\n\n【12】Methods\n\n【13】A sequential explanatory mixed-methods study design was implemented to develop the WaM protocol and then assess the feasibility, acceptability, and implementation of the protocol among a sample of white-collar workers during an 8-month period (January 2015–August 2015) at a university in an urban setting in southern Florida. We recruited by email and obtained consent from 18 university faculty members, staff leaders, and staff members (8 groups of 2 or 3 members) to participate in a 1-week baseline assessment and 2-week walking meeting protocol. One participant lost an accelerometer and was released from the study, so the final analytic sample was 17 participants.\n\n【14】Eligibility criteria for study participation included groups that had an established meeting time and met at least weekly for 30 or 60 minutes. Study participants had to be aged 18 years or older, full-time university employees, able to wear an accelerometer at the hip for 3 consecutive work weeks, able to read and write in English, not absent from work for more than 1 workday during study participation, and able and willing to walk for 30 minutes during the group’s regular meeting time. Individuals who self-reported chest pain; prior myocardial infarction; cardiopulmonary, spinal or lower limb surgery in previous 6 months; or a recent history of uncontrolled hypertension were not eligible to participate.\n\n【15】Accelerometers were calibrated at baseline, and the WaM protocol was created and then delivered to participants in a workshop on the first day of the baseline week. Each group was instructed to carry on with their traditional sitting meetings and regular work schedule during the first week of baseline data collection. After the baseline week, each group was asked to modify one standard sitting meeting according to the WaM protocol for 2 consecutive weeks. At the start of the study and at the end of each of the 3 weeks of participation, researchers met with each group to administer a survey that collected self-reported PA measures. At the end of both walking meeting implementation weeks, accelerometers were collected, the last survey was administered, and a focus group meeting was conducted with each walking group. The WaM pilot study was approved by the institutional review board of the University of Miami.\n\n【16】### Development of the WaM protocol\n\n【17】We developed a 7-item core component walking meeting protocol that included a safe 25- to 30-minute walking path on the university campus. To develop the WaM protocol, we conducted formative research on traditional meeting protocols. For example, every traditional meeting has an agenda, an objective, and someone who directs the meeting. Next, we reviewed research on how walking meetings are conducted. The mainstream literature outlined a common approach to conducting a walking meeting, and the grey literature provided anecdotal tips and instructions that suggested a similar setup and design (ie, create an agenda, assign roles, take notes, and so on) . From this formative research on traditional and walking meetings , common themes were combined and outlined into a 7-item core component list that reflects best practices for walking meetings and serves as the foundation for a uniform WaM protocol (Box). The WaM protocol was assessed across a 3-week period: the first week was a baseline observation period for each participant’s typical work-related PA levels, and the subsequent 2 weeks were observation periods for each participant’s walking meeting–related PA levels.\n\n【18】Box. Core Components of the Walking Meeting Protocol in the Walking Meeting (WaM) Pilot Study\n\n【19】*   *   Set a time and place to meet before your WaM.\n\n【20】    *   Create an agenda for your WaM.\n\n【21】    *   To make the walk more comfortable, bring items such as water, sunglasses, and sunscreen. Wear comfortable shoes.\n\n【22】    *   Have the group leader assign roles to each walking meeting group member. (ie, time checker, note taker, path leader).\n\n【23】    *   Follow the prescribed route.\n\n【24】    *   Walk for at least 30 minutes.\n\n【25】    *   After the walking meeting, sit and conclude to wrap up meeting; take care of paperwork or other tasks that could not be accomplished during WaM.\n\n【26】### Study measures (survey and accelerometry)\n\n【27】After recruitment and consent, study participants were asked at baseline to complete a paper-based survey with questions on sociodemographic characteristics and standardized measures of self-reported PA (the International Physical Activity Questionnaire \\[IPAQ\\]) . Although we administered the entire IPAQ to study participants, for this analysis we used only the 6 work-related PA questions. The identical survey measures were administered 3 more times: during the first week of the walking meeting, during the second week of the walking meeting, and during the focus groups. Participants were asked at baseline to wear a GT3XP Actigraph accelerometer (ActiGraph, LLC) on their waist for 21 consecutive days. On the first day, the accelerometer was initialized by the research team and participants were shown how to attach and remove the device. Participants were instructed to wear the accelerometer all day except while they were sleeping, taking a shower, or swimming. Participants also kept a daily log throughout the 3 weeks of the pilot study documenting the beginning and end of work shifts along with times they wore the accelerometer and when they removed it. The accelerometers recorded data in 1-minute intervals, providing the number of counts for each minute for 21 days. A count was defined as any activity that was measured by the accelerometer. At the end of 21 days, the researchers retrieved the accelerometer from the worker and downloaded its data to the study computer.\n\n【28】Accelerometer data were parsed by using ActiLife software version 6 (ActiGraph, LLC) into 2 data sets: a set of minutes associated with the workday and a set of minutes associated with the non-workday. For minutes associated with the workday, each minute was assigned a level of PA intensity according to definitions established by Freedson and colleagues : light (101–1,952 counts per minute), moderate (1,953–5,724 counts per minute), vigorous (5,725–9,498 counts per minute), and very vigorous (≥ 9,499 counts per minute). PA levels were analyzed on the day of the week each group had its walking meeting. For example, if a group held its walking meeting on Wednesday during week 2 and week 3, we collected data on the number of steps taken on those implementation days and on the previous Wednesday of their baseline week.\n\n【29】Measures of WaM protocol implementation, feasibility, and acceptability were defined . Focus group sessions were conducted to ask participants questions about the extent to which the WaM protocol was implemented, whether the WaM protocol was feasible, and whether it was acceptable to study participants.\n\n【30】### Focus groups\n\n【31】To complement our survey data, we conducted a 1-hour focus group with each walking group at the end of the pilot study to discuss 3 broad domains: working experience and job organization, organizing the walking meeting, and conducting the walking meetings. The focus group team consisted of a moderator and another researcher who collected paperwork and took notes. The focus group questions were developed after collecting the quantitative data and were based on the WaM protocol. We used grounded-theory methodology  to develop a discussion guide, which consisted of a mix of 4 to 7 open-ended questions for each domain.\n\n【32】### Statistical analysis\n\n【33】To examine how the walking meetings affected objectively measured PA, we compared accelerometry data for the day of the baseline week with the accelerometry data for the 2 days of the walking meetings. Four levels of PA were examined: light, moderate, vigorous, and very vigorous. CDC’s guidelines for adequate PA suggest that aerobic activity of moderate intensity or higher (vigorous or very vigorous) is important for health benefits . Therefore, we combined moderate, vigorous, and very vigorous PA levels into one category to examine changes in PA across the 3-week study period. Using STATA version 13.0 (StataCorp LP), we calculated mean (standard deviation \\[SD\\]) number of minutes spent during the workday for each walking meeting day and the corresponding baseline day. Differences between periods were assessed by using paired _t_ tests. We used conservative 2-tail tests and emphasize that this descriptive study was not designed to determine cause and effect.\n\n【34】Focus group discussions were transcribed, and text was entered into NVivo version 10 (QSR International Pty Ltd) for electronic coding, data retrieval, and analysis. Inductive thematic analysis was used to identify main themes across groups. Emerging themes were verified through discussion, and a coding framework based on the themes and domains was developed . Transcripts were coded by H.E.K. and reviewed by the study team. The coding was reviewed by 2 coauthors (A.J.C-M. and X.Y.); existing codes were developed and new codes were identified.\n\n【35】Results\n\n【36】The 17 participants had a mean age of 39.8 years (SD, 12.2 y); 4 were men and 13 were women; 4 were white and 13 of another race; 12 self-identified as Hispanic. Mean body mass index was 25.8 kg/m 2  . Two participants completed some college or technical school, 9 were college graduates, and 6 had a master’s degree or higher. Seven team leaders who were invited to participate expressed interest in the study but did not meet eligibility requirements. The overall response rate was 53% (8/15).\n\n【37】Initially, the sample comprised 8 team leaders, 2 groups of 3 members, and 6 groups of 2 members; one participant eventually dropped out. Of the 8 groups, all completed at least one walking meeting and 7 groups completed both walking meetings. Taken together, the study enrollment, response rate (17/18 participants, or 94%), and completion of walking meetings (7/8 groups, or 88%) suggest that walking meetings were feasible for this sample of white-collar workers.\n\n【38】All participants agreed to the WaM strategy after being taught how to organize and conduct the walking meeting during study initiation. Although the protocol suggested 30 minutes as a minimum for the walking meeting, all groups walked from 30 to 40 minutes. Five groups followed the prescribed path. Of the 3 groups that did not follow the path, 2 groups took a different route to accomplish tasks at other campus locations during their walking meeting; the third group took a wrong turn but later rejoined the prescribed path. All groups agreed that the WaM protocol was acceptable according to the measures defined , including the suitability of walking meetings for their work setting.\n\n【39】When we asked focus group participants about the extent to which they implemented the components of the WaM protocol, 7 groups created and printed an agenda to take on their walk, 3 groups took written notes, and one group engaged in a sit-and-conclude session after their walk. All groups used their established meeting times and places for their walking meetings, and all groups felt they had proper attire and items to make their walk comfortable. Six groups completed 5 of 7 prescribed WaM protocol components. The 2 components least frequently completed were the sit-and-conclude session and creating an agenda.\n\n【40】The average number of minutes (standard deviation) participants engaged in combined work-related moderate/vigorous physical activity per week during the 3 weeks increased from an average of 107  minutes during the baseline week to 114  minutes at week 2 and to 117  minutes at week 3. The mean (SD) number of minutes in light PA on walking-meeting days among all participants decreased from 169.8 (83.3) minutes at baseline to 129.2 (62.1) minutes at week 3 ; however, the differences between baseline and each follow-up week were not significant. On the day of the walking meeting, the mean number of minutes spent in moderate, vigorous, or very vigorous PA increased from 34.1 minutes at baseline to 43.5 minutes at week 2 ( _P_ \\= .31) and to 43.0 minutes at week 3 ( _P_ \\= .33 for difference from baseline). Nine participants did not meet the 10,000 steps goal during baseline week; by week 3, two of these 9 participants reached 10,000 steps.\n\n【41】When asked about their typical meeting at work, participants had the following comments: “Sometimes people come into our meeting. Sometimes our meetings get cut short. Sometimes we have to come back to our meeting” \\[Group 4, participant 1\\]. “\\[We\\] talk about personal stuff and other study-related stuff. Scheduling, vacation, coverage, etc. They (the meetings) are very informal” \\[Group 7, participant 1\\]. “Most meetings go overtime” \\[Group 1, participant 1\\].\n\n【42】When asked how teams organized their walking meetings, participants stated the following:\n\n【43】> Often times, for our meetings, I know I’m going to be in front of a computer. So, I kinda multitask during our meetings. So I really had to set time for these walking meetings versus plan to be in front of my computer, talking to \\[another participant\\] and doing something else, which is rude but you know, it’s reality” \\[Group 4, participant 2\\].\n\n【44】“We both agreed on a time. Scheduled it on our calendar” \\[Group 3, participant 1\\]. “Made mental bullet points. Didn’t take paper with us” \\[Group 2, participant 1\\].\n\n【45】When asked about their general experience at engaging in walking meetings, participants stated, “Being outside with other people, you’re never really alone, per se, and you never feel as if you are this small little bubble doing some random thing, like you’re in the world and you’re doing something” \\[Group 5, participant 2\\]. “Today I used it as a way of letting go of my stress” \\[Group 5, participant 3\\].\n\n【46】> I loved it \\[walking meeting\\]. It was very energizing, very invigorating. We got a lot done, we went through our agenda completely, efficiently, and it helped us generate ideas as we were discussing a topic. We were generating ideas for solutions and came up with solutions and tasks, like who’s going to do what. I was incredibly efficient” \\[Group 6, participant 3\\].\n\n【47】Discussion\n\n【48】Traditional seated meetings that were converted into a walking format using the WaM protocol increased moderate, vigorous, and very vigorous PA levels by 10 minutes among our sample of white-collar workers. Many jobs in the white-collar workforce involve a disproportionate amount of sitting time, which can increase the risk for being overweight or obese . Although several interventions have aimed to increase PA levels in the workplace (eg, by using stability balls instead of chairs and sit–stand work stations instead of traditional desks), the scientific literature consists of either low-quality evidence or equivocal results on the effect of these interventions. Data from this pilot study suggest that walking meetings might provide an alternative to the sedentary workdays of white-collar workers.\n\n【49】Focus groups suggest that the WaM protocol was feasible, accepted, and successfully implemented by study participants. Among the 8 participating groups, 7 completed both walking meetings. These findings are in contrast with those of Cooper et al, who found that university employees who reported a lack of time for PA and perceived that fitness facilities at work were expensive did not engage in PA . We found that walking meetings were accepted and implemented by white-collar university employees in this pilot study and that these workers could easily fit a walking meeting into the workday with little to no burden to their workflow.\n\n【50】This study has several limitations, including a small number of participants and a short duration. Time constraints (a 7-month academic year) and a limited number of accelerometers prohibited the research team from recruiting more participants and conducting additional cycles. The use of accelerometers may have caused a Hawthorne-type or reactivity event : participants may have increased their PA at work because they knew they were being monitored.\n\n【51】Despite these limitations, this pilot study has several strengths, including the collection of both quantitative and qualitative data, a flexible walking meeting protocol, and a uniform community of workers who engaged in similar tasks. A unique aspect of this study was the flexible walking meeting protocol. Rather than allowing participants to engage in a walking meeting without guidance, the research team provided participants with suggestions and a prescribed route with the aim of eliminating confusion, providing autonomy, and stimulating productivity while walking in a safe work environment. This study adds information on walking meetings to the scientific literature, where little research exists.\n\n【52】The data collected from this pilot study suggest that walking meetings, a simple modification of traditional seated meetings, were not only well accepted by our sample of white-collar workers but were easy to implement and feasible to conduct during regular working hours. PA interventions such as the WaM protocol that encourage walking and raise levels of PA in the workplace are needed to counter the negative health effects of sedentary behavior. Future studies should consider more frequent and repeated measures of walking meetings with larger groups of white-collar workers.\n\n【53】Tables\n------\n\n【54】#####  Table 1. Metrics Used to Assess Feasibility, Acceptability, and Implementation of the Walking Meeting (WaM) Pilot Study,\n\n| Objective | Key Questions | Data Source | Indicators |\n| --- | --- | --- | --- |\n| Feasibility | Are white-collar workers willing to consider adopting walking meetings? | Response rate | Number of eligible employers who agree to participate |\n| Feasibility | Can research teams and their members be trained to deliver the walking meeting strategy? | Interviews with teams involved in walking meeting training | Percentage of teams and team members that complete both walking meetings (week 2 and week 3) |\n| Feasibility | How much and what types of support are needed by worksite staff to enact walking meetings? | Interviews with WaM team members | Time spent by research team members providing technical assistance and other support |\n| Acceptability | How do those involved with walking meetings view the strategy? | Interviews with WaM team members | Percentage of respondents who report that the walking meetings strategy is acceptable |\n| Acceptability | To what extent is the walking meetings strategy viewed as suitable for the setting and population? | Interviews with WaM team members | Percentage of respondents who report that the strategy is suitable for their setting |\n| Acceptability | Can the walking meeting strategy be adapted to suit research team leadership and membership needs and preferences? | Interviews with WaM team members | Recommendations and preferences communicated |\n| Implementation | To what degree are core components of the walking meeting strategy implemented? | Team meeting discussion plan and process tracking | Integrated worksite policy is written and communicated; cost and resources expended. |\n| Implementation | What noncore or adaptive elements are implemented? | Team meeting discussion plan and process tracking | Number and types of trainings and information delivered |\n| Implementation | To what extent is walking meeting strategy implemented with fidelity? | Interviews with WaM team members | Percentage of participants that implemented the suggested components of a walking meeting |\n\n【56】#####  Table 2. Mean (Standard Deviation) Number of Minutes of Work-Related Physical Activity a  on Baseline Day (Nonwalking Day) and Walking Days (Week 2 and Week 3) in the Walking Meeting (WaM) Pilot Study, 2015 b  \n\n| Level of Physical Activity | Baseline b (n = 17 Participants) | Week 2 b (n = 17 Participants) | Week 3 b (n = 14 Participants) |\n| --- | --- | --- | --- |\n| Light | 169.8 (83.3) | 143.3 (46.1) | 129.2 (62.1) |\n| Moderate | 31.4 (19.9) | 41.6 (25.2) | 40.4 (22.4) |\n| Vigorous | 1.2 (3.2) | 1.83 (6.3) | 1.6 (4.4) |\n| Very vigorous | 1.5 (5.3) | 0.01 (0.04) | 1.1 (4.0) |\n| Moderate, vigorous, and very vigorous | 34.1 (23.6) | 43.5 (29.6) | 43.0 (26.6) |\n\n【58】a  Work-related physical activity is all activity that took place and was measured by accelerometry between the hours of 9:00 a.m. and 5:00 p.m. on a workday.  \nb  No significant difference (using paired _t_ test) in the number of light, moderate, vigorous, or very vigorous physical activity minutes between baseline and week 2 or week 3.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "13a5e810-ea71-4a3b-a35c-d5c8f9ee72a9", "title": "Population-Based Analysis of Invasive Fungal Infections, France, 2001–2010", "text": "【0】Population-Based Analysis of Invasive Fungal Infections, France, 2001–2010\nInvasive fungal infections (IFI) are reportedly increasing in many countries, especially candidemia and invasive aspergillosis (IA) among immunocompromised patients . Conversely, a decline of AIDS-associated _Pneumocystis jirovecii_ pneumonia (Pjp) and cryptococcosis has been observed in Western countries since the advent of highly active antiretroviral treatments . Many publications provide insight on a given IFI and its trends in specific risk groups, but the overall burden of illness associated with IFI and its trends at a country level have not been described . To describe the epidemiology and trends of IFIs and to better identify public health priorities (e.g. surveillance, research, prevention strategies), we analyzed the national hospital discharge database of France, Programme de Médicalisation du Système d’Information, spanning 2001–2010.\n\n【1】### Materials and Methods\n\n【2】The national hospital database covers >95% of the country's hospitals . An anonymous subset of this database can be made available for epidemiologic studies without need for ethical approval or consent of patients, according to legislation by the government of France. A unique anonymous patient identifier enables distinction among first and subsequent hospital admissions. Information filed at discharge includes the major cause of admission and associated diseases, coded according to the International Classification of Diseases, Tenth Revision, the medical and surgical procedures performed, and the outcome including transfer, discharge, or death. Details on the data source, case definitions, and methods used are available in Technical Appendix 1 .\n\n【3】Records of all hospital stays for which an IFI was recorded as the principal cause of admission or as a related disease were extracted from the national database for the period of January 2001 through December 2010. Records of the 5 most frequent IFIs were retained for this analysis. To facilitate comparisons with published studies, we restricted the study of invasive candidiasis to candidemia (i.e _._ , excluding _Candida_ endocarditis and meningitis), and invasive aspergillosis (IA) included pulmonary and disseminated cases. All cryptococcosis cases were included. Gastrointestinal mucormycoses were excluded because results of a previous study showed that cases were mostly identified on the basis of false-positive test findings . Finally, codes corresponding to “pneumocystosis” or “HIV infection resulting in pneumocystosis” were designated as Pjp only if pneumonia was associated. We excluded rare IFIs (<40 cases per year each) and endemic mycoses (histoplasmosis, blastomycosis, coccidioidomycosis, sporotrichosis). Analysis focused on metropolitan areas of France, excluding overseas territories.\n\n【4】After checking for multiple stays and inconsistent records within and between hospitals, we retained “incident cases,” i.e. unique stays and first admissions. To reduce underreporting bias, we ensured that a risk factor that occurred during subsequent stays was integrated into the incident record (e.g. a diagnosis of diabetes recorded after a patient's transfer from a first- to a third–level hospital). Similarly, in-hospital fatality rates were estimated from the cumulative stays.\n\n【5】To describe risk factors associated with IFIs, we selected 9 conditions on the basis of expert opinion and published studies on the epidemiology of IFI. Considering the high diversity of conditions, and to provide a description relevant for clinical practice and health policy makers, we used hierarchical ranking to assign 1 risk factor per patient. Given that the preponderant risk factors differ among IFIs, IFIs were divided into 2 groups. In the first group, which included candidemia, IA, and mucormycosis, risk-factor ranking started with hematologic malignancies (HM, including by priority order, HM associated with hematologic stem cell transplantation \\[HSCT\\], HM not associated with HSCT but with neutropenia, and HM with none of the above factors). The following illnesses and conditions were subsequent risk factors in the first group: HIV/AIDS, solid organ transplantations, solid tumors, systemic inflammatory diseases (including inflammatory bowel diseases, sarcoidosis, rheumatoid arthritis, and systemic lupus or vasculitis of other origins), diabetes mellitus, chronic respiratory diseases (including chronic obstructive pulmonary diseases, asthma, and cystic fibrosis), chronic renal failure, and a group labeled “other diseases” that includes acute renal failure, liver cirrhosis, morbid obesity, acute or chronic pancreatitis, and severe burns. Thus, a case-patient with HM and diabetes was recorded as HM. For the second IFI group (Pjp and cryptococcosis), HIV/AIDS was the first risk factor, followed by other risk factors as described above. For all case-patients with IFI, additional risk factors were explored without hierarchical ranking: a stay in an intensive care unit; surgery; and extreme age, defined as neonates (≤28 days of age) and elderly adults (≥80 years of age). Because of lack of precise coding for several risk factors until 2003, only those documented during the 2004–2010 period were analyzed.\n\n【6】We expressed annual incidence rates among the general population, by gender and age groups, as cases per 100,000 population, using data from the 1999 national population census and its updates. We also analyzed trends in groups with selected risk factors, for which the respective denominators were available from routine surveillance data or from prevalence estimates, as detailed in Technical Appendix 1 : patients with HM, HIV/AIDS, solid tumors, chronic renal failure, diabetes, and HSCT recipients. In these specific populations, we estimated the annual proportion of each IFI using the given risk factor per 100,000 population . Finally, we used an age-polynomial fractional logistic regression  to calculate age- and sex-adjusted risk for death categorized by risk factor, and analyzed each risk factor independently from the others without hierarchical ranking. We applied Fisher or χ 2  tests to compare groups, and a Poisson regression to assess trends, considering p≤0.05 as significant, using Stata version 11.2 (StataCorp LP, College Station, TX, USA) software for all calculations.\n\n【7】### Results\n\n【8】##### Characteristics of Case-Patients, 2001–2010\n\n【9】There were 35,876 cases of IFI registered in metropolitan France during 2001–2010 . Candidemia accounted for the highest proportion of cases (43.4%); the next most frequently identified diseases were Pjp (26.1%), IA (23.9%), cryptococcosis (5.2%), and mucormycosis (1.5%). The overall incidence was 5.9/100,000 population per year. A total of 9,889 (27.6%) case-patients died while in a hospital. Candidemia and IA accounted for 87.6% of these deaths. Male patients predominated in all IFIs (64.0%), especially in Pjp and cryptococcosis (>70%). The mean age was 54.7 years (range 0–107 years). Gender and age characteristics of case-patients and of those who died differed according to the IFI. Details are provided in online Technical Appendix 2 , Table 1 . Incidence and fatality rates of candidemia and IA were particularly high in patients ≥60 years of age, and male patients predominated in all age groups, except in those \\> 80 years of age. Case-patients in extreme age groups included 185 neonates (mainly with candidemia: 174 cases, 61.5% male patients, specific incidence 2.2/100,000 population) and 3,030 adults >80 years of age (2,283 with candidemia: 50.5% male, incidence 8.1/10 5  ). Among case-patients with Pjp and cryptococcosis, the proportion of male case-patients was higher among HIV-infected persons than in non–HIV-infected persons (Pjp 74.0% vs. 62.2%; cryptococcosis 77.9% vs. 62.3%, respectively).\n\n【10】The highest incidences of Pjp and cryptococcosis were observed among persons 30–59 years of age with AIDS and among those ≥60 years of age who were not infected with HIV (p<0.001 for each IFI). For these 2 IFIs, the fatality rate was lower in HIV-infected patients than in non-HIV-infected patients (Pjp 5.7% vs. 21.5%, p<0.001; cryptococcosis 13.4% vs. 17.9%, p<0.009).\n\n【11】##### Trends in the General Population, 2001–2010\n\n【12】The incidence of IFI increased by 1.5% per year and that of deaths by 2.9% per year (p<0.001 each) over the 10-year period of observation. Specifically, the incidence of candidemia, IA, and mucormycosis increased by 7.8%, 4.4%, and 7.3% per year, respectively (p<0.001 each). The fatality rate decreased by 1.6% per year (p<0.001) among persons with candidemia and 1.4% per year (p = 0.04) among those with IA, but increased by 9.3% per year (p = 0.03) for those with mucormycosis. Regarding Pjp and cryptococcosis, incidence decreased by 8.6% and 9.8% per year (p<0.001 each), and the fatality rate increased by 11.7% (p<0.001) and 4.7% (p = 0.03) per year, respectively . However, trends differed according to HIV status ; incidence of both IFIs decreased among HIV-infected patients (Pjp −14.3%; cryptococcosis −14.9% per year, p<0.001 each), and Pjp increased in non–HIV-infected patients (+13.3% per year, p<0.001); there was no significant trend for cryptococcosis in non–HIV-infected patients. The fatality rate trend was only significant for HIV-associated Pjp (+5.6% per year, p = 0.001).\n\n【13】##### Risk Factor Distribution and Trends in the General Population, 2004–2010\n\n【14】We studied risk factors among 25,933 IFI case-patients identified during the 2004–2010 period. Candidemia remained the most frequent IFI (46.4%) followed by IA (24.8%) and Pjp (22.9%). The distribution of risk factors differed for each IFI . Solid tumors were mainly found in patients with candidemia (30.6%), HM in those with IA and mucormycosis (54.3% and 34.8%, respectively), and HIV/AIDS in those with Pjp and cryptococcosis (>55% each). The incidence of candidemia, IA, and mucormycosis in patients with HM (especially with neutropenia) increased significantly, as did the incidence of candidemia and IA in solid organ transplant recipients, and patients with solid tumors or chronic renal failure. The incidence of Pjp decreased in patients with HM and increased in patients with solid organ transplants, solid tumors, and chronic renal failure.\n\n【15】##### IFI Trends in Specific Risk Groups, 2004–2010\n\n【16】We estimated trends from the annual proportion of risk factor–associated IFIs in the corresponding risk population. Only statistically significant trends are shown in Figure 2 . In the general population, the number of patients with HM, solid organ transplantations, chronic renal failure, HIV/AIDS, and diabetes substantially increased over time, and the population of HSCT recipients remained unchanged. In patients with HM, there was a statistically significant increase of candidemia, IA, and mucormycosis, and a decrease of Pjp . In HSCT recipients, candidemia and IA increased .\n\n【17】During the study period, candidemia increased among patients who had solid tumors . Among patients with chronic renal failure, the incidence of candidemia, IA, and Pjp increased . Among patients with HIV/AIDS, the incidence of Pjp and cryptococcosis decreased . There was no substantial trend among patients with diabetes (data not shown).\n\n【18】##### Odds Ratio of Death by Risk Factors, 2004–2010\n\n【19】We assessed the risk for death associated with each risk factor by logistic regression, considering each factor independently and expressed as an odds ratio for death; except for age, significant results are shown in Technical Appendix 2 , Table 3 . The risk for death was lower in female patients with IA, but did not differ by sex for other infections. The role of age varied according to the IFI type; for instance, in-hospital fatality rates increased in persons >20 years of age who had candidemia and Pjp, and in those >70 years who had IA. HM represented a substantial risk factor for death in patients with candidemia, IA, mucormycosis, and in non-HIV cryptococcosis. Solid tumors were a substantial risk factor for death in patients with candidemia, IA, and Pjp, regardless of HIV status. Cirrhosis and acute renal failure were also substantial risk factors for death in patients with candidemia, IA, and non-HIV Pjp and cryptococcosis. Hospitalization in an intensive care unit was associated with a higher risk for death among patients with all IFIs except candidemia. Inversely, chronic renal failure decreased the risk for death among those with IA or Pjp, respiratory diseases decreased the risk in patients with IA, and surgical procedures decreased the risk for those with candidemia.\n\n【20】### Discussion\n\n【21】This nationwide study provides evidence that ≈3,600 patients have IFI each year in France, of whom 28% die. The incidence of candidemia, IA, mucormycosis, and non-HIV Pjp has increased over the last decade, predicting a protracted trend over the coming years.\n\n【22】Studies on the epidemiology of the 5 predominant IFIs have reached conflicting results, depending on the IFI studied (most studies focused on a single IFI), the study design, and source of data (active surveillance system, cohorts, multicentric or monocentric, laboratory-based diagnosis, hospital discharge data), the population of interest (neutropenic patients, HM, HSCT and solid organ transplant recipients), and the practices regarding antifungal agents use (prophylactic, empiric, preemptive, or curative therapy). Here, we analyzed the hospital dataset at a country level, covering all persons who were admitted to hospitals over a period of 10 years, regardless of age or underlying conditions. We included those with illness caused by IFIs that have straightforward diagnostic criteria (candidemia, cryptococcosis) or well-characterized clinical entities (pulmonary or disseminated IA, pulmonary Pjp), as well as mucormycosis, for which we previously validated the accuracy of diagnostic coding in the hospital national database . Despite potential bias in the precise classification of cases, particularly for mold infections, and other limitations of administrative datasets that have been previously discussed , several points validate the findings obtained through this large database. The predominance of candidemia and IA has been described in other studies of a variety of IFIs in the general population or in other groups . For candidemia, the incidence and trends we estimated are comparable to many other, although smaller scale, population-based studies from Europe and North America . For IA in France, we observed a lower incidence and higher mortality rate than were found by Dasbach et al. in their analysis of US hospital discharge data . The differences may be explained by the researchers’ use of the International Classification of Diseases, Ninth Revision case definitions in that study, which would impair the comparison of invasive and noninvasive forms.\n\n【23】The decreasing incidence of Pjp and cryptococcosis was expected after the advent of active antiretroviral therapy . However, we observed some noteworthy changes: Pjp incidence in non-HIV–infected patients has currently reached the levels observed in HIV-infected patients, as observed in the United Kingdom during the same period ; incidence of cryptococcosis is also increasing in the seronegative population, and the mortality rate of both IFIs among non-HIV–infected patients is higher than among HIV-infected patients.\n\n【24】Most risk factors described in this study are well known in clinical practice. The major risk factors for candidemia, IA, and mucormycosis, i.e. HM, HSCT, and solid tumors, are described in many studies, such as those by the Transplant Associated Infections Surveillance Network, known as TRANSNET, and Prospective Antifungal Therapy Alliance, known as PATH , albeit sometimes reported as differently distributed. The hierarchical ranking process used here may have influenced the risk factor distribution, underestimating some conditions. Most studies of risk factors are performed on the basis of cohorts of cases in referral centers where a large number of high-risk patients are recruited, whereas in our population-based approach, we used a national dataset covering all levels of care, thus selecting a wider range of underlying conditions, including those less commonly recognized as risk factors. As a result, we documented substantial increases of candidemia, IA, and Pjp in patients with chronic renal failure, suggesting that the increase is not uniquely caused by the growing number of persons at risk . The growing number and longer survival of patients with protracted immunosuppression beyond traditional hematology patients, transplant patients, and HIV/AIDS populations are major challenges. The fact that 2 IFIs that are frequently associated with health care settings (candidemia and IA) are still on the rise despite existing infection control recommendations is of specific concern .\n\n【25】Hospital data are not collected for clinical research purposes. Thus, it is very hazardous to explain the trends on the basis of our limited observations. Specific analyses should be encouraged, aiming at better understanding the role of comorbid conditions in the occurrence of IFI (e.g. chronic renal failure) or the effect of the improved overall survival of patients, even those who are immunocompromised.\n\n【26】Another noteworthy finding of this study is that the risk for death was altered by factors that were not frequently documented before. For instance, cirrhosis was found in 1.3% of all patients with IFIs but was an independent risk factor for death among all except those with mucormycosis, suggesting underrecognition of IFIs in such populations, possibly leading to delayed prevention or treatment. Similarly, patients with HM showed an increased risk for death when cryptococcosis was also diagnosed, as did those with cirrhosis and acute renal failure, which suggest that specific attention should be paid to patients with these conditions; this could modify their clinical management.\n\n【27】This population-based study has limitations. The increase in IFIs observed parallels a better awareness of clinicians and microbiologists of the threat of IFIs in at-risk populations, improving the sensitivity of the hospital-based dataset. The availability of a broader antifungal drug armamentarium and efficient treatment could have the paradoxical effect of improving the prevention of IFI for selected groups of at-risk patients, thus lowering the population of infected patients. We report trends and risk factors for invasive mycosis in France. Hence, our findings may not apply to other countries with different endemic mycoses, population structures, and health care systems. Our observations are based on hospital discharge coding, which is subject to many biases, including misdiagnosis and incorrect coding. More notably, the advent of new diagnostic tools for the detection of many invasive mycoses may have affected our ability to diagnose these diseases over the study period, which may have had a substantial impact on the temporal trends observed.\n\n【28】Nevertheless, this large-scale study provides benchmarking data on the current burden of illness of major IFIs and shows the effects of disease trends and death rates spanning a decade in a Western European country. The need for baseline data was recently highlighted . Our data provide complementary information to specific studies or investigations linked to outbreaks . IFIs in this study occurred among a broad spectrum of patients and the fatality rate was high; clinicians should be made aware of risk factors, signs, and symptoms. Beyond the specific issues addressed by our study, such as the identification and management of patients in potentially under-recognized risk groups, the expected consequences of the increasing incidence of IFIs should be anticipated in terms of hospital and laboratory workload, antifungal use, and need for new systemic antifungal drugs and strategies . The development of epidemiologic studies is also of specific concern to clarify the determinants of the trends and identify effective interventions that can reduce deaths and the general public health burden of illness. These questions should be addressed jointly by clinicians and public health authorities at national and international levels.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "9761dc1f-ff2d-4624-9761-e9ad90ae0933", "title": "Comparing the Maryland Comprehensive Cancer Control Plan With Federal Cancer Prevention and Control Recommendations", "text": "【0】Comparing the Maryland Comprehensive Cancer Control Plan With Federal Cancer Prevention and Control Recommendations\nAbstract\n\n【1】**Introduction**\n\n【2】Since the introduction of the Affordable Care Act (ACA) in 2012, 11 million more Americans now have access to preventive services via health care coverage. Several prevention-related recommendations issued by the US Preventive Services Task Force (USPSTF), Centers for Disease Control and Prevention (CDC), and Advisory Committee on Immunization Practices (ACIP) are covered under the ACA. State cancer plans often provide prevention strategies, but whether these strategies correspond to federal evidence-based recommendations is unclear. The objective of this article is to assess whether federal evidence-based recommendations, including those covered under the ACA, are included in the Maryland Comprehensive Cancer Control Plan (MCCCP).\n\n【3】**Methods**\n\n【4】A total of 19 federal recommendations pertaining to cancer prevention and control were identified. Inclusion of federal cancer-related recommendations by USPSTF, CDC, and ACIP in the MCCCP’s goals, objectives, and strategies was examined.\n\n【5】**Results**\n\n【6】Nine of the federal recommendations were issued after the MCCCP’s publication. MCCCP recommendations corresponded completely with 4 federal recommendations and corresponded only partially with 3. Reasons for partial correspondence included specification of less restrictive at-risk populations or different intervention implementers. Three federal recommendations were not mentioned in the MCCCP’s goals, objectives, and strategies.\n\n【7】**Conclusion**\n\n【8】Many cancer-related federal recommendations were released after the MCCCP’s publication and therefore do not appear in the most current version. We recommend that the results of this analysis be considered in the update of the MCCCP. Our findings underscore the need for a periodic scan for changes to federal recommendations and for adjusting state policies and programs to correspond with federal recommendations, as appropriate for Marylanders.\n\n【9】Introduction\n\n【10】The Centers for Disease Control and Prevention (CDC) funds all 50 states through the National Comprehensive Cancer Control Program (NCCCP) to develop and implement comprehensive cancer control plans . There is no standard national cancer plan; instead, state cancer plans vary on the basis of content and organization, depending on a host of factors including state priorities, political climate, and stakeholder involvement and partnership. Furthermore, cancer plans are not static but are updated and revised on an ongoing basis. For example, Maryland revises its plan every 3 to 5 years . According to CDC, one goal of comprehensive cancer control planning is to disseminate and implement best practices and evidence-based strategies . The US Preventive Services Task Force (USPSTF), CDC, and Advisory Committee on Immunization Practices (ACIP) all issue cancer-related evidence-based recommendations . USPSTF’s grade A and B recommendations and many of CDC’s and ACIP’s recommendations and guidelines are covered under the Affordable Care Act (ACA) . To our knowledge, there has not been an assessment of any state cancer plan examining the extent to which these recommendations are included.\n\n【11】The first Maryland cancer plan was written in 1991 and has been updated several times since. The most recent plan, published in 2011, was developed by more than 200 contributors including representatives from hospitals, academia, nongovernmental organizations, and government, as well as citizens. Contributors formed committees to write specific chapters in the Maryland Comprehensive Cancer Control Plan (MCCCP). The process included an extensive review of evidence-based recommendations from multiple sources that were published when the plan was being developed. Recommendations from reputable sources such as USPSTF, CDC, the American Cancer Society, and many others relevant to the specific topic area were reviewed and considered by the MCCCP contributors. In addition, other recently published Maryland health plans were reviewed to achieve a cohesive approach to the development of goals, objectives, and strategies. MCCCP contributors used these recommendations, Maryland-specific data, and the expertise of many local physicians, academicians, public health practitioners, community health workers, cancer survivors, and others to determine the final goals, objectives, and strategies to be included in the MCCCP.\n\n【12】The 15-chapter plan focuses on primary, secondary, and tertiary cancer prevention and control and includes cross-cutting surveillance and health disparities sections. Priorities for overcoming the issues described in the MCCCP are presented at the end of each chapter in the form of goals, objectives, and strategies , all of which are actionable activities that can be implemented by professionals working in the field of cancer prevention and control.\n\n【13】The purpose of this investigation was to compare the goals, objectives, and strategies in the Maryland Comprehensive Cancer Control Plan (MCCCP)  with the cancer-related, evidence-based recommendations of federal agencies that are covered under the ACA. We examined the extent to which cancer-related USPSTF recommendations (grade A and B only), ACIP recommendations, and CDC guidelines were included in the 2011 MCCCP’s goals, objectives, and strategies. The goal of this assessment is to inform future updates of the MCCCP and to provide a model for other state cancer plans.\n\n【14】Methods\n\n【15】We reviewed recommendations issued by the USPSTF, ACIP, and CDC. The rationale for using these authoritative bodies was their extensive reliance on evidence-based medicine in addition to their cancer-related recommendations covered under the ACA. The Agency for Healthcare Research and Quality  and the National Cancer Institute  websites refer to USPSTF recommendations, which supports our selection of the USPSTF. USPSTF and ACIP recommendations and CDC guidelines are updated periodically. Nineteen cancer prevention and control-related recommendations or guidelines released by May 31, 2014 were included in our assessment .\n\n【16】For this analysis, we selected federal recommendations or guidelines pertaining to cancer prevention and control for adults. Because tobacco use, physical inactivity, obesity, and alcohol misuse increase the risk of developing cancer , we included in our analysis recommendations and guidelines pertaining to these behaviors and conditions. We identified 16 USPSTF recommendations  concerning breastfeeding, dietary counseling, risk reduction and genetic screening for women at high risk for breast cancer, early cancer detection screening, obesity, tobacco use cessation, diet, alcohol use, sun exposure, and hepatitis B virus (HBV) and hepatitis C virus (HCV) screening. In addition, we identified one ACIP recommendation  that did not correspond to the USPSTF recommendation pertaining to human papillomavirus (HPV) vaccination. From CDC guidelines , one physical activity and one healthy alcohol use guideline was identified. There was complete overlap between the USPSTF and ACIP recommending screening for HBV infection in pregnant women and in adults at high risk. Both the USPSTF and ACIP recommend that “all pregnant women should be tested routinely for HBsAg during an early prenatal visit” and that “all adults at risk for HBV infection be screened” (HbsAg is an HBV surface antigen). We used the USPSTF recommendations, because they were rated as grade A (testing for surface antigens in pregnant women) and grade B (testing for surface antigens in high-risk adults). Additionally, the USPSTF 2002 (as opposed to the 2009) mammography recommendation was adopted by the ACA and is the one included in this assessment.\n\n【17】If a federal recommendation or guideline was issued after the release of the MCCCP (July 2011), it was classified as “MCCCP published prior to recommendation.” If the MCCCP was released after the federal recommendation was issued, it was classified as “complete correspondence” if the MCCCP met 3 specific criteria: the goal, objective, and strategies had to 1) recommend the same implementer as the federal recommendation or guideline, 2) recommend the same intervention as the federal recommendation or guideline, and 3) direct the implementation to the same target population as the federal recommendation or guideline. If an MCCCP strategy met at least one but not all of these criteria, it was classified as “partial correspondence.” If a federal recommendation or guideline was issued before the 2011 MCCCP but did not appear in the MCCCP’s goals, objectives, and strategies, it was classified as “no mention.”\n\n【18】Results\n\n【19】We identified 19 federal recommendations and guidelines pertaining to primary and secondary cancer prevention and control for adults . Nine recommendations, all from the USPSTF, were issued after the release of the MCCCP in July 2011 (47.4%). Of the remaining 10, complete correspondence was found for 4 recommendations (21.1%) in the domains of breast feeding, early detection screening, vaccinations, and physical activity. There was partial correspondence for 3 recommendations and guidelines (15.8%). Reasons for partial correspondence stemmed from the MCCCP strategy having less restrictive at-risk populations (eg, screening age for colorectal cancer ending at 75 \\[USPSTF\\] vs no age limit \\[MCCCP\\], n = 1) and from having specified alternate intervention implementers (eg, physicians \\[USPSTF\\] vs hospitals \\[MCCCP\\] engaged in smoking cessation, n = 2). Three of the federal recommendations or guidelines were in place at the time of the MCCCP’s publication but were not mentioned in the MCCCP’s goals, objectives, and strategies (15.8%).\n\n【20】Discussion\n\n【21】We assessed the extent to which selected federal evidence-based recommendations covered under the ACA were included in the current MCCCP. The MCCCP did not include many of the federal recommendations considered by this study or had partial correspondence. The MCCCP is updated every 5 years, so although it is intended to be current and inclusive, the 2011 MCCCP predated the issuance of many of the federal recommendations. Reasons for partial correspondence stemmed from selected federal recommendations differing from other authoritative sources consulted during the development of the MCCCP, such as the American Cancer Society and the American Congress of Obstetricians and Gynecologists. Because the MCCCP predated the introduction of the ACA, the current assessment approach could not have been used to select strategies during the development of the current MCCCP. However, going forward, such an assessment could be used to update the MCCCP and other state cancer plans. Below we discuss how to ensure that recommendations covered by the ACA are included in the updated MCCCP.\n\n【22】### MCCCP published before recommendations\n\n【23】Of the 19 federal recommendations or guidelines identified, 9 were issued after the MCCCP was released in July 2011. USPSTF and ACIP recommendations and CDC guidelines are revised as the scientific evidence base is refined; the timing of those revisions and additions do not necessarily align with the 5-year cycles of the MCCCP. Full revision of the MCCCP is a massive undertaking by hundreds of contributors resulting in a consensus document. Therefore, wholesale revision of the MCCCP each time a recommendation is revised or added may not be feasible. Hence, including links to sources of up-to-date recommendations in the MCCCP and providing access to updated recommendations through addendums released online between the more comprehensive revisions that occur every 5 years is recommended. The Center for Cancer Prevention and Control (CCPC) in the Cancer and Chronic Disease Bureau of the Maryland Department of Health and Mental Hygiene (DHMH) uses this practice to keep the MCCCP current between the 5-year cycles.\n\n【24】### No mention in goals, objectives, and strategies\n\n【25】Three of the national recommendations were in place before July 2011 but were not included in the MCCCP’s goals, objectives, and strategies. The site-specific chapters of the MCCCP focus on 7 cancers that are targeted by the state’s Cigarette Restitution Fund (CRF) Program (breast, cervical, colorectal, lung, melanoma, oral, and prostate), which were selected in 2000 as priorities because of their high burden in the state, modifiable risk factors, or effective early detection methods. One of the 3 recommendations not included addresses HBV, a risk factor for liver cancer . Since liver cancer is not one of the CRF-targeted cancers, MCCCP authors decided to address liver cancer only in the disparities chapter and not in its own chapter or in the goals, objectives, and strategies of the current MCCCP. To keep the MCCCP current, it may be advisable to broaden the scope of future versions of the plan to include additional cancer prevention recommendations that address cancer sites beyond the CRF-targeted cancers.\n\n【26】In addition, alcohol misuse, an important modifiable risk factor that contributes to risk of cancer, was not mentioned in the goals, objects, and strategies. The CCPC is “responsible for assessing cancer control in Maryland and developing cancer control priorities through collaboration with the State Council on Cancer Control and other state agencies”  and develops and periodically updates the MCCCP. Although other arms within the DHMH coordinate programs and implement primary and secondary cancer prevention and control-related interventions (eg, DHMH Infectious Disease Bureau), alcohol misuse is handled by the Alcohol and Drug Abuse Administration, an arm of the DHMH that did not contribute to the 2011 MCCCP. As a means of facilitating the use of the assessment used in current study for updating future cancer plans, participation from this and other relevant DHMH administrations and centers is recommended.\n\n【27】### Partial correspondence between the MCCCP and national recommendations and guidelines\n\n【28】Three of the federal recommendations and guidelines were included in the MCCCP, but they differed on either the implementer or the targeted population.\n\n【29】**Different implementer.** Two recommendations and guidelines had partial correspondence because the recommended implementer of the interventions differed. Specifically, in 2 cases the MCCCP identified the hospital as the implementer whereas the federal recommendation specified the physician as the implementer of the intervention. Although physician groups did participate in the development of the MCCCP, this highlights the importance of soliciting a more direct and central role in the writing and implementation of the plan’s goals, objectives, and strategies.\n\n【30】**Less restrictive at-risk population in the MCCCP.** Another reason for partial correspondence occurred with the specifics of screening recommendations, primarily the relevant age groups for screening. Although our assessment was limited to USPSTF and CDC recommendations and CDC guidelines, the MCCCP refers to additional national recommendations throughout, including screening recommendations set by the American Cancer Society, the American College of Gastroenterology, the American Dental Association, and the American Congress of Obstetricians and Gynecologists. In addition, the Maryland DHMH convenes state medical advisory groups to address various screening guidelines, including those provided by USPSTF; these recommendations are used in the implementation of the Cancer Prevention, Education, Screening and Treatment Program for the uninsured and underinsured. It may be possible to harmonize payment guidelines with the ACA and MCCCP recommendations. In light of additional national health quality and cost guidelines and life expectancy calculations , revisiting these recommendations may also be in order.\n\n【31】This assessment assumed that all current cancer-related USPSTF, ACIP, and CDC evidence-based recommendations should be included in state comprehensive cancer control plans. Comparison of the MCCCP’s goals, objectives, and strategies with the selected federal recommendations highlighted the degree of correspondence with the MCCCP, which is the most comprehensive and inclusive cancer plan to date for Maryland. Of the 5 state plans published since 1991, this plan had the most contributors, had the greatest sector representation, and addressed individual, system, and environment concerns. To help maximize health care coverage and access to preventive services, we recommend using this assessment as part of the planning process for future updates of the MCCCP. Incorporating up-to-date evidence-based recommendations, in particular those covered by the ACA, into MCCCP goals, objectives, and strategies is crucial for encouraging state cancer plan users to close the gaps in coverage of these important services. The first step in enabling plan users to implement evidence-based objectives and strategies proven effective at reducing the incidence and burden of cancer is to first incorporate them into the plan. Therefore, comprehensive cancer control plans should be evidence-based to help guide development of statewide priorities that are reflective of the state’s burden to inform implementation strategies and to evaluate impact. In addition to providing important cancer plan funding, another role of CDC in keeping plans up to date would be to support state research priorities through its partnerships with academic institutions and care delivery organizations as a means of informing evolving priorities and generating new evidence.\n\n【32】Updates of the MCCCP may also benefit from the ongoing consideration of recommendations from the Task Force on Community Preventive Services , which provides evidence-based recommendations for community-level activities and behaviors, and the expert opinion-based National Comprehensive Cancer Network . The main impetus for including the current set of criteria in our assessment was that these interventions are covered under the ACA and would, thus, enhance health  and increase access to preventive services for virtually all Marylanders. Other states could assess their plans in a similar manner to the methods described here to optimize population uptake and availability of preventive services  that are both evidence-based and cost saving .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "cb167975-fddc-484e-8e2e-4a206a6159c7", "title": "Effectiveness of Personal Protective Equipment and Oseltamivir Prophylaxis during Avian Influenza A (H7N7) Epidemic, the Netherlands, 2003", "text": "【0】Effectiveness of Personal Protective Equipment and Oseltamivir Prophylaxis during Avian Influenza A (H7N7) Epidemic, the Netherlands, 2003\nAvian influenza A viruses are considered a threat to public health because they may result in new human influenza A strains. Thus, knowledge about preventing human infections with avian influenza viruses is essential. In 2003, a devastating epidemic caused by an avian influenza virus of subtype H7N7 occurred among the poultry sector of the Netherlands . During this epidemic, an unexpectedly high number of persons reported illness that appeared to be associated with subtype H7N7 infection after they were exposed to infected poultry ; 1 veterinarian died of acute respiratory distress syndrome . Previous reports have documented transmission between poultry, between humans, and from poultry to humans . In this report, we extend earlier work by analyzing the effect of personal protective measures on poultry-to-human transmission. Specifically, we investigated the effects of use of respirators and safety glasses and the prophylactic use of oseltamivir on the risk for infection during depopulation of infected farms. Our quantitative estimates of the effect of personal protective measures can guide efforts to prevent human infections with avian influenza.\n\n【1】### Materials and Methods\n\n【2】##### Data\n\n【3】Immediately after the epidemic, the National Institute of Public Health and the Environment of the Netherlands sent a questionnaire to 1,747 persons, of whom 872 (49.9%) responded. Response was lowest among persons who actively handled the culling of poultry . Of the 872 persons who responded, 450 could be linked to a farm-visits database kept during the epidemic that contained information about who had visited which farm on which date for what reason. Of these 450 persons, 194 had been actively involved in hands-on culling during the depopulation; activities included catching live poultry and picking up dead poultry. Because this group had the highest exposure , it was used to analyze the effect of personal preventive measures.\n\n【4】The questionnaire asked for information about symptoms of infection of the eyes, from which a self-reported conjunctivitis result was derived as an outcome measure for subtype H7N7 infection. The presence of \\> 2 of the following eye symptoms was classified as conjunctivitis: redness, tearing, itchiness, pain, burning, purulence, or sensitivity to light. Blood samples were collected from survey respondents 3 weeks after possible exposure and tested by a hemagglutination-inhibition (HI) assay for antibodies by using a modified cutoff based on validation studies of persons known to be infected and of nonexposed controls . The serologic result was available in addition to the self-reported conjunctivitis information.\n\n【5】We used a case definition that combined both outcomes, i.e. a person needed to self-report conjunctivitis and have a positive HI assay result. For sensitivity analysis, both the conjunctivitis and the serologic result were used in separate case definitions.\n\n【6】The survey also contained information about prophylaxis with a neuraminidase inhibitor (oseltamivir \\[Tamiflu; Roche, Basel, Switzerland\\]) and the use of personal protective equipment (PPE \\[safety glasses and respirator\\]). Beginning on March 14, prophylaxis with oseltamivir (75 mg daily) was prescribed to persons in contact with potentially infected poultry (2 weeks after the first infection was diagnosed and 10 days after culling began). Continuation of oseltamivir treatment was recommended until 2 days after possible exposure. The subtype H7N7 strain that circulated was susceptible to oseltamivir . To determine oseltamivir use, persons were asked the following questions: 1) Were you prescribed oseltamivir by a medical doctor? (yes/no). 2) If oseltamivir was prescribed, when was it prescribed and when did you stop using it? 3) How often did you fail to take a capsule? Persons who were not prescribed oseltamivir were classified as “did not use.” On the basis of the period of use stated in the survey, persons who were prescribed oseltamivir were classified per visit as “used” or “did not use.” Within the used category, persons who missed < 3 capsules were classified as consistent users; if more capsules were missed, they were classified as inconsistent users.\n\n【7】PPE (respirator and safety glasses) were provided during the entire epidemic to persons involved in the depopulation. The Dutch Food and Consumer Product Safety Authority provided the PPE and supervised its use. For each visit, workers received new PPE. They were instructed how to use the PPE but received no extensive training. For both respirators and safety glasses, workers were asked the following questions: 1) Did you use respirators/safety glasses (yes/no)? 2) How often did you use respirators/safety glasses (always, almost always, sometimes, almost never, never)? 3) How often did you not use respirators/safety glasses? From responses to these questions, we classified persons into 3 categories: used, sometimes used, and did not use. Persons who had either always or almost always used respirators or safety glasses and who stated that they had not missed using them more than twice were placed in the “used” category. Persons who had not used them were classified as did not use, and all remaining persons with answers were classified as sometimes used. For safety glasses and respirators, information was available only about their use during the whole epidemic and not per visit. We therefore assumed that a person’s use of a respirator and safety glasses did not change during the epidemic. The respirators provided were type FFP2 (US equivalent N95), which protected both nose and mouth and were all the same size. Safety glasses covered only the front of the eyes and were open above, below, and on both sides of the eyes . They were effective against splashes but not against dust. No information was available about the overall health of the workers.\n\n【8】##### Statistical Analysis\n\n【9】The probability of becoming infected during a visit was calculated from the number of visits each person had made and whether each person was infected (according to the case definitions). The analysis comprised only visits to farms with infected poultry. During each visit, a person could either escape infection or become infected. The probability of each sequence of events could be added, from which the probability of infection per visit could be estimated by maximum likelihood . The dependent variable in this analysis was the case definition, i.e. whether a person was infected. The probability of infection was calculated according to the categorical level of personal protection used, e.g. use of oseltamivir or not, as independent variables. We calculated confidence intervals (CIs) using profile likelihood methods . Statistical testing was performed by using likelihood ratio tests .\n\n【10】For oseltamivir, risk for infection was calculated per group and then aggregated over the groups of persons who had and had not used oseltamivir . Use of respirators, safety glasses, and oseltamivir were classified into the earlier described groups. We then calculated the risk for infection for each combination of groups, e.g. always used respirators/sometimes used safety glasses . Although onset date of infection was unknown, the likelihood function took into account that a person could become infected only once and that infection could occur at any farm visit.\n\n【11】### Results\n\n【12】The 194 persons analyzed together had made 458 active culling visits. A conjunctivitis result was available for 193 persons (36 positive), a serologic result for 131 persons (81 positive), and a combined conjunctivitis/serology result for 130 persons (19 positive). Mean age of the study population was 40 years (range 18–64 years). Most  persons were veterinarians. Ninety percent of the study population was male. Persons made an average of 2.4 (range 1–9) active culling visits.\n\n【13】##### Oseltamivir\n\n【14】Oseltamivir had been prescribed for 159 persons; 35 stated that oseltamivir had not been prescribed. Forty-five persons for whom oseltamivir had been prescribed did not use it during some of their visits. For the group of 130 (combined case definition), oseltamivir was prescribed for 114 and not prescribed for 16; for 34 persons, oseltamivir was prescribed but not used.\n\n【15】The estimated risk for infection per visit without use of oseltamivir was 0.145 (95% CI 0.078–0.233). This risk dropped significantly to 0.031 (95% CI 0.008–0.073; p = 0.005) per visit when oseltamivir was used . When calculated over the infection probabilities, oseltamivir use had a protective effect of 79% (95% CI 40%–97%; relative risk \\[RR\\] 0.21, 95% CI 0.03–0.60). The risk for infection seemed to increase when oseltamivir was used inconsistently, but this risk did not reach significance.\n\n【16】Results with case definitions that used either the serologic results or self-reported conjunctivitis differed slightly. With the serologic result, the estimated risk for infection without oseltamivir use was 0.513 (95% CI 0.370–0.656). This risk dropped to 0.274 (95% CI 0.157–0.418; p = 0.01) with oseltamivir use , which resulted in a protective effect of 46% (95% CI 19%–68%; RR 0.54, 95% CI 0.32–0.81). With self-reported conjunctivitis used as case definition, the estimated risk for infection without oseltamivir use was 0.115 (95% CI 0.064–0.180). This risk dropped to 0.073 (95% CI 0.032–0.135; p = 0.08) with oseltamivir use , which resulted in a protective effect of 53% (95% CI 2%–82%; RR 0.47, 95% CI 0.18–0.98).\n\n【17】We could not analyze interactions between use of oseltamivir and PPE because the resulting groups would have had too few visits. In the group that always used respirators and protective glasses and the group that sometimes used respirators and protective glasses, oseltamivir use was relatively equal (63%–74%). In the group that sometimes used respirators and did not use safety glasses, oseltamivir was used in 38% of the visits. In the group that did not use safety glasses or respirators, oseltamivir was used in 25% of the visits. These findings indicate no strong correlation.\n\n【18】##### Respirators and Safety Glasses\n\n【19】Persons generally were more inclined to use respirators than safety glasses . Persons who always used safety glasses also used a respirator. Only a small number of persons stated they had used no respirator and no safety glasses.\n\n【20】We gauged the effect of using safety glasses by comparing the risk for infection in persons with different safety glasses use within the group who always used respirators . Within this group, risk for infection decreased with use of safety glasses; however, this trend was not significant. We gauged the effect of respirator use by comparing the risk for infection in persons with equal levels of safety glasses use but different respirator use . Risk for infection was lower in persons who sometimes used a respirator than in persons who always used a respirator (within the groups that sometimes used or did not use safety glasses). Risk was higher for persons who had not used respirators than for those who had sometimes or always used them (within the group that had not used safety glasses). However, the group that had used neither respirators nor safety glasses was small. In both comparisons, the protective effect of respirators was not statistically significant. In the sensitivity analysis, results for which conjunctivitis and the serologic result were used as case definitions were similar to results with the combined case definition .\n\n【21】### Discussion\n\n【22】Quantifications of the effect of prophylactic use of oseltamivir on the risk for infection with avian influenza A (H7N7) virus can be used to guide efforts to reduce human infections with avian influenza. The risk for infection per visit was remarkably high among persons who did not use prophylactic oseltamivir (0.145, 95% CI 0.078–0.233). Although significantly lower, the risk for infection per visit for persons who did use oseltamivir prophylactically was still considerable (0.031, 95% CI 0.008–0.073).\n\n【23】Given the lack of research on the prophylactic effect of oseltamivir use on avian influenza in humans, we compared our result with studies on human seasonal influenza. Our estimated protective effect of oseltamivir use (79%) compares with that reported for laboratory-confirmed symptomatic human seasonal influenza, which ranges from 68% to 90% . The protective effect estimated with the case definition based on the serologic result only (46%) is close to the range of 49%–68% found with laboratory-confirmed human influenza . This finding suggests that the specificity of the serologic outcome measure is high, even though in our study it has a relatively low cutoff . The standard for use of HI assays in serologic studies is a cutoff of 40. This cutoff was lowered on the basis of evidence from an epidemiologic study in which no serologic responses were found in any of the 89 known infected persons by using the standard criteria, but a high proportion had low-level antibody reactivity with high specificity, which has triggered some debate about the validity of this serologic approach. However, finding a significantly lower risk for persons that used oseltamivir and had antibody reactivity cannot be explained by nonspecific reactivity. Therefore, we conclude that, for some reason, subtype H7N7 infections do not provoke a strong immune response, possibly related to the ocular tropism. Since then, similar findings have been reported . Finally, the protective effect when conjunctivitis is used as case definition was 53% in our study, compared with 29% for human influenza based on influenza-like illness , which suggests a reasonable specificity of conjunctivitis as indicator for subtype H7N7 infection.\n\n【24】Considering the effect of oseltamivir in reducing the risk for infection, prophylactic treatment of all persons involved in depopulating farms may seem wise. However, oseltamivir is also the fallback drug used for treating patients with severe influenza, and resistance against oseltamivir has increased in seasonal influenza viruses . A decision regarding prophylactic use of oseltamivir in a future epidemic would need to account for the risk for infection, risk for resistance, and severity of the infection.\n\n【25】Although our results provided clear evidence that prophylactic use of oseltamivir is effective in reducing risk for infection, the results were less clear with regard to PPE. In fact, although use of safety glasses appeared to reduce the risk for subtype H7N7 infection, this effect was not significant. Considering that the main symptom of subtype H7N7 is conjunctivitis, safety glasses may protect the eyes to some extent against influenza infection. For respirators, we also did not find a clear protective effect. Persons who always wore respirators may have done work that exposed them more. For both respirators and safety glasses, people received a limited amount of training that perhaps led to ineffective use of PPE and unsafe removal of contaminated PPE. Respirators were available in only 1 size and thus may not have fit well. The safety glasses were effective against splashes but were open on all sides and were not effective against dust. Possibly the PPE used were not appropriate for the high-exposure work. The number of available visits per group was low, data were also of limited temporal resolution (because PPE use per visit was not available), and potential for a recall bias also existed. Because of these limitations, we cannot conclusively determine from this study that respirators and safety glasses do not provide a protective effect. Findings of an experimental situation  indicated that respirators are likely to modify exposure and thus risk for infection. In Norfolk, United Kingdom, in 2006, incomplete use of PPE (safety glasses and respirator) was associated with conjunctivitis and influenza-like illness in an outbreak of avian influenza A (H7N3) .\n\n【26】In our study, prophylactic use of oseltamivir greatly reduced risk for infection with avian influenza A (H7N7). However, even with oseltamivir use, risk for infection remains considerable. Oseltamivir use should be part of an integrated approach to reduce human exposure, together with the use and appropriate training of PPE.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d5722936-af55-4315-ac22-ba44062ab653", "title": "Incidence of Clinician-Diagnosed Lyme Disease, United States, 2005–2010", "text": "【0】Incidence of Clinician-Diagnosed Lyme Disease, United States, 2005–2010\nLyme disease (LD) is a zoonotic infection transmitted by _Ixodes_ spp. ticks and caused by the spirochete _Borrelia burgdorferi_ . Signs and symptoms of infection range in severity and can include erythema migrans, arthritis, facial palsy, radiculoneuropathy, arrhythmia, and meningitis. Most patients recover fully after antimicrobial treatment ; however, serious illness and even deaths have been reported, although rarely . In the United States, LD is the fifth most commonly reported nationally notifiable disease; ≈36,000 confirmed and probable cases were reported in 2013 . US cases are concentrated heavily in the Northeast and upper Midwest .\n\n【1】Surveillance for LD in the United States is based on reports submitted by laboratories and health care providers to state and local health departments. These reports provide valuable insight into the age and sex distribution of patients with LD and the seasonality and geographic distribution of cases, and they enable monitoring of disease trends over time. Unfortunately, underreporting and variation in surveillance practices limit the ability of routine surveillance to capture the true overall frequency of LD within the population . Studies conducted during the 1990s in high-incidence states suggest that LD cases are underreported by a factor of 3 to 12 . These studies were limited to specific states and do not necessarily reflect underreporting nationwide.\n\n【2】Medical claims data provide an additional source of information about the epidemiology and public health importance of LD. Because these data are based on billing records submitted by clinicians for reimbursement, they are less prone to underreporting than are routine surveillance data that require additional documentation. We used information from a large, nationwide medical claims database to 1) describe the epidemiology of LD diagnosed by clinicians, 2) identify similarities and differences with surveillance data, and 3) estimate the number of LD cases per year in the United States.\n\n【3】### Methods\n\n【4】##### Medical Claims Database\n\n【5】During 2013–2014, we retrospectively analyzed the 2005–2010 Truven Health MarketScan Commercial Claims and Encounters Database, which contains health insurance claims information for a median of 27 million persons each year. The database contains records for persons 0–64 years of age with employer-provided health insurance and includes information about employees and their spouses and dependents from all 50 states. Deidentified data on enrollee demographics, outpatient and emergency department visits, inpatient admissions, and prescription drugs are included.\n\n【6】Each patient encounter record is assigned \\> 1 diagnostic code from the International Classification of Diseases, Ninth Revision, Clinical Modification (ICD-9-CM), by a clinician or billing specialist. Inpatient admissions in the database include 1 principal diagnosis and up to 14 secondary diagnoses. Outpatient encounters include up to 4 associated ICD-9-CM codes but do not distinguish between principal and secondary diagnoses. Medication information is available for most enrollees for prescription drugs filled at outpatient pharmacies.\n\n【7】##### Epidemiology of Clinician-Diagnosed LD in the MarketScan Database\n\n【8】The study population comprised persons enrolled in a participating health plan for the entirety of any year during 2005–2010 and for whom prescription drug information was available. For this analysis, we defined an inpatient event as a hospital admission with the ICD-9-CM code for LD (088.81) as the principal diagnosis or the 088.81 code as a secondary diagnosis plus a principal diagnosis consistent with an established manifestation of LD or plausible co-infection .\n\n【9】We defined an outpatient event as any outpatient or emergency department visit with the 088.81 code plus a prescription filled for an antimicrobial drug recommended by the Infectious Diseases Society of America for LD treatment . Three additional antimicrobial drugs also were included because they were closely related to a recommended antimicrobial drug or were a known historical treatment that some practitioners might still prescribe . Only prescriptions of at least 7 days’ duration and filled ±30 days from the visit date were considered.\n\n【10】The first outpatient or inpatient event of each year that met the study definition was considered the incident diagnosis for a patient. The date of admission or first outpatient visit that met study inclusion criteria was considered the date of the event. A separate LD diagnosis that met inclusion criteria at least 1 year after the previous diagnosis was included as a new incident event. When both an outpatient event and inpatient admission occurred within 1 year, only the inpatient admission was considered. To maintain consistency with US surveillance data, location was based on the patient’s county of residence, not where care was provided.\n\n【11】##### National Surveillance and US Population Data\n\n【12】State and local health officials report LD cases to the Centers for Disease Control and Prevention (CDC) through the National Notifiable Diseases Surveillance System according to standardized case definitions . For comparison with MarketScan findings, we analyzed surveillance cases reported during 2005–2010. Cases reported during 2005–2007 reflected a surveillance case definition comprising confirmed cases only. Beginning in 2008, a revised case definition was in place that altered the laboratory criteria and distinguished between confirmed and probable cases; cases reported during 2008–2010 included both categories . US Census 2010 population data were used for population comparisons and extrapolations .\n\n【13】##### Estimation of the Number of Clinician-Diagnosed LD Cases\n\n【14】To estimate the total number of patients with clinician-diagnosed LD in the United States, we calculated age- and county-specific rates derived from the MarketScan database and applied them to the 2010 population of each corresponding county. Counts for all US counties were then summed. Because the MarketScan database is limited to persons <65 years of age, these calculations do not include clinician-diagnosed cases among persons \\> 65 years. To adjust for this exclusion, we multiplied by a correction factor of 1.17. This correction factor was inferred from the age distribution of LD patients reported through national surveillance. During 2005–2010, persons <65 years of age accounted for 85.8% of LD cases reported through national surveillance. Therefore, we multiplied the estimated number of cases among persons <65 years by 1.00/0.858, or 1.17, to arrive at an estimate of cases in all age groups.\n\n【15】The estimated number of patients with clinician-diagnosed LD was based on extraction of a single ICD-9-CM code. Research has shown, however, that clinician diagnosis of a medical condition does not necessarily correlate with existence of the ICD-9-CM code in the chart . The primary reasons are coding errors and inclusion of codes for accompanying symptoms but not the specific disease (e.g. coding for joint pain but not LD) . To correct for omission of the 088.81 code, we relied on 4 evaluations of coding patterns for patients in whom LD was diagnosed. The Minnesota Department of Health found the 088.81 code was present in 145 (56.4%) of 257 charts for which a clinician documented a new case of LD . A Maryland Department of Health and Mental Hygiene study found the 088.81 code in 45 (44.6%) of 101 charts from patients in whom LD was diagnosed and reported by clinicians or clinical centers . Furthermore, the New York State Department of Health found the 088.81 code in 114 (41.8%) of 273 charts from patients in whom LD was diagnosed . Finally, the Tennessee Department of Health found the 088.81 code listed at least once in 9 (37.5%) of 24 charts from patients with Blue Cross Blue Shield insurance in whom LD was diagnosed and who were reported to the Department of Health . Thus, of 655 collective charts from LD patients, 313 charts had 088.81. Therefore, to account for patients in whom LD was diagnosed but whose charts were not coded with 088.81, we multiplied the estimated number of cases with 088.81 by a correction factor calculated as follows: 313/655 = 1/ _x_ , where _x_ \\= 2.09.\n\n【16】##### Statistical Methods\n\n【17】We calculated direct standardization and descriptive statistics using SAS software version 9.3 (SAS Institute, Cary, NC, USA). The χ 2  test was used to compare categorical data. Cramer’s V values were calculated to compare distributions by using R statistical software version 3.1.1 . Methods for credible interval calculation are provided in the online Technical Appendix.\n\n【18】##### Ethics Review\n\n【19】CDC human subjects review of the protocol determined it was not research involving human subjects. Thus, Institutional Review Board approval was not required.\n\n【20】### Results\n\n【21】##### Study Population\n\n【22】The final study dataset comprised 103,647,966 person-years of observation (median 17,309,054 persons/year). Median age of the study population was 37.0 years; 51.9% of patients were female. For comparison, the median age of the US population in 2010 was 37.2 years, and 50.8% of the population was female.\n\n【23】##### Epidemiology of Clinician-Diagnosed LD and Comparisons with Surveillance Data\n\n【24】A total of 45,430 clinician-diagnosed LD events were identified during 2005–2010; 985 (2.2%) were inpatient admissions and 44,445 (97.8%) were outpatient events . Average annual incidence within the MarketScan population was 44.8 events per 100,000 persons, with a peak of 56.3 events per 100,000 persons in 2009 . Interannual fluctuation in incidence in MarketScan data was similar to that in surveillance data (χ 2  test, p = 0.81; Cramer’s V = 0.037).\n\n【25】Clinician-diagnosed LD events peaked during the summer months, although more so for inpatient admissions (61.9% occurred during June–August) than for outpatient events (50.0% occurred during June–August). In comparison, 65.0% of cases reported through surveillance occurred during June–August . Seasonal distribution of LD events in MarketScan differed significantly from cases reported through surveillance, though this is likely an artifact of the large sample sizes since the magnitude of Cramer's V suggests little difference in the distributions (inpatients: χ 2  test, p<0.001, Cramer’s V = 0.019; outpatients: χ 2  test, p<0.001, Cramer’s V = 0.154).\n\n【26】Age distribution for both male and female patients did not differ significantly from the distributions reported through surveillance (male: χ 2  test, p = 0.57, Cramer’s V = 0.054; female: χ 2  test, p = 0.43, Cramer’s V = 0.054) . For inpatients, the highest average annual admission rates were for boys 5–9 years of age (1.8 admissions/100,000 persons) and men 60–64 years of age (1.9 admissions/100,000 persons). For outpatient events, the highest annual incidences were for boys 5–9 years of age (54.5 events/100,000 persons), men 60–64 years of age (55.4 events/100,000 persons), and women 60–64 years of age (54.7 events/100,000 persons). Relative to surveillance data, the incidence of clinician-diagnosed LD was higher than expected for women 15–34 years of age.\n\n【27】The 15 states and district with the highest average incidence represented 80.6% of clinician-diagnosed LD and were as follows, in descending order: Connecticut, Rhode Island, Maryland, New Jersey, Massachusetts, New York, New Hampshire, Pennsylvania, Maine, Delaware, Virginia, Vermont, Wisconsin, District of Columbia, and Minnesota . These same 15 states and district were seen in surveillance data, although the rank order differed slightly, and they constituted a significantly greater proportion (96.3%) of reported cases (χ 2  test, p<0.001).\n\n【28】##### Estimated Number of Clinician-Diagnosed LD Cases\n\n【29】Direct standardization of clinician-diagnosed LD and addition of estimated cases in persons \\> 65 years of age produced an estimate of 157,137 cases per year, which was multiplied by 2.09 to correct for omission of the 088.81 code in patient charts. This calculation yielded a national estimate of 329,000 LD cases per year during 2005–2010 (95% credible interval 296,000–376,000). On the basis of this number, the estimated incidence of clinician-diagnosed LD in the United States during this period was 106.6 cases per 100,000 persons per year. In comparison, average US incidence according to surveillance data during this period was 9.4 cases per 100,000 persons per year.\n\n【30】Sensitivity analyses showed that the correction factor for patients in whom LD was diagnosed but who were not given the 088.81 code had the greatest influence on the final estimate . For example, a 10% increase in this correction factor led to a 6% increase in the final estimate, and a 30% decrease led to a 12% decrease in the final estimate.\n\n【31】### Discussion\n\n【32】Using medical claims data, we estimated that 329,000 (95% credible interval 296,000–376,000) LD cases occur annually in the United States, which emphasizes the substantial public health effect of this disease. This estimate is consistent with findings from a recent study of diagnostic laboratories that yielded an estimate of 288,000 (range 240,000–444,000) infections among patients for whom a laboratory specimen was submitted in 2008 . As expected, our estimate is slightly higher because it also includes LD cases diagnosed without laboratory testing (i.e. clinical diagnosis based on presence of erythema migrans after exposure in a Lyme-endemic area).\n\n【33】Presence of a diagnostic code in the chart or a clinician diagnosis of an infectious condition does not necessarily signify a true infection . Possible reasons include rule-out diagnoses, codes for medical history but not incident infections, and overdiagnosis (incorrect diagnosis of LD when the patient has a different condition). Rule-out diagnoses and medical history codes most likely were reduced—but not completely eliminated—by including only outpatients treated with an antimicrobial drug recommended for LD. Overdiagnosis of LD is not uncommon given that, in some circumstances, the differential diagnosis for symptoms of LD can be broad . Studies of patient charts with the 088.81 code found that 37.9% in Maryland and 55.2% in Wisconsin were classified after chart review as noncases according to the surveillance case definition . Thus, we cannot exclude the possibility that some of the ≈329,000 patients in whom LD was diagnosed were not infected with _B. burgdorferi_ .\n\n【34】Epidemiologic patterns of clinician-diagnosed LD were similar to patterns among cases reported through national surveillance; for example, incidence was highest among boys 5–9 years of age and persons 60–64 years of age of both sexes, which is believed to be attributable partially to behavioral factors and increased exposure to ticks in these age groups. However, some discrepancies were also noted. Specifically, incidence of clinician-diagnosed LD was higher than expected among women 15–44 years of age. A study of records with the 088.81 code using Maine’s statewide electronic database of inpatient and outpatient encounters also found a higher percentage female patients compared with surveillance data . This finding might be attributable to differential overdiagnosis of LD in these groups, variations in insurance coverage and health care–seeking behavior, or other factors. Studies in Europe have found sex discrepancies in risk for tick bites and clinical presentation of LD that should be explored further in US research studies .\n\n【35】The estimated number of clinician-diagnosed LD cases in the United States is higher than the number reported through routine surveillance and consistent with previous estimates of LD underreporting . Underreporting occurs with other notifiable conditions and should not be confused with lack of treatment . Indeed, our study confirms that many LD cases not formally reported are nevertheless diagnosed and treated by clinicians. Furthermore, underreporting aside, the general concordance in LD epidemiology seen in MarketScan and surveillance data underscores that LD surveillance serves its central purpose: to identify and track patterns of disease.\n\n【36】Primary advantages of this study are the large sample size, ability to circumvent the obstacles and biases inherent in routine reporting mechanisms, detailed information about clinical and prescription data, and ability to follow patient data over time. Unfortunately, use of the 088.81 code to estimate _B. burgdorferi_ infections required several assumptions and correction factors. We calculated these correction factors using data from several analyses, each of which has its own inherent limitations and some of which have not yet been published. Nevertheless, the findings from these analyses were generally consistent with each other and with results expected on the basis of public health experience.\n\n【37】Our findings are subject to additional limitations. The MarketScan population is a convenience sample of the US population <65 years of age; although it is overall fairly representative, some differences exist. For example, certain age groups (20- to 29-year-olds) were 2%–3% underrepresented, and others (50- to 59-year-olds) were 2% overrepresented, compared with the US population. Although our calculations adjust for age and geographic differences for all persons <65 years of age, other differences from the general population probably remain. In addition, the MarketScan database does not include military personnel, uninsured persons, or Medicaid/Medicare enrollees for whom risk for LD might differ from that of privately insured persons.\n\n【38】Our study highlights the need for continued coding research, particularly as health departments explore the feasibility of using electronic medical records to facilitate LD reporting. Additional information about LD coding practices will enable robust comparisons of ICD codes related to actual cases and facilitate future research using medical databases. In addition, ongoing research using the MarketScan databases and other sources will elucidate detailed epidemiologic and clinical aspects of LD that are not apparent in standard surveillance data.\n\n【39】In conclusion, our findings underscore that LD is a considerable public health problem, both in terms of number of cases and overall health care use. Furthermore, as with other conditions, underreporting in the national surveillance system remains a challenge. Continued research and education are necessary to enhance prevention efforts and improve diagnostic accuracy to reduce the effects of this disease.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7e84434e-0d47-4e60-8c94-5e5b4c01ffc3", "title": "Group B Strep: Successful Model of \"From Science to Action\"", "text": "【0】Group B Strep: Successful Model of \"From Science to Action\"\nThis panel session on the prevention of perinatal group B streptococcal disease (GBS) offered three perspectives on the history of the disease as an emerging concern in the United States. The panel addressed how recent epidemiologic research has effected a change in screening policy and a reduction in disease incidence, as well as the ongoing and future challenges presented by this disease.\n\n【1】### Perspective on U.S. Transformation and Global GBS Needs\n\n【2】The session opened with an overview of GBS epidemiology and policy, demonstrating progress that has been made since screening recommendations were created in the early 1990s, including a consensus statement issued in 1996. Since then, GBS has dramatically declined. Further evidence led the Centers for Disease Control and Prevention to issue universal screening policy guidelines in 2002. The guidelines were a transition from the risk-based and screening-based strategies used earlier. As guidelines become more firmly entrenched as part of current obstetric care, new data must be collected to determine whether the trend of the past few years toward a plateau in screening efficacy, with particular distinctions between black and white women, will be eliminated by the new universal screening guidelines.\n\n【3】### Obstetric Perspective on GBS Progress and Remaining Challenges\n\n【4】An obstetrician's point of view on the history of GBS detection and treatment and the current challenges in enacting a change in policy was presented. While screening and treatment rates rise, do adverse effects of intrapartum antibiotics, including antibiotic resistance or neonatal _Escherichia coli_ sepsis, also increase? Also reiterated was the need to continue to educate clinicians, both obstetricians and pediatricians, to help keep GBS identification and treatment in perspective.\n\n【5】### How Science, Advocacy, and Policy Help Women and Children\n\n【6】To offer a historic perspective of the successes and challenges posed by incorporating epidemiologic evidence into everyday policy, the session closed with a discussion of how political timing, advocacy groups, dedicated researchers, and skillful partnership development all have interacted simultaneously. As key surveillance revealed the epidemiologic picture of GBS, other factors played equally important roles in helping bring the science of GBS into the political limelight, which resulted in an active change in screening policy. GBS policy has the advantage of being firmly linked to a true reduction in disease; since 1993, >39,000 cases of GBS are estimated to have been prevented. As a working model of prevention, GBS policy continues to evolve to address ongoing issues.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "1734d463-9133-4d61-8305-00f79c6fa14b", "title": "Extensively Drug-Resistant Mycobacterium tuberculosis from Aspirates, Rural South Africa", "text": "【0】Extensively Drug-Resistant Mycobacterium tuberculosis from Aspirates, Rural South Africa\n### Extensively Drug-Resistant _Mycobacterium tuberculosis_ from Aspirates, Rural South Africa\n\n【1】Tuberculosis (TB) is the leading cause of death among HIV-infected persons in sub-Saharan Africa . Drug-resistant TB is an emerging public health threat in HIV-prevalent settings, but diagnosis is challenging because of the severely limited laboratory capacity for culture and drug-susceptibility testing (DST). TB diagnosis for HIV-infected patients is particularly challenging because these patients may be more likely to have smear-negative pulmonary disease or extrapulmonary TB . Extrapulmonary TB often is diagnosed by clinical findings, indirect measures (e.g. chemistry and cell count of cerebrospinal or pleural fluid, ultrasound of lymph nodes, or pericardial effusions), or smear microscopy for acid-fast bacilli from aspirated extrapulmonary fluid. However, drug-resistant TB is impossible to diagnose by these methods, instead requiring mycobacterial culture and DST .\n\n【2】The prevalence of multidrug-resistant and extensively drug-resistant TB (XDR TB) in South Africa has risen exponentially during the past decade. At our rural study site, ≈10% of all TB cases now are drug resistant, and >90% of TB patients are HIV infected . Death from XDR TB exceeds 80%; most infected persons die before sputum culture and DST results are known . To improve case detection and decrease diagnostic delay of drug-resistant TB among patients with suspected extrapulmonary TB, we initiated a program to aspirate lymph nodes and pleural fluid for culture and DST. We quantified the yield of these lymph node and pleural fluid aspirates for diagnosing XDR TB.\n\n【3】### The Study\n\n【4】We performed a retrospective cross-sectional study to determine the proportion of patients in whom TB and XDR TB could be diagnosed by culture of fine-needle aspiration of a lymph node or pleural fluid. Additionally, we sought to determine the yield of lymph node and pleural fluid aspiration beyond culture of sputum alone. All patients were eligible who had an aspirate of a noninguinal lymph node or pleural fluid sent for mycobacterial culture and DST during September 1, 2006–December 31, 2008, from the district hospital in rural Tugela Ferry, South Africa. This 355-bed hospital serves 200,000 Zulu persons.\n\n【5】Hospital protocol was for lymph node aspirates to be obtained at the bedside by using sterile technique and large-bore needle at the point of maximal swelling. Pleural fluid was obtained by standard thoracentesis. Care was taken not to introduce air while injecting the fluid specimen into mycobacterial blood culture bottles (BACTEC MycoF-lytic, Becton Dickinson, Sparks, MD, USA). Smear microscopy was not performed on the fluid aspirate. Bottles were transported to the provincial TB referral laboratory in Durban and cultured by using the automated BACTEC 9240 analyzer (Becton Dickinson) in which growth is continuously monitored for 42 days. _Mycobacterium tuberculosis_ was confirmed with niacin and nitrate reductase tests. DST of positive cultures was performed by using the 1% proportional method on Middlebrook 7H11 agar for isoniazid (critical concentration, 0.2 µg/mL), rifampin (1 µg/mL), ethambutol (7.5 µg/mL), ofloxacin (2 µg/mL), kanamycin (6 µg/mL), and streptomycin (2 µg/mL) . XDR TB was defined as _M. tuberculosis_ resistant to at least isoniazid, rifampin, ofloxacin, and kanamycin . Aspirate culture results were compared with sputum culture results if the patient also had a sputum culture performed within 2 weeks of the lymph node or pleural fluid culture. Standard practice was for 1 sputum specimen to be collected for smear microscopy and another specimen to be collected for culture, depending on the patient’s ability to expectorate. Sputum was cultured by using the automated BACTEC MGIT 960 system (Becton Dickinson); DST of positive specimens was performed as described above .\n\n【6】Medical records were reviewed for basic demographic and clinical data, including age, sex, HIV status, antiretroviral therapy, and TB history. The yield of lymph node and pleural fluid aspirates for detecting _M. tuberculosis_ and drug resistance was described by using simple frequencies. Incremental yield of aspirate was calculated for patients who had collection for sputum and either lymph node or pleural fluid aspirate. Ethical approval was obtained from the University of KwaZulu-Natal, Yale University, and Albert Einstein College of Medicine.\n\n【7】For 77 patients, either a lymph node (n = 34) or pleural fluid (n = 33) was aspirated for culture and DST during the study period . No patient had both pleural fluid and lymph node aspirates performed.\n\n【8】Of the 34 lymph node cultures performed, 12 (35%) grew _M. tuberculosis_ , 1 (3%) grew nontuberculous mycobacteria, and 2 (6%) were other bacteria that were not further speciated . Of the 12 positive _M. tuberculosis_ cultures, 9 (75%) were drug-susceptible _M. tuberculosis_ , and 1 (8%) was XDR _M. tuberculosis_ ; for 2, DST results were missing. Concurrent sputum samples were available for 6 (50%) of the 12 culture-positive _M. tuberculosis_ lymph node aspirates: 3 (50%) were concordant with the aspirate culture (2 drug-susceptible and 1 XDR), and 3 (50%) were sputum culture negative.\n\n【9】Of the 33 pleural fluid cultures performed, 9 (27%) grew _M. tuberculosis_ , 1 grew _Cryptococcus_ sp. and 1 grew another bacterium . Of the 9 _M. tuberculosis_ culture-positive pleural fluid specimens, 3 (33%) were drug-susceptible and 6 (67%) were XDR. Among these 9 patients, 5 (55%) had concurrent sputum samples available: 3 (60%) were concordant with the aspirate culture (1 drug-susceptible and 2 XDR), and 2 (40%) were sputum culture negative.\n\n【10】From 17 patients, a sputum sample and either a lymph node or a pleural fluid aspirate was collected for culture and DST . For 14, at least 1 specimen was culture positive for _M. tuberculosis_ , of which 9 (64%) were positive for sputum, 11 (79%) were positive in the lymph node or pleural fluid, and 5 (36%) were positive by fluid aspirate alone, including 2 patients with XDR _M. tuberculosis_ .\n\n【11】### Conclusions\n\n【12】In this study of predominately HIV-infected patients suspected of having extrapulmonary TB, one third of positive _M. tuberculosis_ cultures from lymph node or pleural fluid aspirates were XDR. The additive yield for the diagnosis of any TB of these aspirates above sputum culture alone was 36%. Our findings suggest that strategies of solitary sputum culture or reliance on microscopy of nonsputum fluid analysis would miss opportunities to diagnose drug-resistant TB. Parallel culture of sputum and aspirate fluid appears to be of substantial added benefit for diagnosing XDR TB in this setting.\n\n【13】Our study has several limitations. Aspirates were collected on the basis of the attending physician’s clinical judgment. Therefore, the yield for _M. tuberculosis_ and XDR _M. tuberculosis_ may be overestimated, and other factors that may have influenced the physician’s suspicion of drug-resistant TB or increased the likelihood of a positive aspirate are not known without additional prospective study. It is also not possible to comment on the true incremental yield after comparing with sputum culture; sputum was not collected from all patients, nor were the reasons for lack of collection documented.\n\n【14】Nonetheless, as co-infection with HIV and TB increases in sub-Saharan Africa, the number of persons with extrapulmonary TB, both drug-susceptible and drug-resistant, is anticipated to rise . Therefore, diagnostic algorithms for extrapulmonary TB must consider the critical importance of extrapulmonary fluid culture and DST for diagnosis of drug-resistant TB, particularly in HIV-infected persons. Furthermore, this study highlights the need to validate novel diagnostic tests for _M. tuberculosis_ drug resistance on nonsputum fluids.\n\n【15】While conducting the study, Dr Heysell was a Burroughs Wellcome Fund/American Society of Tropical Medicine and Hygiene postdoctoral fellow in tropical infectious disease at Yale University and living in Tugela Ferry, South Africa. He is currently a fellow in infectious diseases and international health at the University of Virginia, with research interests in the epidemiology, diagnosis, and susceptibility testing of drug-resistant tuberculosis.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "38b6d8e3-2db6-430b-8dbf-693922377111", "title": "Carbapenemase-producing Bacteria in Patients Hospitalized Abroad, France", "text": "【0】Carbapenemase-producing Bacteria in Patients Hospitalized Abroad, France\n**To the Editor:** The emergence and rapid worldwide dissemination of carbapenemase-producing bacteria (CPB), especially carbapenemase-producing _Enterobacteriaceae_ (CPE), have prompted public health authorities to reconsider prevention strategies to control the spread of these organisms . In France, national guidelines recommend systematic screening for commensal CPE and glycopeptide-resistant enterococci (GRE) in all patients admitted to hospitals who have been hospitalized in other countries during the preceding 12 months  (repatriated patients), independently of whether transfer was direct from hospital to hospital (DT) or not (NDT). These guidelines also recommend implementation of presumptive patient isolation and contact precautions on admission . We conducted a 33-month survey at Hôpital Européen Georges Pompidou (HEGP), a university teaching hospital in Paris, of CPE and GRE in repatriated patients; we also investigated incidence of extended-spectrum β-lactamase (ESBL)–producing _Enterobacteriaceae_ and carbapenemase-producing _Acinetobacter baumannii_ and _Pseudomonas_ spp. in the same patient group.\n\n【1】During November 2010–July 2013, a total of 541 patients who had previously been hospitalized in a total of 71 other countries were admitted to HEGP. Rectal swab specimens were taken from 510 patients; 82 (16.1%) were DT, 415 (81.4%) were NDT, and 13 (2.5%) had an unclear history of transfer. Median patient age was 61 (range 12–98) years; 70% of patients were male. Results of screening by using antibiotic-containing Luria Bertani broths for enrichment and plating on selective media were negative for 354 (69.4%) of the 510 patients surveyed; 33 (6.5%; 16 DT, 17 NDT) patients were colonized with \\> 1 CPB and/or GRE and 123 (24.1%; 22 DT, 99 NDT, 2 unclear) with ESBL producers only. More specifically, 19.5% (16/82) of DT patients and 4.1% (17/415) of NDT patients were colonized with CPB and/or GRE (p<10 −5  by χ 2  test); 26.8% (22/82) of DT patients and 23.9% (99/415) of NDT patients were colonized with ESBL producers only (p = 0.67). Characteristics of the 33 patients carrying CPB and/or GRE are shown in the Table . Of all isolates, 191 produced ESBLs only.\n\n【2】Rates of resistance for ESBL _\\-_ producing _Enterobacteriaceae_ and CPE were, respectively, 53.1% and 57.1% to gentamicin, 16.7% and 32.1% to amikacin, 77.1% and 82.1% to nalidixic acid, 63% and 75% to levofloxacin, and 70.3% and 75% to ciprofloxacin. The _Pseudomonas_ spp. and _A. baumannii_ isolates were also multidrug resistant; all isolates were colistin susceptible.\n\n【3】Among the 33 colonized patients, 13 (39.4%) were not infected; 1 of the uninfected patients died. Seven patients were infected with \\> 1 CPB (health care–related in 2 patients, 1 of whom died), 4 patients with ESBL-producing _Enterobacteriaceae_ (health care–related in 1 patient, who died), and 9 patients with other bacteria (health care–related in 4 patients, 1 of whom died). No patients were infected with GRE. Overall, 60.6% of colonized patients were infected and 12.1% died; 35% (7/20) of the infections were health care–related (3 urinary tract device–related infections, 2 cases of ventilator-associated pneumonia, 1 infection at the site of a portacath, and 1 case of cellulitis).\n\n【4】Almost 25% of the repatriated patients carried ESBL-producing _Enterobacteriaceae_ (mostly CTX-M-15 producers); 6.7% carried CPB and/or GRE. By comparison, during the study period, only 10.8% of 2,314 systematically screened patients in the medical and general surgery intensive care units at HEGP (repatriated patients excluded) carried ESBL-producing _Enterobacteriaceae_ ; 1 carried _vanA_ _Enterococcus faecium_ (data not shown). For patients with no record of hospitalization abroad, no CPE isolates were found; other bacterial isolates included 1 _vanA_ _E. faecalis_ , 13 _vanA E. faecium_ (all known from previous outbreaks), 4 OXA-23–producing _A. baumannii_ , and 4 VIM- and 1 IMP-producing _P. aeruginosa_ .\n\n【5】Of the repatriated patients, 19.5% of DT patients (vs. 4.1% of NDT) and 23.9% (7 DT, 4 NDT) of those who were transferred to medical and general surgery intensive care units (ICUs) were CPB and/or GRE carriers. This finding highlights the role of severe underlying disease or injury and recent antimicrobial drug treatment. Among ICU patients, 3 died, most likely from underlying conditions, findings in line with the observation that carriage of or infection with multidrug-resistant bacteria is not the only predictor of death . Most of the 28 CPE isolates were resistant to fluoroquinolones and aminoglycosides except amikacin; 21 carried OXA-48–type genes, 7 of which were non-ESBL producers and were detected only around an ertapenem disk on Drigalski agar (Bio-Rad, Marnes-la-Coquette, France). All CPB, irrespective of species, showed imipenem hydrolysis in a recently described test  that was shortened and simplified by incubating colonies directly in antibiotic solution.\n\n【6】Although time-consuming and certainly perfectible, implementation of strict control measures to limit CPB and GRE spread  seems justified, a conclusion supported by the occurrence, since November 2010, of just 1 cross-transmission–linked CPB outbreak in an ICU at HEGP (after urgent intervention for cardiac arrest). Of particular concern is the high proportion of OXA-48–producing isolates in persons with no documented link to repatriation in France . This finding could be explained in part by the historical and demographic relationships between France and North Africa, where prevalence of OXA-48 is high, reflected in results from patients repatriated from that part of the continent.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "c835b0cc-2e95-4aff-84bf-6afead8acff1", "title": "Proximity to Goat Farms and Coxiella burnetii Seroprevalence among Pregnant Women", "text": "【0】Proximity to Goat Farms and Coxiella burnetii Seroprevalence among Pregnant Women\nDairy goat farms were implicated in the large Q fever epidemic (>3,500 human cases) in the Netherlands during 2007–2009 . However, most human infections remain asymptomatic or appear as a self-limiting febrile illness and are therefore not reported. Seroprevalence studies are needed to discover the true infection pressure in the population. We aimed to establish whether the presence of antibodies to _Coxiella burnetii_ , the etiologic agent of Q fever, is associated with physical proximity to infected small ruminant (dairy sheep and goat) farms.\n\n【1】### The Study\n\n【2】Serum samples from pregnant women were obtained from laboratories located in the high-incidence Q fever area in the province of Noord-Brabant. The samples had been collected during June 20, 2007–May 26, 2009, during a screening program for syphilis, hepatitis B, and HIV infection, which was routinely offered to all pregnant women. Serum samples were analyzed by using an immunofluorescence assay (Focus Diagnostics, Cypress, CA, USA) for detecting IgG and IgM against phases I and II of _C. burnetii_ infection in a single dilution of 1:64. An IgG II titer \\> 64 was considered indicative of past infection. Possible recent infection was defined as IgM II \\> 64 combined with either an IgG II or IgM I titer \\> 64.\n\n【3】We included 10 adjacent municipalities  that had an incidence of Q fever notifications over the study period of 0.5–19.6 per 1,000 population and with clear seasonal peaks . Median age of the 2,004 women in the study was 30 years (interquartile range 27–33 years) with no significant differences between the municipalities. Seroprevalence for past infection was 9.0% (181/2,004), ranging from 0% to 21% between the 10 municipalities . In 57 (31%) of 181 women, IgM II titer was \\> 64\\. Only 2 women had an IgM II titer \\> 64 and IgM I titer \\> 64 without an IgG II titer \\> 64\\. Seroprevalence for possible recent infection was therefore 2.9% (59/2,004), with a range of 0%–10% between the 10 municipalities.\n\n【4】For each pregnant woman, we calculated the distance from her home address to the closest farms in the following 3 categories: 1) dairy goat or dairy sheep farm where clinical Q fever (i.e. abortion waves) occurred during 2005–2009 (data from Animal Health Service) (8 farms \\[all goat farms\\] were identified in this way, of which 6 were located within or just outside of the study area) ; 2) dairy goat or dairy sheep farms at which bulk tank milk tested positive for _C. burnetii_ antibodies in the mandatory bulk tank milk monitoring program in 2009 (data from Food and Consumer Product Safety Authority; 12 goat farms were identified); and 3) a farm with >100 goats or sheep, irrespective of the infection status and production type (milk or meat) of the farm (data from the Ministry of Agriculture, Nature and Food Quality). Details of category 1 and category 2 farms are provided in Table A1 .\n\n【5】Univariate regression analysis showed that pregnant women living <2 km from a farm that had experienced clinical Q fever had a higher risk of testing positive for antibodies to _C. burnetii_ than those living \\> 5 km away (odds ratio 2.63, 95% confidence interval 1.33–5.20 for IgG II titer \\> 64 and odds ratio 6.58, 95% confidence interval 2.78–15.55 for possible recent infection). The increased risk for farms that had positive test results during monitoring of bulk tank milk was not significant. No increased risk was found for women who lived close to any farm with >100 animals. However, 98% of the population in the study area live within 5 km of such farms. In multivariate logistic regression analyses, taking into account address density of the neighborhood and other relevant variables, living <2 km from a farm with clinical Q fever remained a strong risk factor .\n\n【6】### Conclusions\n\n【7】The presence of antibodies against _C. burnetii_ , especially levels suggesting recent infection, is associated with living near a farm with infected dairy goats. This finding applied only to farms where animals had clinical Q fever and not for farms where tests of bulk tank milk were positive. A study in 2008 showed that persons who lived near (<2 km) an infected dairy goat farm had a much higher risk for Q fever than did persons who lived further away (>5 km) . However, these results were based on notified cases, i.e. patients with clinical signs. Because the link to goat farming has received substantial attention in the public media, persons living close to goat farms may have sought medical care for suspected Q fever more rapidly than those who lived distantly from goat farms. Our population-based serologic study had a control group of seronegative women and identified asymptomatic infections. Combined, both studies provide evidence that living near dairy goat farms that experience abortion waves increases the risk in humans for symptomatic and asymptomatic _C. burnetii_ infection.\n\n【8】A limitation of this study is that the exact infectious periods for each of the 20 farms in the study are unknown. Mandatory systematic monitoring of bulk tank milk only started in October 2009. However, for 17 of the 20 farms identified in the distance calculations in the present study, bulk tank milk testing results from 2008 were available from the records of the Animal Health Service. When an ELISA was performed, animals at 13 of the 17 farms tested positive for _C. burnetii_ antibodies . Furthermore, for 15 of the 17 farms, bulk tank milk tested positive by PCR in 2008. The assumption that persons became infected where they lived, although infection might have occurred elsewhere, might have weakened the association between house location and infected farms because of nondifferential misclassification. We did not account for circumstances that may play a role in transmission from farm to humans, such as wind, vegetation patterns, and soil conditions around infected farms . We assume that the effect of the voluntary vaccination program of small ruminants in 2008 had only limited effects on the study. Culling of pregnant animals on infected farms did not affect the results of the study because that began in December 2009.\n\n【9】We found an overall prevalence of IgG II antibodies of 9.0%. In comparison, during 2006–2007, a seroprevalence of 2.4% was found in a nationwide seroprevalence survey in the Netherlands, just before the first major outbreak . The present Q fever epidemic peaked right after the last data were collected for the present study. In the second half of 2009, seroprevalence for blood donors in the high-incidence area was estimated at 12.2% . The findings of the different seroprevalence studies are consistent with the view that Q fever newly emerged in the Netherlands, peaking in 2009, and that a high infection pressure has resulted in increased seroprevalence in the general population, including in pregnant women.\n\n【10】Whether infection during pregnancy is associated with adverse pregnancy outcomes remains uncertain. The international literature suggests this conclusion, but an analysis based on 1,174 of the 2,004 women included in the present study showed no evidence of adverse pregnancy outcome among women with antibodies to _C. burnetii_ . Despite uncertainties surrounding the clinical significance of asymptomatic seropositivity, this study supports the recommendation that pregnant women and persons at risk for chronic Q fever, such as patients with certain cardiovascular conditions, should avoid visiting infected farms.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "ed2301b9-ad22-45fd-8865-3fc62b62042d", "title": "Emergence of pstS-Null Vancomycin-Resistant Enterococcus faecium Clone ST1478, Canada, 2013–2018", "text": "【0】Emergence of pstS-Null Vancomycin-Resistant Enterococcus faecium Clone ST1478, Canada, 2013–2018\nVancomycin-resistant enterococci (VRE) are major nosocomial pathogens that have been observed worldwide . VRE were identified in Canada in 1993 , but rates of colonization and infection have remained relatively low for years . VRE bloodstream infections (BSIs) are of particular concern because they are associated with increased illness, length of hospital stay, healthcare costs, and death . Furthermore, increased rates of VRE BSI have been reported .\n\n【1】Since 1999, the Canadian Nosocomial Infection Surveillance Program (CNISP) has conducted surveillance of VRE BSIs, which includes collection of patient epidemiologic data and laboratory analysis of blood isolates, including multilocus sequence typing (MLST) and antimicrobial drug susceptibility testing . The MLST scheme for _Enterococcus faecium_ relies on the sequences of 7 essential housekeeping genes . However, in recent years, MLST nontypeable strains of _E. faecium_ have emerged that do not harbor the _pstS_ gene . These _pstS_ \\-null sequence types (e.g. sequence type \\[ST\\] 1421 and ST1424) are believed to have occurred through multiple inversion events and have been reported to be rapidly spreading in Australia, Denmark, and the United Kingdom . We report emergence and molecular characterization of a _pstS_ \\-null sequence type (ST1478) that is rapidly disseminating across acute care hospitals in Canada.\n\n【2】### The Study\n\n【3】CNISP is administered by the Public Health Agency of Canada and has conducted prospective surveillance for VRE infection and colonization since 1999 . CNISP is a partnership between the Centre for Communicable Disease and Infection Control and the National Microbiology Laboratory at the Public Health Agency of Canada and sentinel hospitals that participate as members of the Canadian Hospital Epidemiology Committee, a subcommittee of the Association of Medical Microbiology and Infectious Diseases Canada. Hospitalized patients with enterococcal bacteremia characterized as having vancomycin MICs \\> 8 mg/L were eligible . A patient was included more than once if a positive VRE blood isolate was identified >14 days after completion of therapy for a previous infection and believed to be unrelated to previous infection in accordance with best clinical judgement . Epidemiologic data were collected and VRE BSI isolates were forwarded to the National Microbiology Laboratory for further characterization .\n\n【4】All isolates that failed to give a sequence type by conventional MLST  were subjected to whole-genome sequencing on the MiSeq Platform . Assembled reads (contigs) were analyzed by using an in-house MLST tool based on 1 from the Center for Genomic Epidemiology website .\n\n【5】During 2013–2018, a total of 797 VRE BSI cases were reported among 62 participating acute care hospitals across 10 provinces for which 608 VRE BSI isolates were submitted. During this surveillance period, the rate of VRE BSI significantly increased from 0.16 cases/10,000 patient-days to 0.34 cases/10,000 patient-days (p<0.001) . These rates are much higher than those reported during 1999–2009 (0.005 cases/1,000 admissions during 1999 to 0.068 cases/1,000 admissions during 2009) . Of the 608 VRE BSI collected, different MLST types were identified, which included 4 of the newly reported _pstS-_ null sequence types ST1478 (n = 115), ST1421 (n = 7), ST1424 (n = 2), and ST1612 (n = 1). The increase of ST1478 isolates from 2.7% in 2013 to 38.7% in 2018 coincides with the increase in VRE BSI rates . After emergence of ST1478, STs that once dominated (ST18, ST117, and ST412) dramatically decreased. This shift in clonal types is similar to what has been reported in other countries after identification of these _pstS-_ null mutants .\n\n【6】As of 2018, ST1478 has been identified in 19 of 62 CNISP participating hospitals in 6 provinces. Of the 19 hospitals, 12 were from western Canada (British Columbia, Alberta, Saskatchewan, and Manitoba), 6 from central Canada (Ontario and Quebec), and 1 from eastern Canada (Newfoundland and Labrador, Prince Edward Island, New Brunswick, and Nova Scotia). Regionally, ST1478 represented 23.7% (65/274) of isolates tested from western Canada, 14.8% (49/332) of isolates tested from central Canada, and 50% (1/2) of isolates tested from eastern Canada during 2013–2018. Furthermore, among the VRE BSI identified in western Canada, a significantly higher proportion were ST1478 (56.5%, 65/115) compared with non-ST1478 (42.4%, 209/493; p = 0.006).\n\n【7】The predominant _van_ gene among ST1478 isolates was _vanA_ (99.1%, n = 114); only 1 isolate harbored a _vanB_ gene (0.9%). We determined resistance to antimicrobial drugs by using broth microdilution and GPALL1F Sensititer panels . We interpreted MICs by using breakpoints described by the Clinical and Laboratory Standards Institute . Antimicrobial drug susceptibility tests showed that ST1478 isolates have increased resistance to chloramphenicol (10.4%), daptomycin (13.0%), HL-gentamicin (80.0%), and tetracycline (91.3%) compared with non-ST1478 isolates . We verified daptomycin nonsusceptibility by using Etest . Clinically, increased daptomycin nonsusceptibility among the ST1478 isolates is of particular concern because daptomycin is 1 of the few remaining treatment options for VRE and resistance to it is an increasing clinical problem . CNISP surveillance data show a nonsignificant increase in the use of daptomycin to treat patients with VRE bloodstream infections. During 2015, a total of 53.7% of patients with a VRE BSI were given daptomycin, and during 2018, this proportion increased to 61.1% (p = 0.5).\n\n【8】During 2013–2018, we identified 115 ST1478 VRE BSI isolates among 110 patients. The median age of ST1478 patients was 58 years (interquartile range 52–69 years), and most (63.1%, 69/111) were male. The most commonly identified risk factors at the time of positive culture included use of a central venous catheter (69.5%, 57/82), solid organ transplant recipient (24.7%, 24/97), receiving hemodialysis (21.4%, 15/70), and receiving chemotherapy (15.7%, 11/70). A total of 31% (28/92) of ST1478 patients were already in an intensive care unit at the time of positive culture, and an additional 11 (12.1%) were admitted within 30 days of positive blood culture. The all cause 30-day mortality rate was 32.4%. Bacteremia patients with ST1478 VRE were similar with respect to age, sex, intensive care unit admission, and mortality rate compared with patients who had non-ST1478 VRE. Patients with ST1478 VRE were more likely to have undergone solid organ transplantation (24.7%) than patients with non-ST1478 VRE (12.9%; p = 0.005) . However a VRE outbreak was reported by 1 center in their multiorgan transplant unit, which might explain this finding.\n\n【9】### Conclusions\n\n【10】The emergence of the _pstS-_ null mutant ST1478 identified in acute care hospitals in Canada coincides with a major increase in VRE BSI rates. This increase might be attributed to an increased virulence/fitness of this strain type or changes in VRE infection prevention and control practices.\n\n【11】Clinically, the increased proportion of daptomycin nonsusceptibility among this emerging strain type is of concern. Future work, including whole-genome sequencing, and collection of enhanced epidemiologic and infection prevention and control practices data are being undertaken to provide a better understanding of the transmission, clonal relatedness, and evolution of this strain type within and between hospitals across Canada. Clinicians should be aware of these drug-resistant bacteria.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "78fad5bc-dfab-4e1f-a8d8-296b49d59a9d", "title": "Statewide School-located Influenza Vaccination Program for Children 5–13 Years of Age, Hawaii, USA", "text": "【0】Statewide School-located Influenza Vaccination Program for Children 5–13 Years of Age, Hawaii, USA\nSeasonal influenza reportedly results in 200,000 hospitalizations and 36,000 deaths annually in the United States . In addition, ≈31 million outpatient visits are attributable to influenza during seasonal epidemics; annual projected direct medical costs are $10.4 billion and lost earnings are $16.3 billion . Although schoolchildren are not considered at high risk of dying from influenza, annual illness attack rates in schoolchildren were >40% in some years . In addition, children 5–18 years of age may be the primary source of communitywide influenza transmission . Focused prevention of influenza infection among children may not only prevent childhood illness but also decrease school absenteeism and reduce the negative impact of influenza infection among working adults and elderly persons . Disease modeling suggests that if influenza vaccine was limited, as might be expected during a pandemic, vaccinating schoolchildren might be the most efficient approach to reducing overall numbers of influenza infections .\n\n【1】On February 27, 2008, the Advisory Committee on Immunization Practices (ACIP) expanded the recommended ages for annual influenza vaccination of children to include all children 6 months–18 years of age . Nationwide, this new recommendation added ≈30 million children to the cohorts targeted for annual influenza vaccination and invoked calls to consider alternatives to the physician’s office for administering the vaccine .\n\n【2】School-located influenza vaccination (SLIV) clinics have been proposed as a way to get more children vaccinated. Results from several trial SLIV programs indicate that vaccinated schoolchildren and their families experience lower rates of influenza-associated illnesses . However, many recent studies used only live attenuated influenza vaccine (LAIV), administered as a nasal spray, donated by the manufacturer . LAIV is currently not recommended for use in children with asthma or other underlying medical conditions that predispose them to complications from wild-type influenza infection; yet these potentially high-risk children may derive the most direct benefit from vaccination against influenza.\n\n【3】As a result, during the 2006–07 school year, the State of Hawaii Department of Health (DOH) conducted a pilot project to assess the feasibility of providing a choice of intranasal LAIV or intramuscular trivalent influenza vaccine (TIV) to students at 3 elementary schools. The pilot project achieved an overall vaccination rate of 35%. On the basis of this success, DOH conducted a statewide SLIV program during the 2007–08 influenza season. We present data on the logistics and outcomes of implementing this large-scale public health program; these data may be relevant to jurisdictions planning annual seasonal influenza vaccination programs for children and for responding to recent ACIP recommendations that place school-aged children among the top priority groups for receiving Influenza A (H1N1) 2009 Monovalent Vaccine .\n\n【4】### Methods\n\n【5】The 2007–08 SLIV program focused on all children 5–13 years of age in Hawaii. Public and private elementary and middle schools were identified through the databases of the Hawaii Department of Education, Hawaii Association of Independent Schools, and Hawaii Catholic Schools. All schools were invited to participate and could register online.\n\n【6】Influenza vaccination was voluntary and required written consent from the child’s parent or guardian. Information packets contained an explanatory letter, the 2007–08 Influenza Vaccine Information Statements for both vaccine formulations, and a consent form. A parent or guardian could specifically consent to their child receiving TIV or LAIV or indicate that either formulation was acceptable. To guide the parental decision, the consent form asked about potential contraindications to each formulation. If the child was <9 years of age and had not been vaccinated with 2 doses of influenza vaccine in any previous year (or if the parent, medical provider, or both were uncertain), parents were asked to consent to 2 doses of influenza vaccine administered \\> 6 weeks apart. The packets were distributed to children at participating schools in August 2008; the consent forms were collected by the schools ≈4 weeks later. The packets were available online in 11 languages other than English commonly spoken in Hawaii . Families incurred no cost and received no incentive for participation. DOH provided the vaccine and all clinic supplies. School-based faculty and staff also were offered influenza vaccination at no cost through SLIV clinics.\n\n【7】Vaccines were administered at school during normal school hours over 59 working days from October 15, 2007, through January 31, 2008. School clinic dates and times were established through dialogue with the principal or other administrator of each school. School administrators at participating schools were asked to provide a large room for up to 4 hours (two 4-hour sessions >6 weeks apart for elementary schools), notify parents of the date and time of the scheduled vaccination clinic, and escort students to and from clinics. DOH staff, assisted by contract courier services, transported clinic supplies from DOH offices to each school 1 day before the clinic. DOH paid for all vaccine from state and federal funds and arranged all clinic staffing. No money was billed to third-party payers or collected on site.\n\n【8】Each school clinic required 1 clinic manager and \\> 1 registration personnel and vaccinators, according to the number of vaccinations anticipated there. Staffing data (position, affiliation, hours spent) were determined prospectively and available for 92% of all clinics. For the remainder, we imputed their values from the mean value of the clinics with data available.\n\n【9】Participating DOH personnel, volunteers, and contract staff received DOH-developed training tailored to their respective program responsibilities. DOH verified that all vaccinators were licensed health professionals in good standing; the exception was nursing school students who worked under the on-site supervision of licensed faculty preceptors. In accordance with Hawaii state law, volunteers for DOH, including Medical Reserve Corps volunteers, were considered state employees for liability purposes.\n\n【10】We defined student age in years as date of first influenza vaccination administered by this program minus the child’s date of birth divided by 365.25. Student population size for the cohort of children aged 5–13 years residing in Hawaii on July 1, 2007, was obtained using publicly available census estimates .\n\n【11】To calculate school-level participation rates, the number of children enrolled in grades kindergarten through 8 (K–8) at individual schools was obtained from the Hawaii Council of Private Schools, Private School Enrollment Report 2007–2008, and the State of Hawaii Department of Education Official 2007–08 Enrollment. To calculate the proportion of school staff vaccinated, we obtained the total number of personnel employed at each school from the participating schools.\n\n【12】To calculate clinic throughput times, a nonrandomized subset of children were given a card documenting the time they arrived at the registration area. The card was collected and exit time noted as the child left the clinic area after vaccination. The difference between exit and arrival times, rounded to the nearest minute, was the clinic throughput time. Although we did not randomly select these students, we tried to obtain a representative sample by distributing the time-stamped cards to 10 children spread over the operational period at each of nearly 200 clinics, totaling ≈3% of vaccinated students.\n\n【13】We collected reports of potential adverse events after vaccination sent to the national passive surveillance system (Vaccine Adverse Event Reporting System) . We also maintained records of calls DOH received directly from parents, clinicians, or schools regarding potential adverse events.\n\n【14】Costs were estimated from the DOH perspective so that in-kind contributions from schools whose staff disseminated or collected consent materials and/or escorted students to clinics were not included. We estimated the cost of in-kind contributions of DOH staff and volunteers using 2007 salary data available from the US Department of Labor Bureau of Labor Statistics . Clinic manager hourly costs were defined as the national hourly wage for registered nurses at the 75th percentile ($35.18); vaccine administrator hourly costs were defined as the median hourly wage for registered nurses ($30.44), and registration personnel hourly costs were defined as the median hourly wage for medical records and health information technicians ($14.08).\n\n【15】We used Epi Info 2000 to calculate means and standard deviations for computing 95% confidence intervals (CIs); Epi Info version 6 χ 2  test for trend was used to assess trends in proportions over successive age cohorts . Correlation coefficients were calculated by using Excel (Microsoft, Redmond, WA, USA).\n\n【16】### Results\n\n【17】Of 67,203 schoolchildren for whom consent was obtained, 63,153 (94%) received \\> 1 doses of influenza vaccine. Of schoolchildren receiving at least 1 influenza vaccination, 1,078 (1.7%) were <5 years of age, 60,760 (96%) were 5–13 years of age, and 1,136 (1.8%) were >13 years of age; age information was missing for 179 (0.3%) children. Thus, 46% of 5–13-year-old children in Hawaii, as determined by census data (n = 132,775), were vaccinated against influenza at SLIV clinics.\n\n【18】Twenty-nine percent (n = 18,173) of children who received a first influenza vaccination at an SLIV clinic received a second dose, representing 60% of the 30,357 children <9 years of age who participated in the program. A total of 81,326 first or second vaccine doses were provided to children at SLIV clinics.\n\n【19】In addition, 9,306 (43%) of the 21,625 school staff members and 1,054 clinic volunteers were vaccinated at SLIV clinics, bringing the overall total number of influenza vaccinations administered through the program to 91,686. These 2 groups represented 14% of vaccinated persons.\n\n【20】A total of 340 (90%) of 377 elementary and middle schools in Hawaii participated in the program. This number constitutes 242 (96%) of 251 public schools and 98 (78%) of 126 private schools.\n\n【21】A total of 622 SLIV clinics were conducted; 345 (55%) were first-dose clinics during October through December 2007; 5 large schools had 2 first-dose clinics because of the number of vaccinees anticipated. Another 277 (45%) were second-dose clinics conducted at elementary schools in January 2008 to fully vaccinate children <9 years of age who had never received 2 doses of influenza vaccine in a prior year.\n\n【22】Unless indicated otherwise, the following analyses are restricted to children 5–13 years of age. In Honolulu County, 48% of schoolchildren were vaccinated; in Hawaii’s other counties, 39%–42% were vaccinated .\n\n【23】Vaccinations peaked at 54% for children 6 years of age. In successive age cohorts, the vaccination rates gradually declined to 30% for children 13 years of age .\n\n【24】Grade-level enrollment data required for calculation of school-level participation rates were available for 291 (86%) of the 340 participating schools. The proportion of children vaccinated at individual schools ranged from 3% to 84% . However, the mean proportion of schoolchildren vaccinated was similar between the 208 public (43.4%; 95% CI 42.1%–44.7%) and 83 private (45.0%; 95% CI 41.5%–48.5%) schools. School size, as measured by the number of students in grades K–8, correlated poorly with vaccination coverage rates obtained at the school (correlation coefficient: –0.05).\n\n【25】Most (56%) parents selected TIV for their child; 27% chose LAIV; and 17% consented to have their child receive either formulation (p<0.001). Vaccine formulation preference did not differ significantly by student sex (p = 0.19). However, the trend for parents to select TIV (only) over LAIV or either vaccine in successive annual age cohorts was significant (p<0.001).\n\n【26】Clinic throughput times were obtained for 1,970 schoolchildren vaccinated at 199 separate clinics. Median throughput time was 4 minutes; >90% of schoolchildren transited the clinic in < 10 minutes.\n\n【27】Physicians submitted vaccine adverse event reports for 3 children who received TIV through the school program; no events were medically serious or required hospitalization. In addition, DOH staff were informed of 4 other minor incidents, of which 3 were vasovagal syncopal episodes after TIV administration.\n\n【28】A total of 16,920 person-hours were expended to conduct 345 first-dose clinics and 277 second-dose clinics . Mean duration of the first-dose clinic was 3.1 hours and typically required 1 clinic manager, 5 or 6 registration staff members, and 6 vaccinators. Second-dose clinics averaged 2.3 hours and used 2 registration staff members and 2 or 3 vaccinators **.**\n\n【29】DOH public health nurses accounted for 80% of clinic manager person-hours; other DOH staff provided almost 75% of person-hours for registration . Contract nurses accounted for almost 50% of all vaccinator staff hours; DOH public health nurses and nursing school students filled most of the remaining need. Other organizations providing substantial staff support included the Hawaii Medical Reserve Corps and the US Department of Defense **.**\n\n【30】Program operation costs were estimated to be $2,480,493; nearly half was used to purchase vaccine. Forty-six percent of vaccine doses were acquired through the federal Vaccines for Children program . The all-inclusive cost of administering 90,632 doses of influenza vaccine to participating children and school staff, comprising vaccine purchase and administration, healthcare staff resources, printing costs, data management, media promotion, and schoolchild participation rewards, was $27.37 per dose.\n\n【31】### Discussion\n\n【32】This is the largest reported school-located influenza vaccination program in the United States and the only one to offer parents a choice of influenza vaccine formulation, i.e. TIV or LAIV . Through this effort, we vaccinated nearly half of all children 5–13 years of age in Hawaii against influenza.\n\n【33】Several valuable lessons can be learned. School vaccine coverage rates varied widely throughout the state, regardless of school size or school type (i.e. public vs. private), suggesting that other factors were important in determining acceptance of mass influenza vaccination at school. More work is needed to examine socioeconomic, cultural, and school-level characteristics that might influence participation in school-based vaccination programs.\n\n【34】A large proportion of public and private schools participated in the SLIV program. DOH promoted the program to school principals as a means to reduce influenza illness among students and staff, potentially resulting in decreased influenza-associated absenteeism. The high participation rates suggest that many principals believed the investment of student and staff time for the SLIV clinics was acceptable.\n\n【35】Vaccination rates declined among older, middle school–aged cohorts, suggesting that obtaining high influenza vaccination rates among high school-aged students may be challenging. This observation is consistent with low influenza vaccination rates attained at a high school in Hawaii during the 2006–07 school year and with our experience with school-located hepatitis B vaccination programs in Hawaii during the 1990s .\n\n【36】The second-dose clinics were resource intensive. These clinics accounted for 45% of clinic sessions but for only 22% of vaccine administered. If the 2006 ACIP recommendation to vaccinate all children beginning at 6 months becomes widely implemented, successively fewer school-aged children would require a second dose of vaccine, potentially obviating the need for second-dose SLIV clinics.\n\n【37】When offered a choice, parents selected the injectable vaccine for their children over the nasally administered attenuated formulation by a 2-to-1 margin. Although some parents would have based their choices on differences in contraindications to the 2 vaccines, the magnitude of this difference and the increasing preference for TIV for older children suggests that many parents simply preferred the injectable formulation. We did not investigate the reasons but speculate that parents were more familiar with TIV. Anecdotal reports from clinics indicated that some children sought the injectable vaccine to demonstrate their machismo to their peers, and several children are known to have switched from the LAIV to the TIV waiting line, despite their parents’ wishes. Ongoing assessment of these attitudes and behaviors are important to determine whether the preference for injectable vaccine persists as parents become more familiar with nasally administered vaccine.\n\n【38】Despite substantial program promotion and favorable media attention, we did not reach our informal target of 50% coverage. Achieving higher rates may require changes in parental perceptions about the risk for influenza illness in children, with the potential benefits of vaccination outweighing any perceived potential risks to the child. Still, almost 30% of schools vaccinated more than half of their students in grades K–8 before the 2008 ACIP expanded recommendation . The new recommendation for school-aged children may facilitate achieving higher vaccination coverage in future years.\n\n【39】Each year, <30% of children 5–13 years of age are estimated to see a medical provider at any time during October through December, i.e. the usual time frame for administering influenza vaccine . Applying this figure to the population of children vaccinated in our SLIV program, we find that ≈42,532 additional visits to a physician’s office would be needed to reach the same level of coverage. Whether sufficient capacity exists to add these additional office visits to providers’ schedules can be debated; what seems clear, however, is that the SLIV clinics remove potentially substantial barriers to vaccination, i.e. the need for parents to secure transportation and take time away from work or other duties to bring their child into the provider’s office. SLIV clinics are convenient for parents and do not generate indirect costs through lost wages. Research among adults indicates that the mean cost of delivering influenza vaccinations is lower when they are provided in mass vaccination settings compared with scheduled office visits . More study is needed to comprehensively assess the costs and benefits of providing influenza vaccines to children at SLIV clinics.\n\n【40】Our study had at least 1 major potential limitation. Because the State of Hawaii has a relatively small population, one might question the generalizability of our findings to larger jurisdictions on the US mainland. However, with a population >900,000, Honolulu County has more residents than 98% of other US counties and one of the nation’s highest population densities . The SLIV program in Honolulu County generated a vaccination coverage rate comparable with that of the other 3 less populous counties, each with 60,000–172,000 residents. The consistency of our experience across large and small counties suggests that our results may be applicable to other jurisdictions considering implementation of an SLIV program.\n\n【41】This program demonstrates the feasibility of large-scale SLIV programs among children in elementary and middle schools offering both TIV and LAIV. No medically serious vaccine-associated adverse events were identified. Anecdotal information indicated that the program was well received; many parents and school staff requested that we repeat the SLIV clinics the following year. In response, the statewide program was continued for the 2008–09 school year and achieved similar rates of vaccination coverage for the 5–13-year-old cohort, as well as school participation rates, compared with the inaugural year (45% and 89%, respectively). The logistical effort required to conduct this program greatly improved our organizational capacity to conduct sustained mass vaccination clinics for children as might be required in response to a pandemic . In addition, this program laid the foundation for an ongoing evaluation of the potential effect of widespread influenza vaccination of school-aged children on the community at large.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "b47bec5a-328d-4596-99b6-ad69170cce3a", "title": "Analytic Errors in Analysis of Public Health Survey Data Are Avoidable", "text": "【0】Analytic Errors in Analysis of Public Health Survey Data Are Avoidable\nPEER REVIEWED\n\n【1】*   Empirical Example\n*   Shared Responsibility\n*   Acknowledgments\n*   Author Information\n*   References\n\n【2】Data from surveys are an invaluable resource for health research, and using correct statistical techniques is important when analyzing public health survey data to produce accurate findings that can inform policy and program decision-making. Yet, as a peer reviewer of scientific journals, I regularly find that many studies that analyze survey data used inappropriate methods of estimation, known as analytic error . Some examples of these errors include not applying data weights, overlooking complex survey design, and not properly subsetting data when analyzing subpopulations. Initially I found this surprising, as by the time a study is under review at a journal, multiple parties (eg, researchers, peer reviewers, journal editorial boards) have had an opportunity to identify these analytic errors.\n\n【3】Consistent with my own experience as a peer reviewer, empirical studies have found this type of error to be common . A meta-analysis of 100 peer-reviewed journal articles that performed secondary analysis of data from nationally representative surveys with complex sample designs found 1,100 instances in which analytic errors may have occurred, and in 616 instances these errors were likely present . Of the articles reviewed, 28% did not mention weighting data, 40% did not mention accounting for complex sample design to properly estimate variance, 59% used improper language when discussing results in the text (eg, estimates generalized to the “sample” as opposed to the “population”), and 79% did not use proper significance testing . These results are particularly alarming because the meta-analysis reviewed articles _published_ in peer-reviewed journals.\n\n【4】Empirical Example\n\n【5】Although previous work has documented the extent of analytic errors in published studies that describe analysis of data from surveys with complex design , understanding how committing such errors could affect one’s empirical findings, and the conclusions drawn from them, is important. To show this, I used data from the 2013 National Ambulatory Medical Care Survey (NAMCS) and provide the following example that illustrates the consequences of committing analytic errors, specifically not weighting or accounting for sample design when analyzing data from a nationally representative survey . This example examines the percentage difference in physician office visits made by non-Hispanic white and Hispanic adults with multiple chronic conditions (MCC).\n\n【6】In the first series of estimates, I considered neither weights nor NAMCS sample design. The result shows that the percentage of office visits among non-Hispanic white adults with MCC was significantly higher ( _P_ < .001) than that of Hispanic adults with MCC (34.3% vs 30.3%, respectively). Without weighting, these estimates are only for the 13,279 visits sampled; therefore, using the estimates to generalize to office visits for all US adults would be inaccurate.\n\n【7】In the second series, I applied weights without accounting for the NAMCS sample design. The percentages change to 35.8% for non-Hispanic white adults and 33.0% for Hispanic adults. The difference still appears significant ( _P_ \\= .02) but only by 2.8 percentage points in magnitude, less than the 4.0 percentage-point difference between the unweighted estimates. Although by using weights these estimates are now generalizable to all physician office visits by US adults, and no longer limited to only the 13,279 visits included in the sample, this difference between the estimates is invalid without accounting for the complex sample design of NAMCS.\n\n【8】In the third series, I used the proper procedure by weighting the data and accounting for the survey’s sample design. The percentages remain unchanged; however, using sample design information properly accounts for additional covariance attributed to clustering from the 2-stage, stratified design of the 2013 NAMCS. This is evident by wider confidence intervals and larger standard errors in the third series of estimates, compared with those in the first and second series. As a result, the difference between the 2 racial/ethnic groups is no longer significant ( _P_ \\= .26).\n\n【9】In this example, not properly accounting for sample weights and complex sample design would have led to inaccurate estimates not generalizable to the target population and erroneous reporting of a significant difference that does not exist (type I error). More resources detailing how to properly analyze public health survey data (eg, Korn and Graubard’s widely used _Analysis of Health Surveys_ ) are available .\n\n【10】Shared Responsibility\n\n【11】Many have argued that prevention of analytic errors lies primarily with the researcher, who must understand the survey data, use appropriate estimation techniques when analyzing them, use proper language when describing methods and results, and ideally have a coauthor or colleague verify estimates and proper use of the software programs used to generate the estimates . In fact, with proper training and understanding, researchers who analyze survey data should be able to remove this source of error from their research. However, other parties have a role as well , and actions are being taken to help prevent and remove analytic error from empirical studies.\n\n【12】For example, the National Center for Health Statistics — the organization that collects and releases NAMCS and other public health survey data — ensures that detailed documentation on each survey is accessible and that it clearly describes how to properly analyze the data. The peer-review process is important to identify analytic errors and preclude manuscripts with these errors from being published. This review process involves both peer reviewers as experts in the field and editors of the journal, who both check for these types of errors. If it is unclear whether an analytic error is present, the reviewer can notify the author and editors and request for clarification. If a reviewer is not familiar with the appropriate estimation techniques, the editor in chief will seek statistical consultation from another reviewer. Some journals’ editorial boards provide guidance for reporting analyses of survey data, and many ensure that statistical reviewers assess certain manuscripts (eg, _Preventing Chronic Disease_ for example, does this). Having statistical reviewers is associated with lower prevalence of analytic errors . Finally, professional organizations such as the American Association for Public Opinion Research have developed guidelines (or “Best Practices”) on analyzing and reporting survey data using appropriate methods of estimation, similar to the approach of guidelines that are present for randomized, controlled trials (ie, Consolidated Standards of Reporting Trials \\[CONSORT\\] statement ) and qualitative research (ie, Consolidated Criteria for Reporting Qualitative Studies \\[COREQ\\] checklist ).\n\n【13】Analytic errors are avoidable, yet if unchecked they can have adverse consequences to our understanding of various health topics and the potential to misguide future research. Although the responsibility of their prevention primarily belongs to the researcher, other parties such as organizations conducting surveys, peer reviewers, journal editors, and professional associations share this responsibility. Making a collective effort to reduce analytic errors in health survey research is important for generating accurate results and making better-informed policy and programmatic decisions.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e5cb2bcb-071f-41fb-bd9a-cb70bda05c62", "title": "Spatial Distribution and the Animal Landscape", "text": "【0】Spatial Distribution and the Animal Landscape\nRoelandt Savery . The Garden of Eden . Oil on wood (55 cm × 107 cm). National Gallery, Prague, Czech Republic\n\n【1】Peace and harmony reign in Roelandt Savery's The Garden of Eden, on this month's cover of Emerging Infectious Diseases. Animals sprawled across the expansive landscape move languidly, unaware of any danger to them from each other or their surroundings, their graceful contours and anatomic details of greater interest to the artist than their natural temperament. The pastoral scene lends itself to the paradoxical rendition, common in Flanders during Savery's time, of the imaginary put in realistic terms.\n\n【2】Flanders (in today's Belgium) was the region of Europe known for great cultural achievements, particularly in the Middle Ages and during the 16th and 17th centuries, when Flemish, also known as Netherlandish, art flourished. Landscape painting, which had been merely the setting for religious scenes, became important in its own right in the art centers of Antwerp and Brussels, and when Joachim Patinir, one of the first to create stand-alone landscapes, was praised by Albrecht Dürer as a \"good landscape painter,\" the term landscape was elevated for the first time outside the context of Italian art . The genre was brought to new heights by such masters as Jan Bruegel the Elder and Peter Paul Rubens.\n\n【3】Contemporary of these great masters and the best-known in a family of artists, Savery was born in the Dutch village of Kortrijk and grew up in Amsterdam, where he studied under his brother Jacob and artist Hans Bol. He traveled widely and worked for Henry IV in France and Emperor Matthias in Vienna, but his stay in Prague, where he was painter and etcher of landscapes, animals, and still lifes in the court of\n\n【4】Rudolf II von Habsburg, produced the work for which he is most remembered .\n\n【5】Rudolf, emperor of the Holy Roman Empire and king of Bohemia and Hungary, was known for his eccentricity, one sign of which was his famed menagerie of exotic animals. Obsessive collector of unusual objects and lover of art and architecture, as well as science, the offbeat emperor surrounded himself with such artists as Bartholomeus Spranger and Giuseppe Arcimboldo, and with scientists, among them astronomers Tycho Brahe and Johannes Kepler . This period saw a rise in the study of mathematics, optics, physics, biology, and the development and propagation of hybrid flowers as scientific and economic pursuits.\n\n【6】Savery's peers in a thriving art community increasingly specialized, producing landscapes unlike any up to this time. He dabbled with and influenced several genres. His spectacular views of precipitous rocks and waterfalls influenced Dutch landscape painting. His much sought-after floral still lifes, along with those of Jan Brueghel the Elder, Ambrosius Bosschaert the Elder, and Gillis van Coninxloo, which they surpassed in tonal quality and realism, mark the beginning of the great age of Dutch flower painting .\n\n【7】The first Dutch artist to do so, Savery turned his talented hand to animal painting. Riding a strong interest in zoology at the Habsburg court and with full access to Rudolf's menagerie, he became a leader in this genre, painting and drawing a great variety of animals: pelicans, ostriches, camels, and the now extinct dodo, which he immortalized in several works. One of these, in which the dodo was the main subject, was presented to the British Museum in 1759 as painted \"in Holland from the living bird\" . Once he was paid 700 guilders, then an enormous sum, to paint \"all the animals of the air and earth\" . Soon his mythologic scenes became thinly disguised opportunities to paint more species. His favorite subjects were Orpheus charming the beasts, Noah's Ark, and scenes of Eden.\n\n【8】The Garden of Eden is an animal painting of the style initiated by Savery amidst increased interest in biologic research and rare creatures. The biblical narrative is only the vehicle for displaying the water buffalo, the dromedary, and all manner of unusual beasts. \"Swarms of living creatures\" of the mountain or the prairie, in any color, texture, shape, and form, inhabit the unlikely locale, predator frolicking with prey, desert beast with tropical. In a virtuoso display of spatial depth, Savery places Adam in the distant horizon, under the Tree of Knowledge, naming the animals \"Whatsoever Adam called every living creature, that was the name thereof\" (Gen. 2:19).\n\n【9】Naming the animals has long fascinated humans, from Aristotle to Linnaeus. And Adam's awesome task has not been completed, as indeed no one has been able to name or paint \"all the animals of the air and earth.\" New species continue to be found (e.g. recently Mus cypriacus ). Others are dead as the dodo.\n\n【10】Like other interpretations of Eden, Savery's is not so much a geographic location as an idealized landscape. And while his beasts' exotic perfection was painted with meticulous attention to detail, the realism extended only to form, species variation clearly at odds with normal species distribution. Today, the imaginary in Savery's painting has become real. Dissimilar and diverse species from around the globe are mingled, and not just for a photo opportunity.\n\n【11】In collections of exotic animals, as well as in homes, parks, and the wild, far from their places of origin, animals wander the globe imported as pets, contraband cargo on board ship or plane, or in natural habitat claimed by urban and agricultural development. And contrary to notions of Eden, coexistence has been all but peaceful and harmonious. From the merger of bats, pigs, and people (Nipah virus) to the mingling of African rodents with their North American relatives (monkeypox), from AIDS to SARS to avian flu, the changing landscape of animal habitat is changing the geography and ecology of disease transmission.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "00275a36-d272-427a-8130-7f2bd3e4031f", "title": "Factors Associated With Avoiding Health Care Among Community-Dwelling Medicare Beneficiaries With Type 2 Diabetes", "text": "【0】Factors Associated With Avoiding Health Care Among Community-Dwelling Medicare Beneficiaries With Type 2 Diabetes\nAbstract\n\n【1】**Introduction**\n\n【2】Health care avoidance by Medicare beneficiaries with chronic conditions such as type 2 diabetes can result in adverse health and economic outcomes. The objective of this study was to describe factors associated with choices to avoid health care among Medicare beneficiaries with type 2 diabetes.\n\n【3】**Methods**\n\n【4】We used a survey-weighted logistic model and the nationally representative 2016 Medicare Current Beneficiary Survey to analyze data on 1,782 Medicare beneficiaries aged ≥65 with type 2 diabetes, to examine associations between Medicare beneficiaries’ decisions to avoid health care and multiple factors (eg, dissatisfaction with information given by providers, health problems that should have been discussed with providers but were not, worry about health more than other people their age).\n\n【5】**Results**\n\n【6】Of our study sample, 26.1% reported they avoid health care. Five factors were associated with avoiding health care: delaying care (vs not) because of costs (adjusted odds ratio \\[aOR\\] = 2.06; _P_ \\= .005); having health problems that should have been discussed with providers but were not (vs having discussions) (aOR = 1.50; _P_ \\= .04); worrying (vs not) about health more than other people their age (aOR = 2.13; _P_ < .001); self-reporting “other” minority race (vs non-Hispanic White) (aOR = 2.01; _P_ \\= .006); and education levels. Participants with less than a high school diploma (aOR = 1.95; _P_ \\= .001) and participants with a high school diploma only (aOR = 1.49; _P_ \\= .049) were more likely than participants with an education beyond high school to report avoiding health care.\n\n【7】**Conclusion**\n\n【8】Approximately 1 in 4 Medicare beneficiaries with type 2 diabetes avoid health care. We found inequities in care-seeking behavior by race/ethnicity and education level. Health care perceptions and lack of appropriate discussion of health care concerns with health care providers are also associated with this behavior. Clinical interventions (eg, improved patient–provider communication) and educational outreach are needed to decrease the numbers of Medicare beneficiaries who avoid health care.\n\n【9】Introduction\n\n【10】Older adults living with type 2 diabetes are faced with a condition that is complex to manage, necessitating frequent interactions with health care professionals . Failure to seek recommended care (eg, checkups or screenings) can make these older adults particularly vulnerable to many diabetes-related complications. Although barriers to health care are well-described in the literature, additional study is needed to understand why people with chronic conditions choose to avoid health care, especially among Medicare beneficiaries who have type 2 diabetes .\n\n【11】Health care avoidance occurs when a person or population “distances itself from utilization of preventive health services, treatment seeking, and treatment adherence” . Health care avoidance is associated with an interplay of demographic, geographic, and psychologic factors, with the psychologic factors being most predictive of the decision to avoid necessary health care . Although cost of care is a factor in avoiding health care, individual factors, such as low health self-efficacy, poor previous experiences with medical providers, fear or dislike of medical procedures, discomfort with body examination, and fear of a serious diagnosis also contribute to these decisions .\n\n【12】A national survey conducted in 2018 found that approximately 40% of Americans skip recommended tests or treatments, and approximately 44% fail to seek care when sick or injured because of costs . Of those who delayed or skipped care, approximately 86% had insurance coverage . These findings have implications for people with chronic illnesses such as diabetes, which affects approximately 25% (approximately 10 million) of people aged 65 or older in the United States . Delayed treatments or skipped medications among people with diabetes can lead to increased risk and severity of complications (eg, amputation, kidney failure, blindness, cardiovascular disease), which may result in greater use of the emergency department and longer inpatient stays. According to the Centers for Medicare & Medicaid Services, Medicare spent an estimated $42 billion on diabetes care in 2016 , and approximately 40% of older adults with diabetes had 3 or more comorbidities .\n\n【13】Type 2 diabetes accounts for 90% to 95% of all diabetes cases . Type 2 diabetes often can be delayed or even prevented through maintenance of a healthy lifestyle and participation in evidence-based lifestyle modification programs . Diabetes-related complications may be reduced through patient-centered disease management programs . Understanding factors associated with people choosing to avoid health care is important to develop targeted prevention strategies. In the absence of such knowledge, clinicians may not effectively engage with this at-risk population, who can benefit from preventive, treatment, and management programs. Engagement in these programs can prevent burdensome and costly diabetes-related complications. The objective of our study was to describe factors associated with decisions to avoid health care among Medicare beneficiaries aged 65 or older with type 2 diabetes.\n\n【14】Methods\n\n【15】We analyzed data from the 2016 Medicare Current Beneficiary Survey Public Use File from the Centers for Medicare & Medicaid Services, which is a continuous and multipurpose survey of a representative, national sample of Medicare beneficiaries . The data set includes data only on community-dwelling beneficiaries, excluding data on beneficiaries living in long-term care facilities . It includes information on Medicare enrollment, sociodemographic characteristics, health conditions, and health care access. The data set is a shorter version of the original Medicare Current Beneficiary Survey; it includes fewer variables and categorizes some variables more broadly . We accessed and analyzed the data set in 2019.\n\n【16】Our study population consisted of 1,782 Medicare beneficiaries aged 65 or older with type 2 diabetes. We identified the presence of type 2 diabetes by examining data from 2 items in the 2016 Medicare Current Beneficiary Survey Public Use File : 1) “Has a doctor ever told you that you had any type of diabetes, including sugar diabetes, high blood sugar, borderline diabetes, prediabetes, or pregnancy-related diabetes/borderline diabetes or prediabetes?” and 2) “Please tell which type of diabetes the doctor said you have.”\n\n【17】### Measures\n\n【18】For the outcome variable, we identified Medicare beneficiaries with type 2 diabetes whose response to the following question indicated if they would avoid health care: “You will do just about anything to avoid going to the doctor. True or false?”\n\n【19】For key factors of interest, we included the following variables: delayed seeking care because of costs (yes/no), dissatisfaction with ease of getting to health care providers (a 5-point Likert scale), dissatisfaction with information given by health care providers (a 5-point Likert scale), health problems that should have been discussed with health care providers but were not (a 5-point Likert scale), and worried about health more than other people their age (true/false) .\n\n【20】We recoded Likert-scaled variables into broader categories to ensure adequate sample sizes for reliable estimates. For example, participants were asked to rate on a 5-point Likert scale their agreement with the statement on health problems that should have been discussed with providers but were not: 1 = strongly agree, 2 = agree, 3 = neither agree nor disagree, 4 = disagree, and 5 = strongly disagree. We recoded the variable by combining response levels of 1 and 2 together (strongly agree and agree) and 4 and 5 together (disagree and strongly disagree), and we created a categorical variable with 3 categories: 1 = strongly agree/agree (yes), 2 = disagree/strongly disagree (no), and 3 = neither agree nor disagree. We created 2 dichotomous variables for dissatisfaction with ease in getting to health care providers and information given by health care providers: 1 = very dissatisfied/dissatisfied (yes); 0 = very satisfied/satisfied (no).\n\n【21】The use of covariates for the analysis was guided by previous research . Covariates were sex (male or female), age group (65–74 or ≥75), race/ethnicity (non-Hispanic White, non-Hispanic Black, Hispanic, or other), education level (<high school diploma, high school diploma only, or >high school diploma), annual household income (<$25,000 or ≥$25,000), marital status (married, widowed, divorced/separated, or never married), residing area (metropolitan or nonmetropolitan), living status (alone or not alone), body mass index (in kg/m 2  ; underweight, <18.5, healthy weight, 18.5 to <25.0, or overweight/obese, ≥25.0), general health status (excellent/very good, good, or fair/poor), and 7 comorbidities (hypertension/high blood pressure, myocardial infarction/heart attack, stroke/brain hemorrhage, emphysema/asthma/chronic obstructive pulmonary disease, rheumatoid arthritis, depression, and urinary incontinence). Additionally, we included covariates on functional status, both activities of daily living (ADLs) and instrumental activities of daily living (IADLs). ADLs are skills required for such everyday activities as bathing, dressing, toileting, transferring to chairs, walking, and eating, whereas IADLs are skills that require more complex planning and thinking, such as managing money, shopping, using the telephone, housekeeping, and preparing meals. We recoded IADL/ADL limitations as no limitations, only IADL limitations, 1 or 2 ADL limitations, or 3 or more ADL limitations.\n\n【22】### Statistical analyses\n\n【23】We calculated proportions for each measure overall and then stratified data by whether health care was avoided. We compared differences in proportions of respondents avoiding health care and not avoiding health care by using Wald χ 2  tests. We used a logistic model adjusted for sociodemographic characteristics and comorbidities to examine associations between these factors and the choice to avoid health care. Results were considered significant at _P_ < .05.\n\n【24】All analyses applied survey weights to account for the complex survey design. We used a subgroup/domain analysis to ensure the accuracy of estimates. We performed all analyses by using SAS Enterprise Guide version 6.1 (SAS Institute Inc) and Stata/IC version 11.2 (StataCorp LLC).\n\n【25】Results\n\n【26】Among 1,782 Medicare beneficiaries with type 2 diabetes in our study sample, 465 (26.1%) reported that they would avoid health care . The proportion of respondents who reported avoiding health care was significantly greater than the proportion who reported not avoiding health care among the following groups: women (55.1% vs 46.7%), Hispanic respondents (15.6% vs 8.8%), respondents of “other” race/ethnicities (12.5% vs 6.8%), respondents with less than a high school diploma (29.2% vs 15.9%) or with a high school diploma only (37.1% vs 31.8%), and respondents with less than $25,000 in annual household income (42.5% vs 32.6%). Respondents who reported avoiding health care had lower levels of functional skills (eg, 1 or 2 ADL limitations, 26.3% vs 21.0%) and reported worse general health status than respondents without this care-seeking behavior (eg, fair/poor health, 34.2% vs 25.4%). Compared with respondents who did not avoid health care, respondents who avoided health care indicated higher levels of cost-based decision making (15.5% vs 7.0%), health problems that should have been discussed with providers but were not (14.2% vs 8.3), and greater worry about health than others their age (35.9% vs 17.8%). Although not significantly different according to Wald χ 2  tests, 5.7% and 6.8% of beneficiaries choosing to avoid health care reported dissatisfaction with ease of getting to providers and dissatisfaction with information given by providers compared with 3.4% and 4.3% of respondents who did not avoid health care, respectively.\n\n【27】Respondents who indicated “other” race/ethnicity were 2.01 (95% CI, 1.23–3.30; _P_ \\= .006) times more likely than non-Hispanic White respondents to avoid health care . Respondents with less than a high school diploma (adjusted odds ratio \\[aOR\\] = 1.95; 95% CI, 1.32–2.90; _P_ \\= .001) and respondents with a high school diploma only (aOR = 1.49; 95% CI, 1.00–2.23; _P_ \\= .049) were more likely than respondents with more than a high school diploma to avoid health care. Respondents who delayed seeking care because of cost were twice as likely to avoid health care (aOR = 2.06; 95% CI, 1.25–3.40; _P_ \\= .005) as respondents who did not delay care because of cost. Respondents who had health problems that should have been discussed with providers but were not were also more likely to avoid health care compared with those able to discuss their health problems (aOR = 1.50; 95% CI, 1.02–2.21; _P_ \\= .04). Respondents who worried about their health more than others their age were twice as likely to avoid health care (aOR = 2.13; 95% CI, 1.49–3.04; _P_ < .001) as those who did not have such worries.\n\n【28】Discussion\n\n【29】Little is known about the characteristics of Medicare beneficiaries with reported type 2 diabetes who choose to avoid health care, nor about underlying factors associated with choosing to do so. In our study, more than 25% of insured, community-dwelling Medicare beneficiaries aged 65 or older with type 2 diabetes reported they would avoid going to the doctor. Older adults with diabetes are at risk of potentially preventable acute and chronic diabetes-related complications that could result in unplanned hospitalizations, poor quality of life, and even death . Consistent with the Behavioral Model of Healthcare Service Use , we showed that choosing to avoid health care among Medicare beneficiaries with type 2 diabetes is multifactorial, and a result of individual, economic, health care provider, and system-level factors.\n\n【30】Our findings, in general, are consistent with the findings of studies that focused on factors associated with avoiding health care, although those studies differed from ours in populations and settings. Kannan and Veazie found that approximately 36% of US adults aged 18 or older avoided physician visits . Although this percentage is higher than our estimate (26% of Medicare beneficiaries with type 2 diabetes avoiding health care), the finding is not surprising. Because older adults with diabetes require more regular medical care than the general US adult population, we would expect a lower proportion of them to avoid health care. Similar to other researchers, we also found that factors such as cost , patient–provider communication , and education  were significantly associated with avoiding health care among Medicare beneficiaries with type 2 diabetes. However, contrary to previous findings, we found an association between race/ethnicity and avoidance of health care , and we did not find an association between income level  or sex  and health care avoidance. These findings, in part, are likely due to differences in study populations and settings. The differences, however, are worth noting and warrant further investigation.\n\n【31】Our study showed that Medicare beneficiaries with type 2 diabetes with lower educational attainment (<high school diploma and a high school diploma only vs >high school diploma) were more likely to avoid health care. Decision makers can use this information to identify Medicare beneficiaries with type 2 diabetes who are at risk of avoidance behavior. For example, tailoring materials (eg, educational resources) to account for potential low literacy may be important in ameliorating inequities among those without education beyond high school. This finding is important because lower health literacy and low levels of education have also been associated with poorer overall health . By extension, diabetes self-management education and support (DSMES) programs  can support Medicare beneficiaries with type 2 diabetes at risk of avoidance behavior, by tailoring materials to improve participants’ diabetes literacy and self-efficacy. For example, previous research evaluating the community-based peer-led DSMES found that participants with at-goal hemoglobin A 1c  values significantly improved communication with physicians , a key factor that affects a person’s health care–seeking behaviors .\n\n【32】We also found a significant association between race/ethnicity of Medicare beneficiaries with type 2 diabetes and choices to avoid health care. This information can be used to tailor culturally sensitive interventions that can reduce the disparity in older adults of racial/ethnic minority populations who already experience racial/ethnic disparities in diabetes-associated care . Culturally and linguistically appropriate interventions that encourage engagement of racial/ethnic minority groups in diabetes-related care and management, which could reduce these disparities , especially among people choosing to avoid health care, are needed.\n\n【33】We found a significant association between choosing to avoid health care among Medicare beneficiaries with type 2 diabetes and the presence of health problems that should have been discussed with health care providers but were not. Medicare beneficiaries with type 2 diabetes who feel their concerns are not being addressed may be less inclined to participate in follow-up health care visits, particularly when they have time and cost concerns. Therefore, adapting interventions to patient–provider communication is important. Because Medicare beneficiaries with type 2 diabetes may expect health care providers to manage multiple health problems in 1 time-limited medical appointment, establishment of shared patient–provider expectations for a visit may bridge this gap. Providers need to express to their patients that they might not able to provide all necessary medical advices at a single appointment. Discussing the reasons for having multiple visits can be important for patients to have appropriate expectations. Previous research demonstrated that consistent high-quality patient–provider relationships are pivotal in optimizing health outcomes for people with chronic conditions such as diabetes .\n\n【34】Our findings indicated that Medicare beneficiaries with type 2 diabetes identified cost as a driving factor in decisions to avoid health care. This finding is consistent with other reports highlighting cost as an important consideration in health care decision making, even among insured people who have access to care. Wharam and colleagues reported delays in care seeking for macrovascular complications among employer-insured beneficiaries with a history of diabetes after a transition from low- to high-deductible health plans . Transitioning to a fixed retirement income with high out-of-pocket costs affects the medical care use of many Medicare beneficiaries . For example, the average annual out-of-pocket costs among Medicare beneficiaries with diabetes ($2,528 in 2020 US$) were approximately 30% higher than costs for beneficiaries without diabetes . Previous research reported that having to pay out-of-pocket expenses is a disincentive to using diabetes-associated preventive care . Therefore, policy discussions should involve the topic of cost sharing for Medicare beneficiaries, especially those with type 2 diabetes.\n\n【35】We did not find significant differences in avoiding health care by sex. However, the results of a recent survey from the Cleveland Clinic , focusing only on men, indicated that 65% of men may “wait as long as possible to see their doctor” when they have injuries or symptoms of a health condition. Although restricted to men, that survey provided additional insights into why men avoid physician visits, indicating that approximately 41% were told as children that “men don’t complain about health” . In addition, the same survey found that, among men who were not already having annual checkups, 61% would be more likely to participate in an annual check-up if it were “more convenient” . Future studies should focus on differences by sex.\n\n【36】Although our study did not find significant differences in avoiding health care by sex, the research at the Cleveland Clinic does provide insight into ways to help encourage people to have annual checkups. This finding is important in diabetes care because decisions to avoid health care may preclude early detection of diabetes, particularly among people with prediabetes. Additionally, avoiding health care increases the risk of undetected, preventable disease-related complications for people with established diabetes. Therefore, research that investigates sex differences in avoiding health care among Medicare beneficiaries with type 2 diabetes would be valuable.\n\n【37】Previous studies, with different populations and settings, found that health anxiety, which is “characterized by persistent preoccupation of having or acquiring a serious illness, misattribution of bodily symptoms and urge to seek medical advice in the absence of physical pathology” , was positively associated with increased health care use and greater medical expenditures . Interestingly, in our study, Medicare beneficiaries with type 2 diabetes who reported worrying about health more than others their age were more likely to avoid going to the doctor than people who did not report such worry. The reason for this health care decision-making process is unclear and warrants further investigation.\n\n【38】This study has several limitations. First, generalizability is limited because we focused only Medicare beneficiaries with type 2 diabetes and surveys were restricted to English or Spanish, potentially excluding those who do not speak these languages. Second, the cross-sectional analysis prevented us from drawing conclusions about cause and effect. Third, we used a dichotomous measure from a single survey item on our outcome of interest instead of using an open-ended question. Such a qualitative research approach, instead of our quantitative approach, could provide a more nuanced understanding of the complex reasons for avoiding health care. Fourth, our study was subject to recall bias (ie, relying on beneficiaries’ recollection of events). Fifth, information on annual household income was restricted to a single dichotomous variable of less than $25,000 and $25,000 or more.\n\n【39】Our findings suggest that further investigation is needed into the causes and economic implications of avoiding health care among Medicare beneficiaries with chronic conditions such as diabetes. Studies that can build on the results of our study are needed to develop screening tools for use by diabetes care providers to identify people at risk of avoiding health care. Additionally, there is a need to examine provider practices that support therapeutic patient–provider communication and effective relationship building. Finally, systems-level changes that limit factors associated with avoiding health care should be explored and implemented.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "3c06a59f-c804-4bea-8381-5fdc242ca14f", "title": "Efficient Surveillance of Plasmodium knowlesi Genetic Subpopulations, Malaysian Borneo, 2000–2018", "text": "【0】Efficient Surveillance of Plasmodium knowlesi Genetic Subpopulations, Malaysian Borneo, 2000–2018\nThe monkey parasite _Plasmodium knowlesi_ was discovered to be a common cause of malaria in humans in 2004, initially from investigations in the Kapit division of Sarawak state, Malaysian Borneo . Humans acquire infection primarily from wild long-tailed ( _Macaca fascicularis_ ) and pig-tailed ( _M. nemestrina_ ) macaque reservoirs ; _Anopheles_ mosquitoes of the leucosphyrus group are vectors . _P. knowlesi_ malaria has been described across Southeast Asia, but most clinical cases are still reported in Malaysian Borneo . In 2017 and 2018, a total of 7,745 cases were reported in Malaysia, 86.8% of which were detected in Malaysian Borneo  . _P. knowlesi_ infections can be asymptomatic , and clinical cases exhibit a wide spectrum of disease ranging from mild symptoms to death .\n\n【1】Population genetic surveys of _P. knowlesi_ infections in humans across Malaysia have revealed 2 divergent subpopulations of the parasite in Malaysian Borneo that are associated with the 2 macaque species locally, suggesting 2 independent zoonoses . The cluster 1 type has been associated with long-tailed macaques and the cluster 2 type with pig-tailed macaques . The existence of 2 sympatric subpopulations also has been confirmed by whole-genome sequencing (WGS) of _P. knowlesi_ from patients in Malaysian Borneo . In peninsular Malaysia on the Asia mainland, all cases have been caused by another subpopulation, cluster 3, that has not been detected in Malaysian Borneo . Limited WGS  and microsatellite  genotyping of _P. knowlesi_ isolates derived from human and only long-tailed macaque hosts from peninsular Malaysia showed allopatric divergence for this subpopulation cluster from those of Malaysian Borneo because of geographic separation by the South China Sea.\n\n【2】Increasing numbers of _P. knowlesi_ malaria cases detected might be due to increased zoonotic exposure along with a reduction of endemic malaria parasite species . With the recent identification of different zoonotic _P. knowlesi_ genetic subpopulations, determining whether these populations vary in frequency over space and time is important. Interactions with vectors and reservoir hosts will affect distributions of these parasite subpopulations, and human contacts with these vectors and reservoir hosts would determine zoonotic incidence. Because the previously used microsatellite genotyping and WGS are time consuming and expensive, we developed a simple PCR to discriminate between the 2 _P. knowlesi_ subpopulations in Malaysian Borneo.\n\n【3】### Materials and Methods\n\n【4】##### Identification of Cluster-Specific Markers\n\n【5】We identified high-quality bi-allelic single-nucleotide polymorphisms (SNPs) from whole-genome sequence data of _P. knowlesi_ isolates from cluster 1 (n = 38) and cluster 2 (n = 10) subpopulations . We identified 9,293 SNPs with complete fixation of alternative alleles between the 2 subpopulations. Oligonucleotides for allele-specific PCRs were designed on the basis of genome regions with a high density of fixed SNPs and having multiple SNPs <5 nt apart, especially at the 3′ ends of the designed primers. For each subpopulation, we chose 5 pairs of forward and reverse PCR primers to evaluate for genotyping assays .\n\n【6】##### Cluster-Specific Genotyping PCR\n\n【7】PCRs to discriminate subpopulation clusters were optimized in 11-μL reaction volume containing 1X Green GoTaq Flexi buffer , 5 mmol/L MgCl2, 0.275 U GoTaq DNA polymerase, 0.2 mmol/L dNTP , 1–2-μL DNA template, and 0.25 μmol/L each forward and reverse primers. We tested 2 thermal cycling methods to determine the optimum annealing temperature. The first conventional method was 94°C for 2 min, followed by 35 cycles of initial denaturation of 94°C for 30 s, gradient annealing from 50°C to 66°C for 1 min, extension at 72°C for 30 s, and final elongation at 72°C for 2 min. For the second method, we used the touchdown PCR to increase the specificity and sensitivity in amplification  with the following 2 phases. We initiated the first phase with denaturation at 94°C for 2 min, followed by 10 cycles of touchdown program of 94°C for 30 s, annealing of 10°C above the primer melting temperature for 45 s and 72°C for 30 s, with a decreased on 1°C of the annealing temperature every cycle. Then, we performed an additional 25 cycles as follows: 94°C for 30 s, annealing of 5°C below the primer annealing primer melting temperature for 45 s, 72°C for 30 s, and final elongation at 72°C for 5 min. We analyzed PCR products by electrophoresis on agarose gel stained with ethidium bromide and visualized them under ultraviolet transillumination.\n\n【8】##### DNA and Blood Samples from Malaria Infections\n\n【9】To validate allele-specific PCRs, we obtained DNA samples of _P. knowlesi_ infections in Malaysian Borneo from previous studies . Samples from humans were from Betong (n = 29), Kanowit (n = 34), Miri (n = 46), Sarikei (n = 23), Kapit (n = 52), Kudat (n = 46), Ranau (n = 62), and Tenom (n = 48); we also used samples from long-tailed macaques (n = 10) and pig-tailed macaques (n = 5) from Kapit . We determined subpopulation assignments (cluster 1 or cluster 2) of each infection by analyzing multilocus microsatellite data  or whole-genome sequence data . _Plasmodium_ DNA controls of humans ( _P. falciparum_ , _P. vivax_ , _P. malariae_ , and _P. ovale_ ) and macaques ( _P. knowlesi_ , _P. inui_ , _P. cynomolgi_ , _P. fieldi_ , and _P. coatneyi_ ) were also used for specificity tests of the PCRs.\n\n【10】To determine the frequency of the 2 _P. knowlesi_ subpopulations over time, we investigated samples derived from Kapit division, Sarawak state, Malaysian Borneo, where high incidence of cases has been reported . New samples were collected as dried blood spots on filter papers  from _P. knowlesi_ clinical cases in Kapit Hospital during September 2016–May 2018 (440 cases). DNA was extracted and _P. knowlesi_ confirmed by species-specific nested PCRs  at the Malaria Research Centre, Universiti Malaysia Sarawak (UNIMAS, Kota Samarahan, Malaysia). We also analyzed 764 DNA samples from persons with _P. knowlesi_ infection at Kapit Hospital collected previously: March 2000–December 2002, 110 samples ; March 2006–February 2008, 176 samples ; and June 2013–mid-September 2016, 478 samples .\n\n【11】##### Data Analysis and Ethics Approval\n\n【12】We tabulated genotyping data in Excel and conducted statistical analysis using R . We also explored temporal patterns for overall _P. knowlesi_ subpopulation cases over the previous 5 years using the R package seasonal decomposition of time series (STL) by LOESS (locally estimated scatterplot smoothing) . We generated STL decomposition using periodic as smoothing parameter for the seasonal component (n s  ) with no robustness iterations. LOESS is the seasonal-trend decomposition method used in estimating trend from the time series data. The Medical Ethics Committees of UNIMAS (NC-21.02/03-02 Jld 2 ) and the Medical Research and Ethics Committee, Malaysian Ministry of Health (NMRR-16-943-31224), approved this study.\n\n【13】### Results\n\n【14】##### Development of Simple PCR to Discriminate _P. knowlesi_ Cluster 1 and 2 Subpopulations\n\n【15】We tested 10 PCRs on the basis of fixed SNP differences for performance in discriminating the 2 sympatric _P. knowlesi_ subpopulations. Of these, 1 assay each for cluster 1 (primer pair C1A) and cluster 2 (primer pair C2J) showed complete specificity when tested on 5 cluster 1 and 9 cluster 2 DNA samples  that were previously confirmed by microsatellites cluster assignments . These primers were also _P. knowlesi_ –specific and did not show amplification on DNA of other simian malaria parasites ( _P. coatneyi_ , _P. cynomolgi_ , _P. fieldi_ , and _P. inui_ ) or human malaria parasites ( _P. falciparum_ , _P. vivax_ , _P. malariae_ , and _P. ovale_ ), or host DNA from humans or either macaque host species .\n\n【16】We further tested these primer sets for sensitivity using pure DNA of cluster 1 samples with starting parasitemias of 13,793 parasites/μL blood and cluster 2 samples with starting parasitemias of 8,017 parasites/μL blood. We prepared 5-fold serial dilutions to serve as template in separate PCRs. Tests of sensitivity showed limit of detection at ≈4 parasites/μL blood for primer pair C1A and ≈13 parasites/μL blood for primer pair C2J .\n\n【17】##### Proportions of _P. knowlesi_ Subpopulations in Different Areas\n\n【18】Analysis of DNA samples of 1,492 _P. knowlesi_ malaria cases in different regions of Malaysian Borneo showed cluster 1 as the predominant subpopulation (70%), followed by cluster 2 (28%) . Only 2% of infections were positive for both primer sets, and these infections had mixed genotypes as identified by previous microsatellite analysis . Cluster 1 infections were more common at most individual locations sampled throughout Malaysian Borneo, except for Miri and Kanowit, where cluster 2 infections were more common.\n\n【19】We also tested the primer sets on 15 natural infections of wild macaques (10 long-tailed and 5 pig-tailed) from Kapit. _P. knowlesi_ infections from all macaques showed concordant macaque host association (cluster 1 in long-tailed macaques and cluster 2 in pig-tailed macaques), except for 1 long-tailed macaque that was infected by both cluster 1 and cluster 2 parasites .\n\n【20】##### Temporal Analysis of _P. knowlesi_ Subpopulation Types in a High-Incidence Area\n\n【21】In Kapit, where a high incidence of cases has been reported since the zoonosis was discovered to be common , we analyzed DNA samples from 1,204 human _P. knowlesi_ infections collected in 3 study periods during 2000–2018. Of those infections, 69% belonged to the cluster 1 subpopulation . Most (28%) of the remaining _P. knowlesi_ infections belonged to the cluster 2 subpopulation, and only 2% had parasites of both types. During 3 study periods of 2000–2002 (n = 110 cases), 2006–2008 (n = 176 cases), and 2013–2018 (n = 918 cases), the proportion of cluster 1 and cluster 2 subpopulations showed similar patterns of distribution with cluster 1 subpopulation as the predominant ones (Pearson p = 0.74). We also observed similar patterns across 12 months of these study periods , indicating a lack of strong seasonal variation between cluster 1 and cluster 2 infections in Kapit (p = 0.56 by Fisher exact test).\n\n【22】We conducted a more intensive analysis on infections sampled during the most recent 5-year study period (June 2013–May 2018); we made a particular effort to recruit most of the case-patients seeking care at Kapit Hospital during this time. Of the 918 infections genotyped from this period, 637 were cluster 1 infections, 258 were cluster 2 infections, and 23 were mixed cluster infections. The proportion of cluster 1 infections always ranged from 63% to 78% (Pearson p = 0.007) and was highest in the most recent year. The total number of _P. knowlesi_ cases was also highest in the most recent year . Using an STL decomposition method of analysis based on numbers plotted on a monthly basis , we noted a trend of increasing numbers of cases overall, as well as of the cluster 1 subpopulation separately, from June 2016 onward. We found no significant trend for cluster 2 cases separately.\n\n【23】### Discussion\n\n【24】On the basis of fixed SNP differences between the 2 _P. knowlesi_ genetic subpopulations in Malaysian Borneo, identified by whole-genome sequence data analysis , we successfully developed an allele-specific PCR for discriminating these in large-scale studies. This method enables identification of cluster 1 and cluster 2 subpopulations from field isolates throughout the sympatric distribution, without the need to perform population genetic analysis of multilocus data on all samples. A different _P. knowlesi_ subpopulation termed cluster 3 has been described to account for all cases in Peninsular Malaysia , but we did not study that subpopulation because it has not been detected in Malaysian Borneo.\n\n【25】The use of these allele-specific PCRs is limited to _P. knowlesi_ infections in Malaysian Borneo because they were designed on the basis of the genome sequences of the cluster 1 and cluster 2 parasites found there. These assays could potentially be applied to analyze _P. knowlesi_ infections in Kalimantan, Indonesian Borneo, which shares international borders with Malaysian Borneo. The allopatric divergence of cluster 3 subpopulations in Peninsular Malaysia  indicates the need for development of a separate allele-specific PCR for those parasites, which would require analysis of whole-genome data from substantially more _P. knowlesi_ isolates from Peninsular Malaysia.\n\n【26】The validation of this allele-specific PCR shows remarkable sensitivity with single-round (nonnested) amplification, even at a parasitemia of ≈4 parasites/μL blood. Although all samples tested from patients at Kapit Hospital could be genotyped, this PCR would have limitations on subpatent _P. knowlesi_ infections when parasites are undetectable under the microscope .\n\n【27】No clear evidence of direct human–mosquito–human transmission of _P. knowlesi_ has yet been detremined, although such transmission might be occurring. Most hospitalized _P. knowlesi_ malaria patients have been adult farmers or logging camp workers, who regularly spend time in forests or forest fringes, or travelers entering forests , suggesting that macaque–vector–human transmission is the primary route of infection . Although the distribution of long-tailed and pig-tailed macaques overlaps throughout Southeast Asia, these macaque species show different habitat preferences . The preference of long-tailed macaques for cropland, wetland, and urban areas brings them in close proximity to humans, but whether transmission occurs outside of the forests is not known. The widespread distribution of long-tailed macaques corresponds with the cluster 1 parasite subpopulation accounting for most _P. knowlesi_ infections in Kapit. The lower frequency of the cluster 2 subpopulation in humans is as expected, given that those parasites are associated with pig-tailed macaques, which occur in more remote forested areas .\n\n【28】Like many other vectorborne parasitic diseases, malaria is sensitive to environmental changes such as deforestation ; the association between incidences of _P. knowlesi_ infections and environmental changes in Sabah, Malaysian Borneo, supports this fact , because overall numbers of cases have increased in the past few years . We have shown that most of the cases in Sabah are of the cluster 1 genetic subpopulation of _P. knowlesi_ , and that numbers of this parasite subpopulation have increased in Kapit in the most recent 2 years of the study period. Whether changes in the landscape in Sarawak or other factors have contributed to the increasing numbers of _P. knowlesi_ cases needs to be studied. Determining whether any significant differences exist between the clinical outcomes after infection with the different subpopulations also is important. Such determination requires collecting detailed clinical and laboratory data on _P. knowlesi_ patients infected with the 2 different subpopulation clusters and performing careful association study analysis to deliver robust inference for the benefits of both clinicians and public health experts.\n\n【29】_P. knowlesi_ infections require separate efforts from those being targeting elimination of human malaria ( _P. falciparum_ and _P. vivax_ ), particularly in Malaysia, where zoonotic malaria is dominant . Although _P. knowlesi_ is not part of the national elimination program , monitoring this parasitic infection is crucial because of its increasing incidence. Zoonotic malaria requires new strategies in prevention and control, including monitoring of the different parasite genetic subpopulations.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "06f35751-9143-41d0-b5dd-6078827184d0", "title": "Molecular Investigation of Tularemia Outbreaks, Spain, 1997–2008", "text": "【0】Molecular Investigation of Tularemia Outbreaks, Spain, 1997–2008\nTularemia is a zoonosis caused by the gram-negative bacterium _Francisella tularensis_ . _F. tularensis_ is a highly contagious facultative intracellular pathogen and has infectious doses as low as 10–50 bacteria; it is transmitted by inhalation, direct contact with infected animals, or ingestion of contaminated water or food. The number of species susceptible to infection by this agent is higher than for any other known zoonotic pathogen . Because of its potential to cause adverse public health effects and mass casualties by bioterrorist attack, the pathogen is 1 of 6 agents listed as a Tier 1 agent by the US Department of Health and Human Services .\n\n【1】_F_ . _tularensis_ includes 4 subspecies ( _F_ . _tularensis_ subsp. _tularensis_ , _F_ . _tularensis_ subsp. _holarctica_ , _F_ . _tularensis_ subsp. _novicida_ , and _F_ . _tularensis_ subsp. _mediasiatica_ ), which show marked differences in many epidemiologic features, including geographic distribution, virulence, and genetic diversity . _F. tularensis_ subsp. _tularensis_ (Jellison type A) and _F. tularensis_ subsp. _holarctica_ (Jellison type B) are major clinical pathogens. _F. tularensis_ subsp. _tularensis_ is the most virulent subspecies and can cause life-threatening disease; its distribution seems to be restricted to North America, although a single report indicated its presence in Europe . _F_ . _tularensis_ subsp. _holarctica_ causes a less severe disease, and although widespread throughout the Northern Hemisphere, it has restricted genetic diversity, which suggests recent emergence and successful geographic spread .\n\n【2】Tularemia was first reported in Spain in 1997, when it caused one of the largest outbreaks in humans ever described . Overall, 559 cases were confirmed during June 1997–April 1998 in 10 provinces. The outbreak was associated with hunting and handling of hares ( _Lepus europaeus_ ) in northwestern Spain. The most common clinical form was ulceroglandular tularemia (55.4%); glandular (15.3%) and typhoid forms (6.6%) of the disease also occurred frequently. A second major human outbreak in humans, which affected 507 persons, occurred in the same area in 2007 and 2008, but in a different epidemiologic context. Its timing coincided with a population peak of the common vole ( _Microtus arvalis_ ), and the most frequent clinical forms of the disease were typhoidal and pneumonic (65% of the cases), which is consistent with infection being acquired through inhalation of _F. tularensis_ . Sporadic tularemia cases and small outbreaks were reported during 2000–2006 in the interval between the 2 major outbreaks in northwestern Spain .\n\n【3】We report comparative genetic analyses of _F. tularensis_ cultured from humans and animals during the 2 main tularemia outbreaks (1997–1998 and 2007–2008). We also studied _F. tularensis_ isolates circulating in Spain during outbreaks with different epidemiologic patterns and investigated whether reemergence of the pathogen after 10 years of no epidemiologic activity was caused by introduction of exotic strains or by establishment of the pathogen in local reservoirs of infection.\n\n【4】### Methods\n\n【5】##### _F. tularensis_ Isolates, Culture Conditions, and Biochemical Characterization\n\n【6】We studied 109 _F_ . _tularensis_ isolates: 37 animal and human _F. tularensis_ subsp. _holarctica_ isolates from the first outbreak in northwestern Spain ; 61 animal and human isolates from the second tularemia epidemic in the same area ; 10 10 isolates obtained in the Czech Republic; and reference strain _F_ . _tularensis_ subsp. _tularensis_ Schu (CAPM 5600). Source of isolates, subspecies, host, geographic origin, and year of isolation are shown in the Technical Appendix .\n\n【7】All isolates were grown on modified Thayer-Martin agar plates containing 36 g/L GC agar base, 10 g/L soluble hemoglobin powder, and 2 vials/L Vitox supplement (Oxoid, Basingstoke, UK) at 37°C for 2–3 days in aerobic conditions. Biochemical characterization included tests for oxidase and catalase activities, glucose and glycerol fermentation, and urea hydrolysis.\n\n【8】##### Genetic Characterization\n\n【9】Species and subspecies were identified by real-time and conventional PCRs specific for the _fopA_ gene and the region of difference 1 (RD1) as described . RD23 was analyzed by PCR for identification of a genetic group of isolates that had been found on the Iberian Peninsula .\n\n【10】Pulsed-field gel electrophoresis (PFGE) and multilocus variable number tandem repeat analysis (MLVA) were used to classify isolates into genetic subpopulations. The PFGE protocol described , which used restriction enzymes _Xho_ I and _Bam_ HI, was optimized to provide major improvements in quality of fingerprint patterns.\n\n【11】Bacterial cells were suspended in SE buffer (25 mmol/L EDTA, 75 mmol/L NaCl, pH 7.5) to an absorbance of 0.5–0.6 at 600 nm. Cells were lysed in agarose plugs and plugs were washed 5 times with Tris-EDTA buffer (10 mmol/L Tris-HCl, 1 mmol/L EDTA, pH 8.0) for 30 min at 50°C. DNA in the plugs was digested with 40 U of _Xho_ I (New England Biolabs, Ipswich, MA, USA) or 40 U of _Bam_ HI (New England Biolabs), for 16 and 3 h, respectively, at 37°C following the manufacturer’s protocol. DNA fragment sizes were determined by electrophoresis and by comparing bands with a Lambda Ladder PFG Marker (New England Biolabs).\n\n【12】MLVA was performed as described for 16 variable number tandem repeat loci . To ensure analysis of identical genetic material by PFGE and MLVA, we used DNA from the same culture for both methods. MLVA markers were amplified by using PCR, and sizes of amplification products were determined by electrophoresis on 3.5% high-resolution agarose MS-8 gels (Conda Pronadisa, Madrid, Spain), except for Ft-M3, Ft-M21, Ft-M22, and Ft-M24 MLVA markers, for which the sizes were determined by using capillary electrophoresis. At least 2 alleles were sequenced for each MLVA marker to confirm that size differences observed resulted from the expected variations in numbers of tandem repeats. Forward and reverse sequences were aligned by using MEGA v.4 software , and consensus sequences were used to predict the number of tandem repeats in each allele.\n\n【13】##### Data Analyses\n\n【14】Simpson index of diversity, which measures the probability that 2 unrelated strains from the test population will be classified into different typing groups , was calculated to compare the discriminative power of PFGE typing with that of MLVA for assessing genetic diversity among isolates. The adjusted Wallace coefficient for quantification of agreement between PFGE typing and MLVA results was also calculated. Both analyses were performed by using Comparing Partitions .\n\n【15】PFGE patterns were analyzed by using Bionumerics 6.6 (Applied-Maths NV, Sint-Martens-Latem, Belgium) to describe genetic relationships among isolates. Dendrograms were constructed by using the Dice similarity coefficient and the unweighted pair group mathematical average clustering algorithm. MLVA data, expressed as allelic profiles for isolates, were analyzed by using Bionumerics 6.6. Minimum spanning trees were calculated with priority rules set at first link allelic profiles and maximum numbers of single-locus variants and then maximal numbers of single-locus variants and double-locus variants. MLVA types were classified as members of a clonal complex if they had the same allele at15 of the 16 MLVA markers. A map of the distribution of isolates showing the geographic origin and number of isolates per province was generated by using Arcgis 9.2 software (ESRI, Redlands, CA, USA).\n\n【16】### Results\n\n【17】##### Subspecies and Genetic Subclade of _F. tularensis_ Isolates\n\n【18】All isolates were negative for oxidase activity, weakly positive for catalase activity, and positive for acid production from glucose; none of the isolates hydrolyzed urea. Only the reference strain _F. tularensis_ subsp. _tularensis_ Schu (CAPM 5600) produced acid from glycerol. Real-time PCR specific for the _fop_ A gene and size determination at the RD1 region showed that all isolates from Spain and the Czech Republic were _F. tularensis_ subsp. _holarctica_ . All isolates from Spain included in this study had the 1.59-kbp deletion at the RD23 loci, which is characteristic of the _F. tularensis_ subsp. _holarctica_ genetic subclade B.Br:FTNF002–00 (also known as the Iberian clone or the central and western European genetic group).\n\n【19】##### Characterization by PFGE\n\n【20】All 107 _F_ . _tularensis_ subsp. _holarctica_ isolates showed the same fingerprint pattern by PFGE with the restriction enzyme _Xho_ I, irrespective of their geographic origin, host, or date of isolation (isolate TU41 was not typeable by PFGE analysis). This pattern consisted of ≈20 DNA fragments >70 kbp. The _F_ . _tularensis_ subsp. _tularensis_ strain Schu (CAPM 5600) showed a different banding pattern.\n\n【21】In contrast, PFGE with the restriction enzyme _Bam_ HI discriminated 5 genotypes among the _F_ . _tularensis_ subsp. _holarctica_ isolates. _F_ . _tularensis_ subsp. _tularensis_ strain Schu (CAPM 5600) showed a highly unrelated banding pattern with maximal pairwise distance to all other isolates. The _Bam_ HI patterns consisted of 20–24 DNA fragments with a size range of 20–245 kbp. All _F_ . _tularensis_ subsp. _holarctica_ genotypes were closely related (93.3% similarity), and there were only 1-band differences between pulsotypes .\n\n【22】Sixty-nine (63.9%) isolates from Spain clustered into pulsotype A and 26 (24.1%) other isolates from Spain clustered into pulsotype B. All isolates from the Czech Republic, except for isolate CAPM 5538, had the same fingerprint pattern, which was designated pulsotype D (8.3%). Isolate CAPM 5538 showed a pulsotype that clustered with the 2 remaining isolates from Spain (TU8 and TU9) in pulsotype C (1.9%). One isolate from Spain, TU41, could not be genotyped by PFGE despite several attempts .\n\n【23】There were some discrepancies between our findings and those reported for the 37 isolates in Spain from 1997 . Improvements in quality of fingerprint patterns enabled us to distinguish between isolates from Spain and those from the Czech Republic by using PFGE and restriction enzyme _Bam_ HI. Furthermore, isolates TU3, TU17, TU21, and TU25 were unequivocally assigned to pulsotype B instead of pulsotype A. In instances of discrepancy, analyses were repeated in triplicate with new cultures, and the findings reported were confirmed. Distribution of the 107 _F_ . _tularensis_ subsp. _holarctica_ isolates into 4 pulsotypes resulted in a Simpson index of diversity of 0.522. There was no obvious correlation between the pulsotype of an isolate from Spain and its geographic origin, host, or tularemia outbreak with which it was associated.\n\n【24】##### Characterization by MLVA\n\n【25】The allele-based analysis of genetic relationships identified 13 MLVA types among the 108 _F_ . _tularensis_ subsp. _holarctica_ isolates and showed that _F_ . _tularensis_ subsp. _tularensis_ strain Schu (CAPM 5600) was more distantly related . The 10 isolates from the Czech Republic were assigned to 5 MLVA types, which differed from isolates from Spain by ≥2 alleles. Marker Ft-M3 provided the highest number of alleles . Six copy numbers were detected among the 109 isolates; for Ft-M6, Ft-M9, and Ft-M20, there were 3 copy numbers. Markers Ft-M5, Ft-M7, Ft-M8, Ft-M10, Ft-M13, FT-M16, Ft-M19, Ft-M21, Ft-M22, Ft-M23, and Ft-M24 each had 2 alleles: these markers with 2 alleles, except for Ft-M24, discriminated only _F_ . _tularensis_ subsp. _tularensis_ strain Schu (CAPM 5600) from all isolates of _F_ . _tularensis_ subsp. _holarctica_ .\n\n【26】Ft-M24 had a 464-bp allele that was found in all isolates from Spain analyzed in this study, but was not present in any other isolates. Ft-M24 has been found only in isolates of genetic subclade B.Br:FTNF002–00 (the Iberian clone or the central and western European genetic group). Sequence analysis of the Ft-M24 DNA fragment showed that the unique allele size was caused by deletion of a 16-bp sequence adjacent to 2 copies of the Ft-M24 tandem repeat . For marker Ft-M12, because all 109 isolates had the same copy number, this marker provided no typing resolution. Distribution of _F_ . _tularensis_ subsp. _holarctica_ isolates among 12 MLVA types was uneven; >70% of the isolates in the MLVA types A (49 isolates, 45%) and B (32 isolates, 29.4%) . Simpson index of diversity, which showed the discriminatory power of MLVA for the 108 _F_ . _tularensis_ subsp. _holarctica_ isolates, was 0.708.\n\n【27】The 98 isolates from Spain were classified into 8 MLVA types, which essentially grouped as 2 closely related clonal complexes that differed at only 1 of the 16 MLVA markers . All MLVA types from Spain were single-locus variants of MLVA type B, which indicated this type was the founder genotype of _F. tularensis_ that caused tularemia in northwestern Spain. No clear relationship was found between genotype and geographic origin , source of infection, or host in Spain. The same genotype was usually isolated from hares, voles, and humans in Spain.\n\n【28】Comparison of isolates from the 2 outbreak periods (37 isolates for 1997–1998 and 61 isolates for 2007–2008) showed that the same _F. tularensis_ genotypes caused tularemia in both outbreaks . Isolates from the second outbreak showed less genetic diversity than those from the first outbreak (Simpson indices 0.62, 95% CI 0.53–0.71 and 0.66, 95% CI 0.57–0.75, respectively; the difference was not significant at the 95% level). Comparison of allele distribution at the most variable marker (Ft-M3) showed an overall similarity between isolates causing the outbreaks, although the most common copy number was 4 during the first outbreak and 5 during the second outbreak, which might indicate a stepwise increase in copy number over time. Overall, our findings for 37 isolates from the first outbreak were consistent with the data reported by Dempsey et al. although there were 2 discrepancies. First, isolate TU18 had a unique allele with 3 tandem repeats at Ft-M9, which distinguished this isolate from all other _F_ . _tularensis_ subsp. _holarctica_ isolates. Second, isolate TU31 had the same FT-M10 allele as all other isolates. We confirmed our results for these discrepancies in triplicate.\n\n【29】##### Quantification of Agreement between PFGE Typing and MLVA\n\n【30】Congruence of the 2 methods (PFGE typing and MLVA) for isolate classification was weak for the 107 _F. tularensis_ subsp. _holarctica_ isolates (1 of the 108 isolates was excluded because it was not typeable by PFGE analysis). This finding was true for reverse comparisons of both methods: if 2 isolates were in the same PFGE pulsotype; they had an 18% chance of having the same MLVA type. Conversely, having the same MLVA type was associated with a 40% chance of having the same PFGE pulsotype. The adjusted Wallace coefficient for PFGE versus MLVA was 0.18 (95% CI 0.04–0.32) and that for MLVA versus PFGE was 0.40 (95% CI 0.20–0.58).\n\n【31】### Discussion\n\n【32】_F_ . _tularensis_ subsp. _holarctica_ has shown limited genetic diversity worldwide . This finding might be the result of a relatively recent bottleneck or clonal expansion event that drastically reduced genetic variation of the bacterial population . Consistent with previous findings, we observed extremely limited genomic diversity among the 98 _F_ . _tularensis_ subsp. _holarctica_ isolates from Spain analyzed by 2 genotyping tools: PFGE with 2 restriction enzymes and MLVA at 16 highly variable tandem repeat loci. PFGE identified 3 genotypes with single band differences (93.3% similarity) . MLVA discriminated these isolates into 8 MLVA types, but in pairwise comparisons they differed at no more than 2 of 16 MLVA markers. Thus, our collection shows extreme genetic homogeneity .\n\n【33】An _F. tularensis_ subsp. _holarctica_ lineage in central and western Europe (France, Spain, and Switzerland) has been defined . Strains belonging to this lineage have 2 unique genetic traits: a 1.59-kbp genomic deletion at the RD23 locus and a unique 464-bp allele at Ft-M24. All isolates from Spain had these alleles, irrespective of the outbreak, geographic origin, or the host from which they were recovered. Thus, all isolates from Spain analyzed belong to the genetic subclade B.Br:FTNF002–00 (the Iberian clone or central and western European genetic group) . Furthermore, all MLVA types for isolates from Spain were single-locus variants of MLVA type B, which suggested that this type might be a founding genotype that has evolved into multiple other genotypes that differ from the founding genotype at a single loci. In this scenario, all strains causing tularemia outbreaks in Spain are linked to this founder (ancestral) genotype.\n\n【34】We found poor congruence between typing results of PFGE and MLVA for 107 _F. tularensis_ subsp. _holarctica_ isolates. However, the 2 methods might indicate different types of genetic variation. PFGE is a suitable approach for detecting rearrangements in a genome, and differences of only 1 band observed among the 98 isolates from Spain are presumably consequences of a single mutation event that might be an inversion, translocation, deletion, or a single-nucleotide polymorphism . In contrast, MLVA detects variation in several, fast-evolving, repeated sequences. However, such rapidly evolving sequences are susceptible to homoplasy, and genetic classification of isolates on the basis of a difference at only 1 of 16 genetic loci, as for isolates from Spain, could be biased because of genetic reversion events.\n\n【35】Because some of the mutations detected by PFGE or MLVA might not be selectively neutral, we might have observed time-dependent mutations that are transient on evolutionary time scales and will frequently be eliminated by selection pressure acting on them . If this hypothesis is true, use of single mutations for genetic discrimination may lead to incorrect phylogenetic inferences. Therefore, poor congruence between PFGE and MLVA for identifying genetic subclade B.Br:FTNF002–00 of _F_ . _tularensis_ subsp. _holarctica_ in Spain might be caused by limited bacterial diversity. Use of more extensive genetic analyses for typing, such as whole genome sequencing, might be useful in subsequent molecular epidemiology studies.\n\n【36】In Spain, tularemia was first reported in late 1997 in association with one of the largest human outbreaks ever described . The most common route of infection of humans was by direct contact when hunting and handling hares ( _L. europaeus_ ). Consistent with infection with _F. tularensis_ through the skin, the most frequent clinical form was ulceroglandular tularemia (55.4%); glandular (15.3%) and typhoid (6.6%) forms of the disease were also observed. A second major human outbreak occurred in the same geographic area in northwest Spain in 2007 and 2008 after 10 years of no epidemiologic activity. The epidemiology of the second outbreak was different from that of the first outbreak. The second outbreak occurred when the population of the common vole ( _Microtus arvalis_ ) peaked, and >65% of case-patients had typhoidal and pneumonic forms of tularemia , which is consistent with infection by inhalation.\n\n【37】Few outbreaks of tularemia caused by airborne transmission of the bacteria have been reported. These outbreaks include a notable outbreak of inhalational tularemia in Sweden in 1966–1967 associated with environmental exposure of farmers in which >600 cases were diagnosed .There were clusters of outbreaks on Martha’s Vineyard (Massachusetts, USA) in 1978 and 2000  and cases of tularemia caused by airborne transmission to 53 farmers in northern Finland during 1982. In Germany in 2005, a total of 39 participants in a hare hunt were infected after exposure to contaminated droplets generated by rinsing infected hares .\n\n【38】Isolates from Spain obtained during the second tularemia outbreak had the same genotypes as those obtained during the first outbreak . Furthermore, we did not observe any relationships between genotype and geographic origin, or host from which the isolates were recovered, which suggested that in that area, the same clones were circulating in all hosts . These results are useful because the 2 outbreaks had substantial epidemiologic differences . Our findings indicate that the outbreak in 2007, after 10 years of no epidemiologic activity, was not caused by introduction of a new strain, but by reemergence of an endemic bacterial population that has been circulating in the region for at least the past 15 years. Furthermore, our findings also suggest that clinical forms of the outbreak are determined by ecologic processes involved in infection (e.g. route of infection, infective dose) rather than by the genotype of the pathogen.\n\n【39】In conclusion, we report genetic characterization of _F_ . _tularensis_ subsp. _holarctica_ isolated in Spain during 2 of the largest tularemia outbreaks worldwide. There were marked epidemiologic differences between the 2 outbreaks, which were separated by 10 years of no epidemic activity. Molecular investigations showed that both outbreaks were caused by the same group of closely related genotypes in subclade B.Br:FTNF002–00. Therefore, the reemergence of tularemia in 2007 was presumably not caused by introduction of a new strain, but by persistence of local reservoirs of infection. These findings, along with sporadic cases of tularemia in 1998 and 2007, suggest that local foci of tularemia have become established in Spain. Further investigations will help identify these endemic foci and clarify biotic and abiotic factors that have favored establishment of the pathogen in northwestern Spain.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "014794b9-c637-4155-850a-21253bce3f81", "title": "Imported Lassa Fever, Pennsylvania, USA, 2010", "text": "【0】Imported Lassa Fever, Pennsylvania, USA, 2010\nLassa fever is a potentially severe viral infection caused by _Lassa virus_ (family _Arenaviridae_ , genus _Arenavirus_ ), with an overall case-fatality rate of 1%–2% and a case-fatality rate of 15%–20% for hospitalized patients . The virus is endemic to West Africa, with the reservoir host being _Mastomys_ spp. rodents . Person-to-person transmission of Lassa virus can occur through direct exposure to infected blood or secretions, and instances of nosocomial transmission have been documented . Primary symptoms of Lassa fever are fever, headache, nausea, diarrhea, sore throat, and myalgia; hemorrhagic signs or deafness may also occur during illness .\n\n【1】Because the incubation period ranges from a few days to >2 weeks  and many symptoms are nonspecific, the potential exists for human carriage of Lassa virus to areas outside those to which it is endemic, putting travel companions, close contacts, and healthcare providers at risk for secondary infection. Before 2010, five instances of imported Lassa virus were recorded in persons from West Africa to the United States. Although early instances involved sick persons who were airlifted to the United States for diagnosis and treatment , the 2 most recent occurrences (1989 and 2004) involved persons who were not identified as potentially infectious until healthcare was sought in the United States . Here we report a case in a person who became infected and sick during a trip to Liberia and sought care upon return to the United States.\n\n【2】### The Case\n\n【3】A Liberian man 47 years of age living in the United States traveled to Liberia in January 2010. He arrived in Monrovia, then spent 5 days traveling throughout Nimba County in north-central Liberia, bordering Guinea and Côte d’Ivoire. He reported sleeping nightly in his rural native village in a dwelling infested with rats and recalled several rat carcasses on the bedroom floor. On the day of his departure from Liberia, he developed fever, chills, joint pain of the knees and ankles, anorexia, sore throat, diffuse skin tenderness, and mild shortness of breath; he began taking amoxicillin and chloroquine before departing Liberia.\n\n【4】The patient’s symptoms persisted upon arrival in the United States, prompting him to seek medical attention on day 5 of his illness. When he sought treatment, he had fever of 103°F, pulse of 99 beats/min, respiratory rate of 15 breaths/min, and blood pressure of 120/80 mm Hg . His physical examination was notable for posterior cervical adenopathy and a palpable spleen tip . He had no evidence of conjunctival, nasal, or oral petechiae; no skin rashes; and no signs of hemorrhage or other lesions. Initial laboratory data showed leukopenia and thrombocytopenia and minimal transaminase elevations . Empiric malaria treatment was initiated upon admission and was subsequently discontinued when _Plasmodium_ spp. antigen testing was negative and thick and thin blood smears showed no evident parasitemia. By the next day, mild pharyngitis with slight tonsillar exudates had developed. On the third hospital day, substernal chest pain and profuse watery diarrhea developed. Increasing transaminases and a slight coagulopathy were noted. Lassa fever was considered in the differential diagnosis; contact precautions and, subsequently, airborne precautions were taken. Because of noted clinical improvement, he was not given empiric intravenous ribavirin.\n\n【5】On day 5 of hospitalization, Lassa virus was identified by real-time PCR by using samples collected 2 days earlier, and sequencing of the amplified fragment yielded a unique sequence similar to sequences from previous Lassa virus isolates from Liberia. Subsequent samples confirmed Lassa fever diagnosis on the basis of real-time PCR, viral culture, and serology .\n\n【6】The patient’s fever resolved by day 16 of his illness. After 2 successive negative blood real-time PCR results, he was discharged from the hospital on day 21 of his illness with instructions to avoid unprotected sexual intercourse for 2 months. No hearing abnormalities were noted at the time of discharge or during telephone conversations 2 weeks and 2 months later.\n\n【7】A contact investigation was undertaken by the hospital and local, state, federal, and international health agencies. Exposed persons were identified as any persons who potentially came into contact with the patient or his body fluids during his illness. Because no contacts had direct exposure to body fluids (other than the patient’s wife in Africa with whom he had sexual intercourse before becoming ill and who remained well, according to telephone follow-up with the patient), no patient contacts were considered high risk for secondary transmission . In total, 140 persons, including the patient’s family in the United States, co-workers, and hospital workers who had contact with him (but did not have direct contact with bodily fluids) were identified as low-risk contacts. Health communication materials were developed on the basis of previous Lassa fever contact tracing activities . All hospital and community contacts were provided a Lassa fever fact sheet and asked to seek medical consultation if fever or other signs and symptoms of Lassa fever appeared. Upon completion of 21 days of follow-up, no secondary cases were identified.\n\n【8】### Conclusions\n\n【9】The spectrum of Lassa fever can run from asymptomatic seroconversion to severe hemorrhagic fever with multiple organ failure and death . Factors supporting the diagnosis of Lassa fever in returning travelers include relevant epidemiologic exposure (travel to rural West Africa), signs and symptoms consistent with Lassa fever, and the absence of other infectious agents that can account for the illness. Although this patient did not seek treatment for hemorrhagic signs, his fever, pharyngitis, chest pain, and diarrhea , as well as thrombocytopenia and elevated transaminases, were consistent with Lassa fever . Early institution of ribavirin can dramatically decrease death rates among patients with severe Lassa fever if given within the first 6 days of illness ; therefore, empiric ribavirin should be considered for an ill patient suspected of having Lassa fever.\n\n【10】As in this case, early suspicion of Lassa fever should prompt isolation measures to avoid secondary transmission; laboratory testing should be limited to essential tests, and all laboratory specimens should be handled with appropriate biosafety precautions to avoid aerosolizing the virus . Experience in regions where Lassa virus is endemic suggests human-to-human transmission occurs through direct contact with blood and body fluids or large-particle inhalation; transmission through viral aerosolization is not seen; and generally, when universal precautions are undertaken, transmission is unlikely . Nonetheless, aerosol in addition to contact precautions were undertaken once Lassa fever was suspected, given the theoretical potential for acquiring infection through inhalation of airborne virus from respiratory secretions or, in this case, copious diarrhea.\n\n【11】The patient described in this report represents the sixth known occurrence of Lassa fever imported to the United States. Clinicians treating recent travelers to West Africa who are febrile should obtain detailed histories from patients to determine whether they have traveled into rural areas in which the potential for exposure to rodents exists. The symptoms, signs, and laboratory abnormalities of Lassa fever are nonspecific and can overlap with other tropical infections. Therefore, efforts should be made to promptly diagnose or rule out other infectious agents in a patient who has the appropriate travel and exposure history so that further diagnostic studies and empiric therapy with ribavirin can be undertaken rapidly. Moreover, as soon as Lassa fever is suspected, patients and their specimens should be handled with adequate precautions, the local health department and Centers for Disease Control and Prevention should be notified, and specimens should be sent for specific diagnostic testing.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "7d507688-e51c-499c-beca-965ee3dc5d91", "title": "Severe Pneumonia and Human Bocavirus in Adult", "text": "【0】Severe Pneumonia and Human Bocavirus in Adult\n**To the Editor:** The newly identified human bocavirus (hBoV), a member of the _Parvovirus_ family, is suspected to infect the cells of the respiratory tract and thus may be an etiologic agent of respiratory disease in humans . Although Koch postulates have not been fulfilled for HboV, it appears likely to cause a substantial number of respiratory tract infections, at least in children . We describe a case of severe atypical pneumonia associated with hBoV DNA in a bronchoalveolar lavage (BAL) sample from an adult.\n\n【1】The patient was a 28-year-old Caucasian woman with an angioimmunoblastic T–non-Hodgkin lymphoma (NHL) that changed into a highly malignant blastic B-cell lymphoma (T-cell–rich B-NHL state I with 70% CD20+ cells, initial stage IIIB). The patient was previously treated with vincristine and prednisone, followed by chemotherapy according to the R-CHOEP-14 protocol (3 cycles) (November 2003 through January 2004). From January through February 2004, chemotherapy was combined with antimicrobial drug therapy according to the R-DHAP protocol (which includes dexamethasone, the chemotherapy drugs cytarabine and cisplatin, and the monoclonal antibody drug rituximab) for persisting symptoms from the B-cell lymphoma. This regimen was followed by a therapy switch to alemtuzumab with ifosfamid, carboplatin, and etoposide (March 2004), which led to a therapy-induced leukopenia, thrombocytopenia, and high fever >40°C by the end of March and the beginning of April 2004. In May 2004, a second round of alemtuzumab with ifosfamid, carboplatin, and etoposide chemotherapy was initiated. In June 2004, a therapy-induced hemorrhagic cystitis occurred. During July 2004, the patient had ongoing high fever and aplasia of bone marrow with unclear etiology. On July 22, hospital treatment was initiated; it consisted of antimicrobial drug treatment with ceftriaxone (1,000 mg once daily) and gentamicin (320 mg once daily), and antimycotic therapy was started with caspofungin (50 mg once daily).\n\n【2】Since cytomegalovirus (CMV) infection was suspected, ganciclovir (250 mg twice daily) was administered IV for 2 weeks. Although the patient reported an ongoing cough and pneumonialike symptoms, a severe atypical pneumonia that was refractive to antibacterial and antimycotic treatment was diagnosed for the first time during this hospital treatment. Computed tomography scan showed bilateral atypical reticulonodular infiltrations predominant in the lower zones of the lungs .\n\n【3】The BAL obtained during exacerbation of the pulmonary symptoms was tested for _Mycobacterium tuberculosis_ , _Chlamydia pneumoniae_ , _Pneumocystis jirovecii_ , _Aspergillus_ sp. _Candida_ sp. _Cryptococcus neoformans_ , CMV, Epstein-Barr virus, hepatitis B virus, hepatitis C virus, HIV, herpes simplex virus, and varicella-zoster virus by PCR and culture cultivation. Results were negative, except for a temporarily weak reactivity for _Aspergillus_ antigen in serum and for CMV DNA in peripheral blood lymphocytes, which was positive before and became negative after ganciclovir therapy. An archived portion of the BAL was assayed retrospectively by PCR/reverse transcriptase–PCR for human bocavirus, respiratory syncytial virus, human coronaviruses including severe acute respiratory syndrome–associated coronavirus, influenza virus, and human metapneumovirus (hMPV). The only positive result was obtained for human bocavirus, which was confirmed by sequence analysis of the PCR product.\n\n【4】Within a few days, the patient's symptoms decreased, and she was discharged from hospital on day 41, despite ongoing bone marrow aplasia with antimicrobial and antimycotic prophylaxis, including trimethoprim/sulfamethoxazole (160 mg/800 mg once daily) and (voriconazole 200 mg twice daily). Clinical observations led to the primary assumption that the fever, cough, and pulmonary symptoms were likely caused by the postchemotherapeutic extended bone marrow aplasia and CMV infection accompanied by an unclear bacterial but fungus-typical infection. Retrospectively, however, human bocavirus DNA in the archived BAL strongly suggests that pulmonary symptoms were caused by this agent rather than by a yet unknown bacterial or fungal infection. Thus, in the clinical episode described here, the likely causative agent responsible for the severe pneumonia was the recently described bocavirus.\n\n【5】Respiratory viruses such as respiratory syncytial virus, hMPV, and hBoV seem to be the most prevalent etiologic agents of acute lower respiratory tract infection in children. Recently, evidence of human bocavirus infection was reported for 3.1% to 5.7% of children <3 years of age . Previously, only limited data on adults, including immunocompromised patients, were available, but the case we describe supports the hypothesis proposed for other newly identified respiratory viruses, namely, that these pathogens also contribute to severe infections in adult patients at high risk. For example, hMPV was found in 3% of stem-cell transplant recipients who underwent BAL because of lower respiratory tract infection . In those high-risk patients, hMPV also induced fatal infections . This finding led to the conclusion that a \"new\" virus that induces the identical clinical symptoms, like the human bocavirus, may also contribute to severe respiratory infections. In summary, this first report of a respiratory tract infection with hBoV in an adult immunocompromised patient strongly supports the assumption that hBoV is an emerging pathogen that requires our attention, even for adult patients .", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "d55419ac-a64a-4123-9c87-44b4ffde6836", "title": "Septicemia Caused by Tick-borne Bacterial Pathogen Candidatus Neoehrlichia mikurensis", "text": "【0】Septicemia Caused by Tick-borne Bacterial Pathogen Candidatus Neoehrlichia mikurensis\nSince the novel bacterial genus _Neoehrlichia_ wass first described in 2004, its pathogenic role in humans has remained unexplained . Related bacteria such as _Ehrlichia chaffeensis_ and _Anaplasma phagocytophilum_ are emerging tick-borne human pathogens that cause monocytic and granulocytic ehrlichiosis, respectively. These tick-borne diseases manifest themselves as febrile illness, mild transient hepatitis, transient thrombocytopenia, and occasionally as a rash. The family _Anaplasmataceae_ compromises the genera _Ehrlichia_ , _Anaplasma_ , _Neorickettsia_ , and _Aegyptianella_ , and the proposed genus _Neoehrlichia._ These are all obligate intracellular bacteria, which currently are difficult or impossible to isolate and culture . Infections caused by agents of this bacterial family have been recognized as an emerging problem in the past 2 decades, possibly due to ecologic changes and the resulting expansion of tick populations .\n\n【1】### Case Report\n\n【2】In August 2009, a 61-year-old Caucasian man who lived in Switzerland sought treatment at the emergency department of University Hospital in Zurich, reporting a 10-day history of malaise, temperature as high as 39.5°C, chills, and moderate dyspnea. Six weeks previously, he had undergone coronary artery bypass graft surgery and mitral valve reconstruction for which prosthetic material was used. The patient had not noticed tick bites or a skin rash; neither did he recall a rodent bite. A pet dog and cat lived in his household.\n\n【3】Physical examination showed a reduced general health condition and a temperature of 38.5°C. Blood pressure was 109/68 mm Hg, heart rate was 86 beats/min, and oxygen saturation was 95% with 2 L nasal oxygen. No murmur was detected on cardiac auscultation. No skin or joint abnormalities were found. Laboratory tests showed elevated leukocytes (12.9 × 10 3  cells/µL), with a high fraction of neutrophils (10.1 × 10 3  cells/µL) and thrombocyte count within reference range (277 × 10 3  cells/µL); aminotransferase levels within reference ranges (aspartate aminotransferase 18 U/L, alanine aminotransferase 20 U/L); and an elevated C-reactive protein (CRP) of 68 mg/L (reference range <5 mg/L). Chest radiograph showed no signs of cardiac decompensation or of pulmonary infiltrates. Transthoracic echocardiograph showed only minor insufficiency of the aortic and tricuspid valves. In addition, degenerative alterations of aortic valve, but no vegetations, were noted with comparable findings in the follow-up echocardiograph 1 week later.\n\n【4】At the follow-up visit, no hints of infectious foci were found. Five sets of blood cultures were drawn with >12 h difference between the first and the last set. Antimicrobial drug treatment for endocarditis with prosthetic material, consisting of vancomycin, gentamicin, and rifampin, was initiated.\n\n【5】Blood cultures remained negative for microbial growth, even after extended incubation. Serologic tests for agents of culture-negative endocarditis and tick-borne diseases were performed. Enzyme immunoassays (EIAs) were positive for immunoglobulin (Ig) G antibodies reactive to _Bartonella henselae_  and _B. quintana_ , _Coxiella burnetii_ (phase II IgG titer 160), _Rickettsia rickettsii/conorii_ (IgG 256), and _Rickettsia typhi_ (IgG 128), _Mycoplasma pneumoniae_ (index 2.7). IgM was positive only for _A._ phagocytophilum (512, atypical fluorescence pattern), presenting a low titer of IgG at this stage. Serologic test results for _Brucella_ spp. _, Chlamydia trachomatis, Chlamydia pneumoniae,_ and _Borrelia burgdorferi_ were negative. Species-specific PCRs for _A._ phagocytophilum, _Tropheryma whipplei_ , _B. henselae_ , _B. quintana_ , _Legionella_ spp. and _L. pneumophila_ were negative.\n\n【6】Bacterial broad-spectrum 16S rRNA gene PCR, followed by sequence analysis, identified _Candidatus_ Neoehrlichia mikurensis in 4 of 8 sequential blood samples; the 4 samples that tested positive were collected before (day 0) and during the initial phase (days 7 and 13) of an effective course of antimicrobial drug therapy . For 16S rRNA gene amplification, DNA was extracted and amplified from anticoagulated blood (4-mL EDTA tubes), uncoagulated blood from a BacT/ALERT SA aerobic blood culture flask (bioMérieux SA, Geneva, Switzerland), and coagulated blood  as described . Sequences derived (GenBank accession nos. GQ501089-GQ501091) were analyzed by SmartGene IDNS software (Zug, Switzerland). We amplified and sequenced  an alternative target gene ( _groEL_ ) with primers  derived from published sequences , which showed 98% homology to previously published _Candidatus_ Neoehrlichia mikurensis _groEL_ sequences .\n\n【7】After being treated with vancomycin, gentamicin, and rifampin for 10 days, the patient became afebrile, and clinical symptoms improved. Leukocyte count was within reference range, and CRP dropped from 68 mg/L to 23 mg/L. At this time, _Candidatus_ Neoehrlichia mikurensis was detected in the first blood sample. Rifampin (450 mg 2×/d) was continued, and vancomycin and gentamicin were switched to oral doxycycline (100 mg 2×/d). Three weeks later, CRP was 1 mg/L, body temperature was within the normal range, and treatment was continued to finish a 6-week course. Two weeks after the end of treatment, the patient was seen for a follow-up visit. Neither clinical nor laboratory results raised any concern of relapse. Results of broad-range PCR of the 16S rRNA gene to detect _Candidatus_ Neoehrlichia mikurensis were negative for the first time in 5 weeks since treatment began initiation and remained negative at the follow-up visit 2 weeks after the end of treatment.\n\n【8】_Candidatus_ Neoehrlichia mikurensis was previously found in _Rattus norvegicus_ rats and _Ixodes ovatus_ ticks in Japan , in _R. norvegicus_ rats in China , and in _I. ricinus_ ticks in the Netherlands , Slovakia , and the Asian part of Russia . Closely related rickettsial bacteria  have been identified in _Procyon lotor_ raccoons in the Piedmont region of Georgia, USA . Another closely related species ( _Candidatus_ Ehrlichia walkeri) has been detected in _I. ricinus_ ticks collected from humans in northern Italy . The geographic distribution of the tick population has also been studied .\n\n【9】Our patient lives in a high-risk area for ticks in Switzerland. _I. ricinus_ is the main tick species in this region. A tick-borne disease appears epidemiologically possible in this patient, who is a golfer and the owner of a large garden and thus is repeatedly exposed to the habitat of the potential vector, _I. ricinus_ , even though he remembered no tick bites. Of note, only 50%–70% of patients with Lyme disease remember receiving a tick bite . Blood of the patient’s pet animals (dog and cat) was examined by broad-range 16S rRNA gene PCR to exclude presence of bacterial pathogens.\n\n【10】In Wister rats, _Candidatus_ Neoehrlichia mikurensis has been shown to infect spleen sinus endothelial cells, forming intracellular inclusions on the side of the endosome (by electron microscopy 60 days after infection) . Accordingly, we assumed that in human hosts, valvular endothelial cells are likely involved. The initial antimicrobial drug therapy, which contained rifampin, may already have contributed to the reduction of the bacterial load but was not completely effective . Thus, following the recommended guidelines for treatment of intracellular rickettsial bacteria with endocardial involvement, we changed to a 6-week course of treatment consisting of rifampin combined with doxycycline . After the end of the course, we observed a successful response.\n\n【11】We detected _Candidatus_ Neoehrlichia mikurensis in 4 of 8 consecutive blood specimens, and repeated analysis showed the disappearance of the pathogen’s DNA during the course of treatment . Laboratory diagnosis of ehrlichiosis is severely hampered because the relevant pathogens cannot be cultured on routine media. Serologic tests depends on samples collected during acute phase of illness, obtaining comparative samples in the course of the disease, and demonstrating a >4-fold increase in antibody titers.\n\n【12】### Conclusions\n\n【13】We have identified _Candidatus_ Neoehrlichia mikurensis in multiple blood samples of a patient who sought treatment for septicemia. Therapeutic success has been shown over time by the fact that the suggested pathogen’s DNA was no longer detectable and by a favorable clinical outcome. Surveys of arthropod populations should be conducted to examine the geographic distribution of _Candidatus_ Neoehrlichia mikurensis, and species-specific assays could determine the relevance of this organism in human ehrlichial diseases.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "8d5272e8-1d20-498d-897f-ae5ba7ca3991", "title": "Past as Prologue—Use of Rubella Vaccination Program Lessons to Inform COVID-19 Vaccination", "text": "【0】Past as Prologue—Use of Rubella Vaccination Program Lessons to Inform COVID-19 Vaccination\nAs COVID-19 spreads throughout the world, we recall a similar experience of a swiftly spreading respiratory disease over half a century earlier. In 1963, a rubella virus epidemic spread from Europe to the United States, causing great alarm among public health officials. The New York Times reported on February 8, 1964: “GERMAN MEASLES AT EPIDEMIC RATE; City and State Affected—2,302 Cases Reported Here Since Dec. 1; Virus Is Termed Mild; But Women Are Warned of Danger During First 3 Months of Pregnancy” .\n\n【1】Although rubella is generally a mild disease, rubella infection during early pregnancy can be devastating. Fetal infection can result in miscarriage, stillbirth, or infants born with life-threatening or disabling congenital malformations, known as congenital rubella syndrome (CRS). A pregnant woman infected with rubella in early pregnancy has up to a 90% chance of giving birth to an infant with CRS and that infant having \\> 1 malformations, such as congenital heart defect, cataracts, and hearing impairment. CRS is the most substantial public health threat of rubella infection and is associated with an infant mortality rate of 20%–40% and lifelong sequelae for many of those infants that survive .\n\n【2】The outbreak of rubella in 1963 necessitated expeditious development of a vaccine to protect pregnant women and their infants and to stem societal disruption from the subsequent epidemic. Later, licensure and widespread availability of vaccines prevented future epidemics of rubella in the United States and other countries. As of October 2021, a total of 173 (89%) of 194 countries have introduced rubella vaccine, and 93 (48%) have been declared free of endemic rubella transmission .\n\n【3】Rubella and SARS-CoV-2 viruses have several similarities . Both viruses are enveloped, positive-stranded RNA viruses  that are transmissible through respiratory droplets. Both viruses can result in asymptomatic infections, fostering silent disease transmission . On average, in countries with no available rubella vaccine, 1 rubella-infected person can infect 6–12 other susceptible persons , an infection rate similar to that of the SARS-CoV-2 Delta and Omicron variants . Both viruses are associated with serious disease complications. For rubella, the most serious complication is CRS ; COVID-19 complications include respiratory failure, multisystem inflammatory syndromes, post—COVID-19 conditions, and preterm delivery or stillbirth . Both viral infections can result in death. Like deaths attributed to rubella infection before vaccine introduction, most COVID-19–related deaths occur in specific high-risk populations. SARS-CoV-2 infections cause higher mortality among the elderly and those with specific underlying conditions than among younger, generally healthy adults. Because of the commonalities of SARS-CoV-2 and rubella, the rubella disease control program might serve as a useful comparator in formulating COVID-19 vaccination strategy and implementation.\n\n【4】We believe that the US rubella disease control program, which incorporated strategic planning, goal communication, program initiation, and program revisions driven by data, provides key insights for developing vaccines to combat the COVID-19 pandemic. Here, we highlight key components of the rubella disease control and elimination program, including vaccination strategies and vaccine selection methods, and describe how these experiences might inform current COVID-19 vaccination programs.\n\n【5】### Vaccination Strategy\n\n【6】The primary goal of a vaccination program is to reduce disease burden by achieving high population immunity levels through strategies aimed at both optimal immunization coverage and high vaccine effectiveness. The success of vaccination strategies depends greatly on practical aspects of implementation. An individual protection, or selective, approach targets specific groups that are defined by such factors as risk or age. The aim of this approach is to protect vulnerable groups against disease and severe outcomes (hospitalization, complications, death). Although the individual protection approach can prevent severe outcomes, some high-risk persons can be missed. In instances when the entire population is at risk of infection, there is ongoing transmission risk to vulnerable persons. Thus, a universal approach might be a better strategy in such circumstances since this approach indirectly impacts vulnerable subgroups by increasing population-level immunity and potentially interrupting or even eliminating virus transmission. A universal approach requires vaccines with high efficacy against infection across a wide range of vaccine recipients and viral subtypes.\n\n【7】The primary goal of national rubella vaccination programs is to prevent rubella infection in pregnant women and thereby prevent the severe outcome of CRS. When rubella emerged as a nationwide threat, 2 vaccination strategies were implemented to achieve this goal: an individual protection approach that prioritized vaccinating high-risk populations (adolescent females and women of childbearing age) to prevent CRS; and a universal approach that aimed to decrease and interrupt transmission at the population level by vaccinating the age group with the highest proportion of susceptible persons: primarily, young children and those potentially at highest risk (e.g. reproductive-age women).\n\n【8】In 1970, the United Kingdom adopted the individual protection approach, primarily vaccinating nonpregnant women of childbearing age. This decision was informed by concerns at that time regarding unknown duration of vaccine-induced immunity in children, as well as the fact that measles vaccination coverage in the United Kingdom was low and rubella vaccine would have been given at the same time as the measles vaccine . Surveillance data showed that this approach decreased the incidence of CRS cases and termination of pregnancies associated with rubella . However, because the approach only focused on individual protection, viral transmission continued in the population at large. Unvaccinated women in the United Kingdom continued to be infected, and children continued to be born with CRS, albeit at a lower rate . Studies demonstrated that unprotected persons still posed a risk. For example, in 1 study, pregnant women with previous pregnancies had a higher risk of rubella infection than did women in their first pregnancies, suggesting that women with previous pregnancies may have been at risk of acquired rubella infection from their own children with rubella . Whereas control of rubella through individual protection was proving to be inadequate, immunization program advancements had occurred, and measles vaccination coverage had increased, which prompted UK policymakers to pivot to the universal approach: vaccinating all young children to protect the larger population.\n\n【9】In contrast to the initial UK approach, the United States launched its rubella vaccination program in 1969, using the universal approach. Children \\> 1 year of age up to puberty were vaccinated against rubella with the aim of eliminating rubella virus transmission and infection. From 1969 through 1977, an estimated 80 million doses of rubella virus vaccine were distributed in the United States . As in the United Kingdom, the rubella vaccination program was systematically monitored through disease surveillance, seroprevalence studies, and vaccination coverage assessments . Those data illustrated that susceptibility remained high among women of childbearing age and that they were still being exposed. To decrease the rubella immunity gaps resulting from the universal approach, which was focused on pediatric vaccination, the United States expanded its rubella vaccination strategy in 1978 to include vaccination of older groups. After this policy shift and through the late 1980s, cases of rubella infection and CRS in the United States declined further . We provide a comparison of the individual protection and universal strategies .\n\n【10】By determining disease burden and monitoring vaccination impact, disease control experts used rubella and CRS surveillance data to iteratively inform rubella disease control strategy and used vaccination coverage data to determine the progress of vaccination programs. Additional activities (e.g. monitoring vaccine safety through pregnancy registries, adverse events surveillance) provided data to ensure vaccine safety and gain public confidence. Those data sources were critical in determining the progress of specific programs. In the United States, surveillance data documented an end to rubella outbreaks by autochthonous transmission, and elimination was verified in 2004. In the United Kingdom, rubella surveillance and vaccination program data prompted a change in program strategy to a universal protection approach, which led to elimination, verified in 2016. The United States and the United Kingdom still continue to experience imported rubella cases from countries with high levels of ongoing transmission, usually from countries with low immunization coverage or those that have not introduced rubella vaccine . As such, both countries would still benefit from global elimination.\n\n【11】Globally, the World Health Organization (WHO) initially recommended an individual protection approach to rubella disease control, which evolved to a universal strategy. The first WHO recommendation in 2000 focused on ensuring that women of childbearing age were protected, without preference for a specific strategy . By 2011, WHO recommended both the individual protection and the universal strategies for countries, with a preference for the universal approach . In 2020, the WHO position shifted to recommending only the universal approach .\n\n【12】Although global strategies have shifted over time in response to new data, inequities in global rubella program implementation have been evident in both introduction and elimination activities. Introduction was initially only in high-income countries, but by 2020, rubella vaccine had been introduced in 48% of low-income countries. Of 21 countries that had not introduced rubella vaccine by the end of 2020, a total of 14 were low-income countries . Of the 93 countries that have eliminated rubella disease, only 3 were identified as low-income countries.\n\n【13】### Vaccine Selection\n\n【14】The 1964–1965 rubella epidemic resulted in an estimated 12.5 million rubella cases in the United States, infecting 6% of the US population. Complications included >2,000 cases of encephalitis, 11,350 cases of miscarriage, and 20,000 cases of CRS. Of the CRS cases, >8,000 children were diagnosed with deafness, 3,580 were diagnosed as blind, and 1,800 children had developmental delays. The total estimated economic impact was $1.5 billion .\n\n【15】The epidemic catalyzed rubella vaccine development, which incorporated new laboratory techniques that, in turn, allowed the quick isolation of the rubella virus. During 1969 and 1970, a total of 4 rubella vaccines were licensed, 3 in the United States (HPV77-DE, HPV-77-DK, Cendehill) and 1 in Europe (RA27/3) . Each vaccine was administered as a single dose that provoked a durable, protective immune response when given to a person \\> 9 months of age.\n\n【16】The 4 rubella vaccines were studied continuously for both effectiveness and safety. Immunogenicity of each rubella vaccine was studied from multiple perspectives. HPV77-DE was implemented widely in the United States and found to be immunogenic in 95% of vaccinees and protected 65%–94% of recipients during outbreaks. In contrast, RA27/3 achieved seroconversion in 95%–100% of vaccine recipients ; in numerous outbreaks, protection from RA27/3 was \\> 95% . Antibody levels in 8 comparative studies demonstrated that RA27/3 generated 2- to 4-fold higher antibody levels than either the Cendehill or HPV-77 vaccine . Furthermore, compared with the Cendehill vaccine, the RA27/3 vaccine produced higher antibody levels 6–8 weeks after vaccination . Later studies demonstrated that such antibody response to RA27/3 persisted many years after receipt of the vaccine . Challenge studies have shown that when vaccinated persons were exposed to wild rubella virus, only 3%–10% of the RA27/3 vaccine recipients experienced reinfection (i.e. had breakthrough infections) compared with 40%–100% of the HPV-77 or Cendehill vaccine recipients .\n\n【17】Research also evaluated and compared the safety of these vaccines. The RA27/3 vaccine provoked lower rates of adverse reactions among adults than did HPV-77-DK or HPV-77-DE, both of which were associated with significant acute joint reactions . The safety of vaccination during pregnancy, especially in regard to vaccine-associated CRS, was a chief concern for disease control experts and limited vaccination strategies initially employed in the United States . However, evidence slowly accumulated, including from mass vaccination campaigns in the Americas, that provided strong evidence that rubella vaccine did not cause CRS .\n\n【18】The higher effectiveness of the RA 27/3 vaccine, coupled with lower rates of adverse reactions, led to the vaccine’s widespread adoption as the preferred rubella vaccine in the United States, resulting in its licensure in 1979 and the withdrawal of HPV-77 and Cendehill vaccines . Additional surveillance and comparative research studies strengthened the RA 27/3 vaccine’s status as being especially effective in eliciting a strong immune response, decreasing risk of rubella virus transmission, and achieving these results with a very favorable safety profile . These findings resulted in this vaccine being accepted and used in almost all countries. A systematic literature review in 2019 showed that both single-dose and 2-dose regimens of rubella vaccine are highly immunogenic for a long period of time .\n\n【19】### Lessons Learned from Rubella Vaccination in the COVID-19 Context\n\n【20】Today, rubella transmission has been eliminated in many countries throughout the world as a result of data-driven strategies and an effective, highly immunogenic, and safe vaccine that was developed and approved over time through rigorous scientific research and surveillance. The success of rubella control and elimination as we have described might inform policymakers as they make decisions regarding the COVID-19 vaccine program.\n\n【21】As was the case for the rubella pandemic, the COVID-19 pandemic has resulted in the rapid development and deployment of multiple vaccines. Unlike rubella virus, which had infected persons prior to its pandemic spread, SARS-CoV-2 emerged as a new virus to which the entire global population was susceptible. Although the rubella vaccine has yet to be introduced in 21 countries, COVID-19 vaccines have been introduced in every country . Vaccination inequities do, however, exist for COVID-19 vaccine introduction and use. High-income countries have achieved higher coverage than middle- and low-income countries. Limited vaccine supply, insufficient immunization program capacity, and socioeconomic issues have contributed to this disparity in regard to global vaccination .\n\n【22】When the highly constrained supply of the first COVID-19 vaccines became available in late 2020, COVID-19 vaccination followed an individual protection approach, focusing on protecting the highest-risk populations and then expanding eligibility as vaccine supplies grew. Much like the United Kingdom’s initial individual protection approach to rubella vaccination that resulted in a substantial decline in CRS cases, this selective approach resulted in sharp declines in COVID-19 hospitalizations and deaths among the vaccinated but left the unvaccinated at risk of infection and serious disease . Now that more COVID-19 vaccines have been approved and the vaccine supply expanded, vaccination has been broadened to a larger pool of eligible persons. This increased supply, coupled with new and ever-growing knowledge of each vaccine’s advantages and disadvantages, has further informed COVID-19 vaccination program goals and efforts. In addition, surveillance measures have helped to identify priority populations for COVID-19 vaccination and monitor progress toward risk mitigation and population recovery. Vaccine safety surveillance systems and clinical studies have provided vital information to identify vaccine-associated adverse events.\n\n【23】As the COVID-19 vaccine supply increases and the pandemic evolves, comparative studies with objective criteria are needed to identify the vaccine(s) that meet the immediate goals of the global COVID-19 vaccination strategy, which is to minimize deaths, severe disease, and overall disease burden; curtail the health system impact; fully resume socio-economic activities; and reduce the risk of new variants . Currently available COVID-19 vaccines must be closely examined to distinguish which are most efficient in providing high seroconversion rates, long-term immunity against infection, serious illness, hospitalization, and death, and low rates of adverse events. The challenge of finding an optimal COVID-19 vaccine is compounded given that, unlike the rubella virus, which has only 1 serotype and no variants, SARS-CoV-2 variants continue to emerge . Ongoing COVID-19 vaccine effectiveness protocols and studies provide critical data that help researchers better understand troublesome trends, such as waning of vaccine-induced immunity or variant immune evasion , which further inform vaccine development and vaccination program goals. Innovative studies that examine varying vaccine schedules and combinations are underway, which will help to identify not only the most ideal vaccines but possibly also the best combination of vaccines .\n\n【24】Beyond vaccination strategies and vaccine choices, governments and public health authorities must consider other factors to help meet COVID-19 control goals. In terms of program implementation, key elements include cost considerations, expiration timeframes, cold chain requirements, and storage capacity. From a community perspective, factors affecting vaccination include preferences regarding administration and delivery, access to health services, and trust in healthcare providers and government information. Epidemiologically informed policy and control goals, when clearly and effectively communicated by trusted and empathetic sources, can create a unified vision for how nations can collectively bring an end to the COVID-19 pandemic, while proactively countering disinformation.\n\n【25】Disease control goals and vaccination strategy go hand in hand. Adopting a universal approach, as clinical trial data and licensure permit, would ensure that the world population can benefit from COVID-19 vaccination. Such an approach would require policymakers to address structural barriers to ensure access, equity, and confidence in vaccination. Thus, the success of achieving the goal of disease control depends on fully implementing the accompanying consensus strategy.\n\n【26】### Conclusions\n\n【27】The success of the rubella disease control program provides valuable insights for the continuing battle against COVID-19. Key elements to a successful program include clearly delineated goals and strategies, regular data-driven revisions to the program based on surveillance, safety, and epidemiologic data, and evaluations to identify the most appropriate vaccine(s) to achieve disease control targets. Comparative vaccine studies are necessary to help identify the most appropriate vaccine to achieve programmatic goals, especially given the increase in both assortment and supply of COVID-19 vaccines. Whereas data guide strategic decision-making in determining a global response to the COVID-19 pandemic, such other factors as vaccine confidence and equity play important roles in defining and clearly communicating the programmatic goals. Those goals, at present, include protecting individuals from severe disease, hospitalization, and death as well as reducing health system strain and limiting emergence of new variants.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "91a5ea49-55dd-4639-b761-8eb5f560a3a0", "title": "Burkholderia pseudomallei Infection in US Traveler Returning from Mexico, 2014", "text": "【0】Burkholderia pseudomallei Infection in US Traveler Returning from Mexico, 2014\n**To the Editor:** Melioidosis is an infection with clinical manifestations ranging from skin abscess to overwhelming sepsis and death. It is caused by _Burkholderia pseudomallei_ , a gram-negative, saprophytic bacillus found in soil and water. Melioidosis is highly endemic to Southeast Asia and northern Australia, and endemic to the Indian subcontinent, southern China, Hong Kong, and Taiwan .\n\n【1】The extent of melioidosis in the Western Hemisphere is unknown. However, new endemic foci have been identified in Puerto Rico and Brazil, and sporadic cases have been reported in other parts of the Caribbean, Central America, and South America . Melioidosis is rare in the United States; 0–5 cases are reported annually, and most cases occur in travelers returning from disease-endemic areas . Case clusters have been associated with extreme weather events, such as tropical storms or heavy rainfall . We report a case of melioidosis in a returned traveler from Los Cabos, Mexico, after Hurricane Odile.\n\n【2】In September 2014, a 59-year-old woman came to a hospital in Chicago, Illinois, USA, with a 4-day history of right-sided upper back and anterior chest pain, fevers, and shortness of breath. She had diabetes mellitus and well-controlled HIV infection; and had received a cadaveric renal transplant 13 months earlier. Her medications included tacrolimus, prednisone, and mycophenolate. She had traveled to Los Cabos, Mexico, 7 days before admission and was present when Hurricane Odile hit the area.\n\n【3】On admission to the hospital, her temperature was 38.5°C, and she had right chest wall tenderness. Her leukocyte count was 22.27 × 10 3  cells/μL. A computed tomography scan of the chest showed an irregular mass in the apical segment of the right upper lobe suggestive of a Pancoast tumor, with ground glass opacities and an enlarged right paratracheal lymph node.\n\n【4】She was given intravenous vancomycin, ceftriaxone, and levofloxacin. One of 2 admission blood cultures grew gram-negative rods after 30 h incubation, and antimicrobial drugs were changed to piperacillin/tazobactam. The isolate grew on blood, MacConkey, and chocolate agar when subcultured and was oxidase positive. It was susceptible to piperacillin/tazobactam, trimethoprim/sulfamethoxazole, and doxycycline but resistant to aminoglycosides and cephalosporins. Pending final identification of the bacterium, the patient was discharged on hospital day 5 and was given oral doxycycline for presumed bacteremic pneumonia.\n\n【5】The isolate was later identified as _B. pseudomallei_ by using phenotype methods and PCR analysis at the Illinois Department of Public Health (Chicago, IL, USA) and confirmed as _B. pseudomallei_ by the Centers for Disease Control and Prevention (Atlanta, GA, USA). Genetic analysis identified multilocus sequence type 92 and internal transcribed sequence type G, which is consistent with an isolate that originated in the Western Hemisphere .\n\n【6】Soon afterwards, the patient was readmitted with recurrent fevers and chest pain. Blood cultures were negative, and a computed tomography scan of the chest showed new partial cavitation of the right lung mass. Antimicrobial drugs were changed to intravenous meropenem, and immunosuppressive drugs were reduced. Oral doxycycline was added to meropenem after a third admission for recurrent fevers.\n\n【7】Although current international guidelines recommend a minimum of 10–14 days of intravenous therapy for melioidosis (intensive phase), relapse rates are high. A newer guideline recommends a longer intensive phase for some infections on the basis of results of an ongoing study in which longer courses were associated with lower relapse rates . Given this patient’s recurrent symptoms and immunosuppression, we extended her intensive treatment phase to 6 weeks, and she showed subsequent clinical improvement.\n\n【8】Transition to oral eradication-phase therapy was complicated by the patient’s allergy to trimethoprim/sulfamethoxazole, which is considered first-line therapy. Because treatment with oral amoxicillin/clavulanate or doxycycline has been associated with high relapse rates , we opted to give our patient combined amoxicillin/clavulanate and doxycycline to complete 6 months of antimicrobial drug therapy. She remains well 10.5 months after presentation.\n\n【9】Clinical diagnosis of melioidosis in nonendemic areas is challenging because signs of the disease are nonspecific and similar to those of more common diseases, such as tuberculosis. Laboratory diagnosis is also challenging. In this case, _B. pseudomallei_ grew readily in culture. However, the MicroScan Walk-Away System (Beckman Coulter Inc. Brea, CA, USA) or matrix-assisted laser desorption/ionization time of flight mass spectrometry did not provide definitive species identification. For this method to be potentially useful for identification of _B. pseudomallei_ , the database used would require optimization with addition of reference spectra for the organism and its close relatives (e.g. _B. thailandensis_ ). _B. pseudomallei_ , although different from other _Burkholderia_ spp. in its pathogenicity and epidemiology, is not easily discriminated from _B. thailandensis_ or _B. cepacia_ complex by using phenotypic tests .\n\n【10】In summary, infection with _B. pseudomallei_ should be considered in patients with pneumonia after travel to the Baja Peninsula in Mexico, and especially after an extreme weather event. Because of risk for transmission to laboratory workers and the potential for _B. pseudomallei_ to be used for bioterrorism, clinical laboratories should perform only limited work up of suspected isolates before referring them to a public health laboratory for definitive identification.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "0b6ee941-5f5f-4a94-8236-6c4814585a34", "title": "Epidemiologic and Clinical Progression of Lobomycosis among Kaiabi Indians, Brazil, 1965–2019", "text": "【0】Epidemiologic and Clinical Progression of Lobomycosis among Kaiabi Indians, Brazil, 1965–2019\nLobomycosis is a chronic and granulomatous fungal disease that affects the skin and subcutaneous tissue. Lobomycosis is classified as a neglected mycosis and is endemic to Latin America, especially the Amazon region . It has been reported in travelers who have visited that area but is rarely reported outside this region . In 1931, Jorge Lobo published a report of this disease in a nonindigenous man in Recife, Pernambuco state, Brazil . Since then, the disease has been given many different names, including lobomycosis, Jorge Lobo’s disease, keloidal blastomycosis, and lacaziosis. Of 550 cases reported worldwide, 332 (58.5%) have occurred in Brazil .\n\n【1】The etiologic agent of lobomycosis is _Lacazia loboi_ , an as-yet uncultured fungus that has phylogenetic and antigenic similarity to _Paracoccidioides brasiliensis_ , a dimorphic fungus. _L. loboi_ also could be a dimorphic fungus . Cutaneous lesions associated with this disease are polymorphic and often clinically appear as a keloid-like nodule. Other manifestations include ulcers, atrophy, tumors, macules, plaques, gummas, scleroderma, infiltrations, or scars; patients can have >1 type of lesion. Lobomycosis can be classified into localized or multicentric forms, depending on the extent of skin lesions. The disease does not usually affect general health, but ulceration or secondary infection can impair quality of life for those affected . No mucosal or systemic involvement has been reported, but the lymphatic system may be affected, and 1 case involving the testicles has been reported .\n\n【2】Many dermatologic diseases occur in indigenous people , but lobomycosis is particularly prevalent among the Kaiabi Indians, an ethnic population that lives in central Brazil. During 1961–1966, most Kaiabi migrated 400 km west from regions where they originally lived (7.3502°S, 58.1383°W) to the Xingu Indigenous Park (XIP; 11.2320°S, 53.1850°W). The migration changed their habits and brought them in contact with other indigenous groups. Most Kaiabi now live in the XIP, an indigenous reserve, providing an opportunity for long-term medical follow-up .\n\n【3】At different times, 60 cases of lobomycosis have been reported among the Kaiabi . We provide a clinical review and epidemiologic update of all registered cases of lobomycosis among the Kaiabi. In addition, we conducted field visits to the various Kaiabi indigenous villages and identified the current state of lobomycosis in each village, including 3 newly identified cases. We compared the new cases with other reported cases to determine whether other particularities are associated with lobomycosis among this population.\n\n【4】### Materials and Methods\n\n【5】##### Study Design and Area\n\n【6】We conducted a clinical-epidemiologic analysis of reported cases of lobomycosis in members of the Kaiabi ethnic group indexed during 1965–2019. We used a previous extensive review of published lobomycosis cases among the Kaiabi conducted in 1986  as the basis of this study . We analyzed medical records obtained during 1965–2019 by the Xingu Project of the Department of Preventive Medicine, Escola Paulista de Medicina, Federal University of São Paulo, São Paulo, Brazil. Each Kaiabi person included in the project had a medical record, and 56 had lobomycosis. We obtained additional unpublished information from the Xingu Project archives and field visits to indigenous villages. We performed an observational follow-up study with information from different time points during 1965–2019. We conducted 2 field visits to reexamine case-patients and identify new cases and analyzed 3 areas in which the Kaiabi have lived . When we did not have access to a medical record, we consulted previous literature reports .\n\n【7】##### Case Definition for Lobomycosis\n\n【8】We used a standard case definition for lobomycosis, which included presence of skin lesions and histopathologic evidence of the disease. Skin lesions related to lobomycosis include keloid-like nodules, plaques, papules, ulcerations, or atrophic lesions. Histopathologic evidence includes the presence of an inflammatory reaction, especially with histiocytic cell infiltrate, and detection of _L. loboi_ fungal cells by hematoxylin-eosin and Grocott’s methenamine silver staining.\n\n【9】##### Parameters Analyzed\n\n【10】We analyzed the following clinical and epidemiologic parameters: the number of lobomycosis cases among the Kaiabi; the regions in which the Kaiabi currently live and previously lived; sex distribution; age of onset; extent of the lesions; anatomic location of lesions; data concerning the progression of skin lesions; and treatment attempts. We classified cases as multicentric, skin lesions in >1 anatomic area ; or localized, lesions restricted to 1 anatomic area .\n\n【11】##### Data Analysis\n\n【12】Initial data analysis was descriptive. We also analyzed the prevalence at different times over a 54-year period. For some quantitative samples, we calculated the mean, median, minimum, and maximum values and SDs. We analyzed qualitative variables by calculating absolute and relative frequencies as percentages. We performed inferential analysis by using χ 2  test for 2 independent qualitative samples, Student _t_ \\-test for independent samples, and analysis of variance for analyzing clinical signs and symptoms and immunologic data. We considered p<0.05 statistically significant.\n\n【13】##### Ethics Considerations\n\n【14】This project was approved by Kaiabi indigenous leaders, the Indian National Foundation, and the Brazilian National Research Ethics Committee . All participants provided written informed consent before the start of the project.\n\n【15】### Results\n\n【16】##### Number of Cases among the Kaiabi\n\n【17】We identified 63 lobomycosis cases among the Kaiabi from 1965–2019. Of these, 60 were reported previously , and we identified 3 new cases. All cases were associated with the case-patients’ original habitat, and none of the reported case-patients were born at the XIP. A temporal series of cross-sectional studies of lobomycosis prevalence reported 51 cases among 400 (12.7%) Kaiabi in 1981. In 2019, a total of 26 cases of lobomycosis were known among 2,242 Kaiabi, a prevalence of 1.16% .\n\n【18】##### Regions Where the Kaiabi Live and Have Lived\n\n【19】We obtained information about the regions in north Mato Grosso state in which 41 case-patients originally resided. Twenty-two (53.7%) case-patients were from the Arinos River region, 17 (41.5%) were from the Teles Pires River region, and 2 (4.9%) had lived in both regions.\n\n【20】##### Sex Distribution and Age of Onset\n\n【21】Among the 63 case-patients, 39 (61.9%) were male and 24 (38.1%) were female. We obtained information about the age of onset for 38 case-patients. In 24 (63.2%) case-patients, lesions developed before the patient was 21 years of age; in 12 (31.6%), lesions developed at 21–40 years of age; and in 2 (5.2%), the lesions developed at >40 years of age. The earliest age of onset was 1 year of age, and the oldest was 63 years of age. The median age of onset was 18.5 years. We did not obtain data regarding patient age for 25 (39.6%) cases.\n\n【22】##### Extent and Location of Lesions\n\n【23】We obtained information on the extent and location of lobomycosis lesions in 62 case-patients, 38 males and 24 females. We noted 34 (54.8%) cases of the multicentric form  and 28 (45.2%) cases of the localized form . The multicentric form more frequently was associated with male sex ; 27 (71.1%) male patients had the multicentric form and 11 (28.9%) had the localized form. Among female patients, 7 (29.2%) had the multicentric form and 17 (70.8%) had the localized form. For 61 case-patients for whom information on lesion location was available, we noted lesions on the lower limbs in 38 cases, on the upper limbs in 32 cases, on the trunk in 24 cases, on the head and neck in 2 cases, and on the prepuce in 1 case. All patients with the multicentric form had lesions at >1 anatomic site.\n\n【24】##### Progression of Cutaneous Lesions\n\n【25】During clinical follow-up, 8/18 (44.1%) patients with the multicentric form seen after 20.2 years had new lesions and 2/17 (11.8%) patients with the localized form seen after 28.3 years had new lesions. In addition, 15/19 (78.9%) patients with the multicentric form seen after 12.3 years had lesions with periods of ulcerations and 3/8 (37.5%) patients with the localized form seen after 27.4 years had lesions with periods of ulcerations. We could not perform statistical analysis of these data because of variations in follow-up times.\n\n【26】##### Treatment Attempts\n\n【27】Some case-patients received systemic treatment with ketoconazole, itraconazole, or clofazimine, which were not successful. Among 15 cases of localized lobomycosis that received surgical treatment, 6 (40%) cases followed for 30 years had recurrence of lesions. In addition, 2 patients had carcinomatous degeneration in the form of cutaneous squamous cell carcinoma, and both died from metastasis .\n\n【28】### Discussion\n\n【29】Pereira Filho reported a case of lobomycosis among the Kaiabi in 1957 , and Nery-Guimarães reported another in 1964 . Since then, many cases in have been reported among this population in the medical literature. The high prevalence of lobomycosis in this population is a notable epidemiologic finding in medical ethnography . The disease is so prevalent that, since 1915, references have been made to a Kaiabi-associated skin disease, called _piraip_ in the Kaiabi language, a branch of the Tupi-Guarani linguistic family.\n\n【30】The prevalence of lobomycosis in the Kaiabi has been declining over time. Changes in the geographic region in which they live after many Kaiabi moved 400 km east to the XIP, changes in cultural behavior, a lack of fungal agents in the new ecosystem, and changes in the immunity of this population are some hypotheses that could explain the reduction in this prevalence. Before moving to the XIP, all Kaiabi reported with lobomycosis lived in the Arinos River or Teles Pires River regions. No new cases have occurred in Kaiabi who have only lived in XIP. The environment of the XIP is similar to Arinos River or Teles Pires River regions, but the regions have distinct watersheds.\n\n【31】Lobomycosis also occurs in some species of estuarine, inshore cetaceans ( _Tursiops truncates_ ) and offshore cetaceans ( _Sotalia guianensis_ ) . Clinical and phenotypic features of the uncultivated agent of the disease in dolphins suggested that this pathogen was the same organism, _L. loboi_ . However, molecular data suggest that the cause of cutaneous lesions in dolphins is a novel strain of _P. brasiliensis_ , _P. brasiliensis_ var. _ceti_ . Moreover, a lobomycosis-like disease among bottlenose dolphins has been reported . Bermudez et al. hypothesized that the marine environment could be a source of the fungus in a human case they reported. Some cetaceous species inhabit the rivers of the Amazon region, including _Sotalia fluviatilis_ dolphins in the Orino River in Venezuela and _Inia geoffrensis_ dolphins, known as boto, in the Amazonas River in Brazil. However, these animals do not inhabit the XIP rivers, and lobomycosis has never been identified in these animals . Curiously, 1 case in Africa and 1 in Greece were reported in persons who had never traveled to South or Central America . These cases might support water as the environmental source of this fungus, but evidence of dolphin–human transmission has been weak.\n\n【32】Of the 63 cases of lobomycosis among the Kaiabi, most patients were male, but female Kaiabi also had a high prevalence of the disease. Early reports indicated that 32% of lobomycosis case-patients among the Kaiabi were female. Among nonindigenous women and girls, the disease occurrence ranged from 10%–12% . In addition, the increased prevalence in female Kaiabi might be because they work in close contact with the environment during activities such as small-scale farming.\n\n【33】Medical reports have shown that the age of onset of the disease is typically 20–40 years of age. However, most Kaiabi case-patients began to exhibit the disease before 21 years of age. The earliest onset we noted was at 1 year of age. The susceptibility and early onset of lobomycosis among Kaiabi children can be explained by their precocious contact with wood and soil.\n\n【34】Some researchers have proposed different clinical classifications for lobomycosis. For their analysis, Baruzzi et al. adopted a modified classification, involving a localized form and multicentric form. The multicentric form is unusual and infrequent among nonindigenous populations. Other authors reported 41 cases with multicentric forms among 249 cases in nonindigenous populations . However, among the Kaiabi, the multicentric form was more common than the localized form. In analyzing the classification for sex, we noted that manifestations of the disease are quite distinct in male and female Kaiabi . The multicentric form was more frequently seen in male case-patients and the localized form was more frequent in female case-patients.\n\n【35】Cellular immunodeficiency is possible in cases of lobomycosis . Different host immune responses against the fungus or relatively high exposure to the fungal agent are possible explanations for the high incidence of lobomycosis among female case-patients. However, hormonal factors could be responsible for the protection against disease dissemination in females.\n\n【36】When reviewing anatomic locations of lesions, we noted that nonindigenous people more frequently have lobomycosis lesions on the outer ear. However, among the Kaiabi population, only 2 case-patients had lesions on the outer ear and many more lesions were noted on the limbs and truck. A possible reason is a variation in behavior. Nonindigenous groups tend to carry heavy burdens on their shoulders, but the Kaiabi tend to carry wood and other materials on their heads and backs. In addition, the Kaiabi do not wear shirts, so the skin of the upper body is exposed and could be exposed to the causative agent in materials they carry. The prepuce lesion was a rare localization found in 1 case.\n\n【37】We based our analysis on data collected by physicians who visited the XIP for >45 years. Among the Kaiabi, most of the patients with localized lobomycosis did not develop new lesions over time, but 44.1% of patients with the multicentric form had new lesions over time. Patients reported that recurrent ulcerations impaired their quality of life, and patients with the multicentric form had periods of ulcerations more frequently than those with the localized form. We did not observe any change in ulcer healing or worsening related to weather or seasons of the year. Among all lobomycosis cases worldwide, 4 reports of progression to squamous cell carcinoma have occurred; 2 were in the Kaiabi population, and both died from metastasis . Outside of the Kaiabi population, no deaths from complications of lobomycosis have been reported. In addition, among indigenous populations living in the XIP, lobomycosis greatly affects quality of life; those who have it are stigmatized, and the disease has been known as Kaiabi leprosy in the past _._\n\n【38】Treatment is a challenge, and systemic drugs are not effective . However, many treatment attempts have been reported in some case-patients. The medical literature reports some isolated positive results with itraconazole, clofazimine, leprosy multidrug therapy, or posaconazole . Other approaches, such as surgical excision, cryosurgery, and electrosurgery, should be considered in addition to drug treatment. However, no known effective treatment for lobomycosis, especially the multicentric form, has been found . Among the Kaiabi, treatment by surgery was successful in some localized cases without recurrence in a 30-year follow-up.\n\n【39】In conclusion, lobomycosis is a rare disease that affects certain geographic regions, especially in countries with low socioeconomic status , but its prevalence among the Kaiabi population in Brazil is exceptionally high. The 63 cases we identified among this population represent 11.5% of all cases of lobomycosis known in the world since the disease was identified. Our clinical and epidemiologic analysis shows cases among the Kaiabi have particular characteristics, including a high incidence of female case-patients, early onset in persons <21 years of age, a high incidence of the multicentric form in male case-patients, and a high incidence of the localized form in female case-patients. Little is known about the prevalence of lobomycosis and the effective treatment remains a challenge, especially for the multicentric form. Phylogenetic analysis of _L. loboi_ could help in the development of treatments for this fungal infection.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "e185a0f0-abf5-4412-bf50-f5197a02546c", "title": "Pulmonary Infections with Nontuberculous Mycobacteria, Catalonia, Spain, 1994–2014", "text": "【0】Pulmonary Infections with Nontuberculous Mycobacteria, Catalonia, Spain, 1994–2014\nNontuberculous mycobacteria (NTM) in low-prevalence settings have been regarded as opportunistic pathogens associated with HIV infection and chronic pulmonary disease. An overall increase in prevalence of pulmonary NTM infections in different geographic areas has been reported . The burden of pulmonary NTM infections in Spain is largely unknown because systematic reporting is not mandatory. We report secular trends in the prevalence of pulmonary infections caused by NTM over a 21-year period in a healthcare region of Catalonia, Spain.\n\n【1】### The Study\n\n【2】We performed a descriptive population-based study in 13 municipalities in the Barcelona-South Health Region of Catalonia for the period January 1994–December 2014. The study population comprised inhabitants \\> 18 years of age in this area of the health region (population at the period midpoint 600,892). During 1994–2009, specialized healthcare for this population was provided by 3 hospitals and from 2010 on by 4 hospitals. The referral hospital for the area is Bellvitge University Hospital (BUH). Throughout the study period, the BUH mycobacterial laboratory processed all samples from the participating hospitals for identification only or for culture and identification. Samples for mycobacteria isolation were processed according to standard methods . From April 1994 through March 2009, liquid cultures were processed by use of the BACTEC TB 460 radiometric method (Becton Dickinson, Sparks, MD, USA); from April 2009 on, the BACTEC MGIT 960 system (Becton Dickinson) was used. From 1994 through 2002, _Mycobacterium abscessus_ was part of the _M. chelonae-abscessus_ group; from 2003 on, it was identified as a separate species.\n\n【3】Using files of NTM isolates from the BUH laboratory, we retrospectively identified cases and included only those for which patients were \\> 18 years of age, resided in the study area, and had NTM isolated from \\> 1 respiratory specimen. We excluded patients who resided outside the study area and patients from whom _M. gordonae_ was isolated. We defined patients as having pulmonary disease if \\> 2 cultures were positive or if antimicrobial chemotherapy considered active against the species isolated had been started. The BUH ethics committee approved the study.\n\n【4】We calculated overall and annual prevalence rates with 95% CIs as the number of patients with \\> 1 isolate (isolation prevalence) and the number of patients with pulmonary disease (disease prevalence), divided by the population, according to the official census of Catalonia . We used Poisson regression models to estimate rate ratios of isolates and pulmonary disease caused by NTM per year. We constructed regression models for age groups 18–49, 50–65, and >65 years; to determine the relative prevalence of respiratory specimens, we adjusted all estimates for sex and year of isolation. Results of the regression models are expressed as relative risk with 95% CIs. We performed analyses by using SPSS version 18.0 for Windows  and Epidat  statistical packages.\n\n【5】During the 21-year period, we identified 680 patients (mean age 60.5 ± 16.6 years; 77.8% men) from whom NTM had been isolated from respiratory specimens. The overall period prevalence of patients from whom NTM was isolated was 113.2 (95% CI 105.0–122.0)/100,000 population; for pulmonary disease, prevalence was 42.8 (95% CI 37.5–48.0)/100,000 population. Prevalence rates were higher for men than for women, for isolation (180.5 and 49.1/100,000 population, respectively), and for disease (68.2 and 18.5/100,000 population, respectively) . The regression models showed a 5% annual decrease in prevalence rates of isolation among those 18–49 years of age, an 11% increase among those >65 years of age, and no significant changes among those 50–65 years of age. Male sex remained an independent factor associated with higher rates of isolation of NTM in the 3 age groups . Similarly, prevalence rates of pulmonary disease showed an annual decrease of 9% among those 18–49 years of age, an annual increase of 7% among those >65 years of age, and remained without significant changes among those 50–65 years of age. Male sex remained associated with higher rates of pulmonary disease .\n\n【6】Trends in prevalence rates differed among species of mycobacteria . Although prevalence rates rose significantly for _M. avium_ complex (MAC) (by 10% for isolation and 13% for pulmonary disease), rates of _M. kansasii_ fell (by 9% for isolation and 11% for pulmonary disease). Since 2003, _M. abscessus_ isolation increased annually by 22% and pulmonary disease increased by 24%. As for rapidly growing mycobacteria other than _M. abscessus_ , isolation but not pulmonary disease increased significantly over the study period .\n\n【7】### Conclusions\n\n【8】We found a significant increase in the prevalence of isolation of NTM from respiratory specimens in the study area. However, the trends varied according to mycobacteria species. The most common NTM causing lung disease during the first half of the study period, _M. kansasii_ , declined progressively from the early 2000s on. In contrast, MAC became the most frequently isolated NTM, and rates increased for _M. abscessus_ , eventually equaling those of _M. kansasii_ . Rapidly growing mycobacteria other than _M. abscessus_ were also increasingly isolated in the most recent years, but most were deemed colonizers .\n\n【9】We do not believe that the increased prevalence of most NTM can be explained by the 2009 implementation of the new culture system MGIT 960 because its sensitivity is equivalent to that of the BACTEC TB 460 system used previously in combination with solid media . Furthermore, changes in MAC and _M. kansasii,_ which are easily recovered in culture, cannot be attributed to the implementation of the newer culture media. The changes recorded in temporal trends can be explained, at least in part, by the dynamics of the HIV epidemic in Spain. The reduction of susceptible HIV-infected patients through improvements in antiretroviral therapy in the second half of the 1990s led to a dramatic fall in the prevalence of NTM infections among these patients. Because _M. kansasii_ was the most frequently isolated NTM among HIV-infected patients in the Barcelona-South Health Region of Catalonia, its prevalence decreased, paralleling the decrease in susceptible persons with HIV infection . This reduction in _M. kansasii_ infections is also illustrated by the reduction among persons 18–49 years of age because HIV-infected patients in Spain at that time were mostly young users of illicit intravenous drugs. Even so, the decrease among HIV patients does not explain the changes in the NTM ecology in our area, in which MAC and _M. abscessus_ seem to be filling the vacancy left by _M. kansasii_ . Although the prevalence is still low, the current distribution of mycobacteria species in this area is closer to that of other countries .\n\n【10】Our study has 2 main limitations. First, although we aimed to capture all NTM patients in the area, we may have missed some patients who sought medical care elsewhere. Second, we defined pulmonary disease patients as having had \\> 2 positive cultures or having received treatment for the NTM species isolated. Although the first criterion may have overestimated prevalence, the second may have been biased toward inclusion of patients with more severe illness and may thus have underestimated prevalence.\n\n【11】Our data add evidence for the increasing prevalence of NTM pulmonary infections in Catalonia. We also observed concurrent species-specific differences in prevalence trends in this area.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
{"seq_id": "a7b6fb2b-f01b-456b-9157-d44830d8d3b6", "title": "Coxiella burnetii Seroprevalence and Risk for Humans on Dairy Cattle Farms, the Netherlands, 2010–2011", "text": "【0】Coxiella burnetii Seroprevalence and Risk for Humans on Dairy Cattle Farms, the Netherlands, 2010–2011\nQ fever is an occupational zoonosis caused by _Coxiella burnetii_ , a gram-negative bacterium . Ruminant farmers, laboratory workers, dairy workers, and veterinarians are at particular risk for infection. Humans usually acquire Q fever by inhalation of _C. burnetii_ aerosolized from contaminated materials originating from infected animals. The primary animal reservoirs responsible for human infections are cattle, sheep, and goats, which can shed _C. burnetii_ in urine, feces, milk, and birth products. Before 2007, the seroprevalence of _C. burnetii_ antibodies within the general population of the Netherlands was 2.4%; keeping ruminants and increasing age were risk factors for seropositivity . During 2007–2009, Q fever was a major public health problem in the Netherlands; >4,000 human cases were reported . Large-scale interventions primarily targeting small ruminants were used to control the epidemic. In 2008, mandatory vaccination was conducted in a defined cluster area and later nationwide. In 2009–2010, a program was implemented to cull pregnant dairy goats and sheep on farms with _C. burnetii_ –positive animals identified through a national bulk tank milk (BTM) screening . Since then, the incidence of acute Q fever cases has diminished substantially , but chronic cases still occur . No epidemiologic associations between Q fever cases in humans and dairy cattle were identified during this epidemic, nor have any been described in other Q fever outbreaks . Nevertheless, recent reports indicate that _C. burnetii_ is widespread among Dutch dairy cattle herds (prevalence 78.6% \\[ELISA\\] or 56.6% \\[PCR\\] in BTM samples) . In 2008, seroprevalence was 16.0% in lactating cows and 1.0% in young animals .\n\n【1】_C. burnetii_ seroprevalence estimates for dairy cattle farm residents in the Netherlands are outdated, and risk factors associated with seropositivity are seldom studied. This lack of data inhibits accurate assessment of the public health risk. To inform control measures and provide advice for persons living/working on a dairy cattle farm (DCF), we conducted a cross-sectional study to investigate the seroprevalence of _C. burnetii_ antibodies in DCF residents/workers and identified participant-based and farm-based risk factors for seropositivity. The study was approved by the Medical Ethics Committee of the University Medical Centre Utrecht .\n\n【2】### Methods\n\n【3】A total of 3,000 DCFs housing \\> 50 adult dairy cows were randomly selected for possible participation in the study from a national database maintained by the Animal Health Service. In September and November 2010, information and recruitment materials were sent to 1,000 and 2,000 farms, respectively. Farms were enrolled in the study after returning a completed informed-consent form. After 4 weeks, nonresponding farms from the first mailing received a written reminder. Nonresponding farms from the second mailing did not receive a reminder because the goal of enrolling 296 farms had been reached; this number was determined on the basis of power calculations assuming 50.0% prevalence and 5.5% precision. We contacted enrolled farms by telephone to confirm participation and determine the number of participants. Dairy cattle farmers and up to 2 family members or farm employees \\> 12 years of age were eligible for participation in the study. Participants completed a questionnaire about personal characteristics (e.g. age, medical history, farm-related activities, contact with livestock and companion animals, consumption of unpasteurized dairy products, and use of personal protective equipment \\[PPE\\]) and provided a serum sample (collected by a laboratory assistant during a home visit). The farm owner or manager completed a questionnaire about herd size, cattle housing, presence of other livestock and companion animals, farm facilities, animal health, and hygiene measures. Participating farms were requested to provide one BTM sample for testing by ELISA and PCR, as described .\n\n【4】##### Serology\n\n【5】We used an immunofluorescence assay (IFA) (Focus Diagnostics, Cypress, CA, USA) to test serum samples for _C. burnetii_ phase I and II IgM and IgG. All samples were screened at an initial dilution of 1:32; those with negative results were considered negative. Positive samples were further classified as indicative of relatively recent infections (IgM phase II titer \\> 32) or past infections (IgG phase II titer \\> 32 and IgM phase II titer <32). Samples with all other outcomes were considered negative. The term relatively recent was chosen because phase II IgM is commonly found up to 1 year after infection in acute Q fever cases, but it may persist up to 3 years . Phase I and II IgG end point titers were determined for all seropositive persons. In agreement with chronic Q fever diagnostic criteria used in the Netherlands , phase I IgG titers ≥1,024 in samples in the past infection group were considered indicative of possible chronic infection.\n\n【6】##### Data Analysis\n\n【7】Participating and nonparticipating farms were compared with respect to herd size; distance to nearest _C. burnetii_ –positive BTM small-ruminant farm; goat, sheep, and cattle density; location by province and region; and degree of urbanization. We used the Mann-Whitney U test to determine differences in continuous variables and the χ 2  test to analyze categorical variables. We performed univariate logistic regression analyses to determine the main factors associated with _C. burnetii_ seropositivity among participants (p<0.20, likelihood ratio test). Potential farm-based risk factors were analyzed by univariate multilevel analyses; a unique farm identifier was used as the cluster variable. Distributions of continuous variables were studied, and variables not linearly related to the outcome variable were categorized on the basis of biological arguments (e.g. nearest _C. burnetii_ –positive BTM small-ruminant farm) or, if those were lacking, on medians (e.g. goat density within 5-km radius). Participant age was always kept in the model because of its frequent relation with seropositivity. Variables with <10.0% of participants in a risk category were excluded from further analysis. If several variables were found interrelated in the univariate analysis, only the most informative and relevant variable was selected for inclusion.\n\n【8】Risk factors determined to be significant (p<0.20) in univariate analyses of the participant-based and farm-based data were incorporated into multivariate logistic regression and multivariate multilevel analyses, respectively. Stratified multivariate analyses for participant risk factors were performed separately for farmers and for the remaining group. Starting with a full model, manual backward elimination was performed; all variables meeting the 10.0% significance level in the likelihood ratio test were kept in the final model. Two-way interactions between biologically plausible variables in the multivariate model were investigated. Last, variables included in the final multivariate model for participant-based factors and those included in the multilevel model for farm-based factors were combined in a multivariate multilevel analysis to identify the independent risk determinants for seropositivity. The final model fit was assessed by the quasi-likelihood under the independence model criterion goodness-of -fit statistic for generalized estimation equation models. SAS version 9.2 (SAS Institute, Cary, NC, USA) was used for all analyses.\n\n【9】### Results\n\n【10】##### Nonresponse Analysis\n\n【11】Of the 3,000 invited farms, 311 provided a BTM sample, and 755 persons from 309 (10.3%) farms participated in this study by providing a serum sample. A farm-based questionnaire was available for 736 (97.5%) persons from 301 farms, and a participant-based questionnaire was completed by 729 (96.6%) persons from 308 farms. Compared with nonparticipating farms, participating farms were a median of 1.5 km closer to small ruminant farms with _C. burnetii_ –positive BTM samples . In addition, the density of sheep within a 5-km radius of participating farms was higher than that for nonparticipating farms; however, the absolute difference was very small (3 sheep/km 2  ).\n\n【12】##### Seroprevalence\n\n【13】Overall _C. burnetii_ seroprevalence was 72.1% (95% CI 68.8%–75.3%), and seroprevalence among farmers, spouses, and children (12–17 years of age) was 87.2%, 54.5%, and 44.2%, respectively . Seroprevalence was univariately significantly higher among male participants, farmers, and participants \\> 35 years of age . The median duration of farm residence was 28 years (range 0–56). IgG phase II end titers were known for 534 (98.9%) of 540 _C. burnetii_ IgG phase II–seropositive participants: 32 (n =166), 64 (n = 92), 128 (n = 119), 256 (n = 106), 512 (n = 39), 1,024 (n = 10), 2,048 (n = 1), and 4,096 (n = 1). IgG phase I end titers were known for 283 (97.6%) of the 290 IgG phase I–seropositive participants: 32 (n = 105), 64 (n = 73), 128 (n = 61), 256 (n = 32), 512 (n = 10), 1,024 (n = 1), and 2,048 (n = 1). These last 2 participants, with phase I titers of 1,024 and 2,048, respectively, had lower IgG phase II titers (512 and 1,024, respectively), and according to chronic Q fever diagnostic criteria used in the Netherlands , these participants met the conditions for possible chronic Q fever infection. We could not confirm that these truly were chronic Q fever cases because clinical information (e.g. presence of vascular infection, endocardial involvement, or other clinical risk factors) was lacking.\n\n【14】Nine (1.2%) participants from 8 farms were classified as having a relatively recent infection (IgM phase II titer range 32–256). All 8 farms were within 2.5–21.2 km of the nearest _C. burnetii_ –positive BTM small-ruminant farm, and 4 of the 8 were within 3 km.\n\n【15】Four participants reported having had Q fever diagnosed by a physician during 2008–2010. On the basis of serum samples obtained at study entry, 3 of these participants had a serologic profile indicating past infection. These 3 participants lived in the southern or eastern region of the Netherlands on farms within a 3-km radius of the nearest small-ruminant farm with _C. burnetii_ –positive BTM samples. The fourth participant had no serologic evidence of a past infection and lived 14 km from the nearest small-ruminant farm with _C. burnetii_ –positive BTM samples.\n\n【16】##### Univariate Analyses at Participant and Farm Levels\n\n【17】Risk factors for seropositivity for farmers/workers and residents included age \\> 35 years; farm employment; directly performing cattle-related tasks; contact with cattle, pigs, hay, cattle food, raw milk, manure, or cattle birth products; presence of rats or mice on the farm; and growing up on a farm . Protective factors included poultry and compost contact and fully compliant use of gloves during and around calving. Farm-based risk factors included a larger herd size, farm location in the southern region, an annual peak in calving, having beef cattle on the farm, and the presence of birds in the stable. Protective factors included automatic milking, having pet cats or rabbits, and having farm clothes and boots available for professional visitors (e.g. veterinarians and feed specialists) . No relationship was found between PCR or ELISA status on the basis of BTM samples and participant seropositivity.\n\n【18】##### Multivariate and Multilevel Analyses\n\n【19】Of the 21 variables considered in the multivariate participant model, 8 were independently associated with seropositivity: age \\> 55 years; working on the farm; fully compliant use of gloves during cattle birth care; contact with pigs, cattle at other farms, poultry, or compost; and indirect contact with rats or mice . Interaction terms did not improve the model.\n\n【20】Of the 9 variables considered in the multilevel farm model, 6 were independently associated with seropositivity; larger herd size, farm location in the southern region, beef cattle on the farm, use of food concentrate, and presence of birds in the stable were risk factors, and automatic milking was a protective factor . In the combined multilevel analysis, the 12 significant factors from the multivariate participant and multilevel farm models, in addition to age, were combined in 1 model. The nonstratified model had a clearly better fit than the stratified model for farmers. Farm location within 8 km of the nearest _C. burnetii_ –positive BTM small-ruminant farm (odds ratio 2.3, 95% CI 1.2%–2.5%) was a risk factor in the final stratified multilevel model among farmers and was therefore included in the combined multilevel analysis. In the final overall model, independent risk factors were age \\> 55 years, farm employment, pig contact, larger herd size, farm location in the southern region, beef cattle on the farm, cattle contact at other farms, and presence of birds in the stable. Indirect contact with rats or mice was borderline significant . Protective factors were contact with poultry or compost, use of automatic milking, and fully compliant use of gloves during birth care. We ran an additional model by adding a protective variable (farm clothes and boots available for professional visitors), as described in Table 5 , in the farm-based and combined multilevel models. Doing so resulted in a final model with the same factors as shown in Table 7 , except that automatic milking was replaced by another protective factor (farm clothes and boots available for professional visitors) and 2 borderline significant risk factors (distance to the nearest _C. burnetii_ –positive BTM small-ruminant farm and use of by-product feedstuffs) (data not shown).\n\n【21】### Discussion\n\n【22】The overall seroprevalence of 72.1% among DCF residents, including employees, was high, indicating a considerable lifetime risk for acquiring _C. burnetii_ infection. Seroprevalence was highest among farmers (87.2%). The observed seroprevalence was similar to that determined by a study from the 1980s that showed an estimated seroprevalence of 68.0% among 94 Dutch dairy farm residents; however, laboratory methods used in that study were different than those used by us . The 72.1% seroprevalence was also compatible with recent estimates among dairy goat farms residents (68.7%) , dairy sheep farms residents (66.7%) , and livestock veterinarians (65.1%) . Estimates for these livestock-associated groups exceed the seroprevalence of 2.4% for the Dutch population during the pre-epidemic period, 2006–2007 , and the seroprevalences of 12.2% and 24.0% among persons residing in the most affected outbreak areas during the epidemic in the Netherlands .\n\n【23】Seroprevalence studies of other farmer populations, particularly dairy cattle farmers, are scarce, and, in general, it is difficult to compare international studies because of different study populations, tests, or cutoff values used. However, published seroprevalence estimates are generally lower than what we observed. A study using IFA with the same cutoff value that we used estimated a seroprevalence of 27.0% among a UK farm cohort (385 residents/workers) . Two other studies used a _C. burnetii_ phase II IgG ELISA, which is somewhat less sensitive than IFA , and obtained seroprevalence estimates of 48.8% among Northern Ireland farmers from all types of farms  and 16.0% among 262 farm residents from 105 DCFs in Germany . A seroprevalence of 3.0% was observed in 163 residents from 100 farms (most likely cattle or pig) in Denmark; the study used the same IFA that we used, but cutoff values of IgG phase I and II were higher (≥512 and \\> 1,024, respectively) . Using the same cutoff, we would obtain a comparable seroprevalence estimate of 2.7%.\n\n【24】Farm residents living in the southern part of the Netherlands were more likely to be seropositive. This was not surprising because living in the south was a risk factor for dairy goat farmers . In general, it is possible that seropositive DCF residents were partially affected by the many _C. burnetii_ –positive BTM small-ruminant farms nearby. This possibility is supported by the close distance between residential addresses of persons who had a relatively recent infection and nearby _C. burnetii_ –positive BTM small-ruminant farms. As determined on the basis of phase II IgM, 1.2% of DCF residents and 11.0% of small-ruminant dairy farm residents had a relatively recent _C. burnetii_ infection , indicating that the infection among DCF residents was generally in the more distant past. Physicians diagnosed Q fever in 0.5% of DCF residents in our study compared with 4.1% in Dutch goat farm residents ; nevertheless, to ensure a timely diagnosis and treatment, physicians should consider Q fever in patients with compatible symptoms and occupational exposure to cattle . In general, clinical illness from _C. burnetii_ infection appears to be rare among DCF residents, which fits the suggestion in the literature that cattle-acquired _C. burnetii_ infection has a milder clinical course . In other European countries and the United States, _C. burnetii_ infection is endemic in cattle and in humans occupationally exposed to cattle, but there are few clinical cases of acute Q fever . A possible explanation is that abortion in late gestation is a key sign of infection in small dairy ruminants, but this is not the case in cattle. _C. burnetii_ shedding by cattle is generally lower than that by small ruminants; concomitant and persistent shedding patterns are more frequent in clinically affected cows than healthy ones . Furthermore, sheep and goats have seasonal reproduction cycles and generally larger herd sizes, leading to huge amounts of bacteria shed during a short period. Multilocus variable-number tandem-repeat analysis genotyping has indicated that _C. burnetii_ genotypes in dairy cattle herds and dairy consumer products , except for 1 placenta sample, are clearly distinct from the predominant outbreak genotype found at Dutch small ruminant dairy farms in 2007–2009 . Upcoming research should elucidate whether the cattle strains circulating in the Netherlands and other countries are less virulent.\n\n【25】Persons \\> 55 years of age were at increased risk for seropositivity, which cannot be explained by differences in specific cattle-related tasks, frequency of cattle contact, or hours worked. It may be that host factors or continuous or regular exposure to the bacterium (booster effect) play a role that cannot adequately be assessed through a questionnaire. Full-time farm employment ( \\> 40 h/week) was a risk factor, which corresponds with a study among Dutch livestock veterinarians in which \\> 30 hours of weekly animal contact was a risk factor for infection . Full-time farm employment and working or residing on a dairy (primarily) farm were risk factors in a UK farm cohort , indicating a dose-response relationship between seropositivity and the number of working hours spent with dairy cattle or in a dairy farm environment in general.\n\n【26】We identified several cattle-related risk factors for seropositivity among cattle farm residents/staff: herd size, cattle contact at other farms, and presence of beef cattle on their own farm. A larger herd size could pose a risk because of an increased chance for _C. burnetii_ introduction or the presence of a larger susceptible population of cows; however, some farm-based risk factors associated with a large herd that were not assessed through the questionnaire might also have caused this effect . Cattle contact at other farms possibly reflects risk from exposure to _C. burnetii_ in other infected herds. The presence of beef cattle as a risk factor for DCF residents is not easily explained, but it might reflect risk from more intense birth care and, therefore, more extensive human contact with cattle and birth products.\n\n【27】Protective factors included use of automatic milking and fully compliant use of gloves during birth care. Birth products of _C. burnetii_ –infected ruminants are a source of human infections. A German study among veterinarians identified an association between increasing numbers of cattle obstetric procedures performed and seropositivity . Pig contact, indirect contact with rats/mice, and presence of wild or domesticated birds in the stable were indicated as risk factors in our study. Studies among veterinarians in the Netherlands and the United States identified swine contact as a risk factor ; however, _C. burnetii_ has not been found in pigs in the Netherlands . Rats and wild birds were identified as _C. burnetii_ reservoirs in several studies  and as reservoirs on cattle farms in the Netherlands .\n\n【28】Fully compliant use of gloves during birth care can help farmers protect themselves against _C. burnetii_ infection . Consistent use of farm boots and working clothes for professional visitors was a protective factor in our additional multilevel model. It might appear that the use of protective clothing by visitors will prevent _C. burnetii_ transmission to the visitor rather than the farmer; however, providing gloves and farm clothes for visitors indicates a state of optimal awareness on the farm with regard to communicable diseases. In addition, automatic milking of cows might reflect less direct cattle exposure, especially through avoiding contact with the udders, raw milk, manure, and genital fluids, and thus might limit the chance of infection. Statistical analyses indicated lower risk for seropositivity among farm residents exposed to poultry and to compost. We have no biologically plausible explanation for this finding, and the statistical effect might have occurred by chance. Raw milk consumption was a risk factor for seropositivity in German dairy cattle farmers . Although consumption of raw milk was not an independent risk factor in our study, 21.8% of farm residents reported daily drinking of raw milk. _C. burnetii_ exposure during nonautomatic milking could still implicate the risk of inhaling contaminated aerosols during pretreatment of the cow or during accidental raw milk ingestion.\n\n【29】The relatively low response rate of 10.4% in this study can be explained by a general lack of motivation or awareness among cattle farmers because Q fever was mainly considered a problem among small-ruminant dairy farms. A general fear of consequences resulting from possible control measures targeting the cattle sector comparable with implemented control measures for Q fever in the small-ruminant sector might also have played a role. Study results are, however, considered representative for the Dutch dairy cattle sector because participating and nonparticipating farms were generally comparable.\n\n【30】The overall _C. burnetii_ seroprevalence of 72.1% among DCF residents is high. Multilevel analysis identified several plausible risk factors (e.g. employment on a farm, larger herd size, and cattle contact at other farms). A farm location in the southern region as risk factor suggests _C. burnetii_ transmission from small-ruminant dairy farms to cattle farm residents living nearby. Use of automatic milking and fully compliant use of gloves during birth care are plausible protective factors, indicating less direct contact with cattle and, thus, a reduced chance of animal-to-human transmission. The dairy cattle sector must inform farmers about potential sources of infection. Biosecurity measures are warranted; for example, wild birds and vermin should be kept out of stables, and farmers/staff should be educated regarding the consistent use of PPE, such as wearing gloves during birth assistance and invasive procedures. Physicians should consider Q fever in the differential diagnosis for dairy cattle farmers with compatible symptoms. Future studies should more explicitly assess the clinical effect of acute and chronic Q fever in humans who live or work on DCFs.", "tags": {}, "lang": "en", "attr": {}, "ext": null, "dataset": "cdc", "batch_name": "20230822", "version": "version0"}
